<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Matteo Courthoud</title>
    <link>https://matteocourthoud.github.io/</link>
      <atom:link href="https://matteocourthoud.github.io/index.xml" rel="self" type="application/rss+xml" />
    <description>Matteo Courthoud</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><copyright>Theme edited by Matteo Courthoud© - Want to have a similar website? [Guide here](https://matteocourthoud.github.io/post/website/).</copyright><lastBuildDate>Sun, 20 Aug 2023 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://matteocourthoud.github.io/media/icon_hu03e9b3967b83fd39296ec9da5ff1ea05_201175_512x512_fill_lanczos_center_3.png</url>
      <title>Matteo Courthoud</title>
      <link>https://matteocourthoud.github.io/</link>
    </image>
    
    <item>
      <title>Data Structures</title>
      <link>https://matteocourthoud.github.io/course/data-science/01_data_structures/</link>
      <pubDate>Sun, 01 May 2022 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/course/data-science/01_data_structures/</guid>
      <description>&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import numpy as np
import pandas as pd
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For most of this course we are going to work with &lt;code&gt;pandas&lt;/code&gt; DataFrames. However, it&amp;rsquo;s important to start with an introduction to the different types of data structures available in Python, their characteristics, their differences and their comparative advantages.&lt;/p&gt;
&lt;p&gt;The most important data structures in Python are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;lists&lt;/li&gt;
&lt;li&gt;tuples&lt;/li&gt;
&lt;li&gt;sets&lt;/li&gt;
&lt;li&gt;dictionaries&lt;/li&gt;
&lt;li&gt;numpy arrays&lt;/li&gt;
&lt;li&gt;pandas DataFrames&lt;/li&gt;
&lt;li&gt;pyspark DataFrames&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;lists&#34;&gt;Lists&lt;/h2&gt;
&lt;p&gt;A &lt;strong&gt;list&lt;/strong&gt; is a mutable array data structure in Python with the following characteristics:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;can hold any type of data&lt;/li&gt;
&lt;li&gt;can hold different types of data at the same time&lt;/li&gt;
&lt;li&gt;can be modified&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We can &lt;strong&gt;generate&lt;/strong&gt; lists using square brackets.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;l = [12, &amp;quot;world&amp;quot;, [3,4,5]]
print(l)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[12, &#39;world&#39;, [3, 4, 5]]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Since lists are &lt;em&gt;ordered&lt;/em&gt;, we can &lt;strong&gt;access&lt;/strong&gt; their element by calling the position of the element in the list.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;l[0]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;12
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Since lists are &lt;em&gt;mutable&lt;/em&gt;, we can &lt;strong&gt;modify&lt;/strong&gt; their elements.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;l[0] = &#39;hello&#39;
print(l)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[&#39;hello&#39;, &#39;world&#39;, [3, 4, 5]]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can &lt;strong&gt;add&lt;/strong&gt; elements to a list using &lt;code&gt;.append()&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;l.append(23)
l
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[&#39;hello&#39;, &#39;world&#39;, [3, 4, 5], 23]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can &lt;strong&gt;remove&lt;/strong&gt; elements by calling &lt;code&gt;del&lt;/code&gt; on it.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;del l[0]
print(l)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[&#39;world&#39;, [3, 4, 5], 23]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can &lt;strong&gt;combine&lt;/strong&gt; two lists using &lt;code&gt;+&lt;/code&gt;. Note that this operation does not modify the list but generates a new one.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;l + [23]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[&#39;world&#39;, [3, 4, 5], 23, 23]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can also generate lists using &lt;strong&gt;comprehensions&lt;/strong&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;l = [n for n in range(3)]
print(l)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[0, 1, 2]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Comprehensions are a powerful tool!&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;l = [n+10 for n in range(10) if (n%2==0) and (n&amp;gt;4)]
print(l)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[16, 18]
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;tuples&#34;&gt;Tuples&lt;/h2&gt;
&lt;p&gt;A &lt;strong&gt;tuple&lt;/strong&gt; is an immutable array data structure in Python with the following characteristics:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;can hold any type of data&lt;/li&gt;
&lt;li&gt;can hold different types of data at the same time&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;can not be modified&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We can &lt;strong&gt;generate&lt;/strong&gt; tuples using curve brackets.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# A list of different data types
t = (12, &amp;quot;world&amp;quot;, [3,4,5])
print(t)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;(12, &#39;world&#39;, [3, 4, 5])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Since tuples are &lt;em&gt;ordered&lt;/em&gt;, we can &lt;strong&gt;access&lt;/strong&gt; their element by calling the position of the element in the list.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;t[0]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;12
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Since tuples are &lt;em&gt;unmutable&lt;/em&gt;, we cannot &lt;strong&gt;modify&lt;/strong&gt; their elements.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Try to modify element
try:
    t[0] = &#39;hello&#39;
except Exception as e:
    print(e)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&#39;tuple&#39; object does not support item assignment
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Try to add element
try:
    t.append(&#39;hello&#39;)
except Exception as e:
    print(e)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&#39;tuple&#39; object has no attribute &#39;append&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Try to remove element
try:
    del t[0]
except Exception as e:
    print(e)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&#39;tuple&#39; object doesn&#39;t support item deletion
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can &lt;strong&gt;combine&lt;/strong&gt; two tuples using &lt;code&gt;+&lt;/code&gt;. Note that this operation does not modify the tuple but generates a new one. Also note that to generate a 1-element tuple we need to insert a comma.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;t + (23,)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;(12, &#39;world&#39;, [3, 4, 5], 23)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can generate tuples using &lt;strong&gt;comprehensions&lt;/strong&gt;, but we need to specify it&amp;rsquo;s a &lt;code&gt;tuple&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;t = tuple(n for n in range(3))
print(t)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;(0, 1, 2)
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;sets&#34;&gt;Sets&lt;/h2&gt;
&lt;p&gt;A &lt;strong&gt;set&lt;/strong&gt; is a mutable array data structure in Python with the following characteristics:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;can only hold hashable types&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;can hold different types of data at the same time&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;cannot be modified&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;cannot contain duplicates&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We can &lt;strong&gt;generate&lt;/strong&gt; using curly brackets.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;s = {12, &amp;quot;world&amp;quot;, (3,4,5)}
print(s)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;{(3, 4, 5), 12, &#39;world&#39;}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Since sets are &lt;em&gt;unordered&lt;/em&gt; and &lt;em&gt;unindexed&lt;/em&gt;, we cannot &lt;strong&gt;access&lt;/strong&gt; single elements by calling their position.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Try to access element by position
try:
    s[0]
except Exception as e:
    print(e)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&#39;set&#39; object is not subscriptable
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Since sets are &lt;em&gt;unordered&lt;/em&gt;, we cannot &lt;strong&gt;modify&lt;/strong&gt; their elements by specifying the position.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Try to modify element
try:
    s[0] = &#39;hello&#39;
except Exception as e:
    print(e)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&#39;set&#39; object does not support item assignment
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;However, since sets are &lt;em&gt;mutable&lt;/em&gt;, we can &lt;strong&gt;add&lt;/strong&gt; elements using &lt;code&gt;.add()&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;s.add(&#39;hello&#39;)
print(s)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;{&#39;hello&#39;, (3, 4, 5), 12, &#39;world&#39;}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;However, we cannot add &lt;strong&gt;duplicates&lt;/strong&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;s.add(&#39;hello&#39;)
print(s)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;{&#39;hello&#39;, (3, 4, 5), 12, &#39;world&#39;}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can &lt;strong&gt;delete&lt;/strong&gt; elements of a set using &lt;code&gt;.remove()&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;s.remove(&#39;hello&#39;)
print(s)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;{(3, 4, 5), 12, &#39;world&#39;}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can also generate sets using &lt;strong&gt;comprehensions&lt;/strong&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;s = {n for n in range(3)}
print(s)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;{0, 1, 2}
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;dictionaries&#34;&gt;Dictionaries&lt;/h2&gt;
&lt;p&gt;A &lt;strong&gt;dictionary&lt;/strong&gt; is a mutable array data structure in Python with the following characteristics:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;can hold any type&lt;/li&gt;
&lt;li&gt;can hold different types at the same time&lt;/li&gt;
&lt;li&gt;can be modified&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;items are named&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We can &lt;strong&gt;generate&lt;/strong&gt; dictionaries using curly brackets. Since elements are indexed by keys, we have to provide one for each element.&lt;/p&gt;
&lt;p&gt;Dictionary keys can be of any &lt;strong&gt;hashable type&lt;/strong&gt;. A hashable object has a hash value that never changes during its lifetime, and it can be compared to other objects. Hashable objects that compare as equal must have the same hash value.&lt;/p&gt;
&lt;p&gt;Immutable types like strings and numbers are hashable and work well as dictionary keys. You can also use tuple objects as dictionary keys as long as they contain only hashable types themselves.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;d = {&amp;quot;first&amp;quot;: 12, 2: &amp;quot;world&amp;quot;, (3,): [3,4,5]}
print(d)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;{&#39;first&#39;: 12, 2: &#39;world&#39;, (3,): [3, 4, 5]}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Since dictionaries are indexed but not ordered, we can only &lt;strong&gt;access&lt;/strong&gt; elements by the corresponding key. If the corresponding key does not exist, we do not access any element.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;try:
    d[0]
except Exception as e:
    print(e)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;0
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can &lt;strong&gt;access all values&lt;/strong&gt; of the dictionary using &lt;code&gt;.values()&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;d.values()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;dict_values([12, &#39;world&#39;, [3, 4, 5]])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can &lt;strong&gt;access all keys&lt;/strong&gt; of the dictionary using &lt;code&gt;.keys()&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;d.keys()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;dict_keys([&#39;first&#39;, 2, (3,)])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can &lt;strong&gt;access both values and keys&lt;/strong&gt; using &lt;code&gt;.items()&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;d.items()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;dict_items([(&#39;first&#39;, 12), (2, &#39;world&#39;), ((3,), [3, 4, 5])])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This gives us a list of tuples which we can iterate on.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;[f&amp;quot;{key}: {value}&amp;quot; for key, value in d.items()]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[&#39;first: 12&#39;, &#39;2: world&#39;, &#39;(3,): [3, 4, 5]&#39;]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Since dictionaries are mutable, we can &lt;strong&gt;modify&lt;/strong&gt; their elements.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;d[&amp;quot;first&amp;quot;] = &#39;hello&#39;
print(d)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;{&#39;first&#39;: &#39;hello&#39;, 2: &#39;world&#39;, (3,): [3, 4, 5]}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If we try to modify an element that does not exist, the element is added to the dictionary.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;d[0] = &#39;hello&#39;
print(d)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;{&#39;first&#39;: &#39;hello&#39;, 2: &#39;world&#39;, (3,): [3, 4, 5], 0: &#39;hello&#39;}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can &lt;strong&gt;remove&lt;/strong&gt; elements using &lt;code&gt;del&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;del d[0]
print(d)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;{&#39;first&#39;: &#39;hello&#39;, 2: &#39;world&#39;, (3,): [3, 4, 5]}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can cannot &lt;strong&gt;combine&lt;/strong&gt; two dictionaries using &lt;code&gt;+&lt;/code&gt;. We can only add one element at the time.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;try:
    d + {&amp;quot;fourth&amp;quot;: (1,2)}
except Exception as e:
    print(e)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;unsupported operand type(s) for +: &#39;dict&#39; and &#39;dict&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can also generate dictionaries using &lt;strong&gt;comprehensions&lt;/strong&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;d = {f&amp;quot;k{n}&amp;quot;: n+1 for n in range(3)}
print(d)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;{&#39;k0&#39;: 1, &#39;k1&#39;: 2, &#39;k2&#39;: 3}
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;numpy-arrays&#34;&gt;Numpy Arrays&lt;/h2&gt;
&lt;p&gt;A &lt;strong&gt;numpy array&lt;/strong&gt; is a mutable array data structure in Python with the following characteristics:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;can hold any type of data&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;can only hold one type at the same time&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;can be modified&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;can be multi-dimensional&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;support matrix operations&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We can &lt;strong&gt;generate&lt;/strong&gt; numpy arrays are generated the &lt;code&gt;np.array()&lt;/code&gt; function on a list.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;a = np.array([1,2,3])
print(a)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1 2 3]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can make 2-dimensional arrays (a matrix) as lists of lists.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;m = np.array([[1,2,3] , [4,5,6]])
print(m)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[[1 2 3]
 [4 5 6]]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Since numpy arrays are mutable, we can &lt;strong&gt;modify&lt;/strong&gt; elements by calling the index of the numpy array.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;m[0,0] = 89
print(m)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[[89  2  3]
 [ 4  5  6]]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can check the &lt;strong&gt;shape&lt;/strong&gt; of a numpy array using &lt;code&gt;.shape&lt;/code&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;m.shape
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;(2, 3)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can &lt;strong&gt;expand dimensions&lt;/strong&gt; of a numpy array using &lt;code&gt;.expand_dims()&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;a = np.expand_dims(a,1)
print(a)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[[1]
 [2]
 [3]]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can &lt;strong&gt;transpose&lt;/strong&gt; matrices using &lt;code&gt;.T&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;a = a.T
print(a)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[[1 2 3]]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We &lt;strong&gt;add&lt;/strong&gt; elements to a numpy array using &lt;code&gt;np.concatenate()&lt;/code&gt;. All elements must have the same number of dimensions and must have the same number of elements along the concatenation axis.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;m = np.concatenate((m, a), axis=0)
print(m)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[[89  2  3]
 [ 4  5  6]
 [ 1  2  3]]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We cannot &lt;strong&gt;remove&lt;/strong&gt; elements of numpy arrays.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;try:
    del a[0]
except Exception as e:
    print(e)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;cannot delete array elements
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can do &lt;strong&gt;matrix operations&lt;/strong&gt; between numpy arrays. For example, we can do multiplication with &lt;code&gt;@&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;a @ m
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;array([[100,  18,  24]])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If we use &lt;code&gt;*&lt;/code&gt; we get dot (or element-wise) multiplication instead.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;a * m
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;array([[89,  4,  9],
       [ 4, 10, 18],
       [ 1,  4,  9]])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There is a wide array of functions available in numpy. For example &lt;code&gt;np.invert()&lt;/code&gt; inverts a squared matrix.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;np.invert(m)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;array([[-90,  -3,  -4],
       [ -5,  -6,  -7],
       [ -2,  -3,  -4]])
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;comparison&#34;&gt;Comparison&lt;/h2&gt;
&lt;p&gt;Which data structure should you use and why? Let&amp;rsquo;s compare different data types&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;K = 100_000
l = [n for n in range(K)]
t = tuple(n for n in range(K))
s = {n for n in range(K)}
a = np.array([n for n in range(K)])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Size&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Which data type is more efficient?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import sys

def compare_size(list_objects):
    for o in list_objects:
        print(f&amp;quot;Size of {type(o)}: {sys.getsizeof(o)}&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;compare_size([l, t, s, a])
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Size of &amp;lt;class &#39;list&#39;&amp;gt;: 800984
Size of &amp;lt;class &#39;tuple&#39;&amp;gt;: 800040
Size of &amp;lt;class &#39;set&#39;&amp;gt;: 4194520
Size of &amp;lt;class &#39;numpy.ndarray&#39;&amp;gt;: 800104
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Size is very similar for lists, tuples and numpy arrays.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Speed&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Which data type is faster?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import time

def compare_time(list_objects):
    for o in list_objects:
        start = time.time()
        [x**2 for x in o]
        end = time.time()
        print(f&amp;quot;Time of {type(o)}: {end - start}&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;compare_time([l, t, s, a])
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Time of &amp;lt;class &#39;list&#39;&amp;gt;: 0.021067142486572266
Time of &amp;lt;class &#39;tuple&#39;&amp;gt;: 0.02071404457092285
Time of &amp;lt;class &#39;set&#39;&amp;gt;: 0.021012067794799805
Time of &amp;lt;class &#39;numpy.ndarray&#39;&amp;gt;: 0.010493993759155273
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Numpy arrays are faster at math operations.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Linear Regression</title>
      <link>https://matteocourthoud.github.io/course/ml-econ/01_regression/</link>
      <pubDate>Wed, 09 Mar 2022 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/course/ml-econ/01_regression/</guid>
      <description>&lt;p&gt;This chapter follows closely Chapter 3 of &lt;a href=&#34;https://hastie.su.domains/ISLR2/ISLRv2_website.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;An Introduction to Statistical Learning&lt;/a&gt; by James, Witten, Tibshirani, Friedman.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Remove warnings
import warnings
warnings.filterwarnings(&#39;ignore&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Import everything
import pandas as pd
import numpy as np
import seaborn as sns
import statsmodels.api as sm

from sklearn.linear_model import LinearRegression
from numpy.linalg import inv
from numpy.random import normal as rnorm
from statsmodels.stats.outliers_influence import OLSInfluence
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Setup matplotlib for graphs
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import axes3d

# Set global parameters
%matplotlib inline
plt.style.use(&#39;seaborn-white&#39;)
plt.rcParams[&#39;lines.linewidth&#39;] = 3
plt.rcParams[&#39;figure.figsize&#39;] = (10,6)
plt.rcParams[&#39;figure.titlesize&#39;] = 20
plt.rcParams[&#39;axes.titlesize&#39;] = 18
plt.rcParams[&#39;axes.labelsize&#39;] = 14
plt.rcParams[&#39;legend.fontsize&#39;] = 14
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can inspect all the available global parameter options &lt;a href=&#34;https://matplotlib.org/3.3.2/tutorials/introductory/customizing.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;11-simple-linear-regression&#34;&gt;1.1 Simple Linear Regression&lt;/h2&gt;
&lt;p&gt;First, let&amp;rsquo;s load the Advertising dataset. It contains information on displays sales (in thousands of units) for a particular product and a list of advertising budgets (in thousands of dollars) for TV, radio, and newspaper media.&lt;/p&gt;
&lt;p&gt;We open the dataset using the &lt;code&gt;pandas&lt;/code&gt; library which is &lt;strong&gt;the&lt;/strong&gt; library for handling datasets and data analysis in Python.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Advertisement spending data
advertising = pd.read_csv(&#39;data/Advertising.csv&#39;, usecols=[1,2,3,4])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s have a look at the content. We can have a glance at the first rows by using the function &lt;code&gt;head&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Preview of the data
advertising.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;TV&lt;/th&gt;
      &lt;th&gt;Radio&lt;/th&gt;
      &lt;th&gt;Newspaper&lt;/th&gt;
      &lt;th&gt;Sales&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;230.1&lt;/td&gt;
      &lt;td&gt;37.8&lt;/td&gt;
      &lt;td&gt;69.2&lt;/td&gt;
      &lt;td&gt;22.1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;44.5&lt;/td&gt;
      &lt;td&gt;39.3&lt;/td&gt;
      &lt;td&gt;45.1&lt;/td&gt;
      &lt;td&gt;10.4&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;17.2&lt;/td&gt;
      &lt;td&gt;45.9&lt;/td&gt;
      &lt;td&gt;69.3&lt;/td&gt;
      &lt;td&gt;9.3&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;151.5&lt;/td&gt;
      &lt;td&gt;41.3&lt;/td&gt;
      &lt;td&gt;58.5&lt;/td&gt;
      &lt;td&gt;18.5&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;180.8&lt;/td&gt;
      &lt;td&gt;10.8&lt;/td&gt;
      &lt;td&gt;58.4&lt;/td&gt;
      &lt;td&gt;12.9&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;We can have a general overview of the dataset using the function &lt;code&gt;info&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Overview of all variables
advertising.info()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;class &#39;pandas.core.frame.DataFrame&#39;&amp;gt;
RangeIndex: 200 entries, 0 to 199
Data columns (total 4 columns):
 #   Column     Non-Null Count  Dtype  
---  ------     --------------  -----  
 0   TV         200 non-null    float64
 1   Radio      200 non-null    float64
 2   Newspaper  200 non-null    float64
 3   Sales      200 non-null    float64
dtypes: float64(4)
memory usage: 6.4 KB
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can have more information on the single variables using the function &lt;code&gt;describe&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Summary of all variables
advertising.describe()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;TV&lt;/th&gt;
      &lt;th&gt;Radio&lt;/th&gt;
      &lt;th&gt;Newspaper&lt;/th&gt;
      &lt;th&gt;Sales&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;count&lt;/th&gt;
      &lt;td&gt;200.000000&lt;/td&gt;
      &lt;td&gt;200.000000&lt;/td&gt;
      &lt;td&gt;200.000000&lt;/td&gt;
      &lt;td&gt;200.000000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;mean&lt;/th&gt;
      &lt;td&gt;147.042500&lt;/td&gt;
      &lt;td&gt;23.264000&lt;/td&gt;
      &lt;td&gt;30.554000&lt;/td&gt;
      &lt;td&gt;14.022500&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;std&lt;/th&gt;
      &lt;td&gt;85.854236&lt;/td&gt;
      &lt;td&gt;14.846809&lt;/td&gt;
      &lt;td&gt;21.778621&lt;/td&gt;
      &lt;td&gt;5.217457&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;min&lt;/th&gt;
      &lt;td&gt;0.700000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.300000&lt;/td&gt;
      &lt;td&gt;1.600000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;25%&lt;/th&gt;
      &lt;td&gt;74.375000&lt;/td&gt;
      &lt;td&gt;9.975000&lt;/td&gt;
      &lt;td&gt;12.750000&lt;/td&gt;
      &lt;td&gt;10.375000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;50%&lt;/th&gt;
      &lt;td&gt;149.750000&lt;/td&gt;
      &lt;td&gt;22.900000&lt;/td&gt;
      &lt;td&gt;25.750000&lt;/td&gt;
      &lt;td&gt;12.900000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;75%&lt;/th&gt;
      &lt;td&gt;218.825000&lt;/td&gt;
      &lt;td&gt;36.525000&lt;/td&gt;
      &lt;td&gt;45.100000&lt;/td&gt;
      &lt;td&gt;17.400000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;max&lt;/th&gt;
      &lt;td&gt;296.400000&lt;/td&gt;
      &lt;td&gt;49.600000&lt;/td&gt;
      &lt;td&gt;114.000000&lt;/td&gt;
      &lt;td&gt;27.000000&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;If you just want to call a variable in &lt;code&gt;pandas&lt;/code&gt;, you have 3 options:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;use squared brackets as if the varaible was a component of a dictionary&lt;/li&gt;
&lt;li&gt;use or dot subscripts as if the variable was a function of the data&lt;/li&gt;
&lt;li&gt;use the &lt;code&gt;loc&lt;/code&gt; function (best practice)&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# 1. Brackets
advertising[&#39;TV&#39;]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;0      230.1
1       44.5
2       17.2
3      151.5
4      180.8
       ...  
195     38.2
196     94.2
197    177.0
198    283.6
199    232.1
Name: TV, Length: 200, dtype: float64
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# 2. Brackets
advertising.TV
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;0      230.1
1       44.5
2       17.2
3      151.5
4      180.8
       ...  
195     38.2
196     94.2
197    177.0
198    283.6
199    232.1
Name: TV, Length: 200, dtype: float64
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# The loc function
advertising.loc[:,&#39;TV&#39;]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;0      230.1
1       44.5
2       17.2
3      151.5
4      180.8
       ...  
195     38.2
196     94.2
197    177.0
198    283.6
199    232.1
Name: TV, Length: 200, dtype: float64
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that the &lt;code&gt;loc&lt;/code&gt; function is more powerful and is generally used to subset lines and columns.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Select multiple columns and subset of rows
advertising.loc[0:5,[&#39;Sales&#39;,&#39;TV&#39;]]
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;Sales&lt;/th&gt;
      &lt;th&gt;TV&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;22.1&lt;/td&gt;
      &lt;td&gt;230.1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;10.4&lt;/td&gt;
      &lt;td&gt;44.5&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;9.3&lt;/td&gt;
      &lt;td&gt;17.2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;18.5&lt;/td&gt;
      &lt;td&gt;151.5&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;12.9&lt;/td&gt;
      &lt;td&gt;180.8&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;5&lt;/th&gt;
      &lt;td&gt;7.2&lt;/td&gt;
      &lt;td&gt;8.7&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;Suppose we are interested in the (linear) relationship between sales and tv advertisement.&lt;/p&gt;
&lt;p&gt;$$
sales ≈ \beta_0 + \beta_1 TV.
$$&lt;/p&gt;
&lt;p&gt;How are the two two variables related? Visual inspection: scatterplot.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Figure 3.1
def make_fig_3_1a():
    
    # Init figure
    fig, ax = plt.subplots(1,1)
    ax.set_title(&#39;Figure 3.1&#39;);

    # Plot scatter and best fit line
    sns.regplot(x=advertising.TV, y=advertising.Sales, ax=ax, order=1, ci=None, scatter_kws={&#39;color&#39;:&#39;r&#39;, &#39;s&#39;:20})
    ax.set_xlim(-10,310); ax.set_ylim(ymin=0)
    ax.legend([&#39;Least Squares Fit&#39;,&#39;Data&#39;]);
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;make_fig_3_1a()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/01_regression_23_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;estimating-the-coefficients&#34;&gt;Estimating the Coefficients&lt;/h3&gt;
&lt;p&gt;How do we estimate the best fit line? Minimize the Residual Sum of Squares (RSS).&lt;/p&gt;
&lt;p&gt;First, suppose we have a dataset $\mathcal D = {x_i, y_i}_{i=1}^N$. We define the prediction of $y$ based on $X$ as&lt;/p&gt;
&lt;p&gt;$$
\hat y_i = \hat \beta X_i
$$&lt;/p&gt;
&lt;p&gt;The residuals are the unexplained component of $y$&lt;/p&gt;
&lt;p&gt;$$
e_i = y_i - \hat y_i
$$&lt;/p&gt;
&lt;p&gt;Our objective function (to be minimized) is the Resdual Sum of Squares (RSS):&lt;/p&gt;
&lt;p&gt;$$
RSS := \sum_{n=1}^N e_i^2
$$&lt;/p&gt;
&lt;p&gt;And the OLS coefficient is defined as its minimizer:&lt;/p&gt;
&lt;p&gt;$$
\hat \beta_{OLS} := \arg\min_{\beta} \sum_{n=1}^N e_i^2 = \arg\min_{\beta} \sum_{n=1}^N (y_i - X_i \beta)^2
$$&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s use the &lt;code&gt;sklearn&lt;/code&gt; library to fit a linear regression model of &lt;em&gt;Sales&lt;/em&gt; on &lt;em&gt;TV&lt;/em&gt; advertisement.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Define X and y
X = advertising.TV.values.reshape(-1,1)
y = advertising.Sales.values

# Fit linear regressions
reg = LinearRegression().fit(X,y)
print(reg.intercept_)
print(reg.coef_)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;7.0325935491276885
[0.04753664]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can visualize the residuals as the vertical distances between the data and the prediction line. The objective function RSS is the sum of the squares of the lengths of vertical lines.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Compute predicted values
y_hat = reg.predict(X)

# Figure 3.1
def make_figure_3_1b():

    # Init figure
    fig, ax = plt.subplots(1,1)
    ax.set_title(&#39;Figure 3.1&#39;);

    # Add residuals
    sns.regplot(x=advertising.TV, y=advertising.Sales, ax=ax, order=1, ci=None, scatter_kws={&#39;color&#39;:&#39;r&#39;, &#39;s&#39;:20})
    ax.vlines(X, np.minimum(y,y_hat), np.maximum(y,y_hat), linestyle=&#39;--&#39;, color=&#39;k&#39;, alpha=0.5, linewidth=1)
    plt.legend([&#39;Least Squares Fit&#39;,&#39;Data&#39;,&#39;Residuals&#39;]);
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;make_figure_3_1b()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/01_regression_29_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;The closed form solution in matrix algebra is
$$
\hat \beta_{OLS} = (X&amp;rsquo;X)^{-1}(X&amp;rsquo;y)
$$&lt;/p&gt;
&lt;p&gt;Python has a series of shortcuts to make the syntax less verbose. However, we still need to import the &lt;code&gt;inv&lt;/code&gt; function from &lt;code&gt;numpy&lt;/code&gt;. In Matlab it would be &lt;code&gt;(X&#39;*X)^{-1}*(X&#39;*y)&lt;/code&gt;, almost literal.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Compute OLS coefficient with matrix algebra
beta = inv(X.T @ X) @ X.T @ y

print(beta)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[0.08324961]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Why is the result different?&lt;/p&gt;
&lt;p&gt;We are missing one coefficient: the intercept. Our regression now looks like this&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# New figure 1
def make_new_figure_1():

    # Init figure
    fig, ax = plt.subplots(1,1)
    fig.suptitle(&#39;Role of the Intercept&#39;)

    # Add new line on the previous plot
    sns.regplot(x=advertising.TV, y=advertising.Sales, ax=ax, order=1, ci=None, scatter_kws={&#39;color&#39;:&#39;r&#39;, &#39;s&#39;:10})
    ax.plot(X, beta*X, color=&#39;g&#39;)
    plt.xlim(-10,310); plt.ylim(ymin=0);
    ax.legend([&#39;With Intercept&#39;, &#39;Without intercept&#39;]);
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;make_new_figure_1()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/01_regression_34_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;How do we insert an intercept using matrix algebra? We add a column of ones.&lt;/p&gt;
&lt;p&gt;$$
X_1 = [\boldsymbol{1}, X]
$$&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# How to insert intercept? Add constant: column of ones
one = np.ones(np.shape(X))
X1 = np.concatenate([one,X],axis=1)

print(np.shape(X1))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;(200, 2)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we compute again the coefficients as before.&lt;/p&gt;
&lt;p&gt;$$
\hat \beta_{OLS} = (X_1&amp;rsquo;X_1)^{-1}(X_1&amp;rsquo;y)
$$&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Compute beta OLS with intercept
beta_OLS = inv(X1.T @ X1) @ X1.T @ y

print(beta_OLS)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[7.03259355 0.04753664]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we have indeed obtained the same exact coefficients.&lt;/p&gt;
&lt;p&gt;What does minimizing the Residual Sum of Squares means in practice? How does the objective function looks like?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from sklearn.preprocessing import scale

# First, scale the data
X = scale(advertising.TV, with_mean=True, with_std=False).reshape(-1,1)
y = advertising.Sales
regr = LinearRegression().fit(X,y)

# Create grid coordinates for plotting
B0 = np.linspace(regr.intercept_-2, regr.intercept_+2, 50)
B1 = np.linspace(regr.coef_-0.02, regr.coef_+0.02, 50)
xx, yy = np.meshgrid(B0, B1, indexing=&#39;xy&#39;)
Z = np.zeros((B0.size,B1.size))

# Calculate Z-values (RSS) based on grid of coefficients
for (i,j),v in np.ndenumerate(Z):
    Z[i,j] =((y - (xx[i,j]+X.ravel()*yy[i,j]))**2).sum()/1000

# Minimized RSS
min_RSS = r&#39;$\beta_0$, $\beta_1$ for minimized RSS&#39;
min_rss = np.sum((regr.intercept_+regr.coef_*X - y.values.reshape(-1,1))**2)/1000
min_rss
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;2.1025305831313514
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Figure 3.2 - Regression coefficients - RSS
def make_fig_3_2():
    fig = plt.figure(figsize=(15,6))
    fig.suptitle(&#39;RSS - Regression coefficients&#39;)

    ax1 = fig.add_subplot(121)
    ax2 = fig.add_subplot(122, projection=&#39;3d&#39;)

    # Left plot
    CS = ax1.contour(xx, yy, Z, cmap=plt.cm.Set1, levels=[2.15, 2.2, 2.3, 2.5, 3])
    ax1.scatter(regr.intercept_, regr.coef_[0], c=&#39;r&#39;, label=min_RSS)
    ax1.clabel(CS, inline=True, fontsize=10, fmt=&#39;%1.1f&#39;)

    # Right plot
    ax2.plot_surface(xx, yy, Z, rstride=3, cstride=3, alpha=0.3)
    ax2.contour(xx, yy, Z, zdir=&#39;z&#39;, offset=Z.min(), cmap=plt.cm.Set1,
                alpha=0.4, levels=[2.15, 2.2, 2.3, 2.5, 3])
    ax2.scatter3D(regr.intercept_, regr.coef_[0], min_rss, c=&#39;r&#39;, label=min_RSS)
    ax2.set_zlabel(&#39;RSS&#39;)
    ax2.set_zlim(Z.min(),Z.max())
    ax2.set_ylim(0.02,0.07)

    # settings common to both plots
    for ax in fig.axes:
        ax.set_xlabel(r&#39;$\beta_0$&#39;)
        ax.set_ylabel(r&#39;$\beta_1$&#39;)
        ax.set_yticks([0.03,0.04,0.05,0.06])
        ax.legend()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;make_fig_3_2()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/01_regression_43_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;assessing-the-accuracy-of-the-coefficient-estimates&#34;&gt;Assessing the Accuracy of the Coefficient Estimates&lt;/h3&gt;
&lt;p&gt;How accurate is our regression fit? Suppose we were drawing different (small) samples from the same data generating process, for example&lt;/p&gt;
&lt;p&gt;$$
y_i = 2 + 3x_i + \varepsilon_i
$$&lt;/p&gt;
&lt;p&gt;where $x_i \sim N(0,1)$ and $\varepsilon \sim N(0,3)$.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Init
N = 30;    # Sample size
K = 100;   # Number of simulations
beta_hat = np.zeros((2,K))
x = np.linspace(-4,4,N)

# Set seed
np.random.seed(1)

# K simulations
for i in range(K):
    # Simulate data
    x1 = np.random.normal(0,1,N).reshape([-1,1])
    X = np.concatenate([np.ones(np.shape(x1)), x1], axis=1)
    epsilon = np.random.normal(0,5,N)
    beta0 = [2,3]
    y = X @ beta0 + epsilon

    # Estimate coefficients
    beta_hat[:,i] = inv(X.T @ X) @ X.T @ y
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# new figure 2
def make_new_fig_2():

    # Init figure
    fig, ax = plt.subplots(1,1)
    
    for i in range(K):
        # Plot line
        ax.plot(x, beta_hat[0,i] + x*beta_hat[1,i], color=&#39;blue&#39;, alpha=0.2, linewidth=1)
        if i==K-1:
            ax.plot(x, beta_hat[0,i] + x*beta_hat[1,i], color=&#39;blue&#39;, alpha=0.2, linewidth=1, label=&#39;Estimated Lines&#39;)

    # Plot true line
    ax.plot(x, 2 + 3*x, color=&#39;red&#39;, linewidth=3, label=&#39;True Line&#39;);
    ax.set_xlabel(&#39;X&#39;); ax.set_ylabel(&#39;Y&#39;); ax.legend();
    ax.set_xlim(-4,4);
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;make_new_fig_2()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/01_regression_48_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;regplot&lt;/code&gt; command lets us automatically draw confidence intervals. Let&amp;rsquo;s draw the last simulated dataset with conficence intervals.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;fig, ax = plt.subplots(1,1)

# Plot last simulation scatterplot with confidence interval
sns.regplot(x=x1, y=y, ax=ax, order=1, scatter_kws={&#39;color&#39;:&#39;r&#39;, &#39;s&#39;:20});
ax.set_xlabel(&#39;X&#39;); ax.set_ylabel(&#39;Y&#39;); 
ax.legend([&#39;Best fit&#39;,&#39;Data&#39;, &#39;Confidence Intervals&#39;]);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/01_regression_50_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;As we can see, depending on the sample, we get a different estimate of the linear relationship between $x$ and $y$. However, there estimates are on average correct. Indeed, we can visualize their distribution.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Plot distribution of coefficients
plot = sns.jointplot(x=beta_hat[0,:], y=beta_hat[1,:], color=&#39;red&#39;, edgecolor=&amp;quot;white&amp;quot;);
plot.ax_joint.axvline(x=2);
plot.ax_joint.axhline(y=3);
plot.set_axis_labels(&#39;beta_0&#39;, &#39;beta_1&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/01_regression_52_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;How do we compute confidence intervals by hand?&lt;/p&gt;
&lt;p&gt;$$
Var(\hat \beta_{OLS}) = \sigma^2 (X&amp;rsquo;X)^{-1}
$$&lt;/p&gt;
&lt;p&gt;where $\sigma^2 = Var(\varepsilon)$. Since we do not know $Var(\varepsilon)$, we estimate it as $Var(e)$.&lt;/p&gt;
&lt;p&gt;$$
\hat Var(\hat \beta_{OLS}) = \hat \sigma^2 (X&amp;rsquo;X)^{-1}
$$&lt;/p&gt;
&lt;p&gt;If we assume the standard errors are normally distributed (or we apply the Central Limit Theorem, assuming $n \to \infty$), a 95% confidence interval for the OLS coefficient takes the form&lt;/p&gt;
&lt;p&gt;$$
CI(\hat \beta_{OLS}) = \Big[ \hat \beta_{OLS} - 1.96 \times \hat SE(\hat \beta_{OLS}) \ , \ \hat \beta_{OLS} + 1.96 \times \hat SE(\hat \beta_{OLS}) \Big]
$$&lt;/p&gt;
&lt;p&gt;where $\hat SE(\hat \beta_{OLS}) = \sqrt{\hat Var(\hat \beta_{OLS})}$.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Import again X and y from example above
X = advertising.TV.values.reshape(-1,1)
X1 = np.concatenate([np.ones(np.shape(X)), X], axis=1)
y = advertising.Sales.values

# Compute residual variance
X_hat = X1 @ beta_OLS
e = y - X_hat
sigma_hat = np.var(e)
var_beta_OLS = sigma_hat * inv(X1.T @ X1)

# Take elements on the diagonal and square them
std_beta_OLS = [var_beta_OLS[0,0]**.5, var_beta_OLS[1,1]**.5]

print(std_beta_OLS)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[0.4555479737400674, 0.0026771203500466564]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;statsmodels&lt;/code&gt; library allows us to produce nice tables with parameter estimates and standard errors.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Table 3.1 &amp;amp; 3.2
est = sm.OLS.from_formula(&#39;Sales ~ TV&#39;, advertising).fit()
est.summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
      &lt;td&gt;&lt;/td&gt;         &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt; &lt;td&gt;    7.0326&lt;/td&gt; &lt;td&gt;    0.458&lt;/td&gt; &lt;td&gt;   15.360&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    6.130&lt;/td&gt; &lt;td&gt;    7.935&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;TV&lt;/th&gt;        &lt;td&gt;    0.0475&lt;/td&gt; &lt;td&gt;    0.003&lt;/td&gt; &lt;td&gt;   17.668&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    0.042&lt;/td&gt; &lt;td&gt;    0.053&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;h3 id=&#34;assessing-the-accuracy-of-the-model&#34;&gt;Assessing the Accuracy of the Model&lt;/h3&gt;
&lt;p&gt;What metrics can we use to assess whether the model is a good model, in terms of capturing the relationship between the variables?&lt;/p&gt;
&lt;p&gt;First, we can compute our objective function: the Residual Sum of Squares (&lt;em&gt;RSS&lt;/em&gt;). Lower values of our objective function imply that we got a better fit.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# RSS with regression coefficients
RSS = sum(e**2)

print(RSS)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;2102.530583131351
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The problem with &lt;em&gt;RSS&lt;/em&gt; as a metric is that it&amp;rsquo;s hard to compare different regressions since its scale depends on the magnitude of the variables.&lt;/p&gt;
&lt;p&gt;One measure of fit that does not depend on the magnitude of the variables is $R^2$: the percentage of our explanatory variable explained by the model&lt;/p&gt;
&lt;p&gt;$$
R^2 = 1 - \frac{\text{RSS}}{\text{TSS}}
$$&lt;/p&gt;
&lt;p&gt;where&lt;/p&gt;
&lt;p&gt;$$
TSS = \sum_{i=1}^N (y_i - \bar y)^2
$$&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# TSS
TSS = sum( (y-np.mean(y))**2 )

# R2
R2 = 1 - RSS/TSS

print(R2)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;0.6118750508500709
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Can the $R^2$ metric be negative? When?&lt;/p&gt;
&lt;h2 id=&#34;22-multiple-linear-regression&#34;&gt;2.2 Multiple Linear Regression&lt;/h2&gt;
&lt;p&gt;What if we have more than one explanatory variable? Spoiler: we already did, but one was a constant.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s have a look at the regression of &lt;em&gt;Sales&lt;/em&gt; on &lt;em&gt;Radio&lt;/em&gt; and &lt;em&gt;TV&lt;/em&gt; advertisement expenditure separately.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Table 3.3 (1)
est = sm.OLS.from_formula(&#39;Sales ~ Radio&#39;, advertising).fit()
est.summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
      &lt;td&gt;&lt;/td&gt;         &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt; &lt;td&gt;    9.3116&lt;/td&gt; &lt;td&gt;    0.563&lt;/td&gt; &lt;td&gt;   16.542&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    8.202&lt;/td&gt; &lt;td&gt;   10.422&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Radio&lt;/th&gt;     &lt;td&gt;    0.2025&lt;/td&gt; &lt;td&gt;    0.020&lt;/td&gt; &lt;td&gt;    9.921&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    0.162&lt;/td&gt; &lt;td&gt;    0.243&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Table 3.3 (2)
est = sm.OLS.from_formula(&#39;Sales ~ Newspaper&#39;, advertising).fit()
est.summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
      &lt;td&gt;&lt;/td&gt;         &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt; &lt;td&gt;   12.3514&lt;/td&gt; &lt;td&gt;    0.621&lt;/td&gt; &lt;td&gt;   19.876&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;   11.126&lt;/td&gt; &lt;td&gt;   13.577&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Newspaper&lt;/th&gt; &lt;td&gt;    0.0547&lt;/td&gt; &lt;td&gt;    0.017&lt;/td&gt; &lt;td&gt;    3.300&lt;/td&gt; &lt;td&gt; 0.001&lt;/td&gt; &lt;td&gt;    0.022&lt;/td&gt; &lt;td&gt;    0.087&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;It seems that both Radio and Newspapers are positively correlated with &lt;em&gt;Sales&lt;/em&gt;. Why don&amp;rsquo;t we estimate a unique regression with both dependent variables?&lt;/p&gt;
&lt;h3 id=&#34;estimating-the-regression-coefficients&#34;&gt;Estimating the Regression Coefficients&lt;/h3&gt;
&lt;p&gt;Suppose now we enrich our previous model adding all different forms of advertisement:&lt;/p&gt;
&lt;p&gt;$$
\text{Sales} = \beta_0 + \beta_1 \text{TV} + \beta_2 \text{Radio} + \beta_3 \text{Newspaper} + \varepsilon
$$&lt;/p&gt;
&lt;p&gt;We estimate it using the &lt;code&gt;statsmodels&lt;/code&gt; &lt;code&gt;ols&lt;/code&gt; library.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Table 3.4
est = sm.OLS.from_formula(&#39;Sales ~ TV + Radio + Newspaper&#39;, advertising).fit()
est.summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
      &lt;td&gt;&lt;/td&gt;         &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt; &lt;td&gt;    2.9389&lt;/td&gt; &lt;td&gt;    0.312&lt;/td&gt; &lt;td&gt;    9.422&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    2.324&lt;/td&gt; &lt;td&gt;    3.554&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;TV&lt;/th&gt;        &lt;td&gt;    0.0458&lt;/td&gt; &lt;td&gt;    0.001&lt;/td&gt; &lt;td&gt;   32.809&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    0.043&lt;/td&gt; &lt;td&gt;    0.049&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Radio&lt;/th&gt;     &lt;td&gt;    0.1885&lt;/td&gt; &lt;td&gt;    0.009&lt;/td&gt; &lt;td&gt;   21.893&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    0.172&lt;/td&gt; &lt;td&gt;    0.206&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Newspaper&lt;/th&gt; &lt;td&gt;   -0.0010&lt;/td&gt; &lt;td&gt;    0.006&lt;/td&gt; &lt;td&gt;   -0.177&lt;/td&gt; &lt;td&gt; 0.860&lt;/td&gt; &lt;td&gt;   -0.013&lt;/td&gt; &lt;td&gt;    0.011&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;Why now it seems that there is no relationship between Sales and Newspaper while the univariate regression told us the opposite?&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s explore the correlation between those variables.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Table 3.5 - Correlation Matrix
advertising.corr()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;TV&lt;/th&gt;
      &lt;th&gt;Radio&lt;/th&gt;
      &lt;th&gt;Newspaper&lt;/th&gt;
      &lt;th&gt;Sales&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;TV&lt;/th&gt;
      &lt;td&gt;1.000000&lt;/td&gt;
      &lt;td&gt;0.054809&lt;/td&gt;
      &lt;td&gt;0.056648&lt;/td&gt;
      &lt;td&gt;0.782224&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;Radio&lt;/th&gt;
      &lt;td&gt;0.054809&lt;/td&gt;
      &lt;td&gt;1.000000&lt;/td&gt;
      &lt;td&gt;0.354104&lt;/td&gt;
      &lt;td&gt;0.576223&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;Newspaper&lt;/th&gt;
      &lt;td&gt;0.056648&lt;/td&gt;
      &lt;td&gt;0.354104&lt;/td&gt;
      &lt;td&gt;1.000000&lt;/td&gt;
      &lt;td&gt;0.228299&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;Sales&lt;/th&gt;
      &lt;td&gt;0.782224&lt;/td&gt;
      &lt;td&gt;0.576223&lt;/td&gt;
      &lt;td&gt;0.228299&lt;/td&gt;
      &lt;td&gt;1.000000&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;Let&amp;rsquo;s try to inspect the relationship visually. Note that now the linear best fit is going to be 3-dimensional. In order to make it visually accessible, we consider only on &lt;em&gt;TV&lt;/em&gt; and &lt;em&gt;Radio&lt;/em&gt; advertisement expediture as dependent variables. The best fit will be a plane instead of a line.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Fit regression
est = sm.OLS.from_formula(&#39;Sales ~ Radio + TV&#39;, advertising).fit()
print(est.params)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Intercept    2.921100
Radio        0.187994
TV           0.045755
dtype: float64
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Create a coordinate grid
Radio = np.arange(0,50)
TV = np.arange(0,300)
B1, B2 = np.meshgrid(Radio, TV, indexing=&#39;xy&#39;)

# Compute predicted plane
Z = np.zeros((TV.size, Radio.size))
for (i,j),v in np.ndenumerate(Z):
        Z[i,j] =(est.params[0] + B1[i,j]*est.params[1] + B2[i,j]*est.params[2])
        
# Compute residuals
e = est.predict() - advertising.Sales
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Figure 3.5 - Multiple Linear Regression
def make_fig_3_5():

    # Init figure
    fig = plt.figure()
    ax = axes3d.Axes3D(fig, auto_add_to_figure=False)
    fig.add_axes(ax)
    fig.suptitle(&#39;Figure 3.5&#39;);


    # Plot best fit plane
    ax.plot_surface(B1, B2, Z, color=&#39;k&#39;, alpha=0.3)
    points = ax.scatter3D(advertising.Radio, advertising.TV, advertising.Sales, c=e, cmap=&amp;quot;seismic&amp;quot;, vmin=-5, vmax=5)
    plt.colorbar(points, cax=fig.add_axes([0.9, 0.1, 0.03, 0.8]))
    ax.set_xlabel(&#39;Radio&#39;); ax.set_xlim(0,50)
    ax.set_ylabel(&#39;TV&#39;); ax.set_ylim(bottom=0)
    ax.set_zlabel(&#39;Sales&#39;);
    ax.view_init(20, 20)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;make_fig_3_5()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/01_regression_79_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;some-important-questions&#34;&gt;Some Important Questions&lt;/h3&gt;
&lt;p&gt;How do you check whether the model fit well the data with multiple regressors? &lt;code&gt;statmodels&lt;/code&gt; and most regression packages automatically outputs more information about the least squares model.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Measires of fit
est.summary().tables[0]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;caption&gt;OLS Regression Results&lt;/caption&gt;
&lt;tr&gt;
  &lt;th&gt;Dep. Variable:&lt;/th&gt;          &lt;td&gt;Sales&lt;/td&gt;      &lt;th&gt;  R-squared:         &lt;/th&gt; &lt;td&gt;   0.897&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Model:&lt;/th&gt;                   &lt;td&gt;OLS&lt;/td&gt;       &lt;th&gt;  Adj. R-squared:    &lt;/th&gt; &lt;td&gt;   0.896&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Method:&lt;/th&gt;             &lt;td&gt;Least Squares&lt;/td&gt;  &lt;th&gt;  F-statistic:       &lt;/th&gt; &lt;td&gt;   859.6&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Date:&lt;/th&gt;             &lt;td&gt;Mon, 03 Jan 2022&lt;/td&gt; &lt;th&gt;  Prob (F-statistic):&lt;/th&gt; &lt;td&gt;4.83e-98&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Time:&lt;/th&gt;                 &lt;td&gt;18:28:21&lt;/td&gt;     &lt;th&gt;  Log-Likelihood:    &lt;/th&gt; &lt;td&gt; -386.20&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;No. Observations:&lt;/th&gt;      &lt;td&gt;   200&lt;/td&gt;      &lt;th&gt;  AIC:               &lt;/th&gt; &lt;td&gt;   778.4&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Df Residuals:&lt;/th&gt;          &lt;td&gt;   197&lt;/td&gt;      &lt;th&gt;  BIC:               &lt;/th&gt; &lt;td&gt;   788.3&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Df Model:&lt;/th&gt;              &lt;td&gt;     2&lt;/td&gt;      &lt;th&gt;                     &lt;/th&gt;     &lt;td&gt; &lt;/td&gt;   
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Covariance Type:&lt;/th&gt;      &lt;td&gt;nonrobust&lt;/td&gt;    &lt;th&gt;                     &lt;/th&gt;     &lt;td&gt; &lt;/td&gt;   
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;First measure: the &lt;strong&gt;F-test&lt;/strong&gt;. The F-test tries to answe the question &amp;ldquo;&lt;em&gt;Is There a Relationship Between the Response and Predictors?&lt;/em&gt;&amp;rdquo;&lt;/p&gt;
&lt;p&gt;In particular, it tests the following hypothesis&lt;/p&gt;
&lt;p&gt;$$
H_1: \text{is at least one coefficient different from zero?}
$$&lt;/p&gt;
&lt;p&gt;against the null hypothesis&lt;/p&gt;
&lt;p&gt;$$
H_0: \beta_0 = \beta_1 = &amp;hellip; = 0
$$&lt;/p&gt;
&lt;p&gt;This hypothesis test is performed by computing the F-statistic,&lt;/p&gt;
&lt;p&gt;$$
F=\frac{(\mathrm{TSS}-\mathrm{RSS}) / p}{\operatorname{RSS} /(n-p-1)}
$$&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s try to compute it by hand.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Init
X = advertising[[&#39;Radio&#39;, &#39;TV&#39;]]
y = advertising.Sales
e = y - est.predict(X)
RSS = np.sum(e**2)
TSS = np.sum((y - np.mean(y))**2)
(n,p) = np.shape(X)

# Compute F
F = ((TSS - RSS)/p) / (RSS/(n-p-1))
print(&#39;F = %.4f&#39; % F)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;F = 859.6177
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;A rule of thumb is to reject $H_0$ if $F &amp;gt; 10$.&lt;/p&gt;
&lt;p&gt;We can also test that a particular subset of coefficients are equal to zero. In that case, we just substitute the Total Sum of Squares (TSS) with the Residual Sum of Squares under the null.&lt;/p&gt;
&lt;p&gt;$$
F=\frac{(\mathrm{RSS_0}-\mathrm{RSS}) / p}{\operatorname{RSS} /(n-p-1)}
$$&lt;/p&gt;
&lt;p&gt;i.e. we perfome the regression under the null hypothesis and we compute&lt;/p&gt;
&lt;p&gt;$$
RSS_0 = \sum_{n=1}^N (y_i - X_i \beta)^2 \quad s.t. \quad  H_0
$$&lt;/p&gt;
&lt;h2 id=&#34;23-other-considerations-in-the-regression-model&#34;&gt;2.3 Other Considerations in the Regression Model&lt;/h2&gt;
&lt;h3 id=&#34;qualitative-predictors&#34;&gt;Qualitative Predictors&lt;/h3&gt;
&lt;p&gt;What if some variables are qualitative instead of quantitative? Let&amp;rsquo;s change dataset and use the &lt;code&gt;credit&lt;/code&gt; dataset.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Credit ratings dataset
credit = pd.read_csv(&#39;data/Credit.csv&#39;, usecols=list(range(1,12)))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This dataset contains information on credit ratings, i.e. each person is assigned a &lt;code&gt;Rating&lt;/code&gt; score based on his/her own individual characteristics.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s have a look at data types.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Summary
credit.info()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;class &#39;pandas.core.frame.DataFrame&#39;&amp;gt;
RangeIndex: 400 entries, 0 to 399
Data columns (total 11 columns):
 #   Column     Non-Null Count  Dtype  
---  ------     --------------  -----  
 0   Income     400 non-null    float64
 1   Limit      400 non-null    int64  
 2   Rating     400 non-null    int64  
 3   Cards      400 non-null    int64  
 4   Age        400 non-null    int64  
 5   Education  400 non-null    int64  
 6   Gender     400 non-null    object 
 7   Student    400 non-null    object 
 8   Married    400 non-null    object 
 9   Ethnicity  400 non-null    object 
 10  Balance    400 non-null    int64  
dtypes: float64(1), int64(6), object(4)
memory usage: 34.5+ KB
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As we can see, some variables like &lt;code&gt;Gender&lt;/code&gt;, &lt;code&gt;Student&lt;/code&gt; or &lt;code&gt;Married&lt;/code&gt; are not numeric.&lt;/p&gt;
&lt;p&gt;We can have a closer look at what these variables look like.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Look at data
credit.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;Income&lt;/th&gt;
      &lt;th&gt;Limit&lt;/th&gt;
      &lt;th&gt;Rating&lt;/th&gt;
      &lt;th&gt;Cards&lt;/th&gt;
      &lt;th&gt;Age&lt;/th&gt;
      &lt;th&gt;Education&lt;/th&gt;
      &lt;th&gt;Gender&lt;/th&gt;
      &lt;th&gt;Student&lt;/th&gt;
      &lt;th&gt;Married&lt;/th&gt;
      &lt;th&gt;Ethnicity&lt;/th&gt;
      &lt;th&gt;Balance&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;14.891&lt;/td&gt;
      &lt;td&gt;3606&lt;/td&gt;
      &lt;td&gt;283&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;34&lt;/td&gt;
      &lt;td&gt;11&lt;/td&gt;
      &lt;td&gt;Male&lt;/td&gt;
      &lt;td&gt;No&lt;/td&gt;
      &lt;td&gt;Yes&lt;/td&gt;
      &lt;td&gt;Caucasian&lt;/td&gt;
      &lt;td&gt;333&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;106.025&lt;/td&gt;
      &lt;td&gt;6645&lt;/td&gt;
      &lt;td&gt;483&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;82&lt;/td&gt;
      &lt;td&gt;15&lt;/td&gt;
      &lt;td&gt;Female&lt;/td&gt;
      &lt;td&gt;Yes&lt;/td&gt;
      &lt;td&gt;Yes&lt;/td&gt;
      &lt;td&gt;Asian&lt;/td&gt;
      &lt;td&gt;903&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;104.593&lt;/td&gt;
      &lt;td&gt;7075&lt;/td&gt;
      &lt;td&gt;514&lt;/td&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;71&lt;/td&gt;
      &lt;td&gt;11&lt;/td&gt;
      &lt;td&gt;Male&lt;/td&gt;
      &lt;td&gt;No&lt;/td&gt;
      &lt;td&gt;No&lt;/td&gt;
      &lt;td&gt;Asian&lt;/td&gt;
      &lt;td&gt;580&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;148.924&lt;/td&gt;
      &lt;td&gt;9504&lt;/td&gt;
      &lt;td&gt;681&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;36&lt;/td&gt;
      &lt;td&gt;11&lt;/td&gt;
      &lt;td&gt;Female&lt;/td&gt;
      &lt;td&gt;No&lt;/td&gt;
      &lt;td&gt;No&lt;/td&gt;
      &lt;td&gt;Asian&lt;/td&gt;
      &lt;td&gt;964&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;55.882&lt;/td&gt;
      &lt;td&gt;4897&lt;/td&gt;
      &lt;td&gt;357&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;68&lt;/td&gt;
      &lt;td&gt;16&lt;/td&gt;
      &lt;td&gt;Male&lt;/td&gt;
      &lt;td&gt;No&lt;/td&gt;
      &lt;td&gt;Yes&lt;/td&gt;
      &lt;td&gt;Caucasian&lt;/td&gt;
      &lt;td&gt;331&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;Let&amp;rsquo;s consider the variable &lt;code&gt;Student&lt;/code&gt;. From a quick inspection it looks like it&amp;rsquo;s a binary &lt;em&gt;Yes/No&lt;/em&gt; variable. Let&amp;rsquo;s check by listing all its values.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# What values does the Student variable take?
credit[&#39;Student&#39;].unique()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;array([&#39;No&#39;, &#39;Yes&#39;], dtype=object)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What happens if you pass a binary varaible to &lt;code&gt;statsmodel&lt;/code&gt;? It automatically generates a dummy out of it.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Table 3.7
est = sm.OLS.from_formula(&#39;Balance ~ Student&#39;, credit).fit()
est.summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
         &lt;td&gt;&lt;/td&gt;           &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt;      &lt;td&gt;  480.3694&lt;/td&gt; &lt;td&gt;   23.434&lt;/td&gt; &lt;td&gt;   20.499&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;  434.300&lt;/td&gt; &lt;td&gt;  526.439&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Student[T.Yes]&lt;/th&gt; &lt;td&gt;  396.4556&lt;/td&gt; &lt;td&gt;   74.104&lt;/td&gt; &lt;td&gt;    5.350&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;  250.771&lt;/td&gt; &lt;td&gt;  542.140&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;If a variable takes more than one value, &lt;code&gt;statsmodel&lt;/code&gt; automatically generates a uniqe dummy for each level (-1).&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Table 3.8
est = sm.OLS.from_formula(&#39;Balance ~ Ethnicity&#39;, credit).fit()
est.summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
             &lt;td&gt;&lt;/td&gt;               &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt;              &lt;td&gt;  531.0000&lt;/td&gt; &lt;td&gt;   46.319&lt;/td&gt; &lt;td&gt;   11.464&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;  439.939&lt;/td&gt; &lt;td&gt;  622.061&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Ethnicity[T.Asian]&lt;/th&gt;     &lt;td&gt;  -18.6863&lt;/td&gt; &lt;td&gt;   65.021&lt;/td&gt; &lt;td&gt;   -0.287&lt;/td&gt; &lt;td&gt; 0.774&lt;/td&gt; &lt;td&gt; -146.515&lt;/td&gt; &lt;td&gt;  109.142&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Ethnicity[T.Caucasian]&lt;/th&gt; &lt;td&gt;  -12.5025&lt;/td&gt; &lt;td&gt;   56.681&lt;/td&gt; &lt;td&gt;   -0.221&lt;/td&gt; &lt;td&gt; 0.826&lt;/td&gt; &lt;td&gt; -123.935&lt;/td&gt; &lt;td&gt;   98.930&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;h3 id=&#34;relaxing-the-additive-assumption&#34;&gt;Relaxing the Additive Assumption&lt;/h3&gt;
&lt;p&gt;We have seen that both TV and Radio advertisement are positively associated with Sales. What if there is a synergy? For example it might be that if someone sees an ad &lt;em&gt;both&lt;/em&gt; on TV and on the radio, s/he is much more likely to buy the product.&lt;/p&gt;
&lt;p&gt;Consider the following model&lt;/p&gt;
&lt;p&gt;$$
\text{Sales} ≈ \beta_0 + \beta_1 \text{TV} + \beta_2 \text{Radio} + \beta_3 \text{TV} \times \text{Radio}
$$&lt;/p&gt;
&lt;p&gt;which can be rewritten as&lt;/p&gt;
&lt;p&gt;$$
\text{Sales} ≈ \beta_0 + (\beta_1 + \beta_3 \text{Radio}) \times \text{TV} + \beta_2 \text{Radio}
$$&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s estimate the linear regression model, with the intercept.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Table 3.9 - Interaction Variables
est = sm.OLS.from_formula(&#39;Sales ~ TV + Radio + TV*Radio&#39;, advertising).fit()
est.summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
      &lt;td&gt;&lt;/td&gt;         &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt; &lt;td&gt;    6.7502&lt;/td&gt; &lt;td&gt;    0.248&lt;/td&gt; &lt;td&gt;   27.233&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    6.261&lt;/td&gt; &lt;td&gt;    7.239&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;TV&lt;/th&gt;        &lt;td&gt;    0.0191&lt;/td&gt; &lt;td&gt;    0.002&lt;/td&gt; &lt;td&gt;   12.699&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    0.016&lt;/td&gt; &lt;td&gt;    0.022&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Radio&lt;/th&gt;     &lt;td&gt;    0.0289&lt;/td&gt; &lt;td&gt;    0.009&lt;/td&gt; &lt;td&gt;    3.241&lt;/td&gt; &lt;td&gt; 0.001&lt;/td&gt; &lt;td&gt;    0.011&lt;/td&gt; &lt;td&gt;    0.046&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;TV:Radio&lt;/th&gt;  &lt;td&gt;    0.0011&lt;/td&gt; &lt;td&gt; 5.24e-05&lt;/td&gt; &lt;td&gt;   20.727&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    0.001&lt;/td&gt; &lt;td&gt;    0.001&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;A positive and significant interaction term indicates a hint of a sinergy effect.&lt;/p&gt;
&lt;h3 id=&#34;heterogeneous-effects&#34;&gt;Heterogeneous Effects&lt;/h3&gt;
&lt;p&gt;We can do interactions with qualitative variables as well. Conside the credit rating dataset.&lt;/p&gt;
&lt;p&gt;What if &lt;code&gt;Balance&lt;/code&gt; depends by &lt;code&gt;Income&lt;/code&gt; differently, depending on whether one is a &lt;code&gt;Student&lt;/code&gt; or not?&lt;/p&gt;
&lt;p&gt;Consider the following model:&lt;/p&gt;
&lt;p&gt;$$
\text{Balance} ≈ \beta_0 + \beta_1 \text{Income} + \beta_2 \text{Student} + \beta_3 \text{Income} \times \text{Student}
$$&lt;/p&gt;
&lt;p&gt;The last coefficient $\beta_3$ should tell us how much &lt;code&gt;Balance&lt;/code&gt; increases in &lt;code&gt;Income&lt;/code&gt; for &lt;code&gt;Students&lt;/code&gt; with respect to non-Students.&lt;/p&gt;
&lt;p&gt;Indeed, we can decompose the regression in the following equivalent way:&lt;/p&gt;
&lt;p&gt;$$
\text{Balance} ≈ \beta_0 + \beta_1 \text{Income} + \beta_2 \text{Student} + \beta_3 \text{Income} \times \text{Student}
$$&lt;/p&gt;
&lt;p&gt;which can be interpreted in the following way since &lt;code&gt;Student&lt;/code&gt; is a binary variable&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;If the person is &lt;em&gt;not&lt;/em&gt; a student
$$
\text{Balance} ≈ \beta_0 + \beta_1 \text{Income}
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;If the person is a student
$$
\text{Balance} ≈ (\beta_0 + \beta_2) + (\beta_1 + \beta_3 ) \text{Income}
$$&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;We are allowing not only for a different intercept for &lt;code&gt;Students&lt;/code&gt;, $\beta_0 \to \beta_0 + \beta_2$,  but also for a different impact of &lt;code&gt;Income&lt;/code&gt;, $\beta_1 \to \beta_1 + \beta_3$.&lt;/p&gt;
&lt;p&gt;We can visually inspect the distribution of &lt;code&gt;Income&lt;/code&gt; across the two groups.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Divide data into students and non-students
x_student = credit.loc[credit.Student==&#39;Yes&#39;,&#39;Income&#39;]
y_student = credit.loc[credit.Student==&#39;Yes&#39;,&#39;Balance&#39;]
x_nonstudent = credit.loc[credit.Student==&#39;No&#39;,&#39;Income&#39;]
y_nonstudent = credit.loc[credit.Student==&#39;No&#39;,&#39;Balance&#39;]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Make figure 3.8
def make_fig_3_8():
    
    # Init figure
    fig, ax = plt.subplots(1,1)
    fig.suptitle(&#39;Figure 3.8&#39;)

    # Relationship betweeen income and balance for students and non-students
    ax.scatter(x=x_nonstudent, y=y_nonstudent, facecolors=&#39;None&#39;, edgecolors=&#39;k&#39;, alpha=0.5);
    ax.scatter(x=x_student, y=y_student, facecolors=&#39;r&#39;, edgecolors=&#39;r&#39;, alpha=0.7);
    ax.legend([&#39;non-student&#39;, &#39;student&#39;]);
    ax.set_xlabel(&#39;Income&#39;); ax.set_ylabel(&#39;Balance&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;make_fig_3_8()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/01_regression_114_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;It is hard from the scatterplot to see whether there is a different relationship between income and balance for students and non-students.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s fit two separate regressions.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Interaction between qualitative and quantative variables
est1 = sm.OLS.from_formula(&#39;Balance ~ Income + Student&#39;, credit).fit()
reg1 = est1.params
est2 = sm.OLS.from_formula(&#39;Balance ~ Income + Student + Income*Student&#39;, credit).fit()
reg2 = est2.params

print(&#39;Regression 1 - without interaction term&#39;)
print(reg1)
print(&#39;\nRegression 2 - with interaction term&#39;)
print(reg2)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Regression 1 - without interaction term
Intercept         211.142964
Student[T.Yes]    382.670539
Income              5.984336
dtype: float64

Regression 2 - with interaction term
Intercept                200.623153
Student[T.Yes]           476.675843
Income                     6.218169
Income:Student[T.Yes]     -1.999151
dtype: float64
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Without the interaction term, the two lines have different levels but the same slope. Introducing an interaction term allows the two groups to have different responses to Income.&lt;/p&gt;
&lt;p&gt;We can visualize the relationship in a graph.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Income (x-axis)
income = np.linspace(0,150)

# Balance without interaction term (y-axis)
student1 = np.linspace(reg1[&#39;Intercept&#39;]+reg1[&#39;Student[T.Yes]&#39;],
                       reg1[&#39;Intercept&#39;]+reg1[&#39;Student[T.Yes]&#39;]+150*reg1[&#39;Income&#39;])
non_student1 =  np.linspace(reg1[&#39;Intercept&#39;], reg1[&#39;Intercept&#39;]+150*reg1[&#39;Income&#39;])

# Balance with iteraction term (y-axis)
student2 = np.linspace(reg2[&#39;Intercept&#39;]+reg2[&#39;Student[T.Yes]&#39;],
                       reg2[&#39;Intercept&#39;]+reg2[&#39;Student[T.Yes]&#39;]+
                       150*(reg2[&#39;Income&#39;]+reg2[&#39;Income:Student[T.Yes]&#39;]))
non_student2 =  np.linspace(reg2[&#39;Intercept&#39;], reg2[&#39;Intercept&#39;]+150*reg2[&#39;Income&#39;])
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Figure 3.7
def make_fig_3_7():
    
    fig, (ax1,ax2) = plt.subplots(1,2, figsize=(12,5))
    fig.suptitle(&#39;Figure 3.7&#39;)

    # Plot best fit with and without interaction
    ax1.plot(income, student1, &#39;r&#39;, income, non_student1, &#39;k&#39;)
    ax2.plot(income, student2, &#39;r&#39;, income, non_student2, &#39;k&#39;)
    
    titles = [&#39;Dummy&#39;, &#39;Dummy + Interaction&#39;]
    for ax, t in zip(fig.axes, titles):
        ax.legend([&#39;student&#39;, &#39;non-student&#39;], loc=2)
        ax.set_xlabel(&#39;Income&#39;)
        ax.set_ylabel(&#39;Balance&#39;)
        ax.set_ylim(ymax=1550)
        ax.set_title(t)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;make_fig_3_7()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/01_regression_120_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;non-linear-relationships&#34;&gt;Non-Linear Relationships&lt;/h3&gt;
&lt;p&gt;What if we allow for further non-linearities? Let&amp;rsquo;s change dataset again and use the &lt;code&gt;car&lt;/code&gt; dataset.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Automobile dataset (dropping missing values)
auto = pd.read_csv(&#39;data/Auto.csv&#39;, na_values=&#39;?&#39;).dropna()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This dataset contains information of a wide variety of car models.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;auto.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;mpg&lt;/th&gt;
      &lt;th&gt;cylinders&lt;/th&gt;
      &lt;th&gt;displacement&lt;/th&gt;
      &lt;th&gt;horsepower&lt;/th&gt;
      &lt;th&gt;weight&lt;/th&gt;
      &lt;th&gt;acceleration&lt;/th&gt;
      &lt;th&gt;year&lt;/th&gt;
      &lt;th&gt;origin&lt;/th&gt;
      &lt;th&gt;name&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;18.0&lt;/td&gt;
      &lt;td&gt;8&lt;/td&gt;
      &lt;td&gt;307.0&lt;/td&gt;
      &lt;td&gt;130&lt;/td&gt;
      &lt;td&gt;3504&lt;/td&gt;
      &lt;td&gt;12.0&lt;/td&gt;
      &lt;td&gt;70&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;chevrolet chevelle malibu&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;15.0&lt;/td&gt;
      &lt;td&gt;8&lt;/td&gt;
      &lt;td&gt;350.0&lt;/td&gt;
      &lt;td&gt;165&lt;/td&gt;
      &lt;td&gt;3693&lt;/td&gt;
      &lt;td&gt;11.5&lt;/td&gt;
      &lt;td&gt;70&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;buick skylark 320&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;18.0&lt;/td&gt;
      &lt;td&gt;8&lt;/td&gt;
      &lt;td&gt;318.0&lt;/td&gt;
      &lt;td&gt;150&lt;/td&gt;
      &lt;td&gt;3436&lt;/td&gt;
      &lt;td&gt;11.0&lt;/td&gt;
      &lt;td&gt;70&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;plymouth satellite&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;16.0&lt;/td&gt;
      &lt;td&gt;8&lt;/td&gt;
      &lt;td&gt;304.0&lt;/td&gt;
      &lt;td&gt;150&lt;/td&gt;
      &lt;td&gt;3433&lt;/td&gt;
      &lt;td&gt;12.0&lt;/td&gt;
      &lt;td&gt;70&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;amc rebel sst&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;17.0&lt;/td&gt;
      &lt;td&gt;8&lt;/td&gt;
      &lt;td&gt;302.0&lt;/td&gt;
      &lt;td&gt;140&lt;/td&gt;
      &lt;td&gt;3449&lt;/td&gt;
      &lt;td&gt;10.5&lt;/td&gt;
      &lt;td&gt;70&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;ford torino&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;Suppose we wanted to understand which car caracteristics are correlated with higher efficiency, i.e. &lt;code&gt;mpg&lt;/code&gt; (miles per gallon).&lt;/p&gt;
&lt;p&gt;Consider in particular the relationship between &lt;code&gt;mpg&lt;/code&gt; and &lt;code&gt;horsepower&lt;/code&gt;. It might be a highly non-linear relationship.&lt;/p&gt;
&lt;p&gt;$$
\text{mpg} ≈ \beta_0 + \beta_1 \text{horsepower} + \beta_2 \text{horsepower}^2 + &amp;hellip; ???
$$&lt;/p&gt;
&lt;p&gt;How many terms should we include?&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s look at the data to understand if it naturally suggests non-linearities.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;fig, ax = plt.subplots(1,1)

# Plot polinomials of different degree
plt.scatter(x=auto.horsepower, y=auto.mpg, facecolors=&#39;None&#39;, edgecolors=&#39;k&#39;, alpha=.3) 
plt.ylim(5,55); plt.xlim(40,240); 
plt.xlabel(&#39;horsepower&#39;); plt.ylabel(&#39;mpg&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/01_regression_128_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;The relationship looks non-linear but in which way exactly? Let&amp;rsquo;s try to fit polinomials of different degrees.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def make_fig_38():
    
    # Figure 3.8 
    fig, ax = plt.subplots(1,1)
    ax.set_title(&#39;Figure 3.8&#39;)

    # Plot polinomials of different degree
    plt.scatter(x=auto.horsepower, y=auto.mpg, facecolors=&#39;None&#39;, edgecolors=&#39;k&#39;, alpha=.3) 
    sns.regplot(x=auto.horsepower, y=auto.mpg, ci=None, label=&#39;Linear&#39;, scatter=False, color=&#39;orange&#39;)
    sns.regplot(x=auto.horsepower, y=auto.mpg, ci=None, label=&#39;Degree 2&#39;, order=2, scatter=False, color=&#39;lightblue&#39;)
    sns.regplot(x=auto.horsepower, y=auto.mpg, ci=None, label=&#39;Degree 5&#39;, order=5, scatter=False, color=&#39;g&#39;)
    plt.legend()
    plt.ylim(5,55)
    plt.xlim(40,240);
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;make_fig_38()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/01_regression_131_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;As we can see, the tails are highly unstable depending on the specification.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s add a quadratic term&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Table 3.10
auto[&#39;horsepower2&#39;] = auto.horsepower**2
auto.head(3)
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;mpg&lt;/th&gt;
      &lt;th&gt;cylinders&lt;/th&gt;
      &lt;th&gt;displacement&lt;/th&gt;
      &lt;th&gt;horsepower&lt;/th&gt;
      &lt;th&gt;weight&lt;/th&gt;
      &lt;th&gt;acceleration&lt;/th&gt;
      &lt;th&gt;year&lt;/th&gt;
      &lt;th&gt;origin&lt;/th&gt;
      &lt;th&gt;name&lt;/th&gt;
      &lt;th&gt;horsepower2&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;18.0&lt;/td&gt;
      &lt;td&gt;8&lt;/td&gt;
      &lt;td&gt;307.0&lt;/td&gt;
      &lt;td&gt;130&lt;/td&gt;
      &lt;td&gt;3504&lt;/td&gt;
      &lt;td&gt;12.0&lt;/td&gt;
      &lt;td&gt;70&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;chevrolet chevelle malibu&lt;/td&gt;
      &lt;td&gt;16900&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;15.0&lt;/td&gt;
      &lt;td&gt;8&lt;/td&gt;
      &lt;td&gt;350.0&lt;/td&gt;
      &lt;td&gt;165&lt;/td&gt;
      &lt;td&gt;3693&lt;/td&gt;
      &lt;td&gt;11.5&lt;/td&gt;
      &lt;td&gt;70&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;buick skylark 320&lt;/td&gt;
      &lt;td&gt;27225&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;18.0&lt;/td&gt;
      &lt;td&gt;8&lt;/td&gt;
      &lt;td&gt;318.0&lt;/td&gt;
      &lt;td&gt;150&lt;/td&gt;
      &lt;td&gt;3436&lt;/td&gt;
      &lt;td&gt;11.0&lt;/td&gt;
      &lt;td&gt;70&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;plymouth satellite&lt;/td&gt;
      &lt;td&gt;22500&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;How does the regression change?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;est = sm.OLS.from_formula(&#39;mpg ~ horsepower + horsepower2&#39;, auto).fit()
est.summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
       &lt;td&gt;&lt;/td&gt;          &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt;   &lt;td&gt;   56.9001&lt;/td&gt; &lt;td&gt;    1.800&lt;/td&gt; &lt;td&gt;   31.604&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;   53.360&lt;/td&gt; &lt;td&gt;   60.440&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;horsepower&lt;/th&gt;  &lt;td&gt;   -0.4662&lt;/td&gt; &lt;td&gt;    0.031&lt;/td&gt; &lt;td&gt;  -14.978&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;   -0.527&lt;/td&gt; &lt;td&gt;   -0.405&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;horsepower2&lt;/th&gt; &lt;td&gt;    0.0012&lt;/td&gt; &lt;td&gt;    0.000&lt;/td&gt; &lt;td&gt;   10.080&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    0.001&lt;/td&gt; &lt;td&gt;    0.001&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;h3 id=&#34;non-linearities&#34;&gt;Non-Linearities&lt;/h3&gt;
&lt;p&gt;How can we assess if there are non-linearities and of which kind? We can look at the residuals.&lt;/p&gt;
&lt;p&gt;If the residuals show some kind of pattern, probably we could have fit the line better. Moreover, we can use the pattern itself to understand how.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Linear fit
X = auto.horsepower.values.reshape(-1,1)
y = auto.mpg
regr = LinearRegression().fit(X, y)

auto[&#39;pred1&#39;] = regr.predict(X)
auto[&#39;resid1&#39;] = auto.mpg - auto.pred1

# Quadratic fit
X2 = auto[[&#39;horsepower&#39;, &#39;horsepower2&#39;]]
regr.fit(X2, y)

auto[&#39;pred2&#39;] = regr.predict(X2)
auto[&#39;resid2&#39;] = auto.mpg - auto.pred2
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Figure 3.9
def make_fig_39():
    
    fig, (ax1,ax2) = plt.subplots(1,2, figsize=(12,5))
    fig.suptitle(&#39;Figure 3.9&#39;)

    # Left plot
    sns.regplot(x=auto.pred1, y=auto.resid1, lowess=True, 
                ax=ax1, line_kws={&#39;color&#39;:&#39;r&#39;, &#39;lw&#39;:1},
                scatter_kws={&#39;facecolors&#39;:&#39;None&#39;, &#39;edgecolors&#39;:&#39;k&#39;, &#39;alpha&#39;:0.5})
    ax1.hlines(0,xmin=ax1.xaxis.get_data_interval()[0],
               xmax=ax1.xaxis.get_data_interval()[1], linestyles=&#39;dotted&#39;)
    ax1.set_title(&#39;Residual Plot for Linear Fit&#39;)

    # Right plot
    sns.regplot(x=auto.pred2, y=auto.resid2, lowess=True,
                line_kws={&#39;color&#39;:&#39;r&#39;, &#39;lw&#39;:1}, ax=ax2,
                scatter_kws={&#39;facecolors&#39;:&#39;None&#39;, &#39;edgecolors&#39;:&#39;k&#39;, &#39;alpha&#39;:0.5})
    ax2.hlines(0,xmin=ax2.xaxis.get_data_interval()[0],
               xmax=ax2.xaxis.get_data_interval()[1], linestyles=&#39;dotted&#39;)
    ax2.set_title(&#39;Residual Plot for Quadratic Fit&#39;)

    for ax in fig.axes:
        ax.set_xlabel(&#39;Fitted values&#39;)
        ax.set_ylabel(&#39;Residuals&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt; make_fig_39()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/01_regression_141_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;It looks like the residuals from the linear fit (on the left) exibit a pattern:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;positive values at the tails&lt;/li&gt;
&lt;li&gt;negative values in the center&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This suggests a quadratic fit. Indeed, the residuals when we include &lt;code&gt;horsepower^2&lt;/code&gt; (on the right) seem more uniformly centered around zero.&lt;/p&gt;
&lt;h3 id=&#34;outliers&#34;&gt;Outliers&lt;/h3&gt;
&lt;p&gt;Observations with high residuals have a good chance of being highly influentials. However, they do not have to be.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s use the following data generating process:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$X \sim N(0,1)$&lt;/li&gt;
&lt;li&gt;$\varepsilon \sim N(0,0.5)$&lt;/li&gt;
&lt;li&gt;$\beta_0 = 3$&lt;/li&gt;
&lt;li&gt;$y = \beta_0 X + \varepsilon$&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;np.random.seed(1)

# Generate random y
n = 50
X = rnorm(1,1,(n,1))
e = rnorm(0,0.5,(n,1))
b0 = 3
y = X*b0 + e
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now let&amp;rsquo;s change observation &lt;code&gt;20&lt;/code&gt; so that it becomes an outlier, i.e. it has a high residual.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Generate outlier
X[20] = 1
y[20] = 7

# Short regression without observation number 41
X_small = np.delete(X, 20)
y_small = np.delete(y, 20)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s now plot the data and the residuals&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Figure 3.12
def make_fig_3_12():
    
    # Init figure
    fig, (ax1,ax2) = plt.subplots(1,2, figsize=(12,5)) 
    fig.suptitle(&#39;Figure 3.12&#39;)

    # Plot 1
    ax1.scatter(x=X, y=y, facecolors=&#39;None&#39;, edgecolors=&#39;k&#39;, alpha=.5) 
    sns.regplot(x=X, y=y, ax=ax1, order=1, ci=None, scatter=False, line_kws={&#39;color&#39;:&#39;r&#39;, &#39;lw&#39;:1})
    sns.regplot(x=X_small, y=y_small, ax=ax1, order=1, ci=None, scatter=False, line_kws={&#39;color&#39;:&#39;b&#39;, &#39;lw&#39;:1})
    ax1.set_xlabel(&#39;X&#39;); ax1.set_ylabel(&#39;Y&#39;); 
    ax1.legend([&#39;With obs. 20&#39;, &#39;Without obs. 20&#39;], fontsize=12);

    # Hihglight outliers
    ax1.scatter(x=X[20], y=y[20], facecolors=&#39;None&#39;, edgecolors=&#39;r&#39;, alpha=1) 
    ax1.annotate(&amp;quot;20&amp;quot;, (1.1, 7), color=&#39;r&#39;)

    # Compute fitted values and residuals
    r = regr.fit(X, y)
    y_hat = r.predict(X)
    e = np.abs(y - y_hat)

    # Plot 2
    ax2.scatter(x=y_hat, y=e, facecolors=&#39;None&#39;, edgecolors=&#39;k&#39;, alpha=.5)
    ax2.set_xlabel(&#39;Fitted Values&#39;); ax2.set_ylabel(&#39;Residuals&#39;);
    ax2.hlines(0,xmin=ax2.xaxis.get_data_interval()[0],
               xmax=ax2.xaxis.get_data_interval()[1], linestyles=&#39;dotted&#39;,color=&#39;k&#39;)

    # Highlight outlier
    ax2.scatter(x=y_hat[20], y=e[20], facecolors=&#39;None&#39;, edgecolors=&#39;r&#39;, alpha=1) 
    ax2.annotate(&amp;quot;20&amp;quot;, (2.2, 3.6), color=&#39;r&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;make_fig_3_12()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/01_regression_150_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;high-leverage-points&#34;&gt;High Leverage Points&lt;/h3&gt;
&lt;p&gt;A better concept of &amp;ldquo;influential observation&amp;rdquo; is the Leverage, which represents how much an observation is distant from the others in terms of observables.&lt;/p&gt;
&lt;p&gt;The leverage formula of observation $i$ is&lt;/p&gt;
&lt;p&gt;$$
h_i = x_i (X&amp;rsquo; X)^{-1} x_i&#39;
$$&lt;/p&gt;
&lt;p&gt;However, leverage alone is not necessarily enough for an observation to being highly influential.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s modify observation &lt;code&gt;41&lt;/code&gt; so that it has a high leverage.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Generate observation with high leverage
X[41] = 4
y[41] = 12

# Short regression without observation number 41
X_small = np.delete(X_small, 41)
y_small = np.delete(y_small, 41)

# Compute leverage
H = X @ inv(X.T @ X) @ X.T
h = np.diagonal(H)

# Compute fitted values and residuals
y_hat = X @ inv(X.T @ X) @ X.T @ y
e = np.abs(y - y_hat) 
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What happens now that we have added an observation with high leverage? How does the levarage look like?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Figure 3.13
def make_fig_3_13():

    # Init figure
    fig, (ax1,ax2) = plt.subplots(1,2, figsize=(12,5)) 
    fig.suptitle(&#39;Figure 3.12&#39;)

    # Plot 1
    ax1.scatter(x=X, y=y, facecolors=&#39;None&#39;, edgecolors=&#39;k&#39;, alpha=.5) 
    ax1.scatter(x=X[[20,41]], y=y[[20,41]], facecolors=&#39;None&#39;, edgecolors=&#39;r&#39;, alpha=1) 
    sns.regplot(x=X, y=y, ax=ax1, order=1, ci=None, scatter=False, line_kws={&#39;color&#39;:&#39;r&#39;, &#39;lw&#39;:1})
    sns.regplot(x=X_small, y=y_small, ax=ax1, order=1, ci=None, scatter=False, line_kws={&#39;color&#39;:&#39;b&#39;, &#39;lw&#39;:1})
    ax1.set_xlabel(&#39;X&#39;); ax1.set_ylabel(&#39;Y&#39;); ax1.axis(xmax=4.5);
    ax1.legend([&#39;With obs. 20,41&#39;, &#39;Without obs. 20,41&#39;]);

    # Highlight points
    ax1.annotate(&amp;quot;20&amp;quot;, (1.1, 7), color=&#39;r&#39;)
    ax1.annotate(&amp;quot;41&amp;quot;, (3.6, 12), color=&#39;r&#39;);



    # Plot 2
    ax2.scatter(x=h, y=e, facecolors=&#39;None&#39;, edgecolors=&#39;k&#39;, alpha=.5)
    ax2.set_xlabel(&#39;Leverage&#39;); ax2.set_ylabel(&#39;Residuals&#39;); 
    ax2.hlines(0,xmin=ax2.xaxis.get_data_interval()[0],
               xmax=ax2.xaxis.get_data_interval()[1], linestyles=&#39;dotted&#39;,color=&#39;k&#39;)
    # Highlight outlier
    ax2.scatter(x=h[[20,41]], y=e[[20,41]], facecolors=&#39;None&#39;, edgecolors=&#39;r&#39;, alpha=1);

    # Highlight points
    ax2.annotate(&amp;quot;20&amp;quot;, (0, 3.7), color=&#39;r&#39;)
    ax2.annotate(&amp;quot;41&amp;quot;, (0.14, 0.4), color=&#39;r&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;make_fig_3_13()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/01_regression_156_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;influential-observations&#34;&gt;Influential Observations&lt;/h3&gt;
&lt;p&gt;As we have seen, being an outliers or having high leverage alone might be not enough to conclude that an observation is influential.&lt;/p&gt;
&lt;p&gt;What really matters is a combination of both: observations with high leverage and high residuals, i.e. observations that are not only different in terms of observables (high leverage) but are also different in terms of their relationship between observables and dependent variable (high residual).&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s now modify observation &lt;code&gt;7&lt;/code&gt; so that it is an outlier and has high leverage.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Generate outlier with high leverage
X[7] = 4
y[7] = 7
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Short regression without observation number 41
X_small = np.delete(X, 7)
y_small = np.delete(y, 7)

# Compute leverage
H = X @ inv(X.T @ X) @ X.T
h = np.diagonal(H)

# Compute fitted values and residuals
r = regr.fit(X, y)
y_hat = r.predict(X)
e = np.abs(y - y_hat)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now the best linear fit line has noticeably moved.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def make_fig_extra_3():

    fig, (ax1,ax2) = plt.subplots(1,2, figsize=(12,5)) 

    # Plot 1
    ax1.scatter(x=X, y=y, facecolors=&#39;None&#39;, edgecolors=&#39;k&#39;, alpha=.5) 
    ax1.scatter(x=X[[7,20,41]], y=y[[7,20,41]], facecolors=&#39;None&#39;, edgecolors=&#39;r&#39;, alpha=1) 
    sns.regplot(x=X, y=y, ax=ax1, order=1, ci=None, scatter=False, line_kws={&#39;color&#39;:&#39;r&#39;, &#39;lw&#39;:1})
    sns.regplot(x=X_small, y=y_small, ax=ax1, order=1, ci=None, scatter=False, line_kws={&#39;color&#39;:&#39;b&#39;, &#39;lw&#39;:1})
    ax1.set_xlabel(&#39;X&#39;); ax1.set_ylabel(&#39;Y&#39;); ax1.axis(xmax=4.5);
    ax1.legend([&#39;With obs. 7,20,41&#39;, &#39;Without obs. 7,20,41&#39;]);

    # Highlight points
    ax1.annotate(&amp;quot;7&amp;quot;, (3.7, 7), color=&#39;r&#39;)
    ax1.annotate(&amp;quot;20&amp;quot;, (1.15, 7.05), color=&#39;r&#39;)
    ax1.annotate(&amp;quot;41&amp;quot;, (3.6, 12), color=&#39;r&#39;);



    # Plot 2
    ax2.scatter(x=h, y=e, facecolors=&#39;None&#39;, edgecolors=&#39;k&#39;, alpha=.5)
    ax2.set_xlabel(&#39;Leverage&#39;); ax2.set_ylabel(&#39;Residuals&#39;); 
    ax2.hlines(0,xmin=ax2.xaxis.get_data_interval()[0],
               xmax=ax2.xaxis.get_data_interval()[1], linestyles=&#39;dotted&#39;,color=&#39;k&#39;)
    # Highlight outlier
    ax2.scatter(x=h[[7,20,41]], y=e[[7,20,41]], facecolors=&#39;None&#39;, edgecolors=&#39;r&#39;, alpha=1);

    # Highlight points
    ax2.annotate(&amp;quot;7&amp;quot;, (0.12, 4.0), color=&#39;r&#39;);
    ax2.annotate(&amp;quot;20&amp;quot;, (0, 3.8), color=&#39;r&#39;)
    ax2.annotate(&amp;quot;41&amp;quot;, (0.12, 0.9), color=&#39;r&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;make_fig_extra_3()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/01_regression_164_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;collinearity&#34;&gt;Collinearity&lt;/h3&gt;
&lt;p&gt;Collinearity is the situation in which two dependent varaibles are higly correlated with each other. Algebraically, this is a problem because the $X&amp;rsquo;X$ matrix becomes almost-non-invertible.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s have a look at the &lt;code&gt;ratings&lt;/code&gt; dataset.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Inspect dataset
sns.pairplot(credit[[&#39;Age&#39;, &#39;Balance&#39;, &#39;Limit&#39;, &#39;Rating&#39;]], height=1.8);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/01_regression_167_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;If we zoom into the variable &lt;code&gt;Limit&lt;/code&gt;, we see that for example it is not very correlated with &lt;code&gt;Age&lt;/code&gt; but is very correlated with &lt;code&gt;Rating&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Figure 3.14
def make_fig_3_14():
    
    # Init figure
    fig, (ax1,ax2) = plt.subplots(1,2, figsize=(12,5)) 
    fig.suptitle(&#39;Figure 3.14&#39;)

    # Left plot
    ax1.scatter(credit.Limit, credit.Age, facecolor=&#39;None&#39;, edgecolor=&#39;brown&#39;)
    ax1.set_ylabel(&#39;Age&#39;)

    # Right plot
    ax2.scatter(credit.Limit, credit.Rating, facecolor=&#39;None&#39;, edgecolor=&#39;brown&#39;)
    ax2.set_ylabel(&#39;Rating&#39;)

    for ax in fig.axes:
        ax.set_xlabel(&#39;Limit&#39;)
        ax.set_xticks([2000,4000,6000,8000,12000])
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;make_fig_3_14()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/01_regression_170_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;If we regress &lt;code&gt;Balance&lt;/code&gt; on &lt;code&gt;Limit&lt;/code&gt; and &lt;code&gt;Age&lt;/code&gt;, the coefficient of &lt;code&gt;Limit&lt;/code&gt; is positive and highly significant.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Regress balance on limit and age
reg1 = sm.OLS.from_formula(&#39;Balance ~ Limit + Age&#39;, credit).fit()
reg1.summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
      &lt;td&gt;&lt;/td&gt;         &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt; &lt;td&gt; -173.4109&lt;/td&gt; &lt;td&gt;   43.828&lt;/td&gt; &lt;td&gt;   -3.957&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt; -259.576&lt;/td&gt; &lt;td&gt;  -87.246&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Limit&lt;/th&gt;     &lt;td&gt;    0.1734&lt;/td&gt; &lt;td&gt;    0.005&lt;/td&gt; &lt;td&gt;   34.496&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    0.163&lt;/td&gt; &lt;td&gt;    0.183&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Age&lt;/th&gt;       &lt;td&gt;   -2.2915&lt;/td&gt; &lt;td&gt;    0.672&lt;/td&gt; &lt;td&gt;   -3.407&lt;/td&gt; &lt;td&gt; 0.001&lt;/td&gt; &lt;td&gt;   -3.614&lt;/td&gt; &lt;td&gt;   -0.969&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;However, if we regress &lt;code&gt;Balance&lt;/code&gt; on &lt;code&gt;Limit&lt;/code&gt; and &lt;code&gt;Rating&lt;/code&gt;, the coefficient of &lt;code&gt;Limit&lt;/code&gt; is now not significant anymore.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Regress balance on limit and rating
reg2 = sm.OLS.from_formula(&#39;Balance ~ Limit + Rating&#39;, credit).fit()
reg2.summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
      &lt;td&gt;&lt;/td&gt;         &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt; &lt;td&gt; -377.5368&lt;/td&gt; &lt;td&gt;   45.254&lt;/td&gt; &lt;td&gt;   -8.343&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt; -466.505&lt;/td&gt; &lt;td&gt; -288.569&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Limit&lt;/th&gt;     &lt;td&gt;    0.0245&lt;/td&gt; &lt;td&gt;    0.064&lt;/td&gt; &lt;td&gt;    0.384&lt;/td&gt; &lt;td&gt; 0.701&lt;/td&gt; &lt;td&gt;   -0.101&lt;/td&gt; &lt;td&gt;    0.150&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Rating&lt;/th&gt;    &lt;td&gt;    2.2017&lt;/td&gt; &lt;td&gt;    0.952&lt;/td&gt; &lt;td&gt;    2.312&lt;/td&gt; &lt;td&gt; 0.021&lt;/td&gt; &lt;td&gt;    0.330&lt;/td&gt; &lt;td&gt;    4.074&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;Looking at the objective function, the Residual Sum of Squares, helps understanding what is the problem.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# First scale variables
y = credit.Balance
regr1 = LinearRegression().fit(scale(credit[[&#39;Age&#39;, &#39;Limit&#39;]].astype(&#39;float&#39;), with_std=False), y)
regr2 = LinearRegression().fit(scale(credit[[&#39;Rating&#39;, &#39;Limit&#39;]], with_std=False), y)

# Create grid coordinates for plotting
B_Age = np.linspace(regr1.coef_[0]-3, regr1.coef_[0]+3, 100)
B_Limit = np.linspace(regr1.coef_[1]-0.02, regr1.coef_[1]+0.02, 100)

B_Rating = np.linspace(regr2.coef_[0]-3, regr2.coef_[0]+3, 100)
B_Limit2 = np.linspace(regr2.coef_[1]-0.2, regr2.coef_[1]+0.2, 100)

X1, Y1 = np.meshgrid(B_Limit, B_Age, indexing=&#39;xy&#39;)
X2, Y2 = np.meshgrid(B_Limit2, B_Rating, indexing=&#39;xy&#39;)
Z1 = np.zeros((B_Age.size,B_Limit.size))
Z2 = np.zeros((B_Rating.size,B_Limit2.size))

Limit_scaled = scale(credit.Limit.astype(&#39;float&#39;), with_std=False)
Age_scaled = scale(credit.Age.astype(&#39;float&#39;), with_std=False)
Rating_scaled = scale(credit.Rating.astype(&#39;float&#39;), with_std=False)

# Calculate Z-values (RSS) based on grid of coefficients
for (i,j),v in np.ndenumerate(Z1):
    Z1[i,j] =((y - (regr1.intercept_ + X1[i,j]*Limit_scaled +
                    Y1[i,j]*Age_scaled))**2).sum()/1000000
    
for (i,j),v in np.ndenumerate(Z2):
    Z2[i,j] =((y - (regr2.intercept_ + X2[i,j]*Limit_scaled +
                    Y2[i,j]*Rating_scaled))**2).sum()/1000000
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Figure 3.15
def make_fig_3_15():

    # Init figure
    fig, (ax1,ax2) = plt.subplots(1,2, figsize=(12,5)) 
    fig.suptitle(&#39;Figure 3.15&#39;)

    # Minimum
    min_RSS = r&#39;$\beta_0$, $\beta_1$ for minimized RSS&#39;

    # Left plot
    CS = ax1.contour(X1, Y1, Z1, cmap=plt.cm.Set1, levels=[21.25, 21.5, 21.8])
    ax1.scatter(reg1.params[1], reg1.params[2], c=&#39;r&#39;, label=min_RSS)
    ax1.clabel(CS, inline=True, fontsize=10, fmt=&#39;%1.1f&#39;)
    ax1.set_ylabel(r&#39;$\beta_{Age}$&#39;)

    # Right plot
    CS = ax2.contour(X2, Y2, Z2, cmap=plt.cm.Set1, levels=[21.5, 21.8])
    ax2.scatter(reg2.params[1], reg2.params[2], c=&#39;r&#39;, label=min_RSS)
    ax2.clabel(CS, inline=True, fontsize=10, fmt=&#39;%1.1f&#39;)
    ax2.set_ylabel(r&#39;$\beta_{Rating}$&#39;)
    #ax2.set_xticks([-0.1, 0, 0.1, 0.2])

    for ax in fig.axes:
        ax.set_xlabel(r&#39;$\beta_{Limit}$&#39;)
        ax.legend()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;make_fig_3_15()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/01_regression_178_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;As we can see, in the left plot the minimum is much better defined than in the right plot.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Matrix Algebra</title>
      <link>https://matteocourthoud.github.io/course/metrics/01_matrices/</link>
      <pubDate>Fri, 29 Oct 2021 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/course/metrics/01_matrices/</guid>
      <description>&lt;h2 id=&#34;basics&#34;&gt;Basics&lt;/h2&gt;
&lt;h3 id=&#34;matrix-definition&#34;&gt;Matrix Definition&lt;/h3&gt;
&lt;p&gt;A real $n \times m$ matrix $A$ is an array&lt;/p&gt;
&lt;p&gt;$$
A=
\begin{bmatrix}
a_{11} &amp;amp; a_{12} &amp;amp; \dots  &amp;amp; a_{1m} \newline
a_{21} &amp;amp; a_{22} &amp;amp; \dots  &amp;amp; a_{2m} \newline
\vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots \newline
a_{n1} &amp;amp; a_{n2} &amp;amp; \dots  &amp;amp; a_{nm}
\end{bmatrix}
$$&lt;/p&gt;
&lt;p&gt;We write $[A]_ {ij} = a_ {ij}$ to indicate the $(i,j)$-element of $A$.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;We will usually take the convention that a real vector
$x \in \mathbb R^n$ is identified with an $n \times 1$ matrix.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The $n \times n$ &lt;strong&gt;identity matrix&lt;/strong&gt; $I_n$ is given by&lt;br&gt;
$$
[I_n] _ {ij} = \begin{cases} 1 \ \ \ \text{if} \ i=j \newline
0 \ \ \ \text{if} \ i \neq j \end{cases}
$$&lt;/p&gt;
&lt;h3 id=&#34;fundamental-operations&#34;&gt;Fundamental Operations&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;Two $n \times m$ matrices, $A,B$, are added element-wise so that
$[A+B]_{ij} = [A] _{ij} + [B] _{ij}$.&lt;/li&gt;
&lt;li&gt;A matrix $A$ can be multiplied by a scalar $c\in \mathbb{R}$ in
which case we set $[cA]_{ij} = c[A] _{ij}$.&lt;/li&gt;
&lt;li&gt;An $n \times m$ matrix $A$ can be multiplied with an $m \times p$
matrix $B$.&lt;/li&gt;
&lt;li&gt;The product $AB$ is defined according to the rule
$[AB] _ {ij} = \sum_{k=1}^m [A] _{ik} [B] _{kj}$.&lt;/li&gt;
&lt;li&gt;An $n \times n$ matrix is invertible if there exists a matrix $B$
such that $AB=I$. In this case, we use the notational convention of
writing $B = A^{-1}$.&lt;/li&gt;
&lt;li&gt;Matrix transposition is defined by $[A&amp;rsquo;] _{ij} = [A] _{ji}$.&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;trace-and-determinant&#34;&gt;Trace and Determinant&lt;/h3&gt;
&lt;p&gt;The &lt;strong&gt;trace&lt;/strong&gt; of a square matrix $A$ with dimension $n \times n$ is
$\text{tr}(A) = \sum_{i=1}^n a_{ii}$.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;determinant&lt;/strong&gt; of a square $n \times n$ matrix A is defined
according to one of the following three (equivalent) definitions.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Recursively as
$det(A) = \sum_{i=1}^n a_{ij} (-1)^{i+j} det([A]&lt;em&gt;{-i,-j})$ where
$[A]&lt;/em&gt;{-i,-j}$ is the matrix obtained by deleting the $i$th row and
the $j$th column.&lt;/li&gt;
&lt;li&gt;$A \mapsto det(A)$ under the unique alternating multilinear map on
$n \times n$ matrices such that $I \mapsto 1$.&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;linear-independence&#34;&gt;Linear Independence&lt;/h3&gt;
&lt;p&gt;Vectors $x_1,&amp;hellip;,x_k$ are &lt;strong&gt;linearly independent&lt;/strong&gt; if the only solution
to the equation $b_1x_1 + &amp;hellip; + b_k x_k=0, \ b_j \in \mathbb R$, is
$b_1=b_2=&amp;hellip;=b_k=0$.&lt;/p&gt;
&lt;h3 id=&#34;useful-identities&#34;&gt;Useful Identities&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;$(A+B)&amp;rsquo; =A&amp;rsquo;+B&#39;$&lt;/li&gt;
&lt;li&gt;$(AB)C = A(BC)$&lt;/li&gt;
&lt;li&gt;$A(B+C) = AB+AC$&lt;/li&gt;
&lt;li&gt;$(AB&amp;rsquo;) = B&amp;rsquo;A&#39;$&lt;/li&gt;
&lt;li&gt;$(A^{-1})&amp;rsquo; = (A&amp;rsquo;)^{-1}$&lt;/li&gt;
&lt;li&gt;$(AB)^{-1} = B^{-1}A^{-1}$&lt;/li&gt;
&lt;li&gt;$\text{tr}(cA) = c\text{tr}(A)$&lt;/li&gt;
&lt;li&gt;$\text{tr}(A+B) = \text{tr}(A) + \text{tr}(B)$&lt;/li&gt;
&lt;li&gt;$\text{tr}(AB) =\text{tr}(BA)$&lt;/li&gt;
&lt;li&gt;$det(I)=1$&lt;/li&gt;
&lt;li&gt;$det(cA) = c^ndet(A)$ if $A$ is $n \times n$ and $c \in \mathbb R$&lt;/li&gt;
&lt;li&gt;$det(A) = det(A&amp;rsquo;)$&lt;/li&gt;
&lt;li&gt;$det(AB) = det(A)det(B)$&lt;/li&gt;
&lt;li&gt;$det(A^{-1}) = (det(A))^{-1}$&lt;/li&gt;
&lt;li&gt;$A^{-1}$ exists iff $det(A) \neq 0$&lt;/li&gt;
&lt;li&gt;$rank(A) = rank(A&amp;rsquo;) = rank(A&amp;rsquo;A) = rank(AA&amp;rsquo;)$&lt;/li&gt;
&lt;li&gt;$A^{-1}$ exists iff $rank(A)=n$ for $A$ $n \times n$&lt;/li&gt;
&lt;li&gt;$rank(AB) \leq \min \lbrace rank(A), rank(B) \rbrace$&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;matrix-rank&#34;&gt;Matrix Rank&lt;/h3&gt;
&lt;p&gt;The &lt;strong&gt;rank&lt;/strong&gt; of a matrix, $rank(A)$ is equal to the maximal number of
linearly independent rows for $A$.&lt;/p&gt;
&lt;p&gt;Let $A$ be an $n \times n$ matrix. The $n \times 1$ vector $x \neq 0$ is
an &lt;strong&gt;eigenvector&lt;/strong&gt; of $A$ with corresponding &lt;strong&gt;eigenvalue&lt;/strong&gt; $\lambda$ is
$Ax = \lambda x$.&lt;/p&gt;
&lt;h3 id=&#34;definitions&#34;&gt;Definitions&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;A matrix $A$ is diagonal if $[A]_ {ij} \neq 0$ only if $i=j$.&lt;/li&gt;
&lt;li&gt;An $n \times n$ matrix $A$ is orthogonal if $A&amp;rsquo;A = I$&lt;/li&gt;
&lt;li&gt;A matrix $A$ is symmetric if $[A]_ {ij} = [A]_ {ji}$.&lt;/li&gt;
&lt;li&gt;An $n \times n$ matrix $A$ is idempotent if $A^2=A$.&lt;/li&gt;
&lt;li&gt;The matrix of zeros ($[A]_ {ij} =0$ for each $i,j$) is simply
denoted 0.&lt;/li&gt;
&lt;li&gt;An $n \times n$ matrix $A$ is nilpotent if $A^k=0$ for some integer
$k&amp;gt;0$.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;spectral-decomposition&#34;&gt;Spectral Decomposition&lt;/h2&gt;
&lt;h3 id=&#34;spectral-theorem&#34;&gt;Spectral Theorem&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt;: Let $A$ be an $n \times n$ symmetric matrix. Then $A$ can
be factored as $A = C \Lambda C&amp;rsquo;$ where $C$ is orthogonal and $\Lambda$
is diagonal.&lt;/p&gt;
&lt;p&gt;If we postmultiply $A$ by $C$, we get&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$AC = C \Lambda C&amp;rsquo;C$ and&lt;/li&gt;
&lt;li&gt;$AC = C \Lambda$.&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;This is a matrix equation which can be split into columns. The $i$th
column of the equation reads $A c_i = \lambda_i c_i$ which corresponds
to the definition of eigenvalues and eigenvectors. So if the
decomposition exists, then $C$ is the eigenvector matrix and $\Lambda$
contains the eigenvalues.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;rank-and-trace&#34;&gt;Rank and Trace&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt;: The rank of a symmetric matrix equals the number of non
zero eigenvalues.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;:
$rank(A) = rank(C\Lambda C&amp;rsquo;) = rank(\Lambda) = | \lbrace i: \lambda_i \neq 0 \rbrace |$.
$$\tag*{$\blacksquare$}$$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt;: The nonzero eigenvalues of $AA&amp;rsquo;$ and $A&amp;rsquo;A$ are identical.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt;: The trace of a symmetric matrix equals the sum of its
eignevalues.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;:
$tr(A) = tr(C \Lambda C&amp;rsquo;) = tr((C \Lambda)C&amp;rsquo;) = tr(C&amp;rsquo;C \Lambda) = tr(\Lambda) = \sum_ {i=1}^n \lambda_i.$
$$\tag*{$\blacksquare$}$$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt;: The determinant of a symmetric matrix equals the product of
its eignevalues.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;:
$det(A) = det(C \Lambda C&amp;rsquo;) = det(C)det(\Lambda)det(C&amp;rsquo;) = det(C)det(C&amp;rsquo;)det(\Lambda) = det(CC&amp;rsquo;) det(\Lambda) = det(I)det(\Lambda) = det(\Lambda) = \prod_ {i=1}^n \lambda_i.$
$$\tag*{$\blacksquare$}$$&lt;/p&gt;
&lt;h3 id=&#34;eigenvalues&#34;&gt;Eigenvalues&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt;: For any symmetric matrix $A$, the eigenvalues of $A^2$ are
the square of the eignevalues of $A$, and the eigenvectors are the same.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;:
$A = C \Lambda C&amp;rsquo; \implies A^2 = C \Lambda C&amp;rsquo; C \Lambda C&amp;rsquo; = C \Lambda I \Lambda C&amp;rsquo; = C \Lambda^2 C&amp;rsquo;$
$$\tag*{$\blacksquare$}$$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt;: For any symmetric matrix $A$, and any integer $k&amp;gt;0$, the
eigenvalues of $A^k$ are the $k$th power of the eigenvalues of $A$, and
the eigenvectors are the same.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt;: Any square symmetric matrix $A$ with positive eigenvalues
can be written as the product of a lower triangular matrix $L$ and its
(upper triangular) transpose $L&amp;rsquo; = U$. That is $A = LU = LL&#39;$&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Note that $$
A = LL&amp;rsquo; = LU = U&amp;rsquo;U  = (L&amp;rsquo;)^{-1}L^{-1} = U^{-1}(U&amp;rsquo;)^{-1}
$$ where $L^{-1}$ is lower triangular and $U^{ -1}$ is upper
trianguar. You can check this for the $2 \times 2$ case. Also note
that the validity of the theorem can be extended to symmetric matrices
with non- negative eigenvalues by a limiting argument. However, then
the proof is not constructive anymore.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;quadratic-forms-and-definite-matrices&#34;&gt;Quadratic Forms and Definite Matrices&lt;/h2&gt;
&lt;h3 id=&#34;definition&#34;&gt;Definition&lt;/h3&gt;
&lt;p&gt;A &lt;strong&gt;quadratic form&lt;/strong&gt; in the $n \times n$ matrix $A$ and $n \times 1$
vector $x$ is defined by the scalar $x&amp;rsquo;Ax$.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;$A$ is negative definite (ND) if for each $x \neq 0$, $x&amp;rsquo;Ax &amp;lt; 0$&lt;/li&gt;
&lt;li&gt;$A$ is negative semidefinite (NSD) if for each $x \neq 0$,
$x&amp;rsquo;Ax \leq 0$&lt;/li&gt;
&lt;li&gt;$A$ is positive definite (PD) if for each $x \neq 0$, $x&amp;rsquo;Ax &amp;gt; 0$&lt;/li&gt;
&lt;li&gt;$A$ is positive semidefinite (PSD) if for each $x \neq 0$,
$x&amp;rsquo;Ax \geq 0$&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;equivalence&#34;&gt;Equivalence&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt;: Let $A$ be a symmetric matrix. Then $A$ is PD(ND) $\iff$
all of its eigenvalues are positive (negative).&lt;/p&gt;
&lt;p&gt;Some more results:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;If a symmetric matrix $A$ is PD (PSD, ND, NSD), then
$\text{det}(A) &amp;gt;(\geq,&amp;lt;,\leq) 0$.&lt;/li&gt;
&lt;li&gt;If symmetric matrix $A$ is PD (ND) then $A^{-1}$ is symmetric PD
(ND).&lt;/li&gt;
&lt;li&gt;The identity matrix is PD (since all eigenvalues are equal to 1).&lt;/li&gt;
&lt;li&gt;Every symmetric idempotent matrix is PSD (since the eigenvalues are
only 0 or 1).&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt;: If $A$ is $n\times k$ with $n&amp;gt;k$ and $rank(A)=k$, then
$A&amp;rsquo;A$ is PD and $AA&amp;rsquo;$ is PSD.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;semidefinite partial order&lt;/strong&gt; is defined by $A \geq B$ iff $A-B$ is
PSD.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt;: Let $A$, $B$ be symmetric,square , PD, conformable. Then
$A-B$ is PD iff $A^{-1}-B^{-1}$ is PD.&lt;/p&gt;
&lt;h2 id=&#34;matrix-calculus&#34;&gt;Matrix Calculus&lt;/h2&gt;
&lt;h3 id=&#34;comformable-matrices&#34;&gt;Comformable Matrices&lt;/h3&gt;
&lt;p&gt;We first define matrices blockwise when they are conformable. In
particular, we assume that if $A_1, A_2, A_3, A_4$ are matrices with
appropriate dimensions then the matrix $$
A = \begin{bmatrix} A_1 &amp;amp; A_1 \newline
A_3 &amp;amp; A_4 \end{bmatrix}
$$ is defined in the obvious way.&lt;/p&gt;
&lt;h3 id=&#34;matrix-functions&#34;&gt;Matrix Functions&lt;/h3&gt;
&lt;p&gt;Let
$F: \mathbb R^m \times \mathbb R^n \rightarrow \mathbb R^p \times \mathbb R^q$
be a matrix valued function. More precisely, given a real $m \times n$
matrix $X$, $F(X)$ returns the $p \times q$ matrix&lt;br&gt;
$$
\begin{bmatrix}
f_ {11}(X) &amp;amp; &amp;hellip; &amp;amp; f_ {1q}(X) \newline \vdots &amp;amp; \ddots &amp;amp; \vdots \newline
f_ {p1}(X)&amp;amp; &amp;hellip; &amp;amp; f_ {pq}(X)
\end{bmatrix}
$$&lt;/p&gt;
&lt;h3 id=&#34;matrix-derivatives&#34;&gt;Matrix Derivatives&lt;/h3&gt;
&lt;p&gt;The derivative of $F$ with respect to the matrix $X$ is the
$mp \times nq$ matrix $$
\frac{\partial F(X)}{\partial X} = \begin{bmatrix}
\frac{\partial F(X)}{\partial x_ {11}} &amp;amp; &amp;hellip; &amp;amp; \frac{\partial F(X)}{\partial x_ {1n}} \newline \vdots &amp;amp; \ddots &amp;amp; \vdots \newline
\frac{\partial F(X)}{\partial x_ {m1}} &amp;amp; &amp;hellip; &amp;amp; \frac{\partial F(X)}{\partial x_ {mn}}
\end{bmatrix}
$$ where each $\frac{\partial F(X)}{\partial x_ {ij}}$ is a $p\times q$
matrix given by&lt;br&gt;
$$
\frac{\partial F(X)}{\partial x_ {ij}} = \begin{bmatrix}
\frac{\partial f_ {11}(X)}{\partial x_ {ij}} &amp;amp; &amp;hellip; &amp;amp; \frac{\partial f_ {1q}(X)}{\partial x_ {ij}} \newline
\vdots &amp;amp; \ddots &amp;amp; \vdots \newline
\frac{\partial f_ {p1}(X)}{\partial x_ {ij}} &amp;amp; &amp;hellip; &amp;amp; \frac{\partial f_ {pq}(X)}{\partial x_ {ij}}
\end{bmatrix}
$$ The most important case is when
$F: \mathbb R^n \rightarrow \mathbb R$ since this simplifies the
derivation of the least squares estimator. Also, the trickiest thing is
to make sure that dimensions are correct.&lt;/p&gt;
&lt;h3 id=&#34;useful-results-in-matrix-calculus&#34;&gt;Useful Results in Matrix Calculus&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;$\frac{\partial b&amp;rsquo;x}{\partial x}= b$ for $dim(b) = dim(x)$&lt;/li&gt;
&lt;li&gt;$\frac{\partial B&amp;rsquo;x}{\partial x}= B$ for arbitrary, conformable $B$&lt;/li&gt;
&lt;li&gt;$\frac{\partial B&amp;rsquo;x}{\partial x&amp;rsquo;}= B&amp;rsquo;$ for arbitrary, conformable
$B$&lt;/li&gt;
&lt;li&gt;$\frac{\partial x&amp;rsquo;Ax}{\partial x} = (A + A&amp;rsquo;)x$&lt;/li&gt;
&lt;li&gt;$\frac{\partial x&amp;rsquo;Ax}{\partial A} = xx&#39;$&lt;/li&gt;
&lt;li&gt;$\frac{\partial x&amp;rsquo;Ax}{\partial x} = det(A) (A^{-1})&#39;$&lt;/li&gt;
&lt;li&gt;$\frac{\partial \ln det(A)}{\partial A} = (A^{-1})&#39;$&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>Data Exploration</title>
      <link>https://matteocourthoud.github.io/course/data-science/02_data_exploration/</link>
      <pubDate>Sun, 01 May 2022 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/course/data-science/02_data_exploration/</guid>
      <description>&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import numpy as np
import pandas as pd
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;setup&#34;&gt;Setup&lt;/h2&gt;
&lt;p&gt;For the scope of this tutorial we are going to use AirBnb Scraped data for the city of Bologna. The data is freely available at &lt;strong&gt;Inside AirBnb&lt;/strong&gt;: &lt;a href=&#34;http://insideairbnb.com/get-the-data.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://insideairbnb.com/get-the-data.html&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;A description of all variables in all datasets is avaliable &lt;a href=&#34;https://docs.google.com/spreadsheets/d/1iWCNJcSutYqpULSQHlNyGInUvHg2BoUGoNRIGa6Szc4/edit#gid=982310896&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;We are going to use 2 datasets:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;listing dataset: contains listing-level information&lt;/li&gt;
&lt;li&gt;pricing dataset: contains pricing data, over time&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;importing-data&#34;&gt;Importing Data&lt;/h2&gt;
&lt;p&gt;Pandas has a variety of function to import data&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;pd.read_csv()&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;pd.read_html()&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;pd.read_parquet()&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Importatly for our purpose, &lt;code&gt;pd.read_csv()&lt;/code&gt; can directly import data from the web.&lt;/p&gt;
&lt;p&gt;The first dataset that we are going to import is the dataset of Airbnb listings in Bologna. It contains listing-level information.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;url_listings = &amp;quot;http://data.insideairbnb.com/italy/emilia-romagna/bologna/2021-12-17/visualisations/listings.csv&amp;quot;
df_listings = pd.read_csv(url_listings)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The second dataset that we are going to use is the dataset of calendar prices. This time the dataset is compressed but we can use the &lt;code&gt;compression&lt;/code&gt; option to import it directly.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;url_prices = &amp;quot;http://data.insideairbnb.com/italy/emilia-romagna/bologna/2021-12-17/data/calendar.csv.gz&amp;quot;
df_prices = pd.read_csv(url_prices, compression=&amp;quot;gzip&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;inspecting-data&#34;&gt;Inspecting Data&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Methods&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;info()&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;head()&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;describe()&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;p&gt;The first way yo have a quick look at the data is the &lt;code&gt;info()&lt;/code&gt; method. If called with the option &lt;code&gt;verbose=False&lt;/code&gt;, it gives a quick overview of the dimensions of the data.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_listings.info(verbose=False)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;class &#39;pandas.core.frame.DataFrame&#39;&amp;gt;
RangeIndex: 3453 entries, 0 to 3452
Columns: 18 entries, id to license
dtypes: float64(4), int64(8), object(6)
memory usage: 485.7+ KB
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If we want to know how the data looks like, we can use the &lt;code&gt;head()&lt;/code&gt; method. It prints the first 5 lines of the data by default.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_listings.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;id&lt;/th&gt;
      &lt;th&gt;name&lt;/th&gt;
      &lt;th&gt;host_id&lt;/th&gt;
      &lt;th&gt;host_name&lt;/th&gt;
      &lt;th&gt;neighbourhood_group&lt;/th&gt;
      &lt;th&gt;neighbourhood&lt;/th&gt;
      &lt;th&gt;latitude&lt;/th&gt;
      &lt;th&gt;longitude&lt;/th&gt;
      &lt;th&gt;room_type&lt;/th&gt;
      &lt;th&gt;price&lt;/th&gt;
      &lt;th&gt;minimum_nights&lt;/th&gt;
      &lt;th&gt;number_of_reviews&lt;/th&gt;
      &lt;th&gt;last_review&lt;/th&gt;
      &lt;th&gt;reviews_per_month&lt;/th&gt;
      &lt;th&gt;calculated_host_listings_count&lt;/th&gt;
      &lt;th&gt;availability_365&lt;/th&gt;
      &lt;th&gt;number_of_reviews_ltm&lt;/th&gt;
      &lt;th&gt;license&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;42196&lt;/td&gt;
      &lt;td&gt;50 sm Studio in the historic centre&lt;/td&gt;
      &lt;td&gt;184487&lt;/td&gt;
      &lt;td&gt;Carlo&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;Santo Stefano&lt;/td&gt;
      &lt;td&gt;44.48507&lt;/td&gt;
      &lt;td&gt;11.34786&lt;/td&gt;
      &lt;td&gt;Entire home/apt&lt;/td&gt;
      &lt;td&gt;68&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;180&lt;/td&gt;
      &lt;td&gt;2021-11-12&lt;/td&gt;
      &lt;td&gt;1.32&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;161&lt;/td&gt;
      &lt;td&gt;6&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;46352&lt;/td&gt;
      &lt;td&gt;A room in Pasolini&#39;s house&lt;/td&gt;
      &lt;td&gt;467810&lt;/td&gt;
      &lt;td&gt;Eleonora&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;Porto - Saragozza&lt;/td&gt;
      &lt;td&gt;44.49168&lt;/td&gt;
      &lt;td&gt;11.33514&lt;/td&gt;
      &lt;td&gt;Private room&lt;/td&gt;
      &lt;td&gt;29&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;300&lt;/td&gt;
      &lt;td&gt;2021-11-30&lt;/td&gt;
      &lt;td&gt;2.20&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;248&lt;/td&gt;
      &lt;td&gt;37&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;59697&lt;/td&gt;
      &lt;td&gt;COZY LARGE BEDROOM in the city center&lt;/td&gt;
      &lt;td&gt;286688&lt;/td&gt;
      &lt;td&gt;Paolo&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;Santo Stefano&lt;/td&gt;
      &lt;td&gt;44.48817&lt;/td&gt;
      &lt;td&gt;11.34124&lt;/td&gt;
      &lt;td&gt;Private room&lt;/td&gt;
      &lt;td&gt;50&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;240&lt;/td&gt;
      &lt;td&gt;2020-10-04&lt;/td&gt;
      &lt;td&gt;2.18&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;327&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;85368&lt;/td&gt;
      &lt;td&gt;Garden House Bologna&lt;/td&gt;
      &lt;td&gt;467675&lt;/td&gt;
      &lt;td&gt;Anna Maria&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;Santo Stefano&lt;/td&gt;
      &lt;td&gt;44.47834&lt;/td&gt;
      &lt;td&gt;11.35672&lt;/td&gt;
      &lt;td&gt;Entire home/apt&lt;/td&gt;
      &lt;td&gt;126&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;40&lt;/td&gt;
      &lt;td&gt;2019-11-03&lt;/td&gt;
      &lt;td&gt;0.34&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;332&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;145779&lt;/td&gt;
      &lt;td&gt;SINGLE ROOM&lt;/td&gt;
      &lt;td&gt;705535&lt;/td&gt;
      &lt;td&gt;Valerio&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;Porto - Saragozza&lt;/td&gt;
      &lt;td&gt;44.49306&lt;/td&gt;
      &lt;td&gt;11.33786&lt;/td&gt;
      &lt;td&gt;Private room&lt;/td&gt;
      &lt;td&gt;50&lt;/td&gt;
      &lt;td&gt;10&lt;/td&gt;
      &lt;td&gt;69&lt;/td&gt;
      &lt;td&gt;2021-12-05&lt;/td&gt;
      &lt;td&gt;0.55&lt;/td&gt;
      &lt;td&gt;9&lt;/td&gt;
      &lt;td&gt;365&lt;/td&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;We can print a description of the data using &lt;code&gt;describe()&lt;/code&gt;. If we have many variables, it&amp;rsquo;s best to print it transposed using the &lt;code&gt;.T&lt;/code&gt; attribute.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_listings.describe().T[:5]
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;count&lt;/th&gt;
      &lt;th&gt;mean&lt;/th&gt;
      &lt;th&gt;std&lt;/th&gt;
      &lt;th&gt;min&lt;/th&gt;
      &lt;th&gt;25%&lt;/th&gt;
      &lt;th&gt;50%&lt;/th&gt;
      &lt;th&gt;75%&lt;/th&gt;
      &lt;th&gt;max&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;id&lt;/th&gt;
      &lt;td&gt;3453.0&lt;/td&gt;
      &lt;td&gt;2.950218e+07&lt;/td&gt;
      &lt;td&gt;1.523988e+07&lt;/td&gt;
      &lt;td&gt;42196.0000&lt;/td&gt;
      &lt;td&gt;1.748597e+07&lt;/td&gt;
      &lt;td&gt;3.078707e+07&lt;/td&gt;
      &lt;td&gt;4.220094e+07&lt;/td&gt;
      &lt;td&gt;5.385496e+07&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;host_id&lt;/th&gt;
      &lt;td&gt;3453.0&lt;/td&gt;
      &lt;td&gt;1.236424e+08&lt;/td&gt;
      &lt;td&gt;1.160756e+08&lt;/td&gt;
      &lt;td&gt;38468.0000&lt;/td&gt;
      &lt;td&gt;2.550007e+07&lt;/td&gt;
      &lt;td&gt;8.845438e+07&lt;/td&gt;
      &lt;td&gt;2.005926e+08&lt;/td&gt;
      &lt;td&gt;4.354316e+08&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;neighbourhood_group&lt;/th&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;latitude&lt;/th&gt;
      &lt;td&gt;3453.0&lt;/td&gt;
      &lt;td&gt;4.449756e+01&lt;/td&gt;
      &lt;td&gt;1.173569e-02&lt;/td&gt;
      &lt;td&gt;44.4236&lt;/td&gt;
      &lt;td&gt;4.449186e+01&lt;/td&gt;
      &lt;td&gt;4.449699e+01&lt;/td&gt;
      &lt;td&gt;4.450271e+01&lt;/td&gt;
      &lt;td&gt;4.455093e+01&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;longitude&lt;/th&gt;
      &lt;td&gt;3453.0&lt;/td&gt;
      &lt;td&gt;1.134509e+01&lt;/td&gt;
      &lt;td&gt;1.986071e-02&lt;/td&gt;
      &lt;td&gt;11.2320&lt;/td&gt;
      &lt;td&gt;1.133732e+01&lt;/td&gt;
      &lt;td&gt;1.134519e+01&lt;/td&gt;
      &lt;td&gt;1.135406e+01&lt;/td&gt;
      &lt;td&gt;1.142027e+01&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;You can select which variables to display using the &lt;code&gt;include&lt;/code&gt; option. &lt;code&gt;include=&#39;all&#39;&lt;/code&gt; includes also categorical variables.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_listings.describe(include=&#39;all&#39;).T[:5]
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;count&lt;/th&gt;
      &lt;th&gt;unique&lt;/th&gt;
      &lt;th&gt;top&lt;/th&gt;
      &lt;th&gt;freq&lt;/th&gt;
      &lt;th&gt;mean&lt;/th&gt;
      &lt;th&gt;std&lt;/th&gt;
      &lt;th&gt;min&lt;/th&gt;
      &lt;th&gt;25%&lt;/th&gt;
      &lt;th&gt;50%&lt;/th&gt;
      &lt;th&gt;75%&lt;/th&gt;
      &lt;th&gt;max&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;id&lt;/th&gt;
      &lt;td&gt;3453.0&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;29502177.118158&lt;/td&gt;
      &lt;td&gt;15239877.346777&lt;/td&gt;
      &lt;td&gt;42196.0&lt;/td&gt;
      &lt;td&gt;17485973.0&lt;/td&gt;
      &lt;td&gt;30787074.0&lt;/td&gt;
      &lt;td&gt;42200938.0&lt;/td&gt;
      &lt;td&gt;53854962.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;name&lt;/th&gt;
      &lt;td&gt;3453&lt;/td&gt;
      &lt;td&gt;3410&lt;/td&gt;
      &lt;td&gt;Luxury Industrial Design LOFT, HEPA UV airpuri...&lt;/td&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;host_id&lt;/th&gt;
      &lt;td&gt;3453.0&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;123642405.854619&lt;/td&gt;
      &lt;td&gt;116075571.230048&lt;/td&gt;
      &lt;td&gt;38468.0&lt;/td&gt;
      &lt;td&gt;25500072.0&lt;/td&gt;
      &lt;td&gt;88454378.0&lt;/td&gt;
      &lt;td&gt;200592620.0&lt;/td&gt;
      &lt;td&gt;435431590.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;host_name&lt;/th&gt;
      &lt;td&gt;3444&lt;/td&gt;
      &lt;td&gt;747&lt;/td&gt;
      &lt;td&gt;Andrea&lt;/td&gt;
      &lt;td&gt;101&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;neighbourhood_group&lt;/th&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;We can get the list of columns using the &lt;code&gt;.columns&lt;/code&gt; attribute.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_listings.columns
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Index([&#39;id&#39;, &#39;name&#39;, &#39;host_id&#39;, &#39;host_name&#39;, &#39;neighbourhood_group&#39;,
       &#39;neighbourhood&#39;, &#39;latitude&#39;, &#39;longitude&#39;, &#39;room_type&#39;, &#39;price&#39;,
       &#39;minimum_nights&#39;, &#39;number_of_reviews&#39;, &#39;last_review&#39;,
       &#39;reviews_per_month&#39;, &#39;calculated_host_listings_count&#39;,
       &#39;availability_365&#39;, &#39;number_of_reviews_ltm&#39;, &#39;license&#39;],
      dtype=&#39;object&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can get the index using the &lt;code&gt;.index&lt;/code&gt; attribute,&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_listings.index
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;RangeIndex(start=0, stop=3453, step=1)
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;data-selection&#34;&gt;Data Selection&lt;/h2&gt;
&lt;p&gt;We can access single columns as if the DataFrame was a dictionary.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_listings[&#39;price&#39;]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;0        68
1        29
2        50
3       126
4        50
       ... 
3448     32
3449     45
3450     50
3451    134
3452    115
Name: price, Length: 3453, dtype: int64
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can select rows and columns by index, using the &lt;code&gt;.iloc&lt;/code&gt; attribute.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_listings.iloc[:7, 5:9]
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;neighbourhood&lt;/th&gt;
      &lt;th&gt;latitude&lt;/th&gt;
      &lt;th&gt;longitude&lt;/th&gt;
      &lt;th&gt;room_type&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;Santo Stefano&lt;/td&gt;
      &lt;td&gt;44.48507&lt;/td&gt;
      &lt;td&gt;11.34786&lt;/td&gt;
      &lt;td&gt;Entire home/apt&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;Porto - Saragozza&lt;/td&gt;
      &lt;td&gt;44.49168&lt;/td&gt;
      &lt;td&gt;11.33514&lt;/td&gt;
      &lt;td&gt;Private room&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;Santo Stefano&lt;/td&gt;
      &lt;td&gt;44.48817&lt;/td&gt;
      &lt;td&gt;11.34124&lt;/td&gt;
      &lt;td&gt;Private room&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;Santo Stefano&lt;/td&gt;
      &lt;td&gt;44.47834&lt;/td&gt;
      &lt;td&gt;11.35672&lt;/td&gt;
      &lt;td&gt;Entire home/apt&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;Porto - Saragozza&lt;/td&gt;
      &lt;td&gt;44.49306&lt;/td&gt;
      &lt;td&gt;11.33786&lt;/td&gt;
      &lt;td&gt;Private room&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;5&lt;/th&gt;
      &lt;td&gt;Navile&lt;/td&gt;
      &lt;td&gt;44.51628&lt;/td&gt;
      &lt;td&gt;11.33074&lt;/td&gt;
      &lt;td&gt;Private room&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;6&lt;/th&gt;
      &lt;td&gt;Santo Stefano&lt;/td&gt;
      &lt;td&gt;44.48787&lt;/td&gt;
      &lt;td&gt;11.35392&lt;/td&gt;
      &lt;td&gt;Entire home/apt&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;If we want to condition only on rows or columns, we have use &lt;code&gt;:&lt;/code&gt; for the unrestricted dimesion, otherwise we get an error.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_listings.iloc[:, 5:9].head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;neighbourhood&lt;/th&gt;
      &lt;th&gt;latitude&lt;/th&gt;
      &lt;th&gt;longitude&lt;/th&gt;
      &lt;th&gt;room_type&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;Santo Stefano&lt;/td&gt;
      &lt;td&gt;44.48507&lt;/td&gt;
      &lt;td&gt;11.34786&lt;/td&gt;
      &lt;td&gt;Entire home/apt&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;Porto - Saragozza&lt;/td&gt;
      &lt;td&gt;44.49168&lt;/td&gt;
      &lt;td&gt;11.33514&lt;/td&gt;
      &lt;td&gt;Private room&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;Santo Stefano&lt;/td&gt;
      &lt;td&gt;44.48817&lt;/td&gt;
      &lt;td&gt;11.34124&lt;/td&gt;
      &lt;td&gt;Private room&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;Santo Stefano&lt;/td&gt;
      &lt;td&gt;44.47834&lt;/td&gt;
      &lt;td&gt;11.35672&lt;/td&gt;
      &lt;td&gt;Entire home/apt&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;Porto - Saragozza&lt;/td&gt;
      &lt;td&gt;44.49306&lt;/td&gt;
      &lt;td&gt;11.33786&lt;/td&gt;
      &lt;td&gt;Private room&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;Instead, the &lt;code&gt;.loc&lt;/code&gt; attribute allows us to use row and column names.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_listings.loc[:, [&#39;neighbourhood&#39;, &#39;latitude&#39;, &#39;longitude&#39;]].head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;neighbourhood&lt;/th&gt;
      &lt;th&gt;latitude&lt;/th&gt;
      &lt;th&gt;longitude&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;Santo Stefano&lt;/td&gt;
      &lt;td&gt;44.48507&lt;/td&gt;
      &lt;td&gt;11.34786&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;Porto - Saragozza&lt;/td&gt;
      &lt;td&gt;44.49168&lt;/td&gt;
      &lt;td&gt;11.33514&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;Santo Stefano&lt;/td&gt;
      &lt;td&gt;44.48817&lt;/td&gt;
      &lt;td&gt;11.34124&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;Santo Stefano&lt;/td&gt;
      &lt;td&gt;44.47834&lt;/td&gt;
      &lt;td&gt;11.35672&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;Porto - Saragozza&lt;/td&gt;
      &lt;td&gt;44.49306&lt;/td&gt;
      &lt;td&gt;11.33786&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;We can also select ranges.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_listings.loc[:, &#39;neighbourhood&#39;:&#39;room_type&#39;].head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;neighbourhood&lt;/th&gt;
      &lt;th&gt;latitude&lt;/th&gt;
      &lt;th&gt;longitude&lt;/th&gt;
      &lt;th&gt;room_type&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;Santo Stefano&lt;/td&gt;
      &lt;td&gt;44.48507&lt;/td&gt;
      &lt;td&gt;11.34786&lt;/td&gt;
      &lt;td&gt;Entire home/apt&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;Porto - Saragozza&lt;/td&gt;
      &lt;td&gt;44.49168&lt;/td&gt;
      &lt;td&gt;11.33514&lt;/td&gt;
      &lt;td&gt;Private room&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;Santo Stefano&lt;/td&gt;
      &lt;td&gt;44.48817&lt;/td&gt;
      &lt;td&gt;11.34124&lt;/td&gt;
      &lt;td&gt;Private room&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;Santo Stefano&lt;/td&gt;
      &lt;td&gt;44.47834&lt;/td&gt;
      &lt;td&gt;11.35672&lt;/td&gt;
      &lt;td&gt;Entire home/apt&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;Porto - Saragozza&lt;/td&gt;
      &lt;td&gt;44.49306&lt;/td&gt;
      &lt;td&gt;11.33786&lt;/td&gt;
      &lt;td&gt;Private room&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;There is an easy way to &lt;strong&gt;select numerical columns&lt;/strong&gt;, the &lt;code&gt;.select_dtypes()&lt;/code&gt; function.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_listings.select_dtypes(include=[&#39;number&#39;]).head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;id&lt;/th&gt;
      &lt;th&gt;host_id&lt;/th&gt;
      &lt;th&gt;neighbourhood_group&lt;/th&gt;
      &lt;th&gt;latitude&lt;/th&gt;
      &lt;th&gt;longitude&lt;/th&gt;
      &lt;th&gt;price&lt;/th&gt;
      &lt;th&gt;minimum_nights&lt;/th&gt;
      &lt;th&gt;number_of_reviews&lt;/th&gt;
      &lt;th&gt;reviews_per_month&lt;/th&gt;
      &lt;th&gt;calculated_host_listings_count&lt;/th&gt;
      &lt;th&gt;availability_365&lt;/th&gt;
      &lt;th&gt;number_of_reviews_ltm&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;42196&lt;/td&gt;
      &lt;td&gt;184487&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;44.48507&lt;/td&gt;
      &lt;td&gt;11.34786&lt;/td&gt;
      &lt;td&gt;68&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;180&lt;/td&gt;
      &lt;td&gt;1.32&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;161&lt;/td&gt;
      &lt;td&gt;6&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;46352&lt;/td&gt;
      &lt;td&gt;467810&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;44.49168&lt;/td&gt;
      &lt;td&gt;11.33514&lt;/td&gt;
      &lt;td&gt;29&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;300&lt;/td&gt;
      &lt;td&gt;2.20&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;248&lt;/td&gt;
      &lt;td&gt;37&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;59697&lt;/td&gt;
      &lt;td&gt;286688&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;44.48817&lt;/td&gt;
      &lt;td&gt;11.34124&lt;/td&gt;
      &lt;td&gt;50&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;240&lt;/td&gt;
      &lt;td&gt;2.18&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;327&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;85368&lt;/td&gt;
      &lt;td&gt;467675&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;44.47834&lt;/td&gt;
      &lt;td&gt;11.35672&lt;/td&gt;
      &lt;td&gt;126&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;40&lt;/td&gt;
      &lt;td&gt;0.34&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;332&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;145779&lt;/td&gt;
      &lt;td&gt;705535&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;44.49306&lt;/td&gt;
      &lt;td&gt;11.33786&lt;/td&gt;
      &lt;td&gt;50&lt;/td&gt;
      &lt;td&gt;10&lt;/td&gt;
      &lt;td&gt;69&lt;/td&gt;
      &lt;td&gt;0.55&lt;/td&gt;
      &lt;td&gt;9&lt;/td&gt;
      &lt;td&gt;365&lt;/td&gt;
      &lt;td&gt;5&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;Other types include&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;object&lt;/code&gt; for strings&lt;/li&gt;
&lt;li&gt;&lt;code&gt;bool&lt;/code&gt; for booleans&lt;/li&gt;
&lt;li&gt;&lt;code&gt;int&lt;/code&gt; for integers&lt;/li&gt;
&lt;li&gt;&lt;code&gt;float&lt;/code&gt; for floats (numbers that are not integers)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We can also use logical operators to selet rows.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_listings.loc[df_listings[&#39;number_of_reviews&#39;]&amp;gt;500, :].head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;id&lt;/th&gt;
      &lt;th&gt;name&lt;/th&gt;
      &lt;th&gt;host_id&lt;/th&gt;
      &lt;th&gt;host_name&lt;/th&gt;
      &lt;th&gt;neighbourhood_group&lt;/th&gt;
      &lt;th&gt;neighbourhood&lt;/th&gt;
      &lt;th&gt;latitude&lt;/th&gt;
      &lt;th&gt;longitude&lt;/th&gt;
      &lt;th&gt;room_type&lt;/th&gt;
      &lt;th&gt;price&lt;/th&gt;
      &lt;th&gt;minimum_nights&lt;/th&gt;
      &lt;th&gt;number_of_reviews&lt;/th&gt;
      &lt;th&gt;last_review&lt;/th&gt;
      &lt;th&gt;reviews_per_month&lt;/th&gt;
      &lt;th&gt;calculated_host_listings_count&lt;/th&gt;
      &lt;th&gt;availability_365&lt;/th&gt;
      &lt;th&gt;number_of_reviews_ltm&lt;/th&gt;
      &lt;th&gt;license&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;52&lt;/th&gt;
      &lt;td&gt;884148&lt;/td&gt;
      &lt;td&gt;APOSA FLAT / CITY CENTER - BO&lt;/td&gt;
      &lt;td&gt;4664996&lt;/td&gt;
      &lt;td&gt;Vie D&#39;Acqua Di Sandra Maria&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;Santo Stefano&lt;/td&gt;
      &lt;td&gt;44.49945&lt;/td&gt;
      &lt;td&gt;11.34566&lt;/td&gt;
      &lt;td&gt;Entire home/apt&lt;/td&gt;
      &lt;td&gt;46&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;668&lt;/td&gt;
      &lt;td&gt;2021-12-11&lt;/td&gt;
      &lt;td&gt;6.24&lt;/td&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;252&lt;/td&gt;
      &lt;td&gt;20&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;92&lt;/th&gt;
      &lt;td&gt;1435627&lt;/td&gt;
      &lt;td&gt;heart of Bologna Piazza Maggiore&lt;/td&gt;
      &lt;td&gt;7714013&lt;/td&gt;
      &lt;td&gt;Carlotta&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;Porto - Saragozza&lt;/td&gt;
      &lt;td&gt;44.49321&lt;/td&gt;
      &lt;td&gt;11.33569&lt;/td&gt;
      &lt;td&gt;Entire home/apt&lt;/td&gt;
      &lt;td&gt;56&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;508&lt;/td&gt;
      &lt;td&gt;2021-12-12&lt;/td&gt;
      &lt;td&gt;5.08&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;131&lt;/td&gt;
      &lt;td&gt;69&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;98&lt;/th&gt;
      &lt;td&gt;1566003&lt;/td&gt;
      &lt;td&gt;&#34;i portici di via Piella &#34;&lt;/td&gt;
      &lt;td&gt;8325248&lt;/td&gt;
      &lt;td&gt;Massimo&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;Santo Stefano&lt;/td&gt;
      &lt;td&gt;44.49855&lt;/td&gt;
      &lt;td&gt;11.34411&lt;/td&gt;
      &lt;td&gt;Entire home/apt&lt;/td&gt;
      &lt;td&gt;51&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;764&lt;/td&gt;
      &lt;td&gt;2021-12-14&lt;/td&gt;
      &lt;td&gt;7.62&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;119&lt;/td&gt;
      &lt;td&gt;120&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;131&lt;/th&gt;
      &lt;td&gt;2282623&lt;/td&gt;
      &lt;td&gt;S.Orsola zone,parking for free and self check-in&lt;/td&gt;
      &lt;td&gt;11658074&lt;/td&gt;
      &lt;td&gt;Cecilia&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;San Donato - San Vitale&lt;/td&gt;
      &lt;td&gt;44.49328&lt;/td&gt;
      &lt;td&gt;11.36650&lt;/td&gt;
      &lt;td&gt;Entire home/apt&lt;/td&gt;
      &lt;td&gt;38&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;689&lt;/td&gt;
      &lt;td&gt;2021-10-24&lt;/td&gt;
      &lt;td&gt;7.20&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;72&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;175&lt;/th&gt;
      &lt;td&gt;3216486&lt;/td&gt;
      &lt;td&gt;Stanza Privata&lt;/td&gt;
      &lt;td&gt;16289536&lt;/td&gt;
      &lt;td&gt;Fabio&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;Navile&lt;/td&gt;
      &lt;td&gt;44.50903&lt;/td&gt;
      &lt;td&gt;11.34200&lt;/td&gt;
      &lt;td&gt;Private room&lt;/td&gt;
      &lt;td&gt;82&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;569&lt;/td&gt;
      &lt;td&gt;2021-12-05&lt;/td&gt;
      &lt;td&gt;6.93&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;7&lt;/td&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;We can use logical operations as well. But remember to use paranthesis.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: the &lt;code&gt;and&lt;/code&gt; and &lt;code&gt;or&lt;/code&gt; expressions do not work in this setting. We have to use &lt;code&gt;&amp;amp;&lt;/code&gt; and &lt;code&gt;|&lt;/code&gt; instead.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_listings.loc[(df_listings[&#39;number_of_reviews&#39;]&amp;gt;300) &amp;amp;
                (df_listings[&#39;reviews_per_month&#39;]&amp;gt;7), 
                :].head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;id&lt;/th&gt;
      &lt;th&gt;name&lt;/th&gt;
      &lt;th&gt;host_id&lt;/th&gt;
      &lt;th&gt;host_name&lt;/th&gt;
      &lt;th&gt;neighbourhood_group&lt;/th&gt;
      &lt;th&gt;neighbourhood&lt;/th&gt;
      &lt;th&gt;latitude&lt;/th&gt;
      &lt;th&gt;longitude&lt;/th&gt;
      &lt;th&gt;room_type&lt;/th&gt;
      &lt;th&gt;price&lt;/th&gt;
      &lt;th&gt;minimum_nights&lt;/th&gt;
      &lt;th&gt;number_of_reviews&lt;/th&gt;
      &lt;th&gt;last_review&lt;/th&gt;
      &lt;th&gt;reviews_per_month&lt;/th&gt;
      &lt;th&gt;calculated_host_listings_count&lt;/th&gt;
      &lt;th&gt;availability_365&lt;/th&gt;
      &lt;th&gt;number_of_reviews_ltm&lt;/th&gt;
      &lt;th&gt;license&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;98&lt;/th&gt;
      &lt;td&gt;1566003&lt;/td&gt;
      &lt;td&gt;&#34;i portici di via Piella &#34;&lt;/td&gt;
      &lt;td&gt;8325248&lt;/td&gt;
      &lt;td&gt;Massimo&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;Santo Stefano&lt;/td&gt;
      &lt;td&gt;44.498550&lt;/td&gt;
      &lt;td&gt;11.344110&lt;/td&gt;
      &lt;td&gt;Entire home/apt&lt;/td&gt;
      &lt;td&gt;51&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;764&lt;/td&gt;
      &lt;td&gt;2021-12-14&lt;/td&gt;
      &lt;td&gt;7.62&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;119&lt;/td&gt;
      &lt;td&gt;120&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;131&lt;/th&gt;
      &lt;td&gt;2282623&lt;/td&gt;
      &lt;td&gt;S.Orsola zone,parking for free and self check-in&lt;/td&gt;
      &lt;td&gt;11658074&lt;/td&gt;
      &lt;td&gt;Cecilia&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;San Donato - San Vitale&lt;/td&gt;
      &lt;td&gt;44.493280&lt;/td&gt;
      &lt;td&gt;11.366500&lt;/td&gt;
      &lt;td&gt;Entire home/apt&lt;/td&gt;
      &lt;td&gt;38&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;689&lt;/td&gt;
      &lt;td&gt;2021-10-24&lt;/td&gt;
      &lt;td&gt;7.20&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;72&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;204&lt;/th&gt;
      &lt;td&gt;4166793&lt;/td&gt;
      &lt;td&gt;Centralissimo a Bologna&lt;/td&gt;
      &lt;td&gt;8325248&lt;/td&gt;
      &lt;td&gt;Massimo&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;Santo Stefano&lt;/td&gt;
      &lt;td&gt;44.500920&lt;/td&gt;
      &lt;td&gt;11.344560&lt;/td&gt;
      &lt;td&gt;Entire home/apt&lt;/td&gt;
      &lt;td&gt;71&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;750&lt;/td&gt;
      &lt;td&gt;2021-12-10&lt;/td&gt;
      &lt;td&gt;9.21&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;233&lt;/td&gt;
      &lt;td&gt;84&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;751&lt;/th&gt;
      &lt;td&gt;15508481&lt;/td&gt;
      &lt;td&gt;Monolocale in zona fiera /centro&lt;/td&gt;
      &lt;td&gt;99632788&lt;/td&gt;
      &lt;td&gt;Walid&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;Navile&lt;/td&gt;
      &lt;td&gt;44.514462&lt;/td&gt;
      &lt;td&gt;11.353731&lt;/td&gt;
      &lt;td&gt;Entire home/apt&lt;/td&gt;
      &lt;td&gt;64&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;475&lt;/td&gt;
      &lt;td&gt;2021-12-01&lt;/td&gt;
      &lt;td&gt;7.56&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;48&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;773&lt;/th&gt;
      &lt;td&gt;15886516&lt;/td&gt;
      &lt;td&gt;Monolocale nel cuore del ghetto ebraico di Bol...&lt;/td&gt;
      &lt;td&gt;103024123&lt;/td&gt;
      &lt;td&gt;Catia&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;Santo Stefano&lt;/td&gt;
      &lt;td&gt;44.495080&lt;/td&gt;
      &lt;td&gt;11.347220&lt;/td&gt;
      &lt;td&gt;Entire home/apt&lt;/td&gt;
      &lt;td&gt;58&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;428&lt;/td&gt;
      &lt;td&gt;2021-12-15&lt;/td&gt;
      &lt;td&gt;7.88&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;285&lt;/td&gt;
      &lt;td&gt;17&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;For a single column (i.e. a Series), we can get the unique values using the &lt;code&gt;unique()&lt;/code&gt; function.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_listings[&#39;neighbourhood&#39;].unique()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;array([&#39;Santo Stefano&#39;, &#39;Porto - Saragozza&#39;, &#39;Navile&#39;,
       &#39;San Donato - San Vitale&#39;, &#39;Savena&#39;, &#39;Borgo Panigale - Reno&#39;],
      dtype=object)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For multiple columns, we can use the &lt;code&gt;drop_duplicates&lt;/code&gt; function.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_listings[[&#39;neighbourhood&#39;, &#39;room_type&#39;]].drop_duplicates()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;neighbourhood&lt;/th&gt;
      &lt;th&gt;room_type&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;Santo Stefano&lt;/td&gt;
      &lt;td&gt;Entire home/apt&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;Porto - Saragozza&lt;/td&gt;
      &lt;td&gt;Private room&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;Santo Stefano&lt;/td&gt;
      &lt;td&gt;Private room&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;5&lt;/th&gt;
      &lt;td&gt;Navile&lt;/td&gt;
      &lt;td&gt;Private room&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;7&lt;/th&gt;
      &lt;td&gt;Navile&lt;/td&gt;
      &lt;td&gt;Entire home/apt&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;8&lt;/th&gt;
      &lt;td&gt;Porto - Saragozza&lt;/td&gt;
      &lt;td&gt;Entire home/apt&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;19&lt;/th&gt;
      &lt;td&gt;San Donato - San Vitale&lt;/td&gt;
      &lt;td&gt;Private room&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;24&lt;/th&gt;
      &lt;td&gt;Savena&lt;/td&gt;
      &lt;td&gt;Private room&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;36&lt;/th&gt;
      &lt;td&gt;Borgo Panigale - Reno&lt;/td&gt;
      &lt;td&gt;Entire home/apt&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;41&lt;/th&gt;
      &lt;td&gt;San Donato - San Vitale&lt;/td&gt;
      &lt;td&gt;Entire home/apt&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;70&lt;/th&gt;
      &lt;td&gt;Porto - Saragozza&lt;/td&gt;
      &lt;td&gt;Hotel room&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;75&lt;/th&gt;
      &lt;td&gt;Borgo Panigale - Reno&lt;/td&gt;
      &lt;td&gt;Private room&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;110&lt;/th&gt;
      &lt;td&gt;Santo Stefano&lt;/td&gt;
      &lt;td&gt;Hotel room&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;111&lt;/th&gt;
      &lt;td&gt;Savena&lt;/td&gt;
      &lt;td&gt;Entire home/apt&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;388&lt;/th&gt;
      &lt;td&gt;Porto - Saragozza&lt;/td&gt;
      &lt;td&gt;Shared room&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;678&lt;/th&gt;
      &lt;td&gt;Navile&lt;/td&gt;
      &lt;td&gt;Shared room&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1393&lt;/th&gt;
      &lt;td&gt;Savena&lt;/td&gt;
      &lt;td&gt;Shared room&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1416&lt;/th&gt;
      &lt;td&gt;San Donato - San Vitale&lt;/td&gt;
      &lt;td&gt;Shared room&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1572&lt;/th&gt;
      &lt;td&gt;San Donato - San Vitale&lt;/td&gt;
      &lt;td&gt;Hotel room&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1637&lt;/th&gt;
      &lt;td&gt;Santo Stefano&lt;/td&gt;
      &lt;td&gt;Shared room&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1751&lt;/th&gt;
      &lt;td&gt;Navile&lt;/td&gt;
      &lt;td&gt;Hotel room&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;h2 id=&#34;aggregation-and-pivot-tables&#34;&gt;Aggregation and Pivot Tables&lt;/h2&gt;
&lt;p&gt;We can compute statistics by group using &lt;code&gt;.groupby()&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_listings.groupby(&#39;neighbourhood&#39;)[[&#39;price&#39;, &#39;reviews_per_month&#39;]].mean()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;price&lt;/th&gt;
      &lt;th&gt;reviews_per_month&lt;/th&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;neighbourhood&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;Borgo Panigale - Reno&lt;/th&gt;
      &lt;td&gt;83.020548&lt;/td&gt;
      &lt;td&gt;0.983488&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;Navile&lt;/th&gt;
      &lt;td&gt;142.200993&lt;/td&gt;
      &lt;td&gt;1.156745&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;Porto - Saragozza&lt;/th&gt;
      &lt;td&gt;129.908312&lt;/td&gt;
      &lt;td&gt;1.340325&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;San Donato - San Vitale&lt;/th&gt;
      &lt;td&gt;91.618138&lt;/td&gt;
      &lt;td&gt;0.933011&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;Santo Stefano&lt;/th&gt;
      &lt;td&gt;119.441841&lt;/td&gt;
      &lt;td&gt;1.344810&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;Savena&lt;/th&gt;
      &lt;td&gt;69.626016&lt;/td&gt;
      &lt;td&gt;0.805888&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;If you want to perform more than one function, maybe on different columns, you can use &lt;code&gt;.aggregate()&lt;/code&gt; which can be shortened to &lt;code&gt;.agg()&lt;/code&gt;. It takes as argument a dictionary with variables as keys and lists of functions as values.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_listings.groupby(&#39;neighbourhood&#39;).agg({&amp;quot;reviews_per_month&amp;quot;: [&amp;quot;mean&amp;quot;],
                                          &amp;quot;price&amp;quot;: [&amp;quot;min&amp;quot;, np.max]}).reset_index()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead tr th {
    text-align: left;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;neighbourhood&lt;/th&gt;
      &lt;th&gt;reviews_per_month&lt;/th&gt;
      &lt;th colspan=&#34;2&#34; halign=&#34;left&#34;&gt;price&lt;/th&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;mean&lt;/th&gt;
      &lt;th&gt;min&lt;/th&gt;
      &lt;th&gt;amax&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;Borgo Panigale - Reno&lt;/td&gt;
      &lt;td&gt;0.983488&lt;/td&gt;
      &lt;td&gt;9&lt;/td&gt;
      &lt;td&gt;1429&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;Navile&lt;/td&gt;
      &lt;td&gt;1.156745&lt;/td&gt;
      &lt;td&gt;14&lt;/td&gt;
      &lt;td&gt;5000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;Porto - Saragozza&lt;/td&gt;
      &lt;td&gt;1.340325&lt;/td&gt;
      &lt;td&gt;7&lt;/td&gt;
      &lt;td&gt;9999&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;San Donato - San Vitale&lt;/td&gt;
      &lt;td&gt;0.933011&lt;/td&gt;
      &lt;td&gt;10&lt;/td&gt;
      &lt;td&gt;1600&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;Santo Stefano&lt;/td&gt;
      &lt;td&gt;1.344810&lt;/td&gt;
      &lt;td&gt;11&lt;/td&gt;
      &lt;td&gt;9999&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;5&lt;/th&gt;
      &lt;td&gt;Savena&lt;/td&gt;
      &lt;td&gt;0.805888&lt;/td&gt;
      &lt;td&gt;9&lt;/td&gt;
      &lt;td&gt;680&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;The problem with this syntax is that it generates a hierarchical structure for variable names, which might not be so easy to work with. In the example above, to access the mean price, you have to use &lt;code&gt;df.price[&amp;quot;min&amp;quot;]&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;To perform variable naming and aggregation and the same time, you can ise the following syntax: &lt;code&gt;agg(output_var = (&amp;quot;input_var&amp;quot;, function))&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_listings.groupby(&#39;neighbourhood&#39;).agg(mean_reviews=(&amp;quot;reviews_per_month&amp;quot;, &amp;quot;mean&amp;quot;),
                                         min_price=(&amp;quot;price&amp;quot;, &amp;quot;min&amp;quot;),
                                         max_price=(&amp;quot;price&amp;quot;, np.max)).reset_index()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;neighbourhood&lt;/th&gt;
      &lt;th&gt;mean_reviews&lt;/th&gt;
      &lt;th&gt;min_price&lt;/th&gt;
      &lt;th&gt;max_price&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;Borgo Panigale - Reno&lt;/td&gt;
      &lt;td&gt;0.983488&lt;/td&gt;
      &lt;td&gt;9&lt;/td&gt;
      &lt;td&gt;1429&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;Navile&lt;/td&gt;
      &lt;td&gt;1.156745&lt;/td&gt;
      &lt;td&gt;14&lt;/td&gt;
      &lt;td&gt;5000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;Porto - Saragozza&lt;/td&gt;
      &lt;td&gt;1.340325&lt;/td&gt;
      &lt;td&gt;7&lt;/td&gt;
      &lt;td&gt;9999&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;San Donato - San Vitale&lt;/td&gt;
      &lt;td&gt;0.933011&lt;/td&gt;
      &lt;td&gt;10&lt;/td&gt;
      &lt;td&gt;1600&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;Santo Stefano&lt;/td&gt;
      &lt;td&gt;1.344810&lt;/td&gt;
      &lt;td&gt;11&lt;/td&gt;
      &lt;td&gt;9999&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;5&lt;/th&gt;
      &lt;td&gt;Savena&lt;/td&gt;
      &lt;td&gt;0.805888&lt;/td&gt;
      &lt;td&gt;9&lt;/td&gt;
      &lt;td&gt;680&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;We can make pivot tables with the &lt;code&gt;.pivot_table()&lt;/code&gt; function. It takes the folling arguments:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;index&lt;/code&gt;: rows&lt;/li&gt;
&lt;li&gt;&lt;code&gt;columns&lt;/code&gt;: columns&lt;/li&gt;
&lt;li&gt;&lt;code&gt;values&lt;/code&gt;: values&lt;/li&gt;
&lt;li&gt;&lt;code&gt;aggfunc&lt;/code&gt;: aggregation function&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_listings.pivot_table(index=&#39;neighbourhood&#39;, columns=&#39;room_type&#39;, values=&#39;price&#39;, aggfunc=&#39;mean&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;room_type&lt;/th&gt;
      &lt;th&gt;Entire home/apt&lt;/th&gt;
      &lt;th&gt;Hotel room&lt;/th&gt;
      &lt;th&gt;Private room&lt;/th&gt;
      &lt;th&gt;Shared room&lt;/th&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;neighbourhood&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;Borgo Panigale - Reno&lt;/th&gt;
      &lt;td&gt;96.700935&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;45.487179&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;Navile&lt;/th&gt;
      &lt;td&gt;172.140000&lt;/td&gt;
      &lt;td&gt;1350.000000&lt;/td&gt;
      &lt;td&gt;68.416107&lt;/td&gt;
      &lt;td&gt;28.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;Porto - Saragozza&lt;/th&gt;
      &lt;td&gt;148.410926&lt;/td&gt;
      &lt;td&gt;102.375000&lt;/td&gt;
      &lt;td&gt;83.070234&lt;/td&gt;
      &lt;td&gt;16.5&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;San Donato - San Vitale&lt;/th&gt;
      &lt;td&gt;106.775000&lt;/td&gt;
      &lt;td&gt;55.000000&lt;/td&gt;
      &lt;td&gt;61.194030&lt;/td&gt;
      &lt;td&gt;59.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;Santo Stefano&lt;/th&gt;
      &lt;td&gt;129.990260&lt;/td&gt;
      &lt;td&gt;103.827586&lt;/td&gt;
      &lt;td&gt;80.734177&lt;/td&gt;
      &lt;td&gt;95.4&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;Savena&lt;/th&gt;
      &lt;td&gt;86.301370&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;46.229167&lt;/td&gt;
      &lt;td&gt;22.5&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Instrumental Variables</title>
      <link>https://matteocourthoud.github.io/course/ml-econ/02_iv/</link>
      <pubDate>Wed, 09 Mar 2022 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/course/ml-econ/02_iv/</guid>
      <description>&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Remove warnings
import warnings
warnings.filterwarnings(&#39;ignore&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Import everything
import pandas as pd
import numpy as np
import seaborn as sns
import statsmodels.api as sm

from numpy.linalg import inv
from statsmodels.iolib.summary2 import summary_col
from linearmodels.iv import IV2SLS
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Import matplotlib for graphs
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import axes3d

# Set global parameters
%matplotlib inline
plt.style.use(&#39;seaborn-white&#39;)
plt.rcParams[&#39;lines.linewidth&#39;] = 3
plt.rcParams[&#39;figure.figsize&#39;] = (10,6)
plt.rcParams[&#39;figure.titlesize&#39;] = 20
plt.rcParams[&#39;axes.titlesize&#39;] = 18
plt.rcParams[&#39;axes.labelsize&#39;] = 14
plt.rcParams[&#39;legend.fontsize&#39;] = 14
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;21-simple-linear-regression&#34;&gt;2.1 Simple Linear Regression&lt;/h2&gt;
&lt;p&gt;In &lt;a href=&#34;https://economics.mit.edu/files/4123&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Acemoglu, Johnson, Robinson (2002), &amp;ldquo;&lt;em&gt;The Colonial Origins of Comparative Development&lt;/em&gt;&amp;rdquo;&lt;/a&gt; the authors wish to determine whether or not differences in institutions can help to explain observed economic outcomes.&lt;/p&gt;
&lt;p&gt;How do we measure &lt;em&gt;institutional differences&lt;/em&gt; and &lt;em&gt;economic outcomes&lt;/em&gt;?&lt;/p&gt;
&lt;p&gt;In this paper,&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;economic outcomes are proxied by log GDP per capita in 1995, adjusted for exchange rates.&lt;/li&gt;
&lt;li&gt;institutional differences are proxied by an index of protection against expropriation on average over 1985-95, constructed by the &lt;a href=&#34;https://www.prsgroup.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Political Risk Services Group&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;These variables and other data used in the paper are available for download on Daron Acemoglu’s &lt;a href=&#34;https://economics.mit.edu/faculty/acemoglu/data/ajr2001&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;webpage&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The original dataset in in Stata &lt;code&gt;.dta&lt;/code&gt; format but has been converted to &lt;code&gt;.csv&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;First, let&amp;rsquo;s load the data and have a look at it.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Load Acemoglu Johnson Robinson Dataset
df = pd.read_csv(&#39;data/AJR02.csv&#39;,index_col=0)
df.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;GDP&lt;/th&gt;
      &lt;th&gt;Exprop&lt;/th&gt;
      &lt;th&gt;Mort&lt;/th&gt;
      &lt;th&gt;Latitude&lt;/th&gt;
      &lt;th&gt;Neo&lt;/th&gt;
      &lt;th&gt;Africa&lt;/th&gt;
      &lt;th&gt;Asia&lt;/th&gt;
      &lt;th&gt;Namer&lt;/th&gt;
      &lt;th&gt;Samer&lt;/th&gt;
      &lt;th&gt;logMort&lt;/th&gt;
      &lt;th&gt;Latitude2&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;8.39&lt;/td&gt;
      &lt;td&gt;6.50&lt;/td&gt;
      &lt;td&gt;78.20&lt;/td&gt;
      &lt;td&gt;0.3111&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;4.359270&lt;/td&gt;
      &lt;td&gt;0.096783&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;7.77&lt;/td&gt;
      &lt;td&gt;5.36&lt;/td&gt;
      &lt;td&gt;280.00&lt;/td&gt;
      &lt;td&gt;0.1367&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;5.634790&lt;/td&gt;
      &lt;td&gt;0.018687&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;9.13&lt;/td&gt;
      &lt;td&gt;6.39&lt;/td&gt;
      &lt;td&gt;68.90&lt;/td&gt;
      &lt;td&gt;0.3778&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;4.232656&lt;/td&gt;
      &lt;td&gt;0.142733&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;9.90&lt;/td&gt;
      &lt;td&gt;9.32&lt;/td&gt;
      &lt;td&gt;8.55&lt;/td&gt;
      &lt;td&gt;0.3000&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;2.145931&lt;/td&gt;
      &lt;td&gt;0.090000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;5&lt;/th&gt;
      &lt;td&gt;9.29&lt;/td&gt;
      &lt;td&gt;7.50&lt;/td&gt;
      &lt;td&gt;85.00&lt;/td&gt;
      &lt;td&gt;0.2683&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;4.442651&lt;/td&gt;
      &lt;td&gt;0.071985&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;Let’s use a scatterplot to see whether any obvious relationship exists between GDP per capita and the protection against expropriation.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Plot relationship between GDP and expropriation rate
fig, ax = plt.subplots(1,1)
ax.set_title(&#39;Figure 1: joint distribution of GDP and expropriation&#39;)
df.plot(x=&#39;Exprop&#39;, y=&#39;GDP&#39;, kind=&#39;scatter&#39;, s=50, ax=ax);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/02_iv_9_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;The plot shows a fairly strong positive relationship between
protection against expropriation and log GDP per capita.&lt;/p&gt;
&lt;p&gt;Specifically, if higher protection against expropriation is a measure of
institutional quality, then better institutions appear to be positively
correlated with better economic outcomes (higher GDP per capita).&lt;/p&gt;
&lt;p&gt;Given the plot, choosing a linear model to describe this relationship
seems like a reasonable assumption.&lt;/p&gt;
&lt;p&gt;We can write our model as&lt;/p&gt;
&lt;p&gt;$$
{GDP}_i = \beta_0 + \beta_1 {Exprop}_i + \varepsilon_i
$$&lt;/p&gt;
&lt;p&gt;where:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$ \beta_0 $ is the intercept of the linear trend line on the
y-axis&lt;/li&gt;
&lt;li&gt;$ \beta_1 $ is the slope of the linear trend line, representing
the &lt;em&gt;marginal effect&lt;/em&gt; of protection against risk on log GDP per
capita&lt;/li&gt;
&lt;li&gt;$ \varepsilon_i $ is a random error term (deviations of observations from
the linear trend due to factors not included in the model)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The most common technique to estimate the parameters ($ \beta $’s)
of the linear model is Ordinary Least Squares (OLS).&lt;/p&gt;
&lt;p&gt;As the name implies, an OLS model is solved by finding the parameters
that minimize &lt;em&gt;the sum of squared residuals&lt;/em&gt;, i.e.&lt;/p&gt;
&lt;p&gt;$$
\underset{\hat{\beta}}{\min} \sum^N_{i=1}{\hat{u}^2_i}
$$&lt;/p&gt;
&lt;p&gt;where $ \hat{u}_i $ is the difference between the observation and
the predicted value of the dependent variable.&lt;/p&gt;
&lt;p&gt;To estimate the constant term $ \beta_0 $, we need to add a column
of 1’s to our dataset (consider the equation if $ \beta_0 $ was
replaced with $ \beta_0 x_i $ and $ x_i = 1 $)&lt;/p&gt;
&lt;p&gt;Now we can construct our model in &lt;code&gt;statsmodels&lt;/code&gt; using the OLS function.&lt;/p&gt;
&lt;p&gt;We will use &lt;code&gt;pandas&lt;/code&gt; dataframes with &lt;code&gt;statsmodels&lt;/code&gt;, however standard arrays can also be used as arguments&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Regress GDP on Expropriation Rate
reg1 = sm.OLS.from_formula(&#39;GDP ~ Exprop&#39;, df)
type(reg1)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;statsmodels.regression.linear_model.OLS
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So far we have simply constructed our model.&lt;/p&gt;
&lt;p&gt;We need to use &lt;code&gt;.fit()&lt;/code&gt; to obtain parameter estimates
$ \hat{\beta}_0 $ and $ \hat{\beta}_1 $&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Fit regression
results = reg1.fit()
type(results)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;statsmodels.regression.linear_model.RegressionResultsWrapper
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We now have the fitted regression model stored in &lt;code&gt;results&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;To view the OLS regression results, we can call the &lt;code&gt;.summary()&lt;/code&gt;
method.&lt;/p&gt;
&lt;p&gt;Note that an observation was mistakenly dropped from the results in the
original paper (see the note located in maketable2.do from Acemoglu’s webpage), and thus the
coefficients differ slightly.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;results.summary()
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;caption&gt;OLS Regression Results&lt;/caption&gt;
&lt;tr&gt;
  &lt;th&gt;Dep. Variable:&lt;/th&gt;           &lt;td&gt;GDP&lt;/td&gt;       &lt;th&gt;  R-squared:         &lt;/th&gt; &lt;td&gt;   0.540&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Model:&lt;/th&gt;                   &lt;td&gt;OLS&lt;/td&gt;       &lt;th&gt;  Adj. R-squared:    &lt;/th&gt; &lt;td&gt;   0.532&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Method:&lt;/th&gt;             &lt;td&gt;Least Squares&lt;/td&gt;  &lt;th&gt;  F-statistic:       &lt;/th&gt; &lt;td&gt;   72.71&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Date:&lt;/th&gt;             &lt;td&gt;Mon, 03 Jan 2022&lt;/td&gt; &lt;th&gt;  Prob (F-statistic):&lt;/th&gt; &lt;td&gt;4.84e-12&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Time:&lt;/th&gt;                 &lt;td&gt;18:31:09&lt;/td&gt;     &lt;th&gt;  Log-Likelihood:    &lt;/th&gt; &lt;td&gt; -68.214&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;No. Observations:&lt;/th&gt;      &lt;td&gt;    64&lt;/td&gt;      &lt;th&gt;  AIC:               &lt;/th&gt; &lt;td&gt;   140.4&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Df Residuals:&lt;/th&gt;          &lt;td&gt;    62&lt;/td&gt;      &lt;th&gt;  BIC:               &lt;/th&gt; &lt;td&gt;   144.7&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Df Model:&lt;/th&gt;              &lt;td&gt;     1&lt;/td&gt;      &lt;th&gt;                     &lt;/th&gt;     &lt;td&gt; &lt;/td&gt;   
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Covariance Type:&lt;/th&gt;      &lt;td&gt;nonrobust&lt;/td&gt;    &lt;th&gt;                     &lt;/th&gt;     &lt;td&gt; &lt;/td&gt;   
&lt;/tr&gt;
&lt;/table&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
      &lt;td&gt;&lt;/td&gt;         &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt; &lt;td&gt;    4.6609&lt;/td&gt; &lt;td&gt;    0.409&lt;/td&gt; &lt;td&gt;   11.402&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    3.844&lt;/td&gt; &lt;td&gt;    5.478&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Exprop&lt;/th&gt;    &lt;td&gt;    0.5220&lt;/td&gt; &lt;td&gt;    0.061&lt;/td&gt; &lt;td&gt;    8.527&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    0.400&lt;/td&gt; &lt;td&gt;    0.644&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
  &lt;th&gt;Omnibus:&lt;/th&gt;       &lt;td&gt; 7.134&lt;/td&gt; &lt;th&gt;  Durbin-Watson:     &lt;/th&gt; &lt;td&gt;   2.081&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Prob(Omnibus):&lt;/th&gt; &lt;td&gt; 0.028&lt;/td&gt; &lt;th&gt;  Jarque-Bera (JB):  &lt;/th&gt; &lt;td&gt;   6.698&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Skew:&lt;/th&gt;          &lt;td&gt;-0.784&lt;/td&gt; &lt;th&gt;  Prob(JB):          &lt;/th&gt; &lt;td&gt;  0.0351&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Kurtosis:&lt;/th&gt;      &lt;td&gt; 3.234&lt;/td&gt; &lt;th&gt;  Cond. No.          &lt;/th&gt; &lt;td&gt;    31.2&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;&lt;br/&gt;&lt;br/&gt;Warnings:&lt;br/&gt;[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
&lt;p&gt;From our results, we see that&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The intercept $ \hat{\beta}_0 = 4.63 $.&lt;/li&gt;
&lt;li&gt;The slope $ \hat{\beta}_1 = 0.53 $.&lt;/li&gt;
&lt;li&gt;The positive $ \hat{\beta}_1 $ parameter estimate implies that.
institutional quality has a positive effect on economic outcomes, as
we saw in the figure.&lt;/li&gt;
&lt;li&gt;The p-value of 0.000 for $ \hat{\beta}_1 $ implies that the
effect of institutions on GDP is statistically significant (using p &amp;lt;
0.05 as a rejection rule).&lt;/li&gt;
&lt;li&gt;The R-squared value of 0.611 indicates that around 61% of variation
in log GDP per capita is explained by protection against
expropriation.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Using our parameter estimates, we can now write our estimated
relationship as&lt;/p&gt;
&lt;p&gt;$$
\widehat{GDP}_i = 4.63 + 0.53 \ {Exprop}_i
$$&lt;/p&gt;
&lt;p&gt;This equation describes the line that best fits our data, as shown in
Figure 2.&lt;/p&gt;
&lt;p&gt;We can use this equation to predict the level of log GDP per capita for
a value of the index of expropriation protection.&lt;/p&gt;
&lt;p&gt;For example, for a country with an index value of 6.51 (the average for
the dataset), we find that their predicted level of log GDP per capita
in 1995 is 8.09.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;mean_expr = np.mean(df[&#39;Exprop&#39;])
mean_expr
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;6.5160937500000005
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;predicted_logpdp95 = results.params[0] + results.params[1] * mean_expr
predicted_logpdp95
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;8.062499999999995
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;An easier (and more accurate) way to obtain this result is to use
&lt;code&gt;.predict()&lt;/code&gt; and set $ constant = 1 $ and
$ {Exprop}_i = mean_expr $&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;results.predict(exog=[1, mean_expr])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can obtain an array of predicted $ {GDP}_i $ for every value
of $ {Exprop}_i $ in our dataset by calling &lt;code&gt;.predict()&lt;/code&gt; on our
results.&lt;/p&gt;
&lt;p&gt;Plotting the predicted values against $ {Exprop}_i $ shows that the
predicted values lie along the linear line that we fitted above.&lt;/p&gt;
&lt;p&gt;The observed values of $ {GDP}_i $ are also plotted for
comparison purposes&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Make first new figure
def make_new_fig_2():

    # Init figure
    fig, ax = plt.subplots(1,1)
    ax.set_title(&#39;Figure 2: OLS predicted values&#39;)

    # Drop missing observations from whole sample
    df_plot = df.dropna(subset=[&#39;GDP&#39;, &#39;Exprop&#39;])
    sns.regplot(x=df_plot[&#39;Exprop&#39;], y=df_plot[&#39;GDP&#39;], ax=ax, order=1, ci=None, line_kws={&#39;color&#39;:&#39;r&#39;})

    ax.legend([&#39;predicted&#39;, &#39;observed&#39;])
    ax.set_xlabel(&#39;Exprop&#39;)
    ax.set_ylabel(&#39;GDP&#39;)
    plt.show()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;make_new_fig_2()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;ERROR! Session/line number was not unique in database. History logging moved to new session 305
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/02_iv_28_1.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;22-extending-the-linear-regression-model&#34;&gt;2.2 Extending the Linear Regression Model&lt;/h2&gt;
&lt;p&gt;So far we have only accounted for institutions affecting economic performance - almost certainly there are numerous other factors affecting GDP that are not included in our model.&lt;/p&gt;
&lt;p&gt;Leaving out variables that affect $ GDP_i $ will result in &lt;strong&gt;omitted variable bias&lt;/strong&gt;, yielding biased and inconsistent parameter estimates.&lt;/p&gt;
&lt;p&gt;We can extend our bivariate regression model to a &lt;strong&gt;multivariate regression model&lt;/strong&gt; by adding in other factors that may affect $ GDP_i $.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://python-programming.quantecon.org/zreferences.html#acemoglu2001&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[AJR01]&lt;/a&gt; consider other factors such as:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the effect of climate on economic outcomes; latitude is used to proxy
this&lt;/li&gt;
&lt;li&gt;differences that affect both economic performance and institutions,
eg. cultural, historical, etc.; controlled for with the use of
continent dummies&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Let’s estimate some of the extended models considered in the paper
(Table 2) using data from &lt;code&gt;maketable2.dta&lt;/code&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Add constant term to dataset
df[&#39;const&#39;] = 1

# Create lists of variables to be used in each regression
X1 = df[[&#39;const&#39;, &#39;Exprop&#39;]]
X2 = df[[&#39;const&#39;, &#39;Exprop&#39;, &#39;Latitude&#39;, &#39;Latitude2&#39;]]
X3 = df[[&#39;const&#39;, &#39;Exprop&#39;, &#39;Latitude&#39;, &#39;Latitude2&#39;, &#39;Asia&#39;, &#39;Africa&#39;, &#39;Namer&#39;, &#39;Samer&#39;]]

# Estimate an OLS regression for each set of variables
reg1 = sm.OLS(df[&#39;GDP&#39;], X1, missing=&#39;drop&#39;).fit()
reg2 = sm.OLS(df[&#39;GDP&#39;], X2, missing=&#39;drop&#39;).fit()
reg3 = sm.OLS(df[&#39;GDP&#39;], X3, missing=&#39;drop&#39;).fit()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now that we have fitted our model, we will use &lt;code&gt;summary_col&lt;/code&gt; to
display the results in a single table (model numbers correspond to those
in the paper)&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;info_dict={&#39;No. observations&#39; : lambda x: f&amp;quot;{int(x.nobs):d}&amp;quot;}

results_table = summary_col(results=[reg1,reg2,reg3],
                            float_format=&#39;%0.2f&#39;,
                            stars = True,
                            model_names=[&#39;Model 1&#39;,&#39;Model 2&#39;,&#39;Model 3&#39;],
                            info_dict=info_dict,
                            regressor_order=[&#39;const&#39;,&#39;Exprop&#39;,&#39;Latitude&#39;,&#39;Latitude2&#39;])

results_table
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
          &lt;td&gt;&lt;/td&gt;         &lt;th&gt;Model 1&lt;/th&gt; &lt;th&gt;Model 2&lt;/th&gt; &lt;th&gt;Model 3&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;const&lt;/th&gt;            &lt;td&gt;4.66***&lt;/td&gt; &lt;td&gt;4.55***&lt;/td&gt; &lt;td&gt;5.95***&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;&lt;/th&gt;                 &lt;td&gt;(0.41)&lt;/td&gt;  &lt;td&gt;(0.45)&lt;/td&gt;  &lt;td&gt;(0.68)&lt;/td&gt; 
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Exprop&lt;/th&gt;           &lt;td&gt;0.52***&lt;/td&gt; &lt;td&gt;0.49***&lt;/td&gt; &lt;td&gt;0.40***&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;&lt;/th&gt;                 &lt;td&gt;(0.06)&lt;/td&gt;  &lt;td&gt;(0.07)&lt;/td&gt;  &lt;td&gt;(0.06)&lt;/td&gt; 
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Latitude&lt;/th&gt;            &lt;td&gt;&lt;/td&gt;      &lt;td&gt;2.16&lt;/td&gt;    &lt;td&gt;0.42&lt;/td&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;&lt;/th&gt;                    &lt;td&gt;&lt;/td&gt;     &lt;td&gt;(1.68)&lt;/td&gt;  &lt;td&gt;(1.47)&lt;/td&gt; 
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Latitude2&lt;/th&gt;           &lt;td&gt;&lt;/td&gt;      &lt;td&gt;-2.12&lt;/td&gt;   &lt;td&gt;0.44&lt;/td&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;&lt;/th&gt;                    &lt;td&gt;&lt;/td&gt;     &lt;td&gt;(2.86)&lt;/td&gt;  &lt;td&gt;(2.48)&lt;/td&gt; 
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Africa&lt;/th&gt;              &lt;td&gt;&lt;/td&gt;        &lt;td&gt;&lt;/td&gt;     &lt;td&gt;-1.06**&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;&lt;/th&gt;                    &lt;td&gt;&lt;/td&gt;        &lt;td&gt;&lt;/td&gt;     &lt;td&gt;(0.41)&lt;/td&gt; 
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Asia&lt;/th&gt;                &lt;td&gt;&lt;/td&gt;        &lt;td&gt;&lt;/td&gt;     &lt;td&gt;-0.74*&lt;/td&gt; 
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;&lt;/th&gt;                    &lt;td&gt;&lt;/td&gt;        &lt;td&gt;&lt;/td&gt;     &lt;td&gt;(0.42)&lt;/td&gt; 
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Namer&lt;/th&gt;               &lt;td&gt;&lt;/td&gt;        &lt;td&gt;&lt;/td&gt;      &lt;td&gt;-0.17&lt;/td&gt; 
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;&lt;/th&gt;                    &lt;td&gt;&lt;/td&gt;        &lt;td&gt;&lt;/td&gt;     &lt;td&gt;(0.40)&lt;/td&gt; 
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Samer&lt;/th&gt;               &lt;td&gt;&lt;/td&gt;        &lt;td&gt;&lt;/td&gt;      &lt;td&gt;-0.12&lt;/td&gt; 
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;&lt;/th&gt;                    &lt;td&gt;&lt;/td&gt;        &lt;td&gt;&lt;/td&gt;     &lt;td&gt;(0.42)&lt;/td&gt; 
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;No. observations&lt;/th&gt;   &lt;td&gt;64&lt;/td&gt;      &lt;td&gt;64&lt;/td&gt;      &lt;td&gt;64&lt;/td&gt;   
&lt;/tr&gt;
&lt;/table&gt;
&lt;h2 id=&#34;23-endogeneity&#34;&gt;2.3 Endogeneity&lt;/h2&gt;
&lt;p&gt;As &lt;a href=&#34;https://python-programming.quantecon.org/zreferences.html#acemoglu2001&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[AJR01]&lt;/a&gt; discuss, the OLS models likely suffer from &lt;strong&gt;endogeneity&lt;/strong&gt; issues, resulting in biased and inconsistent model estimates.&lt;/p&gt;
&lt;p&gt;Namely, there is likely a two-way relationship between institutions an economic outcomes:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;richer countries may be able to afford or prefer better institutions&lt;/li&gt;
&lt;li&gt;variables that affect income may also be correlated with institutional differences&lt;/li&gt;
&lt;li&gt;the construction of the index may be biased; analysts may be biased towards seeing countries with higher income having better institutions&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;To deal with endogeneity, we can use &lt;strong&gt;two-stage least squares (2SLS) regression&lt;/strong&gt;, which is an extension of OLS regression.&lt;/p&gt;
&lt;p&gt;This method requires replacing the endogenous variable $ {Exprop}_i $ with a variable that is:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;correlated with $ {Exprop}_i $&lt;/li&gt;
&lt;li&gt;not correlated with the error term (ie. it should not directly affect the dependent variable, otherwise it would be correlated with $ u_i $ due to omitted variable bias)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;We can write our model as&lt;/p&gt;
&lt;p&gt;$$
{GDP}_i = \beta_0 + \beta_1 {Exprop}_i + \varepsilon_i \
{Exprop}_i = \delta_0 + \delta_1 {logMort}_i + v_i
$$&lt;/p&gt;
&lt;p&gt;The new set of regressors &lt;code&gt;logMort&lt;/code&gt; is called an &lt;strong&gt;instrument&lt;/strong&gt;, which aims to remove endogeneity in our proxy of institutional differences.&lt;/p&gt;
&lt;p&gt;The main contribution of &lt;a href=&#34;https://python-programming.quantecon.org/zreferences.html#acemoglu2001&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[AJR01]&lt;/a&gt; is the use of settler mortality rates to instrument for institutional differences.&lt;/p&gt;
&lt;p&gt;They hypothesize that higher mortality rates of colonizers led to the establishment of institutions that were more extractive in nature (less protection against expropriation), and these institutions still persist today.&lt;/p&gt;
&lt;p&gt;Using a scatterplot (Figure 3 in &lt;a href=&#34;https://python-programming.quantecon.org/zreferences.html#acemoglu2001&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[AJR01]&lt;/a&gt;), we can see protection against expropriation is negatively correlated with settler mortality rates, coinciding with the authors’ hypothesis and satisfying the first condition of a valid instrument.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Dropping NA&#39;s is required to use numpy&#39;s polyfit
df2 = df.dropna(subset=[&#39;logMort&#39;, &#39;Exprop&#39;])
X = df2[&#39;logMort&#39;]
y = df2[&#39;Exprop&#39;]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Make new figure 2
def make_new_figure_2():

    # Init figure
    fig, ax = plt.subplots(1,1)
    ax.set_title(&#39;Figure 3: First-stage&#39;)

    # Fit a linear trend line
    sns.regplot(x=X, y=y, ax=ax, order=1, scatter=True, ci=None, line_kws={&amp;quot;color&amp;quot;: &amp;quot;r&amp;quot;})

    ax.set_xlim([1.8,8.4])
    ax.set_ylim([3.3,10.4])
    ax.set_xlabel(&#39;Log of Settler Mortality&#39;)
    ax.set_ylabel(&#39;Average Expropriation Risk 1985-95&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;make_new_figure_2()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/02_iv_43_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;The second condition may not be satisfied if settler mortality rates in the 17th to 19th centuries have a direct effect on current GDP (in addition to their indirect effect through institutions).&lt;/p&gt;
&lt;p&gt;For example, settler mortality rates may be related to the current disease environment in a country, which could affect current economic performance.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://python-programming.quantecon.org/zreferences.html#acemoglu2001&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[AJR01]&lt;/a&gt; argue this is unlikely because:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The majority of settler deaths were due to malaria and yellow fever
and had a limited effect on local people.&lt;/li&gt;
&lt;li&gt;The disease burden on local people in Africa or India, for example,
did not appear to be higher than average, supported by relatively
high population densities in these areas before colonization.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;As we appear to have a valid instrument, we can use 2SLS regression to
obtain consistent and unbiased parameter estimates.&lt;/p&gt;
&lt;h3 id=&#34;first-stage&#34;&gt;First stage&lt;/h3&gt;
&lt;p&gt;The first stage involves regressing the endogenous variable
($ {Exprop}_i $) on the instrument.&lt;/p&gt;
&lt;p&gt;The instrument is the set of all exogenous variables in our model (and
not just the variable we have replaced).&lt;/p&gt;
&lt;p&gt;Using model 1 as an example, our instrument is simply a constant and
settler mortality rates $ {logMort}_i $.&lt;/p&gt;
&lt;p&gt;Therefore, we will estimate the first-stage regression as&lt;/p&gt;
&lt;p&gt;$$
{Exprop}_i = \delta_0 + \delta_1 {logMort}_i + v_i
$$&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Add a constant variable
df[&#39;const&#39;] = 1

# Fit the first stage regression and print summary
results_fs = sm.OLS(df[&#39;Exprop&#39;],
                    df.loc[:,[&#39;const&#39;, &#39;logMort&#39;]],
                    missing=&#39;drop&#39;).fit()
results_fs.summary()
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;caption&gt;OLS Regression Results&lt;/caption&gt;
&lt;tr&gt;
  &lt;th&gt;Dep. Variable:&lt;/th&gt;         &lt;td&gt;Exprop&lt;/td&gt;      &lt;th&gt;  R-squared:         &lt;/th&gt; &lt;td&gt;   0.274&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Model:&lt;/th&gt;                   &lt;td&gt;OLS&lt;/td&gt;       &lt;th&gt;  Adj. R-squared:    &lt;/th&gt; &lt;td&gt;   0.262&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Method:&lt;/th&gt;             &lt;td&gt;Least Squares&lt;/td&gt;  &lt;th&gt;  F-statistic:       &lt;/th&gt; &lt;td&gt;   23.34&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Date:&lt;/th&gt;             &lt;td&gt;Mon, 03 Jan 2022&lt;/td&gt; &lt;th&gt;  Prob (F-statistic):&lt;/th&gt; &lt;td&gt;9.27e-06&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Time:&lt;/th&gt;                 &lt;td&gt;18:31:10&lt;/td&gt;     &lt;th&gt;  Log-Likelihood:    &lt;/th&gt; &lt;td&gt; -104.69&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;No. Observations:&lt;/th&gt;      &lt;td&gt;    64&lt;/td&gt;      &lt;th&gt;  AIC:               &lt;/th&gt; &lt;td&gt;   213.4&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Df Residuals:&lt;/th&gt;          &lt;td&gt;    62&lt;/td&gt;      &lt;th&gt;  BIC:               &lt;/th&gt; &lt;td&gt;   217.7&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Df Model:&lt;/th&gt;              &lt;td&gt;     1&lt;/td&gt;      &lt;th&gt;                     &lt;/th&gt;     &lt;td&gt; &lt;/td&gt;   
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Covariance Type:&lt;/th&gt;      &lt;td&gt;nonrobust&lt;/td&gt;    &lt;th&gt;                     &lt;/th&gt;     &lt;td&gt; &lt;/td&gt;   
&lt;/tr&gt;
&lt;/table&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
     &lt;td&gt;&lt;/td&gt;        &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;const&lt;/th&gt;   &lt;td&gt;    9.3659&lt;/td&gt; &lt;td&gt;    0.611&lt;/td&gt; &lt;td&gt;   15.339&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    8.145&lt;/td&gt; &lt;td&gt;   10.586&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;logMort&lt;/th&gt; &lt;td&gt;   -0.6133&lt;/td&gt; &lt;td&gt;    0.127&lt;/td&gt; &lt;td&gt;   -4.831&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;   -0.867&lt;/td&gt; &lt;td&gt;   -0.360&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
  &lt;th&gt;Omnibus:&lt;/th&gt;       &lt;td&gt; 0.047&lt;/td&gt; &lt;th&gt;  Durbin-Watson:     &lt;/th&gt; &lt;td&gt;   1.592&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Prob(Omnibus):&lt;/th&gt; &lt;td&gt; 0.977&lt;/td&gt; &lt;th&gt;  Jarque-Bera (JB):  &lt;/th&gt; &lt;td&gt;   0.154&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Skew:&lt;/th&gt;          &lt;td&gt; 0.060&lt;/td&gt; &lt;th&gt;  Prob(JB):          &lt;/th&gt; &lt;td&gt;   0.926&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Kurtosis:&lt;/th&gt;      &lt;td&gt; 2.792&lt;/td&gt; &lt;th&gt;  Cond. No.          &lt;/th&gt; &lt;td&gt;    19.4&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;&lt;br/&gt;&lt;br/&gt;Warnings:&lt;br/&gt;[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
&lt;p&gt;We need to retrieve the predicted values of $ {Exprop}_i $ using
&lt;code&gt;.predict()&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;We then replace the endogenous variable $ {Exprop}_i $ with the
predicted values $ \widehat{Exprop}_i $ in the original linear model.&lt;/p&gt;
&lt;p&gt;Our second stage regression is thus&lt;/p&gt;
&lt;p&gt;$$
{GDP}_i = \beta_0 + \beta_1 \widehat{Exprop}_i + u_i
$$&lt;/p&gt;
&lt;h3 id=&#34;second-stage&#34;&gt;Second stage&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Second stage
df[&#39;predicted_Exprop&#39;] = results_fs.predict()
results_ss = sm.OLS.from_formula(&#39;GDP ~ predicted_Exprop&#39;, df).fit()

# Print
results_ss.summary()
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;caption&gt;OLS Regression Results&lt;/caption&gt;
&lt;tr&gt;
  &lt;th&gt;Dep. Variable:&lt;/th&gt;           &lt;td&gt;GDP&lt;/td&gt;       &lt;th&gt;  R-squared:         &lt;/th&gt; &lt;td&gt;   0.462&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Model:&lt;/th&gt;                   &lt;td&gt;OLS&lt;/td&gt;       &lt;th&gt;  Adj. R-squared:    &lt;/th&gt; &lt;td&gt;   0.453&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Method:&lt;/th&gt;             &lt;td&gt;Least Squares&lt;/td&gt;  &lt;th&gt;  F-statistic:       &lt;/th&gt; &lt;td&gt;   53.24&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Date:&lt;/th&gt;             &lt;td&gt;Mon, 03 Jan 2022&lt;/td&gt; &lt;th&gt;  Prob (F-statistic):&lt;/th&gt; &lt;td&gt;6.58e-10&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Time:&lt;/th&gt;                 &lt;td&gt;18:31:10&lt;/td&gt;     &lt;th&gt;  Log-Likelihood:    &lt;/th&gt; &lt;td&gt; -73.208&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;No. Observations:&lt;/th&gt;      &lt;td&gt;    64&lt;/td&gt;      &lt;th&gt;  AIC:               &lt;/th&gt; &lt;td&gt;   150.4&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Df Residuals:&lt;/th&gt;          &lt;td&gt;    62&lt;/td&gt;      &lt;th&gt;  BIC:               &lt;/th&gt; &lt;td&gt;   154.7&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Df Model:&lt;/th&gt;              &lt;td&gt;     1&lt;/td&gt;      &lt;th&gt;                     &lt;/th&gt;     &lt;td&gt; &lt;/td&gt;   
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Covariance Type:&lt;/th&gt;      &lt;td&gt;nonrobust&lt;/td&gt;    &lt;th&gt;                     &lt;/th&gt;     &lt;td&gt; &lt;/td&gt;   
&lt;/tr&gt;
&lt;/table&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
          &lt;td&gt;&lt;/td&gt;            &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt;        &lt;td&gt;    2.0448&lt;/td&gt; &lt;td&gt;    0.830&lt;/td&gt; &lt;td&gt;    2.463&lt;/td&gt; &lt;td&gt; 0.017&lt;/td&gt; &lt;td&gt;    0.385&lt;/td&gt; &lt;td&gt;    3.705&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;predicted_Exprop&lt;/th&gt; &lt;td&gt;    0.9235&lt;/td&gt; &lt;td&gt;    0.127&lt;/td&gt; &lt;td&gt;    7.297&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    0.671&lt;/td&gt; &lt;td&gt;    1.177&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
  &lt;th&gt;Omnibus:&lt;/th&gt;       &lt;td&gt;10.463&lt;/td&gt; &lt;th&gt;  Durbin-Watson:     &lt;/th&gt; &lt;td&gt;   2.052&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Prob(Omnibus):&lt;/th&gt; &lt;td&gt; 0.005&lt;/td&gt; &lt;th&gt;  Jarque-Bera (JB):  &lt;/th&gt; &lt;td&gt;  10.693&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Skew:&lt;/th&gt;          &lt;td&gt;-0.806&lt;/td&gt; &lt;th&gt;  Prob(JB):          &lt;/th&gt; &lt;td&gt; 0.00476&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Kurtosis:&lt;/th&gt;      &lt;td&gt; 4.188&lt;/td&gt; &lt;th&gt;  Cond. No.          &lt;/th&gt; &lt;td&gt;    57.8&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;&lt;br/&gt;&lt;br/&gt;Warnings:&lt;br/&gt;[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
&lt;p&gt;The second-stage regression results give us an unbiased and consistent
estimate of the effect of institutions on economic outcomes.&lt;/p&gt;
&lt;p&gt;The result suggests a stronger positive relationship than what the OLS
results indicated.&lt;/p&gt;
&lt;p&gt;Note that while our parameter estimates are correct, our standard errors
are not and for this reason, computing 2SLS ‘manually’ (in stages with
OLS) is not recommended.&lt;/p&gt;
&lt;p&gt;We can correctly estimate a 2SLS regression in one step using the
&lt;a href=&#34;https://github.com/bashtage/linearmodels&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;linearmodels&lt;/a&gt; package, an extension of &lt;code&gt;statsmodels&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Note that when using &lt;code&gt;IV2SLS&lt;/code&gt;, the exogenous and instrument variables
are split up in the function arguments (whereas before the instrument
included exogenous variables)&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# IV regression
iv = IV2SLS(dependent=df[&#39;GDP&#39;],
            exog=df[&#39;const&#39;],
            endog=df[&#39;Exprop&#39;],
            instruments=df[&#39;logMort&#39;]).fit()

# Print
iv.summary
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;caption&gt;IV-2SLS Estimation Summary&lt;/caption&gt;
&lt;tr&gt;
  &lt;th&gt;Dep. Variable:&lt;/th&gt;           &lt;td&gt;GDP&lt;/td&gt;       &lt;th&gt;  R-squared:         &lt;/th&gt; &lt;td&gt;0.2205&lt;/td&gt; 
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Estimator:&lt;/th&gt;             &lt;td&gt;IV-2SLS&lt;/td&gt;     &lt;th&gt;  Adj. R-squared:    &lt;/th&gt; &lt;td&gt;0.2079&lt;/td&gt; 
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;No. Observations:&lt;/th&gt;        &lt;td&gt;64&lt;/td&gt;        &lt;th&gt;  F-statistic:       &lt;/th&gt; &lt;td&gt;29.811&lt;/td&gt; 
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Date:&lt;/th&gt;             &lt;td&gt;Mon, Jan 03 2022&lt;/td&gt; &lt;th&gt;  P-value (F-stat)   &lt;/th&gt; &lt;td&gt;0.0000&lt;/td&gt; 
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Time:&lt;/th&gt;                 &lt;td&gt;18:31:10&lt;/td&gt;     &lt;th&gt;  Distribution:      &lt;/th&gt; &lt;td&gt;chi2(1)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Cov. Estimator:&lt;/th&gt;        &lt;td&gt;robust&lt;/td&gt;      &lt;th&gt;                     &lt;/th&gt;    &lt;td&gt;&lt;/td&gt;    
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;&lt;/th&gt;                          &lt;td&gt;&lt;/td&gt;         &lt;th&gt;                     &lt;/th&gt;    &lt;td&gt;&lt;/td&gt;    
&lt;/tr&gt;
&lt;/table&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;caption&gt;Parameter Estimates&lt;/caption&gt;
&lt;tr&gt;
     &lt;td&gt;&lt;/td&gt;    &lt;th&gt;Parameter&lt;/th&gt; &lt;th&gt;Std. Err.&lt;/th&gt; &lt;th&gt;T-stat&lt;/th&gt; &lt;th&gt;P-value&lt;/th&gt; &lt;th&gt;Lower CI&lt;/th&gt; &lt;th&gt;Upper CI&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;const&lt;/th&gt;   &lt;td&gt;2.0448&lt;/td&gt;    &lt;td&gt;1.1273&lt;/td&gt;   &lt;td&gt;1.8139&lt;/td&gt; &lt;td&gt;0.0697&lt;/td&gt;   &lt;td&gt;-0.1647&lt;/td&gt;  &lt;td&gt;4.2542&lt;/td&gt; 
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Exprop&lt;/th&gt;  &lt;td&gt;0.9235&lt;/td&gt;    &lt;td&gt;0.1691&lt;/td&gt;   &lt;td&gt;5.4599&lt;/td&gt; &lt;td&gt;0.0000&lt;/td&gt;   &lt;td&gt;0.5920&lt;/td&gt;   &lt;td&gt;1.2550&lt;/td&gt; 
&lt;/tr&gt;
&lt;/table&gt;&lt;br/&gt;&lt;br/&gt;Endogenous: Exprop&lt;br/&gt;Instruments: logMort&lt;br/&gt;Robust Covariance (Heteroskedastic)&lt;br/&gt;Debiased: False
&lt;p&gt;Given that we now have consistent and unbiased estimates, we can infer
from the model we have estimated that institutional differences
(stemming from institutions set up during colonization) can help
to explain differences in income levels across countries today.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://python-programming.quantecon.org/zreferences.html#acemoglu2001&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[AJR01]&lt;/a&gt; use a marginal effect of 0.94 to calculate that the
difference in the index between Chile and Nigeria (ie. institutional
quality) implies up to a 7-fold difference in income, emphasizing the
significance of institutions in economic development.&lt;/p&gt;
&lt;h2 id=&#34;24-matrix-algebra&#34;&gt;2.4 Matrix Algebra&lt;/h2&gt;
&lt;p&gt;The OLS parameter $ \beta $ can also be estimated using matrix
algebra and &lt;code&gt;numpy&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;The linear equation we want to estimate is (written in matrix form)&lt;/p&gt;
&lt;p&gt;$$
y = X\beta + \varepsilon
$$&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Init 
X = df[[&#39;const&#39;, &#39;Exprop&#39;]].values
Z = df[[&#39;const&#39;, &#39;logMort&#39;]].values
y = df[&#39;GDP&#39;].values
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To solve for the unknown parameter $ \beta $, we want to minimize
the sum of squared residuals&lt;/p&gt;
&lt;p&gt;$$
\underset{\hat{\beta}}{\min} \ \hat{\varepsilon}&amp;rsquo;\hat{\varepsilon}
$$&lt;/p&gt;
&lt;p&gt;Rearranging the first equation and substituting into the second
equation, we can write&lt;/p&gt;
&lt;p&gt;$$
\underset{\hat{\beta}}{\min} \ (Y - X\hat{\beta})&amp;rsquo; (Y - X\hat{\beta})
$$&lt;/p&gt;
&lt;p&gt;Solving this optimization problem gives the solution for the
$ \hat{\beta} $ coefficients&lt;/p&gt;
&lt;p&gt;$$
\hat{\beta} = (X&amp;rsquo;X)^{-1}X&amp;rsquo;y
$$&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Compute beta OLS
beta_OLS = inv(X.T @ X) @ X.T @ y

print(beta_OLS)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[4.66087966 0.52203367]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As we as see above, the OLS coefficient might suffer from endogeneity bias. We can solve the issue by instrumenting the predicted average expropriation rate with the average settler mortality.&lt;/p&gt;
&lt;p&gt;If we define settler mortality as $Z$, our full model is&lt;/p&gt;
&lt;p&gt;$$
y = X\beta + \varepsilon \
X = Z\gamma + \mu
$$&lt;/p&gt;
&lt;p&gt;Where we refer to the second equation as second stage and to the first equation as the reduced form equation. In our case, since the number of endogenous varaibles is equal to the number of insturments, there are two equivalent estimators that do not suffer from endogeneity bias: 2SLS and IV.&lt;/p&gt;
&lt;p&gt;IV, the one stage estimator&lt;/p&gt;
&lt;p&gt;$$
\hat \beta_{IV} = (Z&amp;rsquo;X)^{-1} Z&amp;rsquo; y
$$&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Compute beta IV
beta_IV = inv(Z.T @ X) @ Z.T @ y

print(beta_IV)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[2.0447613  0.92351936]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;One of the hypothesis behind the IV estimator is the &lt;em&gt;relevance&lt;/em&gt; of the instrument, i.e. we have a strong predictor in the first stage. This is the only hypothesis that we can empirically assess by checking the significance of the first stage coefficient.&lt;/p&gt;
&lt;p&gt;$$
\hat \gamma = (Z&amp;rsquo; Z)^{-1} Z&amp;rsquo;X \
\hat Var (\hat \gamma) = \sigma_u^2 (Z&amp;rsquo; Z)^{-1}
$$&lt;/p&gt;
&lt;p&gt;where&lt;/p&gt;
&lt;p&gt;$$
u = X - Z \hat \gamma
$$&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Estimate first stage coefficient
gamma_hat = (inv(Z.T @ Z) @ Z.T @ X)

print(gamma_hat[1,1])
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;-0.613289272386864
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Compute variance of the estimator
u = X - Z @ gamma_hat
var_gamma_hat = np.var(u) * inv(Z.T @ Z)

# Compute standard errors
std_gamma_hat = var_gamma_hat[1,1]**.5
print(std_gamma_hat)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;0.08834733362858548
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Compute 95% confidence interval
CI = [gamma_hat[1,1] - 1.96*std_gamma_hat, gamma_hat[1,1] + 1.96*std_gamma_hat]

print(CI)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[-0.7864500462988916, -0.4401284984748365]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The first stage coefficient is negative and significant, i.e. settler mortality is negatively correlated with the expropriation rate.&lt;/p&gt;
&lt;p&gt;How does it work when we have more instruments than endogenous variables? Two-State Least Squares.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Regress $X$ on $Z$ and obtain $\hat X$:
$$
\hat X = Z (Z&amp;rsquo; Z)^{-1} Z&amp;rsquo;X
$$&lt;/li&gt;
&lt;li&gt;Regress $Y$ on $\hat X$ and obtain $\hat \beta_{2SLS}$
$$
\hat \beta_{2SLS} = (\hat X&amp;rsquo; \hat X)^{-1} \hat X&amp;rsquo; y
$$&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In our case, just for the sake of exposition, let&amp;rsquo;s generate a second instrument: the settler mortality squared, &lt;code&gt;logMort_2&lt;/code&gt; = &lt;code&gt;logMort&lt;/code&gt;^2.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df[&#39;logMort_2&#39;] = df[&#39;logMort&#39;]**2
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Define Z
Z1 = df[[&#39;const&#39;, &#39;logMort&#39;, &#39;logMort_2&#39;]].values

# Compute beta 2SLS in two steps
X_hat = Z1 @ inv(Z1.T @ Z1) @ Z1.T @ X
beta_2SLS = inv(X_hat.T @ X_hat) @ X_hat.T @ y

print(beta_2SLS)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[3.08817432 0.76339075]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The 2SLS estimator does not have to be actually estimated in two stages. Combining the two formulas above, we get&lt;/p&gt;
&lt;p&gt;$$
\hat{\beta} _ {2SLS} =  \Big( X&amp;rsquo;Z (Z&amp;rsquo;Z)^{-1} Z&amp;rsquo;X \Big)^{-1} \Big( X&amp;rsquo;Z (Z&amp;rsquo;Z)^{-1} Z&amp;rsquo;y \Big)
$$&lt;/p&gt;
&lt;p&gt;which can be computed in one step.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Compute beta 2SLS in one step
beta_2SLS = inv(X_hat.T @ Z1 @ inv(Z1.T @ Z1) @ Z1.T @ X_hat) @ X_hat.T @ Z1 @ inv(Z1.T @ Z1) @ Z1.T @ y
    
print(beta_2SLS)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[3.08817432 0.76339075]
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Demand Estimation</title>
      <link>https://matteocourthoud.github.io/course/empirical-io/02_demand_estimation/</link>
      <pubDate>Fri, 29 Oct 2021 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/course/empirical-io/02_demand_estimation/</guid>
      <description>&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;h3 id=&#34;setting&#34;&gt;Setting&lt;/h3&gt;
&lt;p&gt;Oligopoly Supply&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;firms produce differentiated goods/products&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;selling to consumers with heterogeneous preferences&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;static model, complete information&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;products are given&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;equilibrium: NE for each product/market&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;cost-function&#34;&gt;Cost Function&lt;/h3&gt;
&lt;p&gt;Variable cost of product $j$:
$C_j (Q_j , w_{jt} , \mathbb \omega_{jt}, \gamma)$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;$Q_j$: total quantity of good $j$ sold&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$w_{jt}$ observable cost shifters; may include product
characteristics $x_{jt}$ that will affect demand (later)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$\omega_{jt}$ unobserved cost shifters (“cost shocks”); may be
correlated with latent demand shocks (later)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$\gamma$: parameters&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Notes&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;for multi-product firms, we’ll assume variable cost additive across
products for simplicity&lt;/li&gt;
&lt;li&gt;we ignore fixed costs: these affect entry/exit/innovation but not
pricing, &lt;em&gt;conditional on these things&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;notation&#34;&gt;Notation&lt;/h3&gt;
&lt;p&gt;Some other variables&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$J_t$: products/goods/choices in market $t$ (for now $J_t = J$)&lt;/li&gt;
&lt;li&gt;$P_t = (p_{1t},&amp;hellip;,p_{Jt})$: prices of all goods&lt;/li&gt;
&lt;li&gt;$\boldsymbol X_t = ( \boldsymbol x_{1t} , … , \boldsymbol x_{Jt})$ :
other characteristics of goods affecting demand (observed and
unobserved to us)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In general&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;I use &lt;strong&gt;bold&lt;/strong&gt; for arrays in dimensions that are not $i$
(consumers), $j$ (firms) or $t$ (markets)
&lt;ul&gt;
&lt;li&gt;For example product characteristics
$\boldsymbol x_{jt} = \lbrace x_{jt}^1,, &amp;hellip;, x_{jt}^K \rbrace$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;I use CAPS for variables aggregated over $j$ (firms)
&lt;ul&gt;
&lt;li&gt;For example vector of prices in market $t$:
$P_t = (p_{1t},&amp;hellip;,p_{Jt})$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;equilibrium-pricing&#34;&gt;Equilibrium Pricing&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Demand system:&lt;/p&gt;
&lt;p&gt;$$
q_{jt} = Q_j ( P_t, \boldsymbol X_t) \quad \text{for} \quad j = 1,&amp;hellip;,J.
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Profit function&lt;/p&gt;
&lt;p&gt;$$
\pi_{jt} = Q_j (P_t, \boldsymbol X_t) \Big[p_{jt} − mc_j (w_{jt}, \omega_{jt}, \gamma) \Big]
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;FOC wrt to $p_{jt}$:&lt;/p&gt;
&lt;p&gt;$$
p_{jt} = mc_{jt} - Q_j (P_t, \boldsymbol X_t) \left(\frac{\partial Q_j}{\partial p_{jt}}\right)^{-1}
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Inverse elasticity pricing (i.e., monopoly pricing) against the
“residual demand curve” $Q_j (P_t, \boldsymbol X_t)$:&lt;/p&gt;
&lt;p&gt;$$
\frac{p_{jt} - mc_{jt}}{p_{jt}} = - \frac{Q_j (P_t, \boldsymbol X_t)}{p_{jt}} \left(\frac{\partial Q_j}{\partial p_{jt}}\right)^{-1}
$$&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;what-do-we-get&#34;&gt;What do we get?&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Holding all else fixed, markups/prices depend on the own-price
elasticities of residual demand. Equilibrium depends, further, on
how a change in price of one good affects the quantities sold of
others, i.e., on cross-price demand elasticities&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;If we known demand, we can also perform a &lt;strong&gt;small miracle&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Re-arrange FOC&lt;/p&gt;
&lt;p&gt;$$
mc_{jt} = p_{jt} + Q_j (P_t, \boldsymbol X_t)\left(\frac{\partial Q_j}{\partial p_{jt}}\right)^{-1}
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Supply model + estimated demand $\to$ estimates of marginal
costs!&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;If we know demand and marginal costs, we can”predict” a lot of
stuff - i.e., give the quantitative implications of the model for
counterfactual worlds&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;issues&#34;&gt;Issues&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Typically we need to know levels/elasticities of demand at
particular points; i.e., effects of one price change holding all
else fixed&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The main challenge: unobserved demand shifters (“demand shocks”) at
the level of the good×market (e.g., unobserved product char or
market-specific variation in mean tastes for products)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;demand shocks are among the things that must be held fixed to
measure the relevant demand elasticities etc.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;explicit modeling of these demand shocks central in the applied IO
literature following S. Berry, Levinsohn, and Pakes
(&lt;a href=&#34;#ref-berry1995automobile&#34;&gt;1995&lt;/a&gt;) (often ignored outside this
literature).&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;key-challenge&#34;&gt;Key Challenge&lt;/h3&gt;
&lt;p&gt;The &lt;strong&gt;demand&lt;/strong&gt; of product $j$&lt;/p&gt;
&lt;p&gt;$$
q_{jt} (\boldsymbol X_{t}, P_t, \Xi_t)
$$&lt;/p&gt;
&lt;p&gt;depends on:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;$P_t$: $J$-vector of &lt;em&gt;all&lt;/em&gt; goods’ prices in market $t$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$\boldsymbol X_t$: $J \times k$ matrix of &lt;em&gt;all&lt;/em&gt; non-price
observables in market $t$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$\Xi_t$: J-vector of demand shocks for &lt;em&gt;all&lt;/em&gt; goods in market $t$&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Key insight&lt;/strong&gt;: we have an endogeneity problem even if prices were
exogenous!&lt;/p&gt;
&lt;h3 id=&#34;price-endogeneity-adds-to-the-challenge&#34;&gt;Price Endogeneity Adds to the Challenge&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;all $J$ endogenous prices are on RHS of demand for each good&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;equilibrium pricing implies that each price depends on all demand
shocks and all cost shocks&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;prices endogenous&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;control function generally is not a valid solution&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;clear that we need sources of exogenous price variation, but&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;what exactly is required?&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;how do we proceed?&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;blp-model&#34;&gt;BLP: Model&lt;/h2&gt;
&lt;h3 id=&#34;goals-of-blp&#34;&gt;Goals of BLP&lt;/h3&gt;
&lt;p&gt;Model of S. Berry, Levinsohn, and Pakes
(&lt;a href=&#34;#ref-berry1995automobile&#34;&gt;1995&lt;/a&gt;)&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;parsimonious specification to generate the distribution
$F_U (\cdot| P, \Xi)$ of random utilities&lt;/li&gt;
&lt;li&gt;sufficiently rich heterogeneity in preferences to permit
reasonable/flexible substitution patterns&lt;/li&gt;
&lt;li&gt;be explicit about unobservables, including the nature of endogeneity
“problem(s)”&lt;/li&gt;
&lt;li&gt;use the model to reveal solutions to the identification problem,
including appropriate instruments&lt;/li&gt;
&lt;li&gt;computationally feasible (in early 1990s!) algorithm for consistent
estimation of the model and standard errors.&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;utility-specification&#34;&gt;Utility Specification&lt;/h3&gt;
&lt;p&gt;Utility of consumer $i$ for product $j$&lt;/p&gt;
&lt;p&gt;$$
u_{ijt} = \boldsymbol x_{jt} \boldsymbol \beta_{it} - \alpha p_{jt} + \xi_{jt} + \epsilon_{ijt}
$$&lt;/p&gt;
&lt;p&gt;Where&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;$\boldsymbol x_{jt}$: $K$-vector of characteristics of product $j$
in market $t$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$\boldsymbol \beta_{it} = (\beta_{it}^{1}, &amp;hellip;, \beta_{it}^K)$:
vector of tastes for characteristics $1,…,K$ in market $t$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;$\beta_{it}^k = \beta_0^k + \sigma_k \zeta_{it}^k$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;$\beta_0^k$: fixed taste for characteristic $k$ (the usual
$\beta$)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$\zeta_{it}^k$: random taste, i.i.d. across consumers and
markets $t$&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$\alpha$: price elasticity&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$p_{jt}$ price of product $j$ in market $t$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$\xi_{jt}$: unobservable product shock at the level of products $j$
$\times$ market $t$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$\epsilon_{ijt}$: idiosyncratic (and latent) taste&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;exogenous-and-endogenous-product-characteristics&#34;&gt;Exogenous and Endogenous Product Characteristics&lt;/h3&gt;
&lt;p&gt;Utility of consumer $i$ for product $j$&lt;/p&gt;
&lt;p&gt;$$
u_{ijt} = \boldsymbol x_{jt} \beta_{it} - \alpha p_{jt} + \xi_{jt} + \epsilon_{ijt}
$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;exogenous characteristics: $\boldsymbol x_{jt} \perp \xi_{jt}$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;endogenous characteristics: $p_{jt}$ (usually a scalar, price)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;typically each $p_{jt}$ will depend on whole vector
$\Xi_t = (\xi_{1t} , . . . , \xi_{Jt} )$
&lt;ul&gt;
&lt;li&gt;and on own costs $mc_{jt}$ and others’ costs $mc_{-jt}$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;we need to distinguish true effects of prices on demand from the&lt;/li&gt;
&lt;li&gt;effects of $\Xi_t$ ; this will require &lt;strong&gt;instruments&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;of course the equation above is not an estimating equation
($u_{ijt}$ not observed)&lt;/li&gt;
&lt;li&gt;because prices and quantities are all endogenous - indeed
determined - simultaneously, you may suspect (correctly) that
instruments for prices alone may not suffice.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;utility-specification-rewritten&#34;&gt;Utility Specification, Rewritten&lt;/h3&gt;
&lt;p&gt;Rewrite&lt;/p&gt;
&lt;p&gt;$$
\begin{align}
u_{ijt} &amp;amp;= \boldsymbol x_{jt} \boldsymbol \beta_{it} - \alpha p_{jt} + \xi_{jt} + \epsilon_{ijt} = \newline
&amp;amp;= \delta_{jt} + \nu_{ijt}
\end{align}
$$&lt;/p&gt;
&lt;p&gt;where&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\delta_{jt} = \boldsymbol x_{jt} \boldsymbol \beta_0 - \alpha p_{jt} + \xi_{jt}$
&lt;ul&gt;
&lt;li&gt;mean utility of good $j$ in market $t$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;$\nu_{ijt} = \sum_{k} x_{jt}^{k} \sigma^{k} \zeta_{i t}^{k} + \epsilon_{ijt} \equiv \boldsymbol x_{jt} \tilde{\boldsymbol \beta}&lt;em&gt;{it} + \epsilon&lt;/em&gt;{ijt}$
&lt;ul&gt;
&lt;li&gt;We split $\beta_{it}$ into its &lt;strong&gt;random&lt;/strong&gt; ($\tilde{\beta}_{it}$)
and &lt;strong&gt;non-random&lt;/strong&gt; ($\beta_0$) part&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;from-consumer-utility-to-demand&#34;&gt;From Consumer Utility to Demand&lt;/h3&gt;
&lt;p&gt;With a &lt;strong&gt;continuum of consumers in each market&lt;/strong&gt;: market shares = choice
probabilities&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;P.S. continuum not needed, enough that sampling error on choice
probs negligible compared to that of moments based on variation
across products/markets&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$
s_{jt} (\Delta_t, \boldsymbol X_t, \boldsymbol \sigma) = \Pr (y_{it} = j) = \int_{\mathcal A_j (\Delta_t)} \text d F_{\nu} \Big(\nu_{i0t}, \nu_{i1t}, &amp;hellip; , \nu_{iJt} \ \Big| \ \boldsymbol X_t, \boldsymbol \sigma \Big)
$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;where $$
\mathcal A_j(\Delta_t) = \Big\lbrace (\nu_{i0t}, \nu_{i1t}, &amp;hellip; , \nu_{iJt} ) \in \mathbb{R}^{J+1}: \delta_{jt} + \nu_{ijt} \geq \delta_{kt} + \nu_{ikt} \ , \ \forall k \Big\rbrace
$$ &lt;strong&gt;In words&lt;/strong&gt;: market share of firm $j$ is the frequency of
consumers buying good $j$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Demand&lt;/strong&gt; is just shares $s_{jt}$ per market size $M_t$ $$
q_{jt} = M_t \times s_j (\Delta_t, \boldsymbol X_t, \boldsymbol \sigma)
$$&lt;/p&gt;
&lt;h3 id=&#34;why-random-coefficients&#34;&gt;Why Random Coefficients?&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Without&lt;/strong&gt; random coefficients $$
\begin{aligned}
u_{ijt} &amp;amp;= \underbrace{\boldsymbol x_{jt} \boldsymbol \beta_0 - \alpha p_{jt} + \xi_{jt}} + \epsilon_{ijt} \newline
&amp;amp;= \hspace{3.4em} \delta_{jt} \hspace{3.4em} + \epsilon_{ijt}
\end{aligned}
$$ If $\epsilon_{ijt}$ are iid and independent of
$(\boldsymbol X_t, P_t)$, e.g. as in the multinomial logit or probit
models,&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;products differ only in mean utilities $\delta_{jt}$&lt;/li&gt;
&lt;li&gt;$\to$ market shares depend only on the mean utilities&lt;/li&gt;
&lt;li&gt;$\to$ price elasticities (own and cross) depend only on mean
utilities too&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Implication&lt;/strong&gt;: two products with the same market shares have the same
cross elasticities w.r.t. &lt;strong&gt;all&lt;/strong&gt; other products&lt;/p&gt;
&lt;h3 id=&#34;does-this-matter&#34;&gt;Does this matter?&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Yes&lt;/strong&gt;!&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Mercedes class-A&lt;/em&gt; and &lt;em&gt;Fiat Panda&lt;/em&gt; might both have low market
shares&lt;/li&gt;
&lt;li&gt;But realistically should have very different cross-price
elasticities w.r.t. &lt;em&gt;BMW series-2&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;What is the issue?&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Models (like MNL) that have only &lt;strong&gt;iid additive taste shocks&lt;/strong&gt;
impose very &lt;strong&gt;restrictive relationships&lt;/strong&gt; between the &lt;strong&gt;levels&lt;/strong&gt; of
market shares and the matrix of own and cross-price &lt;strong&gt;derivatives&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Impact on &lt;strong&gt;counterfactuals&lt;/strong&gt;!&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Restrictions only coming from model assumptions (analytical
convenience)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Models always imporse restrictions&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;necessary for estimation&lt;/li&gt;
&lt;li&gt;but must allow flexibility in the relevant dimensions&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;how-do-random-coefficients-help&#34;&gt;How do random coefficients help?&lt;/h3&gt;
&lt;p&gt;In &lt;strong&gt;reality&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;goods differ in multiple dimensions&lt;/li&gt;
&lt;li&gt;consumers have (heterogeneous) preferences over these differences&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;How do &lt;strong&gt;random coefficients&lt;/strong&gt; capture it?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;large $\beta_i^k$ $\leftrightarrow$ strong taste for characteristic
$k$
&lt;ul&gt;
&lt;li&gt;e.g., maximum speed for sport car&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Consumer $i$’s first choice likely to have high value of $x^k$&lt;/li&gt;
&lt;li&gt;$i$’s second choice too!
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Mark&lt;/strong&gt;: cross elasticities are always about 1st vs. 2nd
choices&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Incorporating this allows more sensible substitution patterns&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;competition is mostly “local”&lt;/li&gt;
&lt;li&gt;i.e., between firms offering products appealing to the same
consumers.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;which-random-coefficients&#34;&gt;Which random coefficients?&lt;/h3&gt;
&lt;p&gt;Which characteristics have random coefficients?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;dummies for subsets of products?
&lt;ul&gt;
&lt;li&gt;S. T. Berry (&lt;a href=&#34;#ref-berry1994estimating&#34;&gt;1994&lt;/a&gt;): covers the
nested logit as a special case&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;certain horizontal or vertical characteristics?
&lt;ul&gt;
&lt;li&gt;parts of $(\boldsymbol X_t, P_t)$?&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;In practice&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Choice depends on the application and data set, including
instruments&lt;/li&gt;
&lt;li&gt;Too many RC’s (w.r.t quantity of data available) $\to$ imprecise
estimates of $\boldsymbol \sigma$&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;blp-estimation&#34;&gt;BLP: Estimation&lt;/h2&gt;
&lt;h3 id=&#34;setting-1&#34;&gt;Setting&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Observables&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\boldsymbol X_t$: product characteristics&lt;/li&gt;
&lt;li&gt;$P_t$: prices&lt;/li&gt;
&lt;li&gt;$S_t$: observed market shares&lt;/li&gt;
&lt;li&gt;$\boldsymbol W_t$: observable cost shifters&lt;/li&gt;
&lt;li&gt;$\boldsymbol Z_t$: excluded instruments&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Sketch of procedure&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;start with demand model alone&lt;/li&gt;
&lt;li&gt;suppose $ F_{&lt;code&gt;\nu&lt;/code&gt;{=tex}} (&lt;code&gt;\cdot  &lt;/code&gt;{=tex} |
 &lt;code&gt;\boldsymbol &lt;/code&gt;{=tex}X, &lt;code&gt;\boldsymbol &lt;/code&gt;{=tex}&lt;code&gt;\sigma &lt;/code&gt;{=tex})$ is
known (i.e., $\sigma$ known)&lt;/li&gt;
&lt;li&gt;for each market $t$, find mean utilities $\Delta_t \in \mathbb R$
such that
$s_{jt} (\Delta_t, \boldsymbol X_t, \boldsymbol \sigma) = s^{obs}_{jt} \ \forall j$
&lt;ul&gt;
&lt;li&gt;i.e.,“invert” model at observed market shares to find mean
utilities $\boldsymbol \delta$&lt;/li&gt;
&lt;li&gt;where $s^{obs}_{jt}$ are the observed market shares&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;using IV ,e.g. $\mathbb E [\boldsymbol z_{jt} | \xi_{jt} ] = 0$,
estimate the equation&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;issues-1&#34;&gt;Issues&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;What instruments?&lt;/li&gt;
&lt;li&gt;Will the “inversion” step actually work?&lt;/li&gt;
&lt;li&gt;What about $\boldsymbol \sigma$??&lt;/li&gt;
&lt;li&gt;Formal estimator?&lt;/li&gt;
&lt;li&gt;Computational algorithm(s)?&lt;/li&gt;
&lt;li&gt;Supply side
&lt;ul&gt;
&lt;li&gt;additional restrictions (moment conditions)
&lt;ul&gt;
&lt;li&gt;help estimation of demand&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;additional parameters: marginal cost function
&lt;ul&gt;
&lt;li&gt;why? may care directly&lt;/li&gt;
&lt;li&gt;and needed for counterfactuals that change equilibrium
quantities unless $mc$ is constant&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;instruments&#34;&gt;Instruments&lt;/h3&gt;
&lt;p&gt;We need intruments for all endogenous variables—&lt;strong&gt;prices and
quantities&lt;/strong&gt;—&lt;strong&gt;independently&lt;/strong&gt;.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Excluded cost shifters $\boldsymbol W_t$ (classic)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Usually: wages, material costs, shipping cost to market $t$,
taxes/tariffs, demand shifters from other markets&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Or proxies for them&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Usually: price of same good in another mkt (“&lt;em&gt;Hausman
instruments&lt;/em&gt;”)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Markup shifters:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Usually: characteristics of “nearby” markets (“&lt;em&gt;Waldfogel
instruments&lt;/em&gt;”)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Logic: income/age/education in San Francisco might affect prices
in Oakland but might be independent fo Oakland preferences&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Product characteristics of other firms in the same market
$\boldsymbol X_{-jt}$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;“&lt;em&gt;BLP instruments&lt;/em&gt;”&lt;/li&gt;
&lt;li&gt;affect quantities directly; affect prices (markups) via
equilibrium only&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;inversion&#34;&gt;Inversion&lt;/h3&gt;
&lt;p&gt;How do we get from market shares to prices??&lt;/p&gt;
&lt;p&gt;Given x,σ and any positive shares sh, define the following &lt;strong&gt;mapping&lt;/strong&gt;
$\Phi : \mathbb R^j \to \mathbb R^j$ $$
\Phi (\Delta_t) = \Delta_t + \log\Big( \hat S^{obs}_t \Big) - \log \Big( S_t (\Delta_t, \boldsymbol X_t, \boldsymbol \sigma) \Big)
$$ S. T. Berry (&lt;a href=&#34;#ref-berry1994estimating&#34;&gt;1994&lt;/a&gt;): for any nonzero
shares sh, Φ is a &lt;strong&gt;contraction&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;under mild conditions on the linear random coefficients random
utility model&lt;/li&gt;
&lt;li&gt;extreme value and normal random coeff not necessary&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;What does it imply?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;It has a unique fixed point: we can compute
$\delta_{jt} = \delta (S_t, \boldsymbol X_t, \boldsymbol \sigma)$&lt;/li&gt;
&lt;li&gt;We can compute the fixed point iterating the contraction from any
initial guess $\Delta_{0t}$&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;what-about-sigma&#34;&gt;What about $\sigma$?&lt;/h3&gt;
&lt;p&gt;What we we got?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;inversion result: for any market shares and any
$\boldsymbol \sigma$, we can find a vector of mean utilities
$\Delta_t$ that rationalizes the data with the BLP model&lt;/li&gt;
&lt;li&gt;a non-identification result? there is no information about
$\boldsymbol \sigma$ from market shares?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;What are we forgetting?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Cross-market variation!&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;We can get the mean utilities
$\delta_{jt} = \boldsymbol x_{jt} \boldsymbol \beta_0 - \alpha p_{jt} + \xi_{jt}$&lt;/li&gt;
&lt;li&gt;As in OLS, use $\boldsymbol z_{jt} \perp \xi_{jt}$ to get
identification of
$(\alpha, \boldsymbol \beta_0, \boldsymbol \sigma)$&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;identification-of-sigma&#34;&gt;Identification of $\sigma$&lt;/h3&gt;
&lt;p&gt;We are trying to estimate
$(\alpha, \boldsymbol \beta_0, \boldsymbol \sigma)$ from $$
\mathbb E \Big[ \xi_{jt} (\alpha, \boldsymbol \beta_0, \boldsymbol \sigma) \cdot \boldsymbol z_{jt} \Big] = \mathbb E \Big[ \big( \delta_{jt}(\boldsymbol \sigma) - \boldsymbol x_{jt} \boldsymbol \beta_0 + \alpha p_{jt} \big) \cdot \boldsymbol z_{jt} \Big]
$$ What kind of &lt;strong&gt;intruments&lt;/strong&gt; $\boldsymbol z_{jt}$ do we need?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\boldsymbol x_{jt}$ (for $\boldsymbol \beta_0$)&lt;/li&gt;
&lt;li&gt;intruments for $p_{jt}$ (for $\alpha$)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;but also&lt;/strong&gt; something for $\boldsymbol \sigma$!&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;blp-estimation-1&#34;&gt;BLP Estimation&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Steps&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Take guess of parameters
$(\alpha, \boldsymbol \beta_0, \boldsymbol \sigma)$&lt;/li&gt;
&lt;li&gt;From observed market shared $S^{obs}&lt;em&gt;{t}$ and $\boldsymbol \sigma$
get mean utilities $\delta&lt;/em&gt;{jt} (\boldsymbol \sigma)$&lt;/li&gt;
&lt;li&gt;Use also $(\alpha, \boldsymbol \beta_0)$ to get
$\xi_{jt} (\alpha, \boldsymbol \beta_0, \boldsymbol \sigma)$&lt;/li&gt;
&lt;li&gt;Compute empirical moments
$\frac{1}{JT} \xi_{jt} (\alpha, \boldsymbol \beta_0, \boldsymbol \sigma) \cdot \boldsymbol z_{jt}$&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The GMM estimator is
$(\hat \alpha, \boldsymbol{\hat{\beta}_0}, \boldsymbol{\hat{\sigma}})$
that get the empirical moments as close to $0$ as possible.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Issues&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Computing $S_t (\Delta_t, \boldsymbol X_t, \boldsymbol \sigma)$
involves a &lt;strong&gt;high dimensional integral&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;Use simulation to approximate distribution of random tastes
$\zeta_{it}^k$&lt;/li&gt;
&lt;li&gt;P.S. recall that we have decomposed random coefficients
$\beta_{it}^k$ as
$\beta_{it}^k = \beta_0^k + \sigma_k \zeta_{it}^k$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;$\xi_{jt} (\alpha, \boldsymbol \beta_0, \boldsymbol \sigma)$ has
&lt;strong&gt;no closed form solution&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;Compute it via contraction&lt;/li&gt;
&lt;li&gt;MPEC?&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;computation&#34;&gt;Computation&lt;/h2&gt;
&lt;h3 id=&#34;nested-fixed-point-algorithm&#34;&gt;Nested fixed point algorithm&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Sketch of the algorithm&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Draw a vector of consumer tastes&lt;/li&gt;
&lt;li&gt;Until you have found a minimum for
$\mathbb E \Big[ \xi_{jt} (\alpha, \boldsymbol \beta_0, \boldsymbol \sigma) \cdot \boldsymbol z_{jt} \Big]$
do
&lt;ul&gt;
&lt;li&gt;Pick a vector of parameter values
$(\alpha, \boldsymbol \beta_0, \boldsymbol \sigma)$&lt;/li&gt;
&lt;li&gt;Initialize mean utilities $\delta_{jt}^0$&lt;/li&gt;
&lt;li&gt;Until
$\big|\big| \Delta_{t}^{n+1} - \Delta_{t}^{n} \big|\big| &amp;lt; tolerance$
do
&lt;ul&gt;
&lt;li&gt;Compute implied shares:
$s_{jt} (\Delta_{t}^{n}, \boldsymbol X_t, \boldsymbol \sigma) = \int \frac{\exp \left[ \boldsymbol x_{j t} \boldsymbol{\tilde{\beta}}&lt;em&gt;{it}+\delta&lt;/em&gt;{j t}\right]}{1+\sum_{j^{\prime}} \exp \left[\boldsymbol x_{j^{\prime} t} \boldsymbol{\tilde{\beta}}&lt;em&gt;{it}+\delta&lt;/em&gt;{j&amp;rsquo; t}\right]} f\left( \boldsymbol{\tilde{\beta}}&lt;em&gt;{it} \mid \theta\right) d \tilde{\beta}&lt;/em&gt;{i t}$&lt;/li&gt;
&lt;li&gt;Update mean utilities:
$\Delta_{t}^{n+1} = \Delta_{t}^{n} + \log\Big( \hat S^{obs}&lt;em&gt;t \Big) - \log \Big( S_t (\Delta&lt;/em&gt;{t}^{n}, \boldsymbol X_t, \boldsymbol \sigma) \Big)$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Compute
$\xi_{jt} = \delta_{jt} - \boldsymbol x_{jt} \boldsymbol \beta_0 + \alpha p_{jt}$&lt;/li&gt;
&lt;li&gt;Compute
$\mathbb E \Big[ \xi_{jt} (\alpha, \boldsymbol \beta_0, \boldsymbol \sigma) \cdot \boldsymbol z_{jt} \Big]$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;notes&#34;&gt;Notes&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Important to draw shocks outside the optimization routine!&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;appendix&#34;&gt;Appendix&lt;/h2&gt;
&lt;h3 id=&#34;references-references&#34;&gt;References [references]&lt;/h3&gt;
&lt;div id=&#34;refs&#34; class=&#34;references csl-bib-body hanging-indent&#34;
markdown=&#34;1&#34;&gt;
&lt;div id=&#34;ref-berry1994estimating&#34; class=&#34;csl-entry&#34; markdown=&#34;1&#34;&gt;
&lt;p&gt;Berry, Steven T. 1994. “Estimating Discrete-Choice Models of Product
Differentiation.” &lt;em&gt;The RAND Journal of Economics&lt;/em&gt;, 242–62.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-berry1995automobile&#34; class=&#34;csl-entry&#34; markdown=&#34;1&#34;&gt;
&lt;p&gt;Berry, Steven, James Levinsohn, and Ariel Pakes. 1995. “Automobile
Prices in Market Equilibrium.” &lt;em&gt;Econometrica: Journal of the Econometric
Society&lt;/em&gt;, 841–90.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Probability Theory</title>
      <link>https://matteocourthoud.github.io/course/metrics/02_probability/</link>
      <pubDate>Fri, 29 Oct 2021 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/course/metrics/02_probability/</guid>
      <description>&lt;h2 id=&#34;probability&#34;&gt;Probability&lt;/h2&gt;
&lt;h3 id=&#34;probability-space&#34;&gt;Probability Space&lt;/h3&gt;
&lt;p&gt;A &lt;strong&gt;probability space&lt;/strong&gt; is a triple $(\Omega, \mathcal A, P)$ where&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\Omega$ is the sample space.&lt;/li&gt;
&lt;li&gt;$\mathcal A$ is the $\sigma$-algebra on $\Omega$.&lt;/li&gt;
&lt;li&gt;$P$ is a probability measure.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The &lt;strong&gt;sample space&lt;/strong&gt; $\Omega$ is the space of all possible events.&lt;/p&gt;
&lt;p&gt;What is a $\sigma$-algebra and a probability measure?&lt;/p&gt;
&lt;h3 id=&#34;sigma-algebra&#34;&gt;Sigma Algebra&lt;/h3&gt;
&lt;p&gt;A nonempty set (of subsets of $\Omega$) $\mathcal A \in 2^\Omega$ is a
&lt;strong&gt;sigma algebra&lt;/strong&gt; ($\sigma$-algebra) of $\Omega$ if the following
conditions hold:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;$\Omega \in \mathcal A$&lt;/li&gt;
&lt;li&gt;If $A \in \mathcal A$, then $(\Omega - A) \in \mathcal A$&lt;/li&gt;
&lt;li&gt;If $A_1, A_2, &amp;hellip; \in \mathcal A$, then
$\bigcup _ {i=1}^{\infty} A_i \in \mathcal A$&lt;/li&gt;
&lt;/ol&gt;
&lt;blockquote&gt;
&lt;p&gt;The smallest $\sigma$-algebra is $\lbrace \emptyset, \Omega \rbrace$
and the largest one is $2^\Omega$ (in cardinality terms).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Suppose $\Omega = \mathbb R$. Let
$\mathcal{C} = \lbrace (a, b],-\infty \leq a&amp;lt;b&amp;lt;\infty \rbrace$. Then the
&lt;strong&gt;Borel&lt;/strong&gt; $\sigma$&lt;strong&gt;- algebra&lt;/strong&gt; on $\mathbb R$ is defined by $$
\mathcal B (\mathbb R) = \sigma (\mathcal C)
$$&lt;/p&gt;
&lt;h3 id=&#34;probability-measure&#34;&gt;Probability Measure&lt;/h3&gt;
&lt;p&gt;A &lt;strong&gt;probability measure&lt;/strong&gt; $P: \mathcal A \to [0,1]$ is a set function
with domain $\mathcal A$ and codomain $[0,1]$ such that&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;$P(A) \geq 0 \ \forall A \in \mathcal A$&lt;/li&gt;
&lt;li&gt;$P$ is $\sigma$-additive: is $A_n \in \mathcal A$ are pairwise
disjoint events ($A_j \cap A_k = \emptyset$ for $j \neq k$), then $$
P\left(\bigcup _ {n=1}^{\infty} A_{n} \right)=\sum _ {n=1}^{\infty} P\left(A_{n}\right)
$$&lt;/li&gt;
&lt;li&gt;$P(\Omega) = 1$&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;properties&#34;&gt;Properties&lt;/h3&gt;
&lt;p&gt;Some properties of probability measures&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$P\left(A^{c}\right)=1-P(A)$&lt;/li&gt;
&lt;li&gt;$P(\emptyset)=0$&lt;/li&gt;
&lt;li&gt;For $A, B \in \mathcal{A}$, $P(A \cup B)=P(A)+P(B)-P(A \cap B)$&lt;/li&gt;
&lt;li&gt;For $A, B \in \mathcal{A}$, if $A \subset B$ then $P(A) \leq P(B)$&lt;/li&gt;
&lt;li&gt;For $A_n \in \mathcal{A}$,
$P \left(\cup _ {n=1}^\infty A_{n} \right) \leq \sum _ {n=1}^\infty P(A_n)$&lt;/li&gt;
&lt;li&gt;For $A_n \in \mathcal{A}$, if $A_n \uparrow A$ then
$\lim _ {n \to \infty} P(A_n) = P(A)$&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;conditional-probability&#34;&gt;Conditional Probability&lt;/h3&gt;
&lt;p&gt;Let $A, B \in \mathcal A$ and $P(B) &amp;gt; 0$, the &lt;strong&gt;conditional
probability&lt;/strong&gt; of $A$ given $B$ is $$
P(A | B)=\frac{P(A \cap B)}{P(B)}
$$&lt;/p&gt;
&lt;p&gt;Two events $A$ and $B$ are &lt;strong&gt;independent&lt;/strong&gt; if $P(A \cap B)=P(A) P(B)$.&lt;/p&gt;
&lt;h3 id=&#34;law-of-total-probability&#34;&gt;Law of Total Probability&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt; (Law of Total Probability)&lt;/p&gt;
&lt;p&gt;Let $(E_n) _ {n \geq 1}$ be a finite or countable partition of $\Omega$.
Then, if $A \in \mathcal A$, $$
P(A) = \sum_n P(A | E_n ) P(E_n)
$$&lt;/p&gt;
&lt;h3 id=&#34;bayes-theorem&#34;&gt;Bayes Theorem&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt; (Bayes Theorem)&lt;/p&gt;
&lt;p&gt;Let $(E_n) _ {n \geq 1}$ be a finite or countable partition of $\Omega$,
and suppose $P(A) &amp;gt; 0$. Then, $$
P(E_n | A) = \frac{P(A | E_n) P(E_n)}{\sum_m P(A | E_m) P(E_m)}
$$&lt;/p&gt;
&lt;p&gt;For a single event $E \in \Omega$, $$
P(E|A) = \frac{P(A|E) P(E)}{P(A)}
$$&lt;/p&gt;
&lt;h2 id=&#34;random-variables&#34;&gt;Random Variables&lt;/h2&gt;
&lt;h3 id=&#34;definition&#34;&gt;Definition&lt;/h3&gt;
&lt;p&gt;A &lt;strong&gt;random variable&lt;/strong&gt; $X$ on a probability space
$(\Omega,\mathcal A, P)$ is a (measurable) mapping
$X : \Omega \to \mathbb{R}$ such that $$
\forall B \in \mathcal{B}(\mathbb{R}), \quad X^{-1}(B) \in \mathcal{A}
$$&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The measurability condition states that the inverse image is a
measurable set of $\Omega$ i.e. $X^{-1}(B) \in \mathcal A$. This is
essential since probabilities are defined only on $\mathcal A$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;In words, a random variable it’s a mapping from events to real numbers
such that each interval on the real line can be mapped back into an
element of the sigma algebra (it can be the empty set).&lt;/p&gt;
&lt;h3 id=&#34;distribution-function&#34;&gt;Distribution Function&lt;/h3&gt;
&lt;p&gt;Let $X$ be a real valued random variable. The &lt;strong&gt;distribution function&lt;/strong&gt;
(also called cumulative distribution function) of $X$, commonly denoted
$F_X(x)$ is defined by $$
F_X(x) = \Pr(X \leq x)
$$&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Properties&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$F$ is monotone non-decreasing&lt;/li&gt;
&lt;li&gt;$F$ is right continuous&lt;/li&gt;
&lt;li&gt;$\lim _ {x \to - \infty} F(x)=0$ and
$\lim _ {x \to + \infty} F(x)=1$&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;p&gt;The random variables $(X_1, .. , X_n)$ are independent if and only if $$
F _ {(X_1, &amp;hellip; , X_n)} (x) = \prod _ {i=1}^n F_{X_i} (x_i) \quad \forall x \in \mathbb R^n
$$&lt;/p&gt;
&lt;h3 id=&#34;density-function&#34;&gt;Density Function&lt;/h3&gt;
&lt;p&gt;Let $X$ be a real valued random variable. $X$ has a &lt;strong&gt;probability
density function&lt;/strong&gt; if there exists $f_X(x)$ such that for all measurable
$A \subset \mathbb{R}$, $$
P(X \in A) = \int_A f_X(x) \mathrm{d} x
$$&lt;/p&gt;
&lt;h2 id=&#34;moments&#34;&gt;Moments&lt;/h2&gt;
&lt;h3 id=&#34;expected-value&#34;&gt;Expected Value&lt;/h3&gt;
&lt;p&gt;The &lt;strong&gt;expected value&lt;/strong&gt; of a random variable, when it exists, is given by
$$
\mathbb{E}[ X ] = \int_ \Omega X(\omega) \mathrm{d} P
$$ When $X$ has a density, then $$
\mathbb{E} [ X ] = \int_ \mathbb{R} x f_X (x) \mathrm{d} x = \int _ \mathbb{R} x \mathrm{d} F_X (x)
$$&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;empirical expectation&lt;/strong&gt; (or &lt;strong&gt;sample average&lt;/strong&gt;) is given by $$
\mathbb{E}_n [x_i] = \frac{1}{n} \sum _ {i=1}^N x_i
$$&lt;/p&gt;
&lt;h3 id=&#34;variance-and-covariance&#34;&gt;Variance and Covariance&lt;/h3&gt;
&lt;p&gt;The &lt;strong&gt;covariance&lt;/strong&gt; of two random variables $X$, $Y$ defined on $\Omega$
is $$
Cov(X, Y ) = \mathbb{E}[ (X - \mathbb{E}[ X ]) (Y - \mathbb{E}[ Y ]) ]  = \mathbb{E}[XY ] - \mathbb{E}[ X ]E[ Y ]
$$ In vector notation,
$Cov(X, Y) = \mathbb{E}[XY&amp;rsquo;] - \mathbb{E}[ X ]\mathbb{E}[Y&amp;rsquo;]$.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;variance&lt;/strong&gt; of a random variable $X$, when it exists, is given by
$$
Var(X) = \mathbb{E}[ (X - \mathbb{E}[ X ])^2 ] = \mathbb{E}[X^2] - \mathbb{E}[ X ]^2
$$ In vector notation,
$Var(X) = \mathbb{E}[XX&amp;rsquo;] - \mathbb{E}[ X ]\mathbb{E}[X&amp;rsquo;]$.&lt;/p&gt;
&lt;h3 id=&#34;properties-1&#34;&gt;Properties&lt;/h3&gt;
&lt;p&gt;Let $X, Y, Z, T \in \mathcal{L}^{2}$ and $a, b, c, d \in \mathbb{R}$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$Cov(X, X) = Var(X)$&lt;/li&gt;
&lt;li&gt;$Cov(X, Y) = Cov(Y, X)$&lt;/li&gt;
&lt;li&gt;$Cov(aX + b, Y) = a \ Cov(X,Y)$&lt;/li&gt;
&lt;li&gt;$Cov(X+Z, Y) = Cov(X,Y) + Cov(Z,Y)$&lt;/li&gt;
&lt;li&gt;$Cov(aX + bZ, cY + dT) = ac * Cov(X,Y) + ad * Cov(X,T) + bc * Cov(Z,Y) + bd * Cov(Z,T)$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Let $X, Y \in \mathcal L^1$ be independent. Then,
$\mathbb E[XY] = \mathbb E[ X ] \mathbb E[ Y ]$.&lt;/p&gt;
&lt;p&gt;If $X$ and $Y$ are independent, then $Cov(X,Y) = 0$.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Note that the converse does not hold:
$Cov(X,Y) = 0 \not \to X \perp Y$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;sample-variance&#34;&gt;Sample Variance&lt;/h3&gt;
&lt;p&gt;The &lt;strong&gt;sample variance&lt;/strong&gt; is given by $$
Var_n (x_i) = \frac{1}{n} \sum _ {i=1}^N (x_i - \bar{x})^2
$$ where
$\bar{x_i} = \mathbb{E}_n [x_i] = \frac{1}{n} \sum _ {i=1}^N x_i$.&lt;/p&gt;
&lt;h3 id=&#34;finite-sample-bias-theorem&#34;&gt;Finite Sample Bias Theorem&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt;: The expected sample variance
$\mathbb{E} [\sigma^2_n] = \mathbb{E} \left[ \frac{1}{n} \sum _ {i=1}^N \left(y_i - \mathbb{E}_n[ Y ] \right)^2 \right]$
gives an estimate of the population variance that is biased by a factor
of $\frac{1}{n}$ and is therefore referred to as &lt;strong&gt;biased sample
variance&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;: $$
\begin{aligned}
&amp;amp;\mathbb{E}[\sigma^2_n] =  \mathbb{E} \left[ \frac{1}{n} \sum _ {i=1}^n \left( y_i - \mathbb{E}_n [ Y ] \right)^2 \right] =
\newline
&amp;amp;= \mathbb{E} \left[ \frac{1}{n} \sum _ {i=1}^n \left( y_i - \frac{1}{n} \sum _ {i=1}^n y_i \right )^2 \right] =
\newline
&amp;amp;= \frac{1}{n} \sum _ {i=1}^n \mathbb{E} \left[ y_i^2 - \frac{2}{n} y_i \sum _ {j=1}^n y_j + \frac{1}{n^2} \sum _ {j=1}^n y_j \sum _ {k=1}^{n}y_k  \right] =
\newline
&amp;amp;= \frac{1}{n} \sum _ {i=1}^n  \left[ \frac{n-2}{n} \mathbb{E}[y_i^2]  - \frac{2}{n} \sum _ {j\neq i} \mathbb{E}[y_i y_j] + \frac{1}{n^2} \sum _ {j=1}^n \sum _ {k\neq j} \mathbb{E}[y_j y_k] + \frac{1}{n^2} \sum _ {j=1}^n \mathbb{E}[y_j^2] \right] =
\newline
&amp;amp;= \frac{1}{n} \sum _ {i=1}^n  \left[ \frac{n-2}{n}(\mu^2 + \sigma^2) - \frac{2}{n} (n-1) \mu^2 + \frac{1}{n^2} n(n-1)\mu^2 + \frac{1}{n^2} n (\mu^2 + \sigma^2)]\right] =
\newline
&amp;amp;= \frac{n-1}{n} \sigma^2
\end{aligned}
$$ $$\tag*{$\blacksquare$}$$&lt;/p&gt;
&lt;h3 id=&#34;inequalities&#34;&gt;Inequalities&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Triangle Inequality&lt;/strong&gt;: if $\mathbb{E} [ X ] &amp;lt; \infty$, then $$
|\mathbb{E} [ X ] | \leq \mathbb{E} [|X|]
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Markov’s Inequality&lt;/strong&gt;: if $\mathbb{E}[ X ] &amp;lt; \infty$, then $$
\Pr(|X| &amp;gt; t) \leq \frac{1}{t} \mathbb{E}[|X|]
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Chebyshev’s Inequality&lt;/strong&gt;: if $\mathbb{E}[X^2] &amp;lt; \infty$, then $$
\Pr(|X- \mu|&amp;gt; t \sigma) \leq \frac{1}{t^2}\Leftrightarrow \Pr(|X- \mu|&amp;gt; t ) \leq \frac{\sigma^2}{t^2}
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Cauchy-Schwarz’s Inequality&lt;/strong&gt;: $$
\mathbb{E} [|XY|] \leq \sqrt{\mathbb{E}[X^2] \mathbb{E}[Y^2]}
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Minkowski Inequality&lt;/strong&gt;: $$
\left( \sum _ {k=1}^n | x_k + y_k |^p \right) ^ {\frac{1}{p}} \leq \left( \sum _ {k=1}^n | x_k |^p \right) ^ {\frac{1}{p}} + \left( \sum _ {k=1}^n | y_k | ^p \right) ^ { \frac{1}{p} }
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Jensen’s Inequality&lt;/strong&gt;: if $g( \cdot)$ is concave (e.g. logarithmic
function), then $$
\mathbb{E}[g(x)] \leq g(\mathbb{E}[ X ])
$$ Similarly, if $g(\cdot)$ is convex (e.g. exponential function),
then $$
\mathbb{E}[g(x)] \geq g(\mathbb{E}[ X ])
$$&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;law-of-iterated-expectations&#34;&gt;Law of Iterated Expectations&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt; (Law of Iterated Expectations) $$
\mathbb{E}(Y) = \mathbb{E}_X [\mathbb{E}(Y|X)]
$$ &amp;gt; This states that the expectation of the conditional expectation is
the unconditional expectation. &amp;gt; &amp;gt; In other words the average of the
conditional averages is the unconditional average.&lt;/p&gt;
&lt;h3 id=&#34;law-of-total-variance&#34;&gt;Law of Total Variance&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt; (Law of Total Variance) $$
Var(Y) = Var_X (\mathbb{E}[Y |X]) + \mathbb{E}_X [Var(Y|X)]
$$&lt;/p&gt;
&lt;p&gt;Since variances are always non-negative, the law of total variance
implies $$
Var(Y) \geq Var_X (\mathbb{E}[Y |X])
$$&lt;/p&gt;
&lt;h2 id=&#34;distributions&#34;&gt;Distributions&lt;/h2&gt;
&lt;h3 id=&#34;normal-distribution&#34;&gt;Normal Distribution&lt;/h3&gt;
&lt;p&gt;We say that a random variable $Z$ has the &lt;strong&gt;standard normal
distribution&lt;/strong&gt;, or &lt;strong&gt;Gaussian&lt;/strong&gt;, written $Z \sim N(0,1)$, if it has the
density $$
\phi(x)=\frac{1}{\sqrt{2 \pi}} \exp \left(-\frac{x^{2}}{2}\right), \quad-\infty&amp;lt;x&amp;lt;\infty
$$ If $Z \sim N(0, 1)$ and $X = \mu + \sigma Z$ for $\mu \in \mathbb R$
and $\sigma \geq 0$, then $X$ has a &lt;strong&gt;univariate normal distribution&lt;/strong&gt;,
written $X \sim N(\mu, \sigma^2)$. By change-of-variables &lt;em&gt;X&lt;/em&gt; has the
density $$
f(x)=\frac{1}{\sqrt{2 \pi \sigma^{2}}} \exp \left(-\frac{(x-\mu)^{2}}{2 \sigma^{2}}\right), \quad-\infty&amp;lt;x&amp;lt;\infty
$$&lt;/p&gt;
&lt;h3 id=&#34;multinomial-normal-distribution&#34;&gt;Multinomial Normal Distribution&lt;/h3&gt;
&lt;p&gt;We say that the &lt;em&gt;k&lt;/em&gt; -vector &lt;em&gt;Z&lt;/em&gt; has a &lt;strong&gt;multivariate standard normal
distribution&lt;/strong&gt;, written $Z \sim N(0, I_k)$ if it has the joint density
$$
f(x)=\frac{1}{(2 \pi)^{k / 2}} \exp \left(-\frac{x^{\prime} x}{2}\right), \quad x \in \mathbb{R}^{k}
$$ If $Z \sim N(0, I_k)$ and $X = \mu + B Z$, then the &lt;em&gt;k&lt;/em&gt;-vector $X$
has a &lt;strong&gt;multivariate normal distribution&lt;/strong&gt;, written
$X \sim N(\mu, \Sigma)$ where $\Sigma = BB&amp;rsquo; \geq 0$. If $\sigma &amp;gt; 0$,
then by change-of-variables $X$ has the joint density function $$
f(x)=\frac{1}{(2 \pi)^{k / 2} \operatorname{det}(\Sigma)^{1 / 2}} \exp \left(-\frac{(x-\mu)^{\prime} \Sigma^{-1}(x-\mu)}{2}\right), \quad x \in \mathbb{R}^{k}
$$&lt;/p&gt;
&lt;h3 id=&#34;properties-2&#34;&gt;Properties&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;The expectation and covariance matrix of $X \sim N(\mu, \Sigma)$ are
$\mathbb E &lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; = \mu$ and $Var&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; =\Sigma$.&lt;/li&gt;
&lt;li&gt;If $(X,Y)$ are multivariate normal, $X$ and $Y$ are uncorrelated if
and only if they are independent.&lt;/li&gt;
&lt;li&gt;If $X \sim N(\mu, \Sigma)$ and $Y = a + bB$, then
$X \sim N(a + B\mu, B \Sigma B&amp;rsquo;)$.&lt;/li&gt;
&lt;li&gt;If $X \sim N(0, I_k)$, then $X&amp;rsquo;X \sim \chi^2_k$, chi-square with $k$
degrees of freedom.&lt;/li&gt;
&lt;li&gt;If $X \sim N(0, \Sigma)$ with $\Sigma&amp;gt;0$, then
$X&amp;rsquo; \Sigma X \sim \chi_k$ where $k = \dim (X)$.&lt;/li&gt;
&lt;li&gt;If $Z \sim N(0,1)$ and $Q \sim \chi^2_k$ are independent then
$\frac{Z}{\sqrt{Q/k}} \sim t_k$, student t with &lt;em&gt;k&lt;/em&gt; degrees of
freedom.&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;normal-distribution-relatives&#34;&gt;Normal Distribution Relatives&lt;/h3&gt;
&lt;p&gt;These distributions are relatives of the normal distribution&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;$\chi^2_q \sim \sum _ {i=1}^q Z_i^2$ where $Z_i \sim N(0,1)$&lt;/li&gt;
&lt;li&gt;$t_n \sim \frac{Z}{\sqrt{\chi^2 _ n}/n }$&lt;/li&gt;
&lt;li&gt;$F(n_1 , n_2) \sim \frac{\chi^2 _ {n_1} / n_1}{\chi^2 _ {n_2}/n_2}$&lt;/li&gt;
&lt;/ol&gt;
&lt;blockquote&gt;
&lt;p&gt;The $t$ distribution is approximately standard normal but has heavier
tails. The approximation is good for $n \geq 30$:
$t_{n\geq 30} \sim N(0,1)$&lt;/p&gt;
&lt;/blockquote&gt;
</description>
    </item>
    
    <item>
      <title>Data Types</title>
      <link>https://matteocourthoud.github.io/course/data-science/03_data_types/</link>
      <pubDate>Sun, 01 May 2022 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/course/data-science/03_data_types/</guid>
      <description>&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import numpy as np
import pandas as pd
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;setup&#34;&gt;Setup&lt;/h2&gt;
&lt;p&gt;For the scope of this tutorial we are going to use AirBnb Scraped data for the city of Bologna. The data is freely available at &lt;strong&gt;Inside AirBnb&lt;/strong&gt;: &lt;a href=&#34;http://insideairbnb.com/get-the-data.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://insideairbnb.com/get-the-data.html&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;A description of all variables in all datasets is avaliable &lt;a href=&#34;https://docs.google.com/spreadsheets/d/1iWCNJcSutYqpULSQHlNyGInUvHg2BoUGoNRIGa6Szc4/edit#gid=982310896&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;We are going to use 2 datasets:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;listing dataset: contains listing-level information&lt;/li&gt;
&lt;li&gt;pricing dataset: contains pricing data, over time&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Import listings data
url_listings = &amp;quot;http://data.insideairbnb.com/italy/emilia-romagna/bologna/2021-12-17/visualisations/listings.csv&amp;quot;
df_listings = pd.read_csv(url_listings)

# Import pricing data
url_prices = &amp;quot;http://data.insideairbnb.com/italy/emilia-romagna/bologna/2021-12-17/data/calendar.csv.gz&amp;quot;
df_prices = pd.read_csv(url_prices, compression=&amp;quot;gzip&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;numerical-data&#34;&gt;Numerical Data&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Methods&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;+&lt;/code&gt;, &lt;code&gt;-&lt;/code&gt;, &lt;code&gt;*&lt;/code&gt;, &lt;code&gt;/&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;numpy functions&lt;/li&gt;
&lt;li&gt;&lt;code&gt;pd.cut()&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;p&gt;Standard mathematical operations between columns are done row-wise.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_prices[&#39;maximum_nights&#39;] - df_prices[&#39;minimum_nights&#39;]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;0           148
1           357
2           357
3           357
4           357
           ... 
1260340    1124
1260341    1124
1260342    1124
1260343    1124
1260344    1124
Length: 1260345, dtype: int64
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can use most &lt;code&gt;numpy&lt;/code&gt; operations element-wise on a single column.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;np.log(df_listings[&#39;price&#39;])
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;0       4.219508
1       3.367296
2       3.912023
3       4.836282
4       3.912023
          ...   
3448    3.465736
3449    3.806662
3450    3.912023
3451    4.897840
3452    4.744932
Name: price, Length: 3453, dtype: float64
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can create a categorical variables from a numerical one using the &lt;code&gt;pd.cut()&lt;/code&gt; function.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;pd.cut(df_listings[&#39;price&#39;], 
       bins = [0, 50, 100, np.inf], 
       labels=[&#39;cheap&#39;, &#39;ok&#39;, &#39;expensive&#39;])
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;0              ok
1           cheap
2           cheap
3       expensive
4           cheap
          ...    
3448        cheap
3449        cheap
3450        cheap
3451    expensive
3452    expensive
Name: price, Length: 3453, dtype: category
Categories (3, object): [&#39;cheap&#39; &amp;lt; &#39;ok&#39; &amp;lt; &#39;expensive&#39;]
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;string-data&#34;&gt;String Data&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Methods&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;+&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;.str.replace&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;.str.contains&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;.astype(str)&lt;/code&gt;
-&lt;code&gt;pd.get_dummies()&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;p&gt;We can use the &lt;code&gt;+&lt;/code&gt; operator between columns, to do pairwise append.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: we cannot do it with strings.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_listings[&#39;host_name&#39;] + df_listings[&#39;neighbourhood&#39;]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;0                     CarloSanto Stefano
1              EleonoraPorto - Saragozza
2                     PaoloSanto Stefano
3                Anna MariaSanto Stefano
4               ValerioPorto - Saragozza
                      ...               
3448                        IleanaNavile
3449           FernandaPorto - Saragozza
3450                        IleanaNavile
3451        Wonderful ItalySanto Stefano
3452    Wonderful ItalyPorto - Saragozza
Length: 3453, dtype: object
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Pandas Series have a lot of vectorized string functions. You can find a list &lt;a href=&#34;https://pandas.pydata.org/docs/user_guide/text.html#method-summary&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;For example, we want to remove the dollar symbol from the &lt;code&gt;price&lt;/code&gt; variable in the &lt;code&gt;df_prices&lt;/code&gt; dataset.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_prices[&#39;price&#39;].str.replace(&#39;$&#39;, &#39;&#39;, regex=False)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;0           70.00
1           68.00
2           68.00
3           68.00
4           68.00
            ...  
1260340    115.00
1260341    115.00
1260342    115.00
1260343    115.00
1260344    115.00
Name: price, Length: 1260345, dtype: object
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Some of these functions use regular expressions.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;match()&lt;/code&gt;: Call re.match() on each element, returning a boolean.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;extract()&lt;/code&gt;: Call re.match() on each element, returning matched groups as strings.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;findall()&lt;/code&gt;: Call re.findall() on each element&lt;/li&gt;
&lt;li&gt;&lt;code&gt;replace()&lt;/code&gt;: Replace occurrences of pattern with some other string&lt;/li&gt;
&lt;li&gt;&lt;code&gt;contains()&lt;/code&gt;: Call re.search() on each element, returning a boolean&lt;/li&gt;
&lt;li&gt;&lt;code&gt;count()&lt;/code&gt;: Count occurrences of pattern&lt;/li&gt;
&lt;li&gt;&lt;code&gt;split()&lt;/code&gt;: Equivalent to str.split(), but accepts regexps
rsplit()&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For example, the next code checks whether in the word &lt;code&gt;centre&lt;/code&gt; or &lt;code&gt;center&lt;/code&gt; are contained in the text description.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_listings[&#39;name&#39;].str.contains(&#39;centre|center&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;0        True
1       False
2        True
3       False
4       False
        ...  
3448    False
3449    False
3450    False
3451    False
3452    False
Name: name, Length: 3453, dtype: bool
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Lastly, we can (try to) convert string variables to numeric using &lt;code&gt;astype(float)&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_prices[&#39;price&#39;].str.replace(&#39;[$|,]&#39;, &#39;&#39;, regex=True).astype(float)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;0           70.0
1           68.0
2           68.0
3           68.0
4           68.0
           ...  
1260340    115.0
1260341    115.0
1260342    115.0
1260343    115.0
1260344    115.0
Name: price, Length: 1260345, dtype: float64
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can also use it to convert numerics to strings using &lt;code&gt;astype(str)&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_listings[&#39;id&#39;].astype(str)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;0          42196
1          46352
2          59697
3          85368
4         145779
          ...   
3448    53810648
3449    53820830
3450    53837098
3451    53837654
3452    53854962
Name: id, Length: 3453, dtype: object
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can generate dummies from a categorical variable using &lt;code&gt;pd.get_dummies()&lt;/code&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;pd.get_dummies(df_listings[&#39;neighbourhood&#39;]).head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;Borgo Panigale - Reno&lt;/th&gt;
      &lt;th&gt;Navile&lt;/th&gt;
      &lt;th&gt;Porto - Saragozza&lt;/th&gt;
      &lt;th&gt;San Donato - San Vitale&lt;/th&gt;
      &lt;th&gt;Santo Stefano&lt;/th&gt;
      &lt;th&gt;Savena&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;h2 id=&#34;time-data&#34;&gt;Time Data&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Methods&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;pd.to_datetime()&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;.dt.year&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;.df.to_period()&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;pd.to_timedelta()&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;p&gt;In the &lt;code&gt;df_prices&lt;/code&gt; we have a date variable, &lt;code&gt;date&lt;/code&gt;. Which format is it in? We can check it with the &lt;code&gt;.dtypes&lt;/code&gt; attribute.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_prices[&#39;date&#39;].dtypes
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;dtype(&#39;O&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can &lt;strong&gt;convert&lt;/strong&gt; a variable into a date using the&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_prices[&#39;datetime&#39;] = pd.to_datetime(df_prices[&#39;date&#39;])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Indeed, if we now check the format of the &lt;code&gt;datetime&lt;/code&gt; variable, it&amp;rsquo;s &lt;code&gt;datetime&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_prices[&#39;datetime&#39;].dtypes
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;dtype(&#39;&amp;lt;M8[ns]&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Once we have a variable in &lt;code&gt;datetime&lt;/code&gt; format, we gain plenty of datetime operations through the &lt;code&gt;dt&lt;/code&gt; accessor object for datetime-like properties.&lt;/p&gt;
&lt;p&gt;For example, we can &lt;strong&gt;extract the year&lt;/strong&gt; using &lt;code&gt;.dt.year&lt;/code&gt;. We can do the same with &lt;code&gt;month&lt;/code&gt;, &lt;code&gt;week&lt;/code&gt; and &lt;code&gt;day&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_prices[&#39;datetime&#39;].dt.year
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;0          2021
1          2021
2          2021
3          2021
4          2021
           ... 
1260340    2022
1260341    2022
1260342    2022
1260343    2022
1260344    2022
Name: datetime, Length: 1260345, dtype: int64
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can change the &lt;strong&gt;level of aggregation&lt;/strong&gt; of a date using &lt;code&gt;.dt.to_period()&lt;/code&gt;. The option &lt;code&gt;M&lt;/code&gt; converts to year-month level.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_prices[&#39;datetime&#39;].dt.to_period(&#39;M&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;0          2021-12
1          2021-12
2          2021-12
3          2021-12
4          2021-12
            ...   
1260340    2022-12
1260341    2022-12
1260342    2022-12
1260343    2022-12
1260344    2022-12
Name: datetime, Length: 1260345, dtype: period[M]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can &lt;strong&gt;add or subtract time periods&lt;/strong&gt; from a date using the &lt;code&gt;pd.to_timedelta()&lt;/code&gt; function. We need to specify the unit of measurement with the &lt;code&gt;unit&lt;/code&gt; option.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_prices[&#39;datetime&#39;] -  pd.to_timedelta(3, unit=&#39;d&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;0         2021-12-14
1         2021-12-14
2         2021-12-15
3         2021-12-16
4         2021-12-17
             ...    
1260340   2022-12-09
1260341   2022-12-10
1260342   2022-12-11
1260343   2022-12-12
1260344   2022-12-13
Name: datetime, Length: 1260345, dtype: datetime64[ns]
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;missing-data&#34;&gt;Missing Data&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Methods&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;.isna()&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;.dropna()&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;.fillna()&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;p&gt;The function &lt;code&gt;isna()&lt;/code&gt; reports missing values.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_listings.isna().head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;id&lt;/th&gt;
      &lt;th&gt;name&lt;/th&gt;
      &lt;th&gt;host_id&lt;/th&gt;
      &lt;th&gt;host_name&lt;/th&gt;
      &lt;th&gt;neighbourhood_group&lt;/th&gt;
      &lt;th&gt;neighbourhood&lt;/th&gt;
      &lt;th&gt;latitude&lt;/th&gt;
      &lt;th&gt;longitude&lt;/th&gt;
      &lt;th&gt;room_type&lt;/th&gt;
      &lt;th&gt;price&lt;/th&gt;
      &lt;th&gt;minimum_nights&lt;/th&gt;
      &lt;th&gt;number_of_reviews&lt;/th&gt;
      &lt;th&gt;last_review&lt;/th&gt;
      &lt;th&gt;reviews_per_month&lt;/th&gt;
      &lt;th&gt;calculated_host_listings_count&lt;/th&gt;
      &lt;th&gt;availability_365&lt;/th&gt;
      &lt;th&gt;number_of_reviews_ltm&lt;/th&gt;
      &lt;th&gt;license&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;True&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;True&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;True&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;True&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;True&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;True&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;True&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;True&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;True&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;True&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;To get a quick description of the amount of missing data in the dataset, we can use&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_listings.isna().sum()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;id                                   0
name                                 0
host_id                              0
host_name                            9
neighbourhood_group               3453
neighbourhood                        0
latitude                             0
longitude                            0
room_type                            0
price                                0
minimum_nights                       0
number_of_reviews                    0
last_review                        409
reviews_per_month                  409
calculated_host_listings_count       0
availability_365                     0
number_of_reviews_ltm                0
license                           3318
dtype: int64
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can drop missing values using &lt;code&gt;dropna()&lt;/code&gt;. It drops all rows with at least one missing value.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_listings.dropna().shape
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;(0, 18)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In this case unfortunately, it drops all the rows. If we wa to drop only rows with all missing values, we can use the parameter &lt;code&gt;how=&#39;all&#39;&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_listings.dropna(how=&#39;all&#39;).shape
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;(3453, 18)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If we want to drop only missing values for one particular value, we can use the &lt;code&gt;subset&lt;/code&gt; option.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_listings.dropna(subset=[&#39;reviews_per_month&#39;]).shape
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;(3044, 18)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can also fill the missing values instead of dropping them, using &lt;code&gt;fillna()&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_listings.fillna(&#39; -- This was NA  -- &#39;).head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;id&lt;/th&gt;
      &lt;th&gt;name&lt;/th&gt;
      &lt;th&gt;host_id&lt;/th&gt;
      &lt;th&gt;host_name&lt;/th&gt;
      &lt;th&gt;neighbourhood_group&lt;/th&gt;
      &lt;th&gt;neighbourhood&lt;/th&gt;
      &lt;th&gt;latitude&lt;/th&gt;
      &lt;th&gt;longitude&lt;/th&gt;
      &lt;th&gt;room_type&lt;/th&gt;
      &lt;th&gt;price&lt;/th&gt;
      &lt;th&gt;minimum_nights&lt;/th&gt;
      &lt;th&gt;number_of_reviews&lt;/th&gt;
      &lt;th&gt;last_review&lt;/th&gt;
      &lt;th&gt;reviews_per_month&lt;/th&gt;
      &lt;th&gt;calculated_host_listings_count&lt;/th&gt;
      &lt;th&gt;availability_365&lt;/th&gt;
      &lt;th&gt;number_of_reviews_ltm&lt;/th&gt;
      &lt;th&gt;license&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;42196&lt;/td&gt;
      &lt;td&gt;50 sm Studio in the historic centre&lt;/td&gt;
      &lt;td&gt;184487&lt;/td&gt;
      &lt;td&gt;Carlo&lt;/td&gt;
      &lt;td&gt;-- This was NA  --&lt;/td&gt;
      &lt;td&gt;Santo Stefano&lt;/td&gt;
      &lt;td&gt;44.48507&lt;/td&gt;
      &lt;td&gt;11.34786&lt;/td&gt;
      &lt;td&gt;Entire home/apt&lt;/td&gt;
      &lt;td&gt;68&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;180&lt;/td&gt;
      &lt;td&gt;2021-11-12&lt;/td&gt;
      &lt;td&gt;1.32&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;161&lt;/td&gt;
      &lt;td&gt;6&lt;/td&gt;
      &lt;td&gt;-- This was NA  --&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;46352&lt;/td&gt;
      &lt;td&gt;A room in Pasolini&#39;s house&lt;/td&gt;
      &lt;td&gt;467810&lt;/td&gt;
      &lt;td&gt;Eleonora&lt;/td&gt;
      &lt;td&gt;-- This was NA  --&lt;/td&gt;
      &lt;td&gt;Porto - Saragozza&lt;/td&gt;
      &lt;td&gt;44.49168&lt;/td&gt;
      &lt;td&gt;11.33514&lt;/td&gt;
      &lt;td&gt;Private room&lt;/td&gt;
      &lt;td&gt;29&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;300&lt;/td&gt;
      &lt;td&gt;2021-11-30&lt;/td&gt;
      &lt;td&gt;2.2&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;248&lt;/td&gt;
      &lt;td&gt;37&lt;/td&gt;
      &lt;td&gt;-- This was NA  --&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;59697&lt;/td&gt;
      &lt;td&gt;COZY LARGE BEDROOM in the city center&lt;/td&gt;
      &lt;td&gt;286688&lt;/td&gt;
      &lt;td&gt;Paolo&lt;/td&gt;
      &lt;td&gt;-- This was NA  --&lt;/td&gt;
      &lt;td&gt;Santo Stefano&lt;/td&gt;
      &lt;td&gt;44.48817&lt;/td&gt;
      &lt;td&gt;11.34124&lt;/td&gt;
      &lt;td&gt;Private room&lt;/td&gt;
      &lt;td&gt;50&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;240&lt;/td&gt;
      &lt;td&gt;2020-10-04&lt;/td&gt;
      &lt;td&gt;2.18&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;327&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;-- This was NA  --&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;85368&lt;/td&gt;
      &lt;td&gt;Garden House Bologna&lt;/td&gt;
      &lt;td&gt;467675&lt;/td&gt;
      &lt;td&gt;Anna Maria&lt;/td&gt;
      &lt;td&gt;-- This was NA  --&lt;/td&gt;
      &lt;td&gt;Santo Stefano&lt;/td&gt;
      &lt;td&gt;44.47834&lt;/td&gt;
      &lt;td&gt;11.35672&lt;/td&gt;
      &lt;td&gt;Entire home/apt&lt;/td&gt;
      &lt;td&gt;126&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;40&lt;/td&gt;
      &lt;td&gt;2019-11-03&lt;/td&gt;
      &lt;td&gt;0.34&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;332&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;-- This was NA  --&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;145779&lt;/td&gt;
      &lt;td&gt;SINGLE ROOM&lt;/td&gt;
      &lt;td&gt;705535&lt;/td&gt;
      &lt;td&gt;Valerio&lt;/td&gt;
      &lt;td&gt;-- This was NA  --&lt;/td&gt;
      &lt;td&gt;Porto - Saragozza&lt;/td&gt;
      &lt;td&gt;44.49306&lt;/td&gt;
      &lt;td&gt;11.33786&lt;/td&gt;
      &lt;td&gt;Private room&lt;/td&gt;
      &lt;td&gt;50&lt;/td&gt;
      &lt;td&gt;10&lt;/td&gt;
      &lt;td&gt;69&lt;/td&gt;
      &lt;td&gt;2021-12-05&lt;/td&gt;
      &lt;td&gt;0.55&lt;/td&gt;
      &lt;td&gt;9&lt;/td&gt;
      &lt;td&gt;365&lt;/td&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;-- This was NA  --&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;We can also make missing values if we want.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_listings.iloc[2, 2] = np.nan
df_listings.iloc[:3, :3]
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;id&lt;/th&gt;
      &lt;th&gt;name&lt;/th&gt;
      &lt;th&gt;host_id&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;42196&lt;/td&gt;
      &lt;td&gt;50 sm Studio in the historic centre&lt;/td&gt;
      &lt;td&gt;184487.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;46352&lt;/td&gt;
      &lt;td&gt;A room in Pasolini&#39;s house&lt;/td&gt;
      &lt;td&gt;467810.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;59697&lt;/td&gt;
      &lt;td&gt;COZY LARGE BEDROOM in the city center&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Non-Parametric Regression</title>
      <link>https://matteocourthoud.github.io/course/ml-econ/03_nonparametric/</link>
      <pubDate>Wed, 09 Mar 2022 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/course/ml-econ/03_nonparametric/</guid>
      <description>&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Setup
%matplotlib inline
from utils.lecture03 import *
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;dataset&#34;&gt;Dataset&lt;/h3&gt;
&lt;p&gt;For this session, we are mostly going to work with the wage dataset.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df = pd.read_csv(&#39;data/Wage.csv&#39;, index_col=0)
df.head(3)
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;year&lt;/th&gt;
      &lt;th&gt;age&lt;/th&gt;
      &lt;th&gt;maritl&lt;/th&gt;
      &lt;th&gt;race&lt;/th&gt;
      &lt;th&gt;education&lt;/th&gt;
      &lt;th&gt;region&lt;/th&gt;
      &lt;th&gt;jobclass&lt;/th&gt;
      &lt;th&gt;health&lt;/th&gt;
      &lt;th&gt;health_ins&lt;/th&gt;
      &lt;th&gt;logwage&lt;/th&gt;
      &lt;th&gt;wage&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;231655&lt;/th&gt;
      &lt;td&gt;2006&lt;/td&gt;
      &lt;td&gt;18&lt;/td&gt;
      &lt;td&gt;1. Never Married&lt;/td&gt;
      &lt;td&gt;1. White&lt;/td&gt;
      &lt;td&gt;1. &amp;lt; HS Grad&lt;/td&gt;
      &lt;td&gt;2. Middle Atlantic&lt;/td&gt;
      &lt;td&gt;1. Industrial&lt;/td&gt;
      &lt;td&gt;1. &amp;lt;=Good&lt;/td&gt;
      &lt;td&gt;2. No&lt;/td&gt;
      &lt;td&gt;4.318063&lt;/td&gt;
      &lt;td&gt;75.043154&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;86582&lt;/th&gt;
      &lt;td&gt;2004&lt;/td&gt;
      &lt;td&gt;24&lt;/td&gt;
      &lt;td&gt;1. Never Married&lt;/td&gt;
      &lt;td&gt;1. White&lt;/td&gt;
      &lt;td&gt;4. College Grad&lt;/td&gt;
      &lt;td&gt;2. Middle Atlantic&lt;/td&gt;
      &lt;td&gt;2. Information&lt;/td&gt;
      &lt;td&gt;2. &amp;gt;=Very Good&lt;/td&gt;
      &lt;td&gt;2. No&lt;/td&gt;
      &lt;td&gt;4.255273&lt;/td&gt;
      &lt;td&gt;70.476020&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;161300&lt;/th&gt;
      &lt;td&gt;2003&lt;/td&gt;
      &lt;td&gt;45&lt;/td&gt;
      &lt;td&gt;2. Married&lt;/td&gt;
      &lt;td&gt;1. White&lt;/td&gt;
      &lt;td&gt;3. Some College&lt;/td&gt;
      &lt;td&gt;2. Middle Atlantic&lt;/td&gt;
      &lt;td&gt;1. Industrial&lt;/td&gt;
      &lt;td&gt;1. &amp;lt;=Good&lt;/td&gt;
      &lt;td&gt;1. Yes&lt;/td&gt;
      &lt;td&gt;4.875061&lt;/td&gt;
      &lt;td&gt;130.982177&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;This dataset contains information on wages and individual characteristics.&lt;/p&gt;
&lt;p&gt;Our main objective is going to be to explain wages using the observables contained in the dataset.&lt;/p&gt;
&lt;h2 id=&#34;polynomial-regression-and-step-functions&#34;&gt;Polynomial Regression and Step Functions&lt;/h2&gt;
&lt;p&gt;As we have seen in the first lecture, the most common way to introduce linearities is to replace the standard linear model&lt;/p&gt;
&lt;p&gt;$$
y_i = \beta_0 + \beta_1 x_i + \varepsilon_i
$$&lt;/p&gt;
&lt;p&gt;with a polynomial function&lt;/p&gt;
&lt;p&gt;$$
y_i = \beta_0 + \beta_1 x_i + \beta_2 x_i^2 + \beta_2 x_i^3 + &amp;hellip; + \varepsilon_i
$$&lt;/p&gt;
&lt;h3 id=&#34;explore-the-data&#34;&gt;Explore the Data&lt;/h3&gt;
&lt;p&gt;Suppose we want to investigate the relationship between &lt;code&gt;wage&lt;/code&gt; and &lt;code&gt;age&lt;/code&gt;. Let&amp;rsquo;s first plot the two variables.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Scatterplot of the data
df.plot.scatter(&#39;age&#39;,&#39;wage&#39;,color=&#39;w&#39;, edgecolors=&#39;k&#39;, alpha=0.3);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/03_nonparametric_10_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;polynomials-of-different-degrees&#34;&gt;Polynomials of different degrees&lt;/h3&gt;
&lt;p&gt;The relationship is highly complex and non-linear. Let&amp;rsquo;s expand our linear regression polynomials of different degrees: 1 to 5.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;X_poly1 = PolynomialFeatures(1).fit_transform(df.age.values.reshape(-1,1))
X_poly2 = PolynomialFeatures(2).fit_transform(df.age.values.reshape(-1,1))
X_poly3 = PolynomialFeatures(3).fit_transform(df.age.values.reshape(-1,1))
X_poly4 = PolynomialFeatures(4).fit_transform(df.age.values.reshape(-1,1))
X_poly5 = PolynomialFeatures(5).fit_transform(df.age.values.reshape(-1,1))
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;variables&#34;&gt;Variables&lt;/h3&gt;
&lt;p&gt;Our dependent varaible is going to be a dummy for income above 250.000 USD.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Get X and y
X = df.age
y = df.wage
y01 = (df.wage &amp;gt; 250).map({False:0, True:1}).values
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;polynomia-regression&#34;&gt;Polynomia Regression&lt;/h3&gt;
&lt;p&gt;If we run a linear regression on a 4-degree polinomial expansion of &lt;code&gt;age&lt;/code&gt;, this is what it looks like`:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Fit ols on 4th degree polynomial
fit = sm.OLS(y, X_poly4).fit()
fit.summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
    &lt;td&gt;&lt;/td&gt;       &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;const&lt;/th&gt; &lt;td&gt; -184.1542&lt;/td&gt; &lt;td&gt;   60.040&lt;/td&gt; &lt;td&gt;   -3.067&lt;/td&gt; &lt;td&gt; 0.002&lt;/td&gt; &lt;td&gt; -301.879&lt;/td&gt; &lt;td&gt;  -66.430&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;x1&lt;/th&gt;    &lt;td&gt;   21.2455&lt;/td&gt; &lt;td&gt;    5.887&lt;/td&gt; &lt;td&gt;    3.609&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    9.703&lt;/td&gt; &lt;td&gt;   32.788&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;x2&lt;/th&gt;    &lt;td&gt;   -0.5639&lt;/td&gt; &lt;td&gt;    0.206&lt;/td&gt; &lt;td&gt;   -2.736&lt;/td&gt; &lt;td&gt; 0.006&lt;/td&gt; &lt;td&gt;   -0.968&lt;/td&gt; &lt;td&gt;   -0.160&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;x3&lt;/th&gt;    &lt;td&gt;    0.0068&lt;/td&gt; &lt;td&gt;    0.003&lt;/td&gt; &lt;td&gt;    2.221&lt;/td&gt; &lt;td&gt; 0.026&lt;/td&gt; &lt;td&gt;    0.001&lt;/td&gt; &lt;td&gt;    0.013&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;x4&lt;/th&gt;    &lt;td&gt;-3.204e-05&lt;/td&gt; &lt;td&gt; 1.64e-05&lt;/td&gt; &lt;td&gt;   -1.952&lt;/td&gt; &lt;td&gt; 0.051&lt;/td&gt; &lt;td&gt;-6.42e-05&lt;/td&gt; &lt;td&gt; 1.45e-07&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;h3 id=&#34;measures-of-fit&#34;&gt;Measures of Fit&lt;/h3&gt;
&lt;p&gt;In this case, the single coefficients are not of particular interest. We are mostly interested in the best capturing the relationship between &lt;code&gt;age&lt;/code&gt; and &lt;code&gt;wage&lt;/code&gt;. How can we pick among thedifferent polynomials?&lt;/p&gt;
&lt;p&gt;We compare different polynomial degrees. For each regression, we are going to look at a series of metrics:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;absolute residuals&lt;/li&gt;
&lt;li&gt;sum of squared residuals&lt;/li&gt;
&lt;li&gt;the difference in SSR w.r (SSR).t the 0-degree case&lt;/li&gt;
&lt;li&gt;F statistic&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Run regressions
fit_1 = sm.OLS(y, X_poly1).fit()
fit_2 = sm.OLS(y, X_poly2).fit()
fit_3 = sm.OLS(y, X_poly3).fit()
fit_4 = sm.OLS(y, X_poly4).fit()
fit_5 = sm.OLS(y, X_poly5).fit()

# Compare fit
sm.stats.anova_lm(fit_1, fit_2, fit_3, fit_4, fit_5, typ=1)
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;df_resid&lt;/th&gt;
      &lt;th&gt;ssr&lt;/th&gt;
      &lt;th&gt;df_diff&lt;/th&gt;
      &lt;th&gt;ss_diff&lt;/th&gt;
      &lt;th&gt;F&lt;/th&gt;
      &lt;th&gt;Pr(&amp;gt;F)&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;2998.0&lt;/td&gt;
      &lt;td&gt;5.022216e+06&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;2997.0&lt;/td&gt;
      &lt;td&gt;4.793430e+06&lt;/td&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;228786.010128&lt;/td&gt;
      &lt;td&gt;143.593107&lt;/td&gt;
      &lt;td&gt;2.363850e-32&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;2996.0&lt;/td&gt;
      &lt;td&gt;4.777674e+06&lt;/td&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;15755.693664&lt;/td&gt;
      &lt;td&gt;9.888756&lt;/td&gt;
      &lt;td&gt;1.679202e-03&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;2995.0&lt;/td&gt;
      &lt;td&gt;4.771604e+06&lt;/td&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;6070.152124&lt;/td&gt;
      &lt;td&gt;3.809813&lt;/td&gt;
      &lt;td&gt;5.104620e-02&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;2994.0&lt;/td&gt;
      &lt;td&gt;4.770322e+06&lt;/td&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;1282.563017&lt;/td&gt;
      &lt;td&gt;0.804976&lt;/td&gt;
      &lt;td&gt;3.696820e-01&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;The polynomial degree 4 seems best.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Set polynomial X to 4th degree
X_poly = X_poly4
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;binary-dependent-variable&#34;&gt;Binary Dependent Variable&lt;/h3&gt;
&lt;p&gt;Since we have a binary dependent variable, it would be best to account for it in our regression framework. One way to do so, is to run a logistic regression.&lt;/p&gt;
&lt;p&gt;How to interpret a Logistic Regression?&lt;/p&gt;
&lt;p&gt;$$
y = \mathbb I \ \Big( \beta_0 + \beta_1 x_i + \beta_2 x_i^2 + \beta_2 x_i^3 + &amp;hellip; + \varepsilon_i \Big)
$$&lt;/p&gt;
&lt;p&gt;where $\mathbb I(\cdot)$ is an indicator function and now $\varepsilon_i$ is the error term.&lt;/p&gt;
&lt;h3 id=&#34;binomial-link-functions&#34;&gt;Binomial Link Functions&lt;/h3&gt;
&lt;p&gt;Depending on the assumed distribution of the error term, we get different results. I list below the error types supported by the &lt;code&gt;Binomial&lt;/code&gt; family.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# List link functions for the Binomial family
sm.families.family.Binomial.links
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[statsmodels.genmod.families.links.logit,
 statsmodels.genmod.families.links.probit,
 statsmodels.genmod.families.links.cauchy,
 statsmodels.genmod.families.links.log,
 statsmodels.genmod.families.links.cloglog,
 statsmodels.genmod.families.links.identity]
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;logit-link-function&#34;&gt;Logit Link Function&lt;/h3&gt;
&lt;p&gt;We are going to pick the &lt;code&gt;logit&lt;/code&gt; link, i.e. we are going to assume that the error term is Type 1 Extreme Value (or Gumbel) distributed. It instead we take the usual standard normal distribution assumption for $\varepsilon_i$, we get &lt;code&gt;probit&lt;/code&gt; regression.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Pick the logit link for the Binomial family
logit_link = sm.families.Binomial(sm.genmod.families.links.logit())
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Given the error distribution, we can write the probability that $y=1$ as&lt;/p&gt;
&lt;p&gt;$$
\Pr(y=1) = \frac{e^{ \beta_0 + \beta_1 x_i + \beta_2 x_i^2 + \beta_2 x_i^3 + &amp;hellip; + \varepsilon_i }}{1 + e^{ \beta_0 + \beta_1 x_i + \beta_2 x_i^2 + \beta_2 x_i^3 + &amp;hellip; + \varepsilon_i } }
$$&lt;/p&gt;
&lt;h3 id=&#34;logistic-regression&#34;&gt;Logistic Regression&lt;/h3&gt;
&lt;p&gt;We now estimate the regression and plot the estimated relationship between &lt;code&gt;age&lt;/code&gt; and &lt;code&gt;wage&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Run logistic regression
logit_poly = sm.GLM(y01, X_poly, family=logit_link).fit()
logit_poly.summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
    &lt;td&gt;&lt;/td&gt;       &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;z&lt;/th&gt;      &lt;th&gt;P&gt;|z|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;const&lt;/th&gt; &lt;td&gt; -109.5530&lt;/td&gt; &lt;td&gt;   47.655&lt;/td&gt; &lt;td&gt;   -2.299&lt;/td&gt; &lt;td&gt; 0.022&lt;/td&gt; &lt;td&gt; -202.956&lt;/td&gt; &lt;td&gt;  -16.150&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;x1&lt;/th&gt;    &lt;td&gt;    8.9950&lt;/td&gt; &lt;td&gt;    4.187&lt;/td&gt; &lt;td&gt;    2.148&lt;/td&gt; &lt;td&gt; 0.032&lt;/td&gt; &lt;td&gt;    0.789&lt;/td&gt; &lt;td&gt;   17.201&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;x2&lt;/th&gt;    &lt;td&gt;   -0.2816&lt;/td&gt; &lt;td&gt;    0.135&lt;/td&gt; &lt;td&gt;   -2.081&lt;/td&gt; &lt;td&gt; 0.037&lt;/td&gt; &lt;td&gt;   -0.547&lt;/td&gt; &lt;td&gt;   -0.016&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;x3&lt;/th&gt;    &lt;td&gt;    0.0039&lt;/td&gt; &lt;td&gt;    0.002&lt;/td&gt; &lt;td&gt;    2.022&lt;/td&gt; &lt;td&gt; 0.043&lt;/td&gt; &lt;td&gt;    0.000&lt;/td&gt; &lt;td&gt;    0.008&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;x4&lt;/th&gt;    &lt;td&gt;-1.949e-05&lt;/td&gt; &lt;td&gt; 9.91e-06&lt;/td&gt; &lt;td&gt;   -1.966&lt;/td&gt; &lt;td&gt; 0.049&lt;/td&gt; &lt;td&gt;-3.89e-05&lt;/td&gt; &lt;td&gt;-6.41e-08&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;h3 id=&#34;linear-model-comparison&#34;&gt;Linear Model Comparison&lt;/h3&gt;
&lt;p&gt;What is the difference with the linear model?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Run OLS regression with binary outcome
ols_poly = sm.OLS(y01, X_poly).fit()
ols_poly.summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
    &lt;td&gt;&lt;/td&gt;       &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;const&lt;/th&gt; &lt;td&gt;   -0.1126&lt;/td&gt; &lt;td&gt;    0.240&lt;/td&gt; &lt;td&gt;   -0.468&lt;/td&gt; &lt;td&gt; 0.640&lt;/td&gt; &lt;td&gt;   -0.584&lt;/td&gt; &lt;td&gt;    0.359&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;x1&lt;/th&gt;    &lt;td&gt;    0.0086&lt;/td&gt; &lt;td&gt;    0.024&lt;/td&gt; &lt;td&gt;    0.363&lt;/td&gt; &lt;td&gt; 0.717&lt;/td&gt; &lt;td&gt;   -0.038&lt;/td&gt; &lt;td&gt;    0.055&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;x2&lt;/th&gt;    &lt;td&gt;   -0.0002&lt;/td&gt; &lt;td&gt;    0.001&lt;/td&gt; &lt;td&gt;   -0.270&lt;/td&gt; &lt;td&gt; 0.787&lt;/td&gt; &lt;td&gt;   -0.002&lt;/td&gt; &lt;td&gt;    0.001&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;x3&lt;/th&gt;    &lt;td&gt; 3.194e-06&lt;/td&gt; &lt;td&gt; 1.23e-05&lt;/td&gt; &lt;td&gt;    0.260&lt;/td&gt; &lt;td&gt; 0.795&lt;/td&gt; &lt;td&gt;-2.09e-05&lt;/td&gt; &lt;td&gt; 2.73e-05&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;x4&lt;/th&gt;    &lt;td&gt;-1.939e-08&lt;/td&gt; &lt;td&gt; 6.57e-08&lt;/td&gt; &lt;td&gt;   -0.295&lt;/td&gt; &lt;td&gt; 0.768&lt;/td&gt; &lt;td&gt;-1.48e-07&lt;/td&gt; &lt;td&gt; 1.09e-07&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;The magnitude of the coefficients is different, but the signs are the same.&lt;/p&gt;
&lt;h3 id=&#34;plot-data-and-predictions&#34;&gt;Plot data and predictions&lt;/h3&gt;
&lt;p&gt;Let&amp;rsquo;s plot the estimated curves against the data distribution.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Generate predictions
x_grid = np.arange(df.age.min(), df.age.max()).reshape(-1,1)
X_poly_test = PolynomialFeatures(4).fit_transform(x_grid)
y_hat1 = sm.OLS(y, X_poly).fit().predict(X_poly_test)
y01_hat1 = logit_poly.predict(X_poly_test)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plot_predictions(X, y, x_grid, y01, y_hat1, y01_hat1, &#39;Figure 7.1: Degree-4 Polynomial&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/03_nonparametric_44_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Which is remindful of
&lt;img src=&#34;../figures/nonlinearities.jpg&#34; alt=&#34;Le Petit Prince - Elephant figure&#34; title=&#34;Nonlinearities&#34;&gt;&lt;/p&gt;
&lt;p&gt;Using polynomial functions of the features as predictors in a linear model imposes a global structure on the non-linear function of &lt;code&gt;age&lt;/code&gt;. We can instead use step functions in order to avoid imposing such a global structure.&lt;/p&gt;
&lt;p&gt;For example, we could break the range of &lt;code&gt;age&lt;/code&gt; into bins, and fit a different constant in each bin.&lt;/p&gt;
&lt;h2 id=&#34;step-functions&#34;&gt;Step Functions&lt;/h2&gt;
&lt;p&gt;Building a step function means first picking $K$ cutpoints $c_1 , c_2 , . . . , c_K$ in the range of &lt;code&gt;age&lt;/code&gt;,
and then construct $K + 1$ new variables&lt;/p&gt;
&lt;p&gt;$$
C_0(&lt;code&gt;age&lt;/code&gt;) = \mathbb I ( &lt;code&gt;age&lt;/code&gt; &amp;lt; c_1) \
C_1(&lt;code&gt;age&lt;/code&gt;) = \mathbb I ( c_1 &amp;lt; &lt;code&gt;age&lt;/code&gt; &amp;lt; c_2) \
C_2(&lt;code&gt;age&lt;/code&gt;) = \mathbb I ( c_2 &amp;lt; &lt;code&gt;age&lt;/code&gt; &amp;lt; c_3) \
&amp;hellip; \
C_{K-1}(&lt;code&gt;age&lt;/code&gt;) = \mathbb I ( c_{K-1} &amp;lt; &lt;code&gt;age&lt;/code&gt; &amp;lt; c_K) \
C_K(&lt;code&gt;age&lt;/code&gt;) = \mathbb I ( c_K &amp;lt; &lt;code&gt;age&lt;/code&gt;) \
$$&lt;/p&gt;
&lt;p&gt;where $\mathbb I(\cdot)$ is the indicator function.&lt;/p&gt;
&lt;h3 id=&#34;binning&#34;&gt;Binning&lt;/h3&gt;
&lt;p&gt;First, we generate the cuts.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Generate cuts for the variable age
df_cut, bins = pd.cut(df.age, 4, retbins=True, right=True)
df_cut.value_counts(sort=False)
type(df_cut)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;pandas.core.series.Series
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s generate a DataFrame out of this series.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Generate bins for &amp;quot;age&amp;quot; from the cuts
df_steps = pd.concat([df.age, df_cut, df.wage], keys=[&#39;age&#39;,&#39;age_cuts&#39;,&#39;wage&#39;], axis=1)
df_steps.head(5)
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;age&lt;/th&gt;
      &lt;th&gt;age_cuts&lt;/th&gt;
      &lt;th&gt;wage&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;231655&lt;/th&gt;
      &lt;td&gt;18&lt;/td&gt;
      &lt;td&gt;(17.938, 33.5]&lt;/td&gt;
      &lt;td&gt;75.043154&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;86582&lt;/th&gt;
      &lt;td&gt;24&lt;/td&gt;
      &lt;td&gt;(17.938, 33.5]&lt;/td&gt;
      &lt;td&gt;70.476020&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;161300&lt;/th&gt;
      &lt;td&gt;45&lt;/td&gt;
      &lt;td&gt;(33.5, 49.0]&lt;/td&gt;
      &lt;td&gt;130.982177&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;155159&lt;/th&gt;
      &lt;td&gt;43&lt;/td&gt;
      &lt;td&gt;(33.5, 49.0]&lt;/td&gt;
      &lt;td&gt;154.685293&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;11443&lt;/th&gt;
      &lt;td&gt;50&lt;/td&gt;
      &lt;td&gt;(49.0, 64.5]&lt;/td&gt;
      &lt;td&gt;75.043154&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;h3 id=&#34;dummy-variables&#34;&gt;Dummy Variables&lt;/h3&gt;
&lt;p&gt;Now we can generate different dummy variables out of each bin.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Create dummy variables for the age groups
df_steps_dummies = pd.get_dummies(df_steps[&#39;age_cuts&#39;])

# Statsmodels requires explicit adding of a constant (intercept)
df_steps_dummies = sm.add_constant(df_steps_dummies)
df_steps_dummies.head(5)
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;const&lt;/th&gt;
      &lt;th&gt;(17.938, 33.5]&lt;/th&gt;
      &lt;th&gt;(33.5, 49.0]&lt;/th&gt;
      &lt;th&gt;(49.0, 64.5]&lt;/th&gt;
      &lt;th&gt;(64.5, 80.0]&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;231655&lt;/th&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;86582&lt;/th&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;161300&lt;/th&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;155159&lt;/th&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;11443&lt;/th&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;h3 id=&#34;stepwise-regression&#34;&gt;Stepwise Regression&lt;/h3&gt;
&lt;p&gt;We are now ready to run our regression&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Generate our new X variable
X_step = df_steps_dummies.drop(df_steps_dummies.columns[1], axis=1)

# OLS Regression on step functions
ols_step = sm.OLS(y, X_step).fit()
ols_step.summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
        &lt;td&gt;&lt;/td&gt;          &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;const&lt;/th&gt;        &lt;td&gt;   94.1584&lt;/td&gt; &lt;td&gt;    1.476&lt;/td&gt; &lt;td&gt;   63.790&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;   91.264&lt;/td&gt; &lt;td&gt;   97.053&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;(33.5, 49.0]&lt;/th&gt; &lt;td&gt;   24.0535&lt;/td&gt; &lt;td&gt;    1.829&lt;/td&gt; &lt;td&gt;   13.148&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;   20.466&lt;/td&gt; &lt;td&gt;   27.641&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;(49.0, 64.5]&lt;/th&gt; &lt;td&gt;   23.6646&lt;/td&gt; &lt;td&gt;    2.068&lt;/td&gt; &lt;td&gt;   11.443&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;   19.610&lt;/td&gt; &lt;td&gt;   27.719&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;(64.5, 80.0]&lt;/th&gt; &lt;td&gt;    7.6406&lt;/td&gt; &lt;td&gt;    4.987&lt;/td&gt; &lt;td&gt;    1.532&lt;/td&gt; &lt;td&gt; 0.126&lt;/td&gt; &lt;td&gt;   -2.139&lt;/td&gt; &lt;td&gt;   17.420&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;From the regression outcome we can see that most bin coefficients are significant, except for the last one.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Put the test data in the same bins as the training data.
bin_mapping = np.digitize(x_grid.ravel(), bins)
bin_mapping
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2,
       2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
       3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4])
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Get dummies, drop first dummy category, add constant
X_step_test = sm.add_constant(pd.get_dummies(bin_mapping).drop(1, axis=1))
X_step_test.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;const&lt;/th&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;th&gt;4&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Step prediction
y_hat2 = ols_step.predict(X_step_test)
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;logistic-step-regression&#34;&gt;Logistic Step Regression&lt;/h3&gt;
&lt;p&gt;We are going again to run a logistic regression, given that our outcome is binary.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Logistic regression on step functions
logit_step = sm.GLM(y01, X_step, family=logit_link).fit()
y01_hat2 = logit_step.predict(X_step_test)
logit_step.summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
        &lt;td&gt;&lt;/td&gt;          &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;z&lt;/th&gt;      &lt;th&gt;P&gt;|z|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;const&lt;/th&gt;        &lt;td&gt;   -5.0039&lt;/td&gt; &lt;td&gt;    0.449&lt;/td&gt; &lt;td&gt;  -11.152&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;   -5.883&lt;/td&gt; &lt;td&gt;   -4.124&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;(33.5, 49.0]&lt;/th&gt; &lt;td&gt;    1.5998&lt;/td&gt; &lt;td&gt;    0.474&lt;/td&gt; &lt;td&gt;    3.378&lt;/td&gt; &lt;td&gt; 0.001&lt;/td&gt; &lt;td&gt;    0.672&lt;/td&gt; &lt;td&gt;    2.528&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;(49.0, 64.5]&lt;/th&gt; &lt;td&gt;    1.7147&lt;/td&gt; &lt;td&gt;    0.488&lt;/td&gt; &lt;td&gt;    3.512&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    0.758&lt;/td&gt; &lt;td&gt;    2.672&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;(64.5, 80.0]&lt;/th&gt; &lt;td&gt;    0.7413&lt;/td&gt; &lt;td&gt;    1.102&lt;/td&gt; &lt;td&gt;    0.672&lt;/td&gt; &lt;td&gt; 0.501&lt;/td&gt; &lt;td&gt;   -1.420&lt;/td&gt; &lt;td&gt;    2.902&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;h3 id=&#34;plotting&#34;&gt;Plotting&lt;/h3&gt;
&lt;p&gt;How does the predicted function looks like?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plot_predictions(X, y, x_grid, y01, y_hat2, y01_hat2, &#39;Figure 7.2: Piecewise Constant&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/03_nonparametric_69_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;regression-splines&#34;&gt;Regression Splines&lt;/h2&gt;
&lt;p&gt;Spline regression, or piece-wise polynomial regression, involves fitting separate low-degree polynomials
over different regions of $X$. The idea is to have one regression specification but with different coefficients in different parts of the $X$ range. The points where the coefficients change are called knots.&lt;/p&gt;
&lt;p&gt;For example, we could have a third degree polynomial &lt;em&gt;and&lt;/em&gt; splitting the sample in two.&lt;/p&gt;
&lt;p&gt;$$
y_{i}=\left{\begin{array}{ll}
\beta_{01}+\beta_{11} x_{i}+\beta_{21} x_{i}^{2}+\beta_{31} x_{i}^{3}+\epsilon_{i} &amp;amp; \text { if } x_{i}&amp;lt;c \
\beta_{02}+\beta_{12} x_{i}+\beta_{22} x_{i}^{2}+\beta_{32} x_{i}^{3}+\epsilon_{i} &amp;amp; \text { if } x_{i} \geq c
\end{array}\right.
$$&lt;/p&gt;
&lt;p&gt;We have now two sets of coefficients, one for each subsample.&lt;/p&gt;
&lt;p&gt;Generally, using more knots leads to a more flexible piecewise polynomial. Also increasing the degree of the polynomial increases the degree of flexibility.&lt;/p&gt;
&lt;h3 id=&#34;example&#34;&gt;Example&lt;/h3&gt;
&lt;p&gt;We are now going to plot 4 different examples for the &lt;code&gt;age&lt;/code&gt; &lt;code&gt;wage&lt;/code&gt; relationship:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Discontinuous piecewise cubic&lt;/li&gt;
&lt;li&gt;Continuous piecewise cubic&lt;/li&gt;
&lt;li&gt;Quadratic (continuous)&lt;/li&gt;
&lt;li&gt;Continuous piecewise linear&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Cut dataset
df_short = df.iloc[:80,:]
X_short = df_short.age
y_short = df_short.wage
x_grid_short = np.arange(df_short.age.min(), df_short.age.max()+1).reshape(-1,1)

# 1. Discontinuous piecewise cubic
spline1 = &amp;quot;bs(x, knots=(50,50,50,50), degree=3, include_intercept=False)&amp;quot;

# 2. Continuous piecewise cubic
spline2 = &amp;quot;bs(x, knots=(50,50,50), degree=3, include_intercept=False)&amp;quot;

# 3. Quadratic (continuous)
spline3 = &amp;quot;bs(x, knots=(%s,%s), degree=2, include_intercept=False)&amp;quot; % (min(df.age), min(df.age))

# 4. Continuous piecewise linear
spline4 = &amp;quot;bs(x, knots=(%s,50), degree=1, include_intercept=False)&amp;quot; % min(df.age)
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;generate-predictions&#34;&gt;Generate Predictions&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Generate spline predictions
def fit_predict_spline(spline, X, y, x_grid):
    transformed_x = dmatrix(spline, {&amp;quot;x&amp;quot;: X}, return_type=&#39;dataframe&#39;)
    fit = sm.GLM(y, transformed_x).fit()
    y_hat = fit.predict(dmatrix(spline, {&amp;quot;x&amp;quot;: x_grid}, return_type=&#39;dataframe&#39;))
    return y_hat

y_hats = [fit_predict_spline(s, X_short, y_short, x_grid_short) for s in [spline1, spline2, spline3, spline4]]
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;plotting-1&#34;&gt;Plotting&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plot_splines(df_short, x_grid_short, y_hats)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/03_nonparametric_78_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;comment&#34;&gt;Comment&lt;/h3&gt;
&lt;p&gt;The first example makes us think on why would we want out function to be discontinuous. Unless we expect a sudden wage jump at a certain age, we would like the function to be continuous. However, if for example we split &lt;code&gt;age&lt;/code&gt; around the retirement age, we might expect a discontinuity.&lt;/p&gt;
&lt;p&gt;The second example (top right) makes us think on why would we want out function not to be differentiable. Unless we have some specific mechanism in mind, ususally there is a trade-off between making the function non-differentiable or increasing the degree of the polynomial, as the last two examples show us. We get a similar fit with a quadratic fit or a discontinuous linear fit. The main difference is that in the second case we are picking the discontinuity point by hand instead of letting the data choose how to change the slope of the curve.&lt;/p&gt;
&lt;h3 id=&#34;the-spline-basis-representation&#34;&gt;The Spline Basis Representation&lt;/h3&gt;
&lt;p&gt;How can we fit a piecewise degree-d polynomial under the constraint that it (and possibly its first d − 1 derivatives) be continuous?&lt;/p&gt;
&lt;p&gt;The most direct way to represent a cubic spline is to start off with a basis for a cubic polynomial—namely, x,x2,x3—and then add one truncated power basis function per knot. A truncated power basis function is defined as&lt;/p&gt;
&lt;p&gt;$$
h(x, c)=(x-c)_{+}^{3} = \Bigg{\begin{array}{cl}
(x-c)^{3} &amp;amp; \text { if } x&amp;gt;c \
0 &amp;amp; \text { otherwise }
\end{array}
$$&lt;/p&gt;
&lt;p&gt;One can show that adding a term of the form $\beta_4 h(x, c)$ to the model for a cubic polynomial will lead to a discontinuity in only the third derivative at $c$; the function will remain continuous, with continuous first and second derivatives, at each of the knots.&lt;/p&gt;
&lt;h3 id=&#34;cubic-splines&#34;&gt;Cubic Splines&lt;/h3&gt;
&lt;p&gt;One way to specify the spline is using nodes and degrees of freedom.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Specifying 3 knots and 3 degrees of freedom
spline5 = &amp;quot;bs(x, knots=(25,40,60), degree=3, include_intercept=False)&amp;quot;
pred5 = fit_predict_spline(spline5, X, y, x_grid)
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;no-knots&#34;&gt;No Knots&lt;/h3&gt;
&lt;p&gt;When we fit a spline, where should we place the knots?&lt;/p&gt;
&lt;p&gt;The regression spline is most flexible in regions that contain a lot of knots, because in those regions the polynomial coefficients can change rapidly. Hence, one option is to place more knots in places where we feel the function might vary most rapidly, and to place fewer knots where it seems more stable. While this option can work well, in practice it is common to place knots in a uniform fashion. One way to do this is to specify the desired degrees of freedom, and then have the software automatically place the corresponding number of knots at uniform quantiles of the data.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Specifying degree 3 and 6 degrees of freedom 
spline6 = &amp;quot;bs(x, df=6, degree=3, include_intercept=False)&amp;quot;
pred6 = fit_predict_spline(spline6, X, y, x_grid)
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;natural-splines&#34;&gt;Natural Splines&lt;/h3&gt;
&lt;p&gt;A natural spline is a regression spline with additional boundary constraints: the function is required to be linear at the boundary (in the region where X is smaller than the smallest knot, or larger than the largest knot). This addi- tional constraint means that natural splines generally produce more stable estimates at the boundaries.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Natural spline with 4 degrees of freedom
spline7 = &amp;quot;cr(x, df=4)&amp;quot;
pred7 = fit_predict_spline(spline7, X, y, x_grid)
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;comparison&#34;&gt;Comparison&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Compare predictons
preds = [pred5, pred6, pred7]
labels = [&#39;degree 3, knots 3&#39;, &#39;degree 3, degrees of freedom 3&#39;, &#39;natural, degrees of freedom 4&#39;]
compare_predictions(X, y, x_grid, preds, labels)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/03_nonparametric_93_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;comparison-to-polynomial-regression&#34;&gt;Comparison to Polynomial Regression&lt;/h3&gt;
&lt;p&gt;Regression splines often give superior results to polynomial regression. This is because unlike polynomials, which must use a high degree to produce flexible fits, splines introduce flexibility by increasing the number of knots but keeping the degree fixed.&lt;/p&gt;
&lt;p&gt;We are now fitting a polynomial of degree 15 and a spline with 15 degrees of freedom.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Polynomial of degree 15
X_poly15 = PolynomialFeatures(15).fit_transform(df.age.values.reshape(-1,1))
ols_poly_15 = sm.OLS(y, X_poly15).fit()
pred8 = ols_poly_15.predict(PolynomialFeatures(15).fit_transform(x_grid))

# Spline with 15 degrees of freedon
spline9 = &amp;quot;bs(x, df=15, degree=3, include_intercept=False)&amp;quot;
pred9 = fit_predict_spline(spline9, X, y, x_grid)
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;plotting-2&#34;&gt;Plotting&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Compare predictons
preds = [pred8, pred9]
labels = [&#39;Polynomial&#39;, &#39;Spline&#39;]
compare_predictions(X, y, x_grid, preds, labels)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/03_nonparametric_98_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;As we can see, despite the two regressions having the same degrees of freedom, the polynomial fit is much more volatile. We can compare them along some dimensions.&lt;/p&gt;
&lt;h2 id=&#34;local-regression&#34;&gt;Local Regression&lt;/h2&gt;
&lt;p&gt;So far we have looked at so-called &amp;ldquo;&lt;em&gt;global methods&lt;/em&gt;&amp;rdquo;: methods that try to fit a unique function specification over the whole data. The function specification can be complex, as in the case of splines, but can be expressed globally.&lt;/p&gt;
&lt;p&gt;Local regression is a different approach for fitting flexible non-linear functions, which involves computing the fit at a target point $x_0$ using only the nearby training observations.&lt;/p&gt;
&lt;h3 id=&#34;details&#34;&gt;Details&lt;/h3&gt;
&lt;p&gt;How does local regression work?&lt;/p&gt;
&lt;p&gt;Ingredients: $X$, $y$.&lt;/p&gt;
&lt;p&gt;How to you output a prediction $\hat y_i$ at a new point $x_i$?&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Take a number of points in $X$ close to $x_i$: $X_{\text{close-to-i}}$&lt;/li&gt;
&lt;li&gt;Assign a weight to each of there points&lt;/li&gt;
&lt;li&gt;Fit a weigthed least squares regression of $X_{\text{close-to-i}}$ on $y_{\text{close-to-i}}$&lt;/li&gt;
&lt;li&gt;Use the estimated coefficients $\hat \beta$ to predict $\hat y_i = \hat \beta_0 + \hat \beta_1 x_i$&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;generate-data&#34;&gt;Generate Data&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Set seed
np.random.seed(1)

# Generate data
X_sim = np.sort(np.random.uniform(0,1,100))
e = np.random.uniform(-.5,.5,100)
y_sim = -4*X_sim**2 + 3*X_sim + e

# True Generating process without noise
X_grid = np.linspace(0,1,100)
y_grid = -4*X_grid**2 + 3*X_grid
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;plotting-3&#34;&gt;Plotting&lt;/h3&gt;
&lt;p&gt;Let&amp;rsquo;s visualize the simulated data and the curve without noise.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plot_simulated_data(X_sim, y_sim, X_grid, y_grid);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/03_nonparametric_108_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;fit-ll-regression&#34;&gt;Fit LL Regression&lt;/h3&gt;
&lt;p&gt;Now we fit a local linear regression.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Settings
spec = &#39;ll&#39;
bandwidth = 0.1
kernel = &#39;gaussian&#39;

# Locally linear regression
local_reg = KernelReg(y_sim, X_sim.reshape(-1,1), 
                      var_type=&#39;c&#39;, 
                      reg_type=spec, 
                      bw=[bandwidth])
y_hat = KernelReg.fit(local_reg)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What do the parameters mean?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;var_type&lt;/code&gt;: dependent variable type (&lt;code&gt;c&lt;/code&gt; i.e. &lt;em&gt;continuous&lt;/em&gt;)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;reg_type&lt;/code&gt;: local regression specification (&lt;code&gt;ll&lt;/code&gt; i.e. &lt;em&gt;locally linear&lt;/em&gt;)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;bw&lt;/code&gt;      : bandwidth length (&lt;em&gt;0.1&lt;/em&gt;)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;ckertype&lt;/code&gt;: kernel type (&lt;em&gt;gaussian&lt;/em&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;prediction&#34;&gt;Prediction&lt;/h3&gt;
&lt;p&gt;What does the prediction looks like?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;fig, ax = plot_simulated_data(X_sim, y_sim, X_grid, y_grid);
make_figure_7_9a(fig, ax, X_sim, y_hat);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/03_nonparametric_115_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;details-1&#34;&gt;Details&lt;/h3&gt;
&lt;p&gt;How exactly was the prediction generated? It was generated pointwise. We are now going to look at the prediction at one particular point: $x_i=0.5$.&lt;/p&gt;
&lt;p&gt;We proceed as follows:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;We select the focal point: $x_i=0.5$&lt;/li&gt;
&lt;li&gt;We select observations close to $\ x_i$, i.e. $x_{\text{close to i}} = { x \in X : |x_i - x| &amp;lt; 0.1 } \ $ and $ \ y_{\text{close to i}} = { y \in Y : |x_i - x| &amp;lt; 0.1 }$&lt;/li&gt;
&lt;li&gt;We apply gaussian weights&lt;/li&gt;
&lt;li&gt;We run a weighted linear regression of $y_{\text{close to i}}$ on $x_{\text{close to i}}$&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Get local X and y
x_i = 0.5
close_to_i = (x_i-bandwidth &amp;lt; X_sim) &amp;amp; (X_sim &amp;lt; x_i+bandwidth)
X_tilde = X_sim[close_to_i]
y_tilde = y_sim[close_to_i]

# Get local estimates
local_estimate = KernelReg.fit(local_reg, data_predict=[x_i])
y_i_hat = local_estimate[0]
beta_i_hat = local_estimate[1]
alpha_i_hat = y_i_hat - beta_i_hat*x_i
print(&#39;Estimates: alpha=%1.4f, beta=%1.4f&#39; % (alpha_i_hat, beta_i_hat))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Estimates: alpha=0.7006, beta=-0.6141
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;visualization&#34;&gt;Visualization&lt;/h3&gt;
&lt;p&gt;Now we can use the locally estimated coefficients to predict the value of $\hat y_i(x_i)$ for $x_i = 0.5$.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Build local predictions
close_to_i_grid = (x_i-bandwidth &amp;lt; X_grid) &amp;amp; (X_grid &amp;lt; x_i+bandwidth)
X_grid_tilde = X_grid[close_to_i_grid].reshape(-1,1)
y_grid_tilde = alpha_i_hat + X_grid_tilde*beta_i_hat
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;fig, ax = plot_simulated_data(X_sim, y_sim, X_grid, y_grid);
make_figure_7_9a(fig, ax, X_sim, y_hat);
make_figure_7_9b(fig, ax, X_tilde, y_tilde, X_grid_tilde, y_grid_tilde, x_i, y_i_hat)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/03_nonparametric_122_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;zooming-in&#34;&gt;Zooming in&lt;/h3&gt;
&lt;p&gt;We can zoom in and look only at the &amp;ldquo;&lt;em&gt;close to i&lt;/em&gt;&amp;rdquo; sample.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sns.regplot(X_tilde, y_tilde);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/03_nonparametric_125_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Why is the line upward sloped? We forgot the gaussian weights.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Weights
w = norm.pdf((X_sim-x_i)/bandwidth)

# Estimate LWS
mod_wls = sm.WLS(y_sim, sm.add_constant(X_sim), weights=w)
results = mod_wls.fit()

print(&#39;Estimates: alpha=%1.4f, beta=%1.4f&#39; % tuple(results.params))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Estimates: alpha=0.7006, beta=-0.6141
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We indeed got the same estimates as before. Note two things:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;the badwidth defines the scale parameter of the gaussian weights&lt;/li&gt;
&lt;li&gt;our locally linear regression is acqually global&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;plotting-4&#34;&gt;Plotting&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;make_figure_7_9d(X_sim, y_sim, w, results, X_grid, x_i, y_i_hat)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/03_nonparametric_130_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Now the slope is indeed negative, as in the locally linear regression.&lt;/p&gt;
&lt;h2 id=&#34;generalized-additive-models&#34;&gt;Generalized Additive Models&lt;/h2&gt;
&lt;p&gt;Generalized additive models (GAMs) provide a general framework for extending a standard linear model by allowing non-linear functions of each of the variables, while maintaining additivity.&lt;/p&gt;
&lt;h3 id=&#34;gam-for-regression-problems&#34;&gt;GAM for Regression Problems&lt;/h3&gt;
&lt;p&gt;Imagine to extend the general regression framework to some separabily additive model of the form&lt;/p&gt;
&lt;p&gt;$$
y_i = \beta_0 + \sum_{k=1}^K \beta_k f_k(x_{ik}) + \varepsilon_i
$$&lt;/p&gt;
&lt;p&gt;It is called an additive model because we calculate a separate $f_k$ for each $X_k$, and then add together all of their contributions.&lt;/p&gt;
&lt;p&gt;Consider for example the following model&lt;/p&gt;
&lt;p&gt;$$
\text{wage} = \beta_0 + f_1(\text{year}) + f_2(\text{age}) + f_3(\text{education}) + \varepsilon
$$&lt;/p&gt;
&lt;h3 id=&#34;example-1&#34;&gt;Example&lt;/h3&gt;
&lt;p&gt;We are going to use the following functions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$f_1$: natural spline with 8 degrees of freedom&lt;/li&gt;
&lt;li&gt;$f_2$: natural spline with 10 degrees of freedom&lt;/li&gt;
&lt;li&gt;$f_3$: step function&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Set X and y
df[&#39;education_&#39;] = LabelEncoder().fit_transform(df[&amp;quot;education&amp;quot;])
X = df[[&#39;year&#39;,&#39;age&#39;,&#39;education_&#39;]].to_numpy()
y = df[[&#39;wage&#39;]].to_numpy()

## model
linear_gam = LinearGAM(s(0, n_splines=8) + s(1, n_splines=10) + f(2))
linear_gam.gridsearch(X, y);
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;100% (11 of 11) |########################| Elapsed Time: 0:00:00 Time:  0:00:00
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;plotting-5&#34;&gt;Plotting&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plot_gam(linear_gam)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/03_nonparametric_140_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;pros-and-cons&#34;&gt;Pros and Cons&lt;/h3&gt;
&lt;p&gt;Before we move on, let us summarize the &lt;strong&gt;advantages&lt;/strong&gt; of a GAM.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;GAMs allow us to fit a non-linear $f_k$ to each $X_k$, so that we can automatically model non-linear relationships that standard linear regression will miss&lt;/li&gt;
&lt;li&gt;The non-linear fits can potentially make more accurate predictions&lt;/li&gt;
&lt;li&gt;Because the model is additive, we can still examine the effect of each $X_k$ on $Y$ separately&lt;/li&gt;
&lt;li&gt;The smoothness of the function $f_k$ for the variable $X_k$ can be summarized via degrees of freedom.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The main &lt;strong&gt;limitation&lt;/strong&gt; of GAMs is that the model is restricted to be additive. With many variables, important interactions can be missed. However, as with linear regression, we can manually add interaction terms to the GAM model by including additional predictors of the form $X_j \times X_k$.&lt;/p&gt;
&lt;h3 id=&#34;gams-for-classification-problems&#34;&gt;GAMs for Classification Problems&lt;/h3&gt;
&lt;p&gt;We can use GAMs also with a binary dependent variable.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Binary dependent variable
y_binary = (y&amp;gt;250)

## Logit link function
logit_gam = LogisticGAM(s(0, n_splines=8) + s(1, n_splines=10) + f(2), fit_intercept=True)
logit_gam.gridsearch(X, y_binary);
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;100% (11 of 11) |########################| Elapsed Time: 0:00:00 Time:  0:00:00
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;plotting-6&#34;&gt;Plotting&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plot_gam(logit_gam)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/03_nonparametric_147_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;The results are qualitatively similar to the non-binary case.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Asymptotic Theory</title>
      <link>https://matteocourthoud.github.io/course/metrics/03_asymptotics/</link>
      <pubDate>Fri, 29 Oct 2021 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/course/metrics/03_asymptotics/</guid>
      <description>&lt;h2 id=&#34;convergence&#34;&gt;Convergence&lt;/h2&gt;
&lt;h3 id=&#34;sequences&#34;&gt;Sequences&lt;/h3&gt;
&lt;p&gt;A sequence of nonrandom numbers $\lbrace a_n \rbrace$ &lt;strong&gt;converges&lt;/strong&gt; to
$a$ (has limit $a$) if for all $\varepsilon&amp;gt;0$, there exists
$n _ \varepsilon$ such that if $n &amp;gt; n_ \varepsilon$, then
$|a_n - a| &amp;lt; \varepsilon$. We write $a_n \to a$ as $n \to \infty$.&lt;/p&gt;
&lt;p&gt;A sequence of nonrandom numbers $\lbrace a_n \rbrace$ is &lt;strong&gt;bounded&lt;/strong&gt; if
and only if there is some $B &amp;lt; \infty$ such that $|a_n| \leq B$ for all
$n=1,2,&amp;hellip;$ Otherwise, we say that $\lbrace a_n \rbrace$ is unbounded.&lt;/p&gt;
&lt;h3 id=&#34;big-o-and-small-o-notation&#34;&gt;Big-O and Small-o Notation&lt;/h3&gt;
&lt;p&gt;A sequence of nonrandom numbers $\lbrace a_n \rbrace$ is $O(N^\delta)$
(at most of order $N^\delta$) if $N^{-\delta} a_n$ is bounded. When
$\delta=0$, $a_n$ is bounded, and we also write $a_n = O(1)$ (big oh
one).&lt;/p&gt;
&lt;p&gt;A sequence of nonrandom numbers $\lbrace a_n \rbrace$ is $o(N^\delta)$
if $N^{-\delta} a_n \to 0$. When $\delta=0$, $a_n$ converges to zero,
and we also write $a_n = o(1)$ (little oh one).&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Properties&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;if $a_n = o(N^{\delta})$, then $a_n = O(N^\delta)$&lt;/li&gt;
&lt;li&gt;if $a_n = o(1)$, then $a_n = O(1)$&lt;/li&gt;
&lt;li&gt;if each element of a sequence of vectors or matrices is
$O(N^\delta)$, we say the sequence of vectors or matrices is
$O(N^\delta)$&lt;/li&gt;
&lt;li&gt;similarly for $o(N^\delta)$.&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;convergence-in-probability&#34;&gt;Convergence in Probability&lt;/h3&gt;
&lt;p&gt;A sequence of random variables $\lbrace X_n \rbrace$ &lt;strong&gt;converges in
probability&lt;/strong&gt; to a constant $c \in \mathbb R$ if for all $\varepsilon&amp;gt;0$
$$
\Pr \big( |X_n - c| &amp;gt; \varepsilon \big) \to 0 \qquad \text{ as } n \to \infty
$$ We write $X_n \overset{p}{\to} c$ and say that $a$ is the probability
limit (&lt;em&gt;plim&lt;/em&gt;) of $X_n$: $\mathrm{plim} X_n = c$. In the special case
where $c=0$, we also say that $\lbrace X_n \rbrace$ is $o_p(1)$ (little
oh p one). We also write $X_n = o_p(1)$ or $X_n \overset{p}{\to} 0$.&lt;/p&gt;
&lt;p&gt;A sequence of random variables $\lbrace X_n \rbrace$ is bounded in
probability if for every $\varepsilon&amp;gt;0$, there exists a
$B _ \varepsilon &amp;lt; \infty$ and an integer $n_ \varepsilon$ such that $$
\Pr \big( |x_ n| &amp;gt; B_ \varepsilon \big) &amp;lt; \varepsilon \qquad \text{ for all } n &amp;gt; n_ \varepsilon
$$ We write $X_n = O_p(1)$ ($\lbrace X_n \rbrace$ is big oh p one).&lt;/p&gt;
&lt;p&gt;A sequence of random variables $\lbrace X_n \rbrace$ is $o_p(a_n)$ where
$\lbrace a_n \rbrace$ is a nonrandom positive sequence, if
$X_n/a_n = o_p(1)$. We write $X_n = o_p(a_n)$.&lt;/p&gt;
&lt;p&gt;A sequence of random variables $\lbrace X_n \rbrace$ is $O_p(a_n)$ where
$\lbrace a_n \rbrace$ is a nonrandom positive sequence, if
$X_n/a_n = O_p(1)$. We write $X_n = O_p(a_n)$.&lt;/p&gt;
&lt;h3 id=&#34;other-convergences&#34;&gt;Other Convergences&lt;/h3&gt;
&lt;p&gt;A sequence of random variables $\lbrace X_n \rbrace$ &lt;strong&gt;converges almost
surely&lt;/strong&gt; to a constant $c \in \mathbb R$ if $$
\Pr \big( X_n \overset{p}{\to} c \big) = 1
$$ We write $X_n \overset{as}{\to} c$.&lt;/p&gt;
&lt;p&gt;A sequence of random variables $\lbrace X_n \rbrace$ &lt;strong&gt;converges in mean
square&lt;/strong&gt; to a constant $c \in \mathbb R$ if $$
\mathbb E [(X_n - c)^2] \to 0  \qquad \text{ as } n \to \infty
$$ We write $X_n \overset{ms}{\to} c$.&lt;/p&gt;
&lt;p&gt;Let $\lbrace X_n \rbrace$ be a sequence of random variables and $F_n$ be
the cumulative distribution function (cdf) of $X_n$. We say that $X_n$
&lt;strong&gt;converges in distribution&lt;/strong&gt; to a random variable $x$ with cdf $F$ if
the cdf $F_n$ of $X_n$ converges to the cdf $F$ of $x$ &lt;em&gt;at every
continuity point&lt;/em&gt; of $F$. We write $X_n \overset{d}{\to} x$ and we call
$F$ the &lt;strong&gt;asymptotic distribution&lt;/strong&gt; of $X_n$.&lt;/p&gt;
&lt;h3 id=&#34;compare-convergences&#34;&gt;Compare Convergences&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Lemma&lt;/strong&gt;: Let $\lbrace X_n \rbrace$ be a sequence of random variables
and $c \in \mathbb R$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$X_n \overset{ms}{\to} c \ \Rightarrow \ X_n \overset{p}{\to} c$&lt;/li&gt;
&lt;li&gt;$X_n \overset{as}{\to} c \ \Rightarrow \ X_n \overset{p}{\to} c$&lt;/li&gt;
&lt;li&gt;$X_n \overset{p}{\to} c \ \Rightarrow \ X_n \overset{d}{\to} c$&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;Note that all the above definitions naturally extend to a sequence of
random vectors by requiring element-by-element convergence. For
example, a sequence of $K \times 1$ random vectors
$\lbrace X_n \rbrace$ &lt;strong&gt;converges in probability&lt;/strong&gt; to a constant
$c \in \mathbb R^K$ if for all $\varepsilon&amp;gt;0$ $$
\Pr \big( |X _ {nk} - c_k| &amp;gt; \varepsilon \big) \to 0 \qquad \text{ as } n \to \infty \quad \forall k = 1&amp;hellip;K
$$&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;theorems&#34;&gt;Theorems&lt;/h2&gt;
&lt;h3 id=&#34;slutsky-theorem&#34;&gt;Slutsky Theorem&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Let $\lbrace X_n \rbrace$ and $\lbrace Y_n \rbrace$ be two sequences of
random variables, $x$ a random variable and $c \in \mathbb R$ a constant
such that $\lbrace X_n \rbrace \overset{d}{\to} X$ and
$\lbrace Y_n \rbrace \overset{p}{\to} c$. Then&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$X_n + Y_n \overset{d}{\to} X + c$&lt;/li&gt;
&lt;li&gt;$X_n \cdot Y_n \overset{d}{\to} X \cdot c$&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;continuous-mapping-theorem&#34;&gt;Continuous Mapping Theorem&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Let $\lbrace X_n \rbrace$ be sequence of $K \times 1$ random vectors and
$g: \mathbb{R}^K \to \mathbb{R}^J$ a continuous function that does not
depend on $n$.Then&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$x _n \overset{as}{\to} x \ \Rightarrow \ g(X_n) \overset{as}{\to} g(x)$&lt;/li&gt;
&lt;li&gt;$x _n \overset{p}{\to} x \ \Rightarrow \ g(X_n) \overset{p}{\to} g(x)$&lt;/li&gt;
&lt;li&gt;$x _n \overset{d}{\to} x \ \Rightarrow \ g(X_n) \overset{d}{\to} g(x)$&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;weak-law-of-large-numbers&#34;&gt;Weak Law of Large Numbers&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Let $\lbrace x_i \rbrace _ {i=1}^n$ be a sequence of independent,
identically distributed random variables such that
$\mathbb{E}[|x_i|] &amp;lt; \infty$. Then the sequence satisfies the &lt;strong&gt;weak law
of large numbers (WLLN)&lt;/strong&gt;: $$
\mathbb{E}_n[x_i] = \frac{1}{n} \sum _ {i=1}^n x_i \overset{p}{\to} \mu \qquad \text{ where } \mu \equiv \mathbb{E}[x_i]
$$&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Intuitions&lt;/strong&gt; for the law of large numbers:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Cancellation with high probability.&lt;/li&gt;
&lt;li&gt;Re-visiting regions of the sample space over and over again.&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;wlln-proof&#34;&gt;WLLN Proof&lt;/h3&gt;
&lt;p&gt;The independence of the random variables implies no correlation between
them, and we have that $$
Var \left( \mathbb{E}_n[x_i] \right) = Var \left( \frac{1}{n} \sum _ {i=1}^n x_i \right) = \frac{1}{n^2} Var\left( \sum _ {i=1}^n x_i \right) = \frac{n \sigma^2}{n^2} = \frac{\sigma^2}{n}
$$ Using Chebyshev’s inequality on $\mathbb{E}_n[x_i]$ results in $$
\Pr \big( \left|\mathbb{E}_n[x_i]-\mu \right| &amp;gt; \varepsilon \big) \leq {\frac {\sigma ^{2}}{n\varepsilon ^{2}}}
$$ As $n$ approaches infinity, the right hand side approaches $0$. And
by definition of convergence in probability, we have obtained
$\mathbb{E}_n[x_i] \overset{p}{\to} \mu$ as $n \to \infty$.
$$\tag*{$\blacksquare$}$$&lt;/p&gt;
&lt;h3 id=&#34;central-limit-theorem&#34;&gt;Central Limit Theorem&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Lindberg-Levy Central Limit Theorem&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Let $\lbrace x_i \rbrace _ {i=1}^n$ be a sequence of independent,
identically distributed random variables such that
$\mathbb{E}[x_i^2] &amp;lt; \infty$, and $\mathbb{E}[x_i] = \mu$. Then
$\lbrace x_i \rbrace$ satisfies the &lt;strong&gt;central limit theorem (CLT)&lt;/strong&gt;;
that is, $$
\frac{1}{\sqrt{n}} \sum _ {i=1}^{n} (x_i - \mu) \overset{d}{\to} N(0,\sigma^2)
$$ where $\sigma^2 = Var(x_i) = \mathbb{E}[x_i x_i&amp;rsquo;]$ is necessarily
positive semidefinite.&lt;/p&gt;
&lt;h3 id=&#34;clt-proof-1&#34;&gt;CLT Proof (1)&lt;/h3&gt;
&lt;p&gt;Suppose $\lbrace x_i \rbrace$ are independent and identically
distributed random variables, each with mean $\mu$ and finite variance
$\sigma^2$. The sum $x_1 + &amp;hellip; + X_n$ has mean $n \mu$ and variance
$n \sigma^2$.&lt;/p&gt;
&lt;p&gt;Consider the random variable $$
Z_n = \frac{x_1 + &amp;hellip; + X_n - n\mu}{\sqrt{n \sigma^2}} = \sum _ {i=1}^n \frac{x_i - \mu}{\sqrt{n \sigma^2}} = \sum _ {i=1}^n \frac{1}{\sqrt{n}} \tilde x_i
$$&lt;/p&gt;
&lt;p&gt;where in the last step we defined the new random variables
$\tilde x_i = \frac{x_i - \mu}{\sigma}$ each with zero mean and unit
variance. The characteristic function of $Z_n$ is given by $$
\varphi _ {Z_n} (t) = \varphi _ { \sum _ {i=1}^n \frac{1}{\sqrt{n} } \tilde{x}_i}(t) = \varphi _ {\tilde x_1} \left( \frac{t}{\sqrt{n}} \right) \times &amp;hellip; \times \varphi _ {Y_n} \left( \frac{t}{\sqrt{n}} \right) = \left[ \varphi _ {\tilde x_1} \left( \frac{t}{\sqrt{n}} \right) \right]^n
$$&lt;/p&gt;
&lt;p&gt;where in the last step we used the fact that all of the $\tilde{x}_i$
are identically distributed.&lt;/p&gt;
&lt;h3 id=&#34;clt-proof-2&#34;&gt;CLT Proof (2)&lt;/h3&gt;
&lt;p&gt;The characteristic function of $\tilde{x}_1$ is, by Taylor’s theorem, $$
\varphi _ {\tilde{x}_1} \left( \frac{t}{\sqrt{n}} \right) = 1 - \frac{t^2}{2n} + o \left( \frac{t^2}{n} \right) \qquad \text{ for } n \to \infty
$$&lt;/p&gt;
&lt;p&gt;where $o(t^2)$ is “little o notation” for some function of $t$ that goes
to zero more rapidly than $t^2$. By the limit of the exponential
function, the characteristic function of $Z_n$ equals $$
\varphi _ {Z_ n}(t) = \left[  1 - \frac{t^2}{2n} + o \left( \frac{t^2}{n} \right) \right]^n \to e^{ -\frac{1}{2}t^2 } \qquad \text{ for } n \to \infty
$$&lt;/p&gt;
&lt;p&gt;Note that all of the higher order terms vanish in the limit
$n \to \infty$. The right hand side equals the characteristic function
of a standard normal distribution $N(0,1)$, which implies through Lévy’s
continuity theorem that the distribution of $Z_ n$ will approach
$N(0,1)$ as $n \to \infty$. Therefore, the sum $x_1 + &amp;hellip; + x_n$ will
approach that of the normal distribution $N(n_{\mu}, n\sigma^2)$, and
the sample average $$
\mathbb{E}_n [x_i] = \frac{1}{n} \sum _ {i=1}^n x_i
$$&lt;/p&gt;
&lt;p&gt;converges to the normal distribution $N(\mu, \sigma^2)$, from which the
central limit theorem follows. $$\tag*{$\blacksquare$}$$&lt;/p&gt;
&lt;h3 id=&#34;delta-method&#34;&gt;Delta Method&lt;/h3&gt;
&lt;p&gt;Let $\lbrace X_n \rbrace$ be a sequence of independent, identically
distributed $K \times 1$ random vectors such that&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\sqrt{n} (X_n - c) \overset{d}{\to} Z$ for some fixed
$c \in \mathbb{R}^K$&lt;/li&gt;
&lt;li&gt;and $\Sigma$ a $K \times K$ positive definite matrix.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Suppose $g : \mathbb{R}^K \to \mathbb{R}^J$ with $J \leq K$ is
continuously differentiable and full rank at $c$, then $$
\sqrt{n} \Big[ g(X_n) - g( c ) \Big] \overset{d}{\to} G Z
$$&lt;/p&gt;
&lt;p&gt;where $G = \frac{\partial g( c )}{\partial x}$ is the $J \times K$
matrix of partial derivatives evaluated at $c$.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Note that the most common utilization is with the random variable
$\mathbb E_n [x_i]$. In fact, under the assumptions of the CLT, we
have that $$
\sqrt{n} \Big[ g \big( \mathbb E_n [x_i] \big) - g(\mu) \Big] \overset{d}{\to} N(0, G \Sigma G&amp;rsquo;)
$$&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;ergodic-theory&#34;&gt;Ergodic Theory&lt;/h2&gt;
&lt;h3 id=&#34;ppt&#34;&gt;PPT&lt;/h3&gt;
&lt;p&gt;Let $(\Omega, \mathcal{B}, P)$ be a probability space and
$T: \Omega \rightarrow \Omega$ a measurable map. $T$ is a &lt;strong&gt;probability
preserving transformation&lt;/strong&gt; if the probability of the pre-image of every
set is the same as the probability of the set itself,
i.e. $\forall G, \Pr(T^{-1}(G)) = \Pr(G)$.&lt;/p&gt;
&lt;p&gt;Let $(\Omega, \mathcal{B}, P)$ be a probability space and
$T: \Omega \rightarrow \Omega$ a PPT. A set $G \in \mathcal{B}$ is
&lt;strong&gt;invariant&lt;/strong&gt; if $T^{-1}(G)=G$.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Note that it does not have to work the other way around:
$G \neq T(G)$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Let $(\Omega, \mathcal{B}, P)$ be a probability space and
$T: \Omega \rightarrow \Omega$ a PPT. $T$ is &lt;strong&gt;ergodic&lt;/strong&gt; if every
invariant set $G \in \mathcal{B}$ has probability zero or one,
i.e. $\Pr(G) = 0 \lor \Pr(G) = 1$.&lt;/p&gt;
&lt;h3 id=&#34;poincarè-recurrence&#34;&gt;Poincarè Recurrence&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Let $(\Omega, \mathcal{B}, P)$ be a probability space and
$T: \Omega \rightarrow \Omega$ a PPT. Suppose $A \in \mathcal{B}$ is
measurable. Then, for almost every $\omega \in A$, $T^n(\omega)\in A$
for infinitely many $n$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;We follow 5 steps:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Let
$G = \lbrace \omega \in A : T^K(\omega) \notin A \quad \forall k &amp;gt;0 \rbrace$:
the set of all points of A that never ``return” in A.&lt;/li&gt;
&lt;li&gt;Note that $\forall j \geq 1$, $T^{-j}(G) \cap G = \emptyset$. In
fact, suppose $\omega \in T^{-j}(G)$. Then $\omega \notin G$ since
otherwise we would have $\omega \in G \subseteq A$ and
$\omega \in T^J(G) \subseteq A$ which contradicts the definition of
$G$.&lt;/li&gt;
&lt;li&gt;It follows that $\forall l,n \geq 1$,
$T^{-l}(G) \cap T^{-n}(G) = \emptyset$&lt;/li&gt;
&lt;li&gt;Since $T$ is a PPT, $\Pr(T^{-j}(G)) = \Pr(G)$ $\forall j$&lt;/li&gt;
&lt;li&gt;Then $$
\Pr (T^{-1}(G) \cup T^{-2}(G) \cup &amp;hellip; \cup T^{-l}(G)) = l \cdot \Pr(G) \leq 1 \Rightarrow \Pr(G) \leq \frac{1}{l} \quad \Rightarrow \quad \lim_ {l \to \infty} \Pr(G) = 0
$$ $$\tag*{$\blacksquare$}$$&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;comment&#34;&gt;Comment&lt;/h3&gt;
&lt;p&gt;Halmos: “&lt;em&gt;The recurrence theorem says that under the appropriate
conditions on a transformation T almost every point of each measurable
set $A$ returns to $A$ infinitely often. It is natural to ask: exactly
how long a time do the images of such recurrent points spend in $A$? The
precise formulation of the problem runs as follows: given a point $x$
(for present purposes it does not matter whether $x$ is in $A$ or not),
and given a positive integer $n$, form the ratio of the number of these
points that belong to $A$ to the total number (i.e., to $n$), and
evaluate the limit of these ratios as $n$ tends to infinity. It is, of
course, not at all obvious in what sense, if any, that limit exists. If
$f$ is the characteristic function of $A$ then the ratio just discussed
is&lt;/em&gt;” $$
\frac{1}{n} \sum _ {i=1}^n f(T^{i}x) = \frac{1}{n} \sum _ {i=1}^n x_i
$$&lt;/p&gt;
&lt;h3 id=&#34;ergodic-theorem&#34;&gt;Ergodic Theorem&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Let $T$ be an ergodic PPT on $\Omega$. Let $x$ be a random variable on
$\Omega$ with $\mathbb{E}[x] &amp;lt; \infty$. Let $x_i = x \circ T^i$. Then,
$$
\frac{1}{n} \sum _ {i=1}^n x_i \overset{as}{\to} \mathbb{E}[x]
$$&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;To figure out whether a PPT is ergodic, it’s useful to draw a graph
with $T^{-1}(G)$ on the y-axis and $G$ on the x-axis.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;comment-1&#34;&gt;Comment&lt;/h3&gt;
&lt;p&gt;From the ergodic theorem, we have that $$
\lim _ {n \to \infty} \frac{1}{n} \sum _ {i=1}^n f(T^{i}x) g(x) = f^* (x)g(x) \quad \Rightarrow \quad  \lim _ {n \to \infty} \Pr(T^{-n}G \cap H) = \Pr(G)\Pr(H)
$$ where $f^* (x) = \int f(x) dx = \mathbb{E}[f]$.&lt;/p&gt;
&lt;p&gt;[Halmos]: &lt;em&gt;We have seen that if a transformation $T$ is ergodic, then
$\Pr(T^{-n}G \cap H)$ converges in the sense of Cesaro to
$\Pr(G)\Pr(H)$. The validity of this condition for all $G$ and $H$ is,
in fact, equivalent to ergodicity. To prove this, suppose that $A$ is a
measurable invariant set, and take both $G$ and $H$ equal to $A$. It
follows that $\Pr(A) = (\Pr(A))^2$, and hence that $\Pr(A)$ is either 0
or 1.&lt;/em&gt;&lt;/p&gt;
&lt;h3 id=&#34;comment-2&#34;&gt;Comment 2&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;The Cesaro convergence condition has a natural intuitive
interpretation. We may visualize the transformation $T$ as a particular
way of stirring the contents of a vessel (of total volume 1) full of an
incompressible fluid, which may be thought of as 90 per cent gin ($G$)
and 10 per cent vermouth ($H$). If $H$ is the region originally occupied
by the vermouth, then, for any part $G$ of the vessel, the relative
amount of vermouth in $G$, after $n$ repetitions of the act of stirring,
is given by $\Pr(T^{-n}G \cap H)/\Pr(H)$. The ergodicity of $T$ implies
therefore that on the average this relative amount is exactly equal to
10 per cent. In general, in physical situations like this one, one
expects to be justified in making a much stronger statement, namely
that, after the liquid has been stirred sufficiently often
($n \to \infty$), every part $G$ of the container will contain
approximately 10 per cent vermouth. In mathematical language this
expectation amounts to replacing Cesaro convergence by ordinary
convergence, i.e., to the condition
$\lim_ {n\to \infty} \Pr(T^{-n}G \cap H) = \Pr(G)\Pr(H)$. If a
transformation $T$ satisfies this condition for every pair $G$ and $H$
of measurable sets, it is called mixing, or, in distinction from a
related but slightly weaker concept, strongly mixing.&lt;/em&gt;”&lt;/p&gt;
&lt;h3 id=&#34;mixing&#34;&gt;Mixing&lt;/h3&gt;
&lt;p&gt;Let $\lbrace\Omega, \mathcal{B}, P \rbrace$ be a probability space. Let
$T$ be a probability preserving transform. Then $T$ is &lt;strong&gt;strongly
mixing&lt;/strong&gt; if for every invariant sets $G$,$H \in \mathcal{B}$ $$
P(G \cap T^{-k}H) \to P(G)P(H) \quad \text{ as } k \to \infty
$$ where $T^{-k}H$ is defined as
$T^{-k}H = T^{-1}(&amp;hellip;T^{-1}(T^{-1} H)&amp;hellip;)$ repeated $k$ times.&lt;/p&gt;
&lt;p&gt;Let $\lbrace X_i\rbrace _ {i=-\infty}^{\infty}$ be a two sided sequence
of random variables. Let $\mathcal{B}_ {-\infty}^n$ be the sigma algebra
generated by $\lbrace X_i\rbrace _ {i=-\infty}^{n}$ and
$\mathcal{B}_ {n+k}^\infty$ the sigma algebra generated by
$\lbrace X_i \rbrace _ {i=n+k}^{\infty}$. Define the mixing coefficient
$$
\alpha(k) = \sup_ {n \in \mathbb{Z}} \sup_ {G \in \mathcal{B}_ {-\infty}^n} \sup_ {H \in \mathcal{B}_ {n+k}^\infty} | \Pr(G \cap H) - \Pr(G) \Pr(H)|
$$ $\lbrace X_i \rbrace$ is $\mathbb{\alpha}$&lt;strong&gt;-mixing&lt;/strong&gt; if
$\alpha(k) \to 0$ if $k \to \infty$.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Note that mixing implies ergodicity.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;stationarity&#34;&gt;Stationarity&lt;/h3&gt;
&lt;p&gt;Let $X_i : \Omega \to \mathbb{R}$ be a (two sided) sequence of random
variables with $i \in \mathbb{Z}$. $X_i$ is &lt;strong&gt;strongly stationary&lt;/strong&gt; or
simply stationary if $$
\Pr (X _ {i_ 1} \leq a_ 1 , &amp;hellip; , X _ {i_ k} \leq a_ k ) = \Pr (X _ { i _ {1-s}} \leq a_ 1 , &amp;hellip; , X _ {i _ {k-s}} \leq a_ k)  \quad \text{ for every } i_ 1, &amp;hellip;, i_ k, a_ 1, &amp;hellip;, a_ k, s \in \mathbb{R}.
$$&lt;/p&gt;
&lt;p&gt;Let $X_i : \Omega \to \mathbb{R}$ be a (two sided) sequence of random
variables with $i \in \mathbb{Z}$. $X_i$ is &lt;strong&gt;covariance stationary&lt;/strong&gt; if
$\mathbb{E}[X_i] = \mathbb{E}[X_j]$ for every $i,j$ and
$\mathbb{E}[X_i X_j] = \mathbb{E}[X _ {i+k} X _ {j+k}]$ for all $i,j,k$.
All of the second moments above are assumed to exist.&lt;/p&gt;
&lt;p&gt;Let $X_t : \Omega \to \mathbb{R}$ be a sequence of random variables
indexed by $t \in \mathbb{Z}$ such that $\mathbb{E}[|X_t|] &amp;lt; 1$ for each
$t$. $X_t$ is a &lt;strong&gt;martingale&lt;/strong&gt; if
$\mathbb{E} [X _ t |X _ {t-1} , X _ {t-2} , &amp;hellip;] = X _ t$. $X_t$ is a
&lt;strong&gt;martingale difference&lt;/strong&gt; if
$\mathbb{E} [X _ t | X _ {t-1} , X _ {t-2} ,&amp;hellip;] = 0$.&lt;/p&gt;
&lt;h3 id=&#34;gordins-central-limit-theorem&#34;&gt;Gordin’s Central Limit Theorem&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Let $\lbrace z_i \rbrace$ be a stationary, $\alpha$-mixing sequence of
random variables. If moreover&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\sum_ {m=1}^\infty \alpha(m)^{\frac{\delta}{2 + \delta}} &amp;lt; \infty$&lt;/li&gt;
&lt;li&gt;$\mathbb{E}[z_i] = 0$&lt;/li&gt;
&lt;li&gt;$\mathbb{E}\Big[ ||z_i || ^ {2+\delta} \Big] &amp;lt; \infty$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Then $$
\sqrt{n} \mathbb{E}_n [z_i] \overset{d}{\to} N(0,\Omega) \quad  \text{ where } \quad  \Omega = \lim _ {n \to \infty} Var(\sqrt{n} \mathbb{E}_n [z_i])
$$&lt;/p&gt;
&lt;p&gt;Let $\Omega_k = \mathbb{E}[ z_i z _ {i+k}&amp;rsquo;]$. Then a necessary condition
for Gordin’s CLT is covariance summability:
$\sum _ {k=1}^\infty \Omega_k &amp;lt; \infty$.&lt;/p&gt;
&lt;h3 id=&#34;ergodic-central-limit-theorem&#34;&gt;Ergodic Central Limit Theorem&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Let $\lbrace z_i \rbrace$ be a stationary, ergodic, martingale
difference sequence. Then $$
\sqrt{n} \mathbb{E}_n [z_i] \overset{d}{\to} N(0,\Omega) \quad  \text{ where } \quad  \Omega = \lim _ {n \to \infty} Var(\sqrt{n}\mathbb{E}_n[z_i])
$$&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Data Wrangling</title>
      <link>https://matteocourthoud.github.io/course/data-science/04_data_wrangling/</link>
      <pubDate>Sun, 01 May 2022 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/course/data-science/04_data_wrangling/</guid>
      <description>&lt;h2 id=&#34;setup&#34;&gt;Setup&lt;/h2&gt;
&lt;p&gt;For the scope of this tutorial we are going to use AirBnb Scraped data for the city of Bologna. The data is freely available at &lt;strong&gt;Inside AirBnb&lt;/strong&gt;: &lt;a href=&#34;http://insideairbnb.com/get-the-data.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://insideairbnb.com/get-the-data.html&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;A description of all variables in all datasets is avaliable &lt;a href=&#34;https://docs.google.com/spreadsheets/d/1iWCNJcSutYqpULSQHlNyGInUvHg2BoUGoNRIGa6Szc4/edit#gid=982310896&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;We are going to use 2 datasets:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;listing dataset: contains listing-level information&lt;/li&gt;
&lt;li&gt;pricing dataset: contains pricing data, over time&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import numpy as np
import pandas as pd
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Import listings data
url_listings = &amp;quot;http://data.insideairbnb.com/italy/emilia-romagna/bologna/2021-12-17/visualisations/listings.csv&amp;quot;
df_listings = pd.read_csv(url_listings)

# Import pricing data
url_prices = &amp;quot;http://data.insideairbnb.com/italy/emilia-romagna/bologna/2021-12-17/data/calendar.csv.gz&amp;quot;
df_prices = pd.read_csv(url_prices, compression=&amp;quot;gzip&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;sorting-and-renaming&#34;&gt;Sorting and Renaming&lt;/h2&gt;
&lt;p&gt;You can &lt;strong&gt;sort&lt;/strong&gt; the data using the &lt;code&gt;sort_values&lt;/code&gt; function.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Options&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;ascending&lt;/code&gt;: bool or list of bool, default True&lt;/li&gt;
&lt;li&gt;&lt;code&gt;na_position&lt;/code&gt;: {‘first’, ‘last’}, default ‘last’&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_listings.sort_values(by=[&#39;name&#39;, &#39;price&#39;], 
                        ascending=[False, True], 
                        na_position=&#39;last&#39;).head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;id&lt;/th&gt;
      &lt;th&gt;name&lt;/th&gt;
      &lt;th&gt;host_id&lt;/th&gt;
      &lt;th&gt;host_name&lt;/th&gt;
      &lt;th&gt;neighbourhood_group&lt;/th&gt;
      &lt;th&gt;neighbourhood&lt;/th&gt;
      &lt;th&gt;latitude&lt;/th&gt;
      &lt;th&gt;longitude&lt;/th&gt;
      &lt;th&gt;room_type&lt;/th&gt;
      &lt;th&gt;price&lt;/th&gt;
      &lt;th&gt;minimum_nights&lt;/th&gt;
      &lt;th&gt;number_of_reviews&lt;/th&gt;
      &lt;th&gt;last_review&lt;/th&gt;
      &lt;th&gt;reviews_per_month&lt;/th&gt;
      &lt;th&gt;calculated_host_listings_count&lt;/th&gt;
      &lt;th&gt;availability_365&lt;/th&gt;
      &lt;th&gt;number_of_reviews_ltm&lt;/th&gt;
      &lt;th&gt;license&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;2280&lt;/th&gt;
      &lt;td&gt;38601411&lt;/td&gt;
      &lt;td&gt;🏡Giardino di Annabella-relax in città-casa intera&lt;/td&gt;
      &lt;td&gt;240803020&lt;/td&gt;
      &lt;td&gt;Annabella&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;Porto - Saragozza&lt;/td&gt;
      &lt;td&gt;44.49303&lt;/td&gt;
      &lt;td&gt;11.31986&lt;/td&gt;
      &lt;td&gt;Entire home/apt&lt;/td&gt;
      &lt;td&gt;90&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;53&lt;/td&gt;
      &lt;td&gt;2021-12-13&lt;/td&gt;
      &lt;td&gt;1.96&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;76&lt;/td&gt;
      &lt;td&gt;27&lt;/td&gt;
      &lt;td&gt;392901&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2988&lt;/th&gt;
      &lt;td&gt;48177313&lt;/td&gt;
      &lt;td&gt;❤ Romantic Suite with SPA Bath ❤ 4starbologna.com&lt;/td&gt;
      &lt;td&gt;239491712&lt;/td&gt;
      &lt;td&gt;4 Star Bologna&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;Santo Stefano&lt;/td&gt;
      &lt;td&gt;44.50271&lt;/td&gt;
      &lt;td&gt;11.34998&lt;/td&gt;
      &lt;td&gt;Entire home/apt&lt;/td&gt;
      &lt;td&gt;309&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;2021-03-14&lt;/td&gt;
      &lt;td&gt;0.11&lt;/td&gt;
      &lt;td&gt;14&lt;/td&gt;
      &lt;td&gt;344&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3302&lt;/th&gt;
      &lt;td&gt;52367336&lt;/td&gt;
      &lt;td&gt;✨House of Alchemy✨&lt;/td&gt;
      &lt;td&gt;140013413&lt;/td&gt;
      &lt;td&gt;Greta&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;Porto - Saragozza&lt;/td&gt;
      &lt;td&gt;44.49072&lt;/td&gt;
      &lt;td&gt;11.30890&lt;/td&gt;
      &lt;td&gt;Entire home/apt&lt;/td&gt;
      &lt;td&gt;96&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;7&lt;/td&gt;
      &lt;td&gt;2021-11-28&lt;/td&gt;
      &lt;td&gt;3.18&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;88&lt;/td&gt;
      &lt;td&gt;7&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2039&lt;/th&gt;
      &lt;td&gt;34495335&lt;/td&gt;
      &lt;td&gt;♥ Romantic for Couple in Love ♥ | 4 Star Boutique&lt;/td&gt;
      &lt;td&gt;239491712&lt;/td&gt;
      &lt;td&gt;4 Star Bologna&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;Santo Stefano&lt;/td&gt;
      &lt;td&gt;44.50368&lt;/td&gt;
      &lt;td&gt;11.34972&lt;/td&gt;
      &lt;td&gt;Entire home/apt&lt;/td&gt;
      &lt;td&gt;143&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;25&lt;/td&gt;
      &lt;td&gt;2021-08-20&lt;/td&gt;
      &lt;td&gt;0.79&lt;/td&gt;
      &lt;td&gt;14&lt;/td&gt;
      &lt;td&gt;262&lt;/td&gt;
      &lt;td&gt;6&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2964&lt;/th&gt;
      &lt;td&gt;47866124&lt;/td&gt;
      &lt;td&gt;♡Amazing Suite with Private SPA ♡ 4starbologna...&lt;/td&gt;
      &lt;td&gt;239491712&lt;/td&gt;
      &lt;td&gt;4 Star Bologna&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;Santo Stefano&lt;/td&gt;
      &lt;td&gt;44.50381&lt;/td&gt;
      &lt;td&gt;11.34951&lt;/td&gt;
      &lt;td&gt;Entire home/apt&lt;/td&gt;
      &lt;td&gt;347&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;2021-10-17&lt;/td&gt;
      &lt;td&gt;0.72&lt;/td&gt;
      &lt;td&gt;14&lt;/td&gt;
      &lt;td&gt;337&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;You can remane columns using the &lt;code&gt;rename()&lt;/code&gt; function. It takes a dictionary as &lt;code&gt;column&lt;/code&gt; argument in the form &lt;code&gt;{&amp;quot;old_name&amp;quot;: &amp;quot;new_name&amp;quot;}&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_listings.rename(columns={&#39;name&#39;: &#39;listing_name&#39;, 
                            &#39;id&#39;: &#39;listing_id&#39;}).head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;listing_id&lt;/th&gt;
      &lt;th&gt;listing_name&lt;/th&gt;
      &lt;th&gt;host_id&lt;/th&gt;
      &lt;th&gt;host_name&lt;/th&gt;
      &lt;th&gt;neighbourhood_group&lt;/th&gt;
      &lt;th&gt;neighbourhood&lt;/th&gt;
      &lt;th&gt;latitude&lt;/th&gt;
      &lt;th&gt;longitude&lt;/th&gt;
      &lt;th&gt;room_type&lt;/th&gt;
      &lt;th&gt;price&lt;/th&gt;
      &lt;th&gt;minimum_nights&lt;/th&gt;
      &lt;th&gt;number_of_reviews&lt;/th&gt;
      &lt;th&gt;last_review&lt;/th&gt;
      &lt;th&gt;reviews_per_month&lt;/th&gt;
      &lt;th&gt;calculated_host_listings_count&lt;/th&gt;
      &lt;th&gt;availability_365&lt;/th&gt;
      &lt;th&gt;number_of_reviews_ltm&lt;/th&gt;
      &lt;th&gt;license&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;42196&lt;/td&gt;
      &lt;td&gt;50 sm Studio in the historic centre&lt;/td&gt;
      &lt;td&gt;184487&lt;/td&gt;
      &lt;td&gt;Carlo&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;Santo Stefano&lt;/td&gt;
      &lt;td&gt;44.48507&lt;/td&gt;
      &lt;td&gt;11.34786&lt;/td&gt;
      &lt;td&gt;Entire home/apt&lt;/td&gt;
      &lt;td&gt;68&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;180&lt;/td&gt;
      &lt;td&gt;2021-11-12&lt;/td&gt;
      &lt;td&gt;1.32&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;161&lt;/td&gt;
      &lt;td&gt;6&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;46352&lt;/td&gt;
      &lt;td&gt;A room in Pasolini&#39;s house&lt;/td&gt;
      &lt;td&gt;467810&lt;/td&gt;
      &lt;td&gt;Eleonora&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;Porto - Saragozza&lt;/td&gt;
      &lt;td&gt;44.49168&lt;/td&gt;
      &lt;td&gt;11.33514&lt;/td&gt;
      &lt;td&gt;Private room&lt;/td&gt;
      &lt;td&gt;29&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;300&lt;/td&gt;
      &lt;td&gt;2021-11-30&lt;/td&gt;
      &lt;td&gt;2.20&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;248&lt;/td&gt;
      &lt;td&gt;37&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;59697&lt;/td&gt;
      &lt;td&gt;COZY LARGE BEDROOM in the city center&lt;/td&gt;
      &lt;td&gt;286688&lt;/td&gt;
      &lt;td&gt;Paolo&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;Santo Stefano&lt;/td&gt;
      &lt;td&gt;44.48817&lt;/td&gt;
      &lt;td&gt;11.34124&lt;/td&gt;
      &lt;td&gt;Private room&lt;/td&gt;
      &lt;td&gt;50&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;240&lt;/td&gt;
      &lt;td&gt;2020-10-04&lt;/td&gt;
      &lt;td&gt;2.18&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;327&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;85368&lt;/td&gt;
      &lt;td&gt;Garden House Bologna&lt;/td&gt;
      &lt;td&gt;467675&lt;/td&gt;
      &lt;td&gt;Anna Maria&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;Santo Stefano&lt;/td&gt;
      &lt;td&gt;44.47834&lt;/td&gt;
      &lt;td&gt;11.35672&lt;/td&gt;
      &lt;td&gt;Entire home/apt&lt;/td&gt;
      &lt;td&gt;126&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;40&lt;/td&gt;
      &lt;td&gt;2019-11-03&lt;/td&gt;
      &lt;td&gt;0.34&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;332&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;145779&lt;/td&gt;
      &lt;td&gt;SINGLE ROOM&lt;/td&gt;
      &lt;td&gt;705535&lt;/td&gt;
      &lt;td&gt;Valerio&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;Porto - Saragozza&lt;/td&gt;
      &lt;td&gt;44.49306&lt;/td&gt;
      &lt;td&gt;11.33786&lt;/td&gt;
      &lt;td&gt;Private room&lt;/td&gt;
      &lt;td&gt;50&lt;/td&gt;
      &lt;td&gt;10&lt;/td&gt;
      &lt;td&gt;69&lt;/td&gt;
      &lt;td&gt;2021-12-05&lt;/td&gt;
      &lt;td&gt;0.55&lt;/td&gt;
      &lt;td&gt;9&lt;/td&gt;
      &lt;td&gt;365&lt;/td&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;h2 id=&#34;aggregating&#34;&gt;Aggregating&lt;/h2&gt;
&lt;p&gt;If we want to count observations across 2 categorical variables, we can use &lt;code&gt;pd.crosstab()&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;pd.crosstab(df_listings[&#39;neighbourhood&#39;], df_listings[&#39;room_type&#39;])
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;room_type&lt;/th&gt;
      &lt;th&gt;Entire home/apt&lt;/th&gt;
      &lt;th&gt;Hotel room&lt;/th&gt;
      &lt;th&gt;Private room&lt;/th&gt;
      &lt;th&gt;Shared room&lt;/th&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;neighbourhood&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;Borgo Panigale - Reno&lt;/th&gt;
      &lt;td&gt;107&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;39&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;Navile&lt;/th&gt;
      &lt;td&gt;250&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;149&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;Porto - Saragozza&lt;/th&gt;
      &lt;td&gt;842&lt;/td&gt;
      &lt;td&gt;16&lt;/td&gt;
      &lt;td&gt;299&lt;/td&gt;
      &lt;td&gt;10&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;San Donato - San Vitale&lt;/th&gt;
      &lt;td&gt;280&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;134&lt;/td&gt;
      &lt;td&gt;4&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;Santo Stefano&lt;/th&gt;
      &lt;td&gt;924&lt;/td&gt;
      &lt;td&gt;29&lt;/td&gt;
      &lt;td&gt;237&lt;/td&gt;
      &lt;td&gt;5&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;Savena&lt;/th&gt;
      &lt;td&gt;73&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;48&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;We can compute statistics by group using &lt;code&gt;groupby()&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_listings.groupby(&#39;neighbourhood&#39;)[[&#39;price&#39;, &#39;reviews_per_month&#39;]].mean()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;price&lt;/th&gt;
      &lt;th&gt;reviews_per_month&lt;/th&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;neighbourhood&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;Borgo Panigale - Reno&lt;/th&gt;
      &lt;td&gt;83.020548&lt;/td&gt;
      &lt;td&gt;0.983488&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;Navile&lt;/th&gt;
      &lt;td&gt;142.200993&lt;/td&gt;
      &lt;td&gt;1.156745&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;Porto - Saragozza&lt;/th&gt;
      &lt;td&gt;129.908312&lt;/td&gt;
      &lt;td&gt;1.340325&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;San Donato - San Vitale&lt;/th&gt;
      &lt;td&gt;91.618138&lt;/td&gt;
      &lt;td&gt;0.933011&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;Santo Stefano&lt;/th&gt;
      &lt;td&gt;119.441841&lt;/td&gt;
      &lt;td&gt;1.344810&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;Savena&lt;/th&gt;
      &lt;td&gt;69.626016&lt;/td&gt;
      &lt;td&gt;0.805888&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;If you want to perform more than one function, maybe on different columns, you can use &lt;code&gt;aggregate()&lt;/code&gt; which can be shortened to &lt;code&gt;agg()&lt;/code&gt;. The sintax is &lt;code&gt;agg(output_var = (&amp;quot;input_var&amp;quot;, function))&lt;/code&gt; and it accepts also &lt;code&gt;numpy&lt;/code&gt; functions.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_listings.groupby(&#39;neighbourhood&#39;).agg(mean_reviews=(&amp;quot;reviews_per_month&amp;quot;, &amp;quot;mean&amp;quot;),
                                         min_price=(&amp;quot;price&amp;quot;, &amp;quot;min&amp;quot;),
                                         max_price=(&amp;quot;price&amp;quot;, np.max)).reset_index()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;neighbourhood&lt;/th&gt;
      &lt;th&gt;mean_reviews&lt;/th&gt;
      &lt;th&gt;min_price&lt;/th&gt;
      &lt;th&gt;max_price&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;Borgo Panigale - Reno&lt;/td&gt;
      &lt;td&gt;0.983488&lt;/td&gt;
      &lt;td&gt;9&lt;/td&gt;
      &lt;td&gt;1429&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;Navile&lt;/td&gt;
      &lt;td&gt;1.156745&lt;/td&gt;
      &lt;td&gt;14&lt;/td&gt;
      &lt;td&gt;5000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;Porto - Saragozza&lt;/td&gt;
      &lt;td&gt;1.340325&lt;/td&gt;
      &lt;td&gt;7&lt;/td&gt;
      &lt;td&gt;9999&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;San Donato - San Vitale&lt;/td&gt;
      &lt;td&gt;0.933011&lt;/td&gt;
      &lt;td&gt;10&lt;/td&gt;
      &lt;td&gt;1600&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;Santo Stefano&lt;/td&gt;
      &lt;td&gt;1.344810&lt;/td&gt;
      &lt;td&gt;11&lt;/td&gt;
      &lt;td&gt;9999&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;5&lt;/th&gt;
      &lt;td&gt;Savena&lt;/td&gt;
      &lt;td&gt;0.805888&lt;/td&gt;
      &lt;td&gt;9&lt;/td&gt;
      &lt;td&gt;680&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;If we want to build a new column by group, we can use &lt;code&gt;transform()&lt;/code&gt; on the grouped data. Unfortunately, it does not work as nicely as &lt;code&gt;aggregate()&lt;/code&gt; and we have to do one column at the time.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_listings.groupby(&#39;neighbourhood&#39;)[[&#39;price&#39;, &#39;reviews_per_month&#39;]].transform(&#39;mean&#39;).head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;price&lt;/th&gt;
      &lt;th&gt;reviews_per_month&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;119.441841&lt;/td&gt;
      &lt;td&gt;1.344810&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;129.908312&lt;/td&gt;
      &lt;td&gt;1.340325&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;119.441841&lt;/td&gt;
      &lt;td&gt;1.344810&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;119.441841&lt;/td&gt;
      &lt;td&gt;1.344810&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;129.908312&lt;/td&gt;
      &lt;td&gt;1.340325&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;h2 id=&#34;combining-datasets&#34;&gt;Combining Datasets&lt;/h2&gt;
&lt;p&gt;We can &lt;strong&gt;concatenate&lt;/strong&gt; datasets using &lt;code&gt;pd.concat()&lt;/code&gt;. It takes as argument a list of dataframes. By default, &lt;code&gt;pd.concat()&lt;/code&gt; performs the outer join. We can change it using the &lt;code&gt;join&lt;/code&gt; option (in this case, it makes no difference).&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_listings1 = df_listings[:2000]
np.shape(df_listings1)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;(2000, 18)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_listings2 = df_listings[1000:]
np.shape(df_listings2)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;(2453, 18)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;np.shape(
    pd.concat([df_listings1, df_listings2])
)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;(4453, 18)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To instead merge dataframes, we can use the &lt;code&gt;pd.merge&lt;/code&gt; function.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Options&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;how&lt;/code&gt;: {‘left’, ‘right’, ‘outer’, ‘inner’, ‘cross’}, default ‘inner’&lt;/li&gt;
&lt;li&gt;&lt;code&gt;on&lt;/code&gt;: label or list&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_merged = pd.merge(df_listings, df_prices, left_on=&#39;id&#39;, right_on=&#39;listing_id&#39;, how=&#39;inner&#39;)
df_merged.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;id&lt;/th&gt;
      &lt;th&gt;name&lt;/th&gt;
      &lt;th&gt;host_id&lt;/th&gt;
      &lt;th&gt;host_name&lt;/th&gt;
      &lt;th&gt;neighbourhood_group&lt;/th&gt;
      &lt;th&gt;neighbourhood&lt;/th&gt;
      &lt;th&gt;latitude&lt;/th&gt;
      &lt;th&gt;longitude&lt;/th&gt;
      &lt;th&gt;room_type&lt;/th&gt;
      &lt;th&gt;price_x&lt;/th&gt;
      &lt;th&gt;...&lt;/th&gt;
      &lt;th&gt;availability_365&lt;/th&gt;
      &lt;th&gt;number_of_reviews_ltm&lt;/th&gt;
      &lt;th&gt;license&lt;/th&gt;
      &lt;th&gt;listing_id&lt;/th&gt;
      &lt;th&gt;date&lt;/th&gt;
      &lt;th&gt;available&lt;/th&gt;
      &lt;th&gt;price_y&lt;/th&gt;
      &lt;th&gt;adjusted_price&lt;/th&gt;
      &lt;th&gt;minimum_nights_y&lt;/th&gt;
      &lt;th&gt;maximum_nights&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;42196&lt;/td&gt;
      &lt;td&gt;50 sm Studio in the historic centre&lt;/td&gt;
      &lt;td&gt;184487&lt;/td&gt;
      &lt;td&gt;Carlo&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;Santo Stefano&lt;/td&gt;
      &lt;td&gt;44.48507&lt;/td&gt;
      &lt;td&gt;11.34786&lt;/td&gt;
      &lt;td&gt;Entire home/apt&lt;/td&gt;
      &lt;td&gt;68&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;161&lt;/td&gt;
      &lt;td&gt;6&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;42196&lt;/td&gt;
      &lt;td&gt;2021-12-17&lt;/td&gt;
      &lt;td&gt;f&lt;/td&gt;
      &lt;td&gt;$68.00&lt;/td&gt;
      &lt;td&gt;$68.00&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;360&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;42196&lt;/td&gt;
      &lt;td&gt;50 sm Studio in the historic centre&lt;/td&gt;
      &lt;td&gt;184487&lt;/td&gt;
      &lt;td&gt;Carlo&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;Santo Stefano&lt;/td&gt;
      &lt;td&gt;44.48507&lt;/td&gt;
      &lt;td&gt;11.34786&lt;/td&gt;
      &lt;td&gt;Entire home/apt&lt;/td&gt;
      &lt;td&gt;68&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;161&lt;/td&gt;
      &lt;td&gt;6&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;42196&lt;/td&gt;
      &lt;td&gt;2021-12-18&lt;/td&gt;
      &lt;td&gt;f&lt;/td&gt;
      &lt;td&gt;$68.00&lt;/td&gt;
      &lt;td&gt;$68.00&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;360&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;42196&lt;/td&gt;
      &lt;td&gt;50 sm Studio in the historic centre&lt;/td&gt;
      &lt;td&gt;184487&lt;/td&gt;
      &lt;td&gt;Carlo&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;Santo Stefano&lt;/td&gt;
      &lt;td&gt;44.48507&lt;/td&gt;
      &lt;td&gt;11.34786&lt;/td&gt;
      &lt;td&gt;Entire home/apt&lt;/td&gt;
      &lt;td&gt;68&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;161&lt;/td&gt;
      &lt;td&gt;6&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;42196&lt;/td&gt;
      &lt;td&gt;2021-12-19&lt;/td&gt;
      &lt;td&gt;f&lt;/td&gt;
      &lt;td&gt;$68.00&lt;/td&gt;
      &lt;td&gt;$68.00&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;360&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;42196&lt;/td&gt;
      &lt;td&gt;50 sm Studio in the historic centre&lt;/td&gt;
      &lt;td&gt;184487&lt;/td&gt;
      &lt;td&gt;Carlo&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;Santo Stefano&lt;/td&gt;
      &lt;td&gt;44.48507&lt;/td&gt;
      &lt;td&gt;11.34786&lt;/td&gt;
      &lt;td&gt;Entire home/apt&lt;/td&gt;
      &lt;td&gt;68&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;161&lt;/td&gt;
      &lt;td&gt;6&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;42196&lt;/td&gt;
      &lt;td&gt;2021-12-20&lt;/td&gt;
      &lt;td&gt;f&lt;/td&gt;
      &lt;td&gt;$68.00&lt;/td&gt;
      &lt;td&gt;$68.00&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;360&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;42196&lt;/td&gt;
      &lt;td&gt;50 sm Studio in the historic centre&lt;/td&gt;
      &lt;td&gt;184487&lt;/td&gt;
      &lt;td&gt;Carlo&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;Santo Stefano&lt;/td&gt;
      &lt;td&gt;44.48507&lt;/td&gt;
      &lt;td&gt;11.34786&lt;/td&gt;
      &lt;td&gt;Entire home/apt&lt;/td&gt;
      &lt;td&gt;68&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;161&lt;/td&gt;
      &lt;td&gt;6&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;42196&lt;/td&gt;
      &lt;td&gt;2021-12-21&lt;/td&gt;
      &lt;td&gt;f&lt;/td&gt;
      &lt;td&gt;$68.00&lt;/td&gt;
      &lt;td&gt;$68.00&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;360&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;5 rows × 25 columns&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;As you can see, since the variable &lt;code&gt;price&lt;/code&gt; was present in both datasets, we now have a &lt;code&gt;price.x&lt;/code&gt; and a &lt;code&gt;price.y&lt;/code&gt;.&lt;/p&gt;
&lt;h2 id=&#34;reshaping&#34;&gt;Reshaping&lt;/h2&gt;
&lt;p&gt;First, let&amp;rsquo;s compute average prices by &lt;code&gt;neighbourhood&lt;/code&gt; and &lt;code&gt;date&lt;/code&gt; using the merged dataset.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_long = df_merged.groupby([&#39;neighbourhood&#39;, &#39;date&#39;])[&#39;price_x&#39;].agg(&#39;mean&#39;).reset_index()
df_long.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;neighbourhood&lt;/th&gt;
      &lt;th&gt;date&lt;/th&gt;
      &lt;th&gt;price_x&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;Borgo Panigale - Reno&lt;/td&gt;
      &lt;td&gt;2021-12-17&lt;/td&gt;
      &lt;td&gt;83.020548&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;Borgo Panigale - Reno&lt;/td&gt;
      &lt;td&gt;2021-12-18&lt;/td&gt;
      &lt;td&gt;83.020548&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;Borgo Panigale - Reno&lt;/td&gt;
      &lt;td&gt;2021-12-19&lt;/td&gt;
      &lt;td&gt;83.020548&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;Borgo Panigale - Reno&lt;/td&gt;
      &lt;td&gt;2021-12-20&lt;/td&gt;
      &lt;td&gt;83.020548&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;Borgo Panigale - Reno&lt;/td&gt;
      &lt;td&gt;2021-12-21&lt;/td&gt;
      &lt;td&gt;83.020548&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;This is what is called &lt;strong&gt;long format&lt;/strong&gt; since it has one or more variables (&lt;code&gt;price_x&lt;/code&gt; in this case) stacked vertically along a categorical variable (&lt;code&gt;neighborhood&lt;/code&gt; and &lt;code&gt;date&lt;/code&gt; here), which acts as index.&lt;/p&gt;
&lt;p&gt;The alternative is the &lt;strong&gt;wide format&lt;/strong&gt; where we have one separate column for each neighborhood.&lt;/p&gt;
&lt;p&gt;We can &lt;strong&gt;reshape&lt;/strong&gt; the dataset from &lt;strong&gt;long to wide&lt;/strong&gt; using the &lt;code&gt;pd.pivot()&lt;/code&gt; command. d&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_wide = pd.pivot(data=df_long, index=&#39;date&#39;, columns=&#39;neighbourhood&#39;).reset_index()
df_wide.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead tr th {
    text-align: left;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;date&lt;/th&gt;
      &lt;th colspan=&#34;6&#34; halign=&#34;left&#34;&gt;price_x&lt;/th&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;neighbourhood&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;Borgo Panigale - Reno&lt;/th&gt;
      &lt;th&gt;Navile&lt;/th&gt;
      &lt;th&gt;Porto - Saragozza&lt;/th&gt;
      &lt;th&gt;San Donato - San Vitale&lt;/th&gt;
      &lt;th&gt;Santo Stefano&lt;/th&gt;
      &lt;th&gt;Savena&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;2021-12-17&lt;/td&gt;
      &lt;td&gt;83.020548&lt;/td&gt;
      &lt;td&gt;142.200993&lt;/td&gt;
      &lt;td&gt;129.908312&lt;/td&gt;
      &lt;td&gt;91.618138&lt;/td&gt;
      &lt;td&gt;119.441841&lt;/td&gt;
      &lt;td&gt;69.626016&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;2021-12-18&lt;/td&gt;
      &lt;td&gt;83.020548&lt;/td&gt;
      &lt;td&gt;142.200993&lt;/td&gt;
      &lt;td&gt;129.908312&lt;/td&gt;
      &lt;td&gt;91.618138&lt;/td&gt;
      &lt;td&gt;119.441841&lt;/td&gt;
      &lt;td&gt;69.626016&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;2021-12-19&lt;/td&gt;
      &lt;td&gt;83.020548&lt;/td&gt;
      &lt;td&gt;142.200993&lt;/td&gt;
      &lt;td&gt;129.908312&lt;/td&gt;
      &lt;td&gt;91.618138&lt;/td&gt;
      &lt;td&gt;119.441841&lt;/td&gt;
      &lt;td&gt;69.626016&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;2021-12-20&lt;/td&gt;
      &lt;td&gt;83.020548&lt;/td&gt;
      &lt;td&gt;142.200993&lt;/td&gt;
      &lt;td&gt;129.908312&lt;/td&gt;
      &lt;td&gt;91.618138&lt;/td&gt;
      &lt;td&gt;119.441841&lt;/td&gt;
      &lt;td&gt;69.626016&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;2021-12-21&lt;/td&gt;
      &lt;td&gt;83.020548&lt;/td&gt;
      &lt;td&gt;142.200993&lt;/td&gt;
      &lt;td&gt;129.908312&lt;/td&gt;
      &lt;td&gt;91.618138&lt;/td&gt;
      &lt;td&gt;119.441841&lt;/td&gt;
      &lt;td&gt;69.626016&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;We can &lt;strong&gt;reshape&lt;/strong&gt; the dataset from &lt;strong&gt;wide to long&lt;/strong&gt; using the &lt;code&gt;pd.melt()&lt;/code&gt; command. It takes the following arguments&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;data&lt;/code&gt;: the dataframe&lt;/li&gt;
&lt;li&gt;&lt;code&gt;id_vars&lt;/code&gt;: the variable that was indexing the old dataset&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;pd.melt(df_wide, id_vars=&#39;date&#39;, value_name=&#39;price&#39;).head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;date&lt;/th&gt;
      &lt;th&gt;None&lt;/th&gt;
      &lt;th&gt;neighbourhood&lt;/th&gt;
      &lt;th&gt;price&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;2021-12-17&lt;/td&gt;
      &lt;td&gt;price_x&lt;/td&gt;
      &lt;td&gt;Borgo Panigale - Reno&lt;/td&gt;
      &lt;td&gt;83.020548&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;2021-12-18&lt;/td&gt;
      &lt;td&gt;price_x&lt;/td&gt;
      &lt;td&gt;Borgo Panigale - Reno&lt;/td&gt;
      &lt;td&gt;83.020548&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;2021-12-19&lt;/td&gt;
      &lt;td&gt;price_x&lt;/td&gt;
      &lt;td&gt;Borgo Panigale - Reno&lt;/td&gt;
      &lt;td&gt;83.020548&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;2021-12-20&lt;/td&gt;
      &lt;td&gt;price_x&lt;/td&gt;
      &lt;td&gt;Borgo Panigale - Reno&lt;/td&gt;
      &lt;td&gt;83.020548&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;2021-12-21&lt;/td&gt;
      &lt;td&gt;price_x&lt;/td&gt;
      &lt;td&gt;Borgo Panigale - Reno&lt;/td&gt;
      &lt;td&gt;83.020548&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;If we do not have &lt;code&gt;MultiIndex&lt;/code&gt; columns, but just a common prefix, we can &lt;strong&gt;reshape&lt;/strong&gt; the dataset from &lt;strong&gt;wide to long&lt;/strong&gt; using the &lt;code&gt;pd.wide_to_long()&lt;/code&gt; command.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_wide2 = df_wide.copy()
df_wide2.columns = [&#39;&#39;.join(col) for col in df_wide2.columns]
df_wide2.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;date&lt;/th&gt;
      &lt;th&gt;price_xBorgo Panigale - Reno&lt;/th&gt;
      &lt;th&gt;price_xNavile&lt;/th&gt;
      &lt;th&gt;price_xPorto - Saragozza&lt;/th&gt;
      &lt;th&gt;price_xSan Donato - San Vitale&lt;/th&gt;
      &lt;th&gt;price_xSanto Stefano&lt;/th&gt;
      &lt;th&gt;price_xSavena&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;2021-12-17&lt;/td&gt;
      &lt;td&gt;83.020548&lt;/td&gt;
      &lt;td&gt;142.200993&lt;/td&gt;
      &lt;td&gt;129.908312&lt;/td&gt;
      &lt;td&gt;91.618138&lt;/td&gt;
      &lt;td&gt;119.441841&lt;/td&gt;
      &lt;td&gt;69.626016&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;2021-12-18&lt;/td&gt;
      &lt;td&gt;83.020548&lt;/td&gt;
      &lt;td&gt;142.200993&lt;/td&gt;
      &lt;td&gt;129.908312&lt;/td&gt;
      &lt;td&gt;91.618138&lt;/td&gt;
      &lt;td&gt;119.441841&lt;/td&gt;
      &lt;td&gt;69.626016&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;2021-12-19&lt;/td&gt;
      &lt;td&gt;83.020548&lt;/td&gt;
      &lt;td&gt;142.200993&lt;/td&gt;
      &lt;td&gt;129.908312&lt;/td&gt;
      &lt;td&gt;91.618138&lt;/td&gt;
      &lt;td&gt;119.441841&lt;/td&gt;
      &lt;td&gt;69.626016&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;2021-12-20&lt;/td&gt;
      &lt;td&gt;83.020548&lt;/td&gt;
      &lt;td&gt;142.200993&lt;/td&gt;
      &lt;td&gt;129.908312&lt;/td&gt;
      &lt;td&gt;91.618138&lt;/td&gt;
      &lt;td&gt;119.441841&lt;/td&gt;
      &lt;td&gt;69.626016&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;2021-12-21&lt;/td&gt;
      &lt;td&gt;83.020548&lt;/td&gt;
      &lt;td&gt;142.200993&lt;/td&gt;
      &lt;td&gt;129.908312&lt;/td&gt;
      &lt;td&gt;91.618138&lt;/td&gt;
      &lt;td&gt;119.441841&lt;/td&gt;
      &lt;td&gt;69.626016&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;The &lt;code&gt;pd.wide_to_long()&lt;/code&gt; command takes the following arguments&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;data&lt;/code&gt;: the dataframe&lt;/li&gt;
&lt;li&gt;&lt;code&gt;stubnames&lt;/code&gt;: the prefixes of the variables that we want to reshape into one&lt;/li&gt;
&lt;li&gt;&lt;code&gt;i&lt;/code&gt;: the variable that was indexing the old dataset&lt;/li&gt;
&lt;li&gt;&lt;code&gt;j&lt;/code&gt;: the name of the new categorical variable that we extract from &lt;code&gt;stubnames&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;suffix&lt;/code&gt;: regular expression of the suffix, the default is &lt;code&gt;\d+&lt;/code&gt;, i.e. digits&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;pd.wide_to_long(df_wide2, stubnames=&#39;price_x&#39;, i=&#39;date&#39;, j=&#39;neighborhood&#39;, suffix=&#39;\D+&#39;).head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;price_x&lt;/th&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;date&lt;/th&gt;
      &lt;th&gt;neighborhood&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;2021-12-17&lt;/th&gt;
      &lt;th&gt;Borgo Panigale - Reno&lt;/th&gt;
      &lt;td&gt;83.020548&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2021-12-18&lt;/th&gt;
      &lt;th&gt;Borgo Panigale - Reno&lt;/th&gt;
      &lt;td&gt;83.020548&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2021-12-19&lt;/th&gt;
      &lt;th&gt;Borgo Panigale - Reno&lt;/th&gt;
      &lt;td&gt;83.020548&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2021-12-20&lt;/th&gt;
      &lt;th&gt;Borgo Panigale - Reno&lt;/th&gt;
      &lt;td&gt;83.020548&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2021-12-21&lt;/th&gt;
      &lt;th&gt;Borgo Panigale - Reno&lt;/th&gt;
      &lt;td&gt;83.020548&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;Note that we had to change the &lt;code&gt;suffix&lt;/code&gt; to &lt;code&gt;\D+&lt;/code&gt;, i.e. not digits.&lt;/p&gt;
&lt;h2 id=&#34;window-functions&#34;&gt;Window Functions&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Methods&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;shift()&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;expanding()&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;rolling()&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;p&gt;When we have time series data, we might want to do operations across time. First, let&amp;rsquo;s aggregate the &lt;code&gt;df_price&lt;/code&gt; dataset at the year-month level.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;temp = df_prices.copy()
temp[&#39;price&#39;] = temp[&#39;price&#39;].str.replace(&#39;[$|,]&#39;, &#39;&#39;, regex=True).astype(float)
temp[&#39;date&#39;] = pd.to_datetime(temp[&#39;date&#39;]).dt.to_period(&#39;M&#39;)
temp = temp.groupby([&#39;listing_id&#39;, &#39;date&#39;])[&#39;price&#39;].mean().reset_index()\
    .sort_values(by=[&#39;listing_id&#39;, &#39;date&#39;], ascending=[False, True])
temp.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;listing_id&lt;/th&gt;
      &lt;th&gt;date&lt;/th&gt;
      &lt;th&gt;price&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;44876&lt;/th&gt;
      &lt;td&gt;53854962&lt;/td&gt;
      &lt;td&gt;2021-12&lt;/td&gt;
      &lt;td&gt;147.400000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;44877&lt;/th&gt;
      &lt;td&gt;53854962&lt;/td&gt;
      &lt;td&gt;2022-01&lt;/td&gt;
      &lt;td&gt;137.645161&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;44878&lt;/th&gt;
      &lt;td&gt;53854962&lt;/td&gt;
      &lt;td&gt;2022-02&lt;/td&gt;
      &lt;td&gt;124.642857&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;44879&lt;/th&gt;
      &lt;td&gt;53854962&lt;/td&gt;
      &lt;td&gt;2022-03&lt;/td&gt;
      &lt;td&gt;285.096774&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;44880&lt;/th&gt;
      &lt;td&gt;53854962&lt;/td&gt;
      &lt;td&gt;2022-04&lt;/td&gt;
      &lt;td&gt;115.000000&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;We can &lt;strong&gt;lead or lag&lt;/strong&gt; one variable using &lt;code&gt;shift()&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;temp[&#39;price1&#39;] = temp[&#39;price&#39;].shift(1)
temp.head(15)
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;listing_id&lt;/th&gt;
      &lt;th&gt;date&lt;/th&gt;
      &lt;th&gt;price&lt;/th&gt;
      &lt;th&gt;price1&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;44876&lt;/th&gt;
      &lt;td&gt;53854962&lt;/td&gt;
      &lt;td&gt;2021-12&lt;/td&gt;
      &lt;td&gt;147.400000&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;44877&lt;/th&gt;
      &lt;td&gt;53854962&lt;/td&gt;
      &lt;td&gt;2022-01&lt;/td&gt;
      &lt;td&gt;137.645161&lt;/td&gt;
      &lt;td&gt;147.400000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;44878&lt;/th&gt;
      &lt;td&gt;53854962&lt;/td&gt;
      &lt;td&gt;2022-02&lt;/td&gt;
      &lt;td&gt;124.642857&lt;/td&gt;
      &lt;td&gt;137.645161&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;44879&lt;/th&gt;
      &lt;td&gt;53854962&lt;/td&gt;
      &lt;td&gt;2022-03&lt;/td&gt;
      &lt;td&gt;285.096774&lt;/td&gt;
      &lt;td&gt;124.642857&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;44880&lt;/th&gt;
      &lt;td&gt;53854962&lt;/td&gt;
      &lt;td&gt;2022-04&lt;/td&gt;
      &lt;td&gt;115.000000&lt;/td&gt;
      &lt;td&gt;285.096774&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;44881&lt;/th&gt;
      &lt;td&gt;53854962&lt;/td&gt;
      &lt;td&gt;2022-05&lt;/td&gt;
      &lt;td&gt;115.000000&lt;/td&gt;
      &lt;td&gt;115.000000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;44882&lt;/th&gt;
      &lt;td&gt;53854962&lt;/td&gt;
      &lt;td&gt;2022-06&lt;/td&gt;
      &lt;td&gt;115.000000&lt;/td&gt;
      &lt;td&gt;115.000000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;44883&lt;/th&gt;
      &lt;td&gt;53854962&lt;/td&gt;
      &lt;td&gt;2022-07&lt;/td&gt;
      &lt;td&gt;115.000000&lt;/td&gt;
      &lt;td&gt;115.000000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;44884&lt;/th&gt;
      &lt;td&gt;53854962&lt;/td&gt;
      &lt;td&gt;2022-08&lt;/td&gt;
      &lt;td&gt;115.000000&lt;/td&gt;
      &lt;td&gt;115.000000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;44885&lt;/th&gt;
      &lt;td&gt;53854962&lt;/td&gt;
      &lt;td&gt;2022-09&lt;/td&gt;
      &lt;td&gt;115.000000&lt;/td&gt;
      &lt;td&gt;115.000000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;44886&lt;/th&gt;
      &lt;td&gt;53854962&lt;/td&gt;
      &lt;td&gt;2022-10&lt;/td&gt;
      &lt;td&gt;115.000000&lt;/td&gt;
      &lt;td&gt;115.000000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;44887&lt;/th&gt;
      &lt;td&gt;53854962&lt;/td&gt;
      &lt;td&gt;2022-11&lt;/td&gt;
      &lt;td&gt;115.000000&lt;/td&gt;
      &lt;td&gt;115.000000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;44888&lt;/th&gt;
      &lt;td&gt;53854962&lt;/td&gt;
      &lt;td&gt;2022-12&lt;/td&gt;
      &lt;td&gt;115.000000&lt;/td&gt;
      &lt;td&gt;115.000000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;44863&lt;/th&gt;
      &lt;td&gt;53837654&lt;/td&gt;
      &lt;td&gt;2021-12&lt;/td&gt;
      &lt;td&gt;184.133333&lt;/td&gt;
      &lt;td&gt;115.000000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;44864&lt;/th&gt;
      &lt;td&gt;53837654&lt;/td&gt;
      &lt;td&gt;2022-01&lt;/td&gt;
      &lt;td&gt;148.741935&lt;/td&gt;
      &lt;td&gt;184.133333&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;If we want to &lt;strong&gt;lead or lag a variable within a group&lt;/strong&gt;, we can combine &lt;code&gt;shift()&lt;/code&gt; with &lt;code&gt;groupby()&lt;/code&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;temp[&#39;price1&#39;] = temp.groupby(&#39;listing_id&#39;)[&#39;price&#39;].shift(1)
temp.head(15)
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;listing_id&lt;/th&gt;
      &lt;th&gt;date&lt;/th&gt;
      &lt;th&gt;price&lt;/th&gt;
      &lt;th&gt;price1&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;44876&lt;/th&gt;
      &lt;td&gt;53854962&lt;/td&gt;
      &lt;td&gt;2021-12&lt;/td&gt;
      &lt;td&gt;147.400000&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;44877&lt;/th&gt;
      &lt;td&gt;53854962&lt;/td&gt;
      &lt;td&gt;2022-01&lt;/td&gt;
      &lt;td&gt;137.645161&lt;/td&gt;
      &lt;td&gt;147.400000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;44878&lt;/th&gt;
      &lt;td&gt;53854962&lt;/td&gt;
      &lt;td&gt;2022-02&lt;/td&gt;
      &lt;td&gt;124.642857&lt;/td&gt;
      &lt;td&gt;137.645161&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;44879&lt;/th&gt;
      &lt;td&gt;53854962&lt;/td&gt;
      &lt;td&gt;2022-03&lt;/td&gt;
      &lt;td&gt;285.096774&lt;/td&gt;
      &lt;td&gt;124.642857&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;44880&lt;/th&gt;
      &lt;td&gt;53854962&lt;/td&gt;
      &lt;td&gt;2022-04&lt;/td&gt;
      &lt;td&gt;115.000000&lt;/td&gt;
      &lt;td&gt;285.096774&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;44881&lt;/th&gt;
      &lt;td&gt;53854962&lt;/td&gt;
      &lt;td&gt;2022-05&lt;/td&gt;
      &lt;td&gt;115.000000&lt;/td&gt;
      &lt;td&gt;115.000000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;44882&lt;/th&gt;
      &lt;td&gt;53854962&lt;/td&gt;
      &lt;td&gt;2022-06&lt;/td&gt;
      &lt;td&gt;115.000000&lt;/td&gt;
      &lt;td&gt;115.000000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;44883&lt;/th&gt;
      &lt;td&gt;53854962&lt;/td&gt;
      &lt;td&gt;2022-07&lt;/td&gt;
      &lt;td&gt;115.000000&lt;/td&gt;
      &lt;td&gt;115.000000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;44884&lt;/th&gt;
      &lt;td&gt;53854962&lt;/td&gt;
      &lt;td&gt;2022-08&lt;/td&gt;
      &lt;td&gt;115.000000&lt;/td&gt;
      &lt;td&gt;115.000000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;44885&lt;/th&gt;
      &lt;td&gt;53854962&lt;/td&gt;
      &lt;td&gt;2022-09&lt;/td&gt;
      &lt;td&gt;115.000000&lt;/td&gt;
      &lt;td&gt;115.000000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;44886&lt;/th&gt;
      &lt;td&gt;53854962&lt;/td&gt;
      &lt;td&gt;2022-10&lt;/td&gt;
      &lt;td&gt;115.000000&lt;/td&gt;
      &lt;td&gt;115.000000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;44887&lt;/th&gt;
      &lt;td&gt;53854962&lt;/td&gt;
      &lt;td&gt;2022-11&lt;/td&gt;
      &lt;td&gt;115.000000&lt;/td&gt;
      &lt;td&gt;115.000000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;44888&lt;/th&gt;
      &lt;td&gt;53854962&lt;/td&gt;
      &lt;td&gt;2022-12&lt;/td&gt;
      &lt;td&gt;115.000000&lt;/td&gt;
      &lt;td&gt;115.000000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;44863&lt;/th&gt;
      &lt;td&gt;53837654&lt;/td&gt;
      &lt;td&gt;2021-12&lt;/td&gt;
      &lt;td&gt;184.133333&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;44864&lt;/th&gt;
      &lt;td&gt;53837654&lt;/td&gt;
      &lt;td&gt;2022-01&lt;/td&gt;
      &lt;td&gt;148.741935&lt;/td&gt;
      &lt;td&gt;184.133333&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;We can perform &lt;strong&gt;cumulative&lt;/strong&gt; operations using the &lt;code&gt;expanding()&lt;/code&gt; function&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;temp[&#39;avg_cum_price&#39;] = temp[&#39;price&#39;].expanding().mean()
temp.head(15)
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;listing_id&lt;/th&gt;
      &lt;th&gt;date&lt;/th&gt;
      &lt;th&gt;price&lt;/th&gt;
      &lt;th&gt;price1&lt;/th&gt;
      &lt;th&gt;avg_cum_price&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;44876&lt;/th&gt;
      &lt;td&gt;53854962&lt;/td&gt;
      &lt;td&gt;2021-12&lt;/td&gt;
      &lt;td&gt;147.400000&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;147.400000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;44877&lt;/th&gt;
      &lt;td&gt;53854962&lt;/td&gt;
      &lt;td&gt;2022-01&lt;/td&gt;
      &lt;td&gt;137.645161&lt;/td&gt;
      &lt;td&gt;147.400000&lt;/td&gt;
      &lt;td&gt;142.522581&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;44878&lt;/th&gt;
      &lt;td&gt;53854962&lt;/td&gt;
      &lt;td&gt;2022-02&lt;/td&gt;
      &lt;td&gt;124.642857&lt;/td&gt;
      &lt;td&gt;137.645161&lt;/td&gt;
      &lt;td&gt;136.562673&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;44879&lt;/th&gt;
      &lt;td&gt;53854962&lt;/td&gt;
      &lt;td&gt;2022-03&lt;/td&gt;
      &lt;td&gt;285.096774&lt;/td&gt;
      &lt;td&gt;124.642857&lt;/td&gt;
      &lt;td&gt;173.696198&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;44880&lt;/th&gt;
      &lt;td&gt;53854962&lt;/td&gt;
      &lt;td&gt;2022-04&lt;/td&gt;
      &lt;td&gt;115.000000&lt;/td&gt;
      &lt;td&gt;285.096774&lt;/td&gt;
      &lt;td&gt;161.956959&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;44881&lt;/th&gt;
      &lt;td&gt;53854962&lt;/td&gt;
      &lt;td&gt;2022-05&lt;/td&gt;
      &lt;td&gt;115.000000&lt;/td&gt;
      &lt;td&gt;115.000000&lt;/td&gt;
      &lt;td&gt;154.130799&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;44882&lt;/th&gt;
      &lt;td&gt;53854962&lt;/td&gt;
      &lt;td&gt;2022-06&lt;/td&gt;
      &lt;td&gt;115.000000&lt;/td&gt;
      &lt;td&gt;115.000000&lt;/td&gt;
      &lt;td&gt;148.540685&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;44883&lt;/th&gt;
      &lt;td&gt;53854962&lt;/td&gt;
      &lt;td&gt;2022-07&lt;/td&gt;
      &lt;td&gt;115.000000&lt;/td&gt;
      &lt;td&gt;115.000000&lt;/td&gt;
      &lt;td&gt;144.348099&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;44884&lt;/th&gt;
      &lt;td&gt;53854962&lt;/td&gt;
      &lt;td&gt;2022-08&lt;/td&gt;
      &lt;td&gt;115.000000&lt;/td&gt;
      &lt;td&gt;115.000000&lt;/td&gt;
      &lt;td&gt;141.087199&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;44885&lt;/th&gt;
      &lt;td&gt;53854962&lt;/td&gt;
      &lt;td&gt;2022-09&lt;/td&gt;
      &lt;td&gt;115.000000&lt;/td&gt;
      &lt;td&gt;115.000000&lt;/td&gt;
      &lt;td&gt;138.478479&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;44886&lt;/th&gt;
      &lt;td&gt;53854962&lt;/td&gt;
      &lt;td&gt;2022-10&lt;/td&gt;
      &lt;td&gt;115.000000&lt;/td&gt;
      &lt;td&gt;115.000000&lt;/td&gt;
      &lt;td&gt;136.344072&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;44887&lt;/th&gt;
      &lt;td&gt;53854962&lt;/td&gt;
      &lt;td&gt;2022-11&lt;/td&gt;
      &lt;td&gt;115.000000&lt;/td&gt;
      &lt;td&gt;115.000000&lt;/td&gt;
      &lt;td&gt;134.565399&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;44888&lt;/th&gt;
      &lt;td&gt;53854962&lt;/td&gt;
      &lt;td&gt;2022-12&lt;/td&gt;
      &lt;td&gt;115.000000&lt;/td&gt;
      &lt;td&gt;115.000000&lt;/td&gt;
      &lt;td&gt;133.060369&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;44863&lt;/th&gt;
      &lt;td&gt;53837654&lt;/td&gt;
      &lt;td&gt;2021-12&lt;/td&gt;
      &lt;td&gt;184.133333&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;136.708438&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;44864&lt;/th&gt;
      &lt;td&gt;53837654&lt;/td&gt;
      &lt;td&gt;2022-01&lt;/td&gt;
      &lt;td&gt;148.741935&lt;/td&gt;
      &lt;td&gt;184.133333&lt;/td&gt;
      &lt;td&gt;137.510671&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;To perform &lt;strong&gt;cumulative operations within a group&lt;/strong&gt;, we can combine &lt;code&gt;expanding()&lt;/code&gt; with &lt;code&gt;groupby()&lt;/code&gt;. Since groups with not enough observations get dropped, we need to merge the dataset back.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;temp.groupby(&#39;listing_id&#39;)[&#39;price&#39;].expanding().mean().reset_index(level=0).head(15)
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;listing_id&lt;/th&gt;
      &lt;th&gt;price&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;42196&lt;/td&gt;
      &lt;td&gt;68.000000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;42196&lt;/td&gt;
      &lt;td&gt;68.000000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;42196&lt;/td&gt;
      &lt;td&gt;68.000000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;42196&lt;/td&gt;
      &lt;td&gt;68.000000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;42196&lt;/td&gt;
      &lt;td&gt;68.000000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;5&lt;/th&gt;
      &lt;td&gt;42196&lt;/td&gt;
      &lt;td&gt;68.000000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;6&lt;/th&gt;
      &lt;td&gt;42196&lt;/td&gt;
      &lt;td&gt;68.000000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;7&lt;/th&gt;
      &lt;td&gt;42196&lt;/td&gt;
      &lt;td&gt;68.000000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;8&lt;/th&gt;
      &lt;td&gt;42196&lt;/td&gt;
      &lt;td&gt;68.000000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;9&lt;/th&gt;
      &lt;td&gt;42196&lt;/td&gt;
      &lt;td&gt;68.000000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;10&lt;/th&gt;
      &lt;td&gt;42196&lt;/td&gt;
      &lt;td&gt;68.000000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;11&lt;/th&gt;
      &lt;td&gt;42196&lt;/td&gt;
      &lt;td&gt;68.000000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;12&lt;/th&gt;
      &lt;td&gt;42196&lt;/td&gt;
      &lt;td&gt;68.000000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;13&lt;/th&gt;
      &lt;td&gt;46352&lt;/td&gt;
      &lt;td&gt;29.333333&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;14&lt;/th&gt;
      &lt;td&gt;46352&lt;/td&gt;
      &lt;td&gt;29.311828&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;If we want to perform an operation over a &lt;strong&gt;rolling window&lt;/strong&gt;, we can use the &lt;code&gt;rolling()&lt;/code&gt; function&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;temp[&#39;avg3_price&#39;] = temp[&#39;price&#39;].rolling(3).mean()
temp.head(15)
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;listing_id&lt;/th&gt;
      &lt;th&gt;date&lt;/th&gt;
      &lt;th&gt;price&lt;/th&gt;
      &lt;th&gt;price1&lt;/th&gt;
      &lt;th&gt;avg_cum_price&lt;/th&gt;
      &lt;th&gt;avg3_price&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;44876&lt;/th&gt;
      &lt;td&gt;53854962&lt;/td&gt;
      &lt;td&gt;2021-12&lt;/td&gt;
      &lt;td&gt;147.400000&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;147.400000&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;44877&lt;/th&gt;
      &lt;td&gt;53854962&lt;/td&gt;
      &lt;td&gt;2022-01&lt;/td&gt;
      &lt;td&gt;137.645161&lt;/td&gt;
      &lt;td&gt;147.400000&lt;/td&gt;
      &lt;td&gt;142.522581&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;44878&lt;/th&gt;
      &lt;td&gt;53854962&lt;/td&gt;
      &lt;td&gt;2022-02&lt;/td&gt;
      &lt;td&gt;124.642857&lt;/td&gt;
      &lt;td&gt;137.645161&lt;/td&gt;
      &lt;td&gt;136.562673&lt;/td&gt;
      &lt;td&gt;136.562673&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;44879&lt;/th&gt;
      &lt;td&gt;53854962&lt;/td&gt;
      &lt;td&gt;2022-03&lt;/td&gt;
      &lt;td&gt;285.096774&lt;/td&gt;
      &lt;td&gt;124.642857&lt;/td&gt;
      &lt;td&gt;173.696198&lt;/td&gt;
      &lt;td&gt;182.461598&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;44880&lt;/th&gt;
      &lt;td&gt;53854962&lt;/td&gt;
      &lt;td&gt;2022-04&lt;/td&gt;
      &lt;td&gt;115.000000&lt;/td&gt;
      &lt;td&gt;285.096774&lt;/td&gt;
      &lt;td&gt;161.956959&lt;/td&gt;
      &lt;td&gt;174.913210&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;44881&lt;/th&gt;
      &lt;td&gt;53854962&lt;/td&gt;
      &lt;td&gt;2022-05&lt;/td&gt;
      &lt;td&gt;115.000000&lt;/td&gt;
      &lt;td&gt;115.000000&lt;/td&gt;
      &lt;td&gt;154.130799&lt;/td&gt;
      &lt;td&gt;171.698925&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;44882&lt;/th&gt;
      &lt;td&gt;53854962&lt;/td&gt;
      &lt;td&gt;2022-06&lt;/td&gt;
      &lt;td&gt;115.000000&lt;/td&gt;
      &lt;td&gt;115.000000&lt;/td&gt;
      &lt;td&gt;148.540685&lt;/td&gt;
      &lt;td&gt;115.000000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;44883&lt;/th&gt;
      &lt;td&gt;53854962&lt;/td&gt;
      &lt;td&gt;2022-07&lt;/td&gt;
      &lt;td&gt;115.000000&lt;/td&gt;
      &lt;td&gt;115.000000&lt;/td&gt;
      &lt;td&gt;144.348099&lt;/td&gt;
      &lt;td&gt;115.000000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;44884&lt;/th&gt;
      &lt;td&gt;53854962&lt;/td&gt;
      &lt;td&gt;2022-08&lt;/td&gt;
      &lt;td&gt;115.000000&lt;/td&gt;
      &lt;td&gt;115.000000&lt;/td&gt;
      &lt;td&gt;141.087199&lt;/td&gt;
      &lt;td&gt;115.000000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;44885&lt;/th&gt;
      &lt;td&gt;53854962&lt;/td&gt;
      &lt;td&gt;2022-09&lt;/td&gt;
      &lt;td&gt;115.000000&lt;/td&gt;
      &lt;td&gt;115.000000&lt;/td&gt;
      &lt;td&gt;138.478479&lt;/td&gt;
      &lt;td&gt;115.000000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;44886&lt;/th&gt;
      &lt;td&gt;53854962&lt;/td&gt;
      &lt;td&gt;2022-10&lt;/td&gt;
      &lt;td&gt;115.000000&lt;/td&gt;
      &lt;td&gt;115.000000&lt;/td&gt;
      &lt;td&gt;136.344072&lt;/td&gt;
      &lt;td&gt;115.000000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;44887&lt;/th&gt;
      &lt;td&gt;53854962&lt;/td&gt;
      &lt;td&gt;2022-11&lt;/td&gt;
      &lt;td&gt;115.000000&lt;/td&gt;
      &lt;td&gt;115.000000&lt;/td&gt;
      &lt;td&gt;134.565399&lt;/td&gt;
      &lt;td&gt;115.000000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;44888&lt;/th&gt;
      &lt;td&gt;53854962&lt;/td&gt;
      &lt;td&gt;2022-12&lt;/td&gt;
      &lt;td&gt;115.000000&lt;/td&gt;
      &lt;td&gt;115.000000&lt;/td&gt;
      &lt;td&gt;133.060369&lt;/td&gt;
      &lt;td&gt;115.000000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;44863&lt;/th&gt;
      &lt;td&gt;53837654&lt;/td&gt;
      &lt;td&gt;2021-12&lt;/td&gt;
      &lt;td&gt;184.133333&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;136.708438&lt;/td&gt;
      &lt;td&gt;138.044444&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;44864&lt;/th&gt;
      &lt;td&gt;53837654&lt;/td&gt;
      &lt;td&gt;2022-01&lt;/td&gt;
      &lt;td&gt;148.741935&lt;/td&gt;
      &lt;td&gt;184.133333&lt;/td&gt;
      &lt;td&gt;137.510671&lt;/td&gt;
      &lt;td&gt;149.291756&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Resampling Methods</title>
      <link>https://matteocourthoud.github.io/course/ml-econ/04_crossvalidation/</link>
      <pubDate>Wed, 09 Mar 2022 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/course/ml-econ/04_crossvalidation/</guid>
      <description>&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Remove warnings
import warnings
warnings.filterwarnings(&#39;ignore&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Import
import pandas as pd
import numpy as np
import seaborn as sns
import time

from numpy.linalg import inv
from numpy.random import normal
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import train_test_split, LeaveOneOut, KFold, cross_val_score
from sklearn.preprocessing import PolynomialFeatures
from sklearn.utils import resample
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Import matplotlib for graphs
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import axes3d

# Set global parameters
%matplotlib inline
plt.style.use(&#39;seaborn-white&#39;)
plt.rcParams[&#39;lines.linewidth&#39;] = 3
plt.rcParams[&#39;figure.figsize&#39;] = (10,6)
plt.rcParams[&#39;figure.titlesize&#39;] = 20
plt.rcParams[&#39;axes.titlesize&#39;] = 18
plt.rcParams[&#39;axes.labelsize&#39;] = 14
plt.rcParams[&#39;legend.fontsize&#39;] = 14
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Resampling methods involve repeatedly drawing samples from a training set and refitting a model of interest on each sample in order to obtain additional information about the fitted model.&lt;/p&gt;
&lt;h2 id=&#34;41-cross-validation&#34;&gt;4.1 Cross-Validation&lt;/h2&gt;
&lt;p&gt;Cross-validation can be used to estimate the test error associated with a given statistical learning method in order to evaluate its performance, or to select the appropriate level of flexibility. The process of evaluating a model’s performance is known as &lt;strong&gt;model assessment&lt;/strong&gt;, whereas the process of selecting the proper level of flexibility for a model is known as &lt;strong&gt;model selection&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s use the &lt;code&gt;auto&lt;/code&gt; dataset we have used for nonparametric models.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Load car dataset
df1 = pd.read_csv(&#39;data/Auto.csv&#39;, na_values=&#39;?&#39;).dropna()
df1.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;mpg&lt;/th&gt;
      &lt;th&gt;cylinders&lt;/th&gt;
      &lt;th&gt;displacement&lt;/th&gt;
      &lt;th&gt;horsepower&lt;/th&gt;
      &lt;th&gt;weight&lt;/th&gt;
      &lt;th&gt;acceleration&lt;/th&gt;
      &lt;th&gt;year&lt;/th&gt;
      &lt;th&gt;origin&lt;/th&gt;
      &lt;th&gt;name&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;18.0&lt;/td&gt;
      &lt;td&gt;8&lt;/td&gt;
      &lt;td&gt;307.0&lt;/td&gt;
      &lt;td&gt;130&lt;/td&gt;
      &lt;td&gt;3504&lt;/td&gt;
      &lt;td&gt;12.0&lt;/td&gt;
      &lt;td&gt;70&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;chevrolet chevelle malibu&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;15.0&lt;/td&gt;
      &lt;td&gt;8&lt;/td&gt;
      &lt;td&gt;350.0&lt;/td&gt;
      &lt;td&gt;165&lt;/td&gt;
      &lt;td&gt;3693&lt;/td&gt;
      &lt;td&gt;11.5&lt;/td&gt;
      &lt;td&gt;70&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;buick skylark 320&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;18.0&lt;/td&gt;
      &lt;td&gt;8&lt;/td&gt;
      &lt;td&gt;318.0&lt;/td&gt;
      &lt;td&gt;150&lt;/td&gt;
      &lt;td&gt;3436&lt;/td&gt;
      &lt;td&gt;11.0&lt;/td&gt;
      &lt;td&gt;70&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;plymouth satellite&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;16.0&lt;/td&gt;
      &lt;td&gt;8&lt;/td&gt;
      &lt;td&gt;304.0&lt;/td&gt;
      &lt;td&gt;150&lt;/td&gt;
      &lt;td&gt;3433&lt;/td&gt;
      &lt;td&gt;12.0&lt;/td&gt;
      &lt;td&gt;70&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;amc rebel sst&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;17.0&lt;/td&gt;
      &lt;td&gt;8&lt;/td&gt;
      &lt;td&gt;302.0&lt;/td&gt;
      &lt;td&gt;140&lt;/td&gt;
      &lt;td&gt;3449&lt;/td&gt;
      &lt;td&gt;10.5&lt;/td&gt;
      &lt;td&gt;70&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;ford torino&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;h3 id=&#34;the-validation-set-approach&#34;&gt;The Validation Set Approach&lt;/h3&gt;
&lt;p&gt;Suppose that we would like to estimate the test error associated with fitting a particular statistical learning method on a set of observations. The validation set approach is a very simple strategy for this task. It involves randomly dividing the available set of observations into two parts&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;a &lt;strong&gt;training set&lt;/strong&gt; and&lt;/li&gt;
&lt;li&gt;a &lt;strong&gt;validation set&lt;/strong&gt; or hold-out set&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The model is fit on the training set, and the fitted model is used to predict the responses for the observations in the validation set. The resulting validation set error rate-typically assessed using MSE in the case of a quantitative response—provides an estimate of the test error rate.&lt;/p&gt;
&lt;p&gt;In the following example we are are going to compute the MSE fit polynomial of different order (one to ten). We are going to split the data 50-50 across training and test set.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Cross-validation function for polynomials
def cv_poly(X, y, p_order, r_states, t_prop):
    start = time.time()
    
    # Init scores
    scores = np.zeros((p_order.size,r_states.size))
    
    # Generate 10 random splits of the dataset
    for j in r_states:
        
        # Split sample in train and test
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=t_prop, random_state=j)
        
            
        # For every polynomial degree
        for i in p_order:

            # Generate polynomial
            X_train_poly = PolynomialFeatures(i+1).fit_transform(X_train)
            X_test_poly = PolynomialFeatures(i+1).fit_transform(X_test)

            # Fit regression                                                                    
            ols = LinearRegression().fit(X_train_poly, y_train)
            pred = ols.predict(X_test_poly)
            scores[i,j]= mean_squared_error(y_test, pred)
    print(&#39;Time elapsed: %.4f seconds&#39; % (time.time()-start))
    return scores
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Init
t_prop = 0.5
p_order = np.arange(10)
r_states = np.arange(10)

# Get X,y 
X = df1.horsepower.values.reshape(-1,1)
y = df1.mpg.ravel()

# Compute scores
cv_scores = cv_poly(X, y, p_order, r_states, t_prop)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Time elapsed: 0.0277 seconds
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s test the score for polynomials of different orders.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Figure 5.2
def make_figure_5_2():
    
    # Init
    fig, (ax1, ax2) = plt.subplots(1,2,figsize=(14,6))
    fig.suptitle(&#39;Figure 5.2&#39;)

    # Left plot (first split)
    ax1.plot(p_order+1,cv_scores[:,0], &#39;-o&#39;)
    ax1.set_title(&#39;Random split of the data set&#39;)

    # Right plot (all splits)
    ax2.plot(p_order+1,cv_scores)
    ax2.set_title(&#39;10 random splits of the data set&#39;)

    for ax in fig.axes:
        ax.set_ylabel(&#39;Mean Squared Error&#39;)
        ax.set_ylim(15,30)
        ax.set_xlabel(&#39;Degree of Polynomial&#39;)
        ax.set_xlim(0.5,10.5)
        ax.set_xticks(range(2,11,2));
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;make_figure_5_2()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/04_crossvalidation_15_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;This figure illustrates a &lt;strong&gt;first drawback&lt;/strong&gt; of the validation approach: the estimate of the test error rate can be highly variable, depending on precisely which observations are included in the training set and which observations are included in the validation set.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;second drawback&lt;/strong&gt; of the validation approach is that only a subset of the observations—those that are included in the training set rather than in the validation set—are used to fit the model. Since statistical methods tend to per- form worse when trained on fewer observations, this suggests that the validation set error rate may tend to overestimate the test error rate for the model fit on the entire data set.&lt;/p&gt;
&lt;h3 id=&#34;leave-one-out-cross-validation&#34;&gt;Leave-One-Out Cross-Validation&lt;/h3&gt;
&lt;p&gt;Leave-one-out cross-validation (LOOCV) attempts to address that method’s drawbacks.&lt;/p&gt;
&lt;p&gt;Like the validation set approach, LOOCV involves splitting the set of observations into two parts. However, instead of creating two subsets of comparable size, a single observation $i$ is used for the validation set, and the remaining $n-1$ observations make up the training set. The statistical learning method is fit on the $n−1$ training observations and the MSE is computed using the excluded observation $i$. The procedure is repeated $n$ times, for $i=1,&amp;hellip;,n$.&lt;/p&gt;
&lt;p&gt;The LOOCV estimate for the test MSE is the average of these $n$ test error estimates:&lt;/p&gt;
&lt;p&gt;$$
\mathrm{CV}&lt;em&gt;{(n)}=\frac{1}{n} \sum&lt;/em&gt;{i=1}^{n} \mathrm{MSE}_{i}
$$&lt;/p&gt;
&lt;p&gt;LOOCV has a couple of major &lt;strong&gt;advantages&lt;/strong&gt; over the validation set approach.&lt;/p&gt;
&lt;p&gt;First, it has far less bias. In LOOCV, we repeatedly fit the statistical learning method using training sets that contain $n − 1$ observations, almost as many as are in the entire data set. However, this also means that LOOCV is more computationally intense.&lt;/p&gt;
&lt;p&gt;Second, in contrast to the validation approach which will yield different results when applied repeatedly due to randomness in the training/validation set splits, performing LOOCV multiple times will always yield the same results: there is no randomness in the training/validation set splits.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# LeaveOneOut CV function for polynomials
def loo_cv_poly(X, y, p_order):
    start = time.time()
    
    # Init
    loo = LeaveOneOut().get_n_splits(y)
    loo_scores = np.zeros((p_order.size,1))
    
    # For every polynomial degree
    for i in p_order:
        # Generate polynomial
        X_poly = PolynomialFeatures(i+1).fit_transform(X)

        # Get score
        loo_scores[i] = cross_val_score(LinearRegression(), X_poly, y, cv=loo, scoring=&#39;neg_mean_squared_error&#39;).mean()
        
    print(&#39;Time elapsed: %.4f seconds&#39; % (time.time()-start))
    return loo_scores
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s compare the validation set approach against LOO in terms of computational time.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Validation set approach
cv_scores = cv_poly(X, y, p_order, r_states, t_prop)
    
# Leave One Out CV
loo_scores = loo_cv_poly(X, y, p_order)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Time elapsed: 0.0270 seconds
Time elapsed: 1.1495 seconds
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As expected, LOOCV is much more computationally intense. Even accounting for the fact that we repeat every the validation set approach 10 times.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s now compare them in terms of accuracy in minimizing the MSE.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Make new figure 1
def make_new_figure_1():

    # Init
    fig, ax = plt.subplots(1,1, figsize=(7,6))

    # Left plot
    ax.plot(p_order+1, np.array(loo_scores)*-1, &#39;-o&#39;, label=&#39;LOOCV&#39;)
    ax.plot(p_order+1, np.mean(cv_scores, axis=1), &#39;-o&#39;, c=&#39;orange&#39;, label=&#39;Standard CV&#39;)
    ax.set_ylabel(&#39;Mean Squared Error&#39;); ax.set_xlabel(&#39;Degree of Polynomial&#39;);
    ax.set_ylim(15,30); ax.set_xlim(0.5,10.5);
    ax.set_xticks(range(2,11,2));
    ax.legend();
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;make_new_figure_1()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/04_crossvalidation_26_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;With least squares linear or polynomial regression, an amazing shortcut makes the cost of LOOCV the same as that of a single model fit! The following formula holds:&lt;/p&gt;
&lt;p&gt;$$
\mathrm{CV}&lt;em&gt;{(n)}=\frac{1}{n} \sum&lt;/em&gt;{i=1}^{n}\left(\frac{y_{i}-\hat{y}&lt;em&gt;{i}}{1-h&lt;/em&gt;{i}}\right)^{2}
$$&lt;/p&gt;
&lt;p&gt;where $\hat y_i$ is the $i^{th}$ fitted value from the original least squares fit, and $h_i$ is the leverage of observation $i$.&lt;/p&gt;
&lt;h3 id=&#34;k-fold-cross-validation&#34;&gt;k-Fold Cross-Validation&lt;/h3&gt;
&lt;p&gt;An alternative to LOOCV is k-fold CV. This approach involves the following steps:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Randomly dividing the set of observations into $k$ groups, or folds, of approximately equal size.&lt;/li&gt;
&lt;li&gt;The first fold is treated as a validation set, and the method is fit on the remaining $k − 1$ folds.&lt;/li&gt;
&lt;li&gt;The mean squared error, MSE1, is then computed on the observations in the held-out fold.&lt;/li&gt;
&lt;li&gt;Steps (1)-(3) are repeated $k$ times; each time, a different group of observations is treated as a validation set.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The k-fold CV estimate is computed by averaging these values&lt;/p&gt;
&lt;p&gt;$$
\mathrm{CV}&lt;em&gt;{(k)}=\frac{1}{k} \sum&lt;/em&gt;{i=1}^{k} \mathrm{MSE}_{i}
$$&lt;/p&gt;
&lt;p&gt;LOOCV is a special case of k-fold CV in which $k$ is set to equal $n$. In practice, one typically performs k-fold CV using $k = 5$ or $k = 10$.&lt;/p&gt;
&lt;p&gt;The most obvious &lt;strong&gt;advantage&lt;/strong&gt; is computational. LOOCV requires fitting the statistical learning method $n$ times, while k-fold CV only requires $k$ splits.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# 10fold CV function for polynomials
def k10_cv_poly(X, y, p_order, r_states, folds):
    start = time.time()
    
    # Init
    k10_scores = np.zeros((p_order.size,r_states.size))

    # Generate 10 random splits of the dataset
    for j in r_states:

        # For every polynomial degree
        for i in p_order:

            # Generate polynomial
            X_poly = PolynomialFeatures(i+1).fit_transform(X)

            # Split sample in train and test
            kf10 = KFold(n_splits=folds, shuffle=True, random_state=j)
            k10_scores[i,j] = cross_val_score(LinearRegression(), X_poly, y, cv=kf10, 
                                               scoring=&#39;neg_mean_squared_error&#39;).mean()  
    
    print(&#39;Time elapsed: %.4f seconds&#39; % (time.time()-start))
    return k10_scores
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s now compare 10 fold cross-validation with LOO in terms of computational time.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Leave One Out CV
loo_scores = loo_cv_poly(X, y, p_order)
    
# 10-fold CV
folds = 10
k10_scores = k10_cv_poly(X, y, p_order, r_states, folds)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Time elapsed: 1.1153 seconds
Time elapsed: 0.3078 seconds
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Indeed we see that the LOOCV approach is more computationally intense. Even accounting for the fact that we repeat every 10-fold cross-validation 10 times.&lt;/p&gt;
&lt;p&gt;We can now compare all the methods in terms of accuracy.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Figure 5.4
def make_figure_5_4():

    fig, (ax1, ax2, ax3) = plt.subplots(1,3,figsize=(17,5))
    fig.suptitle(&#39;Figure 5.4&#39;)

    # Left plot
    ax1.plot(p_order+1, np.array(loo_scores)*-1, &#39;-o&#39;)
    ax1.set_title(&#39;LOOCV&#39;, fontsize=12)

    # Center plot
    ax2.plot(p_order+1,k10_scores*-1)
    ax2.set_title(&#39;10-fold CV&#39;, fontsize=12)

    # Right plot
    ax3.plot(p_order+1, np.array(loo_scores)*-1, &#39;-o&#39;, label=&#39;LOOCV&#39;)
    ax3.plot(p_order+1, np.mean(cv_scores, axis=1), label=&#39;Standard CV&#39;)
    ax3.plot(p_order+1,np.mean(k10_scores,axis=1)*-1, label=&#39;10-fold CV&#39;)
    ax3.set_title(&#39;Comparison&#39;, fontsize=12);
    ax3.legend();

    for ax in fig.axes:
        ax.set_ylabel(&#39;Mean Squared Error&#39;)
        ax.set_ylim(15,30)
        ax.set_xlabel(&#39;Degree of Polynomial&#39;)
        ax.set_xlim(0.5,10.5)
        ax.set_xticks(range(2,11,2));
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;make_figure_5_4()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/04_crossvalidation_37_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;10-fold cross-validation outputs a very similar MSE with respect to LOOCV, but with considerably less computational time.&lt;/p&gt;
&lt;h2 id=&#34;42-the-bootstrap&#34;&gt;4.2 The Bootstrap&lt;/h2&gt;
&lt;p&gt;The bootstrap is a widely applicable and extremely powerful statistical tool that can be used to &lt;strong&gt;quantify the uncertainty&lt;/strong&gt; associated with a given estimator or statistical learning method. In the specific case of linear regression, this is not particularly useful since there exist a formula for the standard errors. However, there are many models (almost all actually) for which there exists no closed for solution to the estimator variance.&lt;/p&gt;
&lt;p&gt;In pricinple, we would like to draw independent samples from the true data generating process and assessing the uncertainty of an estimator by comparing its values across the different samples. However, this is clearly unfeasible since we do not know the true data generating process.&lt;/p&gt;
&lt;p&gt;With the bootstrap, rather than repeatedly obtaining independent data sets from the population, we instead obtain distinct data sets by repeatedly sampling observations from the original data set. The power of the bootstrap lies in the fact that it can be easily applied to a wide range of statistical learning methods, including some for which a measure of variability is otherwise difficult to obtain and is not automatically output by statistical software.&lt;/p&gt;
&lt;p&gt;We are now going to assess its usefulness through simulation. Take the following model:&lt;/p&gt;
&lt;p&gt;$$
y_i = \beta_0 \cdot x_i + \varepsilon_i
$$&lt;/p&gt;
&lt;p&gt;where $\beta_0 = 0.6$ and $\varepsilon \sim N(0,1)$. We are now going to assess the variance of the OLS estimator $\hat \beta$ with the standard formula, simulating different samples and with bootstrap.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Set seed
np.random.seed(1)

# Init
simulations = 1000
N = 1000
beta_0 = 0.6
beta_sim = np.zeros((simulations,1))

# Generate X
X = normal(0,3,N).reshape(-1,1)

# Loop over simulations
for i in range(simulations):
    
    # Generate y
    e = normal(0,1,N).reshape(-1,1)
    y = beta_0*X + e
    
    # Estimate beta OLS
    beta_sim[i] = inv(X.T @ X) @ X.T @ y
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Init Bootstrap
beta_boot = np.zeros((simulations,1))

# Loop over simulations
for i in range(simulations):
    
    # Sample y
    X_sample, y_sample = resample(X, y, random_state=i)
    
    # Estimate beta OLS
    beta_boot[i] = inv(X_sample.T @ X_sample) @ X_sample.T @ y_sample
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can first compare the means.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Print means
print(&#39;True value      : %.4f&#39; % beta_0)
print(&#39;Mean Simulations: %.4f&#39; % np.mean(beta_sim))
print(&#39;Mean One Sample : %.4f&#39; % beta_sim[-1])
print(&#39;Mean Boostrap   : %.4f&#39; % np.mean(beta_boot))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;True value      : 0.6000
Mean Simulations: 0.6003
Mean One Sample : 0.5815
Mean Boostrap   : 0.5816
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The mean of the bootstrap estimtor is quite off. But this is not its actual purpose: it is designed to assess the uncertainty of an estimator, not its value.&lt;/p&gt;
&lt;p&gt;Now we compare the variances.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Print variances
print(&#39;True std       : %.6f&#39; % np.sqrt(inv(X.T @ X)))
print(&#39;Std Simulations: %.6f&#39; % np.std(beta_sim))
print(&#39;Std One Sample : %.6f&#39; % np.sqrt(inv(X.T @ X) * np.var(y - beta_sim[-1]*X)))
print(&#39;Std Boostrap   : %.6f&#39; % np.std(beta_boot))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;True std       : 0.010737
Std Simulations: 0.010830
Std One Sample : 0.010536
Std Boostrap   : 0.010812
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Bootstrap gets as close to the true standard deviation of the estimator as the simulation with the true data generating process. Impressive!&lt;/p&gt;
&lt;p&gt;We can now have a visual inspection.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Figure 5.10
def make_figure_5_10():

    fig, (ax1, ax2, ax3) = plt.subplots(1,3,figsize=(14,6))
    fig.suptitle(&#39;Figure 5.10&#39;)

    # Left plot
    ax1.hist(beta_sim, bins=10, edgecolor=&#39;black&#39;);
    ax1.axvline(x=beta_0, color=&#39;r&#39;, label=&#39;beta_0&#39;)
    ax1.set_xlabel(&#39;beta simulated&#39;);

    # Center plot
    ax2.hist(beta_boot, bins=10, color=&#39;orange&#39;, edgecolor=&#39;black&#39;);
    ax2.axvline(x=beta_0, color=&#39;r&#39;, label=&#39;beta_0&#39;)
    ax2.set_xlabel(&#39;beta bootstrap&#39;);

    # Right plot
    df_bootstrap = pd.DataFrame({&#39;simulated&#39;: beta_sim.ravel(), &#39;bootstrap&#39;:beta_boot.ravel()}, 
                                index=range(simulations))
    ax3 = sns.boxplot(data=df_bootstrap, width=0.5, linewidth=2);
    ax3.axhline(y=beta_0, color=&#39;r&#39;, label=&#39;beta_0&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;make_figure_5_10()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/04_crossvalidation_52_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;As we can see, the bootstrap is a powerful tool to assess the uncertainty of an estimator.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Inference</title>
      <link>https://matteocourthoud.github.io/course/metrics/04_inference/</link>
      <pubDate>Fri, 29 Oct 2021 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/course/metrics/04_inference/</guid>
      <description>&lt;h2 id=&#34;statistical-models&#34;&gt;Statistical Models&lt;/h2&gt;
&lt;h3 id=&#34;definition&#34;&gt;Definition&lt;/h3&gt;
&lt;p&gt;A &lt;strong&gt;statistical model&lt;/strong&gt; is a set of probability distributions
$\lbrace P \rbrace$.&lt;/p&gt;
&lt;p&gt;More precisely, a &lt;strong&gt;statistical model over data&lt;/strong&gt; $D \in \mathcal{D}$ is
a set of probability distribution over datasets $D$ which takes values
in $\mathcal{D}$.&lt;/p&gt;
&lt;p&gt;Suppose you have regression data $\lbrace x_i , y_i \rbrace _ {i=1}^N$
with $x_i \in \mathbb{R}^p$ and $y_i \in \mathbb{R}$. The statistical
model is&lt;/p&gt;
&lt;p&gt;$$
\Big\lbrace   P : y_i = f(x_i) + \varepsilon_i, \ x_i \sim F_x , \ \varepsilon_i \sim F _\varepsilon , \ \varepsilon_i \perp x_i , \ f \in C^2 (\mathbb{R}^p) \Big\rbrace
$$&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;In words&lt;/strong&gt;: the statistical model is the set of distributions $P$
such that an additive decomposition of $y_i$ as
$f(x_i) + \varepsilon_i$ exists for some $x_i$; where $f$ is twice
continuously differentiable.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;A &lt;strong&gt;data generating process&lt;/strong&gt; (DGP) is a single statistical distribution
over&lt;/p&gt;
&lt;h3 id=&#34;parametrization&#34;&gt;Parametrization&lt;/h3&gt;
&lt;p&gt;A statistical model parameterized by $\theta \in \Theta$ is &lt;strong&gt;well
specified&lt;/strong&gt; if the data generating process corresponds to some
$\theta_0$ and $\theta_0 \in \Theta$. Otherwise, the statistical model
is &lt;strong&gt;misspecified&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;A statistical model can be parametrized as
$\mathcal{F} = \lbrace P_\theta \rbrace _ {\lbrace \theta \in \Theta \rbrace }$.&lt;/p&gt;
&lt;p&gt;We can divide statistical models into 3 classes&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;h2 id=&#34;parametric-the-stochastic-features-of-the-model-are-completly-specified-up-to-a-finite-dimensional-parameter-lbrace-p_theta-rbrace-_--lbrace-theta-in-theta-rbrace--with-theta-subseteq-mathbbrk-kinfty&#34;&gt;&lt;strong&gt;Parametric&lt;/strong&gt;: the stochastic features of the model are completly specified up to a finite dimensional parameter: $\lbrace P_\theta \rbrace _ { \lbrace \theta \in \Theta \rbrace }$ with $\Theta \subseteq \mathbb{R}^k, k&amp;lt;\infty$;&lt;/h2&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Semiparametric&lt;/strong&gt;: it is a partially specified model, e.g.,
$\lbrace P_\theta \rbrace _ { \lbrace \theta \in \Theta, \gamma \in \Gamma \rbrace }$
with $\Theta$ of finite dimension and $\Gamma$ of infinite
dimension;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Non parametric&lt;/strong&gt;: there is no finite dimensional component of the
model.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;estimation&#34;&gt;Estimation&lt;/h3&gt;
&lt;p&gt;Let $\mathcal{D}$ be the set of possible data realizations. Let
$D \in \mathcal{D}$ be your data. Let $\mathcal{F}$ be a statistical
model indexed by some parameter $\theta \in \Theta$. An &lt;strong&gt;estimator&lt;/strong&gt; is
a map $$
\mathcal{D} \to \mathcal{F} \quad , \quad  D \mapsto \hat{\theta}
$$&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;In words&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;An estimator is a map from the set of data realizations to the set
of statistical models.&lt;/li&gt;
&lt;li&gt;It takes as inputs a dataset $D$ and outputs a parameter estimate
$\hat \theta$.&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;inference&#34;&gt;Inference&lt;/h3&gt;
&lt;p&gt;Let $\alpha &amp;gt; 0$ be a small tolerance. Statistical &lt;strong&gt;inference&lt;/strong&gt; is a
map into subsets of $\mathcal{F}$ given by $$
\mathcal{D} \to \mathcal{G} \subseteq \mathcal{F}: \min _ \theta P_\theta (\mathcal{G} | \theta \in \mathcal{G}) \geq 1-\alpha
$$&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;In words&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Inference maps datasets into sets of models&lt;/li&gt;
&lt;li&gt;The set contains only models that generate the observed data with
high probability&lt;/li&gt;
&lt;li&gt;I.e. at least $1-\alpha$&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;hypotesis-testing&#34;&gt;Hypotesis Testing&lt;/h2&gt;
&lt;h3 id=&#34;hypothesis&#34;&gt;Hypothesis&lt;/h3&gt;
&lt;p&gt;A &lt;strong&gt;statistical hypothesis&lt;/strong&gt; $H_0$, is a subset of a statistical model,
$\mathcal K \subset \mathcal F$.&lt;/p&gt;
&lt;p&gt;If $\mathcal F$ is the statistical model and $\mathcal K$ is the
statistical hypothesis, we use the notation $H_0 : P \in \mathcal K$.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Example&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Common hypothesis are&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A single coefficient being equal to zero,
$\beta_k = c \in \mathbb R$&lt;/li&gt;
&lt;li&gt;Multiple linear combination of coefficients being equal to some
values: $\boldsymbol R&amp;rsquo; \beta = r \in \mathbb R^p$&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;test&#34;&gt;Test&lt;/h3&gt;
&lt;p&gt;A &lt;strong&gt;hypothesis test&lt;/strong&gt; $T$ is a map from the space of datasets to a
decision, rejection (0) or acceptance (1) $$
\mathcal D \to \lbrace 0, 1 \rbrace \quad, \quad D \mapsto T
$$&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Generally, we are interested in understanding whether it is likely
that data $D$ are drawn from a model $\mathcal K$ or not.&lt;/p&gt;
&lt;p&gt;A hypothesis test, $T$ is our tool for deciding whether the hypothesis
is consistent with the data.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$T(D) = 0 \to$ fail to reject $H_0$ and test inconclusive&lt;/li&gt;
&lt;li&gt;$T (D) = 1 \to$ reject $H_0$ and D is inconsistent with any
$P \in \mathcal K$&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;errors&#34;&gt;Errors&lt;/h3&gt;
&lt;p&gt;Let $\mathcal K \subset \mathcal F$ be a statistical hypothesis and $T$
a hypothesis test.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;A &lt;strong&gt;Type I error&lt;/strong&gt; is an event $T(D)=1$ under $P \in \mathcal K$.
&lt;ul&gt;
&lt;li&gt;In words: rejecting the null hypothesis, when it is is true&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;A &lt;strong&gt;Type II error&lt;/strong&gt; is an event $T(D)=0$ under $P \in \mathcal K^C$.
&lt;ul&gt;
&lt;li&gt;In words: not rejecting the null hypothesis, when it is false&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The corresponding probability of a type I error is called &lt;strong&gt;size&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;The corresponding probability of a type II error is called &lt;strong&gt;power&lt;/strong&gt;
(against the alternative P).&lt;/p&gt;
&lt;h3 id=&#34;type-i-error-and-test-size&#34;&gt;Type I Error and Test Size&lt;/h3&gt;
&lt;p&gt;Test &lt;strong&gt;size&lt;/strong&gt; is the probability of a Type I error, i.e. $$
\Pr \Big[ \text{ Reject } H_0 \Big| H_0 \text{ is true } \Big] = \Pr \Big[ T(D)=1 \Big| P \in \mathcal K \Big]
$$ A primary goal of test construction is to limit the incidence of Type
I error by bounding the size of the test.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;In the dominant approach to hypothesis testing the researcher
pre-selects a &lt;strong&gt;significance level&lt;/strong&gt; $\alpha \in (0,1)$ and then
selects the test so that its size is no larger than $\alpha$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;type-ii-error-and-power&#34;&gt;Type II Error and Power&lt;/h3&gt;
&lt;p&gt;Test &lt;strong&gt;power&lt;/strong&gt; is the probability of a Type II error, i.e. $$
\Pr \Big[ \text{ Not Reject } H_0 \Big| H_0 \text{ is false } \Big] = \Pr \Big[ T(D)=0 \Big| P \in \mathcal K^C \Big]
$$&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;In the dominant approach to hypothesis testing the goal of test
construction is to have high power subject to the constraint that the
size of the test is lower than the pre-specified significance level.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;statistical-significance&#34;&gt;Statistical Significance&lt;/h3&gt;
&lt;p&gt;TBD&lt;/p&gt;
&lt;h3 id=&#34;p-values&#34;&gt;P-Values&lt;/h3&gt;
&lt;h3 id=&#34;recap&#34;&gt;Recap&lt;/h3&gt;
&lt;p&gt;We now summarize the main features of hypothesis testing.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Select a significance level $\alpha$.&lt;/li&gt;
&lt;li&gt;Select a test statistic $T$ with asymptotic distribution $T\to \xi$
under $H_0$.&lt;/li&gt;
&lt;li&gt;Set the asymptotic critical value $c$ so that 1−&lt;em&gt;G&lt;/em&gt;(&lt;em&gt;c&lt;/em&gt;)=α, where
&lt;em&gt;G&lt;/em&gt; is the distribution function of $\xi$.&lt;/li&gt;
&lt;li&gt;Calculate the asymptotic p-value &lt;em&gt;p&lt;/em&gt;=1−&lt;em&gt;G&lt;/em&gt;(&lt;em&gt;T&lt;/em&gt;).&lt;/li&gt;
&lt;li&gt;Reject $H_0$ if &lt;em&gt;T&lt;/em&gt; &amp;gt; &lt;em&gt;c&lt;/em&gt;, or equivalently &lt;em&gt;p&lt;/em&gt; &amp;lt; α.&lt;/li&gt;
&lt;li&gt;Accept $H_0$ if &lt;em&gt;T&lt;/em&gt; ≤ &lt;em&gt;c&lt;/em&gt;, or equivalently &lt;em&gt;p&lt;/em&gt; ≥ α.&lt;/li&gt;
&lt;li&gt;Report $p$ to summarize the evidence concerning $H_0$ versus $H_1$.&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;examples&#34;&gt;Examples&lt;/h3&gt;
&lt;p&gt;Let’s focus two hypotheses:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;$\beta_k = c \in \mathbb R$&lt;/li&gt;
&lt;li&gt;$\boldsymbol R&amp;rsquo; \beta = r \in \mathbb R^p$&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;t-test-with-known-variance&#34;&gt;t-test with Known Variance&lt;/h3&gt;
&lt;p&gt;Consider the testing problem $H_0 : \beta_k = c$, where $c$ is a
pre-specified value under the null. Suppose the variance of the esimator
$\hat \beta_k$ is &lt;strong&gt;known&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;The t-statistic for this problem is defined by $$
n_{k}:=\frac{\hat \beta_{k} - c}{\sigma_{\hat \beta_{k}}}
$$ In the testing procedure above, the sampling distribution under the
null $H_0$ is given by $$
n_k \sim N(0,1)
$$ Where $N(0,1)$ the standard normal distribution.&lt;/p&gt;
&lt;h3 id=&#34;t-test-with-unknown-variance&#34;&gt;t-test with Unknown Variance&lt;/h3&gt;
&lt;p&gt;Consider the testing problem $H_0 : \beta_k = c$, where $c$ is a
presepecified value under the null. In case the variance of the
estimator $\hat \beta_k$ is &lt;strong&gt;not known&lt;/strong&gt;, we have to replace it with a
consistent estimate $\hat \sigma^2_{\hat \beta}$&lt;/p&gt;
&lt;p&gt;The t-statistic for this problem is defined by $$
t_{k}:=\frac{\hat \beta_{k} - c}{\hat \sigma_{\hat \beta_{k}}}
$$ In the testing procedure above, the sampling distribution under the
null $H_0$ is given by $$
t_k \sim t_{n-K}
$$ Where $t_{n-K}$ denotes the t-distribution with $n-K$ degress of
freedom.&lt;/p&gt;
&lt;h3 id=&#34;wald-test&#34;&gt;Wald-test&lt;/h3&gt;
&lt;p&gt;Consider the testing problem $\boldsymbol R&amp;rsquo; \beta = r$, where
$\boldsymbol R \in \mathbb R^{p+K}$ is a pre-specified set of linear
combinations and $r \in \mathbb R^p$ is a restriction vector. Suppose
the variance of the esimator $\hat \beta$ is &lt;strong&gt;known&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;The Wald statistic for this problem is given by $$
W := \frac{(R \hat \beta-r)^{\prime}(R \hat \beta-r) }{R&amp;rsquo; \sigma^{2}&lt;em&gt;{\hat \beta} R}
$$ In the testing procedure above, the sampling distribution under the
null $H_0$ is given by $$
W \sim \chi^2&lt;/em&gt;{n-K}
$$ Where $\chi^2_{n-K}$ denotes the chi-squared distribution with $n-K$
degress of freedom.&lt;/p&gt;
&lt;h3 id=&#34;comments-on-the-wald-test&#34;&gt;Comments on the Wald test&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;The Wald statistic $W$ is a weighted Euclidean measure of the length
of the vector $R \hat \beta-r$&lt;/li&gt;
&lt;li&gt;The Wald test is intrinsecally 2-sided&lt;/li&gt;
&lt;li&gt;When $p=1$ then $W = |T|$ , the square of the t-statistic, so
hypothesis tests based on $W$ and $|T|$ are equivalent.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;f-test&#34;&gt;F-test&lt;/h3&gt;
&lt;p&gt;Consider the testing problem $\boldsymbol R&amp;rsquo; \beta = r$, where
$\boldsymbol R \in \mathbb R^{p+K}$ is a pre-specified set of linear
combinations and $r \in \mathbb R^p$ is a restriction vector. In case
the variance of the estimator $\hat \beta$ is &lt;strong&gt;not known&lt;/strong&gt;, we have to
replace it with a consistent estimate $\hat \sigma^2_{\hat \beta}$.&lt;/p&gt;
&lt;p&gt;The F-statistic for this problem is given by $$
F := \frac{(R \hat \beta-r)^{\prime}(R \hat \beta-r) / p }{R&amp;rsquo; \hat \sigma^{2} _ {\hat \beta} R}
$$ In the testing procedure above, the sampling distribution under the
null $H_0$ is given by $$
F \sim F_{p, n-K}
$$ Where $F_{p, n-K}$ denotes the F-distribution with $n-K$ degress of
freedom, with $p$ restrictions.&lt;/p&gt;
&lt;h3 id=&#34;f-test-equivalence&#34;&gt;F-test Equivalence&lt;/h3&gt;
&lt;p&gt;Consider the testing problem $\boldsymbol R&amp;rsquo; \beta = r$, where
$\boldsymbol R \in \mathbb R^{p+K}$ is a pre-specified set of linear
combinations and $r \in \mathbb R^p$ is a restriction vector. Consider
two estimators&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\hat \beta_U = \arg \min_b \frac{1}{n} (y - X \beta)&amp;rsquo; (y - X\beta)$&lt;/li&gt;
&lt;li&gt;$\hat \beta_R = \arg \min_{b : \boldsymbol R&amp;rsquo; \beta = r} \frac{1}{n} (y - X \beta)&amp;rsquo; (y - X\beta)$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Then the F statistic is numerically equivalent to the following
expression $$
F = \frac{\left(S S R_{R}-S S R_{U}\right) / p}{S S R_{U} /(n-K)}
$$ where SSR is the sum of squared residuals.&lt;/p&gt;
&lt;h3 id=&#34;confidence-intervals&#34;&gt;Confidence Intervals&lt;/h3&gt;
&lt;p&gt;TBD&lt;/p&gt;
&lt;h3 id=&#34;minimum-distance-tests&#34;&gt;Minimum Distance Tests&lt;/h3&gt;
&lt;p&gt;TBD&lt;/p&gt;
&lt;h2 id=&#34;asymptotics&#34;&gt;Asymptotics&lt;/h2&gt;
&lt;h3 id=&#34;estimator-properties&#34;&gt;Estimator Properties&lt;/h3&gt;
&lt;p&gt;Given a sequence of well specified data generating processes
$\mathcal F_n$, each indexed by the same parameter space $\Theta$, with
$\theta_0$ a component of the true parameter for each $n$.&lt;/p&gt;
&lt;p&gt;Then estimator $\hat \theta$ is&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;unbiased&lt;/strong&gt; if $\mathbb E [\hat \theta] = \theta_0$&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;consistent&lt;/strong&gt; if $\hat \theta \overset{p}{\to} \theta_0$&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;asymptotically normal&lt;/strong&gt;
$\sqrt{n} (\hat \theta - \theta_0) \overset{d}{\to} N(0, V)$ for
some positive definite $V$&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;test-consistency&#34;&gt;Test Consistency&lt;/h3&gt;
&lt;p&gt;The &lt;strong&gt;asymptotic size&lt;/strong&gt; of a testing procedure is defined as the
limiting probability of rejecting $H_0$ when $H_0$ is true.
Mathematically, we can write this as
$\lim _ {n \to \infty} \Pr_n ( \text{reject } H_0 | H_0)$, where the $n$
subscript indexes the sample size.&lt;/p&gt;
&lt;p&gt;A test is said to be &lt;strong&gt;consistent&lt;/strong&gt; against the alternative $H_1$ if the
null hypothesis is rejected with probability approaching $1$ when $H_1$
is true:
$\lim _ {N \to \infty} \Pr_N (\text{reject } H_0 | H_1) \overset{p}{\to} 1$.&lt;/p&gt;
&lt;h3 id=&#34;convergence&#34;&gt;Convergence&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt;: Suppose that
$\sqrt{n}(\hat{\theta} - \theta_0) \overset{d}{\to} N(0, V)$, where $V$
is positive definite. Then for any non-stochastic $Q\times P$ matrix
$R$, $Q \leq P$, with rank$( R ) = Q$ $$
\sqrt{n} R (\hat{\theta} - \theta_0) \sim N(0, R VR&amp;rsquo;)
$$ and $$
[\sqrt{n}R(\hat{\theta} - \theta_0)]&amp;rsquo;[RVR&amp;rsquo;]^{-1}[\sqrt{n}R(\hat{\theta} - \theta_0)] \overset{d}{\to} \chi^2_Q
$$ In addition, if $\text{plim} \hat{V} _n = V$, then $$
(\hat{\theta} - \theta_0)&amp;rsquo; R&amp;rsquo;[R (\hat{V} _n/n) R&amp;rsquo;]^{-1}R (\hat{\theta} - \theta_0) \overset{d}{\to} \chi^2_Q
$$&lt;/p&gt;
&lt;h3 id=&#34;wald-statistic&#34;&gt;Wald Statistic&lt;/h3&gt;
&lt;p&gt;For testing the null hypothesis $H_0: R\theta_0 = r$, where $r$ is a
$Q\times1$ random vector, define the &lt;strong&gt;Wald statistic&lt;/strong&gt; for testing
$H_0$ against $H_1 : R\theta_0 \neq r$ as $$
W_n = (R\hat{\theta} - r)&amp;rsquo;[R (\hat{V} _n/n) R&amp;rsquo;]^{-1} (R\hat{\theta} - r)
$$ Under $H_0$, $W_n \overset{d}{\to} \chi^2_Q$. If we abuse the
asymptotics and we treat $\hat{\theta}$ as being distributed as Normal
we get the equation exactly.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Plotting</title>
      <link>https://matteocourthoud.github.io/course/data-science/05_plotting/</link>
      <pubDate>Sun, 01 May 2022 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/course/data-science/05_plotting/</guid>
      <description>&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import numpy as np
import pandas as pd

import folium
import geopandas
import contextily
import matplotlib as mpl
import matplotlib.pyplot as plt
import seaborn as sns

from src.import_data import import_data
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For the scope of this tutorial we are going to use AirBnb Scraped data for the city of Bologna. The data is freely available at &lt;strong&gt;Inside AirBnb&lt;/strong&gt;: &lt;a href=&#34;http://insideairbnb.com/get-the-data.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://insideairbnb.com/get-the-data.html&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;A description of all variables in all datasets is avaliable &lt;a href=&#34;https://docs.google.com/spreadsheets/d/1iWCNJcSutYqpULSQHlNyGInUvHg2BoUGoNRIGa6Szc4/edit#gid=982310896&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;We are going to use 2 datasets:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;listing dataset: contains listing-level information&lt;/li&gt;
&lt;li&gt;pricing dataset: contains pricing data, over time&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We import and clean them with a script. If you want more details, have a look at the &lt;a href=&#34;https://matteocourthoud.github.io/course/data-science/01_data_exploration/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;data exploration&lt;/a&gt; and &lt;a href=&#34;https://matteocourthoud.github.io/course/data-science/03_data_wrangling&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;data wrangling&lt;/a&gt; sections.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_listings, df_prices, df = import_data()
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;intro&#34;&gt;Intro&lt;/h2&gt;
&lt;p&gt;The default library for plotting in python is &lt;code&gt;matplotlib&lt;/code&gt;. However, a more modern package that builds on top of it, is &lt;code&gt;seaborn&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;We start by telling the notebook to display the plots inline.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;%matplotlib inline
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Another important configuration is the plot resulution. We set it to &lt;code&gt;retina&lt;/code&gt; to have high resolution plots.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;%config InlineBackend.figure_format = &#39;retina&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can choose set a general theme using &lt;code&gt;plt.style.use()&lt;/code&gt;. The list of themes is available &lt;a href=&#34;https://matplotlib.org/3.5.1/gallery/style_sheets/style_sheets_reference.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plt.style.use(&#39;seaborn&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you want to further customize some aspects of a theme, you can set some global paramters for all plots. You can find a list of all the options &lt;a href=&#34;https://matplotlib.org/stable/tutorials/introductory/customizing.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;. If you want to customize all plots in a project in the samy way, you can create a &lt;code&gt;filename.mplstyle&lt;/code&gt; file and call it at the beginning of each file as &lt;code&gt;plt.style.use(&#39;filename.mplstyle&#39;)&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;mpl.rcParams[&#39;figure.figsize&#39;] = (10,6)
mpl.rcParams[&#39;axes.labelsize&#39;] = 16
mpl.rcParams[&#39;axes.titlesize&#39;] = 18
mpl.rcParams[&#39;axes.titleweight&#39;] = &#39;bold&#39;
mpl.rcParams[&#39;figure.titlesize&#39;] = 18
mpl.rcParams[&#39;figure.titleweight&#39;] = &#39;bold&#39;
mpl.rcParams[&#39;axes.titlepad&#39;] = 20
mpl.rcParams[&#39;legend.facecolor&#39;] = &#39;w&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;distributions&#34;&gt;Distributions&lt;/h2&gt;
&lt;p&gt;Suppose you have a numerical variable and you want to see how it&amp;rsquo;s distributed. The best option is to use an &lt;strong&gt;histogram&lt;/strong&gt;. Seaborn function is &lt;code&gt;sns.histplot&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_listings[&#39;log_price&#39;] = np.log(1+df_listings[&#39;mean_price&#39;])
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sns.histplot(df_listings[&#39;log_price&#39;], bins=50)\
.set(title=&#39;Distribution of log-prices&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/05_plotting_17_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;We can add a smooth kernel density approximation with the &lt;code&gt;kde&lt;/code&gt; option.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sns.histplot(df_listings[&#39;log_price&#39;], bins=50, kde=True)\
.set(title=&#39;Distribution of log-prices with density&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/05_plotting_19_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;If we have a categorical variable, we might want to plot the distribution of the data across its values. We can use a &lt;strong&gt;barplot&lt;/strong&gt;. Seaborn function is &lt;code&gt;sns.countplot()&lt;/code&gt; for count data.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sns.countplot(x=&amp;quot;neighborhood&amp;quot;, data=df_listings)\
.set(title=&#39;Number of observations by neighborhood&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/05_plotting_21_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;If instead we want to see the distribution of another variable across some group, we can use the &lt;code&gt;sns.barplot()&lt;/code&gt; function.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sns.barplot(x=&amp;quot;neighborhood&amp;quot;, y=&amp;quot;mean_price&amp;quot;, data=df_listings)\
.set(title=&#39;Average price by neighborhood&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/05_plotting_23_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;We can also use other metrics besides the mean with the &lt;code&gt;estimator&lt;/code&gt; option.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sns.barplot(x=&amp;quot;neighborhood&amp;quot;, y=&amp;quot;mean_price&amp;quot;, data=df_listings, estimator=np.median)\
.set(title=&#39;Median price by neighborhood&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/05_plotting_25_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;We can also plot the full distribution using, for example &lt;strong&gt;boxplots&lt;/strong&gt; with &lt;code&gt;sns.boxplot()&lt;/code&gt;. Boxplots display quartiles and outliers.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sns.boxplot(x=&amp;quot;neighborhood&amp;quot;, y=&amp;quot;log_price&amp;quot;, data=df_listings)\
.set(title=&#39;Price distribution across neighborhoods&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/05_plotting_27_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;If we want to see the full distribution, we can use the &lt;code&gt;sns.violinplot()&lt;/code&gt; function.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sns.violinplot(x=&amp;quot;neighborhood&amp;quot;, y=&amp;quot;log_price&amp;quot;, data=df_listings)\
.set(title=&#39;Price distribution across neighborhoods&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/05_plotting_29_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;time-series&#34;&gt;Time Series&lt;/h2&gt;
&lt;p&gt;If the dataset has a time dimension, we might want to explore how a variable evolves over time. Seaborn function is &lt;code&gt;sns.lineplot()&lt;/code&gt;. If the data has multiple observations for each time period, it will also display a 95% confidence interval around the mean.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sns.lineplot(data=df, x=&#39;date&#39;, y=&#39;price&#39;)\
.set(title=&amp;quot;Price distribution over time&amp;quot;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/05_plotting_32_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;We can do the samy by group, with the &lt;code&gt;hue&lt;/code&gt; option. We can suppress confidence intervals setting &lt;code&gt;ci=None&lt;/code&gt; (making the code much faster).&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sns.lineplot(data=df, x=&#39;date&#39;, y=&#39;price&#39;, hue=&#39;neighborhood&#39;, ci=None)\
.set(title=&amp;quot;Price distribution over time&amp;quot;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/05_plotting_34_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;correlations&#34;&gt;Correlations&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_listings[&amp;quot;log_reviews&amp;quot;] = np.log(1 + df_listings[&amp;quot;number_of_reviews&amp;quot;])
df_listings[&amp;quot;log_rpm&amp;quot;] = np.log(1 + df_listings[&amp;quot;reviews_per_month&amp;quot;])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The most intuitive way to plot a correlation between two variables is a &lt;strong&gt;scatterplot&lt;/strong&gt;. Seaborn function is &lt;code&gt;sns.scatterplot()&lt;/code&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sns.scatterplot(data=df_listings, x=&amp;quot;log_rpm&amp;quot;, y=&amp;quot;log_price&amp;quot;, alpha=0.3)\
.set(title=&#39;Prices and Reviews&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/05_plotting_38_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;We can highlight the best linear approximation adding a line of fit using &lt;code&gt;sns.regplot()&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sns.regplot(x=&amp;quot;log_rpm&amp;quot;, y=&amp;quot;log_price&amp;quot;, data=df_listings,
            scatter_kws={&#39;alpha&#39;:.1},
            line_kws={&#39;color&#39;:&#39;C1&#39;})\
.set(title=&#39;Price and Reviews&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/05_plotting_40_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;If we want a more flexible representation of the data, we can use the &lt;code&gt;binscatter&lt;/code&gt; package. &lt;code&gt;binscatter&lt;/code&gt; splits the data into equally sized bins and displays a scatterplot of the averages.&lt;/p&gt;
&lt;p&gt;The main difference between a binscatterplot and an histogram is that in a histogram bins have the same &lt;em&gt;width&lt;/em&gt; while in a binscatterplot bins have the same &lt;em&gt;number of observations&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;An advantage of &lt;code&gt;binscatter&lt;/code&gt; is that it makes the nature of the data much more transparent, at the cost of hiding some of the background noise.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import binscatter

# Remove nans
temp = df_listings[[&amp;quot;log_rpm&amp;quot;, &amp;quot;log_price&amp;quot;]].dropna()

# Binned scatter plot of Wage vs Tenure
fig, ax = plt.subplots()
ax.binscatter(temp[&amp;quot;log_rpm&amp;quot;], temp[&amp;quot;log_price&amp;quot;]);
ax.set_title(&#39;Price and Reviews&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/05_plotting_42_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;As usual, we can split the data by group with the &lt;code&gt;hue&lt;/code&gt; option.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sns.scatterplot(data=df_listings, x=&amp;quot;log_rpm&amp;quot;, y=&amp;quot;log_price&amp;quot;, 
                hue=&amp;quot;room_type&amp;quot;, alpha=0.3)\
.set(title=&amp;quot;Prices and Ratings, by room type&amp;quot;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/05_plotting_44_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;We can also add the marginal distributions using the &lt;code&gt;sns.jointplot()&lt;/code&gt; function.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sns.jointplot(data=df_listings, x=&amp;quot;log_rpm&amp;quot;, y=&amp;quot;log_price&amp;quot;, kind=&amp;quot;hex&amp;quot;)\
.fig.suptitle(&amp;quot;Prices and Reviews, with marginals&amp;quot;)  
plt.subplots_adjust(top=0.9);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/05_plotting_46_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;If we want to plot correlations (and marginals) of multiple variables, we can use the &lt;code&gt;sns.pairplot()&lt;/code&gt; function.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sns.pairplot(data=df_listings,
             vars=[&amp;quot;log_rpm&amp;quot;, &amp;quot;log_reviews&amp;quot;, &amp;quot;log_price&amp;quot;],
             plot_kws={&#39;s&#39;:2})\
.fig.suptitle(&amp;quot;Correlations&amp;quot;);
plt.subplots_adjust(top=0.9)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/05_plotting_48_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;We can distinguish across groups with the &lt;code&gt;hue&lt;/code&gt; option.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sns.pairplot(data=df_listings,
             vars=[&amp;quot;log_rpm&amp;quot;, &amp;quot;log_reviews&amp;quot;, &amp;quot;log_price&amp;quot;],
             hue=&#39;room_type&#39;,
             plot_kws={&#39;s&#39;:2})\
.fig.suptitle(&amp;quot;Correlations, by room type&amp;quot;);
plt.subplots_adjust(top=0.9)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/05_plotting_50_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;If we want to plot all the correlations in the data, we can use the &lt;code&gt;sns.heatmap()&lt;/code&gt; function on top of a correlation matrix generated by &lt;code&gt;.corr()&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Plot
sns.heatmap(df.corr(), vmin=-1, vmax=1, linewidths=.5, cmap=&amp;quot;RdBu&amp;quot;)\
 .set(title=&amp;quot;Correlations&amp;quot;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/05_plotting_52_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;geographical-data&#34;&gt;Geographical data&lt;/h2&gt;
&lt;p&gt;We can in principle plot geographical data as a simple scatterplot.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sns.scatterplot(data=df_listings, x=&amp;quot;longitude&amp;quot;, y=&amp;quot;latitude&amp;quot;)\
.set(title=&#39;Listing coordinates&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/05_plotting_55_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;However, we can do better and do the scatterplot over a map layer.&lt;/p&gt;
&lt;p&gt;First, we neeed to convert the &lt;code&gt;latitude&lt;/code&gt; and &lt;code&gt;longitude&lt;/code&gt; variables into coordinates. We use the library &lt;code&gt;geopandas&lt;/code&gt;. Note that the original coordinate system is &lt;code&gt;4326&lt;/code&gt; (3D) and we need to &lt;code&gt;3857&lt;/code&gt; (2D).&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;geom = geopandas.points_from_xy(df_listings.longitude, df_listings.latitude)
gdf = geopandas.GeoDataFrame(
    df_listings, 
    geometry=geom,
    crs=4326).to_crs(3857)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We import a map of Bologna using the library &lt;code&gt;contextily&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;bologna = contextily.Place(&amp;quot;Bologna&amp;quot;, source=contextily.providers.Stamen.TonerLite)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We are now ready to plot it with the airbnb listings.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;ax = bologna.plot()
ax.set_ylim([5530000, 5555000])
gdf.plot(ax=ax, c=df_listings[&#39;mean_price&#39;], cmap=&#39;viridis&#39;, alpha=0.8);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/05_plotting_61_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Model Selection and Regularization</title>
      <link>https://matteocourthoud.github.io/course/ml-econ/05_regularization/</link>
      <pubDate>Wed, 09 Mar 2022 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/course/ml-econ/05_regularization/</guid>
      <description>&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Remove warnings
import warnings
warnings.filterwarnings(&#39;ignore&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Import
import pandas as pd
import numpy as np
import time
import itertools
import statsmodels.api as sm
import seaborn as sns

from numpy.random import normal, uniform
from itertools import combinations
from statsmodels.api import add_constant
from statsmodels.regression.linear_model import OLS
from sklearn.linear_model import LinearRegression, Ridge, Lasso, RidgeCV, LassoCV
from sklearn.cross_decomposition import PLSRegression, PLSSVD
from sklearn.model_selection import KFold, cross_val_score, train_test_split, LeaveOneOut, ShuffleSplit
from sklearn.preprocessing import scale
from sklearn.decomposition import PCA
from sklearn.metrics import mean_squared_error
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Import matplotlib for graphs
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import axes3d

# Set global parameters
%matplotlib inline
plt.style.use(&#39;seaborn-white&#39;)
plt.rcParams[&#39;lines.linewidth&#39;] = 3
plt.rcParams[&#39;figure.figsize&#39;] = (12,5)
plt.rcParams[&#39;figure.titlesize&#39;] = 20
plt.rcParams[&#39;axes.titlesize&#39;] = 18
plt.rcParams[&#39;axes.labelsize&#39;] = 14
plt.rcParams[&#39;legend.fontsize&#39;] = 14
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;When we talk about big data, we do not only talk about bigger sample size, $n$, but also about a larger number of explanatory variables, $p$. However, with ordinary least squares, we are limited by the identification constraint that $p &amp;lt; n$. Moreover, for inference and prediction accuracy, we would actually like to have $k &amp;laquo; n$.&lt;/p&gt;
&lt;p&gt;This session adresses methods to use a least squares fit in a setting in which the number of regressors, $p$, is large with respect to the sample size, $n$&lt;/p&gt;
&lt;h2 id=&#34;51-subset-selection&#34;&gt;5.1 Subset Selection&lt;/h2&gt;
&lt;p&gt;The Subset Selection approach involves identifying a subset of the $p$ predictors that we believe to be related to the response. We then fit a model using least squares on the reduced set of variables.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s load the &lt;code&gt;credit rating&lt;/code&gt; dataset.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Credit ratings dataset
credit = pd.read_csv(&#39;data/Credit.csv&#39;, usecols=list(range(1,12)))
credit.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;Income&lt;/th&gt;
      &lt;th&gt;Limit&lt;/th&gt;
      &lt;th&gt;Rating&lt;/th&gt;
      &lt;th&gt;Cards&lt;/th&gt;
      &lt;th&gt;Age&lt;/th&gt;
      &lt;th&gt;Education&lt;/th&gt;
      &lt;th&gt;Gender&lt;/th&gt;
      &lt;th&gt;Student&lt;/th&gt;
      &lt;th&gt;Married&lt;/th&gt;
      &lt;th&gt;Ethnicity&lt;/th&gt;
      &lt;th&gt;Balance&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;14.891&lt;/td&gt;
      &lt;td&gt;3606&lt;/td&gt;
      &lt;td&gt;283&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;34&lt;/td&gt;
      &lt;td&gt;11&lt;/td&gt;
      &lt;td&gt;Male&lt;/td&gt;
      &lt;td&gt;No&lt;/td&gt;
      &lt;td&gt;Yes&lt;/td&gt;
      &lt;td&gt;Caucasian&lt;/td&gt;
      &lt;td&gt;333&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;106.025&lt;/td&gt;
      &lt;td&gt;6645&lt;/td&gt;
      &lt;td&gt;483&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;82&lt;/td&gt;
      &lt;td&gt;15&lt;/td&gt;
      &lt;td&gt;Female&lt;/td&gt;
      &lt;td&gt;Yes&lt;/td&gt;
      &lt;td&gt;Yes&lt;/td&gt;
      &lt;td&gt;Asian&lt;/td&gt;
      &lt;td&gt;903&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;104.593&lt;/td&gt;
      &lt;td&gt;7075&lt;/td&gt;
      &lt;td&gt;514&lt;/td&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;71&lt;/td&gt;
      &lt;td&gt;11&lt;/td&gt;
      &lt;td&gt;Male&lt;/td&gt;
      &lt;td&gt;No&lt;/td&gt;
      &lt;td&gt;No&lt;/td&gt;
      &lt;td&gt;Asian&lt;/td&gt;
      &lt;td&gt;580&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;148.924&lt;/td&gt;
      &lt;td&gt;9504&lt;/td&gt;
      &lt;td&gt;681&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;36&lt;/td&gt;
      &lt;td&gt;11&lt;/td&gt;
      &lt;td&gt;Female&lt;/td&gt;
      &lt;td&gt;No&lt;/td&gt;
      &lt;td&gt;No&lt;/td&gt;
      &lt;td&gt;Asian&lt;/td&gt;
      &lt;td&gt;964&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;55.882&lt;/td&gt;
      &lt;td&gt;4897&lt;/td&gt;
      &lt;td&gt;357&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;68&lt;/td&gt;
      &lt;td&gt;16&lt;/td&gt;
      &lt;td&gt;Male&lt;/td&gt;
      &lt;td&gt;No&lt;/td&gt;
      &lt;td&gt;Yes&lt;/td&gt;
      &lt;td&gt;Caucasian&lt;/td&gt;
      &lt;td&gt;331&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;We are going to look at the relationship between individual characteristics and account &lt;code&gt;Balance&lt;/code&gt; in the &lt;code&gt;Credit&lt;/code&gt; dataset.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# X and y
X = credit.loc[:, credit.columns != &#39;Balance&#39;]
y = credit.loc[:,&#39;Balance&#39;]
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;best-subset-selection&#34;&gt;Best Subset Selection&lt;/h3&gt;
&lt;p&gt;To perform best subset selection, we fit a separate least squares regression for each possible combination of the $p$ predictors. That is, we fit all $p$ models that contain exactly one predictor, all $p = p(p−1)/2$ models that contain 2 exactly two predictors, and so forth. We then look at all of the resulting models, with the goal of identifying the one that is best.&lt;/p&gt;
&lt;p&gt;Clearly the &lt;strong&gt;main disadvantage&lt;/strong&gt; of &lt;em&gt;best subset selection&lt;/em&gt; is computational power.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def model_selection(X, y, *args):
    
    # Init 
    scores = list(itertools.repeat(np.zeros((0,2)), len(args)))

    # Categorical variables 
    categ_cols = {&amp;quot;Gender&amp;quot;, &amp;quot;Student&amp;quot;, &amp;quot;Married&amp;quot;, &amp;quot;Ethnicity&amp;quot;}

    # Loop over all admissible number of regressors
    K = np.shape(X)[1]
    for k in range(K+1):
        print(&amp;quot;Computing k=%1.0f&amp;quot; % k, end =&amp;quot;&amp;quot;)
        
        # Loop over all combinations
        for i in combinations(range(K), k):

            # Subset X
            X_subset = X.iloc[:,list(i)]

            # Get dummies for categorical variables
            if k&amp;gt;0:
                categ_subset = list(categ_cols &amp;amp; set(X_subset.columns))
                X_subset = pd.get_dummies(X_subset, columns=categ_subset, drop_first=True)

            # Regress
            reg = OLS(y,add_constant(X_subset)).fit()

            # Metrics
            for i,metric in enumerate(args):
                score = np.reshape([k,metric(reg)], (1,-1))
                scores[i] = np.append(scores[i], score, axis=0)
        print(&amp;quot;&amp;quot;, end=&amp;quot;\r&amp;quot;)
    return scores
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We are going to consider 10 variables and two difference metrics: the Sum of Squares Residuals and $R^2$.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Set metrics
rss = lambda reg : reg.ssr
r2 = lambda reg : reg.rsquared

# Compute scores
scores = model_selection(X, y, rss, r2)
ms_RSS = scores[0]
ms_R2 = scores[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Computing k=10
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Save best scores
K = np.shape(X)[1]
ms_RSS_best = [np.min(ms_RSS[ms_RSS[:,0]==k,1]) for k in range(K+1)]
ms_R2_best = [np.max(ms_R2[ms_R2[:,0]==k,1]) for k in range(K+1)]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s plot the best scores.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Figure 6.1
def make_figure_6_1():

    fig, (ax1,ax2) = plt.subplots(1,2)
    fig.suptitle(&#39;Figure 6.1: Best Model Selection&#39;)

    # RSS
    ax1.scatter(x=ms_RSS[:,0], y=ms_RSS[:,1], facecolors=&#39;None&#39;, edgecolors=&#39;k&#39;, alpha=0.5);
    ax1.plot(range(K+1), ms_RSS_best, c=&#39;r&#39;);
    ax1.scatter(np.argmin(ms_RSS_best), np.min(ms_RSS_best), marker=&#39;x&#39;, s=300)
    ax1.set_ylabel(&#39;RSS&#39;);

    # R2
    ax2.scatter(x=ms_R2[:,0], y=ms_R2[:,1], facecolors=&#39;None&#39;, edgecolors=&#39;k&#39;, alpha=0.5);
    ax2.plot(range(K+1), ms_R2_best, c=&#39;r&#39;);
    ax2.scatter(np.argmax(ms_R2_best), np.max(ms_R2_best), marker=&#39;x&#39;, s=300)
    ax2.set_ylabel(&#39;R2&#39;);

    # All axes;
    for ax in fig.axes:
        ax.set_xlabel(&#39;Number of Predictors&#39;); 
        ax.set_yticks([]);
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;make_figure_6_1()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/05_regularization_19_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;The figure shows that, as expected, both metrics improve as the number of variables increases; however, from the three-variable model on, there is little improvement in RSS and $R^2$ as a result of including additional predictors.&lt;/p&gt;
&lt;h3 id=&#34;forward-stepwise-selection&#34;&gt;Forward Stepwise Selection&lt;/h3&gt;
&lt;p&gt;For computational reasons, best subset selection cannot be applied with very large $p$.&lt;/p&gt;
&lt;p&gt;While the best subset selection procedure considers all $2^p$ possible models containing subsets of the p predictors, forward step-wise considers a much smaller set of models. Forward stepwise selection begins with a model containing no predictors, and then adds predictors to the model, one-at-a-time, until all of the predictors are in the model. In particular, at each step the variable that gives the greatest additional improvement to the fit is added to the model.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def forward_selection(X, y, f):

    # Init RSS and R2
    K = np.shape(X)[1]
    fms_scores = np.zeros((K,1))
    
    # Categorical variables 
    categ_cols = {&amp;quot;Gender&amp;quot;, &amp;quot;Student&amp;quot;, &amp;quot;Married&amp;quot;, &amp;quot;Ethnicity&amp;quot;}

    # Loop over p
    selected_cols = []
    for k in range(1,K+1):

        # Loop over selected columns
        remaining_cols = [col for col in X.columns if col not in selected_cols]
        temp_scores = np.zeros((0,1))

        # Loop on remaining columns    
        for col in remaining_cols:
            # Subset X
            X_subset = X.loc[:,selected_cols + [col]]
            if k&amp;gt;0:
                categ_subset = list(categ_cols &amp;amp; set(X_subset.columns))
                X_subset = pd.get_dummies(X_subset, columns=categ_subset, drop_first=True)

            # Regress
            reg = OLS(y,add_constant(X_subset).values).fit()

            # Metrics
            temp_scores = np.append(temp_scores, f(reg))

        # Pick best variable
        best_col = remaining_cols[np.argmin(temp_scores)]
        print(best_col)
        selected_cols += [best_col]
        fms_scores[k-1] = np.min(temp_scores)
        
    return fms_scores
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s select the best model according, using the sum of squared residuals as a metric.&lt;/p&gt;
&lt;p&gt;What are the most important variables?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Forward selection by RSS
rss = lambda reg : reg.ssr
fms_RSS = forward_selection(X, y, rss)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Rating
Income
Student
Limit
Cards
Age
Ethnicity
Gender
Married
Education
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What happens if we use $R^2$ instead?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Forward selection by R2
r2 = lambda reg : -reg.rsquared
fms_R2 = -forward_selection(X, y, r2)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Rating
Income
Student
Limit
Cards
Age
Ethnicity
Gender
Married
Education
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Unsurprisingly, both methods select the same models. Why? In the end $R^2$ is just a normalized version of RSS.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s plot the scores of the two methods, for different number of predictors.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# New figure 1
def make_new_figure_1():

    # Init
    fig, (ax1,ax2) = plt.subplots(1,2)
    fig.suptitle(&#39;Forward Model Selection&#39;)

    # RSS
    ax1.plot(range(1,K+1), fms_RSS, c=&#39;r&#39;);
    ax1.scatter(np.argmin(fms_RSS)+1, np.min(fms_RSS), marker=&#39;x&#39;, s=300)
    ax1.set_ylabel(&#39;RSS&#39;);

    # R2
    ax2.plot(range(1,K+1), fms_R2, c=&#39;r&#39;);
    ax2.scatter(np.argmax(fms_R2)+1, np.max(fms_R2), marker=&#39;x&#39;, s=300)
    ax2.set_ylabel(&#39;R2&#39;);

    # All axes;
    for ax in fig.axes:
        ax.set_xlabel(&#39;Number of Predictors&#39;); 
        ax.set_yticks([]);
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;make_new_figure_1()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/05_regularization_31_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;backward-stepwise-selection&#34;&gt;Backward Stepwise Selection&lt;/h3&gt;
&lt;p&gt;Like forward stepwise selection, backward stepwise selection provides an efficient alternative to best subset selection. However, unlike forward stepwise selection, it begins with the full least squares model containing all p predictors, and then iteratively removes the least useful predictor, one-at-a-time.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def backward_selection(X, y, f):

    # Init RSS and R2
    K = np.shape(X)[1]
    fms_scores = np.zeros((K,1))
    
    # Categorical variables 
    categ_cols = {&amp;quot;Gender&amp;quot;, &amp;quot;Student&amp;quot;, &amp;quot;Married&amp;quot;, &amp;quot;Ethnicity&amp;quot;}

    # Loop over p
    selected_cols = list(X.columns)
    for k in range(K,0,-1):

        # Loop over selected columns
        temp_scores = np.zeros((0,1))

        # Loop on remaining columns    
        for col in selected_cols:
            # Subset X
            X_subset = X.loc[:,[x for x in selected_cols if x != col]]
            if k&amp;gt;1:
                categ_subset = list(categ_cols &amp;amp; set(X_subset.columns))
                X_subset = pd.get_dummies(X_subset, columns=categ_subset, drop_first=True)

            # Regress
            reg = OLS(y,add_constant(X_subset).values).fit()

            # Metrics
            temp_scores = np.append(temp_scores, f(reg))

        # Pick best variable
        worst_col = selected_cols[np.argmin(temp_scores)]
        print(worst_col)
        selected_cols.remove(worst_col)
        fms_scores[k-1] = np.min(temp_scores)
        
    return fms_scores
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s select the best model according, using the sum of squared residuals as a metric.&lt;/p&gt;
&lt;p&gt;What are the most important variables?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Backward selection by RSS
rss = lambda reg : reg.ssr
bms_RSS = backward_selection(X, y, rss)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Education
Married
Gender
Ethnicity
Age
Rating
Cards
Student
Income
Limit
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What if we use $R^2$ instead?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Backward selection by R2
r2 = lambda reg : -reg.rsquared
bms_R2 = -backward_selection(X, y, r2)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Education
Married
Gender
Ethnicity
Age
Rating
Cards
Student
Income
Limit
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The interesting part here is that the the variable &lt;code&gt;Rating&lt;/code&gt; that was selected first by forward model selection, is now dropped $5^{th}$ to last. Why? It&amp;rsquo;s probably because it contains a lot of information by itself (hence first in FMS) but it&amp;rsquo;s highly correlated with &lt;code&gt;Student&lt;/code&gt;, &lt;code&gt;Income&lt;/code&gt; and &lt;code&gt;Limit&lt;/code&gt; while these variables are more ortogonal to each other, and hence it gets dropped before them in BMS.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Plot correlations
sns.pairplot(credit[[&#39;Rating&#39;,&#39;Student&#39;,&#39;Income&#39;,&#39;Limit&#39;]], height=1.8);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/05_regularization_40_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;If is indeed what we see: &lt;code&gt;Rating&lt;/code&gt; and &lt;code&gt;Limit&lt;/code&gt; are highly correlated.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s plot the scores for different number of predictors.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# New figure 2
def make_new_figure_2():

    # Init
    fig, (ax1,ax2) = plt.subplots(1,2)
    fig.suptitle(&#39;Backward Model Selection&#39;)

    # RSS
    ax1.plot(range(1,K+1), bms_RSS, c=&#39;r&#39;);
    ax1.scatter(np.argmin(bms_RSS)+1, np.min(bms_RSS), marker=&#39;x&#39;, s=300)
    ax1.set_ylabel(&#39;RSS&#39;);

    # R2
    ax2.plot(range(1,K+1), bms_R2, c=&#39;r&#39;);
    ax2.scatter(np.argmax(bms_R2)+1, np.max(bms_R2), marker=&#39;x&#39;, s=300)
    ax2.set_ylabel(&#39;R2&#39;);

    # All axes;
    for ax in fig.axes:
        ax.set_xlabel(&#39;Number of Predictors&#39;); 
        ax.set_yticks([]);
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;make_new_figure_2()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/05_regularization_44_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;choosing-the-optimal-model&#34;&gt;Choosing the Optimal Model&lt;/h3&gt;
&lt;p&gt;So far we have use the trainint error in order to select the model. However, the training error can be a poor estimate of the test error. Therefore, RSS and R2 are not suitable for selecting the best model among a collection of models with different numbers of predictors.&lt;/p&gt;
&lt;p&gt;In order to select the best model with respect to test error, we need to estimate this test error. There are two common approaches:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;We can indirectly estimate test error by making an adjustment to the training error to account for the bias due to overfitting.&lt;/li&gt;
&lt;li&gt;We can directly estimate the test error, using either a validation set approach or a cross-validation approach.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Some metrics that account for the trainint error are&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Akaike Information Criterium (AIC)&lt;/li&gt;
&lt;li&gt;Bayesian Information Criterium (BIC)&lt;/li&gt;
&lt;li&gt;Adjusted $R^2$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The idea behind all these varaibles is to insert a penalty for the number of parameters used in the model. All these measure have theoretical fundations which are beyond the scope of this session.&lt;/p&gt;
&lt;p&gt;We are now going to test the three metrics&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Set metrics
aic = lambda reg : reg.aic
bic = lambda reg : reg.bic
r2a = lambda reg : reg.rsquared_adj

# Compute best model selection scores
scores = model_selection(X, y, aic, bic, r2a)
ms_AIC = scores[0]
ms_BIC = scores[1]
ms_R2a = scores[2]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Computing k=10
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Save best scores
ms_AIC_best = [np.min(ms_AIC[ms_AIC[:,0]==k,1]) for k in range(K+1)]
ms_BIC_best = [np.min(ms_BIC[ms_BIC[:,0]==k,1]) for k in range(K+1)]
ms_R2a_best = [np.max(ms_R2a[ms_R2a[:,0]==k,1]) for k in range(K+1)]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We plot the scores for different model selection methods.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Figure 6.2
def make_figure_6_2():

    # Init
    fig, (ax1,ax2,ax3) = plt.subplots(1,3, figsize=(16,5))
    fig.suptitle(&#39;Figure 6.2&#39;)

    # AIC
    ax1.scatter(x=ms_AIC[:,0], y=ms_AIC[:,1], facecolors=&#39;None&#39;, edgecolors=&#39;k&#39;, alpha=0.5);
    ax1.plot(range(K+1),ms_AIC_best, c=&#39;r&#39;);
    ax1.scatter(np.argmin(ms_AIC_best), np.min(ms_AIC_best), marker=&#39;x&#39;, s=300)
    ax1.set_ylabel(&#39;AIC&#39;);

    # BIC
    ax2.scatter(x=ms_BIC[:,0], y=ms_BIC[:,1], facecolors=&#39;None&#39;, edgecolors=&#39;k&#39;, alpha=0.5);
    ax2.plot(range(K+1), ms_BIC_best, c=&#39;r&#39;);
    ax2.scatter(np.argmin(ms_BIC_best), np.min(ms_BIC_best), marker=&#39;x&#39;, s=300)
    ax2.set_ylabel(&#39;BIC&#39;);

    # R2 adj
    ax3.scatter(x=ms_R2a[:,0], y=ms_R2a[:,1], facecolors=&#39;None&#39;, edgecolors=&#39;k&#39;, alpha=0.5);
    ax3.plot(range(K+1), ms_R2a_best, c=&#39;r&#39;);
    ax3.scatter(np.argmax(ms_R2a_best), np.max(ms_R2a_best), marker=&#39;x&#39;, s=300)
    ax3.set_ylabel(&#39;R2_adj&#39;);

    # All axes;
    for ax in fig.axes:
        ax.set_xlabel(&#39;Number of Predictors&#39;); 
        ax.set_yticks([]);
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;make_figure_6_2()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/05_regularization_52_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;As we can see, all three metrics select more parsimonious models, with BIC being particularly conservative with only 4 variables and $R^2_{adj}$ selecting the larger model with 7 variables.&lt;/p&gt;
&lt;h3 id=&#34;validation-and-cross-validation&#34;&gt;Validation and Cross-Validation&lt;/h3&gt;
&lt;p&gt;As an alternative to the approaches just discussed, we can directly estimate the test error using the validation set and cross-validation methods discussed in the previous session.&lt;/p&gt;
&lt;p&gt;The main problem with cross-validation is the computational burden. We are now going to perform &lt;em&gt;best model selection&lt;/em&gt; using the following cross-validation algorithms:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Validation set approach, 50-50 split, repeated 10 times&lt;/li&gt;
&lt;li&gt;5-fold cross-validation&lt;/li&gt;
&lt;li&gt;10-fold cross-validation&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We are not going to perform Leave-One-Out cross-validation for computational reasons.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def cv_scores(X, y, *args):

    # Init 
    scores = list(itertools.repeat(np.zeros((0,2)), len(args)))

    # Categorical variables 
    categ_cols = {&amp;quot;Gender&amp;quot;, &amp;quot;Student&amp;quot;, &amp;quot;Married&amp;quot;, &amp;quot;Ethnicity&amp;quot;}

    # Loop over all possible combinations of regressions
    K = np.shape(X)[1]
    for k in range(K+1):
        print(&amp;quot;Computing k=%1.0f&amp;quot; % k, end =&amp;quot;&amp;quot;)
        for i in combinations(range(K), k):

            # Subset X
            X_subset = X.iloc[:,list(i)]

            # Get dummies for categorical variables
            if k&amp;gt;0:
                categ_subset = list(categ_cols &amp;amp; set(X_subset.columns))
                X_subset = pd.get_dummies(X_subset, columns=categ_subset, drop_first=True)

            # Metrics
            for i,cv_method in enumerate(args):
                score = cross_val_score(LinearRegression(), add_constant(X_subset), y, 
                                        cv=cv_method, scoring=&#39;neg_mean_squared_error&#39;).mean()
                score_pair = np.reshape([k,score], (1,-1))
                scores[i] = np.append(scores[i], score_pair, axis=0)
        print(&amp;quot;&amp;quot;, end=&amp;quot;\r&amp;quot;)
                
    return scores
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s compute the scores for different model selection methods.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Define cv methods
vset = ShuffleSplit(n_splits=10, test_size=0.5)
kf5 = KFold(n_splits=5, shuffle=True)
kf10 = KFold(n_splits=10, shuffle=True)

# Get best model selection scores
scores = cv_scores(X, y, vset, kf5, kf10)
ms_vset = scores[0]
ms_kf5 = scores[1]
ms_kf10 = scores[2]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Computing k=10
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Save best scores
ms_vset_best = [np.max(ms_vset[ms_vset[:,0]==k,1]) for k in range(K+1)]
ms_kf5_best = [np.max(ms_kf5[ms_kf5[:,0]==k,1]) for k in range(K+1)]
ms_kf10_best = [np.max(ms_kf10[ms_kf10[:,0]==k,1]) for k in range(K+1)]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We not plot the scores.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Figure 6.3
def make_figure_6_3():

    # Init
    fig, (ax1,ax2,ax3) = plt.subplots(1,3, figsize=(16,5))
    fig.suptitle(&#39;Figure 6.3&#39;)

    # Validation Set
    ax1.scatter(x=ms_vset[:,0], y=ms_vset[:,1], facecolors=&#39;None&#39;, edgecolors=&#39;k&#39;, alpha=0.5);
    ax1.plot(range(K+1),ms_vset_best, c=&#39;r&#39;);
    ax1.scatter(np.argmax(ms_vset_best), np.max(ms_vset_best), marker=&#39;x&#39;, s=300)
    ax1.set_ylabel(&#39;Validation Set&#39;);


    # 5-Fold Cross Validation
    ax2.scatter(x=ms_kf5[:,0], y=ms_kf5[:,1], facecolors=&#39;None&#39;, edgecolors=&#39;k&#39;, alpha=0.5);
    ax2.plot(range(K+1), ms_kf5_best, c=&#39;r&#39;);
    ax2.scatter(np.argmax(ms_kf5_best), np.max(ms_kf5_best), marker=&#39;x&#39;, s=300)
    ax2.set_ylabel(&#39;5-Fold Cross Validation&#39;);


    # 10-Fold Cross-Validation
    ax3.scatter(x=ms_kf10[:,0], y=ms_kf10[:,1], facecolors=&#39;None&#39;, edgecolors=&#39;k&#39;, alpha=0.5);
    ax3.plot(range(K+1), ms_kf10_best, c=&#39;r&#39;);
    ax3.scatter(np.argmax(ms_kf10_best), np.max(ms_kf10_best), marker=&#39;x&#39;, s=300)
    ax3.set_ylabel(&#39;10-Fold Cross-Validation&#39;);

    # All axes;
    for ax in fig.axes:
        ax.set_xlabel(&#39;Number of Predictors&#39;); 
        ax.set_yticks([]);
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;make_figure_6_3()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/05_regularization_62_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;From the figure we see that each cross-validation method selects a different model and the most accurate one, K-fold CV, select 5 predictors.&lt;/p&gt;
&lt;h2 id=&#34;52-shrinkage-methods&#34;&gt;5.2 Shrinkage Methods&lt;/h2&gt;
&lt;p&gt;Model selection methods constrained the number of varaibles &lt;em&gt;before&lt;/em&gt; running a linear regression. Shrinkage methods attempt to do the two things simultaneously. In particular they &lt;em&gt;constrain&lt;/em&gt; or &lt;em&gt;shrink&lt;/em&gt; coefficients by imposing penalties in the objective functions for high values of the parameters.&lt;/p&gt;
&lt;h3 id=&#34;ridge-regression&#34;&gt;Ridge Regression&lt;/h3&gt;
&lt;p&gt;The Least Squares Regression minimizes the Residual Sum of Squares&lt;/p&gt;
&lt;p&gt;$$
\mathrm{RSS}=\sum_{i=1}^{n}\left(y_{i}-\beta_{0}-\sum_{j=1}^{p} \beta_{j} x_{i j}\right)^{2}
$$&lt;/p&gt;
&lt;p&gt;The Ridge Regression objective function instead is&lt;/p&gt;
&lt;p&gt;$$
\sum_{i=1}^{n}\left(y_{i}-\beta_{0}-\sum_{j=1}^{p} \beta_{j} x_{i j}\right)^{2}+\lambda \sum_{j=1}^{p} \beta_{j}^{2}=\mathrm{RSS}+\lambda \sum_{j=1}^{p} \beta_{j}^{2}
$$&lt;/p&gt;
&lt;p&gt;where $\lambda&amp;gt;0$ is a tuning parameter that regulates the extent to which large parameters are penalized.&lt;/p&gt;
&lt;p&gt;In matrix notation, the objective function is&lt;/p&gt;
&lt;p&gt;$$
||X\beta - y||^2_2 + \alpha ||\beta||^2_2
$$&lt;/p&gt;
&lt;p&gt;which is equivalent to optimizing&lt;/p&gt;
&lt;p&gt;$$
\frac{1}{N}||X\beta - y||^2_2 + \frac{\alpha}{N} ||\beta||^2_2
$$&lt;/p&gt;
&lt;p&gt;We are now going to run Ridge Regression on the &lt;code&gt;Credit&lt;/code&gt; dataset trying to explain account &lt;code&gt;Balance&lt;/code&gt; with a set of observable individual characteristics.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# X and y
categ_cols = [&amp;quot;Gender&amp;quot;, &amp;quot;Student&amp;quot;, &amp;quot;Married&amp;quot;, &amp;quot;Ethnicity&amp;quot;]
X = credit.loc[:, credit.columns != &#39;Balance&#39;]
X = pd.get_dummies(X, columns=categ_cols, drop_first=True)
y = credit.loc[:,&#39;Balance&#39;]
n = len(credit)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We run ridge regression over a range of values for the penalty paramenter $\lambda$.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Init alpha grid
n_grid = 100
alphas = 10**np.linspace(-2,5,n_grid).reshape(-1,1)
ridge = Ridge()
ridge_coefs = []

# Loop over values of alpha
for a in alphas:
    ridge.set_params(alpha=a)
    ridge.fit(scale(X), y)
    ridge_coefs.append(ridge.coef_)
ridge_coefs = np.reshape(ridge_coefs,(n_grid,-1))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We use linear regression as a comparison.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# OLS regression
ols = LinearRegression().fit(scale(X),y)
ols_coefs = ols.coef_;
mod_ols = np.linalg.norm(ols_coefs)

# Relative magnitude
rel_beta = [np.linalg.norm(ridge_coefs[k,:])/mod_ols for k in range(n_grid)]
rel_beta = np.reshape(rel_beta, (-1,1))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We plot the results&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Figure 6.4
def make_figure_6_4():

    # Init
    fig, (ax1,ax2) = plt.subplots(1,2)
    fig.suptitle(&#39;Figure 6.4: Ridge Regression Coefficients&#39;)

    highlight = [0,1,2,7];

    # Plot coefficients - absolute
    ax1.plot(alphas, ridge_coefs[:,highlight], alpha=1)
    ax1.plot(alphas, ridge_coefs, c=&#39;grey&#39;, alpha=0.3)
    ax1.set_xscale(&#39;log&#39;)
    ax1.set_xlabel(&#39;lambda&#39;); ax1.set_ylabel(&#39;Standardized coefficients&#39;);
    ax1.legend([&#39;Income&#39;, &#39;Limit&#39;, &#39;Rating&#39;, &#39;Student&#39;])

    # Plot coefficients - relative
    ax2.plot(rel_beta, ridge_coefs[:,highlight], alpha=1)
    ax2.plot(rel_beta, ridge_coefs, c=&#39;grey&#39;, alpha=0.3)
    ax2.set_xlabel(&#39;Relative Beta&#39;); ax2.set_ylabel(&#39;Standardized coefficients&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;make_figure_6_4()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/05_regularization_76_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;As we decrease $\lambda$, the Ridge coefficients get larger. Moreover, the variables with the consistently largest coefficients are &lt;code&gt;Income&lt;/code&gt;, &lt;code&gt;Limit&lt;/code&gt;, &lt;code&gt;Rating&lt;/code&gt; and &lt;code&gt;Student&lt;/code&gt;.&lt;/p&gt;
&lt;h3 id=&#34;bias-variance-trade-off&#34;&gt;Bias-Variance Trade-off&lt;/h3&gt;
&lt;p&gt;Ridge regression’s advantage over least squares is rooted in the bias-variance trade-off. As $\lambda$ increases, the flexibility of the ridge regression fit decreases, leading to decreased variance but increased bias.&lt;/p&gt;
&lt;p&gt;$$
y_0 = f(x_0) + \varepsilon
$$&lt;/p&gt;
&lt;p&gt;Recap: we can decompose the Mean Squared Error of an estimator into two components: the &lt;em&gt;variance&lt;/em&gt; and the squared &lt;em&gt;bias&lt;/em&gt;:&lt;/p&gt;
&lt;p&gt;$$
\mathbb E\left(y_{0}-\hat{f}\left(x_{0}\right)\right)^{2} = \mathbb E\left(f(x_0) + \varepsilon - \hat f(x_{0})\right)^{2} = \
= \mathbb E\left(f(x_0) - \mathbb E[\hat f(x_{0})] + \varepsilon - \hat f(x_{0}) + \mathbb E[\hat f(x_{0})] \right)^{2} = \
= \mathbb E \left[ \mathbb E [\hat{f} (x_{0}) ] - f(x_0) \right]^2 + \mathbb E \left[ \left( \hat{f} (x_{0}) - \mathbb E [\hat{f} (x_{0})] \right)^2 \right] + \mathbb E[\varepsilon^2] \
= \operatorname{Bias} \left( \hat{f} (x_{0}) \right)^2 + \operatorname{Var}\left(\hat{f}\left(x_{0}\right)\right) + \operatorname{Var}(\varepsilon)
$$&lt;/p&gt;
&lt;p&gt;The last term is the variance of the error term, sometimes also called the &lt;em&gt;irreducible error&lt;/em&gt; since it&amp;rsquo;s pure noise, and we cannot account for it.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Compute var-bias
def compute_var_bias(X_train, b0, x0, a, k, n, sim, f):
    
    # Init 
    y_hat = np.zeros(sim)
    coefs = np.zeros((sim, k))
    
    # Loop over simulations
    for s in range(sim):
        e_train = normal(0,1,(n,1))
        y_train = X_train @ b0 + e_train
        fit = f(a).fit(X_train, y_train)
        y_hat[s] = fit.predict(x0)
        coefs[s,:] = fit.coef_
        
    # Compute MSE, Var and Bias2   
    e_test = normal(0,1,(sim,1))
    y_test = x0 @ b0 + e_test
    mse = np.mean((y_test - y_hat)**2)
    var = np.var(y_hat)
    bias2 = np.mean(x0 @ b0 - y_hat)**2
    
    return [mse, var, bias2], np.mean(coefs, axis=0)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;np.random.seed(1)

# Generate random data
n = 50
k = 45
N = 50000
X_train = normal(0.2,1,(n,k))
x0 = normal(0.2,1,(1,k))
e_train = normal(0,1,(n,1))
b0 = uniform(0,1,(k,1))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Init alpha grid
sim = 1000
n_grid = 30
df = pd.DataFrame({&#39;alpha&#39;:10**np.linspace(-5,5,n_grid)})
ridge_coefs2 = []

# Init simulations
sim = 1000
ridge = lambda a: Ridge(alpha=a, fit_intercept=False)

# Loop over values of alpha
for i in range(len(df)):
    print(&amp;quot;Alpha %1.0f/%1.0f&amp;quot; % (i+1,len(df)), end =&amp;quot;&amp;quot;)
    a = df.loc[i,&#39;alpha&#39;]
    df.loc[i,[&#39;mse&#39;,&#39;var&#39;,&#39;bias2&#39;]], c = compute_var_bias(X_train, b0, x0, a, k, n, sim, ridge)
    ridge_coefs2.append(c)
    print(&amp;quot;&amp;quot;, end=&amp;quot;\r&amp;quot;)
ridge_coefs2 = np.reshape(ridge_coefs2,(n_grid,-1))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Alpha 30/30
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# OLS regression
y_train = X_train @ b0 + e_train
ols = LinearRegression().fit(X_train,y_train)
ols_coefs = ols.coef_;
mod_ols = np.linalg.norm(ols_coefs)

# Relative magnitude
rel_beta = [np.linalg.norm(ridge_coefs2[i,:])/mod_ols for i in range(n_grid)]
rel_beta = np.reshape(rel_beta, (-1,1))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Figure 6.5
def make_figure_6_5():
    
    # Init
    fig, (ax1,ax2) = plt.subplots(1,2)
    fig.suptitle(&#39;Figure 6.5: Ridge Bias-Var decomposition&#39;)

    # MSE
    ax1.plot(df[&#39;alpha&#39;], df[[&#39;bias2&#39;,&#39;var&#39;,&#39;mse&#39;]]);
    ax1.set_xscale(&#39;log&#39;);
    ax1.set_xlabel(&#39;lambda&#39;); ax1.set_ylabel(&#39;Mean Squared Error&#39;);
    ax1.legend([&#39;Bias2&#39;,&#39;Variance&#39;,&#39;MSE&#39;], fontsize=12);

    # MSE
    ax2.plot(rel_beta, df[[&#39;bias2&#39;,&#39;var&#39;,&#39;mse&#39;]]);
    ax2.set_xlabel(&#39;Relative Beta&#39;); ax2.set_ylabel(&#39;Mean Squared Error&#39;);
    ax2.legend([&#39;Bias2&#39;,&#39;Variance&#39;,&#39;MSE&#39;], fontsize=12);
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;make_figure_6_5()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/05_regularization_86_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Ridge regression has the advantage of shrinking coefficients. However, unlike best subset, forward stepwise, and backward stepwise selection, which will generally select models that involve just a subset of the variables, ridge regression will include all $p$ predictors in the final model.&lt;/p&gt;
&lt;p&gt;Lasso solves that problem by using a different penalty function.&lt;/p&gt;
&lt;h3 id=&#34;lasso&#34;&gt;Lasso&lt;/h3&gt;
&lt;p&gt;The lasso coefficients minimize the following objective function:&lt;/p&gt;
&lt;p&gt;$$
\sum_{i=1}^{n}\left(y_{i}-\beta_{0}-\sum_{j=1}^{p} \beta_{j} x_{i j}\right)^{2}+\lambda \sum_{j=1}^{p}\left|\beta_{j}\right| = \mathrm{RSS} + \lambda \sum_{j=1}^p|\beta_j|
$$&lt;/p&gt;
&lt;p&gt;so that the main difference with respect to ridge regression is the penalty function $\lambda \sum_{j=1}^{p}\left|\beta_{j}\right|$ instead of $\lambda \sum_{j=1}^p (\beta_j)^2$.&lt;/p&gt;
&lt;p&gt;A consequence of this objective function is that Lasso is much more likely to shrink coefficients to exactly zero, while Ridge only decreases their magnitude. The reason why lies in the shape of the objective function. You can rewrite the Ridge and Lasso minimization problems as constrained optimization:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Ridge
$$
\underset{\beta}{\operatorname{min}} \ \left{\sum_{i=1}^{n}\left(y_{i}-\beta_{0}-\sum_{j=1}^{p} \beta_{j} x_{i j}\right)^{2}\right} \quad \text { subject to } \quad \sum_{j=1}^{p}\left|\beta_{j}\right| \leq s
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Lasso
$$
\underset{\beta}{\operatorname{min}} \ \left{\sum_{i=1}^{n}\left(y_{i}-\beta_{0}-\sum_{j=1}^{p} \beta_{j} x_{i j}\right)^{2}\right} \quad \text { subject to } \quad \sum_{j=1}^{p} \beta_{j}^{2} \leq s
$$&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In pictures, constrained optimization problem lookes like this.&lt;/p&gt;
&lt;img src=&#34;figures/ridgelasso.png&#34; alt=&#34;Drawing&#34; style=&#34;width: 600px;&#34;/&gt;
&lt;p&gt;The red curves represents the contour sets of the RSS. They are elliptical since the objective function is quadratic. The blue area represents the admissible set, i.e. the constraints. As we can see, it is much easier with Lasso to have the constrained optimum on one of the edges of the rhombus.&lt;/p&gt;
&lt;p&gt;We are now going to repeat the same exercise on the &lt;code&gt;Credit&lt;/code&gt; dataset, trying to predict account &lt;code&gt;Balance&lt;/code&gt; with a set of obsevable induvidual characteristics, for different values of the penalty paramenter $\lambda$.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# X and y
categ_cols = [&amp;quot;Gender&amp;quot;, &amp;quot;Student&amp;quot;, &amp;quot;Married&amp;quot;, &amp;quot;Ethnicity&amp;quot;]
X = credit.loc[:, credit.columns != &#39;Balance&#39;]
X = pd.get_dummies(X, columns=categ_cols, drop_first=True)
y = credit.loc[:,&#39;Balance&#39;]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The $\lambda$ grid is going to be slightly different now.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Init alpha grid
n_grid = 100
alphas = 10**np.linspace(0,3,n_grid).reshape(-1,1)
lasso = Lasso()
lasso_coefs = []

# Loop over values of alpha
for a in alphas:
    lasso.set_params(alpha=a)
    lasso.fit(scale(X), y)
    lasso_coefs.append(lasso.coef_)
lasso_coefs = np.reshape(lasso_coefs,(n_grid,-1))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We run OLS to plot the relative magnitude of the Lasso coefficients.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Relative magnitude
mod_ols = np.linalg.norm(ols_coefs)
rel_beta = [np.linalg.norm(lasso_coefs[i,:])/mod_ols for i in range(n_grid)]
rel_beta = np.reshape(rel_beta, (-1,1))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We plot the magnitude of the coefficients $\beta$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;for different values of $\lambda$&lt;/li&gt;
&lt;li&gt;for different values of of $||\beta||$&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Figure 6.6
def make_figure_6_6():

    # Init
    fig, (ax1,ax2) = plt.subplots(1,2)
    fig.suptitle(&#39;Figure 6.6&#39;)

    highlight = [0,1,2,7];

    # Plot coefficients - absolute
    ax1.plot(alphas, lasso_coefs[:,highlight], alpha=1)
    ax1.plot(alphas, lasso_coefs, c=&#39;grey&#39;, alpha=0.3)
    ax1.set_xscale(&#39;log&#39;)
    ax1.set_xlabel(&#39;lambda&#39;); ax1.set_ylabel(&#39;Standardized coefficients&#39;);
    ax1.legend([&#39;Income&#39;, &#39;Limit&#39;, &#39;Rating&#39;, &#39;Student&#39;], fontsize=12)

    # Plot coefficients - relative
    ax2.plot(rel_beta, lasso_coefs[:,highlight], alpha=1)
    ax2.plot(rel_beta, lasso_coefs, c=&#39;grey&#39;, alpha=0.3)
    ax2.set_xlabel(&#39;relative mod beta&#39;); ax2.set_ylabel(&#39;Standardized coefficients&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;make_figure_6_6()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/05_regularization_100_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Rating&lt;/code&gt; seems to be the most important variable, followed by &lt;code&gt;Limit&lt;/code&gt; and &lt;code&gt;Student&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;As with ridge regression, the lasso shrinks the coefficient estimates towards zero. However, in the case of the lasso, the $l_1$ penalty has the effect of forcing some of the coefficient estimates to be exactly equal to zero when the tuning parameter $\lambda$ is sufficiently large. Hence, much like best subset selection, the lasso performs variable selection.&lt;/p&gt;
&lt;p&gt;We say that the lasso yields &lt;strong&gt;sparse&lt;/strong&gt; models — that is, models that involve only a subset of the variable&lt;/p&gt;
&lt;p&gt;We now plot how the choice of $\lambda$ affects the bias-variance trade-off.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Init alpha grid
sim = 1000
n_grid = 30
df = pd.DataFrame({&#39;alpha&#39;:10**np.linspace(-1,1,n_grid)})
lasso_coefs2 = []

# Init simulations
sim = 1000
lasso = lambda a: Lasso(alpha=a, fit_intercept=False)

# Loop over values of alpha
for i in range(len(df)):
    print(&amp;quot;Alpha %1.0f/%1.0f&amp;quot; % (i+1,len(df)), end =&amp;quot;&amp;quot;)
    a = df.loc[i,&#39;alpha&#39;]
    df.loc[i,[&#39;mse&#39;,&#39;var&#39;,&#39;bias2&#39;]], c = compute_var_bias(X_train, b0, x0, a, k, n, sim, lasso)
    lasso_coefs2.append(c)
    print(&amp;quot;&amp;quot;, end=&amp;quot;\r&amp;quot;)
lasso_coefs2 = np.reshape(lasso_coefs2,(n_grid,-1))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Alpha 30/30
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Relative magnitude
mod_ols = np.linalg.norm(ols_coefs)
rel_beta = [np.linalg.norm(lasso_coefs2[k,:])/mod_ols for k in range(n_grid)]
rel_beta = np.reshape(rel_beta, (-1,1))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# OLS regression
y_train = X_train @ b0 + e_train
ols = LinearRegression().fit(X_train,y_train)
ols_coefs = ols.coef_;
mod_ols = np.linalg.norm(ols_coefs)

# Relative magnitude
mod_ols = np.linalg.norm(ols_coefs)
rel_beta = [np.linalg.norm(lasso_coefs2[k,:])/mod_ols for k in range(n_grid)]
rel_beta = np.reshape(rel_beta, (-1,1))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Figure 6.8
def make_figure_6_8():

    fig, (ax1,ax2) = plt.subplots(1,2, figsize=(12,5))
    fig.suptitle(&#39;Figure 6.8: Lasso Bias-Var decomposition&#39;)

    # MSE
    ax1.plot(df[&#39;alpha&#39;], df[[&#39;bias2&#39;,&#39;var&#39;,&#39;mse&#39;]]);
    ax1.set_xscale(&#39;log&#39;);
    ax1.set_xlabel(&#39;lambda&#39;); ax1.set_ylabel(&#39;Mean Squared Error&#39;);
    ax1.legend([&#39;Bias2&#39;,&#39;Variance&#39;,&#39;MSE&#39;], fontsize=12);

    # MSE
    ax2.plot(rel_beta, df[[&#39;bias2&#39;,&#39;var&#39;,&#39;mse&#39;]]);
    ax2.set_xlabel(&#39;Relative Beta&#39;); ax1.set_ylabel(&#39;Mean Squared Error&#39;);
    ax2.legend([&#39;Bias2&#39;,&#39;Variance&#39;,&#39;MSE&#39;], fontsize=12);
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;make_figure_6_8()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/05_regularization_108_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;As $\lambda$ increases the squared bias increases and the variance decreases.&lt;/p&gt;
&lt;h3 id=&#34;comparing-the-lasso-and-ridge-regression&#34;&gt;Comparing the Lasso and Ridge Regression&lt;/h3&gt;
&lt;p&gt;In order to obtain a better intuition about the behavior of ridge regression and the lasso, consider a simple special case with $n = p$, and $X$ a diagonal matrix with $1$’s on the diagonal and $0$’s in all off-diagonal elements. To simplify the problem further, assume also that we are performing regression without an intercept.&lt;/p&gt;
&lt;p&gt;With these assumptions, the usual least squares problem simplifies to the coefficients that minimize&lt;/p&gt;
&lt;p&gt;$$
\sum_{j=1}^{p}\left(y_{j}-\beta_{j}\right)^{2}
$$&lt;/p&gt;
&lt;p&gt;In this case, the least squares solution is given by&lt;/p&gt;
&lt;p&gt;$$
\hat \beta_j = y_j
$$&lt;/p&gt;
&lt;p&gt;One can show that in this setting, the ridge regression estimates take the form&lt;/p&gt;
&lt;p&gt;$$
\hat \beta_j^{RIDGE} = \frac{y_j}{1+\lambda}
$$&lt;/p&gt;
&lt;p&gt;and the lasso estimates take the form&lt;/p&gt;
&lt;p&gt;$$
\hat{\beta}&lt;em&gt;{j}^{LASSO}=\left{\begin{array}{ll}
y&lt;/em&gt;{j}-\lambda / 2 &amp;amp; \text { if } y_{j}&amp;gt;\lambda / 2 \
y_{j}+\lambda / 2 &amp;amp; \text { if } y_{j}&amp;lt;-\lambda / 2 \
0 &amp;amp; \text { if }\left|y_{j}\right| \leq \lambda / 2
\end{array}\right.
$$&lt;/p&gt;
&lt;p&gt;We plot the relationship visually.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;np.random.seed(3)

# Generate random data
n = 100
k = n
X = np.eye(k)
e = normal(0,1,(n,1))
b0 = uniform(-1,1,(k,1))
y = X @ b0 + e
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# OLS regression
reg = LinearRegression().fit(X,y)
ols_coefs = reg.coef_;

# Ridge regression
ridge = Ridge(alpha=1).fit(X,y)
ridge_coefs = ridge.coef_;

# Ridge regression
lasso = Lasso(alpha=0.01).fit(X,y)
lasso_coefs = lasso.coef_.reshape(1,-1);

# sort
order = np.argsort(y.reshape(1,-1), axis=1)
y_sorted = np.take_along_axis(ols_coefs, order, axis=1) 
ols_coefs = np.take_along_axis(ols_coefs, order, axis=1) 
ridge_coefs = np.take_along_axis(ridge_coefs, order, axis=1) 
lasso_coefs = np.take_along_axis(lasso_coefs, order, axis=1) 
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Figure 6.10
def make_figure_6_10():
    
    # Init
    fig, (ax1,ax2) = plt.subplots(1,2)
    fig.suptitle(&#39;Figure 6.10&#39;)

    # Ridge
    ax1.plot(y_sorted.T, ols_coefs.T)
    ax1.plot(y_sorted.T, ridge_coefs.T)
    ax1.set_xlabel(&#39;True Coefficient&#39;); ax1.set_ylabel(&#39;Estimated Coefficient&#39;);
    ax1.legend([&#39;OLS&#39;,&#39;Ridge&#39;], fontsize=12);

    # Lasso
    ax2.plot(y_sorted.T, ols_coefs.T)
    ax2.plot(y_sorted.T, lasso_coefs.T)
    ax2.set_xlabel(&#39;True Coefficient&#39;); ax2.set_ylabel(&#39;Estimated Coefficient&#39;);
    ax2.legend([&#39;OLS&#39;,&#39;Lasso&#39;], fontsize=12);
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt; make_figure_6_10()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/05_regularization_117_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;We see that ridge regression shrinks every dimension of the data by the same proportion, whereas the lasso hrinks all coefficients toward zero by a similar amount, and sufficiently small coefficients are shrunken all the way to zero.&lt;/p&gt;
&lt;h3 id=&#34;selecting-the-tuning-parameter&#34;&gt;Selecting the Tuning Parameter&lt;/h3&gt;
&lt;p&gt;Implementing ridge regression and the lasso requires a method for selecting a value for the tuning parameter $\lambda$.&lt;/p&gt;
&lt;p&gt;Cross-validation provides a simple way to tackle this problem. We choose a grid of $\lambda$ values, and compute the cross-validation error for each value of $\lambda$. We then select the tuning parameter value for which the cross-validation error is smallest. Finally, the model is re-fit using all of the available observations and the selected value of the tuning parameter.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# X and y
categ_cols = [&amp;quot;Gender&amp;quot;, &amp;quot;Student&amp;quot;, &amp;quot;Married&amp;quot;, &amp;quot;Ethnicity&amp;quot;]
X = credit.loc[:, credit.columns != &#39;Balance&#39;]
X = pd.get_dummies(X, columns=categ_cols, drop_first=True).values
y = credit.loc[:,&#39;Balance&#39;]
n = len(credit)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We are going to use 10-fold CV as cross-validation algorithm.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Get MSE
def cv_lasso(X,y,a):
    # Init mse
    mse = []
    
    # Generate splits
    kf10 = KFold(n_splits=10, random_state=None, shuffle=False)
    kf10.get_n_splits(X)
    
    # Loop over splits
    for train_index, test_index in kf10.split(X):
        X_train, X_test = X[train_index], X[test_index]
        y_train, y_test = y[train_index], y[test_index]
        lasso = Lasso(alpha=a).fit(X_train, y_train)
        y_hat = lasso.predict(X_test)
        mse.append(mean_squared_error(y_test, y_hat))
    return np.mean(mse)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Compute MSE over grid of alphas
n_grid = 30
alphas = 10**np.linspace(0,3,n_grid).reshape(-1,1)
MSE = [cv_lasso(X,y,a) for a in alphas]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What is the optimal $\lambda$?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Find minimum alpha
alpha_min = alphas[np.argmin(MSE)]
print(&#39;Best alpha by 10fold CV:&#39;,alpha_min[0])
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Best alpha by 10fold CV: 2.592943797404667
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We now plot the objective function and the implied coefficients at the optimal $\lambda$.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Get coefficients
coefs = []

# Loop over values of alpha
for a in alphas:
    lasso = Lasso(alpha=a).fit(scale(X), y)
    coefs.append(lasso.coef_)
coefs = np.reshape(coefs,(n_grid,-1))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;np.shape(coefs)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;(30, 11)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Figure 6.12
def make_figure_6_12():
    
    # Init
    fig, (ax1,ax2) = plt.subplots(1,2)
    fig.suptitle(&#39;Figure 6.12: Lasso 10-fold CV&#39;)

    # MSE by LOO CV
    ax1.plot(alphas, MSE, alpha=1);
    ax1.axvline(alpha_min, c=&#39;k&#39;, ls=&#39;--&#39;)
    ax1.set_xscale(&#39;log&#39;)
    ax1.set_xlabel(&#39;lambda&#39;); ax1.set_ylabel(&#39;MSE&#39;);

    highlight = [0,1,2,7];

    # Plot coefficients - absolute
    ax2.plot(alphas, coefs[:,highlight], alpha=1)
    ax2.plot(alphas, coefs, c=&#39;grey&#39;, alpha=0.3)
    ax2.axvline(alpha_min, c=&#39;k&#39;, ls=&#39;--&#39;)
    ax2.set_xscale(&#39;log&#39;)
    ax2.set_xlabel(&#39;lambda&#39;); ax2.set_ylabel(&#39;Standardized coefficients&#39;);
    ax2.legend([&#39;Income&#39;, &#39;Limit&#39;, &#39;Rating&#39;, &#39;Student&#39;], fontsize=10);
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;make_figure_6_12()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/05_regularization_131_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>OLS Algebra</title>
      <link>https://matteocourthoud.github.io/course/metrics/05_ols_algebra/</link>
      <pubDate>Fri, 29 Oct 2021 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/course/metrics/05_ols_algebra/</guid>
      <description>&lt;h2 id=&#34;the-gauss-markov-model&#34;&gt;The Gauss Markov Model&lt;/h2&gt;
&lt;h3 id=&#34;definition&#34;&gt;Definition&lt;/h3&gt;
&lt;p&gt;A statistical model for regression data is the &lt;strong&gt;Gauss Markov Model&lt;/strong&gt; if
each of its distributions satisfies the conditions&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Linearity&lt;/strong&gt;: a statistical model $\mathcal{F}$ over data
$\mathcal{D}$ satisfies linearity if for each element of
$\mathcal{F}$, the data can be decomposed in $$
\begin{aligned}
y_ i &amp;amp;= \beta_ 1 x _ {i1} + \dots + \beta_ k x _ {ik} + \varepsilon_ i = x_ i&amp;rsquo;\beta + \varepsilon_ i \newline
\underset{n \times 1}{\vphantom{\beta_ \beta} y} &amp;amp;= \underset{n \times k}{\vphantom{\beta}X} \cdot \underset{k \times 1}{\beta} + \underset{n \times 1}{\vphantom{\beta}\varepsilon}
\end{aligned}
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Strict Exogeneity&lt;/strong&gt;:
$\mathbb E [\varepsilon_i|x_1, \dots, x_n] = 0, \forall i$.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;No Multicollinerity&lt;/strong&gt;: $\mathbb E_n [x_i x_i&amp;rsquo;]$ is strictly
positive definite almost surely. Equivalent to require $rank(X)=k$
with probability $p \to 1$. Intuition: no regressor is a linear
combination of other regressors.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Spherical Error Variance&lt;/strong&gt;:
-$\mathbb E[\varepsilon_i^2 | x] = \sigma^2 &amp;gt; 0, \ \forall i$
-$\mathbb E [\varepsilon_i \varepsilon_j |x ] = 0, \ \forall$
$1 \leq i &amp;lt; j \leq n$&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The &lt;strong&gt;Extended Gauss Markov Model&lt;/strong&gt; also satisfies assumption&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Normal error term&lt;/strong&gt;: $\varepsilon|X \sim N(0, \sigma^2 I_n)$ and
$\varepsilon \perp X$.&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;implications&#34;&gt;Implications&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Note that by (2) and (4) you get &lt;strong&gt;homoskedasticity&lt;/strong&gt;:&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$
Var(\varepsilon_i|x) = \mathbb E[\varepsilon_i^2|x]- \mathbb E[\varepsilon_i|x]^2 = \sigma^2 I \qquad \forall i
$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Strict exogeneity is not restrictive since it is sufficient to
include a constant in the regression to enforce it $$
y_i = \alpha + x_i&amp;rsquo;\beta + (\varepsilon_i - \alpha) \quad \Rightarrow \quad \mathbb E[\varepsilon_i] = \mathbb E_x [ \mathbb E[ \varepsilon_i | x]] = 0
$$&lt;/li&gt;
&lt;li&gt;This implies $\mathbb E[x _ {jk} \varepsilon_i ] = 0$ by the LIE.&lt;/li&gt;
&lt;li&gt;These two conditions together imply
$Cov (x _ {jk} \varepsilon_i ) = 0$.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;projection&#34;&gt;Projection&lt;/h3&gt;
&lt;p&gt;A map $\Pi: V \to V$ is a &lt;strong&gt;projection&lt;/strong&gt; if $\Pi \circ \Pi = \Pi$.&lt;/p&gt;
&lt;p&gt;The Gauss Markov Model assumes that the &lt;strong&gt;conditional expectation
function (CEF)&lt;/strong&gt; $f(X) = \mathbb E[Y|X]$ and the &lt;strong&gt;linear projection&lt;/strong&gt;
$g(X) = X \beta$ coincide.&lt;/p&gt;
&lt;h3 id=&#34;code---dgp&#34;&gt;Code - DGP&lt;/h3&gt;
&lt;p&gt;This code draws 100 observations from the model
$y = 2 x_1 - x_2 + \varepsilon$ where $x_1, x_2 \sim U[0,1]$ and
$\varepsilon \sim N(0,1)$.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# Set seed
Random.seed!(123);

# Set the number of observations
n = 100;

# Set the dimension of X
k = 2;

# Draw a sample of explanatory variables
X = rand(Uniform(0,1), n, k);

# Draw the error term
σ = 1;
ε = rand(Normal(0,1), n, 1) * sqrt(σ);

# Set the parameters
β = [2; -1];

# Calculate the dependent variable
y = X*β + ε;
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;the-ols-estimator&#34;&gt;The OLS estimator&lt;/h2&gt;
&lt;h3 id=&#34;definition-1&#34;&gt;Definition&lt;/h3&gt;
&lt;p&gt;The &lt;strong&gt;sum of squared residuals (SSR)&lt;/strong&gt; is given by $$
Q_n (\beta) \equiv   \frac{1}{n} \sum _ {i=1}^n \left( y_i - x_i&amp;rsquo;\beta \right)^2 = \frac{1}{n} (y - X\beta)&amp;rsquo; (y - X \beta)
$$&lt;/p&gt;
&lt;p&gt;Consider a dataset $\mathcal{D}$ and define
$Q_n(\beta) = \mathbb E_n[(y_i - x_i&amp;rsquo;\beta )^2 ]$. Then the &lt;strong&gt;ordinary
least squares (OLS)&lt;/strong&gt; estimator $\hat \beta _ {OLS}$ is the value of
$\beta$ that minimizes $Q_n(\beta)$.&lt;/p&gt;
&lt;p&gt;When we can write $D = (y, X)$ in matrix form, then $$
\hat \beta _ {OLS} = \arg \min_\beta \frac{1}{n} (y - X \beta)&amp;rsquo; (y - X\beta)
$$&lt;/p&gt;
&lt;h3 id=&#34;derivation&#34;&gt;Derivation&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Under the assumption that $X$ has full rank, the OLS estimator is unique
and it is determined by the normal equations. More explicitly,
$\hat \beta$ is the OLS estimate precisely when $X&amp;rsquo;X \hat \beta = X&amp;rsquo;y$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Taking the FOC: $$
\frac{\partial Q_n (\beta)}{\partial \beta} = -\frac{2}{n} X&amp;rsquo; y  + \frac{2}{n} X&amp;rsquo;X\beta = 0 \quad \Leftrightarrow \quad X&amp;rsquo;X \beta = X&amp;rsquo;y
$$ Since $(X&amp;rsquo;X)^{-1}$ exists by assumption,&lt;/p&gt;
&lt;p&gt;Finally,
$\frac{\partial^2 Q_n (\beta)}{\partial \beta \partial \beta&amp;rsquo;} = X&amp;rsquo;X/n$
is positive definite since $X&amp;rsquo;X$ is positive semi-definite and
$(X&amp;rsquo;X)^{-1}$ exists because $X$ is full rank. Therefore, $Q_n(\beta)$
minimized at $\hat \beta_n$. $$\tag*{$\blacksquare$}$$&lt;/p&gt;
&lt;p&gt;The $k$ equations $X&amp;rsquo;X \hat \beta = X&amp;rsquo;y$ are called &lt;strong&gt;normal
equations&lt;/strong&gt;.&lt;/p&gt;
&lt;h3 id=&#34;futher-objects&#34;&gt;Futher Objects&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Fitted coefficient:
$\hat \beta _ {OLS} = (X&amp;rsquo;X)^{-1} X&amp;rsquo;y = \mathbb E_n [x_i x_i&amp;rsquo;] \mathbb E_n [x_i y_i]$&lt;/li&gt;
&lt;li&gt;Fitted residual: $\hat \varepsilon_i = y_i - x_i&amp;rsquo;\hat \beta$&lt;/li&gt;
&lt;li&gt;Fitted value: $\hat y_i = x_i&amp;rsquo; \hat \beta$&lt;/li&gt;
&lt;li&gt;Predicted coefficient:
$\hat \beta _ {-i} = \mathbb E_n [x _ {-i} x&amp;rsquo; _ {-i}] \mathbb E_n [x _ {-i} y _ {-i}]$&lt;/li&gt;
&lt;li&gt;Prediction error:
$\hat \varepsilon _ {-i} = y_i - x_i&amp;rsquo;\hat \beta _ {-i}$&lt;/li&gt;
&lt;li&gt;Predicted value: $\hat y_i = x_i&amp;rsquo; \hat \beta _ {-i}$&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;notes-on-orthogonality-conditions&#34;&gt;Notes on Orthogonality Conditions&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;The normal equations are equivalent to the moment condition
$\mathbb E_n [x_i \varepsilon_i]= 0$.&lt;/li&gt;
&lt;li&gt;The algebraic result $\mathbb E_n [x_i \hat \varepsilon_i]= 0$ is
called &lt;strong&gt;ortogonality property&lt;/strong&gt; of the OLS residual
$\hat \varepsilon_i$.&lt;/li&gt;
&lt;li&gt;If we have included a constant in the regression,
$\mathbb E_n [\hat \varepsilon_i] = 0$.&lt;/li&gt;
&lt;li&gt;$\mathbb E \Big[\mathbb E_n [x_i \varepsilon_i ] \Big] = 0$ by
strict exogeneity (assumed in GM), but
$\mathbb E_n [x_i \varepsilon_i] \ne \mathbb E [x_i \varepsilon_i] = 0$.
This is why $\hat \beta _ {OLS}$ is just an estimate of $\beta_0$.&lt;/li&gt;
&lt;li&gt;Calculating OLS is like replacing the $j$ equations
$\mathbb E [x _ {ij} \varepsilon_i] = 0$ $\forall j$ with
$\mathbb E_n [x _ {ij} \varepsilon_i] = 0$ $\forall j$ and forcing
them to hold (remindful of GMM).&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;the-projection-matrix&#34;&gt;The Projection Matrix&lt;/h3&gt;
&lt;p&gt;The &lt;strong&gt;projection matrix&lt;/strong&gt; is given by $P = X(X&amp;rsquo;X)^{-1} X&amp;rsquo;$. It has the
following properties: - $PX = X$ - $P \hat \varepsilon = 0 \quad$ ($P$,
$\varepsilon$ orthogonal) -
$P y = X(X&amp;rsquo;X)^{-1} X&amp;rsquo;y = X\hat \beta = \hat y$ - Symmetric: $P=P&amp;rsquo;$,
Idempotent: $PP = P$ -
$tr(P) = tr( X(X&amp;rsquo;X)^{-1} X&amp;rsquo;) = tr( X&amp;rsquo;X(X&amp;rsquo;X)^{-1}) = tr(I_k) = k$ - Its
diagonal elements are $h_{ii} = x_i (X&amp;rsquo;X)^{-1} x_i&amp;rsquo;$ and are called
&lt;strong&gt;leverage&lt;/strong&gt;.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;$h _ {ii} \in [0,1]$ is a normalized length of the observed regressor
vector $x_i$. In the OLS regression framework it captures the relative
influence of observation $i$ on the estimated coefficient. Note that
$\sum _ n h_{ii} = k$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;the-annihilator-matrix&#34;&gt;The Annihilator Matrix&lt;/h3&gt;
&lt;p&gt;The &lt;strong&gt;annihilator matrix&lt;/strong&gt; is given by $M = I_n - P$. It has the
following properties: - $MX = 0 \quad$ ($M$, $X$ orthogonal) -
$M \hat \varepsilon = \hat \varepsilon$ - $M y = \hat \varepsilon$ -
Symmetric: $M=M&amp;rsquo;$, idempotent: $MM = M$ - $tr(M) = n - k$ - Its diagonal
elements are $1 - h_{ii} \in [0,1]$&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Then we can equivalently write $\hat y$ (defined by stacking
$\hat y_i$ into a vector) as $\hat y = Py$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;estimating-beta&#34;&gt;Estimating Beta&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# Estimate beta
β_hat = inv(X&#39;*X)*(X&#39;*y)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 2×1 Array{Float64,2}:
##   1.8821600407711814
##  -0.9429354944506099
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# Equivalent but faster formulation
β_hat = (X&#39;*X)\(X&#39;*y)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 2×1 Array{Float64,2}:
##   1.8821600407711816
##  -0.9429354944506098
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# Even faster (but less intuitive) formulation
β_hat = X\y
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 2×1 Array{Float64,2}:
##   1.8821600407711807
##  -0.9429354944506088
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;equivalent-formulation&#34;&gt;Equivalent Formulation?&lt;/h3&gt;
&lt;p&gt;Generally it’s not true that $$
\hat \beta_{OLS} = \frac{Var(X)}{Cov(X,y)}
$$&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# Wrong formulation
β_wrong = inv(cov(X)) * cov(X, y)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 2×1 Array{Float64,2}:
##   1.8490257777704475
##  -0.9709213554007003
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;equivalent-formulation-correct&#34;&gt;Equivalent Formulation (correct)&lt;/h3&gt;
&lt;p&gt;But it’s true if you include a constant, $\alpha$ $$
y = \alpha + X \beta  + \varepsilon
$$&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# Correct, with constant
α = 3;
y1 = α .+ X*β + ε;
β_hat1 = [ones(n,1) X] \ y1
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 3×1 Array{Float64,2}:
##   3.0362313477745615
##   1.8490257777704477
##  -0.9709213554007007
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;β_correct1 = inv(cov(X)) * cov(X, y1)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 2×1 Array{Float64,2}:
##   1.8490257777704477
##  -0.9709213554007006
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;some-more-objects&#34;&gt;Some More Objects&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# Predicted y
y_hat = X*β_hat;

# Residuals
ε_hat = y - X*β_hat;

# Projection matrix
P = X * inv(X&#39;*X) * X&#39;;

# Annihilator matrix
M = I - P;

# Leverage
h = diag(P);
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;ols-residuals&#34;&gt;OLS Residuals&lt;/h2&gt;
&lt;h3 id=&#34;homoskedasticity&#34;&gt;Homoskedasticity&lt;/h3&gt;
&lt;p&gt;The error is &lt;strong&gt;homoskedastic&lt;/strong&gt; if
$\mathbb E [\varepsilon^2 | x] = \sigma^2$ does not depend on $x$. $$
Var(\varepsilon) = I \sigma^2 = \begin{bmatrix}
\sigma^2 &amp;amp; \dots &amp;amp; 0 \newline\newline&lt;br&gt;
\vdots &amp;amp; \ddots &amp;amp; \vdots \newline
0 &amp;amp; \dots &amp;amp; \sigma^2
\end{bmatrix}
$$&lt;/p&gt;
&lt;p&gt;The error is &lt;strong&gt;heteroskedastic&lt;/strong&gt; if
$\mathbb E [\varepsilon^2 | x] = \sigma^2(x)$ does depend on $x$. $$
Var(\varepsilon) = I \sigma_i^2 =
\begin{bmatrix}
\sigma_1^2 &amp;amp; \dots &amp;amp; 0 \newline
\vdots &amp;amp; \ddots &amp;amp; \vdots \newline
0 &amp;amp; \dots &amp;amp; \sigma_n^2
\end{bmatrix}
$$&lt;/p&gt;
&lt;h3 id=&#34;residual-variance&#34;&gt;Residual Variance&lt;/h3&gt;
&lt;p&gt;The OLS &lt;strong&gt;residual variance&lt;/strong&gt; can be an object of interest even in a
heteroskedastic regression. Its method of moments estimator is given by
$$
\hat \sigma^2 = \frac{1}{n} \sum _ {i=1}^n \hat \varepsilon_i^2
$$&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Note that $\hat \sigma^2$ can be rewritten as $$
\hat \sigma^2 = \frac{1}{n} \varepsilon&amp;rsquo; M&amp;rsquo; M \varepsilon = \frac{1}{n} tr(\varepsilon&amp;rsquo; M \varepsilon) = \frac{1}{n} tr(M \varepsilon&amp;rsquo; \varepsilon)
$$&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;However, the method of moments estimator is a biesed estimator. In fact
$$
\mathbb E[\hat \sigma^2 | X] = \frac{1}{n} \mathbb E [ tr(M \varepsilon&amp;rsquo; \varepsilon) | X] =  \frac{1}{n} tr( M\mathbb E[\varepsilon&amp;rsquo; \varepsilon |X]) = \frac{1}{n} \sum _ {i=1}^n (1-h_{ii}) \sigma^2_i
$$&lt;/p&gt;
&lt;p&gt;Under conditional homoskedasticity, the above expression simplifies to
$$
\mathbb E[\hat \sigma^2 | X] = \frac{1}{n} tr(M) \sigma^2 = \frac{n-k}{n} \sigma^2
$$&lt;/p&gt;
&lt;h3 id=&#34;sample-variance&#34;&gt;Sample Variance&lt;/h3&gt;
&lt;p&gt;The OLS &lt;strong&gt;residual sample variance&lt;/strong&gt; is denoted by $s^2$ and is given by
$$
s^2 = \frac{SSR}{n-k} = \frac{\hat \varepsilon&amp;rsquo;\hat \varepsilon}{n-k} = \frac{1}{n-k}\sum _ {i=1}^n \hat \varepsilon_i^2
$$ Furthermore, the square root of $s^2$, denoted $s$, is called the
standard error of the regression (SER) or the standard error of the
equation (SEE). Not to be confused with other notions of standard error
to be defined later in the course.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The sum of squared residuals can be rewritten as:
$SSR = \hat \varepsilon&amp;rsquo; \hat \varepsilon = \varepsilon&amp;rsquo; M \varepsilon$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The OLS residual sample variance is an unbiased estimator of the error
variance $\sigma^2$.&lt;/p&gt;
&lt;p&gt;Another unbiased estimator of $\sigma^2$ is given by $$
\bar \sigma^2 = \frac{1}{n} \sum _ {i=1}^n (1-h_{ii})^{-1} \hat \varepsilon_i^2
$$&lt;/p&gt;
&lt;h3 id=&#34;uncentered-r2&#34;&gt;Uncentered R^2&lt;/h3&gt;
&lt;p&gt;One measure of the variability of the dependent variable $y_i$ is the
sum of squares $\sum _ {i=1}^n y_i^2 = y&amp;rsquo;y$. There is a decomposition:
$$
\begin{aligned}
y&amp;rsquo;y &amp;amp;= (\hat y + e)&amp;rsquo; (\hat y + \hat \varepsilon) \newline
&amp;amp;= \hat y&amp;rsquo; \hat y + 2 \hat y&amp;rsquo; \hat \varepsilon + \hat \varepsilon&amp;rsquo; \hat \varepsilon e \newline
&amp;amp;= \hat y&amp;rsquo; \hat y + 2 b&amp;rsquo;X&amp;rsquo;\hat \varepsilon + \hat \varepsilon&amp;rsquo; \hat \varepsilon \ \ (\text{since} \ \hat y = Xb) \newline
&amp;amp;= \hat y&amp;rsquo; \hat y + \hat \varepsilon&amp;rsquo;\hat \varepsilon \ \ (\text{since} \ X&amp;rsquo;\hat \varepsilon =0)
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;uncentered&lt;/strong&gt; $\mathbf{R^2}$ is defined as: $$
R^2 _ {uc} \equiv 1 - \frac{\hat \varepsilon&amp;rsquo;\hat \varepsilon}{y&amp;rsquo;y} = 1 - \frac{\mathbb E_n[\hat \varepsilon_i^2]}{\mathbb E_n[y_i^2]} = \frac{ \mathbb E [\hat y_i^2]}{ \mathbb E [y_i^2]}
$$&lt;/p&gt;
&lt;h3 id=&#34;centered-r2&#34;&gt;Centered R^2&lt;/h3&gt;
&lt;p&gt;A more natural measure of variability is the sum of centered squares
$\sum _ {i=1}^n (y_i - \bar y)^2,$ where
$\bar y := \frac{1}{n}\sum _ {i=1}^n y_i$. If the regressors include a
constant, it can be decomposed as $$
\sum _ {i=1}^n (y_i - \bar y)^2 = \sum _ {i=1}^n (\hat y_i - \bar y)^2 + \sum _ {i=1}^n \hat \varepsilon_i^2
$$&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;coefficient of determination&lt;/strong&gt;, $\mathbf{R^2}$, is defined as $$
R^2 \equiv 1 - \frac{\sum _ {i=1}^n \hat \varepsilon_i^2}{\sum _ {i=1}^n (y_i - \bar y)^2 }= \frac{  \sum _ {i=1}^n (\hat y_i - \bar y)^2 } { \sum _ {i=1}^n (y_i - \bar y)^2} = \frac{\mathbb E_n[(\hat y_i - \bar y)^2]}{\mathbb E_n[(y_i - \bar y)^2]}
$$&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Always use the centered $R^2$ unless you really know what you are
doing.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;code---variance&#34;&gt;Code - Variance&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# Biased variance estimator
σ_hat = ε_hat&#39;*ε_hat / n;

# Unbiased estimator 1
σ_hat_2 = ε_hat&#39;*ε_hat / (n-k);

# Unbiased estimator 2
σ_hat_3 = mean( ε_hat.^2 ./ (1 .- h) );
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;code---r2&#34;&gt;Code - R^2&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# R squared - uncentered
R2_uc = (y_hat&#39;*y_hat)/ (y&#39;*y);

# R squared
y_bar = mean(y);
R2 = ((y_hat .- y_bar)&#39;*(y_hat .- y_bar))/ ((y .- y_bar)&#39;*(y .- y_bar));
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;finite-sample-properties-of-ols&#34;&gt;Finite Sample Properties of OLS&lt;/h2&gt;
&lt;h3 id=&#34;conditional-unbiasedness&#34;&gt;Conditional Unbiasedness&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Under the GM assumptions (1)-(3), the OLS estimator is &lt;strong&gt;conditionally
unbiased&lt;/strong&gt;, i.e. the distribution of $\hat \beta _ {OLS}$ is centered at
$\beta_0$: $\mathbb E [\hat \beta | X] = \beta_0$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt; $$
\begin{aligned}
\mathbb E [\hat \beta  | X] &amp;amp;= \mathbb E [ (X&amp;rsquo;X)^{-1} X&amp;rsquo;y | X] = \newline
&amp;amp;= (X&amp;rsquo;X)^{-1} X &amp;rsquo; \mathbb E  [y | X] = \newline
&amp;amp;= (X&amp;rsquo;X)^{-1} X&amp;rsquo; \mathbb E  [X \beta + \varepsilon | X] = \newline
&amp;amp;= (X&amp;rsquo;X)^{-1} X&amp;rsquo;X \beta + (X&amp;rsquo;X)^{-1} X&amp;rsquo; \mathbb E  [\varepsilon | X] = \newline
&amp;amp;= \beta
\end{aligned}
$$ $$\tag*{$\blacksquare$}$$&lt;/p&gt;
&lt;h3 id=&#34;ols-variance&#34;&gt;OLS Variance&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Under the GM assumptions (1)-(3),
$Var(\hat \beta |X) = \sigma^2 (X&amp;rsquo;X)^{-1}$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;: $$
\begin{aligned}
Var(\hat \beta |X) &amp;amp;= Var( (X&amp;rsquo;X)^{-1} X&amp;rsquo;y|X) = \newline
&amp;amp;= ((X&amp;rsquo;X)^{-1} X&amp;rsquo; ) Var(y|X) ((X&amp;rsquo;X)^{-1} X&amp;rsquo; )&amp;rsquo; = \newline
&amp;amp;= ((X&amp;rsquo;X)^{-1} X&amp;rsquo; ) Var(X\beta + \varepsilon|X) ((X&amp;rsquo;X)^{-1} X&amp;rsquo; )&amp;rsquo; = \newline
&amp;amp;= ((X&amp;rsquo;X)^{-1} X&amp;rsquo; ) Var(\varepsilon|X) ((X&amp;rsquo;X)^{-1} X&amp;rsquo; )&amp;rsquo; = \newline
&amp;amp;= ((X&amp;rsquo;X)^{-1} X&amp;rsquo; ) \sigma^2 I ((X&amp;rsquo;X)^{-1} X&amp;rsquo; )&amp;rsquo; =  \newline
&amp;amp;= \sigma^2 (X&amp;rsquo;X)^{-1}
\end{aligned}
$$ $$\tag*{$\blacksquare$}$$&lt;/p&gt;
&lt;p&gt;Higher correlation of the $X$ implies higher variance of the OLS
estimator.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Intuition: individual observations carry less information. You are
exploring a smaller region of the $X$ space.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;blue&#34;&gt;BLUE&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Under the GM assumptions (1)-(3),
$Cov (\hat \beta, \hat \varepsilon ) = 0$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Under the GM assumptions (1)-(3), $\hat \beta _ {OLS}$ is the best (most
efficient) linear, unbiased estimator (&lt;strong&gt;BLUE&lt;/strong&gt;), i.e., for any unbiased
linear estimator $b$: $Var (b|X) \geq Var (\hat \beta |X)$.&lt;/p&gt;
&lt;h3 id=&#34;blue-proof&#34;&gt;BLUE Proof&lt;/h3&gt;
&lt;p&gt;Consider four steps:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Define three objects: (i) $b= Cy$, (ii) $A = (X&amp;rsquo;X)^{-1} X&amp;rsquo;$ such
that $\hat \beta = A y$, and (iii) $D = C-A$.&lt;/li&gt;
&lt;li&gt;Decompose $b$ as $$
\begin{aligned}
b &amp;amp;= (D + A) y = \newline
&amp;amp;=  Dy + Ay = \newline&lt;br&gt;
&amp;amp;= D (X\beta + \varepsilon) + \hat \beta = \newline
&amp;amp;= DX\beta + D \varepsilon + \hat \beta
\end{aligned}
$$&lt;/li&gt;
&lt;li&gt;By assumption, $b$ must be unbiased: $$
\begin{aligned}
\mathbb E [b|X] &amp;amp;= \mathbb E [D(X\beta + \varepsilon) + Ay |X] = \newline
&amp;amp;= \mathbb E [DX\beta|X] + \mathbb E [D\varepsilon |X] + \mathbb E [\hat \beta |X] = \newline
&amp;amp;= DX\beta + D \mathbb E [\varepsilon |X] +\beta \newline&lt;br&gt;
&amp;amp;= DX\beta + \beta
\end{aligned}
$$ Hence, it must be that $DX = 0$&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;blue-proof-2&#34;&gt;BLUE Proof (2)&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;We know by (2)-(3) that $b = D \varepsilon + \hat \beta$. We can now
calculate its variance. $$
\begin{aligned}
Var (b|X) &amp;amp;= Var (\hat \beta + D\varepsilon|X) = \newline
&amp;amp;= Var (Ay + D\varepsilon|X) = \newline
&amp;amp;= Var (AX\beta + (D + A)\varepsilon|X) = \newline
&amp;amp;= Var((D+A)\varepsilon |X) = \newline
&amp;amp;= (D+A)\sigma^2 I (D+A)&amp;rsquo; = \newline
&amp;amp;= \sigma^2 I (DD&amp;rsquo; + AA&amp;rsquo; + DA&amp;rsquo; + AD&amp;rsquo;) = \newline
&amp;amp;= \sigma^2 I (DD&amp;rsquo; + AA&amp;rsquo;) \geq \newline
&amp;amp;\geq \sigma^2 AA&amp;rsquo;= \newline
&amp;amp;= \sigma^2 (X&amp;rsquo;X)^{-1} = \newline
&amp;amp;= Var (\hat \beta|X)
\end{aligned}
$$ since $DA&amp;rsquo;= AD&amp;rsquo; = 0$, $DX = 0$ and $AA&amp;rsquo; = (X&amp;rsquo;X)^{-1}$.
$$\tag*{$\blacksquare$}$$&lt;/li&gt;
&lt;/ol&gt;
&lt;blockquote&gt;
&lt;p&gt;$Var(b | X) \geq Var (\hat{\beta} | X)$ is meant in a positive
definite sense.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;code---variance-1&#34;&gt;Code - Variance&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# Ideal variance of the OLS estimator
var_β = σ * inv(X&#39;*X)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 2×2 Array{Float64,2}:
##   0.0609402  -0.0467732
##  -0.0467732   0.0656808
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# Standard errors
std_β = sqrt.(diag(var_β))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 2-element Array{Float64,1}:
##  0.24686077212177054
##  0.25628257446345265
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Machine Learning Pipeline</title>
      <link>https://matteocourthoud.github.io/course/data-science/06_ml_pipeline/</link>
      <pubDate>Sun, 01 May 2022 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/course/data-science/06_ml_pipeline/</guid>
      <description>&lt;p&gt;In this notebook, we are going to build a pipeline for a general prediction problem.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Standard Imports
from src.utils import *
from src.get_feature_names import get_feature_names
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Set inline graphs
plt.style.use(&#39;seaborn&#39;)
%matplotlib inline
%config InlineBackend.figure_format = &#39;retina&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Usually, in machine learning prediction tasks, the data consists in 3 files:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;X_train.csv&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;y_train.csv&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;X_test.csv&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The purpose of the exercise is to produce a &lt;em&gt;y_test.csv&lt;/em&gt; file, with the predicted values corresponding to the &lt;em&gt;X_test.csv&lt;/em&gt; observations.&lt;/p&gt;
&lt;p&gt;The functions we will write are going to be general and will adapt to any type of dataset, and we will test them on the &lt;a href=&#34;https://www.kaggle.com/c/house-prices-advanced-regression-techniques&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;House Prices Dataset&lt;/a&gt; which is a standard dataset for these kind of tasks. The data consists of 2 files:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;train.csv&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;test.csv&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The target variable that we want to predict is &lt;code&gt;SalePrice&lt;/code&gt;.&lt;/p&gt;
&lt;h2 id=&#34;setup&#34;&gt;Setup&lt;/h2&gt;
&lt;p&gt;First we want to import the data.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Import data
df_train = pd.read_csv(&amp;quot;data/train.csv&amp;quot;)
df_test = pd.read_csv(&amp;quot;data/test.csv&amp;quot;)

print(f&amp;quot;Training data: {np.shape(df_train)} \n Testing data: {np.shape(df_test)}&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Training data: (1460, 81) 
 Testing data: (1459, 80)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The training data also includes the target variable &lt;code&gt;SalePrice&lt;/code&gt;, while, as usual, the testing data does not. We need to separate the training data into two parts:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;X&lt;/code&gt;: the &lt;strong&gt;features&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;y&lt;/code&gt;: the &lt;strong&gt;target&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Select the features
X_train = df_train.drop([&#39;SalePrice&#39;], axis=1)
X_test = df_test

# Check size
print(f&amp;quot;Training features: {np.shape(X_train)} \n Testing features: {np.shape(X_test)}&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Training features: (1460, 80) 
 Testing features: (1459, 80)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Select the target
y_train = df_train[&#39;SalePrice&#39;]

# Check size
print(f&amp;quot;Training target: {np.shape(y_train)}&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Training target: (1460,)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It&amp;rsquo;s good practice to immediately set aside a &lt;strong&gt;validation&lt;/strong&gt; sample with 20% of the observations. The purpose of the validation sample is to give us unbiased estimate of the prediction score. Therefore, we want to set it aside as soon as possible, not to be conditioned in any way by it. Possibly, set it away even before data exploration.&lt;/p&gt;
&lt;p&gt;The more we tune the algorithm based on the feedback received from the validation sample, the more biased our estimate is going to be. Ideally, one would use only cross-validation on the training data and tune only a couple of times using the validation data.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Set aside the validation sample
X_train, X_validation, y_train, y_validation = train_test_split(X_train, y_train, test_size=0.2)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we are ready to build and test our pipeline.&lt;/p&gt;
&lt;h2 id=&#34;data-exploration&#34;&gt;Data Exploration&lt;/h2&gt;
&lt;p&gt;First, let&amp;rsquo;s have a quick look at the data.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;X_train.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;Id&lt;/th&gt;
      &lt;th&gt;MSSubClass&lt;/th&gt;
      &lt;th&gt;MSZoning&lt;/th&gt;
      &lt;th&gt;LotFrontage&lt;/th&gt;
      &lt;th&gt;LotArea&lt;/th&gt;
      &lt;th&gt;Street&lt;/th&gt;
      &lt;th&gt;Alley&lt;/th&gt;
      &lt;th&gt;LotShape&lt;/th&gt;
      &lt;th&gt;LandContour&lt;/th&gt;
      &lt;th&gt;Utilities&lt;/th&gt;
      &lt;th&gt;...&lt;/th&gt;
      &lt;th&gt;ScreenPorch&lt;/th&gt;
      &lt;th&gt;PoolArea&lt;/th&gt;
      &lt;th&gt;PoolQC&lt;/th&gt;
      &lt;th&gt;Fence&lt;/th&gt;
      &lt;th&gt;MiscFeature&lt;/th&gt;
      &lt;th&gt;MiscVal&lt;/th&gt;
      &lt;th&gt;MoSold&lt;/th&gt;
      &lt;th&gt;YrSold&lt;/th&gt;
      &lt;th&gt;SaleType&lt;/th&gt;
      &lt;th&gt;SaleCondition&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;822&lt;/th&gt;
      &lt;td&gt;823&lt;/td&gt;
      &lt;td&gt;60&lt;/td&gt;
      &lt;td&gt;RL&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;12394&lt;/td&gt;
      &lt;td&gt;Pave&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;IR1&lt;/td&gt;
      &lt;td&gt;Lvl&lt;/td&gt;
      &lt;td&gt;AllPub&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;10&lt;/td&gt;
      &lt;td&gt;2007&lt;/td&gt;
      &lt;td&gt;WD&lt;/td&gt;
      &lt;td&gt;Family&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;648&lt;/th&gt;
      &lt;td&gt;649&lt;/td&gt;
      &lt;td&gt;60&lt;/td&gt;
      &lt;td&gt;RL&lt;/td&gt;
      &lt;td&gt;70.0&lt;/td&gt;
      &lt;td&gt;7700&lt;/td&gt;
      &lt;td&gt;Pave&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;Reg&lt;/td&gt;
      &lt;td&gt;Lvl&lt;/td&gt;
      &lt;td&gt;AllPub&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;6&lt;/td&gt;
      &lt;td&gt;2010&lt;/td&gt;
      &lt;td&gt;WD&lt;/td&gt;
      &lt;td&gt;Normal&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;897&lt;/th&gt;
      &lt;td&gt;898&lt;/td&gt;
      &lt;td&gt;90&lt;/td&gt;
      &lt;td&gt;RL&lt;/td&gt;
      &lt;td&gt;64.0&lt;/td&gt;
      &lt;td&gt;7018&lt;/td&gt;
      &lt;td&gt;Pave&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;Reg&lt;/td&gt;
      &lt;td&gt;Lvl&lt;/td&gt;
      &lt;td&gt;AllPub&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;6&lt;/td&gt;
      &lt;td&gt;2009&lt;/td&gt;
      &lt;td&gt;WD&lt;/td&gt;
      &lt;td&gt;Alloca&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1131&lt;/th&gt;
      &lt;td&gt;1132&lt;/td&gt;
      &lt;td&gt;20&lt;/td&gt;
      &lt;td&gt;RL&lt;/td&gt;
      &lt;td&gt;63.0&lt;/td&gt;
      &lt;td&gt;10712&lt;/td&gt;
      &lt;td&gt;Pave&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;Reg&lt;/td&gt;
      &lt;td&gt;Lvl&lt;/td&gt;
      &lt;td&gt;AllPub&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;MnPrv&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;9&lt;/td&gt;
      &lt;td&gt;2007&lt;/td&gt;
      &lt;td&gt;Oth&lt;/td&gt;
      &lt;td&gt;Abnorml&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1038&lt;/th&gt;
      &lt;td&gt;1039&lt;/td&gt;
      &lt;td&gt;160&lt;/td&gt;
      &lt;td&gt;RM&lt;/td&gt;
      &lt;td&gt;21.0&lt;/td&gt;
      &lt;td&gt;1533&lt;/td&gt;
      &lt;td&gt;Pave&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;Reg&lt;/td&gt;
      &lt;td&gt;Lvl&lt;/td&gt;
      &lt;td&gt;AllPub&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;2009&lt;/td&gt;
      &lt;td&gt;WD&lt;/td&gt;
      &lt;td&gt;Normal&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;5 rows × 80 columns&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;The &lt;code&gt;Id&lt;/code&gt; column is clearly not useful for prediction, let&amp;rsquo;s drop it from both datasets.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Drop Id 
X_train.drop([&amp;quot;Id&amp;quot;], axis=1, inplace=True)
X_test.drop([&amp;quot;Id&amp;quot;], axis=1, inplace=True)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we want to identify categorical and numerical variables.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Save column types
numerical_cols = list(X_train.describe().columns)
categorical_cols = list(X_train.describe(include=object).columns)
print(&amp;quot;There are %i numerical and %i categorical variables&amp;quot; % (len(numerical_cols), len(categorical_cols)))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;There are 36 numerical and 43 categorical variables
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s start by analyzing the numerical variables.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;X_numerical = X_train.loc[:, numerical_cols]
corr = X_numerical.corr()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;fig, ax = plt.subplots(1, 1, figsize=(10,10))
fig.suptitle(&amp;quot;Correlation between categorical variables&amp;quot;, fontsize=16)
cbar_ax = fig.add_axes([.95, .12, .05, .76])
sns.heatmap(corr, vmin=-1, vmax=1, center=0, cmap=sns.diverging_palette(20, 220, n=20), 
            square=True, ax=ax, cbar_ax = cbar_ax)
plt.show()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/06_ml_pipeline_24_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;For the non/numeric columns, we need a further option.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;unique_values = X_train.describe(include=object).T.unique
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Plot
fig, ax = plt.subplots(1, 1, figsize=(10,6))
fig.suptitle(&amp;quot;Distribution of unique values for categorical variables&amp;quot;, fontsize=16)
sns.histplot(data=unique_values)
plt.show();
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/06_ml_pipeline_27_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s save the identity of the numerical and categorical columns.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Save column types
numerical_cols = list(X_train.describe().columns)
categorical_cols = list(X_train.describe(include=object).columns)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;How many missing values are there in the dataset?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;missing_values = X_train.isnull().sum().sort_values(ascending=True)[-20:] / len(X_train)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;fig, ax = plt.subplots(figsize=(10,8))
ax.set_title(&amp;quot;Variables with most missing values&amp;quot;, fontsize=16)
ax.barh(np.arange(len(missing_values)), missing_values)
ax.set_yticks(np.arange(len(missing_values)))
ax.set_yticklabels(missing_values.index)
ax.set_xlabel(&#39;Percentage of missig values&#39;)
plt.show()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/06_ml_pipeline_32_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Around 10% of each feature is missing. We will have to deal with that.&lt;/p&gt;
&lt;h2 id=&#34;pre-processing&#34;&gt;Pre-processing&lt;/h2&gt;
&lt;p&gt;First, let&amp;rsquo;s process &lt;strong&gt;numerical variables&lt;/strong&gt;. We want to do two things:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;inpute missing values&lt;/li&gt;
&lt;li&gt;standardize all variables&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Which imputer should to use? It depends on the &lt;strong&gt;type of missing data&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Missing absolutely at random&lt;/strong&gt;: as the name says, in this case we believe that missing values are distributed uniformly at random, independently across variables.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;In this case, the only information on missing values comes from the distribution of non-missing values of the same variable.&lt;/li&gt;
&lt;li&gt;No information on missing values is contained in other variables.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Missing at random&lt;/strong&gt;: in this case, missing values are random, conditional on values of other observed variables.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;In this case, information in other variables might help filling missing values.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Missing non at random&lt;/strong&gt;: in this last case, missing values depend on information that we do not observe.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;This is the most tricky category of missing values since data alone does not tell us which values might be missing. For example, we might have that older women might be less likely to report the age.&lt;/li&gt;
&lt;li&gt;If we consider the data missing at random (absolutely or not), we would underestimate the missing ages.&lt;/li&gt;
&lt;li&gt;External information such as the sample population might help. For example, we could estimate the probability of not reporting the age and fill the missing values with the expected age, &lt;em&gt;conditional&lt;/em&gt; on age not being reported.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;So, which imputers are readily available in &lt;code&gt;sklearn&lt;/code&gt; for numerical data?&lt;/p&gt;
&lt;p&gt;For data &lt;strong&gt;missing absolutely at random&lt;/strong&gt;, there is one standard &lt;code&gt;sklearn&lt;/code&gt; library: &lt;code&gt;SimpleImputer()&lt;/code&gt;. It allows different &lt;code&gt;strategy&lt;/code&gt; options such as&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;&amp;quot;mean&amp;quot;&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;&amp;quot;median&amp;quot;&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;&amp;quot;most_frequent&amp;quot;&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For data &lt;strong&gt;missing at random&lt;/strong&gt;, there are multiple &lt;code&gt;sklearn&lt;/code&gt; libraries:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;KNNImputer()&lt;/code&gt;: uses KNN&lt;/li&gt;
&lt;li&gt;&lt;code&gt;IterativeImputer()&lt;/code&gt;: uses a variety of ML algorithms
&lt;ul&gt;
&lt;li&gt;see comparison &lt;a href=&#34;https://scikit-learn.org/stable/auto_examples/impute/plot_iterative_imputer_variants_comparison.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;After we have inputed missing values, we want to standardize numerical variables to make the algorithm more efficient and robust to outliers.&lt;/p&gt;
&lt;p&gt;The two main options for standardization are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;StandardScaler()&lt;/code&gt;: which normalizes each variable to mean zero and unit variance&lt;/li&gt;
&lt;li&gt;&lt;code&gt;MinMaxScaler()&lt;/code&gt;: which normalizes each variable to an interval between zero an one&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Inputer for numerical variables
num = Pipeline(steps=[
    (&#39;ii&#39;, IterativeImputer()),
    (&#39;ss&#39;, StandardScaler())
    ])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For &lt;strong&gt;categorical variables&lt;/strong&gt;, we do not have to worry about scaling. However, we still need to impute missing values and, crucially, we need to transform them into numerical variables. This process is called &lt;strong&gt;encoding&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Which imputer should to use?&lt;/p&gt;
&lt;p&gt;For data &lt;strong&gt;missing absolutely at random&lt;/strong&gt;, the only available &lt;code&gt;strategy&lt;/code&gt; option for &lt;code&gt;SimpleImputer()&lt;/code&gt; is&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;&amp;quot;most_frequent&amp;quot;&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For data &lt;strong&gt;missing at random&lt;/strong&gt;, we can still use both&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;KNNImputer()&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;IterativeImputer()&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For &lt;strong&gt;encoding&lt;/strong&gt; categorical variables, the standard option is &lt;code&gt;OneHotEncoder()&lt;/code&gt; which generates unique binary variables out of every values of the categorical variable.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# One Hot Encoder for categorical data
cat = Pipeline(steps=[
    (&#39;si&#39;, SimpleImputer(strategy=&amp;quot;most_frequent&amp;quot;)),
    (&#39;ohe&#39;, OneHotEncoder(handle_unknown=&amp;quot;ignore&amp;quot;)),
    ])
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Preprocess column transformer for preprocessing data
preprocess = ColumnTransformer(
                    transformers=[
                        (&#39;num&#39;, num, numerical_cols),
                        (&#39;cat&#39;, cat, categorical_cols),
                    ])
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;information-and-components&#34;&gt;Information and components&lt;/h2&gt;
&lt;p&gt;How much information is contained in our dataset? It is a dense or sparse dataset?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;X_clean = num.fit_transform(X_numerical)
pca = PCA().fit(X_clean)
explained_variance = pca.explained_variance_ratio_
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;fig, (ax1, ax2) = plt.subplots(1,2, figsize=(15,6))
fig.suptitle(&#39;Principal Component Analysis&#39;, fontsize=16);

# Relative 
ax1.plot(range(len(explained_variance)), explained_variance)
ax1.set_ylabel(&#39;Prop. Variance Explained&#39;)
ax1.set_xlabel(&#39;Principal Component&#39;);

# Cumulative
ax2.plot(range(len(explained_variance)), np.cumsum(explained_variance))
ax2.set_ylabel(&#39;Cumulative Variance Explained&#39;);
ax2.set_xlabel(&#39;Principal Component&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/06_ml_pipeline_48_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;feature-importance&#34;&gt;Feature Importance&lt;/h2&gt;
&lt;p&gt;Before starting our prediction analysis, we would like to understand which variables are most important for our prediction problem.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def plot_featureimportance(importance, preprocess):
    df = pd.DataFrame({&amp;quot;names&amp;quot;: get_feature_names(preprocess), &amp;quot;values&amp;quot;: importance})
    df = df.sort_values(&amp;quot;values&amp;quot;).iloc[:20, :]
    # plot
    fig, ax = plt.subplots(figsize=(10,8))
    ax.set_title(&amp;quot;Feature importance&amp;quot;, fontsize=16)
    sns.barplot(y=&amp;quot;names&amp;quot;, x=&amp;quot;values&amp;quot;, data=df)
    ax.barh(np.arange(len(df)), df[&amp;quot;values&amp;quot;])
    plt.show()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We start with linear regression feature importance: we standardize all variables to be mean vero and unit variance, and we run a linear regression over the test set.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def featureimportance_lr(X, y):
    X_clean = preprocess.fit_transform(X)
    # fit the model
    model = LinearRegression()
    model.fit(X_clean, y)
    # get importance
    importance = np.abs(model.coef_)
    plot_featureimportance(importance, preprocess)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Plot linear feature importance
featureimportance_lr(X_train, y_train)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/06_ml_pipeline_54_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;We now look at regression tree feature importance.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def featureimportance_forest(X, y):
    X_clean = preprocess.fit_transform(X)
    # fit the model
    model = RandomForestRegressor()
    model.fit(X_clean, y)
    # get importance
    importance = model.feature_importances_
    plot_featureimportance(importance, preprocess)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Plot tree feature importance
featureimportance_forest(X_train, y_train)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/06_ml_pipeline_57_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;weighting&#34;&gt;Weighting&lt;/h2&gt;
&lt;p&gt;Another important check to perform concerns weighting. Is the distribution of our objective variable the same in the training and in the test sample? If it is not the case, we might get a poor performance just because our training sample is not representative of our testing sample.&lt;/p&gt;
&lt;p&gt;This is something that usually &lt;strong&gt;we cannot test&lt;/strong&gt;, since we do not have access to the distribution of the target variable in the test data. However, we might be given the information ex-ante as a warning.&lt;/p&gt;
&lt;p&gt;In this case, we perform the analysis on the validation set. Since we have selected the validation set at random, we do not expect significant differences.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15,6))

# Plot 1
sns.histplot(data=y_train, kde=True, ax=ax1)
sns.histplot(data=y_validation, kde=True, ax=ax1, color=&#39;orange&#39;)
ax1.set_title(&amp;quot;Density Function of y&amp;quot;, fontsize=16);
ax1.legend([&#39;y train&#39;, &#39;y validation&#39;])

# Plot 2
sns.histplot(data=y_train,  element=&amp;quot;step&amp;quot;, fill=False,
    cumulative=True, stat=&amp;quot;density&amp;quot;, common_norm=False, ax=ax2)
sns.histplot(data=y_validation, element=&amp;quot;step&amp;quot;, fill=False,
    cumulative=True, stat=&amp;quot;density&amp;quot;, common_norm=False, ax=ax2, color=&#39;orange&#39;)
ax2.set_title(&amp;quot;Cumulative Distribution of y&amp;quot;, fontsize=16);
ax2.legend([&#39;y train&#39;, &#39;y validation&#39;]);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/06_ml_pipeline_60_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Since the size of the test sample is smaller than the size of the training sample, the two densities are different. However, the distributions indicate that the standardized distributions are the same.&lt;/p&gt;
&lt;h2 id=&#34;model&#34;&gt;Model&lt;/h2&gt;
&lt;p&gt;There are many models to choose among.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# prepare models
models = {&amp;quot;Lasso&amp;quot;: Lasso(alpha=100),
          &amp;quot;Ridge&amp;quot;: BayesianRidge(),
          &amp;quot;KNN&amp;quot;: KNeighborsRegressor(),
          &amp;quot;Kernel&amp;quot;: KernelRidge(),
          &amp;quot;Naive&amp;quot;: GaussianNB(),
          &amp;quot;SVM&amp;quot;: SVR(),
          &amp;quot;Ada&amp;quot;: AdaBoostRegressor(),
          &amp;quot;Tree&amp;quot;: DecisionTreeRegressor(),
          &amp;quot;Forest&amp;quot;: RandomForestRegressor(),
          &amp;quot;GBoost&amp;quot;: GradientBoostingRegressor(),
          &amp;quot;XGBoost&amp;quot;: XGBRegressor(),
          &amp;quot;LGBoost&amp;quot;: LGBMRegressor()}
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def evaluate_model(model, name, X, y, cv, scoring):
    X_clean = preprocess.fit_transform(X)
    start = time.perf_counter()
    cv_results = cross_val_score(model, X_clean, y, cv=cv, scoring=scoring)
    t = time.perf_counter()-start
    score = {&amp;quot;model&amp;quot;:name, &amp;quot;mean&amp;quot;:-np.mean(cv_results), &amp;quot;std&amp;quot;:np.std(cv_results), &amp;quot;time&amp;quot;:t}
    print(&amp;quot;%s: %f (%f) in %f seconds&amp;quot; % (name, -np.mean(cv_results), np.std(cv_results), t))
    return score
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def plot_model_scores(scores):
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15,6))
    fig.suptitle(&amp;quot;Comparing algorithms&amp;quot;, fontsize=16)
    
    # Plot 1
    scores.sort_values(&amp;quot;mean&amp;quot;, ascending=False, inplace=True)
    ax1.set_title(&amp;quot;Mean squared error&amp;quot;, fontsize=16)
    ax1.barh(range(len(scores)), scores[&amp;quot;mean&amp;quot;], xerr=scores[&amp;quot;std&amp;quot;])
    ax1.set_yticks(range(len(scores)))
    ax1.set_yticklabels([s for s in scores[&amp;quot;model&amp;quot;]])
    
    # Plot 2
    scores.sort_values(&amp;quot;time&amp;quot;, ascending=False, inplace=True)
    ax2.set_title(&amp;quot;Time&amp;quot;, fontsize=16)
    ax2.barh(range(len(scores)), scores[&amp;quot;time&amp;quot;], color=&#39;tab:orange&#39;)
    ax2.set_yticks(range(len(scores)))
    ax2.set_yticklabels([s for s in scores[&amp;quot;model&amp;quot;]])
    plt.show()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def compare_models(models):
    scores = pd.DataFrame()
    cv = KFold(n_splits=5)
    scoring = &#39;neg_mean_squared_error&#39;
    for name, model in models.items():
        score = evaluate_model(model, name, X_validation, y_validation, cv, scoring)
        scores = scores.append(score, ignore_index=True)
    return scores
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;scores = compare_models(models)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Lasso: 747411443.913101 (462917309.181485) in 0.109821 seconds
Ridge: 718774315.061634 (487089023.387329) in 0.472070 seconds
KNN: 1756639001.600806 (1476470798.673143) in 0.019063 seconds
Kernel: 844681295.934677 (476183041.447080) in 0.085055 seconds
Naive: 5254835359.080946 (2916476370.114636) in 0.045415 seconds
SVM: 6141030577.726756 (3241262535.954060) in 0.046852 seconds
Ada: 1513638885.120911 (1332241015.479751) in 0.306255 seconds
Tree: 3258264310.733547 (2139525308.773295) in 0.018476 seconds
Forest: 1324403652.968275 (1246235286.003631) in 1.105161 seconds
GBoost: 1200654655.518314 (1053677796.098979) in 0.494536 seconds
XGBoost: 1819197282.034136 (1587393748.901112) in 0.692401 seconds
LGBoost: 1318077152.379926 (1278188928.507894) in 0.157495 seconds
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plot_model_scores(scores)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/06_ml_pipeline_69_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;pipeline&#34;&gt;Pipeline&lt;/h2&gt;
&lt;p&gt;We are now ready to pick a model.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Set model
model = LGBMRegressor()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We need to choose a cross-validation procedure to test our model.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;cv = KFold()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally, we can combine all the parts into a single pipeline.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;final_pipeline = Pipeline(steps=[
        (&#39;preprocess&#39;, preprocess),
        (&#39;model&#39;, model)
        ])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we can decide which parts of the pipeline to test.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Select parameters to explore
param_grid = {&#39;preprocess__num__ii&#39;: [SimpleImputer(), KNNImputer(), IterativeImputer()],
              &#39;preprocess__cat__si__strategy&#39;: [&amp;quot;most_frequent&amp;quot;, &amp;quot;constant&amp;quot;],
              &#39;model__learning_rate&#39;: [0.1, 0.2],
              &#39;model__subsample&#39;: [1.0, 0.5],
              &#39;model__max_depth&#39;: [30, -1]}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We now generate a grid of parameters we want to search over.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Save pipeline
grid_search = GridSearchCV(final_pipeline, 
                           param_grid, 
                           cv=cv,
                           n_jobs=-1, 
                           scoring=&#39;neg_mean_squared_error&#39;,
                           verbose=3)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We fit the pipeline and pick the best estimator, from the cross-validation score.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Fit pipeline
grid_search.fit(X_train, y_train)
grid_search.best_estimator_
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Fitting 5 folds for each of 48 candidates, totalling 240 fits





Pipeline(steps=[(&#39;preprocess&#39;,
                 ColumnTransformer(transformers=[(&#39;num&#39;,
                                                  Pipeline(steps=[(&#39;ii&#39;,
                                                                   KNNImputer()),
                                                                  (&#39;ss&#39;,
                                                                   StandardScaler())]),
                                                  [&#39;MSSubClass&#39;, &#39;LotFrontage&#39;,
                                                   &#39;LotArea&#39;, &#39;OverallQual&#39;,
                                                   &#39;OverallCond&#39;, &#39;YearBuilt&#39;,
                                                   &#39;YearRemodAdd&#39;, &#39;MasVnrArea&#39;,
                                                   &#39;BsmtFinSF1&#39;, &#39;BsmtFinSF2&#39;,
                                                   &#39;BsmtUnfSF&#39;, &#39;TotalBsmtSF&#39;,
                                                   &#39;1stFlrSF&#39;, &#39;2ndFlrSF&#39;,
                                                   &#39;LowQualFinSF&#39;, &#39;GrLivArea&#39;,
                                                   &#39;BsmtFullBat...
                                                   &#39;LotConfig&#39;, &#39;LandSlope&#39;,
                                                   &#39;Neighborhood&#39;, &#39;Condition1&#39;,
                                                   &#39;Condition2&#39;, &#39;BldgType&#39;,
                                                   &#39;HouseStyle&#39;, &#39;RoofStyle&#39;,
                                                   &#39;RoofMatl&#39;, &#39;Exterior1st&#39;,
                                                   &#39;Exterior2nd&#39;, &#39;MasVnrType&#39;,
                                                   &#39;ExterQual&#39;, &#39;ExterCond&#39;,
                                                   &#39;Foundation&#39;, &#39;BsmtQual&#39;,
                                                   &#39;BsmtCond&#39;, &#39;BsmtExposure&#39;,
                                                   &#39;BsmtFinType1&#39;,
                                                   &#39;BsmtFinType2&#39;, &#39;Heating&#39;,
                                                   &#39;HeatingQC&#39;, &#39;CentralAir&#39;,
                                                   &#39;Electrical&#39;, ...])])),
                (&#39;model&#39;, LGBMRegressor(max_depth=30))])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We have three ways of testing the quality of fit of our model:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;score on the training data&lt;/li&gt;
&lt;li&gt;score on the validation data&lt;/li&gt;
&lt;li&gt;score on the test data&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Score on the training data&lt;/strong&gt;: this is a biased score since we have picked the model that was best fitting the training data. Kfold cross-validation is efficient in terms of data use, but still evaluates the model over the same data it was trained.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Cross/validation score
y_train_hat = grid_search.best_estimator_.predict(X_train)
train_rmse = mean_squared_error(y_train, y_train_hat, squared=False)
print(&#39;RMSE on training data :&#39;, train_rmse)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;RMSE on training data : 12151.309344378069
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Score on the validation data&lt;/strong&gt;: this is an unbiased score since we have left out this sample exactly for this purpose. However, be aware that the validation score is unbiased on on the first run. Once we change the grid and pick the algorithm based on previous validation data scores, also this score becomes biased.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Validation set score
y_validation_hat = grid_search.best_estimator_.predict(X_validation)
validation_rmse = mean_squared_error(y_validation, y_validation_hat, squared=False)
print(&#39;RMSE on validation data :&#39;, validation_rmse)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;RMSE on validation data : 27676.358798908263
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Final predictions&lt;/strong&gt;: we can now use our model to output the predictions.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Validation score
y_test_hat = grid_search.best_estimator_.predict(X_test)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[CV 2/5] END model__learning_rate=0.1, model__max_depth=30, model__subsample=1.0, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=KNNImputer();, score=-945800410.003 total time=   0.2s
[CV 4/5] END model__learning_rate=0.1, model__max_depth=30, model__subsample=1.0, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=IterativeImputer();, score=-570100776.470 total time=   0.4s
[CV 5/5] END model__learning_rate=0.1, model__max_depth=30, model__subsample=1.0, preprocess__cat__si__strategy=constant, preprocess__num__ii=KNNImputer();, score=-1239206018.769 total time=   0.2s
[CV 2/5] END model__learning_rate=0.1, model__max_depth=30, model__subsample=0.5, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=SimpleImputer();, score=-997691407.890 total time=   0.1s
[CV 5/5] END model__learning_rate=0.1, model__max_depth=30, model__subsample=0.5, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=SimpleImputer();, score=-1226710083.913 total time=   0.2s
[CV 2/5] END model__learning_rate=0.1, model__max_depth=30, model__subsample=0.5, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=IterativeImputer();, score=-1024476538.628 total time=   0.4s
[CV 3/5] END model__learning_rate=0.1, model__max_depth=30, model__subsample=0.5, preprocess__cat__si__strategy=constant, preprocess__num__ii=KNNImputer();, score=-541737588.393 total time=   0.2s
[CV 3/5] END model__learning_rate=0.1, model__max_depth=-1, model__subsample=1.0, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=SimpleImputer();, score=-513492880.363 total time=   0.1s
[CV 1/5] END model__learning_rate=0.1, model__max_depth=-1, model__subsample=1.0, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=KNNImputer();, score=-1085304295.641 total time=   0.2s
[CV 2/5] END model__learning_rate=0.1, model__max_depth=-1, model__subsample=1.0, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=IterativeImputer();, score=-1024476538.628 total time=   0.4s
[CV 2/5] END model__learning_rate=0.1, model__max_depth=-1, model__subsample=1.0, preprocess__cat__si__strategy=constant, preprocess__num__ii=KNNImputer();, score=-1033297772.352 total time=   0.2s
[CV 5/5] END model__learning_rate=0.1, model__max_depth=-1, model__subsample=1.0, preprocess__cat__si__strategy=constant, preprocess__num__ii=IterativeImputer();, score=-1260432508.211 total time=   0.4s
[CV 1/5] END model__learning_rate=0.1, model__max_depth=-1, model__subsample=0.5, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=IterativeImputer();, score=-1038744584.272 total time=   0.6s
[CV 5/5] END model__learning_rate=0.1, model__max_depth=-1, model__subsample=0.5, preprocess__cat__si__strategy=constant, preprocess__num__ii=IterativeImputer();, score=-1260432508.211 total time=   0.4s
[CV 2/5] END model__learning_rate=0.2, model__max_depth=30, model__subsample=1.0, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=IterativeImputer();, score=-1045009894.967 total time=   0.4s
[CV 2/5] END model__learning_rate=0.2, model__max_depth=30, model__subsample=1.0, preprocess__cat__si__strategy=constant, preprocess__num__ii=KNNImputer();, score=-1064162610.680 total time=   0.2s
[CV 4/5] END model__learning_rate=0.2, model__max_depth=30, model__subsample=1.0, preprocess__cat__si__strategy=constant, preprocess__num__ii=IterativeImputer();, score=-600837629.536 total time=   0.4s
[CV 5/5] END model__learning_rate=0.2, model__max_depth=30, model__subsample=0.5, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=KNNImputer();, score=-1243376201.323 total time=   0.2s
[CV 3/5] END model__learning_rate=0.2, model__max_depth=30, model__subsample=0.5, preprocess__cat__si__strategy=constant, preprocess__num__ii=SimpleImputer();, score=-539187722.319 total time=   0.2s
[CV 1/5] END model__learning_rate=0.2, model__max_depth=30, model__subsample=0.5, preprocess__cat__si__strategy=constant, preprocess__num__ii=KNNImputer();, score=-1101298878.719 total time=   0.2s
[CV 3/5] END model__learning_rate=0.2, model__max_depth=30, model__subsample=0.5, preprocess__cat__si__strategy=constant, preprocess__num__ii=IterativeImputer();, score=-533278666.873 total time=   0.4s
[CV 4/5] END model__learning_rate=0.2, model__max_depth=-1, model__subsample=1.0, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=KNNImputer();, score=-544430870.856 total time=   0.2s
[CV 2/5] END model__learning_rate=0.2, model__max_depth=-1, model__subsample=1.0, preprocess__cat__si__strategy=constant, preprocess__num__ii=SimpleImputer();, score=-1024576434.974 total time=   0.1s
[CV 5/5] END model__learning_rate=0.2, model__max_depth=-1, model__subsample=1.0, preprocess__cat__si__strategy=constant, preprocess__num__ii=SimpleImputer();, score=-1271758627.677 total time=   0.2s
[CV 1/5] END model__learning_rate=0.2, model__max_depth=-1, model__subsample=1.0, preprocess__cat__si__strategy=constant, preprocess__num__ii=IterativeImputer();, score=-1064924532.138 total time=   0.5s
[CV 4/5] END model__learning_rate=0.2, model__max_depth=-1, model__subsample=0.5, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=IterativeImputer();, score=-643767514.223 total time=   0.4s
[CV 4/5] END model__learning_rate=0.2, model__max_depth=-1, model__subsample=0.5, preprocess__cat__si__strategy=constant, preprocess__num__ii=KNNImputer();, score=-590633071.038 total time=   0.2s
[CV 3/5] END model__learning_rate=0.1, model__max_depth=30, model__subsample=1.0, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=KNNImputer();, score=-515317030.231 total time=   0.2s
[CV 1/5] END model__learning_rate=0.1, model__max_depth=30, model__subsample=1.0, preprocess__cat__si__strategy=constant, preprocess__num__ii=SimpleImputer();, score=-1102984449.306 total time=   0.1s
[CV 4/5] END model__learning_rate=0.1, model__max_depth=30, model__subsample=1.0, preprocess__cat__si__strategy=constant, preprocess__num__ii=SimpleImputer();, score=-543635446.429 total time=   0.2s
[CV 4/5] END model__learning_rate=0.1, model__max_depth=30, model__subsample=1.0, preprocess__cat__si__strategy=constant, preprocess__num__ii=KNNImputer();, score=-513174790.496 total time=   0.2s
[CV 1/5] END model__learning_rate=0.1, model__max_depth=30, model__subsample=0.5, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=SimpleImputer();, score=-1094302605.510 total time=   0.2s
[CV 4/5] END model__learning_rate=0.1, model__max_depth=30, model__subsample=0.5, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=SimpleImputer();, score=-530900512.729 total time=   0.1s
[CV 4/5] END model__learning_rate=0.1, model__max_depth=30, model__subsample=0.5, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=KNNImputer();, score=-518644185.752 total time=   0.2s
[CV 2/5] END model__learning_rate=0.1, model__max_depth=30, model__subsample=0.5, preprocess__cat__si__strategy=constant, preprocess__num__ii=SimpleImputer();, score=-995475493.332 total time=   0.2s
[CV 5/5] END model__learning_rate=0.1, model__max_depth=30, model__subsample=0.5, preprocess__cat__si__strategy=constant, preprocess__num__ii=SimpleImputer();, score=-1261991041.562 total time=   0.2s
[CV 1/5] END model__learning_rate=0.1, model__max_depth=30, model__subsample=0.5, preprocess__cat__si__strategy=constant, preprocess__num__ii=IterativeImputer();, score=-1047546222.332 total time=   0.6s
[CV 5/5] END model__learning_rate=0.1, model__max_depth=-1, model__subsample=1.0, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=IterativeImputer();, score=-1219549323.929 total time=   0.4s
[CV 3/5] END model__learning_rate=0.1, model__max_depth=-1, model__subsample=1.0, preprocess__cat__si__strategy=constant, preprocess__num__ii=IterativeImputer();, score=-532594533.605 total time=   0.4s
[CV 3/5] END model__learning_rate=0.1, model__max_depth=-1, model__subsample=0.5, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=KNNImputer();, score=-515317030.231 total time=   0.2s
[CV 1/5] END model__learning_rate=0.1, model__max_depth=-1, model__subsample=0.5, preprocess__cat__si__strategy=constant, preprocess__num__ii=SimpleImputer();, score=-1102984449.306 total time=   0.1s
[CV 4/5] END model__learning_rate=0.1, model__max_depth=-1, model__subsample=0.5, preprocess__cat__si__strategy=constant, preprocess__num__ii=SimpleImputer();, score=-543635446.429 total time=   0.2s
[CV 3/5] END model__learning_rate=0.1, model__max_depth=-1, model__subsample=0.5, preprocess__cat__si__strategy=constant, preprocess__num__ii=KNNImputer();, score=-541737588.393 total time=   0.2s
[CV 1/5] END model__learning_rate=0.2, model__max_depth=30, model__subsample=1.0, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=SimpleImputer();, score=-1174830723.077 total time=   0.1s
[CV 4/5] END model__learning_rate=0.2, model__max_depth=30, model__subsample=1.0, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=SimpleImputer();, score=-589395877.632 total time=   0.1s
[CV 5/5] END model__learning_rate=0.2, model__max_depth=30, model__subsample=1.0, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=KNNImputer();, score=-1243376201.323 total time=   0.2s
[CV 2/5] END model__learning_rate=0.2, model__max_depth=30, model__subsample=1.0, preprocess__cat__si__strategy=constant, preprocess__num__ii=SimpleImputer();, score=-1024576434.974 total time=   0.1s
[CV 5/5] END model__learning_rate=0.2, model__max_depth=30, model__subsample=1.0, preprocess__cat__si__strategy=constant, preprocess__num__ii=SimpleImputer();, score=-1271758627.677 total time=   0.1s
[CV 1/5] END model__learning_rate=0.2, model__max_depth=30, model__subsample=1.0, preprocess__cat__si__strategy=constant, preprocess__num__ii=IterativeImputer();, score=-1064924532.138 total time=   0.6s
[CV 5/5] END model__learning_rate=0.2, model__max_depth=30, model__subsample=0.5, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=IterativeImputer();, score=-1249207388.654 total time=   0.3s
[CV 1/5] END model__learning_rate=0.2, model__max_depth=30, model__subsample=0.5, preprocess__cat__si__strategy=constant, preprocess__num__ii=IterativeImputer();, score=-1064924532.138 total time=   0.6s
[CV 4/5] END model__learning_rate=0.2, model__max_depth=-1, model__subsample=1.0, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=IterativeImputer();, score=-643767514.223 total time=   0.4s
[CV 5/5] END model__learning_rate=0.2, model__max_depth=-1, model__subsample=1.0, preprocess__cat__si__strategy=constant, preprocess__num__ii=KNNImputer();, score=-1210584444.764 total time=   0.2s
[CV 3/5] END model__learning_rate=0.2, model__max_depth=-1, model__subsample=0.5, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=SimpleImputer();, score=-524058509.993 total time=   0.1s
[CV 1/5] END model__learning_rate=0.2, model__max_depth=-1, model__subsample=0.5, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=KNNImputer();, score=-1079192998.183 total time=   0.2s
[CV 3/5] END model__learning_rate=0.2, model__max_depth=-1, model__subsample=0.5, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=IterativeImputer();, score=-576066791.159 total time=   0.4s
[CV 3/5] END model__learning_rate=0.2, model__max_depth=-1, model__subsample=0.5, preprocess__cat__si__strategy=constant, preprocess__num__ii=KNNImputer();, score=-564186901.908 total time=   0.2s
[CV 3/5] END model__learning_rate=0.1, model__max_depth=30, model__subsample=1.0, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=SimpleImputer();, score=-513492880.363 total time=   0.2s
[CV 1/5] END model__learning_rate=0.1, model__max_depth=30, model__subsample=1.0, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=IterativeImputer();, score=-1038744584.272 total time=   0.7s
[CV 3/5] END model__learning_rate=0.1, model__max_depth=30, model__subsample=0.5, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=SimpleImputer();, score=-513492880.363 total time=   0.2s
[CV 1/5] END model__learning_rate=0.1, model__max_depth=30, model__subsample=0.5, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=KNNImputer();, score=-1085304295.641 total time=   0.2s
[CV 5/5] END model__learning_rate=0.1, model__max_depth=30, model__subsample=0.5, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=IterativeImputer();, score=-1219549323.929 total time=   0.4s
[CV 2/5] END model__learning_rate=0.1, model__max_depth=30, model__subsample=0.5, preprocess__cat__si__strategy=constant, preprocess__num__ii=IterativeImputer();, score=-1016845641.469 total time=   0.4s
[CV 2/5] END model__learning_rate=0.1, model__max_depth=-1, model__subsample=1.0, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=KNNImputer();, score=-945800410.003 total time=   0.2s
[CV 4/5] END model__learning_rate=0.1, model__max_depth=-1, model__subsample=1.0, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=IterativeImputer();, score=-570100776.470 total time=   0.4s
[CV 2/5] END model__learning_rate=0.1, model__max_depth=-1, model__subsample=1.0, preprocess__cat__si__strategy=constant, preprocess__num__ii=IterativeImputer();, score=-1016845641.469 total time=   0.3s
[CV 2/5] END model__learning_rate=0.1, model__max_depth=-1, model__subsample=0.5, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=KNNImputer();, score=-945800410.003 total time=   0.2s
[CV 4/5] END model__learning_rate=0.1, model__max_depth=-1, model__subsample=0.5, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=IterativeImputer();, score=-570100776.470 total time=   0.4s
[CV 5/5] END model__learning_rate=0.1, model__max_depth=-1, model__subsample=0.5, preprocess__cat__si__strategy=constant, preprocess__num__ii=KNNImputer();, score=-1239206018.769 total time=   0.2s
[CV 3/5] END model__learning_rate=0.2, model__max_depth=30, model__subsample=1.0, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=SimpleImputer();, score=-524058509.993 total time=   0.1s
[CV 1/5] END model__learning_rate=0.2, model__max_depth=30, model__subsample=1.0, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=KNNImputer();, score=-1079192998.183 total time=   0.2s
[CV 3/5] END model__learning_rate=0.2, model__max_depth=30, model__subsample=1.0, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=IterativeImputer();, score=-576066791.159 total time=   0.4s
[CV 4/5] END model__learning_rate=0.2, model__max_depth=30, model__subsample=1.0, preprocess__cat__si__strategy=constant, preprocess__num__ii=KNNImputer();, score=-590633071.038 total time=   0.2s
[CV 2/5] END model__learning_rate=0.2, model__max_depth=30, model__subsample=0.5, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=SimpleImputer();, score=-1031476671.446 total time=   0.2s
[CV 5/5] END model__learning_rate=0.2, model__max_depth=30, model__subsample=0.5, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=SimpleImputer();, score=-1267733161.543 total time=   0.1s
[CV 1/5] END model__learning_rate=0.2, model__max_depth=30, model__subsample=0.5, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=IterativeImputer();, score=-1051746575.599 total time=   0.6s
[CV 4/5] END model__learning_rate=0.2, model__max_depth=30, model__subsample=0.5, preprocess__cat__si__strategy=constant, preprocess__num__ii=IterativeImputer();, score=-600837629.536 total time=   0.4s
[CV 1/5] END model__learning_rate=0.2, model__max_depth=-1, model__subsample=1.0, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=IterativeImputer();, score=-1051746575.599 total time=   0.6s
[CV 3/5] END model__learning_rate=0.2, model__max_depth=-1, model__subsample=1.0, preprocess__cat__si__strategy=constant, preprocess__num__ii=IterativeImputer();, score=-533278666.873 total time=   0.4s
[CV 3/5] END model__learning_rate=0.2, model__max_depth=-1, model__subsample=0.5, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=KNNImputer();, score=-540602961.526 total time=   0.2s
[CV 2/5] END model__learning_rate=0.2, model__max_depth=-1, model__subsample=0.5, preprocess__cat__si__strategy=constant, preprocess__num__ii=SimpleImputer();, score=-1024576434.974 total time=   0.2s
[CV 5/5] END model__learning_rate=0.2, model__max_depth=-1, model__subsample=0.5, preprocess__cat__si__strategy=constant, preprocess__num__ii=SimpleImputer();, score=-1271758627.677 total time=   0.1s
[CV 5/5] END model__learning_rate=0.2, model__max_depth=-1, model__subsample=0.5, preprocess__cat__si__strategy=constant, preprocess__num__ii=KNNImputer();, score=-1210584444.764 total time=   0.2s
[CV 1/5] END model__learning_rate=0.1, model__max_depth=30, model__subsample=1.0, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=KNNImputer();, score=-1085304295.641 total time=   0.2s
[CV 5/5] END model__learning_rate=0.1, model__max_depth=30, model__subsample=1.0, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=IterativeImputer();, score=-1219549323.929 total time=   0.4s
[CV 2/5] END model__learning_rate=0.1, model__max_depth=30, model__subsample=1.0, preprocess__cat__si__strategy=constant, preprocess__num__ii=IterativeImputer();, score=-1016845641.469 total time=   0.4s
[CV 2/5] END model__learning_rate=0.1, model__max_depth=30, model__subsample=0.5, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=KNNImputer();, score=-945800410.003 total time=   0.2s
[CV 1/5] END model__learning_rate=0.1, model__max_depth=30, model__subsample=0.5, preprocess__cat__si__strategy=constant, preprocess__num__ii=SimpleImputer();, score=-1102984449.306 total time=   0.1s
[CV 4/5] END model__learning_rate=0.1, model__max_depth=30, model__subsample=0.5, preprocess__cat__si__strategy=constant, preprocess__num__ii=SimpleImputer();, score=-543635446.429 total time=   0.3s
[CV 5/5] END model__learning_rate=0.1, model__max_depth=30, model__subsample=0.5, preprocess__cat__si__strategy=constant, preprocess__num__ii=IterativeImputer();, score=-1260432508.211 total time=   0.4s
[CV 3/5] END model__learning_rate=0.1, model__max_depth=-1, model__subsample=1.0, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=IterativeImputer();, score=-519293006.931 total time=   0.4s
[CV 4/5] END model__learning_rate=0.1, model__max_depth=-1, model__subsample=1.0, preprocess__cat__si__strategy=constant, preprocess__num__ii=KNNImputer();, score=-513174790.496 total time=   0.2s
[CV 2/5] END model__learning_rate=0.1, model__max_depth=-1, model__subsample=0.5, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=SimpleImputer();, score=-997691407.890 total time=   0.1s
[CV 5/5] END model__learning_rate=0.1, model__max_depth=-1, model__subsample=0.5, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=SimpleImputer();, score=-1226710083.913 total time=   0.2s
[CV 5/5] END model__learning_rate=0.1, model__max_depth=-1, model__subsample=0.5, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=KNNImputer();, score=-1224677483.241 total time=   0.2s
[CV 3/5] END model__learning_rate=0.1, model__max_depth=-1, model__subsample=0.5, preprocess__cat__si__strategy=constant, preprocess__num__ii=SimpleImputer();, score=-529011858.629 total time=   0.1s
[CV 1/5] END model__learning_rate=0.1, model__max_depth=-1, model__subsample=0.5, preprocess__cat__si__strategy=constant, preprocess__num__ii=KNNImputer();, score=-1060914076.633 total time=   0.2s
[CV 3/5] END model__learning_rate=0.1, model__max_depth=-1, model__subsample=0.5, preprocess__cat__si__strategy=constant, preprocess__num__ii=IterativeImputer();, score=-532594533.605 total time=   0.4s
[CV 3/5] END model__learning_rate=0.2, model__max_depth=30, model__subsample=1.0, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=KNNImputer();, score=-540602961.526 total time=   0.2s
[CV 1/5] END model__learning_rate=0.2, model__max_depth=30, model__subsample=1.0, preprocess__cat__si__strategy=constant, preprocess__num__ii=SimpleImputer();, score=-1088255251.747 total time=   0.1s
[CV 4/5] END model__learning_rate=0.2, model__max_depth=30, model__subsample=1.0, preprocess__cat__si__strategy=constant, preprocess__num__ii=SimpleImputer();, score=-599626460.012 total time=   0.1s
[CV 5/5] END model__learning_rate=0.2, model__max_depth=30, model__subsample=1.0, preprocess__cat__si__strategy=constant, preprocess__num__ii=KNNImputer();, score=-1210584444.764 total time=   0.2s
[CV 3/5] END model__learning_rate=0.2, model__max_depth=30, model__subsample=0.5, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=SimpleImputer();, score=-524058509.993 total time=   0.1s
[CV 1/5] END model__learning_rate=0.2, model__max_depth=30, model__subsample=0.5, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=KNNImputer();, score=-1079192998.183 total time=   0.2s
[CV 3/5] END model__learning_rate=0.2, model__max_depth=30, model__subsample=0.5, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=IterativeImputer();, score=-576066791.159 total time=   0.4s
[CV 4/5] END model__learning_rate=0.2, model__max_depth=30, model__subsample=0.5, preprocess__cat__si__strategy=constant, preprocess__num__ii=KNNImputer();, score=-590633071.038 total time=   0.2s
[CV 2/5] END model__learning_rate=0.2, model__max_depth=-1, model__subsample=1.0, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=SimpleImputer();, score=-1031476671.446 total time=   0.2s
[CV 5/5] END model__learning_rate=0.2, model__max_depth=-1, model__subsample=1.0, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=SimpleImputer();, score=-1267733161.543 total time=   0.1s
[CV 5/5] END model__learning_rate=0.2, model__max_depth=-1, model__subsample=1.0, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=KNNImputer();, score=-1243376201.323 total time=   0.2s
[CV 3/5] END model__learning_rate=0.2, model__max_depth=-1, model__subsample=1.0, preprocess__cat__si__strategy=constant, preprocess__num__ii=SimpleImputer();, score=-539187722.319 total time=   0.2s
[CV 1/5] END model__learning_rate=0.2, model__max_depth=-1, model__subsample=1.0, preprocess__cat__si__strategy=constant, preprocess__num__ii=KNNImputer();, score=-1101298878.719 total time=   0.2s
[CV 4/5] END model__learning_rate=0.2, model__max_depth=-1, model__subsample=1.0, preprocess__cat__si__strategy=constant, preprocess__num__ii=IterativeImputer();, score=-600837629.536 total time=   0.4s
[CV 5/5] END model__learning_rate=0.2, model__max_depth=-1, model__subsample=0.5, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=KNNImputer();, score=-1243376201.323 total time=   0.2s
[CV 1/5] END model__learning_rate=0.2, model__max_depth=-1, model__subsample=0.5, preprocess__cat__si__strategy=constant, preprocess__num__ii=SimpleImputer();, score=-1088255251.747 total time=   0.1s
[CV 4/5] END model__learning_rate=0.2, model__max_depth=-1, model__subsample=0.5, preprocess__cat__si__strategy=constant, preprocess__num__ii=SimpleImputer();, score=-599626460.012 total time=   0.2s
[CV 2/5] END model__learning_rate=0.2, model__max_depth=-1, model__subsample=0.5, preprocess__cat__si__strategy=constant, preprocess__num__ii=IterativeImputer();, score=-1034331933.536 total time=   0.3s
[CV 4/5] END model__learning_rate=0.1, model__max_depth=30, model__subsample=1.0, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=SimpleImputer();, score=-530900512.729 total time=   0.1s
[CV 3/5] END model__learning_rate=0.1, model__max_depth=30, model__subsample=1.0, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=IterativeImputer();, score=-519293006.931 total time=   0.3s
[CV 2/5] END model__learning_rate=0.1, model__max_depth=30, model__subsample=1.0, preprocess__cat__si__strategy=constant, preprocess__num__ii=KNNImputer();, score=-1033297772.352 total time=   0.2s
[CV 4/5] END model__learning_rate=0.1, model__max_depth=30, model__subsample=1.0, preprocess__cat__si__strategy=constant, preprocess__num__ii=IterativeImputer();, score=-551768174.727 total time=   0.4s
[CV 5/5] END model__learning_rate=0.1, model__max_depth=30, model__subsample=0.5, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=KNNImputer();, score=-1224677483.241 total time=   0.2s
[CV 3/5] END model__learning_rate=0.1, model__max_depth=30, model__subsample=0.5, preprocess__cat__si__strategy=constant, preprocess__num__ii=SimpleImputer();, score=-529011858.629 total time=   0.2s
[CV 1/5] END model__learning_rate=0.1, model__max_depth=30, model__subsample=0.5, preprocess__cat__si__strategy=constant, preprocess__num__ii=KNNImputer();, score=-1060914076.633 total time=   0.2s
[CV 4/5] END model__learning_rate=0.1, model__max_depth=30, model__subsample=0.5, preprocess__cat__si__strategy=constant, preprocess__num__ii=IterativeImputer();, score=-551768174.727 total time=   0.3s
[CV 4/5] END model__learning_rate=0.1, model__max_depth=-1, model__subsample=1.0, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=KNNImputer();, score=-518644185.752 total time=   0.2s
[CV 2/5] END model__learning_rate=0.1, model__max_depth=-1, model__subsample=1.0, preprocess__cat__si__strategy=constant, preprocess__num__ii=SimpleImputer();, score=-995475493.332 total time=   0.1s
[CV 5/5] END model__learning_rate=0.1, model__max_depth=-1, model__subsample=1.0, preprocess__cat__si__strategy=constant, preprocess__num__ii=SimpleImputer();, score=-1261991041.562 total time=   0.1s
[CV 5/5] END model__learning_rate=0.1, model__max_depth=-1, model__subsample=1.0, preprocess__cat__si__strategy=constant, preprocess__num__ii=KNNImputer();, score=-1239206018.769 total time=   0.2s
[CV 3/5] END model__learning_rate=0.1, model__max_depth=-1, model__subsample=0.5, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=SimpleImputer();, score=-513492880.363 total time=   0.2s
[CV 1/5] END model__learning_rate=0.1, model__max_depth=-1, model__subsample=0.5, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=KNNImputer();, score=-1085304295.641 total time=   0.2s
[CV 3/5] END model__learning_rate=0.1, model__max_depth=-1, model__subsample=0.5, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=IterativeImputer();, score=-519293006.931 total time=   0.4s
[CV 1/5] END model__learning_rate=0.1, model__max_depth=-1, model__subsample=0.5, preprocess__cat__si__strategy=constant, preprocess__num__ii=IterativeImputer();, score=-1047546222.332 total time=   0.6s
[CV 4/5] END model__learning_rate=0.2, model__max_depth=30, model__subsample=1.0, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=IterativeImputer();, score=-643767514.223 total time=   0.3s
[CV 3/5] END model__learning_rate=0.2, model__max_depth=30, model__subsample=1.0, preprocess__cat__si__strategy=constant, preprocess__num__ii=KNNImputer();, score=-564186901.908 total time=   0.2s
[CV 1/5] END model__learning_rate=0.2, model__max_depth=30, model__subsample=0.5, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=SimpleImputer();, score=-1174830723.077 total time=   0.1s
[CV 4/5] END model__learning_rate=0.2, model__max_depth=30, model__subsample=0.5, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=SimpleImputer();, score=-589395877.632 total time=   0.1s
[CV 4/5] END model__learning_rate=0.2, model__max_depth=30, model__subsample=0.5, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=KNNImputer();, score=-544430870.856 total time=   0.2s
[CV 4/5] END model__learning_rate=0.2, model__max_depth=30, model__subsample=0.5, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=IterativeImputer();, score=-643767514.223 total time=   0.4s
[CV 2/5] END model__learning_rate=0.2, model__max_depth=30, model__subsample=0.5, preprocess__cat__si__strategy=constant, preprocess__num__ii=IterativeImputer();, score=-1034331933.536 total time=   0.4s
[CV 3/5] END model__learning_rate=0.2, model__max_depth=-1, model__subsample=1.0, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=KNNImputer();, score=-540602961.526 total time=   0.2s
[CV 1/5] END model__learning_rate=0.2, model__max_depth=-1, model__subsample=1.0, preprocess__cat__si__strategy=constant, preprocess__num__ii=SimpleImputer();, score=-1088255251.747 total time=   0.1s
[CV 4/5] END model__learning_rate=0.2, model__max_depth=-1, model__subsample=1.0, preprocess__cat__si__strategy=constant, preprocess__num__ii=SimpleImputer();, score=-599626460.012 total time=   0.2s
[CV 4/5] END model__learning_rate=0.2, model__max_depth=-1, model__subsample=1.0, preprocess__cat__si__strategy=constant, preprocess__num__ii=KNNImputer();, score=-590633071.038 total time=   0.2s
[CV 2/5] END model__learning_rate=0.2, model__max_depth=-1, model__subsample=0.5, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=SimpleImputer();, score=-1031476671.446 total time=   0.2s
[CV 5/5] END model__learning_rate=0.2, model__max_depth=-1, model__subsample=0.5, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=SimpleImputer();, score=-1267733161.543 total time=   0.1s
[CV 1/5] END model__learning_rate=0.2, model__max_depth=-1, model__subsample=0.5, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=IterativeImputer();, score=-1051746575.599 total time=   0.5s
[CV 3/5] END model__learning_rate=0.2, model__max_depth=-1, model__subsample=0.5, preprocess__cat__si__strategy=constant, preprocess__num__ii=IterativeImputer();, score=-533278666.873 total time=   0.3s
[CV 2/5] END model__learning_rate=0.1, model__max_depth=30, model__subsample=1.0, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=SimpleImputer();, score=-997691407.890 total time=   0.2s
[CV 5/5] END model__learning_rate=0.1, model__max_depth=30, model__subsample=1.0, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=KNNImputer();, score=-1224677483.241 total time=   0.2s
[CV 3/5] END model__learning_rate=0.1, model__max_depth=30, model__subsample=1.0, preprocess__cat__si__strategy=constant, preprocess__num__ii=SimpleImputer();, score=-529011858.629 total time=   0.1s
[CV 1/5] END model__learning_rate=0.1, model__max_depth=30, model__subsample=1.0, preprocess__cat__si__strategy=constant, preprocess__num__ii=KNNImputer();, score=-1060914076.633 total time=   0.2s
[CV 3/5] END model__learning_rate=0.1, model__max_depth=30, model__subsample=1.0, preprocess__cat__si__strategy=constant, preprocess__num__ii=IterativeImputer();, score=-532594533.605 total time=   0.3s
[CV 3/5] END model__learning_rate=0.1, model__max_depth=30, model__subsample=0.5, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=KNNImputer();, score=-515317030.231 total time=   0.1s
[CV 3/5] END model__learning_rate=0.1, model__max_depth=30, model__subsample=0.5, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=IterativeImputer();, score=-519293006.931 total time=   0.4s
[CV 2/5] END model__learning_rate=0.1, model__max_depth=30, model__subsample=0.5, preprocess__cat__si__strategy=constant, preprocess__num__ii=KNNImputer();, score=-1033297772.352 total time=   0.2s
[CV 3/5] END model__learning_rate=0.1, model__max_depth=30, model__subsample=0.5, preprocess__cat__si__strategy=constant, preprocess__num__ii=IterativeImputer();, score=-532594533.605 total time=   0.4s
[CV 1/5] END model__learning_rate=0.1, model__max_depth=-1, model__subsample=1.0, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=IterativeImputer();, score=-1038744584.272 total time=   0.6s
[CV 4/5] END model__learning_rate=0.1, model__max_depth=-1, model__subsample=1.0, preprocess__cat__si__strategy=constant, preprocess__num__ii=IterativeImputer();, score=-551768174.727 total time=   0.4s
[CV 2/5] END model__learning_rate=0.1, model__max_depth=-1, model__subsample=0.5, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=IterativeImputer();, score=-1024476538.628 total time=   0.4s
[CV 2/5] END model__learning_rate=0.1, model__max_depth=-1, model__subsample=0.5, preprocess__cat__si__strategy=constant, preprocess__num__ii=KNNImputer();, score=-1033297772.352 total time=   0.2s
[CV 4/5] END model__learning_rate=0.1, model__max_depth=-1, model__subsample=0.5, preprocess__cat__si__strategy=constant, preprocess__num__ii=IterativeImputer();, score=-551768174.727 total time=   0.4s
[CV 4/5] END model__learning_rate=0.2, model__max_depth=30, model__subsample=1.0, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=KNNImputer();, score=-544430870.856 total time=   0.2s
[CV 3/5] END model__learning_rate=0.2, model__max_depth=30, model__subsample=1.0, preprocess__cat__si__strategy=constant, preprocess__num__ii=SimpleImputer();, score=-539187722.319 total time=   0.1s
[CV 1/5] END model__learning_rate=0.2, model__max_depth=30, model__subsample=1.0, preprocess__cat__si__strategy=constant, preprocess__num__ii=KNNImputer();, score=-1101298878.719 total time=   0.2s
[CV 3/5] END model__learning_rate=0.2, model__max_depth=30, model__subsample=1.0, preprocess__cat__si__strategy=constant, preprocess__num__ii=IterativeImputer();, score=-533278666.873 total time=   0.4s
[CV 3/5] END model__learning_rate=0.2, model__max_depth=30, model__subsample=0.5, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=KNNImputer();, score=-540602961.526 total time=   0.2s
[CV 1/5] END model__learning_rate=0.2, model__max_depth=30, model__subsample=0.5, preprocess__cat__si__strategy=constant, preprocess__num__ii=SimpleImputer();, score=-1088255251.747 total time=   0.1s
[CV 4/5] END model__learning_rate=0.2, model__max_depth=30, model__subsample=0.5, preprocess__cat__si__strategy=constant, preprocess__num__ii=SimpleImputer();, score=-599626460.012 total time=   0.1s
[CV 2/5] END model__learning_rate=0.2, model__max_depth=30, model__subsample=0.5, preprocess__cat__si__strategy=constant, preprocess__num__ii=KNNImputer();, score=-1064162610.680 total time=   0.2s
[CV 5/5] END model__learning_rate=0.2, model__max_depth=30, model__subsample=0.5, preprocess__cat__si__strategy=constant, preprocess__num__ii=IterativeImputer();, score=-1226474594.655 total time=   0.4s
[CV 2/5] END model__learning_rate=0.2, model__max_depth=-1, model__subsample=1.0, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=IterativeImputer();, score=-1045009894.967 total time=   0.4s
[CV 2/5] END model__learning_rate=0.2, model__max_depth=-1, model__subsample=1.0, preprocess__cat__si__strategy=constant, preprocess__num__ii=KNNImputer();, score=-1064162610.680 total time=   0.2s
[CV 5/5] END model__learning_rate=0.2, model__max_depth=-1, model__subsample=1.0, preprocess__cat__si__strategy=constant, preprocess__num__ii=IterativeImputer();, score=-1226474594.655 total time=   0.4s
[CV 2/5] END model__learning_rate=0.2, model__max_depth=-1, model__subsample=0.5, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=IterativeImputer();, score=-1045009894.967 total time=   0.4s
[CV 1/5] END model__learning_rate=0.2, model__max_depth=-1, model__subsample=0.5, preprocess__cat__si__strategy=constant, preprocess__num__ii=KNNImputer();, score=-1101298878.719 total time=   0.2s
[CV 4/5] END model__learning_rate=0.2, model__max_depth=-1, model__subsample=0.5, preprocess__cat__si__strategy=constant, preprocess__num__ii=IterativeImputer();, score=-600837629.536 total time=   0.3s
[CV 1/5] END model__learning_rate=0.1, model__max_depth=30, model__subsample=1.0, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=SimpleImputer();, score=-1094302605.510 total time=   0.2s
[CV 4/5] END model__learning_rate=0.1, model__max_depth=30, model__subsample=1.0, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=KNNImputer();, score=-518644185.752 total time=   0.2s
[CV 2/5] END model__learning_rate=0.1, model__max_depth=30, model__subsample=1.0, preprocess__cat__si__strategy=constant, preprocess__num__ii=SimpleImputer();, score=-995475493.332 total time=   0.1s
[CV 5/5] END model__learning_rate=0.1, model__max_depth=30, model__subsample=1.0, preprocess__cat__si__strategy=constant, preprocess__num__ii=SimpleImputer();, score=-1261991041.562 total time=   0.1s
[CV 1/5] END model__learning_rate=0.1, model__max_depth=30, model__subsample=1.0, preprocess__cat__si__strategy=constant, preprocess__num__ii=IterativeImputer();, score=-1047546222.332 total time=   0.6s
[CV 4/5] END model__learning_rate=0.1, model__max_depth=30, model__subsample=0.5, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=IterativeImputer();, score=-570100776.470 total time=   0.3s
[CV 4/5] END model__learning_rate=0.1, model__max_depth=30, model__subsample=0.5, preprocess__cat__si__strategy=constant, preprocess__num__ii=KNNImputer();, score=-513174790.496 total time=   0.2s
[CV 2/5] END model__learning_rate=0.1, model__max_depth=-1, model__subsample=1.0, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=SimpleImputer();, score=-997691407.890 total time=   0.1s
[CV 5/5] END model__learning_rate=0.1, model__max_depth=-1, model__subsample=1.0, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=SimpleImputer();, score=-1226710083.913 total time=   0.1s
[CV 5/5] END model__learning_rate=0.1, model__max_depth=-1, model__subsample=1.0, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=KNNImputer();, score=-1224677483.241 total time=   0.2s
[CV 3/5] END model__learning_rate=0.1, model__max_depth=-1, model__subsample=1.0, preprocess__cat__si__strategy=constant, preprocess__num__ii=SimpleImputer();, score=-529011858.629 total time=   0.1s
[CV 1/5] END model__learning_rate=0.1, model__max_depth=-1, model__subsample=1.0, preprocess__cat__si__strategy=constant, preprocess__num__ii=KNNImputer();, score=-1060914076.633 total time=   0.2s
[CV 1/5] END model__learning_rate=0.1, model__max_depth=-1, model__subsample=1.0, preprocess__cat__si__strategy=constant, preprocess__num__ii=IterativeImputer();, score=-1047546222.332 total time=   0.6s
[CV 5/5] END model__learning_rate=0.1, model__max_depth=-1, model__subsample=0.5, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=IterativeImputer();, score=-1219549323.929 total time=   0.4s
[CV 2/5] END model__learning_rate=0.1, model__max_depth=-1, model__subsample=0.5, preprocess__cat__si__strategy=constant, preprocess__num__ii=IterativeImputer();, score=-1016845641.469 total time=   0.4s
[CV 2/5] END model__learning_rate=0.2, model__max_depth=30, model__subsample=1.0, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=KNNImputer();, score=-1017028960.525 total time=   0.2s
[CV 5/5] END model__learning_rate=0.2, model__max_depth=30, model__subsample=1.0, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=IterativeImputer();, score=-1249207388.654 total time=   0.4s
[CV 2/5] END model__learning_rate=0.2, model__max_depth=30, model__subsample=1.0, preprocess__cat__si__strategy=constant, preprocess__num__ii=IterativeImputer();, score=-1034331933.536 total time=   0.4s
[CV 2/5] END model__learning_rate=0.2, model__max_depth=30, model__subsample=0.5, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=KNNImputer();, score=-1017028960.525 total time=   0.2s
[CV 2/5] END model__learning_rate=0.2, model__max_depth=30, model__subsample=0.5, preprocess__cat__si__strategy=constant, preprocess__num__ii=SimpleImputer();, score=-1024576434.974 total time=   0.1s
[CV 5/5] END model__learning_rate=0.2, model__max_depth=30, model__subsample=0.5, preprocess__cat__si__strategy=constant, preprocess__num__ii=SimpleImputer();, score=-1271758627.677 total time=   0.2s
[CV 5/5] END model__learning_rate=0.2, model__max_depth=30, model__subsample=0.5, preprocess__cat__si__strategy=constant, preprocess__num__ii=KNNImputer();, score=-1210584444.764 total time=   0.2s
[CV 3/5] END model__learning_rate=0.2, model__max_depth=-1, model__subsample=1.0, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=SimpleImputer();, score=-524058509.993 total time=   0.2s
[CV 1/5] END model__learning_rate=0.2, model__max_depth=-1, model__subsample=1.0, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=KNNImputer();, score=-1079192998.183 total time=   0.2s
[CV 3/5] END model__learning_rate=0.2, model__max_depth=-1, model__subsample=1.0, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=IterativeImputer();, score=-576066791.159 total time=   0.4s
[CV 3/5] END model__learning_rate=0.2, model__max_depth=-1, model__subsample=1.0, preprocess__cat__si__strategy=constant, preprocess__num__ii=KNNImputer();, score=-564186901.908 total time=   0.2s
[CV 1/5] END model__learning_rate=0.2, model__max_depth=-1, model__subsample=0.5, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=SimpleImputer();, score=-1174830723.077 total time=   0.1s
[CV 4/5] END model__learning_rate=0.2, model__max_depth=-1, model__subsample=0.5, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=SimpleImputer();, score=-589395877.632 total time=   0.1s
[CV 4/5] END model__learning_rate=0.2, model__max_depth=-1, model__subsample=0.5, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=KNNImputer();, score=-544430870.856 total time=   0.3s
[CV 3/5] END model__learning_rate=0.2, model__max_depth=-1, model__subsample=0.5, preprocess__cat__si__strategy=constant, preprocess__num__ii=SimpleImputer();, score=-539187722.319 total time=   0.2s
[CV 2/5] END model__learning_rate=0.2, model__max_depth=-1, model__subsample=0.5, preprocess__cat__si__strategy=constant, preprocess__num__ii=KNNImputer();, score=-1064162610.680 total time=   0.2s
[CV 5/5] END model__learning_rate=0.2, model__max_depth=-1, model__subsample=0.5, preprocess__cat__si__strategy=constant, preprocess__num__ii=IterativeImputer();, score=-1226474594.655 total time=   0.3s
[CV 5/5] END model__learning_rate=0.1, model__max_depth=30, model__subsample=1.0, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=SimpleImputer();, score=-1226710083.913 total time=   0.2s
[CV 2/5] END model__learning_rate=0.1, model__max_depth=30, model__subsample=1.0, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=IterativeImputer();, score=-1024476538.628 total time=   0.4s
[CV 3/5] END model__learning_rate=0.1, model__max_depth=30, model__subsample=1.0, preprocess__cat__si__strategy=constant, preprocess__num__ii=KNNImputer();, score=-541737588.393 total time=   0.2s
[CV 5/5] END model__learning_rate=0.1, model__max_depth=30, model__subsample=1.0, preprocess__cat__si__strategy=constant, preprocess__num__ii=IterativeImputer();, score=-1260432508.211 total time=   0.4s
[CV 1/5] END model__learning_rate=0.1, model__max_depth=30, model__subsample=0.5, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=IterativeImputer();, score=-1038744584.272 total time=   0.4s
[CV 5/5] END model__learning_rate=0.1, model__max_depth=30, model__subsample=0.5, preprocess__cat__si__strategy=constant, preprocess__num__ii=KNNImputer();, score=-1239206018.769 total time=   0.1s
[CV 1/5] END model__learning_rate=0.1, model__max_depth=-1, model__subsample=1.0, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=SimpleImputer();, score=-1094302605.510 total time=   0.1s
[CV 4/5] END model__learning_rate=0.1, model__max_depth=-1, model__subsample=1.0, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=SimpleImputer();, score=-530900512.729 total time=   0.2s
[CV 3/5] END model__learning_rate=0.1, model__max_depth=-1, model__subsample=1.0, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=KNNImputer();, score=-515317030.231 total time=   0.2s
[CV 1/5] END model__learning_rate=0.1, model__max_depth=-1, model__subsample=1.0, preprocess__cat__si__strategy=constant, preprocess__num__ii=SimpleImputer();, score=-1102984449.306 total time=   0.1s
[CV 4/5] END model__learning_rate=0.1, model__max_depth=-1, model__subsample=1.0, preprocess__cat__si__strategy=constant, preprocess__num__ii=SimpleImputer();, score=-543635446.429 total time=   0.1s
[CV 3/5] END model__learning_rate=0.1, model__max_depth=-1, model__subsample=1.0, preprocess__cat__si__strategy=constant, preprocess__num__ii=KNNImputer();, score=-541737588.393 total time=   0.2s
[CV 1/5] END model__learning_rate=0.1, model__max_depth=-1, model__subsample=0.5, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=SimpleImputer();, score=-1094302605.510 total time=   0.1s
[CV 4/5] END model__learning_rate=0.1, model__max_depth=-1, model__subsample=0.5, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=SimpleImputer();, score=-530900512.729 total time=   0.2s
[CV 4/5] END model__learning_rate=0.1, model__max_depth=-1, model__subsample=0.5, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=KNNImputer();, score=-518644185.752 total time=   0.2s
[CV 2/5] END model__learning_rate=0.1, model__max_depth=-1, model__subsample=0.5, preprocess__cat__si__strategy=constant, preprocess__num__ii=SimpleImputer();, score=-995475493.332 total time=   0.1s
[CV 5/5] END model__learning_rate=0.1, model__max_depth=-1, model__subsample=0.5, preprocess__cat__si__strategy=constant, preprocess__num__ii=SimpleImputer();, score=-1261991041.562 total time=   0.1s
[CV 4/5] END model__learning_rate=0.1, model__max_depth=-1, model__subsample=0.5, preprocess__cat__si__strategy=constant, preprocess__num__ii=KNNImputer();, score=-513174790.496 total time=   0.2s
[CV 2/5] END model__learning_rate=0.2, model__max_depth=30, model__subsample=1.0, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=SimpleImputer();, score=-1031476671.446 total time=   0.1s
[CV 5/5] END model__learning_rate=0.2, model__max_depth=30, model__subsample=1.0, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=SimpleImputer();, score=-1267733161.543 total time=   0.1s
[CV 1/5] END model__learning_rate=0.2, model__max_depth=30, model__subsample=1.0, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=IterativeImputer();, score=-1051746575.599 total time=   0.6s
[CV 5/5] END model__learning_rate=0.2, model__max_depth=30, model__subsample=1.0, preprocess__cat__si__strategy=constant, preprocess__num__ii=IterativeImputer();, score=-1226474594.655 total time=   0.4s
[CV 2/5] END model__learning_rate=0.2, model__max_depth=30, model__subsample=0.5, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=IterativeImputer();, score=-1045009894.967 total time=   0.4s
[CV 3/5] END model__learning_rate=0.2, model__max_depth=30, model__subsample=0.5, preprocess__cat__si__strategy=constant, preprocess__num__ii=KNNImputer();, score=-564186901.908 total time=   0.2s
[CV 1/5] END model__learning_rate=0.2, model__max_depth=-1, model__subsample=1.0, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=SimpleImputer();, score=-1174830723.077 total time=   0.1s
[CV 4/5] END model__learning_rate=0.2, model__max_depth=-1, model__subsample=1.0, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=SimpleImputer();, score=-589395877.632 total time=   0.2s
[CV 2/5] END model__learning_rate=0.2, model__max_depth=-1, model__subsample=1.0, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=KNNImputer();, score=-1017028960.525 total time=   0.2s
[CV 5/5] END model__learning_rate=0.2, model__max_depth=-1, model__subsample=1.0, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=IterativeImputer();, score=-1249207388.654 total time=   0.4s
[CV 2/5] END model__learning_rate=0.2, model__max_depth=-1, model__subsample=1.0, preprocess__cat__si__strategy=constant, preprocess__num__ii=IterativeImputer();, score=-1034331933.536 total time=   0.4s
[CV 2/5] END model__learning_rate=0.2, model__max_depth=-1, model__subsample=0.5, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=KNNImputer();, score=-1017028960.525 total time=   0.2s
[CV 5/5] END model__learning_rate=0.2, model__max_depth=-1, model__subsample=0.5, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=IterativeImputer();, score=-1249207388.654 total time=   0.4s
[CV 1/5] END model__learning_rate=0.2, model__max_depth=-1, model__subsample=0.5, preprocess__cat__si__strategy=constant, preprocess__num__ii=IterativeImputer();, score=-1064924532.138 total time=   0.5s
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Convexity and Optimization</title>
      <link>https://matteocourthoud.github.io/course/ml-econ/06_convexity/</link>
      <pubDate>Wed, 09 Mar 2022 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/course/ml-econ/06_convexity/</guid>
      <description>&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Remove warnings
import warnings
warnings.filterwarnings(&#39;ignore&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Import
import autograd.numpy as np
from autograd import grad
import seaborn as sns
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Import matplotlib for graphs
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import axes3d

# Set global parameters
%matplotlib inline
plt.style.use(&#39;seaborn-white&#39;)
plt.rcParams[&#39;lines.linewidth&#39;] = 3
plt.rcParams[&#39;figure.figsize&#39;] = (10,6)
plt.rcParams[&#39;figure.titlesize&#39;] = 20
plt.rcParams[&#39;axes.titlesize&#39;] = 18
plt.rcParams[&#39;axes.labelsize&#39;] = 14
plt.rcParams[&#39;legend.fontsize&#39;] = 14
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Function to plot errors
def error_plot(ys, yscale=&#39;log&#39;):
    plt.figure()
    plt.xlabel(&#39;Step&#39;)
    plt.ylabel(&#39;Error&#39;)
    plt.yscale(yscale)
    plt.plot(range(len(ys)), ys)
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;61-gradient-descent&#34;&gt;6.1 Gradient Descent&lt;/h2&gt;
&lt;p&gt;We start with a basic implementation of projected gradient descent.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def gradient_descent(init, steps, grad, proj=lambda x: x):
    &amp;quot;&amp;quot;&amp;quot;Projected gradient descent.
    
    Inputs:
        initial: starting point
        steps: list of scalar step sizes
        grad: function mapping points to gradients
        proj (optional): function mapping points to points
        
    Returns:
        List of all points computed by projected gradient descent.
    &amp;quot;&amp;quot;&amp;quot;
    xs = [init]
    for step in steps:
        xs.append(proj(xs[-1] - step * grad(xs[-1])))
    return xs
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that this implementation keeps around all points computed along the way. This is clearly not what you would do on large instances. We do this for illustrative purposes to be able to easily inspect the computed sequence of points.&lt;/p&gt;
&lt;h3 id=&#34;warm-up-optimizing-a-quadratic&#34;&gt;Warm-up: Optimizing a quadratic&lt;/h3&gt;
&lt;p&gt;As a toy example, let&amp;rsquo;s optimize $$f(x)=\frac12|x|^2,$$ which has the gradient map $\nabla f(x)=x.$&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def quadratic(x):
    return 0.5*x.dot(x)

def quadratic_gradient(x):
    return x
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note the function is $1$-smooth and $1$-strongly convex. Our theorems would then suggest that we use a constant step size of $1.$ If you think about it, for this step size the algorithm will actually find the optimal solution in just one step.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;x0 = np.random.normal(0, 1, (1000))
_, x1 = gradient_descent(x0, [1.0], quadratic_gradient)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Indeed, it does.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;x1.all() == 0
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;True
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s say we don&amp;rsquo;t have the right learning rate.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;xs = gradient_descent(x0, [0.1]*50, quadratic_gradient)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Plot errors along steps
error_plot([quadratic(x) for x in xs])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/06_convexity_18_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;constrained-optimization&#34;&gt;Constrained Optimization&lt;/h3&gt;
&lt;p&gt;Let&amp;rsquo;s say we want to optimize the function inside some affine subspace. Recall that affine subspaces are convex sets. Below we pick a random low dimensional affine subspace $b+U$ and define the corresponding linear projection operator.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# U is an orthonormal basis of a random 100-dimensional subspace.
U = np.linalg.qr(np.random.normal(0, 1, (1000, 100)))[0]
b = np.random.normal(0, 1, 1000)

def proj(x):
    &amp;quot;&amp;quot;&amp;quot;Projection of x onto an affine subspace&amp;quot;&amp;quot;&amp;quot;
    return b + U.dot(U.T).dot(x-b)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;x0 = np.random.normal(0, 1, (1000))
xs = gradient_descent(x0, [0.1]*50, quadratic_gradient, proj)
# the optimal solution is the projection of the origin
x_opt = proj(0)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s plot the results.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;error_plot([quadratic(x) for x in xs])
plt.plot(range(len(xs)), [quadratic(x_opt)]*len(xs),
        label=&#39;$\\frac{1}{2}|\!|x_{\mathrm{opt}}|\!|^2$&#39;)
plt.legend();
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/06_convexity_24_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;The orangle line shows the optimal error, which the algorithm reaches quickly.&lt;/p&gt;
&lt;p&gt;The iterates also converge to the optimal solution in domain as the following plot shows.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;error_plot([np.linalg.norm(x_opt-x)**2 for x in xs])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/06_convexity_27_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;least-squares&#34;&gt;Least Squares&lt;/h3&gt;
&lt;p&gt;One of the most fundamental data analysis tools is &lt;em&gt;linear least squares&lt;/em&gt;. Given an $m\times n$ matrix $A$ and a vector $b$ our goal is to find a vector $x\in\mathbb{R}^n$ that minimizes the following objective:&lt;/p&gt;
&lt;p&gt;
$$f(x) = \frac 1{2m}\sum_{i=1}^m (a_i^\top x - b_j)^2 
=\frac1{2m}\|Ax-b\|^2$$
&lt;/p&gt;
&lt;p&gt;We can verify that $\nabla f(x) = A^\top(Ax-b)$ and
$\nabla^2 f(x) = A^\top A.$&lt;/p&gt;
&lt;p&gt;Hence, the objective is $\beta$-smooth with
$\beta=\lambda_{\mathrm{max}}(A^\top A)$, and $\alpha$-strongly convex with $\alpha=\lambda_{\mathrm{min}}(A^\top A)$.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def least_squares(A, b, x):
    &amp;quot;&amp;quot;&amp;quot;Least squares objective.&amp;quot;&amp;quot;&amp;quot;
    return (0.5/m) * np.linalg.norm(A.dot(x)-b)**2

def least_squares_gradient(A, b, x):
    &amp;quot;&amp;quot;&amp;quot;Gradient of least squares objective at x.&amp;quot;&amp;quot;&amp;quot;
    return A.T.dot(A.dot(x)-b)/m
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;overdetermined-case-mge-n&#34;&gt;Overdetermined case $m\ge n$&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;m, n = 1000, 100
A = np.random.normal(0, 1, (m, n))
x_opt = np.random.normal(0, 1, n)
noise = np.random.normal(0, 0.1, m)
b = A.dot(x_opt) + noise
objective = lambda x: least_squares(A, b, x)
gradient = lambda x: least_squares_gradient(A, b, x)
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;convergence-in-objective&#34;&gt;Convergence in Objective&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;x0 = np.random.normal(0, 1, n)
xs = gradient_descent(x0, [0.1]*100, gradient)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;error_plot([objective(x) for x in xs])
plt.plot(range(len(xs)), [np.linalg.norm(noise)**2]*len(xs),
        label=&#39;noise level&#39;)
plt.plot(range(len(xs)), [least_squares(A,b,x_opt)]*len(xs),
        label=&#39;optimal&#39;)
plt.legend();
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/06_convexity_35_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;convergence-in-domain&#34;&gt;Convergence in Domain&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;error_plot([np.linalg.norm(x-x_opt)**2 for x in xs])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/06_convexity_37_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;underdetermined-case-m--n&#34;&gt;Underdetermined Case $m &amp;lt; n$&lt;/h3&gt;
&lt;p&gt;In the underdetermined case, the least squares objective is inevitably not strongly convex, since $A^\top A$ is a rank deficient matrix and hence $\lambda_{\mathrm{min}}(A^\top A)=0.$&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;m, n = 100, 1000
A = np.random.normal(0, 1, (m, n))
b = np.random.normal(0, 1, m)
# The least norm solution is given by the pseudo-inverse
x_opt = np.linalg.pinv(A).dot(b)
objective = lambda x: least_squares(A, b, x)
gradient = lambda x: least_squares_gradient(A, b, x)
x0 = np.random.normal(0, 1, n)
xs = gradient_descent(x0, [0.1]*100, gradient)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Results.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;error_plot([objective(x) for x in xs])
plt.plot(range(len(xs)), [least_squares(A,b,x_opt)]*len(xs),
        label=&#39;optimal&#39;)
plt.legend();
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/06_convexity_41_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;While we quickly reduce the error, we don&amp;rsquo;t actually converge in domain to the least norm solution. This is just because the function is no longer strongly convex in the underdetermined case.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;error_plot([np.linalg.norm(x-x_opt)**2 for x in xs], yscale=&#39;linear&#39;)
plt.plot(range(len(xs)), [np.linalg.norm(x_opt)**2]*len(xs),
         label=&#39;$|\!|x_{\mathrm{opt}}|\!|^2$&#39;)
plt.legend();
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/06_convexity_43_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;ell_2-regularized-least-squares&#34;&gt;$\ell_2$-regularized least squares&lt;/h2&gt;
&lt;p&gt;In the underdetermined case, it is often desirable to restore strong convexity of the objective function by adding an $\ell_2^2$-penality, also known as &lt;em&gt;Tikhonov regularization&lt;/em&gt;, $\ell_2$-regularization, or &lt;em&gt;weight decay&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;
$$\frac1{2m}\|Ax-b\|^2 + \frac{\alpha}2\|x\|^2$$
&lt;/p&gt;
&lt;p&gt;Note: With this modification the objective is $\alpha$-strongly convex again.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def least_squares_l2(A, b, x, alpha=0.1):
    return least_squares(A, b, x) + (alpha/2) * x.dot(x)

def least_squares_l2_gradient(A, b, x, alpha=0.1):
    return least_squares_gradient(A, b, x) + alpha * x
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s create a least squares instance.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;m, n = 100, 1000
A = np.random.normal(0, 1, (m, n))
b = A.dot(np.random.normal(0, 1, n))
objective = lambda x: least_squares_l2(A, b, x)
gradient = lambda x: least_squares_l2_gradient(A, b, x)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that we can find the optimal solution to the optimization problem in closed form without even running gradient descent by computing $x_{\mathrm{opt}}=(A^\top+\alpha I)^{-1}A^\top b.$ Please verify that this point is indeed optimal.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;x_opt = np.linalg.inv(A.T.dot(A) + 0.1*np.eye(1000)).dot(A.T).dot(b)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here&amp;rsquo;s how gradient descent fares.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;x0 = np.random.normal(0, 1, n)
xs = gradient_descent(x0, [0.1]*500, gradient)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We plot the descent.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;error_plot([objective(x) for x in xs])
plt.plot(range(len(xs)), [least_squares_l2(A,b,x_opt)]*len(xs),
        label=&#39;optimal&#39;)
plt.legend();
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/06_convexity_54_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;You see that the error doesn&amp;rsquo;t decrease below a certain level due to the regularization term. This is not a bad thing. In fact, the regularization term gives as &lt;em&gt;strong convexity&lt;/em&gt; which leads to convergence in domain again:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;xs = gradient_descent(x0, [0.1]*500, gradient)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;error_plot([np.linalg.norm(x-x_opt)**2 for x in xs])
plt.plot(range(len(xs)), [np.linalg.norm(x_opt)**2]*len(xs),
        label=&#39;squared norm of $x_{\mathrm{opt}}$&#39;)
plt.legend();
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/06_convexity_57_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;the-magic-of-implicit-regularization&#34;&gt;The Magic of Implicit Regularization&lt;/h2&gt;
&lt;p&gt;Sometimes simply running gradient descent from a suitable initial point has a regularizing effect on its own &lt;strong&gt;without introducing an explicit regularization term&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;We will see this below where we revisit the unregularized least squares objective, but initialize gradient descent from the origin rather than a random gaussian point.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# We initialize from 0
x0 = np.zeros(n)
# Note this is the gradient w.r.t. the unregularized objective!
gradient = lambda x: least_squares_gradient(A, b, x)
xs = gradient_descent(x0, [0.1]*50, gradient)
error_plot([np.linalg.norm(x_opt-x)**2 for x in xs], yscale=&#39;linear&#39;)
plt.plot(range(len(xs)), [np.linalg.norm(x_opt)**2]*len(xs),
         label=&#39;$|\!|x_{\mathrm{opt}}|\!|^2$&#39;)
plt.legend();
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/06_convexity_60_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Incredible!&lt;/em&gt; We converge to the minimum norm solution!&lt;/p&gt;
&lt;p&gt;Implicit regularization is a deep phenomenon that&amp;rsquo;s an active research topic in learning and optimization. It&amp;rsquo;s exciting that we see it play out in this simple least squares problem already!&lt;/p&gt;
&lt;h2 id=&#34;lasso&#34;&gt;LASSO&lt;/h2&gt;
&lt;p&gt;LASSO is the name for $\ell_1$-regularized least squares regression:&lt;/p&gt;
&lt;p&gt;
$$\frac1{2m}\|Ax-b\|^2 + \alpha\|x\|_1$$
&lt;/p&gt;
&lt;p&gt;We will see that LASSO is able to fine &lt;em&gt;sparse&lt;/em&gt; solutions if they exist. This is a common motivation for using an $\ell_1$-regularizer.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def lasso(A, b, x, alpha=0.1):
    return least_squares(A, b, x) + alpha * np.linalg.norm(x, 1)

def ell1_subgradient(x):
    &amp;quot;&amp;quot;&amp;quot;Subgradient of the ell1-norm at x.&amp;quot;&amp;quot;&amp;quot;
    g = np.ones(x.shape)
    g[x &amp;lt; 0.] = -1.0
    return g

def lasso_subgradient(A, b, x, alpha=0.1):
    &amp;quot;&amp;quot;&amp;quot;Subgradient of the lasso objective at x&amp;quot;&amp;quot;&amp;quot;
    return least_squares_gradient(A, b, x) + alpha*ell1_subgradient(x)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;m, n = 100, 1000
A = np.random.normal(0, 1, (m, n))
x_opt = np.zeros(n)
x_opt[:10] = 1.0
b = A.dot(x_opt)
x0 = np.random.normal(0, 1, n)
xs = gradient_descent(x0, [0.1]*500, lambda x: lasso_subgradient(A, b, x))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;error_plot([lasso(A, b, x) for x in xs])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/06_convexity_66_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plt.figure()
plt.title(&#39;Comparison of initial, optimal, and computed point&#39;)
idxs = range(50)
plt.plot(idxs, x0[idxs], &#39;--&#39;, color=&#39;#aaaaaa&#39;, label=&#39;initial&#39;)
plt.plot(idxs, x_opt[idxs], &#39;r-&#39;, label=&#39;optimal&#39;)
plt.plot(idxs, xs[-1][idxs], &#39;g-&#39;, label=&#39;final&#39;)
plt.xlabel(&#39;Coordinate&#39;)
plt.ylabel(&#39;Value&#39;)
plt.legend();
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/06_convexity_67_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;As promised, LASSO correctly identifies the significant coordinates of the optimal solution. This is why, in practice, LASSO is a popular tool for feature selection.&lt;/p&gt;
&lt;p&gt;Play around with this plot to inspect other points along the way, e.g., the point that achieves lowest objective value. Why does the objective value go up even though we continue to get better solutions?&lt;/p&gt;
&lt;h2 id=&#34;support-vector-machines&#34;&gt;Support Vector Machines&lt;/h2&gt;
&lt;p&gt;In a linear classification problem, we&amp;rsquo;re given $m$ labeled points $(a_i, y_i)$ and we wish to find a hyperplane given by a point $x$ that separates them so that&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\langle a_i, x\rangle \ge 1$ when $y_i=1$, and&lt;/li&gt;
&lt;li&gt;$\langle a_i, x\rangle \le -1$ when $y_i = -1$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The smaller the norm $|x|$ the larger the &lt;em&gt;margin&lt;/em&gt; between positive and negative instances. Therefore, it makes sense to throw in a regularizer that penalizes large norms. This leads to the objective.&lt;/p&gt;
&lt;p&gt;
$$\frac 1m \sum_{i=1}^m \max\{1-y_i(a_i^\top x), 0\} + \frac{\alpha}2\|x\|^2$$
&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def hinge_loss(z):
    return np.maximum(1.-z, np.zeros(z.shape))

def svm_objective(A, y, x, alpha=0.1):
    &amp;quot;&amp;quot;&amp;quot;SVM objective.&amp;quot;&amp;quot;&amp;quot;
    m, _ = A.shape
    return np.mean(hinge_loss(np.diag(y).dot(A.dot(x))))+(alpha/2)*x.dot(x)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;z = np.linspace(-2, 2, 100)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plt.figure()
plt.plot(z, hinge_loss(z));
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/06_convexity_73_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def hinge_subgradient(z):
    g = np.zeros(z.shape)
    g[z &amp;lt; 1] = -1.
    return g

def svm_subgradient(A, y, x, alpha=0.1):
    g1 = hinge_subgradient(np.diag(y).dot(A.dot(x)))
    g2 = np.diag(y).dot(A)
    return g1.dot(g2) + alpha*x
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plt.figure()
plt.plot(z, hinge_subgradient(z));
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/06_convexity_75_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;m, n = 1000, 100
A = np.vstack([np.random.normal(0.1, 1, (m//2, n)),
               np.random.normal(-0.1, 1, (m//2, n))])
y = np.hstack([np.ones(m//2), -1.*np.ones(m//2)])
x0 = np.random.normal(0, 1, n)
xs = gradient_descent(x0, [0.01]*100, 
                      lambda x: svm_subgradient(A, y, x, 0.05))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;error_plot([svm_objective(A, y, x) for x in xs])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/06_convexity_77_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s see if averaging out the solutions gives us an improved function value.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;xavg = 0.0
for x in xs:
    xavg += x
svm_objective(A, y, xs[-1]), svm_objective(A, y, xavg/len(xs))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;(1.0710162653835846, 0.9069593413738611)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can also look at the accuracy of our linear model for predicting the labels. From how we defined the data, we can see that the all ones vector is the highest accuracy classifier in the limit of infinite data (very large $m$). For a finite data set, the accuracy could be even higher due to random fluctuations.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def accuracy(A, y, x):
    return np.mean(np.diag(y).dot(A.dot(x))&amp;gt;0)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plt.figure()
plt.ylabel(&#39;Accuracy&#39;)
plt.xlabel(&#39;Step&#39;)
plt.plot(range(len(xs)), [accuracy(A, y, x) for x in xs])
plt.plot(range(len(xs)), [accuracy(A, y, np.ones(n))]*len(xs),
        label=&#39;Population optimum&#39;)
plt.legend();
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/06_convexity_82_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;We see that the accuracy spikes pretty early and drops a bit as we train for too long.&lt;/p&gt;
&lt;h2 id=&#34;sparse-inverse-covariance-estimation&#34;&gt;Sparse Inverse Covariance Estimation&lt;/h2&gt;
&lt;p&gt;Given a positive semidefinite matrix $S\in\mathbb{R}^{n\times n}$ the objective function in sparse inverse covariance estimation is as follows:&lt;/p&gt;
&lt;p&gt;
$$ \min_{X\in\mathbb{R}^{n\times n}, X\succeq 0} 
\langle S, X\rangle - \log\det(X) + \alpha\|X\|_1$$
&lt;/p&gt;
&lt;p&gt;Here, we define
$$\langle S, X\rangle = \mathrm{trace}(S^\top X)$$
and
$$|X|&lt;em&gt;1 = \sum&lt;/em&gt;{ij}|X_{ij}|.$$&lt;/p&gt;
&lt;p&gt;Typically, we think of the matrix $S$ as a sample covariance matrix of a set of vectors $a_1,\dots, a_m,$ defined as:
$$
S = \frac1{m-1}\sum_{i=1}^n a_ia_i^\top
$$
The example also highlights the utility of automatic differentiation as provided by the &lt;code&gt;autograd&lt;/code&gt; package that we&amp;rsquo;ll regularly use. In a later lecture we will understand exactly how automatic differentiation works. For now we just treat it as a blackbox that gives us gradients.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;np.random.seed(1337)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def sparse_inv_cov(S, X, alpha=0.1):
    return (np.trace(S.T.dot(X))
            - np.log(np.linalg.det(X))
            + alpha * np.sum(np.abs(X)))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;n = 5
A = np.random.normal(0, 1, (n, n))
S = A.dot(A.T)
objective = lambda X: sparse_inv_cov(S, X)
# autograd provides a &amp;quot;gradient&amp;quot;, yay!
gradient = grad(objective)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We also need to worry about the projection onto the positive semidefinite cone, which corresponds to truncating eigenvalues.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def projection(X):
    &amp;quot;&amp;quot;&amp;quot;Projection onto positive semidefinite cone.&amp;quot;&amp;quot;&amp;quot;
    es, U = np.linalg.eig(X)
    es[es&amp;lt;0] = 0.
    return U.dot(np.diag(es).dot(U.T))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;A0 = np.random.normal(0, 1, (n,n))
X0 = A0.dot(A0.T)
Xs = gradient_descent(X0, [0.01]*500, gradient, projection)
error_plot([objective(X) for X in Xs])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/06_convexity_91_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;going-crazy-with-autograd&#34;&gt;Going crazy with autograd&lt;/h2&gt;
&lt;p&gt;Just for fun, we&amp;rsquo;ll go through a crazy example below. We can use &lt;code&gt;autograd&lt;/code&gt; not just for getting gradients for natural objectives, we can in principle also use it to tune hyperparameters of our optimizer, like the step size schedulde.&lt;/p&gt;
&lt;p&gt;Below we see how we can find a better 10-step learning rate schedules for optimizing a quadratic. This is mostly just for illustrative purposes (although some researchers are exploring these kinds of ideas more seriously).&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;x0 = np.random.normal(0, 1, 1000)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def f(x):
    return 0.5*np.dot(x,x)

def optimizer(steps):
    &amp;quot;&amp;quot;&amp;quot;Optimize a quadratic with the given steps.&amp;quot;&amp;quot;&amp;quot;
    xs = gradient_descent(x0, steps, grad(f))
    return f(xs[-1])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The function &lt;code&gt;optimizer&lt;/code&gt; is a non-differentiable function of its input &lt;code&gt;steps&lt;/code&gt;. Nontheless, &lt;code&gt;autograd&lt;/code&gt; will provide a gradient that we can stick into gradient descent. That is, we&amp;rsquo;re tuning gradient descent with gradient descent.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;grad_optimizer = grad(optimizer)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;initial_steps = np.abs(np.random.normal(0, 0.1, 10))
better_steps = gradient_descent(initial_steps, [0.001]*500, grad_optimizer)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;error_plot([optimizer(steps) for steps in better_steps])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/06_convexity_99_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;As we can see, the learning rate schedules improve dramatically over time. Of course, we already know from the first example that there is a step size schedule that converges in one step. Interestingly, the last schedule we find here doesn&amp;rsquo;t look at all like what we might expect:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plt.figure()
plt.xticks(range(len(better_steps[-1])))
plt.ylabel(&#39;Step size&#39;)
plt.xlabel(&#39;Step number&#39;)
plt.plot(range(len(better_steps[-1])), better_steps[-1]);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/06_convexity_101_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Endogeneity</title>
      <link>https://matteocourthoud.github.io/course/metrics/06_endogeneity/</link>
      <pubDate>Fri, 29 Oct 2021 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/course/metrics/06_endogeneity/</guid>
      <description>&lt;h2 id=&#34;instrumental-variables&#34;&gt;Instrumental Variables&lt;/h2&gt;
&lt;h3 id=&#34;endogeneity&#34;&gt;Endogeneity&lt;/h3&gt;
&lt;p&gt;We say that there is &lt;strong&gt;endogeneity&lt;/strong&gt; in the linear regression model if
$\mathbb E[x_i \varepsilon_i] \neq 0$.&lt;/p&gt;
&lt;p&gt;The random vector $z_i$ is an &lt;strong&gt;instrumental variable&lt;/strong&gt; in the linear
regression model if the following conditions are met.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Exclusion restriction&lt;/strong&gt;: the instruments are uncorrelated with the
regression error $$
\mathbb E_n[z_i \varepsilon_i] = 0
$$ almost surely, i.e. with probability $p \to 1$.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Rank condition&lt;/strong&gt;: no linearly redundant instruments $$
\mathbb E_n[z_i z_i&amp;rsquo;] \neq 0
$$ almost surely, i.e. with probability $p \to 1$.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Relevance condition&lt;/strong&gt; (need $L &amp;gt; K$): $$
rank \ (\mathbb E_n[z_i x_i&amp;rsquo;]) = K
$$ almost surely, i.e. with probability $p \to 1$.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;iv-and-2sls&#34;&gt;IV and 2SLS&lt;/h3&gt;
&lt;p&gt;Let $K = dim(x_i)$ and $L = dim(z_i)$. We say that the model is
&lt;strong&gt;just-identified&lt;/strong&gt; if $L = K$ (method: IV) and &lt;strong&gt;over-identified&lt;/strong&gt; if
$L &amp;gt; K$ (method: 2SLS).&lt;/p&gt;
&lt;p&gt;Assume $z_i$ satisfies the instrumental variable assumptions above and
$dim(z_i) = dim(x_i)$, then the &lt;strong&gt;instrumental variables (IV)&lt;/strong&gt;
estimator $\hat{\beta} _ {IV}$ is given by $$
\begin{aligned}
\hat{\beta} _ {IV} &amp;amp;= \mathbb E_n[z_i x_i&amp;rsquo;]^{-1} \mathbb E_n[z_i y_i] = \newline
&amp;amp;= \left( \frac{1}{n} \sum _ {i=1}^n z_i x_i\right)^{-1} \left( \frac{1}{n} \sum _ {i=1}^n z_i y_i\right) = \newline
&amp;amp;= (Z&amp;rsquo;X)^{-1} (Z&amp;rsquo;y)
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;Assume $z_i$ satisfies the instrumental variable assumptions above and
$dim(z_i) &amp;gt; dim(x_i)$, then the &lt;strong&gt;two-stage-least squares (2SLS)&lt;/strong&gt;
estimator $\hat{\beta} _ {2SLS}$ is given by $$
\hat{\beta} _ {2SLS} =  \Big( X&amp;rsquo;Z (Z&amp;rsquo;Z)^{-1} Z&amp;rsquo;X \Big)^{-1} \Big( X&amp;rsquo;Z (Z&amp;rsquo;Z)^{-1} Z&amp;rsquo;y \Big)
$$ Where $\hat{x}_i$ is the predicted $x_i$ from the &lt;strong&gt;first stage&lt;/strong&gt;
regression of $x_i$ on $z_i$. This is equivalent to the IV estimator
using $\hat{x}_i$ as an instrument for $x_i$.&lt;/p&gt;
&lt;h3 id=&#34;2sls-algebra&#34;&gt;2SLS Algebra&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;The estimator is called &lt;strong&gt;two-stage-least squares&lt;/strong&gt; since it can be
rewritten as an IV estimator that uses $\hat{X}$ as instrument: $$
\begin{aligned}
\hat{\beta} _ {\text{2SLS}} &amp;amp;= \Big( X&amp;rsquo;Z (Z&amp;rsquo;Z)^{-1} Z&amp;rsquo;X \Big)^{-1} \Big( X&amp;rsquo;Z (Z&amp;rsquo;Z)^{-1} Z&amp;rsquo;y \Big) = \newline
&amp;amp;= (\hat{X}&amp;rsquo; X)^{-1} \hat{X}&amp;rsquo; y = \newline
&amp;amp;= \mathbb E_n[\hat{x}_i x_i&amp;rsquo;]^{-1} \mathbb E_n[\hat{x}_i y_i]
\end{aligned}
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Moreover it can be rewritten as $$
\begin{aligned}
\hat{\beta} _ {\text{2SLS}} &amp;amp;= (\hat{X}&amp;rsquo; X)^{-1} \hat{X}&amp;rsquo; y = \newline
&amp;amp;= (X&amp;rsquo; P_Z X)^{-1} X&amp;rsquo; P_Z y = \newline
&amp;amp;= (X&amp;rsquo; P_Z P_Z X)^{-1} X&amp;rsquo; P_Z y = \newline
&amp;amp;= (\hat{X}&amp;rsquo; \hat{X})^{-1} \hat{X}&amp;rsquo; y = \newline
&amp;amp;= \mathbb E_n [\hat{x}_i \hat{x}_i]^{-1} \mathbb E_n[\hat{x}_i y_i]
\end{aligned}
$$&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;rule-of-thumb&#34;&gt;Rule of Thumb&lt;/h3&gt;
&lt;p&gt;How to the test the relevance condition? Rule of thumb: $F$-test in the
first stage $&amp;gt;10$ (joint test on $z_i$).&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Problem&lt;/strong&gt;: as $n \to \infty$, with finite $L$, $F \to \infty$ (bad
rule of thumb).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;equivalence&#34;&gt;Equivalence&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;If $K=L$, $\hat{\beta} _ {\text{2SLS}} = \hat{\beta} _ {\text{IV}}$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;If $K=L$, $X&amp;rsquo;Z$ and $Z&amp;rsquo;X$ are squared matrices and, by the relevance
condition, non-singular (invertible). $$
\begin{aligned}
\hat{\beta} _ {\text{2SLS}} &amp;amp;= \Big( X&amp;rsquo;Z (Z&amp;rsquo;Z)^{-1} Z&amp;rsquo;X \Big)^{-1} \Big( X&amp;rsquo;Z (Z&amp;rsquo;Z)^{-1} Z&amp;rsquo;y \Big) = \newline
&amp;amp;= (Z&amp;rsquo;X)^{-1} (Z&amp;rsquo;Z) (X&amp;rsquo;Z)^{-1} X&amp;rsquo;Z (Z&amp;rsquo;Z)^{-1} Z&amp;rsquo;y = \newline
&amp;amp;= (Z&amp;rsquo;X)^{-1} (Z&amp;rsquo;Z) (Z&amp;rsquo;Z)^{-1} Z&amp;rsquo;y = \newline
&amp;amp;= (Z&amp;rsquo;X)^{-1} (Z&amp;rsquo;y) = \newline
&amp;amp;= \hat{\beta} _ {\text{IV}}
\end{aligned}
$$ $$\tag*{$\blacksquare$}$$&lt;/p&gt;
&lt;h3 id=&#34;demand-example&#34;&gt;Demand Example&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Example&lt;/strong&gt; from Hayiashi (2000) page 187: demand and supply
simultaneous equations. $$
\begin{aligned}
&amp;amp; q_i^D(p_i) = \alpha_0 + \alpha_1 p_i + u_i \newline
&amp;amp; q_i^S(p_i) = \beta_0 + \beta_1 p_i + v_i
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;We have an endogeneity problem. To see why, we solve the system of
equations for $(p_i, q_i)$: $$
\begin{aligned}
&amp;amp; p_i = \frac{\beta_0 - \alpha_0}{\alpha_1 - \beta_1} + \frac{v_i - u_i}{\alpha_1 - \beta_1 } \newline
&amp;amp; q_i = \frac{\alpha_1\beta_0 - \alpha_0 \beta_1}{\alpha_1 - \beta_1} + \frac{\alpha_1 v_i - \beta_1 u_i}{\alpha_1 - \beta_1 }
\end{aligned}
$$&lt;/p&gt;
&lt;h3 id=&#34;demand-example-2&#34;&gt;Demand Example (2)&lt;/h3&gt;
&lt;p&gt;Then the price variable is not independent from the error term in
neither equation: $$
\begin{aligned}
&amp;amp; Cov(p_i, u_i) = - \frac{Var(u_i)}{\alpha_1 - \beta_1 } \newline
&amp;amp; Cov(p_i, v_i) = \frac{Var(v_i)}{\alpha_1 - \beta_1 }
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;As a consequence, the OLS estimators are not consistent: $$
\begin{aligned}
&amp;amp; \hat{\alpha} _ {1, OLS} \overset{p}{\to} \alpha_1 + \frac{Cov(p_i, u_i)}{Var(p_i)} \newline
&amp;amp; \hat{\beta} _ {1, OLS} \overset{p}{\to} \beta_1 + \frac{Cov(p_i, v_i)}{Var(p_i)}
\end{aligned}
$$&lt;/p&gt;
&lt;h3 id=&#34;demand-example-3&#34;&gt;Demand Example (3)&lt;/h3&gt;
&lt;p&gt;In general, running regressing $q$ on $p$ you estimate $$
\begin{aligned}
\hat{\gamma} _ {OLS} &amp;amp;\overset{p}{\to} \frac{Cov(p_i, q_i)}{Var(p_i)} = \newline
&amp;amp;= \frac{\alpha_1 Var(v_i) + \beta_1 Var(u_i)}{(\alpha_1 - \beta_1)^2} \left( \frac{Var(v_i) + Var(u_i)}{(\alpha_1 - \beta_1)^2} \right)^{-1} = \newline
&amp;amp;= \frac{\alpha_1 Var(v_i) + \beta_1 Var(u_i)}{Var(v_i) + Var(u_i)}
\end{aligned}
$$ Which is neither $\alpha_1$ nor $\beta_1$ but a variance weighted
average of the two.&lt;/p&gt;
&lt;h3 id=&#34;demand-example-4&#34;&gt;Demand Example (4)&lt;/h3&gt;
&lt;p&gt;Suppose we have a supply shifter $z_i$ such that&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\mathbb E[z_i v_i] \neq 0$&lt;/li&gt;
&lt;li&gt;$\mathbb E[z_i u_i] = 0$.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We combine the second condition and $\mathbb E[u_i] = 0$ to get a system
of 2 equations in 2 unknowns: $\alpha_0$ and $\alpha_1$. $$
\begin{aligned}
&amp;amp; \mathbb E[z_i u_i] = \mathbb E[ z_i (q_i^D(p_i) - \alpha_0 - \alpha_1 p_i) ] = 0 \newline
&amp;amp; \mathbb E[u_i] = \mathbb E[q_i^D(p_i) - \alpha_0 - \alpha_1 p_i] = 0&lt;br&gt;
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;We could try to solve for the vector $\alpha$ that solves $$
\begin{aligned}
&amp;amp; \mathbb E_n[z_i (q_i^D - x_i\alpha)] = 0 \newline
&amp;amp; \mathbb E_n[z_i q_i^D] -  \mathbb E_n[z_ix_i\alpha] = 0
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;If $\mathbb E_n[z_ix_i]$ is invertible, we get
$\hat{\alpha} = \mathbb E_n[z_ix_i]^{-1} \mathbb E_n[z_i q^D_i]$ which
is indeed the IV estimator of $\alpha$ using $z_i$ as an instrument for
the endogenous variable $p_i$.&lt;/p&gt;
&lt;h3 id=&#34;code---dgp&#34;&gt;Code - DGP&lt;/h3&gt;
&lt;p&gt;This code draws 100 observations from the model
$y = 2 x_1 - x_2 + \varepsilon$ where $x_1, x_2 \sim U[0,1]$ and
$\varepsilon \sim N(0,1)$.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# Set seed
Random.seed!(123);

# Set the number of observations
n = 100;

# Set the dimension of Z
l = 3;

# Draw instruments
Z = rand(Uniform(0,1), n, l);

# Correlation matrix for error terms
S = [1 0.8; 0.8 1];

# Endogenous X
γ = [2 0; 0 -1; -1 3];
ε = rand(Normal(0,1), n, 2) * cholesky(S).U;
X = Z*γ .+ ε[:,1];

# Calculate y
y = X*β .+ ε[:,2];
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;code---iv&#34;&gt;Code - IV&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# Estimate beta OLS
β_OLS = (X&#39;*X)\(X&#39;*y)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 2-element Array{Float64,1}:
##   2.335699233358403
##  -0.8576266209987325
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# IV: l=k=2 instruments
Z_IV = Z[:,1:k];
β_IV = (Z_IV&#39;*X)\(Z_IV&#39;*y)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 2-element Array{Float64,1}:
##   1.6133344277861439
##  -0.6678537395714547
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# Calculate standard errors
ε_hat = y - X*β_IV;
V_NHC_IV = var(ε_hat) * inv(Z_IV&#39;*X)*Z_IV&#39;*Z_IV*inv(Z_IV&#39;*X);
V_HC0_IV = inv(Z_IV&#39;*X)*Z_IV&#39; * (I(n) .* ε_hat.^2) * Z_IV*inv(Z_IV&#39;*X);
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;code---2sls&#34;&gt;Code - 2SLS&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# 2SLS: l=3 instruments
Pz = Z*inv(Z&#39;*Z)*Z&#39;;
β_2SLS = (X&#39;*Pz*X)\(X&#39;*Pz*y)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 2-element Array{Float64,1}:
##   1.904553638377971
##  -0.8810907510370429
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# Calculate standard errors
ε_hat = y - X*β_2SLS;
V_NCH_2SLS = var(ε_hat) * inv(X&#39;*Pz*X);
V_HC0_2SLS = inv(X&#39;*Pz*X)*X&#39;*Pz * (I(n) .* ε_hat.^2) *Pz*X*inv(X&#39;*Pz*X);
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;gmm&#34;&gt;GMM&lt;/h2&gt;
&lt;h3 id=&#34;setting&#34;&gt;Setting&lt;/h3&gt;
&lt;p&gt;We have a system of $L$ moment conditions $$
\begin{aligned}
&amp;amp; \mathbb E[g_1(\omega_i, \delta_0)] = 0 \newline
&amp;amp; \vdots \newline
&amp;amp; \mathbb E[g_L(\omega_i, \delta_0)] = 0
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;If $L = \dim (\delta_0)$, no problem. If $L &amp;gt; \dim (\delta_0)$, there
may be no solution to the system of equations.&lt;/p&gt;
&lt;h3 id=&#34;options&#34;&gt;Options&lt;/h3&gt;
&lt;p&gt;There are two possibilities.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;First Solution&lt;/strong&gt;: add moment conditions until the system is
identified $$
\mathbb E[ a&amp;rsquo; g(\omega_i, \delta_0)] = 0
$$ Solve $\mathbb E[Ag(\omega_i, \delta)] = 0$ for $\hat{\delta}$.
How to choose $A$? Such that it minimizes $Var(\hat{\delta})$.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Second Solution&lt;/strong&gt;: generalized method of moments (GMM) $$
\begin{aligned}
\hat{\delta} _ {GMM} &amp;amp;= \arg \min _ \delta \quad  \Big| \Big| \mathbb E_n [ g(\omega_i, \delta) ] \Big| \Big| = \newline
&amp;amp;= \arg \min _ \delta \quad n \mathbb E_n[g(\omega_i, \delta)]&amp;rsquo; W \mathbb E_n [g(\omega_i, \delta)]
\end{aligned}
$$&lt;/li&gt;
&lt;/ol&gt;
&lt;blockquote&gt;
&lt;p&gt;The choice of $A$ and $W$ are closely related to each other.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;1-step-gmm&#34;&gt;1-step GMM&lt;/h3&gt;
&lt;p&gt;Since $J(\delta,W)$ is a quadratic form, a closed form solution exists:
$$
\hat{\delta}(W) = \Big(\mathbb E_n[z_i x_i&amp;rsquo;] W \mathbb E_n[z_i x_i&amp;rsquo;] \Big)^{-1}\mathbb E_n[z_i x_i&amp;rsquo;] W \mathbb E_n[z_i y_i]
$$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Assumptions&lt;/strong&gt; for consistency of the GMM estimator given data
$\mathcal D = \lbrace y_i, x_i, z_i \rbrace _ {i=1}^n$:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Linearity&lt;/strong&gt;: $y_i = x_i\gamma_0 + \varepsilon_i$&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;IID&lt;/strong&gt;: $(y_i, x_i, z_i)$ iid&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Orthogonality&lt;/strong&gt;:
$\mathbb E [z_i(y_i - x_i\gamma_0)] = \mathbb E[z_i \varepsilon_i] = 0$&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Rank identification&lt;/strong&gt;: $\Sigma_{xz} = \mathbb E[z_i x_i&amp;rsquo;]$ has
full rank&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;convergence&#34;&gt;Convergence&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Under linearity, independence, orthogonality and rank conditions, if
$\hat{W} \overset{p}{\to} W$ positive definite, then $$
\hat{\delta}(\hat{W}) \to \delta(W)
$$ If in addition to the above assumption,
$\sqrt{n} \mathbb E_n [g(\omega_i, \delta_0)] \overset{d}{\to} N(0,S)$
for a fixed positive definite $S$, then $$
\sqrt{n} (\hat{\delta} (\hat{W}) - \delta(W)) \overset{d}{\to} N(0,V)
$$ where
$V = (\Sigma&amp;rsquo; _ {xz} W \Sigma _ {xz})^{-1} \Sigma _ {xz} W S W \Sigma _ {xz}(\Sigma&amp;rsquo; _ {xz} W \Sigma _ {xz})^{-1}$.&lt;/p&gt;
&lt;p&gt;Finally, if a consistent estimator $\hat{S}$ of $S$ is available, then
using sample analogues $\hat{\Sigma}_{xz}$ it follows that $$
\hat{V} \overset{p}{\to} V
$$&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;If $W = S^{-1}$ then $V$ reduces to
$V = (\Sigma&amp;rsquo; _ {xz} W \Sigma _ {xz})^{-1}$. Moreover,
$(\Sigma&amp;rsquo; _ {xz} W \Sigma _ {xz})^{-1}$ is the smallest possible form
of $V$, in a positive definite sense.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Therefore, to have an efficient estimator, you want to construct
$\hat{W}$ such that $\hat{W} \overset{p}{\to} S^{-1}$.&lt;/p&gt;
&lt;h3 id=&#34;2-step-gmm&#34;&gt;2-step GMM&lt;/h3&gt;
&lt;p&gt;Estimation steps:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Choose an arbitrary weighting matrix $\hat{W}_{init}$ (usually the
identity matrix $I_K$)&lt;/li&gt;
&lt;li&gt;Estimate $\hat{\delta} _ {init}(\hat{W} _ {init})$&lt;/li&gt;
&lt;li&gt;Estimate $\hat{S}$ (asymptotic variance of the moment condition)&lt;/li&gt;
&lt;li&gt;Estimate $\hat{\delta}(\hat{S}^{-1})$&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;On the procedure:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;This estimator achieves the semiparametric efficiency bound.&lt;/li&gt;
&lt;li&gt;This strategy works only if $\hat{S} \overset{p}{\to} S$ exists.&lt;/li&gt;
&lt;li&gt;For iid cases: we can use
$\hat{\delta} = \mathbb E_n[(\hat{\varepsilon}_i z_i)(\hat{\varepsilon}_i z_i) &amp;rsquo; ]$
where
$\hat{\varepsilon}_i = y_i - x_i \hat{\delta}(\hat{W} _ {init})$.&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;code---1-step-gmm&#34;&gt;Code - 1-step GMM&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# GMM 1-step: inefficient weighting matrix
W_1 = I(l);

# Objective function
gmm_1(b) = ( y - X*b )&#39; * Z * W_1 *  Z&#39; * ( y - X*b );

# Estimate GMM
β_gmm_1 = optimize(gmm_1, β_OLS).minimizer
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 2-element Array{Float64,1}:
##   1.91556882526808
##  -0.8769689391885799
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# Standard errors GMM
ε_hat = y - X*β_gmm_1;
S_hat = Z&#39; * (I(n) .* ε_hat.^2) * Z;
d_hat = -X&#39;*Z;
V_gmm_1 = inv(d_hat * inv(S_hat) * d_hat&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 2×2 Array{Float64,2}:
##   0.0158497   -0.00346601
##  -0.00346601   0.00616531
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;code---2-step-gmm&#34;&gt;Code - 2-step GMM&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# GMM 2-step: efficient weighting matrix
W_2 = inv(S_hat);

# Objective function
gmm_2(b) = ( y - X*b )&#39; * Z * W_2 *  Z&#39; * ( y - X*b );

# Estimate GMM
β_gmm_2 = optimize(gmm_2, β_OLS).minimizer
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 2-element Array{Float64,1}:
##   1.905326742963115
##  -0.881808949213345
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# Standard errors GMM
ε_hat = y - X*β_gmm_2;
S_hat = Z&#39; * (I(n) .* ε_hat.^2) * Z;
d_hat = -X&#39;*Z;
V_gmm_2 = inv(d_hat * inv(S_hat) * d_hat&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 2×2 Array{Float64,2}:
##   0.0162603   -0.00357632
##  -0.00357632   0.00631259
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;testing-overidentifying-restrictions&#34;&gt;Testing Overidentifying Restrictions&lt;/h3&gt;
&lt;p&gt;If the equations are &lt;strong&gt;exactly identified&lt;/strong&gt;, then it is possible to
choose $\delta$ so that all the elements of the sample moments
$\mathbb E_n[g(\omega_i; \delta)]$ are zero and thus that the distance
$$
J(\delta, \hat{W}) = n \mathbb E_n[g(\omega_i, \delta)]&amp;rsquo; \hat{W} \mathbb E_n[g(\omega_i, \delta)]
$$ is zero. (The $\delta$ that does it is the IV estimator.)&lt;/p&gt;
&lt;p&gt;If the equations are &lt;strong&gt;overidentified&lt;/strong&gt;, i.e. $L$ (number of
instruments) $&amp;gt; K$ (number of equations), then the distance cannot be
zero exactly in general, but we would expect the minimized distance to
be &lt;em&gt;close&lt;/em&gt; to zero.&lt;/p&gt;
&lt;h3 id=&#34;naive-test&#34;&gt;Naive Test&lt;/h3&gt;
&lt;p&gt;Suppose your model is overidentified ($L &amp;gt; K$) and you use the following
naive testing procedure:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Estimate $\hat{\delta}$ using a subset of dimension $K$ of
instruments $\lbrace z_1 , .. , z_K\rbrace$ for
$\lbrace x_1 , &amp;hellip; , x_K\rbrace$&lt;/li&gt;
&lt;li&gt;Set $\hat{\varepsilon}_i = y_i - x_i \hat{\delta} _ {\text{GMM}}$&lt;/li&gt;
&lt;li&gt;Infer the size of the remaining $L-K$ moment conditions
$\mathbb E[z _{i, K+1} \varepsilon_i], &amp;hellip;, \mathbb E[z _{i, L} \varepsilon_i]$
looking at their empirical counterparts
$\mathbb E_n[z _{i, K+1} \hat{\varepsilon}_i], &amp;hellip;, \mathbb E_n[z _{i, L} \hat{\varepsilon}_i]$&lt;/li&gt;
&lt;li&gt;Reject exogeneity if the empirical expectations are high. How high?
Calculate p-values.&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;example&#34;&gt;Example&lt;/h3&gt;
&lt;p&gt;If you have two invalid instruments and you use one to test the validity
of the other, it might happen by chance that you don’t reject it.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Model: $y_i = x_i + \varepsilon_i$ and
$x_i = \frac{1}{2} z _{i1} - \frac{1}{2} z _{i2} + u_i$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Have $$
Cov (z _{i1}, z _{i2}, \varepsilon_i, u_i) =
\begin{bmatrix}
1 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 \newline 0 &amp;amp; 1 &amp;amp; 0 &amp;amp; 0 \newline 0 &amp;amp; 0 &amp;amp; 1 &amp;amp; 0.5 \newline 0 &amp;amp; 0 &amp;amp; 0.5 &amp;amp; 1
\end{bmatrix}
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;You want to test whether the second instrument is valid (is not
since $\mathbb E[z_2 \varepsilon] \neq 0$). You use $z_1$ and
estimate $\hat{\beta} \to$ the estimator is consistent.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;You obtain $\mathbb E_n[z _{i2} \hat{\varepsilon}_i] \simeq 0$ even
if $z_2$ is invalid&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Problem: you are using an invalid instrument in the first place.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;hansens-test&#34;&gt;Hansen’s Test&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt;: We are interested in testing
$H_0: \mathbb E[z_i \varepsilon_i] = 0$ against
$H_1: \mathbb E[z_i \varepsilon_i] \neq 0$. Suppose
$\hat{S} \overset{p}{\to} S$. Then $$
J(\hat{\delta}(\hat{S}^{-1}) , \hat{S}^{-1}) \overset{d}{\to} \chi^2 _ {L-K}
$$ For $c$ satisfying $\alpha = 1- G_{L - K} ( c )$,
$\Pr(J&amp;gt;c | H_0) \to \alpha$ so the test &lt;em&gt;reject $H_0$ if $J &amp;gt; c$&lt;/em&gt; has
asymptotic size $\alpha$.&lt;/p&gt;
&lt;h3 id=&#34;comments&#34;&gt;Comments&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;The degrees of freedom of the asymptotic distribution are the number
of overidentifying restrictions.&lt;/li&gt;
&lt;li&gt;This is a specification test, testing whether all model assumptions
are true jointly. Only when we are confident that about the other
assumptions, can we interpret a large $J$ statistic as evidence for
the endogeneity of some of the $L$ instruments included in $x$.&lt;/li&gt;
&lt;li&gt;Unlike the tests we have encountered so far, the test is not
consistent against some failures of the orthogonality conditions
(that is, it is not consistent against some fixed elements of the
alternative).&lt;/li&gt;
&lt;li&gt;Several papers in the July 1996 issue of JBES report that the
finite-sample null rejection probability of the test can far exceed
the nominal significance level $\alpha$.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;special-case-conditional-homoskedasticity&#34;&gt;Special Case: Conditional Homoskedasticity&lt;/h3&gt;
&lt;p&gt;The main implication of conditional homoskedasticity is that efficient
GMM becomes 2SLS. With efficient GMM estimation, the weighting matrix is
$\hat{S}^{-1} = \mathbb En [z_i z_i&amp;rsquo; \varepsilon_i^2]^{-1}$. With
conditional homoskedasticity, the efficient weighting matrix is
$\mathbb E_n[z_iz_i&amp;rsquo;]^{-1} \sigma^{-2}$, or equivalently
$\mathbb E_n[z_iz_i&amp;rsquo;]^{-1}$. Then, the GMM estimator becomes $$
\hat{\delta}(\hat{S}^{-1}) = \Big(\mathbb E_n[z_i x_i&amp;rsquo;]&amp;rsquo; \underbrace{\mathbb E_n[z_iz_i&amp;rsquo;]^{-1} \mathbb E[z_i x_i&amp;rsquo;]} _ {\text{ols of } x_i \text{ on }z_i} \Big)^{-1}\mathbb E_n[z_i x_i&amp;rsquo;]&amp;rsquo; \underbrace{\mathbb E_n[z_iz_i&amp;rsquo;]^{-1} \mathbb E[z_i y_i&amp;rsquo;]} _ {\text{ols of } y_i \text{ on }z_i}= \hat{\delta} _ {2SLS}
$$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;: Consider the matrix notation. $$
\begin{aligned}
\hat{\delta} \left( \frac{Z&amp;rsquo;Z}{n}\right) &amp;amp;= \left( \frac{X&amp;rsquo;Z}{n} \left( \frac{Z&amp;rsquo;Z}{n}\right)^{-1} \frac{Z&amp;rsquo;X}{n} \right)^{-1} \frac{X&amp;rsquo;Z}{n} \left( \frac{Z&amp;rsquo;Z}{n}\right)^{-1} \frac{Z&amp;rsquo;Y}{n} = \newline
&amp;amp;= \left( X&amp;rsquo;Z(Z&amp;rsquo;Z)^{-1} Z&amp;rsquo;X \right)^{-1} X&amp;rsquo;Z(Z&amp;rsquo;Z)^{-1} Z&amp;rsquo;Y = \newline
&amp;amp;= \left(X&amp;rsquo;P_ZX\right)^{-1} X&amp;rsquo;P_ZY = \newline
&amp;amp;= \left(X&amp;rsquo;P_ZP_ZX\right)^{-1} X&amp;rsquo;P_ZY = \newline
&amp;amp;= \left(\hat{X}&amp;rsquo;_Z \hat{X}_Z\right)^{-1} \hat{X}&amp;rsquo;_ZY = \newline
&amp;amp;= \hat{\delta} _ {2SLS}
\end{aligned}
$$ $$\tag*{$\blacksquare$}$$&lt;/p&gt;
&lt;h3 id=&#34;small-sample-properties-of-2sls&#34;&gt;Small-Sample Properties of 2SLS&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt;: When the number of instruments is equal to the sample size
($L = n$), then $\hat{\delta} _ {2SLS} = \hat{\delta} _ {OLS}$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;: We have a perfect prediction problem. The first stage
estimated coefficient $\hat{\gamma}$ is such that it solves the normal
equations: $\hat{\gamma} = z_i^{-1} x_i$. Then $$
\begin{aligned}
\hat{\delta} _ {2SLS} &amp;amp;= \mathbb E_n[\hat{x}_i x&amp;rsquo;_i]^{-1} \mathbb E_n[\hat{x}_i y_i] = \newline
&amp;amp;= \mathbb E_n[z_i z_i^{-1} x_i x&amp;rsquo;_i]^{-1} \mathbb E_n[z_i z_i^{-1} x_i y_i] = \newline
&amp;amp;= \mathbb E_n[x_i x&amp;rsquo;_i]^{-1} \mathbb E_n[x_i y_i] = \newline
&amp;amp;= \hat{\delta} _ {OLS}
\end{aligned}
$$ $$\tag*{$\blacksquare$}$$&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;You have this overfitting problem in general when the number of
instruments is large relative to the sample size. This problem arises
even if the instruments are valid.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;example-from-angrist-1992&#34;&gt;Example from Angrist (1992)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;They regress wages on years of schooling.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Problem&lt;/strong&gt;: endogeneity: both variables are correlated with skills
which are unobserved.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Solution&lt;/strong&gt;: instrument years of schooling with the quarter of
birth.
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Idea&lt;/strong&gt;: if born in the first three quarters, can attend school
from the year of your sixth birthday. Otherwise, you have to
wait one more year.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Problem&lt;/strong&gt;: quarters of birth are three dummies.
&lt;ul&gt;
&lt;li&gt;In order to ``improve the first stage fit” they interact them
with year of birth (180 effective instruments) and also with the
state (1527 effective instruments).&lt;/li&gt;
&lt;li&gt;This mechanically increases the $R^2$ but also increases the
bias of the 2SLS estimator.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Solutions&lt;/strong&gt;: LIML, JIVE, RJIVE (Hansen et al., 2014), Post-Lasso
(Belloni et al., 2012).&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;example-from-angrist-1992-1&#34;&gt;Example from Angrist (1992)&lt;/h3&gt;
&lt;img src=&#34;../img/Fig_441.png&#34; style=&#34;width:80.0%&#34; /&gt;
&lt;h2 id=&#34;many-instrument-robust-estimation&#34;&gt;Many Instrument Robust Estimation&lt;/h2&gt;
&lt;h3 id=&#34;issue&#34;&gt;Issue&lt;/h3&gt;
&lt;p&gt;Why having too many instruments is problematic? As the number of
instruments increases, the estimated coefficient gets closer to OLS
which is biased. As seen in the theorem above, for $L=n$, the two
estimators coincide.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;../img/Fig_451.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;liml&#34;&gt;LIML&lt;/h3&gt;
&lt;p&gt;An alternative method to estimate the parameters of the structural
equation is by maximum likelihood. Anderson and Rubin (1949) derived the
maximum likelihood estimator for the joint distribution of $(y_i, x_i)$.
The estimator is known as &lt;strong&gt;limited information maximum likelihood&lt;/strong&gt;, or
&lt;strong&gt;LIML&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;This estimator is called “limited information” because it is based on
the structural equation for $(y_i, x_i)$ combined with the reduced form
equation for $x_i$. If maximum likelihood is derived based on a
structural equation for $x_i$ as well, then this leads to what is known
as &lt;strong&gt;full information maximum likelihood (FIML)&lt;/strong&gt;. The advantage of the
LIML approach relative to FIML is that the former does not require a
structural model for $x_i$, and thus allows the researcher to focus on
the structural equation of interest - that for $y_i$.&lt;/p&gt;
&lt;h3 id=&#34;k-class-estimators&#34;&gt;K-class Estimators&lt;/h3&gt;
&lt;p&gt;The &lt;strong&gt;k-class&lt;/strong&gt; estimators have the form $$
\hat{\delta}(\alpha) = (X&amp;rsquo; P_Z X - \alpha X&amp;rsquo; X)^{-1} (X&amp;rsquo; P_Z Y - \alpha X&amp;rsquo; Y)
$$&lt;/p&gt;
&lt;p&gt;The limited information maximum likelihood estimator &lt;strong&gt;LIML&lt;/strong&gt; is the
k-class estimator $\hat{\delta}(\alpha)$ where $$
\alpha = \lambda_{min} \Big( ([X&amp;rsquo; , Y]^{-1} [X&amp;rsquo; , Y])^{-1} [X&amp;rsquo; , Y]^{-1} P_Z [X&amp;rsquo; , Y] \Big)
$$&lt;/p&gt;
&lt;p&gt;If $\alpha = 0$ then
$\hat{\delta} _ {\text{LIML}} = \hat{\delta} _ {\text{2SLS}}$ while for
$\alpha \to \infty$,
$\hat{\delta} _ {\text{LIML}} \to \hat{\delta} _ {\text{OLS}}$.&lt;/p&gt;
&lt;h3 id=&#34;comments-on-liml&#34;&gt;Comments on LIML&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;The particular choice of $\alpha$ gives a many instruments robust
estimate&lt;/li&gt;
&lt;li&gt;The LIML estimator has no finite sample moments.
$\mathbb E[\delta(\alpha_{LIML})]$ does not exist in general&lt;/li&gt;
&lt;li&gt;In simulation studies performs well&lt;/li&gt;
&lt;li&gt;Has good asymptotic properties&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Asymptotically the LIML estimator has the same distribution as 2SLS.
However, they can have quite different behaviors in finite samples.
There is considerable evidence that the LIML estimator has superior
finite sample performance to 2SLS when there are many instruments or the
reduced form is weak. However, on the other hand there is worry that
since the LIML estimator is derived under normality it may not be robust
in non-normal settings.&lt;/p&gt;
&lt;h3 id=&#34;jive&#34;&gt;JIVE&lt;/h3&gt;
&lt;p&gt;The &lt;strong&gt;Jacknife IV&lt;/strong&gt; procedure is the following&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Regress $\lbrace x_j \rbrace _ {j \neq i}$ on
$\lbrace z_j \rbrace _ {j \neq i}$ and estimate $\pi_{-i}$ (leave
the $i^{th}$ observation out).&lt;/li&gt;
&lt;li&gt;Form $\hat{x}_i = \hat{\pi} _ {-i} z_i$.&lt;/li&gt;
&lt;li&gt;Run IV using $\hat{x}_i$ as instruments. $$
\hat{\delta} _ {JIVE} = \mathbb E_n[\hat{x}_i x_i&amp;rsquo;]^{-1} \mathbb E_n[\hat{x}_i y_i&amp;rsquo;]
$$&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;comments-on-jive&#34;&gt;Comments on JIVE:&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Prevents overfitting.&lt;/li&gt;
&lt;li&gt;With many instruments you get bad out of sample prediction which
implies low correlation between $\hat{x}_i$ and $x_i$:
$\mathbb E_n[\hat{x}_i x_i&amp;rsquo;] \simeq 0$.&lt;/li&gt;
&lt;li&gt;Use lasso/ridge regression in the first stage in case of too many
instruments.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;hausman-test&#34;&gt;Hausman Test&lt;/h3&gt;
&lt;p&gt;Here we consider testing the validity of OLS. OLS is generally preferred
to IV in terms of precision. Many researchers only doubt the (joint)
validity of the regressor $z_i$ instead of being certain that it is
invalid (in the sense of not being predetermined). So then they wish to
choose between OLS and 2SLS, assuming that they have an instrument
vector $x_i$ whose validity is not in question. Further, assume for
simplicity that $L = K$ so that the efficient GMM estimator is the IV
estimator.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;Hausman test statistic&lt;/strong&gt; $$
H \equiv n (\hat{\delta} _ {IV} - \hat{\delta} _ {OLS})&amp;rsquo; [\hat{Avar} (\hat{\delta} _ {IV} - \hat{\delta} _ {OLS})]^{-1} (\hat{\delta} _ {IV} - \hat{\delta} _ {OLS})
$$ is asymptotically distributed as a $\chi^2_{L-s}$ under the null
where $s = | z_i \cup x_i |$: the number of regressors that are retained
as instruments in $x_i$.&lt;/p&gt;
&lt;h3 id=&#34;comments-1&#34;&gt;Comments&lt;/h3&gt;
&lt;p&gt;In general, the idea of the Hausman test is the following. If you have
two estimators, one which is efficient under $H_0$ but inconsistent
under $H_1$ (in this case, OLS), and another which is consistent under
$H_1$ (in this case, IV), then construct a test as a quadratic form in
the differences of the estimators. Another classic example arises in
panel data with the hypothesis $H_0$ of unconditional strict exogeneity.
In that case, under $H_0$ Random Effects estimators are efficient but
under $H_1$ they are inconsistent. Fixed Effects estimators instead are
consistent under $H_1$.&lt;/p&gt;
&lt;p&gt;The Hausman test statistic can be used as a pretest procedure: select
either OLS or IV according to the outcome of the test. Although widely
used, this pretest procedure is not advisable. When the null is false,
it is still possible that the test &lt;em&gt;accepts&lt;/em&gt; the null (committing a Type
2 error). In particular, this can happen with a high probability when
the sample size is &lt;em&gt;small&lt;/em&gt; and/or when the regressor $z_i$ is &lt;em&gt;almost
valid&lt;/em&gt;. In such an instance, estimation and also inference will be based
on incorrect methods. Therefore, the overall properties of the Hausman
pretest procedure are undesirable.&lt;/p&gt;
&lt;p&gt;The Hausman test is an example of a specification test. There are many
other specification tests. One could for example test for conditional
homoskedasticity. Unlike for the OLS case, there does not exist a
convenient test for conditional homoskedasticity for the GMM case. A
test statistic that is asymptotically chi-squared under the null is
available but is extremely cumbersome; see White (1982, note 2). If in
doubt, it is better to use the more generally valid inference methods
that allow for conditional heteroskedasticity. Similarly, there does not
exist a convenient test for serial correlation for the GMM case. If in
doubt, it is better to use the more generally valid inference methods
that allow for serial correlation; for example, when data are collected
over time (that is, time-series data).&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>OLS Inference</title>
      <link>https://matteocourthoud.github.io/course/metrics/06_ols_inference/</link>
      <pubDate>Fri, 29 Oct 2021 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/course/metrics/06_ols_inference/</guid>
      <description>&lt;h2 id=&#34;asymptotic-theory-of-the-ols-estimator&#34;&gt;Asymptotic Theory of the OLS Estimator&lt;/h2&gt;
&lt;h3 id=&#34;ols-consistency&#34;&gt;OLS Consistency&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt;: Assume that $(x_i, y_i) _ {i=1}^n$ i.i.d. ,
$\mathbb E[x_i x_i&amp;rsquo;] = Q$ positive definite,
$\mathbb E[x_i x_i&amp;rsquo;] &amp;lt; \infty$ and $\mathbb E [y_i^2] &amp;lt; \infty$, then
$\hat \beta _ {OLS}$ is a &lt;strong&gt;consistent&lt;/strong&gt; estimator of $\beta_0$,
i.e. $\hat \beta = \mathbb E_n [x_i x_i&amp;rsquo;] \mathbb E_n [x_i y_i]\overset{p}{\to} \beta_0$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;:&lt;br&gt;
We consider 4 steps:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;$\mathbb E_n [x_i x_i&amp;rsquo;] \xrightarrow{p} \mathbb E [x_i x_i&amp;rsquo;]$ by
WLLN since $x_i x_i&amp;rsquo;$ iid and $\mathbb E[x_i x_i&amp;rsquo;] &amp;lt; \infty$.&lt;/li&gt;
&lt;li&gt;$\mathbb E_n [x_i y_i] \xrightarrow{p} \mathbb E [x_i y_i]$ by WLLN,
due to $x_i y_i$ iid, Cauchy-Schwarz and finite second moments of
$x_i$ and $y_i$ $$
\mathbb E \left[ x_i y_i \right]  \leq \sqrt{ \mathbb E[x_i^2] \mathbb E[y_i^2]} &amp;lt; \infty
$$&lt;/li&gt;
&lt;li&gt;$\mathbb E_n [x_i x_i&amp;rsquo;]^{-1} \xrightarrow{p} \mathbb E [x_i x_i&amp;rsquo;]^{-1}$
by CMT.&lt;/li&gt;
&lt;li&gt;$\mathbb E_n [x_i x_i&amp;rsquo;]^{-1} \mathbb E_n [x_i y_i] \xrightarrow{p} \mathbb E [x_i x_i&amp;rsquo;]^{-1} \mathbb E [x_i y_i] = \beta$
by CMT. $$\tag*{$\blacksquare$}$$&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;variance-and-assumptions&#34;&gt;Variance and Assumptions&lt;/h3&gt;
&lt;p&gt;Now we are going to investigate the variance of $\hat \beta _ {OLS}$
progressively relaxing the underlying assumptions.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Gaussian error term.&lt;/li&gt;
&lt;li&gt;Homoskedastic error term.&lt;/li&gt;
&lt;li&gt;Heteroskedastic error term.&lt;/li&gt;
&lt;li&gt;Heteroskedastic and autocorrelated error term.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;gaussian-error-term&#34;&gt;Gaussian Error Term&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt;: Under the GM assumption (1)-(5),
$\hat \beta - \beta |X \sim N(0, \sigma^2 (X&amp;rsquo;X)^{-1})$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;:&lt;br&gt;
We follow 2 steps:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;We can rewrite $\hat \beta$ as $$
\begin{aligned}
\hat \beta &amp;amp; = (X&amp;rsquo;X)^{-1} X&amp;rsquo;y = (X&amp;rsquo;X)^{-1} X&amp;rsquo;(X\beta + \varepsilon) \newline
&amp;amp;= \beta + (X&amp;rsquo;X)^{-1} X&amp;rsquo; \varepsilon = \newline
&amp;amp;= \beta + \mathbb E_n [x_i x_i&amp;rsquo;]^{-1} \mathbb E_n [x_i \varepsilon_i]
\end{aligned}
$$&lt;/li&gt;
&lt;li&gt;Therefore:
$\hat \beta-\beta = \mathbb E_n [x_i x_i&amp;rsquo;]^{-1} \mathbb E_n [x_i \varepsilon_i]$.
$$
\begin{aligned}
\hat \beta-\beta |X &amp;amp; \sim (X&amp;rsquo;X)^{-1} X&amp;rsquo; N(0, \sigma^2 I_n) = \newline
&amp;amp;= N(0, \sigma^2 (X&amp;rsquo;X)^{-1} X&amp;rsquo;X (X&amp;rsquo;X)^{-1}) = \newline
&amp;amp;= N(0, \sigma^2 (X&amp;rsquo;X)^{-1})
\end{aligned}
$$ $$\tag*{$\blacksquare$}$$&lt;/li&gt;
&lt;/ol&gt;
&lt;blockquote&gt;
&lt;p&gt;Does it make sense to assume that $\varepsilon$ is gaussian? Not much.
But does it make sense that $\hat \beta$ is gaussian? Yes, because
it’s an average.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;homoskedastic-error-term&#34;&gt;Homoskedastic Error Term&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt;: Under the assumptions of the previous theorem, plus
$\mathbb E[x^4] &amp;lt; \infty$, the OLS estimate has an asymptotic normal
distribution:
$\hat \beta|X \overset{d}{\to} N(\beta, \sigma^2 (X&amp;rsquo;X)^{-1})$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;: $$
\sqrt{n} (\hat \beta - \beta ) = \underbrace{\mathbb E_n [x_i x_i&amp;rsquo;]^{-1}} _ {\xrightarrow{p} Q^{-1} }   \underbrace{\sqrt{n} \mathbb E_n [x_i \varepsilon_i ]} _ {\xrightarrow{d} N(0, \Omega)} \rightarrow N(0, \Sigma )
$$ where in general
$\Omega = Var (x_i \varepsilon_i) = \mathbb E [(x_i \varepsilon_i)^2]$
and $\Sigma = Q^{-1} \Omega Q^{-1}$. $$\tag*{$\blacksquare$}$$&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Given that $Q = \mathbb E [x_i x_i&amp;rsquo;]$ is unobserved, we estimate it
with $\hat{Q} = \mathbb E_n [x_i x_i&amp;rsquo;]$. Since we have assumed
homoskedastic error term, we have $\Omega = \sigma^2 (X&amp;rsquo;X)^{-1}$.
Since we do not observe $\sigma^2$ we estimate it as
$\hat{\sigma}^2 = \mathbb E_n[\hat{\varepsilon}_i^2]$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The terms $x_i \varepsilon_i$ are called &lt;strong&gt;scores&lt;/strong&gt; and we can already
see their central importance for inference.&lt;/p&gt;
&lt;h3 id=&#34;heteroskedastic-error-term&#34;&gt;Heteroskedastic Error Term&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Assumption&lt;/strong&gt;: $\mathbb E [\varepsilon_i x_i \varepsilon_j&amp;rsquo; x_j&amp;rsquo;] = 0$,
for all $j \ne i$ and $\mathbb E [\varepsilon_i^4] \leq \infty$,
$\mathbb E [|| x_i||^4] \leq C &amp;lt; \infty$ a.s.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt;: Under GM assumptions (1)-(4) plus heteroskedastic error
term, the following estimators are consistent,
i.e. $\hat{\Sigma}\xrightarrow{p} \Sigma$.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Note that we are only looking at $\Omega$ of the
$\Sigma = Q^{-1} \Omega Q^{-1}$ matrix.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;HC0&lt;/strong&gt;: use the observed residual $\hat{\varepsilon}_i$ $$
\Omega _ {HC0} = \mathbb E_n [x_i x_i&amp;rsquo; \hat{\varepsilon}_i^2]
$$ When $k$ is too big relative to $n$ – i.e.,
$k/n \rightarrow c &amp;gt;0$ – $\hat{\varepsilon}_i^2$ are too small
($\Omega _ {HC0}$ biased towards zero). $\Omega _ {HC1}$,
$\Omega _ {HC2}$ and $\Omega _ {HC3}$ try to correct this small
sample bias.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;HC1&lt;/strong&gt;: degree of freedom correction (default &lt;code&gt;robust&lt;/code&gt; in Stata) $$
\Omega _ {HC1} = \frac{1}{n - k }\mathbb E_n [x_i x_i&amp;rsquo; \hat{\varepsilon}_i^2]
$$&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;HC2&lt;/strong&gt;: use standardized residuals $$
\Omega _ {HC2} = \mathbb E_n [x_i x_i&amp;rsquo; \hat{\varepsilon}_i^2 (1-h _ {ii})^{-1}]
$$ where $h _ {ii} = [X(X&amp;rsquo;X)^{-1} X&amp;rsquo;] _ {ii}$ is the &lt;strong&gt;leverage&lt;/strong&gt;
of the $i^{th}$ observation. A large $h _ {ii}$ means that
observation $i$ is unusual in the sense that the regressor $x_i$ is
far from its sample mean.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;HC3&lt;/strong&gt;: use prediction error, equivalent to Jack-knife estimator,
i.e., $\mathbb E_n [x_i x_i&amp;rsquo; \hat{\varepsilon} _ {(-i)}^2]$ $$
\Omega _ {HC3} = \mathbb E_n [x_i x_i&amp;rsquo; \hat{\varepsilon}_i^2 (1-h _ {ii})^{-2}]
$$ This estimator does not overfit when $k$ is relatively big with
respect to $n$. Idea: you exclude the corresponding observation when
estimating a particular $\varepsilon_i$:
$\hat{\varepsilon}_i = y_i - x_i&amp;rsquo; \hat \beta _ {-i}$.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;hc0-consistency&#34;&gt;HC0 Consistency&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Under regularity conditions HC0 is consistent,
i.e. $\hat{\Sigma} _ {HC0} \overset{p}{\to} \Sigma$. $$
\hat{\Sigma} = \hat{Q}^{-1} \hat{\Omega} \hat{Q}^{-1} \xrightarrow{p} \Sigma \qquad  \text{ with } \hat{\Omega} = \mathbb E_n [x_i x_i&amp;rsquo;     \hat{\varepsilon}_i^2] \quad \text{ and } \hat{Q} = \mathbb E_n [x_i x_i&amp;rsquo;]^{-1}
$$&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Why is the proof relevant? You cannot directly apply the WLLN to
$\hat \Sigma$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;For the case $\mathrm{dim}(x_i) =1$.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;$\hat{Q}^{-1} \xrightarrow{p} Q^{-1}$ by WLLN since $x_i$ is iid,
$\mathbb E[x_i^4] &amp;lt; \infty$&lt;/li&gt;
&lt;li&gt;$\bar{\Omega} = \mathbb E_n [\varepsilon_i^2 x_i x_i&amp;rsquo;] \xrightarrow{p} \Omega$
by WLLN since $\mathbb E_n [\varepsilon_i^4] &amp;lt; c$ and $x_i$ bounded.&lt;/li&gt;
&lt;li&gt;By the triangle inequality, $$
| \hat{\Omega} - \hat{\Omega}| \leq \underbrace{|\Omega - \bar{\Omega}|} _ {\overset{p}{\to} 0} + \underbrace{|\bar{\Omega} - \hat{\Omega}|} _ {\text{WTS:} \overset{p}{\to} 0}
$$&lt;/li&gt;
&lt;li&gt;We want to show $|\bar{\Omega} - \hat{\Omega}| \overset{p}{\to} 0$
$$
\begin{aligned}
|\bar{\Omega} - \hat{\Omega}| &amp;amp;= \mathbb E_n [\varepsilon_i^2 x_i^2] - \mathbb E_n [\hat{\varepsilon}_i^2 x_i^2]  = \newline
&amp;amp;= \mathbb E_n [\left( \varepsilon_i^2 - \hat{\varepsilon}_i^2 \right) x_i^2] \leq \newline
&amp;amp; \leq \mathbb E_n \left[ \left( \varepsilon_i^2 - \hat{\varepsilon}_i^2 \right)^2\right]^{\frac{1}{2}} \mathbb E_n [x_i^4]^{\frac{1}{2}}
\end{aligned}
$$ where
$\mathbb E_n [x_i^4]^{\frac{1}{2}} \xrightarrow{p} \mathbb E [x_i^4]^{\frac{1}{2}}$
by $x_i$ bounded, iid and CMT.&lt;/li&gt;
&lt;li&gt;We want to show that
$\mathbb E_n \left[ \left( \varepsilon_i^2 - \hat{\varepsilon}_i^2 \right)^2\right] \leq \eta$
with $\eta \rightarrow 0$. Let
$L = \max_i |\hat{\varepsilon}_i - \varepsilon_i|$ (RV depending on
$n$), with $L \xrightarrow{p} 0$ since $$
|\hat{\varepsilon}_i - \varepsilon_i| = |x_i \hat \beta - x_i \beta| \leq |x_i||\hat \beta - \beta|\xrightarrow{p} c \cdot 0
$$ We can depompose $$
\begin{aligned}
\left(\varepsilon_i^2 - \hat{\varepsilon}_i^2 \right)^2 &amp;amp; = \left(\varepsilon_i - \hat{\varepsilon}_i \right)^2 \left(\varepsilon_i + \hat{\varepsilon}_i \right)^2 \leq \newline&lt;br&gt;
&amp;amp; \leq \left(\varepsilon_i + \hat{\varepsilon}_i \right)^2 L^2 = \newline
&amp;amp;= \left(2\varepsilon_i - \varepsilon_i + \hat{\varepsilon}_i \right)^2 L^2\leq  \newline
&amp;amp; \leq  \left( 2(2\varepsilon_i)^2 + 2(\hat{\varepsilon}_i - \varepsilon_i)^2 \right)^2 L^2 \leq \newline
&amp;amp; \leq (8 \varepsilon_i^2 + 2 L^2) L^2
\end{aligned}
$$ Hence $$
\mathbb E \left[ \left(\varepsilon_i^2 - \hat{\varepsilon}_i^2 \right)^2 \right] \leq  L^2 \left( 8 \mathbb E_n [ \varepsilon_i^2] + 2 \mathbb E_n [L^2] \right)  \xrightarrow{p}0
$$ $$\tag*{$\blacksquare$}$$&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;heteroskedastic-and-autocorrelated-error-term&#34;&gt;Heteroskedastic and Autocorrelated Error Term&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Assumption&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;There esists a $\bar{d}$ such that:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\mathbb E[\varepsilon_i x_i \varepsilon&amp;rsquo; _ {i-d} x&amp;rsquo; _ {i-d}] \neq 0 \quad$
for $d \leq \bar{d}$&lt;/li&gt;
&lt;li&gt;$\mathbb E[\varepsilon_i x_i \varepsilon&amp;rsquo; _ {i-d} x&amp;rsquo; _ {i-d}] = 0 \quad$
for $d &amp;gt; \bar{d}$&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;Intuition: observations far enough from each other are not correlated.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;We can express the variance of the score as $$
\begin{aligned}
\Omega_n &amp;amp;= Var(\sqrt{n} \mathbb E_n[x_i \varepsilon_i]) = \newline
&amp;amp;= \mathbb E \left[ \left( \frac{1}{n} \sum _ {i=1}^n x_i \varepsilon_i \right) \left( \frac{1}{n} \sum _ {j=1}^n x_j \varepsilon_j \right) \right] = \newline
&amp;amp;= \frac{1}{n} \sum _ {i=1}^n \sum _ {j=1}^n \mathbb E[x_i \varepsilon_i x_j&amp;rsquo; \varepsilon_j&amp;rsquo;] = \newline
&amp;amp;= \frac{1}{n} \sum _ {i=1}^n \sum _ {j : |i-j|\leq \bar{d}} \mathbb E[x_i \varepsilon_i x_j&amp;rsquo; \varepsilon_j&amp;rsquo;] = \newline
&amp;amp;= \frac{1}{n} \sum _ {d=0}^{\bar{d}} \sum _ {i = d}^{n} \mathbb E[x_i \varepsilon_i x _ {i-d}&amp;rsquo; \varepsilon _ {i-d}&amp;rsquo;]
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;We estimate $\Omega_n$ by $$
\hat{\Omega}_n = \frac{1}{n} \sum _ {d=0}^{\bar{d}} \sum _ {i = d}^{n} x_i \hat{\varepsilon}_i x _ {i-d}&amp;rsquo; \hat{\varepsilon} _ {i-d}&#39;
$$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;If $\bar{d}$ is a fixed integer, then $$
\hat{\Omega}_n - \Omega_n \overset{p}{\to} 0
$$&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;What if $\bar{d}$ does not exist (all $x_i, x_j$ are correlated)? $$
\hat{\Omega}_n = \frac{1}{n} \sum _ {d=0}^{n} \sum _ {i = d}^{n} x_i \hat{\varepsilon}_i x _ {i-d}&amp;rsquo; \hat{\varepsilon} _ {i-d}&amp;rsquo; = n \mathbb E_n[x_i \hat{\varepsilon}_i]^2 = 0
$$ By the orthogonality property of the OLS residual.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;HAC with Uniform Kernel&lt;/strong&gt; $$
\hat{\Omega}_h = \frac{1}{n} \sum _ {i,j} x_i \hat{\varepsilon}_i x_j&amp;rsquo; \hat{\varepsilon}_j&amp;rsquo; \mathbb{I} \lbrace |i-j| \leq h \rbrace
$$ where $h$ is the &lt;strong&gt;bandwidth&lt;/strong&gt; of the kernel. The bandwidth is chosen
such that
$\mathbb E[x_i \varepsilon_i x _ {i-d}&amp;rsquo; \varepsilon _ {i-d}&amp;rsquo; ]$ is small
for $d &amp;gt; h$. How small? Small enough for the estimates to be consistent.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;HAC with General Kernel&lt;/strong&gt; $$
\hat{\Omega}^{HAC} _ {k,h} = \frac{1}{n} \sum _ {i,j} x_i \hat{\varepsilon}_i x_j&amp;rsquo; \hat{\varepsilon}_j&amp;rsquo; k \left( \frac{|i-j|}{n} \right)
$$&lt;/p&gt;
&lt;h3 id=&#34;hac-consistency&#34;&gt;HAC Consistency&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt; If the joint distribution is stationary and $\alpha$-mixing
with $\sum _ {k=1}^\infty k^2 \alpha(k) &amp;lt; \infty$ and&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\mathbb E[ | x _ {ij} \varepsilon_i |^\nu ] &amp;lt; \infty$ $\forall \nu$&lt;/li&gt;
&lt;li&gt;$\hat{\varepsilon}_i = y_i - x_i&amp;rsquo; \hat \beta$ for some
$\hat \beta \overset{p}{\to} \beta_0$&lt;/li&gt;
&lt;li&gt;$k$ smooth, symmetric, $k(0) \to \infty$ as $z \to \infty$,
$\int k^2 &amp;lt; \infty$&lt;/li&gt;
&lt;li&gt;$\frac{h}{n} \to 0$&lt;/li&gt;
&lt;li&gt;$h \to \infty$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Then the HAC estimator is &lt;strong&gt;consistent&lt;/strong&gt;. $$
\hat{\Omega}^{HAC} _ {k,h} - \Omega_n \overset{p}{\to} 0
$$&lt;/p&gt;
&lt;h3 id=&#34;comments&#34;&gt;Comments&lt;/h3&gt;
&lt;p&gt;We want to choose $h$ small relative to $n$ in order to avoid estimation
problems. But we also want to choose $h$ large so that the remainder is
small: $$
\begin{aligned}
\Omega_n &amp;amp;= Var(\sqrt{n} \mathbb E_n[x_i \varepsilon_i]) = \newline
&amp;amp;= \underbrace{\frac{1}{n} \sum _ {i,j : |i-j|\leq h} \mathbb E[x_i \varepsilon_i x_j&amp;rsquo; \varepsilon_j&amp;rsquo;]} _ {\Omega^h_n} + \underbrace{\frac{1}{n} \sum _ {i,j : |i-j|&amp;gt; h} \mathbb E[x_i \varepsilon_i x_j&amp;rsquo; \varepsilon_j&amp;rsquo;]} _ {\text{remainder: } R_n} = \newline
&amp;amp;= \Omega_n^h + R_n
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;In particular, HAC theory requires: $$
\hat{\Omega}^{HAC} \overset{p}{\to} \Omega \quad \text{ if } \quad
\begin{cases}
&amp;amp; \frac{h}{n} \to 0 \newline
&amp;amp; h \to \infty
\end{cases}
$$&lt;/p&gt;
&lt;p&gt;But in practice, long-run estimation implies $\frac{h}{n} \simeq 0$
which is not ``safe” in the sense that it does not imply
$R_n \simeq 0$. On the other hand, if $h \simeq n$, $\hat{\Omega}^{HAC}$
does not converge in probability because it’s too noisy.&lt;/p&gt;
&lt;h3 id=&#34;choice-of-h&#34;&gt;Choice of h&lt;/h3&gt;
&lt;p&gt;How to choose $h$? Look at the score autocorrelation function (ACF).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;../img/Fig_331.jpg&#34; alt=&#34;Autocorrelation Function&#34;&gt;&lt;/p&gt;
&lt;p&gt;It looks like after 10 periods the empirical autocorrelation is quite
small but still not zero.&lt;/p&gt;
&lt;h3 id=&#34;fixed-b-asymptotics&#34;&gt;Fixed b Asymptotics&lt;/h3&gt;
&lt;p&gt;[Neave, 1970]: “&lt;em&gt;When proving results on the asymptotic behavior of
estimates of the spectrum of a stationary time series, it is invariably
assumed that as the sample size $n$ tends to infinity, so does the
truncation point $h$, but at a slower rate, so that $\frac{h}{n}$ tends
to zero. This is a convenient assumption mathematically in that, in
particular, it ensures consistency of the estimates, but it is
unrealistic when such results are used as approximations to the finite
case where the value of $\frac{h}{n}$ cannot be zero.&lt;/em&gt;””&lt;/p&gt;
&lt;h3 id=&#34;fixed-b-theorem&#34;&gt;Fixed b Theorem&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Under regularity conditions, $$
\sqrt{n} \Big( V^{HAC} _ {k,h} \Big)(\hat \beta - \beta_0) \overset{d}{\to} F
$$&lt;/p&gt;
&lt;p&gt;The asymptotic critical values of the $F$ statistic depend on the choice
of the kernel. In order to do hypothesis testing, Kiefer and
Vogelsang(2005) provide critical value functions for the t-statistic for
each kernel-confidence level combination using a cubic equation: $$
cv(b) = a_0 + a_1 b + a_2 b^2 + a_3 b^3
$$&lt;/p&gt;
&lt;h3 id=&#34;example&#34;&gt;Example&lt;/h3&gt;
&lt;p&gt;Example for the Bartlett kernel:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;../img/Fig_332.png&#34; alt=&#34;Fixed-b&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;fixed-g-asymptotics&#34;&gt;Fixed G Asymptotics&lt;/h3&gt;
&lt;p&gt;[Bester, 2013]: “&lt;em&gt;Cluster covariance estimators are routinely used
with data that has a group structure with independence assumed across
groups. Typically, inference is conducted in such settings under the
assumption that there are a large number of these independent groups.&lt;/em&gt;””&lt;/p&gt;
&lt;p&gt;“&lt;em&gt;However, with enough weakly dependent data, we show that groups can be
chosen by the researcher so that group-level averages are approximately
independent. Intuitively, if groups are large enough and well shaped
(e.g. do not have gaps), the majority of points in a group will be far
from other groups, and hence approximately independent of observations
from other groups provided the data are weakly dependent. The key
prerequisite for our methods is the researcher’s ability to construct
groups whose averages are approximately independent. As we show later,
this often requires that the number of groups be kept relatively small,
which is why our main results explicitly consider a fixed (small) number
of groups.&lt;/em&gt;””&lt;/p&gt;
&lt;h3 id=&#34;assumption&#34;&gt;Assumption&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Assumption&lt;/strong&gt; Suppose you have data
$D = (y _ {it} , x _ {it}) _ {i=1, t=1}^{N, T}$ where
$y _ {it} = x _ {it}&amp;rsquo; \beta + \alpha_i + \varepsilon _ {it}$ where $i$
indexes the observational unit and $t$ indexes time (could also be
space).&lt;/p&gt;
&lt;p&gt;Let $$
\begin{aligned}
&amp;amp; \tilde{y} _ {it} = y _ {it} - \frac{1}{T} \sum _ {t=1}^T y _ {it} \newline
&amp;amp; \tilde{x} _ {it} = x _ {it} - \frac{1}{T} \sum _ {t=1}^T x _ {it} \newline
&amp;amp; \tilde{\varepsilon} _ {it} = \varepsilon _ {it} - \frac{1}{T} \sum _ {t=1}^T \varepsilon _ {it}
\end{aligned}
$$ Then $$
\tilde{y} _ {it} = \tilde{x} _ {it}&amp;rsquo; \beta + \tilde{\varepsilon} _ {it}
$$&lt;/p&gt;
&lt;p&gt;The $\tilde{\varepsilon} _ {it}$ are by construction correlated between
each other even if the original $\varepsilon$ was iid. The &lt;strong&gt;cluster
score variance estimator&lt;/strong&gt; is given by: $$
\hat{\Omega}^{CL} = \frac{1}{T-1} \sum _ {i=1}^n  \sum _ {t=1}^T  \sum _ {s=1}^T \tilde{x} _ {it} \hat{\tilde{\varepsilon}} _ {it} \tilde{x} _ {is}     \hat{\tilde{\varepsilon}} _ {is}
$$&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;It’s very similar too the HAC estimator since we have &lt;em&gt;dependent
cross-products&lt;/em&gt; here as well. However, here we do not consider the
$i \times j$ cross-products. We only have time-dependency (state).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;comments-1&#34;&gt;Comments (1)&lt;/h3&gt;
&lt;p&gt;On $T$ and $n$:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;If $T$ is fixed and $n \to \infty$, then the number of
cross-products considered is much smaller than the total number of
cross-products.&lt;/li&gt;
&lt;li&gt;If $T &amp;raquo; n$ issues arise since the number of cross products
considered is close to the total number of cross products. As in HAC
estimation, this is a problem because it implies that the algebraic
estimate of the cluster score variance gets close to zero because of
the orthogonality property of the residuals.&lt;/li&gt;
&lt;li&gt;The panel assumption is that observations across individuals are not
correlated.&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;Strategy: as in HAC, we want to limit the correlation across clusters
(individuals). We hope that observations are &lt;strong&gt;negligibly dependent&lt;/strong&gt;
between cluster sufficiently distant from each other.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;comments-2&#34;&gt;Comments (2)&lt;/h3&gt;
&lt;p&gt;Classical cluster robust estimator: $$
\hat{\Omega}^{CL} = \frac{1}{n} \sum _ {i=1}^n x_i \varepsilon_i x_j&amp;rsquo; \varepsilon_j&amp;rsquo; \mathbb{I}   \lbrace i,j \text{ in the same cluster} \rbrace
$$&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;On clusters:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;If the number of observations near a boundary is small relative to
the sample size, ignoring the dependence should not affect
inference too adversely.&lt;/li&gt;
&lt;li&gt;The higher the dimension of the data, the easier it is to have
observations near boundaries (&lt;em&gt;curse of dimensionality&lt;/em&gt;).&lt;/li&gt;
&lt;li&gt;We would like to have few clusters in order to make less
independence assumptions. However, few clusters means bigger
blocks and hence a larger number of cross-products to estimate. If
the number of cross-products is too large (relative to the sample
size), $\hat{\Omega}^{CL}$ does not converge&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt;: Under regularity conditions: $$
\hat{t} \overset{d}{\to} \sqrt{\frac{G}{G-1}} t _ {G-1}
$$&lt;/p&gt;
&lt;h3 id=&#34;code---dgp&#34;&gt;Code - DGP&lt;/h3&gt;
&lt;p&gt;This code draws 100 observations from the model
$y = 2 x_1 - x_2 + \varepsilon$ where $x_1, x_2 \sim U[0,1]$ and
$\varepsilon \sim N(0,1)$.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# Set seed
Random.seed!(123);

# Set the number of observations
n = 100;

# Set the dimension of X
k = 2;

# Draw a sample of explanatory variables
X = rand(Uniform(0,1), n, k);

# Draw the error term
σ = 1;
ε = rand(Normal(0,1), n, 1) * sqrt(σ);

# Set the parameters
β = [2; -1];

# Calculate the dependent variable
y = X*β + ε;
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;ideal-estimate&#34;&gt;Ideal Estimate&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# OLS estimator
β_hat = (X&#39;*X)\(X&#39;*y);

# Residuals
ε_hat = y - X*β_hat;

# Homoskedastic standard errors
std_h = var(ε_hat) * inv(X&#39;*X);

# Projection matrix
P = X * inv(X&#39;*X) * X&#39;;

# Leverage
h = diag(P);
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;hc-estimates&#34;&gt;HC Estimates&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# HC0 variance and standard errors
Ω_hc0 = X&#39; * (I(n) .* ε_hat.^2) * X;
std_hc0 = sqrt.(diag(inv(X&#39;*X) * Ω_hc0 * inv(X&#39;*X)))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 2-element Array{Float64,1}:
##  0.24691300271914793
##  0.28044707935951835
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# HC1 variance and standard errors
Ω_hc1 = n/(n-k) * X&#39; * (I(n) .* ε_hat.^2) * X;
std_hc1 = sqrt.(diag(inv(X&#39;*X) * Ω_hc1 * inv(X&#39;*X)))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 2-element Array{Float64,1}:
##  0.24941979797977423
##  0.2832943308272532
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# HC2 variance and standard errors
Ω_hc2 = X&#39; * (I(n) .* ε_hat.^2 ./ (1 .- h)) * X;
std_hc2 = sqrt.(diag(inv(X&#39;*X) * Ω_hc2 * inv(X&#39;*X)))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 2-element Array{Float64,1}:
##  0.2506509902982869
##  0.2850878737103963
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# HC3 variance and standard errors
Ω_hc3 = X&#39; * (I(n) .* ε_hat.^2 ./ (1 .- h).^2) * X;
std_hc3 = sqrt.(diag(inv(X&#39;*X) * Ω_hc3 * inv(X&#39;*X)))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 2-element Array{Float64,1}:
##  0.25446321015850176
##  0.2898264779289438
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# Note what happens if you allow for full autocorrelation
omega_full = X&#39;*ε_hat*ε_hat&#39;*X;
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;inference&#34;&gt;Inference&lt;/h2&gt;
&lt;h3 id=&#34;hypothesis-testing&#34;&gt;Hypothesis Testing&lt;/h3&gt;
&lt;p&gt;In order to do inference on $\hat \beta$ we need to know its
distribution. We have two options: (i) assume gaussian error term
(extended GM) or (ii) rely on asymptotic approximations (CLT).&lt;/p&gt;
&lt;p&gt;A statistical hypothesis is a subset of a statistical model,
$\mathcal K \subset \mathcal F$. A hypothesis test is a map
$\mathcal D \rightarrow \lbrace 0,1 \rbrace$, $D \mapsto T$. If
$\mathcal F$ is the statistical model and $\mathcal K$ is the
statistical hypothesis, we use the notation $H_0: \Pr \in \mathcal K$.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Generally, we are interested in understanding whether it is likely
that data $D$ are drawn from $\mathcal K$ or not.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;A hypothesis test, $T$ is our tool for deciding whether the hypothesis
is consistent with the data. $T(D)= 0$ implies fail to reject $H_0$ and
test inconclusive $T(D)=1$ $\implies$ reject $H_0$ and $D$ is
inconsistent with any $\Pr \in \mathcal K$.&lt;/p&gt;
&lt;p&gt;Let $\mathcal K \subseteq \mathcal F$ be a statistical hypothesis and
$T$ a hypothesis test.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Suppose $\Pr \in \mathcal K$. A Type I error (relative to $\Pr$) is
an event $T(D)=1$ under $\Pr$.&lt;/li&gt;
&lt;li&gt;Suppose $\Pr \in \mathcal K^c$. A Type II error (relative to $\Pr$)
is an event $T(D)=0$ under $\Pr$.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The corresponding probability of a type I error is called &lt;strong&gt;size&lt;/strong&gt;. The
corresponding probability of a type II error is called &lt;strong&gt;power&lt;/strong&gt;
(against the alternative $\Pr$).&lt;/p&gt;
&lt;p&gt;In this section, we are interested in testing three hypotheses, under
the assumptions of linearity, strict exogeneity, no multicollinearity,
normality on the error term. They are:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;$H_0: \beta _ {0k} = \bar \beta _ {0k}$ (single coefficient,
$\bar \beta _ {0k} \in \mathbb R$, $k \leq K$)&lt;/li&gt;
&lt;li&gt;$a&amp;rsquo; \beta_0 = c$ (linear combination,
$a \in \mathbb R^K, c \in \mathbb R$)&lt;/li&gt;
&lt;li&gt;$R \beta_0 = r$ (linear restrictions,
$R \in \mathbb R^{p \times K}$, full rank, $r \in \mathbb R^p$)&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;testing-problem&#34;&gt;Testing Problem&lt;/h3&gt;
&lt;p&gt;Consider the testing problem $H_0: \beta _ {0k} = \bar \beta _ {0k}$
where $\bar \beta _ {0k}$ is a pre-specified value under the null. The
t-statistic for this problem is defined by $$
t_k:= \frac{b_k - \bar \beta _ {0k}}{SE(b_k)}, \ \ SE(b_k):= \sqrt{s^2 [(X&amp;rsquo;X)^{-1}] _ {kk}}
$$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt;: In the testing procedure above, the sampling distribution
under the null $H_0$ is given by $$
t_k|X \sim t _ {n-k} \ \ \text{and so} \ \ t_k \sim t _ {n-k}
$$&lt;/p&gt;
&lt;p&gt;$t _ {(n-K)}$ denotes the t-distribution with $(n-k)$ degress of
freedom. The test can be one sided or two sided. The above sampling
distribution can be used to construct a confidence interval.&lt;/p&gt;
&lt;h3 id=&#34;example-1&#34;&gt;Example&lt;/h3&gt;
&lt;p&gt;We want to asses whether or not the ``true” coefficient $\beta_0$
equals a specific value $\hat \beta$. Specifically, we are interested in
testing $H_0$ against $H_1$, where:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Null Hypothesis&lt;/em&gt;: $H_0: \beta_0 = \hat \beta$&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Alternative Hypothesis&lt;/em&gt;: $H_1: \beta_0 \ne \hat \beta$.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Hence, we are interested in a statistic informative about $H_1$, which
is the Wald test statistic $$
|T^*| = \bigg| \frac{\hat \beta - \beta_0}{\sigma(\hat \beta)}\bigg|  \sim N(0,1)
$$&lt;/p&gt;
&lt;p&gt;However, the true variance $\sigma^2(\hat \beta )$ is not known and has
to be estimated. Therefore we plug in the sample variance
$\hat \sigma^2(\hat \beta) = \frac{n}{n-1} \mathbb E_n[\hat e_i^2]$ and
we use $$
|T| = \bigg| \frac{\hat \beta - \beta_0}{\hat \sigma (\hat \beta)}\bigg|  \sim t _ {(n-k)}
$$&lt;/p&gt;
&lt;h3 id=&#34;comments-3&#34;&gt;Comments&lt;/h3&gt;
&lt;p&gt;Hypothesis testing is like proof by contradiction. Imagine the sampling
distribution was generated by $\beta$. If it is highly improbable to
observe $\hat \beta$ given $\beta_0 = \beta$ then we reject the
hypothesis that the sampling distribution was generated by $\beta$.&lt;/p&gt;
&lt;p&gt;Then, given a realized value of the statistic $|T|$, we take the
following decision:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Do not reject $H_0$&lt;/em&gt;: it is consistent with random variation under
true $H_0$—i.e., $|T|$ small as it has an exact student t
distribution with $(n-k)$ degree of freedom in the normal regression
model.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Reject $H_0$ in favor of $H_1$&lt;/em&gt;: $|T| &amp;gt; c$, with $c$ being the
critical values selected to control for false rejections:
$\Pr(|t _ {n-k}| \geq c) = \alpha$. Moreover, you can also reject
$H_0$ if the p-value $p$ is such that: $p &amp;lt; \alpha$.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;comments-2-1&#34;&gt;Comments (2)&lt;/h3&gt;
&lt;p&gt;The probability of false rejection is decreasing in $c$, i.e. the
critical value for a given significant level. $$
\begin{aligned}
\Pr (\text{Reject } H_0 | H_0)  &amp;amp; = \Pr (|T|&amp;gt; c | H_0 ) = \newline
&amp;amp; = \Pr (T &amp;gt; c | H_0 ) +     \Pr (T &amp;lt; -c | H_0 ) = \newline
&amp;amp; = 1 - F(c) + F(-c) = 2(1-F(c))
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Example&lt;/strong&gt;: Consider the testing problem $H_0: a&amp;rsquo;\beta_0=c$ where $a$
is a pre-specified linear combination under study. The t-statistic for
this problem is defined by: $$
t_k:= \frac{a&amp;rsquo;b - c}{SE(a&amp;rsquo;b)}, \ \ SE(a&amp;rsquo;b):= \sqrt{s^2 a&amp;rsquo;(X&amp;rsquo;X)^{-1}a}
$$&lt;/p&gt;
&lt;h3 id=&#34;t-stat&#34;&gt;t Stat&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In the testing procedure above, the sampling distribution under the null
$H_0$ is given by $$
t_a|X \sim t _ {n-K} \quad\text{and so} \quad t_a \sim t _ {n-K}
$$&lt;/p&gt;
&lt;p&gt;Like in the previous test, $t _ {(n-K)}$ denotes the t-distribution with
$(n-K)$ degress of freedom. The test can again be one sided or two
sided. The above sampling distribution can be used to construct a
confidence interval&lt;/p&gt;
&lt;h3 id=&#34;f-stat&#34;&gt;F Stat&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Example&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Consider the testing problem $$
H_0: R \beta_0 = r
$$ where $R \in \mathbb R^{p \times k}$ is a presepecified set of linear
combinations and $r \in \mathbb R^p$ is a restriction vector.&lt;/p&gt;
&lt;p&gt;The F-statistic for this problem is given by $$
F:= \frac{(Rb-r)&amp;rsquo;[R(X&amp;rsquo;X)R&amp;rsquo;]^{-1}(Rb-r)/p }{s^2}
$$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;For the problem, the sampling distribution of the F-statistic under the
null $H_0:$ $$
F|X \sim F _ {p,n-K} \ \ \text{and so} \ \ F \sim F _ {p,n-K}
$$&lt;/p&gt;
&lt;p&gt;The test is intrinsically two-sided. The above sampling distribution can
be used to construct a confidence interval.&lt;/p&gt;
&lt;h3 id=&#34;equivalence&#34;&gt;Equivalence&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Consider the testing problem $H_0: R \beta_0 = r$ where
$R \in \mathbb R^{p\times K}$ is a presepecified set of linear
combinations and $r \in \mathbb R^p$ is a restriction vector.&lt;/p&gt;
&lt;p&gt;Consider the restricted least squares estimator, denoted $\hat \beta_R$:
$\hat \beta_R: = \text{arg} \min _ { \beta: R \beta = r } Q( \beta)$.
Let $SSR_U = Q(b), \ \ SSR_R=Q(\hat \beta_R)$. Then the $F$ statistic is
numerically equivalent to the following expression:
$F = \frac{(SSR_R - SSR_U)/p}{SSR_U/(n-K)}$.&lt;/p&gt;
&lt;h3 id=&#34;confidence-intervals&#34;&gt;Confidence Intervals&lt;/h3&gt;
&lt;p&gt;A &lt;strong&gt;confidence interval at $(1-\alpha)$&lt;/strong&gt; is a random set $C$ such that
$$
\Pr(\beta_0 \in C) \geq 1- \alpha
$$ i.e. the probability that $C$ covers the true value $\beta$ is fixed
at $(1-\alpha)$.&lt;/p&gt;
&lt;p&gt;Since $C$ is not known, it has to be estimated ($\hat{C}$). We construct
confidence intervals such that:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;they are symmetric around $\hat \beta$;&lt;/li&gt;
&lt;li&gt;their length is proportional to
$\sigma(\hat \beta) = \sqrt{Var(\hat \beta)}$.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;A CI is equivalent to the set of parameter values such that the
t-statistic is less than $c$, i.e., $$
\hat{C} = \bigg\lbrace \beta: |T(\beta) | \leq c \bigg\rbrace = \bigg\lbrace \beta: - c\leq \frac{\beta - \hat \beta}{\sigma(\hat \beta)} \leq c \bigg\rbrace
$$&lt;/p&gt;
&lt;p&gt;In practice, to construct a 95% confidence interval for a single
coefficient estimate $\hat \beta_j$, we use the fact that $$
\Pr \left( \frac{| \hat \beta_j - \beta _ {0,j} |}{ \sqrt{\sigma^2 [(X&amp;rsquo;X)^{-1}] _ {jj} }} &amp;gt; 1.96 \right) = 0.05
$$&lt;/p&gt;
&lt;h3 id=&#34;code&#34;&gt;Code&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# t-test for beta=0
t = abs.(β_hat ./ (std_hc1));

# p-value
p_val = 1 .- cdf.(Normal(0,1), t);

# F statistic of joint significance
SSR_u = ε_hat&#39;*ε_hat;
SSR_r = y&#39;*y;
F = (SSR_r - SSR_u)/k / (SSR_u/(n-k));

# 95# confidente intervals
conf_int = [β_hat - 1.96*std_hc1, β_hat + 1.96*std_hc1];
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Web Scraping</title>
      <link>https://matteocourthoud.github.io/course/data-science/07_web_scraping/</link>
      <pubDate>Sun, 01 May 2022 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/course/data-science/07_web_scraping/</guid>
      <description>&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import os
import re
import time
import requests
import pandas as pd

from bs4 import BeautifulSoup
from pprint import pprint
from selenium import webdriver
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There is no silver bullet to getting info from the internet.
The coding requirements in these notes start easy and will gradually become more demanding. We will cover the following web scraping techniques:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Pandas&lt;/li&gt;
&lt;li&gt;APIs&lt;/li&gt;
&lt;li&gt;Scraping static webpages with BeautifulSoup&lt;/li&gt;
&lt;li&gt;Scraping dynamic wepages with Selenium&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;pandas&#34;&gt;Pandas&lt;/h2&gt;
&lt;p&gt;The Pandas library has a very useful webscraping command: &lt;code&gt;read_html&lt;/code&gt;. The &lt;code&gt;read_html&lt;/code&gt; command works for webpages that contain tables that are particularly well behaved. Let&amp;rsquo;s see an example: &lt;a href=&#34;https://en.wikipedia.org/wiki/List_of_countries_by_GDP_%28nominal%29&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://en.wikipedia.org/wiki/List_of_countries_by_GDP_(nominal)&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;At first glance, it seems that there are three tables in this Wikipedia page:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;data from the IMF&lt;/li&gt;
&lt;li&gt;data from the World Bank&lt;/li&gt;
&lt;li&gt;data from the UN&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Let&amp;rsquo;s see which tables pandas recognizes.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Scrape all tables from Wikipedia page
url = &#39;https://en.wikipedia.org/wiki/List_of_countries_by_GDP_(nominal)&#39;
df_list = pd.read_html(url)

# Check number of tables on the page
print(len(df_list))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;7
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Apparently Pandas has found 10 tables in this webpage. Let&amp;rsquo;s see what is their content.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Check headers of each table
for df in df_list: print(df.shape)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;(1, 1)
(1, 3)
(216, 9)
(9, 2)
(7, 2)
(13, 2)
(2, 2)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It seems that pandas has found many more tables that we could see. The ones that are of interest to us are probably the 3rd, 4th and 5th. But that are the others? Let&amp;rsquo;s look at the them.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Check first
df_list[0].head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;0&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;Largest economies by nominal GDP in 2021[1]&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Check second
df_list[1].head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;th&gt;2&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;.mw-parser-output .legend{page-break-inside:av...&lt;/td&gt;
      &lt;td&gt;$750 billion – $1 trillion $500–50 billion $25...&lt;/td&gt;
      &lt;td&gt;$50–100 billion $25–50 billion $5–25 billion &amp;lt;...&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;Apparently, the first two are simply picture captions.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Check third
df_list[2].head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead tr th {
    text-align: left;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;Country/Territory&lt;/th&gt;
      &lt;th&gt;Subregion&lt;/th&gt;
      &lt;th&gt;Region&lt;/th&gt;
      &lt;th colspan=&#34;2&#34; halign=&#34;left&#34;&gt;IMF[1]&lt;/th&gt;
      &lt;th colspan=&#34;2&#34; halign=&#34;left&#34;&gt;United Nations[12]&lt;/th&gt;
      &lt;th colspan=&#34;2&#34; halign=&#34;left&#34;&gt;World Bank[13][14]&lt;/th&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;Country/Territory&lt;/th&gt;
      &lt;th&gt;Subregion&lt;/th&gt;
      &lt;th&gt;Region&lt;/th&gt;
      &lt;th&gt;Estimate&lt;/th&gt;
      &lt;th&gt;Year&lt;/th&gt;
      &lt;th&gt;Estimate&lt;/th&gt;
      &lt;th&gt;Year&lt;/th&gt;
      &lt;th&gt;Estimate&lt;/th&gt;
      &lt;th&gt;Year&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;United States&lt;/td&gt;
      &lt;td&gt;Northern America&lt;/td&gt;
      &lt;td&gt;Americas&lt;/td&gt;
      &lt;td&gt;22939580.0&lt;/td&gt;
      &lt;td&gt;2021&lt;/td&gt;
      &lt;td&gt;20893746.0&lt;/td&gt;
      &lt;td&gt;2020&lt;/td&gt;
      &lt;td&gt;20936600.0&lt;/td&gt;
      &lt;td&gt;2020&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;China&lt;/td&gt;
      &lt;td&gt;Eastern Asia&lt;/td&gt;
      &lt;td&gt;Asia&lt;/td&gt;
      &lt;td&gt;16862979.0&lt;/td&gt;
      &lt;td&gt;[n 2]2021&lt;/td&gt;
      &lt;td&gt;14722801.0&lt;/td&gt;
      &lt;td&gt;[n 3]2020&lt;/td&gt;
      &lt;td&gt;14722731.0&lt;/td&gt;
      &lt;td&gt;2020&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;Japan&lt;/td&gt;
      &lt;td&gt;Eastern Asia&lt;/td&gt;
      &lt;td&gt;Asia&lt;/td&gt;
      &lt;td&gt;5103110.0&lt;/td&gt;
      &lt;td&gt;2021&lt;/td&gt;
      &lt;td&gt;5057759.0&lt;/td&gt;
      &lt;td&gt;2020&lt;/td&gt;
      &lt;td&gt;4975415.0&lt;/td&gt;
      &lt;td&gt;2020&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;Germany&lt;/td&gt;
      &lt;td&gt;Western Europe&lt;/td&gt;
      &lt;td&gt;Europe&lt;/td&gt;
      &lt;td&gt;4230172.0&lt;/td&gt;
      &lt;td&gt;2021&lt;/td&gt;
      &lt;td&gt;3846414.0&lt;/td&gt;
      &lt;td&gt;2020&lt;/td&gt;
      &lt;td&gt;3806060.0&lt;/td&gt;
      &lt;td&gt;2020&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;United Kingdom&lt;/td&gt;
      &lt;td&gt;Western Europe&lt;/td&gt;
      &lt;td&gt;Europe&lt;/td&gt;
      &lt;td&gt;3108416.0&lt;/td&gt;
      &lt;td&gt;2021&lt;/td&gt;
      &lt;td&gt;2764198.0&lt;/td&gt;
      &lt;td&gt;2020&lt;/td&gt;
      &lt;td&gt;2707744.0&lt;/td&gt;
      &lt;td&gt;2020&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;This is clearly what we were looking for. A part from the footnotes, the table is already clean and organized.&lt;/p&gt;
&lt;p&gt;If we knew the name of the table, we could directly retrieve it. However, we will see more about it in the next lecture.&lt;/p&gt;
&lt;h2 id=&#34;specific-libraries&#34;&gt;Specific Libraries&lt;/h2&gt;
&lt;p&gt;Sometimes, there are libraries that are already written down to do the scraping for you. Each one is tailored for a specific website and they are usually userwritten and prone to bugs and errors. However, they are often efficient and save you the time to worry about getting around some website-specific issues.&lt;/p&gt;
&lt;p&gt;One example is the &lt;code&gt;pytrends&lt;/code&gt; library for scraping Google Trends. Let&amp;rsquo;s first install it&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;pip3 install pytrends
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s see how it works. Imagine we want to do the following search:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;words &amp;ldquo;python&amp;rdquo;, &amp;ldquo;matlab&amp;rdquo;, &amp;ldquo;stata&amp;rdquo;&lt;/li&gt;
&lt;li&gt;the the second half of in 2019&lt;/li&gt;
&lt;li&gt;daily&lt;/li&gt;
&lt;li&gt;in the US&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We can get more details on how pytrends works &lt;a href=&#34;https://github.com/GeneralMills/pytrends#historical-hourly-interest&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;. The important thing to know is that if you query a time period of more than 200 days, Google will give you weekly results, instead of daily.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Pytrends search
from pytrends.request import TrendReq

# Set parameters
words = [&#39;python&#39;, &#39;matlab&#39;, &#39;stata&#39;]
timeframe = &#39;2019-07-01 2019-12-31&#39;
country = &#39;US&#39;

# Get data
pytrend = TrendReq()
pytrend.build_payload(kw_list=words, timeframe=timeframe, geo=country)
df_trends = pytrend.interest_over_time()

# Plot
trends_plot = df_trends.plot.line()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/07_web_scraping_16_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Apparently people don&amp;rsquo;t code during the weekend&amp;hellip;.&lt;/p&gt;
&lt;h2 id=&#34;apis&#34;&gt;APIs&lt;/h2&gt;
&lt;p&gt;From Wikipedia&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;An application programming interface (API) is an interface or communication protocol between different parts of a computer program intended to simplify the implementation and maintenance of software.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;In practice, it means that the are some webpages that are structured not to be user-readable but to be computer-readable. Let&amp;rsquo;s see one example.&lt;/p&gt;
&lt;p&gt;Google provides many APIs for its services. However, they now all need identification, which means that you have to log in into your Google account and request an API key from there. This allows Google to monitor your behavior since the number of API requests is limited and beyond a certain treshold, one need to pay (a lot).&lt;/p&gt;
&lt;p&gt;There are however some free APIs. One&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s have a look at one of these: zippopotam. Zippopotam lets you retrieve location information from a zip code in the US. Other countries are supported as well.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Let&#39;s search the department locatiton
import requests

zipcode = &#39;90210&#39;
url = &#39;https://api.zippopotam.us/us/&#39;+zipcode

response = requests.get(url)
data = response.json()
data
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;{&#39;post code&#39;: &#39;90210&#39;,
 &#39;country&#39;: &#39;United States&#39;,
 &#39;country abbreviation&#39;: &#39;US&#39;,
 &#39;places&#39;: [{&#39;place name&#39;: &#39;Beverly Hills&#39;,
   &#39;longitude&#39;: &#39;-118.4065&#39;,
   &#39;state&#39;: &#39;California&#39;,
   &#39;state abbreviation&#39;: &#39;CA&#39;,
   &#39;latitude&#39;: &#39;34.0901&#39;}]}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Data is in JSON (JavaScript Object Notation) format which is basically a nested dictionary-list format. Indeed, we see that in our case, data is a dictionary where the last elements is a list with one element - another dictionary.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Check type of value
for d in data.values():
    print(type(d))
    
# Check list length
print(len(data[&#39;places&#39;]))

# Check type of content of list
print(type(data[&#39;places&#39;][0]))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;class &#39;str&#39;&amp;gt;
&amp;lt;class &#39;str&#39;&amp;gt;
&amp;lt;class &#39;str&#39;&amp;gt;
&amp;lt;class &#39;list&#39;&amp;gt;
1
&amp;lt;class &#39;dict&#39;&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The part that could be interesting to us is contained in the &lt;code&gt;places&lt;/code&gt; category. We can easily extract it and transform it into a dataframe.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Add zipcode to data
data[&#39;places&#39;][0][&#39;zipcode&#39;] = zipcode

# Export data
df = pd.DataFrame(data[&#39;places&#39;])
df
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;place name&lt;/th&gt;
      &lt;th&gt;longitude&lt;/th&gt;
      &lt;th&gt;state&lt;/th&gt;
      &lt;th&gt;state abbreviation&lt;/th&gt;
      &lt;th&gt;latitude&lt;/th&gt;
      &lt;th&gt;zipcode&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;Beverly Hills&lt;/td&gt;
      &lt;td&gt;-118.4065&lt;/td&gt;
      &lt;td&gt;California&lt;/td&gt;
      &lt;td&gt;CA&lt;/td&gt;
      &lt;td&gt;34.0901&lt;/td&gt;
      &lt;td&gt;90210&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;h2 id=&#34;static-webscraping&#34;&gt;Static Webscraping&lt;/h2&gt;
&lt;p&gt;We have so far used pre-made tools in order to do web-scraping. When the website contains the data in a nice table or an API is available, we do not need to worry much and we can directly retrieve the data. However, most of web scraping is much more complicated. Data is often the product of webscraping and is not readily available. Moreover, sometimes webscraping knowledge can supplement the need to pay for an API.&lt;/p&gt;
&lt;h3 id=&#34;http&#34;&gt;HTTP&lt;/h3&gt;
&lt;p&gt;What happens when you open a page on the internet? In short, your web browser is sending a request to the website that, in turn, sends back a reply/response. The exchange of messages is complex but its core involves a HyperText Transfer Protocol (HTTP) request message to a web server, followed by a HTTP response (or reply). All static webscraping is build on HTTP so let&amp;rsquo;s have a closer look.&lt;/p&gt;
&lt;p&gt;An HTTP message essentially has 4 components:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;A request line&lt;/li&gt;
&lt;li&gt;A number of request headers&lt;/li&gt;
&lt;li&gt;An empty line&lt;/li&gt;
&lt;li&gt;An optional message&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;em&gt;Example&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;A request message could be&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;GET /hello.htm HTTP/1.1
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The response would be&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;HTTP/1.1 200 OK
Date: Sun, 10 Oct 2010 23:26:07 GMT
Server: Apache/2.2.8 (Ubuntu) mod_ssl/2.2.8 OpenSSL/0.9.8g
Last-Modified: Sun, 26 Sep 2010 22:04:35 GMT
ETag: &amp;quot;45b6-834-49130cc1182c0&amp;quot;
Accept-Ranges: bytes
Content-Length: 12
Connection: close
Content-Type: text/html

&amp;lt;html&amp;gt;
   &amp;lt;body&amp;gt;
   
      &amp;lt;h1&amp;gt;Hello, World!&amp;lt;/h1&amp;gt;
   
   &amp;lt;/body&amp;gt;
&amp;lt;/html&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So, in this case the parts are:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;The &lt;strong&gt;request line&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code&gt;HTTP/1.1 200 OK
&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;The &lt;strong&gt;request headers&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code&gt;Date: Sun, 10 Oct 2010 23:26:07 GMT
Server: Apache/2.2.8 (Ubuntu) mod_ssl/2.2.8 OpenSSL/0.9.8g
Last-Modified: Sun, 26 Sep 2010 22:04:35 GMT
ETag: &amp;quot;45b6-834-49130cc1182c0&amp;quot;
Accept-Ranges: bytes
Content-Length: 12
Connection: close
Content-Type: text/html
&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;3&#34;&gt;
&lt;li&gt;The &lt;strong&gt;empty line&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;The &lt;strong&gt;optional message&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;html&amp;gt;
   &amp;lt;body&amp;gt;
   
      &amp;lt;h1&amp;gt;Hello, World!&amp;lt;/h1&amp;gt;
   
   &amp;lt;/body&amp;gt;
&amp;lt;/html&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We are interested in the optional message, which is essentially the content of the page we want to scrape. The content is usually written in HTML which is not a proper programming language but rather a &lt;em&gt;typesetting language&lt;/em&gt; since it is the language underlying web pages and is usually generated from other programming languages.&lt;/p&gt;
&lt;h3 id=&#34;requests&#34;&gt;Requests&lt;/h3&gt;
&lt;p&gt;There are many different packages in python to send requests to a web page and read its response. The most user-friendly is the &lt;code&gt;requests&lt;/code&gt; package. You can find plenty of useful information on the &lt;code&gt;requests&lt;/code&gt; library on its website: &lt;a href=&#34;https://requests.readthedocs.io/en/master/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://requests.readthedocs.io/en/master/&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;We are now going to have a look at a simple example: &lt;a href=&#34;http://pythonscraping.com/pages/page1.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://pythonscraping.com/pages/page1.html&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Request a simple web page
url1 = &#39;http://pythonscraping.com/pages/page1.html&#39;
response = requests.get(url1)
print(response)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;Response [200]&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We are (hopefully) getting a &lt;code&gt;&amp;lt;Response [200]&amp;gt;&lt;/code&gt; message. In short, what we got is the status code of the request we sent to the website. The status code is a 3-digit code and essentially there are two broad categories of status codes:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;2XX: success&lt;/li&gt;
&lt;li&gt;4XX, 5XX: failure&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;It can be useful to know this codes as they are a fast way to check whether your request has failed or not. When webscraping the most common reasons you get an error are&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;The link does not exist: wither the link is old/expired or you misspelled it and hence there is no page to request&lt;/li&gt;
&lt;li&gt;You have been &amp;ldquo;caught&amp;rdquo;. This is pretty common when webscraping and happens every time you are too aggressive with your scraping. How much &amp;ldquo;aggressive&amp;rdquo; is &amp;ldquo;too agrressive&amp;rdquo; depends on the website. Usually big tech websites are particularly hard to scrape and anything that is &amp;ldquo;faster than human&amp;rdquo; gets blocked. Sometimes also slow but persistent requests get blocked as well.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;We have now analyzed the response status but, what is actually the response content? Let&amp;rsquo;s inspect the response object more in detail.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Print response attributes
dir(response)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[&#39;__attrs__&#39;,
 &#39;__bool__&#39;,
 &#39;__class__&#39;,
 &#39;__delattr__&#39;,
 &#39;__dict__&#39;,
 &#39;__dir__&#39;,
 &#39;__doc__&#39;,
 &#39;__enter__&#39;,
 &#39;__eq__&#39;,
 &#39;__exit__&#39;,
 &#39;__format__&#39;,
 &#39;__ge__&#39;,
 &#39;__getattribute__&#39;,
 &#39;__getstate__&#39;,
 &#39;__gt__&#39;,
 &#39;__hash__&#39;,
 &#39;__init__&#39;,
 &#39;__init_subclass__&#39;,
 &#39;__iter__&#39;,
 &#39;__le__&#39;,
 &#39;__lt__&#39;,
 &#39;__module__&#39;,
 &#39;__ne__&#39;,
 &#39;__new__&#39;,
 &#39;__nonzero__&#39;,
 &#39;__reduce__&#39;,
 &#39;__reduce_ex__&#39;,
 &#39;__repr__&#39;,
 &#39;__setattr__&#39;,
 &#39;__setstate__&#39;,
 &#39;__sizeof__&#39;,
 &#39;__str__&#39;,
 &#39;__subclasshook__&#39;,
 &#39;__weakref__&#39;,
 &#39;_content&#39;,
 &#39;_content_consumed&#39;,
 &#39;_next&#39;,
 &#39;apparent_encoding&#39;,
 &#39;close&#39;,
 &#39;connection&#39;,
 &#39;content&#39;,
 &#39;cookies&#39;,
 &#39;elapsed&#39;,
 &#39;encoding&#39;,
 &#39;headers&#39;,
 &#39;history&#39;,
 &#39;is_permanent_redirect&#39;,
 &#39;is_redirect&#39;,
 &#39;iter_content&#39;,
 &#39;iter_lines&#39;,
 &#39;json&#39;,
 &#39;links&#39;,
 &#39;next&#39;,
 &#39;ok&#39;,
 &#39;raise_for_status&#39;,
 &#39;raw&#39;,
 &#39;reason&#39;,
 &#39;request&#39;,
 &#39;status_code&#39;,
 &#39;text&#39;,
 &#39;url&#39;]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We are actually interested in the text of the response.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Print response content
response.text
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&#39;&amp;lt;html&amp;gt;\n&amp;lt;head&amp;gt;\n&amp;lt;title&amp;gt;A Useful Page&amp;lt;/title&amp;gt;\n&amp;lt;/head&amp;gt;\n&amp;lt;body&amp;gt;\n&amp;lt;h1&amp;gt;An Interesting Title&amp;lt;/h1&amp;gt;\n&amp;lt;div&amp;gt;\nLorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\n&amp;lt;/div&amp;gt;\n&amp;lt;/body&amp;gt;\n&amp;lt;/html&amp;gt;\n&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is the whole content of the table. There is a large chunk of text and other parts which look more obscure. In order to understand the structure of the page, we need to have a closer look at the language in which the webpage is written: HTML. We will do it in the next section.&lt;/p&gt;
&lt;p&gt;However, let&amp;rsquo;s first analyze the other relevant components of the response. We have already had a look at the status. Let&amp;rsquo;s inspect the headers.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Print response headers
response.headers
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;{&#39;Server&#39;: &#39;nginx&#39;, &#39;Date&#39;: &#39;Thu, 10 Feb 2022 11:11:41 GMT&#39;, &#39;Content-Type&#39;: &#39;text/html&#39;, &#39;Content-Length&#39;: &#39;361&#39;, &#39;Connection&#39;: &#39;keep-alive&#39;, &#39;X-Accel-Version&#39;: &#39;0.01&#39;, &#39;Last-Modified&#39;: &#39;Sat, 09 Jun 2018 19:15:58 GMT&#39;, &#39;ETag&#39;: &#39;&amp;quot;234-56e3a58a63780-gzip&amp;quot;&#39;, &#39;Accept-Ranges&#39;: &#39;bytes&#39;, &#39;Vary&#39;: &#39;Accept-Encoding&#39;, &#39;Content-Encoding&#39;: &#39;gzip&#39;, &#39;X-Powered-By&#39;: &#39;PleskLin&#39;}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;From the headers we can see&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the present date&lt;/li&gt;
&lt;li&gt;the name of the server hosting the page&lt;/li&gt;
&lt;li&gt;the last time the page was modified&lt;/li&gt;
&lt;li&gt;other stuff&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Let&amp;rsquo;s now look at the headers of our request.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Request headers
def check_headers(r):
    test_headers = dict(zip(r.request.headers.keys(), r.request.headers.values()))
    pprint(test_headers)
    
check_headers(response)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;{&#39;Accept&#39;: &#39;*/*&#39;,
 &#39;Accept-Encoding&#39;: &#39;gzip, deflate, br&#39;,
 &#39;Connection&#39;: &#39;keep-alive&#39;,
 &#39;User-Agent&#39;: &#39;python-requests/2.27.1&#39;}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The headers of our request are pretty minimal. In order to see what normal headers look like, go to &lt;a href=&#34;https://www.whatismybrowser.com/developers/what-http-headers-is-my-browser-sending&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.whatismybrowser.com/developers/what-http-headers-is-my-browser-sending&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Normal headers look something like:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;{&#39;Accept&#39;: &#39;text/html,application/xhtml+xml,application/xml;q = 0.9, image / &#39;
           &#39;webp, * / *;q = 0.8&#39;,
 &#39;Accept-Encoding&#39;: &#39;gzip, deflate, br&#39;,
 &#39;Accept-Language&#39;: &#39;en-US,en;q=0.9,it-IT;q=0.8,it;q=0.7,de-DE;q=0.6,de;q=0.5&#39;,
 &#39;Connection&#39;: &#39;keep-alive&#39;,
 &#39;Host&#39;: &#39;www.whatismybrowser.com&#39;,
 &#39;Referer&#39;: &#39;http://localhost:8888/&#39;,
 &#39;User-Agent&#39;: &#39;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_6) &#39;
               &#39;AppleWebKit/537.36 (KHTML, like Gecko) Chrome/79.0.3945.88 &#39;
               &#39;Safari/537.36&#39;}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The most important difference is that the &lt;code&gt;requests&lt;/code&gt; model default &lt;em&gt;User-Agent&lt;/em&gt; is &lt;code&gt;python-requests/2.22.0&lt;/code&gt; which means that we are walking around the web with a big &lt;strong&gt;WARNING: web scrapers&lt;/strong&gt; sign. This is the simplest way to get caught and blocked by a website. Luckily, we can easily change our headers in order to be more subtle.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Change headers
headers = {&amp;quot;User-Agent&amp;quot;: &amp;quot;Mozilla/5.0&amp;quot;,
               &amp;quot;Accept&amp;quot;: &amp;quot;webp, * / *;q = 0.8&amp;quot;,
               &amp;quot;Accept-Language&amp;quot;: &amp;quot;en-US,en;q=0.9&amp;quot;,
               &amp;quot;Accept-Encoding&amp;quot;: &amp;quot;br, gzip, deflate&amp;quot;,
               &amp;quot;Referer&amp;quot;: &amp;quot;https://www.google.ch/&amp;quot;}

# Test if change worked
response = requests.get(url1, headers=headers)
check_headers(response)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;{&#39;Accept&#39;: &#39;webp, * / *;q = 0.8&#39;,
 &#39;Accept-Encoding&#39;: &#39;br, gzip, deflate&#39;,
 &#39;Accept-Language&#39;: &#39;en-US,en;q=0.9&#39;,
 &#39;Connection&#39;: &#39;keep-alive&#39;,
 &#39;Referer&#39;: &#39;https://www.google.ch/&#39;,
 &#39;User-Agent&#39;: &#39;Mozilla/5.0&#39;}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Nice! Now we are a little more stealthy.&lt;/p&gt;
&lt;p&gt;You might now be asking yourself what are the ethical limits of webscraping. Information on the internet is public but scraping a website imposes a workload on the website&amp;rsquo;s server. If the website is not protected against aggressive scrapers (most websites are), your activity could significantly slower the website or even crash it.&lt;/p&gt;
&lt;p&gt;Usually websites include their policies for scraping in a text file named &lt;code&gt;robots.txt&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s have a look at the &lt;code&gt;robots.txt&lt;/code&gt; file of &lt;a href=&#34;http://pythonscraping.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://pythonscraping.com/&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Read robots.txt
response = requests.get(&#39;http://pythonscraping.com/robots.txt&#39;)
print(response.text)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;#
# robots.txt
#
# This file is to prevent the crawling and indexing of certain parts
# of your site by web crawlers and spiders run by sites like Yahoo!
# and Google. By telling these &amp;quot;robots&amp;quot; where not to go on your site,
# you save bandwidth and server resources.
#
# This file will be ignored unless it is at the root of your host:
# Used:    http://example.com/robots.txt
# Ignored: http://example.com/site/robots.txt
#
# For more information about the robots.txt standard, see:
# http://www.robotstxt.org/robotstxt.html
#
# For syntax checking, see:
# http://www.frobee.com/robots-txt-check

User-agent: *
Crawl-delay: 10
# Directories
Disallow: /includes/
Disallow: /misc/
Disallow: /modules/
Disallow: /profiles/
Disallow: /scripts/
Disallow: /themes/
# Files
Disallow: /CHANGELOG.txt
Disallow: /cron.php
Disallow: /INSTALL.mysql.txt
Disallow: /INSTALL.pgsql.txt
Disallow: /INSTALL.sqlite.txt
Disallow: /install.php
Disallow: /INSTALL.txt
Disallow: /LICENSE.txt
Disallow: /MAINTAINERS.txt
Disallow: /update.php
Disallow: /UPGRADE.txt
Disallow: /xmlrpc.php
# Paths (clean URLs)
Disallow: /admin/
Disallow: /comment/reply/
Disallow: /filter/tips/
Disallow: /node/add/
Disallow: /search/
Disallow: /user/register/
Disallow: /user/password/
Disallow: /user/login/
Disallow: /user/logout/
# Paths (no clean URLs)
Disallow: /?q=admin/
Disallow: /?q=comment/reply/
Disallow: /?q=filter/tips/
Disallow: /?q=node/add/
Disallow: /?q=search/
Disallow: /?q=user/password/
Disallow: /?q=user/register/
Disallow: /?q=user/login/
Disallow: /?q=user/logout/
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As we can see, this &lt;code&gt;robots.txt&lt;/code&gt; file mostly deals with crawlers, i.e. scripts that are designed to recover the structure of a website by exploring it. Crawlers are mostly used by browsers that want to index websites.&lt;/p&gt;
&lt;p&gt;Now we have explored most of the issues around HTTP requests. We can now proceed to what we are interested in: the content of the web page. In order to do that, we need to know the language in which wabpages are written: HTML.&lt;/p&gt;
&lt;h3 id=&#34;html&#34;&gt;HTML&lt;/h3&gt;
&lt;p&gt;Hypertext Markup Language (HTML) is the standard markup language for documents designed to be displayed in a web browser. Web browsers receive HTML documents from a web server or from local storage and render the documents into multimedia web pages. HTML describes the structure of a web page semantically and originally included cues for the appearance of the document.&lt;/p&gt;
&lt;p&gt;HTML elements are delineated by tags, written using angle brackets.&lt;/p&gt;
&lt;h4 id=&#34;tags&#34;&gt;Tags&lt;/h4&gt;
&lt;p&gt;Tags are the cues that HTML uses to surround content and provide information about its nature. There is a very large amount of tags but some of the most common are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;&amp;lt;head&amp;gt;&lt;/code&gt; and &lt;code&gt;&amp;lt;body&amp;gt;&lt;/code&gt; for head and body of the page&lt;/li&gt;
&lt;li&gt;&lt;code&gt;&amp;lt;p&amp;gt;&lt;/code&gt; for paragraphs&lt;/li&gt;
&lt;li&gt;&lt;code&gt;&amp;lt;br&amp;gt;&lt;/code&gt; for line breaks&lt;/li&gt;
&lt;li&gt;&lt;code&gt;&amp;lt;table&amp;gt;&lt;/code&gt; for tables. These are the ones that &lt;code&gt;pandas&lt;/code&gt; reads. However, we have seen that not all elements that look like tables are actually &lt;code&gt;&amp;lt;table&amp;gt;&lt;/code&gt; and viceversa. Table elements are tagged as &lt;code&gt;&amp;lt;th&amp;gt;&lt;/code&gt; (table header), &lt;code&gt;&amp;lt;tr&amp;gt;&lt;/code&gt; (table row) and &lt;code&gt;&amp;lt;td&amp;gt;&lt;/code&gt; (table data: a cell)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;&amp;lt;img&amp;gt;&lt;/code&gt; for images&lt;/li&gt;
&lt;li&gt;&lt;code&gt;&amp;lt;h1&amp;gt;&lt;/code&gt; to &lt;code&gt;&amp;lt;h6&amp;gt;&lt;/code&gt; for headers (titles and subtitles)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;&amp;lt;div&amp;gt;&lt;/code&gt; dor divisions, i.e. for grouping elements&lt;/li&gt;
&lt;li&gt;&lt;code&gt;&amp;lt;a&amp;gt;&lt;/code&gt; for hyperlinks&lt;/li&gt;
&lt;li&gt;&lt;code&gt;&amp;lt;ul&amp;gt;&lt;/code&gt; and &lt;code&gt;&amp;lt;ol&amp;gt;&lt;/code&gt; for unordered and ordered lists where list elements are tagged as &lt;code&gt;&amp;lt;li&amp;gt;&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Let&amp;rsquo;s have a look at the previous page&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Inspect HTML
response.text
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&#39;#\n# robots.txt\n#\n# This file is to prevent the crawling and indexing of certain parts\n# of your site by web crawlers and spiders run by sites like Yahoo!\n# and Google. By telling these &amp;quot;robots&amp;quot; where not to go on your site,\n# you save bandwidth and server resources.\n#\n# This file will be ignored unless it is at the root of your host:\n# Used:    http://example.com/robots.txt\n# Ignored: http://example.com/site/robots.txt\n#\n# For more information about the robots.txt standard, see:\n# http://www.robotstxt.org/robotstxt.html\n#\n# For syntax checking, see:\n# http://www.frobee.com/robots-txt-check\n\nUser-agent: *\nCrawl-delay: 10\n# Directories\nDisallow: /includes/\nDisallow: /misc/\nDisallow: /modules/\nDisallow: /profiles/\nDisallow: /scripts/\nDisallow: /themes/\n# Files\nDisallow: /CHANGELOG.txt\nDisallow: /cron.php\nDisallow: /INSTALL.mysql.txt\nDisallow: /INSTALL.pgsql.txt\nDisallow: /INSTALL.sqlite.txt\nDisallow: /install.php\nDisallow: /INSTALL.txt\nDisallow: /LICENSE.txt\nDisallow: /MAINTAINERS.txt\nDisallow: /update.php\nDisallow: /UPGRADE.txt\nDisallow: /xmlrpc.php\n# Paths (clean URLs)\nDisallow: /admin/\nDisallow: /comment/reply/\nDisallow: /filter/tips/\nDisallow: /node/add/\nDisallow: /search/\nDisallow: /user/register/\nDisallow: /user/password/\nDisallow: /user/login/\nDisallow: /user/logout/\n# Paths (no clean URLs)\nDisallow: /?q=admin/\nDisallow: /?q=comment/reply/\nDisallow: /?q=filter/tips/\nDisallow: /?q=node/add/\nDisallow: /?q=search/\nDisallow: /?q=user/password/\nDisallow: /?q=user/register/\nDisallow: /?q=user/login/\nDisallow: /?q=user/logout/\n&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The response looks a little bit messy and not really readable.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;BeautifulSoup&lt;/code&gt; is a python library that renders http responses in a user friendly format and helps recovering elements from tags and attributes.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;pip3 install bs4
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s have a look.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Make response readable
soup = BeautifulSoup(response.text, &#39;lxml&#39;)
print(soup)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;html&amp;gt;&amp;lt;body&amp;gt;&amp;lt;p&amp;gt;#
# robots.txt
#
# This file is to prevent the crawling and indexing of certain parts
# of your site by web crawlers and spiders run by sites like Yahoo!
# and Google. By telling these &amp;quot;robots&amp;quot; where not to go on your site,
# you save bandwidth and server resources.
#
# This file will be ignored unless it is at the root of your host:
# Used:    http://example.com/robots.txt
# Ignored: http://example.com/site/robots.txt
#
# For more information about the robots.txt standard, see:
# http://www.robotstxt.org/robotstxt.html
#
# For syntax checking, see:
# http://www.frobee.com/robots-txt-check

User-agent: *
Crawl-delay: 10
# Directories
Disallow: /includes/
Disallow: /misc/
Disallow: /modules/
Disallow: /profiles/
Disallow: /scripts/
Disallow: /themes/
# Files
Disallow: /CHANGELOG.txt
Disallow: /cron.php
Disallow: /INSTALL.mysql.txt
Disallow: /INSTALL.pgsql.txt
Disallow: /INSTALL.sqlite.txt
Disallow: /install.php
Disallow: /INSTALL.txt
Disallow: /LICENSE.txt
Disallow: /MAINTAINERS.txt
Disallow: /update.php
Disallow: /UPGRADE.txt
Disallow: /xmlrpc.php
# Paths (clean URLs)
Disallow: /admin/
Disallow: /comment/reply/
Disallow: /filter/tips/
Disallow: /node/add/
Disallow: /search/
Disallow: /user/register/
Disallow: /user/password/
Disallow: /user/login/
Disallow: /user/logout/
# Paths (no clean URLs)
Disallow: /?q=admin/
Disallow: /?q=comment/reply/
Disallow: /?q=filter/tips/
Disallow: /?q=node/add/
Disallow: /?q=search/
Disallow: /?q=user/password/
Disallow: /?q=user/register/
Disallow: /?q=user/login/
Disallow: /?q=user/logout/
&amp;lt;/p&amp;gt;&amp;lt;/body&amp;gt;&amp;lt;/html&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;First of all, what is the &lt;code&gt;html5lib&lt;/code&gt; option? It&amp;rsquo;s the parser. In short, there are often small mistakes/variations in HTML and each parser interprets it differently. In principles, the latest HTML standard is HTML5, therefore the &lt;code&gt;html5lib&lt;/code&gt; parser should be the most &amp;ldquo;correct&amp;rdquo; parser. It might happen that the same code does not work for another person if you use a different parser.&lt;/p&gt;
&lt;p&gt;This is much better but it can be improved.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Prettify response
print(soup.prettify())
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;html&amp;gt;
 &amp;lt;body&amp;gt;
  &amp;lt;p&amp;gt;
   #
# robots.txt
#
# This file is to prevent the crawling and indexing of certain parts
# of your site by web crawlers and spiders run by sites like Yahoo!
# and Google. By telling these &amp;quot;robots&amp;quot; where not to go on your site,
# you save bandwidth and server resources.
#
# This file will be ignored unless it is at the root of your host:
# Used:    http://example.com/robots.txt
# Ignored: http://example.com/site/robots.txt
#
# For more information about the robots.txt standard, see:
# http://www.robotstxt.org/robotstxt.html
#
# For syntax checking, see:
# http://www.frobee.com/robots-txt-check

User-agent: *
Crawl-delay: 10
# Directories
Disallow: /includes/
Disallow: /misc/
Disallow: /modules/
Disallow: /profiles/
Disallow: /scripts/
Disallow: /themes/
# Files
Disallow: /CHANGELOG.txt
Disallow: /cron.php
Disallow: /INSTALL.mysql.txt
Disallow: /INSTALL.pgsql.txt
Disallow: /INSTALL.sqlite.txt
Disallow: /install.php
Disallow: /INSTALL.txt
Disallow: /LICENSE.txt
Disallow: /MAINTAINERS.txt
Disallow: /update.php
Disallow: /UPGRADE.txt
Disallow: /xmlrpc.php
# Paths (clean URLs)
Disallow: /admin/
Disallow: /comment/reply/
Disallow: /filter/tips/
Disallow: /node/add/
Disallow: /search/
Disallow: /user/register/
Disallow: /user/password/
Disallow: /user/login/
Disallow: /user/logout/
# Paths (no clean URLs)
Disallow: /?q=admin/
Disallow: /?q=comment/reply/
Disallow: /?q=filter/tips/
Disallow: /?q=node/add/
Disallow: /?q=search/
Disallow: /?q=user/password/
Disallow: /?q=user/register/
Disallow: /?q=user/login/
Disallow: /?q=user/logout/
  &amp;lt;/p&amp;gt;
 &amp;lt;/body&amp;gt;
&amp;lt;/html&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is much better. Now the tree structure of the HTML page is clearly visible and we can visually separate the different elements.&lt;/p&gt;
&lt;p&gt;In particular, the structure of the page is:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;page head
&lt;ul&gt;
&lt;li&gt;with ttle: &amp;ldquo;A Useful Page&amp;rdquo;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;page body
&lt;ul&gt;
&lt;li&gt;with level 1 header &amp;ldquo;An Interesting Title&amp;rdquo;&lt;/li&gt;
&lt;li&gt;a division with text &amp;ldquo;Lorem ipsum&amp;hellip;&amp;rdquo;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;How do we work with these elements? Suppose we want to recover the title and the text. The requests library has some useful functions.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Find the title
url = &#39;http://pythonscraping.com/pages/page1.html&#39;
response = requests.get(url)
soup = BeautifulSoup(response.text, &#39;lxml&#39;)
soup.find(&#39;title&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;title&amp;gt;A Useful Page&amp;lt;/title&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Extract text
soup.find(&#39;title&#39;).text
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&#39;A Useful Page&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Find all h1 elements
soup.find_all(&#39;h1&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[&amp;lt;h1&amp;gt;An Interesting Title&amp;lt;/h1&amp;gt;]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Find all title or h1 elements
soup.find_all([&#39;title&#39;,&#39;h1&#39;])
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[&amp;lt;title&amp;gt;A Useful Page&amp;lt;/title&amp;gt;, &amp;lt;h1&amp;gt;An Interesting Title&amp;lt;/h1&amp;gt;]
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;regular-expressions&#34;&gt;Regular Expressions&lt;/h3&gt;
&lt;p&gt;Note that there is always a more direct alternative: using regular expressions directly on the response!&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Find the title
re.findall(&#39;&amp;lt;title&amp;gt;(.*)&amp;lt;/title&amp;gt;&#39;, response.text)[0]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&#39;A Useful Page&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Find all h1 elements
re.findall(&#39;&amp;lt;h1&amp;gt;(.*)&amp;lt;/h1&amp;gt;&#39;, response.text)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[&#39;An Interesting Title&#39;]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Find all title or h1 elements
[x[1] for x in re.findall(&#39;&amp;lt;(title|h1)&amp;gt;(.*)&amp;lt;&#39;, response.text)]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[&#39;A Useful Page&#39;, &#39;An Interesting Title&#39;]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This was a very simple page and there was not so much to look for. Let&amp;rsquo;s now look at a more realistic example.&lt;/p&gt;
&lt;h3 id=&#34;attributes&#34;&gt;Attributes&lt;/h3&gt;
&lt;p&gt;Let&amp;rsquo;s inspect a slightly more complicated page: &lt;a href=&#34;http://pythonscraping.com/pages/page3.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://pythonscraping.com/pages/page3.html&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;In this page, there is much more content than in the previous one. There seems to be a table, there are images, hyperlinks, etc&amp;hellip; It&amp;rsquo;s the perfect playground. Let&amp;rsquo;s have a look at what does the HTML code look like.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Inspect HTML code
url2 = &#39;http://pythonscraping.com/pages/page3.html&#39;
response = requests.get(url2)
soup = BeautifulSoup(response.text,&#39;lxml&#39;)
print(soup.prettify())
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;html&amp;gt;
 &amp;lt;head&amp;gt;
  &amp;lt;style&amp;gt;
   img{
	width:75px;
}
table{
	width:50%;
}
td{
	margin:10px;
	padding:10px;
}
.wrapper{
	width:800px;
}
.excitingNote{
	font-style:italic;
	font-weight:bold;
}
  &amp;lt;/style&amp;gt;
 &amp;lt;/head&amp;gt;
 &amp;lt;body&amp;gt;
  &amp;lt;div id=&amp;quot;wrapper&amp;quot;&amp;gt;
   &amp;lt;img src=&amp;quot;../img/gifts/logo.jpg&amp;quot; style=&amp;quot;float:left;&amp;quot;/&amp;gt;
   &amp;lt;h1&amp;gt;
    Totally Normal Gifts
   &amp;lt;/h1&amp;gt;
   &amp;lt;div id=&amp;quot;content&amp;quot;&amp;gt;
    Here is a collection of totally normal, totally reasonable gifts that your friends are sure to love! Our collection is
hand-curated by well-paid, free-range Tibetan monks.
    &amp;lt;p&amp;gt;
     We haven&#39;t figured out how to make online shopping carts yet, but you can send us a check to:
     &amp;lt;br/&amp;gt;
     123 Main St.
     &amp;lt;br/&amp;gt;
     Abuja, Nigeria
We will then send your totally amazing gift, pronto! Please include an extra $5.00 for gift wrapping.
    &amp;lt;/p&amp;gt;
   &amp;lt;/div&amp;gt;
   &amp;lt;table id=&amp;quot;giftList&amp;quot;&amp;gt;
    &amp;lt;tr&amp;gt;
     &amp;lt;th&amp;gt;
      Item Title
     &amp;lt;/th&amp;gt;
     &amp;lt;th&amp;gt;
      Description
     &amp;lt;/th&amp;gt;
     &amp;lt;th&amp;gt;
      Cost
     &amp;lt;/th&amp;gt;
     &amp;lt;th&amp;gt;
      Image
     &amp;lt;/th&amp;gt;
    &amp;lt;/tr&amp;gt;
    &amp;lt;tr class=&amp;quot;gift&amp;quot; id=&amp;quot;gift1&amp;quot;&amp;gt;
     &amp;lt;td&amp;gt;
      Vegetable Basket
     &amp;lt;/td&amp;gt;
     &amp;lt;td&amp;gt;
      This vegetable basket is the perfect gift for your health conscious (or overweight) friends!
      &amp;lt;span class=&amp;quot;excitingNote&amp;quot;&amp;gt;
       Now with super-colorful bell peppers!
      &amp;lt;/span&amp;gt;
     &amp;lt;/td&amp;gt;
     &amp;lt;td&amp;gt;
      $15.00
     &amp;lt;/td&amp;gt;
     &amp;lt;td&amp;gt;
      &amp;lt;img src=&amp;quot;../img/gifts/img1.jpg&amp;quot;/&amp;gt;
     &amp;lt;/td&amp;gt;
    &amp;lt;/tr&amp;gt;
    &amp;lt;tr class=&amp;quot;gift&amp;quot; id=&amp;quot;gift2&amp;quot;&amp;gt;
     &amp;lt;td&amp;gt;
      Russian Nesting Dolls
     &amp;lt;/td&amp;gt;
     &amp;lt;td&amp;gt;
      Hand-painted by trained monkeys, these exquisite dolls are priceless! And by &amp;quot;priceless,&amp;quot; we mean &amp;quot;extremely expensive&amp;quot;!
      &amp;lt;span class=&amp;quot;excitingNote&amp;quot;&amp;gt;
       8 entire dolls per set! Octuple the presents!
      &amp;lt;/span&amp;gt;
     &amp;lt;/td&amp;gt;
     &amp;lt;td&amp;gt;
      $10,000.52
     &amp;lt;/td&amp;gt;
     &amp;lt;td&amp;gt;
      &amp;lt;img src=&amp;quot;../img/gifts/img2.jpg&amp;quot;/&amp;gt;
     &amp;lt;/td&amp;gt;
    &amp;lt;/tr&amp;gt;
    &amp;lt;tr class=&amp;quot;gift&amp;quot; id=&amp;quot;gift3&amp;quot;&amp;gt;
     &amp;lt;td&amp;gt;
      Fish Painting
     &amp;lt;/td&amp;gt;
     &amp;lt;td&amp;gt;
      If something seems fishy about this painting, it&#39;s because it&#39;s a fish!
      &amp;lt;span class=&amp;quot;excitingNote&amp;quot;&amp;gt;
       Also hand-painted by trained monkeys!
      &amp;lt;/span&amp;gt;
     &amp;lt;/td&amp;gt;
     &amp;lt;td&amp;gt;
      $10,005.00
     &amp;lt;/td&amp;gt;
     &amp;lt;td&amp;gt;
      &amp;lt;img src=&amp;quot;../img/gifts/img3.jpg&amp;quot;/&amp;gt;
     &amp;lt;/td&amp;gt;
    &amp;lt;/tr&amp;gt;
    &amp;lt;tr class=&amp;quot;gift&amp;quot; id=&amp;quot;gift4&amp;quot;&amp;gt;
     &amp;lt;td&amp;gt;
      Dead Parrot
     &amp;lt;/td&amp;gt;
     &amp;lt;td&amp;gt;
      This is an ex-parrot!
      &amp;lt;span class=&amp;quot;excitingNote&amp;quot;&amp;gt;
       Or maybe he&#39;s only resting?
      &amp;lt;/span&amp;gt;
     &amp;lt;/td&amp;gt;
     &amp;lt;td&amp;gt;
      $0.50
     &amp;lt;/td&amp;gt;
     &amp;lt;td&amp;gt;
      &amp;lt;img src=&amp;quot;../img/gifts/img4.jpg&amp;quot;/&amp;gt;
     &amp;lt;/td&amp;gt;
    &amp;lt;/tr&amp;gt;
    &amp;lt;tr class=&amp;quot;gift&amp;quot; id=&amp;quot;gift5&amp;quot;&amp;gt;
     &amp;lt;td&amp;gt;
      Mystery Box
     &amp;lt;/td&amp;gt;
     &amp;lt;td&amp;gt;
      If you love suprises, this mystery box is for you! Do not place on light-colored surfaces. May cause oil staining.
      &amp;lt;span class=&amp;quot;excitingNote&amp;quot;&amp;gt;
       Keep your friends guessing!
      &amp;lt;/span&amp;gt;
     &amp;lt;/td&amp;gt;
     &amp;lt;td&amp;gt;
      $1.50
     &amp;lt;/td&amp;gt;
     &amp;lt;td&amp;gt;
      &amp;lt;img src=&amp;quot;../img/gifts/img6.jpg&amp;quot;/&amp;gt;
     &amp;lt;/td&amp;gt;
    &amp;lt;/tr&amp;gt;
   &amp;lt;/table&amp;gt;
   &amp;lt;div id=&amp;quot;footer&amp;quot;&amp;gt;
    © Totally Normal Gifts, Inc.
    &amp;lt;br/&amp;gt;
    +234 (617) 863-0736
   &amp;lt;/div&amp;gt;
  &amp;lt;/div&amp;gt;
 &amp;lt;/body&amp;gt;
&amp;lt;/html&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As we can see, now the page is much more complicated than before. An important distintion is that now some tags have classes. For example, the first &lt;code&gt;&amp;lt;img&amp;gt;&lt;/code&gt; tag now has a class &lt;code&gt;src&lt;/code&gt; and a class &lt;code&gt;style&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;img src=&amp;quot;../img/gifts/logo.jpg&amp;quot; style=&amp;quot;float:left;&amp;quot;&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Moreover, even though &lt;code&gt;BeautifulSoup&lt;/code&gt; is formatting the page in a nicer way, it&amp;rsquo;s still pretty hard to go through it. How can one locate one specific element? And, most importantly, if you know the element only graphically, how do you recover the equivalent in the HTML code?&lt;/p&gt;
&lt;p&gt;The best way is to use the &lt;code&gt;inspect&lt;/code&gt; function from Chrome. Firefox has an equivalent function. Let&amp;rsquo;s inspect the original page.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;Suppose now you want to recover all item names. Let&amp;rsquo;s inspect the first. The corresponding line looks like this:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;    &amp;lt;tr class=&amp;quot;gift&amp;quot; id=&amp;quot;gift1&amp;quot;&amp;gt;
     &amp;lt;td&amp;gt;
      Vegetable Basket
     &amp;lt;/td&amp;gt;
     &amp;lt;td&amp;gt;
      This vegetable basket is the perfect gift for your health conscious (or overweight) friends!
      &amp;lt;span class=&amp;quot;excitingNote&amp;quot;&amp;gt;
       Now with super-colorful bell peppers!
      &amp;lt;/span&amp;gt;
     &amp;lt;/td&amp;gt;
     &amp;lt;td&amp;gt;
      $15.00
     &amp;lt;/td&amp;gt;
     &amp;lt;td&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s see some alternative ways.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Select the first td element
soup.find(&#39;td&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;td&amp;gt;
Vegetable Basket
&amp;lt;/td&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Select the first td element of the second tr element (row)
second_row = soup.find_all(&#39;tr&#39;)[1]
second_row.find(&#39;td&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;td&amp;gt;
Vegetable Basket
&amp;lt;/td&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Select the first element of the table with id=&amp;quot;giftList&amp;quot;
table = soup.find(&#39;table&#39;, {&amp;quot;id&amp;quot;:&amp;quot;giftList&amp;quot;})
table.find(&#39;td&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;td&amp;gt;
Vegetable Basket
&amp;lt;/td&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The last is the most robust way to scrape. In fact, the first two methods are likely to fail if the page gets modified. If another &lt;code&gt;td&lt;/code&gt; element gets added on top of the table, the code will recover something else entirely. In general it&amp;rsquo;s a good practice, to look if the element we want to scrape can be identified by some attribute that is likely to be invariant to changes to other parts of the web page. In this case, the table with &lt;code&gt;id=&amp;quot;giftList&amp;quot;&lt;/code&gt; is likely to be our object of interest even if another table id added, for example.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s say no we want to recover the whole table. What would you do?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import pandas as pd

# Shortcut
df = pd.read_html(url2)[0]
df
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;Item Title&lt;/th&gt;
      &lt;th&gt;Description&lt;/th&gt;
      &lt;th&gt;Cost&lt;/th&gt;
      &lt;th&gt;Image&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;Vegetable Basket&lt;/td&gt;
      &lt;td&gt;This vegetable basket is the perfect gift for ...&lt;/td&gt;
      &lt;td&gt;$15.00&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;Russian Nesting Dolls&lt;/td&gt;
      &lt;td&gt;Hand-painted by trained monkeys, these exquisi...&lt;/td&gt;
      &lt;td&gt;$10,000.52&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;Fish Painting&lt;/td&gt;
      &lt;td&gt;If something seems fishy about this painting, ...&lt;/td&gt;
      &lt;td&gt;$10,005.00&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;Dead Parrot&lt;/td&gt;
      &lt;td&gt;This is an ex-parrot! Or maybe he&#39;s only resting?&lt;/td&gt;
      &lt;td&gt;$0.50&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;Mystery Box&lt;/td&gt;
      &lt;td&gt;If you love suprises, this mystery box is for ...&lt;/td&gt;
      &lt;td&gt;$1.50&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Scraping with response
table = soup.find(&#39;table&#39;, {&amp;quot;id&amp;quot;:&amp;quot;giftList&amp;quot;})

# Create empty dataframe
col_names = [x.text.strip() for x in table.find_all(&#39;th&#39;)]
df = pd.DataFrame(columns=col_names)

# Loop over rows and append them to dataframe
for row in table.find_all(&#39;tr&#39;)[1:]:
    columns = [x.text.strip() for x in row.find_all(&#39;td&#39;)]
    df_row = dict(zip(col_names, columns))
    df = df.append(df_row, ignore_index=True)

df
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;/var/folders/xy/szm3b7211cj16ldwcywbvfzdqv7yhd/T/ipykernel_92159/3999490009.py:12: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  df = df.append(df_row, ignore_index=True)
/var/folders/xy/szm3b7211cj16ldwcywbvfzdqv7yhd/T/ipykernel_92159/3999490009.py:12: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  df = df.append(df_row, ignore_index=True)
/var/folders/xy/szm3b7211cj16ldwcywbvfzdqv7yhd/T/ipykernel_92159/3999490009.py:12: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  df = df.append(df_row, ignore_index=True)
/var/folders/xy/szm3b7211cj16ldwcywbvfzdqv7yhd/T/ipykernel_92159/3999490009.py:12: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  df = df.append(df_row, ignore_index=True)
/var/folders/xy/szm3b7211cj16ldwcywbvfzdqv7yhd/T/ipykernel_92159/3999490009.py:12: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  df = df.append(df_row, ignore_index=True)
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;Item Title&lt;/th&gt;
      &lt;th&gt;Description&lt;/th&gt;
      &lt;th&gt;Cost&lt;/th&gt;
      &lt;th&gt;Image&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;Vegetable Basket&lt;/td&gt;
      &lt;td&gt;This vegetable basket is the perfect gift for ...&lt;/td&gt;
      &lt;td&gt;$15.00&lt;/td&gt;
      &lt;td&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;Russian Nesting Dolls&lt;/td&gt;
      &lt;td&gt;Hand-painted by trained monkeys, these exquisi...&lt;/td&gt;
      &lt;td&gt;$10,000.52&lt;/td&gt;
      &lt;td&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;Fish Painting&lt;/td&gt;
      &lt;td&gt;If something seems fishy about this painting, ...&lt;/td&gt;
      &lt;td&gt;$10,005.00&lt;/td&gt;
      &lt;td&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;Dead Parrot&lt;/td&gt;
      &lt;td&gt;This is an ex-parrot! Or maybe he&#39;s only resting?&lt;/td&gt;
      &lt;td&gt;$0.50&lt;/td&gt;
      &lt;td&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;Mystery Box&lt;/td&gt;
      &lt;td&gt;If you love suprises, this mystery box is for ...&lt;/td&gt;
      &lt;td&gt;$1.50&lt;/td&gt;
      &lt;td&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Compact alternative
table = soup.find(&#39;table&#39;, {&amp;quot;id&amp;quot;:&amp;quot;giftList&amp;quot;})
content = [[x.text.strip() for x in row.find_all([&#39;th&#39;,&#39;td&#39;])] for row in table.find_all(&#39;tr&#39;)]
df = pd.DataFrame(content[1:], columns=content[0])

df
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;Item Title&lt;/th&gt;
      &lt;th&gt;Description&lt;/th&gt;
      &lt;th&gt;Cost&lt;/th&gt;
      &lt;th&gt;Image&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;Vegetable Basket&lt;/td&gt;
      &lt;td&gt;This vegetable basket is the perfect gift for ...&lt;/td&gt;
      &lt;td&gt;$15.00&lt;/td&gt;
      &lt;td&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;Russian Nesting Dolls&lt;/td&gt;
      &lt;td&gt;Hand-painted by trained monkeys, these exquisi...&lt;/td&gt;
      &lt;td&gt;$10,000.52&lt;/td&gt;
      &lt;td&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;Fish Painting&lt;/td&gt;
      &lt;td&gt;If something seems fishy about this painting, ...&lt;/td&gt;
      &lt;td&gt;$10,005.00&lt;/td&gt;
      &lt;td&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;Dead Parrot&lt;/td&gt;
      &lt;td&gt;This is an ex-parrot! Or maybe he&#39;s only resting?&lt;/td&gt;
      &lt;td&gt;$0.50&lt;/td&gt;
      &lt;td&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;Mystery Box&lt;/td&gt;
      &lt;td&gt;If you love suprises, this mystery box is for ...&lt;/td&gt;
      &lt;td&gt;$1.50&lt;/td&gt;
      &lt;td&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;We have now seen how to scrape a simple but realistic webpage. Let&amp;rsquo;s proceed with a practical example.&lt;/p&gt;
&lt;h3 id=&#34;css-selectors&#34;&gt;CSS Selectors&lt;/h3&gt;
&lt;p&gt;One alternative way of doing exactly the same thing is to use &lt;code&gt;select&lt;/code&gt;. The &lt;code&gt;select&lt;/code&gt; function is very similar to &lt;code&gt;find_all&lt;/code&gt; but has a different syntax. In particular, to search an element with a certain &lt;code&gt;tag&lt;/code&gt; and &lt;code&gt; attribute&lt;/code&gt;, we have to pass the following input:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;soup.select(tag[attribute=&amp;quot;attribute_name&amp;quot;])
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Select the first element of the table whose id contains &amp;quot;List&amp;quot;
table = soup.select(&#39;table[id*=&amp;quot;List&amp;quot;]&#39;)[0]
table.find(&#39;td&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;td&amp;gt;
Vegetable Basket
&amp;lt;/td&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;forms-and-post-requests&#34;&gt;Forms and post requests&lt;/h3&gt;
&lt;p&gt;When you are scraping, you sometimes have to fill-in forms, either to log-in into an account, or to input the arguments for a search query. Often forms are dynamic objects, but not always. Sometimes we can fill in forms also using the &lt;code&gt;requests&lt;/code&gt; library. In whis section we see a simple example.&lt;/p&gt;
&lt;h4 id=&#34;shortcut&#34;&gt;Shortcut&lt;/h4&gt;
&lt;p&gt;Often we can bypass forms, if the form redirects us to another page whose URL contains the parameters of the form. These are &amp;ldquo;well-behaved&amp;rdquo; forms and are actually quite frequent.&lt;/p&gt;
&lt;p&gt;We can find a simple example at: &lt;a href=&#34;http://www.webscrapingfordatascience.com/basicform/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://www.webscrapingfordatascience.com/basicform/&lt;/a&gt;. This form takes as input a bunch of information and when we click on &amp;ldquo;&lt;em&gt;Submit my information&lt;/em&gt;&amp;rdquo;, we get exactly the same page but with a different URL that contains the information we have inserted.&lt;/p&gt;
&lt;p&gt;Suppose I insert the following information:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Your gender: &amp;ldquo;Male&amp;rdquo;&lt;/li&gt;
&lt;li&gt;Food you like: &amp;ldquo;Pizza!&amp;rdquo; and &amp;ldquo;Fries please&amp;rdquo;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We should get the following url: &lt;a href=&#34;http://www.webscrapingfordatascience.com/basicform/?name=&amp;amp;gender=M&amp;amp;pizza=like&amp;amp;fries=like&amp;amp;haircolor=black&amp;amp;comments=&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://www.webscrapingfordatascience.com/basicform/?name=&amp;gender=M&amp;pizza=like&amp;fries=like&amp;haircolor=black&amp;comments=&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;We can decompose the url in various components, separated by one &amp;ldquo;?&amp;rdquo; and multiple &amp;ldquo;&amp;amp;&amp;rdquo;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://www.webscrapingfordatascience.com/basicform/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://www.webscrapingfordatascience.com/basicform/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;name=&lt;/li&gt;
&lt;li&gt;gender=M&lt;/li&gt;
&lt;li&gt;pizza=like&lt;/li&gt;
&lt;li&gt;fries=like&lt;/li&gt;
&lt;li&gt;haircolor=black&lt;/li&gt;
&lt;li&gt;comments=&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We can clearly see a pattern: the first component is the cose of the url and the other components are the form options. The ones we didn&amp;rsquo;t fill have the form &lt;code&gt;option=&lt;/code&gt; while the ones we did fill are &lt;code&gt;option=value&lt;/code&gt;. Knowing the syntax of a particular form we could fill it ourselves.&lt;/p&gt;
&lt;p&gt;For example, we could remove the fries and change the hair color to &lt;em&gt;brown&lt;/em&gt;: &lt;a href=&#34;http://www.webscrapingfordatascience.com/basicform/?name=&amp;amp;gender=M&amp;amp;pizza=like&amp;amp;fries=&amp;amp;haircolor=brown&amp;amp;comments=&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://www.webscrapingfordatascience.com/basicform/?name=&amp;gender=M&amp;pizza=like&amp;fries=&amp;haircolor=brown&amp;comments=&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Moreover, most forms work even if you remove the empty options. For example, the url above is equivalent to:http://www.webscrapingfordatascience.com/basicform/?gender=M&amp;amp;pizza=like&amp;amp;haircolor=brown&amp;amp;comments=&lt;/p&gt;
&lt;p&gt;One way to scrape websites with such forms is to create a string with the url with all the empty options and fill them using string formatting functions.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Building form url
url_core = &#39;http://www.webscrapingfordatascience.com/basicform/?&#39;
url_options = &#39;name=%s&amp;amp;gender=%s&amp;amp;pizza=%s&amp;amp;fries=%s&amp;amp;haircolor=%s&amp;amp;comments=%s&#39;
options = (&#39;&#39;,&#39;M&#39;,&#39;like&#39;,&#39;&#39;,&#39;brown&#39;,&#39;&#39;)
url = url_core + url_options % options

print(url)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;http://www.webscrapingfordatascience.com/basicform/?name=&amp;amp;gender=M&amp;amp;pizza=like&amp;amp;fries=&amp;amp;haircolor=brown&amp;amp;comments=
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;An alternative way is to name the options. This alternative is more verbose but more precise and does not require you to provide always all the options, even if empty.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Alternative 1
url_core = &#39;http://www.webscrapingfordatascience.com/basicform/?&#39;
url_options = &#39;name={name}&amp;amp;gender={gender}&amp;amp;pizza={pizza}&amp;amp;fries={fries}&amp;amp;haircolor={haircolor}&amp;amp;comments={comments}&#39;
options = {
    &#39;name&#39;: &#39;&#39;,
    &#39;gender&#39;: &#39;M&#39;,
    &#39;pizza&#39;: &#39;like&#39;,
    &#39;fries&#39;: &#39;&#39;,
    &#39;haircolor&#39;: &#39;brown&#39;,
    &#39;comments&#39;: &#39;&#39;
    }
url = url_core + url_options.format(**options)

print(url)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;http://www.webscrapingfordatascience.com/basicform/?name=&amp;amp;gender=M&amp;amp;pizza=like&amp;amp;fries=&amp;amp;haircolor=brown&amp;amp;comments=
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Lastly, one could build the url on the go.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Alternative 2
url = &#39;http://www.webscrapingfordatascience.com/basicform/?&#39;
options = {
    &#39;gender&#39;: &#39;M&#39;,
    &#39;pizza&#39;: &#39;like&#39;,
    &#39;haircolor&#39;: &#39;brown&#39;,
    }
for key, value in options.items():
    url += key + &#39;=&#39; + value + &#39;&amp;amp;&#39;

print(url)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;http://www.webscrapingfordatascience.com/basicform/?gender=M&amp;amp;pizza=like&amp;amp;haircolor=brown&amp;amp;
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;post-forms&#34;&gt;Post forms&lt;/h4&gt;
&lt;p&gt;Sometimes however, forms do not provide nice URLs as output. This is particularly true for login forms. There is however still a method, for some of them, to deal with them.&lt;/p&gt;
&lt;p&gt;For this section, we will use the same form example as before: &lt;a href=&#34;http://www.webscrapingfordatascience.com/postform2/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://www.webscrapingfordatascience.com/postform2/&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;This looks like the same form but now when the user clicks on &amp;ldquo;&lt;em&gt;Submit my information&lt;/em&gt;&amp;rdquo;, we get a page with a summary of the information. The biggest difference however, is that the output URL is exactly the same. Hence, we cannot rely on the same URL-bulding strategy as before.&lt;/p&gt;
&lt;p&gt;If we inspect the page, we observe the following line at the very beginning&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;form method=&amp;quot;POST&amp;quot;&amp;gt;
[...]
&amp;lt;/form&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And inside there are various input fields:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;&amp;lt;input type=&amp;quot;text&amp;quot;&amp;gt;&lt;/code&gt; for name&lt;/li&gt;
&lt;li&gt;&lt;code&gt;&amp;lt;input type=&amp;quot;radio&amp;quot;&amp;gt;&lt;/code&gt; for gender&lt;/li&gt;
&lt;li&gt;&lt;code&gt;&amp;lt;input type=&amp;quot;checkbox&amp;quot;&amp;gt;&lt;/code&gt; for food&lt;/li&gt;
&lt;li&gt;&lt;code&gt;&amp;lt;select&amp;gt;...&amp;lt;/select&amp;gt;&lt;/code&gt; for the hair color&lt;/li&gt;
&lt;li&gt;&lt;code&gt;&amp;lt;textarea&amp;gt;...&amp;lt;/textarea&amp;gt;&lt;/code&gt; for comments&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;These are all fields with which we can interact using the &lt;code&gt;response&lt;/code&gt; package. The main difference is that we won&amp;rsquo;t use the &lt;code&gt;get&lt;/code&gt; method to get the response from the URL but we will use the &lt;code&gt;post&lt;/code&gt; method to post our form parameters and get a response.&lt;/p&gt;
&lt;p&gt;If we input the following options:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;gender&lt;/em&gt;: male&lt;/li&gt;
&lt;li&gt;&lt;em&gt;pizza&lt;/em&gt;: yes&lt;/li&gt;
&lt;li&gt;&lt;em&gt;hair color&lt;/em&gt;: brown hair&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;and we click &amp;ldquo;&lt;em&gt;Submit my information&lt;/em&gt;&amp;rdquo; we get to a page with the following text:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Thanks for submitting your information
Here&#39;s a dump of the form data that was submitted:

array(5) {
  [&amp;quot;name&amp;quot;]=&amp;gt;
  string(0) &amp;quot;&amp;quot;
  [&amp;quot;gender&amp;quot;]=&amp;gt;
  string(1) &amp;quot;M&amp;quot;
  [&amp;quot;pizza&amp;quot;]=&amp;gt;
  string(4) &amp;quot;like&amp;quot;
  [&amp;quot;haircolor&amp;quot;]=&amp;gt;
  string(5) &amp;quot;brown&amp;quot;
  [&amp;quot;comments&amp;quot;]=&amp;gt;
  string(0) &amp;quot;&amp;quot;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We will not try to get to the same page using the &lt;code&gt;requests&lt;/code&gt; package.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# URL
url = &#39;http://www.webscrapingfordatascience.com/postform2/&#39;

# Options
options = {
    &#39;gender&#39;: &#39;M&#39;,
    &#39;pizza&#39;: &#39;like&#39;,
    &#39;haircolor&#39;: &#39;brown&#39;,
    }

# Post request
response = requests.post(url, data=options)
print(response.text)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;html&amp;gt;
	&amp;lt;body&amp;gt;


&amp;lt;h2&amp;gt;Thanks for submitting your information&amp;lt;/h2&amp;gt;

&amp;lt;p&amp;gt;Here&#39;s a dump of the form data that was submitted:&amp;lt;/p&amp;gt;

&amp;lt;pre&amp;gt;array(3) {
  [&amp;quot;gender&amp;quot;]=&amp;gt;
  string(1) &amp;quot;M&amp;quot;
  [&amp;quot;pizza&amp;quot;]=&amp;gt;
  string(4) &amp;quot;like&amp;quot;
  [&amp;quot;haircolor&amp;quot;]=&amp;gt;
  string(5) &amp;quot;brown&amp;quot;
}
&amp;lt;/pre&amp;gt;


	&amp;lt;/body&amp;gt;
&amp;lt;/html&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We have obtained exactly what we wanted! However, sometimes, websites block direct &lt;code&gt;post&lt;/code&gt; requests.&lt;/p&gt;
&lt;p&gt;One simple example is: &lt;a href=&#34;http://www.webscrapingfordatascience.com/postform3/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://www.webscrapingfordatascience.com/postform3/&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Post request
url = &#39;http://www.webscrapingfordatascience.com/postform3/&#39;
response = requests.post(url, data=options)
print(response.text)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;html&amp;gt;
	&amp;lt;body&amp;gt;


Are you trying to submit information from somewhere else?

	&amp;lt;/body&amp;gt;
&amp;lt;/html&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What happened? If we inspect the page, we can see that there is a new line at the beginning:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;input type=&amp;quot;hidden&amp;quot; name=&amp;quot;protection&amp;quot; value=&amp;quot;2c17abf5d5b4e326bea802600ff88405&amp;quot;&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now the form contains one more value - &lt;em&gt;protection&lt;/em&gt; which is conventiently hidden. In order to bypass the protection, we need to provide the correct &lt;em&gt;protection&lt;/em&gt; value to the form.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Post request
url = &#39;http://www.webscrapingfordatascience.com/postform3/&#39;
response = requests.get(url)

# Get out the value for protection
soup = BeautifulSoup(response.text, &#39;lxml&#39;)
options[&#39;protection&#39;] = soup.find(&#39;input&#39;, attrs={&#39;name&#39;: &#39;protection&#39;}).get(&#39;value&#39;)

# Post request
response = requests.post(url, data=options)
print(response.text)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;html&amp;gt;
	&amp;lt;body&amp;gt;



&amp;lt;h2&amp;gt;Thanks for submitting your information&amp;lt;/h2&amp;gt;

&amp;lt;p&amp;gt;Here&#39;s a dump of the form data that was submitted:&amp;lt;/p&amp;gt;

&amp;lt;pre&amp;gt;array(4) {
  [&amp;quot;gender&amp;quot;]=&amp;gt;
  string(1) &amp;quot;M&amp;quot;
  [&amp;quot;pizza&amp;quot;]=&amp;gt;
  string(4) &amp;quot;like&amp;quot;
  [&amp;quot;haircolor&amp;quot;]=&amp;gt;
  string(5) &amp;quot;brown&amp;quot;
  [&amp;quot;protection&amp;quot;]=&amp;gt;
  string(32) &amp;quot;16c87fc858e4d9fcb8d9c920b699388d&amp;quot;
}
&amp;lt;/pre&amp;gt;



	&amp;lt;/body&amp;gt;
&amp;lt;/html&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Indeed, now the post request was successful.&lt;/p&gt;
&lt;h3 id=&#34;proxies&#34;&gt;Proxies&lt;/h3&gt;
&lt;p&gt;We have discussed at the beginning how to be more subtle while scraping, by changing headers. In this section we will explore one step forward in anonimity: proxies.&lt;/p&gt;
&lt;p&gt;When we send an HTTP request, first the request is sent to a proxy server. The important thing is that the destination web server will which is the origin proxy server. Therefore, when one destination web server sees too many requests coming from one machine, it will block the proxy server.&lt;/p&gt;
&lt;p&gt;How can we change proxy? There are many websites that offer proxies for money but there are also some that offer proxies for free. The problem with free proxies (but often also with premium ones) is that there are many users using the same proxy, hence they are&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;slow&lt;/li&gt;
&lt;li&gt;blocked fast by many websites&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Nevertheless, it might be still useful to know how to change proxies.&lt;/p&gt;
&lt;h4 id=&#34;get-proxy-list&#34;&gt;Get proxy list&lt;/h4&gt;
&lt;p&gt;One website where we can get some free proxies to use for scraping is &lt;a href=&#34;https://free-proxy-list.net/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://free-proxy-list.net/&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;If we open the page, we see that there is a long list of proxies, from different countries and with different characteristics. Importantly, we are mostly interested in &lt;em&gt;https&lt;/em&gt; proxies. We are now going to retrieve a list of them. Note that the proxy list of this website is updated quite often. However, free proxies usually &amp;ldquo;expire&amp;rdquo; even faster.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Retrieve proxy list
def get_proxies():
    response = requests.get(&#39;https://free-proxy-list.net/&#39;)
    soup = BeautifulSoup(response.text, &#39;lxml&#39;)
    table = soup.find(&#39;table&#39;, {&#39;class&#39;:&#39;table&#39;})
    proxies = []
    rows = table.find_all(&#39;tr&#39;)
    for row in rows:
        cols = row.find_all(&#39;td&#39;)
        if len(cols)&amp;gt;0:
            line = [col.text for col in cols]
            if line[6]==&#39;yes&#39;:
                proxies += [line[0]+&#39;:&#39;+line[1]]
    return proxies
            
len(get_proxies())
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;176
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We have found many proxies. How do we use them? We have to provide them as an argment to a &lt;code&gt;requests&lt;/code&gt; session.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Test proxies
url = &#39;https://www.google.com&#39;
proxies = get_proxies()

for proxy in proxies[:10]:
    try:
        response = session.get(url, proxies={&amp;quot;https&amp;quot;: proxy}, timeout=5)
        print(response)
    except Exception as e:
        print(type(e))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;class &#39;NameError&#39;&amp;gt;
&amp;lt;class &#39;NameError&#39;&amp;gt;
&amp;lt;class &#39;NameError&#39;&amp;gt;
&amp;lt;class &#39;NameError&#39;&amp;gt;
&amp;lt;class &#39;NameError&#39;&amp;gt;
&amp;lt;class &#39;NameError&#39;&amp;gt;
&amp;lt;class &#39;NameError&#39;&amp;gt;
&amp;lt;class &#39;NameError&#39;&amp;gt;
&amp;lt;class &#39;NameError&#39;&amp;gt;
&amp;lt;class &#39;NameError&#39;&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Yes, most proxies were extremely slow (and consider we are opening &lt;em&gt;Google&lt;/em&gt;&amp;hellip;) and we got a &lt;code&gt;ConnetTimeout&lt;/code&gt; error. Other proxies worked and for one or two of the others we might have got  a &lt;code&gt;ProxyError&lt;/code&gt;.&lt;/p&gt;
&lt;h2 id=&#34;dynamic-webscraping&#34;&gt;Dynamic Webscraping&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s try to scrape the quotes from this link: &lt;a href=&#34;http://www.webscrapingfordatascience.com/simplejavascript/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://www.webscrapingfordatascience.com/simplejavascript/&lt;/a&gt;. It seems like a straightforward job.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Scrape javascript page
url = &#39;http://www.webscrapingfordatascience.com/simplejavascript/&#39;
response = requests.get(url)
print(response.text)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;html&amp;gt;

&amp;lt;head&amp;gt;
	&amp;lt;script src=&amp;quot;https://code.jquery.com/jquery-3.2.1.min.js&amp;quot;&amp;gt;&amp;lt;/script&amp;gt;
	&amp;lt;script&amp;gt;
	$(function() {
	document.cookie = &amp;quot;jsenabled=1&amp;quot;;
	$.getJSON(&amp;quot;quotes.php&amp;quot;, function(data) {
		var items = [];
		$.each(data, function(key, val) {
			items.push(&amp;quot;&amp;lt;li id=&#39;&amp;quot; + key + &amp;quot;&#39;&amp;gt;&amp;quot; + val + &amp;quot;&amp;lt;/li&amp;gt;&amp;quot;);
		});
		$(&amp;quot;&amp;lt;ul/&amp;gt;&amp;quot;, {
			html: items.join(&amp;quot;&amp;quot;)
			}).appendTo(&amp;quot;body&amp;quot;);
		});
	});
	&amp;lt;/script&amp;gt;
&amp;lt;/head&amp;gt;

&amp;lt;body&amp;gt;

&amp;lt;h1&amp;gt;Here are some quotes&amp;lt;/h1&amp;gt;

&amp;lt;/body&amp;gt;

&amp;lt;/html&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Weird. Our response does not contain the quotes on the page, even though they are clearly visible when we open it in our browser.&lt;/p&gt;
&lt;h3 id=&#34;selenium&#34;&gt;Selenium&lt;/h3&gt;
&lt;p&gt;Selenium is a python library that emulates a browser and lets us see pages exactly as with a normal browser. This is the most user-friendly way to do web scraping, however it has a huge cost: speed. This is by far the slowest way to do web scraping.&lt;/p&gt;
&lt;p&gt;After installing &lt;code&gt;selenium&lt;/code&gt;, we need to download a browser to simulate. We will use Google&amp;rsquo;s chromedriver. You can download it from here: &lt;a href=&#34;https://sites.google.com/a/chromium.org/chromedriver/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://sites.google.com/a/chromium.org/chromedriver/&lt;/a&gt;. Make sure to select &amp;ldquo;&lt;strong&gt;latest stable release&lt;/strong&gt;&amp;rdquo; and not &amp;ldquo;latest beta release&amp;rdquo;.&lt;/p&gt;
&lt;p&gt;Move the downloaded &lt;code&gt;chromedriver&lt;/code&gt; in the current directory (&amp;quot;&lt;em&gt;/11-python-webscraping&lt;/em&gt;&amp;quot; for me). We will now try open the url above with selenium and see if we can scrape the quotes in it.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Set your chromedriver name
chromedriver_name = &#39;/chromedriver_mac&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Open url
path = os.getcwd()
print(path)
driver = webdriver.Chrome(path+chromedriver_name)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;/Users/mcourt/Dropbox/Projects/Data-Science-Python/notebooks


/var/folders/xy/szm3b7211cj16ldwcywbvfzdqv7yhd/T/ipykernel_92159/2846782857.py:4: DeprecationWarning: executable_path has been deprecated, please pass in a Service object
  driver = webdriver.Chrome(path+chromedriver_name)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Awesome! Now, if everything went smooth, you should have a new Chrome window with a banner that says &amp;ldquo;&lt;em&gt;Chrome is being controlled by automated test software&lt;/em&gt;&amp;rdquo;. We can now open the web page and check that the list appears.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Open url
url = &#39;http://www.webscrapingfordatascience.com/simplejavascript/&#39;
driver.get(url)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Again, if averything went well, we are now abl to see our page with all the quotes in it. How do we scrape them?&lt;/p&gt;
&lt;p&gt;If we inspect the elements of the list with the right-click &lt;code&gt;inspect&lt;/code&gt; option, we should see something like:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;html&amp;gt;&amp;lt;head&amp;gt;
	&amp;lt;script src=&amp;quot;https://code.jquery.com/jquery-3.2.1.min.js&amp;quot;&amp;gt;&amp;lt;/script&amp;gt;
	&amp;lt;script&amp;gt;
	$(function() {
	document.cookie = &amp;quot;jsenabled=1&amp;quot;;
	$.getJSON(&amp;quot;quotes.php&amp;quot;, function(data) {
		var items = [];
		$.each(data, function(key, val) {
			items.push(&amp;quot;&amp;lt;li id=&#39;&amp;quot; + key + &amp;quot;&#39;&amp;gt;&amp;quot; + val + &amp;quot;&amp;lt;/li&amp;gt;&amp;quot;);
		});
		$(&amp;quot;&amp;lt;ul/&amp;gt;&amp;quot;, {
			html: items.join(&amp;quot;&amp;quot;)
			}).appendTo(&amp;quot;body&amp;quot;);
		});
	});
	&amp;lt;/script&amp;gt;
&amp;lt;/head&amp;gt;

&amp;lt;body&amp;gt;

&amp;lt;h1&amp;gt;Here are some quotes&amp;lt;/h1&amp;gt;




&amp;lt;ul&amp;gt;&amp;lt;li id=&amp;quot;0&amp;quot;&amp;gt;Every strike brings me closer to the next home run. –Babe Ruth&amp;lt;/li&amp;gt;&amp;lt;li id=&amp;quot;1&amp;quot;&amp;gt;The two most important days in your life are the day you are born and the day you find out why. –Mark Twain&amp;lt;/li&amp;gt;&amp;lt;li id=&amp;quot;2&amp;quot;&amp;gt;Whatever you can do, or dream you can, begin it.  Boldness has genius, power and magic in it. –Johann Wolfgang von Goethe&amp;lt;/li&amp;gt;&amp;lt;/ul&amp;gt;&amp;lt;/body&amp;gt;&amp;lt;/html&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we can see the content! Can we actually retrieve it? Let&amp;rsquo;s try.&lt;/p&gt;
&lt;p&gt;The most common selenium functions to get elements of a page, have a very intuitive syntax and are:
find_element_by_id&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;find_element_by_name&lt;/li&gt;
&lt;li&gt;find_element_by_xpath&lt;/li&gt;
&lt;li&gt;find_element_by_link_text&lt;/li&gt;
&lt;li&gt;find_element_by_partial_link_text&lt;/li&gt;
&lt;li&gt;find_element_by_tag_name&lt;/li&gt;
&lt;li&gt;find_element_by_class_name&lt;/li&gt;
&lt;li&gt;find_element_by_css_selector&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We will not try to recover all elements with tag &lt;code&gt;&amp;lt;li&amp;gt;&lt;/code&gt; (element of list &lt;code&gt;&amp;lt;ul&amp;gt;&lt;/code&gt;).&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Scrape content
quotes = [li.text for li in driver.find_elements_by_tag_name(&#39;li&#39;)]
quotes
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;/var/folders/xy/szm3b7211cj16ldwcywbvfzdqv7yhd/T/ipykernel_92159/157107938.py:2: DeprecationWarning: find_elements_by_* commands are deprecated. Please use find_elements() instead
  quotes = [li.text for li in driver.find_elements_by_tag_name(&#39;li&#39;)]





[]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Yes! It worked! But why?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Headless option
headless_option = webdriver.ChromeOptions()
headless_option.add_argument(&#39;--headless&#39;)

# Scraping
driver = webdriver.Chrome(path+chromedriver_name, options=headless_option)
driver.get(url)
quotes = [li.text for li in driver.find_elements_by_tag_name(&#39;li&#39;)]
quotes
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;/var/folders/xy/szm3b7211cj16ldwcywbvfzdqv7yhd/T/ipykernel_92159/2173692453.py:6: DeprecationWarning: executable_path has been deprecated, please pass in a Service object
  driver = webdriver.Chrome(path+chromedriver_name, options=headless_option)
/var/folders/xy/szm3b7211cj16ldwcywbvfzdqv7yhd/T/ipykernel_92159/2173692453.py:8: DeprecationWarning: find_elements_by_* commands are deprecated. Please use find_elements() instead
  quotes = [li.text for li in driver.find_elements_by_tag_name(&#39;li&#39;)]





[]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Mmm, it (probably) didn&amp;rsquo;t work. Why?&lt;/p&gt;
&lt;p&gt;The problem is that we are trying to retrieve the content of the page too fast. The page hasn&amp;rsquo;t loaded yet. This is a common issue with &lt;code&gt;selenium&lt;/code&gt;. Where are two ways to solve it:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;waiting&lt;/li&gt;
&lt;li&gt;waiting for the element to load&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The second way is the best way but we will first try the first and simpler one: we will just ask the browser to wait for 1 second before searching for &lt;code&gt;&amp;lt;li&amp;gt;&lt;/code&gt; tags&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Scraping
driver = webdriver.Chrome(path+chromedriver_name, options=headless_option)
driver.get(url)
time.sleep(1)
quotes = [li.text for li in driver.find_elements_by_tag_name(&#39;li&#39;)]
quotes
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;/var/folders/xy/szm3b7211cj16ldwcywbvfzdqv7yhd/T/ipykernel_92159/2964398594.py:2: DeprecationWarning: executable_path has been deprecated, please pass in a Service object
  driver = webdriver.Chrome(path+chromedriver_name, options=headless_option)
/var/folders/xy/szm3b7211cj16ldwcywbvfzdqv7yhd/T/ipykernel_92159/2964398594.py:5: DeprecationWarning: find_elements_by_* commands are deprecated. Please use find_elements() instead
  quotes = [li.text for li in driver.find_elements_by_tag_name(&#39;li&#39;)]





[&#39;The best time to plant a tree was 20 years ago. The second best time is now. –Chinese Proverb&#39;,
 &#39;The most common way people give up their power is by thinking they don’t have any. –Alice Walker&#39;,
 &#39;I am not a product of my circumstances. I am a product of my decisions. –Stephen Covey&#39;]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Nice! Now you should have obtained the list that we could not scrape with &lt;code&gt;requests&lt;/code&gt;. If it didn&amp;rsquo;t work, just increase the waiting time and it should work.&lt;/p&gt;
&lt;p&gt;We can now have a look at the &amp;ldquo;better&amp;rdquo; way to use a series of built-in functions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;WebDriverWait&lt;/code&gt;: the waiting function. We will call the &lt;code&gt;until&lt;/code&gt; method&lt;/li&gt;
&lt;li&gt;&lt;code&gt;expected_conditions&lt;/code&gt;: the condition function. We will call the &lt;code&gt;visibility_of_all_elements_located&lt;/code&gt; method&lt;/li&gt;
&lt;li&gt;&lt;code&gt;By&lt;/code&gt;: the selector function. Some of the options are:
&lt;ul&gt;
&lt;li&gt;By.ID&lt;/li&gt;
&lt;li&gt;By.XPATH&lt;/li&gt;
&lt;li&gt;By.NAME&lt;/li&gt;
&lt;li&gt;By.TAG_NAME&lt;/li&gt;
&lt;li&gt;By.CLASS_NAME&lt;/li&gt;
&lt;li&gt;By.CSS_SELECTOR&lt;/li&gt;
&lt;li&gt;By.LINK_TEXT&lt;/li&gt;
&lt;li&gt;By.PARTIAL_LINK_TEXT&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

# Scraping
driver = webdriver.Chrome(path+chromedriver_name, options=headless_option)
driver.get(url)
quotes = WebDriverWait(driver, 10).until(EC.visibility_of_all_elements_located((By.TAG_NAME, &#39;li&#39;)))
quotes = [quote.text for quote in quotes]
quotes
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;/var/folders/xy/szm3b7211cj16ldwcywbvfzdqv7yhd/T/ipykernel_92159/152412441.py:6: DeprecationWarning: executable_path has been deprecated, please pass in a Service object
  driver = webdriver.Chrome(path+chromedriver_name, options=headless_option)





[&#39;The most common way people give up their power is by thinking they don’t have any. –Alice Walker&#39;,
 &#39;The best time to plant a tree was 20 years ago. The second best time is now. –Chinese Proverb&#39;,
 &#39;An unexamined life is not worth living. –Socrates&#39;]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In this case, we have told the browser to wait until either all elements with tag &lt;code&gt;&amp;lt;li&amp;gt;&lt;/code&gt; are visible or 10 seconds have passed. After one condition is realized, the &lt;code&gt;WebDriverWait&lt;/code&gt; function also automatically retrieves all the elements which the &lt;code&gt;expected_condition&lt;/code&gt; function is conditioning on. There are many different conditions we can use. A list can be found here: &lt;a href=&#34;https://selenium-python.readthedocs.io/waits.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://selenium-python.readthedocs.io/waits.html&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;We can easily generalize the function above as follows.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Find element function
def find_elements(driver, function, identifier):
    element = WebDriverWait(driver, 10).until(EC.visibility_of_all_elements_located((function, identifier)))
    return element

quotes = [quote.text for quote in find_elements(driver, By.TAG_NAME, &#39;li&#39;)]
quotes
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[&#39;The most common way people give up their power is by thinking they don’t have any. –Alice Walker&#39;,
 &#39;The best time to plant a tree was 20 years ago. The second best time is now. –Chinese Proverb&#39;,
 &#39;An unexamined life is not worth living. –Socrates&#39;]
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Tree-based Methods</title>
      <link>https://matteocourthoud.github.io/course/ml-econ/07_trees/</link>
      <pubDate>Wed, 09 Mar 2022 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/course/ml-econ/07_trees/</guid>
      <description>&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;%matplotlib inline
from utils.lecture07 import *
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;decision-trees&#34;&gt;Decision Trees&lt;/h2&gt;
&lt;p&gt;Decision trees involve &lt;strong&gt;segmenting the predictor space into a number of simple regions&lt;/strong&gt;. In order to make a prediction for a given observation, we typically use the mean or the mode of the training observations in the region to which it belongs. Since the set of splitting rules used to segment the predictor space can be summarized in a tree, these types of approaches are known as decision tree methods.&lt;/p&gt;
&lt;h3 id=&#34;regression-trees&#34;&gt;Regression Trees&lt;/h3&gt;
&lt;p&gt;For this session we will consider the &lt;code&gt;Hitters&lt;/code&gt; dataset. It consists in individual level data of baseball players. In our applications, we are interested in predicting the players &lt;code&gt;Salary&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Load the data
hitters = pd.read_csv(&#39;data/Hitters.csv&#39;).dropna()
hitters.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;Unnamed: 0&lt;/th&gt;
      &lt;th&gt;AtBat&lt;/th&gt;
      &lt;th&gt;Hits&lt;/th&gt;
      &lt;th&gt;HmRun&lt;/th&gt;
      &lt;th&gt;Runs&lt;/th&gt;
      &lt;th&gt;RBI&lt;/th&gt;
      &lt;th&gt;Walks&lt;/th&gt;
      &lt;th&gt;Years&lt;/th&gt;
      &lt;th&gt;CAtBat&lt;/th&gt;
      &lt;th&gt;CHits&lt;/th&gt;
      &lt;th&gt;...&lt;/th&gt;
      &lt;th&gt;CRuns&lt;/th&gt;
      &lt;th&gt;CRBI&lt;/th&gt;
      &lt;th&gt;CWalks&lt;/th&gt;
      &lt;th&gt;League&lt;/th&gt;
      &lt;th&gt;Division&lt;/th&gt;
      &lt;th&gt;PutOuts&lt;/th&gt;
      &lt;th&gt;Assists&lt;/th&gt;
      &lt;th&gt;Errors&lt;/th&gt;
      &lt;th&gt;Salary&lt;/th&gt;
      &lt;th&gt;NewLeague&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;-Alan Ashby&lt;/td&gt;
      &lt;td&gt;315&lt;/td&gt;
      &lt;td&gt;81&lt;/td&gt;
      &lt;td&gt;7&lt;/td&gt;
      &lt;td&gt;24&lt;/td&gt;
      &lt;td&gt;38&lt;/td&gt;
      &lt;td&gt;39&lt;/td&gt;
      &lt;td&gt;14&lt;/td&gt;
      &lt;td&gt;3449&lt;/td&gt;
      &lt;td&gt;835&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;321&lt;/td&gt;
      &lt;td&gt;414&lt;/td&gt;
      &lt;td&gt;375&lt;/td&gt;
      &lt;td&gt;N&lt;/td&gt;
      &lt;td&gt;W&lt;/td&gt;
      &lt;td&gt;632&lt;/td&gt;
      &lt;td&gt;43&lt;/td&gt;
      &lt;td&gt;10&lt;/td&gt;
      &lt;td&gt;475.0&lt;/td&gt;
      &lt;td&gt;N&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;-Alvin Davis&lt;/td&gt;
      &lt;td&gt;479&lt;/td&gt;
      &lt;td&gt;130&lt;/td&gt;
      &lt;td&gt;18&lt;/td&gt;
      &lt;td&gt;66&lt;/td&gt;
      &lt;td&gt;72&lt;/td&gt;
      &lt;td&gt;76&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;1624&lt;/td&gt;
      &lt;td&gt;457&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;224&lt;/td&gt;
      &lt;td&gt;266&lt;/td&gt;
      &lt;td&gt;263&lt;/td&gt;
      &lt;td&gt;A&lt;/td&gt;
      &lt;td&gt;W&lt;/td&gt;
      &lt;td&gt;880&lt;/td&gt;
      &lt;td&gt;82&lt;/td&gt;
      &lt;td&gt;14&lt;/td&gt;
      &lt;td&gt;480.0&lt;/td&gt;
      &lt;td&gt;A&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;-Andre Dawson&lt;/td&gt;
      &lt;td&gt;496&lt;/td&gt;
      &lt;td&gt;141&lt;/td&gt;
      &lt;td&gt;20&lt;/td&gt;
      &lt;td&gt;65&lt;/td&gt;
      &lt;td&gt;78&lt;/td&gt;
      &lt;td&gt;37&lt;/td&gt;
      &lt;td&gt;11&lt;/td&gt;
      &lt;td&gt;5628&lt;/td&gt;
      &lt;td&gt;1575&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;828&lt;/td&gt;
      &lt;td&gt;838&lt;/td&gt;
      &lt;td&gt;354&lt;/td&gt;
      &lt;td&gt;N&lt;/td&gt;
      &lt;td&gt;E&lt;/td&gt;
      &lt;td&gt;200&lt;/td&gt;
      &lt;td&gt;11&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;500.0&lt;/td&gt;
      &lt;td&gt;N&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;-Andres Galarraga&lt;/td&gt;
      &lt;td&gt;321&lt;/td&gt;
      &lt;td&gt;87&lt;/td&gt;
      &lt;td&gt;10&lt;/td&gt;
      &lt;td&gt;39&lt;/td&gt;
      &lt;td&gt;42&lt;/td&gt;
      &lt;td&gt;30&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;396&lt;/td&gt;
      &lt;td&gt;101&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;48&lt;/td&gt;
      &lt;td&gt;46&lt;/td&gt;
      &lt;td&gt;33&lt;/td&gt;
      &lt;td&gt;N&lt;/td&gt;
      &lt;td&gt;E&lt;/td&gt;
      &lt;td&gt;805&lt;/td&gt;
      &lt;td&gt;40&lt;/td&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;91.5&lt;/td&gt;
      &lt;td&gt;N&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;5&lt;/th&gt;
      &lt;td&gt;-Alfredo Griffin&lt;/td&gt;
      &lt;td&gt;594&lt;/td&gt;
      &lt;td&gt;169&lt;/td&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;74&lt;/td&gt;
      &lt;td&gt;51&lt;/td&gt;
      &lt;td&gt;35&lt;/td&gt;
      &lt;td&gt;11&lt;/td&gt;
      &lt;td&gt;4408&lt;/td&gt;
      &lt;td&gt;1133&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;501&lt;/td&gt;
      &lt;td&gt;336&lt;/td&gt;
      &lt;td&gt;194&lt;/td&gt;
      &lt;td&gt;A&lt;/td&gt;
      &lt;td&gt;W&lt;/td&gt;
      &lt;td&gt;282&lt;/td&gt;
      &lt;td&gt;421&lt;/td&gt;
      &lt;td&gt;25&lt;/td&gt;
      &lt;td&gt;750.0&lt;/td&gt;
      &lt;td&gt;A&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;5 rows × 21 columns&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;In particular, we are interested in looking how the number of &lt;code&gt;Hits&lt;/code&gt; and the &lt;code&gt;Years&lt;/code&gt; of experience predict the &lt;code&gt;Salary&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Get Features
features = [&#39;Years&#39;, &#39;Hits&#39;]
X = hitters[features].values
y = np.log(hitters.Salary.values)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We are actually going to use log(salary) since it has a more gaussian distribution.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;fig, (ax1, ax2) = plt.subplots(1,2, figsize=(11,4))

# Plot salary distribution
ax1.hist(hitters.Salary.values)
ax1.set_xlabel(&#39;Salary&#39;)
ax2.hist(y)
ax2.set_xlabel(&#39;Log(Salary)&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/07_trees_10_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;In order to understand what is a tree, let&amp;rsquo;s first have a look at one. We fit a regression three with 3 leaves or, equivalently put, 2 nodes.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Fit regression tree
tree = DecisionTreeRegressor(max_leaf_nodes=3)
tree.fit(X, y)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;DecisionTreeRegressor(max_leaf_nodes=3)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We are now going to plot the results visually. The biggest avantage of trees is interpretability.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Figure 8.1
fig, ax = plt.subplots(1,1)
ax.set_title(&#39;Figure 8.1&#39;);

# Plot tree
plot_tree(tree, filled=True, feature_names=features, fontsize=14, ax=ax);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/07_trees_14_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;The tree consists of a series of splitting rules, starting at the top of the tree. The top split assigns observations having &lt;code&gt;Years&lt;/code&gt;&amp;lt;4.5 to the left branch.1 The predicted salary for these players is given by the mean response value for the players in the data set with &lt;code&gt;Years&lt;/code&gt;&amp;lt;4.5. For such players, the mean log salary is 5.107, and so we make a prediction of 5.107 thousands of dollars, i.e. $165,174, for these players. Players with &lt;code&gt;Years&lt;/code&gt;&amp;gt;=4.5 are assigned to the right branch, and then that group is further subdivided by &lt;code&gt;Hits&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Overall, the tree stratifies or segments the players into three regions of predictor space:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;players who have played for four or fewer years&lt;/li&gt;
&lt;li&gt;players who have played for five or more years and who made fewer than 118 hits last year, and&lt;/li&gt;
&lt;li&gt;players who have played for five or more years and who made at least 118 hits last year.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;These three regions can be written as&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;R1&lt;/strong&gt; = {X | &lt;code&gt;Years&lt;/code&gt;&amp;lt;4.5}&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;R2&lt;/strong&gt; = {X | &lt;code&gt;Years&lt;/code&gt;&amp;gt;=4.5, &lt;code&gt;Hits&lt;/code&gt;&amp;lt;117.5}, and&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;R3&lt;/strong&gt; = {X | &lt;code&gt;Years&lt;/code&gt;&amp;gt;=4.5, &lt;code&gt;Hits&lt;/code&gt;&amp;gt;=117.5}.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Since the dimension of $X$ is 2, we can visualize the space and the regions in a 2-dimensional graph.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Figure 8.2
def make_figure_8_2():
    
    # Init
    hitters.plot(&#39;Years&#39;, &#39;Hits&#39;, kind=&#39;scatter&#39;, color=&#39;orange&#39;, figsize=(7,6))
    plt.title(&#39;Figure 8.2&#39;)
    plt.xlim(0,25); plt.ylim(ymin=-5);
    plt.xticks([1, 4.5, 24]); plt.yticks([1, 117.5, 238]);

    # Split lines
    plt.vlines(4.5, ymin=-5, ymax=250, color=&#39;g&#39;)
    plt.hlines(117.5, xmin=4.5, xmax=25, color=&#39;g&#39;)

    # Regions
    plt.annotate(&#39;R1&#39;, xy=(2,117.5), fontsize=&#39;xx-large&#39;)
    plt.annotate(&#39;R2&#39;, xy=(11,60), fontsize=&#39;xx-large&#39;)
    plt.annotate(&#39;R3&#39;, xy=(11,170), fontsize=&#39;xx-large&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt; make_figure_8_2()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/07_trees_19_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;We might &lt;strong&gt;interpret&lt;/strong&gt; the above regression tree as follows: Years is the most important factor in determining Salary, and players with less experience earn lower salaries than more experienced players. Given that a player is less experienced, the number of hits that he made in the previous year seems to play little role in his salary. But among players who have been in the major leagues for five or more years, the number of hits made in the previous year does affect salary, and players who made more hits last year tend to have higher salaries.&lt;/p&gt;
&lt;h3 id=&#34;building-a-tree&#34;&gt;Building a Tree&lt;/h3&gt;
&lt;p&gt;There are two main steps in the construction of a tree:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;We divide the predictor space—that is, the set of possible values for $X_1, X_2, &amp;hellip; , X_p$ into $J$ distinct and non-overlapping regions, $R_1,R_2,&amp;hellip;,R_J$.&lt;/li&gt;
&lt;li&gt;For every observation that falls into the region $R_j$ , we make the same prediction, which is simply the mean of the response values for the training observations in $R_j$.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The second step is easy. But how does one construct the regions? Our purpose is to minimize the Sum of Squared Residuals, across the different regions:&lt;/p&gt;
&lt;p&gt;$$
\sum_{j=1}^{J} \sum_{i \in R_{j}}\left(y_{i}-\hat{y}&lt;em&gt;{R&lt;/em&gt;{j}}\right)^{2}
$$&lt;/p&gt;
&lt;p&gt;Unfortunately, it is computationally infeasible to consider every possible partition of the feature space into $J$ boxes.&lt;/p&gt;
&lt;p&gt;For this reason, we take a &lt;strong&gt;top-down&lt;/strong&gt;, &lt;strong&gt;greedy&lt;/strong&gt; approach that is known as &lt;em&gt;recursive binary splitting&lt;/em&gt;. The approach is top-down because it begins at the top of the tree (at which point all observations belong to a single region) and then successively splits the predictor space; each split is indicated via two new branches further down on the tree. It is greedy because at each step of the tree-building process, the best split is made at that particular step, rather than looking ahead and picking a split that will lead to a better tree in some future step.&lt;/p&gt;
&lt;p&gt;In practice, the method is the following:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;we select the predictor $X_j$&lt;/li&gt;
&lt;li&gt;we select the cutpoint $s$ such that splitting the predictor space into the regions ${X|X_j &amp;lt; s}$ and ${X|X_j \geq s}$ leads to the greatest possible reduction in RSS&lt;/li&gt;
&lt;li&gt;we repeat (1)-(2) for all predictors $X_1, &amp;hellip; , X_p$, i.e. we solve&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;$$
\arg \min_{j,s} \ \sum_{i: x_{i} \in {X|X_j &amp;lt; s}}\left(y_{i}-\hat{y}&lt;em&gt;i\right)^{2}+\sum&lt;/em&gt;{i: x_{i} \in {X|X_j \geq s}}\left(y_{i}-\hat{y}_i\right)^{2}
$$&lt;/p&gt;
&lt;ol start=&#34;4&#34;&gt;
&lt;li&gt;we choose the predictor and cutpoint such that the resulting tree has the lowest RSS&lt;/li&gt;
&lt;li&gt;we keep repeating (1)-(4) until a certain condition is met. However, after the first iteration we also have to pick which region to split which adds a further dimension to optimize over.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Let&amp;rsquo;s build our own &lt;code&gt;Node&lt;/code&gt; class to play around with trees.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;class Node:
    &amp;quot;&amp;quot;&amp;quot;
    Class used to represent nodes in a Regression Tree
    
    Attributes
    ----------
    x : np.array
        independent variables
    y : np.array
        dependent variables
    idxs : np.array
        indexes fo x and y for current node
    depth : int
        depth of the sub-tree (default 5)

    Methods
    -------
    find_next_nodes(self)
        Keep growing the tree
        
    find_best_split(self)
        Find the best split
        
    split(self)
        Split the tree
    &amp;quot;&amp;quot;&amp;quot;
    
    def __init__(self, x, y, idxs, depth=5):
        &amp;quot;&amp;quot;&amp;quot;Initialize node&amp;quot;&amp;quot;&amp;quot;
        self.x = x
        self.y = y
        self.idxs = idxs 
        self.depth = depth
        self.get_next_nodes()

    def get_next_nodes(self):
        &amp;quot;&amp;quot;&amp;quot;If the node is not terminal, get further splits&amp;quot;&amp;quot;&amp;quot;
        if self.is_last_leaf: return 
        self.find_best_split()       
        self.split()             
        
    def find_best_split(self):
        &amp;quot;&amp;quot;&amp;quot;Loop over variables and their values to find the best split&amp;quot;&amp;quot;&amp;quot;
        best_score = float(&#39;inf&#39;)
        # Loop over variables
        for col in range(self.x.shape[1]):
            x = self.x[self.idxs, col]
            # Loop over all splits
            for s in np.unique(x):
                lhs = x &amp;lt;= s
                rhs = x &amp;gt; s
                curr_score = self.get_score(lhs, rhs)
                # If best score, save it 
                if curr_score &amp;lt; best_score: 
                    best_score = curr_score
                    self.split_col = col
                    self.split_val = s
        return self
    
    def get_score(self, lhs, rhs):
        &amp;quot;&amp;quot;&amp;quot;Get score of a given split&amp;quot;&amp;quot;&amp;quot;
        y = self.y[self.idxs]
        lhs_mse = self.get_mse(y[lhs])
        rhs_mse = self.get_mse(y[rhs])
        return lhs_mse * lhs.sum() + rhs_mse * rhs.sum()
        
    def get_mse(self, y): return np.mean((y-np.mean(y))**2)
    
    def split(self): 
        &amp;quot;&amp;quot;&amp;quot;Split a node into 2 sub-nodes (recursive)&amp;quot;&amp;quot;&amp;quot;
        x = self.x[self.idxs, self.split_col]
        lhs = x &amp;lt;= self.split_val
        rhs = x &amp;gt; self.split_val
        self.lhs = Node(self.x, self.y, self.idxs[lhs], self.depth-1)
        self.rhs = Node(self.x, self.y, self.idxs[rhs], self.depth-1)
        to_print = (self.depth, self.split_col, self.split_val, sum(lhs), sum(rhs))
        print(&#39;Split on layer %.0f: var%1.0f = %.4f (%.0f/%.0f)&#39; % to_print)
        return self
    
    @property
    def is_last_leaf(self): return self.depth&amp;lt;=1

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What does a &lt;code&gt;Node&lt;/code&gt; look like?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Init first node
tree1 = Node(X, y, np.arange(len(y)), 1)

# Documentation (always comment and document your code!)
print(tree1.__doc__)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;    Class used to represent nodes in a Regression Tree
    
    Attributes
    ----------
    x : np.array
        independent variables
    y : np.array
        dependent variables
    idxs : np.array
        indexes fo x and y for current node
    depth : int
        depth of the sub-tree (default 5)

    Methods
    -------
    find_next_nodes(self)
        Keep growing the tree
        
    find_best_split(self)
        Find the best split
        
    split(self)
        Split the tree
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Which properties does it have?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Inspect the class
dir(tree1)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[&#39;__class__&#39;,
 &#39;__delattr__&#39;,
 &#39;__dict__&#39;,
 &#39;__dir__&#39;,
 &#39;__doc__&#39;,
 &#39;__eq__&#39;,
 &#39;__format__&#39;,
 &#39;__ge__&#39;,
 &#39;__getattribute__&#39;,
 &#39;__gt__&#39;,
 &#39;__hash__&#39;,
 &#39;__init__&#39;,
 &#39;__init_subclass__&#39;,
 &#39;__le__&#39;,
 &#39;__lt__&#39;,
 &#39;__module__&#39;,
 &#39;__ne__&#39;,
 &#39;__new__&#39;,
 &#39;__reduce__&#39;,
 &#39;__reduce_ex__&#39;,
 &#39;__repr__&#39;,
 &#39;__setattr__&#39;,
 &#39;__sizeof__&#39;,
 &#39;__str__&#39;,
 &#39;__subclasshook__&#39;,
 &#39;__weakref__&#39;,
 &#39;depth&#39;,
 &#39;find_best_split&#39;,
 &#39;get_mse&#39;,
 &#39;get_next_nodes&#39;,
 &#39;get_score&#39;,
 &#39;idxs&#39;,
 &#39;is_last_leaf&#39;,
 &#39;split&#39;,
 &#39;x&#39;,
 &#39;y&#39;]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What is the depth? How many observations are there?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Get info
print(&#39;Tree of depth %.0f with %.0f observations&#39; % (tree1.depth, len(tree1.idxs)))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Tree of depth 1 with 263 observations
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Fair enough, the tree is just a single leaf.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Check if terminal
tree1.is_last_leaf
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;True
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s find the first split.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Find best split
tree1.find_best_split()
print(&#39;Split at var%1.0f = %.4f&#39; % (tree1.split_col, tree1.split_val))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Split at var0 = 4.0000
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If has selected the first variable, at the value $4$.&lt;/p&gt;
&lt;p&gt;If we call the &lt;code&gt;split&lt;/code&gt; function, it will also tell us how many observations per leaf the split generates.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Split tree
tree1.split();
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Split on layer 1: var0 = 4.0000 (90/173)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we are ready to compute even deeper trees&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Check depth-3 tree
tree3 = Node(X, y, np.arange(len(y)), 3)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Split on layer 2: var1 = 4.0000 (2/88)
Split on layer 2: var1 = 117.0000 (90/83)
Split on layer 3: var0 = 4.0000 (90/173)
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;pruning&#34;&gt;Pruning&lt;/h3&gt;
&lt;p&gt;The process described above may produce good predictions on the training set, but is likely to &lt;strong&gt;overfit&lt;/strong&gt; the data, leading to poor test set performance. This is because the resulting tree might be too complex. A smaller tree with fewer splits might lead to lower variance and better interpretation at the cost of a little bias.&lt;/p&gt;
&lt;p&gt;We can see it happening if we build the same tree as above, but with 5 leaves.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Compute tree
overfit_tree = DecisionTreeRegressor(max_leaf_nodes=5).fit(X, y)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We plot the 5-leaf tree.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Plot tree
fig, ax = plt.subplots(1,1)
plot_tree(overfit_tree, filled=True, feature_names=features, fontsize=14, ax=ax);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/07_trees_45_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;split on the far left&lt;/strong&gt; is predicting a very high &lt;code&gt;Salary&lt;/code&gt; (7.243) for players with few &lt;code&gt;Years&lt;/code&gt; of experience and few &lt;code&gt;Hits&lt;/code&gt;. Indeed this prediction is based on an extremely tiny subsample (2). They are probably outliers and our tree is most likely overfitting.&lt;/p&gt;
&lt;p&gt;One possible alternative is to insert a minimum number of observation per leaf.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Compute tree
no_overfit_tree = DecisionTreeRegressor(max_leaf_nodes=5, min_samples_leaf=10).fit(X, y)

# Plot tree
fig, ax = plt.subplots(1,1)
plot_tree(no_overfit_tree, filled=True, feature_names=features, fontsize=14, ax=ax);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/07_trees_48_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Now the tree makes much more sense: the lower the &lt;code&gt;Years&lt;/code&gt; and the &lt;code&gt;Hits&lt;/code&gt;, the lower the predicted &lt;code&gt;Salary&lt;/code&gt; as we can see from the shades getting darker and darker as we move left to right&lt;/p&gt;
&lt;p&gt;Another possible alternative to the process described above is to build the tree only so long as the decrease in the RSS due to each split exceeds some (high) threshold.&lt;/p&gt;
&lt;p&gt;This strategy will result in smaller trees, but is too short-sighted since a seemingly worthless split early on in the tree might be followed by a very good split—that is, a split that leads to a large reduction in RSS later on.&lt;/p&gt;
&lt;p&gt;We can use cross-validation to pick the optimal tree length.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Import original split
features = [&#39;Years&#39;, &#39;Hits&#39;, &#39;RBI&#39;, &#39;PutOuts&#39;, &#39;Walks&#39;, &#39;Runs&#39;, &#39;AtBat&#39;, &#39;HmRun&#39;]
X_train = pd.read_csv(&#39;data/Hitters_X_train.csv&#39;).dropna()[features]
X_test = pd.read_csv(&#39;data/Hitters_X_test.csv&#39;).dropna()[features]
y_train = pd.read_csv(&#39;data/Hitters_y_train.csv&#39;).dropna()
y_test = pd.read_csv(&#39;data/Hitters_y_test.csv&#39;).dropna()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Init
params = range(2,11)
reg_scores = np.zeros((len(params),3))
best_score = 10**6

# Loop over all parameters
for i,k in enumerate(params):
    
    # Model
    tree = DecisionTreeRegressor(max_leaf_nodes=k)

    # Loop over splits
    tree.fit(X_train, y_train)
    reg_scores[i,0] = mean_squared_error(tree.predict(X_train), y_train)
    reg_scores[i,1] = mean_squared_error(tree.predict(X_test), y_test)

    # Get CV score
    kf6 = KFold(n_splits=6)
    reg_scores[i,2] = -cross_val_score(tree, X_train, y_train, cv=kf6, scoring=&#39;neg_mean_squared_error&#39;).mean()
    
    # Save best model
    if reg_scores[i,2]&amp;lt;best_score:
        best_model = tree
        best_score = reg_scores[i,2]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s plot the optimal tree depth, using 6-fold cv.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Figure 8.5
def make_figure_8_5():
    
    # Init
    fig, (ax1,ax2) = plt.subplots(1,2,figsize=(16,6))
    fig.suptitle(&#39;Figure 8.5&#39;)

    # Plot scores
    ax1.plot(params, reg_scores);
    ax1.axvline(params[np.argmin(reg_scores[:,2])], c=&#39;k&#39;, ls=&#39;--&#39;)
    ax1.legend([&#39;Train&#39;,&#39;Test&#39;,&#39;6-fold CV&#39;]);
    ax1.set_title(&#39;Cross-Validation Scores&#39;);

    # Plot best tree
    plot_tree(best_model, filled=True, impurity=False, feature_names=features, fontsize=12, ax=ax2);
    ax2.set_title(&#39;Best Model&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;make_figure_8_5()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/07_trees_55_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;The optimal tree has 4 leaves.&lt;/p&gt;
&lt;h3 id=&#34;classification-trees&#34;&gt;Classification Trees&lt;/h3&gt;
&lt;p&gt;A classification tree is very similar to a regression tree, except that it is used to predict a qualitative response rather than a quantitative one.&lt;/p&gt;
&lt;p&gt;For a classification tree, we predict that each observation belongs to the most commonly occurring class of training observations in the region to which it belongs.&lt;/p&gt;
&lt;h3 id=&#34;building-a-classification-tree&#34;&gt;Building a Classification Tree&lt;/h3&gt;
&lt;p&gt;The task of growing a classification tree is similar to the task of growing a regression tree. However, in the classification setting, RSS cannot be used as a criterion for making the binary splits.&lt;/p&gt;
&lt;p&gt;We define $\hat p_{mk}$ as the proportion of training observations in the $m^{th}$ region that are from the $k^{th}$ class. Possible loss functions to decide the splits are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Classification error rate&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$
E = 1 - \max &lt;em&gt;{k}\left(\hat{p}&lt;/em&gt;{m k}\right)
$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Gini index&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$
G=\sum_{k=1}^{K} \hat{p}&lt;em&gt;{m k}\left(1-\hat{p}&lt;/em&gt;{m k}\right)
$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Entropy&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$
D=-\sum_{k=1}^{K} \hat{p}&lt;em&gt;{m k} \log \hat{p}&lt;/em&gt;{m k}
$$&lt;/p&gt;
&lt;p&gt;In 2-class classification problems, this is what the different scores look like, for different proportions of class 2 ($p$), when the true proportion is $p_0 =0.5$.&lt;/p&gt;
&lt;img src=&#34;figures/impurity.png&#34; alt=&#34;Drawing&#34; style=&#34;width: 600px;&#34;/&gt;
&lt;p&gt;When building a classification tree, either the Gini index or the entropy are typically used to evaluate the quality of a particular split, since these two approaches are more sensitive to node purity than is the classification error rate.&lt;/p&gt;
&lt;p&gt;For this section we will work with the &lt;code&gt;Heart&lt;/code&gt; dataset on individual heart failures. We will try to use individual characteristics in order to predict heart deseases (&lt;code&gt;HD&lt;/code&gt;). The varaible is binary: Yes, No.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Load heart dataset
heart = pd.read_csv(&#39;data/Heart.csv&#39;).drop(&#39;Unnamed: 0&#39;, axis=1).dropna()
heart.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;Age&lt;/th&gt;
      &lt;th&gt;Sex&lt;/th&gt;
      &lt;th&gt;ChestPain&lt;/th&gt;
      &lt;th&gt;RestBP&lt;/th&gt;
      &lt;th&gt;Chol&lt;/th&gt;
      &lt;th&gt;Fbs&lt;/th&gt;
      &lt;th&gt;RestECG&lt;/th&gt;
      &lt;th&gt;MaxHR&lt;/th&gt;
      &lt;th&gt;ExAng&lt;/th&gt;
      &lt;th&gt;Oldpeak&lt;/th&gt;
      &lt;th&gt;Slope&lt;/th&gt;
      &lt;th&gt;Ca&lt;/th&gt;
      &lt;th&gt;Thal&lt;/th&gt;
      &lt;th&gt;AHD&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;63&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;typical&lt;/td&gt;
      &lt;td&gt;145&lt;/td&gt;
      &lt;td&gt;233&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;150&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;2.3&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;fixed&lt;/td&gt;
      &lt;td&gt;No&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;67&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;asymptomatic&lt;/td&gt;
      &lt;td&gt;160&lt;/td&gt;
      &lt;td&gt;286&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;108&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1.5&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;3.0&lt;/td&gt;
      &lt;td&gt;normal&lt;/td&gt;
      &lt;td&gt;Yes&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;67&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;asymptomatic&lt;/td&gt;
      &lt;td&gt;120&lt;/td&gt;
      &lt;td&gt;229&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;129&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;2.6&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;2.0&lt;/td&gt;
      &lt;td&gt;reversable&lt;/td&gt;
      &lt;td&gt;Yes&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;37&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;nonanginal&lt;/td&gt;
      &lt;td&gt;130&lt;/td&gt;
      &lt;td&gt;250&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;187&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;3.5&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;normal&lt;/td&gt;
      &lt;td&gt;No&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;41&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;nontypical&lt;/td&gt;
      &lt;td&gt;130&lt;/td&gt;
      &lt;td&gt;204&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;172&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1.4&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;normal&lt;/td&gt;
      &lt;td&gt;No&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Fastorize variables
heart.ChestPain = pd.factorize(heart.ChestPain)[0]
heart.Thal = pd.factorize(heart.Thal)[0]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Set features
features = [col for col in heart.columns if col!=&#39;AHD&#39;]
X2 = heart[features]
y2 = pd.factorize(heart.AHD)[0]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We now fit our classifier.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Fit classification tree
clf = DecisionTreeClassifier(max_depth=None, max_leaf_nodes=11)
clf.fit(X2,y2)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;DecisionTreeClassifier(max_leaf_nodes=11)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What is the score?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Final score
clf.score(X2,y2)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;0.8686868686868687
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s have a look at the whole tree.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Figure 8.6 a
def make_fig_8_6a():
    
    # Init
    fig, ax = plt.subplots(1,1, figsize=(16,12))
    ax.set_title(&#39;Figure 8.6&#39;);

    # Plot tree
    plot_tree(clf, filled=True, feature_names=features, class_names=[&#39;No&#39;,&#39;Yes&#39;], fontsize=12, ax=ax);
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;make_fig_8_6a()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/07_trees_72_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;This figure has a surprising characteristic: some of the splits yield two terminal nodes that have the same predicted value.&lt;/p&gt;
&lt;p&gt;For instance, consider the split &lt;code&gt;Age&lt;/code&gt;&amp;lt;=57.5 near the bottom left of the unpruned tree. Regardless of the value of &lt;code&gt;Age&lt;/code&gt;, a response value of &lt;em&gt;No&lt;/em&gt; is predicted for those observations. Why, then, is the split performed at all?&lt;/p&gt;
&lt;p&gt;The split is performed because it leads to increased node purity. That is, 2/81 of the observations corresponding to the left-hand leaf have a response value of &lt;em&gt;Yes&lt;/em&gt;, whereas 9/36 of those corresponding to the right-hand leaf have a response value of &lt;em&gt;Yes&lt;/em&gt;. Why is node purity important? Suppose that we have a test observation that belongs to the region given by that left-hand leaf. Then we can be pretty certain that its response value is &lt;em&gt;No&lt;/em&gt;. In contrast, if a test observation belongs to the region given by the right-hand leaf, then its response value is probably &lt;em&gt;No&lt;/em&gt;, but we are much less certain. Even though the split &lt;code&gt;Age&lt;/code&gt;&amp;lt;=57.5 does not reduce the classification error, it improves the Gini index and the entropy, which are more sensitive to node purity.&lt;/p&gt;
&lt;h3 id=&#34;pruning-for-classification&#34;&gt;Pruning for Classification&lt;/h3&gt;
&lt;p&gt;We can repeat the pruning exercise also for the classification task.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Figure 8.6 b
def make_figure_8_6b():
    
    # Init
    fig, (ax1, ax2) = plt.subplots(1,2,figsize=(14,6))
    fig.suptitle(&#39;Figure 8.6&#39;)

    # Plot scores
    ax1.plot(params, clf_scores);
    ax1.legend([&#39;Train&#39;,&#39;Test&#39;,&#39;6-fold CV&#39;]);

    # Plot best tree
    plot_tree(best_model, filled=True, impurity=False, feature_names=features, fontsize=12, ax=ax2);
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Init
J = 10
params = range(2,11)
clf_scores = np.zeros((len(params),3))
best_score = 100

# Loop over all parameters
for i,k in enumerate(params):
    
    # Model
    tree = DecisionTreeClassifier(max_leaf_nodes=k)
    
    # Loop J times
    temp_scores = np.zeros((J,3))
    for j in range (J):

        # Loop over splits
        X2_train, X2_test, y2_train, y2_test = train_test_split(X2, y2, test_size=0.5, random_state=j)
        m = tree.fit(X2_train, y2_train)
        temp_scores[j,0] = mean_squared_error(m.predict(X2_train), y2_train)
        temp_scores[j,1] = mean_squared_error(m.predict(X2_test), y2_test)

        # Get CV score
        kf6 = KFold(n_splits=6)
        temp_scores[j,2] = -cross_val_score(tree, X2_train, y2_train, cv=kf6, scoring=&#39;neg_mean_squared_error&#39;).mean()
        
        # Save best model
        if temp_scores[j,2]&amp;lt;best_score:
            best_model = m
            best_score = temp_scores[j,2]
        
    # Average
    clf_scores[i,:] = np.mean(temp_scores, axis=0)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;make_figure_8_6b()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/07_trees_78_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;other-issues&#34;&gt;Other Issues&lt;/h3&gt;
&lt;h4 id=&#34;missing-predictor-values&#34;&gt;Missing Predictor Values&lt;/h4&gt;
&lt;p&gt;There are usually 2 main ways to deal with missing values:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;discard the observations&lt;/li&gt;
&lt;li&gt;fill the missing values with predictions using the other observations (e.g. mean)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;With trees we can do better:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;code them as a separate class (e.g. &amp;lsquo;missing&amp;rsquo;)&lt;/li&gt;
&lt;li&gt;generate splits using non-missing data and use non-missing variables on missing data to mimic the splits with missing data&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;categorical-predictors&#34;&gt;Categorical Predictors&lt;/h4&gt;
&lt;p&gt;When splitting a predictor having q possible unordered values, there are $2^{q−1} − 1$ possible partitions of the q values into two groups, and the computations become prohibitive for large $q$. However, with a $0 − 1$ outcome, this computation simplifies.&lt;/p&gt;
&lt;h4 id=&#34;linear-combination-splits&#34;&gt;Linear Combination Splits&lt;/h4&gt;
&lt;p&gt;Rather than restricting splits to be of the form $X_j \leq s$, one can allow splits along linear combinations of the form $a_j X_j \leq s$. The weights $a_j$ become part of the optimization procedure.&lt;/p&gt;
&lt;h4 id=&#34;other-tree-building-procedures&#34;&gt;Other Tree-Building Procedures&lt;/h4&gt;
&lt;p&gt;The procedure we have seen for building trees is called CART (Classification and Regression Tree). There are other procedures.&lt;/p&gt;
&lt;h4 id=&#34;the-loss-matrix&#34;&gt;The Loss Matrix&lt;/h4&gt;
&lt;p&gt;With respect to other methods, the choice of the loss functions plays a much more important role.&lt;/p&gt;
&lt;h4 id=&#34;binary-splits&#34;&gt;Binary Splits&lt;/h4&gt;
&lt;p&gt;You can do non-binary splits but in the end they are just weaker versions of binary splits.&lt;/p&gt;
&lt;h4 id=&#34;instability&#34;&gt;Instability&lt;/h4&gt;
&lt;p&gt;Trees have very &lt;strong&gt;high variance&lt;/strong&gt;.&lt;/p&gt;
&lt;h4 id=&#34;difficulty-in-capturing-additive-structure&#34;&gt;Difficulty in Capturing Additive Structure&lt;/h4&gt;
&lt;p&gt;Trees are quite bad at modeling additive structures.&lt;/p&gt;
&lt;h4 id=&#34;lack-of-smoothness&#34;&gt;Lack of Smoothness&lt;/h4&gt;
&lt;p&gt;Trees are not smooth.&lt;/p&gt;
&lt;h3 id=&#34;trees-vs-regression&#34;&gt;Trees vs Regression&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Advantages&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Trees are very easy to explain to people. In fact, they are even easier to explain than linear regression!&lt;/li&gt;
&lt;li&gt;Some people believe that decision trees more closely mirror human decision-making than do the regression and classification approaches seen in previous chapters.&lt;/li&gt;
&lt;li&gt;Trees can be displayed graphically, and are easily interpreted even by a non-expert (especially if they are small).&lt;/li&gt;
&lt;li&gt;Trees can easily handle qualitative predictors without the need to create dummy variables.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Disadvantages&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;trees generally do not have the same level of predictive accuracy as some of the other regression and classification approaches seen in this book.&lt;/li&gt;
&lt;li&gt;trees can be very non-robust. In other words, a small change in the data can cause a large change in the final estimated tree.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;72-bagging-random-forests-boosting&#34;&gt;7.2 Bagging, Random Forests, Boosting&lt;/h2&gt;
&lt;p&gt;Bagging, random forests, and boosting use trees as building blocks to construct more powerful prediction models.&lt;/p&gt;
&lt;h3 id=&#34;bagging&#34;&gt;Bagging&lt;/h3&gt;
&lt;p&gt;The main problem of decision trees is that they suffer from &lt;strong&gt;high variance&lt;/strong&gt;. &lt;em&gt;Bootstrap aggregation&lt;/em&gt;, or &lt;em&gt;bagging&lt;/em&gt;, is a general-purpose procedure for reducing the variance of a statistical learning method.&lt;/p&gt;
&lt;p&gt;The main idea behind &lt;em&gt;bagging&lt;/em&gt; is that, given a set of n independent observations $Z_1,&amp;hellip;,Z_n$, each with variance $\sigma^2$, the variance of the mean $\bar Z$ of the observations is given by $\sigma^2/n$. In other words, averaging a set of observations reduces variance.&lt;/p&gt;
&lt;p&gt;Indeed &lt;em&gt;bagging&lt;/em&gt; consists in taking many training sets from the population, build a separate prediction model using each training set, and &lt;strong&gt;average the resulting predictions&lt;/strong&gt;. Since we do not have access to many training sets, we resort to bootstrapping.&lt;/p&gt;
&lt;h3 id=&#34;out-of-bag-error-estimation&#34;&gt;Out-of-Bag Error Estimation&lt;/h3&gt;
&lt;p&gt;It turns out that there is a very straightforward way to estimate the test error of a bagged model, without the need to perform cross-validation or the validation set approach. Recall that the key to bagging is that trees are repeatedly fit to bootstrapped subsets of the observations. One can show that on average, each bagged tree makes use of around two-thirds of the observations. The remaining one-third of the observations not used to fit a given bagged tree are referred to as the out-of-bag (OOB) observations. We can predict the response for the ith observation using each of the trees in which that observation was OOB.&lt;/p&gt;
&lt;p&gt;We are now going to compute the Gini index for the &lt;code&gt;Heart&lt;/code&gt; dataset using different numbers of trees.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Init (takes a lot of time with J=30)
params = range(2,50)
bagging_scores = np.zeros((len(params),2))
J = 30;

# Loop over parameters
for i, k in enumerate(params):
    print(&amp;quot;Computing k=%1.0f&amp;quot; % k, end =&amp;quot;&amp;quot;)
    
    # Repeat J 
    temp_scores = np.zeros((J,2))
    for j in range(J):
        X2_train, X2_test, y2_train, y2_test = train_test_split(X2, y2, test_size=0.5, random_state=j)
        bagging = BaggingClassifier(DecisionTreeClassifier(), max_samples=k, oob_score=True)
        bagging.fit(X2_train,y2_train)
        temp_scores[j,0] = bagging.score(X2_test, y2_test)
        temp_scores[j,1] = bagging.oob_score_
        
    # Average
    bagging_scores[i,:] = np.mean(temp_scores, axis=0)
    print(&amp;quot;&amp;quot;, end=&amp;quot;\r&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Computing k=49
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s plot the Out-of-Bag error computed while generating the bagged estimator.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Make new figure 1
def make_new_figure_1():
    
    # Init
    fig, ax = plt.subplots(1,1,figsize=(10,6))
    fig.suptitle(&amp;quot;Estimated $R^2$&amp;quot;)

    # Plot scores
    ax.plot(params, bagging_scores);
    ax.legend([&#39;Test&#39;,&#39;OOB&#39;]);
    ax.set_xlabel(&#39;Number of Trees&#39;); ax.set_ylabel(&#39;R^2&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;make_new_figure_1()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/07_trees_94_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;It can be shown that with B sufficiently large, OOB error is virtually equivalent to leave-one-out cross-validation error. The OOB approach for estimating the test error is particularly convenient when performing bagging on large data sets for which cross-validation would be computationally onerous.&lt;/p&gt;
&lt;h3 id=&#34;variable-importance-measures&#34;&gt;Variable Importance Measures&lt;/h3&gt;
&lt;p&gt;As we have discussed, the main advantage of bagging is to reduce prediction variance. However, with bagging it can be &lt;strong&gt;difficult to interpret&lt;/strong&gt; the resulting model. In fact we cannot draw trees anymore given we have too many of them.&lt;/p&gt;
&lt;p&gt;However, one can obtain an overall summary of the importance of each predictor using the RSS (for bagging regression trees) or the Gini index (for bagging classification trees). In the case of bagging regression trees, we can record the total amount that the RSS is decreased due to splits over a given predictor, averaged over all trees. A large value indicates an important predictor. Similarly, in the context of bagging classification trees, we can add up the total amount that the Gini index is decreased by splits over a given predictor, averaged over all trees.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Compute feature importance
feature_importances = np.mean([tree.feature_importances_ for tree in bagging.estimators_], axis=0)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can have a look at the importance of each feature.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Figure 8.9
def make_figure_8_9():
    
    # Init
    fig, ax = plt.subplots(1,1,figsize=(8,8))
    ax.set_title(&#39;Figure 8.9: Feature Importance&#39;);

    # Plot feature importance
    h1 = pd.DataFrame({&#39;Importance&#39;:feature_importances*100}, index=features)
    h1 = h1.sort_values(by=&#39;Importance&#39;, axis=0, ascending=False)
    h1.plot(kind=&#39;barh&#39;, color=&#39;r&#39;, ax=ax)
    ax.set_xlabel(&#39;Variable Importance&#39;); 
    plt.yticks(fontsize=14);
    plt.gca().legend_ = None;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;make_figure_8_9()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/07_trees_101_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;random-forests&#34;&gt;Random Forests&lt;/h3&gt;
&lt;p&gt;Random forests provide an improvement over bagged trees by way of a &lt;strong&gt;small tweak that decorrelates the trees&lt;/strong&gt;. As in bagging, we build a number of decision trees on bootstrapped training samples. But when building these decision trees, each time a split in a tree is considered, a random sample of $m$ predictors is chosen as split candidates from the full set of $p$ predictors. The split is allowed to use only one of those m predictors. A fresh sample of $m$ predictors is taken at each split, and typically we choose $m \sim \sqrt{p}$ — that is, the number of predictors considered at each split is approximately equal to the square root of the total number of predictors&lt;/p&gt;
&lt;p&gt;In other words, in building a random forest, at each split in the tree, the algorithm is not even allowed to consider a majority of the available predictors. This may sound crazy, but it has a clever rationale. Suppose that there is one very strong predictor in the data set, along with a number of other moderately strong predictors. Then in the collection of bagged trees, most or all of the trees will use this strong predictor in the top split. Consequently, all of the bagged trees will look quite similar to each other. Hence the predictions from the bagged trees will be highly correlated. Unfortunately, averaging many highly correlated quantities does not lead to as large of a reduction in variance as averaging many uncorrelated quantities. In particular, this means that bagging will not lead to a substantial reduction in variance over a single tree in this setting.&lt;/p&gt;
&lt;p&gt;Random forests overcome this problem by &lt;strong&gt;forcing each split to consider only a subset of the predictors&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s split the data in 2 and compute test and estimated $R^2$, for both forest and trees.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import warnings
warnings.simplefilter(&#39;ignore&#39;)

# Init (takes a lot of time with J=30)
params = range(2,50)
forest_scores = np.zeros((len(params),2))
J = 30

# Loop over parameters
for i, k in enumerate(params):
    print(&amp;quot;Computing k=%1.0f&amp;quot; % k, end =&amp;quot;&amp;quot;)
    
    # Repeat J 
    temp_scores = np.zeros((J,2))
    for j in range(J):
        X2_train, X2_test, y2_train, y2_test = train_test_split(X2, y2, test_size=0.5, random_state=j)
        forest = RandomForestClassifier(n_estimators=k, oob_score=True, max_features=&amp;quot;sqrt&amp;quot;)
        forest.fit(X2_train,y2_train)
        temp_scores[j,0] = forest.score(X2_test, y2_test)
        temp_scores[j,1] = forest.oob_score_
        
    # Average
    forest_scores[i,:] = np.mean(temp_scores, axis=0)
    print(&amp;quot;&amp;quot;, end=&amp;quot;\r&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Computing k=49
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Figure 8.8
def make_figure_8_8():
    
    # Init
    fig, ax = plt.subplots(1,1,figsize=(10,6))
    ax.set_title(&#39;Figure 8.8&#39;);

    # Plot scores
    ax.plot(params, bagging_scores);
    ax.plot(params, forest_scores);
    ax.legend([&#39;Test - Bagging&#39;,&#39;OOB - Bagging&#39;, &#39;Test - Forest&#39;,&#39;OOB - Forest&#39;]);
    ax.set_xlabel(&#39;Number of Trees&#39;); ax.set_ylabel(&#39;R^2&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;make_figure_8_8()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/07_trees_108_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;As for bagging, we can plot feature importance.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Make new figure 2
def make_new_figure_2():
    
    # Init
    fig, (ax1,ax2) = plt.subplots(1,2,figsize=(15,7))

    # Plot feature importance - Bagging
    h1 = pd.DataFrame({&#39;Importance&#39;:feature_importances*100}, index=features)
    h1 = h1.sort_values(by=&#39;Importance&#39;, axis=0, ascending=False)
    h1.plot(kind=&#39;barh&#39;, color=&#39;r&#39;, ax=ax1)
    ax1.set_xlabel(&#39;Variable Importance&#39;); 
    ax1.set_title(&#39;Tree Bagging&#39;)

    # Plot feature importance
    h2 = pd.DataFrame({&#39;Importance&#39;:forest.feature_importances_*100}, index=features)
    h2 = h2.sort_values(by=&#39;Importance&#39;, axis=0, ascending=False)
    h2.plot(kind=&#39;barh&#39;, color=&#39;r&#39;, ax=ax2)
    ax2.set_title(&#39;Random Forest&#39;)

    # All plots
    for ax in fig.axes:
        ax.set_xlabel(&#39;Variable Importance&#39;); 
        ax.legend([])
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;make_new_figure_2()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/07_trees_111_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;From the figure we observe that varaible importance ranking is similar with bagging and random forests, but there are significant differences.&lt;/p&gt;
&lt;p&gt;We are now going to look at the importance of random forests using the &lt;code&gt;Khan&lt;/code&gt; gene dataset. This dataset has the peculiarity of having a large number of features and very few observations.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Load data
gene = pd.read_csv(&#39;data/Khan.csv&#39;)
print(len(gene))
gene.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;83
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;x&lt;/th&gt;
      &lt;th&gt;V1&lt;/th&gt;
      &lt;th&gt;V2&lt;/th&gt;
      &lt;th&gt;V3&lt;/th&gt;
      &lt;th&gt;V4&lt;/th&gt;
      &lt;th&gt;V5&lt;/th&gt;
      &lt;th&gt;V6&lt;/th&gt;
      &lt;th&gt;V7&lt;/th&gt;
      &lt;th&gt;V8&lt;/th&gt;
      &lt;th&gt;V9&lt;/th&gt;
      &lt;th&gt;...&lt;/th&gt;
      &lt;th&gt;V2299&lt;/th&gt;
      &lt;th&gt;V2300&lt;/th&gt;
      &lt;th&gt;V2301&lt;/th&gt;
      &lt;th&gt;V2302&lt;/th&gt;
      &lt;th&gt;V2303&lt;/th&gt;
      &lt;th&gt;V2304&lt;/th&gt;
      &lt;th&gt;V2305&lt;/th&gt;
      &lt;th&gt;V2306&lt;/th&gt;
      &lt;th&gt;V2307&lt;/th&gt;
      &lt;th&gt;V2308&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;0.773344&lt;/td&gt;
      &lt;td&gt;-2.438405&lt;/td&gt;
      &lt;td&gt;-0.482562&lt;/td&gt;
      &lt;td&gt;-2.721135&lt;/td&gt;
      &lt;td&gt;-1.217058&lt;/td&gt;
      &lt;td&gt;0.827809&lt;/td&gt;
      &lt;td&gt;1.342604&lt;/td&gt;
      &lt;td&gt;0.057042&lt;/td&gt;
      &lt;td&gt;0.133569&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;-0.238511&lt;/td&gt;
      &lt;td&gt;-0.027474&lt;/td&gt;
      &lt;td&gt;-1.660205&lt;/td&gt;
      &lt;td&gt;0.588231&lt;/td&gt;
      &lt;td&gt;-0.463624&lt;/td&gt;
      &lt;td&gt;-3.952845&lt;/td&gt;
      &lt;td&gt;-5.496768&lt;/td&gt;
      &lt;td&gt;-1.414282&lt;/td&gt;
      &lt;td&gt;-0.647600&lt;/td&gt;
      &lt;td&gt;-1.763172&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;-0.078178&lt;/td&gt;
      &lt;td&gt;-2.415754&lt;/td&gt;
      &lt;td&gt;0.412772&lt;/td&gt;
      &lt;td&gt;-2.825146&lt;/td&gt;
      &lt;td&gt;-0.626236&lt;/td&gt;
      &lt;td&gt;0.054488&lt;/td&gt;
      &lt;td&gt;1.429498&lt;/td&gt;
      &lt;td&gt;-0.120249&lt;/td&gt;
      &lt;td&gt;0.456792&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;-0.657394&lt;/td&gt;
      &lt;td&gt;-0.246284&lt;/td&gt;
      &lt;td&gt;-0.836325&lt;/td&gt;
      &lt;td&gt;-0.571284&lt;/td&gt;
      &lt;td&gt;0.034788&lt;/td&gt;
      &lt;td&gt;-2.478130&lt;/td&gt;
      &lt;td&gt;-3.661264&lt;/td&gt;
      &lt;td&gt;-1.093923&lt;/td&gt;
      &lt;td&gt;-1.209320&lt;/td&gt;
      &lt;td&gt;-0.824395&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;-0.084469&lt;/td&gt;
      &lt;td&gt;-1.649739&lt;/td&gt;
      &lt;td&gt;-0.241308&lt;/td&gt;
      &lt;td&gt;-2.875286&lt;/td&gt;
      &lt;td&gt;-0.889405&lt;/td&gt;
      &lt;td&gt;-0.027474&lt;/td&gt;
      &lt;td&gt;1.159300&lt;/td&gt;
      &lt;td&gt;0.015676&lt;/td&gt;
      &lt;td&gt;0.191942&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;-0.696352&lt;/td&gt;
      &lt;td&gt;0.024985&lt;/td&gt;
      &lt;td&gt;-1.059872&lt;/td&gt;
      &lt;td&gt;-0.403767&lt;/td&gt;
      &lt;td&gt;-0.678653&lt;/td&gt;
      &lt;td&gt;-2.939352&lt;/td&gt;
      &lt;td&gt;-2.736450&lt;/td&gt;
      &lt;td&gt;-1.965399&lt;/td&gt;
      &lt;td&gt;-0.805868&lt;/td&gt;
      &lt;td&gt;-1.139434&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;0.965614&lt;/td&gt;
      &lt;td&gt;-2.380547&lt;/td&gt;
      &lt;td&gt;0.625297&lt;/td&gt;
      &lt;td&gt;-1.741256&lt;/td&gt;
      &lt;td&gt;-0.845366&lt;/td&gt;
      &lt;td&gt;0.949687&lt;/td&gt;
      &lt;td&gt;1.093801&lt;/td&gt;
      &lt;td&gt;0.819736&lt;/td&gt;
      &lt;td&gt;-0.284620&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;0.259746&lt;/td&gt;
      &lt;td&gt;0.357115&lt;/td&gt;
      &lt;td&gt;-1.893128&lt;/td&gt;
      &lt;td&gt;0.255107&lt;/td&gt;
      &lt;td&gt;0.163309&lt;/td&gt;
      &lt;td&gt;-1.021929&lt;/td&gt;
      &lt;td&gt;-2.077843&lt;/td&gt;
      &lt;td&gt;-1.127629&lt;/td&gt;
      &lt;td&gt;0.331531&lt;/td&gt;
      &lt;td&gt;-2.179483&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;0.075664&lt;/td&gt;
      &lt;td&gt;-1.728785&lt;/td&gt;
      &lt;td&gt;0.852626&lt;/td&gt;
      &lt;td&gt;0.272695&lt;/td&gt;
      &lt;td&gt;-1.841370&lt;/td&gt;
      &lt;td&gt;0.327936&lt;/td&gt;
      &lt;td&gt;1.251219&lt;/td&gt;
      &lt;td&gt;0.771450&lt;/td&gt;
      &lt;td&gt;0.030917&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;-0.200404&lt;/td&gt;
      &lt;td&gt;0.061753&lt;/td&gt;
      &lt;td&gt;-2.273998&lt;/td&gt;
      &lt;td&gt;-0.039365&lt;/td&gt;
      &lt;td&gt;0.368801&lt;/td&gt;
      &lt;td&gt;-2.566551&lt;/td&gt;
      &lt;td&gt;-1.675044&lt;/td&gt;
      &lt;td&gt;-1.082050&lt;/td&gt;
      &lt;td&gt;-0.965218&lt;/td&gt;
      &lt;td&gt;-1.836966&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;5 rows × 2309 columns&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;The dataset has 83 rows and 2309 columns.&lt;/p&gt;
&lt;p&gt;Since it&amp;rsquo;s a very &lt;em&gt;wide&lt;/em&gt; dataset, selecting the right features is crucial.&lt;/p&gt;
&lt;p&gt;Also note that we cannot run linear regression on this dataset.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Reduce dataset size
gene_small = gene.iloc[:,0:202]
X = gene_small.iloc[:,1:]
y = gene_small.iloc[:,0]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s now cross-validate over number of trees and maximum number of features considered.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Init (takes a lot of time with J=30)
params = range(50,150,10)
m_scores = np.zeros((len(params),3))
p = np.shape(X)[1]
J = 30;

# Loop over parameters
for i, k in enumerate(params):
    
    # Array of features
    ms = [round(p/2), round(np.sqrt(p)), round(np.log(p))]
    
    # Repeat L times
    temp_scores = np.zeros((J,3))
    for j in range(J):
        print(&amp;quot;Computing k=%1.0f (iter=%1.0f)&amp;quot; % (k,j+1), end =&amp;quot;&amp;quot;)
    
        # Loop over values of m
        for index, m in enumerate(ms):
            forest = RandomForestClassifier(n_estimators=k, max_features=m, oob_score=True)
            forest.fit(X, y)
            temp_scores[j,index] = forest.oob_score_
        print(&amp;quot;&amp;quot;, end=&amp;quot;\r&amp;quot;)
            
    # Average
    m_scores[i,:] = np.mean(temp_scores, axis=0)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Computing k=140 (iter=30)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Figure 8.10
def make_figure_8_10():
    
    # Init
    fig, ax = plt.subplots(1,1,figsize=(10,6))
    ax.set_title(&#39;Figure 8.10&#39;);

    # Plot scores
    ax.plot(params, m_scores);
    ax.legend([&#39;m=p/2&#39;,&#39;m=sqrt(p)&#39;,&#39;m=log(p)&#39;]);
    ax.set_xlabel(&#39;Number of Trees&#39;); ax.set_ylabel(&#39;Test Classification Accuracy&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt; make_figure_8_10()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/07_trees_120_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;It seems that the best scores are achieved with few features and many trees.&lt;/p&gt;
&lt;h3 id=&#34;boosting&#34;&gt;Boosting&lt;/h3&gt;
&lt;p&gt;Like bagging, boosting is a general approach that can be applied to many statistical learning methods for regression or classification. Here we restrict our discussion of boosting to the context of decision trees.&lt;/p&gt;
&lt;p&gt;Boosting works similarly to bagging, except that the trees are grown sequentially: each tree is grown using information from previously grown trees. Boosting does not involve bootstrap sampling; instead each tree is fit on a modified version of the original data set.&lt;/p&gt;
&lt;p&gt;What is the idea behind this procedure? Given the current model, we fit a decision tree to the residuals from the model. That is, &lt;strong&gt;we fit a tree using the current residuals&lt;/strong&gt;, rather than the outcome $y$, as the response. We then add this new decision tree into the fitted function in order to update the residuals. Each of these trees can be rather small, with just a few terminal nodes, determined by the parameter $d$ in the algorithm. By fitting small trees to the residuals, &lt;strong&gt;we slowly improve $\hat f$ in areas where it does not perform well&lt;/strong&gt;. The shrinkage parameter λ slows the process down even further, allowing more and different shaped trees to attack the resid- uals. In general, statistical learning approaches that learn slowly tend to perform well.&lt;/p&gt;
&lt;h3 id=&#34;algorithm&#34;&gt;Algorithm&lt;/h3&gt;
&lt;p&gt;The boosting algorithm works as follows:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Set $\hat f(x)=0$ and $r_i=y_i$ for all $i$ in the training set.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;For $b=1,2,&amp;hellip;,B$ repeat:&lt;/p&gt;
&lt;p&gt;a. Fit a tree $\hat f^b $ with $d$ splits ($d+1$ terminal nodes) to the training data $(X,r)$.&lt;/p&gt;
&lt;p&gt;b. Update $\hat f$ by adding in a shrunken version of the new tree:
$$
\hat f(x) \leftarrow \hat f(x) + \lambda \hat f^b(x)
$$&lt;/p&gt;
&lt;p&gt;c. Update the residuals
$$
r_i = r_i - \lambda \hat f^b(x_i)
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Output the boosted model
$$
\hat{f}(x)=\sum_{b=1}^{B} \lambda \hat{f}^{b}(x)
$$&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Boosting has three tuning parameters:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;The &lt;strong&gt;number of trees&lt;/strong&gt; $B$&lt;/li&gt;
&lt;li&gt;The &lt;strong&gt;shrinkage parameter&lt;/strong&gt; $\lambda$. This controls the rate at which boosting learns.&lt;/li&gt;
&lt;li&gt;The &lt;strong&gt;number of splits in each tree&lt;/strong&gt; $d$ , which controls the complexity of the boosted ensemble. Often d = 1 works well, in which case each tree is a stump, consisting of a single split.&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Init , oob_score=True
params = range(50,150,10)
boost_scores = np.zeros((len(params),3))
p = np.shape(X)[1]
J = 30

# Loop over parameters
for i, k in enumerate(params):
    
    # Repeat L times
    temp_scores = np.zeros((J,3))
    for j in range(J):
        print(&amp;quot;Computing k=%1.0f (iter=%1.0f)&amp;quot; % (k,j+1), end =&amp;quot;&amp;quot;)
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=20, random_state=j)
    
        # First score: random forest
        forest = RandomForestClassifier(n_estimators=k, max_features=&amp;quot;sqrt&amp;quot;)
        forest.fit(X_train, y_train)
        temp_scores[j,0] = forest.score(X_test, y_test)

        # Second score: boosting with 1-split trees
        boost1 = GradientBoostingClassifier(learning_rate=0.01, max_depth=1, n_estimators=k, max_features=&amp;quot;sqrt&amp;quot;)
        boost1.fit(X_train, y_train)
        temp_scores[j,1] = boost1.score(X_test, y_test)

        # Third score: boosting with 1-split trees
        boost2 = GradientBoostingClassifier(learning_rate=0.01, max_depth=2, n_estimators=k, max_features=&amp;quot;sqrt&amp;quot;)
        boost2.fit(X_train, y_train)
        temp_scores[j,2] = boost2.score(X_test, y_test)
        print(&amp;quot;&amp;quot;, end=&amp;quot;\r&amp;quot;)
    
    # Average
    boost_scores[i,:] = np.mean(temp_scores, axis=0)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Computing k=140 (iter=30)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s compare boosting and forest.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Figure 8.11
def make_figure_8_11():
    
    # Init
    fig, ax = plt.subplots(1,1,figsize=(10,6))
    ax.set_title(&#39;Figure 8.11&#39;);

    # Plot scores
    ax.plot(params, m_scores);
    ax.legend([&#39;forest&#39;,&#39;boosting with d=1&#39;,&#39;boosting with d=2&#39;]);
    ax.set_xlabel(&#39;Number of Trees&#39;); ax.set_ylabel(&#39;Test Classification Accuracy&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;make_figure_8_11()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/07_trees_131_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Endogeneity</title>
      <link>https://matteocourthoud.github.io/course/metrics/07_endogeneity/</link>
      <pubDate>Fri, 29 Oct 2021 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/course/metrics/07_endogeneity/</guid>
      <description>&lt;h2 id=&#34;instrumental-variables&#34;&gt;Instrumental Variables&lt;/h2&gt;
&lt;h3 id=&#34;endogeneity&#34;&gt;Endogeneity&lt;/h3&gt;
&lt;p&gt;We say that there is &lt;strong&gt;endogeneity&lt;/strong&gt; in the linear regression model if
$\mathbb E[x_i \varepsilon_i] \neq 0$.&lt;/p&gt;
&lt;p&gt;The random vector $z_i$ is an &lt;strong&gt;instrumental variable&lt;/strong&gt; in the linear
regression model if the following conditions are met.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Exclusion restriction&lt;/strong&gt;: the instruments are uncorrelated with the
regression error $$
\mathbb E_n[z_i \varepsilon_i] = 0
$$ almost surely, i.e. with probability $p \to 1$.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Rank condition&lt;/strong&gt;: no linearly redundant instruments $$
\mathbb E_n[z_i z_i&amp;rsquo;] \neq 0
$$ almost surely, i.e. with probability $p \to 1$.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Relevance condition&lt;/strong&gt; (need $L &amp;gt; K$): $$
rank \ (\mathbb E_n[z_i x_i&amp;rsquo;]) = K
$$ almost surely, i.e. with probability $p \to 1$.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;iv-and-2sls&#34;&gt;IV and 2SLS&lt;/h3&gt;
&lt;p&gt;Let $K = dim(x_i)$ and $L = dim(z_i)$. We say that the model is
&lt;strong&gt;just-identified&lt;/strong&gt; if $L = K$ (method: IV) and &lt;strong&gt;over-identified&lt;/strong&gt; if
$L &amp;gt; K$ (method: 2SLS).&lt;/p&gt;
&lt;p&gt;Assume $z_i$ satisfies the instrumental variable assumptions above and
$dim(z_i) = dim(x_i)$, then the &lt;strong&gt;instrumental variables (IV)&lt;/strong&gt;
estimator $\hat{\beta} _ {IV}$ is given by $$
\begin{aligned}
\hat{\beta} _ {IV} &amp;amp;= \mathbb E_n[z_i x_i&amp;rsquo;]^{-1} \mathbb E_n[z_i y_i] = \newline
&amp;amp;= \left( \frac{1}{n} \sum _ {i=1}^n z_i x_i\right)^{-1} \left( \frac{1}{n} \sum _ {i=1}^n z_i y_i\right) = \newline
&amp;amp;= (Z&amp;rsquo;X)^{-1} (Z&amp;rsquo;y)
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;Assume $z_i$ satisfies the instrumental variable assumptions above and
$dim(z_i) &amp;gt; dim(x_i)$, then the &lt;strong&gt;two-stage-least squares (2SLS)&lt;/strong&gt;
estimator $\hat{\beta} _ {2SLS}$ is given by $$
\hat{\beta} _ {2SLS} =  \Big( X&amp;rsquo;Z (Z&amp;rsquo;Z)^{-1} Z&amp;rsquo;X \Big)^{-1} \Big( X&amp;rsquo;Z (Z&amp;rsquo;Z)^{-1} Z&amp;rsquo;y \Big)
$$ Where $\hat{x}_i$ is the predicted $x_i$ from the &lt;strong&gt;first stage&lt;/strong&gt;
regression of $x_i$ on $z_i$. This is equivalent to the IV estimator
using $\hat{x}_i$ as an instrument for $x_i$.&lt;/p&gt;
&lt;h3 id=&#34;2sls-algebra&#34;&gt;2SLS Algebra&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;The estimator is called &lt;strong&gt;two-stage-least squares&lt;/strong&gt; since it can be
rewritten as an IV estimator that uses $\hat{X}$ as instrument: $$
\begin{aligned}
\hat{\beta} _ {\text{2SLS}} &amp;amp;= \Big( X&amp;rsquo;Z (Z&amp;rsquo;Z)^{-1} Z&amp;rsquo;X \Big)^{-1} \Big( X&amp;rsquo;Z (Z&amp;rsquo;Z)^{-1} Z&amp;rsquo;y \Big) = \newline
&amp;amp;= (\hat{X}&amp;rsquo; X)^{-1} \hat{X}&amp;rsquo; y = \newline
&amp;amp;= \mathbb E_n[\hat{x}_i x_i&amp;rsquo;]^{-1} \mathbb E_n[\hat{x}_i y_i]
\end{aligned}
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Moreover it can be rewritten as $$
\begin{aligned}
\hat{\beta} _ {\text{2SLS}} &amp;amp;= (\hat{X}&amp;rsquo; X)^{-1} \hat{X}&amp;rsquo; y = \newline
&amp;amp;= (X&amp;rsquo; P_Z X)^{-1} X&amp;rsquo; P_Z y = \newline
&amp;amp;= (X&amp;rsquo; P_Z P_Z X)^{-1} X&amp;rsquo; P_Z y = \newline
&amp;amp;= (\hat{X}&amp;rsquo; \hat{X})^{-1} \hat{X}&amp;rsquo; y = \newline
&amp;amp;= \mathbb E_n [\hat{x}_i \hat{x}_i]^{-1} \mathbb E_n[\hat{x}_i y_i]
\end{aligned}
$$&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;rule-of-thumb&#34;&gt;Rule of Thumb&lt;/h3&gt;
&lt;p&gt;How to the test the relevance condition? Rule of thumb: $F$-test in the
first stage $&amp;gt;10$ (joint test on $z_i$).&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Problem&lt;/strong&gt;: as $n \to \infty$, with finite $L$, $F \to \infty$ (bad
rule of thumb).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;equivalence&#34;&gt;Equivalence&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;If $K=L$, $\hat{\beta} _ {\text{2SLS}} = \hat{\beta} _ {\text{IV}}$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;If $K=L$, $X&amp;rsquo;Z$ and $Z&amp;rsquo;X$ are squared matrices and, by the relevance
condition, non-singular (invertible). $$
\begin{aligned}
\hat{\beta} _ {\text{2SLS}} &amp;amp;= \Big( X&amp;rsquo;Z (Z&amp;rsquo;Z)^{-1} Z&amp;rsquo;X \Big)^{-1} \Big( X&amp;rsquo;Z (Z&amp;rsquo;Z)^{-1} Z&amp;rsquo;y \Big) = \newline
&amp;amp;= (Z&amp;rsquo;X)^{-1} (Z&amp;rsquo;Z) (X&amp;rsquo;Z)^{-1} X&amp;rsquo;Z (Z&amp;rsquo;Z)^{-1} Z&amp;rsquo;y = \newline
&amp;amp;= (Z&amp;rsquo;X)^{-1} (Z&amp;rsquo;Z) (Z&amp;rsquo;Z)^{-1} Z&amp;rsquo;y = \newline
&amp;amp;= (Z&amp;rsquo;X)^{-1} (Z&amp;rsquo;y) = \newline
&amp;amp;= \hat{\beta} _ {\text{IV}}
\end{aligned}
$$ $$\tag*{$\blacksquare$}$$&lt;/p&gt;
&lt;h3 id=&#34;demand-example&#34;&gt;Demand Example&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Example&lt;/strong&gt; from Hayiashi (2000) page 187: demand and supply
simultaneous equations. $$
\begin{aligned}
&amp;amp; q_i^D(p_i) = \alpha_0 + \alpha_1 p_i + u_i \newline
&amp;amp; q_i^S(p_i) = \beta_0 + \beta_1 p_i + v_i
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;We have an endogeneity problem. To see why, we solve the system of
equations for $(p_i, q_i)$: $$
\begin{aligned}
&amp;amp; p_i = \frac{\beta_0 - \alpha_0}{\alpha_1 - \beta_1} + \frac{v_i - u_i}{\alpha_1 - \beta_1 } \newline
&amp;amp; q_i = \frac{\alpha_1\beta_0 - \alpha_0 \beta_1}{\alpha_1 - \beta_1} + \frac{\alpha_1 v_i - \beta_1 u_i}{\alpha_1 - \beta_1 }
\end{aligned}
$$&lt;/p&gt;
&lt;h3 id=&#34;demand-example-2&#34;&gt;Demand Example (2)&lt;/h3&gt;
&lt;p&gt;Then the price variable is not independent from the error term in
neither equation: $$
\begin{aligned}
&amp;amp; Cov(p_i, u_i) = - \frac{Var(u_i)}{\alpha_1 - \beta_1 } \newline
&amp;amp; Cov(p_i, v_i) = \frac{Var(v_i)}{\alpha_1 - \beta_1 }
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;As a consequence, the OLS estimators are not consistent: $$
\begin{aligned}
&amp;amp; \hat{\alpha} _ {1, OLS} \overset{p}{\to} \alpha_1 + \frac{Cov(p_i, u_i)}{Var(p_i)} \newline
&amp;amp; \hat{\beta} _ {1, OLS} \overset{p}{\to} \beta_1 + \frac{Cov(p_i, v_i)}{Var(p_i)}
\end{aligned}
$$&lt;/p&gt;
&lt;h3 id=&#34;demand-example-3&#34;&gt;Demand Example (3)&lt;/h3&gt;
&lt;p&gt;In general, running regressing $q$ on $p$ you estimate $$
\begin{aligned}
\hat{\gamma} _ {OLS} &amp;amp;\overset{p}{\to} \frac{Cov(p_i, q_i)}{Var(p_i)} = \newline
&amp;amp;= \frac{\alpha_1 Var(v_i) + \beta_1 Var(u_i)}{(\alpha_1 - \beta_1)^2} \left( \frac{Var(v_i) + Var(u_i)}{(\alpha_1 - \beta_1)^2} \right)^{-1} = \newline
&amp;amp;= \frac{\alpha_1 Var(v_i) + \beta_1 Var(u_i)}{Var(v_i) + Var(u_i)}
\end{aligned}
$$ Which is neither $\alpha_1$ nor $\beta_1$ but a variance weighted
average of the two.&lt;/p&gt;
&lt;h3 id=&#34;demand-example-4&#34;&gt;Demand Example (4)&lt;/h3&gt;
&lt;p&gt;Suppose we have a supply shifter $z_i$ such that&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\mathbb E[z_i v_i] \neq 0$&lt;/li&gt;
&lt;li&gt;$\mathbb E[z_i u_i] = 0$.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We combine the second condition and $\mathbb E[u_i] = 0$ to get a system
of 2 equations in 2 unknowns: $\alpha_0$ and $\alpha_1$. $$
\begin{aligned}
&amp;amp; \mathbb E[z_i u_i] = \mathbb E[ z_i (q_i^D(p_i) - \alpha_0 - \alpha_1 p_i) ] = 0 \newline
&amp;amp; \mathbb E[u_i] = \mathbb E[q_i^D(p_i) - \alpha_0 - \alpha_1 p_i] = 0&lt;br&gt;
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;We could try to solve for the vector $\alpha$ that solves $$
\begin{aligned}
&amp;amp; \mathbb E_n[z_i (q_i^D - x_i\alpha)] = 0 \newline
&amp;amp; \mathbb E_n[z_i q_i^D] -  \mathbb E_n[z_ix_i\alpha] = 0
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;If $\mathbb E_n[z_ix_i]$ is invertible, we get
$\hat{\alpha} = \mathbb E_n[z_ix_i]^{-1} \mathbb E_n[z_i q^D_i]$ which
is indeed the IV estimator of $\alpha$ using $z_i$ as an instrument for
the endogenous variable $p_i$.&lt;/p&gt;
&lt;h3 id=&#34;code---dgp&#34;&gt;Code - DGP&lt;/h3&gt;
&lt;p&gt;This code draws 100 observations from the model
$y = 2 x_1 - x_2 + \varepsilon$ where $x_1, x_2 \sim U[0,1]$ and
$\varepsilon \sim N(0,1)$.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# Set seed
Random.seed!(123);

# Set the number of observations
n = 100;

# Set the dimension of Z
l = 3;

# Draw instruments
Z = rand(Uniform(0,1), n, l);

# Correlation matrix for error terms
S = [1 0.8; 0.8 1];

# Endogenous X
γ = [2 0; 0 -1; -1 3];
ε = rand(Normal(0,1), n, 2) * cholesky(S).U;
X = Z*γ .+ ε[:,1];

# Calculate y
y = X*β .+ ε[:,2];
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;code---iv&#34;&gt;Code - IV&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# Estimate beta OLS
β_OLS = (X&#39;*X)\(X&#39;*y)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 2-element Array{Float64,1}:
##   2.335699233358403
##  -0.8576266209987325
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# IV: l=k=2 instruments
Z_IV = Z[:,1:k];
β_IV = (Z_IV&#39;*X)\(Z_IV&#39;*y)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 2-element Array{Float64,1}:
##   1.6133344277861439
##  -0.6678537395714547
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# Calculate standard errors
ε_hat = y - X*β_IV;
V_NHC_IV = var(ε_hat) * inv(Z_IV&#39;*X)*Z_IV&#39;*Z_IV*inv(Z_IV&#39;*X);
V_HC0_IV = inv(Z_IV&#39;*X)*Z_IV&#39; * (I(n) .* ε_hat.^2) * Z_IV*inv(Z_IV&#39;*X);
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;code---2sls&#34;&gt;Code - 2SLS&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# 2SLS: l=3 instruments
Pz = Z*inv(Z&#39;*Z)*Z&#39;;
β_2SLS = (X&#39;*Pz*X)\(X&#39;*Pz*y)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 2-element Array{Float64,1}:
##   1.904553638377971
##  -0.8810907510370429
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# Calculate standard errors
ε_hat = y - X*β_2SLS;
V_NCH_2SLS = var(ε_hat) * inv(X&#39;*Pz*X);
V_HC0_2SLS = inv(X&#39;*Pz*X)*X&#39;*Pz * (I(n) .* ε_hat.^2) *Pz*X*inv(X&#39;*Pz*X);
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;gmm&#34;&gt;GMM&lt;/h2&gt;
&lt;h3 id=&#34;setting&#34;&gt;Setting&lt;/h3&gt;
&lt;p&gt;We have a system of $L$ moment conditions $$
\begin{aligned}
&amp;amp; \mathbb E[g_1(\omega_i, \delta_0)] = 0 \newline
&amp;amp; \vdots \newline
&amp;amp; \mathbb E[g_L(\omega_i, \delta_0)] = 0
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;If $L = \dim (\delta_0)$, no problem. If $L &amp;gt; \dim (\delta_0)$, there
may be no solution to the system of equations.&lt;/p&gt;
&lt;h3 id=&#34;options&#34;&gt;Options&lt;/h3&gt;
&lt;p&gt;There are two possibilities.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;First Solution&lt;/strong&gt;: add moment conditions until the system is
identified $$
\mathbb E[ a&amp;rsquo; g(\omega_i, \delta_0)] = 0
$$ Solve $\mathbb E[Ag(\omega_i, \delta)] = 0$ for $\hat{\delta}$.
How to choose $A$? Such that it minimizes $Var(\hat{\delta})$.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Second Solution&lt;/strong&gt;: generalized method of moments (GMM) $$
\begin{aligned}
\hat{\delta} _ {GMM} &amp;amp;= \arg \min _ \delta \quad  \Big| \Big| \mathbb E_n [ g(\omega_i, \delta) ] \Big| \Big| = \newline
&amp;amp;= \arg \min _ \delta \quad n \mathbb E_n[g(\omega_i, \delta)]&amp;rsquo; W \mathbb E_n [g(\omega_i, \delta)]
\end{aligned}
$$&lt;/li&gt;
&lt;/ol&gt;
&lt;blockquote&gt;
&lt;p&gt;The choice of $A$ and $W$ are closely related to each other.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;1-step-gmm&#34;&gt;1-step GMM&lt;/h3&gt;
&lt;p&gt;Since $J(\delta,W)$ is a quadratic form, a closed form solution exists:
$$
\hat{\delta}(W) = \Big(\mathbb E_n[z_i x_i&amp;rsquo;] W \mathbb E_n[z_i x_i&amp;rsquo;] \Big)^{-1}\mathbb E_n[z_i x_i&amp;rsquo;] W \mathbb E_n[z_i y_i]
$$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Assumptions&lt;/strong&gt; for consistency of the GMM estimator given data
$\mathcal D = \lbrace y_i, x_i, z_i \rbrace _ {i=1}^n$:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Linearity&lt;/strong&gt;: $y_i = x_i\gamma_0 + \varepsilon_i$&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;IID&lt;/strong&gt;: $(y_i, x_i, z_i)$ iid&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Orthogonality&lt;/strong&gt;:
$\mathbb E [z_i(y_i - x_i\gamma_0)] = \mathbb E[z_i \varepsilon_i] = 0$&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Rank identification&lt;/strong&gt;: $\Sigma_{xz} = \mathbb E[z_i x_i&amp;rsquo;]$ has
full rank&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;convergence&#34;&gt;Convergence&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Under linearity, independence, orthogonality and rank conditions, if
$\hat{W} \overset{p}{\to} W$ positive definite, then $$
\hat{\delta}(\hat{W}) \to \delta(W)
$$ If in addition to the above assumption,
$\sqrt{n} \mathbb E_n [g(\omega_i, \delta_0)] \overset{d}{\to} N(0,S)$
for a fixed positive definite $S$, then $$
\sqrt{n} (\hat{\delta} (\hat{W}) - \delta(W)) \overset{d}{\to} N(0,V)
$$ where
$V = (\Sigma&amp;rsquo; _ {xz} W \Sigma _ {xz})^{-1} \Sigma _ {xz} W S W \Sigma _ {xz}(\Sigma&amp;rsquo; _ {xz} W \Sigma _ {xz})^{-1}$.&lt;/p&gt;
&lt;p&gt;Finally, if a consistent estimator $\hat{S}$ of $S$ is available, then
using sample analogues $\hat{\Sigma}_{xz}$ it follows that $$
\hat{V} \overset{p}{\to} V
$$&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;If $W = S^{-1}$ then $V$ reduces to
$V = (\Sigma&amp;rsquo; _ {xz} W \Sigma _ {xz})^{-1}$. Moreover,
$(\Sigma&amp;rsquo; _ {xz} W \Sigma _ {xz})^{-1}$ is the smallest possible form
of $V$, in a positive definite sense.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Therefore, to have an efficient estimator, you want to construct
$\hat{W}$ such that $\hat{W} \overset{p}{\to} S^{-1}$.&lt;/p&gt;
&lt;h3 id=&#34;2-step-gmm&#34;&gt;2-step GMM&lt;/h3&gt;
&lt;p&gt;Estimation steps:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Choose an arbitrary weighting matrix $\hat{W}_{init}$ (usually the
identity matrix $I_K$)&lt;/li&gt;
&lt;li&gt;Estimate $\hat{\delta} _ {init}(\hat{W} _ {init})$&lt;/li&gt;
&lt;li&gt;Estimate $\hat{S}$ (asymptotic variance of the moment condition)&lt;/li&gt;
&lt;li&gt;Estimate $\hat{\delta}(\hat{S}^{-1})$&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;On the procedure:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;This estimator achieves the semiparametric efficiency bound.&lt;/li&gt;
&lt;li&gt;This strategy works only if $\hat{S} \overset{p}{\to} S$ exists.&lt;/li&gt;
&lt;li&gt;For iid cases: we can use
$\hat{\delta} = \mathbb E_n[(\hat{\varepsilon}_i z_i)(\hat{\varepsilon}_i z_i) &amp;rsquo; ]$
where
$\hat{\varepsilon}_i = y_i - x_i \hat{\delta}(\hat{W} _ {init})$.&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;code---1-step-gmm&#34;&gt;Code - 1-step GMM&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# GMM 1-step: inefficient weighting matrix
W_1 = I(l);

# Objective function
gmm_1(b) = ( y - X*b )&#39; * Z * W_1 *  Z&#39; * ( y - X*b );

# Estimate GMM
β_gmm_1 = optimize(gmm_1, β_OLS).minimizer
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 2-element Array{Float64,1}:
##   1.91556882526808
##  -0.8769689391885799
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# Standard errors GMM
ε_hat = y - X*β_gmm_1;
S_hat = Z&#39; * (I(n) .* ε_hat.^2) * Z;
d_hat = -X&#39;*Z;
V_gmm_1 = inv(d_hat * inv(S_hat) * d_hat&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 2×2 Array{Float64,2}:
##   0.0158497   -0.00346601
##  -0.00346601   0.00616531
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;code---2-step-gmm&#34;&gt;Code - 2-step GMM&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# GMM 2-step: efficient weighting matrix
W_2 = inv(S_hat);

# Objective function
gmm_2(b) = ( y - X*b )&#39; * Z * W_2 *  Z&#39; * ( y - X*b );

# Estimate GMM
β_gmm_2 = optimize(gmm_2, β_OLS).minimizer
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 2-element Array{Float64,1}:
##   1.905326742963115
##  -0.881808949213345
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# Standard errors GMM
ε_hat = y - X*β_gmm_2;
S_hat = Z&#39; * (I(n) .* ε_hat.^2) * Z;
d_hat = -X&#39;*Z;
V_gmm_2 = inv(d_hat * inv(S_hat) * d_hat&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 2×2 Array{Float64,2}:
##   0.0162603   -0.00357632
##  -0.00357632   0.00631259
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;testing-overidentifying-restrictions&#34;&gt;Testing Overidentifying Restrictions&lt;/h3&gt;
&lt;p&gt;If the equations are &lt;strong&gt;exactly identified&lt;/strong&gt;, then it is possible to
choose $\delta$ so that all the elements of the sample moments
$\mathbb E_n[g(\omega_i; \delta)]$ are zero and thus that the distance
$$
J(\delta, \hat{W}) = n \mathbb E_n[g(\omega_i, \delta)]&amp;rsquo; \hat{W} \mathbb E_n[g(\omega_i, \delta)]
$$ is zero. (The $\delta$ that does it is the IV estimator.)&lt;/p&gt;
&lt;p&gt;If the equations are &lt;strong&gt;overidentified&lt;/strong&gt;, i.e. $L$ (number of
instruments) $&amp;gt; K$ (number of equations), then the distance cannot be
zero exactly in general, but we would expect the minimized distance to
be &lt;em&gt;close&lt;/em&gt; to zero.&lt;/p&gt;
&lt;h3 id=&#34;naive-test&#34;&gt;Naive Test&lt;/h3&gt;
&lt;p&gt;Suppose your model is overidentified ($L &amp;gt; K$) and you use the following
naive testing procedure:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Estimate $\hat{\delta}$ using a subset of dimension $K$ of
instruments $\lbrace z_1 , .. , z_K\rbrace$ for
$\lbrace x_1 , &amp;hellip; , x_K\rbrace$&lt;/li&gt;
&lt;li&gt;Set $\hat{\varepsilon}_i = y_i - x_i \hat{\delta} _ {\text{GMM}}$&lt;/li&gt;
&lt;li&gt;Infer the size of the remaining $L-K$ moment conditions
$\mathbb E[z _{i, K+1} \varepsilon_i], &amp;hellip;, \mathbb E[z _{i, L} \varepsilon_i]$
looking at their empirical counterparts
$\mathbb E_n[z _{i, K+1} \hat{\varepsilon}_i], &amp;hellip;, \mathbb E_n[z _{i, L} \hat{\varepsilon}_i]$&lt;/li&gt;
&lt;li&gt;Reject exogeneity if the empirical expectations are high. How high?
Calculate p-values.&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;example&#34;&gt;Example&lt;/h3&gt;
&lt;p&gt;If you have two invalid instruments and you use one to test the validity
of the other, it might happen by chance that you don’t reject it.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Model: $y_i = x_i + \varepsilon_i$ and
$x_i = \frac{1}{2} z _{i1} - \frac{1}{2} z _{i2} + u_i$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Have $$
Cov (z _{i1}, z _{i2}, \varepsilon_i, u_i) =
\begin{bmatrix}
1 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 \newline 0 &amp;amp; 1 &amp;amp; 0 &amp;amp; 0 \newline 0 &amp;amp; 0 &amp;amp; 1 &amp;amp; 0.5 \newline 0 &amp;amp; 0 &amp;amp; 0.5 &amp;amp; 1
\end{bmatrix}
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;You want to test whether the second instrument is valid (is not
since $\mathbb E[z_2 \varepsilon] \neq 0$). You use $z_1$ and
estimate $\hat{\beta} \to$ the estimator is consistent.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;You obtain $\mathbb E_n[z _{i2} \hat{\varepsilon}_i] \simeq 0$ even
if $z_2$ is invalid&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Problem: you are using an invalid instrument in the first place.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;hansens-test&#34;&gt;Hansen’s Test&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt;: We are interested in testing
$H_0: \mathbb E[z_i \varepsilon_i] = 0$ against
$H_1: \mathbb E[z_i \varepsilon_i] \neq 0$. Suppose
$\hat{S} \overset{p}{\to} S$. Then $$
J(\hat{\delta}(\hat{S}^{-1}) , \hat{S}^{-1}) \overset{d}{\to} \chi^2 _ {L-K}
$$ For $c$ satisfying $\alpha = 1- G_{L - K} ( c )$,
$\Pr(J&amp;gt;c | H_0) \to \alpha$ so the test &lt;em&gt;reject $H_0$ if $J &amp;gt; c$&lt;/em&gt; has
asymptotic size $\alpha$.&lt;/p&gt;
&lt;h3 id=&#34;comments&#34;&gt;Comments&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;The degrees of freedom of the asymptotic distribution are the number
of overidentifying restrictions.&lt;/li&gt;
&lt;li&gt;This is a specification test, testing whether all model assumptions
are true jointly. Only when we are confident that about the other
assumptions, can we interpret a large $J$ statistic as evidence for
the endogeneity of some of the $L$ instruments included in $x$.&lt;/li&gt;
&lt;li&gt;Unlike the tests we have encountered so far, the test is not
consistent against some failures of the orthogonality conditions
(that is, it is not consistent against some fixed elements of the
alternative).&lt;/li&gt;
&lt;li&gt;Several papers in the July 1996 issue of JBES report that the
finite-sample null rejection probability of the test can far exceed
the nominal significance level $\alpha$.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;special-case-conditional-homoskedasticity&#34;&gt;Special Case: Conditional Homoskedasticity&lt;/h3&gt;
&lt;p&gt;The main implication of conditional homoskedasticity is that efficient
GMM becomes 2SLS. With efficient GMM estimation, the weighting matrix is
$\hat{S}^{-1} = \mathbb En [z_i z_i&amp;rsquo; \varepsilon_i^2]^{-1}$. With
conditional homoskedasticity, the efficient weighting matrix is
$\mathbb E_n[z_iz_i&amp;rsquo;]^{-1} \sigma^{-2}$, or equivalently
$\mathbb E_n[z_iz_i&amp;rsquo;]^{-1}$. Then, the GMM estimator becomes $$
\hat{\delta}(\hat{S}^{-1}) = \Big(\mathbb E_n[z_i x_i&amp;rsquo;]&amp;rsquo; \underbrace{\mathbb E_n[z_iz_i&amp;rsquo;]^{-1} \mathbb E[z_i x_i&amp;rsquo;]} _ {\text{ols of } x_i \text{ on }z_i} \Big)^{-1}\mathbb E_n[z_i x_i&amp;rsquo;]&amp;rsquo; \underbrace{\mathbb E_n[z_iz_i&amp;rsquo;]^{-1} \mathbb E[z_i y_i&amp;rsquo;]} _ {\text{ols of } y_i \text{ on }z_i}= \hat{\delta} _ {2SLS}
$$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;: Consider the matrix notation. $$
\begin{aligned}
\hat{\delta} \left( \frac{Z&amp;rsquo;Z}{n}\right) &amp;amp;= \left( \frac{X&amp;rsquo;Z}{n} \left( \frac{Z&amp;rsquo;Z}{n}\right)^{-1} \frac{Z&amp;rsquo;X}{n} \right)^{-1} \frac{X&amp;rsquo;Z}{n} \left( \frac{Z&amp;rsquo;Z}{n}\right)^{-1} \frac{Z&amp;rsquo;Y}{n} = \newline
&amp;amp;= \left( X&amp;rsquo;Z(Z&amp;rsquo;Z)^{-1} Z&amp;rsquo;X \right)^{-1} X&amp;rsquo;Z(Z&amp;rsquo;Z)^{-1} Z&amp;rsquo;Y = \newline
&amp;amp;= \left(X&amp;rsquo;P_ZX\right)^{-1} X&amp;rsquo;P_ZY = \newline
&amp;amp;= \left(X&amp;rsquo;P_ZP_ZX\right)^{-1} X&amp;rsquo;P_ZY = \newline
&amp;amp;= \left(\hat{X}&amp;rsquo;_Z \hat{X}_Z\right)^{-1} \hat{X}&amp;rsquo;_ZY = \newline
&amp;amp;= \hat{\delta} _ {2SLS}
\end{aligned}
$$ $$\tag*{$\blacksquare$}$$&lt;/p&gt;
&lt;h3 id=&#34;small-sample-properties-of-2sls&#34;&gt;Small-Sample Properties of 2SLS&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt;: When the number of instruments is equal to the sample size
($L = n$), then $\hat{\delta} _ {2SLS} = \hat{\delta} _ {OLS}$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;: We have a perfect prediction problem. The first stage
estimated coefficient $\hat{\gamma}$ is such that it solves the normal
equations: $\hat{\gamma} = z_i^{-1} x_i$. Then $$
\begin{aligned}
\hat{\delta} _ {2SLS} &amp;amp;= \mathbb E_n[\hat{x}_i x&amp;rsquo;_i]^{-1} \mathbb E_n[\hat{x}_i y_i] = \newline
&amp;amp;= \mathbb E_n[z_i z_i^{-1} x_i x&amp;rsquo;_i]^{-1} \mathbb E_n[z_i z_i^{-1} x_i y_i] = \newline
&amp;amp;= \mathbb E_n[x_i x&amp;rsquo;_i]^{-1} \mathbb E_n[x_i y_i] = \newline
&amp;amp;= \hat{\delta} _ {OLS}
\end{aligned}
$$ $$\tag*{$\blacksquare$}$$&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;You have this overfitting problem in general when the number of
instruments is large relative to the sample size. This problem arises
even if the instruments are valid.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;example-from-angrist-1992&#34;&gt;Example from Angrist (1992)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;They regress wages on years of schooling.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Problem&lt;/strong&gt;: endogeneity: both variables are correlated with skills
which are unobserved.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Solution&lt;/strong&gt;: instrument years of schooling with the quarter of
birth.
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Idea&lt;/strong&gt;: if born in the first three quarters, can attend school
from the year of your sixth birthday. Otherwise, you have to
wait one more year.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Problem&lt;/strong&gt;: quarters of birth are three dummies.
&lt;ul&gt;
&lt;li&gt;In order to ``improve the first stage fit” they interact them
with year of birth (180 effective instruments) and also with the
state (1527 effective instruments).&lt;/li&gt;
&lt;li&gt;This mechanically increases the $R^2$ but also increases the
bias of the 2SLS estimator.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Solutions&lt;/strong&gt;: LIML, JIVE, RJIVE (Hansen et al., 2014), Post-Lasso
(Belloni et al., 2012).&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;example-from-angrist-1992-1&#34;&gt;Example from Angrist (1992)&lt;/h3&gt;
&lt;img src=&#34;../img/Fig_441.png&#34; style=&#34;width:80.0%&#34; /&gt;
&lt;h2 id=&#34;many-instrument-robust-estimation&#34;&gt;Many Instrument Robust Estimation&lt;/h2&gt;
&lt;h3 id=&#34;issue&#34;&gt;Issue&lt;/h3&gt;
&lt;p&gt;Why having too many instruments is problematic? As the number of
instruments increases, the estimated coefficient gets closer to OLS
which is biased. As seen in the theorem above, for $L=n$, the two
estimators coincide.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;../img/Fig_451.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;liml&#34;&gt;LIML&lt;/h3&gt;
&lt;p&gt;An alternative method to estimate the parameters of the structural
equation is by maximum likelihood. Anderson and Rubin (1949) derived the
maximum likelihood estimator for the joint distribution of $(y_i, x_i)$.
The estimator is known as &lt;strong&gt;limited information maximum likelihood&lt;/strong&gt;, or
&lt;strong&gt;LIML&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;This estimator is called “limited information” because it is based on
the structural equation for $(y_i, x_i)$ combined with the reduced form
equation for $x_i$. If maximum likelihood is derived based on a
structural equation for $x_i$ as well, then this leads to what is known
as &lt;strong&gt;full information maximum likelihood (FIML)&lt;/strong&gt;. The advantage of the
LIML approach relative to FIML is that the former does not require a
structural model for $x_i$, and thus allows the researcher to focus on
the structural equation of interest - that for $y_i$.&lt;/p&gt;
&lt;h3 id=&#34;k-class-estimators&#34;&gt;K-class Estimators&lt;/h3&gt;
&lt;p&gt;The &lt;strong&gt;k-class&lt;/strong&gt; estimators have the form $$
\hat{\delta}(\alpha) = (X&amp;rsquo; P_Z X - \alpha X&amp;rsquo; X)^{-1} (X&amp;rsquo; P_Z Y - \alpha X&amp;rsquo; Y)
$$&lt;/p&gt;
&lt;p&gt;The limited information maximum likelihood estimator &lt;strong&gt;LIML&lt;/strong&gt; is the
k-class estimator $\hat{\delta}(\alpha)$ where $$
\alpha = \lambda_{min} \Big( ([X&amp;rsquo; , Y]^{-1} [X&amp;rsquo; , Y])^{-1} [X&amp;rsquo; , Y]^{-1} P_Z [X&amp;rsquo; , Y] \Big)
$$&lt;/p&gt;
&lt;p&gt;If $\alpha = 0$ then
$\hat{\delta} _ {\text{LIML}} = \hat{\delta} _ {\text{2SLS}}$ while for
$\alpha \to \infty$,
$\hat{\delta} _ {\text{LIML}} \to \hat{\delta} _ {\text{OLS}}$.&lt;/p&gt;
&lt;h3 id=&#34;comments-on-liml&#34;&gt;Comments on LIML&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;The particular choice of $\alpha$ gives a many instruments robust
estimate&lt;/li&gt;
&lt;li&gt;The LIML estimator has no finite sample moments.
$\mathbb E[\delta(\alpha_{LIML})]$ does not exist in general&lt;/li&gt;
&lt;li&gt;In simulation studies performs well&lt;/li&gt;
&lt;li&gt;Has good asymptotic properties&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Asymptotically the LIML estimator has the same distribution as 2SLS.
However, they can have quite different behaviors in finite samples.
There is considerable evidence that the LIML estimator has superior
finite sample performance to 2SLS when there are many instruments or the
reduced form is weak. However, on the other hand there is worry that
since the LIML estimator is derived under normality it may not be robust
in non-normal settings.&lt;/p&gt;
&lt;h3 id=&#34;jive&#34;&gt;JIVE&lt;/h3&gt;
&lt;p&gt;The &lt;strong&gt;Jacknife IV&lt;/strong&gt; procedure is the following&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Regress $\lbrace x_j \rbrace _ {j \neq i}$ on
$\lbrace z_j \rbrace _ {j \neq i}$ and estimate $\pi_{-i}$ (leave
the $i^{th}$ observation out).&lt;/li&gt;
&lt;li&gt;Form $\hat{x}_i = \hat{\pi} _ {-i} z_i$.&lt;/li&gt;
&lt;li&gt;Run IV using $\hat{x}_i$ as instruments. $$
\hat{\delta} _ {JIVE} = \mathbb E_n[\hat{x}_i x_i&amp;rsquo;]^{-1} \mathbb E_n[\hat{x}_i y_i&amp;rsquo;]
$$&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;comments-on-jive&#34;&gt;Comments on JIVE:&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Prevents overfitting.&lt;/li&gt;
&lt;li&gt;With many instruments you get bad out of sample prediction which
implies low correlation between $\hat{x}_i$ and $x_i$:
$\mathbb E_n[\hat{x}_i x_i&amp;rsquo;] \simeq 0$.&lt;/li&gt;
&lt;li&gt;Use lasso/ridge regression in the first stage in case of too many
instruments.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;hausman-test&#34;&gt;Hausman Test&lt;/h3&gt;
&lt;p&gt;Here we consider testing the validity of OLS. OLS is generally preferred
to IV in terms of precision. Many researchers only doubt the (joint)
validity of the regressor $z_i$ instead of being certain that it is
invalid (in the sense of not being predetermined). So then they wish to
choose between OLS and 2SLS, assuming that they have an instrument
vector $x_i$ whose validity is not in question. Further, assume for
simplicity that $L = K$ so that the efficient GMM estimator is the IV
estimator.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;Hausman test statistic&lt;/strong&gt; $$
H \equiv n (\hat{\delta} _ {IV} - \hat{\delta} _ {OLS})&amp;rsquo; [\hat{Avar} (\hat{\delta} _ {IV} - \hat{\delta} _ {OLS})]^{-1} (\hat{\delta} _ {IV} - \hat{\delta} _ {OLS})
$$ is asymptotically distributed as a $\chi^2_{L-s}$ under the null
where $s = | z_i \cup x_i |$: the number of regressors that are retained
as instruments in $x_i$.&lt;/p&gt;
&lt;h3 id=&#34;comments-1&#34;&gt;Comments&lt;/h3&gt;
&lt;p&gt;In general, the idea of the Hausman test is the following. If you have
two estimators, one which is efficient under $H_0$ but inconsistent
under $H_1$ (in this case, OLS), and another which is consistent under
$H_1$ (in this case, IV), then construct a test as a quadratic form in
the differences of the estimators. Another classic example arises in
panel data with the hypothesis $H_0$ of unconditional strict exogeneity.
In that case, under $H_0$ Random Effects estimators are efficient but
under $H_1$ they are inconsistent. Fixed Effects estimators instead are
consistent under $H_1$.&lt;/p&gt;
&lt;p&gt;The Hausman test statistic can be used as a pretest procedure: select
either OLS or IV according to the outcome of the test. Although widely
used, this pretest procedure is not advisable. When the null is false,
it is still possible that the test &lt;em&gt;accepts&lt;/em&gt; the null (committing a Type
2 error). In particular, this can happen with a high probability when
the sample size is &lt;em&gt;small&lt;/em&gt; and/or when the regressor $z_i$ is &lt;em&gt;almost
valid&lt;/em&gt;. In such an instance, estimation and also inference will be based
on incorrect methods. Therefore, the overall properties of the Hausman
pretest procedure are undesirable.&lt;/p&gt;
&lt;p&gt;The Hausman test is an example of a specification test. There are many
other specification tests. One could for example test for conditional
homoskedasticity. Unlike for the OLS case, there does not exist a
convenient test for conditional homoskedasticity for the GMM case. A
test statistic that is asymptotically chi-squared under the null is
available but is extremely cumbersome; see White (1982, note 2). If in
doubt, it is better to use the more generally valid inference methods
that allow for conditional heteroskedasticity. Similarly, there does not
exist a convenient test for serial correlation for the GMM case. If in
doubt, it is better to use the more generally valid inference methods
that allow for serial correlation; for example, when data are collected
over time (that is, time-series data).&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Single Agent Dynamics</title>
      <link>https://matteocourthoud.github.io/course/empirical-io/07_dynamics_singleagent/</link>
      <pubDate>Fri, 29 Oct 2021 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/course/empirical-io/07_dynamics_singleagent/</guid>
      <description>&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;h3 id=&#34;motivation&#34;&gt;Motivation&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;IO&lt;/strong&gt;: role of &lt;em&gt;market structure&lt;/em&gt; on &lt;em&gt;equilibrium outcomes&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Dynamics&lt;/strong&gt;: study the &lt;strong&gt;endogenous evolution&lt;/strong&gt; of &lt;em&gt;market structure&lt;/em&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Supply&lt;/strong&gt; side dynamics
&lt;ul&gt;
&lt;li&gt;Irreversible investment&lt;/li&gt;
&lt;li&gt;Entry sunk costs&lt;/li&gt;
&lt;li&gt;Product repositioning costs&lt;/li&gt;
&lt;li&gt;Price adjustment costs&lt;/li&gt;
&lt;li&gt;Learning by doing&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Demand&lt;/strong&gt; side dynamics
&lt;ul&gt;
&lt;li&gt;Switching costs&lt;/li&gt;
&lt;li&gt;Durable or storable products&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Bonus motivation&lt;/strong&gt;: AI literature studies essentially the same set of
problems with similar tools (&lt;a href=&#34;#ref-igami2020artificial&#34;&gt;Igami 2020&lt;/a&gt;)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Irony: niche topic in IO (super niche in econ), but at the core of
the frontier in computer science
&lt;ul&gt;
&lt;li&gt;Why? Computation is hard, estimation harder, but extremely
powerful prediction tool&lt;/li&gt;
&lt;li&gt;The world is intrinsecally dynamic&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;examples-1&#34;&gt;Examples (1)&lt;/h3&gt;
&lt;p&gt;Some examples in empirical IO&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Investment&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;Rust (&lt;a href=&#34;#ref-rust1987optimal&#34;&gt;1987&lt;/a&gt;): bus engine replacement
decision&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Durable goods&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;Gowrisankaran and Rysman
(&lt;a href=&#34;#ref-gowrisankaran2012dynamics&#34;&gt;2012&lt;/a&gt;): consumer demand in the
digital camcorder industry&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Stockpiling&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;Erdem, Imai, and Keane (&lt;a href=&#34;#ref-erdem2003brand&#34;&gt;2003&lt;/a&gt;): promotions
and stockpiling of ketchup&lt;/li&gt;
&lt;li&gt;Hendel and Nevo (&lt;a href=&#34;#ref-hendel2006measuring&#34;&gt;2006&lt;/a&gt;): stockpiling
of laundry detergents&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Learning&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;Erdem and Keane (&lt;a href=&#34;#ref-erdem1996decision&#34;&gt;1996&lt;/a&gt;): brand learning
in the laundry detergent industry&lt;/li&gt;
&lt;li&gt;Crawford and Shum (&lt;a href=&#34;#ref-crawford2005uncertainty&#34;&gt;2005&lt;/a&gt;): demand
learning of anti‐ulcer drug prescriptions&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Switching costs&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;Handel (&lt;a href=&#34;#ref-handel2013adverse&#34;&gt;2013&lt;/a&gt;): inertia in demand for
health insurance&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;examples-2&#34;&gt;Examples (2)&lt;/h3&gt;
&lt;p&gt;But also in other applied micro fields:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Labor economics&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;Should you go to college? (&lt;a href=&#34;#ref-keane1997career&#34;&gt;Keane and Wolpin
1997&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Health economics&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Which health insurance to pick given there are switching costs?
(&lt;a href=&#34;#ref-handel2013adverse&#34;&gt;Handel 2013&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Addiction (&lt;a href=&#34;#ref-becker1988theory&#34;&gt;Becker and Murphy 1988&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Public finance&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;How should you set optimal taxes in a dynamic environment?
(&lt;a href=&#34;#ref-golosov2006new&#34;&gt;Golosov et al. 2006&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;do-we-really-need-dynamics&#34;&gt;Do we really need dynamics?&lt;/h3&gt;
&lt;p&gt;In some cases, we can &lt;strong&gt;reduce&lt;/strong&gt; a dynamic problem to a:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Static problem&lt;/li&gt;
&lt;li&gt;Reduced-form problem&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;E.g., Investment decision&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Dynamic problem, as gains are realized after costs&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;“Static” solution: invest if $\mathbb E (NPV ) &amp;gt; TC$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Action today ($a_t=0$ or $1$) does not affect the amount of future
payoffs (NPV)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;But many cases where it’s hard to evaluate dynamic questions in a
static/reduced-form setting.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Typically, cases where decision today would affect payoffs tomorrow&lt;/li&gt;
&lt;li&gt;And you care about those payoffs ($\neq$ myopia)&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;“&lt;em&gt;A dynamic model can do anything a static model can.&lt;/em&gt;”&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;new-empirical-io&#34;&gt;New Empirical IO&lt;/h3&gt;
&lt;p&gt;So-called New Empirical IO (summary in Bresnahan
(&lt;a href=&#34;#ref-bresnahan1989empirical&#34;&gt;1989&lt;/a&gt;))&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Some &lt;strong&gt;decisions today&lt;/strong&gt; might affect &lt;strong&gt;payoffs tomorrow&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;But the decision today depends on the &lt;strong&gt;state today&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;And the state today might have been the result of a &lt;strong&gt;decision
yesterday&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Etc…&lt;/li&gt;
&lt;li&gt;Need &lt;strong&gt;dynamics&lt;/strong&gt; to study these questions&lt;/li&gt;
&lt;li&gt;Where does it all start?
&lt;ul&gt;
&lt;li&gt;Pakes (&lt;a href=&#34;#ref-pakes1986patents&#34;&gt;1986&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Berry (&lt;a href=&#34;#ref-berry1992estimation&#34;&gt;1992&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;pros-and-cons&#34;&gt;Pros and Cons&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Advantages&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;We can adress &lt;strong&gt;intertemporal trade-offs&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Flow vs stock stocks and benefits&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;We can examine &lt;strong&gt;transitions&lt;/strong&gt; and not only steady states&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;We are able to address &lt;strong&gt;policy questions&lt;/strong&gt; that cannot be addressed
with reduced-form methods&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Standard advantage of structural estimation&lt;/li&gt;
&lt;li&gt;But in a context with relevant intertemporal trade-offs /
decisions&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Disadvantages&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;We typically need more &lt;strong&gt;assumptions&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Robustness testing will therefore be important&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Identification&lt;/strong&gt; in dynamic models is less transparent&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Thus time should be spent articulating what variation in the
data identifies our parameters of interest)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;It is often &lt;strong&gt;computationally intensive&lt;/strong&gt; (i.e., slow / unfeasible)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;from-statics-to-dynamics&#34;&gt;From Statics to Dynamics&lt;/h3&gt;
&lt;p&gt;Typical steps&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Specify the primitives of the model
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Static&lt;/strong&gt;: single period agents’ payoff functions (utility or
profit)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Dynamic&lt;/strong&gt;: static payoffs + &lt;em&gt;evolution of state variables&lt;/em&gt;
&lt;ul&gt;
&lt;li&gt;Can be exogenous&lt;/li&gt;
&lt;li&gt;… or endogenous: decision today has an effect on the state
tomorrow&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Solve for optimal behavior
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Static&lt;/strong&gt;: tipically agents maximize current utility or profit&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Dynamic&lt;/strong&gt;: agents maximize &lt;em&gt;present discounted value&lt;/em&gt; of
future utilities or profits&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Search for parameter values that result in the “best match” between
our model predictions and observed behavior&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;1st-year-macro-recap&#34;&gt;1st year Macro Recap&lt;/h2&gt;
&lt;h3 id=&#34;markov-decision-processes&#34;&gt;Markov Decision Processes&lt;/h3&gt;
&lt;p&gt;Formally, a discrete-time MDP consists of the following &lt;strong&gt;objects&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;A discrete &lt;strong&gt;time index&lt;/strong&gt; $t \in \lbrace 0,1,2,&amp;hellip;,T \rbrace$, for
$T \leq \infty$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;A &lt;strong&gt;state space&lt;/strong&gt; $\mathcal S$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;An &lt;strong&gt;action space&lt;/strong&gt; $\mathcal A$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;and a family of &lt;strong&gt;constraint sets&lt;/strong&gt;
$\lbrace \mathcal a_t(s_t) \subseteq \mathcal A \rbrace$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;A family of &lt;strong&gt;transition probabilities&lt;/strong&gt;
$\lbrace \Pr_{t}(s_{t+1}|s_t,a_t) \rbrace$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;A &lt;strong&gt;discount factor&lt;/strong&gt;, $\beta$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;A family of single-period &lt;strong&gt;reward functions&lt;/strong&gt;
$\lbrace (u_t(s_t,a_t) \rbrace$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;so that the utility functional $U$ has an additively separable
decomposition $$
U(\boldsymbol s, \boldsymbol a) = \sum_{t=0}^{T} \beta^{t} u_{t}\left(s_t, a_{t}\right)
$$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;mdp-2&#34;&gt;MDP (2)&lt;/h3&gt;
&lt;p&gt;In words&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;The &lt;strong&gt;state space&lt;/strong&gt; $\mathcal S$ contains all the information needed
to&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;compute static utilities $u_t (s_t, a_t)$&lt;/li&gt;
&lt;li&gt;compute transition probabilities
$\lbrace \Pr_{t} (s_{t+1}|s_t,a_t) \rbrace$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The (conditional) &lt;strong&gt;action space&lt;/strong&gt; $\mathcal A (s_t)$ contains all
the actions available in state $s_t$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;How can it be different by state? E.g. entry/exit decision if
you’re in/out of the market&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The &lt;strong&gt;transition probabilities&lt;/strong&gt;
$\lbrace \Pr_{t+1}(s_{t+1}|s_t,a_t) \rbrace$ define the
probabilities of future states $s_{t+1}$ conditional on&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Present state $s_t$&lt;/li&gt;
&lt;li&gt;Present decision $a_t$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The &lt;strong&gt;discount factor&lt;/strong&gt; $\beta$ together with the static &lt;strong&gt;reward
functions&lt;/strong&gt; $\lbrace (u_t(s_t,a_t) \rbrace$ determines the
&lt;strong&gt;objective function&lt;/strong&gt; $$
\mathbb E_{\boldsymbol s&amp;rsquo;} \Bigg[ \sum_{t=0}^{T} \beta^{t} u_{t}\left(s_t, a_{t}\right) \Bigg]
$$&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;notation&#34;&gt;Notation&lt;/h3&gt;
&lt;p&gt;Brief parenthesis on notation&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;I have seen &lt;strong&gt;states&lt;/strong&gt; denoted as&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$s$ (for state)&lt;/li&gt;
&lt;li&gt;$x$&lt;/li&gt;
&lt;li&gt;$\omega$&lt;/li&gt;
&lt;li&gt;others, depending on the specific context, e.g. $e$ for
experience&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;I will try to stick to $s$ all the time&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;I have seen &lt;strong&gt;decisions&lt;/strong&gt; denoted as&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$a$ (for action)&lt;/li&gt;
&lt;li&gt;$d$ (for decision)&lt;/li&gt;
&lt;li&gt;$x$&lt;/li&gt;
&lt;li&gt;others, depending on the specific context, e.g. $i$ for
investment&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;I will try to stick to $a$ all the time&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;maximization-problem&#34;&gt;Maximization Problem&lt;/h3&gt;
&lt;p&gt;The objective is to pick the decision rule (or &lt;strong&gt;policy function&lt;/strong&gt;)
$P = \boldsymbol a^* = \lbrace a_1^*, &amp;hellip;, a_t ^ * \rbrace$ that solves
$$
\max_{\boldsymbol a} \ \mathbb E_{\boldsymbol s&amp;rsquo;} \Bigg[ \sum_{t=0}^{T} \beta^{t} u_{t} \left(s_{t}, a_{t} \right) \Bigg]
$$ Where the expectation is taken over transition probabilities
generated by the decision rule $\boldsymbol a$.&lt;/p&gt;
&lt;h3 id=&#34;stationarity&#34;&gt;Stationarity&lt;/h3&gt;
&lt;p&gt;In many applications, we assume &lt;strong&gt;stationarity&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;The &lt;strong&gt;transition probabilities and utility functions do not directly
depend on&lt;/strong&gt; $t$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;i.e., are the same for all $t$
&lt;ul&gt;
&lt;li&gt;$\Pr_{{\color{red}{t}}} (s_{t+1}|s_t,a_t) \  \to \ \Pr(s_{t+1}|s_t,a_t)$&lt;/li&gt;
&lt;li&gt;$u_{{\color{red}{t}}} (s_t,a_t) \ \to \ u(s_t,a_t)$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Uncomfortable assumption?&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;You think there is some reason (variable) why today’s probabilities
should be different from tomorrow’s?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;If &lt;strong&gt;observable&lt;/strong&gt;, include that variable in the state space&lt;/li&gt;
&lt;li&gt;If &lt;strong&gt;unobservable&lt;/strong&gt;, integrate it out&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;stationarity-2&#34;&gt;Stationarity (2)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;In the &lt;strong&gt;finite horizon&lt;/strong&gt; case ($T \leq \infty$), stationarity does
not help much&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\sum_{t=0}^{T} \beta^{t} u(s_t, a_{t})$ still depends on $t$,
conditional on $s_t$&lt;/li&gt;
&lt;li&gt;Why? Difference between $t$ and $T$ matters in the sum&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;In &lt;strong&gt;infinite-horizon&lt;/strong&gt; problems, stationarity helps &lt;strong&gt;a lot&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Now the difference between $t$ and $T$ is always the same,
i.e. $\infty$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$\sum_{t=0}^{\infty} \beta^{t} u(s_t, a_{t})$ does &lt;strong&gt;not&lt;/strong&gt;
depend on $t$, conditional on $s_t$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;The future looks the same whether the agent is in state $s_t$
at time $t$ or in state $s_{t+\tau} = s_t$ at time $t + \tau$&lt;/em&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;value-function&#34;&gt;Value Function&lt;/h3&gt;
&lt;p&gt;Consider a &lt;strong&gt;stationary infinite-horizon&lt;/strong&gt; problem&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;The only variable which affects the agent’s view about the future is
the current value of the state, $s_t$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;We can rewrite the &lt;strong&gt;agent’s problem&lt;/strong&gt; as $$
V_0(s_0) = \max_{\boldsymbol a} \ \mathbb E_{\boldsymbol s&amp;rsquo;} \Bigg[ \sum_{t=0}^{\infty} \beta^{t} u\left(s_t, a_{t}\right) \Bigg]
$$ where&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$a_t \in \mathcal A(s_t) \ \forall t$&lt;/li&gt;
&lt;li&gt;The expectation is taken over future states $\boldsymbol s&#39;$
&lt;ul&gt;
&lt;li&gt;that evolve according to
$\lbrace \Pr(s_{t+1}|s_t,a_t) \rbrace$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;$V(\cdot)$ is called the &lt;strong&gt;value function&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;how-to-solve&#34;&gt;How to solve?&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;One could try to solve it by &lt;strong&gt;brute force&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;i.e. try to solve for the structure of all of the optimal
decisions, $\boldsymbol a^*$&lt;/li&gt;
&lt;li&gt;Indeed, for finite-horizon problems, that might be necessary&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;For &lt;strong&gt;stationary infinite-horizon&lt;/strong&gt; problems, the value and policy
function should be &lt;strong&gt;time invariant&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;$V_{\color{red}{t}} (s_t) = V(s_t)$&lt;/li&gt;
&lt;li&gt;$P_{\color{red}{t}} (s_t) = P(s_t)$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;What do we gain?&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;bellman-equation&#34;&gt;Bellman Equation&lt;/h3&gt;
&lt;p&gt;$$
\begin{align}
V(s_0) &amp;amp;= \max_{\boldsymbol a} \ \mathbb E_{\boldsymbol s&amp;rsquo;} \Bigg[ \sum_{t=0}^{\infty} \beta^{t} u(s_t, a_{t}) \Bigg]
= \newline
&amp;amp;= \max_{\boldsymbol a} \ \mathbb E_{\boldsymbol s&amp;rsquo;} \Bigg[ {\color{red}{u(s_{0}, a_{0})}} + \sum_{{\color{red}{t=1}}}^{\infty} \beta^{t} u(s_t, a_{t}) \Bigg]
= \newline
&amp;amp;= \max_{\boldsymbol a} \ \Bigg\lbrace u(s_{0}, a_{0}) + {\color{red}{\mathbb E_{\boldsymbol s&amp;rsquo;}}} \Bigg[ \sum_{t=1}^{\infty} \beta^{t} u(s_t, a_{t}) \Bigg] \Bigg\rbrace
= \newline
&amp;amp;= \max_{\boldsymbol a} \ \Bigg\lbrace u(s_{0}, a_{0}) + {\color{red}{\beta}} \ \mathbb E_{\boldsymbol s&amp;rsquo;} \Bigg[ \sum_{t=1}^{\infty} \beta^{{\color{red}{t-1}}} u(s_t, a_{t}) \Bigg] \Bigg\rbrace
= \newline
&amp;amp;= \max_{{\color{red}{a_0}}} \ \Bigg\lbrace u(s_{0}, a_{0}) + \beta \ {\color{red}{\max_{\boldsymbol a}}}\ \mathbb E_{\boldsymbol s&amp;rsquo;} \Bigg[ \sum_{t=1}^{\infty} \beta^{t-1} u(s_t, a_{t}) \Bigg] \Bigg\rbrace
= \newline
&amp;amp;= \max_{a_0} \ \Bigg\lbrace u(s_{0}, a_{0}) + \beta \ {\color{red}{\int V(s_1) \Pr(s_1 | s_0, a_0)}} \Bigg\rbrace
\end{align}
$$&lt;/p&gt;
&lt;h3 id=&#34;bellman-equation-2&#34;&gt;Bellman Equation (2)&lt;/h3&gt;
&lt;p&gt;We have now a &lt;strong&gt;recursive formulation&lt;/strong&gt; of the value function: the
&lt;strong&gt;Bellman Equation&lt;/strong&gt; $$
{\color{red}{V(s_0)}} = \max_{a_0} \ \Bigg\lbrace u(s_{0}, a_{0}) + \beta \ \int {\color{red}{V(s_1)}} \Pr(s_1 | s_0, a_0) \Bigg\rbrace
$$ &lt;strong&gt;Intuition&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The Bellman Equation is a &lt;strong&gt;functional equation&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;Has to be satisfied in every state&lt;/li&gt;
&lt;li&gt;Can be written as ${\color{red}{V}} = T({\color{red}{V}})$&lt;/li&gt;
&lt;li&gt;We are actually looking for a &lt;strong&gt;fixed point&lt;/strong&gt; of $T$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The decision rule that satisfies the Bellman Equation is called the
&lt;strong&gt;policy function&lt;/strong&gt; $$
a(s_0) =  \arg \max_{a_0} \ \Bigg\lbrace u(s_{0}, a_{0}) + \beta \ \int V(s_1) \Pr(s_1 | s_0, a_0) \Bigg\rbrace
$$&lt;/p&gt;
&lt;h3 id=&#34;contractions&#34;&gt;Contractions&lt;/h3&gt;
&lt;p&gt;Under regularity conditions&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$u(s, a)$ is jointly continuous and bounded in $(s, a)$&lt;/li&gt;
&lt;li&gt;$\mathcal A (s)$ is a continuous correspondence&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;It is possible to show that $$
T(W)(s) = \max_{a \in \mathcal A(s)} \ \Bigg\lbrace u(s, a) + \beta \ \int W(s&amp;rsquo;) \Pr(s&amp;rsquo; | s, a) \Bigg\rbrace
$$ is a &lt;strong&gt;contraction mapping&lt;/strong&gt; of modulus $\beta$.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Contraction Mapping Theorem&lt;/strong&gt;: then $T$ has a unique fixed point!&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;solving-for-the-value-function&#34;&gt;Solving for the Value Function&lt;/h3&gt;
&lt;p&gt;How do we actually do it in practice?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;For &lt;strong&gt;finite horizon&lt;/strong&gt; MDPs: &lt;strong&gt;backward induction&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;Start from the last period: static maximization problem&lt;/li&gt;
&lt;li&gt;Move backwards taking the future value as given&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;For &lt;strong&gt;infinite horizon&lt;/strong&gt; MDPs: different options
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;value function iteration&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;most common&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;policy function iteration&lt;/li&gt;
&lt;li&gt;successive approximations&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;difference-with-1st-year-macro&#34;&gt;Difference with 1st year Macro&lt;/h3&gt;
&lt;p&gt;So what’s going to be new here?&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Estimation&lt;/strong&gt;: retrieve model primitives from observed behavior
&lt;ul&gt;
&lt;li&gt;And related: uncertainty&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Strategic interaction&lt;/strong&gt;: multiple agents taking dynamic decisions
&lt;ul&gt;
&lt;li&gt;Next lecture&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;rust-1987&#34;&gt;Rust (1987)&lt;/h2&gt;
&lt;h3 id=&#34;setting&#34;&gt;Setting&lt;/h3&gt;
&lt;p&gt;Rust (&lt;a href=&#34;#ref-rust1987optimal&#34;&gt;1987&lt;/a&gt;): &lt;em&gt;An Empirical Model of Harold
Zurcher&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Harold Zurcher (HZ) is the city bus superintendant in Madison, WI&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;As bus engines get older, the probability of malfunctions increases&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;HZ decides when to replace old bus engines with new ones&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Optimal stopping / investment problem&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Tradeoff&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Cost of a new engine (fixed, stock)&lt;/li&gt;
&lt;li&gt;Repair costs, because of engine failures (continuous, flow)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Do we care about Harold Zurcher?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Obviously not (and neither did Rust), it’s a method paper&lt;/li&gt;
&lt;li&gt;But referee asked for an application&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;data&#34;&gt;Data&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Units of observation&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Rust observes 162 buses over time&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Observables&lt;/strong&gt;: for each bus, he sees&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;monthly mileage (RHS, state variable)&lt;/li&gt;
&lt;li&gt;and whether the engine was replaced (LHS, choice variable),&lt;/li&gt;
&lt;li&gt;in a given month&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Variation&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;on average, bus engines were replaced every 5 years with over
200,000 elapsed miles&lt;/li&gt;
&lt;li&gt;considerable variation in the &lt;em&gt;time&lt;/em&gt; and &lt;em&gt;mileage&lt;/em&gt; at which
replacement occurs&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;idea&#34;&gt;Idea&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Construct a (parametric) &lt;strong&gt;model&lt;/strong&gt; which predicts the time and
mileage at which engine replacement occurs&lt;/li&gt;
&lt;li&gt;Use the model predictions (conditional on parameter values) to
&lt;strong&gt;estimate parameters&lt;/strong&gt; that “fit” the data
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;predicted&lt;/strong&gt; replacements, given mileage VS &lt;strong&gt;observed&lt;/strong&gt;
replacements, given mileage&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Ideally use the estimates to &lt;strong&gt;learn something&lt;/strong&gt; new
&lt;ul&gt;
&lt;li&gt;e.g. the correct &lt;em&gt;dynamic&lt;/em&gt; demand curve for bus engine
replacement&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;static-alternative&#34;&gt;Static Alternative&lt;/h3&gt;
&lt;p&gt;What would you do otherwise?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;You observe replacement decisions&lt;/li&gt;
&lt;li&gt;… and replacement costs&lt;/li&gt;
&lt;li&gt;$\to$ &lt;strong&gt;Regress&lt;/strong&gt; replacement decision on replacement costs&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Problem&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Replacement benefits are a flow (lower maintenance costs)&lt;/li&gt;
&lt;li&gt;… while the cost is a stock&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Outcome&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;We expect the &lt;em&gt;overestimate&lt;/em&gt; demand elasticity. Why?&lt;/li&gt;
&lt;li&gt;Overpredict substitutions at low costs&lt;/li&gt;
&lt;li&gt;and underpredict substitution at high cost&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;model&#34;&gt;Model&lt;/h3&gt;
&lt;p&gt;Assumptions of the structural model&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;State&lt;/strong&gt;: $s_t \in \lbrace 0, &amp;hellip; , s_{max} \rbrace$
&lt;ul&gt;
&lt;li&gt;engine accumulated mileage at time $t$&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Note&lt;/strong&gt;: “continuous” in the data but has to be discretized
into bins&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Action&lt;/strong&gt;: $a_t \in \lbrace 0, 1 \rbrace$
&lt;ul&gt;
&lt;li&gt;replace engine at time $t$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;State transitions&lt;/strong&gt;:
$\Pr ( s_{t+1} | s_{0}, &amp;hellip; , s_t ; \theta)= \Pr (s_{t+1} | s_t ; \theta )$
&lt;ul&gt;
&lt;li&gt;mileage $s_t$ evolves exogenously according to a 1st-order
Markov process&lt;/li&gt;
&lt;li&gt;The transition function is the same for every bus.&lt;/li&gt;
&lt;li&gt;If HZ replaces in period $t$ ($a_t = 1$), then $s_t = 0$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;model-2&#34;&gt;Model (2)&lt;/h3&gt;
&lt;p&gt;HZ &lt;strong&gt;static utility function&lt;/strong&gt; (for a single bus) $$
u\left(s_t, a_{t} ; \theta\right)= \begin{cases}-c\left(s_t ; \theta\right) &amp;amp; \text { if } a_{t}=0 \text { (not replace) } \newline -R-c(0 ; \theta) &amp;amp; \text { if } a_{t}=1 \text { (replace) }\end{cases}
$$ where&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$c(s_t ; \theta)$: expected &lt;strong&gt;costs of operating&lt;/strong&gt; a bus with
mileage $s_t$
&lt;ul&gt;
&lt;li&gt;​ including maintenance costs &amp;amp; social costs of breakdown&lt;/li&gt;
&lt;li&gt;We would expect $\frac{\partial c}{\partial s}&amp;gt;0$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;$R$ is the &lt;strong&gt;cost of replacement&lt;/strong&gt; (i.e., a new engine)
&lt;ul&gt;
&lt;li&gt;Note that replacement occurs immediately&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;$u(s_t , a_t ; \theta)$: expected current utility from operating a
bus with mileage $s_t$ and making replacement decision $a_t$&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;model-3&#34;&gt;Model (3)&lt;/h3&gt;
&lt;p&gt;HZ &lt;strong&gt;objective function&lt;/strong&gt; is to maximize the expected present discounted
sum of future utilities $$
V(s_t ; \theta) = \max_{\boldsymbol a} \mathbb E_{s_{t+1}} \left[\sum_{\tau=t}^{\infty} \beta^{\tau-t} u\left(s_{\tau}, a_{\tau} ; \theta\right) \ \Bigg| \ s_t, \boldsymbol a ; \theta\right]
$$ where&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The expectation $\mathbb E$ is over future $x$, which evolve
according to Markov process&lt;/li&gt;
&lt;li&gt;$\max$ is over future choices $a_{t+1}, &amp;hellip; ,a_{\infty}$,
&lt;ul&gt;
&lt;li&gt;because HZ will observe future states $s_{\tau}$ before choosing
future actions $a_\tau$, this is a functional&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Notes&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;This is for one bus (but multiple engines).&lt;/li&gt;
&lt;li&gt;HZ has an infinite horizon for his decision making&lt;/li&gt;
&lt;li&gt;$s_t$ summarizes state at time $t$, i.e., the expected value of
future utilities only depends on $s_t$&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;bellman-equation-1&#34;&gt;Bellman Equation&lt;/h3&gt;
&lt;p&gt;This (sequential) representation of HZ’s problem is very cumbersome to
work with.&lt;/p&gt;
&lt;p&gt;We can rewrite $V (s_t; \theta)$ with the following Bellman equation $$
V\left(s_t ; \theta\right) = \max_{a_{t}} \Bigg\lbrace u\left(s_t, a_{t} ; \theta\right)+\beta \mathbb E_{s_{t+1}} \Big[V\left(s_{t+1} ; \theta\right) \Big| s_t, a_{t} ; \theta\Big] \Bigg\rbrace
$$ Basically we are dividing the infinite sum (in the sequential form)
into a present component and a future component.&lt;/p&gt;
&lt;p&gt;Notes:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Same $V$ on both sides of equation because of infinite horizon - the
future looks the same as the present for a given $s$ (i.e., it
doesn’t matter where you are in time).&lt;/li&gt;
&lt;li&gt;The expectation $\mathbb E$ is over the state-transition
probabilities, $\Pr (s_{t+1} | s_t, a_t ; \theta)$&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;order-of-markow-process&#34;&gt;Order of Markow Process&lt;/h3&gt;
&lt;p&gt;Suppose for a moment that $s_t$ follows a second-order markov process $$
s_{t+1}=f\left(s_t, {\color{red}{s_{t-1}}}, \varepsilon ; \theta\right)
$$ Now $s_t$ is not sufficient to describe current $V$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;We need both $s_t$ and $s_{t-1}$ in the state space (i.e.,
$V (s_t , {\color{red}{s_{t-1}}}; \theta)$ contains $s_{t-1}$, too),&lt;/li&gt;
&lt;li&gt;and the expectation is over the transition probability
$\Pr (s_{t+1} | s_t, {\color{red}{s_{t-1}}}, a_t ; \theta)$&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;parenthesis-state-variables&#34;&gt;Parenthesis: State Variables&lt;/h3&gt;
&lt;p&gt;Which variables should be state variables? I.e. should be included in
the state space?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;General rule&lt;/strong&gt; for 1st order markow processes: variables need to&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;define expected current payoff, &lt;strong&gt;and&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;define expectations over next period state (i.e., distribution of
$s_{t+1}$)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;What do you do otherwise? Integrate them out! &lt;strong&gt;Examples&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Weather affects static utitilies but not transition probabilities
&lt;ul&gt;
&lt;li&gt;More annoying to replace the engine if it rains&lt;/li&gt;
&lt;li&gt;Integration means: &lt;em&gt;“compute expected utility of Harold Zurcher
before he opens the window”&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Month of the year affects transition probabilities but not utilities
&lt;ul&gt;
&lt;li&gt;Buses are used more in the winter&lt;/li&gt;
&lt;li&gt;Integration means: &lt;em&gt;“compute average transition probabilities
over months”&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: you can always get the non-expected value function if you
know the probability of raining or the transition probabilities by
month&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;policy-function&#34;&gt;Policy Function&lt;/h3&gt;
&lt;p&gt;Along with this value function comes a corresponding &lt;strong&gt;policy (or
choice) function&lt;/strong&gt; mapping the state $s_t$ into HZ’s optimal replacement
choice $a_t$ $$
P \left(s_t ; \theta\right) =  \max_{a_{t}} \Bigg\lbrace u\left(s_t, a_{t} ; \theta\right) + \beta \mathbb E_{s_{t+1}} \Big[ V \left(s_{t+1} ; \theta\right) \Big| s_t, a_{t} ; \theta\Big] \Bigg\rbrace
$$ Given $\frac{\partial c}{\partial s}&amp;gt;0$, the policy function has the
form $$
P \left(s_t ; \theta\right) =  \begin{cases}1 &amp;amp; \text { if } s_t \geq \gamma(\theta) \newline 0 &amp;amp; \text { if } s_t&amp;lt;\gamma(\theta)\end{cases}
$$ where $\gamma$ is the replacement mileage.&lt;/p&gt;
&lt;p&gt;How would this compare with the optimal replacement mileage if HZ was
myopic?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Answer: HZ would wait until $R \leq c(s)$ for the replacement action&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;solving-the-model&#34;&gt;Solving the Model&lt;/h3&gt;
&lt;p&gt;Why do we want to solve for the value and policy functions?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;We want to know the agent’s optimal behavior and the equilibrium
outcomes&lt;/li&gt;
&lt;li&gt;and be able to conduct comparative statics/dynamics (a.k.a.
counterfactual simulations)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We have the &lt;strong&gt;Bellman Equation&lt;/strong&gt; $$
V\left(s_t ; \theta\right) = \max_{a_{t}} \Bigg\lbrace u\left(s_t, a_{t} ; \theta\right)+\beta \mathbb E_{s_{t+1}} \Big[V\left(s_{t+1} ; \theta\right) \ \Big|
\ s_t, a_{t} ; \theta\Big] \Bigg\rbrace
$$ Which we can compactly write as $$
V\left(s_t ; \theta\right) = T \Big( V\left(s_{t+1} ; \theta\right) \Big)
$$ &lt;strong&gt;Blackwell’s Theorem&lt;/strong&gt;: under regularity conditions, $T$ is a
contraction mapping with modulus $\beta$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Contraction Mapping Theorem&lt;/strong&gt;: $T$ has a fixed point and we can find
it by iterating $T$ from any starting value $V^{(0)}$.&lt;/p&gt;
&lt;h3 id=&#34;value-function-iteration&#34;&gt;Value Function Iteration&lt;/h3&gt;
&lt;p&gt;What does &lt;strong&gt;Blackwell’s Theorem&lt;/strong&gt; allow us to do?&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Start with any arbitrary function $V^{(0)}(\cdot)$&lt;/li&gt;
&lt;li&gt;Apply the mapping $T$ to get $V^{(1)}(\cdot) = T (V^{(0)}(\cdot))$&lt;/li&gt;
&lt;li&gt;Apply again $V^{(2)}(\cdot) = T (V^{(1)}(\cdot))$&lt;/li&gt;
&lt;li&gt;Continue applying $T$ , and $V^{(k)}$ will converge to the unique
fixed point of $T$
&lt;ul&gt;
&lt;li&gt;i.e., the true value function $V(s_t; \theta)$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Once we have $V(s_t; \theta)$, it’s fairly trivial to compute the
policy function $P(s_t; \theta)$
&lt;ul&gt;
&lt;li&gt;Static optimization problem (given $V$)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;This process is called &lt;strong&gt;value function iteration&lt;/strong&gt;&lt;/p&gt;
&lt;h3 id=&#34;how-to-reconcile-model-and-data&#34;&gt;How to Reconcile Model and Data?&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Ideal Estimation Routine&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Pick a parameter value $\theta$&lt;/li&gt;
&lt;li&gt;Solve value and policy function (&lt;em&gt;inner loop&lt;/em&gt;)&lt;/li&gt;
&lt;li&gt;Match &lt;em&gt;predicted choices&lt;/em&gt; with &lt;em&gt;observed choices&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;Find the parameter value $\hat \theta$ that best fits the data
(&lt;em&gt;outer loop&lt;/em&gt;)
&lt;ul&gt;
&lt;li&gt;Makes the observed choices “closest” to the predicted choices&lt;/li&gt;
&lt;li&gt;(or maximizes the likelihood of the observed choices)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;Issue&lt;/strong&gt;: model easily &lt;strong&gt;rejected&lt;/strong&gt; by the data&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;The policy function takes the the form: replace iff
$s_t \geq \gamma(\theta)$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Can’t explain the coexistence of e.g. “&lt;em&gt;a bus without replacement at
22K miles&lt;/em&gt;” and “&lt;em&gt;another bus being replaced at 17K mile&lt;/em&gt;s” in the
data&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;We need some &lt;strong&gt;unobservables&lt;/strong&gt; in the model to explain why observed
choices do not exactly match predicted choices&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;rust-1987---estimation&#34;&gt;Rust (1987) - Estimation&lt;/h2&gt;
&lt;h3 id=&#34;uncertainty&#34;&gt;Uncertainty&lt;/h3&gt;
&lt;p&gt;How can we explain different replacement actions at different mileages
in the data?&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Add other observables&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Add some stochastic element&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;But &lt;strong&gt;where&lt;/strong&gt;? Two options&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Randomness in decisions
&lt;ul&gt;
&lt;li&gt;I.e. &lt;em&gt;“Harold Zurcher sometimes would like to replace the bus
engine but he forgets”&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;Probably still falsifiable&lt;/li&gt;
&lt;li&gt;Also need “&lt;em&gt;Harold Zurcher sometimes would like not to replace
but replacement happens”&lt;/em&gt; 🤔🤔🤔&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Randomness in the state
&lt;ul&gt;
&lt;li&gt;Harold Zurcher knows something that we don’t&lt;/li&gt;
&lt;li&gt;He &lt;strong&gt;always makes the optimal decision&lt;/strong&gt; but based on somethig
we don’t observe&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;unobservables&#34;&gt;Unobservables&lt;/h3&gt;
&lt;p&gt;Rust uses the following &lt;strong&gt;utility specification&lt;/strong&gt;: $$
u\left(s_t, a_{t}, {\color{red}{\epsilon_{t}}} ; \theta\right) = u\left(s_t, a_{t} ; \theta\right) + {\color{red}{\epsilon_{a_{t} t}}} = \begin{cases} - c\left(s_t ; \theta\right) + {\color{red}{\epsilon_{0 t}}} &amp;amp; \text { if } \ a_{t}=0 \newline \newline -R-c(0 ; \theta) + {\color{red}{\epsilon_{1 t}}} &amp;amp; \text { if } \ a_{t}=1 \end{cases}
$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The $\epsilon_{it}$ are components of utility of alternative $a$
that are observed by HZ but not by us, the econometrician.
&lt;ul&gt;
&lt;li&gt;E.g., the fact that an engine is running unusually smoothly
given its mileage,&lt;/li&gt;
&lt;li&gt;or the fact that HZ is sick and doesn’t feel like replacing the
engine this month&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Note&lt;/strong&gt;: we have assumed addictive separability of $\epsilon$&lt;/li&gt;
&lt;li&gt;The $\epsilon_a$s also affect HZ’s replacement decision&lt;/li&gt;
&lt;li&gt;$\epsilon_{it}$ are &lt;strong&gt;both observed and relevant&lt;/strong&gt; $\to$ part of the
state space&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;Can we still &lt;strong&gt;solve&lt;/strong&gt; the model? Can we &lt;strong&gt;estimate&lt;/strong&gt; it?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;unobservables-2&#34;&gt;Unobservables (2)&lt;/h3&gt;
&lt;p&gt;The &lt;strong&gt;Bellman Equation&lt;/strong&gt; becomes $$
V \Big( {\color{red}{ \lbrace s_\tau \rbrace_{\tau=1}^t , \lbrace \epsilon_\tau \rbrace_{\tau=1}^t }} ; \theta \Big) = \max_{a_{t}} \Bigg\lbrace u\left(s_t, a_{t} ; \theta\right) + {\color{red}{\epsilon_{it}}} + \beta \mathbb E_{s_{t+1}, {\color{red}{\epsilon_{t+1}}}} \Big[V\left(s_{t+1}, {\color{red}{\epsilon_{it+1}}} ; \theta\right) \ \Big|
\ {\color{red}{ \lbrace s_\tau \rbrace_{\tau=1}^t , \lbrace \epsilon_\tau \rbrace_{\tau=1}^t }}, a_{t} ; \theta\Big] \Bigg\rbrace
$$ &lt;strong&gt;Issues&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The problem is &lt;strong&gt;not Markow&lt;/strong&gt; anymore
&lt;ul&gt;
&lt;li&gt;Is $\epsilon_t$ correlated with $\epsilon_{t-\tau}$? How?&lt;/li&gt;
&lt;li&gt;Is $\epsilon_t$ correlated with $s_t$? And $s_{t-\tau}$? How?&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Dimension of the &lt;strong&gt;state space&lt;/strong&gt; has increased
&lt;ul&gt;
&lt;li&gt;From
$k = (k \text{ points})^{1 \text{ variable} \times 1 \text{ period}}$
points, to
$\infty = (k \text{ points})^{3 \text{ variables} \times \infty \text{ periods}}$
🤯🤯&lt;/li&gt;
&lt;li&gt;Assuming all variables assume $k$ values&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Number of variables to integrate over to compute &lt;strong&gt;expectation&lt;/strong&gt;
$\mathbb E$ has increased
&lt;ul&gt;
&lt;li&gt;From one variable, $s$, to three,
$(s, \epsilon_{0}, \epsilon_{1})$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;assumptions&#34;&gt;Assumptions&lt;/h3&gt;
&lt;p&gt;Rust makes &lt;strong&gt;4 assumptions&lt;/strong&gt; to make the problem tractable:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;First order Markow process of $\epsilon$&lt;/li&gt;
&lt;li&gt;Conditional independence of $\epsilon_t | s_t$ from $\epsilon_{t-1}$
and $s_{t-1}$&lt;/li&gt;
&lt;li&gt;Independence of $\epsilon_t$ from $s_t$&lt;/li&gt;
&lt;li&gt;Logit distribution of $\epsilon$&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;assumption-1&#34;&gt;Assumption 1&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;A1&lt;/strong&gt;: first-order markov process of $\epsilon$ $$
\Pr \Big(s_{t+1}, \epsilon_{t+1} \Big| s_{1}, &amp;hellip;, s_t, \epsilon_{1}, &amp;hellip;, \epsilon_{t}, a_{t} ; \theta\Big) = \Pr \Big(s_{t+1}, \epsilon_{t+1} \Big| s_t, \epsilon_{t}, a_{t} ; \theta \Big)
$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;What it buys&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$s$ and $\epsilon$ prior to current period are irrelevant&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;What it still allows&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;allows $s_t$ to be correlated with $\epsilon_t$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;What are we assuming away&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Any sort of longer run dependence&lt;/li&gt;
&lt;li&gt;Does it matter? If yes, just re-consider what is one time period&lt;/li&gt;
&lt;li&gt;Or make the state space larger (as usual in Markow processes)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;assumption-1---implications&#34;&gt;Assumption 1 - Implications&lt;/h3&gt;
&lt;p&gt;The Bellman Equation becomes $$
V\left(s_t, {\color{red}{\epsilon_{t}}} ; \theta\right) = \max_{a_{t}} \Bigg\lbrace u\left(s_t, a_{t} ; \theta\right) + {\color{red}{\epsilon_{a_{t} t}}} + \beta \mathbb E_{s_{t+1}, {\color{red}{\epsilon_{t+1}}}} \Big[V(s_{t+1}, {\color{red}{\epsilon_{t+1}}} ; \theta) \ \Big| \ s_t, a_{t}, {\color{red}{\epsilon_{t}}} ; \theta \Big] \Bigg\rbrace
$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Now the &lt;strong&gt;state&lt;/strong&gt; is $(s_t, \epsilon_t)$
&lt;ul&gt;
&lt;li&gt;sufficient, because defines both current utility and (the
expectation of) next-period state, under the first-order Markov
assumption&lt;/li&gt;
&lt;li&gt;$\epsilon_t$ is now analogous to $s_t$&lt;/li&gt;
&lt;li&gt;State space now is
$k^3 = (k \text{ points})^{3 \text{ variables} \times 1 \text{ period}}$
&lt;ul&gt;
&lt;li&gt;From
$\infty = (k \text{ points})^{3 \text{ variables} \times \infty \text{ periods}}$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Now we could use &lt;strong&gt;value function iteration&lt;/strong&gt; to solve the problem
&lt;ul&gt;
&lt;li&gt;If $\epsilon_t$ is continuous, it has to be discretised&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;assumption-1---issues&#34;&gt;Assumption 1 - Issues&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Open issues&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Curse of dimensionality in the state space&lt;/strong&gt;:
($s_t, \epsilon_{0t}, \epsilon_{1t}$)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Before, there were $k$ points in state space (discrete values of
$x$)&lt;/li&gt;
&lt;li&gt;Now there are $k^3$ : $k$ each for $s$, $\epsilon_0$,
$\epsilon_1$
&lt;ul&gt;
&lt;li&gt;(Assuming we discretize all state variables into $k$ values)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Generally, number of points in state space (and thus
computational time) increases exponentially in the number of
variables&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Curse of dimensionality in the expected value&lt;/strong&gt;:
$\mathbb E_{s_{t+1}, \epsilon_{0,t+1}, \epsilon_{1,t+1}}$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;For each point in state space (at each iteration of the
contraction mapping), need to compute&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$
\mathbb E_{s_{t+1}, \epsilon_{t+1}} \Big[V (s_{t+1}, \epsilon_{t+1} ; \theta) \ \Big|  \ s_t, a_{t}, \epsilon_{t} ; \theta \Big]
$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Before, this was a 1-dimensional integral (or sum), now it’s
3-dimensional&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Initial conditions&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;assumption-2&#34;&gt;Assumption 2&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;A2&lt;/strong&gt;: conditional independence of $\epsilon_t | s_t$ from
$\epsilon_{t-1}$ and $s_{t-1}$ $$
\Pr \Big(s_{t+1}, \epsilon_{t+1} \Big| s_t, \epsilon_{t}, a_{t} ; \theta \Big) = \Pr \Big( \epsilon_{t+1} \Big| s_{t+1} ; \theta \Big) \Pr \Big( s_{t+1} \Big| s_t, a_{t} ; \theta \Big)
$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;What it buys&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$s_{t+1}$ is independent of $\epsilon_t$&lt;/li&gt;
&lt;li&gt;$\epsilon_{t+1}$ is independent of $\epsilon_t$ and $s_t$,
conditional on $s_{t+1}$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;What it still allows&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\epsilon$ can be correlated across time, but only through the
$s$ process&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;What are we assuming away&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Any time of persistent heterogeneity&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Does it matter? Easily yes&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;There are tons of applications where the unobservables are
either fixed or correlated over time&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;If fixed, there are methods to handle unobserved
heterogeneity (i.e. bus “types”)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;assumption-2---implications&#34;&gt;Assumption 2 - Implications&lt;/h3&gt;
&lt;p&gt;The Bellman Equation is $$
V\left(s_t, {\color{red}{\epsilon_{t}}} ; \theta\right) = \max_{a_{t}} \Bigg\lbrace u\left(s_t, a_{t} ; \theta\right) + {\color{red}{\epsilon_{a_{t} t}}} + \beta \mathbb E_{s_{t+1}, {\color{red}{\epsilon_{t+1}}}} \Big[V (s_{t+1}, {\color{red}{\epsilon_{t+1}}} ; \theta) \Big| s_t, a_{t} ; \theta \Big] \Bigg\rbrace
$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Now $\epsilon_{t}$ is noise that &lt;strong&gt;doesn’t affect the future&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;That is, conditional on $s_{t+1}$, $\epsilon_{t+1}$ is
uncorrelated with $\epsilon_{t}$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Remeber&lt;/strong&gt;: if $\epsilon$ does not affect the future, it should’t be
in the state space!&lt;/p&gt;
&lt;p&gt;How? Integrate it out.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;rust-shortcut-asv&#34;&gt;Rust Shortcut: ASV&lt;/h3&gt;
&lt;p&gt;Rust: define the &lt;strong&gt;alternative-specific value function&lt;/strong&gt; $$
\begin{align}
&amp;amp;\bar V_0 \left(s_t ; \theta\right) = u\left(s_t, 0 ; \theta\right) + \beta \mathbb E_{s_{t+1}, {\color{red}{\epsilon_{t+1}}}} \Big[V\left(s_{t+1}, {\color{red}{\epsilon_{t+1}} }; \theta\right) | s_t, a_{t}=0 ; \theta\Big] \newline
&amp;amp;\bar V_1 \left(s_t ; \theta\right) = u\left(s_t, 1 ; \theta\right) + \beta \mathbb E_{s_{t+1}, {\color{red}{\epsilon_{t+1}}}} \Big[V\left(s_{t+1}, {\color{red}{\epsilon_{t+1}}} ; \theta\right) | s_t, a_{t}=1 ; \theta\Big]
\end{align}
$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;$\bar V_0 (s_t)$ is the present discounted value of not replacing,
net of $\epsilon_{0t}$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;The state does not depend on&lt;/strong&gt; $\epsilon_{t}$!&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;What is the relationship with the value function? $$
V\left(s_t, \epsilon_{t} ; \theta\right) = \max_{a_{t}} \Bigg\lbrace \begin{array}{l}
\bar V_0 \left(s_t ; \theta\right)+\epsilon_{0 t}
\ ; \newline
\bar V_1 \left(s_t ; \theta\right)+\epsilon_{1 t}
\end{array} \Bigg\rbrace
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;We have a 1-to-1 mapping between
$V\left(s_t, \epsilon_{t} ; \theta\right)$ and
$\bar V_a \left(s_t ; \theta\right)$ !&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;If we have one, we can get the other&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;rust-shortcut&#34;&gt;Rust Shortcut&lt;/h3&gt;
&lt;p&gt;Can we solve for $\bar V$?&lt;/p&gt;
&lt;p&gt;Yes! They have a &lt;strong&gt;recursive formulation&lt;/strong&gt; $$
\begin{aligned}
&amp;amp;
\bar V_0 \left(s_t ; \theta\right) = u\left(s_t, 0 ; \theta\right) + \beta \mathbb E_{s_{t+1}, {\color{red}{\epsilon_{t+1}}}} \Bigg[ \max_{a_{t+1}} \Bigg\lbrace \begin{array}{l}
\bar V_0 \left(s_{t+1} ; \theta\right) + {\color{red}{\epsilon_{0 t+1}}}
\ ; \newline
\bar V_1 \left(s_{t+1} ; \theta\right) + {\color{red}{\epsilon_{1 t+1}}}
\end{array} \Bigg\rbrace \ \Bigg| \ s_t, a_{t}=0 ; \theta \Bigg] \newline
&amp;amp;
\bar V_1 \left(s_t ; \theta\right) = u\left(s_t, 1 ; \theta\right) + \beta \mathbb E_{s_{t+1}, {\color{red}{\epsilon_{t+1}}}} \Bigg[ \max_{a_{t+1}} \Bigg\lbrace \begin{array}{l}
\bar V_0 \left(s_{t+1} ; \theta\right) + {\color{red}{\epsilon_{0 t+1}}}
\ ; \newline
\bar V_1 \left(s_{t+1} ; \theta\right) + {\color{red}{\epsilon_{1 t+1}}}
\end{array} \Bigg\rbrace \ \Bigg| \ s_t, a_{t}=1 ; \theta \Bigg] \newline
\end{aligned}
$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Rust (&lt;a href=&#34;#ref-rust1988maximum&#34;&gt;1988&lt;/a&gt;) shows that it’s a joint
contraction mapping&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Memo&lt;/strong&gt;: the state space now is
$2k = (2 \text{ actions}) \times (k \text{ points})^{1 \text{ variables} \times 1 \text{ period}}$
&lt;ul&gt;
&lt;li&gt;instead of
$3^k = (k \text{ points})^{3 \text{ variables} \times 1 \text{ period}}$&lt;/li&gt;
&lt;li&gt;Much smaller!&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Lesson&lt;/strong&gt;: any state variable that does not affect continuation
values (the future) does not have to be in the “actual” state space&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;assumption-2---implications-1&#34;&gt;Assumption 2 - Implications&lt;/h3&gt;
&lt;p&gt;We can also &lt;strong&gt;split the expectation&lt;/strong&gt; in the alternative-specific value
function $$
\begin{aligned}
&amp;amp;
\bar V_0 \left(s_t ; \theta\right) = u\left(s_t, 0 ; \theta\right) + \beta \mathbb E_{s_{t+1}} \Bigg[ \mathbb E_{{\color{red}{\epsilon_{t+1}}}} \Bigg[ \max_{a_{t+1}} \Bigg\lbrace \begin{array}{l}
\bar V_0 \left(s_{t+1} ; \theta\right) + {\color{red}{\epsilon_{0 t+1}}}
\ ; \newline
\bar V_1 \left(s_{t+1} ; \theta\right) + {\color{red}{\epsilon_{1 t+1}}}
\end{array} \Bigg\rbrace \ \Bigg| \ s_t \Bigg] \ \Bigg| \ s_t, a_{t}=0 ; \theta \Bigg] \newline
&amp;amp;
\bar V_1 \left(s_t ; \theta\right) = u\left(s_t, 1 ; \theta\right) + \beta \mathbb E_{s_{t+1}} \Bigg[ \mathbb E_{{\color{red}{\epsilon_{t+1}}}} \Bigg[ \max_{a_{t+1}} \Bigg\lbrace \begin{array}{l}
\bar V_0 \left(s_{t+1} ; \theta\right) + {\color{red}{\epsilon_{0 t+1}}}
\ ; \newline
\bar V_1 \left(s_{t+1} ; \theta\right) + {\color{red}{\epsilon_{1 t+1}}}
\end{array} \Bigg\rbrace \ \Bigg| \ s_t \Bigg] \ \Bigg| \ s_t, a_{t}=1 ; \theta \Bigg] \newline
\end{aligned}
$$ This allows us to concentrate on one single term $$
\mathbb E_{{\color{red}{\epsilon_{t+1}}}} \Bigg[ \max_{a_{t+1}} \Bigg\lbrace \begin{array}{l}
\bar V_0 \left(s_{t+1} ; \theta\right) + {\color{red}{\epsilon_{0 t+1}}}
\ ; \newline
\bar V_1 \left(s_{t+1} ; \theta\right) + {\color{red}{\epsilon_{1 t+1}}}
\end{array} \Bigg\rbrace \ \Bigg| \ s_t \Bigg]
$$ &lt;strong&gt;Open issues&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Distribution of $\epsilon_{t+1}$ has to be simulated&lt;/li&gt;
&lt;li&gt;Distribution of $\epsilon_{t+1}$ depends on $s_t$&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;assumption-3&#34;&gt;Assumption 3&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;A3&lt;/strong&gt;: independence of $\epsilon_t$ from $s_t$ $$
\Pr \Big( \epsilon_{t+1} \Big| s_{t+1} ; \theta \Big) \Pr \Big( s_{t+1} \Big| s_t, a_{t} ; \theta \Big) = \Pr \big( \epsilon_{t+1} \big| \theta \big) \Pr \Big( s_{t+1} \Big| s_t, a_{t} ; \theta \Big)
$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;What it buys&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\epsilon$ not correlated with anything $$
\mathbb E_{{\color{red}{\epsilon_{t+1}}}} \Bigg[ \max_{a_{t+1} \in \lbrace 0, 1 \rbrace } \Bigg\lbrace \begin{array}{l}
\bar V_0 \left(s_{t+1} ; \theta\right) + {\color{red}{\epsilon_{0 t+1}}}
\ ; \newline
\bar V_1 \left(s_{t+1} ; \theta\right) + {\color{red}{\epsilon_{1 t+1}}}
\end{array} \Bigg\rbrace \ \Bigg]
$$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;What are we assuming away&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Some state-specific noise… probably irrelevant&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Open Issues&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Distribution of $\epsilon_{t+1}$ has to be simulated&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;assumption-4&#34;&gt;Assumption 4&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;A4&lt;/strong&gt;: $\epsilon$ is type 1 extreme value distributed (logit)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;What it buys&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Closed form solution for $\mathbb E_{\epsilon_{t+1}}$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;What are we assuming away&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Different substitution patterns&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Relevant? Maybe, if there are at least three options (here
binary choice)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;As logit assumption in demand estimation&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Logit magic&lt;/strong&gt; 🧙🪄 $$
\mathbb E_{\epsilon} \Bigg[ \max_n \bigg( \Big\lbrace \delta_n + \epsilon_n \Big\rbrace_{n=1}^N \bigg) \Bigg] = 0.5772 + \ln \bigg( \sum_{n=1}^N e^{\delta_n} \bigg)
$$&lt;/p&gt;
&lt;p&gt;where $0.5772$ is Euler’s constant&lt;/p&gt;
&lt;h3 id=&#34;assumption-4---implications&#34;&gt;Assumption 4 - Implications&lt;/h3&gt;
&lt;p&gt;The Bellman equation becomes $$
\begin{aligned}
&amp;amp;
\bar V_0 \left(s_t ; \theta\right) = u\left(s_t, 0 ; \theta\right) + \beta \mathbb E_{s_{t+1}} \Bigg[ 0.5772 + \ln \Bigg( \sum_{a&amp;rsquo; \in \lbrace 0, 1 \rbrace} e^{\bar V_{a&amp;rsquo;} (s_{t+1} ; \theta)} \Bigg) \ \Bigg| \ s_t, a_{t}=0 ; \theta \Bigg] \newline
&amp;amp;
\bar V_1 \left(s_t ; \theta\right) = u\left(s_t, 1 ; \theta\right) + \beta \mathbb E_{s_{t+1}} \Bigg[ 0.5772 + \ln \Bigg( \sum_{a&amp;rsquo; \in \lbrace 0, 1 \rbrace} e^{\bar V_{a&amp;rsquo;} (s_{t+1} ; \theta)} \Bigg) \ \Bigg| \ s_t, a_{t}=1 ; \theta \Bigg] \newline
\end{aligned}
$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;We got &lt;strong&gt;fully rid of $\epsilon$&lt;/strong&gt;!
&lt;ul&gt;
&lt;li&gt;How? With a lot of assumptions&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;estimation&#34;&gt;Estimation&lt;/h3&gt;
&lt;p&gt;So far we have analysized how the &lt;strong&gt;4 assumptions&lt;/strong&gt; help &lt;strong&gt;solving the
model&lt;/strong&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;What about &lt;strong&gt;estimation&lt;/strong&gt;?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Maximum Likelihood&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;For a single bus, the &lt;strong&gt;likelihood function&lt;/strong&gt; is&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$
\mathcal L = \Pr \Big(s_{1}, &amp;hellip; , s_T, a_{0}, &amp;hellip; , a_{T} \ \Big| \ s_{0} ; \theta\Big)
$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;i.e. probability of observed decisions
$\lbrace a_{0}, &amp;hellip; , a_{T} \rbrace$&lt;/li&gt;
&lt;li&gt;and sequence of states $\lbrace s_{1}, &amp;hellip; , s_T \rbrace$&lt;/li&gt;
&lt;li&gt;conditional on the initial state $s_0$&lt;/li&gt;
&lt;li&gt;and the parameter values $\theta$&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;What is the impact of the 4 assumptions on the likelihood function?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;likelihood-function-a1&#34;&gt;Likelihood Function (A1)&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;A1&lt;/strong&gt;: First order Markow process of $\epsilon$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;We gain independence across time&lt;/li&gt;
&lt;li&gt;We can decompose the joint distribution in marginals across time&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$
\begin{align}
\mathcal L(\theta) &amp;amp;=  \Pr \Big(s_{1}, &amp;hellip; , s_T, a_{0}, &amp;hellip; , a_{T} \Big| s_{0} ; \theta\Big)\newline
&amp;amp;=  \prod_{t=1}^T \Pr \Big(a_{t+1} , s_{t+1} \Big| s_t, a_t ; \theta\Big)
\end{align}
$$&lt;/p&gt;
&lt;h3 id=&#34;likelihood-function-a2&#34;&gt;Likelihood Function (A2)&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;A2&lt;/strong&gt;: independence of $\epsilon_t$ from $\epsilon_{t-1}$ and $s_{t-1}$
on $s_t$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;We can decompose the joint distribution of $a_t$ and $s_{t+1}$ into
marginals $$
\begin{align}
\mathcal L(\theta) &amp;amp;= \prod_{t=1}^T \Pr \Big(a_{t+1} , s_{t+1} \Big| s_t, a_t ; \theta\Big)
= \newline
&amp;amp;= \prod_{t=1}^T \Pr \big(a_t \big| s_t ; \theta\big) \Pr \Big(s_{t+1} \Big| s_t, a_t ; \theta\Big)
\end{align}
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$\Pr \big(s_{t+1} \big| s_t, a_t ; \theta\big)$ can be estimated
from the data&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;we’ll come back to it&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;for $\Pr \big(a_t \big| s_t ; \theta\big)$ we need the two remaining
assumptions&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;likelihood-function-a3&#34;&gt;Likelihood Function (A3)&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;A3&lt;/strong&gt;: Independence of $\epsilon_t$ from $s_t$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;No need to condition on $s_t$&lt;/li&gt;
&lt;li&gt;E.g. probability of replacement&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$
\begin{align}
\Pr \big(a_t=1 \big| s_t ; \theta \big) &amp;amp;= \Pr \Big( \bar V_1 (s_{t+1} ; \theta) + \epsilon_{1 t+1} \geq \bar V_0 (s_{t+1} ; \theta) + \epsilon_{0 t+1} \ \Big| \ s_t ; \theta \Big)
= \newline
&amp;amp;= \Pr \Big( \bar V_1 (s_{t+1} ; \theta) + \epsilon_{1 t+1} \geq \bar V_0 (s_{t+1} ; \theta) + \epsilon_{0 t+1} \ \Big| \ \theta \Big)
\end{align}
$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;In words: same distribution of shocks in every state&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;likelihood-function-a4&#34;&gt;Likelihood Function (A4)&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;A4&lt;/strong&gt;: Logit distribution of $\epsilon$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;E.g. probability of replacement becomes&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$
\begin{align}
\Pr \big(a_t=1 \big| s_t ; \theta \big) &amp;amp;= \Pr \Big( \bar V_1 (s_{t+1} ; \theta) + \epsilon_{1 t+1} \geq \bar V_0 (s_{t+1} ; \theta) + \epsilon_{0 t+1} \ \Big| \ \theta \Big)
= \newline
&amp;amp;= \frac{e^{\bar V_1 (s_{t+1} ; \theta)}}{e^{\bar V_0 (s_{t+1} ; \theta)} + e^{\bar V_1 (s_{t+1} ; \theta)}}
\end{align}
$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;We have a closed form expression!&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;likelihood-function&#34;&gt;Likelihood Function&lt;/h3&gt;
&lt;p&gt;The &lt;strong&gt;final form&lt;/strong&gt; of the likelihood function for one bus is $$
\mathcal L(\theta) = \prod_{t=1}^T \Pr\big(a_t \big| s_t ; \theta \big) \Pr \Big(s_{t+1} \ \Big| \ s_t, a_t ; \theta\Big)
$$ where&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\Pr \Big(s_{t+1} \ \Big| \ s_t, a_t ; \theta\Big)$ can be estimated
from the data
&lt;ul&gt;
&lt;li&gt;given mileage $x$ and investment decision $a$, what are the
observed frequencies of future states $x&amp;rsquo;$?&lt;/li&gt;
&lt;li&gt;does not have to depend on $\theta$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;$\Pr\big(a_t \big| s_t ; \theta \big)$ depends on
$\bar V_a (s ; \theta)$
&lt;ul&gt;
&lt;li&gt;$\bar V_a (s ; \theta)$ we know how to compute&lt;/li&gt;
&lt;li&gt;given a value of $\theta$&lt;/li&gt;
&lt;li&gt;solve by value function iteration&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;likelihood-function-2&#34;&gt;Likelihood Function (2)&lt;/h3&gt;
&lt;p&gt;Since we have may buses, $j$, the likelihood of the data is $$
\mathcal L(\theta) = \prod_{j} \mathcal L_j (\theta) = \prod_{j} \prod_{t=1}^T \Pr\big(a_{jt} \big| s_{jt} ; \theta \big) \Pr \Big(s_{j,t+1} \ \Big| \ s_{jt}, a_{jt} ; \theta\Big)
$$ And, as usual, we prefer to work with log-likelihoods $$
\log \mathcal L(\theta) = \sum_{j} \sum_{t=1}^T \Bigg( \log \Pr\big(a_{jt} \big| s_{jt} ; \theta \big) + \log\Pr \Big(s_{j,t+1} \ \Big| \ s_{jt}, a_{jt} ; \theta\Big) \Bigg)
$$&lt;/p&gt;
&lt;h3 id=&#34;estimation-1&#34;&gt;Estimation&lt;/h3&gt;
&lt;p&gt;Now we have all the pieces to estimate $\theta$!&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Procedure&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Estimate the state transition probabilities
$\Pr \big(s_{t+1} \big| s_t, a_t ; \theta\big)$&lt;/li&gt;
&lt;li&gt;Select a value of $\theta$&lt;/li&gt;
&lt;li&gt;Init a choice-specific value function
$\bar V_a^{(0)} (s_{t+1} ; \theta)$
&lt;ol&gt;
&lt;li&gt;Apply the Bellman operator to compute
$\bar V_a^{(1)} (s_{t+1} ; \theta)$&lt;/li&gt;
&lt;li&gt;Iterate until convergence to
$\bar V_d^{(k \to \infty)} (s_{t+1} ; \theta)$ (&lt;em&gt;inner loop&lt;/em&gt;)&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;Compute the choice probabilities
$\Pr \big(a_t\big| s_t ; \theta \big)$&lt;/li&gt;
&lt;li&gt;Compute the likelihood
$\mathcal L = \prod_j \prod_{t=1}^T \Pr \big(a_t \big| s_t ; \theta\big) \Pr \Big(s_{t+1} \Big| s_t, a_t ; \theta\Big)$&lt;/li&gt;
&lt;li&gt;Iterate (2-5) until you are have found a (possibly global) minimum
(&lt;em&gt;outer loop&lt;/em&gt;)&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;results&#34;&gt;Results&lt;/h3&gt;
&lt;p&gt;What do &lt;strong&gt;dynamics&lt;/strong&gt; add?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Static&lt;/strong&gt; demand curve ($\beta =0$) is much more sensitive to the
price of engine replacement. Why?
&lt;ul&gt;
&lt;li&gt;Compares present price with present savings&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;If you compare present price with flow of future benefits, you are
less price sensitive
&lt;ul&gt;
&lt;li&gt;More realistic&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;img src=&#34;../img/7_01.png&#34; style=&#34;width:50.0%&#34; /&gt;
&lt;h3 id=&#34;extensions&#34;&gt;Extensions&lt;/h3&gt;
&lt;p&gt;Main &lt;strong&gt;limitation&lt;/strong&gt; of Rust (&lt;a href=&#34;#ref-rust1987optimal&#34;&gt;1987&lt;/a&gt;): &lt;strong&gt;value
function iteration&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Costly: has to be done for each parameter explored during
optimization&lt;/li&gt;
&lt;li&gt;Particularly costly if the state space is large&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Solutions&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Solve the model without solving a fixed point problem
&lt;ul&gt;
&lt;li&gt;Hotz and Miller (&lt;a href=&#34;#ref-hotz1993conditional&#34;&gt;1993&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Solve the model and estimate the parameters at the same time
&lt;ul&gt;
&lt;li&gt;Inner and outer loop in parallel&lt;/li&gt;
&lt;li&gt;Imai, Jain, and Ching (&lt;a href=&#34;#ref-imai2009bayesian&#34;&gt;2009&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Treat the estimation as a constrained optimization problem
&lt;ul&gt;
&lt;li&gt;MPEC, as for demand&lt;/li&gt;
&lt;li&gt;Use off-the-shelf optimization algorithms&lt;/li&gt;
&lt;li&gt;Su and Judd (&lt;a href=&#34;#ref-su2012constrained&#34;&gt;2012&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;We’ll cover Hotz and Miller (&lt;a href=&#34;#ref-hotz1993conditional&#34;&gt;1993&lt;/a&gt;) since at
the core of the estimation of dynamic games.&lt;/p&gt;
&lt;h2 id=&#34;hotz--miller-1993&#34;&gt;Hotz &amp;amp; Miller (1993)&lt;/h2&gt;
&lt;h3 id=&#34;motivation-1&#34;&gt;Motivation&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Setting&lt;/strong&gt;: Harold Zurcher problem&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;same model&lt;/li&gt;
&lt;li&gt;same assumptions&lt;/li&gt;
&lt;li&gt;same notation&lt;/li&gt;
&lt;li&gt;same objective&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Problem&lt;/strong&gt;: computationally intense to do value function iteration&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Can we solve the model without solving a fixed point problem?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;estimation-in-rust&#34;&gt;Estimation in Rust&lt;/h3&gt;
&lt;p&gt;How did we estimate the model in Rust? &lt;strong&gt;Two main equations&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Solve the &lt;strong&gt;Bellman equation&lt;/strong&gt; of the alternative-specific value
function $$
{\color{green}{\bar V(s; \theta)}} = \tilde f( {\color{green}{\bar V(s; \theta)}})
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Compute the &lt;strong&gt;expected policy function&lt;/strong&gt; $$
{\color{blue}{P( \cdot | s; \theta)}} = \tilde g( {\color{green}{\bar V(s; \theta)}} ; \theta)
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Maximize the likelihood function&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;$$
\mathcal L(\theta) = \prod_{j} \prod_{t=1}^T {\color{blue}{ \Pr\big(a_{jt} \big| s_{jt} ; \theta \big)}} \Pr \Big(s_{j,t+1} \ \Big| \ s_{jt}, a_{jt} ; \theta\Big)
$$&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Can we remove step 1?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;hotz--miller-ideas&#34;&gt;Hotz &amp;amp; Miller Idea(s)&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Idea 1&lt;/strong&gt;: it would be great if we could start from something like $$
{\color{blue}{P(\cdot|s; \theta)}} = T( {\color{blue}{P(\cdot|s; \theta)}} ; \theta)
$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;No need to solve for the value function&lt;/li&gt;
&lt;li&gt;But we would still need a to solve a &lt;strong&gt;fixed point&lt;/strong&gt; problem&lt;/li&gt;
&lt;li&gt;Back from the start? No&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Idea 2&lt;/strong&gt;: could replace the RHS element with a &lt;strong&gt;consistent estimate&lt;/strong&gt;
$$
{\color{blue}{P(\cdot|s; \theta)}} = T( {\color{red}{\hat P(\cdot|s; \theta)}} ; \theta)
$$ And this could give us an estimating equation!&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Unclear? No problem, let’s go slowly step by step&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;two-main-equations&#34;&gt;Two Main Equations&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Bellman equation&lt;/strong&gt; $$
{\color{green}{\bar V_a \left(s_t ; \theta\right)}} = u\left(s_t, a ; \theta\right) + \beta \mathbb E_{s_{t+1}, \epsilon_{t+1}} \Bigg[ \max_{a&amp;rsquo;} \Big\lbrace {\color{green}{\bar V_{a&amp;rsquo;}}} \left(s_{t+1}; \theta\right) + \epsilon_{a&amp;rsquo;,t+1} \Big\rbrace \ \Big| \ s_t, a_t=a ; \theta \Bigg]
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Expected policy function&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;$$
{\color{blue}{\Pr \big(a_t=a \big| s_t ; \theta \big)}} = \Pr \Big( {\color{green}{\bar V_a (s_{t+1} ; \theta)}} + \epsilon_{a, t+1} \geq {\color{green}{\bar V_{a&amp;rsquo;} (s_{t+1} ; \theta)}} + \epsilon_{a&amp;rsquo;, t+1} , \ \forall a&amp;rsquo; \ \Big| \ \theta \Big)
$$&lt;/p&gt;
&lt;p&gt;Expected decision &lt;strong&gt;before&lt;/strong&gt; the shocks $\epsilon_t$ are realized&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Not&lt;/strong&gt; the policy function
&lt;ul&gt;
&lt;li&gt;The policy function maps
$s_t \times \epsilon \to \lbrace 0 , 1 \rbrace$&lt;/li&gt;
&lt;li&gt;The expected policy function maps $s_t \to [ 0 , 1 ]$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Easier to work with: does not depend on the shocks
&lt;ul&gt;
&lt;li&gt;Not a deterministic policy, but a stochastic one&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;hotz--miller---idea-1&#34;&gt;Hotz &amp;amp; Miller - Idea 1&lt;/h3&gt;
&lt;p&gt;How do we get from the two equations $$
\begin{aligned}
{\color{green}{\bar V(s; \theta)}} &amp;amp;= \tilde f( {\color{green}{\bar V(s; \theta)}}) \newline
{\color{blue}{P(\cdot|s; \theta)}} &amp;amp;= \tilde g( {\color{green}{\bar V(s; \theta)}} ; \theta)
\end{aligned}
$$ To one? $$
{\color{blue}{P(\cdot|s; \theta)}} = T ({\color{blue}{P(\cdot|s; \theta)}}; \theta)
$$ If we could express $\bar V$ in terms of $P$, … $$
\begin{aligned}
{\color{green}{\bar V(s; \theta)}} &amp;amp; = \tilde h( {\color{blue}{P(\cdot|s; \theta)}})  \newline
{\color{blue}{P(\cdot|s; \theta)}} &amp;amp;= \tilde g( {\color{green}{\bar V(s; \theta)}} ; \theta)
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;…. we could then substitute the first equation into the second …&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;But&lt;/strong&gt;, easier to work with a different representation of the value
function.&lt;/p&gt;
&lt;h3 id=&#34;expected-value-function&#34;&gt;Expected Value Function&lt;/h3&gt;
&lt;p&gt;Recall Rust &lt;strong&gt;value function&lt;/strong&gt; (not the alternative-specific $\bar V$)
$$
V\left(s_t, \epsilon_t ; \theta\right) = \max_{a_{t}} \Bigg\lbrace u \left( s_t, a_{t} ; \theta \right)  + \epsilon_{a_{t} t} + \beta \mathbb E_{s_{t+1}, \epsilon_{t+1}} \Big[V\left(s_{t+1}, \epsilon_{t+1} ; \theta\right) \Big| s_t, a_{t} ; \theta\Big] \Bigg\rbrace
$$ We can express it in terms of &lt;strong&gt;expected value function&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;$$
V\left(s_t ; \theta\right) = \mathbb E_{\epsilon_t} \Bigg[ \max_{a_{t}} \Bigg\lbrace u\left(s_t, a_t ; \theta\right) + \epsilon_{a_{t} t}+ \beta \mathbb E_{s_{t+1}} \Big[V\left(s_{t+1}; \theta\right) \Big| s_t, a_{t} ; \theta\Big] \Bigg\rbrace \Bigg]
$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Value of being in state $s_t$ without knowing the realization of the
shock $\epsilon_t$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;“Value of Harold Zurcher before opening the window and seeing
if it’s raining or not”&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Analogous to the relationship between policy funciton and expected
policy function&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;expectation of future value now is only over $s_{t+1}$&lt;/li&gt;
&lt;li&gt;$V\left(s_t ; \theta\right)$ can be solved via value function
iteration as the operator on the RHS is a contraction&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;representation-equivalence&#34;&gt;Representation Equivalence&lt;/h3&gt;
&lt;p&gt;Recall the &lt;strong&gt;alternative-specific value function&lt;/strong&gt; of Rust&lt;/p&gt;
&lt;p&gt;$$
\begin{align}
{\color{green}{\bar V_a \left( s_t ; \theta\right)}} &amp;amp;= u\left(s_t, d ; \theta\right) + \beta \mathbb E_{s_{t+1}, \epsilon_{t+1}} \Bigg[ \max_{a&amp;rsquo;} \Big\lbrace {\color{green}{\bar V_{a&amp;rsquo;} \left(s_{t+1}; \theta\right)}} + \epsilon_{a&amp;rsquo;,t+1} \Big\rbrace \ \Big| \ s_t, a_t=a ; \theta \Bigg] \newline
&amp;amp;=u\left(s_t, a ; \theta\right)+\beta \mathbb E_{s_{t+1}, \epsilon_{t+1}} \Big[ {\color{orange}{V \left( s_{t+1}, \epsilon_{t+1} ; \theta \right)}} \Big| s_t, a_t=a ; \theta \Big]
\newline
&amp;amp;= u \left( s_t, a ; \theta \right) + \beta \mathbb E_{s_{t+1}} \Big[ {\color{red}{V \left( s_{t+1} ; \theta \right)}} \Big| s_t, a_t=a; \theta \Big]
\end{align}
$$&lt;/p&gt;
&lt;p&gt;Relationship with the &lt;strong&gt;value function&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;$$
{\color{orange}{V \left(s_t, \epsilon_{t} ; \theta \right)}} = \max_{a_{t}} \Big\lbrace {\color{green}{ \bar  V_0 \left( s_t ; \theta \right)}} + \epsilon_{0t}, {\color{green}{\bar V_1 \left( s_t ; \theta \right)}} + \epsilon_{1t} \Big\rbrace
$$&lt;/p&gt;
&lt;p&gt;Relationship with the &lt;strong&gt;expected value function&lt;/strong&gt; $$
{\color{red}{V\left(s_t ; \theta\right)}} = \mathbb E_{\epsilon_t} \Big[ {\color{orange}{V\left(s_t, \epsilon_{t} ; \theta\right)}} \ \Big| \ s_t \Big]
$$&lt;/p&gt;
&lt;h3 id=&#34;goal&#34;&gt;Goal&lt;/h3&gt;
&lt;p&gt;We switched from &lt;strong&gt;alternative-specific value function&lt;/strong&gt;
${\color{green}{\bar V (s_t ; \theta)}}$ to &lt;strong&gt;expected value function&lt;/strong&gt;
${\color{red}{V(s_t ; \theta)}}$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;But the goal is the same&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Go from this representation $$
\begin{align}
{\color{red}{V(s ; \theta)}} &amp;amp; = f( {\color{red}{V(s ; \theta)}}) \newline
{\color{blue}{P(\cdot | s ; \theta)}} &amp;amp; = g( {\color{red}{V(s ; \theta)}}; \theta)
\end{align}
$$ To this $$
\begin{align}
{\color{red}{V(s ; \theta)}} &amp;amp; = h( {\color{blue}{P(\cdot|s ; \theta)}} ; \theta) \newline
{\color{blue}{P(\cdot|s ; \theta)}} &amp;amp; = g({\color{red}{V(s ; \theta)}}; \theta)
\end{align}
$$ I.e. we want to express the &lt;strong&gt;expected value function (EV)&lt;/strong&gt; in terms
of the &lt;strong&gt;expected policy function (EP)&lt;/strong&gt;.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;**Note **: the $f$, $g$ and $h$ functions are different functions now.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;express-ev-in-terms-of-ep-1&#34;&gt;Express EV in terms of EP (1)&lt;/h3&gt;
&lt;p&gt;First, let’s ged rid of one operator: the &lt;strong&gt;max&lt;/strong&gt; operator $$
V\left(s_t ; \theta\right)
= \sum_a \Pr \Big(a_t=a | s_t ; \theta \Big) * \left[\begin{array}{c}
u\left(s_t, a ; \theta\right) + \mathbb E_{\epsilon_t} \Big[\epsilon_{at}\Big| a_t=a, s_t\Big] \newline \qquad + \beta \mathbb E_{s_{t+1}} \Big[V\left(s_{t+1} ; \theta\right) \Big| s_t, a_t=a ; \theta\Big]
\end{array}\right]
$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;We are just substituting the $\max$ with the policy
$\Pr\left(a_t=a| s_t ; \theta\right)$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Important: we got rid of the $\max$ operator&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;But we are still taking the expectation over&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Future states $s_{t+1}$&lt;/li&gt;
&lt;li&gt;Shocks $\epsilon_t$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;express-ev-in-terms-of-ep-2&#34;&gt;Express EV in terms of EP (2)&lt;/h3&gt;
&lt;p&gt;Now we get rid of another operator: the expectation over $s_{t+1}$ $$
\mathbb E_{s_{t+1}} \Big[V\left(s_{t+1} ; \theta\right) \Big| s_t, a_t=a ; \theta\Big] \qquad \to \qquad \sum_{s_{t+1}} V\left(s_{t+1} ; \theta\right) \Pr \Big(s_{t+1} \Big| s_t, a_t=a ; \theta \Big)
$$ where&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\sum_{s_{t+1}}$ is the summation over the next states&lt;/li&gt;
&lt;li&gt;$\Pr (s_{t+1} | s_t, a_t=a ; \theta )$ is the transition probability
(conditional on a particular choice)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;so that the expected value function becomes $$
V\left(s_t ; \theta\right)
= \sum_a \Pr \Big(a_t=a | s_t ; \theta \Big) * \left[\begin{array}{c}
u\left(s_t, a ; \theta\right) + \mathbb E_{\epsilon_t} \Big[\epsilon_{at}\Big| a_t=a, s_t\Big] \newline + \beta \sum_{s_{t+1}} V\left(s_{t+1} ; \theta\right) \Pr \Big(s_{t+1} \Big| s_t, a_t=a ; \theta \Big)
\end{array}\right]
$$&lt;/p&gt;
&lt;h3 id=&#34;express-ev-in-terms-of-ep-3&#34;&gt;Express EV in terms of EP (3)&lt;/h3&gt;
&lt;p&gt;The previous equation, was defined at the state level $s_t$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;system of $k$ equations, 1 for each state (value of $x$)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If we &lt;strong&gt;stack&lt;/strong&gt; them, we can write them as $$
V\left(s ; \theta\right)
= \sum_a \Pr \Big(a \ \Big| \ s ; \theta \Big) .* \Bigg[
u\left(s, a ; \theta\right) + \mathbb E_{\epsilon} \Big[\epsilon_{a} \ \Big| \ a, s \Big] + \beta \ T(a ; \theta) \ V(s ; \theta) \Bigg]
$$ where&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$T(a)$: $k \times k$ matrix of transition probabilities from state
$s_t$ to $s_{t+1}$, given decision $a$&lt;/li&gt;
&lt;li&gt;$.*$ is the dot product operator (or element-wise matrix
multiplication)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;express-ev-in-terms-of-ep-4&#34;&gt;Express EV in terms of EP (4)&lt;/h3&gt;
&lt;p&gt;Now we have a system of $k$ equations in $k$ unknowns that we can solve.&lt;/p&gt;
&lt;p&gt;Tearing down notation to the bare minimum, we have $$
V = \sum_a P_a .* \bigg[ u_a + \mathbb E [\epsilon_a ] + \beta \ T_a \ V \bigg]
$$ which we can rewrite as $$
V - \beta \  \left( \sum_a P_a .* T_a \right) V = \sum_a P_a .* \bigg[ u_a + \mathbb E [\epsilon_a ] \bigg]
$$&lt;/p&gt;
&lt;p&gt;and finally we can solve for $V$ through the famous &lt;strong&gt;Hotz and Miller
inversion&lt;/strong&gt; $$
V = \left[I - \beta \ \sum_a P_a .* T_a \right]^{-1} \ * \ \left( \sum_a P_a \ .* \ \bigg[ u_a + \mathbb E [\epsilon_a] \bigg] \right)
$$ Solved? No. We still need to do something about
$\mathbb E [\epsilon_a]$.&lt;/p&gt;
&lt;h3 id=&#34;express-ev-in-terms-of-ep-5&#34;&gt;Express EV in terms of EP (5)&lt;/h3&gt;
&lt;p&gt;What is $\mathbb E [\epsilon_a]$?&lt;/p&gt;
&lt;p&gt;Let’s consider for example the expected value of the shock, conditional
on investment $$
\begin{aligned}
\mathbb E \Big[ \epsilon_{1 t} \ \Big| \ a_t = 1, \cdot \Big] &amp;amp;= \mathbb E \Big[ \epsilon_{t} \ \Big| \ \bar V_1 \left( s_t ; \theta \right) + \epsilon_{1 t} &amp;gt; \bar V_0 \left( s_t ; \theta \right) + \epsilon_{0 t} \Big] \newline
&amp;amp; = \mathbb E \Big[ \epsilon_{1 t} \ \Big| \ \bar V_1 \left( s_t ; \theta \right)  - \bar V_0 \left( s_t ; \theta \right) &amp;gt; \epsilon_{0 t} - \epsilon_{1 t} \Big]
\end{aligned}
$$ with &lt;strong&gt;logit magic&lt;/strong&gt; 🧙🪄 is $$
\mathbb E\left[\epsilon_{1 t} | a_{t}=1, s_t\right] = 0.5772 - \ln \left(P\left(s_t ; \theta\right)\right)
$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;where $0.5772$ is Euler’s constant.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We again got rid of another $\max$ operator!&lt;/p&gt;
&lt;h3 id=&#34;express-ev-in-terms-of-ep-6&#34;&gt;Express EV in terms of EP (6)&lt;/h3&gt;
&lt;p&gt;Now we can substitute it back and we have an equation which is &lt;em&gt;just&lt;/em&gt; a
function of primitives $$
\begin{aligned}
V(\cdot ; \theta) =&amp;amp; \Big[I-(1-P(\cdot ; \theta)) \beta T(0 ; \theta)-P(\cdot ; \theta) \beta T(1 ; \theta)\Big]^{-1}
\newline * &amp;amp; \left[
\begin{array}{c}
(1-P(\cdot ; \theta))\Big[u(\cdot, 0 ; \theta)+0.5772-\ln (1-P(\cdot ; \theta))\Big] \newline + P(\cdot ; \theta)\Big[u(\cdot, 1 ; \theta) + 0.5772 - \ln (P(\cdot ; \theta))\Big]
\end{array}
\right]
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;Or more compactly $$
V = \left[I - \beta \ \sum_a P_a .* T_a \right]^{-1} \ * \ \left( \sum_a P_a \ .* \ \bigg[ u_a + 0.5772 - \ln(P_d) \bigg] \right)
$$&lt;/p&gt;
&lt;h3 id=&#34;first-equation&#34;&gt;First Equation&lt;/h3&gt;
&lt;p&gt;What is the first equation? $$
V = \left[I - \beta \ \sum_a P_a .* T_a \right]^{-1} \ * \ \left( \sum_a P_a \ .* \ \bigg[ u_a + 0.5772 - \ln(P_a) \bigg] \right)
$$ &lt;strong&gt;Expected static payoff&lt;/strong&gt;:
$\sum_a P_a \ .* \ \bigg[ u_a + 0.5772 + \ln(P_a) \bigg]$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Is the expected static payoff of choice $a$ in each state,
$u_a + 0.5772 + \ln(P_a)$&lt;/li&gt;
&lt;li&gt;… integrated over the choice probabilities, $P_a$&lt;/li&gt;
&lt;li&gt;It’s a $k \times 1$ vector&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Unconditional transition probabilities&lt;/strong&gt;: $\sum_a P_a .* T_a$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Are the transition probabilities conditional on a choice $a$ for
every present and future state, $T_a$&lt;/li&gt;
&lt;li&gt;… integrated over the choice probabilities, $P_a$&lt;/li&gt;
&lt;li&gt;It’s a $k \times k$ matrix&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;recap&#34;&gt;Recap&lt;/h3&gt;
&lt;p&gt;We got our first equation $$
{\color{red}{V}} = \left[I - \beta \ \sum_a {\color{blue}{P_a}} .* T_a \right]^{-1} \ * \ \left( \sum_a {\color{blue}{P_a}} \ .* \ \bigg[ u_a + 0.5772 - \ln({\color{blue}{P_a}}) \bigg] \right)
$$&lt;/p&gt;
&lt;p&gt;I.e. $$
\begin{align}
{\color{red}{V(s ; \theta)}} &amp;amp; = h( {\color{blue}{P(s ; \theta)}} ; \theta) \newline
\end{align}
$$&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;What about the second equation
${\color{blue}{P(\cdot|s ; \theta)}} = g({\color{red}{V(s ; \theta)}}; \theta)$?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;from-v-to-p&#34;&gt;From V to P&lt;/h3&gt;
&lt;p&gt;In general, the expected probability of investment is $$
P(a=1; \theta)= \Pr \left[\begin{array}{c}
u(\cdot, 1 ; \theta)+\epsilon_{1 t}+\beta \mathbb E \Big[V(\cdot ; \theta) \Big| \cdot, a_{t}=1 ; \theta \Big]&amp;gt; \newline \qquad
u(\cdot, 0 ; \theta) + \epsilon_{0 t}+\beta \mathbb E \Big[V(\cdot ; \theta) \Big| \cdot, a_{t}=0 ; \theta \Big]
\end{array}\right]
$$&lt;/p&gt;
&lt;p&gt;With the &lt;strong&gt;logit assumption&lt;/strong&gt;, simplifies to $$
{\color{blue}{P(a=1 ; \theta)}} = \frac{\exp \Big(u(\cdot, 1 ; \theta)+\beta T(1 ; \theta) V(\cdot ; \theta) \Big)}{\sum_{a&amp;rsquo;} \exp \Big(u(\cdot, a&amp;rsquo; ; \theta)+\beta T(a&amp;rsquo; ; \theta) V(\cdot ; \theta) \Big)} = \frac{\exp (u_1 +\beta T_1 {\color{red}{V}} )}{\sum_{a&amp;rsquo;} \exp (u_{a&amp;rsquo;} +\beta T_{a&amp;rsquo;} {\color{red}{V}} )}
$$&lt;/p&gt;
&lt;p&gt;Now we have also the second equation! $$
\begin{align}
{\color{blue}{P(s ; \theta)}} &amp;amp; = g({\color{red}{V(s ; \theta)}}; \theta)
\end{align}
$$&lt;/p&gt;
&lt;h3 id=&#34;hotz--miller---idea-2&#34;&gt;Hotz &amp;amp; Miller - Idea 2&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Idea 2&lt;/strong&gt;: Replace ${\color{blue}{P} (\cdot)}$ on the RHS with a
&lt;em&gt;consistent&lt;/em&gt; estimator ${\color{Turquoise}{\hat P (\cdot)}}$ $$
{\color{cyan}{\bar P(\cdot ; \theta)}} = g(h({\color{Turquoise}{\hat P(\cdot)}} ; \theta); \theta)
$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;${\color{cyan}{\bar P(\cdot ; \theta_0)}}$ will converge to the true
${\color{blue}{P(\cdot ; \theta_0)}}$, because
${\color{Turquoise}{\hat P (\cdot)}}$ is converging to
${\color{blue}{P(\cdot ; \theta_0)}}$ asymptotically.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Note&lt;/strong&gt;: pay attention to $\theta_0$ vs $\theta$ here:
${\color{cyan}{\bar P(\cdot ; \theta)}}$ does &lt;strong&gt;not&lt;/strong&gt; generally
converge to ${\color{blue}{P(\cdot ; \theta)}}$for arbitrary
$\theta$, because ${\color{Turquoise}{\hat P(\cdot)}}$ is
converging to ${\color{blue}{P(\cdot ; \theta_0)}}$ but &lt;strong&gt;not&lt;/strong&gt;
${\color{blue}{P(\cdot ; \theta)}}$ with any $\theta$.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;How to compute ${\color{Turquoise}{\hat P(\cdot)}}$?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;From the data, you observe states and decisions&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;You can compute frequency of decisions given states&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;In Rust: frequency of engine replacement, given a mileage
(discretized)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Assumption&lt;/strong&gt;: you have enough data&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;What if a state is not realised?&lt;/li&gt;
&lt;li&gt;Use frequencies in &lt;em&gt;observed&lt;/em&gt; states to extrapolate frequencies
in &lt;em&gt;unobserved&lt;/em&gt; states&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;recap-1&#34;&gt;Recap&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Steps&lt;/strong&gt; so far&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Estimate the &lt;strong&gt;conditional choice probabilities&lt;/strong&gt;
${\color{Turquoise}{\hat P}}$ from the data&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Nonparametrically: frequency of each decision in each state&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Solve for the expected value function with the inverstion step $$
{\color{orange}{\hat V}} = \left[I - \beta \ \sum_a  {\color{Turquoise}{\hat P_a}} .* T_a \right]^{-1} \ * \ \left( \sum_a {\color{Turquoise}{\hat P_a}} \ .* \ \bigg[ u_a + 0.5772 - \ln({\color{Turquoise}{\hat P_a}}) \bigg] \right)
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Compute the &lt;strong&gt;predicted CCP&lt;/strong&gt;, given $V$ $$
{\color{cyan}{\bar P(a=1 ; \theta)}} = \frac{\exp (u_1 +\beta T_1 {\color{orange}{\hat V}} )}{\sum_{a&amp;rsquo;} \exp (u_{a&amp;rsquo;} +\beta T_{a&amp;rsquo;} {\color{orange}{\hat V}} )}
$$&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;blockquote&gt;
&lt;p&gt;What now? Use the estimated CCP to build an objective function.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;objective-function&#34;&gt;Objective Function&lt;/h3&gt;
&lt;p&gt;We have (at least) 2 options&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Hotz and Miller (&lt;a href=&#34;#ref-hotz1993conditional&#34;&gt;1993&lt;/a&gt;) use GMM&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;$$
\mathbb E \Big[a_t - \bar P(s_t, \theta) \ \Big| \ s_t \Big] = 0 \quad \text{ at } \quad \theta = \theta_0
$$&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Aguirregabiria and Mira (&lt;a href=&#34;#ref-aguirregabiria2002swapping&#34;&gt;2002&lt;/a&gt;)
use MLE
&lt;ul&gt;
&lt;li&gt;by putting $\bar P(s_t, \theta)$ in the likelihood function
instead of $P(s_t, \theta)$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;We will follow the second approach&lt;/p&gt;
&lt;h3 id=&#34;pseudo-likelihood&#34;&gt;Pseudo-Likelihood&lt;/h3&gt;
&lt;p&gt;The likelihood function for one bus is $$
\mathcal{L}(\theta) = \prod_{t=1}^{T}\left(\hat{\operatorname{Pr}}\left(a=1 \mid s_{t}; \theta\right) \mathbb{1}\left(a_{t}=1\right)+\left(1-\hat{\operatorname{Pr}}\left(a=0 \mid s_{t}; \theta\right)\right) \mathbb{1}\left(a_{t}=0\right)\right)
$$ where $\hat \Pr\big(a_{t} \big| s_{t} ; \theta \big)$ is a function
of&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;CCPs $\hat P$: estimated from data&lt;/li&gt;
&lt;li&gt;transition matrix $T$: estimated from the data, given $\theta$&lt;/li&gt;
&lt;li&gt;static payoffs $u$: known, given $\theta$&lt;/li&gt;
&lt;li&gt;discount factor $\beta$ : assumed&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;Why &lt;strong&gt;pseudo-likelihood&lt;/strong&gt;? We have inputed something that is not a
primitive but a consistent estimate of an equilibrium object, $\hat P$&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;comments&#34;&gt;Comments&lt;/h3&gt;
&lt;p&gt;Now a few comments on Hotz and Miller (&lt;a href=&#34;#ref-hotz1993conditional&#34;&gt;1993&lt;/a&gt;)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Computational bottleneck&lt;/li&gt;
&lt;li&gt;Aguirregabiria and Mira (&lt;a href=&#34;#ref-aguirregabiria2002swapping&#34;&gt;2002&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Importance of the T1EV assumption&lt;/li&gt;
&lt;li&gt;Data requirements&lt;/li&gt;
&lt;li&gt;Unobserved heterogeneity&lt;/li&gt;
&lt;li&gt;Identification&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;computational-bottleneck&#34;&gt;Computational Bottleneck&lt;/h3&gt;
&lt;p&gt;There is still 1 computational &lt;strong&gt;bottleneck&lt;/strong&gt; in HM: the inversion step
$$
V = \left[I - \beta \ \sum_a P_a .* T_a \right]^{-1} \ * \ \left( \sum_a P_a \ .* \ \bigg[ u_a + 0.5772 - \ln(P_a) \bigg] \right)
$$ The $\left[I - \beta \ \sum_a P_a .* T_a \right]$ matrix has
dimension $k \times k$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;With large state space, hard to invert&lt;/li&gt;
&lt;li&gt;Even with modern computational power&lt;/li&gt;
&lt;li&gt;Hotz et al. (&lt;a href=&#34;#ref-hotz1994simulation&#34;&gt;1994&lt;/a&gt;): forward simulation of
the value function
&lt;ul&gt;
&lt;li&gt;You have the policy, the transitions and the utilities&lt;/li&gt;
&lt;li&gt;Just compute discounted flow of payoffs&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Core&lt;/strong&gt; idea behind the estimation of dynamic games&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;aguirregabiria-mira-2002&#34;&gt;Aguirregabiria, Mira (2002)&lt;/h3&gt;
&lt;p&gt;Hotz and Miller (&lt;a href=&#34;#ref-hotz1993conditional&#34;&gt;1993&lt;/a&gt;) inversion gets us a
recursive equation in &lt;strong&gt;probability space&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;instead of the Bellman Equation in the &lt;strong&gt;value space&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$
\bar P(\cdot ; \theta) = g(h(\hat P(\cdot) ; \theta); \theta)
$$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Idea&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Do you gain something by iterating $K$ imes?
&lt;ul&gt;
&lt;li&gt;$K=1$: Hotz and Miller (&lt;a href=&#34;#ref-hotz1993conditional&#34;&gt;1993&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;$K \to \infty$: Rust (&lt;a href=&#34;#ref-rust1987optimal&#34;&gt;1987&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Monte Carlo simulations&lt;/strong&gt;: finite sample properties of K−stage
estimators improve monotonically with K
&lt;ul&gt;
&lt;li&gt;But especially for $K=2$!&lt;/li&gt;
&lt;li&gt;Really &lt;strong&gt;worth iterating once&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;type-1-ev-errors&#34;&gt;Type 1 EV errors&lt;/h3&gt;
&lt;p&gt;Crucial assumption&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Without logit errors, we need to simulate their distribution&lt;/li&gt;
&lt;li&gt;True also for Rust&lt;/li&gt;
&lt;li&gt;But it’s generally accepted
&lt;ul&gt;
&lt;li&gt;doesn’t imply it’s innocuous&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;data-requirements&#34;&gt;Data Requirements&lt;/h3&gt;
&lt;p&gt;For both Hotz et al. (&lt;a href=&#34;#ref-hotz1994simulation&#34;&gt;1994&lt;/a&gt;) and Rust
(&lt;a href=&#34;#ref-rust1987optimal&#34;&gt;1987&lt;/a&gt;), we need to &lt;strong&gt;discretize the state
space&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Can be complicated with continuous variables&lt;/li&gt;
&lt;li&gt;Problem also in Rust&lt;/li&gt;
&lt;li&gt;But particularly problematic in Hotz et al.
(&lt;a href=&#34;#ref-hotz1994simulation&#34;&gt;1994&lt;/a&gt;)
&lt;ul&gt;
&lt;li&gt;Relies crucially on consistency of CCP estimates&lt;/li&gt;
&lt;li&gt;Need sufficient &lt;strong&gt;variation in actions for each state&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;unobserved-heterogeneity&#34;&gt;Unobserved Heterogeneity&lt;/h3&gt;
&lt;p&gt;Hotz et al. (&lt;a href=&#34;#ref-hotz1994simulation&#34;&gt;1994&lt;/a&gt;) cannot handle unobserved
heterogeneity or “unobserved state variables” that are persistent over
time.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Example&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Suppose there are 2 bus types $\tau$: high and low quality&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;We &lt;strong&gt;don’t know the share&lt;/strong&gt; of types in the data&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;With Rust&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Parametrize the effect of the difference in qualities&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;E.g. high quality engines break less often&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Parametrize the proportion of high quality buses&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Solve the value function by type $V(s_t, \tau ; \theta)$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Integrate over types when computing choice probabilities $$
P(a|s) = \int P(a|s,\tau) P(\tau) =  \Pr(a|s, \tau=0) * \Pr(\tau=0) + \Pr(a|s, \tau=1) * \Pr(\tau=1)
$$&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;unobserved-heterogeneity-2&#34;&gt;Unobserved Heterogeneity (2)&lt;/h3&gt;
&lt;p&gt;What is the problem with Hotz et al. (&lt;a href=&#34;#ref-hotz1994simulation&#34;&gt;1994&lt;/a&gt;)?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;The unobserved heterogeneity generates &lt;strong&gt;persistency in choices&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;I don’t replace today because it’s high quality, but I also
probably don’t replace tomorrow either&lt;/li&gt;
&lt;li&gt;Decisions independent across time &lt;strong&gt;only conditional on type&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Likelihood of decisions must be integrated over types $$
\mathcal L (\theta) = \sum_{\tau_a} \prod_{t=1}^{T} \Pr (a_{jt}| s_{jt}, \tau_a) \Pr(\tau_a)
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Hotz &amp;amp; Miller needs &lt;strong&gt;consistent estimates&lt;/strong&gt; of $P(a, s, \tau)$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Difficult when $\tau$ is not observed!&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;identification&#34;&gt;Identification&lt;/h3&gt;
&lt;p&gt;Work on identification&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Rust (&lt;a href=&#34;#ref-rust1994structural&#34;&gt;1994&lt;/a&gt;) and Magnac and Thesmar
(&lt;a href=&#34;#ref-magnac2002identifying&#34;&gt;2002&lt;/a&gt;)
&lt;ul&gt;
&lt;li&gt;Rust (&lt;a href=&#34;#ref-rust1987optimal&#34;&gt;1987&lt;/a&gt;) is non-paramentrically
underidentified $\to$ parametric assumptions are essential&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Aguirregabiria and Suzuki
(&lt;a href=&#34;#ref-aguirregabiria2014identification&#34;&gt;2014&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Kalouptsidi, Scott, and Souza-Rodrigues
(&lt;a href=&#34;#ref-kalouptsidi2017non&#34;&gt;2017&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Abbring and Daljord (&lt;a href=&#34;#ref-abbring2020identifying&#34;&gt;2020&lt;/a&gt;)
&lt;ul&gt;
&lt;li&gt;Can identify discount factor with some “instrument” that shifts
future utilities but not current payoff&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Kalouptsidi et al. (&lt;a href=&#34;#ref-kalouptsidi2020partial&#34;&gt;2020&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;appendix&#34;&gt;Appendix&lt;/h2&gt;
&lt;h3 id=&#34;references-references&#34;&gt;References [references]&lt;/h3&gt;
&lt;div id=&#34;refs&#34; class=&#34;references csl-bib-body hanging-indent&#34;
markdown=&#34;1&#34;&gt;
&lt;div id=&#34;ref-abbring2020identifying&#34; class=&#34;csl-entry&#34; markdown=&#34;1&#34;&gt;
&lt;p&gt;Abbring, Jaap H, and Øystein Daljord. 2020. “Identifying the Discount
Factor in Dynamic Discrete Choice Models.” &lt;em&gt;Quantitative Economics&lt;/em&gt; 11
(2): 471–501.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-aguirregabiria2002swapping&#34; class=&#34;csl-entry&#34; markdown=&#34;1&#34;&gt;
&lt;p&gt;Aguirregabiria, Victor, and Pedro Mira. 2002. “Swapping the Nested Fixed
Point Algorithm: A Class of Estimators for Discrete Markov Decision
Models.” &lt;em&gt;Econometrica&lt;/em&gt; 70 (4): 1519–43.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-aguirregabiria2014identification&#34; class=&#34;csl-entry&#34;
markdown=&#34;1&#34;&gt;
&lt;p&gt;Aguirregabiria, Victor, and Junichi Suzuki. 2014. “Identification and
Counterfactuals in Dynamic Models of Market Entry and Exit.”
&lt;em&gt;Quantitative Marketing and Economics&lt;/em&gt; 12 (3): 267–304.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-becker1988theory&#34; class=&#34;csl-entry&#34; markdown=&#34;1&#34;&gt;
&lt;p&gt;Becker, Gary S, and Kevin M Murphy. 1988. “A Theory of Rational
Addiction.” &lt;em&gt;Journal of Political Economy&lt;/em&gt; 96 (4): 675–700.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-berry1992estimation&#34; class=&#34;csl-entry&#34; markdown=&#34;1&#34;&gt;
&lt;p&gt;Berry, Steven T. 1992. “Estimation of a Model of Entry in the Airline
Industry.” &lt;em&gt;Econometrica: Journal of the Econometric Society&lt;/em&gt;, 889–917.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-bresnahan1989empirical&#34; class=&#34;csl-entry&#34; markdown=&#34;1&#34;&gt;
&lt;p&gt;Bresnahan, Timothy F. 1989. “Empirical Studies of Industries with Market
Power.” &lt;em&gt;Handbook of Industrial Organization&lt;/em&gt; 2: 1011–57.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-crawford2005uncertainty&#34; class=&#34;csl-entry&#34; markdown=&#34;1&#34;&gt;
&lt;p&gt;Crawford, Gregory S, and Matthew Shum. 2005. “Uncertainty and Learning
in Pharmaceutical Demand.” &lt;em&gt;Econometrica&lt;/em&gt; 73 (4): 1137–73.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-erdem2003brand&#34; class=&#34;csl-entry&#34; markdown=&#34;1&#34;&gt;
&lt;p&gt;Erdem, Tülin, Susumu Imai, and Michael P Keane. 2003. “Brand and
Quantity Choice Dynamics Under Price Uncertainty.” &lt;em&gt;Quantitative
Marketing and Economics&lt;/em&gt; 1 (1): 5–64.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-erdem1996decision&#34; class=&#34;csl-entry&#34; markdown=&#34;1&#34;&gt;
&lt;p&gt;Erdem, Tülin, and Michael P Keane. 1996. “Decision-Making Under
Uncertainty: Capturing Dynamic Brand Choice Processes in Turbulent
Consumer Goods Markets.” &lt;em&gt;Marketing Science&lt;/em&gt; 15 (1): 1–20.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-golosov2006new&#34; class=&#34;csl-entry&#34; markdown=&#34;1&#34;&gt;
&lt;p&gt;Golosov, Mikhail, Aleh Tsyvinski, Ivan Werning, Peter Diamond, and
Kenneth L Judd. 2006. “New Dynamic Public Finance: A User’s Guide [with
Comments and Discussion].” &lt;em&gt;NBER Macroeconomics Annual&lt;/em&gt; 21: 317–87.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-gowrisankaran2012dynamics&#34; class=&#34;csl-entry&#34; markdown=&#34;1&#34;&gt;
&lt;p&gt;Gowrisankaran, Gautam, and Marc Rysman. 2012. “Dynamics of Consumer
Demand for New Durable Goods.” &lt;em&gt;Journal of Political Economy&lt;/em&gt; 120 (6):
1173–1219.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-handel2013adverse&#34; class=&#34;csl-entry&#34; markdown=&#34;1&#34;&gt;
&lt;p&gt;Handel, Benjamin R. 2013. “Adverse Selection and Inertia in Health
Insurance Markets: When Nudging Hurts.” &lt;em&gt;American Economic Review&lt;/em&gt; 103
(7): 2643–82.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-hendel2006measuring&#34; class=&#34;csl-entry&#34; markdown=&#34;1&#34;&gt;
&lt;p&gt;Hendel, Igal, and Aviv Nevo. 2006. “Measuring the Implications of Sales
and Consumer Inventory Behavior.” &lt;em&gt;Econometrica&lt;/em&gt; 74 (6): 1637–73.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-hotz1993conditional&#34; class=&#34;csl-entry&#34; markdown=&#34;1&#34;&gt;
&lt;p&gt;Hotz, V Joseph, and Robert A Miller. 1993. “Conditional Choice
Probabilities and the Estimation of Dynamic Models.” &lt;em&gt;The Review of
Economic Studies&lt;/em&gt; 60 (3): 497–529.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-hotz1994simulation&#34; class=&#34;csl-entry&#34; markdown=&#34;1&#34;&gt;
&lt;p&gt;Hotz, V Joseph, Robert A Miller, Seth Sanders, and Jeffrey Smith. 1994.
“A Simulation Estimator for Dynamic Models of Discrete Choice.” &lt;em&gt;The
Review of Economic Studies&lt;/em&gt; 61 (2): 265–89.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-igami2020artificial&#34; class=&#34;csl-entry&#34; markdown=&#34;1&#34;&gt;
&lt;p&gt;Igami, Mitsuru. 2020. “Artificial Intelligence as Structural Estimation:
Deep Blue, Bonanza, and AlphaGo.” &lt;em&gt;The Econometrics Journal&lt;/em&gt; 23 (3):
S1–24.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-imai2009bayesian&#34; class=&#34;csl-entry&#34; markdown=&#34;1&#34;&gt;
&lt;p&gt;Imai, Susumu, Neelam Jain, and Andrew Ching. 2009. “Bayesian Estimation
of Dynamic Discrete Choice Models.” &lt;em&gt;Econometrica&lt;/em&gt; 77 (6): 1865–99.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-kalouptsidi2020partial&#34; class=&#34;csl-entry&#34; markdown=&#34;1&#34;&gt;
&lt;p&gt;Kalouptsidi, Myrto, Yuichi Kitamura, Lucas Lima, and Eduardo A
Souza-Rodrigues. 2020. “Partial Identification and Inference for Dynamic
Models and Counterfactuals.” National Bureau of Economic Research.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-kalouptsidi2017non&#34; class=&#34;csl-entry&#34; markdown=&#34;1&#34;&gt;
&lt;p&gt;Kalouptsidi, Myrto, Paul T Scott, and Eduardo Souza-Rodrigues. 2017. “On
the Non-Identification of Counterfactuals in Dynamic Discrete Games.”
&lt;em&gt;International Journal of Industrial Organization&lt;/em&gt; 50: 362–71.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-keane1997career&#34; class=&#34;csl-entry&#34; markdown=&#34;1&#34;&gt;
&lt;p&gt;Keane, Michael P, and Kenneth I Wolpin. 1997. “The Career Decisions of
Young Men.” &lt;em&gt;Journal of Political Economy&lt;/em&gt; 105 (3): 473–522.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-magnac2002identifying&#34; class=&#34;csl-entry&#34; markdown=&#34;1&#34;&gt;
&lt;p&gt;Magnac, Thierry, and David Thesmar. 2002. “Identifying Dynamic Discrete
Decision Processes.” &lt;em&gt;Econometrica&lt;/em&gt; 70 (2): 801–16.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-pakes1986patents&#34; class=&#34;csl-entry&#34; markdown=&#34;1&#34;&gt;
&lt;p&gt;Pakes, Ariel. 1986. “Patents as Options: Some Estimates of the Value of
Holding European Patent Stocks.” &lt;em&gt;Econometrica&lt;/em&gt; 54 (4): 755–84.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-rust1987optimal&#34; class=&#34;csl-entry&#34; markdown=&#34;1&#34;&gt;
&lt;p&gt;Rust, John. 1987. “Optimal Replacement of GMC Bus Engines: An Empirical
Model of Harold Zurcher.” &lt;em&gt;Econometrica: Journal of the Econometric
Society&lt;/em&gt;, 999–1033.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-rust1988maximum&#34; class=&#34;csl-entry&#34; markdown=&#34;1&#34;&gt;
&lt;p&gt;———. 1988. “Maximum Likelihood Estimation of Discrete Control
Processes.” &lt;em&gt;SIAM Journal on Control and Optimization&lt;/em&gt; 26 (5): 1006–24.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-rust1994structural&#34; class=&#34;csl-entry&#34; markdown=&#34;1&#34;&gt;
&lt;p&gt;———. 1994. “Structural Estimation of Markov Decision Processes.”
&lt;em&gt;Handbook of Econometrics&lt;/em&gt; 4: 3081–3143.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-su2012constrained&#34; class=&#34;csl-entry&#34; markdown=&#34;1&#34;&gt;
&lt;p&gt;Su, Che-Lin, and Kenneth L Judd. 2012. “Constrained Optimization
Approaches to Estimation of Structural Models.” &lt;em&gt;Econometrica&lt;/em&gt; 80 (5):
2213–30.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Neural Networks</title>
      <link>https://matteocourthoud.github.io/course/ml-econ/08_neuralnets/</link>
      <pubDate>Wed, 09 Mar 2022 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/course/ml-econ/08_neuralnets/</guid>
      <description>&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Remove warnings
import warnings
warnings.filterwarnings(&#39;ignore&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Import everything
import pandas as pd
import numpy as np
import seaborn as sns
import statsmodels.api as sm

import copy
import torch 
import torch.nn as nn
import torch.utils.data as Data
from torch.autograd import Variable
from sklearn.linear_model import LinearRegression
from torchviz import make_dot
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Import matplotlib for graphs
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import axes3d
from IPython.display import clear_output

# Set global parameters
%matplotlib inline
plt.style.use(&#39;seaborn-white&#39;)
plt.rcParams[&#39;lines.linewidth&#39;] = 3
plt.rcParams[&#39;figure.figsize&#39;] = (10,6)
plt.rcParams[&#39;figure.titlesize&#39;] = 20
plt.rcParams[&#39;axes.titlesize&#39;] = 18
plt.rcParams[&#39;axes.labelsize&#39;] = 14
plt.rcParams[&#39;legend.fontsize&#39;] = 14
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;While &lt;code&gt;sklearn&lt;/code&gt; has a library for neural networks, it is very basic and not the standard in the industry. The most commonly used libraries as of 2020 are &lt;strong&gt;Tensorflow&lt;/strong&gt; and &lt;strong&gt;Pytorch&lt;/strong&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;TensorFlow is developed by Google Brain and actively used at Google both for research and production needs. Its closed-source predecessor is called DistBelief.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;PyTorch is a cousin of lua-based Torch framework which was developed and used at Facebook. However, PyTorch is not a simple set of wrappers to support popular language, it was rewritten and tailored to be fast and feel native.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Here is an article that explains very well the difference between the two libraries: &lt;a href=&#34;https://towardsdatascience.com/pytorch-vs-tensorflow-spotting-the-difference-25c75777377b&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;pytorch-vs-tensorflow&lt;/a&gt;. In short, pytorch is much more intuitive for a python programmer and more user friendly. It also has a superior development and debugging experience. However, if you want more control on the fundamentals, a better community support and you need to train large models, Tensorflow is better.&lt;/p&gt;
&lt;h2 id=&#34;81-introduction&#34;&gt;8.1 Introduction&lt;/h2&gt;
&lt;p&gt;The term neural network has evolved to encompass a large class of models and learning methods. Here I describe the most widely used “vanilla” neural net, sometimes called the single hidden layer back-propagation network, or single layer perceptron.&lt;/p&gt;
&lt;h3 id=&#34;regression&#34;&gt;Regression&lt;/h3&gt;
&lt;p&gt;Imagine a setting with two &lt;strong&gt;inputs&lt;/strong&gt; available (let’s denote these inputs $i_1$ and $i_2$), and no special knowledge about the relationship between these inputs and the &lt;strong&gt;output&lt;/strong&gt; that we want to predict (denoted by $o$) except that this relationship is, a priori, pretty complex and non-linear.&lt;/p&gt;
&lt;p&gt;So we want to learn the function $f$ such that f($i_1$, $i_2$) is a good estimator of $o$. We could then suggest the following first model:&lt;/p&gt;
&lt;p&gt;$$
o = w_{11} i_1 + w_{12} i_2
$$&lt;/p&gt;
&lt;p&gt;where $w_{11}$ and $w_{12}$ are just weights/coefficients (do not take care about the indices for now). Before going any further, we should notice that, here, there is no constant term in the model. However, we could have introduced such term by setting $f(i_1, i_2) = w_{11} i_1 + w_{12} i_2 + c$. The constant is often called &lt;strong&gt;bias&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;We can represent the setting as follows.&lt;/p&gt;
&lt;img src=&#34;../figures/nn1.jpeg&#34; alt=&#34;Drawing&#34; style=&#34;width: 600px;&#34;/&gt;
&lt;p&gt;In this case, the model is easy to understand and to fit but has a big drawback : there is no non-linearity! This obviously do not respect our non-linear assumption.&lt;/p&gt;
&lt;h3 id=&#34;activation-functions&#34;&gt;Activation Functions&lt;/h3&gt;
&lt;p&gt;In order to introduce a non-linearity, let us make a little modification in the previous model and suggest the following one.&lt;/p&gt;
&lt;p&gt;$$
o = a ( w_{11} i_1 + w_{12} i_2)
$$&lt;/p&gt;
&lt;p&gt;where $a$ is a function called &lt;strong&gt;activation function&lt;/strong&gt; which is non-linear.&lt;/p&gt;
&lt;img src=&#34;../figures/nn2.jpeg&#34; alt=&#34;Drawing&#34; style=&#34;width: 600px;&#34;/&gt;
&lt;p&gt;One activation function that is well known in economics (and other disciplines) is the &lt;em&gt;sigmoid&lt;/em&gt; function or logit function&lt;/p&gt;
&lt;p&gt;$$
a (w_{11} i_1 + w_{12} i_2) = \frac{1}{1 + e^{w_{11} i_1 + w_{12} i_2}}
$$&lt;/p&gt;
&lt;h3 id=&#34;layers&#34;&gt;Layers&lt;/h3&gt;
&lt;p&gt;However, even if better than multilinear model, this model is still too simple and can’t handle the assumed underlying complexity of the relationship between inputs and output. We can make a step further and enrich the model the following way.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;First we could consider that the quantity $a ( w_{11} i_1 + w_{12} i_2)$ is no longer the final output but instead a new intermediate feature of our function, called $l_1$, which stands for &lt;strong&gt;layer&lt;/strong&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;$$
l_1 = a ( w_{11} i_1 + w_{12} i_2)
$$&lt;/p&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;Second we could consider that we build several (3 in our example) such features in the same way, but possibly with different weights and different activation functions&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;$$
l_1 = a ( w_{11} i_1 + w_{12} i_2) \
l_2 = a ( w_{21} i_1 + w_{22} i_2) \
l_3 = a ( w_{31} i_1 + w_{32} i_2)
$$&lt;/p&gt;
&lt;p&gt;where the $a$’s are just activation functions and the $w$’s are weights.&lt;/p&gt;
&lt;ol start=&#34;3&#34;&gt;
&lt;li&gt;Finally, we can consider that our final output is build based on these intermediate features with the same “template”&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;$$
a_2 ( v_1 l_1 + v_2 l_2 + v_3 * l_3 )
$$&lt;/p&gt;
&lt;p&gt;If we aggregate all the pieces, we then get our &lt;strong&gt;prediction&lt;/strong&gt; $p$&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
p = f_{3}\left(i_{1}, i_{2}\right) &amp;amp;=a_{2}\left(v_{1} l_{1}+v_{2} l_{2}+v_{3} l_{3}\right) \
&amp;amp;=a_{2}\left(v_{1} \times a_{11}\left(w_{11} i_{1}+w_{12} i_{2}\right)+v_{2} \times a_{12}\left(w_{21} i_{1}+w_{22} i_{2}\right)+v_{3} \times a_{13}\left(w_{31} i_{1}+w_{32} i_{2}\right)\right)
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;where we should mainly keep in mind that $a$’s are non-linear activation functions and $w$’s and $v$’s are weights.&lt;/p&gt;
&lt;p&gt;Graphically:&lt;/p&gt;
&lt;img src=&#34;../figures/nn3.jpeg&#34; alt=&#34;Drawing&#34; style=&#34;width: 900px;&#34;/&gt;
&lt;p&gt;This last model is a basic feedforward neural network with:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;2 entries ($i_1$ and $i_2$)&lt;/li&gt;
&lt;li&gt;1 hidden layer with 3 hidden neurones (whose outputs are $l_1$, $l_2$ and $l_3$)&lt;/li&gt;
&lt;li&gt;1 final output ($p$)&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;pytorch&#34;&gt;Pytorch&lt;/h2&gt;
&lt;h3 id=&#34;tensors&#34;&gt;Tensors&lt;/h3&gt;
&lt;p&gt;We can express the data as a &lt;code&gt;numpy&lt;/code&gt; array.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;x_np = np.arange(6).reshape((3, 2))
x_np
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;array([[0, 1],
       [2, 3],
       [4, 5]])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Or equivalently as a &lt;code&gt;pytorch&lt;/code&gt; tensor.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;x_tensor = torch.from_numpy(x_np)
x_tensor
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;tensor([[0, 1],
        [2, 3],
        [4, 5]])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can also translate tensors back to arrays.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;tensor2array = x_tensor.numpy()
tensor2array
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;array([[0, 1],
       [2, 3],
       [4, 5]])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can make operations over this data. For example we can take the mean&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;try:
    torch.mean(x_tensor)
except Exception as e:
    print(e)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;mean(): input dtype should be either floating point or complex dtypes. Got Long instead.
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We first have to convert the data in float&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;x_tensor = torch.FloatTensor(x_np)
x_tensor
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;tensor([[0., 1.],
        [2., 3.],
        [4., 5.]])
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;print(np.mean(x_np), &#39;\n\n&#39;, torch.mean(x_tensor))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;2.5 

 tensor(2.5000)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can also apply compontent-wise functions&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;print(np.sin(x_np), &#39;\n\n&#39;, torch.sin(x_tensor))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[[ 0.          0.84147098]
 [ 0.90929743  0.14112001]
 [-0.7568025  -0.95892427]] 

 tensor([[ 0.0000,  0.8415],
        [ 0.9093,  0.1411],
        [-0.7568, -0.9589]])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can multiply tensors as we multiply matrices&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;print(np.matmul(x_np.T, x_np), &#39;\n\n&#39;, torch.mm(x_tensor.T, x_tensor))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[[20 26]
 [26 35]] 

 tensor([[20., 26.],
        [26., 35.]])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;But the element-wise multiplication does not work&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;try:
    x_tensor.dot(x_tensor)
except Exception as e:
    print(e)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;1D tensors expected, but got 2D and 2D tensors
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;variables&#34;&gt;Variables&lt;/h3&gt;
&lt;p&gt;Variable in torch is to build a computational graph, but this graph is dynamic compared with a static graph in Tensorflow or Theano. So torch does not have placeholder, torch can just pass variable to the computational graph.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# build a variable, usually for compute gradients
x_variable = Variable(x_tensor, requires_grad=True)   

x_variable
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;tensor([[0., 1.],
        [2., 3.],
        [4., 5.]], requires_grad=True)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Until now the tensor and variable seem the same. However, the variable is a part of the graph, it&amp;rsquo;s a part of the auto-gradient.&lt;/p&gt;
&lt;p&gt;Suppose we are interested in:&lt;/p&gt;
&lt;p&gt;$$
y = \text{mean} (x_1^2) = \frac{1}{6} x^2
$$&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;y = torch.mean(x_variable*x_variable)
print(y)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;tensor(9.1667, grad_fn=&amp;lt;MeanBackward0&amp;gt;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can compute the gradient by backpropagation&lt;/p&gt;
&lt;p&gt;$$
\nabla y(x) = \frac{2}{3} x
$$&lt;/p&gt;
&lt;p&gt;i.e. if we call the &lt;code&gt;backward&lt;/code&gt; method on our outcome &lt;code&gt;y&lt;/code&gt;, we see that the gradient of our variable &lt;code&gt;x&lt;/code&gt; gets updated.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;print(x_variable.grad)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;None
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;y.backward()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;print(x_variable.grad)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;tensor([[0.0000, 0.3333],
        [0.6667, 1.0000],
        [1.3333, 1.6667]])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;However, its value has not changed.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;print(x_variable)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;tensor([[0., 1.],
        [2., 3.],
        [4., 5.]], requires_grad=True)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can also access the &lt;code&gt;tensor&lt;/code&gt; part of the variable alone by calling the &lt;code&gt;data&lt;/code&gt; method.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;print(x_variable.data)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;tensor([[0., 1.],
        [2., 3.],
        [4., 5.]])
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;activation-function&#34;&gt;Activation Function&lt;/h3&gt;
&lt;p&gt;The main advantage of neural networks is that they introduce non-linearities among the layers. The standard non-linear function&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ReLu&lt;/li&gt;
&lt;li&gt;Sigmoid&lt;/li&gt;
&lt;li&gt;TanH&lt;/li&gt;
&lt;li&gt;Softmax&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# X grid
x_grid = torch.linspace(-5, 5, 200)  # x data (tensor), shape=(100, 1)
x_grid = Variable(x_grid)
x_grid_np = x_grid.data.numpy()   # numpy array for plotting

# Activation functions
y_relu = torch.relu(x_grid).data.numpy()
y_sigmoid = torch.sigmoid(x_grid).data.numpy()
y_tanh = torch.tanh(x_grid).data.numpy()
y_softmax = torch.softmax(x_grid, dim=0).data.numpy() 
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# New figure 1
def make_new_figure_1():

    # Init figure
    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2,2, figsize=(8,6))
    fig.suptitle(&#39;Activation Functions&#39;)

    # Relu
    ax1.plot(x_grid_np, y_relu, c=&#39;red&#39;, label=&#39;relu&#39;)
    ax1.set_ylim((-1, 6)); ax1.legend()

    # Sigmoid
    ax2.plot(x_grid_np, y_sigmoid, c=&#39;red&#39;, label=&#39;sigmoid&#39;)
    ax2.set_ylim((-0.2, 1.2)); ax2.legend()

    # Tanh
    ax3.plot(x_grid_np, y_tanh, c=&#39;red&#39;, label=&#39;tanh&#39;)
    ax3.set_ylim((-1.2, 1.2)); ax3.legend()

    # Softmax
    ax4.plot(x_grid_np, y_softmax, c=&#39;red&#39;, label=&#39;softmax&#39;)
    ax4.set_ylim((-0.01, 0.06)); ax4.legend();
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s compare the different activation functions.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;make_new_figure_1()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/08_neuralnets_56_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;ReLu is very popular since it&amp;rsquo;s non-linear.&lt;/p&gt;
&lt;h2 id=&#34;83-optimization-and-gradient-descent&#34;&gt;8.3 Optimization and Gradient Descent&lt;/h2&gt;
&lt;h3 id=&#34;gradient-descent&#34;&gt;Gradient Descent&lt;/h3&gt;
&lt;p&gt;Gradient descent works as follows:&lt;/p&gt;
&lt;ol start=&#34;0&#34;&gt;
&lt;li&gt;Initialize the parameters&lt;/li&gt;
&lt;li&gt;Compute the Loss&lt;/li&gt;
&lt;li&gt;Compute the Gradients&lt;/li&gt;
&lt;li&gt;Update the Parameters&lt;/li&gt;
&lt;li&gt;Repeat (1)-(3) until convergence&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;gradient-descent-in-linear-regression&#34;&gt;Gradient Descent in Linear Regression&lt;/h3&gt;
&lt;p&gt;In order to understand how are NN optimized, we start with a linear regression example. Remember that linear regression can be interpreted as the simplest possible NN.&lt;/p&gt;
&lt;p&gt;We generate the following data:&lt;/p&gt;
&lt;p&gt;$$
y = 1 + 2 x - 3 x^2 + \varepsilon
$$&lt;/p&gt;
&lt;p&gt;with $x \sim N(0,1)$ and $\varepsilon \sim N(0,0.1)$&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Data Generation
np.random.seed(42)
N = 100

x = np.sort(np.random.rand(N, 1), axis=0)
e = .1*np.random.randn(N, 1)
y_true = 1 + 2*x - 3*x**2
y = y_true + e
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s plot the data.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# New figure 2
def make_new_figure_2():
    
    # Init
    fig, ax = plt.subplots(figsize=(8,6))
    fig.suptitle(&#39;Activation Functions&#39;)

    # Scatter
    ax.scatter(x,y); 
    ax.plot(x,y_true,color=&#39;orange&#39;); 
    ax.set_xlabel(&#39;X&#39;); ax.set_ylabel(&#39;Y&#39;);
    ax.legend([&#39;y true&#39;,&#39;y&#39;]);
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;make_new_figure_2()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/08_neuralnets_66_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Suppose we try to fit the data with a linear model&lt;/p&gt;
&lt;p&gt;$$
y = a + b x
$$&lt;/p&gt;
&lt;p&gt;We proceed iteratively by gradient descent. Our objective function is the Mean Squared Error.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Algorithm&lt;/strong&gt;&lt;/p&gt;
&lt;ol start=&#34;0&#34;&gt;
&lt;li&gt;
&lt;p&gt;Take an initial guess of the parameters
$$
a = a_0 \
b = b_0
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Compute the Mean Squared Error
$$
\begin{array}
\text{MSE} &amp;amp;= \frac{1}{N} \sum_{i=1}^{N}\left(y_{i}-\hat{y}&lt;em&gt;{i}\right)^{2} \
&amp;amp;= \frac{1}{N} \sum&lt;/em&gt;{i=1}^{N}\left(y_{i}-a-b x_{i}\right)^{2}
\end{array}
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Compute its derivative
$$
\begin{array}{l}
\frac{\partial M S E}{\partial a}=\frac{\partial M S E}{\partial \hat{y}&lt;em&gt;{i}} \cdot \frac{\partial \hat{y}&lt;/em&gt;{i}}{\partial a}=\frac{1}{N} \sum_{i=1}^{N} 2\left(y_{i}-a-b x_{i}\right) \cdot(-1)=-2 \frac{1}{N} \sum_{i=1}^{N}\left(y_{i}-\hat{y}&lt;em&gt;{i}\right) \
\frac{\partial M S E}{\partial b}=\frac{\partial M S E}{\partial \hat{y}&lt;/em&gt;{i}} \cdot \frac{\partial \hat{y}&lt;em&gt;{i}}{\partial b}=\frac{1}{N} \sum&lt;/em&gt;{i=1}^{N} 2\left(y_{i}-a-b x_{i}\right) \cdot\left(-x_{i}\right)=-2 \frac{1}{N} \sum_{i=1}^{N} x_{i}\left(y_{i}-\hat{y}_{i}\right)
\end{array}
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Update the parameters
$$
\begin{array}{l}
a=a-\eta \frac{\partial M S E}{\partial a} \
b=b-\eta \frac{\partial M S E}{\partial b}
\end{array}
$$&lt;/p&gt;
&lt;p&gt;Where $\eta$ is the &lt;strong&gt;learning rate&lt;/strong&gt;. A lower learning rate makes learning more stable but slower.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Repeat (1)-(3) $T$ times, where the number of total iterations $T$ is called &lt;strong&gt;epochs&lt;/strong&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;We start by taking a random guess of $\alpha$ and $\beta$.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Initializes parameters &amp;quot;a&amp;quot; and &amp;quot;b&amp;quot; randomly
np.random.seed(42)
a = np.random.randn(1)
b = np.random.randn(1)

print(a, b)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[0.49671415] [-0.1382643]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Plot gradient 
def gradient_plot(x, y, y_hat, y_true, EPOCHS, losses):
    clear_output(wait=True)
    fig, (ax1, ax2) = plt.subplots(1,2, figsize=(12,5))
    
    # First figure
    ax1.clear()
    ax1.scatter(x, y)
    ax1.plot(x, y_true, &#39;orange&#39;)
    ax1.plot(x, y_hat, &#39;r-&#39;)
    ax1.set_title(&#39;Data and Fit&#39;)
    ax1.legend([&#39;True&#39;, &#39;Predicted&#39;])
    
    # Second figure
    ax2.clear()
    ax2.plot(range(len(losses)), losses, color=&#39;g&#39;)
    ax2.set_xlim(0,EPOCHS); ax2.set_ylim(0,1.1*np.max(losses))
    ax2.set_title(&#39;True MSE = %.4f&#39; % losses[-1])
    
    # Plot
    plt.show();
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We set the learning rate $\eta = 0.1$ and the number of epochs $T=200$&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# parameters
LR = 0.1        # learning rate
EPOCHS = 200    # number of epochs
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can now plot the training and the result.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# New figure 3
def make_new_figure_3(a, b):
    
    # Init
    losses = []

    # train
    for t in range(EPOCHS):

        # compute loss
        y_hat = a + b * x
        error = (y - y_hat)
        loss = (error**2).mean()

        # compute gradient
        a_grad = -2 * error.mean()
        b_grad = -2 * (x * error).mean()

        # update parameters
        a -= LR * a_grad
        b -= LR * b_grad

        # plot
        losses += [loss]
        if (t+1) % (EPOCHS/25) == 0: # print 25 times
            gradient_plot(x, y, y_hat, y_true, EPOCHS, losses)

    print(a, b)
    return a, b
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;a_fit, b_fit = make_new_figure_3(a, b)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/08_neuralnets_76_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[1.40589939] [-0.83739496]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Sanity Check: do we get the same results as our gradient descent?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# OLS estimates
ols = LinearRegression()
ols.fit(x, y)
print(ols.intercept_, ols.coef_[0])
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1.4345303] [-0.89397853]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Close enough!&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s plot both lines in the graph.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# New figure 4
def make_new_figure_4():
    
    # Init
    fig, ax = plt.subplots(figsize=(8,6))

    # Scatter
    ax.plot(x,y_true,color=&#39;orange&#39;); 
    ax.plot(x,a_fit + b_fit*x,color=&#39;red&#39;); 
    ax.plot(x,ols.predict(x),color=&#39;green&#39;); 
    ax.legend([&#39;y true&#39;,&#39;y gd&#39;, &#39;y ols&#39;])
    ax.scatter(x,y); 
    ax.set_xlabel(&#39;X&#39;); ax.set_ylabel(&#39;Y&#39;); ax.set_title(&amp;quot;Data&amp;quot;);
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;make_new_figure_4()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/08_neuralnets_82_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Now we are going to do exactly the same but with &lt;code&gt;pytorch&lt;/code&gt;.&lt;/p&gt;
&lt;h3 id=&#34;autograd&#34;&gt;Autograd&lt;/h3&gt;
&lt;p&gt;Autograd is PyTorch’s automatic differentiation package. Thanks to it, we don’t need to worry about partial derivatives, chain rule or anything like it.&lt;/p&gt;
&lt;p&gt;So, how do we tell PyTorch to do its thing and compute all gradients? That’s what &lt;code&gt;backward()&lt;/code&gt; is good for.
§
Do you remember the starting point for computing the gradients? It was the loss, as we computed its partial derivatives w.r.t. our parameters. Hence, we need to invoke the &lt;code&gt;backward()&lt;/code&gt; method from the corresponding Python variable, like, &lt;code&gt;loss.backward()&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;What about the actual values of the gradients? We can inspect them by looking at the grad attribute of a tensor.&lt;/p&gt;
&lt;p&gt;If you check the method’s documentation, it clearly states that gradients are accumulated. So, every time we use the
gradients to update the parameters, we need to zero the gradients afterwards. And that’s what zero_() is good for.&lt;/p&gt;
&lt;p&gt;What does the underscore (_) at the end of the method name mean? Do you remember? If not, scroll back to the previous section and find out.&lt;/p&gt;
&lt;p&gt;So, let’s ditch the manual computation of gradients and use both backward() and zero_() methods instead.&lt;/p&gt;
&lt;p&gt;First, we convert our variables to tensors.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Convert data to tensors
x_tensor = torch.from_numpy(x).float().to(&#39;cpu&#39;)
y_tensor = torch.from_numpy(y).float().to(&#39;cpu&#39;)
print(type(x), type(x_tensor))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;class &#39;numpy.ndarray&#39;&amp;gt; &amp;lt;class &#39;torch.Tensor&#39;&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We take the initial parameters guess&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# initial parameter guess
torch.manual_seed(42)
a = torch.randn(1, requires_grad=True, dtype=torch.float, device=&#39;cpu&#39;)
b = torch.randn(1, requires_grad=True, dtype=torch.float, device=&#39;cpu&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we are ready to fit the model.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# New figure 5
def make_new_figure_5(a, b):
    
    # Init
    losses = []

    # parameters
    LR = 0.1
    EPOCHS = 200

    # train
    for t in range(EPOCHS):

        # compute loss
        y_hat = a + b * x_tensor
        error = y_tensor - y_hat
        loss = (error ** 2).mean()

        # compute gradient
        loss.backward()

        # update parameters
        with torch.no_grad():
            a -= LR * a.grad
            b -= LR * b.grad

        # clear gradients
        a.grad.zero_()
        b.grad.zero_()

        # Plot
        losses += [((y_true - y_hat.detach().numpy())**2).mean()]
        if (t+1) % (EPOCHS/25) == 0: # print 25 times
            gradient_plot(x, y, y_hat.data.numpy(), y_true, EPOCHS, losses)

    print(a, b)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;make_new_figure_5(a, b)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/08_neuralnets_93_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;tensor([1.3978], requires_grad=True) tensor([-0.8214], requires_grad=True)
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;optimizer&#34;&gt;Optimizer&lt;/h3&gt;
&lt;p&gt;So far, we’ve been manually updating the parameters using the computed gradients. That’s probably fine for two parameters… but what if we had a whole lot of them?! We use one of PyTorch’s optimizers.&lt;/p&gt;
&lt;p&gt;An optimizer takes the following arguments:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the parameters we want to update&lt;/li&gt;
&lt;li&gt;he learning rate we want to use&lt;/li&gt;
&lt;li&gt;(possibly many other hyper-parameters)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Moreover, we can now call the function &lt;code&gt;zero_grad()&lt;/code&gt; to automatically update the parameters. In particular, we will need to perform the following steps at each iteration:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Clear the parameters: &lt;code&gt;optimizer.zero_grad()&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Compute the gradient: &lt;code&gt;loss.backward()&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Update the parameters: &lt;code&gt;optimizer.step()&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In the code below, we create a Stochastic Gradient Descent (&lt;code&gt;SGD&lt;/code&gt;) optimizer to update our parameters $a$ and $b$.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Init parameters
torch.manual_seed(42)
a = torch.randn(1, requires_grad=True, dtype=torch.float, device=&#39;cpu&#39;)
b = torch.randn(1, requires_grad=True, dtype=torch.float, device=&#39;cpu&#39;)

# Defines a SGD optimizer to update the parameters
optimizer = torch.optim.SGD([a, b], lr=LR)
print(optimizer)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;SGD (
Parameter Group 0
    dampening: 0
    lr: 0.1
    momentum: 0
    nesterov: False
    weight_decay: 0
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can also define a default loss function so that we don&amp;rsquo;t have to compute it by hand. We are going to use the &lt;code&gt;MSE&lt;/code&gt; loss function.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Define a loss function
loss_func = torch.nn.MSELoss()
print(loss_func)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;MSELoss()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s plot the estimator and the MSE.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# New figure 6
def make_new_figure_6(a, b):
    
    # parameters
    EPOCHS = 200

    # init 
    losses = []

    # train
    for t in range(EPOCHS):

        # compute loss
        y_hat = a + b * x_tensor
        error = y_tensor - y_hat
        loss = (error ** 2).mean()  

        # update parameters
        optimizer.zero_grad()   # clear gradients for next train
        loss.backward()         # backpropagation, compute gradients
        optimizer.step()        # apply gradients, update parameters

        # Plot
        losses += [((y_true - y_hat.detach().numpy())**2).mean()]
        if (t+1) % (EPOCHS/25) == 0:
            gradient_plot(x, y, y_hat.data.numpy(), y_true, EPOCHS, losses)

    print(a, b)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;make_new_figure_6(a, b)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/08_neuralnets_103_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;tensor([1.3978], requires_grad=True) tensor([-0.8214], requires_grad=True)
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;building-a-nn&#34;&gt;Building a NN&lt;/h3&gt;
&lt;p&gt;In our model, we manually created two parameters to perform a linear regression. Let’s use PyTorch’s &lt;code&gt;Sequential&lt;/code&gt; module to create our neural network.&lt;/p&gt;
&lt;p&gt;We first want to build the linear regression framework&lt;/p&gt;
&lt;p&gt;$$
y = a + b x
$$&lt;/p&gt;
&lt;p&gt;Which essentially is a network with&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;1 input&lt;/li&gt;
&lt;li&gt;no hidden layer&lt;/li&gt;
&lt;li&gt;no activation function&lt;/li&gt;
&lt;li&gt;1 output&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Let&amp;rsquo;s build the simplest possible neural network with &lt;code&gt;PyTorch&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Simplest possible neural network
linear_net = torch.nn.Sequential(
    torch.nn.Linear(1, 1)
)

print(linear_net)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Sequential(
  (0): Linear(in_features=1, out_features=1, bias=True)
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, if we call the &lt;code&gt;parameters()&lt;/code&gt; method of this model, PyTorch will figure the parameters of its attributes in a recursive way.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;[*linear_net.parameters()]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[Parameter containing:
 tensor([[-0.2191]], requires_grad=True),
 Parameter containing:
 tensor([0.2018], requires_grad=True)]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can now define the definitive training function.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def train_NN(x, y, y_true, net, optimizer, loss_func, EPOCHS):
    
    # transform variables
    x_tensor = torch.from_numpy(x).float().to(&#39;cpu&#39;)
    y_tensor = torch.from_numpy(y).float().to(&#39;cpu&#39;)

    # init 
    losses = []
    
    # train
    for t in range(EPOCHS):        

        # compute loss
        y_hat = net(x_tensor)     
        loss = loss_func(y_hat, y_tensor)    
        
        # update parameters
        optimizer.zero_grad()   # clear gradients for next train
        loss.backward()         # backpropagation, compute gradients
        optimizer.step()        # apply gradients, update parameters

        # plot
        losses += [((y_true - y_hat.detach().numpy())**2).mean()]
        if (t+1) % (EPOCHS/25) == 0: # print 25 times
            gradient_plot(x, y, y_hat.data.numpy(), y_true, EPOCHS, losses)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we are ready to train our neural network.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;optimizer = torch.optim.SGD(linear_net.parameters(), lr=LR)

# train
train_NN(x, y, y_true, linear_net, optimizer, loss_func, EPOCHS)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/08_neuralnets_113_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;We now define a more complicated NN. In particular we, build a neural network with&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;1 input&lt;/li&gt;
&lt;li&gt;1 hidden layer with 10 neurons and Relu activation function&lt;/li&gt;
&lt;li&gt;1 output layer&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Relu Net
relu_net = torch.nn.Sequential(
    torch.nn.Linear(1, 10),
    torch.nn.ReLU(),
    torch.nn.Linear(10, 1)
)

print(relu_net)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Sequential(
  (0): Linear(in_features=1, out_features=10, bias=True)
  (1): ReLU()
  (2): Linear(in_features=10, out_features=1, bias=True)
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This network has much more parameters.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;[*relu_net.parameters()]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[Parameter containing:
 tensor([[-0.4869],
         [ 0.5873],
         [ 0.8815],
         [-0.7336],
         [ 0.8692],
         [ 0.1872],
         [ 0.7388],
         [ 0.1354],
         [ 0.4822],
         [-0.1412]], requires_grad=True),
 Parameter containing:
 tensor([ 0.7709,  0.1478, -0.4668,  0.2549, -0.4607, -0.1173, -0.4062,  0.6634,
         -0.7894, -0.4610], requires_grad=True),
 Parameter containing:
 tensor([[-0.0893, -0.1901,  0.0298, -0.3123,  0.2856, -0.2686,  0.2441,  0.0526,
          -0.1027,  0.1954]], requires_grad=True),
 Parameter containing:
 tensor([0.0493], requires_grad=True)]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We are again using Stochastic Gradient Descent (&lt;code&gt;SGD&lt;/code&gt;) as optimization algorithm and Mean Squared Error (&lt;code&gt;MSELoss&lt;/code&gt;) as objective function.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# parameters
LR = 0.1
EPOCHS = 1000

# optimizer and loss function
optimizer = torch.optim.SGD(relu_net.parameters(), lr=LR)
loss_func = torch.nn.MSELoss()

# Train
train_NN(x, y, y_true, relu_net, optimizer, loss_func, EPOCHS)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/08_neuralnets_119_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;It seems that we can use fewer nodes to get the same result.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s make a smallet network.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Relu Net
relu_net2 = torch.nn.Sequential(
    torch.nn.Linear(1, 4),
    torch.nn.ReLU(),
    torch.nn.Linear(4, 1)
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And train it.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# parameters
LR = 0.1
EPOCHS = 1000

# optimizer and loss function
optimizer = torch.optim.SGD(relu_net2.parameters(), lr=LR)
loss_func = torch.nn.MSELoss()

# Train
train_NN(x, y, y_true, relu_net2, optimizer, loss_func, EPOCHS)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/08_neuralnets_124_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;We can try different activation functions.&lt;/p&gt;
&lt;p&gt;For example the tangent.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# TanH Net
tanh_net = torch.nn.Sequential(
    torch.nn.Linear(1, 10),
    torch.nn.Tanh(),
    torch.nn.Linear(10, 1)
)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# parameters
LR = 0.2
EPOCHS = 1000

# optimizer and loss function
optimizer = torch.optim.SGD(tanh_net.parameters(), lr=LR)
loss_func = torch.nn.MSELoss()

# train
train_NN(x, y, y_true, tanh_net, optimizer, loss_func, EPOCHS)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/08_neuralnets_127_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;loss-functions&#34;&gt;Loss functions&lt;/h3&gt;
&lt;p&gt;So far we have used the Stochastic  as loss function.&lt;/p&gt;
&lt;p&gt;Notice that &lt;code&gt;nn.MSELoss&lt;/code&gt; actually creates a loss function for us — it is NOT the loss function itself. Moreover, you can specify a reduction method to be applied, that is, how do you want to aggregate the results for individual points — you can average them (reduction=&lt;code&gt;mean&lt;/code&gt;) or simply sum them up (reduction=&lt;code&gt;sum&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;We are now going to use different ones.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# parameters
LR = 0.1
EPOCHS = 25

# nets
n = torch.nn.Sequential(torch.nn.Linear(1, 10), torch.nn.ReLU(), torch.nn.Linear(10, 1))
nets = [n,n,n,n]

# optimizers
optimizers = [torch.optim.SGD(n.parameters(), lr=LR) for n in nets]

# different loss functions
loss_MSE        = torch.nn.MSELoss()
loss_L1         = torch.nn.L1Loss()
loss_NLL        = torch.nn.NLLLoss()
loss_KLD        = torch.nn.KLDivLoss()
loss_funcs = [loss_MSE, loss_L1, loss_NLL, loss_KLD]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is the description of the loss functions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;MSELoss&lt;/code&gt;: Creates a criterion that measures the mean squared error (squared L2 norm) between each element in the input $x$ and target $y$.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;L1Loss&lt;/code&gt;: Creates a criterion that measures the mean absolute error (MAE) between each element in the input $x$ and target $y$.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;NLLLoss&lt;/code&gt;: The negative log likelihood loss.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;KLDivLoss&lt;/code&gt;: The Kullback-Leibler divergence loss measure&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Train multiple nets
def train_nets(x, y, y_true, nets, optimizers, loss_funcs, labels, EPOCHS):

    # Put dateset into torch dataset
    x_tensor = torch.from_numpy(x).float().to(&#39;cpu&#39;)
    y_tensor = torch.from_numpy(y).float().to(&#39;cpu&#39;)
    torch_dataset = Data.TensorDataset(x_tensor, y_tensor)
    
    # Init
    losses = np.zeros((0,4))
    
    # Train
    for epoch in range(EPOCHS): # for each epoch
        losses = np.vstack((losses, np.zeros((1,4))))
        for k, net, opt, lf in zip(range(4), nets, optimizers, loss_funcs):
            y_hat = net(x_tensor)              # get output for every net
            loss = loss_func(y_hat, y_tensor)  # compute loss for every net
            opt.zero_grad()                    # clear gradients for next train
            loss.backward()                    # backpropagation, compute gradients
            opt.step()                         # apply gradients
            losses[-1,k] = ((y_true - y_hat.detach().numpy())**2).mean()
        plot_losses(losses, labels, EPOCHS)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Plot losses 
def plot_losses(losses, labels, EPOCHS):
    clear_output(wait=True)
    fig, ax = plt.subplots(1,1, figsize=(10,6))
    
    # Plot
    ax.clear()
    ax.plot(range(len(losses)), losses)
    ax.set_xlim(0,EPOCHS-1); ax.set_ylim(0,1.1*np.max(losses))
    ax.set_title(&#39;Compare Losses&#39;); ax.set_ylabel(&#39;True MSE&#39;)
    legend_txt = [&#39;%s=%.4f&#39; % (label, loss) for label,loss in zip(labels, losses[-1,:])]
    ax.legend(legend_txt)
    
    # Shot
    plt.show();
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s compare them.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Train
labels = [&#39;MSE&#39;, &#39;L1&#39;, &#39;LogL&#39;, &#39;KLdiv&#39;]
train_nets(x, y, y_true, nets, optimizers, loss_funcs, labels, EPOCHS)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/08_neuralnets_135_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;In this very simple case, all loss functions are very similar.&lt;/p&gt;
&lt;h3 id=&#34;optimizers&#34;&gt;Optimizers&lt;/h3&gt;
&lt;p&gt;So far we have used the Stochastic Gradient Descent to fit the neural network. We are now going to use different ones.&lt;/p&gt;
&lt;p&gt;This is the &lt;a href=&#34;https://pytorch.org/docs/stable/optim.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;description of the optimizers&lt;/a&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;SGD&lt;/code&gt;: Implements stochastic gradient descent (optionally with momentum).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;Momentum&lt;/code&gt;: Nesterov momentum is based on the formula from &lt;a href=&#34;http://www.cs.toronto.edu/~hinton/absps/momentum.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;On the importance of initialization and momentum in deep learning&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;RMSprop&lt;/code&gt;: Proposed by G. Hinton in his &lt;a href=&#34;https://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;course&lt;/a&gt;. The centered version first appears in &lt;a href=&#34;https://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Generating Sequences With Recurrent Neural Networks&lt;/a&gt;. The implementation here takes the square root of the gradient average before adding epsilon (note that TensorFlow interchanges these two operations). The effective learning rate is thus $\frac{\alpha}{\sqrt{v} + \epsilon}$ where $\alpha$ is the scheduled learning rate and $v$ is the weighted moving average of the squared gradient.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;Adam&lt;/code&gt;: Proposed in &lt;a href=&#34;https://arxiv.org/abs/1412.6980&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Adam: A Method for Stochastic Optimization&lt;/a&gt;. The implementation of the L2 penalty follows changes proposed in &lt;a href=&#34;https://arxiv.org/abs/1711.05101&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Decoupled Weight Decay Regularization&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# parameters
LR = 0.1
EPOCHS = 25

# nets
n = torch.nn.Sequential(torch.nn.Linear(1, 10), torch.nn.ReLU(), torch.nn.Linear(10, 1))
nets = [n,n,n,n]

# different optimizers
opt_SGD         = torch.optim.SGD(nets[0].parameters(), lr=LR)
opt_Momentum    = torch.optim.SGD(nets[1].parameters(), lr=LR, momentum=0.8)
opt_RMSprop     = torch.optim.RMSprop(nets[2].parameters(), lr=LR, alpha=0.9)
opt_Adam        = torch.optim.Adam(nets[3].parameters(), lr=LR, betas=(0.9, 0.99))
optimizers = [opt_SGD, opt_Momentum, opt_RMSprop, opt_Adam]

# loss functions
l = torch.nn.MSELoss()
loss_funcs = [l,l,l,l]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s prot the loss functions over training, for different optimizers.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# train
labels = [&#39;SGD&#39;, &#39;Momentum&#39;, &#39;RMSprop&#39;, &#39;Adam&#39;]
train_nets(x, y, y_true, nets, optimizers, loss_funcs, labels, EPOCHS)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/08_neuralnets_141_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;training-on-batch&#34;&gt;Training on batch&lt;/h3&gt;
&lt;p&gt;Until now, we have used the whole training data at every training step. It has been batch gradient descent all along.&lt;/p&gt;
&lt;p&gt;This is fine for our ridiculously small dataset, sure, but if we want to go serious about all this, we must use mini-batch gradient descent. Thus, we need mini-batches. Thus, we need to slice our dataset accordingly. Do you want to do it manually?! Me neither!&lt;/p&gt;
&lt;p&gt;So we use PyTorch’s &lt;code&gt;DataLoader&lt;/code&gt; class for this job. We tell it which dataset to use (the one we just built in the previous section), the desired mini-batch size and if we’d like to shuffle it or not. That’s it!&lt;/p&gt;
&lt;p&gt;Our loader will behave like an iterator, so we can loop over it and fetch a different mini-batch every time.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Init data
x_tensor = torch.from_numpy(x).float().to(&#39;cpu&#39;)
y_tensor = torch.from_numpy(y).float().to(&#39;cpu&#39;)
torch_dataset = Data.TensorDataset(x_tensor, y_tensor)

# Build DataLoader
BATCH_SIZE = 25
loader = Data.DataLoader(
    dataset=torch_dataset,      # torch TensorDataset format
    batch_size=BATCH_SIZE,      # mini batch size
    shuffle=True,               # random shuffle for training
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s try using sub-samples of dimension &lt;code&gt;BATCH_SIZE = 25&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def train_NN_batch(loader, y_true, net, optimizer, loss_func, EPOCHS):
    
    # init
    losses = []

    # train
    for t in range(EPOCHS):   
        # train entire dataset 3 times
        for step, (batch_x, batch_y) in enumerate(loader):

            # compute loss
            y_hat = net(batch_x)     
            loss = loss_func(y_hat, batch_y)    

            # update parameters
            optimizer.zero_grad()   # clear gradients for next train
            loss.backward()         # backpropagation, compute gradients
            optimizer.step()        # apply gradients

        # plt every epoch
        y_hat = net(x_tensor)  
        losses += [((y_true - y_hat.detach().numpy())**2).mean()]
        if (t+1) % (EPOCHS/25) == 0:
            gradient_plot(x, y, y_hat.data.numpy(), y_true, EPOCHS, losses)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# parameters
LR = 0.1
EPOCHS = 1000
net = torch.nn.Sequential(torch.nn.Linear(1, 10), torch.nn.ReLU(), torch.nn.Linear(10, 1))
optimizer = torch.optim.SGD(net.parameters(), lr=LR)
loss_func = torch.nn.MSELoss()

# Train
train_NN_batch(loader, y_true, net, optimizer, loss_func, EPOCHS)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/08_neuralnets_147_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Two things are different now: not only we have an inner loop to load each and every mini-batch from our DataLoader but, more importantly, we are now sending only one mini-batch to the device.&lt;/p&gt;
&lt;p&gt;For bigger datasets, loading data sample by sample (into a CPU tensor) using Dataset’s &lt;code&gt;__get_item__&lt;/code&gt; and then sending all samples that belong to the same mini-batch at once to your GPU (device) is the way to go in order to make the best use of your graphics card’s RAM.&lt;/p&gt;
&lt;p&gt;Moreover, if you have many GPUs to train your model on, it is best to keep your dataset “agnostic” and assign the batches to different GPUs during training.&lt;/p&gt;
&lt;h2 id=&#34;84-advanced-topics&#34;&gt;8.4 Advanced Topics&lt;/h2&gt;
&lt;h3 id=&#34;issues&#34;&gt;Issues&lt;/h3&gt;
&lt;h4 id=&#34;starting-values&#34;&gt;Starting Values&lt;/h4&gt;
&lt;p&gt;Usually starting values for weights are chosen to be random values near zero. Hence the model starts out nearly linear, and becomes nonlinear as the weights increase.&lt;/p&gt;
&lt;h4 id=&#34;overfitting&#34;&gt;Overfitting&lt;/h4&gt;
&lt;p&gt;In early developments of neural networks, either by design or by accident, an early stopping rule was used to avoid overfitting.&lt;/p&gt;
&lt;p&gt;A more explicit method for regularization is &lt;em&gt;weight decay&lt;/em&gt;.&lt;/p&gt;
&lt;h4 id=&#34;scaling-of-the-inputs&#34;&gt;Scaling of the Inputs&lt;/h4&gt;
&lt;p&gt;Since the scaling of the inputs determines the effective scaling of the weights in the bottom layer, it can have a large effect on the quality of the final solution. At the outset it is best to standardize all inputs to have mean zero and standard deviation one.&lt;/p&gt;
&lt;h4 id=&#34;number-of-hidden-units-and-layers&#34;&gt;Number of Hidden Units and Layers&lt;/h4&gt;
&lt;p&gt;Generally speaking it is better to have too many hidden units than too few. With too few hidden units, the model might not have enough flexibility to capture the nonlinearities in the data; with too many hidden units, the extra weights can be shrunk toward zero if appropriate regularization is used.&lt;/p&gt;
&lt;p&gt;Choice of the number of hidden layers is guided by background knowledge and experimentation. Each layer extracts features of the input for regression or classification. Use of multiple hidden layers allows construction of hierarchical features at different levels of resolution.&lt;/p&gt;
&lt;p&gt;You can get an intuition on the role of hidden layers here: &lt;a href=&#34;https://playground.tensorflow.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://playground.tensorflow.org/&lt;/a&gt;&lt;/p&gt;
&lt;h4 id=&#34;multiple-minima&#34;&gt;Multiple Minima&lt;/h4&gt;
&lt;p&gt;The error function R(θ) is nonconvex, possessing many local minima. One approach is to use the average predictions over the collection of networks as the final prediction. Another approach is via &lt;em&gt;bagging&lt;/em&gt;.&lt;/p&gt;
&lt;h3 id=&#34;deep-neural-networks-and-deep-learning&#34;&gt;Deep Neural Networks and Deep Learning&lt;/h3&gt;
&lt;p&gt;Deep Neural Networks are just Neural Networks with more than one hidden layer.&lt;/p&gt;
&lt;h3 id=&#34;convolutional-neural-nets&#34;&gt;Convolutional Neural Nets&lt;/h3&gt;
&lt;p&gt;Convolutional Neural Nets are often applied when dealing with image/video data. They are usually coded with each feature being a pixel and its value is the pixel color (3 dimensional RGB array).&lt;/p&gt;
&lt;img src=&#34;../figures/cnn1.png&#34; style=&#34;width: 400px;&#34;/&gt;
&lt;p&gt;Videos and images have 2 main characteristics:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;have lots of features&lt;/li&gt;
&lt;li&gt;&amp;ldquo;close&amp;rdquo; features are often similar&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Convolutional Neural Nets exploit the second characteristic to alleviate the computational problems arising from the first. They do it by constructing a first layer that does not build on evey feature but only on adjacent ones.&lt;/p&gt;
&lt;img src=&#34;../figures/cnn1.gif&#34; style=&#34;width: 400px;&#34;/&gt;
&lt;p&gt;In this way, most of the information is preserved, on a lower dimensional representation.&lt;/p&gt;
&lt;h3 id=&#34;recurrent-neural-nets&#34;&gt;Recurrent Neural Nets&lt;/h3&gt;
&lt;p&gt;Recurrent Neural Networks are often applied in contexts in which the data generating process is dynamic. The most important example is Natural Language Processing. The idea is that you want to make predictions &amp;ldquo;live&amp;rdquo; as data comes in. Moreover, the order of the data is relevant, so that you also what to keep track of what the model has learned so far.&lt;/p&gt;
&lt;p&gt;While RNNs learn similarly while training, in addition, they remember things learnt from prior input(s) while generating output(s). It’s part of the network. RNNs can take one or more input vectors and produce one or more output vectors and the output(s) are influenced not just by weights applied on inputs like a regular NN, but also by a “hidden” state vector representing the context based on prior input(s)/output(s). So, the same input could produce a different output depending on previous inputs in the series.&lt;/p&gt;
&lt;p&gt;Grafically:&lt;/p&gt;
&lt;img src=&#34;../figures/rnn1.png&#34; style=&#34;width: 600px;&#34;/&gt;
&lt;p&gt;In summary, in a vanilla neural network, a fixed size input vector is transformed into a fixed size output vector. Such a network becomes “recurrent” when you repeatedly apply the transformations to a series of given input and produce a series of output vectors.&lt;/p&gt;
&lt;h3 id=&#34;bidirectional-rnn&#34;&gt;Bidirectional RNN&lt;/h3&gt;
&lt;p&gt;Sometimes it’s not just about learning from the past to predict the future, but we also need to look into the future to fix the past. In speech recognition and handwriting recognition tasks, where there could be considerable ambiguity given just one part of the input, we often need to know what’s coming next to better understand the context and detect the present.&lt;/p&gt;
&lt;img src=&#34;../figures/rnn2.png&#34; style=&#34;width: 600px;&#34;/&gt;
&lt;p&gt;This does introduce the obvious challenge of how much into the future we need to look into, because if we have to wait to see all inputs then the entire operation will become costly.&lt;/p&gt;
&lt;h3 id=&#34;recursive-neural-netw&#34;&gt;Recursive Neural Netw&lt;/h3&gt;
&lt;p&gt;A recurrent neural network parses the inputs in a sequential fashion. A recursive neural network is similar to the extent that the transitions are repeatedly applied to inputs, but not necessarily in a sequential fashion. Recursive Neural Networks are a more general form of Recurrent Neural Networks. It can operate on any hierarchical tree structure. Parsing through input nodes, combining child nodes into parent nodes and combining them with other child/parent nodes to create a tree like structure. Recurrent Neural Networks do the same, but the structure there is strictly linear. i.e. weights are applied on the first input node, then the second, third and so on.&lt;/p&gt;
&lt;img src=&#34;../figures/rnn3.png&#34; style=&#34;width: 600px;&#34;/&gt;
&lt;p&gt;But this raises questions pertaining to the structure. How do we decide that? If the structure is fixed like in Recurrent Neural Networks then the process of training, backprop etc makes sense in that they are similar to a regular neural network. But if the structure isn’t fixed, is that learnt as well?&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Dynamic Games</title>
      <link>https://matteocourthoud.github.io/course/empirical-io/08_dynamics_games/</link>
      <pubDate>Fri, 29 Oct 2021 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/course/empirical-io/08_dynamics_games/</guid>
      <description>&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;h3 id=&#34;intro&#34;&gt;Intro&lt;/h3&gt;
&lt;p&gt;Setting: agents making &lt;strong&gt;strategic decisions&lt;/strong&gt; (new) in &lt;strong&gt;dynamic
environments&lt;/strong&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Entry and exit: Collard-Wexler (&lt;a href=&#34;#ref-collard2013demand&#34;&gt;2013&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Sunk costs: Ryan (&lt;a href=&#34;#ref-ryan2012costs&#34;&gt;2012&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Innovation: Goettler and Gordon (&lt;a href=&#34;#ref-goettler2011does&#34;&gt;2011&lt;/a&gt;)
&lt;ul&gt;
&lt;li&gt;(or whatever changes in response to investment)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Exploitation of natural resources: Huang and Smith
(&lt;a href=&#34;#ref-huang2014dynamic&#34;&gt;2014&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Durable goods: Esteban and Shum (&lt;a href=&#34;#ref-esteban2007durable&#34;&gt;2007&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Lit review&lt;/strong&gt;: forthcoming IO Handbook chapter Aguirregabiria,
Collard-Wexler, and Ryan (&lt;a href=&#34;#ref-aguirregabiria2021dynamic&#34;&gt;2021&lt;/a&gt;)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;single--vs-multi-agent&#34;&gt;Single- vs Multi-Agent&lt;/h3&gt;
&lt;p&gt;Typically in IO we study agents in &lt;strong&gt;strategic&lt;/strong&gt; environments.
Complicated in dynamic environments.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Curse of dimensionality&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;Single agent: need to track what the agent sees ($k$ states)&lt;/li&gt;
&lt;li&gt;Multi-agent: need to keep track what every agent sees
($k^J$states)&lt;/li&gt;
&lt;li&gt;Difference exponential in the number of agents&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Expectations&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;Need not only to keep track of how the environment evolves&lt;/li&gt;
&lt;li&gt;… but also of how other players act&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Equilibrium&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;Because of the strategic interaction, the Bellman equation is
&lt;em&gt;not a contraction&lt;/em&gt; anymore
&lt;ul&gt;
&lt;li&gt;Equilibrium existence?&lt;/li&gt;
&lt;li&gt;Equilibrium uniqueness?&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;plan&#34;&gt;Plan&lt;/h3&gt;
&lt;p&gt;We will cover &lt;strong&gt;first the estimation&lt;/strong&gt; and then the computation of
dynamic games&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Weird…&lt;/li&gt;
&lt;li&gt;Standard estimation method: Bajari, Benkard, and Levin
(&lt;a href=&#34;#ref-bajari2007estimating&#34;&gt;2007&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Does &lt;strong&gt;not&lt;/strong&gt; require to solve the model&lt;/li&gt;
&lt;li&gt;Indeed, that’s the &lt;strong&gt;advantage&lt;/strong&gt; of the method&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Disadvantages&lt;/strong&gt;: still need to solve the model for counterfactuals&lt;/li&gt;
&lt;li&gt;So we’ll cover computation afterwards&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Last&lt;/strong&gt;: bridge between Structural IO and Artificial Intelligence&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Different &lt;em&gt;objectives&lt;/em&gt; but similar &lt;em&gt;methods&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;Dynamic tools niche in IO but at the core of AI&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;bajari-benkard-levin-2008&#34;&gt;Bajari, Benkard, Levin (2008)&lt;/h2&gt;
&lt;h3 id=&#34;model&#34;&gt;Model&lt;/h3&gt;
&lt;p&gt;Stylized version of Ericson and Pakes (&lt;a href=&#34;#ref-ericson1995markov&#34;&gt;1995&lt;/a&gt;)
(no entry/exit)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;$J$ firms (products) indexed by $j \in \lbrace 1, &amp;hellip;, J \rbrace$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Time $t$ is dicrete, horizon is infinite&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;States&lt;/strong&gt; $s_{jt} \in \lbrace 1, &amp;hellip; \bar s \rbrace$: quality of
product $j$ in period $t$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Actions&lt;/strong&gt; $a_{jt} \in \mathbb R^+$: investment decision of firm
$j$ in period $t$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Static payoffs&lt;/strong&gt; $$
\pi_j (s_{jt}, \boldsymbol s_{-jt}, a_{jt}; \theta^\pi)
$$ where&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\boldsymbol s_{-it}$: state vector of all other firms in period
$t$&lt;/li&gt;
&lt;li&gt;$\theta^\pi$: parameters that govern static profits&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: if we micro-fund $\pi(\cdot)$ , e.g. with some demand and
supply model, we have 2 strategic decisions: prices (static) and
investment (dynamic).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;model-2&#34;&gt;Model (2)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;State transitions&lt;/strong&gt; $$
\boldsymbol s_{t+1} = f(\boldsymbol s_t, \boldsymbol a_t, \boldsymbol \epsilon_t; \theta^f)
$$ where&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\boldsymbol a_t$: vector of actions of all firm&lt;/li&gt;
&lt;li&gt;$\boldsymbol \epsilon_t$: vector of idiosyncratic shocks&lt;/li&gt;
&lt;li&gt;$\theta^f$: parameters that govern state transitions&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Objective function&lt;/strong&gt;: firms maximize expected discounted future
profits $$
\max_{\boldsymbol a} \ \mathbb E_t \left[ \sum_{\tau=0}^\infty \beta^{\tau} \pi_{j, t+\tau} (\theta^\pi) \right]
$$&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;value-function&#34;&gt;Value Function&lt;/h3&gt;
&lt;p&gt;The value function of firm $j$ at time $t$ in state $\boldsymbol s_{t}$,
under a set of strategy functions $\boldsymbol P$ (one for each firm) is
$$
V^{\boldsymbol P_{-j}}&lt;em&gt;{j} (\mathbf{s}&lt;/em&gt;{t}) = \max_{a_{jt} \in \mathcal{A}&lt;em&gt;j \left(\mathbf{s}&lt;/em&gt;{t}\right)} \Bigg\lbrace \pi_{j}^{\boldsymbol P_{-j}} (a_{jt}, \mathbf{s}&lt;em&gt;{t} ; \theta^\pi ) + \beta \mathbb E&lt;/em&gt;{\boldsymbol s_{t+1}} \Big[  V_{j}^{\boldsymbol P_{-j}} \left(\mathbf{s}&lt;em&gt;{t+1}\right) \ \Big| \ a&lt;/em&gt;{jt}, \boldsymbol s_{t} ; \theta^f \Big] \Bigg\rbrace
$$ where&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;$\pi_{j}^{\boldsymbol P_{-j}} (a_{jt}, \mathbf{s}&lt;em&gt;{t} ; \theta^\pi )$
are the static profits of firm $j$ given action $a&lt;/em&gt;{jt}$ and policy
functions $\boldsymbol P_{-j}$ for all firms a part from $j$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The expecation $\mathbb E$ is taken with respect to the conditional
transition probabilities
$f^{\boldsymbol P_{-j}} (\mathbf{s}&lt;em&gt;{t+1} | \mathbf{s}&lt;/em&gt;{t}, a_{jt} ; \theta^f)$&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;equilibrium&#34;&gt;Equilibrium&lt;/h3&gt;
&lt;p&gt;Equillibrium notion: &lt;strong&gt;Markow Perfect Equilibrium&lt;/strong&gt; (&lt;a href=&#34;#ref-maskin1988theory&#34;&gt;Maskin and Tirole
1988&lt;/a&gt;)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Assumption&lt;/strong&gt;: players’ strategies at period $t$ are functions only
of payoff-relevant state variables at the same period&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Definition&lt;/strong&gt;: a set of $J$ value and policy functions,
$\boldsymbol V$ and $\boldsymbol P$ such that each firm
&lt;ol&gt;
&lt;li&gt;maximizes its value function $V_j$&lt;/li&gt;
&lt;li&gt;given the policy function of every other firm
$\boldsymbol P_{-j}$&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;What is it basically?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Nash Equilibrium in the policy functions&lt;/li&gt;
&lt;li&gt;What are we ruling out?
&lt;ul&gt;
&lt;li&gt;Strategies that depend on longer histories&lt;/li&gt;
&lt;li&gt;E.g. “has anyone ever cheated in a cartel?”&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;estimation&#34;&gt;Estimation&lt;/h3&gt;
&lt;p&gt;We want to estimate 2 sets of &lt;strong&gt;parameters&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\theta^\pi$: parameterizes period profit function $\pi(\cdot)$&lt;/li&gt;
&lt;li&gt;$\theta^f$: parameterizes state transition function $f(\cdot)$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Generally 2 &lt;strong&gt;approaches&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Full solution
&lt;ul&gt;
&lt;li&gt;Impractical (we’ll see more details later)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Rely on some sort of Hotz and Miller
(&lt;a href=&#34;#ref-hotz1993conditional&#34;&gt;1993&lt;/a&gt;) CCP inversion
&lt;ul&gt;
&lt;li&gt;Aguirregabiria and Mira
(&lt;a href=&#34;#ref-aguirregabiria2007sequential&#34;&gt;2007&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Bajari, Benkard, and Levin (&lt;a href=&#34;#ref-bajari2007estimating&#34;&gt;2007&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Pakes, Ostrovsky, and Berry (&lt;a href=&#34;#ref-pakes2007simple&#34;&gt;2007&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Pesendorfer and Schmidt-Dengler
(&lt;a href=&#34;#ref-pesendorfer2008asymptotic&#34;&gt;2008&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;bbl-overview&#34;&gt;BBL Overview&lt;/h3&gt;
&lt;p&gt;Bajari, Benkard, and Levin (&lt;a href=&#34;#ref-bajari2007estimating&#34;&gt;2007&lt;/a&gt;) plan&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Estimate &lt;strong&gt;transition probabilities&lt;/strong&gt; and &lt;strong&gt;conditional choice
probabilities&lt;/strong&gt; from the data&lt;/li&gt;
&lt;li&gt;Use them to simulate the &lt;strong&gt;expected value function&lt;/strong&gt;, given a set of
parameters&lt;/li&gt;
&lt;li&gt;Use optimality of estimated choices to pin down static profit
parameters
&lt;ul&gt;
&lt;li&gt;I.e. repeat (2) for alternative strategies
&lt;ul&gt;
&lt;li&gt;By definition suboptimal&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Estimating equation&lt;/strong&gt;: values implied by observed strategies
should be higher than values implied by alternative strategies&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;bbl-first-stage&#34;&gt;BBL: First Stage&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Estimate the &lt;strong&gt;transition probabilities&lt;/strong&gt;
$f ( \cdot | a_{jt}, \boldsymbol s_t; \hat \theta^f )$
&lt;ul&gt;
&lt;li&gt;I.e. what is the observed frequency of any state-to-state
transition?&lt;/li&gt;
&lt;li&gt;For any given action of firm $j$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;… and &lt;strong&gt;conditional choice probabilities&lt;/strong&gt;
$\hat P_j(\cdot | \boldsymbol s_t)$
&lt;ul&gt;
&lt;li&gt;I.e. what is the probability of each action, for each firm $j$
in each state $\boldsymbol s$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Can be done &lt;strong&gt;non-parametrically&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;i.e. just observe frequencies&lt;/li&gt;
&lt;li&gt;Conditional on having enough data&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Note&lt;/strong&gt;: need to estimate transitions, conditional on each
state and action&lt;/li&gt;
&lt;li&gt;Problem with many states and actions, but especially with &lt;strong&gt;many
players&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;Curse of dimensionality&lt;/li&gt;
&lt;li&gt;Number of states increases exponentially in number of
players&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Important&lt;/strong&gt;: parametric assumptions would contradict the model for
the estimation of value/policy functions&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;bbl-second-stage&#34;&gt;BBL: Second Stage&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;First step&lt;/strong&gt;: from transitions $f(\hat \theta^f)$ and CCPs
$\boldsymbol{\hat P}$ to values&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;We can use transitions and CCPs to simulate &lt;strong&gt;histories&lt;/strong&gt; (of length
$\tilde T$)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;of states
$\lbrace \boldsymbol{\tilde{s}&lt;em&gt;{\tau}} \rbrace&lt;/em&gt;{\tau = 1}^{\tilde T}$&lt;/li&gt;
&lt;li&gt;and actions
$\lbrace \boldsymbol{\tilde{a}&lt;em&gt;{\tau}} \rbrace&lt;/em&gt;{\tau = 1}^{\tilde T}$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Given a parameter value $\tilde \theta^\pi$, we can compute &lt;strong&gt;static
payoffs&lt;/strong&gt;:
$\pi_{j}^{\boldsymbol {\hat{P}&lt;em&gt;{-j}}} \left( \tilde a&lt;/em&gt;{j\tau}, \boldsymbol{\tilde s}_{\tau} ; \tilde \theta^\pi \right)$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Simulated history + static payoffs = &lt;strong&gt;simulated value function&lt;/strong&gt; $$
{V}&lt;em&gt;{j}^{\boldsymbol {\hat{P}}} \left(\boldsymbol{s}&lt;/em&gt;{t} ; \tilde \theta^\pi \right) =  \sum_{\tau=0}^{\tilde T} \beta^{\tau} \pi_{j}^{\boldsymbol {\hat{P}&lt;em&gt;{-j}}} \left( \tilde a&lt;/em&gt;{j\tau}, \boldsymbol{\tilde s}_{\tau} ; \tilde \theta^\pi \right)
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;We can average over many, e.g. $R$, simulated value functions to get
an &lt;strong&gt;expected value function&lt;/strong&gt; $$
{V}&lt;em&gt;{j}^{\boldsymbol {\hat{P}}, R} \left( \boldsymbol{s}&lt;/em&gt;{t} ; \tilde \theta^\pi \right) = \frac{1}{R}  \sum_{r=0}^{R}\Bigg( \sum_{\tau=0}^{\tilde T} \beta^{\tau} \pi_{j}^{\boldsymbol {\hat{P}&lt;em&gt;{-j}}} \left(\tilde a^{(r)}&lt;/em&gt;{j\tau}, \boldsymbol{\tilde s}^{(r)}_{\tau} ; \tilde \theta^\pi \right) \Bigg)
$$&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;in-practice-for-a-parameter-value-tilde-thetapi&#34;&gt;In practice, for a parameter value $\tilde \theta^\pi$&lt;/h3&gt;
&lt;p&gt;For $r = 1, &amp;hellip;, R$ simulations do:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Initialize firms value to zero&lt;/li&gt;
&lt;li&gt;Fot $\tau=0, &amp;hellip;, \tilde T$ do
&lt;ul&gt;
&lt;li&gt;For each state in $\boldsymbol{\tilde s}^{(r)}_{\tau}$ do:
&lt;ul&gt;
&lt;li&gt;Use $\boldsymbol{\hat P}$ to &lt;em&gt;draw&lt;/em&gt; a vector of firm actions
$\boldsymbol{\tilde a}^{(r)}_{\tau}$&lt;/li&gt;
&lt;li&gt;For each firm $j = 1, &amp;hellip;, J$ do:
&lt;ul&gt;
&lt;li&gt;Compute static profits
$\pi_{j}^{\boldsymbol {\hat{P}&lt;em&gt;{-j}}} \left(\tilde a^{(r)}&lt;/em&gt;{j\tau}, \boldsymbol{\tilde s}^{(r)}_{\tau} ; \tilde \theta^\pi \right)$&lt;/li&gt;
&lt;li&gt;Add discounted profits
$\beta^{\tau} \pi_{j}^{\boldsymbol {\hat{P}&lt;em&gt;{-j}}} \left(\tilde a^{(r)}&lt;/em&gt;{j\tau}, \boldsymbol{\tilde s}^{(r)}_{\tau} ; \tilde \theta^\pi \right)$
to the value function&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Use
$f ( \cdot | \boldsymbol {a_{t}}, \boldsymbol s_t; \hat \theta^f )$
to &lt;em&gt;draw&lt;/em&gt; the next state
$\boldsymbol{\tilde s}^{(r)}_{\tau + 1}$&lt;/li&gt;
&lt;li&gt;Use the next state, $\boldsymbol{\tilde s}^{(r)}_{\tau + 1}$
as current state for the next iteration&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Then average all the value functions together to obtain an &lt;strong&gt;expected
value function&lt;/strong&gt;
$V_{j}^{\boldsymbol {\hat{P}}, R} \left(\boldsymbol{s}_{t} ; \tilde \theta^\pi \right)$&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: advantage of simulations: can be parallelized&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;objective-function&#34;&gt;Objective Function&lt;/h3&gt;
&lt;p&gt;What have we done so far?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Given some parameters $\theta^\pi$, we computed the &lt;strong&gt;expected value
function&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;How do we pick the $\theta^\pi$ that best rationalizes the data?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;I.e. what is the &lt;strong&gt;objective function&lt;/strong&gt;?&lt;/li&gt;
&lt;li&gt;Potentially many options&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;BBL idea&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the expected value function has to be optimal, given the CCPs&lt;/li&gt;
&lt;li&gt;I.e. any other policy function should give a lower expected value&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;“Best”&lt;/strong&gt; $\theta^\pi$: those for which the implied expected value
function under the estimated CCPs is greater than the one implied by
&lt;em&gt;any other&lt;/em&gt; CCP&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: it’s an inequality statement&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;objective-function-2&#34;&gt;Objective Function (2)&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Idea&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;If the observed policy ${\color{green}{\boldsymbol{\hat P}}}$ is
optimal,&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;All other policies ${\color{red}{\boldsymbol{\tilde P}}}$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;… at the true parameters $\theta^f$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;… should give a lower expected value $$
V_{j}^{{\color{red}{\boldsymbol{\tilde P}}}, R} \left( \boldsymbol{s}&lt;em&gt;{t} ; \tilde \theta^\pi \right) \leq V&lt;/em&gt;{j}^{{\color{green}{\boldsymbol{\hat P}}}, R} \left( \boldsymbol{s}_{t} ; \tilde \theta^\pi \right)
$$&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;So which are the true parameters?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Those for which any deviation from the observed policy
${\color{green}{\boldsymbol{\hat P}}}$ yields a lower value&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Objective function&lt;/strong&gt; to minimize: &lt;strong&gt;violations&lt;/strong&gt; under
alternative policies ${\color{red}{\boldsymbol{\tilde P}}}$ $$
\min_{\tilde \theta^\pi} \sum_{\boldsymbol s_{t}} \sum_{{\color{red}{\boldsymbol{\tilde P}}}} \Bigg[\min \bigg\lbrace V_{j}^{{\color{green}{\boldsymbol{\hat P}}}, R} \left( \boldsymbol{s}&lt;em&gt;{t} ; \tilde \theta^\pi \right) - V&lt;/em&gt;{j}^{{\color{red}{\boldsymbol{\tilde P}}}, R} \left( \boldsymbol{s}_{t} ; \tilde \theta^\pi \right) \ , \ 0 \bigg\rbrace \Bigg]^{2}
$$&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;estimator&#34;&gt;Estimator&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Estimator&lt;/strong&gt;: $\theta^\pi$ that minimizes the average (squared)
magnitude of violations for any alternative policy
${\color{red}{\boldsymbol{\tilde P}}}$ $$
\hat{\theta}^\pi= \arg \min_{\tilde \theta^\pi} \sum_{\boldsymbol s_{t}} \sum_{{\color{red}{\boldsymbol{\tilde P}}}} \Bigg[\min \bigg\lbrace V_{j}^{{\color{green}{\boldsymbol{\hat P}}}, R} \left( \boldsymbol{s}&lt;em&gt;{t} ; \tilde \theta^\pi \right) - V&lt;/em&gt;{j}^{{\color{red}{\boldsymbol{\tilde P}}}, R} \left( \boldsymbol{s}_{t} ; \tilde \theta^\pi \right) \ , \ 0 \bigg\rbrace \Bigg]^{2}
$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\min \Big\lbrace V_{j}^{{\color{green}{\boldsymbol{\hat P}}}, R} - V_j^{{\color{red}{\boldsymbol{\tilde P}}}, R} \ , \ 0 \Big\rbrace$
to pick only the violations
&lt;ul&gt;
&lt;li&gt;If ${\color{green}{\boldsymbol{\hat P}}}$ implies higher value,
we can ignore&lt;/li&gt;
&lt;li&gt;Doesn’t matter by how much you respect the inequality&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Which alternative policies&lt;/strong&gt;
${\color{red}{\boldsymbol{\tilde P}}}$ should we use?
&lt;ul&gt;
&lt;li&gt;In principle, any perturbation is ok&lt;/li&gt;
&lt;li&gt;But &lt;strong&gt;in practice&lt;/strong&gt;, if we perturbe it too much, we can go too
far off&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Tip 1&lt;/strong&gt;: start with very &lt;em&gt;small&lt;/em&gt; perturbations&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Tip 2&lt;/strong&gt;: use perturbation that &lt;em&gt;sensibly&lt;/em&gt; affect the dynamics
&lt;ul&gt;
&lt;li&gt;E.g. exiting in a state in which a firm is not a competitive
threat&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Tip 3&lt;/strong&gt;: use perturbations on dimensions that are &lt;em&gt;relevant&lt;/em&gt;
for the research question
&lt;ul&gt;
&lt;li&gt;E.g. they affect dimensions where you want to make
counterfactual predictions&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;advantages&#34;&gt;Advantages&lt;/h3&gt;
&lt;p&gt;We have seen that there are &lt;strong&gt;competing methods&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;What are the &lt;strong&gt;advantages&lt;/strong&gt; of Bajari, Benkard, and Levin
(&lt;a href=&#34;#ref-bajari2007estimating&#34;&gt;2007&lt;/a&gt;) over those?&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Continuous actions&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;BBL does not require actions to be discretised&lt;/li&gt;
&lt;li&gt;You can just sample actions from the data!&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Choice of alternative CCPs&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;The researcher is free to choose the alternative CCPs
${\color{red}{\boldsymbol{\tilde P}}}$&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Pros&lt;/strong&gt;: can make source of variation more transparent
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;allows the researcher to focus on those predictions of the
model that are key for the specific research questions&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Cons&lt;/strong&gt;: it’s a &lt;em&gt;very&lt;/em&gt; high dimensional space
&lt;ul&gt;
&lt;li&gt;There are &lt;em&gt;very very&lt;/em&gt; many alternative policy functions&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;problems&#34;&gt;Problems&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;Computational &lt;strong&gt;curse of dimensionality&lt;/strong&gt; is gone (in the state
space)
&lt;ul&gt;
&lt;li&gt;But we have a curse of dimensionality in data&lt;/li&gt;
&lt;li&gt;Need a lot of markets because &lt;strong&gt;now 1 market is 1 observation&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Multiple equilibria&lt;/strong&gt;??
&lt;ul&gt;
&lt;li&gt;We are basically assuming it away&lt;/li&gt;
&lt;li&gt;Estimating the CCPs in the first stage we assume that is the
equilibrium that is played in all markets at all times&lt;/li&gt;
&lt;li&gt;To run counterfactuals, we &lt;strong&gt;still need to solve the model&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Unobserved heterogeneity
&lt;ul&gt;
&lt;li&gt;Kasahara and Shimotsu (&lt;a href=&#34;#ref-kasahara2009nonparametric&#34;&gt;2009&lt;/a&gt;):
how to identify the (minimum) number of unobserved types&lt;/li&gt;
&lt;li&gt;Arcidiacono and Miller
(&lt;a href=&#34;#ref-arcidiacono2011conditional&#34;&gt;2011&lt;/a&gt;): how to use an EM
algorithm for the 1st stage estimation with unobserved types,
conditional on the number of types&lt;/li&gt;
&lt;li&gt;Berry and Compiani (&lt;a href=&#34;#ref-berry2021empirical&#34;&gt;2021&lt;/a&gt;):
instrumental variables approach, relying on observed states in
the distant past&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Non-stationarity&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;If we have a long time period, something fundamentally might
have changed&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;ericson-pakes-1995&#34;&gt;Ericson Pakes (1995)&lt;/h2&gt;
&lt;h3 id=&#34;introduction-1&#34;&gt;Introduction&lt;/h3&gt;
&lt;p&gt;Ericson and Pakes (&lt;a href=&#34;#ref-ericson1995markov&#34;&gt;1995&lt;/a&gt;) and companion paper
Pakes and McGuire (&lt;a href=&#34;#ref-pakes1994computing&#34;&gt;1994&lt;/a&gt;) for the computation&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;$J$ firms indexed by $j \in \lbrace 1, &amp;hellip;, J \rbrace$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Time $t$ is dicrete $t$, horizon is infinite&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;State $s_{jt}$: quality of firm $j$ in period $t$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Per period profits $$
\pi (s_{jt}, \boldsymbol s_{-jt}, ; \theta^\pi)
$$ where&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\boldsymbol s_{-it}$: state vector of all other firms in period
$t$&lt;/li&gt;
&lt;li&gt;$\theta^\pi$: parameters that govern static profits&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;We can micro-fund profits with some demand and supply functions&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;There can be some underlying static strategic interaction&lt;/li&gt;
&lt;li&gt;E.g. logit demand and bertrand competition in prices $p_{it}$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;state-transitions&#34;&gt;State Transitions&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Investment&lt;/strong&gt;: firms can invest an dollar amount $x$ to increase their
future quality&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Continuous decision variable ($\neq$ Rust)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Probability that investment is successful $$
\Pr \big(i_{jt} \ \big| \ a_{it} = x \big) = \frac{\alpha x}{1 + \alpha x}
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Higher investment, higher success probability&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$\alpha$ parametrizes the returns on investment&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Quality depreciation&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;With probability $\delta$, quality decreases by one level&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Law of motion&lt;/strong&gt; $$
s_{j,t+1} = s_{jt} + i_{jt} - \delta
$$&lt;/p&gt;
&lt;h3 id=&#34;decision-variables&#34;&gt;Decision Variables&lt;/h3&gt;
&lt;p&gt;Note that in Ericson and Pakes (&lt;a href=&#34;#ref-ericson1995markov&#34;&gt;1995&lt;/a&gt;) we have
two separate decision variables&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Static&lt;/strong&gt; decision variable: price $p_{jt}$&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Dynamic&lt;/strong&gt; decision variable: investment $i_{jt}$&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Does not have to be the case!&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Example&lt;/strong&gt;: Besanko et al. (&lt;a href=&#34;#ref-besanko2010learning&#34;&gt;2010&lt;/a&gt;)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Model of &lt;strong&gt;learning-by-doing&lt;/strong&gt;: firms decrease their marginal cost
through sales&lt;/li&gt;
&lt;li&gt;State variable: firm stock of know how $e$
&lt;ul&gt;
&lt;li&gt;The higher the stock of know-how, the lower the marginal cost&lt;/li&gt;
&lt;li&gt;Increases when a firm manages to make a sale
&lt;ul&gt;
&lt;li&gt;$q \in [0,1]$ now is both static quantity and transition
probability&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Single&lt;/strong&gt; decision variable: price $p$
&lt;ul&gt;
&lt;li&gt;Usual static effects on profits
$\pi_{jt} = (p_{jt} - c(e_{jt})) \cdot q_j(\boldsymbol p_t)$&lt;/li&gt;
&lt;li&gt;But also dynamic effect through transition probabilities
&lt;ul&gt;
&lt;li&gt;Probability of increasing $e_t$: $q_j(\boldsymbol p_t)$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;equilibrium-1&#34;&gt;Equilibrium&lt;/h3&gt;
&lt;p&gt;Firms maximize the expected flow of discounted profits $$
\max_{\boldsymbol a} \ \mathbb E_t \left[ \sum_{\tau=0}^\infty \beta^{\tau} \pi_{j, t+\tau} (\theta^\pi) \right]
$$ &lt;strong&gt;Markow Perfect Equilibrium&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Equillibrium notion: &lt;strong&gt;Markow Perfect Equilibrium&lt;/strong&gt; (&lt;a href=&#34;#ref-maskin1988theory&#34;&gt;Maskin and Tirole
1988&lt;/a&gt;)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A set of $J$ value and policy functions, $\boldsymbol V$ and
$\boldsymbol P$ such that each firm
&lt;ol&gt;
&lt;li&gt;maximizes its value function $V_j$&lt;/li&gt;
&lt;li&gt;given the policy function of every other firm
$\boldsymbol P_{-j}$&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;exit&#34;&gt;Exit&lt;/h3&gt;
&lt;p&gt;One important extension is &lt;strong&gt;exit&lt;/strong&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;In each time period, incuments decide whether to stay&lt;/li&gt;
&lt;li&gt;… or exit and get a scrap value $\phi^{exit}$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The Belman Equation of incumbent $j$ at time $t$ is $$
V^{\boldsymbol P_{-j}}&lt;em&gt;{j} (\mathbf{s}&lt;/em&gt;{t}) = \max_{d^{exit}&lt;em&gt;{jt} \in \lbrace 0, 1 \rbrace} \Bigg\lbrace
\begin{array}{c}
\beta \phi^{exit} \ , \newline
\max&lt;/em&gt;{a_{jt} \in \mathcal{A}&lt;em&gt;j \left(\mathbf{s}&lt;/em&gt;{t}\right)} \Big\lbrace  \pi_{j}^{\boldsymbol P_{-j}} (a_{jt}, \mathbf{s}&lt;em&gt;{t} ; \theta^\pi ) + \beta \mathbb E&lt;/em&gt;{\boldsymbol s_{t+1}} \Big[  V_{j}^{\boldsymbol P_{-j}} \left(\mathbf{s}&lt;em&gt;{t+1}\right) \ \Big| \ a&lt;/em&gt;{jt}, \boldsymbol s_{t} ; \theta^f \Big] \Big\rbrace
\end{array}
\Bigg\rbrace
$$ where&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\phi^{exit}$: exit scrap value&lt;/li&gt;
&lt;li&gt;$d^{exit}_{jt} \in \lbrace 0,1 \rbrace$: exit decision&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;entry&#34;&gt;Entry&lt;/h3&gt;
&lt;p&gt;We can also incorporate endogenous &lt;strong&gt;entry&lt;/strong&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;One or more &lt;strong&gt;potential entrants&lt;/strong&gt; exist outside the market&lt;/li&gt;
&lt;li&gt;They can pay an entry cost $\phi^{entry}$ and enter the market at a
quality state $\bar s$&lt;/li&gt;
&lt;li&gt;… or remain outside at no cost&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Value function $$
V_{j}^{\boldsymbol P_{-j}} (e, \boldsymbol x_{-jt} ; \theta) = \max_{d^{entry} \in \lbrace 0,1 \rbrace }
\Bigg\lbrace
\begin{array}{c}
0 \ ; \newline&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;\phi^{entry} + \beta \mathbb E_{\boldsymbol s_{t+1}} \Big[ V_{j}^{\boldsymbol P_{-j}} (\bar s, \boldsymbol s_{-j, t+1} ; \theta) \ \Big| \ \boldsymbol s_{t} ; \theta^f \Big]
\end{array}
\Bigg\rbrace
$$ where&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$d^{entry} \in \lbrace 0,1 \rbrace$: entry decision&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$\phi^{entry}$: entry cost&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$\bar s$: state in which entrants enters (could be random)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Do we observe potential entrants?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Igami (&lt;a href=&#34;#ref-igami2017estimating&#34;&gt;2017&lt;/a&gt;): tech industry announce
their entry&lt;/li&gt;
&lt;li&gt;Critique: not really potential entrants, they are half-way inside&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;equilibrium-existence&#34;&gt;Equilibrium Existence&lt;/h3&gt;
&lt;p&gt;Doraszelski and Satterthwaite (&lt;a href=&#34;#ref-doraszelski2010computable&#34;&gt;2010&lt;/a&gt;):
a MPE might not exist in Ericson and Pakes
(&lt;a href=&#34;#ref-ericson1995markov&#34;&gt;1995&lt;/a&gt;) model.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Solution&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Replace fixed entry costs $\phi^{entry}$ and exit scrap values
$\phi^{exit}$ with random ones&lt;/li&gt;
&lt;li&gt;It becomes a game of incomplete information
&lt;ul&gt;
&lt;li&gt;First explored in Rust (&lt;a href=&#34;#ref-rust1994structural&#34;&gt;1994&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;New equilibrium concept&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Markov Perfect Bayesian Nash Equilibrium (MPBNE)&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Basically the same, with rational beliefs on random variables&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;solving-the-model&#34;&gt;Solving the Model&lt;/h3&gt;
&lt;p&gt;Solving the model is very similar to Rust&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Given parameter values $\theta$&lt;/li&gt;
&lt;li&gt;Start with a guess for the value and policy functions&lt;/li&gt;
&lt;li&gt;Until convergence, do:
&lt;ul&gt;
&lt;li&gt;For each firm $j = 1, &amp;hellip;, J$, do:
&lt;ul&gt;
&lt;li&gt;Take the policy functions of all other firms&lt;/li&gt;
&lt;li&gt;Compute the implied transition probabilities&lt;/li&gt;
&lt;li&gt;Use them to compute the new policy function for firm $j$&lt;/li&gt;
&lt;li&gt;Compute the implied value function&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;Where do things get complicated / tricky? Policy function update&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;policy-update-example-exit-game&#34;&gt;Policy Update Example: exit game&lt;/h3&gt;
&lt;p&gt;Imagine a stylized exit game with 2 firms&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Easy to get an update rule of the form: &lt;em&gt;“exit if opponent stays,
stay if opponent exits”&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Computationally&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Initialize policy functions to $(exit, exit)$&lt;/li&gt;
&lt;li&gt;Iteration 1:
&lt;ul&gt;
&lt;li&gt;Each firm takes opponent policy as given: $exit$&lt;/li&gt;
&lt;li&gt;Update own optimal policy: $stay$&lt;/li&gt;
&lt;li&gt;New policy: $(stay, stay)$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Iteration 2: $(stay, stay) \to (exit, exit)$&lt;/li&gt;
&lt;li&gt;Iteration 2: $(exit, exit) \to (stay, stay)$&lt;/li&gt;
&lt;li&gt;Etc…&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Issues&lt;/strong&gt;: value function iteration might not converge and
equilibrium multeplicity.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;convergence-tips&#34;&gt;Convergence Tips&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Try different &lt;strong&gt;starting values&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;Often it’s what makes the biggest difference&lt;/li&gt;
&lt;li&gt;Ideally, start as close as possible to true values&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Approximation methods&lt;/strong&gt; can help (we’ll see more later)
&lt;ul&gt;
&lt;li&gt;I.e. get a fast approximation to use as starting vlaue for
solution algorithm&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Partial/stochastic &lt;strong&gt;value function update rule&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;Instead of $V&amp;rsquo; = T(V)$, use $V&amp;rsquo; = \alpha T(V) + (1-\alpha)V$&lt;/li&gt;
&lt;li&gt;Very good to break loops, especially if $\alpha$ is stochastic,
e.g. $\alpha \sim U(0,1)$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;How large is the &lt;strong&gt;support of the entry/exit costs&lt;/strong&gt;?
&lt;ul&gt;
&lt;li&gt;If support is too small, you end up back in the entry/exit loop&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Try alternative &lt;strong&gt;non-parallel updating schemes&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;E.g. update value one state at the time (in random order?)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Last but not least: &lt;strong&gt;change the model&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;In particular, from simultaneous to alternating moves&lt;/li&gt;
&lt;li&gt;or continuous time&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;multiple-equilibria&#34;&gt;Multiple Equilibria&lt;/h3&gt;
&lt;p&gt;How to find them?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Besanko et al. (&lt;a href=&#34;#ref-besanko2010learning&#34;&gt;2010&lt;/a&gt;) and Borkovsky,
Doraszelski, and Kryukov (&lt;a href=&#34;#ref-borkovsky2010user&#34;&gt;2010&lt;/a&gt;):
&lt;strong&gt;homotopy method&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;can find some equilibria, but not all&lt;/li&gt;
&lt;li&gt;complicated to implement: need to compute first order conditions
$H(\boldsymbol V, \theta) = 0$ and their Jacobian
$\Delta H(\boldsymbol V, \theta)$&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Idea&lt;/strong&gt;: trace the equilibrium correspondence
$H^{-1} = \lbrace (\boldsymbol V, \theta) : H(\boldsymbol V, \theta) = 0 \rbrace$
in the value-parameter space&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Eibelshäuser and Poensgen (&lt;a href=&#34;#ref-eibelshauser2019markov&#34;&gt;2019&lt;/a&gt;)
&lt;ul&gt;
&lt;li&gt;Markov Quantal Response Equilibrium&lt;/li&gt;
&lt;li&gt;approact dynamic games from a evolutionary game theory
perspective
&lt;ul&gt;
&lt;li&gt;actions played at random and those bringing highest payoffs
survive&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;$\to$ homothopy method guaranteed to find one equilibrium&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Pesendorfer and Schmidt-Dengler
(&lt;a href=&#34;#ref-pesendorfer2010sequential&#34;&gt;2010&lt;/a&gt;): some equilibria are not
Lyapunov-stable
&lt;ul&gt;
&lt;li&gt;BR iteration cannot find them unless you start exactly at the
solution&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Su and Judd (&lt;a href=&#34;#ref-su2012constrained&#34;&gt;2012&lt;/a&gt;) and Egesdal, Lai, and
Su (&lt;a href=&#34;#ref-egesdal2015estimating&#34;&gt;2015&lt;/a&gt;): same point, but numerically
&lt;ul&gt;
&lt;li&gt;using MPEC approach&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;multiple-equilibria-2&#34;&gt;Multiple Equilibria (2)&lt;/h3&gt;
&lt;p&gt;Can we assume them away?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Igami (&lt;a href=&#34;#ref-igami2017estimating&#34;&gt;2017&lt;/a&gt;)
&lt;ul&gt;
&lt;li&gt;Finite horizon&lt;/li&gt;
&lt;li&gt;Homogenous firms (in profit functions and state transitions)&lt;/li&gt;
&lt;li&gt;One dynamic move per period (overall, not per-firm)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Abbring and Campbell (&lt;a href=&#34;#ref-abbring2010last&#34;&gt;2010&lt;/a&gt;)
&lt;ul&gt;
&lt;li&gt;Entry/exit game&lt;/li&gt;
&lt;li&gt;Homogeneous firms&lt;/li&gt;
&lt;li&gt;Entry and exit decisions are follow a last-in first-out (LIFO)
structure
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;“An entrant expects to produce no longer than any
incumbent”&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Iskhakov, Rust, and Schjerning (&lt;a href=&#34;#ref-iskhakov2016recursive&#34;&gt;2016&lt;/a&gt;)
&lt;ul&gt;
&lt;li&gt;can find all equilibria, but for very specific class of dynamic
games&lt;/li&gt;
&lt;li&gt;must always proceed “forward”
&lt;ul&gt;
&lt;li&gt;e.g. either entry or exit but not both&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Idea: can solve by backward induction even if horizon is
infinite&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;curse-of-dimensionality&#34;&gt;Curse of Dimensionality&lt;/h3&gt;
&lt;p&gt;What are the computational bottlenecks? $$
V^{\boldsymbol P_{-j}}&lt;em&gt;{j} ({\color{red}{\mathbf{s}&lt;/em&gt;{t}}}) = \max_{a_{jt} \in \mathcal{A}&lt;em&gt;j \left(\mathbf{s}&lt;/em&gt;{t}\right)} \Bigg\lbrace \pi_{j}^{\boldsymbol P_{-j}} (a_{jt}, \mathbf{s}&lt;em&gt;{t} ; \theta^\pi ) + \beta \mathbb E&lt;/em&gt;{{\color{red}{\mathbf{s}&lt;em&gt;{t+1}}}} \Big[  V&lt;/em&gt;{j}^{\boldsymbol P_{-j}} \left(\mathbf{s}&lt;em&gt;{t+1}\right) \ \Big| \ a&lt;/em&gt;{jt}, \boldsymbol s_{t} ; \theta^f \Big] \Bigg\rbrace
$$&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Dimension of the state space&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;In single agent problems, we have as many states as many values
of $s_{jt}$ ($k$)&lt;/li&gt;
&lt;li&gt;In dynamics games, the state space goes from $k$ to $k^J$&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;symmetry helps&lt;/strong&gt;: state $[1,2,3]$ and $[1,3,2]$ become the
same for firm 1&lt;/li&gt;
&lt;li&gt;How much do we gain? From $k^J$ to
$k \cdot {k + J - 2 \choose k - 1}$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Dimension of the integrand&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;If in single agent problems, we have to integrate over $\kappa$
outcomes,
&lt;ul&gt;
&lt;li&gt;4 in Rust: engine replaced (yes|no) $\times$ mileage
increases (yes|no)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;… in dynamic games, we have to consider $\kappa^J$ outcomes&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: bottlenecks are not addittive but multiplicative: have to
solve the expectation for each point in the state space. Improving on
any of the two helps a lot.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;curse-of-dimensionality-2&#34;&gt;Curse of Dimensionality (2)&lt;/h3&gt;
&lt;p&gt;Two and a half classes of solutions:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Computational&lt;/strong&gt;: approximate the equilibrium
&lt;ul&gt;
&lt;li&gt;Doraszelski (&lt;a href=&#34;#ref-doraszelski2003r&#34;&gt;2003&lt;/a&gt;): use Chebyshev
polynomials for a basis function&lt;/li&gt;
&lt;li&gt;Farias, Saure, and Weintraub
(&lt;a href=&#34;#ref-farias2012approximate&#34;&gt;2012&lt;/a&gt;): combine approximations
with a MPEC-like approach&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Conceptual&lt;/strong&gt;: define another game
&lt;ul&gt;
&lt;li&gt;Weintraub, Benkard, and Van Roy
(&lt;a href=&#34;#ref-weintraub2008markov&#34;&gt;2008&lt;/a&gt;): oblivious equilibrium&lt;/li&gt;
&lt;li&gt;Ifrach and Weintraub (&lt;a href=&#34;#ref-ifrach2017framework&#34;&gt;2017&lt;/a&gt;): moment
based equilibrium&lt;/li&gt;
&lt;li&gt;Doraszelski and Judd (&lt;a href=&#34;#ref-doraszelski2012avoiding&#34;&gt;2012&lt;/a&gt;):
games in continuous time&lt;/li&gt;
&lt;li&gt;Doraszelski and Judd (&lt;a href=&#34;#ref-doraszelski2019dynamic&#34;&gt;2019&lt;/a&gt;):
games with random moves&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Kind of both: Pakes and McGuire (&lt;a href=&#34;#ref-pakes2001stochastic&#34;&gt;2001&lt;/a&gt;)
&lt;ul&gt;
&lt;li&gt;experience-based equilibrium (&lt;a href=&#34;#ref-fershtman2012dynamic&#34;&gt;Fershtman and Pakes
2012&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: useful also to get good starting values for a full solution
method!&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;oblivious-equilibrium&#34;&gt;Oblivious Equilibrium&lt;/h3&gt;
&lt;p&gt;Weintraub, Benkard, and Van Roy (&lt;a href=&#34;#ref-weintraub2008markov&#34;&gt;2008&lt;/a&gt;): what
if &lt;strong&gt;firms had no idea about the state of other firms&lt;/strong&gt;?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;or atomistic firms&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The value function becomes $$
V_{j} ({\color{red}{s_{t}}}) = \max_{a_{jt} \in \mathcal{A}&lt;em&gt;j \left({\color{red}{s&lt;/em&gt;{t}}}\right)} \Bigg\lbrace {\color{red}{\mathbb E_{\boldsymbol s_t}}} \Big[ \pi_{j} (a_{jt}, \mathbf{s}&lt;em&gt;{t} ; \theta^\pi ) \Big|  P  \Big] + \beta \mathbb E&lt;/em&gt;{{\color{red}{s_{t+1}}}} \Big[  V_{j} \left({\color{red}{s_{t+1}}}\right) \ \Big| \ a_{jt}, {\color{red}{s_{t}}} ; \theta^f \Big] \Bigg\rbrace
$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Now the state is just $s_t$ instead of $\boldsymbol s_t$
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Huge&lt;/strong&gt; computational gain: from $k^J$ points to $k$&lt;/li&gt;
&lt;li&gt;Also the expectation of future states is taken over $3$ instead
of $3^J$ points
&lt;ul&gt;
&lt;li&gt;(3 because quality can go up, down or stay the same)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;But need to compute static profits as the expected value given the
current policy function
&lt;ul&gt;
&lt;li&gt;Need to keep track of the asymptotic state distribution as you
iterate the value&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;games-with-random-moves&#34;&gt;Games with Random Moves&lt;/h3&gt;
&lt;p&gt;Doraszelski and Judd (&lt;a href=&#34;#ref-doraszelski2019dynamic&#34;&gt;2019&lt;/a&gt;): what if
instead of simultaneously, firms would &lt;strong&gt;move one at the time at
random&lt;/strong&gt;?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Important&lt;/strong&gt;: to have the same frequency of play, &lt;strong&gt;a period now is
$J$ times shorter&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The value function becomes $$
V^{\boldsymbol P_{-j}}&lt;em&gt;{j} (\mathbf{s}&lt;/em&gt;{t}, {\color{red}{n=j}}) = \max_{a_{jt} \in \mathcal{A}&lt;em&gt;j \left(\mathbf{s}&lt;/em&gt;{t}\right)} \Bigg\lbrace {\color{red}{\frac{1}{J}}}\pi_{j}^{\boldsymbol P_{-j}} (a_{jt}, \mathbf{s}&lt;em&gt;{t} ; \theta^\pi ) + {\color{red}{\sqrt[J]{\beta}}} \mathbb E&lt;/em&gt;{{\color{red}{n,  s_{j, t+1}}}} \Big[  V_{j}^{\boldsymbol P_{-j}} \left(\mathbf{s}&lt;em&gt;{t+1}, {\color{red}{n}} \right) \ \Big| \ a&lt;/em&gt;{jt}, \boldsymbol s_{t} ; \theta^f \Big] \Bigg\rbrace
$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$n$: indicates whose turn is to play&lt;/li&gt;
&lt;li&gt;since a turn is $J$ times shorter, profits are $\frac{1}{J} \pi$ and
discount factor is $\sqrt[J]{\beta}$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Computational gain&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Expectation now taken over $n, s_{j, t+1}$ instead of
$\boldsymbol s_{t+1}$&lt;/li&gt;
&lt;li&gt;I.e. $Jk$ points instead of $3^k$ (3 because quality can go up, down
or stay the same)&lt;/li&gt;
&lt;li&gt;Huge computational difference!&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;games-in-continuous-time&#34;&gt;Games in Continuous Time&lt;/h3&gt;
&lt;p&gt;Doraszelski and Judd (&lt;a href=&#34;#ref-doraszelski2012avoiding&#34;&gt;2012&lt;/a&gt;): what’s the
advantage of continuous time?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Probability that two firms take a decision simultaneously is zero&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;With continuous time, the value function becomes $$
V^{\boldsymbol P_{-j}}&lt;em&gt;{j} (\mathbf{s}&lt;/em&gt;{t}) = \max_{a_{jt} \in \mathcal{A}&lt;em&gt;j \left(\mathbf{s}&lt;/em&gt;{t}\right)} \Bigg\lbrace \frac{1}{\lambda(a_{jt}) - \log(\beta)} \Bigg( \pi_{j}^{\boldsymbol P_{-j}} (a_{jt}, \mathbf{s}&lt;em&gt;{t} ; \theta^\pi ) + \lambda(a&lt;/em&gt;{jt}) \mathbb E_{\boldsymbol s_{t+1}} \Big[  V_{j}^{\boldsymbol P_{-j}} \left(\mathbf{s}&lt;em&gt;{t+1}\right) \ \Big| \ a&lt;/em&gt;{jt}, \boldsymbol s_{t} ; \theta^f \Big] \Bigg) \Bigg\rbrace
$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\lambda(a_{jt}) = \delta + \frac{\alpha a_{jt}}{1 + \alpha a_{jt}}$
is the hazard rate for firm $j$ that &lt;strong&gt;something happens&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;i.e. either an increase in quality, with probability
$\frac{\alpha a_{jt}}{1 + \alpha a_{jt}}$&lt;/li&gt;
&lt;li&gt;… or a decrease in quality with probability $\delta$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Computational gain&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Now the expectation over future states
$\mathbb E_{\boldsymbol s_{t+1}}$ is over $2J$ points instead of
$3^J$
&lt;ul&gt;
&lt;li&gt;3 because quality can go up, down or stay the same&lt;/li&gt;
&lt;li&gt;2 because in continuous time we don’t care if the state does not
change (investment fails)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;comparison&#34;&gt;Comparison&lt;/h3&gt;
&lt;p&gt;Which method is &lt;strong&gt;best&lt;/strong&gt;?&lt;/p&gt;
&lt;p&gt;I compare them in Courthoud (&lt;a href=&#34;#ref-courthoud2020approximation&#34;&gt;2020&lt;/a&gt;)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Fastest: Weintraub, Benkard, and Van Roy
(&lt;a href=&#34;#ref-weintraub2008markov&#34;&gt;2008&lt;/a&gt;)
&lt;ul&gt;
&lt;li&gt;Effectively transforms the game into single-agent dynamics&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Best trade-off: Doraszelski and Judd
(&lt;a href=&#34;#ref-doraszelski2019dynamic&#34;&gt;2019&lt;/a&gt;)
&lt;ul&gt;
&lt;li&gt;Simple, practical and also helps in terms of equilibrium
multeplicity&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Also in Courthoud (&lt;a href=&#34;#ref-courthoud2020approximation&#34;&gt;2020&lt;/a&gt;): games
with random order
&lt;ul&gt;
&lt;li&gt;Better approximation than Doraszelski and Judd
(&lt;a href=&#34;#ref-doraszelski2019dynamic&#34;&gt;2019&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;And similar similar time&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;applications&#34;&gt;Applications&lt;/h3&gt;
&lt;p&gt;Some applications of these methods include&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Approximation methods
&lt;ul&gt;
&lt;li&gt;Sweeting (&lt;a href=&#34;#ref-sweeting2013dynamic&#34;&gt;2013&lt;/a&gt;): product
repositioning among radio stations&lt;/li&gt;
&lt;li&gt;Barwick and Pathak (&lt;a href=&#34;#ref-barwick2015costs&#34;&gt;2015&lt;/a&gt;): entry and
exit in the real estate brokerage industry&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Oblivious equilibrium
&lt;ul&gt;
&lt;li&gt;Xu and Chen (&lt;a href=&#34;#ref-xu2020structural&#34;&gt;2020&lt;/a&gt;): R&amp;amp;D investment in
the Korean electric motor industry&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Moment based equilibrium
&lt;ul&gt;
&lt;li&gt;Jeon (&lt;a href=&#34;#ref-jeon2020learning&#34;&gt;2020&lt;/a&gt;): demand learning in the
container shipping industry&lt;/li&gt;
&lt;li&gt;Caoui (&lt;a href=&#34;#ref-caoui2019estimating&#34;&gt;2019&lt;/a&gt;): technology adoption
with network effects in the movie industry&lt;/li&gt;
&lt;li&gt;Vreugdenhil (&lt;a href=&#34;#ref-vreugdenhil2020booms&#34;&gt;2020&lt;/a&gt;): search and
matching in the oil drilling industry&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Games in continuous time
&lt;ul&gt;
&lt;li&gt;Arcidiacono et al. (&lt;a href=&#34;#ref-arcidiacono2016estimation&#34;&gt;2016&lt;/a&gt;):
entry, exit and scale decisions in retail competition&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Games with random moves
&lt;ul&gt;
&lt;li&gt;Igami (&lt;a href=&#34;#ref-igami2017estimating&#34;&gt;2017&lt;/a&gt;): innovation, entry,
exit in the hard drive industry&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;from-io-to-ai&#34;&gt;From IO to AI&lt;/h2&gt;
&lt;h3 id=&#34;bridging-two-literatures&#34;&gt;Bridging two Literatures&lt;/h3&gt;
&lt;p&gt;There is one method to approximate the equilibrium in dynamic games that
is a bit different from the others: Pakes and McGuire
(&lt;a href=&#34;#ref-pakes2001stochastic&#34;&gt;2001&lt;/a&gt;)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Idea&lt;/strong&gt;: approximate the value function by Monte-Carlo simulation&lt;/li&gt;
&lt;li&gt;Firms start with a guess for the alternative-specific value function&lt;/li&gt;
&lt;li&gt;Act according to it&lt;/li&gt;
&lt;li&gt;Observe realized payoffs and state transitions&lt;/li&gt;
&lt;li&gt;And update the alternative-specific value function according to the
realized outcomes&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Experience-Based Equilibrium&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Defined in Fershtman and Pakes (&lt;a href=&#34;#ref-fershtman2012dynamic&#34;&gt;2012&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Def&lt;/strong&gt;: &lt;em&gt;policy is optimal given beliefs of state transitions and
observed transitions are consistent with the beliefs&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Note&lt;/strong&gt;: definition silent on off-equilibrium path beliefs&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;pakes-and-mcguire-2001&#34;&gt;Pakes and McGuire (2001)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Players start with &lt;strong&gt;alternative-specific value function&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;yes, the ASV from Rust (&lt;a href=&#34;#ref-rust1994structural&#34;&gt;1994&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;$\bar V_{j,a}^{(0)} (\boldsymbol s ; \theta)$: initial value of
player $j$ for action $a$ in state $\boldsymbol s$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Until convergence, do:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Compute optimal action, given
$\bar V_{j, a}^{(t)} (\boldsymbol s ; \theta)$ $$
a^* = \arg \max_a \bar V_{j, a}^{(t)} (\boldsymbol s ; \theta)
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Observe the realized payoff
$\pi_{j, a^&lt;em&gt;}(\boldsymbol s ; \theta)$ and the realized next
state $\boldsymbol {s&amp;rsquo;}(\boldsymbol s, a^&lt;/em&gt;; \theta)$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Update the alternative-specific value function of the chosen
action $k^&lt;em&gt;$ $$
\bar V_{j, a^&lt;/em&gt;}^{(t+1)} (\boldsymbol s ; \theta) = (1-\alpha_{\boldsymbol s, t}) \bar V_{j, a^&lt;em&gt;}^{(t)} (\boldsymbol s ; \theta) + \alpha_{\boldsymbol s, t} \Big[\pi_{j, a^&lt;/em&gt;}(\boldsymbol s ; \theta) + \arg \max_a \bar V_{j, a}^{(t)} (\boldsymbol s&amp;rsquo; ; \theta) \Big]
$$ where&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\alpha_{\boldsymbol s, t} = \frac{1}{\text{number of times state } \boldsymbol s \text{ has been visited}}$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;comments&#34;&gt;Comments&lt;/h3&gt;
&lt;p&gt;Where is the &lt;strong&gt;strategic interaction&lt;/strong&gt;?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Firm always take &lt;em&gt;“best action so far”&lt;/em&gt; in each state
&lt;ul&gt;
&lt;li&gt;Start to take a new action only when the previous best has
performed badly for many periods&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Remindful of literature of evolutionary game theory&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Importance of &lt;strong&gt;starting values&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Imagine, all payoffs are positive but value initialized to zero&lt;/li&gt;
&lt;li&gt;First action in each state $\to$ only action ever taken in that
state&lt;/li&gt;
&lt;li&gt;Loophole.
&lt;ul&gt;
&lt;li&gt;Why? Firms always take $\arg \max_a \bar V_a$ and never
&lt;em&gt;explore&lt;/em&gt; the alternatives&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Convergence&lt;/strong&gt; by desing&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;As $\lim_{t \to \infty} \alpha_{\boldsymbol s, t} = 1$&lt;/li&gt;
&lt;li&gt;Firms stop updating the value by design&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;q-learning&#34;&gt;Q-Learning&lt;/h3&gt;
&lt;p&gt;Computer Science reinforcement learning literature (AI): &lt;strong&gt;Q-learning&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Differences&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\bar V_a( \boldsymbol s)$ called $Q_a(\boldsymbol s)$, hence the
name&lt;/li&gt;
&lt;li&gt;Firms don’t always take the optimal action
&lt;ul&gt;
&lt;li&gt;At the beginning of the algorithm: &lt;strong&gt;exploration&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;Firms take actions at random&lt;/li&gt;
&lt;li&gt;Just to explore what happens taking different actions&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Gradually shift towards &lt;strong&gt;exploitation&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;I.e. take the optimal action, given
$\bar V^{(t)}( \boldsymbol s)$ at iteration $t$&lt;/li&gt;
&lt;li&gt;I.e. shift towards Pakes and McGuire
(&lt;a href=&#34;#ref-pakes2001stochastic&#34;&gt;2001&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;applications-1&#34;&gt;Applications&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Doraszelski, Lewis, and Pakes (&lt;a href=&#34;#ref-doraszelski2018just&#34;&gt;2018&lt;/a&gt;)
&lt;ul&gt;
&lt;li&gt;Firm do actually learn by trial and error&lt;/li&gt;
&lt;li&gt;Setting: demand learning in the UK frequency response market
(electricity)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Asker et al. (&lt;a href=&#34;#ref-asker2020computational&#34;&gt;2020&lt;/a&gt;)
&lt;ul&gt;
&lt;li&gt;Uses Pakes and McGuire (&lt;a href=&#34;#ref-pakes2001stochastic&#34;&gt;2001&lt;/a&gt;) for
estimation&lt;/li&gt;
&lt;li&gt;Setting: dynamic timber auctions with information sharing&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Calvano et al. (&lt;a href=&#34;#ref-calvano2020artificial&#34;&gt;2020&lt;/a&gt;)
&lt;ul&gt;
&lt;li&gt;Study Q-learning pricing algorithms&lt;/li&gt;
&lt;li&gt;In repeated price competition with differentiated products&lt;/li&gt;
&lt;li&gt;(Computational) lab experiment: what do these algorithms
converge to?&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Finding&lt;/strong&gt;: algorithms learn reward-punishment collusive
strategies&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;appendix&#34;&gt;Appendix&lt;/h2&gt;
&lt;h3 id=&#34;references-references&#34;&gt;References [references]&lt;/h3&gt;
&lt;div id=&#34;refs&#34; class=&#34;references csl-bib-body hanging-indent&#34;
markdown=&#34;1&#34;&gt;
&lt;div id=&#34;ref-abbring2010last&#34; class=&#34;csl-entry&#34; markdown=&#34;1&#34;&gt;
&lt;p&gt;Abbring, Jaap H, and Jeffrey R Campbell. 2010. “Last-in First-Out
Oligopoly Dynamics.” &lt;em&gt;Econometrica&lt;/em&gt; 78 (5): 1491–1527.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-aguirregabiria2021dynamic&#34; class=&#34;csl-entry&#34; markdown=&#34;1&#34;&gt;
&lt;p&gt;Aguirregabiria, Victor, Allan Collard-Wexler, and Stephen P Ryan. 2021.
“Dynamic Games in Empirical Industrial Organization.” National Bureau of
Economic Research.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-aguirregabiria2007sequential&#34; class=&#34;csl-entry&#34;
markdown=&#34;1&#34;&gt;
&lt;p&gt;Aguirregabiria, Victor, and Pedro Mira. 2007. “Sequential Estimation of
Dynamic Discrete Games.” &lt;em&gt;Econometrica&lt;/em&gt; 75 (1): 1–53.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-arcidiacono2016estimation&#34; class=&#34;csl-entry&#34; markdown=&#34;1&#34;&gt;
&lt;p&gt;Arcidiacono, Peter, Patrick Bayer, Jason R Blevins, and Paul B
Ellickson. 2016. “Estimation of Dynamic Discrete Choice Models in
Continuous Time with an Application to Retail Competition.” &lt;em&gt;The Review
of Economic Studies&lt;/em&gt; 83 (3): 889–931.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-arcidiacono2011conditional&#34; class=&#34;csl-entry&#34; markdown=&#34;1&#34;&gt;
&lt;p&gt;Arcidiacono, Peter, and Robert A Miller. 2011. “Conditional Choice
Probability Estimation of Dynamic Discrete Choice Models with Unobserved
Heterogeneity.” &lt;em&gt;Econometrica&lt;/em&gt; 79 (6): 1823–67.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-asker2020computational&#34; class=&#34;csl-entry&#34; markdown=&#34;1&#34;&gt;
&lt;p&gt;Asker, John, Chaim Fershtman, Jihye Jeon, and Ariel Pakes. 2020. “A
Computational Framework for Analyzing Dynamic Auctions: The Market
Impact of Information Sharing.” &lt;em&gt;The RAND Journal of Economics&lt;/em&gt; 51 (3):
805–39.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-bajari2007estimating&#34; class=&#34;csl-entry&#34; markdown=&#34;1&#34;&gt;
&lt;p&gt;Bajari, Patrick, C Lanier Benkard, and Jonathan Levin. 2007. “Estimating
Dynamic Models of Imperfect Competition.” &lt;em&gt;Econometrica&lt;/em&gt; 75 (5):
1331–70.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-barwick2015costs&#34; class=&#34;csl-entry&#34; markdown=&#34;1&#34;&gt;
&lt;p&gt;Barwick, Panle Jia, and Parag A Pathak. 2015. “The Costs of Free Entry:
An Empirical Study of Real Estate Agents in Greater Boston.” &lt;em&gt;The RAND
Journal of Economics&lt;/em&gt; 46 (1): 103–45.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-berry2021empirical&#34; class=&#34;csl-entry&#34; markdown=&#34;1&#34;&gt;
&lt;p&gt;Berry, Steven T, and Giovanni Compiani. 2021. “Empirical Models of
Industry Dynamics with Endogenous Market Structure.” &lt;em&gt;Annual Review of
Economics&lt;/em&gt; 13.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-besanko2010learning&#34; class=&#34;csl-entry&#34; markdown=&#34;1&#34;&gt;
&lt;p&gt;Besanko, David, Ulrich Doraszelski, Yaroslav Kryukov, and Mark
Satterthwaite. 2010. “Learning-by-Doing, Organizational Forgetting, and
Industry Dynamics.” &lt;em&gt;Econometrica&lt;/em&gt; 78 (2): 453–508.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-borkovsky2010user&#34; class=&#34;csl-entry&#34; markdown=&#34;1&#34;&gt;
&lt;p&gt;Borkovsky, Ron N, Ulrich Doraszelski, and Yaroslav Kryukov. 2010. “A
User’s Guide to Solving Dynamic Stochastic Games Using the Homotopy
Method.” &lt;em&gt;Operations Research&lt;/em&gt; 58 (4-part-2): 1116–32.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-calvano2020artificial&#34; class=&#34;csl-entry&#34; markdown=&#34;1&#34;&gt;
&lt;p&gt;Calvano, Emilio, Giacomo Calzolari, Vincenzo Denicolo, and Sergio
Pastorello. 2020. “Artificial Intelligence, Algorithmic Pricing, and
Collusion.” &lt;em&gt;American Economic Review&lt;/em&gt; 110 (10): 3267–97.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-caoui2019estimating&#34; class=&#34;csl-entry&#34; markdown=&#34;1&#34;&gt;
&lt;p&gt;Caoui, El Hadi. 2019. “Estimating the Costs of Standardization: Evidence
from the Movie Industry.” &lt;em&gt;R&amp;amp;R, Review of Economic Studies&lt;/em&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-collard2013demand&#34; class=&#34;csl-entry&#34; markdown=&#34;1&#34;&gt;
&lt;p&gt;Collard-Wexler, Allan. 2013. “Demand Fluctuations in the Ready-Mix
Concrete Industry.” &lt;em&gt;Econometrica&lt;/em&gt; 81 (3): 1003–37.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-courthoud2020approximation&#34; class=&#34;csl-entry&#34; markdown=&#34;1&#34;&gt;
&lt;p&gt;Courthoud, Matteo. 2020. “Approximation Methods for Large Dynamic
Stochastic Games.” &lt;em&gt;Working Paper&lt;/em&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-doraszelski2003r&#34; class=&#34;csl-entry&#34; markdown=&#34;1&#34;&gt;
&lt;p&gt;Doraszelski, Ulrich. 2003. “An r&amp;amp;d Race with Knowledge Accumulation.”
&lt;em&gt;Rand Journal of Economics&lt;/em&gt;, 20–42.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-doraszelski2012avoiding&#34; class=&#34;csl-entry&#34; markdown=&#34;1&#34;&gt;
&lt;p&gt;Doraszelski, Ulrich, and Kenneth L Judd. 2012. “Avoiding the Curse of
Dimensionality in Dynamic Stochastic Games.” &lt;em&gt;Quantitative Economics&lt;/em&gt; 3
(1): 53–93.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-doraszelski2019dynamic&#34; class=&#34;csl-entry&#34; markdown=&#34;1&#34;&gt;
&lt;p&gt;———. 2019. “Dynamic Stochastic Games with Random Moves.” &lt;em&gt;Quantitative
Marketing and Economics&lt;/em&gt; 17 (1): 59–79.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-doraszelski2018just&#34; class=&#34;csl-entry&#34; markdown=&#34;1&#34;&gt;
&lt;p&gt;Doraszelski, Ulrich, Gregory Lewis, and Ariel Pakes. 2018. “Just
Starting Out: Learning and Equilibrium in a New Market.” &lt;em&gt;American
Economic Review&lt;/em&gt; 108 (3): 565–615.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-doraszelski2010computable&#34; class=&#34;csl-entry&#34; markdown=&#34;1&#34;&gt;
&lt;p&gt;Doraszelski, Ulrich, and Mark Satterthwaite. 2010. “Computable
Markov-Perfect Industry Dynamics.” &lt;em&gt;The RAND Journal of Economics&lt;/em&gt; 41
(2): 215–43.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-egesdal2015estimating&#34; class=&#34;csl-entry&#34; markdown=&#34;1&#34;&gt;
&lt;p&gt;Egesdal, Michael, Zhenyu Lai, and Che-Lin Su. 2015. “Estimating Dynamic
Discrete-Choice Games of Incomplete Information.” &lt;em&gt;Quantitative
Economics&lt;/em&gt; 6 (3): 567–97.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-eibelshauser2019markov&#34; class=&#34;csl-entry&#34; markdown=&#34;1&#34;&gt;
&lt;p&gt;Eibelshäuser, Steffen, and David Poensgen. 2019. “Markov Quantal
Response Equilibrium and a Homotopy Method for Computing and Selecting
Markov Perfect Equilibria of Dynamic Stochastic Games.” &lt;em&gt;Working Paper&lt;/em&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-ericson1995markov&#34; class=&#34;csl-entry&#34; markdown=&#34;1&#34;&gt;
&lt;p&gt;Ericson, Richard, and Ariel Pakes. 1995. “Markov-Perfect Industry
Dynamics: A Framework for Empirical Work.” &lt;em&gt;The Review of Economic
Studies&lt;/em&gt; 62 (1): 53–82.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-esteban2007durable&#34; class=&#34;csl-entry&#34; markdown=&#34;1&#34;&gt;
&lt;p&gt;Esteban, Susanna, and Matthew Shum. 2007. “Durable-Goods Oligopoly with
Secondary Markets: The Case of Automobiles.” &lt;em&gt;The RAND Journal of
Economics&lt;/em&gt; 38 (2): 332–54.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-farias2012approximate&#34; class=&#34;csl-entry&#34; markdown=&#34;1&#34;&gt;
&lt;p&gt;Farias, Vivek, Denis Saure, and Gabriel Y Weintraub. 2012. “An
Approximate Dynamic Programming Approach to Solving Dynamic Oligopoly
Models.” &lt;em&gt;The RAND Journal of Economics&lt;/em&gt; 43 (2): 253–82.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-fershtman2012dynamic&#34; class=&#34;csl-entry&#34; markdown=&#34;1&#34;&gt;
&lt;p&gt;Fershtman, Chaim, and Ariel Pakes. 2012. “Dynamic Games with Asymmetric
Information: A Framework for Empirical Work.” &lt;em&gt;The Quarterly Journal of
Economics&lt;/em&gt; 127 (4): 1611–61.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-goettler2011does&#34; class=&#34;csl-entry&#34; markdown=&#34;1&#34;&gt;
&lt;p&gt;Goettler, Ronald L, and Brett R Gordon. 2011. “Does AMD Spur Intel to
Innovate More?” &lt;em&gt;Journal of Political Economy&lt;/em&gt; 119 (6): 1141–1200.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-hotz1993conditional&#34; class=&#34;csl-entry&#34; markdown=&#34;1&#34;&gt;
&lt;p&gt;Hotz, V Joseph, and Robert A Miller. 1993. “Conditional Choice
Probabilities and the Estimation of Dynamic Models.” &lt;em&gt;The Review of
Economic Studies&lt;/em&gt; 60 (3): 497–529.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-huang2014dynamic&#34; class=&#34;csl-entry&#34; markdown=&#34;1&#34;&gt;
&lt;p&gt;Huang, Ling, and Martin D Smith. 2014. “The Dynamic Efficiency Costs of
Common-Pool Resource Exploitation.” &lt;em&gt;American Economic Review&lt;/em&gt; 104 (12):
4071–4103.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-ifrach2017framework&#34; class=&#34;csl-entry&#34; markdown=&#34;1&#34;&gt;
&lt;p&gt;Ifrach, Bar, and Gabriel Y Weintraub. 2017. “A Framework for Dynamic
Oligopoly in Concentrated Industries.” &lt;em&gt;The Review of Economic Studies&lt;/em&gt;
84 (3): 1106–50.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-igami2017estimating&#34; class=&#34;csl-entry&#34; markdown=&#34;1&#34;&gt;
&lt;p&gt;Igami, Mitsuru. 2017. “Estimating the Innovator’s Dilemma: Structural
Analysis of Creative Destruction in the Hard Disk Drive Industry,
1981–1998.” &lt;em&gt;Journal of Political Economy&lt;/em&gt; 125 (3): 798–847.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-iskhakov2016recursive&#34; class=&#34;csl-entry&#34; markdown=&#34;1&#34;&gt;
&lt;p&gt;Iskhakov, Fedor, John Rust, and Bertel Schjerning. 2016. “Recursive
Lexicographical Search: Finding All Markov Perfect Equilibria of Finite
State Directional Dynamic Games.” &lt;em&gt;The Review of Economic Studies&lt;/em&gt; 83
(2): 658–703.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-jeon2020learning&#34; class=&#34;csl-entry&#34; markdown=&#34;1&#34;&gt;
&lt;p&gt;Jeon, Jihye. 2020. “Learning and Investment Under Demand Uncertainty in
Container Shipping.” &lt;em&gt;The RAND Journal of Economics&lt;/em&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-kasahara2009nonparametric&#34; class=&#34;csl-entry&#34; markdown=&#34;1&#34;&gt;
&lt;p&gt;Kasahara, Hiroyuki, and Katsumi Shimotsu. 2009. “Nonparametric
Identification of Finite Mixture Models of Dynamic Discrete Choices.”
&lt;em&gt;Econometrica&lt;/em&gt; 77 (1): 135–75.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-maskin1988theory&#34; class=&#34;csl-entry&#34; markdown=&#34;1&#34;&gt;
&lt;p&gt;Maskin, Eric, and Jean Tirole. 1988. “A Theory of Dynamic Oligopoly, II:
Price Competition, Kinked Demand Curves, and Edgeworth Cycles.”
&lt;em&gt;Econometrica: Journal of the Econometric Society&lt;/em&gt;, 571–99.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-pakes1994computing&#34; class=&#34;csl-entry&#34; markdown=&#34;1&#34;&gt;
&lt;p&gt;Pakes, Ariel, and Paul McGuire. 1994. “Computing Markov-Perfect Nash
Equilibria: Numerical Implications of a Dynamic Differentiated Product
Model.” &lt;em&gt;RAND Journal of Economics&lt;/em&gt; 25 (4): 555–89.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-pakes2001stochastic&#34; class=&#34;csl-entry&#34; markdown=&#34;1&#34;&gt;
&lt;p&gt;———. 2001. “Stochastic Algorithms, Symmetric Markov Perfect Equilibrium,
and the ‘Curse’of Dimensionality.” &lt;em&gt;Econometrica&lt;/em&gt; 69 (5): 1261–81.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-pakes2007simple&#34; class=&#34;csl-entry&#34; markdown=&#34;1&#34;&gt;
&lt;p&gt;Pakes, Ariel, Michael Ostrovsky, and Steven Berry. 2007. “Simple
Estimators for the Parameters of Discrete Dynamic Games (with Entry/Exit
Examples).” &lt;em&gt;The RAND Journal of Economics&lt;/em&gt; 38 (2): 373–99.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-pesendorfer2008asymptotic&#34; class=&#34;csl-entry&#34; markdown=&#34;1&#34;&gt;
&lt;p&gt;Pesendorfer, Martin, and Philipp Schmidt-Dengler. 2008. “Asymptotic
Least Squares Estimators for Dynamic Games.” &lt;em&gt;The Review of Economic
Studies&lt;/em&gt; 75 (3): 901–28.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-pesendorfer2010sequential&#34; class=&#34;csl-entry&#34; markdown=&#34;1&#34;&gt;
&lt;p&gt;———. 2010. “Sequential Estimation of Dynamic Discrete Games: A Comment.”
&lt;em&gt;Econometrica&lt;/em&gt; 78 (2): 833–42.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-rust1994structural&#34; class=&#34;csl-entry&#34; markdown=&#34;1&#34;&gt;
&lt;p&gt;Rust, John. 1994. “Structural Estimation of Markov Decision Processes.”
&lt;em&gt;Handbook of Econometrics&lt;/em&gt; 4: 3081–3143.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-ryan2012costs&#34; class=&#34;csl-entry&#34; markdown=&#34;1&#34;&gt;
&lt;p&gt;Ryan, Stephen P. 2012. “The Costs of Environmental Regulation in a
Concentrated Industry.” &lt;em&gt;Econometrica&lt;/em&gt; 80 (3): 1019–61.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-su2012constrained&#34; class=&#34;csl-entry&#34; markdown=&#34;1&#34;&gt;
&lt;p&gt;Su, Che-Lin, and Kenneth L Judd. 2012. “Constrained Optimization
Approaches to Estimation of Structural Models.” &lt;em&gt;Econometrica&lt;/em&gt; 80 (5):
2213–30.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-sweeting2013dynamic&#34; class=&#34;csl-entry&#34; markdown=&#34;1&#34;&gt;
&lt;p&gt;Sweeting, Andrew. 2013. “Dynamic Product Positioning in Differentiated
Product Markets: The Effect of Fees for Musical Performance Rights on
the Commercial Radio Industry.” &lt;em&gt;Econometrica&lt;/em&gt; 81 (5): 1763–803.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-vreugdenhil2020booms&#34; class=&#34;csl-entry&#34; markdown=&#34;1&#34;&gt;
&lt;p&gt;Vreugdenhil, Nicholas. 2020. “Booms, Busts, and Mismatch in Capital
Markets: Evidence from the Offshore Oil and Gas Industry.” &lt;em&gt;R&amp;amp;R at
Journal of Political Economy&lt;/em&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-weintraub2008markov&#34; class=&#34;csl-entry&#34; markdown=&#34;1&#34;&gt;
&lt;p&gt;Weintraub, Gabriel Y, C Lanier Benkard, and Benjamin Van Roy. 2008.
“Markov Perfect Industry Dynamics with Many Firms.” &lt;em&gt;Econometrica&lt;/em&gt; 76
(6): 1375–1411.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-xu2020structural&#34; class=&#34;csl-entry&#34; markdown=&#34;1&#34;&gt;
&lt;p&gt;Xu, Daniel Yi, and Yanyou Chen. 2020. “A Structural Empirical Model of
r&amp;amp;d, Firm Heterogeneity, and Industry Evolution.” &lt;em&gt;Journal of Industrial
Economics.&lt;/em&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Non-Parametric Estimation</title>
      <link>https://matteocourthoud.github.io/course/metrics/08_nonparametric/</link>
      <pubDate>Fri, 29 Oct 2021 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/course/metrics/08_nonparametric/</guid>
      <description>&lt;h3 id=&#34;introduction&#34;&gt;Introduction&lt;/h3&gt;
&lt;p&gt;Non-parametric regression is a flexible estimation procedure for&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;regression functions $\mathbb E [y|x ] = g (x)$ and&lt;/li&gt;
&lt;li&gt;density functions $f(x)$.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;You want to let your data to tell you how flexible you can afford to be
in terms of estimation procedures. Non-parametric regression is
naturally introduced in terms of fitting a curve.&lt;/p&gt;
&lt;p&gt;Consider the problem of estimating the Conditional Expectation Function,
defined as $\mathbb E [y_i |x_i ] = g(x_i)$ given data
$D = (x_i, y_i)_{i=1}^n$ under minimal assumption of $g(\cdot)$,
e.g. smoothness. There are two main methods:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Local methods: Kernel-based estimation&lt;/li&gt;
&lt;li&gt;Global methods: Series-based estimation&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Another way of looking at non-parametrics is to do estimation/inference
without specifying functional forms. With no assumptions, informative
inference is impossible. Non parametrics tries to work with functional
restrictions—continuity, differentiability, etc.—rather than
pre-specifying functional form.&lt;/p&gt;
&lt;h3 id=&#34;discrete-x---cell-estimator&#34;&gt;Discrete x - Cell Estimator&lt;/h3&gt;
&lt;p&gt;Suppose that $x$ can take $R$ distinct values, e.g. gender $R=2$, years
of schooling $R=20$, gender $\times$ years of schooling
$R = 2 \times 20$.&lt;/p&gt;
&lt;p&gt;A simple way for estimating $\mathbb E \left[ y |x \right] = g(x)$ is to
split the sample to include observations with $x_i = x$ and calculate
the sample mean of $\bar{y}$ for these observations. Note that this
requires no assumptions about how $\mathbb E [y_i |x_i]$ varies with $x$
since we fit a different value for each value $x$. $$
\hat{g}(x) = \frac{1}{| i: x_i = x |} \sum_{i : x_i = x} y_i
$$&lt;/p&gt;
&lt;p&gt;Issues:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Curse of dimensionality&lt;/strong&gt;: if $R$ is big compared to $n$, there
will be only a small number of observations per $x$ values. If $x_i$
is continuous, $R=n$ with probability 1. Solution: we can borrow
information about $g_0(x)$ using neighboring observations of $x$.&lt;/li&gt;
&lt;li&gt;Averaging for each separate $x_r$ value is only feasible in cases
where $x_i$ is coarsely discrete.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;local-non-parametric-estimation&#34;&gt;Local Non-Parametric Estimation&lt;/h2&gt;
&lt;h3 id=&#34;kernels&#34;&gt;Kernels&lt;/h3&gt;
&lt;p&gt;Suppose we believe that $\mathbb E [y_i |x_i]$ is a smooth function of
$x_i$ – e.g. continuous, differentiable, etc. Then it should not change
too much across values of $x$ that are close to each other: we can
estimate the conditional expectation at $x = \bar{x}$ by averaging $y$’s
over the values of $x$ that are “close”” to $\bar{x}$. This procedure
relies on two (three) arbitrary choices:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Choice of the &lt;strong&gt;kernel function&lt;/strong&gt; $K (\cdot)$; it is used to weight
“far out”” observations, such that
&lt;ul&gt;
&lt;li&gt;$K: \mathbb R \to \mathbb R$&lt;/li&gt;
&lt;li&gt;$K$ is symmetric: $K(\bar{x} + x_i) = K(\bar{x} - x_i)$&lt;/li&gt;
&lt;li&gt;$\lim_{x_i \to \infty}K(x_i - \bar{x}) = 0$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Choice of the &lt;strong&gt;bandwidth&lt;/strong&gt; $h$: it measures the size of a
``small’’ window around $\bar{x}$,
e.g. $(\bar{x} - h, \bar{x} + h)$.&lt;/li&gt;
&lt;li&gt;Choice of the local estimation procedure. Examples are locally
constant, a.k.a. Nadaraya-Watson (&lt;strong&gt;NW&lt;/strong&gt;), and locally linear
(&lt;strong&gt;LL&lt;/strong&gt;).&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;Generally, the choice of $h$ is more important than $K(\cdot)$ in low
dimensional settings.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;optimal-h&#34;&gt;Optimal h&lt;/h3&gt;
&lt;p&gt;We need to define what is an “optimal” $h$, depending on the smoothness
level of $g_0$, typically unknown. The choice of $h$ relates to the
bias-variance trade-off:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;large $h$: small variance, higher bias;&lt;/li&gt;
&lt;li&gt;small $h$: high variance, smaller bias.&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;Note that $K_h (\cdot) = K (\cdot / h)$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;locally-constant-estimator&#34;&gt;Locally Constant Estimator&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Nadaraya-Watson&lt;/strong&gt; estimator, or locally constant estimator. It
assumes the CEF locally takes the form $g(x) = \beta_0(x)$. The
local parameter is estimated as: $$
\hat{\beta}&lt;em&gt;0 (\bar{x}) = \arg\min&lt;/em&gt;{\beta_0}  \quad  \mathbb E_n \Big[ K_h (x_i - \bar{x}) \cdot  \big(y_i - \beta_0 \big)^2 \Big]
$$&lt;/li&gt;
&lt;/ul&gt;
&lt;img src=&#34;../img/Fig_521.png&#34; style=&#34;width:50.0%&#34; /&gt;
&lt;h3 id=&#34;cef&#34;&gt;CEF&lt;/h3&gt;
&lt;p&gt;The Nadaraya-Watson estimate of the CEF takes the form: $$
\mathbb E_n \left[ y | x = \bar{x}\right] = \hat{g}(\bar{x}) = \frac{\sum_{i=1}^n y_i K_h (x_i - \bar{x})}{\sum_{i=1}^n K_h (x_i - \bar{x})}
$$&lt;/p&gt;
&lt;h3 id=&#34;locally-linear-estimator&#34;&gt;Locally Linear Estimator&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Local Linear&lt;/strong&gt; estimator. It assumes the CEF locally takes the
form $g(x) = \beta_0(x) + \beta_1(x) x$. The local parameters are
estimated as:&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$
\left( \hat{\beta}_0 (\bar{x}), \hat{\beta}&lt;em&gt;1 (\bar{x}) \right) = \arg\min&lt;/em&gt;{\beta_0, \beta_1}  \quad   \mathbb E_n \Big[ K_h (x_i - \bar{x}) \cdot  \big(y_i - \beta_0 - (x_i - \bar{x}) \beta_1 \big)^2 \Big]
$$&lt;/p&gt;
&lt;img src=&#34;../img/Fig_522.png&#34; style=&#34;width:50.0%&#34; /&gt;
&lt;h3 id=&#34;cef-1&#34;&gt;CEF&lt;/h3&gt;
&lt;p&gt;In this case, we do LS estimate with $i$’s contribution of residual
weighted by the kernel $K_h (x_i - \bar{x})$. The final estimate at
$\bar{x}$ is given by: $$
\hat{g} (\bar{x}) = \hat{\beta}_0 (\bar{x}) + (\bar{x} - \bar{x}) \hat{\beta}_1 (\bar{x}) = \hat{\beta}_0 (\bar{x})
$$ since we have centered the $x_s$ at $\bar{x}$ in the kernel. - It is
possible to add linearly higher order polynomials, e.g. do locally
quadratic least squares using loss function:&lt;/p&gt;
&lt;p&gt;$$
\mathbb E_n \left[ K_h (x_i - \bar{x}) \big(y_i - \beta_0 - (x_i - \bar{x}) \beta_1 - (x_i - \bar{x})^2 \beta_2 \big)^2 \right]
$$&lt;/p&gt;
&lt;h3 id=&#34;uniform-kernel&#34;&gt;Uniform Kernel&lt;/h3&gt;
&lt;p&gt;LS restricted to sample $i$ such that $x_i$ within $h$ of $\bar{x}$. $$
\begin{aligned}
&amp;amp; K (\cdot) = \mathbb I\lbrace \cdot \in [-1, 1] \rbrace  \newline
&amp;amp; K_h (\cdot) = \mathbb I\lbrace \cdot/h \in [-1, 1] \rbrace = \mathbb I\lbrace \cdot \in [-h, h] \rbrace  \newline
&amp;amp; K_h (x_i - \bar{x}) = \mathbb I\lbrace x_i - \bar{x} \in [-h, h] \rbrace  = \mathbb I\lbrace x_i \in [\bar{x}-h, \bar{x} + h] \rbrace
\end{aligned}
$$ Employed together with the locally linear estimator, the estimation
procedure reduces to **local least squares}. The loss function is: $$
\mathbb E_n \Big[ K_n (x_i - \bar{x}) \big(y_i -\beta_0 - \beta_1 (x_i - \bar{x}) \big)^2 \Big] = \frac{1}{n} \sum_{i: x_i \in [\bar{x}-h, \bar{x} +h ]}  \big(y_i -\beta_0 - \beta_1 (x_i - \bar{x}) \big)^2
$$&lt;/p&gt;
&lt;p&gt;The more local is the estimation, the more appropriate the linear
regression: if $g_0$ is smooth,
$g_0(\bar{x}) + g_0&amp;rsquo;(\bar{x}) (x_i - \bar{x})$ is a better approximation
for $g_0 (x_i)$.&lt;/p&gt;
&lt;p&gt;However, the uniform density is not a good kernel choice as it produces
discontinuous CEF estimates. The following are two popular alternative
choices that produce continuous CEF estimates.&lt;/p&gt;
&lt;h3 id=&#34;other-kernels&#34;&gt;Other Kernels&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Epanechnikov kernel&lt;/strong&gt; $$
K_h(x_i - \bar{x}) = \frac { 3 } { 4 } \left( 1 - (x_i - \bar{x}) ^ { 2 } \right)  \mathbb I\lbrace x_i \in [\bar{x}-h, \bar{x} + h] \rbrace
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Normal or Gaussian kernel&lt;/strong&gt; $$
K_\phi (x_i - \bar{x})  = \frac { 1 } { \sqrt { 2 \pi } } \exp \left( - \frac { (x_i - \bar{x}) ^ { 2 } } { 2 } \right)
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;K-Nearest Neighbors (KNN)&lt;/strong&gt;: choose bandwidth so that there is a
fixed number of observations in each kernel. This kernel is
different from the others since it takes a nonparamentric form.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;example&#34;&gt;Example&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;../img/Fig_523.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;choice-of-the-optimal-bandwidth&#34;&gt;Choice of the optimal bandwidth&lt;/h3&gt;
&lt;p&gt;Practical methods:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Eyeball Method.&lt;/strong&gt; (i) Choose a bandwidth (ii) Estimate the
regression function (iii) Look at the result: if it looks more
wiggly than you would like, increase the bandwidth: if it looks more
smooth than you would like, decrease the bandwidth. Con: It only
works for $\dim(x_i) = 1$ or $2$.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Rule of Thumb.&lt;/strong&gt; For example, Silverman’s rule of thumb:
$h = \left( \frac{4 \hat{\sigma}^5}{3n} \right)^{\frac{1}{5}}$. Con:
It requires too much knowledge about $g_0$ (i.e. normality) which
you don’t have.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Cross Validation.&lt;/strong&gt; Under some assumptions, CV will approximately
gives the MSE optimal bandwidth. The basic idea is to evaluate
quality of the bandwidth by looking at how well the resulting
estimator forecasts in the given sample.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Leave-one-out CV. For each $h &amp;gt; 0$ and each $i$, $\hat{g}&lt;em&gt;{-i} (x_i)$ is
the estimate of the conditional expectation at $x_i$ using bandwidth $h$
and all observations expect observation $i$. The CV bandwidth is defined
as $$
\hat{h} = \arg \min_h CV(h) = \arg \min_h \sum&lt;/em&gt;{i=1}^n  \Big( y_i -  \hat{g}_{-i} (x_i) \Big)^2
$$&lt;/p&gt;
&lt;h3 id=&#34;practical-tips&#34;&gt;Practical Tips&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Select a value for $h$.&lt;/li&gt;
&lt;li&gt;For each observation $i$, calculate $$
\hat{g}&lt;em&gt;{-i} (x_i) = \frac{\sum&lt;/em&gt;{j \ne i} y_j K_h (x_j - x_i) }{\sum_{i=1}^n K_h (x_j - x_i)}, \qquad e_{i,h}^2 = \left(y_i - \hat{g}_{-i} (x_i) \right)^2
$$&lt;/li&gt;
&lt;li&gt;Calculate $\text{CV}(h) = \sum_{i=1}^n e^2_{i,h}$.&lt;/li&gt;
&lt;li&gt;Repeat for each $h$ and choose the one that minimizes
$\text{CV}(h)$.&lt;/li&gt;
&lt;/ul&gt;
&lt;img src=&#34;../img/Fig_524.png&#34; style=&#34;width:50.0%&#34; /&gt;
&lt;h3 id=&#34;inference&#34;&gt;Inference&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt;: Consider data $\lbrace y_i, x_i \rbrace_{i=1}^n$, iid and
suppose that $y_i = g(x_i) + \varepsilon_i$ where
$\mathbb E[\varepsilon_i|x_i] = 0$. Assume that $x_i \in Interior(X)$
where $X \subseteq \mathbb R$, $g(x)$ and $f(x)$ are three times
continuously differentiable, and $f(x) &amp;gt; 0$ on $X$. $f(x)$ is the
probability density of $x \in X$ , and $g(x)$ is the function of
interest. Suppose that $K(\cdot)$ is a kernel function. Suppose
$n\to\infty$, $h\to0$ , $nh\to\infty$, and $nh^7\to0$. Then for any
fixed $x\in X$, $$
AMSE = \sqrt{nh} \Big( \hat{g}(x) - g(x) - h^2 B(x)\Big) \overset{d}{\to} N \left( 0, \frac{\kappa \sigma^2(x)}{f(x)}\right)
$$ for $\sigma^2(x) = Var(y_i|x_i = x)$, $\kappa = \int K^2(v)dv$, and
$B(x) = \frac{\kappa_2}{2} \frac{f&amp;rsquo;(x)g&amp;rsquo;(x) + f(x) g&amp;rsquo;&amp;rsquo;(x)}{f(x)}$ where
$\kappa_2 = \int v^2 K(v)dv$.&lt;/p&gt;
&lt;h3 id=&#34;remarks&#34;&gt;Remarks&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;If the function is smooth enough and the bandwidth small enough, you
can ignore the bias relative to sampling variation. To make this
plausible, use a smaller bandwidth than would be the “optimal”.&lt;/li&gt;
&lt;li&gt;All kernel regression estimators can be written as a weighted
average $$
\hat{g}(x) = \frac{1}{n} \sum_{i=1}^n w_i (x) y_i, \quad \text{ with } \quad w_i (x) = \frac{n K_h (x_i - x)}{\sum_{i=1}^n K_h (x_i - x)}
$$ Do inference as if you were estimating a mean $\mathbb E[z_i]$
with sample mean $\frac{1}{n} \sum_{i=1}^n z_i$ using
$z_i = w_i (x) y_i$.&lt;/li&gt;
&lt;li&gt;If you are doing inference at more than one value of $x$, do
inference as in the previous point, treating each value of $x$ as a
different sample mean and note that even with independent data,
these means will be correlated in general because there will
generally be some common observations in to each of the averages. If
you have a time series, make sure you account for correlation
between the observations going in the different averages even if
they don’t overlap.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Issue when doing inference: the estimation of the bandwidth from the
data is generally not accounted for in the distributional approximation
(when doing inference). In large-samples, this is unlikely to lead to
large changes, but uncertainty is understated in small samples.&lt;/p&gt;
&lt;h3 id=&#34;bias-variance-trade-off&#34;&gt;Bias-Variance Trade-off&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;For any estimator mean-square error MSE is decomposable into variance
and bias-squared: $$
\text{MSE} (\bar{x}, \hat{g}) = \mathbb E \left[ \left( \hat{g}(\bar{x}) - g_0 (\bar{x}) \right)^2 \right] = \mathbb E \Big[\underbrace{ \hat{g}(\bar{x}) - g_0 (\bar{x}) }_{\text{Bias}} \Big]^2 +  Var (\hat{g} (\bar{x})).
$$&lt;/p&gt;
&lt;h3 id=&#34;proof&#34;&gt;Proof&lt;/h3&gt;
&lt;p&gt;The theorem follows from the following corollary.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Corollary&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Let $A$ be a random variable and $\theta_0$ a fixed parameter. Then, $$
\mathbb E [ (A - \theta_0)^2] = Var (A) + \mathbb E [A-\theta_0]^2
$$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt; $$
\begin{aligned}
\mathbb E [ (A - \theta_0)^2] &amp;amp; = \mathbb E[A^2] - 2 \mathbb E [A \theta_0] + \mathbb E [\theta_0] \newline
&amp;amp;  = \mathbb E[A^2] \underbrace{-  \mathbb E[A]^2 + E[A]^2}_{\text{add and subtract}} - 2 \mathbb E [A \theta_0] + \mathbb E [\theta_0] \newline
&amp;amp;  = Var(A) + \mathbb E [A]^2 - 2 \theta_0 \mathbb E [A ] + \mathbb E [\theta_0] \newline
&amp;amp; = Var(A) + \mathbb E [A - \theta_0]^2
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;Note that $\mathbb E [ (A - \theta_0)^2] = \mathbb E [A - \theta_0]^2$.
$$\tag*{$\blacksquare$}$$&lt;/p&gt;
&lt;h3 id=&#34;criteria&#34;&gt;Criteria&lt;/h3&gt;
&lt;p&gt;Which criteria should we use with non-parametric estimators?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Mean squared error (MSE)&lt;/strong&gt;: $$
\text{MSE} (\bar{x}) (\hat{g}) = \mathbb E \left[ \left( \hat{g}(\bar{x}) - g_0 (\bar{x}) \right)^2 \right]
$$ &lt;strong&gt;NB!&lt;/strong&gt; This is the criterium we are going to use.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Integrated mean squared error (IMSE)&lt;/strong&gt;: $$
\text{IMSE} ( \hat{g} ) = \mathbb E \left[ \int | \hat{g} (x) - g_0 (x) |^2 \mathrm{d} F(x)  \right]
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Type I - Type II error.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;comments&#34;&gt;Comments&lt;/h3&gt;
&lt;p&gt;Hansen (2019): the theorem above implies that we can asymptotically
approximate the MSE as $$
\text{AMSE} = \Big( h^2 \sigma_k^2 B(x) \Big)^2 + \frac{\kappa \sigma^2(x)}{nh f(x)} \approx \text{const} \cdot \left( h^4 + \frac{1}{n h} \right)
$$&lt;/p&gt;
&lt;p&gt;Where&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$Var \propto \frac{1}{h n}$, where you can think of $n h$ as the
&lt;strong&gt;effective sample size&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;Bias $\propto h^2$, derived if $g_0$ is twice continuously
differentiable using Taylor expansion.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;trade-off&#34;&gt;Trade-Off&lt;/h3&gt;
&lt;p&gt;The asymptotic MSE is dominated by the larger of $h^4$ and
$\frac{1}{h n}$. Notice that the bias is increasing in $h$ and the
variance is decreasing in $h$ (more smoothing means more observations
are used for local estimation: this increases the bias but decreases
estimation variance). To select $h$ to minimize the asymptotic MSE,
these two components should balance each other: $$
\frac{1}{h n} \propto h^4 \quad \Rightarrow \quad  h \propto n^{-1/5}
$$&lt;/p&gt;
&lt;p&gt;This result means that the bandwidth should take the form
$h = c \cdot n^{-1/5}$. The optimal constant $c$ depends on the kernel
$k$ the bias function $B(x)$ and the marginal density $f_x(x)$. A common
misinterpretation is to set $h = n^{-1/5}$ which is equivalent to
setting $c = 1$ and is completely arbitrary. Instead, an empirical
bandwidth selection rule such as cross-validation should be used in
practice.&lt;/p&gt;
&lt;h2 id=&#34;global-non-parametric-estimation&#34;&gt;Global Non-Parametric Estimation&lt;/h2&gt;
&lt;h3 id=&#34;series&#34;&gt;Series&lt;/h3&gt;
&lt;p&gt;The goal is to try to globally approximate the CEF with a function
$g(x)$. Series methods are based on the &lt;strong&gt;Stone-Weierstrass theorem&lt;/strong&gt;: a
real-valued continuous function $g(x)$ defined in a compact set can be
approximated with polynomials for any degree of accuracy $$
g_0 (x) = p_1 (x) \beta _1 + \dots + p_K (x) \beta_K + r(x)
$$ where $p_1(x), \dots, p_K(x)$ are called ``a dictionary of
approximating series’’ and $r(x)$ is a remainder function. If
$p_1(x), \dots, p_K(x)$ are sufficiently rich, $r(x)$ will be small. If
$K \to \infty$, then $r \to 0$.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Example - Taylor series: if $g(x)$ is infinitely differentiable, then
$$
g(x) = \sum_{k=0}^{\infty } a_k x^k
$$ where $a_k = \frac{1}{k!} \frac{\partial^k g_0}{\partial x^k}$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;in-practice&#34;&gt;In Practice&lt;/h3&gt;
&lt;p&gt;The basic idea is to approximate the infinite sum by chopping it off
after $K$ terms and then estimate the coefficients by OLS.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Series estimation&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Choose $K$, i.e. the number of series terms, and an approximating
dictionary $p_1(x), \dots, p_K(x)$&lt;/li&gt;
&lt;li&gt;Expand data to
$D = \left( y_i, p_1(x_i), \dots, p_K(x_i) \right)_{i=1}^n$&lt;/li&gt;
&lt;li&gt;Estimate OLS to get $\hat{\beta}_1, \dots, \hat{\beta}_K$&lt;/li&gt;
&lt;li&gt;Set
$\hat{g}(x) = p_1 (x)\hat{\beta}_1 + \dots + p_K(x) \hat{\beta}_K$&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;examples&#34;&gt;Examples&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Monomials&lt;/strong&gt;: $p_1(x) = 1, p_2(x) = x, p_3(x)=x^2, \dots$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Hermite Polynomials&lt;/strong&gt;: $p_1(x) = 1$, $p_2(x) = x$,
$p_3(x)=x^2 -1$, $p_4(x)= x^3 - 3x, \dots$. Con: &lt;strong&gt;edge effects&lt;/strong&gt;.
The estimated function is particularly volatile at the edges of the
sample space (Gibbs effect)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Trig Polynomials&lt;/strong&gt;: $p_1(x) = 1$, $p_2(x) = \cos 2 \pi x$,
$p_3(x)= \sin 2 \pi x$, $p_4(x) = \cos 2 \pi x \cdot 2 x \dots$.
Pro: cyclical therefore good for series. Con: edge effects&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;B-splines&lt;/strong&gt;: recursively constructed using knot points $$
B_{i, 0} = \begin{cases}
1 &amp;amp; \text{if } t_i \leq x &amp;lt; t_{i+1} \newline 0 &amp;amp; \text{otherwise}
\end{cases} \qquad B_{i_k} (x) = \frac{x - t_i}{ t_{i+k} - t_i} B_{i, k-1} (x) +  \frac{t_{i+k+1}-x}{t_{i+k+1} - t_{i+1}} B_{i+1, k-1} (x)
$$ where $t_0, \dots, t_i, \dots$ are knot points and $k$ is the
order of the spline. Pro: faster rate of convergence and lower
asymptotic bias.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;hermite-polynomials&#34;&gt;Hermite Polynomials&lt;/h3&gt;
&lt;img src=&#34;../img/Fig_531.png&#34; style=&#34;width:50.0%&#34; /&gt;
&lt;h3 id=&#34;estimation&#34;&gt;Estimation&lt;/h3&gt;
&lt;p&gt;Given $K$, inference proceeds exactly as if one had run an OLS of $y$ on
$(p_k)_{k=1}^K$. The idea is that you ignore that you are doing
non-parametric regression as long as you believe you have put enough
terms (high $K$). Then the function is smooth enough so that the bias of
the approximation is small relative to the variance (see Newey, 1997).
Note that his approximation does not account for data-dependent
estimation of the bandwidth.&lt;/p&gt;
&lt;h3 id=&#34;consistency&#34;&gt;Consistency&lt;/h3&gt;
&lt;p&gt;Newey (1997): results about consistency of $\hat{g}$ and asymptotic
normality of $\hat{g}$.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;OLS: $\hat{\beta} \overset{p}{\to} \beta_0$&lt;/li&gt;
&lt;li&gt;Non-parametric: you have a sequence $\lbrace\beta_k\rbrace_{k=1}^K$
with $\hat{\beta}_k \overset{p}{\to} \beta_k$ as $n \to \infty$ (as
$k \to \infty$). However, this does not make sense because
$\lbrace\beta_k\rbrace$ is not constant. Moreover, $\beta_k$ is not
the quantity of interest. We want to make inference on $\hat{g}(x)$.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Under regularity conditions, including
$| | \hat{\beta} - \beta_0 | | \overset{p}{\to} 0$,&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Uniform Consistency:
$\sup_x | \hat{g}(x) - g_0(x)| \overset{p}{\to} 0$&lt;/li&gt;
&lt;li&gt;Mean-square Consistency:
$\int | \hat{g}(x) - g_0(x)|^2 \mathrm{d} F(x) \overset{p}{\to} 0$&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;imse&#34;&gt;IMSE&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Under the following assumptions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$(x_i, y_i)$ are iid and $Var(y_i|x_i)$ is bounded;&lt;/li&gt;
&lt;li&gt;For all $K$, there exists a non-singular matrix $B$ such that
$A = \left[ (B p(x)) (B p(x))&amp;rsquo; \right]$ where
$p(x) = \left( p_1(x), \dots, p_K (x) \right)$ has the properties
that $\lambda_{\min} (A)^{-1} = O(1)$. In addition,
$\sup_x | | B p(x) | | = o(\sqrt{K/n})$.&lt;/li&gt;
&lt;li&gt;There exists $\alpha$ and $\beta_K$ for all $K$ such that $$
\sup_x | g_0 (x) - p(x) \beta_K | = O_p(K^{-\alpha})
$$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Then, it holds that&lt;/p&gt;
&lt;p&gt;$$
\text{IMSE = }\int \left( g_0 (x) - \hat{g} (x) \right)^2 \mathrm{d} F(x) = O_p \left( \frac{K}{n} + K^{-2\alpha}\right)
$$&lt;/p&gt;
&lt;h3 id=&#34;choice-of-the-optimal-k&#34;&gt;Choice of the optimal $K$&lt;/h3&gt;
&lt;p&gt;The bias-variance trade-off for series comes in through the choice of
$K$:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Higher $K$: smaller bias, since we are leaving out less terms form
the infinite sum.&lt;/li&gt;
&lt;li&gt;Smaller $K$: smaller variance, since we are estimating less
regression coefficients from the same amount of data.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Cross-validation for series&lt;/strong&gt;: For each $K \geq 0$ and for each
$i=1, \dots, n$, consider&lt;/p&gt;
&lt;p&gt;$$
D_{-i} = \lbrace (x_1, y_1), \dots, (x_{i-1}, y_{i-1}),(x_{i+1}, y_{i+1}), \dots (x_n, y_n) \rbrace
$$ and calculate $\hat{g}^{(K)}_{-i} (x)$ using series estimate with
$p_1(x), \dots, p_K (x)$ in order to get
$e^{(K)}&lt;em&gt;i = y_i - \hat{g}^{(K)}&lt;/em&gt;{-i} (x_i)$. Choose $\hat{K}$ such that&lt;/p&gt;
&lt;p&gt;$$
\hat{K} = \arg \min_K \mathbb E_n \left[ {e^{(K)}_i}^2 \right]
$$&lt;/p&gt;
&lt;h3 id=&#34;inference-1&#34;&gt;Inference&lt;/h3&gt;
&lt;p&gt;Consider the data $D = \lbrace (x_i, y_i) \rbrace_{i=1}^n$ such that
$y_i = g_0 (x_i) + \varepsilon_i$. You may want to form confidence
intervals for quantities that depends on $g_0$.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Example: $\theta_0$ functional forms of interests:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Point estimate: $\theta_0 = g_0 (\bar{x} )$ for fixed $\bar{x}$&lt;/li&gt;
&lt;li&gt;Interval estimate: $\theta_0 = g_0 (\bar{x}_2) - g_0 (\bar{x}_1)$&lt;/li&gt;
&lt;li&gt;Point derivative estimate: $\theta_0 = g_0 &amp;rsquo; (\bar{x})$ at
$\bar{x}$&lt;/li&gt;
&lt;li&gt;Average derivative $\theta_0 = \mathbb E [g_0 &amp;rsquo; (x) ]$&lt;/li&gt;
&lt;li&gt;Consumer surplus: $\theta_0 = \int_a^b g_0(x)dx \quad$ when $g_0$
is a demand function.&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;p&gt;Those estimates are functionals: maps from a function to a real number.
We are doing inference on a function now, not on a point estimate.&lt;/p&gt;
&lt;h3 id=&#34;inference-2&#34;&gt;Inference&lt;/h3&gt;
&lt;p&gt;In order to form a confidence interval for $\theta_0$, with series you
can&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Undersmooth&lt;/strong&gt;: in order to apply a
&lt;code&gt;\textit{central limit theorem}&lt;/code&gt;{=tex}, you need deviations around
the function to be approximately gaussian. Undersmoothing makes the
function oscillate much more than the curve you are estimating in
order to obtain such guassian deviations.&lt;/li&gt;
&lt;li&gt;Use the &lt;strong&gt;delta method&lt;/strong&gt;. It would usually require more series terms
than a criterion like cross-validation would suggest.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;undersmoothing&#34;&gt;Undersmoothing&lt;/h3&gt;
&lt;img src=&#34;../img/Fig_541.png&#34; style=&#34;width:50.0%&#34; /&gt;
&lt;p&gt;If on the contrary you oversmooth (e.g. $g_0$ linear), errors are going
to constantly be on either one or the other side of the curve $\to$ not
gaussian!&lt;/p&gt;
&lt;h3 id=&#34;delta-method&#34;&gt;Delta Method&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt;: Under the assumptions of the consistency theorem $$
\frac{\sqrt{n} \Big(\hat{\theta} - \theta_0 + B(r_K) \Big)}{\sqrt{v_K}} \overset{d}{\to} N (0,1)
$$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt;: Under the assumptions of the consistency theorem and
$\sqrt{n} K^{-\alpha} = o(1)$ (or equivalently $n K^{-2\alpha} = O(1)$
in Hansen), $$
\frac{\sqrt{n} \Big(\hat{\theta} - \theta_0 \Big)}{\sqrt{v_K}} \overset{d}{\to} N (0,1)
$$&lt;/p&gt;
&lt;h3 id=&#34;remark&#34;&gt;Remark&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;The rate of convergence of splines is faster than for power series
(Newey 1997).&lt;/li&gt;
&lt;li&gt;We have &lt;strong&gt;undersmoothing&lt;/strong&gt; if $\sqrt{n} K^{\alpha} = o(1)$ (see
comment below)&lt;/li&gt;
&lt;li&gt;Usually, in order to prove asymptotic normality, we first prove
unbiasedness. However here we have a &lt;strong&gt;biased&lt;/strong&gt; estimator but we
make the bias converge to zero faster than the variance.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Hansen (2019): The critical condition is the assumption that
$\sqrt{n} K^{\alpha} = o(1)$ This requires that $K \to \infty$ at a rate
faster than $n^{\frac{1}{2\alpha}}$ This is a troubling condition. The
optimal rate for estimation of $g(x)$ is
$K = O(n^{\frac{1}{1+ 2\alpha}})$. If we set
$K = n^{\frac{1}{1+ 2\alpha}}$ by this rule then
$n K^{-2\alpha} = n^{\frac{1}{1+ 2\alpha}} \to \infty$ not zero. Thus
this assumption is equivalent to assuming that $K$ is much larger than
optimal. The reason why this trick works (that is, why the bias is
negligible) is that by increasing $K$ the asymptotic bias decreases and
the asymptotic variance increases and thus the variance dominates.
Because $K$ is larger than optimal, we typically say that $\hat{g}(x)$
is &lt;strong&gt;undersmoothed&lt;/strong&gt; relative to the optimal series estimator.&lt;/p&gt;
&lt;h3 id=&#34;more-remarks&#34;&gt;More Remarks&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;Many authors like to focus their asymptotic theory on the assumptions
in the theorem, as the distribution of $\theta$ appears cleaner.
However, it is a poor use of asymptotic theory. There are three
problems with the assumption $\sqrt{n} K^{-\alpha} = o(1)$ and the
approximation of the theorem.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;First, it says that if we intentionally pick $K$ to be larger than
optimal, we can increase the estimation variance relative to the
bias so the variance will dominate the bias. But why would we want
to intentionally use an estimator which is sub-optimal?&lt;/li&gt;
&lt;li&gt;Second, the assumption $\sqrt{n} K^{-\alpha} = o(1)$ does not
eliminate the asymptotic bias, it only makes it of lower order
than the variance. So the approximation of the theorem is
technically valid, but the missing asymptotic bias term is just
slightly smaller in asymptotic order, and thus still relevant in
finite samples.&lt;/li&gt;
&lt;li&gt;Third, the condition $\sqrt{n} K^{\alpha} = o(1)$ is just an
assumption, it has nothing to do with actual empirical practice.
Thus the difference between the two theorems is in the
assumptions, not in the actual reality or in the actual empirical
practice. Eliminating a nuisance (the asymptotic bias) through an
assumption is a trick, not a substantive use of theory. My strong
view is that the result (1) is more informative than (2). It shows
that the asymptotic distribution is normal but has a non-trivial
finite sample bias.&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;kernel-vs-series&#34;&gt;Kernel vs Series&lt;/h3&gt;
&lt;p&gt;Hansen (2019): in this and the previous chapter we have presented two
distinct methods of nonparametric regression based on kernel methods and
series methods. Which should be used in practice? Both methods have
advantages and disadvantages and there is no clear overall winner.&lt;/p&gt;
&lt;p&gt;First, while the asymptotic theory of the two estimators appear quite
different, they are actually rather closely related. When the regression
function $g(x)$ is twice differentiable $(s = 2)$ then the rate of
convergence of both the MSE of the kernel regression estimator with
optimal bandwidth $h$ and the series estimator with optimal $K$ is
$n^{-\frac{2}{k+4}}$ (where $k = \dim(x)$). There is no difference. If
the regression function is smoother than twice differentiable ($s &amp;gt; 2$)
then the rate of the convergence of the series estimator improves. This
may appear to be an advantage for series methods, but kernel regression
can also take advantage of the higher smoothness by using so-called
higher-order kernels or local polynomial regression, so perhaps this
advantage is not too large.&lt;/p&gt;
&lt;p&gt;Both estimators are asymptotically normal and have straightforward
asymptotic standard error formulae. The series estimators are a bit more
convenient for this purpose, as classic parametric standard error
formula work without amendment.&lt;/p&gt;
&lt;h3 id=&#34;advantages-of-kernels&#34;&gt;Advantages of Kernels&lt;/h3&gt;
&lt;p&gt;An advantage of kernel methods is that their distributional theory is
easier to derive. The theory is all based on local averages which is
relatively straightforward. In contrast, series theory is more
challenging, dealing with increasing parameter spaces. An important
difference in the theory is that for kernel estimators we have explicit
representations for the bias while we only have rates for series
methods. This means that plug-in methods can be used for bandwidth
selection in kernel regression. However, typically we rely on
cross-validation, which is equally applicable in both kernel and series
regression.&lt;/p&gt;
&lt;p&gt;Kernel methods are also relatively easy to implement when the dimension
of $x$, $k$, is large. There is not a major change in the methodology as
$k$ increases. In contrast, series methods become quite cumbersome as
$k$ increases as the number of cross-terms increases exponentially. E.g
($K=2$) with $k=1$ you have only $\lbrace x_1, x_1^2\rbrace$; with $k=2$
you have to add $\lbrace x_2, x_2^2, x_1 x_2 \rbrace$; with $k=3$ you
have to add $\lbrace x_3, x_3^2, x_1 x_3, x_2 x_3\rbrace$, etc..&lt;/p&gt;
&lt;h3 id=&#34;advantages-of-series&#34;&gt;Advantages of Series&lt;/h3&gt;
&lt;p&gt;A major advantage of series methods is that it has inherently a high
degree of flexibility, and the user is able to implement shape
restrictions quite easily. For example, in series estimation it is
relatively simple to implement a partial linear CEF, an additively
separable CEF, monotonicity, concavity or convexity. These restrictions
are harder to implement in kernel regression.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Post-Double Selection</title>
      <link>https://matteocourthoud.github.io/course/ml-econ/09_postdoubleselection/</link>
      <pubDate>Wed, 09 Mar 2022 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/course/ml-econ/09_postdoubleselection/</guid>
      <description>&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Remove warnings
import warnings
warnings.filterwarnings(&#39;ignore&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Import everything
import pandas as pd
import numpy as np
import seaborn as sns
import statsmodels.api as sm

from numpy.linalg import inv
from statsmodels.iolib.summary2 import summary_col
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression, Lasso, Ridge
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.preprocessing import PolynomialFeatures, StandardScaler
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Import matplotlib for graphs
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import axes3d

# Set global parameters
%matplotlib inline
plt.style.use(&#39;seaborn-white&#39;)
plt.rcParams[&#39;lines.linewidth&#39;] = 3
plt.rcParams[&#39;figure.figsize&#39;] = (10,6)
plt.rcParams[&#39;figure.titlesize&#39;] = 20
plt.rcParams[&#39;axes.titlesize&#39;] = 18
plt.rcParams[&#39;axes.labelsize&#39;] = 14
plt.rcParams[&#39;legend.fontsize&#39;] = 14
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;91-frisch-waugh-theorem&#34;&gt;9.1 Frisch-Waugh theorem&lt;/h2&gt;
&lt;p&gt;Consider the data $D = { x_i, y_i, z_i }_{i=1}^n$ with DGP:&lt;/p&gt;
&lt;p&gt;$$
y_i = x_i&amp;rsquo; \alpha_0+ z_i&amp;rsquo; \beta_0 + \varepsilon_i
$$&lt;/p&gt;
&lt;p&gt;. The following estimators of $\alpha$ are numerically equivalent (if $[x, z]$ has full rank):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;OLS: $\hat{\alpha}$ from regressing $y$ on $x, z$&lt;/li&gt;
&lt;li&gt;Partialling out: $\tilde{\alpha}$ from regressing $y$ on $\tilde{x}$&lt;/li&gt;
&lt;li&gt;&amp;ldquo;Double&amp;rdquo; partialling out: $\bar{\alpha}$ from regressing $\tilde{y}$ on $\tilde{x}$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;where the operation of passing to $y, x$ to $\tilde{y}, \tilde{x}$ is called &lt;em&gt;projection  out $z$&lt;/em&gt;, e.g. $\tilde{x}$ are the residuals from regressing $x$ on $z$.&lt;/p&gt;
&lt;p&gt;$$
\tilde{x} = x - \hat \gamma z = (I - z (z&amp;rsquo; z)^{-1} z&amp;rsquo; ) x = (I-P_z) x = M_z x
$$&lt;/p&gt;
&lt;p&gt;I.e we have done the following:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;regress $x$ on $z$&lt;/li&gt;
&lt;li&gt;compute $\hat x$&lt;/li&gt;
&lt;li&gt;compute the residuals $\tilde x = x - \hat x$&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;We now explore the theorem through simulation. In particular, we generate a sample from the following model:&lt;/p&gt;
&lt;p&gt;$$
y_i = x_i - 0.3 z_i + \varepsilon_i
$$&lt;/p&gt;
&lt;p&gt;where $x_i,z_i,\varepsilon_i \sim N(0,1)$ and $n=1000$.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;np.random.seed(1)

# Init
n = 1000
a = 1
b = -.3

# Generate data
x = np.random.uniform(0,1,n).reshape(-1,1)
z = np.random.uniform(0,1,n).reshape(-1,1)
e = np.random.normal(0,1,n).reshape(-1,1)
y = a*x + b*z + e
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s compute the value of the OLS estimator.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Estimate alpha by OLS
xz = np.concatenate([x,z], axis=1)
ols_coeff = inv(xz.T @ xz) @ xz.T @ y
alpha_ols = ols_coeff[0][0]

print(&#39;alpha OLS: %.4f (true=%1.0f)&#39; % (alpha_ols, a))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;alpha OLS: 1.0928 (true=1)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The partialling out estimator.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Partialling out
x_tilde = (np.eye(n) - z @ inv(z.T @ z) @ z.T ) @ x
alpha_po = inv(x_tilde.T @ x_tilde) @ x_tilde.T @ y

print(&#39;alpha partialling out: %.4f (true=%1.0f)&#39; % (alpha_po, a))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;alpha partialling out: 1.0928 (true=1)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And lastly, the double-partialling out estimator.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# &amp;quot;Double&amp;quot; partialling out
y_tilde = (np.eye(n) - z @ inv(z.T @ z) @ z.T ) @ y
alpha_po2 = inv(x_tilde.T @ x_tilde) @ x_tilde.T @ y_tilde

print(&#39;alpha double partialling out: %.4f (true=%1.0f)&#39; % (alpha_po2, a))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;alpha double partialling out: 1.0928 (true=1)
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;92-omitted-variable-bias&#34;&gt;9.2 Omitted Variable Bias&lt;/h2&gt;
&lt;p&gt;Consider two separate statistical models. Assume the following &lt;strong&gt;long regression&lt;/strong&gt; of interest:&lt;/p&gt;
&lt;p&gt;$$
y_i = x_i&amp;rsquo; \alpha_0+ z_i&amp;rsquo; \beta_0 + \varepsilon_i
$$&lt;/p&gt;
&lt;p&gt;Define the corresponding &lt;strong&gt;short regression&lt;/strong&gt; as&lt;/p&gt;
&lt;p&gt;$$
y_i = x_i&amp;rsquo; \alpha_0 + v_i \quad \text{ with } \quad x_i = z_i&amp;rsquo; \gamma_0 + u_i
$$&lt;/p&gt;
&lt;h4 id=&#34;ovb-theorem&#34;&gt;OVB Theorem&lt;/h4&gt;
&lt;p&gt;Suppose that the DGP for the long regression corresponds to $\alpha_0$, $\beta_0$. Suppose further that $\mathbb E[x_i] = 0$, $\mathbb E[z_i] = 0$, $\mathbb E[\varepsilon_i |x_i,z_i] = 0$. Then, unless $\beta_0 = 0$ or $z_i$ is orthogonal to $x_i$, the (sole) stochastic regressor $x_i$ is correlated with the error term in the short regression which implies that the OLS estimator of the short regression is inconsistent for $\alpha_0$ due to the omitted variable bias. In particular, one can show that the plim of the OLS estimator of $\hat{\alpha}_{SHORT}$ from the short regression is&lt;/p&gt;
&lt;p&gt;$$
\hat{\alpha}_{SHORT} \overset{p}{\to} \frac{Cov(y_i, x_i)}{Var(x_i)} = \alpha_0 + \beta_0 \frac{Cov(z_i, x_i)}{Var(x_i)}
$$&lt;/p&gt;
&lt;p&gt;Consider data $D= (y_i, x_i, z_i)_{i=1}^n$, where the true model is:&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
&amp;amp; y_i = x_i&amp;rsquo; \alpha_0  + z_i&amp;rsquo; \beta_0 + \varepsilon_i \
&amp;amp; x_i = z_i&amp;rsquo; \gamma_0 + u_i
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s investigate the Omitted Variable Bias by simulation. In particular, we generate a sample from the following model:&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
&amp;amp; y_i = x_i - 0.3 z_i + \varepsilon_i \
&amp;amp; x_i = 3 z_i + u_i \
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;where $z_i,\varepsilon_i,u_i \sim N(0,1)$ and $n=1000$.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def generate_data(a, b, c, n):

    # Generate data
    z = np.random.normal(0,1,n).reshape(-1,1)
    u = np.random.normal(0,1,n).reshape(-1,1)
    x = c*z + u
    e = np.random.normal(0,1,n).reshape(-1,1)
    y = a*x + b*z + e
    
    return x, y, z
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;First let&amp;rsquo;s compute the value of the OLS estimator.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Init
n = 1000
a = 1
b = -.3
c = 3
x, y, z = generate_data(a, b, c, n)

# Estimate alpha by OLS
ols_coeff = inv(x.T @ x) @ x.T @ y
alpha_short = ols_coeff[0][0]

print(&#39;alpha OLS: %.4f (true=%1.0f)&#39; % (alpha_short, a))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;alpha OLS: 0.9115 (true=1)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In our case the expected bias is:&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
Bias &amp;amp; = \beta_0 \frac{Cov(z_i, x_i)}{Var(x_i)} = \
&amp;amp; = \beta_0 \frac{Cov(z_i&amp;rsquo; \gamma_0 + u_i, x_i)}{Var(z_i&amp;rsquo; \gamma_0 + u_i)} = \
&amp;amp; = \beta_0 \frac{\gamma_0 Var(z_i)}{\gamma_0^2 Var(z_i) + Var(u_i)}
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;which in our case is $b \frac{c}{c^2 + 1}$.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Expected bias
bias = alpha_short - a
exp_bias = b * c / (c**2 + 1)

print(&#39;Empirical bias: %.4f \nExpected bias:  %.4f&#39; % (bias, exp_bias))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Empirical bias: -0.0885 
Expected bias:  -0.0900
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;93-pre-test-bias&#34;&gt;9.3 Pre-Test Bias&lt;/h2&gt;
&lt;p&gt;Consider data $D= (y_i, x_i, z_i)_{i=1}^n$, where the true model is:&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
&amp;amp; y_i = x_i&amp;rsquo; \alpha_0  + z_i&amp;rsquo; \beta_0 + \varepsilon_i \
&amp;amp; x_i = z_i&amp;rsquo; \gamma_0 + u_i
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;Where $x_i$ is the variable of interest (we want to make inference on $\alpha_0$) and $z_i$ is a high dimensional set of control variables.&lt;/p&gt;
&lt;p&gt;From now on, we will work under the following assumptions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\dim(x_i)=1$ for all $n$&lt;/li&gt;
&lt;li&gt;$\beta_0$ uniformely bounded in $n$&lt;/li&gt;
&lt;li&gt;Strict exogeneity: $\mathbb E[\varepsilon_i | x_i, z_i] = 0$ and $\mathbb E[u_i | z_i] = 0$&lt;/li&gt;
&lt;li&gt;$\beta_0$ and $\gamma_0$ have dimension (and hence value) that depend on $n$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Pre-Testing procedure:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Regress $y_i$ on $x_i$ and $z_i$&lt;/li&gt;
&lt;li&gt;For each $j = 1, &amp;hellip;, p = \dim(z_i)$ calculate a test statistic $t_j$&lt;/li&gt;
&lt;li&gt;Let $\hat{T} = { j: |t_j| &amp;gt; C &amp;gt; 0 }$ for some constant $C$ (set of statistically significant coefficients).&lt;/li&gt;
&lt;li&gt;Re-run the new &amp;ldquo;model&amp;rdquo; using $(x_i, z_{\hat{T},i})$ (i.e. using the selected covariates with statistically significant coefficients).&lt;/li&gt;
&lt;li&gt;Perform statistical inference (i.e. confidence intervals and hypothesis tests) as if no model selection had been done.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Pre-testing leads to incorrect inference. Why? Because of test errors in the first stage.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# T-test
def t_test(y, x, k):
    beta_hat = inv(x.T @ x) @ x.T @ y
    residuals = y - x @ beta_hat
    sigma2_hat = np.var(residuals)
    beta_std = np.sqrt(np.diag(inv(x.T @ x)) * sigma2_hat )
    return beta_hat[k,0]/beta_std[k]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;First of all the t-test for $H_0: \beta_0 = 0$:&lt;/p&gt;
&lt;p&gt;$$
t = \frac{\hat \beta_k}{\hat \sigma_{\beta_k}}
$$&lt;/p&gt;
&lt;p&gt;where the standard deviation of the ols coefficient is given by&lt;/p&gt;
&lt;p&gt;$$
\hat \sigma_{\beta_k} = \sqrt{ \hat \sigma^2 \cdot (X&amp;rsquo;X)^{-1}_{[k,k]} }
$$&lt;/p&gt;
&lt;p&gt;where we estimate the variance of the error term with the variance of the residuals&lt;/p&gt;
&lt;p&gt;$$
\hat \sigma^2 = Var \big( y - \hat y \big) = Var \big( y - X (X&amp;rsquo;X)^{-1}X&amp;rsquo;y \big)
$$&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Pre-testing
def pre_testing(a, b, c, n, simulations=1000):
    np.random.seed(1)
    
    # Init
    alpha = {&#39;Long&#39;: np.zeros((simulations,1)),
            &#39;Short&#39;: np.zeros((simulations,1)),
            &#39;Pre-test&#39;: np.zeros((simulations,1))}

    # Loop over simulations
    for i in range(simulations):
        
        # Generate data
        x, y, z = generate_data(a, b, c, n)
        xz = np.concatenate([x,z], axis=1)
        
        # Compute coefficients
        alpha[&#39;Long&#39;][i] = (inv(xz.T @ xz) @ xz.T @ y)[0][0]
        alpha[&#39;Short&#39;][i] = inv(x.T @ x) @ x.T @ y
        
        # Compute significance of z on y
        t = t_test(y, xz, 1)
        
        # Select specification based on test
        if np.abs(t)&amp;gt;1.96:
            alpha[&#39;Pre-test&#39;][i] = alpha[&#39;Long&#39;][i]
        else:
            alpha[&#39;Pre-test&#39;][i] = alpha[&#39;Short&#39;][i]
    
    return alpha
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s compare the different estimates.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Get pre_test alpha
alpha = pre_testing(a, b, c, n)

for key, value in alpha.items():
    print(&#39;Mean alpha %s = %.4f&#39; % (key, np.mean(value)))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Mean alpha Long = 0.9994
Mean alpha Short = 0.9095
Mean alpha Pre-test = 0.9925
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The pre-testing coefficient is very close to the true coefficient.&lt;/p&gt;
&lt;p&gt;However, the main effect of pre-testing is on inference. With pre-testing, the distribution of the estimator is not gaussian anymore.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def plot_alpha(alpha, a):
    
    fig = plt.figure(figsize=(17,6))

    # Plot distributions
    x_max = np.max([np.max(np.abs(x-a)) for x in alpha.values()])

    # All axes
    for i, key in enumerate(alpha.keys()):
        
        # Reshape exisiting subplots
        k = len(fig.axes)
        for i in range(k):
            fig.axes[i].change_geometry(1, k+1, i+1)
            
        # Add new plot
        ax = fig.add_subplot(1, k+1, k+1)
        ax.hist(alpha[key], bins=30)
        ax.set_title(key)
        ax.set_xlim([a-x_max, a+x_max])
        ax.axvline(a, c=&#39;r&#39;, ls=&#39;--&#39;)
        legend_text = [r&#39;$\alpha_0=%.0f$&#39; % a, r&#39;$\hat \alpha=%.4f$&#39; % np.mean(alpha[key])]
        ax.legend(legend_text, prop={&#39;size&#39;: 10})
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s compare the long, short and pre-test estimators.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Plot
plot_alpha(alpha, a)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/09_postdoubleselection_36_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;As we can see, the main problem of pre-testing is inference.&lt;/p&gt;
&lt;p&gt;Because of the testing procedure, the distribution of the estimator is a combination of tho different distributions: the one resulting from the long regression and the one resulting from the short regression. &lt;strong&gt;Pre-testing is not a problem in 3 cases&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;when $\beta_0$ is very large: in this case the test always rejects the null hypothesis $H_0 : \beta_0=0$ and we always run the correct specification, i.e. the long regression&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;when $\beta_0$ is very small: in this case the test has very little power. However, as we saw from the Omitted Variable Bias formula, the bias is small.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;when $\gamma_0$ is very small: also in this case the test has very little power. However, as we saw from the Omitted Variable Bias formula, the bias is small.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Let&amp;rsquo;s compare the pre-test estimates for different values of the true parameter $\beta$.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Case 1: different betas and same sample size
b_sequence = b*np.array([0.1,0.3,1,3])
alpha = {}

# Get sequence
for k, b_ in enumerate(b_sequence):
    label = &#39;beta = %.2f&#39; % b_
    alpha[label] = pre_testing(a, b_, c, n)[&#39;Pre-test&#39;]
    print(&#39;Mean alpha with beta=%.2f: %.4f&#39; % (b_, np.mean(alpha[label])))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Mean alpha with beta=-0.03: 0.9926
Mean alpha with beta=-0.09: 0.9826
Mean alpha with beta=-0.30: 0.9925
Mean alpha with beta=-0.90: 0.9994
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The means are similar, but let&amp;rsquo;s look at the distributions.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Plot
plot_alpha(alpha, a)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/09_postdoubleselection_41_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;When $\beta_0$ is &amp;ldquo;small&amp;rdquo;, the distribution of the pre-testing estimator for $\alpha$ is not normal.&lt;/p&gt;
&lt;p&gt;However, the magnitue of $\beta_0$ is a relative concept. For an infinite sample size, $\beta_0$ is always going to be &amp;ldquo;big enough&amp;rdquo;, in the sense that with an infinite sample size the probability fo false positives in testing $H_0: \beta_0 = 0$ is going to zero. I.e. we always select the correct model specification, the long regression.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s have a look at the distibution of $\hat \alpha_{\text{PRE-TEST}}$ when the sample size increaes.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Case 2: same beta and different sample sizes
n_sequence = [100,300,1000,3000]
alpha = {}

# Get sequence
for k, n_ in enumerate(n_sequence):
    label = &#39;n = %.0f&#39; % n_
    alpha[label] = pre_testing(a, b, c, n_)[&#39;Pre-test&#39;]
    print(&#39;Mean alpha with n=%.0f: %.4f&#39; % (n_, np.mean(alpha[label])))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Mean alpha with n=100: 0.9442
Mean alpha with n=300: 0.9635
Mean alpha with n=1000: 0.9925
Mean alpha with n=3000: 0.9989
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Plot
plot_alpha(alpha, a)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/09_postdoubleselection_44_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;As we can see, for large samples, $\beta_0$ is never &amp;ldquo;small&amp;rdquo;. In the limit, when $n \to \infty$, the probability of false positives while testing $H_0: \beta_0 = 0$ goes to zero.&lt;/p&gt;
&lt;p&gt;We face a dilemma:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;pre-testing is clearlly a problem in finite samples&lt;/li&gt;
&lt;li&gt;all our econometric results are based on the assumption that $n \to \infty$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The problem is solved by assuming that the value of $\beta_0$ depends on the sample size. This might seems like a weird assumption but is just to have an asymptotically meaningful concept of &amp;ldquo;big&amp;rdquo; and &amp;ldquo;small&amp;rdquo;.&lt;/p&gt;
&lt;p&gt;We now look at what happens in the simulations when $\beta_0$ is proportional to $\frac{1}{\sqrt{n}}$.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Case 3: beta proportional to 1/sqrt(n) and different sample sizes
beta =  b * 30 / np.sqrt(n_sequence)

# Get sequence
alpha = {}
for k, n_ in enumerate(n_sequence):
    label = &#39;n = %.0f&#39; % n_
    alpha[label] = pre_testing(a, beta[k], c, n_)[&#39;Pre-test&#39;]
    print(&#39;Mean alpha with n=%.0f: %.4f&#39; % (n_, np.mean(alpha[label])))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Mean alpha with n=100: 0.9703
Mean alpha with n=300: 0.9838
Mean alpha with n=1000: 0.9914
Mean alpha with n=3000: 0.9947
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Plot
plot_alpha(alpha, a)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/09_postdoubleselection_48_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Now the distribution of $\hat \alpha$ does not converge to a normal when the sample size increases.&lt;/p&gt;
&lt;h3 id=&#34;pre-testing-and-machine-learning&#34;&gt;Pre-Testing and Machine Learning&lt;/h3&gt;
&lt;p&gt;How are machine learning and pre-testing related? The best example is Lasso. Suppose you have a dataset with many variables. This means that you have very few degrees of freedom and your OLS estimates are going to be very imprecise. At the extreme, you have more variables than observations so that your OLS coefficient is undefined since you cannot invert the design matrix $X&amp;rsquo;X$.&lt;/p&gt;
&lt;p&gt;In this case, you might want to do variable selection. One way of doing variable selection is pre-testing. Another way is Lasso. A third alternative is to use machine learning methods that do not suffer this curse of dimensionality.&lt;/p&gt;
&lt;p&gt;The purpose and outcome of pre-testing and Lasso are the same:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;you have too many variables&lt;/li&gt;
&lt;li&gt;you exclude some of them from the regression / set their coefficients to zero&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;As a consequence, also the problems are the same, i.e. pre-test bias.&lt;/p&gt;
&lt;h2 id=&#34;94-post-double-selection&#34;&gt;9.4 Post-Double Selection&lt;/h2&gt;
&lt;p&gt;Consider again data $D= (y_i, x_i, z_i)_{i=1}^n$, where the true model is:&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
&amp;amp; y_i = x_i&amp;rsquo; \alpha_0  + z_i&amp;rsquo; \beta_0 + \varepsilon_i \
&amp;amp; x_i = z_i&amp;rsquo; \gamma_0 + u_i
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;We would like to guard against pretest bias if possible, in order to handle high dimensional models. A good pathway towards motivating procedures which guard against pretest bias is a discussion of classical partitioned regression.&lt;/p&gt;
&lt;p&gt;Consider a regression $y_i$ on $x_i$ and $z_i$. $x_i$ is the 1-dimensional variable of interest, $z_i$ is a high-dimensional set of control variables. We have the following procedure:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;First Stage&lt;/strong&gt; selection: regress $x_i$ on $z_i$. Select the statistically significant variables in the set $S_{FS} \subseteq z_i$&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Reduced Form&lt;/strong&gt; selection: lasso $y_i$ on $z_i$. Select the statistically significant variables in the set $S_{RF} \subseteq z_i$&lt;/li&gt;
&lt;li&gt;Regress $y_i$ on $x_i$ and $S_{FS} \cup S_{RF}$&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt;:
Let ${P^n}$ be a sequence of data-generating processes for $D_n = (y_i, x_i, z_i)^n_{i=1} \in (\mathbb R \times \mathbb R \times \mathbb R^p) ^n$ where $p$ depends on $n$. For each $n$, the data are iid with $yi =  x_i&amp;rsquo;\alpha_0^{(n)} + z_i&amp;rsquo; \beta_0^{(n)} + \varepsilon_i$ and $x_i = z_i&amp;rsquo; \gamma_0^{(n)} + u_i$ where $\mathbb E[\varepsilon_i | x_i,z_i] = 0$ and $\mathbb E[u_i|z_i] = 0$. The sparsity of the vectors  $\beta_0^{(n)}$, $\gamma_0^{(n)}$ is controlled by $|| \beta_0^{(n)} ||_0 \leq s$ with $s^2 (\log p)^2/n \to 0$. Suppose that additional regularity conditions on the model selection procedures and moments of the random variables $y_i$ , $x_i$ , $z_i$ as documented in Belloni et al. (2014). Then the confidence intervals, CI, from the post double selection procedure are uniformly valid. That is, for any confidence level $\xi \in (0, 1)$
$$
\Pr(\alpha_0 \in CI) \to 1- \xi
$$&lt;/p&gt;
&lt;p&gt;In order to have valid confidence intervals you want their bias to be negligibly. Since
$$
CI = \left[ \hat{\alpha} \pm \frac{1.96 \cdot \hat{\sigma}}{\sqrt{n}} \right]
$$&lt;/p&gt;
&lt;p&gt;If the bias is $o \left( \frac{1}{\sqrt{n}} \right)$ then there is no problem since it is asymptotically negligible w.r.t. the magnitude of the confidence interval. If however the the bias is $O \left( \frac{1}{\sqrt{n}} \right)$ then it has the same magnitude of the confidence interval and it does not asymptotically vanish.&lt;/p&gt;
&lt;p&gt;The idea of the proof is to use partitioned regression. An alternative way to think about the argument is: bound the omitted variables bias. Omitted variable bias comes from the product of 2 quantities related to the omitted variable:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Its partial correlation with the outcome, and&lt;/li&gt;
&lt;li&gt;Its partial correlation with the variable of interest.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;If both those partial correlations are $O( \sqrt{\log p/n})$, then the omitted variables bias is $(s \times O( \sqrt{\log p/n})^2 = o \left( \frac{1}{\sqrt{n}} \right)$, provided $s^2 (\log p)^2/n \to 0$. Relative to the $ \frac{1}{\sqrt{n}} $ convergence rate, the omitted variables bias is negligible.&lt;/p&gt;
&lt;p&gt;In our omitted variable bias case, we want $| \beta_0 \gamma_0 | = o \left( \frac{1}{\sqrt{n}} \right)$.  Post-double selection guarantees that&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Reduced form&lt;/em&gt; selection (pre-testing): any &amp;ldquo;missing&amp;rdquo; variable has $|\beta_{0j}| \leq \frac{c}{\sqrt{n}}$&lt;/li&gt;
&lt;li&gt;&lt;em&gt;First stage&lt;/em&gt; selection (additional): any &amp;ldquo;missing&amp;rdquo; variable has $|\gamma_{0j}| \leq \frac{c}{\sqrt{n}}$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;As a consequence, as long as the number of omitted variables is finite, the omitted variable bias is
$$
OVB(\alpha) = |\beta_{0j}| \cdot|\gamma_{0j}| \leq \frac{c}{\sqrt{n}} \cdot \frac{c}{\sqrt{n}} = \frac{c^2}{n} = o \left(\frac{1}{\sqrt{n}}\right)
$$&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Pre-testing code
def post_double_selection(a, b, c, n, simulations=1000):
    np.random.seed(1)
    
    # Init
    alpha = {&#39;Long&#39;: np.zeros((simulations,1)),
            &#39;Short&#39;: np.zeros((simulations,1)),
            &#39;Pre-test&#39;: np.zeros((simulations,1)),
            &#39;Post-double&#39;: np.zeros((simulations,1))}

    # Loop over simulations
    for i in range(simulations):
        
        # Generate data
        x, y, z = generate_data(a, b, c, n)
        
        # Compute coefficients
        xz = np.concatenate([x,z], axis=1)
        alpha[&#39;Long&#39;][i] = (inv(xz.T @ xz) @ xz.T @ y)[0][0]
        alpha[&#39;Short&#39;][i] = inv(x.T @ x) @ x.T @ y
        
        # Compute significance of z on y (beta hat)
        t1 = t_test(y, xz, 1)
        
        # Compute significance of z on x (gamma hat)
        t2 = t_test(x, z, 0)
        
        # Select specification based on first test
        if np.abs(t1)&amp;gt;1.96:
            alpha[&#39;Pre-test&#39;][i] = alpha[&#39;Long&#39;][i]
        else:
            alpha[&#39;Pre-test&#39;][i] = alpha[&#39;Short&#39;][i]
            
        # Select specification based on both tests
        if np.abs(t1)&amp;gt;1.96 or np.abs(t2)&amp;gt;1.96:
            alpha[&#39;Post-double&#39;][i] = alpha[&#39;Long&#39;][i]
        else:
            alpha[&#39;Post-double&#39;][i] = alpha[&#39;Short&#39;][i]
    
    return alpha
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s now repeat the same exercise as above, but with also post-double selection&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Get pre_test alpha
alpha = post_double_selection(a, b, c, n)

for key, value in alpha.items():
    print(&#39;Mean alpha %s = %.4f&#39; % (key, np.mean(value)))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Mean alpha Long = 0.9994
Mean alpha Short = 0.9095
Mean alpha Pre-test = 0.9925
Mean alpha Post-double = 0.9994
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Plot
plot_alpha(alpha, a)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/09_postdoubleselection_62_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;As we can see, post-double selection has solved the pre-testing problem. Does it work for any magnitude of $\beta$ (relative to the sample size)?&lt;/p&gt;
&lt;p&gt;We first have a look at the case in which the sample size is fixed and $\beta_0$ changes.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Case 1: different betas and same sample size
b_sequence = b*np.array([0.1,0.3,1,3])
alpha = {}

# Get sequence
for k, b_ in enumerate(b_sequence):
    label = &#39;beta = %.2f&#39; % b_
    alpha[label] = post_double_selection(a, b_, c, n)[&#39;Post-double&#39;]
    print(&#39;Mean alpha with beta=%.2f: %.4f&#39; % (b_, np.mean(alpha[label])))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Mean alpha with beta=-0.03: 0.9994
Mean alpha with beta=-0.09: 0.9994
Mean alpha with beta=-0.30: 0.9994
Mean alpha with beta=-0.90: 0.9994
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Plot
plot_alpha(alpha, a)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/09_postdoubleselection_65_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Post-double selection always selects the correct specification, the long regression, even when $\beta$ is very small.&lt;/p&gt;
&lt;p&gt;Now we check the same but for fixed $\beta_0$ and different sample sizes.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Case 2: same beta and different sample sizes
n_sequence = [100,300,1000,3000]
alpha = {}

# Get sequence
for k, n_ in enumerate(n_sequence):
    label = &#39;N = %.0f&#39; % n_
    alpha[label] = post_double_selection(a, b, c, n_)[&#39;Post-double&#39;]
    print(&#39;Mean alpha with n=%.0f: %.4f&#39; % (n_, np.mean(alpha[label])))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Mean alpha with n=100: 0.9964
Mean alpha with n=300: 0.9985
Mean alpha with n=1000: 0.9994
Mean alpha with n=3000: 0.9990
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Plot
plot_alpha(alpha, a)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/09_postdoubleselection_69_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Post-double selection always selects the correct specification, the long regression, even when the sample size is very small.&lt;/p&gt;
&lt;p&gt;Last, we check the case of $\beta_0$ proportional to $\frac{1}{\sqrt{n}}$.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Case 3: beta proportional to 1/sqrt(n) and different sample sizes
beta =  b * 30 / np.sqrt(n_sequence)

# Get sequence
alpha = {}
for k, n_ in enumerate(n_sequence):
    label = &#39;N = %.0f&#39; % n_
    alpha[label] = post_double_selection(a, beta[k], c, n_)[&#39;Post-double&#39;]
    print(&#39;Mean alpha with n=%.0f: %.4f&#39; % (n_, np.mean(alpha[label])))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Mean alpha with n=100: 0.9964
Mean alpha with n=300: 0.9985
Mean alpha with n=1000: 0.9994
Mean alpha with n=3000: 0.9990
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Plot
plot_alpha(alpha, a)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/09_postdoubleselection_73_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Once again post-double selection always selects the correct specification, the long regression.&lt;/p&gt;
&lt;h3 id=&#34;post-double-selection-and-machine-learning&#34;&gt;Post-double Selection and Machine Learning&lt;/h3&gt;
&lt;p&gt;As we have seen at the end of the previous section, Lasso can be used to perform variable selection in high dimensional settings. Therefore, post-double selection solves the pre-test bias problem in those settings. The post-double selection procedure with Lasso is:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;First Stage&lt;/strong&gt; selection: lasso $x_i$ on $z_i$. Let the selected variables be collected in the set $S_{FS} \subseteq z_i$&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Reduced Form&lt;/strong&gt; selection: lasso $y_i$ on $z_i$. Let the selected variables be collected in the set $S_{RF} \subseteq z_i$&lt;/li&gt;
&lt;li&gt;Regress $y_i$ on $x_i$ and $S_{FS} \cup S_{RF}$&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;95-doubledebiased-machine-learning&#34;&gt;9.5 Double/debiased Machine Learning&lt;/h2&gt;
&lt;p&gt;This section is taken from &lt;a href=&#34;https://onlinelibrary.wiley.com/doi/abs/10.1111/ectj.12097&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Chernozhukov, V., Chetverikov, D., Demirer, M., Duflo, E., Hansen, C., Newey, W., &amp;amp; Robins, J. (2018). &amp;ldquo;&lt;em&gt;Double/debiased machine learning for treatment and structural parameters&lt;/em&gt;&amp;rdquo;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Consider the following partially linear model&lt;/p&gt;
&lt;p&gt;$$
y = \beta_0 D + g_0(X) + u \
D = m_0(X) + v
$$&lt;/p&gt;
&lt;p&gt;where $y$ is the outcome variable, $D$ is the treatment to interest and $X$ is a potentially high-dimensional set of controls.&lt;/p&gt;
&lt;h4 id=&#34;naive-approach&#34;&gt;Naive approach&lt;/h4&gt;
&lt;p&gt;A naive approach to estimation of $\beta_0$ using ML methods would be, for example, to construct a sophisticated ML estimator $\beta_0 D + g_0(X)$ for learning the regression function $\beta_0 D$ + $g_0(X)$.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Split the sample in two: main sample and auxiliary sample&lt;/li&gt;
&lt;li&gt;Use the auxiliary sample to estimate $\hat g_0(X)$&lt;/li&gt;
&lt;li&gt;Use the main sample to compute the orthogonalized component of $Y$ on $X$: $\hat u = \left(Y_{i}-\hat{g}&lt;em&gt;{0}\left(X&lt;/em&gt;{i}\right)\right)$&lt;/li&gt;
&lt;li&gt;Use the main sample to estimate the residualized OLS estimator&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;$$
\hat{\beta}&lt;em&gt;{0}=\left(\frac{1}{n} \sum&lt;/em&gt;{i \in I} D_{i}^{2}\right)^{-1} \frac{1}{n} \sum_{i \in I} D_{i} \hat u_i
$$&lt;/p&gt;
&lt;p&gt;This estimator is going to have two problems:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Slow rate of convergence, i.e. slower than $\sqrt(n)$&lt;/li&gt;
&lt;li&gt;It will be biased because we are employing highdimensional regularized estimators (e.g. we are doing variable selection)&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;orthogonalization&#34;&gt;Orthogonalization&lt;/h4&gt;
&lt;p&gt;Now consider a second construction that employs an orthogonalized formulation obtained by directly partialling out the effect of $X$ from $D$ to obtain the orthogonalized regressor $v = D − m_0(X)$.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Split the sample in two: main sample and auxiliary sample&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Use the auxiliary sample to estimate $\hat g_0(X)$ from&lt;/p&gt;
&lt;p&gt;$$
y = \beta_0 D + g_0(X) + u \
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Use the auxiliary sample to estimate $\hat m_0(X)$ from&lt;/p&gt;
&lt;p&gt;$$
D = m_0(X) + v
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Use the main sample to compute the orthogonalized component of $D$ on $X$ as&lt;/p&gt;
&lt;p&gt;$$
\hat v = D - \hat m_0(X)
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Use the main sample to estimate the double-residualized OLS estimator as&lt;/p&gt;
&lt;p&gt;$$
\hat{\beta}&lt;em&gt;{0}=\left(\frac{1}{n} \sum&lt;/em&gt;{i \in I} \hat v_i D_{i} \right)^{-1} \frac{1}{n} \sum_{i \in I} \hat v_i \left( Y - \hat g_0(X) \right)
$$&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The estimator is unbiased but still has a lower rate of convergence because of sample splitting. The problem is solved by inverting the split sample, re-estimating the coefficient and averaging the two estimates. Note that this procedure is valid since the two estimates are independent by the sample splitting procedure.&lt;/p&gt;
&lt;h3 id=&#34;application-to-ajr02&#34;&gt;Application to AJR02&lt;/h3&gt;
&lt;p&gt;In this section we are going to replicate 6.3 of the &amp;ldquo;&lt;em&gt;Double/debiased machine learning&lt;/em&gt;&amp;rdquo; paper based on &lt;a href=&#34;https://economics.mit.edu/files/4123&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Acemoglu, Johnson, Robinson (2002), &amp;ldquo;&lt;em&gt;The Colonial Origins of Comparative Development&lt;/em&gt;&amp;rdquo;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;We first load the dataset&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Load Acemoglu Johnson Robinson Dataset
df = pd.read_csv(&#39;data/AJR02.csv&#39;,index_col=0)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;GDP&lt;/th&gt;
      &lt;th&gt;Exprop&lt;/th&gt;
      &lt;th&gt;Mort&lt;/th&gt;
      &lt;th&gt;Latitude&lt;/th&gt;
      &lt;th&gt;Neo&lt;/th&gt;
      &lt;th&gt;Africa&lt;/th&gt;
      &lt;th&gt;Asia&lt;/th&gt;
      &lt;th&gt;Namer&lt;/th&gt;
      &lt;th&gt;Samer&lt;/th&gt;
      &lt;th&gt;logMort&lt;/th&gt;
      &lt;th&gt;Latitude2&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;8.39&lt;/td&gt;
      &lt;td&gt;6.50&lt;/td&gt;
      &lt;td&gt;78.20&lt;/td&gt;
      &lt;td&gt;0.3111&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;4.359270&lt;/td&gt;
      &lt;td&gt;0.096783&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;7.77&lt;/td&gt;
      &lt;td&gt;5.36&lt;/td&gt;
      &lt;td&gt;280.00&lt;/td&gt;
      &lt;td&gt;0.1367&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;5.634790&lt;/td&gt;
      &lt;td&gt;0.018687&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;9.13&lt;/td&gt;
      &lt;td&gt;6.39&lt;/td&gt;
      &lt;td&gt;68.90&lt;/td&gt;
      &lt;td&gt;0.3778&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;4.232656&lt;/td&gt;
      &lt;td&gt;0.142733&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;9.90&lt;/td&gt;
      &lt;td&gt;9.32&lt;/td&gt;
      &lt;td&gt;8.55&lt;/td&gt;
      &lt;td&gt;0.3000&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;2.145931&lt;/td&gt;
      &lt;td&gt;0.090000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;5&lt;/th&gt;
      &lt;td&gt;9.29&lt;/td&gt;
      &lt;td&gt;7.50&lt;/td&gt;
      &lt;td&gt;85.00&lt;/td&gt;
      &lt;td&gt;0.2683&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;4.442651&lt;/td&gt;
      &lt;td&gt;0.071985&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df.info()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;class &#39;pandas.core.frame.DataFrame&#39;&amp;gt;
Int64Index: 64 entries, 1 to 64
Data columns (total 11 columns):
 #   Column     Non-Null Count  Dtype  
---  ------     --------------  -----  
 0   GDP        64 non-null     float64
 1   Exprop     64 non-null     float64
 2   Mort       64 non-null     float64
 3   Latitude   64 non-null     float64
 4   Neo        64 non-null     int64  
 5   Africa     64 non-null     int64  
 6   Asia       64 non-null     int64  
 7   Namer      64 non-null     int64  
 8   Samer      64 non-null     int64  
 9   logMort    64 non-null     float64
 10  Latitude2  64 non-null     float64
dtypes: float64(6), int64(5)
memory usage: 6.0 KB
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In their paper, AJR note that their IV strategy will be invalidated if other factors are also highly persistent and related to the development of institutions within a country and to the country’s GDP. A leading candidate for such a factor, as they discuss, is geography. AJR address this by assuming that the confounding effect of geography is adequately captured by a linear term in distance from the equator and a set of continent dummy variables.&lt;/p&gt;
&lt;p&gt;They inclue their results in table 2.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Add constant term to dataset
df[&#39;const&#39;] = 1

# Create lists of variables to be used in each regression
X1 = df[[&#39;const&#39;, &#39;Exprop&#39;]]
X2 = df[[&#39;const&#39;, &#39;Exprop&#39;, &#39;Latitude&#39;, &#39;Latitude2&#39;]]
X3 = df[[&#39;const&#39;, &#39;Exprop&#39;, &#39;Latitude&#39;, &#39;Latitude2&#39;, &#39;Asia&#39;, &#39;Africa&#39;, &#39;Namer&#39;, &#39;Samer&#39;]]
y = df[&#39;GDP&#39;]

# Estimate an OLS regression for each set of variables
reg1 = sm.OLS(y, X1, missing=&#39;drop&#39;).fit()
reg2 = sm.OLS(y, X2, missing=&#39;drop&#39;).fit()
reg3 = sm.OLS(y, X3, missing=&#39;drop&#39;).fit()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s replicate Table 2 in AJR.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Make table 2
def make_table_2():

    info_dict={&#39;No. observations&#39; : lambda x: f&amp;quot;{int(x.nobs):d}&amp;quot;}

    results_table = summary_col(results=[reg1,reg2,reg3],
                                float_format=&#39;%0.2f&#39;,
                                stars = True,
                                model_names=[&#39;Model 1&#39;,&#39;Model 2&#39;,&#39;Model 3&#39;],
                                info_dict=info_dict,
                                regressor_order=[&#39;const&#39;,&#39;Exprop&#39;,&#39;Latitude&#39;,&#39;Latitude2&#39;])
    return results_table
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;table_2 = make_table_2()
table_2
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
          &lt;td&gt;&lt;/td&gt;         &lt;th&gt;Model 1&lt;/th&gt; &lt;th&gt;Model 2&lt;/th&gt; &lt;th&gt;Model 3&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;const&lt;/th&gt;            &lt;td&gt;4.66***&lt;/td&gt; &lt;td&gt;4.55***&lt;/td&gt; &lt;td&gt;5.95***&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;&lt;/th&gt;                 &lt;td&gt;(0.41)&lt;/td&gt;  &lt;td&gt;(0.45)&lt;/td&gt;  &lt;td&gt;(0.68)&lt;/td&gt; 
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Exprop&lt;/th&gt;           &lt;td&gt;0.52***&lt;/td&gt; &lt;td&gt;0.49***&lt;/td&gt; &lt;td&gt;0.40***&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;&lt;/th&gt;                 &lt;td&gt;(0.06)&lt;/td&gt;  &lt;td&gt;(0.07)&lt;/td&gt;  &lt;td&gt;(0.06)&lt;/td&gt; 
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Latitude&lt;/th&gt;            &lt;td&gt;&lt;/td&gt;      &lt;td&gt;2.16&lt;/td&gt;    &lt;td&gt;0.42&lt;/td&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;&lt;/th&gt;                    &lt;td&gt;&lt;/td&gt;     &lt;td&gt;(1.68)&lt;/td&gt;  &lt;td&gt;(1.47)&lt;/td&gt; 
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Latitude2&lt;/th&gt;           &lt;td&gt;&lt;/td&gt;      &lt;td&gt;-2.12&lt;/td&gt;   &lt;td&gt;0.44&lt;/td&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;&lt;/th&gt;                    &lt;td&gt;&lt;/td&gt;     &lt;td&gt;(2.86)&lt;/td&gt;  &lt;td&gt;(2.48)&lt;/td&gt; 
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Africa&lt;/th&gt;              &lt;td&gt;&lt;/td&gt;        &lt;td&gt;&lt;/td&gt;     &lt;td&gt;-1.06**&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;&lt;/th&gt;                    &lt;td&gt;&lt;/td&gt;        &lt;td&gt;&lt;/td&gt;     &lt;td&gt;(0.41)&lt;/td&gt; 
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Asia&lt;/th&gt;                &lt;td&gt;&lt;/td&gt;        &lt;td&gt;&lt;/td&gt;     &lt;td&gt;-0.74*&lt;/td&gt; 
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;&lt;/th&gt;                    &lt;td&gt;&lt;/td&gt;        &lt;td&gt;&lt;/td&gt;     &lt;td&gt;(0.42)&lt;/td&gt; 
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Namer&lt;/th&gt;               &lt;td&gt;&lt;/td&gt;        &lt;td&gt;&lt;/td&gt;      &lt;td&gt;-0.17&lt;/td&gt; 
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;&lt;/th&gt;                    &lt;td&gt;&lt;/td&gt;        &lt;td&gt;&lt;/td&gt;     &lt;td&gt;(0.40)&lt;/td&gt; 
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Samer&lt;/th&gt;               &lt;td&gt;&lt;/td&gt;        &lt;td&gt;&lt;/td&gt;      &lt;td&gt;-0.12&lt;/td&gt; 
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;&lt;/th&gt;                    &lt;td&gt;&lt;/td&gt;        &lt;td&gt;&lt;/td&gt;     &lt;td&gt;(0.42)&lt;/td&gt; 
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;No. observations&lt;/th&gt;   &lt;td&gt;64&lt;/td&gt;      &lt;td&gt;64&lt;/td&gt;      &lt;td&gt;64&lt;/td&gt;   
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;Using DML allows us to relax this assumption and to replace it by a weaker assumption that geography can be sufficiently controlled by an unknown function of distance from the equator and continent dummies, which can be learned by ML methods.&lt;/p&gt;
&lt;p&gt;In particular, our framework is&lt;/p&gt;
&lt;p&gt;$$
{GDP} = \beta_0 \times {Exprop} + g_0({geography}) + u \
{Exprop} = m_0({geography}) + u
$$&lt;/p&gt;
&lt;p&gt;So that the double/debiased machine learning procedure is&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Split the sample in two: main sample and auxiliary sample&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Use the auxiliary sample to estimate $\hat g_0({geography})$ from&lt;/p&gt;
&lt;p&gt;$$
{GDP} = \beta_0 \times {Exprop} + g_0({geography}) + u
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Use the auxiliary sample to estimate $\hat m_0({geography})$ from&lt;/p&gt;
&lt;p&gt;$$
{Exprop} = m_0({geography}) + v
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Use the main sample to compute the orthogonalized component of ${Exprop}$ on ${geography}$ as&lt;/p&gt;
&lt;p&gt;$$
\hat v = {Exprop} - \hat m_0({geography})
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Use the main sample to estimate the double-residualized OLS estimator as&lt;/p&gt;
&lt;p&gt;$$
\hat{\beta}&lt;em&gt;{0}=\left(\frac{1}{n} \sum&lt;/em&gt;{i \in I} \hat v_i \times {Exprop}&lt;em&gt;{i} \right)^{-1} \frac{1}{n} \sum&lt;/em&gt;{i \in I} \hat v_i \times \left( {GDP} - \hat g_0({geography}) \right)
$$&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Since we employ an &lt;strong&gt;intrumental variable&lt;/strong&gt; strategy, we replace $m_0({geography})$ with $m_0({geography},{logMort})$ in the first stage.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Generate variables
D = df[&#39;Exprop&#39;].values.reshape(-1,1)
X = df[[&#39;const&#39;, &#39;Latitude&#39;, &#39;Latitude2&#39;, &#39;Asia&#39;, &#39;Africa&#39;, &#39;Namer&#39;, &#39;Samer&#39;]].values
y = df[&#39;GDP&#39;].values.reshape(-1,1)
Z = df[[&#39;const&#39;, &#39;Latitude&#39;, &#39;Latitude2&#39;, &#39;Asia&#39;, &#39;Africa&#39;, &#39;Namer&#39;, &#39;Samer&#39;,&#39;logMort&#39;]].values
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def estimate_beta(algorithm, alg_name, D, X, y, Z, sample):

    # Split sample
    D_main, D_aux = (D[sample==1], D[sample==0])
    X_main, X_aux = (X[sample==1], X[sample==0])
    y_main, y_aux = (y[sample==1], y[sample==0])
    Z_main, Z_aux = (Z[sample==1], Z[sample==0])

    # Residualize y on D
    b_hat = inv(D_aux.T @ D_aux) @ D_aux.T @ y_aux
    y_resid_aux = y_aux - D_aux @ b_hat
    
    # Estimate g0
    alg_fitted = algorithm.fit(X=X_aux, y=y_resid_aux.ravel())
    g0 = alg_fitted.predict(X_main).reshape(-1,1)

    # Compute v_hat
    u_hat = y_main - g0

    # Estimate m0
    alg_fitted = algorithm.fit(X=Z_aux, y=D_aux.ravel())
    m0 = algorithm.predict(Z_main).reshape(-1,1)
    
    # Compute u_hat
    v_hat = D_main - m0

    # Estimate beta
    beta = inv(v_hat.T @ D_main) @ v_hat.T @ u_hat
        
    return beta 
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def ddml(algorithm, alg_name, D, X, y, Z, p=0.5, verbose=False):
    
    # Expand X if Lasso or Ridge
    if alg_name in [&#39;Lasso   &#39;,&#39;Ridge   &#39;]:
        X = PolynomialFeatures(degree=2).fit_transform(X)

    # Generate split (fixed proportions)
    split = np.array([i in train_test_split(range(len(D)), test_size=p)[0] for i in range(len(D))])
    
    # Compute beta
    beta = [estimate_beta(algorithm, alg_name, D, X, y, Z, split==k) for k in range(2)]
    beta = np.mean(beta)
     
    # Print and return
    if verbose:
        print(&#39;%s : %.4f&#39; % (alg_name, beta))
    return beta
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Generate sample split
p = 0.5
split = np.random.binomial(1, p, len(D))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We inspect different algorithms. In particular, we consider:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Lasso Regression&lt;/li&gt;
&lt;li&gt;Ridge Regression&lt;/li&gt;
&lt;li&gt;Regression Trees&lt;/li&gt;
&lt;li&gt;Random Forest&lt;/li&gt;
&lt;li&gt;Boosted Forests&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# List all algorithms
algorithms = {&#39;Ridge   &#39;: Ridge(alpha=.1),
              &#39;Lasso   &#39;: Lasso(alpha=.01),
              &#39;Tree    &#39;: DecisionTreeRegressor(),
              &#39;Forest  &#39;: RandomForestRegressor(n_estimators=30),
              &#39;Boosting&#39;: GradientBoostingRegressor(n_estimators=30)}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s compare the results.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Loop over algorithms
for alg_name, algorithm in algorithms.items():
    ddml(algorithm, alg_name, D, X, y, Z, verbose=True)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Ridge    : 0.1289
Lasso    : -8.7963
Tree     : 1.2879
Forest   : 2.4938
Boosting : 0.5977
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The results are extremely volatile.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Repeat K times
def estimate_beta_median(algorithms, D, X, y, Z, K):
    
    # Loop over algorithms
    for alg_name, algorithm in algorithms.items():
        betas = []
            
        # Iterate n times
        for k in range(K):
            beta = ddml(algorithm, alg_name, D, X, y, Z)
            betas = np.append(betas, beta)
    
        print(&#39;%s : %.4f&#39; % (alg_name, np.median(betas)))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s try using the median to have a more stable estimator.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;np.random.seed(123)

# Repeat 100 times and take median
estimate_beta_median(algorithms, D, X, y, Z, 100)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Ridge    : 0.6670
Lasso    : 1.2511
Tree     : 0.9605
Forest   : 0.5327
Boosting : 1.0327
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The results differ slightly from the ones in the paper, but they are at least closer.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Variable Selection</title>
      <link>https://matteocourthoud.github.io/course/metrics/09_selection/</link>
      <pubDate>Fri, 29 Oct 2021 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/course/metrics/09_selection/</guid>
      <description>&lt;h2 id=&#34;lasso&#34;&gt;Lasso&lt;/h2&gt;
&lt;h3 id=&#34;issue&#34;&gt;Issue&lt;/h3&gt;
&lt;p&gt;Lasso (Least Absolute Shrinkage and Selection Operator) is a popular
method for high dimensional regression. It does variable selection and
estimation simultaneously. It is a non-parametric (series) estimation
technique part of a general class of estimators called &lt;em&gt;penalized
estimators&lt;/em&gt;. It allows the number of regressors, $p$, to be larger than
the sample size, $n$.&lt;/p&gt;
&lt;p&gt;Consider data $D = \lbrace x_i, y_i \rbrace_{i=1}^n$ with
$\dim (x_i) = p$. Assume that $p$ is large relative to $n$. Two possible
reasons:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;we have an intrinsic problem of high dimensionality&lt;/li&gt;
&lt;li&gt;$p$ indicates the number of expansion terms of small number of
underlying important variables (e.g. series estimation)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Assumption&lt;/strong&gt;: $y_i = x_i&amp;rsquo; \beta_0 + r_i + \varepsilon_i$ where
$\beta_0$ depends on $p$, $r_i$ is a remainder term.&lt;/p&gt;
&lt;p&gt;Note that in classic non-parametrics, we have $x_i&amp;rsquo;\beta_0$ as
$p_1(x_i) \beta_{1,K} + \dots + p_K(x_i) \beta_{K,K}$. For simplicity,
we assume $r_i = 0$, as if we had extreme undersmoothing. Hence the
model becomes: $$
y_i = x_i&amp;rsquo; \beta_0 + \varepsilon_i, \qquad p \geq n
$$ We cannot run OLS because $p \geq n$, thus the rank condition is
violated.&lt;/p&gt;
&lt;h3 id=&#34;definition&#34;&gt;Definition&lt;/h3&gt;
&lt;p&gt;We define the &lt;strong&gt;Lasso estimator&lt;/strong&gt; as $$
\hat{\beta}_L = \arg \min \quad \underbrace{\mathbb E_n \Big[ (y_i - x_i&amp;rsquo; \beta)^2 \Big]} _ {\text{SSR term}} + \underbrace{\frac{\lambda}{n} \sum _ {j=1}^{P} | \beta_j |} _ {\text{Penalty term}}
$$ where $\lambda$ is called &lt;strong&gt;penalty parameter&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;penalty term&lt;/strong&gt; discourages large values of $| \beta_j |$. The
choice of $\lambda$ is analogous to the choice of $K$ in series
estimation and $h$ in kernel estimation.&lt;/p&gt;
&lt;h3 id=&#34;penalties&#34;&gt;Penalties&lt;/h3&gt;
&lt;p&gt;The shrinkage to zero of the coefficients directly follows from the
$|| \cdot ||_1$ norm. On the contrary, another famous penalized
estimator, &lt;em&gt;ridge regression&lt;/em&gt;, uses the $|| \cdot ||_2$ norm and does
not have this property.&lt;/p&gt;
&lt;img src=&#34;../img/Fig_551.png&#34; style=&#34;width:50.0%&#34; /&gt;
&lt;blockquote&gt;
&lt;p&gt;Minimizing SSR + penalty is equivalent to minimize SSR $s.t.$ pen
$\leq c$ (clear from the picture).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;sparsity&#34;&gt;Sparsity&lt;/h3&gt;
&lt;p&gt;Let $S_0 = \lbrace j: \beta_{0,j} \ne 0 \rbrace$, we define
$s_0 = |S_0|$ as the &lt;strong&gt;sparsity&lt;/strong&gt; of $\beta_0$. If $s_0/n \to 0$, we are
dealing with a &lt;strong&gt;sparse regression&lt;/strong&gt; (analogous of smooth regression).&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Remark on sparsity:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;In words, sparsity means that even if we have a lot of variables,
only a small number of them (relative to $n$) have an effect on
the dependent variable.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Approximate sparsity imposes a restriction that only $s_0$
variables among all of $x_{ij}$, where $s_0$ is much smaller than
$n$, have associated coefficients $\beta_{0j}$ that are different
from zero, while permitting a nonzero approximation error. Thus,
estimators for this kind of model attempt to learn the identities
of the variables with large nonzero coefficients, while
simultaneously estimating these coefficients.&lt;/em&gt; (Belloni et al.,
2004)&lt;/li&gt;
&lt;li&gt;Sparsity is an assumption. $\beta_0$ is said to be $s_0$-sparse
with $s_0 &amp;lt; n$ if $$
| \lbrace j: \beta_{0j} \neq 0 \rbrace | \leq s_0
$$&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;lasso-theorem&#34;&gt;Lasso Theorem&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Suppose that for data $D_n = (y_i, x_i)&lt;em&gt;{i=1}^N$ with
$y_i = x_i&amp;rsquo; \beta + \varepsilon_i$. Let $\hat{\beta}&lt;em&gt;L$ be the Lasso
estimator. Let
$\mathcal{S} = 2 \max_j | \mathbb E[ x&lt;/em&gt;{ij} \varepsilon_i] |$. Suppose
$|support(\beta_0) \leq s_0$ (sparsity assumption). Let
$c_0 = (\mathcal{S} + \lambda/n )/(-\mathcal{S} + \lambda/n )$. Let $$
\kappa&lt;/em&gt;{c_0, s_0} = \min_{  d \in \mathbb R^p, A \subseteq \lbrace 1, &amp;hellip; , p \rbrace : |A| \leq s_0 ,  || d_{A^c}|| \leq c_0 || d_A ||_1  }  \sqrt{  \frac{ s_0 d&amp;rsquo; \mathbb E_n [x_i x_i&amp;rsquo;] d }{|| d_A ||_1^2}  }
$$ Then&lt;/p&gt;
&lt;p&gt;$$
\mathbb I_{ \left\lbrace \frac{\lambda}{n} &amp;gt; \mathcal{S}  \right\rbrace} \mathbb E_n [(x_i \beta_0 - x_i \beta_L)^2]^{\frac{1}{2}}  \leq 2 \frac{\lambda}{n} \frac{\sqrt{s_0}}{\kappa_{c_0, s_0}}
$$&lt;/p&gt;
&lt;p&gt;Intuition: for a sufficiently high lambda the root mean squared error of
Lasso is approximately zero.&lt;/p&gt;
&lt;p&gt;$$
\text{ RMSE }:  \mathbb E_n [(x_i \beta_0 - x_i \beta_L)^2]^{\frac{1}{2}}  \simeq 0  \quad \Leftrightarrow \quad \frac{\lambda}{n} &amp;gt; \mathcal{S}
$$&lt;/p&gt;
&lt;h3 id=&#34;remarks&#34;&gt;Remarks&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;The minimization region is the set of “essentially sparse” vectors
$d \in \mathbb R^p$, where “essentially sparse” is defined by
$\mathcal{C}, \mathcal{S}$. In particular the condition
$k_{\mathcal{C}, \mathcal{S}}&amp;gt;0$ means that no essentially sparse
vector $d$ has $\mathbb E[x_i x_i&amp;rsquo;]d = 0$, i.e. regressors were not
added multiple times.&lt;/li&gt;
&lt;li&gt;Need to dominate the score with the penalty term $\lambda$.&lt;/li&gt;
&lt;li&gt;Need no collinearity on a small ($\leq s_0$) subset of regressors
($\to k_{c_0, s_0}&amp;gt;0$).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;When Lasso?&lt;/strong&gt; For prediction problems in high dimensional
environments. &lt;strong&gt;NB!&lt;/strong&gt; Lasso is not good for inference, only for
prediction.&lt;/p&gt;
&lt;p&gt;In particular, in econometrics it’s used for selecting either&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;instruments (predicting $\hat{x}$ in the first stage)&lt;/li&gt;
&lt;li&gt;control variables (next section: double prediction problem, in the
first stage and in the reduced form)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;choosing-the-optimal-lambda&#34;&gt;Choosing the Optimal Lambda&lt;/h3&gt;
&lt;p&gt;The choice of $\lambda$ determines the bias-variance tradeoff:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;if $\lambda$ is too big:
$\lambda \approx \infty \mathbb \Rightarrow \hat{\beta} \approx 0$;&lt;/li&gt;
&lt;li&gt;if $\lambda$ is too small: $\lambda \approx 0 \mathbb \Rightarrow$
overfitting.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Possible solutions: Bonferroni correction, bootstrapping or
$\frac{\lambda}{n} \asymp \sqrt{\frac{\log(p)}{n}}$ (asymptotically
equal to), $\mathcal{S}$ behaves like the maximum of gaussians.&lt;/p&gt;
&lt;h3 id=&#34;lasso-path&#34;&gt;Lasso Path&lt;/h3&gt;
&lt;p&gt;How the estimated $\hat{\beta}$ depends on the penalty parameter
$\lambda$?&lt;/p&gt;
&lt;img src=&#34;../img/Fig_642.png&#34; style=&#34;width:50.0%&#34; /&gt;
&lt;p&gt;&lt;strong&gt;Post Lasso&lt;/strong&gt;: fit OLS without the penalty with all the nonzero
coeficients selected by Lasso in the first step.&lt;/p&gt;
&lt;h3 id=&#34;remarks-1&#34;&gt;Remarks&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Do not do inference with post-Lasso because standard errors are not
uniformely valid.&lt;/li&gt;
&lt;li&gt;As $n \to \infty$ the CV and the &lt;strong&gt;score domination&lt;/strong&gt; bounds
converge to a unique bound.&lt;/li&gt;
&lt;li&gt;What is the problem of cross-validation? In high dimensional
settings you can overfit in so many ways that CV doesn’t work and
still overfits.&lt;/li&gt;
&lt;li&gt;Using $\lambda$ with $\frac{\lambda}{n} &amp;gt; \mathcal{S}$ small
coefficients get shrunk to zero with high probability. In this case
with small we mean $\propto \frac{1}{\sqrt{n}}$ or
$2 \max_j | \mathbb E_n[\varepsilon_i x_{ij}] |$.&lt;/li&gt;
&lt;li&gt;If $| \beta_{0j}| \leq \frac{c}{\sqrt{n}}$ for a sufficiently small
constant $c$, then $\hat{\beta}_{LASSO} \overset{p}{\to} 0$.&lt;/li&gt;
&lt;li&gt;In standard t-tests $c = 1.96$.&lt;/li&gt;
&lt;li&gt;$\sqrt{n}$ factor is important since it is the demarcation line for
reliable statistical detection.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;optimal-lambda&#34;&gt;Optimal Lambda&lt;/h3&gt;
&lt;p&gt;What is the criterium that should guide the selection of $\lambda$? $$
\frac{\lambda}{n} \geq 2 \mathbb E_n[x_{ij} \varepsilon_i] \qquad \forall j \quad \text{ if } Var(x_{ij} \varepsilon_i) = 1
$$&lt;/p&gt;
&lt;p&gt;How to choose the optimal $\lambda$:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Decide the coverage of the confidence intervals ($1-\alpha$): $$
\Pr \left( \sqrt{n} \Big| \mathbb E_n [x_{ij} \varepsilon_i] \Big| &amp;gt; t \right) = 1- \alpha
$$&lt;/li&gt;
&lt;li&gt;Solve for $t$&lt;/li&gt;
&lt;li&gt;Get $\lambda$ such that all scores are dominated by
$\frac{\lambda}{n}$ with $\alpha%$ probability.&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;It turns out that the optimal $t \propto \sqrt{\log(p)}$&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;pre-testing&#34;&gt;Pre-Testing&lt;/h2&gt;
&lt;h3 id=&#34;omitted-variable-bias&#34;&gt;Omitted Variable Bias&lt;/h3&gt;
&lt;p&gt;Consider two separate statistical models. Assume the following &lt;strong&gt;long
regression&lt;/strong&gt; of interest:&lt;/p&gt;
&lt;p&gt;$$
y_i = x_i&amp;rsquo; \alpha_0+ z_i&amp;rsquo; \beta_0 + \varepsilon_i
$$&lt;/p&gt;
&lt;p&gt;Define the corresponding &lt;strong&gt;short regression&lt;/strong&gt; as&lt;/p&gt;
&lt;p&gt;$$
y_i = x_i&amp;rsquo; \alpha_0 + v_i \quad \text{ with } v_i = z_i&amp;rsquo; \beta_0 + \varepsilon_i
$$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Suppose that the DGP for the long regression corresponds to $\alpha_0$,
$\beta_0$. Suppose further that $\mathbb E[x_i] = 0$,
$\mathbb E[z_i] = 0$, $\mathbb E[\varepsilon_i |x_i,z_i] = 0$. Then,
unless $\beta_0 = 0$ or $z_i$ is orthogonal to $x_i$, the (sole)
stochastic regressor $x_i$ is correlated with the error term in the
short regression which implies that the OLS estimator of the short
regression is inconsistent for $\alpha_0$ due to the omitted variable
bias. In particular, one can show that the plim of the OLS estimator of
$\hat{\alpha}&lt;em&gt;{SHORT}$ from the short regression is $$
\hat{\alpha}&lt;/em&gt;{SHORT} \overset{p}{\to} \frac{Cov(y_i, x_i)}{Var(x_i)} = \alpha_0 + \beta_0 \frac{Cov(z_i, x_i)}{Var(x_i)}
$$&lt;/p&gt;
&lt;h3 id=&#34;pre-test-bias&#34;&gt;Pre-test bias&lt;/h3&gt;
&lt;p&gt;Consider data $D= (y_i, x_i, z_i)_{i=1}^n$, where the true model is: $$
\begin{aligned}
&amp;amp; y_i = x_i&amp;rsquo; \alpha_0  + z_i&amp;rsquo; \beta_0 + \varepsilon_i \newline
&amp;amp; x_i = z_i&amp;rsquo; \gamma_0 + u_i
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;Where $x_i$ is the variable of interest (we want to make inference on
$\alpha_0$) and $z_i$ is a high dimensional set of control variables.&lt;/p&gt;
&lt;p&gt;From now on, we will work under the following assumptions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\dim(x_i)=1$ for all $n$&lt;/li&gt;
&lt;li&gt;$\beta_0$ uniformely bounded in $n$&lt;/li&gt;
&lt;li&gt;Strict exogeneity: $\mathbb E[\varepsilon_i | x_i, z_i] = 0$ and
$\mathbb E[u_i | z_i] = 0$&lt;/li&gt;
&lt;li&gt;$\beta_0$ and $\gamma_0$ have dimension (and hence value) that
depend on $n$&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;pre-testing-procedure&#34;&gt;Pre-Testing procedure&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;Regress $y_i$ on $x_i$ and $z_i$&lt;/li&gt;
&lt;li&gt;For each $j = 1, &amp;hellip;, p = \dim(z_i)$ calculate a test statistic
$t_j$&lt;/li&gt;
&lt;li&gt;Let $\hat{T} = \lbrace j: |t_j| &amp;gt; C &amp;gt; 0 \rbrace$ for some constant
$C$ (set of statistically significant coefficients).&lt;/li&gt;
&lt;li&gt;Re-run the new “model” using $(x_i, z_{\hat{T},i})$ (i.e. using the
selected covariates with statistically significant coefficients).&lt;/li&gt;
&lt;li&gt;Perform statistical inference (i.e. confidence intervals and
hypothesis tests) as if no model selection had been done.&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;bias&#34;&gt;Bias&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;../img/Fig_621.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;As we can see from the figure above (code below), running the short
regression instead of the long one introduces Omitted Variable Bias
(second column). Instead, the Pre-Testing estimator is consistent but
not normally distributed (third column).&lt;/p&gt;
&lt;h3 id=&#34;issue-1&#34;&gt;Issue&lt;/h3&gt;
&lt;p&gt;Pre-testing is problematic because the post-selection estimator is not
asymptotically normal. Moreover, for particular data generating
processes, it even fails to be consistent at the rate of $\sqrt{n}$
(Belloni et al., 2014).&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Intuition: when performing pre-testing, we might have an Omitted
Variable Bias problem when $\beta_0&amp;gt;0$ but we fail to reject the null
hypothesis $H_0 : \beta_0 = 0$ because of lack of statistical power,
i.e. $|\beta_0|$ is small with respect to the sample size. In
particular, we fail to reject the null hypothesis for
$\beta_0(n) = O \left( \frac{1}{\sqrt{n}}\right)$. However, note that
the problem vanishes asymptotically, as the resulting estimator is
consistent. In fact, if
$\beta_0(n) = O \left( \frac{1}{\sqrt{n}}\right)$, then
$\alpha_0 - \hat \alpha_{PRETEST} \overset{p}{\to} \lim_{n \to \infty} \beta_0 \gamma_0 = \lim_{n \to \infty} O \left( \frac{1}{\sqrt{n}} \right) = 0$.
We now clarify what it means to have a coefficient depending on the
sample size, $\beta_0(n)$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;uniformity&#34;&gt;Uniformity&lt;/h3&gt;
&lt;p&gt;Concept of &lt;strong&gt;uniformity&lt;/strong&gt;: the DGP varies with $n$. Instead of having a
fixed “true” parameter $\beta_0$, you have a sequence $\beta_0(n)$.
Having a cofficient that depends on the sample size $n$ is useful to
preserve the concept of “small with respect to the sample size” in
asymptotic theory.&lt;/p&gt;
&lt;p&gt;In the context of Pre-Testing, all problems vanish asymptotically since
we are able to always reject the null hypothesis $H_0 : \beta_0 = 0$
when $\beta_0 \neq 0$. In the figure below, I plot simulation results
for $\hat \alpha_{PRETESTING}$ for a fixed coefficient $\beta_0$ (first
row) and variable coefficient $\beta_0(n)$ that depends on the sample
size (second row), for different sample sizes (columns). We see that if
$\beta_0$ is independent from the sample size (first row), the
distribution of $\hat \alpha_{PRETEST}$ is not normal in small samples
and it displays the bimodality that characterizes pre-testing. However,
it becomes normal in large samples. On the other hand, when $\beta_0(n)$
depends on the sample size, and in particular
$\beta_0 = O \left( \frac{1}{\sqrt{n}} \right)$ (second row), the
distribution of $\hat \alpha_{PRETEST}$ stays bimodal even when the
sample size increases.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Note that the estimator is always consistent!&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;where-is-pre-testing-a-problem&#34;&gt;Where is Pre-Testing a Problem?&lt;/h3&gt;
&lt;p&gt;If we were to draw a map of where the gaussianity assumption of
$\beta_0(n)$ holds well and where it fails, it would look like the
following figure.&lt;/p&gt;
&lt;img src=&#34;../img/Fig_623.png&#34; style=&#34;width:50.0%&#34; /&gt;
&lt;h3 id=&#34;intuition&#34;&gt;Intuition&lt;/h3&gt;
&lt;p&gt;The intuition for the three different regions (from bottom to top) is
the following.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;When $\beta_0 = o \left( \frac{1}{\sqrt{n}} \right)$, $z_i$ is
excluded with probability $p \to 1$. But, given that $\beta_0$ is
small enough, failing to control for $z_i$ does not introduce large
omitted variables bias (Belloni et al., 2014).&lt;/li&gt;
&lt;li&gt;If however the coefficient on the control is “moderately close to
zero”, $\beta_0 = O \left( \frac{1}{\sqrt{n}} \right)$, the t-test
set-up above cannot distinguish this coefficient from $0$, and the
control $z_i$ is dropped with probability $p \to 1$. However, in
this case the omitted variable bias generated by excluding $z_i$
scaled by $\sqrt{n}$ does not converge to zero. That is, the
standard post-selection estimator is not asymptotically normal and
even fails to be consistent at the rate of $\sqrt{n}$ (Belloni et
al., 2014).&lt;/li&gt;
&lt;li&gt;Lastly, when $\beta_0$ is large enough, the null pre-testing
hypothesis $H_0 : \beta_0 = 0$ will be rejected sufficiently often
so that the bias is negligible.&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;post-double-selection&#34;&gt;Post-Double Selection&lt;/h3&gt;
&lt;p&gt;The post-double-selection estimator, $\hat{\alpha}_{PDS}$ solves this
problem by doing variable selection via standard t-tests or Lasso-type
selectors with the two “true model” equations (&lt;strong&gt;first stage&lt;/strong&gt; and
&lt;strong&gt;reduced form&lt;/strong&gt;) that contain the information from the model and then
estimating $\alpha_0$ by regressing $y_i$ on $x_i$ and the union of the
selected controls. By doing so, $z_i$ is omitted only if its coefficient
in both equations is small which greatly limits the potential for
omitted variables bias (Belloni et al., 2014).&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Intuition: by performing post-double selection, we ensure that both
$\beta_0 = O \left( \frac{1}{\sqrt{n}} \right)$ and
$\gamma_0 = O \left( \frac{1}{\sqrt{n}} \right)$ so that
$\sqrt{n} ( \hat \alpha _ {PRETEST} - \alpha _ 0) \overset{p}{\to} \lim_{n \to \infty} \sqrt{n} \beta_0 \gamma_0 = \lim_{n \to \infty} \sqrt{n} O \left( \frac{1}{n} \right) = 0$
and the estimator is gaussian.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;frisch-waugh-theorem&#34;&gt;Frisch-Waugh Theorem&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Consider the data $D = \lbrace x_i, y_i, z_i \rbrace_{i=1}^\infty$ with
DGP: $Y = X \alpha + Z \beta + \varepsilon$. The following estimators of
$\alpha$ are numerically equivalent (if $[X, Z]$ has full rank):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\hat{\alpha}$ from regressing $Y$ on $X, Z$&lt;/li&gt;
&lt;li&gt;$\tilde{\alpha}$ from regressing $Y$ on $\tilde{X}$&lt;/li&gt;
&lt;li&gt;$\bar{\alpha}$ from regressing $\tilde{Y}$ on $\tilde{X}$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;where the operation of passing to $Y, X$ to $\tilde{Y}, \tilde{X}$ is
called &lt;em&gt;projection out $Z$&lt;/em&gt;, e.g.$\tilde{X}$ are the residuals from
regressing $X$ on $Z$.&lt;/p&gt;
&lt;h3 id=&#34;proof-1&#34;&gt;Proof (1)&lt;/h3&gt;
&lt;p&gt;We want to show that $\hat{\alpha} = \tilde{\alpha}$.&lt;/p&gt;
&lt;p&gt;Claim:
$\hat{\alpha } = \tilde{\alpha} \Leftrightarrow \tilde{X}&amp;rsquo; \left[ (X - \tilde{X})\hat{\alpha} + Z \hat{\beta} +\hat{\varepsilon} \right] = 0$.&lt;/p&gt;
&lt;p&gt;Proof of the claim: if $\hat{\alpha} = \tilde{\alpha}$, we can write $Y$
as $$
Y =  X \hat{\alpha} + Z \hat{\beta} + \hat{\varepsilon}  = \tilde{X} \hat{\alpha} + \underbrace{(X - \tilde{X}) \hat{\alpha } + Z \hat{\beta} + \hat{\varepsilon}}_\text{residual of $Y$ on $\tilde{X} $} = \tilde{X} \tilde{\alpha} + \nu_i
$$&lt;/p&gt;
&lt;p&gt;Therefore, by the orthogonality property of the OLS residual, it must be
that $\tilde{X}&amp;rsquo;\nu_i= 0$. $$\tag*{$\blacksquare$}$$&lt;/p&gt;
&lt;h3 id=&#34;proof-1-1&#34;&gt;Proof (1)&lt;/h3&gt;
&lt;p&gt;Having established the claim, we want to show that the normal equation
$\tilde{X}&amp;rsquo; \left[ (X - \tilde{X})\hat{\alpha} + Z \hat{\beta} +\hat{\varepsilon} \right] = 0$
is satisfied. We follow 3 steps:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;First we have that $\tilde{X}&amp;rsquo; (X - \tilde{X})\hat{\alpha} = 0$.
This follows from the fact that $\tilde{X}&amp;rsquo; = X&amp;rsquo; M_Z$ and hence: $$
\begin{aligned}
\tilde{X}&amp;rsquo; (X - \tilde{X})  &amp;amp;  = X&amp;rsquo; M_Z (X - M_Z) = X&amp;rsquo; M_Z X - X&amp;rsquo; \overbrace{M_Z M_Z}^{M_Z} X \newline &amp;amp; = X&amp;rsquo;M_Z X - X&amp;rsquo; M_Z X = 0
\end{aligned}
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$\tilde{X}&amp;rsquo; Z \hat{\beta} = 0$ since $\tilde{X}$ is the residual
from the regression of $X$ on $Z$, by normal equation it holds that
$\tilde{X}&amp;rsquo; Z = 0$.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$\tilde{X}&amp;rsquo; \hat{\varepsilon} = 0$. This follows from (i)
$M_Z &amp;rsquo; M_{X, Z} = M_{X,Z}$ and (ii) $X&amp;rsquo; M_{X, Z} = 0$: $$
\tilde{X}&amp;rsquo; \hat{\varepsilon} = (M_Z X)&amp;rsquo; (M_{X, Z} \varepsilon)  = X&amp;rsquo;M_Z&amp;rsquo; M_{X, Z} \varepsilon = \underbrace{X&amp;rsquo; M_{X, Z}}_0 \varepsilon = 0.
$$ $$\tag*{$\blacksquare$}$$&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The coefficient $\hat{\alpha}$ is a &lt;em&gt;partial regression&lt;/em&gt; coefficient
identified from the variation in $X$ that is orthogonal to $Z$. This is
often known as &lt;strong&gt;residual variation&lt;/strong&gt;.&lt;/p&gt;
&lt;h2 id=&#34;post-double-selection-1&#34;&gt;Post Double Selection&lt;/h2&gt;
&lt;h3 id=&#34;setting&#34;&gt;Setting&lt;/h3&gt;
&lt;p&gt;Consider again data $D= (y_i, x_i, z_i)_{i=1}^n$, where the true model
is: $$
\begin{aligned}
&amp;amp; y_i = x_i&amp;rsquo; \alpha_0  + z_i&amp;rsquo; \beta_0 + \varepsilon_i \newline
&amp;amp; x_i = z_i&amp;rsquo; \gamma_0 + u_i
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;We would like to guard against pretest bias if possible, in order to
handle high dimensional models. A good pathway towards motivating
procedures which guard against pretest bias is a discussion of classical
partitioned regression.&lt;/p&gt;
&lt;p&gt;Consider a regression $y_i$ on $x_i$ and $z_i$. $x_i$ is the
1-dimensional variable of interest, $z_i$ is a high-dimensional set of
control variables. We have the following procedure:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;First Stage&lt;/strong&gt; selection: lasso $x_i$ on $z_i$. Let the selected
variables be collected in the set $S_{FS} \subseteq z_i$&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Reduced Form&lt;/strong&gt; selection: lasso $y_i$ on $z_i$. Let the selected
variables be collected in the set $S_{RF} \subseteq z_i$&lt;/li&gt;
&lt;li&gt;Regress $y_i$ on $x_i$ and $S_{FS} \cup S_{RF}$&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;pds-theorem&#34;&gt;PDS Theorem&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Let $\lbrace P^n\rbrace$ be a sequence of data-generating processes for
$D_n = (y_i, x_i, z_i)^n_{i=1} \in (\mathbb R \times \mathbb R \times \mathbb R^p) ^n$
where $p$ depends on $n$. For each $n$, the data are iid with
$yi = x_i&amp;rsquo;\alpha_0^{(n)} + z_i&amp;rsquo; \beta_0^{(n)} + \varepsilon_i$ and
$x_i = z_i&amp;rsquo; \gamma_0^{(n)} + u_i$ where
$\mathbb E[\varepsilon_i | x_i,z_i] = 0$ and $\mathbb E[u_i|z_i] = 0$.
The sparsity of the vectors $\beta_0^{(n)}$, $\gamma_0^{(n)}$ is
controlled by $|| \beta_0^{(n)} ||_0 \leq s$ with
$s^2 (\log p)^2/n \to 0$. Suppose that additional regularity conditions
on the model selection procedures and moments of the random variables
$y_i$ , $x_i$ , $z_i$ as documented in Belloni et al. (2014). Then the
confidence intervals, CI, from the post double selection procedure are
uniformly valid. That is, for any confidence level $\xi \in (0, 1)$ $$
\Pr(\alpha_0 \in CI) \to 1- \xi
$$&lt;/p&gt;
&lt;p&gt;In order to have valid confidence intervals you want their bias to be
negligibly. Since $$
CI = \left[ \hat{\alpha} \pm \frac{1.96 \cdot \hat{\sigma}}{\sqrt{n}} \right]
$$&lt;/p&gt;
&lt;p&gt;If the bias is $o \left( \frac{1}{\sqrt{n}} \right)$ then there is no
problem since it is asymptotically negligible w.r.t. the magnitude of
the confidence interval. If however the the bias is
$O \left( \frac{1}{\sqrt{n}} \right)$ then it has the same magnitude of
the confidence interval and it does not asymptotically vanish.&lt;/p&gt;
&lt;h3 id=&#34;proof-idea&#34;&gt;Proof (Idea)&lt;/h3&gt;
&lt;p&gt;The idea of the proof is to use partitioned regression. An alternative
way to think about the argument is: bound the omitted variables bias.
Omitted variable bias comes from the product of 2 quantities related to
the omitted variable:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Its partial correlation with the outcome, and&lt;/li&gt;
&lt;li&gt;Its partial correlation with the variable of interest.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;If both those partial correlations are $O( \sqrt{\log p/n})$, then the
omitted variables bias is
$(s \times O( \sqrt{\log p/n})^2 = o \left( \frac{1}{\sqrt{n}} \right)$,
provided $s^2 (\log p)^2/n \to 0$. Relative to the $\frac{1}{\sqrt{n}}$
convergence rate, the omitted variables bias is negligible.&lt;/p&gt;
&lt;p&gt;In our omitted variable bias case, we want
$| \beta_0 \gamma_0 | = o \left( \frac{1}{\sqrt{n}} \right)$.
Post-double selection guarantees that&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Reduced form&lt;/em&gt; selection (pre-testing): any “missing” variable has
$|\beta_{0j}| \leq \frac{c}{\sqrt{n}}$&lt;/li&gt;
&lt;li&gt;&lt;em&gt;First stage&lt;/em&gt; selection (additional): any “missing” variable has
$|\gamma_{0j}| \leq \frac{c}{\sqrt{n}}$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;As a consequence, as long as the number of omitted variables is finite,
the omitted variable bias is $$
OVB(\alpha) = |\beta_{0j}| \cdot|\gamma_{0j}| \leq \frac{c}{\sqrt{n}} \cdot \frac{c}{\sqrt{n}} = \frac{c^2}{n} = o \left(\frac{1}{\sqrt{n}}\right)
$$&lt;/p&gt;
&lt;h3 id=&#34;distribution&#34;&gt;Distribution&lt;/h3&gt;
&lt;p&gt;We can plot the distribution of the post-double selection estimator
against the pre-testing one.&lt;/p&gt;
&lt;img src=&#34;../img/Fig_641.png&#34; style=&#34;width:60.0%&#34; /&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Remark&lt;/strong&gt;: under homoskedasticity, the above estimator achieves the
semiparametric efficiency bound.&lt;/p&gt;
&lt;/blockquote&gt;
</description>
    </item>
    
    <item>
      <title>Unsupervised Learning</title>
      <link>https://matteocourthoud.github.io/course/ml-econ/10_unsupervised/</link>
      <pubDate>Wed, 09 Mar 2022 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/course/ml-econ/10_unsupervised/</guid>
      <description>&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Setup
from utils.lecture10 import *
%matplotlib inline
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;supervised-vs-unsupervised-learning&#34;&gt;Supervised vs Unsupervised Learning&lt;/h3&gt;
&lt;p&gt;The difference between &lt;em&gt;supervised learning&lt;/em&gt; and &lt;em&gt;unsupervised learning&lt;/em&gt; is that in the first case we have a variable $y$ which we want to predict, given a set of variables ${ X_1, . . . , X_p }$.&lt;/p&gt;
&lt;p&gt;In &lt;em&gt;unsupervised learning&lt;/em&gt; are not interested in prediction, because we do not have an associated response variable $y$. Rather, the goal is to discover interesting properties about the measurements on ${ X_1, . . . , X_p }$.&lt;/p&gt;
&lt;p&gt;Questions that we are usually interested in are&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Clustering&lt;/li&gt;
&lt;li&gt;Dimensionality reduction&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In general, unsupervised learning can be viewed as an extention of exploratory data analysis.&lt;/p&gt;
&lt;h3 id=&#34;dimensionality-reduction&#34;&gt;Dimensionality Reduction&lt;/h3&gt;
&lt;p&gt;Working in high-dimensional spaces can be undesirable for many reasons; raw data are often sparse as a consequence of the curse of dimensionality, and analyzing the data is usually computationally intractable (hard to control or deal with).&lt;/p&gt;
&lt;p&gt;Dimensionality reduction can also be useful to plot high-dimensional data.&lt;/p&gt;
&lt;h3 id=&#34;clustering&#34;&gt;Clustering&lt;/h3&gt;
&lt;p&gt;Clustering refers to a very broad set of techniques for finding subgroups, or clusters, in a data set. When we cluster the observations of a data set, we seek to partition them into distinct groups so that the observations within each group are quite similar to each other, while observations in different groups are quite different from each other.&lt;/p&gt;
&lt;p&gt;In this section we focus on the following algorithms:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;K-means clustering&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Hierarchical clustering&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Gaussian Mixture Models&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;principal-component-analysis&#34;&gt;Principal Component Analysis&lt;/h2&gt;
&lt;p&gt;Suppose that we wish to visualize $n$ observations with measurements on a set of $p$ features, ${X_1, . . . , X_p}$, as part of an exploratory data analysis.&lt;/p&gt;
&lt;p&gt;We could do this by examining two-dimensional scatterplots of the data, each of which contains the n observations’ measurements on two of the features. However, there are $p(p−1)/2$ such scatterplots; for example,
with $p = 10$ there are $45$ plots!&lt;/p&gt;
&lt;p&gt;PCA provides a tool to do just this. It finds a low-dimensional represen- tation of a data set that contains as much as possible of the variation.&lt;/p&gt;
&lt;h3 id=&#34;first-principal-component&#34;&gt;First Principal Component&lt;/h3&gt;
&lt;p&gt;The &lt;strong&gt;first principal component&lt;/strong&gt; of a set of features ${X_1, . . . , X_p}$ is the normalized linear combination of the features $Z_1$&lt;/p&gt;
&lt;p&gt;$$
Z_1 = \phi_{11} X_1 + \phi_{21} X_2 + &amp;hellip; + \phi_{p1} X_p
$$&lt;/p&gt;
&lt;p&gt;that has the largest variance.&lt;/p&gt;
&lt;p&gt;By normalized, we mean that $\sum_{i=1}^p \phi^2_{i1} = 1$.&lt;/p&gt;
&lt;h3 id=&#34;pca-computation&#34;&gt;PCA Computation&lt;/h3&gt;
&lt;p&gt;In other words, the first principal component loading vector solves the optimization problem&lt;/p&gt;
&lt;p&gt;$$
\underset{\phi_{11}, \ldots, \phi_{p 1}}{\max} \ \Bigg \lbrace \frac{1}{n} \sum _ {i=1}^{n}\left(\sum _ {j=1}^{p} \phi _ {j1} x _ {ij} \right)^{2} \Bigg \rbrace \quad \text { subject to } \quad \sum _ {j=1}^{p} \phi _ {j1}^{2}=1
$$&lt;/p&gt;
&lt;p&gt;The objective that we are maximizing is just the sample variance of the $n$ values of $z_{i1}$.&lt;/p&gt;
&lt;p&gt;After the first principal component $Z_1$ of the features has been determined, we can find the second principal component $Z_2$. The &lt;strong&gt;second principal component&lt;/strong&gt; is the linear combination of ${X_1, . . . , X_p}$ that has maximal variance out of all linear combinations that are &lt;em&gt;uncorrelated&lt;/em&gt; with $Z_1$.&lt;/p&gt;
&lt;h3 id=&#34;example&#34;&gt;Example&lt;/h3&gt;
&lt;p&gt;We illustrate the use of PCA on the &lt;code&gt;USArrests&lt;/code&gt; data set.&lt;/p&gt;
&lt;p&gt;For each of the 50 states in the United States, the data set contains the number of arrests per $100,000$ residents for each of three crimes: &lt;code&gt;Assault&lt;/code&gt;, &lt;code&gt;Murder&lt;/code&gt;, and &lt;code&gt;Rape.&lt;/code&gt; We also record the percent of the population in each state living in urban areas, &lt;code&gt;UrbanPop&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Load crime data
df = pd.read_csv(&#39;data/USArrests.csv&#39;, index_col=0)
df.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;Murder&lt;/th&gt;
      &lt;th&gt;Assault&lt;/th&gt;
      &lt;th&gt;UrbanPop&lt;/th&gt;
      &lt;th&gt;Rape&lt;/th&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;State&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;Alabama&lt;/th&gt;
      &lt;td&gt;13.2&lt;/td&gt;
      &lt;td&gt;236&lt;/td&gt;
      &lt;td&gt;58&lt;/td&gt;
      &lt;td&gt;21.2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;Alaska&lt;/th&gt;
      &lt;td&gt;10.0&lt;/td&gt;
      &lt;td&gt;263&lt;/td&gt;
      &lt;td&gt;48&lt;/td&gt;
      &lt;td&gt;44.5&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;Arizona&lt;/th&gt;
      &lt;td&gt;8.1&lt;/td&gt;
      &lt;td&gt;294&lt;/td&gt;
      &lt;td&gt;80&lt;/td&gt;
      &lt;td&gt;31.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;Arkansas&lt;/th&gt;
      &lt;td&gt;8.8&lt;/td&gt;
      &lt;td&gt;190&lt;/td&gt;
      &lt;td&gt;50&lt;/td&gt;
      &lt;td&gt;19.5&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;California&lt;/th&gt;
      &lt;td&gt;9.0&lt;/td&gt;
      &lt;td&gt;276&lt;/td&gt;
      &lt;td&gt;91&lt;/td&gt;
      &lt;td&gt;40.6&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;h3 id=&#34;data-scaling&#34;&gt;Data Scaling&lt;/h3&gt;
&lt;p&gt;To make all the features comparable, we first need to scale them. In this case, we use the &lt;code&gt;sklearn.preprocessing.scale()&lt;/code&gt; function to normalize each variable to have zero mean and unit variance.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Scale data
X_scaled = pd.DataFrame(scale(df), index=df.index, columns=df.columns).values
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We will see later what are the practical implications of (not) scaling.&lt;/p&gt;
&lt;h3 id=&#34;fitting&#34;&gt;Fitting&lt;/h3&gt;
&lt;p&gt;Let&amp;rsquo;s fit PCA with 2 components.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Fit PCA with 2 components
pca2 = PCA(n_components=2).fit(X_scaled)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Get weights
weights = pca2.components_.T
df_weights = pd.DataFrame(weights, index=df.columns, columns=[&#39;PC1&#39;, &#39;PC2&#39;])
df_weights
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;PC1&lt;/th&gt;
      &lt;th&gt;PC2&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;Murder&lt;/th&gt;
      &lt;td&gt;0.535899&lt;/td&gt;
      &lt;td&gt;0.418181&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;Assault&lt;/th&gt;
      &lt;td&gt;0.583184&lt;/td&gt;
      &lt;td&gt;0.187986&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;UrbanPop&lt;/th&gt;
      &lt;td&gt;0.278191&lt;/td&gt;
      &lt;td&gt;-0.872806&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;Rape&lt;/th&gt;
      &lt;td&gt;0.543432&lt;/td&gt;
      &lt;td&gt;-0.167319&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;h3 id=&#34;projecting-the-data&#34;&gt;Projecting the data&lt;/h3&gt;
&lt;p&gt;What does the trasformed data looks like?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Transform X to get the principal components
X_dim2 = pca2.transform(X_scaled)
df_dim2 = pd.DataFrame(X_dim2, columns=[&#39;PC1&#39;, &#39;PC2&#39;], index=df.index)
df_dim2.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;PC1&lt;/th&gt;
      &lt;th&gt;PC2&lt;/th&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;State&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;Alabama&lt;/th&gt;
      &lt;td&gt;0.985566&lt;/td&gt;
      &lt;td&gt;1.133392&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;Alaska&lt;/th&gt;
      &lt;td&gt;1.950138&lt;/td&gt;
      &lt;td&gt;1.073213&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;Arizona&lt;/th&gt;
      &lt;td&gt;1.763164&lt;/td&gt;
      &lt;td&gt;-0.745957&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;Arkansas&lt;/th&gt;
      &lt;td&gt;-0.141420&lt;/td&gt;
      &lt;td&gt;1.119797&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;California&lt;/th&gt;
      &lt;td&gt;2.523980&lt;/td&gt;
      &lt;td&gt;-1.542934&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;h3 id=&#34;visualization&#34;&gt;Visualization&lt;/h3&gt;
&lt;p&gt;The advantage og PCA is that it allows us to see the variation in lower dimesions.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;make_figure_10_1a(df_dim2, df_weights)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/10_unsupervised_30_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;pca-and-spectral-analysis&#34;&gt;PCA and Spectral Analysis&lt;/h3&gt;
&lt;p&gt;In case you haven&amp;rsquo;t noticed, calculating principal components, is equivalent to calculating the eigenvectors of the design matrix $X&amp;rsquo;X$, i.e. the variance-covariance matrix of $X$. Indeed what we performed above is a decomposition of the variance of $X$ into orthogonal components.&lt;/p&gt;
&lt;p&gt;The constrained maximization problem above can be re-written in matrix notation as&lt;/p&gt;
&lt;p&gt;$$
\max \ \phi&amp;rsquo; X&amp;rsquo;X \phi \quad \text{ s. t. } \quad \phi&amp;rsquo;\phi = 1
$$&lt;/p&gt;
&lt;p&gt;Which has the following dual representation&lt;/p&gt;
&lt;p&gt;$$
\mathcal L (\phi, \lambda) = \phi&amp;rsquo; X&amp;rsquo;X \phi - \lambda (\phi&amp;rsquo;\phi - 1)
$$&lt;/p&gt;
&lt;p&gt;If we take the first order conditions&lt;/p&gt;
&lt;p&gt;$$
\begin{align}
&amp;amp; \frac{\partial \mathcal L}{\partial \lambda} = \phi&amp;rsquo;\phi - 1 \
&amp;amp; \frac{\partial \mathcal L}{\partial \phi} = 2 X&amp;rsquo;X \phi - 2 \lambda \phi
\end{align}
$$&lt;/p&gt;
&lt;p&gt;Setting the derivatives to zero at the optimum, we get&lt;/p&gt;
&lt;p&gt;$$
\begin{align}
&amp;amp; \phi&amp;rsquo;\phi = 1 \
&amp;amp; X&amp;rsquo;X \phi = \lambda \phi
\end{align}
$$&lt;/p&gt;
&lt;p&gt;Thus, $\phi$ is an &lt;strong&gt;eigenvector&lt;/strong&gt; of the covariance matrix $X&amp;rsquo;X$, and the maximizing vector will be the one associated with the largest &lt;strong&gt;eigenvalue&lt;/strong&gt; $\lambda$.&lt;/p&gt;
&lt;h3 id=&#34;eigenvalues-and-eigenvectors&#34;&gt;Eigenvalues and eigenvectors&lt;/h3&gt;
&lt;p&gt;We can now double-check it using &lt;code&gt;numpy&lt;/code&gt; linear algebra package.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;eigenval, eigenvec = np.linalg.eig(X_scaled.T @ X_scaled)
data = np.concatenate((eigenvec,eigenval.reshape(1,-1)))
idx = list(df.columns) + [&#39;Eigenvalue&#39;]
df_eigen = pd.DataFrame(data, index=idx, columns=[&#39;PC1&#39;, &#39;PC2&#39;,&#39;PC3&#39;,&#39;PC4&#39;])

df_eigen
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;PC1&lt;/th&gt;
      &lt;th&gt;PC2&lt;/th&gt;
      &lt;th&gt;PC3&lt;/th&gt;
      &lt;th&gt;PC4&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;Murder&lt;/th&gt;
      &lt;td&gt;0.535899&lt;/td&gt;
      &lt;td&gt;0.418181&lt;/td&gt;
      &lt;td&gt;0.649228&lt;/td&gt;
      &lt;td&gt;-0.341233&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;Assault&lt;/th&gt;
      &lt;td&gt;0.583184&lt;/td&gt;
      &lt;td&gt;0.187986&lt;/td&gt;
      &lt;td&gt;-0.743407&lt;/td&gt;
      &lt;td&gt;-0.268148&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;UrbanPop&lt;/th&gt;
      &lt;td&gt;0.278191&lt;/td&gt;
      &lt;td&gt;-0.872806&lt;/td&gt;
      &lt;td&gt;0.133878&lt;/td&gt;
      &lt;td&gt;-0.378016&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;Rape&lt;/th&gt;
      &lt;td&gt;0.543432&lt;/td&gt;
      &lt;td&gt;-0.167319&lt;/td&gt;
      &lt;td&gt;0.089024&lt;/td&gt;
      &lt;td&gt;0.817778&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;Eigenvalue&lt;/th&gt;
      &lt;td&gt;124.012079&lt;/td&gt;
      &lt;td&gt;49.488258&lt;/td&gt;
      &lt;td&gt;8.671504&lt;/td&gt;
      &lt;td&gt;17.828159&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;The spectral decomposition of the variance of $X$ generates a set of orthogonal vectors (eigenvectors) with different magnitudes (eigenvalues). The eigenvalues tell us the amount of variance of the data in that direction.&lt;/p&gt;
&lt;p&gt;If we combine the eigenvectors together, we form a projection matrix $P$ that we can use to transform the original variables: $\tilde X = P X$&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;X_transformed = X_scaled @ eigenvec
df_transformed = pd.DataFrame(X_transformed, index=df.index, columns=[&#39;PC1&#39;, &#39;PC2&#39;,&#39;PC3&#39;,&#39;PC4&#39;])

df_transformed.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;PC1&lt;/th&gt;
      &lt;th&gt;PC2&lt;/th&gt;
      &lt;th&gt;PC3&lt;/th&gt;
      &lt;th&gt;PC4&lt;/th&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;State&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;Alabama&lt;/th&gt;
      &lt;td&gt;0.985566&lt;/td&gt;
      &lt;td&gt;1.133392&lt;/td&gt;
      &lt;td&gt;0.156267&lt;/td&gt;
      &lt;td&gt;-0.444269&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;Alaska&lt;/th&gt;
      &lt;td&gt;1.950138&lt;/td&gt;
      &lt;td&gt;1.073213&lt;/td&gt;
      &lt;td&gt;-0.438583&lt;/td&gt;
      &lt;td&gt;2.040003&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;Arizona&lt;/th&gt;
      &lt;td&gt;1.763164&lt;/td&gt;
      &lt;td&gt;-0.745957&lt;/td&gt;
      &lt;td&gt;-0.834653&lt;/td&gt;
      &lt;td&gt;0.054781&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;Arkansas&lt;/th&gt;
      &lt;td&gt;-0.141420&lt;/td&gt;
      &lt;td&gt;1.119797&lt;/td&gt;
      &lt;td&gt;-0.182811&lt;/td&gt;
      &lt;td&gt;0.114574&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;California&lt;/th&gt;
      &lt;td&gt;2.523980&lt;/td&gt;
      &lt;td&gt;-1.542934&lt;/td&gt;
      &lt;td&gt;-0.341996&lt;/td&gt;
      &lt;td&gt;0.598557&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;This is exactly the dataset that we obtained before.&lt;/p&gt;
&lt;h3 id=&#34;scaling-the-variables&#34;&gt;Scaling the Variables&lt;/h3&gt;
&lt;p&gt;The results obtained when we perform PCA will also depend on whether the variables have been individually scaled. In fact, the variance of a variable depends on its magnitude.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Variables variance
df.var(axis=0)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Murder        18.970465
Assault     6945.165714
UrbanPop     209.518776
Rape          87.729159
dtype: float64
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Consequently, if we perform PCA on the unscaled variables, then the first principal component loading vector will have a very large loading for &lt;code&gt;Assault&lt;/code&gt;, since that variable has by far the highest variance.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Fit PCA with unscaled varaibles
X = df.values
pca2_u = PCA(n_components=2).fit(X)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Get weights
weights_u = pca2_u.components_.T
df_weights_u = pd.DataFrame(weights_u, index=df.columns, columns=[&#39;PC1&#39;, &#39;PC2&#39;])
df_weights_u
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;PC1&lt;/th&gt;
      &lt;th&gt;PC2&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;Murder&lt;/th&gt;
      &lt;td&gt;0.041704&lt;/td&gt;
      &lt;td&gt;0.044822&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;Assault&lt;/th&gt;
      &lt;td&gt;0.995221&lt;/td&gt;
      &lt;td&gt;0.058760&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;UrbanPop&lt;/th&gt;
      &lt;td&gt;0.046336&lt;/td&gt;
      &lt;td&gt;-0.976857&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;Rape&lt;/th&gt;
      &lt;td&gt;0.075156&lt;/td&gt;
      &lt;td&gt;-0.200718&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Transform X to get the principal components
X_dim2_u = pca2_u.transform(X)
df_dim2_u = pd.DataFrame(X_dim2_u, columns=[&#39;PC1&#39;, &#39;PC2&#39;], index=df.index)
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;plotting&#34;&gt;Plotting&lt;/h3&gt;
&lt;p&gt;We can compare the lower dimensional representations with and without scaling.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;make_figure_10_1b(df_dim2, df_dim2_u, df_weights, df_weights_u)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/10_unsupervised_49_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;As predicted, the first principal component loading vector places almost all of its weight on &lt;code&gt;Assault&lt;/code&gt;, while the second principal component loading vector places almost all of its weight on &lt;code&gt;UrpanPop&lt;/code&gt;. Comparing this to the left-hand plot, we see that scaling does indeed have a substantial effect on the results obtained. However, this result is simply a consequence of the scales on which the variables were measured.&lt;/p&gt;
&lt;h3 id=&#34;the-proportion-of-variance-explained&#34;&gt;The Proportion of Variance Explained&lt;/h3&gt;
&lt;p&gt;We can now ask a natural question: how much of the information in a given data set is lost by projecting the observations onto the first few principal components? That is, how much of the variance in the data is not contained in the first few principal components? More generally, we are interested in knowing the &lt;strong&gt;proportion of variance explained (PVE)&lt;/strong&gt; by each principal component.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Four components
pca4 = PCA(n_components=4).fit(X_scaled)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Variance of the four principal components
pca4.explained_variance_
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;array([2.53085875, 1.00996444, 0.36383998, 0.17696948])
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;interpretation&#34;&gt;Interpretation&lt;/h3&gt;
&lt;p&gt;We can compute it in percentage of the total variance.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# As a percentage of the total variance
pca4.explained_variance_ratio_
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;array([0.62006039, 0.24744129, 0.0891408 , 0.04335752])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In the &lt;code&gt;Arrest&lt;/code&gt; dataset, the first principal component explains $62.0%$ of the variance in the data, and the next principal component explains $24.7%$ of the variance. Together, the first two principal components explain almost $87%$ of the variance in the data, and the last two principal components explain only $13%$ of the variance.&lt;/p&gt;
&lt;h3 id=&#34;plotting-1&#34;&gt;Plotting&lt;/h3&gt;
&lt;p&gt;We can plot in a graph the percentage of the variance explained, relative to the number of components.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;make_figure_10_2(pca4)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/10_unsupervised_61_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;how-many-principal-components&#34;&gt;How Many Principal Components?&lt;/h3&gt;
&lt;p&gt;In general, a $n \times p$ data matrix $X$ has $\min{n − 1, p}$ distinct principal components. However, we usually are not interested in all of them; rather, we would like to use just the first few principal components in order to visualize or interpret the data.&lt;/p&gt;
&lt;p&gt;We typically decide on the number of principal components required to visualize the data by examining a &lt;em&gt;scree plot&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;However, there is no well-accepted objective way to decide how many principal com- ponents are enough.&lt;/p&gt;
&lt;h2 id=&#34;k-means-clustering&#34;&gt;K-Means Clustering&lt;/h2&gt;
&lt;p&gt;The idea behind K-means clustering is that a good clustering is one for which the within-cluster variation is as small as possible. Hence we want to solve the problem&lt;/p&gt;
&lt;p&gt;$$
\underset{C_{1}, \ldots, C_{K}}{\operatorname{minimize}} \Bigg\lbrace \sum_{k=1}^{K} W\left(C_{k}\right) \Bigg\rbrace
$$&lt;/p&gt;
&lt;p&gt;where $C_k$ is a cluster and $ W(C_k)$ is a measure of the amount by which the observations within a cluster differ from each other.&lt;/p&gt;
&lt;p&gt;There are many possible ways to define this concept, but by far the most common choice involves &lt;strong&gt;squared Euclidean distance&lt;/strong&gt;. That is, we define&lt;/p&gt;
&lt;p&gt;$$
W\left(C_{k}\right)=\frac{1}{\left|C_{k}\right|} \sum_{i, i^{\prime} \in C_{k}} \sum_{j=1}^{p}\left(x_{i j}-x_{i^{\prime} j}\right)^2
$$&lt;/p&gt;
&lt;p&gt;where $|C_k|$ denotes the number of observations in the $k^{th}$ cluster.&lt;/p&gt;
&lt;h3 id=&#34;algorithm&#34;&gt;Algorithm&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Randomly assign a number, from $1$ to $K$, to each of the observations. These serve as initial cluster assignments for the observations.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Iterate until the cluster assignments stop changing:&lt;/p&gt;
&lt;p&gt;a) For each of the $K$ clusters, compute the cluster centroid. The kth cluster centroid is the vector of the $p$ feature means for the observations in the $k^{th}$ cluster.&lt;/p&gt;
&lt;p&gt;b) Assign each observation to the cluster whose centroid is closest (where closest is defined using Euclidean distance).&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;generate-the-data&#34;&gt;Generate the data&lt;/h3&gt;
&lt;p&gt;We first generate a 2-dimensional dataset.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Simulate data
np.random.seed(123)
X = np.random.randn(50,2)
X[0:25, 0] = X[0:25, 0] + 3
X[0:25, 1] = X[0:25, 1] - 4
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;make_new_figure_1(X)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/10_unsupervised_71_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;step-1-random-assignement&#34;&gt;Step 1: random assignement&lt;/h3&gt;
&lt;p&gt;Now let&amp;rsquo;s randomly assign the data to two clusters, at random.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Init clusters
K = 2
clusters0 = np.random.randint(K,size=(np.size(X,0)))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;make_new_figure_2(X, clusters0)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/10_unsupervised_75_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;step-2-estimate-distributions&#34;&gt;Step 2: estimate distributions&lt;/h3&gt;
&lt;p&gt;What are the new centroids?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Compute new centroids
def compute_new_centroids(X, clusters):
    K = len(np.unique(clusters))
    centroids = np.zeros((K,np.size(X,1)))
    for k in range(K):
        if sum(clusters==k)&amp;gt;0:
            centroids[k,:] = np.mean(X[clusters==k,:], axis=0)
        else:
            centroids[k,:] = np.mean(X, axis=0)
    return centroids
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Print
centroids0 = compute_new_centroids(X, clusters0)
print(centroids0)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[[ 1.54179703 -1.65922379]
 [ 1.67917325 -2.36272948]]
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;plotting-the-centroids&#34;&gt;Plotting the centroids&lt;/h3&gt;
&lt;p&gt;Let&amp;rsquo;s add the centroids to the graph.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Plot
plot_assignment(X, centroids0, clusters0, 0, 0)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/10_unsupervised_82_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;step-3-assign-data-to-clusters&#34;&gt;Step 3: assign data to clusters&lt;/h3&gt;
&lt;p&gt;Now we can assign the data to the clusters, according to the closest centroid.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Assign X to clusters
def assign_to_cluster(X, centroids):
    K = np.size(centroids,0)
    dist = np.zeros((np.size(X,0),K))
    for k in range(K):
        dist[:,k] = np.mean((X - centroids[k,:])**2, axis=1)
    clusters = np.argmin(dist, axis=1)
    
    # Compute inertia
    inertia = 0
    for k in range(K):
        if sum(clusters==k)&amp;gt;0:
            inertia += np.sum((X[clusters==k,:] - centroids[k,:])**2)
    return clusters, inertia
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;plotting-assigned-data&#34;&gt;Plotting assigned data&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Get cluster assignment
[clusters1,d] = assign_to_cluster(X, centroids0)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Plot
plot_assignment(X, centroids0, clusters1, d, 1)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/10_unsupervised_88_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;full-algorithm&#34;&gt;Full Algorithm&lt;/h3&gt;
&lt;p&gt;We now have all the components to proceed iteratively.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def kmeans_manual(X, K):

    # Init
    i = 0
    d0 = 1e4
    d1 = 1e5
    clusters = np.random.randint(K,size=(np.size(X,0)))

    # Iterate until convergence
    while np.abs(d0-d1) &amp;gt; 1e-10:
        d1 = d0
        centroids = compute_new_centroids(X, clusters)
        [clusters, d0] = assign_to_cluster(X, centroids)
        plot_assignment(X, centroids, clusters, d0, i)
        i+=1
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;plotting-k-means-clustering&#34;&gt;Plotting k-means clustering&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Test
kmeans_manual(X, K)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/10_unsupervised_93_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Here the observations can be easily plotted because they are two-dimensional.
If there were more than two variables then we could instead perform PCA
and plot the first two principal components score vectors.&lt;/p&gt;
&lt;h3 id=&#34;more-clusters&#34;&gt;More clusters&lt;/h3&gt;
&lt;p&gt;In the previous example, we knew that there really were two clusters because
we generated the data. However, for real data, in general we do not know
the true number of clusters. We could instead have performed K-means
clustering on this example with &lt;code&gt;K  =  3&lt;/code&gt;. If we do this, K-means clustering will split up the two &amp;ldquo;real&amp;rdquo; clusters, since it has no information about them:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# K=3
kmeans_manual(X, 3)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/10_unsupervised_97_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;sklearn-package&#34;&gt;Sklearn package&lt;/h3&gt;
&lt;p&gt;The automated function in &lt;code&gt;sklearn&lt;/code&gt; to persorm $K$-means clustering is &lt;code&gt;KMeans&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# SKlearn algorithm
km1 = KMeans(n_clusters=3, n_init=1, random_state=1)
km1.fit(X)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;KMeans(n_clusters=3, n_init=1, random_state=1)
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;plotting-2&#34;&gt;Plotting&lt;/h3&gt;
&lt;p&gt;We can plot the asssignment generated by the &lt;code&gt;KMeans&lt;/code&gt; function.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Plot
plot_assignment(X, km1.cluster_centers_, km1.labels_, km1.inertia_, km1.n_iter_)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/10_unsupervised_103_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;As we can see, the results are different in the two algorithms? Why? $K$-means is susceptible to the initial values. One way to solve this problem is to run the algorithm multiple times and report only the best results&lt;/p&gt;
&lt;h3 id=&#34;initial-assignment&#34;&gt;Initial Assignment&lt;/h3&gt;
&lt;p&gt;To run the &lt;code&gt;Kmeans()&lt;/code&gt; function in python with multiple initial cluster assignments, we use the &lt;code&gt;n_init&lt;/code&gt; argument (default: 10). If a value of &lt;code&gt;n_init&lt;/code&gt; greater than one is used, then K-means clustering will be performed using multiple random assignments, and the &lt;code&gt;Kmeans()&lt;/code&gt; function will report only the best results.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# 30 runs
km_30run = KMeans(n_clusters=3, n_init=30, random_state=1).fit(X)
plot_assignment(X, km_30run.cluster_centers_, km_30run.labels_, km_30run.inertia_, km_30run.n_iter_)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/10_unsupervised_107_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;best-practices&#34;&gt;Best Practices&lt;/h3&gt;
&lt;p&gt;It is generally recommended to always run K-means clustering with a large value of &lt;code&gt;n_init&lt;/code&gt;, such as 20 or 50 to avoid getting stuck in an undesirable local optimum.&lt;/p&gt;
&lt;p&gt;When performing K-means clustering, in addition to using multiple initial cluster assignments, it is also important to set a random seed using the &lt;code&gt;random_state&lt;/code&gt; parameter. This way, the initial cluster assignments can be replicated, and the K-means output will be fully reproducible.&lt;/p&gt;
&lt;h2 id=&#34;hierarchical-clustering&#34;&gt;Hierarchical Clustering&lt;/h2&gt;
&lt;p&gt;One potential disadvantage of K-means clustering is that it requires us to pre-specify the number of clusters $K$.&lt;/p&gt;
&lt;p&gt;Hierarchical clustering is an alternative approach which does not require that we commit to a particular choice of $K$.&lt;/p&gt;
&lt;h3 id=&#34;the-dendogram&#34;&gt;The Dendogram&lt;/h3&gt;
&lt;p&gt;Hierarchical clustering has an added advantage over K-means clustering in that it results in an attractive tree-based representation of the observations, called a &lt;strong&gt;dendrogram&lt;/strong&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;d = dendrogram(
        linkage(X, &amp;quot;complete&amp;quot;),
        leaf_rotation=90.,  # rotates the x axis labels
        leaf_font_size=8.,  # font size for the x axis labels
    )
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/10_unsupervised_114_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;interpretation-1&#34;&gt;Interpretation&lt;/h3&gt;
&lt;p&gt;Each leaf of the &lt;em&gt;dendrogram&lt;/em&gt; represents one observation.&lt;/p&gt;
&lt;p&gt;As we move up the tree, some leaves begin to fuse into branches. These correspond to observations that are similar to each other. As we move higher up the tree, branches themselves fuse, either with leaves or other branches. The earlier (lower in the tree) fusions occur, the more similar the groups of observations are to each other.&lt;/p&gt;
&lt;p&gt;We can use de &lt;em&gt;dendogram&lt;/em&gt; to understand how similar two observations are: we can look for the point in the tree where branches containing those two obse rvations are first fused. The height of this fusion, as measured on the vertical axis, indicates how different the two observations are. Thus, observations that fuse at the very bottom of the tree are quite similar to each other, whereas observations that fuse close to the top of the tree will tend to be quite different.&lt;/p&gt;
&lt;p&gt;The term &lt;strong&gt;hierarchical&lt;/strong&gt; refers to the fact that clusters obtained by cutting the dendrogram at a given height are necessarily nested within the clusters obtained by cutting the dendrogram at any greater height.&lt;/p&gt;
&lt;h3 id=&#34;the-hierarchical-clustering-algorithm&#34;&gt;The Hierarchical Clustering Algorithm&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Begin with $n$ observations and a measure (such as Euclidean distance) of all the $n(n − 1)/2$ pairwise dissimilarities. Treat each 2 observation as its own cluster.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;For $i=n,n−1,&amp;hellip;,2$&lt;/p&gt;
&lt;p&gt;a) Examine all pairwise inter-cluster dissimilarities among the $i$ clusters and identify the &lt;strong&gt;pair of clusters that are least dissimilar&lt;/strong&gt; (that is, most similar). Fuse these two clusters. The dissimilarity between these two clusters indicates the height in the dendrogram at which the fusion should be placed.&lt;/p&gt;
&lt;p&gt;b) Compute the new pairwise inter-cluster dissimilarities among the $i−1$ remaining clusters.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;the-linkage-function&#34;&gt;The Linkage Function&lt;/h3&gt;
&lt;p&gt;We have a concept of the dissimilarity between pairs of observations, but how do we define the dissimilarity between two clusters if one or both of the clusters contains multiple observations?&lt;/p&gt;
&lt;p&gt;The concept of dissimilarity between a pair of observations needs to be extended to a pair of groups of observations. This extension is achieved by developing the notion of &lt;strong&gt;linkage&lt;/strong&gt;, which defines the dissimilarity between two groups of observations.&lt;/p&gt;
&lt;h3 id=&#34;linkages&#34;&gt;Linkages&lt;/h3&gt;
&lt;p&gt;The four most common types of linkage are:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Complete&lt;/strong&gt;: Maximal intercluster dissimilarity. Compute all pairwise dissimilarities between the observations in cluster A and the observations in cluster B, and record the largest of these dissimilarities.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Single&lt;/strong&gt;: Minimal intercluster dissimilarity. Compute all pairwise dissimilarities between the observations in cluster A and the observations in cluster B, and record the smallest of these dissimilarities.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Average&lt;/strong&gt;: Mean intercluster dissimilarity. Compute all pairwise dissimilarities between the observations in cluster A and the observations in cluster B, and record the average of these dissimilarities.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Centroid&lt;/strong&gt;: Dissimilarity between the centroid for cluster A (a mean vector of length p) and the centroid for cluster B. Centroid linkage can result in undesirable inversions.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Average, complete, and single linkage are most popular among statisticians. Average and complete linkage are generally preferred over single linkage, as they tend to yield more balanced dendrograms. Centroid linkage is often used in genomics, but suffers from a major drawback in that an inversion can occur, whereby two clusters are fused at a height below either of the individual clusters in the dendrogram. This can lead to difficulties in visualization as well as in interpretation of the dendrogram.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Init
linkages = [hierarchy.complete(X), hierarchy.average(X), hierarchy.single(X)]
titles = [&#39;Complete Linkage&#39;, &#39;Average Linkage&#39;, &#39;Single Linkage&#39;]
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;plotting-3&#34;&gt;Plotting&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;make_new_figure_4(linkages, titles)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/10_unsupervised_126_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;For this data, both &lt;em&gt;complete&lt;/em&gt; and &lt;em&gt;average&lt;/em&gt; linkage generally separates the observations into their correct groups.&lt;/p&gt;
&lt;h2 id=&#34;gaussian-mixture-models&#34;&gt;Gaussian Mixture Models&lt;/h2&gt;
&lt;p&gt;Clustering methods such as hierarchical clustering and K-means are based on heuristics and rely primarily on finding clusters whose members are close to one another, as measured directly with the data (no probability model involved).&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Gaussian Mixture Models&lt;/em&gt; assume that the data was generated by multiple multivariate gaussian distributions. The objective of the algorithm is to recover these latent distributions.&lt;/p&gt;
&lt;p&gt;The advantages with respect to K-means are&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;a structural interpretaion of the parameters&lt;/li&gt;
&lt;li&gt;automatically generates class probabilities&lt;/li&gt;
&lt;li&gt;can generate clusters of observations that are not necessarily close to each other&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;algorithm-1&#34;&gt;Algorithm&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Randomly assign a number, from $1$ to $K$, to each of the observations. These serve as initial cluster assignments for the observations.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Iterate until the cluster assignments stop changing:&lt;/p&gt;
&lt;p&gt;a) For each of the $K$ clusters, compute its mean and variance. The main difference with K-means is that we also compute the variance matrix.&lt;/p&gt;
&lt;p&gt;b) Assign each observation to its most likely cluster.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;dataset&#34;&gt;Dataset&lt;/h3&gt;
&lt;p&gt;Let&amp;rsquo;s use the same data we have used for k-means, for a direct comparison.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;make_new_figure_1(X)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/10_unsupervised_134_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;step-1-random-assignement-1&#34;&gt;Step 1: random assignement&lt;/h3&gt;
&lt;p&gt;Let&amp;rsquo;s also use the same random assignment of the K-means algorithm.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;make_new_figure_2(X, clusters0)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/10_unsupervised_137_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;step-2-compute-distirbutions&#34;&gt;Step 2: compute distirbutions&lt;/h3&gt;
&lt;p&gt;What are the new distributions?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Compute new centroids
def compute_distributions(X, clusters):
    K = len(np.unique(clusters))
    distr = []
    for k in range(K):
        if sum(clusters==k)&amp;gt;0:
            distr += [multivariate_normal(np.mean(X[clusters==k,:], axis=0), np.cov(X[clusters==k,:].T))]
        else:
            distr += [multivariate_normal(np.mean(X, axis=0), np.cov(X.T))]
    return distr
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Print
distr0 = compute_distributions(X, clusters0)
print(&amp;quot;Mean of the first distribution: \n&amp;quot;, distr0[0].mean)
print(&amp;quot;\nVariance of the first distribution: \n&amp;quot;, distr0[0].cov)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Mean of the first distribution: 
 [ 1.54179703 -1.65922379]

Variance of the first distribution: 
 [[ 3.7160256  -2.27290036]
 [-2.27290036  4.67223237]]
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;plotting-the-distributions&#34;&gt;Plotting the distributions&lt;/h3&gt;
&lt;p&gt;Let&amp;rsquo;s add the distributions to the graph.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Plot
plot_assignment_gmm(X, clusters0, distr0, i=0, logL=0.0)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/10_unsupervised_144_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;likelihood&#34;&gt;Likelihood&lt;/h3&gt;
&lt;p&gt;The main difference with respect with K-means is that we can now compute the probability that each observation belongs to each cluster. This is the probability that each observation was generated by one of the two bi-variate normal distributions. These probabilities are called &lt;strong&gt;likelihoods&lt;/strong&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Print first 5 likelihoods
pdfs0 = np.stack([d.pdf(X) for d in distr0], axis=1)
pdfs0[:5]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;array([[0.03700522, 0.05086876],
       [0.00932081, 0.02117353],
       [0.04092453, 0.04480732],
       [0.00717854, 0.00835799],
       [0.01169199, 0.01847373]])
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;step-3-assign-data-to-clusters-1&#34;&gt;Step 3: assign data to clusters&lt;/h3&gt;
&lt;p&gt;Now we can assign the data to the clusters, via maximum likelihood.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Assign X to clusters
def assign_to_cluster_gmm(X, distr):
    pdfs = np.stack([d.pdf(X) for d in distr], axis=1)
    clusters = np.argmax(pdfs, axis=1)
    log_likelihood = 0
    for k, pdf in enumerate(pdfs):
        log_likelihood += np.log(pdf[clusters[k]])
    return clusters, log_likelihood
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Get cluster assignment
clusters1, logL1 = assign_to_cluster_gmm(X, distr0)
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;plotting-assigned-data-1&#34;&gt;Plotting assigned data&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Compute new distributions
distr1 = compute_distributions(X, clusters1)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Plot
plot_assignment_gmm(X, clusters1, distr1, 1, logL1);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/10_unsupervised_154_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;expectation---maximization&#34;&gt;Expectation - Maximization&lt;/h3&gt;
&lt;p&gt;The two steps we have just seen, are part of a broader family of algorithms to maximize likelihoods called &lt;strong&gt;expectation&lt;/strong&gt;-&lt;strong&gt;maximization&lt;/strong&gt; algorithms.&lt;/p&gt;
&lt;p&gt;In the expectation step, we computed the expectation of the parameters, given the current cluster assignment.&lt;/p&gt;
&lt;p&gt;In the maximization step, we assigned observations to the cluster that maximized the likelihood of the single observation.&lt;/p&gt;
&lt;p&gt;The alternative, and more computationally intensive procedure, would have been to specify a global likelihood function and find the mean and variance paramenters of the two normal distributions that maximized those likelihoods.&lt;/p&gt;
&lt;h3 id=&#34;full-algorithm-1&#34;&gt;Full Algorithm&lt;/h3&gt;
&lt;p&gt;We can now deploy the full algorithm.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def gmm_manual(X, K):

    # Init
    i = 0
    logL0 = 1e4
    logL1 = 1e5
    clusters = np.random.randint(K,size=(np.size(X,0)))

    # Iterate until convergence
    while np.abs(logL0-logL1) &amp;gt; 1e-10:
        logL1 = logL0
        distr = compute_distributions(X, clusters)
        clusters, logL0 = assign_to_cluster_gmm(X, distr)
        plot_assignment_gmm(X, clusters, distr, i, logL0)
        i+=1
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;plotting-k-means-clustering-1&#34;&gt;Plotting k-means clustering&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Test
gmm_manual(X, K)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/10_unsupervised_161_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;In this case, GMM does a very poor job identifying the original clusters.&lt;/p&gt;
&lt;h3 id=&#34;overlapping-clusters&#34;&gt;Overlapping Clusters&lt;/h3&gt;
&lt;p&gt;Let&amp;rsquo;s now try with a different dataset, where the data is drawn from two overlapping bi-variate gaussian distributions, forming a cross.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Simulate data
X = np.random.randn(50,2)
X[0:25, :] = np.random.multivariate_normal([0,0], [[50,0],[0,1]], size=25)
X[25:, :] = np.random.multivariate_normal([0,0], [[1,0],[0,50]], size=25)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;make_new_figure_1(X)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/10_unsupervised_166_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;gmm-with-overlapping-distributions&#34;&gt;GMM with overlapping distributions&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# GMM
gmm_manual(X, K)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/10_unsupervised_168_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;As we can see, GMM is able to correctly recover the original clusters.&lt;/p&gt;
&lt;h3 id=&#34;k-means-with-overlapping-distributions&#34;&gt;K-means with overlapping distributions&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# K-means
kmeans_manual(X, K)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/10_unsupervised_171_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;K-means generates completely different clusters.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Coding: Logit Demand</title>
      <link>https://matteocourthoud.github.io/course/empirical-io/11_logit_demand/</link>
      <pubDate>Wed, 10 Nov 2021 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/course/empirical-io/11_logit_demand/</guid>
      <description>&lt;h3 id=&#34;intro&#34;&gt;Intro&lt;/h3&gt;
&lt;p&gt;In this session, I am going to cover demand estimation.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Compute equilibrium outcomes with Logit demand&lt;/li&gt;
&lt;li&gt;Simulate a dataset&lt;/li&gt;
&lt;li&gt;Estimate Logit demand&lt;/li&gt;
&lt;li&gt;Compare different instruments&lt;/li&gt;
&lt;li&gt;Include supply&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;model&#34;&gt;Model&lt;/h3&gt;
&lt;p&gt;In this first part, we are going to assume that consumer
$i \in \lbrace1,&amp;hellip;,I\rbrace$ utility from good
$j \in \lbrace1,&amp;hellip;,J\rbrace$ in market $t \in \lbrace1,&amp;hellip;,T\rbrace$
takes the form&lt;/p&gt;
&lt;p&gt;$$
u_{ijt} = \boldsymbol x_{jt} \boldsymbol \beta_{it} - \alpha p_{jt} + \xi_{jt} + \epsilon_{ijt}
$$&lt;/p&gt;
&lt;p&gt;where&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\xi_{jt}$ is type-1 extreme value distributed&lt;/li&gt;
&lt;li&gt;$\boldsymbol \beta$ has dimension $K$
&lt;ul&gt;
&lt;li&gt;i.e. goods have $K$ characteristics&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;setup&#34;&gt;Setup&lt;/h3&gt;
&lt;p&gt;We have $J$ firms and each product has $K$ characteristics&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;J = 3;                            # 3 firms == products
K = 2;                            # 2 product characteristics
c = rand(Uniform(0, 1), J);       # Random uniform marginal costs
ξ = rand(Normal(0, 1), J+1);      # Random normal individual shocks
X = rand(Exponential(1), J, K);   # Random exponential product characteristics
β = [.5, 2, -1];                  # Preferences (last one is for prices, i.e. alpha)
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;code-demand&#34;&gt;Code Demand&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;function demand(p::Vector, X::Matrix, β::Vector, ξ::Vector)::Tuple{Vector, Number}
    &amp;quot;&amp;quot;&amp;quot;Compute demand&amp;quot;&amp;quot;&amp;quot;
    δ = 1 .+ [X p] * β              # Mean value
    u = [δ; 0] + ξ                  # Utility
    e = exp.(u)                     # Take exponential
    q = e ./ sum(e)                 # Compute demand
    return q[1:end-1], q[end]
end;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can try with an example.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;p = 2 .* c;
demand(p, X, β, ξ)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## ([0.4120077746005573, 0.26650568009936, 0.24027826270165709], 0.08120828259842561)
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;code-supply&#34;&gt;Code Supply&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;function profits(p::Vector, c::Vector, X::Matrix, β::Vector, ξ::Vector)::Vector
    &amp;quot;&amp;quot;&amp;quot;Compute profits&amp;quot;&amp;quot;&amp;quot;
    q, _ = demand(p, X, β, ξ)       # Compute demand
    pr = (p - c) .* q               # Compute profits
    return pr
end;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can try with an example.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;profits(p, c, X, β, ξ)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 3-element Array{Float64,1}:
##  0.20289186252172428
##  0.1596422305025479
##  0.1270874470740512
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;code-best-reply&#34;&gt;Code Best Reply&lt;/h3&gt;
&lt;p&gt;We first code the best reply of firm $j$&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;function profits_j(pj::Number, j::Int, p::Vector, c::Vector, X::Matrix, β::Vector, ξ::Vector)::Number
    &amp;quot;&amp;quot;&amp;quot;Compute profits of firm j&amp;quot;&amp;quot;&amp;quot;
    p[j] = pj                       # Insert price of firm j
    pr = profits(p, c, X, β, ξ)     # Compute profits
    return pr[j]
end;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s test it.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;j = 1;
obj_fun(pj) = - profits_j(pj[1], j, copy(p), c, X, β, ξ);
pj = optimize(x -&amp;gt; obj_fun(x), [1.0], LBFGS()).minimizer[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 1.8019637881982011
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What are the implied profits now?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;print(&amp;quot;Profits old: &amp;quot;,  round.(profits(p, c, X, β, ξ), digits=4))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Profits old: [0.2029, 0.1596, 0.1271]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;p_new = copy(p);
p_new[j] = pj;
print(&amp;quot;Profits new: &amp;quot;,  round.(profits(p_new, c, X, β, ξ), digits=4))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Profits new: [0.3095, 0.2073, 0.1651]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Indeed firm 1 has increased its profits.&lt;/p&gt;
&lt;h3 id=&#34;code-equilibrium&#34;&gt;Code Equilibrium&lt;/h3&gt;
&lt;p&gt;We can now compute equilibrium prices&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;function equilibrium(c::Vector, X::Matrix, β::Vector, ξ::Vector)::Vector
    &amp;quot;&amp;quot;&amp;quot;Compute equilibrium prices and profits&amp;quot;&amp;quot;&amp;quot;
    p = 2 .* c;
    dist = 1;
    iter = 0;

    # Until convergence
    while (dist &amp;gt; 1e-8) &amp;amp;&amp;amp; (iter&amp;lt;1000)

        # Compute best reply for each firm
        p1 = copy(p);
        for j=1:length(p)
            obj_fun(pj) = - profits_j(pj[1], j, p, c, X, β, ξ);
            optimize(x -&amp;gt; obj_fun(x), [1.0], LBFGS()).minimizer[1];
        end

        # Update distance
        dist = max(abs.(p - p1)...);
        iter += 1;
    end
    return p
end;
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;code-equilibrium-1&#34;&gt;Code Equilibrium&lt;/h3&gt;
&lt;p&gt;Let’s test it&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# Compute equilibrium prices
p_eq = equilibrium(c, X, β, ξ);
print(&amp;quot;Equilibrium prices: &amp;quot;,  round.(p_eq, digits=4))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Equilibrium prices: [1.9764, 1.9602, 1.8366]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# And profits
pi_eq = profits(p_eq, c, X, β, ξ);
print(&amp;quot;Equilibrium profits: &amp;quot;,  round.(pi_eq, digits=4))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Equilibrium profits: [0.484, 0.3612, 0.3077]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As expected the prices of the first 2 firms are lower and their profits
are higher.&lt;/p&gt;
&lt;h3 id=&#34;dgp&#34;&gt;DGP&lt;/h3&gt;
&lt;p&gt;Let’s generate our Data Generating Process (DGP).&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\boldsymbol x \sim exp(V_{x})$&lt;/li&gt;
&lt;li&gt;$\xi \sim N(0, V_{\xi})$&lt;/li&gt;
&lt;li&gt;$w \sim N(0, 1)$&lt;/li&gt;
&lt;li&gt;$\omega \sim N(0, 1)$&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;function draw_data(J::Int, K::Int, rangeJ::Vector, varX::Number, varξ::Number)::Tuple
    &amp;quot;&amp;quot;&amp;quot;Draw data for one market&amp;quot;&amp;quot;&amp;quot;
    J_ = rand(rangeJ[1]:rangeJ[2])              # Number of firms (products)
    X_ = rand(Exponential(varX), J_, K)         # Product characteristics
    ξ_ = rand(Normal(0, varξ), J_+1)            # Product-level utility shocks
    w_ = rand(Uniform(0, 1), J_)                # Cost shifters
    ω_ = rand(Uniform(0, 1), J_)                # Cost shocks
    c_ = w_ + ω_                                # Cost
    j_ = sort(sample(1:J, J_, replace=false))   # Subset of firms
    return X_, ξ_, w_, c_, j_
end;
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;equilibrium&#34;&gt;Equilibrium&lt;/h3&gt;
&lt;p&gt;We first compute the equilibrium in one market.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;function compute_mkt_eq(J::Int, b::Vector, rangeJ::Vector, varX::Number, varξ::Number)::DataFrame
    &amp;quot;&amp;quot;&amp;quot;Compute equilibrium one market&amp;quot;&amp;quot;&amp;quot;

    # Initialize variables
    K = size(β, 1) - 1
    X_, ξ_, w_, c_, j_ = draw_data(J, K, rangeJ, varX, varξ)

    # Compute equilibrium
    p_ = equilibrium(c_, X_, β, ξ_)      # Equilibrium prices
    q_, q0 = demand(p_, X_, β, ξ_)       # Demand with shocks
    pr_ = (p_ - c_) .* q_               # Profits

    # Save to data
    q0_ = ones(length(j_)) .* q0
    df = DataFrame(j=j_, w=w_, p=p_, q=q_, q0=q0_, pr=pr_)
    for k=1:K
      df[!,&amp;quot;x$k&amp;quot;] = X_[:,k]
      df[!,&amp;quot;z$k&amp;quot;] = sum(X_[:,k]) .- X_[:,k]
    end
    return df
end;
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;simulate-dataset&#34;&gt;Simulate Dataset&lt;/h3&gt;
&lt;p&gt;We can now write the code to simulate the whole dataset.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;function simulate_data(J::Int, b::Vector, T::Int, rangeJ::Vector, varX::Number, varξ::Number)
    &amp;quot;&amp;quot;&amp;quot;Simulate full dataset&amp;quot;&amp;quot;&amp;quot;
    df = compute_mkt_eq(J, β, rangeJ, varX, varξ)
    df[!, &amp;quot;t&amp;quot;] = ones(nrow(df)) * 1
    for t=2:T
        df_temp = compute_mkt_eq(J, β, rangeJ, varX, varξ)
        df_temp[!, &amp;quot;t&amp;quot;] = ones(nrow(df_temp)) * t
        append!(df, df_temp)
    end
    CSV.write(&amp;quot;../data/logit.csv&amp;quot;, df)
end;
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;simulate-dataset-2&#34;&gt;Simulate Dataset (2)&lt;/h3&gt;
&lt;p&gt;We generate the dataset by simulating many markets that differ by&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;number of firms (and their identity)&lt;/li&gt;
&lt;li&gt;their marginal costs&lt;/li&gt;
&lt;li&gt;their product characteristics&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# Set parameters
J = 10;                 # Number of firms
K = 2;                  # Product caracteristics
T = 500;                # Markets
β = [.5, 2, -1];        # Preferences
rangeJ = [2, 6];        # Min and max firms per market
varX = 1;               # Variance of X
varξ = 2;               # Variance of xi

# Simulate
df = simulate_data(J, β, T, rangeJ, varX, varξ);
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;the-data&#34;&gt;The Data&lt;/h3&gt;
&lt;p&gt;What does the data look like? Let’s switch to R!&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Read data
df = fread(&amp;quot;../data/logit.csv&amp;quot;)
kable(df[1:6,], digits=4)
&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:right&#34;&gt;j&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;w&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;p&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;q&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;q0&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;pr&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;x1&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;z1&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;x2&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;z2&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;t&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right&#34;&gt;1&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;0.1491&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;1.9616&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;0.0932&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;0.0013&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;0.1028&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;2.4517&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;0.8219&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;1.6918&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;3.6779&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right&#34;&gt;2&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;0.8352&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;2.1112&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;0.0193&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;0.0013&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;0.0197&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;0.1328&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;3.1408&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;0.2075&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;5.1622&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right&#34;&gt;7&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;0.2749&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;2.2789&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;0.2710&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;0.0013&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;0.3717&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;0.1449&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;3.1287&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;1.1493&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;4.2205&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right&#34;&gt;10&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;0.4118&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;3.6386&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;0.6151&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;0.0013&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;1.5982&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;0.5442&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;2.7294&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;2.3212&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;3.0485&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right&#34;&gt;5&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;0.6071&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;2.1457&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;0.0886&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;0.0003&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;0.0972&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;0.9239&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;2.4182&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;3.6551&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;10.0474&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right&#34;&gt;6&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;0.1615&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;1.1626&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;0.0000&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;0.0003&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;0.0000&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;0.1763&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;3.1657&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;0.3629&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;13.3396&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id=&#34;estimation&#34;&gt;Estimation&lt;/h3&gt;
&lt;p&gt;First we need to compute the dependent variable&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;df$y = log(df$q) - log(df$q0)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we can estimate the logit model. The true values are $alpha=1$.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;ols &amp;lt;- lm(y ~ x1 + x2 + p, data=df)
kable(tidy(ols), digits=4)
&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left&#34;&gt;term&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;estimate&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;std.error&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;statistic&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;p.value&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;(Intercept)&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;-1.3558&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;0.1476&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;-9.1874&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;0e+00&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;x1&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;0.4176&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;0.0537&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;7.7782&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;0e+00&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;x2&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;1.1494&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;0.0719&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;15.9903&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;0e+00&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;p&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;0.2406&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;0.0656&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;3.6664&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;3e-04&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The estimate of $\alpha = 1$ is biased (positive and significant) since
$p$ is endogenous. We need instruments.&lt;/p&gt;
&lt;h3 id=&#34;iv-1-cost-shifters&#34;&gt;IV 1: Cost Shifters&lt;/h3&gt;
&lt;p&gt;First set of instruments: &lt;strong&gt;cost shifters&lt;/strong&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;fm_costiv &amp;lt;- ivreg(y ~ x1 + x2 + p | x1 + x2 + w, data=df)
kable(tidy(fm_costiv), digits=4)
&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left&#34;&gt;term&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;estimate&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;std.error&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;statistic&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;p.value&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;(Intercept)&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;0.2698&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;0.4923&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;0.5480&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;0.5837&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;x1&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;0.5249&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;0.0643&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;8.1679&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;0.0000&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;x2&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;1.8178&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;0.2064&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;8.8059&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;0.0000&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;p&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;-0.7034&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;0.2800&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;-2.5123&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;0.0121&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Now the estimate of $\alpha$ is negative and significant.&lt;/p&gt;
&lt;h3 id=&#34;iv-2-blp-instruments&#34;&gt;IV 2: BLP Instruments&lt;/h3&gt;
&lt;p&gt;Second set of instruments: &lt;strong&gt;product characteristics of other firms in
the same market&lt;/strong&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;fm_blpiv &amp;lt;- ivreg(y ~ x1 + x2 + p | x1 + x2 + z1 + z2, data=df)
kable(tidy(fm_blpiv), digits=4)
&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left&#34;&gt;term&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;estimate&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;std.error&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;statistic&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;p.value&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;(Intercept)&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;1.6616&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;0.5014&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;3.3139&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;9e-04&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;x1&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;0.6167&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;0.0698&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;8.8380&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;0e+00&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;x2&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;2.3901&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;0.2110&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;11.3279&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;0e+00&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;p&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;-1.5117&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;0.2840&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;-5.3221&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;0e+00&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Also the BLP instruments deliver an estimate of $\alpha$ is negative and
significant.&lt;/p&gt;
&lt;h2 id=&#34;appendix&#34;&gt;Appendix&lt;/h2&gt;
&lt;h3 id=&#34;references&#34;&gt;References&lt;/h3&gt;
</description>
    </item>
    
    <item>
      <title>Coding: BLP (1995)</title>
      <link>https://matteocourthoud.github.io/course/empirical-io/12_blp_1995/</link>
      <pubDate>Fri, 29 Oct 2021 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/course/empirical-io/12_blp_1995/</guid>
      <description>&lt;h3 id=&#34;intro&#34;&gt;Intro&lt;/h3&gt;
&lt;p&gt;In this session, I am going to cover demand estimation.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Compute equilibrium outcomes with RCL demand&lt;/li&gt;
&lt;li&gt;Simulate market-level data
&lt;ul&gt;
&lt;li&gt;Extremely similar to the logit demand simulation&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Build the BLP estimator from Berry, Levinsohn, and Pakes
(&lt;a href=&#34;#ref-berry1995automobile&#34;&gt;1995&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;model&#34;&gt;Model&lt;/h3&gt;
&lt;p&gt;In this first part, we are going to assume that consumer
$i \in \lbrace1,&amp;hellip;,I\rbrace$ utility from good
$j \in \lbrace1,&amp;hellip;,J\rbrace$ in market $t \in \lbrace1,&amp;hellip;,T\rbrace$
takes the form&lt;/p&gt;
&lt;p&gt;$$
u_{ijt} = \boldsymbol x_{jt} \boldsymbol \beta_{it} - \alpha p_{jt} + \xi_{jt} + \epsilon_{ijt}
$$&lt;/p&gt;
&lt;p&gt;where&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\xi_{jt}$ is type-1 extreme value distributed&lt;/li&gt;
&lt;li&gt;$\boldsymbol \beta_{it}$: has dimension $K$
$$\beta_{it}^k = \beta_0^k + \sigma_k \zeta_{it}^k$$
&lt;ul&gt;
&lt;li&gt;$\beta_0^k$: fixed taste for characteristic $k$ (the usual
$\beta$)&lt;/li&gt;
&lt;li&gt;$\zeta_{it}^k$: random taste, i.i.d. across consumers and
markets $t$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;setup&#34;&gt;Setup&lt;/h3&gt;
&lt;p&gt;We have $J$ firms and each product has $K$ characteristics.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;i = 100;                # Number of consumers
J = 10;                 # Number of firms
K = 2;                  # Product characteristics
T = 100;                # Number of markets
β = [.5, 2, -1];        # Preferences
varζ = 5;               # Variance of the random taste
rangeJ = [2, 6];        # Min and max firms per market
varX = 1;               # Variance of X
varξ = 2;               # Variance of xi
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;demand&#34;&gt;Demand&lt;/h3&gt;
&lt;p&gt;Demand is the main difference w.r.t. the logit model. Now we have
individual shocks $\zeta$ we have to integrate over.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;function demand(p::Vector, X::Matrix, β::Vector, ξ::Matrix, ζ::Matrix)::Tuple{Vector, Number}
    &amp;quot;&amp;quot;&amp;quot;Compute demand&amp;quot;&amp;quot;&amp;quot;
    δ = [X p] * (β .+ ζ)                    # Mean value
    δ0 = zeros(1, size(ζ, 2))               # Mean value of the outside option
    u = [δ; δ0] + ξ                         # Utility
    e = exp.(u)                             # Take exponential
    q = mean(e ./ sum(e, dims=1), dims=2)   # Compute demand
    return q[1:end-1], q[end]
end;
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;supply&#34;&gt;Supply&lt;/h3&gt;
&lt;p&gt;Computing profits is instead exactly the same as before. We just have to
save the shocks $\zeta$ to be sure demand is stable.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;function profits(p::Vector, c::Vector, X::Matrix, β::Vector, ξ::Matrix, ζ::Matrix)::Vector
    &amp;quot;&amp;quot;&amp;quot;Compute profits&amp;quot;&amp;quot;&amp;quot;
    q, _ = demand(p, X, β, ξ, ζ)            # Compute demand
    pr = (p - c) .* q                       # Compute profits
    return pr
end;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;function profits_j(pj::Number, j::Int, p::Vector, c::Vector, X::Matrix, β::Vector, ξ::Matrix, ζ::Matrix)::Number
    &amp;quot;&amp;quot;&amp;quot;Compute profits of firm j&amp;quot;&amp;quot;&amp;quot;
    p[j] = pj                               # Insert price of firm j
    pr = profits(p, c, X, β, ξ, ζ)          # Compute profits
    return pr[j]
end;
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;equilibrium&#34;&gt;Equilibrium&lt;/h3&gt;
&lt;p&gt;We can now compute the equilibrium for a specific market, as before.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;function equilibrium(c::Vector, X::Matrix, β::Vector, ξ::Matrix, ζ::Matrix)::Vector
    &amp;quot;&amp;quot;&amp;quot;Compute equilibrium prices and profits&amp;quot;&amp;quot;&amp;quot;
    p = 2 .* c;
    dist = 1;
    iter = 0;

    # Iterate until convergence
    while (dist &amp;gt; 1e-8) &amp;amp;&amp;amp; (iter&amp;lt;1000)

        # Compute best reply for each firm
        p_old = copy(p);
        for j=1:length(p)
            obj_fun(pj) = - profits_j(pj[1], j, p, c, X, β, ξ, ζ);
            optimize(x -&amp;gt; obj_fun(x), [1.0], LBFGS());
        end

        # Update distance
        dist = max(abs.(p - p_old)...);
        iter += 1;
    end
    return p
end;
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;simulating-data&#34;&gt;Simulating Data&lt;/h3&gt;
&lt;p&gt;We are now ready to simulate the data, i.e. equilibrium outcomes across
different markets. We first draw all the variables.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;function draw_data(I::Int, J::Int, K::Int, rangeJ::Vector, varζ::Number, varX::Number, varξ::Number)::Tuple
    &amp;quot;&amp;quot;&amp;quot;Draw data for one market&amp;quot;&amp;quot;&amp;quot;
    J_ = rand(rangeJ[1]:rangeJ[2])              # Number of firms (products)
    X_ = rand(Exponential(varX), J_, K)         # Product characteristics
    ξ_ = rand(Normal(0, varξ), J_+1, I)         # Product-level utility shocks
    # Consumer-product-level preference shocks
    ζ_ = [rand(Normal(0,1), 1, I) * varζ; zeros(K,I)]
    w_ = rand(Uniform(0, 1), J_)                # Cost shifters
    ω_ = rand(Uniform(0, 1), J_)                # Cost shocks
    c_ = w_ + ω_                                # Cost
    j_ = sort(sample(1:J, J_, replace=false))   # Subset of firms
    return X_, ξ_, ζ_, w_, c_, j_
end;
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;simulating-data-1&#34;&gt;Simulating Data&lt;/h3&gt;
&lt;p&gt;Then we simulate the data for one market.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;function compute_mkt_eq(I::Int, J::Int, β::Vector, rangeJ::Vector, varζ::Number, varX::Number, varξ::Number)::DataFrame
    &amp;quot;&amp;quot;&amp;quot;Compute equilibrium one market&amp;quot;&amp;quot;&amp;quot;

    # Initialize variables
    K = size(β, 1) - 1
    X_, ξ_, ζ_, w_, c_, j_ = draw_data(I, J, K, rangeJ, varζ, varX, varξ)

    # Compute equilibrium
    p_ = equilibrium(c_, X_, β, ξ_, ζ_)    # Equilibrium prices
    q_, q0 = demand(p_, X_, β, ξ_, ζ_)     # Demand with shocks
    pr_ = (p_ - c_) .* q_                       # Profits

    # Save to data
    q0_ = ones(length(j_)) .* q0
    df = DataFrame(j=j_, w=w_, p=p_, q=q_, q0=q0_, pr=pr_)
    for k=1:K
      df[!,&amp;quot;x$k&amp;quot;] = X_[:,k]
      df[!,&amp;quot;z$k&amp;quot;] = sum(X_[:,k]) .- X_[:,k]
    end
    return df
end;
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;simultate-the-data-2&#34;&gt;Simultate the Data (2)&lt;/h3&gt;
&lt;p&gt;We repeat for $T$ markets.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;function simulate_data(I::Int, J::Int, β::Vector, T::Int, rangeJ::Vector, varζ::Number, varX::Number, varξ::Number)
    &amp;quot;&amp;quot;&amp;quot;Simulate full dataset&amp;quot;&amp;quot;&amp;quot;
    df = compute_mkt_eq(I, J, β, rangeJ, varζ, varX, varξ)
    df[!, &amp;quot;t&amp;quot;] = ones(nrow(df)) * 1
    for t=2:T
        df_temp = compute_mkt_eq(I, J, β, rangeJ, varζ, varX, varξ)
        df_temp[!, &amp;quot;t&amp;quot;] = ones(nrow(df_temp)) * t
        append!(df, df_temp)
    end
    CSV.write(&amp;quot;../data/blp.csv&amp;quot;, df)
    return df
end;
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;simulate-the-data-3&#34;&gt;Simulate the Data (3)&lt;/h3&gt;
&lt;p&gt;Now let’s run the code&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# Simulate
df = simulate_data(i, J, β, T, rangeJ, varζ, varX, varξ);
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;the-data&#34;&gt;The Data&lt;/h3&gt;
&lt;p&gt;What does the data look like? Let’s switch to R!&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Read data
df = fread(&amp;quot;../data/blp.csv&amp;quot;)
kable(df[1:6,], digits=4)
&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:right&#34;&gt;j&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;w&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;p&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;q&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;q0&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;pr&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;x1&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;z1&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;x2&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;z2&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;t&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right&#34;&gt;4&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;0.6481&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;3.5918&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;0.2929&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;0.4558&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;0.8165&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;0.6531&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;1.0002&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;1.7063&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;1.8207&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right&#34;&gt;7&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;0.9997&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;5.1926&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;0.2513&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;0.4558&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;0.8717&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;1.0002&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;0.6531&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;1.8207&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;1.7063&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right&#34;&gt;1&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;0.5842&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;2.6999&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;0.0800&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;0.3919&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;0.1572&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;0.3591&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;7.1958&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;0.4217&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;2.1996&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right&#34;&gt;4&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;0.5291&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;4.5934&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;0.1404&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;0.3919&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;0.4467&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;2.3801&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;5.1748&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;0.1283&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;2.4929&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right&#34;&gt;5&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;0.5012&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;4.4196&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;0.1368&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;0.3919&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;0.4461&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;2.2638&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;5.2911&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;0.4408&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;2.1804&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right&#34;&gt;8&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;0.9359&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;3.2923&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;0.0863&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;0.3919&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;0.1477&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;0.5182&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;7.0367&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;1.0271&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;1.5942&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id=&#34;estimation&#34;&gt;Estimation&lt;/h3&gt;
&lt;p&gt;The BLP estimation procedure&lt;/p&gt;
&lt;h3 id=&#34;from-deltas-to-shares&#34;&gt;From deltas to shares&lt;/h3&gt;
&lt;p&gt;First, we need to compute the shares implied by aspecific vector of
$\delta$s&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;function implied_shares(Xt_::Matrix, ζt_::Matrix, δt_::Vector, δ0::Matrix)::Vector
    &amp;quot;&amp;quot;&amp;quot;Compute shares implied by deltas and shocks&amp;quot;&amp;quot;&amp;quot;
    u = [δt_ .+ (Xt_ * ζt_); δ0]                  # Utility
    e = exp.(u)                                 # Take exponential
    q = mean(e ./ sum(e, dims=1), dims=2)       # Compute demand
    return q[1:end-1]
end;
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;inner-loop&#34;&gt;Inner Loop&lt;/h3&gt;
&lt;p&gt;We can now compute the inner loop and invert the demand function: from
shares $q$ to $\delta$s&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;function inner_loop(qt_::Vector, Xt_::Matrix, ζt_::Matrix)::Vector
    &amp;quot;&amp;quot;&amp;quot;Solve the inner loop: compute delta, given the shares&amp;quot;&amp;quot;&amp;quot;
    δt_ = ones(size(qt_))
    δ0 = zeros(1, size(ζt_, 2))
    dist = 1

    # Iterate until convergence
    while (dist &amp;gt; 1e-8)
        q = implied_shares(Xt_, ζt_, δt_, δ0)
        δt2_ = δt_ + log.(qt_) - log.(q)
        dist = max(abs.(δt2_ - δt_)...)
        δt_ = δt2_
    end
    return δt_
end;
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;compute-delta&#34;&gt;Compute Delta&lt;/h3&gt;
&lt;p&gt;We can now repeat the inversion for every market and get the vector of
mean utilities $\delta$s from the observed market shares $q$.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;function compute_delta(q_::Vector, X_::Matrix, ζ_::Matrix, T::Vector)::Vector
    &amp;quot;&amp;quot;&amp;quot;Compute residuals&amp;quot;&amp;quot;&amp;quot;
    δ_ = zeros(size(T))

    # Loop over each market
    for t in unique(T)
        qt_ = q_[T.==t]                             # Quantity in market t
        Xt_ = X_[T.==t,:]                           # Characteristics in mkt t
        δ_[T.==t] = inner_loop(qt_, Xt_, ζ_)        # Solve inner loop
    end
    return δ_
end;
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;compute-xi&#34;&gt;Compute Xi&lt;/h3&gt;
&lt;p&gt;Now that we have $\delta$, it is pretty straightforward to compute
$\xi$. We just need to perform a linear regression (with instruments) of
mean utilities $\delta$ on prices $p$ and product characteristics $X$
and compute the residuals $\xi$.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;function compute_xi(X_::Matrix, IV_::Matrix, δ_::Vector)::Tuple
    &amp;quot;&amp;quot;&amp;quot;Compute residual, given delta (IV)&amp;quot;&amp;quot;&amp;quot;
    β_ = inv(IV_&#39; * X_) * (IV_&#39; * δ_)           # Compute coefficients (IV)
    ξ_ = δ_ - X_ * β_                           # Compute errors
    return ξ_, β_
end;
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;objective-function&#34;&gt;Objective Function&lt;/h3&gt;
&lt;p&gt;We now have all the ingredients to set up the GMM objective function.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;function GMM(varζ_::Number)::Tuple
    &amp;quot;&amp;quot;&amp;quot;Compute GMM objective function&amp;quot;&amp;quot;&amp;quot;
    δ_ = compute_delta(q_, X_, ζ_ * varζ_, T)   # Compute deltas
    ξ_, β_ = compute_xi(X_, IV_, δ_)            # Compute residuals
    gmm = ξ_&#39; * Z_ * Z_&#39; * ξ_ / length(ξ_)^2    # Compute ortogonality condition
    return gmm, β_
end;
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;estimation-1&#34;&gt;Estimation (1)&lt;/h3&gt;
&lt;p&gt;First, we need to set up our objects&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# Retrieve data
T = Int.(df.t)
X_ = [df.x1 df.x2 df.p]
q_ = df.q
q0_ = df.q0
IV_ = [df.x1 df.x2 df.w]
Z_ = [df.x1 df.x2 df.z1 df.z2]
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;estimation-2&#34;&gt;Estimation (2)&lt;/h3&gt;
&lt;p&gt;What would a logit regression estimate?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# Compute logit estimate
y = log.(df.q) - log.(df.q0);
β_logit = inv(IV_&#39; * X_) * (IV_&#39; * y);
print(&amp;quot;Estimated logit coefficients: $β_logit&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Estimated logit coefficients: [2.063144844221613, 1.2888511782561123, -0.9824308271558686]
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;estimation-3&#34;&gt;Estimation (3)&lt;/h3&gt;
&lt;p&gt;We can now run the BLP machinery&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# Draw shocks (less)
ζ_ = [rand(Normal(0,1), 1, i); zeros(K, i)];

# Minimize GMM objective function
varζ_ = optimize(x -&amp;gt; GMM(x[1])[1], [2.0], LBFGS()).minimizer[1];
β_blp = GMM(varζ_)[2];
print(&amp;quot;Estimated BLP coefficients: $β_blp&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Estimated BLP coefficients: [0.549234645269979, 1.1243451127088748, -0.6229637255651461]
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;appendix&#34;&gt;Appendix&lt;/h2&gt;
&lt;h3 id=&#34;references-references&#34;&gt;References [references]&lt;/h3&gt;
&lt;div id=&#34;refs&#34; class=&#34;references csl-bib-body hanging-indent&#34;
markdown=&#34;1&#34;&gt;
&lt;div id=&#34;ref-berry1995automobile&#34; class=&#34;csl-entry&#34; markdown=&#34;1&#34;&gt;
&lt;p&gt;Berry, Steven, James Levinsohn, and Ariel Pakes. 1995. “Automobile
Prices in Market Equilibrium.” &lt;em&gt;Econometrica: Journal of the Econometric
Society&lt;/em&gt;, 841–90.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Coding: Rust (1987)</title>
      <link>https://matteocourthoud.github.io/course/empirical-io/17_rust_1987/</link>
      <pubDate>Fri, 29 Oct 2021 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/course/empirical-io/17_rust_1987/</guid>
      <description>&lt;h3 id=&#34;setting&#34;&gt;Setting&lt;/h3&gt;
&lt;p&gt;From Rust (&lt;a href=&#34;#ref-rust1988maximum&#34;&gt;1988&lt;/a&gt;)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;An agent owns a fleet to buses&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Buses get old over time&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The older the bus is, the most costly it is to maintain&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The agent can decide to replace the bus engine with a new one, at a
cost&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Dynamic trade-off&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;What is the best moment to replace the engine?&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;You don’t want to replace an engine too early&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;doesn’t change much&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;You don’t want to replace an engine too late&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;avoid unnecessary maintenance costs&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;state&#34;&gt;State&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;State&lt;/strong&gt;: mileage of the bus&lt;/p&gt;
&lt;p&gt;$$s_t \in \lbrace 1, &amp;hellip;, 10 \rbrace $$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;State transitions&lt;/strong&gt;: with probability $\lambda$ the mileage of the
bus increases&lt;/p&gt;
&lt;p&gt;$$
s_{t+1} = \begin{cases}
\min \lbrace s_t + 1,10 \rbrace  &amp;amp; \text { with probability } \lambda \newline
s_t &amp;amp; \text { with probability } 1 - \lambda
\end{cases}
$$&lt;/p&gt;
&lt;p&gt;Note that $\lambda$ does not depend on the value of the state&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;actions&#34;&gt;Actions&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Action&lt;/strong&gt;: replacement decision $$
a_t \in \lbrace 0, 1 \rbrace
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Payoffs&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Per-period maintenance cost&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Cost of replacement $$
u\left(s_{t}, a_{t}, \epsilon_{1 t}, \epsilon_{2 t} ; \theta\right)=
\begin{cases}
-\theta_{1} s_{t}-\theta_{2} s_{t}^{2}+\epsilon_{0 t}, &amp;amp; \text { if } a_{t}=0 \newline
-\theta_{3} + \epsilon_{1t}, &amp;amp; \text { if } a_{t}=1
\end{cases}
$$&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;solving-the-model&#34;&gt;Solving the Model&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Start with an initial expected value function $V(s_t)=0$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Compute the alternative-specific value function $$
\bar V(s_t) = \begin{cases}
-\theta_1 s_t - \theta_2 s_t^2 + \beta \Big[(1-\lambda) V(s_t) + \lambda V(\min \lbrace s_t+1,10 \rbrace ) \Big] , &amp;amp; \text { if } a_t=0 \newline
-\theta_3 + \beta \Big[(1-\lambda) V(0) + \lambda V(1) \Big] , &amp;amp; \text { if } a_t=1
\end{cases}
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Compute the new expected value function $$
V&amp;rsquo;(a_t) = \log \Big( e^{\bar V(a_t|s_t=0)} + e^{\bar V(a_t|s_t=1)} \Big)
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Repeat until convergence&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;code&#34;&gt;Code&lt;/h3&gt;
&lt;p&gt;First we set the parameter values.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# Set parameters
θ = [0.13; -0.004; 3.1];
λ = 0.82;
β = 0.95;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then we set the state space.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# State space
k = 10;
s = Vector(1:k);
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;static-utility&#34;&gt;Static Utility&lt;/h3&gt;
&lt;p&gt;First, we can compute static utility. $$
u\left(s_{t}, a_{t}, \epsilon_{1 t}, \epsilon_{2 t} ; \theta\right)=
\begin{cases}
-\theta_{1} s_{t}-\theta_{2} s_{t}^{2}+\epsilon_{0 t}, &amp;amp; \text { if } a_{t}=0 \newline
-\theta_{3} + \epsilon_{1 t}, &amp;amp; \text { if } a_{t}=1
\end{cases}
$$&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;function compute_U(θ::Vector, s::Vector)::Matrix
    &amp;quot;&amp;quot;&amp;quot;Compute static utility&amp;quot;&amp;quot;&amp;quot;
    u1 = - θ[1]*s - θ[2]*s.^2       # Utility of not investing
    u2 = - θ[3]*ones(size(s))       # Utility of investing
    U = [u1 u2]                     # Combine in a matrix
    return U
end;
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;value-function&#34;&gt;Value Function&lt;/h3&gt;
&lt;p&gt;We can now set up the value function iteration&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;function compute_Vbar(θ::Vector, λ::Number, β::Number, s::Vector)::Matrix
    &amp;quot;&amp;quot;&amp;quot;Compute value function by Bellman iteration&amp;quot;&amp;quot;&amp;quot;
    k = length(s)                                 # Dimension of the state space
    U = compute_U(θ, s)                           # Static utility
    index_λ = Int[1:k [2:k; k]];                  # Mileage index
    index_A = Int[1:k ones(k,1)];                 # Investment index
    γ = Base.MathConstants.eulergamma             # Euler&#39;s gamma

    # Iterate the Bellman equation until convergence
    Vbar = zeros(k, 2);
    Vbar1 = Vbar;
    dist = 1;
    iter = 0;
    while dist&amp;gt;1e-8
        V = γ .+ log.(sum(exp.(Vbar), dims=2))     # Compute value
        expV = V[index_λ] * [1-λ; λ]               # Compute expected value
        Vbar1 =  U + β * expV[index_A]             # Compute v-specific
        dist = max(abs.(Vbar1 - Vbar)...);         # Check distance
        iter += 1;
        Vbar = Vbar1                               # Update value function
    end
    return Vbar
end;
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;solving-the-model-1&#34;&gt;Solving the Model&lt;/h3&gt;
&lt;p&gt;We can now solve for the value function.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# Compute value function
V_bar = compute_Vbar(θ, λ, β, s);
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;dgp&#34;&gt;DGP&lt;/h3&gt;
&lt;p&gt;Now that we know how to compute the equilibrium, we can simulate the
data.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;function generate_data(θ::Vector, λ::Number, β::Number, s::Vector, N::Int)::Tuple
    &amp;quot;&amp;quot;&amp;quot;Generate data from primitives&amp;quot;&amp;quot;&amp;quot;
    Vbar = compute_Vbar(θ, λ, β, s)             # Solve model
    ε = rand(Gumbel(0,1), N, 2)                 # Draw shocks
    St = rand(s, N)                             # Draw states
    A = (((Vbar[St,:] + ε) * [-1;1]) .&amp;gt; 0)      # Compute investment decisions
    δ = (rand(Uniform(0,1), N) .&amp;lt; λ)            # Compute mileage shock
    St1 = min.(St .* (A.==0) + δ, max(s...))    # Compute neSr state
    df = DataFrame(St=St, A=A, St1=St1)         # Dataframe
    CSV.write(&amp;quot;../data/rust.csv&amp;quot;, df)
    return St, A, St1
end;
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;generate-the-data&#34;&gt;Generate the DAta&lt;/h3&gt;
&lt;p&gt;We can now generate the data&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# Generate data
N = Int(1e5);
St, A, St1 = generate_data(θ, λ, β, s, N);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;How many investment decisions do we observe?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;print(&amp;quot;we observe &amp;quot;, sum(A), &amp;quot; investment decisions in &amp;quot;, N, &amp;quot; observations&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## we observe 19207 investment decisions in 100000 observations
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;the-data&#34;&gt;The Data&lt;/h3&gt;
&lt;p&gt;What does the data look like?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Read data
df = fread(&amp;quot;../data/rust.csv&amp;quot;)
kable(df[1:6,], digits=4)
&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:right&#34;&gt;St&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;A&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;St1&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right&#34;&gt;3&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;FALSE&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;4&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right&#34;&gt;9&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;TRUE&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right&#34;&gt;8&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;TRUE&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right&#34;&gt;3&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;FALSE&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;4&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right&#34;&gt;9&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;FALSE&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;10&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right&#34;&gt;10&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;FALSE&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;10&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id=&#34;estimation---lambda&#34;&gt;Estimation - Lambda&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;First we can estimate the value of lambda as the probability of
mileage increase&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Conditional on not investing&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;And not being in the last state (mileage cannot increase any more)&lt;/p&gt;
&lt;p&gt;$$
\hat \lambda = \mathbb E_n \Big[ (s_{t+1}-s_t) \mid a_{t}=0 \wedge s_{t}&amp;lt;10 \Big]
$$&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# Estimate lambda
Δ = St1 - St;
λ_ = mean(Δ[(A.==0) .&amp;amp; (St.&amp;lt;10)]);

print(&amp;quot;Estimated lambda: $λ_ (true = $λ)&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Estimated lambda: 0.8206570869594549 (true = 0.82)
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;estimation---theta&#34;&gt;Estimation - Theta&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Take a parameter guess $\theta_0$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Compute the alternative-specific value function
$\bar V(s_t ; \hat \lambda, \theta_0)$ by iteration&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Compute the implied choice probabilities&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Compute the likelihood $$
\mathcal{L}(\theta) = \prod_{t=1}^{T}\left(\hat{\operatorname{Pr}}\left(a=1 \mid s_{t}, \theta\right) \mathbb{1}\left(a_{t}=1\right)+\left(1-\hat{\operatorname{Pr}}\left(a=0 \mid s_{t}, \theta\right)\right) \mathbb{1}\left(a_{t}=0\right)\right)
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Repeat the above to find a minimum of the likelihood function&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;likelihood-function&#34;&gt;Likelihood Function&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;function logL_Rust(θ0::Vector, λ::Number, β::Number, s::Vector, St::Vector, A::BitVector)::Number
    &amp;quot;&amp;quot;&amp;quot;Compute log-likelihood functionfor Rust problem&amp;quot;&amp;quot;&amp;quot;
    # Compute value
    Vbar = compute_Vbar(θ0, λ_, β, s)

    # Expected choice probabilities
    EP = exp.(Vbar[:,2]) ./ (exp.(Vbar[:,1]) + exp.(Vbar[:,2]))

    # Likelihood
    logL = sum(log.(EP[St[A.==1]])) + sum(log.(1 .- EP[St[A.==0]]))
    return -logL
end;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can check the likelihood at the true value:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# True likelihood value
logL_trueθ = logL_Rust(θ, λ, β, s, St, A);
print(&amp;quot;The likelihood at the true parameter is $logL_trueθ&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The likelihood at the true parameter is 45937.866092460084
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;estimating-theta&#34;&gt;Estimating Theta&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# Select starting values
θ0 = Float64[0,0,0];

# Optimize
θ_R = optimize(x -&amp;gt; logL_Rust(x, λ, β, s, St, A), θ0).minimizer;
print(&amp;quot;Estimated thetas: $θ_R (true = $θ)&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Estimated thetas: [0.12063838656559037, -0.003220197034620527, 3.0865668144650487] (true = [0.13, -0.004, 3.1])
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;starting-values&#34;&gt;Starting Values&lt;/h3&gt;
&lt;p&gt;Starting values are important!&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# Not all initial values are equally good
θ0 = Float64[1,1,1];

# Optimize
θ_R2 = optimize(x -&amp;gt; logL_Rust(x, λ, β, s, St, A), θ0).minimizer;
print(&amp;quot;Estimated thetas: $θ_R2 (true = $θ)&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Estimated thetas: [1.0, 1.0, 1.0] (true = [0.13, -0.004, 3.1])
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;hotz--miller&#34;&gt;Hotz &amp;amp; Miller&lt;/h2&gt;
&lt;h3 id=&#34;recap&#34;&gt;Recap&lt;/h3&gt;
&lt;p&gt;Hotz &amp;amp; Miller estimation procedure works as follows&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Estimate the CCPs from the data&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Hotz &amp;amp; Miller inversion $$
\hat V = \Big[I - \beta \ \sum_a P_a .* T_a \Big]^{-1} \ * \ \left( \sum_a P_a \ .* \ \bigg[ u_a + \mathbb E [\epsilon_a] \bigg] \right)
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Compute EP from EV $$
\hat \Pr(a=1 ; \theta) = \frac{\exp (u_1 +\beta T_1 \hat V )}{\sum_{a} \exp (u_a +\beta T_a \hat V )}
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Compute the objective function: the (log)likelihood $$
\mathcal{L}(\theta) = \prod_{t=1}^{T}\left(\hat{\operatorname{Pr}}\left(a=1 \mid s_{t}; \theta\right) \mathbb{1}\left(a_{t}=1\right)+\left(1-\hat{\operatorname{Pr}}\left(a=0 \mid s_{t}; \theta\right)\right) \mathbb{1}\left(a_{t}=0\right)\right)
$$&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;ccps&#34;&gt;CCPs&lt;/h3&gt;
&lt;p&gt;First, we need to estimate the &lt;strong&gt;Conditional Choice Proabilities (CCP)&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;can be done non-parametrically&lt;/li&gt;
&lt;li&gt;i.e. just look at the frequency of investment in each state&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# Estimate CCP
P = [mean(A[St.==i]) for i=s];
CCP = [(1 .- P) P]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 10×2 Array{Float64,2}:
##  0.952419  0.0475814
##  0.923046  0.0769535
##  0.894443  0.105557
##  0.853306  0.146694
##  0.819293  0.180707
##  0.788935  0.211065
##  0.747248  0.252752
##  0.717915  0.282085
##  0.6947    0.3053
##  0.678452  0.321548
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;transition-probabilities&#34;&gt;Transition Probabilities&lt;/h3&gt;
&lt;p&gt;NeSr, we need $T$, the matrices of transition probabilities, conditional
on the investment choice.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;function compute_T(k::Int, λ_::Number)::Array
    &amp;quot;&amp;quot;&amp;quot;Compute transition matrix&amp;quot;&amp;quot;&amp;quot;
    T = zeros(k, k, 2);

    # Conditional on not investing
    T[k,k,1] = 1;
    for i=1:k-1
        T[i,i,1] = 1-λ_
        T[i,i+1,1] = λ_
    end

    # Conditional on investing
    T[:,1,2] .= 1-λ_;
    T[:,2,2] .= λ_;

    return(T)
end;
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;t&#34;&gt;T&lt;/h3&gt;
&lt;p&gt;What form does the transition matrix $T$ take?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# Compute T
T = compute_T(k, λ_);

# Conditional on not investing
T[:,:,1]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 10×10 Array{Float64,2}:
##  0.179343  0.820657  0.0       0.0       …  0.0       0.0       0.0
##  0.0       0.179343  0.820657  0.0          0.0       0.0       0.0
##  0.0       0.0       0.179343  0.820657     0.0       0.0       0.0
##  0.0       0.0       0.0       0.179343     0.0       0.0       0.0
##  0.0       0.0       0.0       0.0          0.0       0.0       0.0
##  0.0       0.0       0.0       0.0       …  0.0       0.0       0.0
##  0.0       0.0       0.0       0.0          0.820657  0.0       0.0
##  0.0       0.0       0.0       0.0          0.179343  0.820657  0.0
##  0.0       0.0       0.0       0.0          0.0       0.179343  0.820657
##  0.0       0.0       0.0       0.0          0.0       0.0       1.0
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;t-2&#34;&gt;T (2)&lt;/h3&gt;
&lt;p&gt;Instead, the transitions conditional on investing are&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# T Conditional on investing
T[:,:,2]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 10×10 Array{Float64,2}:
##  0.179343  0.820657  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0
##  0.179343  0.820657  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0
##  0.179343  0.820657  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0
##  0.179343  0.820657  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0
##  0.179343  0.820657  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0
##  0.179343  0.820657  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0
##  0.179343  0.820657  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0
##  0.179343  0.820657  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0
##  0.179343  0.820657  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0
##  0.179343  0.820657  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;hotz--miller-inversion&#34;&gt;Hotz &amp;amp; Miller Inversion&lt;/h3&gt;
&lt;p&gt;We now have all the pieces to compute the &lt;strong&gt;expected value function&lt;/strong&gt;
$V$ through the Hotz &amp;amp; Miller &lt;strong&gt;inversion&lt;/strong&gt;. $$
\hat V = \left[I - \beta \ \sum_a P_a .* T_a \right]^{-1} \ * \ \left( \sum_a P_a \ .* \ \bigg[ u_a + \mathbb E [\epsilon_a] \bigg] \right)
$$&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;function HM_inversion(CCP::Matrix, T::Array, U::Matrix, β::Number)::Vector
    &amp;quot;&amp;quot;&amp;quot;Perform HM inversion&amp;quot;&amp;quot;&amp;quot;

    # Compute LHS (to be inverted)
    γ = Base.MathConstants.eulergamma
    LEFT = I - β .* (CCP[:,1] .* T[:,:,1] + CCP[:,2] .* T[:,:,2])

    # Compute LHS (not to be inverted)
    RIGHT = γ .+ sum(CCP .* (U .- log.(CCP)) , dims=2)

    # Compute V
    EV_ = inv(LEFT) * RIGHT
    return vec(EV_)
end;
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;from-ev-to-ep&#34;&gt;From EV to EP&lt;/h3&gt;
&lt;p&gt;We can now compute the expected policy function from the expected value
function $$
\hat \Pr(a=1 ; \theta) = \frac{\exp (u_1 +\beta T_1 \hat V )}{\sum_{a} \exp (u_a +\beta T_a \hat V )}
$$&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;function from_EV_to_EP(EV_::Vector, T::Array, U::Matrix, β::Number)::Vector
    &amp;quot;&amp;quot;&amp;quot;Compute expected policy from expected value&amp;quot;&amp;quot;&amp;quot;
    E = exp.( U + β .* [(T[:,:,1] * EV_) (T[:,:,2] * EV_)] )
    EP_ = E[:,2] ./ sum(E, dims=2)
    return vec(EP_)
end;
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;likelihood&#34;&gt;Likelihood&lt;/h3&gt;
&lt;p&gt;We now have all the pieces to build the likelihood function $$
\mathcal{L}(\theta) = \prod_{t=1}^{T} \left(\hat \Pr \left(a=1 \mid s_{t}; \theta\right) \mathbb{1} \left(a_{t}=1\right) + \left(1-\hat \Pr \left(a=0 \mid s_{t}; \theta\right)\right) \mathbb{1} \left(a_{t}=0\right)\right)
$$&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;function logL_HM(θ0::Vector, λ::Number, β::Number, s::Vector, St::Vector, A::BitVector, T::Array, CCP::Matrix)::Number
    &amp;quot;&amp;quot;&amp;quot;Compute log-likelihood function for HM problem&amp;quot;&amp;quot;&amp;quot;
    # Compute static utility
    U = compute_U(θ0, s)

    # Espected value by inversion
    EV_ = HM_inversion(CCP, T, U, β)

    # Implies choice probabilities
    EP_ = from_EV_to_EP(EV_, T, U, β)

    # Likelihood
    logL = sum(log.(EP_[St[A.==1]])) + sum(log.(1 .- EP_[St[A.==0]]))
    return -logL
end;
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;estimation&#34;&gt;Estimation&lt;/h3&gt;
&lt;p&gt;We can now estimate the parameters&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# Optimize
θ0 = Float64[0,0,0];
θ_HM = optimize(x -&amp;gt; logL_HM(x, λ, β, s, St, A, T, CCP), θ0).minimizer;
print(&amp;quot;Estimated thetas: $θ_HM (true = $θ)&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Estimated thetas: [0.12064911403839335, -0.003220614484856523, 3.086621855583483] (true = [0.13, -0.004, 3.1])
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;aguirregabiria-mira-2002&#34;&gt;Aguirregabiria, Mira (2002)&lt;/h3&gt;
&lt;p&gt;With Hotz and Miller, we have generated a mapping of the form&lt;/p&gt;
&lt;p&gt;$$
\bar P(\cdot ; \theta) = g(h(\hat P(\cdot) ; \theta); \theta)
$$&lt;/p&gt;
&lt;p&gt;Aguirregabiria and Mira (&lt;a href=&#34;#ref-aguirregabiria2002swapping&#34;&gt;2002&lt;/a&gt;): why
don’t we iterate it?&lt;/p&gt;
&lt;h3 id=&#34;am-likelihood-function&#34;&gt;AM Likelihood Function&lt;/h3&gt;
&lt;p&gt;The likelihood function in Aguirregabiria and Mira
(&lt;a href=&#34;#ref-aguirregabiria2002swapping&#34;&gt;2002&lt;/a&gt;) is extremely similar to Hotz
and Miller (&lt;a href=&#34;#ref-hotz1993conditional&#34;&gt;1993&lt;/a&gt;)&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;function logL_AM(θ0::Vector, λ::Number, β::Number, s::Vector, St::Vector, A::BitVector, T::Array, CCP::Matrix, K::Int)::Number
    &amp;quot;&amp;quot;&amp;quot;Compute log-likelihood function for AM problem&amp;quot;&amp;quot;&amp;quot;
    # Compute static utility
    U = compute_U(θ0, s)
    EP_ = CCP[:,2]

    # Iterate HM mapping
    for _=1:K
        EV_ = HM_inversion(CCP, T, U, β)    # Expected value by inversion
        EP_ = from_EV_to_EP(EV_, T, U, β)   # Implies choice probabilities
        CCP = [(1 .- EP_) EP_]
    end

    # Likelihood
    logL = sum(log.(EP_[St[A.==1]])) + sum(log.(1 .- EP_[St[A.==0]]))
    return -logL
end;
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;estimation-1&#34;&gt;Estimation&lt;/h3&gt;
&lt;p&gt;We can now estimate the parameters&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# Set number of iterations
K = 2;

# Optimize
θ0 = Float64[0,0,0];
θ_AM = optimize(x -&amp;gt; logL_AM(x, λ, β, s, St, A, T, CCP, K), θ0).minimizer;
print(&amp;quot;Estimated thetas: $θ_AM (true = $θ)&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Estimated thetas: [0.12063890836521114, -0.0032202282942220464, 3.086571461772538] (true = [0.13, -0.004, 3.1])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Not much changes in our case.&lt;/p&gt;
&lt;h3 id=&#34;speed&#34;&gt;Speed&lt;/h3&gt;
&lt;p&gt;We can compare the methods in terms of speed.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# Compare speed
θ0 = Float64[0,0,0];
optimize(x -&amp;gt; logL_Rust(x, λ, β, s, St, A), θ0).time_run
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 0.5477378368377686
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;optimize(x -&amp;gt; logL_HM(x, λ, β, s, St, A, T, CCP), θ0).time_run
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 0.3244161605834961
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;optimize(x -&amp;gt; logL_AM(x, λ, β, s, St, A, T, CCP, K), θ0).time_run
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 0.35499119758605957
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Even in this simple example with a very small state space, the
difference is significant.&lt;/p&gt;
&lt;h2 id=&#34;appendix&#34;&gt;Appendix&lt;/h2&gt;
&lt;h3 id=&#34;references-references&#34;&gt;References [references]&lt;/h3&gt;
&lt;div id=&#34;refs&#34; class=&#34;references csl-bib-body hanging-indent&#34;
markdown=&#34;1&#34;&gt;
&lt;div id=&#34;ref-aguirregabiria2002swapping&#34; class=&#34;csl-entry&#34; markdown=&#34;1&#34;&gt;
&lt;p&gt;Aguirregabiria, Victor, and Pedro Mira. 2002. “Swapping the Nested Fixed
Point Algorithm: A Class of Estimators for Discrete Markov Decision
Models.” &lt;em&gt;Econometrica&lt;/em&gt; 70 (4): 1519–43.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-hotz1993conditional&#34; class=&#34;csl-entry&#34; markdown=&#34;1&#34;&gt;
&lt;p&gt;Hotz, V Joseph, and Robert A Miller. 1993. “Conditional Choice
Probabilities and the Estimation of Dynamic Models.” &lt;em&gt;The Review of
Economic Studies&lt;/em&gt; 60 (3): 497–529.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-rust1988maximum&#34; class=&#34;csl-entry&#34; markdown=&#34;1&#34;&gt;
&lt;p&gt;Rust, John. 1988. “Maximum Likelihood Estimation of Discrete Control
Processes.” &lt;em&gt;SIAM Journal on Control and Optimization&lt;/em&gt; 26 (5): 1006–24.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>XYZ</title>
      <link>https://matteocourthoud.github.io/post/cin-2023-08-20/</link>
      <pubDate>Sun, 20 Aug 2023 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/post/cin-2023-08-20/</guid>
      <description>&lt;p&gt;&lt;em&gt;A causal inference newsletter&lt;/em&gt;&lt;/p&gt;
&lt;h2 id=&#34;news&#34;&gt;News&lt;/h2&gt;
&lt;h3 id=&#34;speeding-up-hellofreshs-bayesian-ab-testinghttpsmediumcomsocial_68653bayes-is-slow-speeding-up-hellofreshs-bayesian-ab-tests-by-60x-fc431a327cd8&#34;&gt;&lt;a href=&#34;https://medium.com/@social_68653/bayes-is-slow-speeding-up-hellofreshs-bayesian-ab-tests-by-60x-fc431a327cd8&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Speeding up HelloFresh’s Bayesian AB Testing&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;PyMC researchers tested different ideas to speed up thousands of concurrent AB tests. First, they decreased model parametrization, and sampling length, leading to marginal speed increases. However, the most gains were achieved by defining a single unpooled model made of many statistically independent models, leading to 60x speed gains.&lt;/p&gt;
&lt;h3 id=&#34;a-tidy-package-for-hte-estimationhttpstwittercomdrewdimstatus1691095106886242304&#34;&gt;&lt;a href=&#34;https://twitter.com/DrewDim/status/1691095106886242304&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;A &lt;code&gt;tidy&lt;/code&gt; Package for HTE Estimation&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;A new R package, &lt;code&gt;tidyhte&lt;/code&gt;, estimates Heterogeneous Treatment Effects with a &lt;a href=&#34;https://www.tidyverse.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;tidy&lt;/a&gt; syntax. The estimator is essentially an R-learner in the spirit of &lt;a href=&#34;https://arxiv.org/abs/2004.14497&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Kennedy (2022)&lt;/a&gt;. It uses &lt;a href=&#34;https://github.com/ecpolley/SuperLearner&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;SuperLearner&lt;/code&gt;&lt;/a&gt; for fitting nuisance parameters and &lt;a href=&#34;http://bdwilliamson.github.io/vimp/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;vimp&lt;/code&gt;&lt;/a&gt; for variable importance. Official website &lt;a href=&#34;https://ddimmery.github.io/tidyhte/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;spotify-shares-some-experimentation-lessonshttpsengineeringatspotifycom202308experimentation-at-spotify-three-lessons-for-maximizing-impact-in-innovation&#34;&gt;&lt;a href=&#34;https://engineering.atspotify.com/2023/08/experimentation-at-spotify-three-lessons-for-maximizing-impact-in-innovation&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Spotify Shares Some Experimentation Lessons&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;First, the researchers suggest thinking backward, starting from the decision that needs to be informed and designing the experiment accordingly. Second, they suggest running local experiments to discover local effects, an effective strategy for their expansion in the Japanese market. Last, they recommend testing changes incrementally and not in bunches.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;old-reads&#34;&gt;Old Reads&lt;/h2&gt;
&lt;h3 id=&#34;experimentation-with-resource-constraintshttpsmultithreadedstitchfixcomblog20201118virtual-warehouse&#34;&gt;&lt;a href=&#34;https://multithreaded.stitchfix.com/blog/2020/11/18/virtual-warehouse/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Experimentation with Resource Constraints&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Researchers at Stitchfix show how to deal with interference bias from budget constraints: enforce virtual constraints, preventing isolated groups from competing for resources. The solution is effective in solving the bias but opens new challenges in how to enforce the new constraints and scale the estimates to the full market.&lt;/p&gt;
&lt;h3 id=&#34;r-guide-for-tmle-in-medical-researchhttpsehsanxgithubiotmleworkshop&#34;&gt;&lt;a href=&#34;https://ehsanx.github.io/TMLEworkshop/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;R Guide for TMLE in Medical Research&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;This code-first guide introduces Targeted Maximum Likelihood Estimation (TMLE) through a medical application on real data: the effects of right heart catheterization on critically ill patients in the intensive care unit. The guide starts with data exploration, then introduces outcome models (G-computation), exposure models (IPW), and lastly combines them into TMLE.&lt;/p&gt;
&lt;h3 id=&#34;an-overview-of-netlix-experimentation-platformhttpsarxivorgabs191003878&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/1910.03878&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;An Overview of Netlix Experimentation Platform&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Netflix&amp;rsquo;s experimentation platform is built on three pillars. The first pillar is a metrics repository where statistics are stored and shared across projects. The second pillar is a causal models library that collects causal inference methods. Lastly, a lightweight interactive visualization library to explore and report estimates. A global causal graph is not mentioned.&lt;/p&gt;
&lt;h3 id=&#34;thumbnack-explains-the-efficiency-gains-of-interleavinghttpsmediumcomthumbtack-engineeringaccelerating-ranking-experimentation-at-thumbtack-with-interleaving-20cbe7837edf&#34;&gt;&lt;a href=&#34;https://medium.com/thumbtack-engineering/accelerating-ranking-experimentation-at-thumbtack-with-interleaving-20cbe7837edf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Thumbnack Explains the Efficiency Gains of Interleaving&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;First, interleaving is a one-sample test. Second, interleaving controls for customer variation since each customer sees both rankings. Third, signals are stronger since consumers have to make a choice (revealed preference). The downside is generalization since the rankings during the experiment differ from the deployed ones.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;For more causal inference resources:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Subscribe to my stories on &lt;a href=&#34;https://medium.com/@matteo.courthoud&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Medium&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Follow me on &lt;a href=&#34;https://www.linkedin.com/in/matteo-courthoud/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;LinkedIn&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Star &lt;a href=&#34;https://github.com/matteocourthoud/awesome-causal-inference&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;awesome-causal-inference&lt;/code&gt;&lt;/a&gt;, where I collect all the articles and additional causal inference resources&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>XYZ</title>
      <link>https://matteocourthoud.github.io/post/cin-2023-08-06/</link>
      <pubDate>Sun, 06 Aug 2023 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/post/cin-2023-08-06/</guid>
      <description>&lt;p&gt;&lt;em&gt;A causal inference newsletter&lt;/em&gt;&lt;/p&gt;
&lt;h2 id=&#34;news&#34;&gt;News&lt;/h2&gt;
&lt;h3 id=&#34;spotify-releases-its-experimentation-platform-confidencehttpsengineeringatspotifycom202308coming-soon-confidence-an-experimentation-platform-from-spotify&#34;&gt;&lt;a href=&#34;https://engineering.atspotify.com/2023/08/coming-soon-confidence-an-experimentation-platform-from-spotify&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Spotify Releases its Experimentation Platform: Confidence&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Spotify has released its internal experimentation platform, used for 10+ years to run and evaluate A/B tests at scale. Currently, it is in closed beta, but it promises large-scale performance, usability, flexibility, but also integration with existing A/B testing tools. A &lt;a href=&#34;https://github.com/spotify/confidence&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;confidence&lt;/code&gt;&lt;/a&gt; Github repo has been existing of a while.&lt;/p&gt;
&lt;h3 id=&#34;microsoft-calls-for-not-worrying-about-interaction-effectshttpswwwmicrosoftcomen-usresearchgroupexperimentation-platform-exparticlesa-b-interactions-a-call-to-relax&#34;&gt;&lt;a href=&#34;https://www.microsoft.com/en-us/research/group/experimentation-platform-exp/articles/a-b-interactions-a-call-to-relax/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Microsoft Calls for not Worrying About Interaction Effects&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Microsoft researchers have looked at four major products, each running hundreds of A/B tests. They could not find any evidence of interaction effects. This does not mean interaction effects should be ignored, but rather that, at least in this scenario, they are of sufficiently low magnitude not to worry about them.&lt;/p&gt;
&lt;h3 id=&#34;updated-version-of-statistical-challenges-in-online-controlled-experimentshttpsarxivorgabs221211366&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2212.11366&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Updated Version of &amp;ldquo;Statistical Challenges in Online Controlled Experiments&amp;rdquo;&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Ron Kohavi and co-authors have updated their paper on A/B test challenges, splitting the paper into a main paper and supplementary material. The main paper covers variance reduction, heterogeneous treatment effects, long-term effects, optimal stopping, and interference. The appendix covers stratified sampling, surrogate outcomes, and network A/B tests.&lt;/p&gt;
&lt;h3 id=&#34;pymc-adds-the-do-operatorhttpswwwpymc-labsioblog-postscausal-analysis-with-pymc-answering-what-if-with-the-new-do-operator&#34;&gt;&lt;a href=&#34;https://www.pymc-labs.io/blog-posts/causal-analysis-with-pymc-answering-what-if-with-the-new-do-operator&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PyMC Adds the do-Operator&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Python&amp;rsquo;s main probabilistic programming library has added a do-operator to do Bayesian Causal Analysis. If you know and specify the whole causal graph, it allows you to do Bayesian inference on causal quantities of interest. The repo is still experimental.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;old-reads&#34;&gt;Old Reads&lt;/h2&gt;
&lt;h3 id=&#34;embrace-overlapping-ab-tests-and-avoid-the-dangers-of-isolating-experimentshttpsblogstatsigcomembracing-overlapping-a-b-tests-and-the-danger-of-isolating-experiments-cb0a69e09d3&#34;&gt;&lt;a href=&#34;https://blog.statsig.com/embracing-overlapping-a-b-tests-and-the-danger-of-isolating-experiments-cb0a69e09d3&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Embrace Overlapping A/B Tests and Avoid the Dangers of Isolating Experiments&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;From the experience of data scientists at Meta, strong interaction effects are rare. It&amp;rsquo;s worth isolating experiments only when it&amp;rsquo;s mechanically necessary, or they might break the user experience, or you need a very precise measurement.&lt;/p&gt;
&lt;h3 id=&#34;an-analysts-guide-to-mmmhttpsfacebookexperimentalgithubiorobyndocsanalysts-guide-to-mmm&#34;&gt;&lt;a href=&#34;https://facebookexperimental.github.io/Robyn/docs/analysts-guide-to-MMM/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;An Analysts guide to MMM&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;While not being a standalone publication, this MMM guide in Meta&amp;rsquo;s &lt;a href=&#34;https://facebookexperimental.github.io/Robyn/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Robyn&lt;/a&gt; documentation is comprehensive and detailed. It goes through all the phases of building an MMM model. Causality is mentioned shortly but vehemently: &amp;ldquo;&lt;em&gt;we strongly recommend using experimental and causal results to calibrate MMM&lt;/em&gt;&amp;rdquo;.&lt;/p&gt;
&lt;h3 id=&#34;an-intuitive-explanation-of-why-π-is-in-the-normal-distributionhttpswwwyoutubecomwatchvcy8r7wsut1i&#34;&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=cy8r7WSuT1I&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;An Intuitive Explanation of Why π is in the Normal Distribution&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;In two dimensions, the normal distribution can be defined by two properties: the pdf depends only on the distance from the origin, and the two dimensions are independent. From here, the relationship with a circle (and hence π) but also the relationship between the normal distribution and the Central Limit Theorem (covered in &lt;a href=&#34;https://www.youtube.com/watch?v=d_qvLDhkg00&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;another video&lt;/a&gt;).&lt;/p&gt;
&lt;h3 id=&#34;a-list-of-company-experimentation-platformshttpswwwlinkedincompulsein-house-experimentation-platforms-denise-visser&#34;&gt;&lt;a href=&#34;https://www.linkedin.com/pulse/in-house-experimentation-platforms-denise-visser/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;A List of Company Experimentation Platforms&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Denise Visser lists the different in-house experimentation platforms of different companies as of 2020.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;For more causal inference resources:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Subscribe to my stories on &lt;a href=&#34;https://medium.com/@matteo.courthoud&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Medium&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Follow me on &lt;a href=&#34;https://www.linkedin.com/in/matteo-courthoud/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;LinkedIn&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Star &lt;a href=&#34;https://github.com/matteocourthoud/awesome-causal-inference&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;awesome-causal-inference&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Beyond Churn Prediction and Churn Uplift</title>
      <link>https://matteocourthoud.github.io/post/beyond_churn/</link>
      <pubDate>Tue, 25 Jul 2023 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/post/beyond_churn/</guid>
      <description>&lt;p&gt;A very common task in data science is &lt;em&gt;churn prediction&lt;/em&gt;. However, predicting churn is often just an intermediate step and rarely the final objective. Usually, what we actually care about is &lt;strong&gt;reducing churn&lt;/strong&gt;, which is a separate objective, not necessarily related. In fact, for example, knowing that long-term customers are less likely to churn than new customers is not an actionable insight since we cannot increase customers&amp;rsquo; tenure. What we would like to know instead is how one (or more) treatment impacts churn. This is often referred to as &lt;strong&gt;churn uplift&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;In this article, we will be going &lt;strong&gt;beyond&lt;/strong&gt; both churn prediction and churn uplift and consider instead the ultimate goal of churn-prevention campaigns: &lt;strong&gt;increasing revenue&lt;/strong&gt;. First of all, a policy that reduces churn might also have an impact on revenue, which should be taken into account. However, and more importantly, increasing revenue is relevant only if the customer does not churn. Vice-versa, decreasing churn is more relevant for high-revenue customers. This &lt;strong&gt;interaction&lt;/strong&gt; between churn and revenue is critical in understanding the profitability of any treatment campaign and should not be overlooked.&lt;/p&gt;
&lt;h2 id=&#34;gifts-and-subscriptions&#34;&gt;Gifts and Subscriptions&lt;/h2&gt;
&lt;p&gt;For the rest of the article, we are going to use a &lt;strong&gt;toy example&lt;/strong&gt; to illustrate the main idea. Suppose we were a company interested in reducing our customer&amp;rsquo;s churn and ultimately increasing our revenue. Suppose we have decided to test a new idea: sending a &lt;strong&gt;gift&lt;/strong&gt; of &lt;em&gt;1\$&lt;/em&gt; to our users. In order to test whether the treatment works, we have randomly sent it only to a subsample of our customer base.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;cost = 1
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s have a look at the data we have at our disposal. I import the data-generating process &lt;code&gt;dgp_gift()&lt;/code&gt; from &lt;code&gt;src.dgp&lt;/code&gt;. I also import some plotting functions and libraries from &lt;code&gt;src.utils&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;%matplotlib inline
%config InlineBackend.figure_format = &#39;retina&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from src.utils import *
from src.dgp import dgp_gift
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;dgp = dgp_gift(n=100_000)
df = dgp.generate_data()
df.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;months&lt;/th&gt;
      &lt;th&gt;rev_old&lt;/th&gt;
      &lt;th&gt;rev_change&lt;/th&gt;
      &lt;th&gt;gift&lt;/th&gt;
      &lt;th&gt;churn&lt;/th&gt;
      &lt;th&gt;revenue&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;3.98&lt;/td&gt;
      &lt;td&gt;3.36&lt;/td&gt;
      &lt;td&gt;0.86&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;7.28&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;6.28&lt;/td&gt;
      &lt;td&gt;14.41&lt;/td&gt;
      &lt;td&gt;-2.77&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;11.60&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;4.62&lt;/td&gt;
      &lt;td&gt;2.89&lt;/td&gt;
      &lt;td&gt;-2.21&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;3.59&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;3.94&lt;/td&gt;
      &lt;td&gt;0.00&lt;/td&gt;
      &lt;td&gt;-3.26&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0.00&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;2.76&lt;/td&gt;
      &lt;td&gt;3.25&lt;/td&gt;
      &lt;td&gt;-3.43&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;5.33&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;We have information on &lt;code&gt;100_000&lt;/code&gt; customers for which we observe the number of &lt;code&gt;months&lt;/code&gt; they have been active customers, the revenue they generated in the last month (&lt;code&gt;rev_old&lt;/code&gt;), the change in revenue between the last month and the previous one (&lt;code&gt;rev_change&lt;/code&gt;), whether they were randomly sent a &lt;code&gt;gift&lt;/code&gt; and the two outcomes of interest: &lt;code&gt;churn&lt;/code&gt;, i.e. whether they are not active customers anymore and the &lt;code&gt;revenue&lt;/code&gt; they have generated in the current month. We denote the outcomes with the letter &lt;em&gt;Y&lt;/em&gt;, the treatment with the letter &lt;em&gt;W&lt;/em&gt; and the other variables with the letter &lt;em&gt;X&lt;/em&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;Y = [&#39;churn&#39;, &#39;revenue&#39;]
W = &#39;gift&#39;
X = [&#39;months&#39;, &#39;rev_old&#39;, &#39;rev_change&#39;]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt; that, for simplicity, we are considering a single-period snapshot of the data and summarizing the panel structure of the data in just a couple of variables. Usually we would have a longer time series but also a longer time horizon for what concerns the outcome (e.g. &lt;a href=&#34;https://en.wikipedia.org/wiki/Customer_lifetime_value&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;customer lifetime value&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;We can represent the underlying data generating process with the following &lt;strong&gt;Directed Acyclic Graph (DAG)&lt;/strong&gt;. Nodes represent variables and arrows represent potential causal relationships. I have highlighted in green the two relationships of interest: the effect of the &lt;code&gt;gift&lt;/code&gt; on &lt;code&gt;churn&lt;/code&gt; and &lt;code&gt;revenue&lt;/code&gt;. Note that &lt;code&gt;churn&lt;/code&gt; is related to revenue since churned customers by definition generate zero revenue.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-mermaid&#34;&gt;flowchart TD
classDef included fill:#DCDCDC,stroke:#000000,stroke-width:3px;
classDef empty width:-10px,height:-10px,fill:#000000,stroke-width:0px;

W((1$ gift))
D1(( ))
D2(( ))
Y1((churn))
Y2((revenue))
X1((months))
X2((revenue change))
X3((revenue old))

W --- D1
X1 --- D1
D1 --&amp;gt; Y1
W --- D2
X1 --- D2
D2 --&amp;gt; Y2
Y1 --&amp;gt; Y2
X2 --&amp;gt; Y1
X3 --&amp;gt; Y1
X3 --&amp;gt; Y2

class W,Y1,Y2,X1,X2,X3 included;
class D1,D2 empty;

linkStyle 0,2,3,5 stroke:#2db88b,stroke-width:6px;
linkStyle 1,4,6,7,8,9 stroke:#003f5c,stroke-width:6px;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Importantly, past revenue and the revenue change are predictors of &lt;code&gt;churn&lt;/code&gt; and &lt;code&gt;revenue&lt;/code&gt; but are not related to our intervention. On the contrary, the intervention affects &lt;code&gt;churn&lt;/code&gt; and &lt;code&gt;revenue&lt;/code&gt; differentially depending on the customers total active &lt;code&gt;months&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;While simplistic, this data generating process aims at captiring an important &lt;strong&gt;insight&lt;/strong&gt;: variables that are good predictors of &lt;code&gt;churn&lt;/code&gt; or &lt;code&gt;revenue&lt;/code&gt;, are not necessarily variables that predict &lt;code&gt;churn&lt;/code&gt; or &lt;code&gt;revenue&lt;/code&gt; &lt;strong&gt;lift&lt;/strong&gt;. We will see later how this impacts our analysis.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s start first by exploring the data.&lt;/p&gt;
&lt;h2 id=&#34;exploratory-data-analysis&#34;&gt;Exploratory Data Analysis&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s start with &lt;code&gt;churn&lt;/code&gt;. How many customers did the company lose last month?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df.churn.mean()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;0.19767
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The company lost almost &lt;em&gt;20%&lt;/em&gt; of customers last month! Did the &lt;code&gt;gift&lt;/code&gt; help in preventing churn?&lt;/p&gt;
&lt;p&gt;We want to compare the churn frequency of customers that received the gift with the churn frequency of customers that did not receive the gift. Since the gift was randomized, the difference-in-means estimator is an unbiased estimator for the &lt;a href=&#34;https://en.wikipedia.org/wiki/Average_treatment_effect&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;average treatment effect (ATE)&lt;/a&gt; of the &lt;code&gt;gift&lt;/code&gt; on &lt;code&gt;churn&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;$$
ATE = \mathbb{E} \Big[ Y \ \Big| \ W = 1 \Big] - \mathbb{E} \Big[ Y \ \Big| \ W = 0 \Big]
$$&lt;/p&gt;
&lt;p&gt;We compute the difference-in-means estimate by linear regression. We also include other covariates to improve the efficiency of the estimator.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;smf.ols(&amp;quot;churn ~ &amp;quot; + W + &amp;quot; + &amp;quot; + &amp;quot; + &amp;quot;.join(X), data=df).fit().summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
       &lt;td&gt;&lt;/td&gt;         &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt;  &lt;td&gt;    0.3271&lt;/td&gt; &lt;td&gt;    0.002&lt;/td&gt; &lt;td&gt;  151.440&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    0.323&lt;/td&gt; &lt;td&gt;    0.331&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;gift&lt;/th&gt;       &lt;td&gt;   -0.1173&lt;/td&gt; &lt;td&gt;    0.002&lt;/td&gt; &lt;td&gt;  -51.521&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;   -0.122&lt;/td&gt; &lt;td&gt;   -0.113&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;months&lt;/th&gt;     &lt;td&gt;    0.0050&lt;/td&gt; &lt;td&gt;    0.000&lt;/td&gt; &lt;td&gt;   21.832&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    0.005&lt;/td&gt; &lt;td&gt;    0.005&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;rev_old&lt;/th&gt;    &lt;td&gt;   -0.0181&lt;/td&gt; &lt;td&gt;    0.000&lt;/td&gt; &lt;td&gt; -108.061&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;   -0.018&lt;/td&gt; &lt;td&gt;   -0.018&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;rev_change&lt;/th&gt; &lt;td&gt;   -0.0497&lt;/td&gt; &lt;td&gt;    0.001&lt;/td&gt; &lt;td&gt;  -87.412&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;   -0.051&lt;/td&gt; &lt;td&gt;   -0.049&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;It looks like the &lt;code&gt;gift&lt;/code&gt; decreased churn by around &lt;em&gt;11&lt;/em&gt; percentage points, i.e. almost one-third of the baseline level of &lt;em&gt;32%&lt;/em&gt;! Did it also have an impact on &lt;code&gt;revenue&lt;/code&gt;?&lt;/p&gt;
&lt;p&gt;As for churn, we regress &lt;code&gt;revenue&lt;/code&gt; on &lt;code&gt;gift&lt;/code&gt;, our treatment variable, to estimate the average treatment effect.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;smf.ols(&amp;quot;revenue ~ &amp;quot; + W + &amp;quot; + &amp;quot; + &amp;quot; + &amp;quot;.join(X), data=df).fit().summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
       &lt;td&gt;&lt;/td&gt;         &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt;  &lt;td&gt;    0.3691&lt;/td&gt; &lt;td&gt;    0.010&lt;/td&gt; &lt;td&gt;   37.910&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    0.350&lt;/td&gt; &lt;td&gt;    0.388&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;gift&lt;/th&gt;       &lt;td&gt;    0.6317&lt;/td&gt; &lt;td&gt;    0.010&lt;/td&gt; &lt;td&gt;   61.560&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    0.612&lt;/td&gt; &lt;td&gt;    0.652&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;months&lt;/th&gt;     &lt;td&gt;    0.0120&lt;/td&gt; &lt;td&gt;    0.001&lt;/td&gt; &lt;td&gt;   11.768&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    0.010&lt;/td&gt; &lt;td&gt;    0.014&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;rev_old&lt;/th&gt;    &lt;td&gt;    0.8251&lt;/td&gt; &lt;td&gt;    0.001&lt;/td&gt; &lt;td&gt; 1092.846&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    0.824&lt;/td&gt; &lt;td&gt;    0.827&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;rev_change&lt;/th&gt; &lt;td&gt;    0.1457&lt;/td&gt; &lt;td&gt;    0.003&lt;/td&gt; &lt;td&gt;   56.777&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    0.141&lt;/td&gt; &lt;td&gt;    0.151&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;It looks like the &lt;code&gt;gift&lt;/code&gt; on average increased revenue by &lt;em&gt;0.63\$&lt;/em&gt;, which means that it was &lt;strong&gt;not profitable&lt;/strong&gt;. Does it mean that we should stop sending gifts to our customers? It depends. In fact, the gift might be effective for certain customer segments. We just need to identify them.&lt;/p&gt;
&lt;h2 id=&#34;targeting-policies&#34;&gt;Targeting Policies&lt;/h2&gt;
&lt;p&gt;In this section, we will try to understand if there is a data-informed way to send the &lt;code&gt;gift&lt;/code&gt; to customers that is profitable. In particular, we will c&lt;strong&gt;compare&lt;/strong&gt; different data-informed targeting &lt;strong&gt;policies&lt;/strong&gt; with the objective of increasing revenue.&lt;/p&gt;
&lt;p&gt;Throughout this section, we will need some algorithms to either predict &lt;code&gt;revenue&lt;/code&gt;, or &lt;code&gt;churn&lt;/code&gt;, or the probability of receiving the &lt;code&gt;gift&lt;/code&gt;. We use gradient-boosted tree models from the &lt;a href=&#34;https://lightgbm.readthedocs.io/en/latest/index.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;lightgbm&lt;/code&gt;&lt;/a&gt; library  We use the same models for all policies, so that we cannot attribute differences in performance to prediction accuracy.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from lightgbm import LGBMClassifier, LGBMRegressor
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To &lt;strong&gt;evaluate&lt;/strong&gt; each policy denoted with &lt;em&gt;τ&lt;/em&gt;, we compare its profits with the policy &lt;em&gt;Π⁽¹⁾&lt;/em&gt;, with its profits without the policy &lt;em&gt;Π⁽⁰⁾&lt;/em&gt;, over every single individual, in a separate validation dataset. Note that this is usually not possible, since, for each customer, we only observe one of the two &lt;strong&gt;potential outcomes&lt;/strong&gt;, with or without the &lt;code&gt;gift&lt;/code&gt;. However, this is synthetic data, so we can do oracle evaluation. If you want to know more about how to evaluate uplift models with real data, I wrote an article about it.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://towardsdatascience.com/8a078996a113&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://towardsdatascience.com/8a078996a113&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;First of all, let&amp;rsquo;s define profits &lt;em&gt;Π&lt;/em&gt; as the net revenue &lt;em&gt;R&lt;/em&gt; when the customer does not churn &lt;em&gt;C&lt;/em&gt;.
$$
\Pi = R (1-C)
$$&lt;/p&gt;
&lt;p&gt;Therefore, the overall effect on profits for treated individuals is given by the difference between the profits when treated &lt;em&gt;Π⁽¹⁾&lt;/em&gt; minus the profits when not treated &lt;em&gt;Π⁽⁰⁾&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;$$
\tau_{\pi} = R_1 (1 - C_1) - R_0 (1 - C_0)
$$&lt;/p&gt;
&lt;p&gt;The effect for untreated individuals is zero.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def evaluate_policy(policy):
    data = dgp.generate_data(seed_data=4, seed_assignment=5, keep_po=True)
    data[&#39;profits&#39;] = (1 - data.churn) * data.revenue
    baseline = (1-data.churn_c) * data.revenue_c
    effect = policy(data) * (1-data.churn_t) * (data.revenue_t-cost) + (1-policy(data)) * (1-data.churn_c) * data.revenue_c
    return np.sum(effect - baseline)
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;1-target-churning-customers&#34;&gt;1. Target Churning Customers&lt;/h3&gt;
&lt;p&gt;A first policy could be to just target &lt;strong&gt;churning customers&lt;/strong&gt;. Let&amp;rsquo;s say, we send the &lt;code&gt;gift&lt;/code&gt; only to customers with above-average redicted churn.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;model_churn = LGBMClassifier().fit(X=df[X], y=df[&#39;churn&#39;])

policy_churn = lambda df : (model_churn.predict_proba(df[X])[:,1] &amp;gt; df.churn.mean())
evaluate_policy(policy_churn)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;-5497.46
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The policy is not profitable and would lead to an aggregate &lt;strong&gt;loss&lt;/strong&gt; of more than &lt;em&gt;5000\$&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;You might think that the problem is the &lt;strong&gt;arbitrary threshold&lt;/strong&gt;, but this is not the case. Below I plot the aggregate effect for all possible policy thresholds.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;x = np.linspace(0, 1, 100)
y = [evaluate_policy(lambda df : (model_churn.predict_proba(df[X])[:,1] &amp;gt; k)) for k in x]

fig, ax = plt.subplots(figsize=(10, 3))
sns.lineplot(x=x, y=y).set(xlabel=&#39;Churn Policy Threshold&#39;, title=&#39;Aggregate Effect&#39;);
ax.axhline(y=0, c=&#39;k&#39;, lw=3, ls=&#39;--&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/beyond_churn_33_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;As we can see, at best we can make zero losses when we decide not to give the gift to any customer.&lt;/p&gt;
&lt;p&gt;The problem is that the fact that a customer is likely to churn does not imply that the &lt;code&gt;gift&lt;/code&gt; will have any impact on their churn probability. The two measures are not completely unrelated (e.g. we cannot decrease the churning probability of customers that have 0% probability of churning), but they are not the same thing.&lt;/p&gt;
&lt;h3 id=&#34;2-target-revenue-customers&#34;&gt;2. Target revenue customers&lt;/h3&gt;
&lt;p&gt;Let&amp;rsquo;s now try a different policy: we send the gift only to &lt;strong&gt;high-revenue customers&lt;/strong&gt;. For example, we might send the gift only to the top-10% of customers by revenue. The idea is that, if the policy indeed decreases churn, these are the customers for whom decreasing churn is more profitable.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;model_revenue = LGBMRegressor().fit(X=df[X], y=df[&#39;revenue&#39;])

policy_revenue = lambda df : (model_revenue.predict(df[X]) &amp;gt; np.quantile(df.revenue, 0.9))
evaluate_policy(policy_revenue)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;-4730.8200000000015
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The policy is again unprofitable, leading to substantial &lt;strong&gt;losses&lt;/strong&gt;. As before, this is not a problem of selecting the threshold as we can see in the plot below. The best we can do is set a threshold so high that we do not treat anyone and we make zero profits.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;x = np.linspace(0, 100, 100)
y = [evaluate_policy(lambda df : (model_revenue.predict(df[X]) &amp;gt; k*cost)) for k in x]

fig, ax = plt.subplots(figsize=(10, 3))
sns.lineplot(x=x, y=y).set(xlabel=&#39;Revenue Policy Threshold&#39;, title=&#39;Aggregate Effect&#39;);
ax.axhline(y=0, c=&#39;k&#39;, lw=3, ls=&#39;--&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/beyond_churn_39_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;The problem is that, in our setting, the churn probability of high-revenue customers does not decrease enough to make the &lt;code&gt;gift&lt;/code&gt; profitable. This is also partially due to the fact, often observed in reality, that high-revenue customers are also the least likely to churn, to begin with.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s now consider a more relevant set of policies: policies based on &lt;strong&gt;uplift&lt;/strong&gt;.&lt;/p&gt;
&lt;h3 id=&#34;3-target-churn-uplift-customers&#34;&gt;3. Target churn uplift customers&lt;/h3&gt;
&lt;p&gt;A more sensible approach would be to target customers whose &lt;code&gt;churn&lt;/code&gt; probability decreases the most when receiving the &lt;em&gt;1\$&lt;/em&gt; &lt;code&gt;gift&lt;/code&gt;. We estimate churn uplift using the &lt;a href=&#34;https://arxiv.org/abs/2004.14497&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;double-robust estimator&lt;/a&gt;, one of the best performing uplift models. If you are unfamiliar with meta-learners, I recommend starting from my introductory article.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://towardsdatascience.com/8a9c1e340832&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://towardsdatascience.com/8a9c1e340832&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;We import the doubly-robust learner from &lt;a href=&#34;https://econml.azurewebsites.net/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;econml&lt;/a&gt;, a Microsoft library.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from econml.dr import DRLearner

DR_learner_churn = DRLearner(model_regression=LGBMRegressor(), model_propensity=LGBMClassifier(), model_final=LGBMRegressor())
DR_learner_churn.fit(df[&#39;churn&#39;], df[W], X=df[X]);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now that we have estimated churn uplift, we might be tempted to just target customers with a high negative uplift (negative, since we want to &lt;em&gt;decrease&lt;/em&gt; churn). For example, we might send the &lt;code&gt;gift&lt;/code&gt; to all customers with an estimated uplift larger than the average churn.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;policy_churn_lift = lambda df : DR_learner_churn.effect(df[X]) &amp;lt; - np.mean(df.churn)
evaluate_policy(policy_churn_lift)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;-3925.2400000000002
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The policy is still unprofitable, leading to almost &lt;em&gt;4000\$&lt;/em&gt; in losses.&lt;/p&gt;
&lt;p&gt;The problem is that we haven&amp;rsquo;t considered the cost of the policy. In fact, decreasing the churn probability is &lt;strong&gt;only profitable for high-revenue customers&lt;/strong&gt;. Take the extreme case: avoiding churn of a customer that does not generate any revenue is not worth any intervention.&lt;/p&gt;
&lt;p&gt;Therefore, let&amp;rsquo;s only send the &lt;code&gt;gift&lt;/code&gt; to customers whose churn probability weighted by revenue decreases more than the cost of the gift.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;model_revenue_1 = LGBMRegressor().fit(X=df.loc[df[W] == 1, X], y=df.loc[df[W] == 1, &#39;revenue&#39;])

policy_churn_lift = lambda df : - DR_learner_churn.effect(df[X]) * model_revenue_1.predict(df[X]) &amp;gt; cost
evaluate_policy(policy_churn_lift)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;318.03000000000003
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This policy is finally profitable!&lt;/p&gt;
&lt;p&gt;However, we still have not considered one channel: the intervention might also affect the revenue of existing customers.&lt;/p&gt;
&lt;h3 id=&#34;4-target-revenue-uplift-customers&#34;&gt;4. Target revenue uplift customers&lt;/h3&gt;
&lt;p&gt;A symmetric approach to the previous one would be to consider only the impact on &lt;code&gt;revenue&lt;/code&gt;, ignoring the impact on churn. We could estimate the &lt;code&gt;revenue&lt;/code&gt; uplift for non-churning customers and treat only customers whose incremental effect on revenue, net of churn, is greater than the cost of the &lt;code&gt;gift&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;DR_learner_netrevenue = DRLearner(model_regression=LGBMRegressor(), model_propensity=LGBMClassifier(), model_final=LGBMRegressor())
DR_learner_netrevenue.fit(df.loc[df.churn==0, &#39;revenue&#39;], df.loc[df.churn==0, W], X=df.loc[df.churn==0, X]);
model_churn_1 = LGBMClassifier().fit(X=df.loc[df[W] == 1, X], y=df.loc[df[W] == 1, &#39;churn&#39;])

policy_netrevenue_lift = lambda df : DR_learner_netrevenue.effect(df[X]) * (1-model_churn_1.predict(df[X])) &amp;gt; cost
evaluate_policy(policy_netrevenue_lift)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;50.800000000000004
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This policy is profitable as well, but ignores the effect on churn. How do we combine this poilcy with the previous one?&lt;/p&gt;
&lt;h3 id=&#34;5-target-revenue-uplift-customers&#34;&gt;5. Target revenue uplift customers&lt;/h3&gt;
&lt;p&gt;The best way to efficiently &lt;strong&gt;combine&lt;/strong&gt; both the effect on churn and the effect on net revenue, is simply to estimate &lt;strong&gt;total revenue uplift&lt;/strong&gt;. The implied optimal policy is to treat customers whose total revenue uplift is greater than the cost of the &lt;code&gt;gift&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;DR_learner_revenue = DRLearner(model_regression=LGBMRegressor(), model_propensity=LGBMClassifier(), model_final=LGBMRegressor())
DR_learner_revenue.fit(df[&#39;revenue&#39;], df[W], X=df[X]);

policy_revenue_lift = lambda df : (DR_learner_revenue.effect(df[X]) &amp;gt; cost)
evaluate_policy(policy_revenue_lift)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;2028.2100000000003
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It looks like this is by far the best policy, generating an aggregate profit of more than &lt;em&gt;2000\$&lt;/em&gt;!&lt;/p&gt;
&lt;p&gt;The result is starking if we compare all the different policies.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;policies = [policy_churn, policy_revenue, policy_churn_lift, policy_netrevenue_lift, policy_revenue_lift] 
df_results = pd.DataFrame()
df_results[&#39;policy&#39;] = [&#39;churn&#39;, &#39;revenue&#39;, &#39;churn_L&#39;, &#39;netrevenue_L&#39;, &#39;revenue_L&#39;]
df_results[&#39;value&#39;] = [evaluate_policy(policy) for policy in policies]

fig, ax = plt.subplots()
sns.barplot(df_results, x=&#39;policy&#39;, y=&#39;value&#39;)
plt.axhline(0, c=&#39;k&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/beyond_churn_59_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;intuition-and-decomposition&#34;&gt;Intuition and Decomposition&lt;/h2&gt;
&lt;p&gt;If we compare the different policies, it is clear that targeting high-revenue or high-churn probability customers directly were the &lt;strong&gt;worst choices&lt;/strong&gt;. This is not necessarily always the case, but it happened in our simulated data because of two facts that are also common in many real scenarios:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Revenue and churn probability are negatively correlated&lt;/li&gt;
&lt;li&gt;The effect of the &lt;code&gt;gift&lt;/code&gt; on &lt;code&gt;churn&lt;/code&gt; (or &lt;code&gt;revenue&lt;/code&gt;) was not strongly negatively (or positively for &lt;code&gt;revenue&lt;/code&gt;) correlated with the baseline values&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Either one of these two facts can be enough to make targeting revenue or churn a bad strategy. What one should target instead is customers with a high &lt;strong&gt;incremental&lt;/strong&gt; effect. And it&amp;rsquo;s best to directly use as outcome the variable of interest, &lt;code&gt;revenue&lt;/code&gt; in this case, whenever available.&lt;/p&gt;
&lt;p&gt;To better understand the mechanism, we can &lt;strong&gt;decompose&lt;/strong&gt; the aggregate effect of a policy on profits into three parts.&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
\tau_{\pi} &amp;amp;= R_1 * (1 - C_1) - R_0 * (1 - C_0) = \newline
&amp;amp;= R_1 * (1 - C_1) - \underbrace{\hat{r}_1 * (1 - \hat{c}_0) + R_1 * (1 - C_0)} _ {\text{add and subtract}} - R_0 * (1 - C_0) = \newline
&amp;amp;= - R_1 * \tau_c + R_1 * (1 - C_0) - R_0 * (1 - C_0) = \newline
&amp;amp;= - R_1 * \tau_c + \tau_r * (1 - C_0) = \newline
&amp;amp;= \underbrace{- R_0 * \tau_c} _ {\text{incremental effect on churn}} + \underbrace{\tau_r * (1 - C_0)} _ {\text{incremental effect on revenue}} + \underbrace{\tau_r * \tau_c} _ {\text{interaction effect}}
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;This implies that there are &lt;strong&gt;three channels&lt;/strong&gt; that make treating a customer profitable.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;If it&amp;rsquo;s a &lt;em&gt;high-revenue&lt;/em&gt; customer and the treatment &lt;em&gt;decreases&lt;/em&gt; its churn probability&lt;/li&gt;
&lt;li&gt;If it&amp;rsquo;s a &lt;em&gt;non-churning&lt;/em&gt; customer and the treatment &lt;em&gt;increases&lt;/em&gt; its revenue&lt;/li&gt;
&lt;li&gt;It the treatment has a strong impact on &lt;em&gt;both&lt;/em&gt; its revenue and churn probability&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Targeting by churn uplift exploits only the first channel, targeting by net revenue uplift exploits only the second channel, and targeting by total revenue uplift exploits all three channels, making it the &lt;strong&gt;most effective&lt;/strong&gt; method.&lt;/p&gt;
&lt;h3 id=&#34;bonus-weighting&#34;&gt;Bonus: weighting&lt;/h3&gt;
&lt;p&gt;As highlighted by &lt;a href=&#34;https://www.hbs.edu/ris/Publication%20Files/14-020_2d6c9da0-94d3-4dd5-9952-d81feb432f61.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Lemmens, Gupta (2020)&lt;/a&gt;, sometimes it might be worth &lt;strong&gt;weighting&lt;/strong&gt; observations when estimating model uplift. In particular, it might be worth giving more weight to observations close to the treatment policy threshold.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;idea&lt;/strong&gt; is that weighting generally decreases the efficiency of the estimator. However, we are not interested in having correct estimates for all the observations, but rather we are interested in estimating the &lt;strong&gt;policy threshold&lt;/strong&gt; correctly. In fact, whether you estimate a net profit of &lt;em&gt;1\$&lt;/em&gt; or &lt;em&gt;1000\$&lt;/em&gt; it does not matter: the implied policy is the same: send the &lt;code&gt;gift&lt;/code&gt;. However, estimating a net profit of &lt;em&gt;1\$&lt;/em&gt; rather than &lt;em&gt;-1\$&lt;/em&gt; reverses the policy implications. Therefore, a large loss in accuracy away from the threshold sometimes is worth a small gain in accuracy at the threshold.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s try using negative exponential weights, decreasing in distance from the threshold.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;DR_learner_revenue_w = DRLearner(model_regression=LGBMRegressor(), model_propensity=LGBMClassifier(), model_final=LGBMRegressor())
w = np.exp(1 + np.abs(DR_learner_revenue.effect(df[X]) - cost))
DR_learner_revenue_w.fit(df[&#39;revenue&#39;], df[W], X=df[X], sample_weight=w);

policy_revenue_lift_w = lambda df : (DR_learner_revenue_w.effect(df[X]) &amp;gt; cost)
evaluate_policy(policy_revenue_lift_w)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;1398.19
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In our case, weighting is not worth: the implied policy is still profitable, but less than the one obtained with the unweighted model, &lt;em&gt;2028\$&lt;/em&gt;.&lt;/p&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;In this article, we have why and how one should go &lt;strong&gt;beyond&lt;/strong&gt; churn prediction and churn uplift modeling. In particular, one should concentrate on the final business objective of increasing profitability. This implies shifting the focus from &lt;em&gt;prediction&lt;/em&gt; to &lt;em&gt;uplift&lt;/em&gt; but also combining churn and revenue into a single outcome.&lt;/p&gt;
&lt;p&gt;An important caveat concerns the dimension of the data available. We have used a toy dataset that highly simplifies the problem in at least &lt;strong&gt;two dimensions&lt;/strong&gt;. First of all, backwards, we normally have longer time series that can (and should) be used for both prediction and modeling purposes. Second, forward, one should combine churn with a longer-run estimate of customer profitability, usually referred to as &lt;em&gt;customer lifetime value&lt;/em&gt;.&lt;/p&gt;
&lt;h3 id=&#34;references&#34;&gt;References&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Kennedy (2022), &lt;a href=&#34;https://arxiv.org/abs/2004.14497&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;ldquo;Towards Optimal Doubly Robust Estimation of Heterogeneous Causal Effects&amp;rdquo;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Bonvini, Kennedy, Keele (2021), &lt;a href=&#34;https://arxiv.org/abs/2306.17464&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;ldquo;Minimax Optimal Subgroup Identification&amp;rdquo;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Lemmens, Gupta (2020), &lt;a href=&#34;https://www.hbs.edu/ris/Publication%20Files/14-020_2d6c9da0-94d3-4dd5-9952-d81feb432f61.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;ldquo;Managing Churn to Maximize Profits&amp;rdquo;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;related-articles&#34;&gt;Related Articles&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://towardsdatascience.com/8a078996a113&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Evaluating Uplift Models&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://towardsdatascience.com/8a9c1e340832&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Understanding Meta Learners&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://towardsdatascience.com/ed4097dab27a&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Understanding AIPW, the Doubly-Robust Estimator&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;code&#34;&gt;Code&lt;/h3&gt;
&lt;p&gt;You can find the original Jupyter Notebook here:&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/beyond_churn.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/beyond_churn.ipynb&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Evaluating Uplift Models</title>
      <link>https://matteocourthoud.github.io/post/evaluate_uplift/</link>
      <pubDate>Wed, 12 Jul 2023 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/post/evaluate_uplift/</guid>
      <description>&lt;p&gt;One of the most widespread applications of causal inference in the industry is &lt;strong&gt;uplift modeling&lt;/strong&gt;, a.k.a. the estimation of Conditional Average Treatment Effects.&lt;/p&gt;
&lt;p&gt;When estimating the causal effect of a &lt;strong&gt;treatment&lt;/strong&gt; (a drug, ad, product, &amp;hellip;) on an &lt;strong&gt;outcome&lt;/strong&gt; of interest (a disease, firm revenue, customer satisfaction, &amp;hellip;), we are often not only interested in understanding whether the treatment works on average, but we would like to know for which &lt;strong&gt;subjects&lt;/strong&gt; (patients, users, customers, &amp;hellip;) it works better or worse.&lt;/p&gt;
&lt;p&gt;Estimating heterogeneous incremental effects, or uplift, is an essential intermediate step to improve &lt;strong&gt;targeting&lt;/strong&gt; of the policy of interest. For example, we might want to warn certain people that they are more likely to experience side effects from a drug or show an advertisement only to a specific set of customers.&lt;/p&gt;
&lt;p&gt;While there exist many methods to model uplift, it is not always clear which one to use in a specific application. Crucially, because of the &lt;strong&gt;fundamental problem of causal inference&lt;/strong&gt;, the objective of interest, the uplift, is never observed, and therefore we cannot validate our estimators as we would do with a machine learning prediction algorithm. We cannot set aside a validation set and pick the best-performing model since we have &lt;strong&gt;no ground truth&lt;/strong&gt;, not even in the validation set, and not even if we ran a randomized experiment.&lt;/p&gt;
&lt;p&gt;What can we do then? In this article, I try to cover the most popular methods used to &lt;strong&gt;evaluate uplift models&lt;/strong&gt;. If you are unfamiliar with uplift models, I suggest first reading my introductory article.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://towardsdatascience.com/8a9c1e340832&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://towardsdatascience.com/8a9c1e340832&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;uplift-and-promotional-emails&#34;&gt;Uplift and Promotional Emails&lt;/h2&gt;
&lt;p&gt;Imagine we were working in the marketing department of a product company interested in improving our &lt;strong&gt;email marketing campaign&lt;/strong&gt;. Historically, we mostly sent emails to new customers. However, now we would like to adopt a data-driven approach and target customers for whom the email has the highest positive impact on revenue. This impact is also called &lt;strong&gt;uplift&lt;/strong&gt; or &lt;strong&gt;incrementality&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s have a look at the data we have at our disposal. I import the data-generating process &lt;code&gt;dgp_promotional_email()&lt;/code&gt; from &lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/src/dgp.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;src.dgp&lt;/code&gt;&lt;/a&gt;. I also import some plotting functions and libraries from &lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/src/utils.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;src.utils&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;%matplotlib inline
%config InlineBackend.figure_format = &#39;retina&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from src.utils import *
from src.dgp import dgp_promotional_email
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;dgp = dgp_promotional_email(n=500)
df = dgp.generate_data()
df.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;new&lt;/th&gt;
      &lt;th&gt;age&lt;/th&gt;
      &lt;th&gt;sales_old&lt;/th&gt;
      &lt;th&gt;mail&lt;/th&gt;
      &lt;th&gt;sales&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;32.42&lt;/td&gt;
      &lt;td&gt;0.11&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0.03&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;34.92&lt;/td&gt;
      &lt;td&gt;0.36&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0.47&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;41.00&lt;/td&gt;
      &lt;td&gt;0.49&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0.40&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;50.02&lt;/td&gt;
      &lt;td&gt;0.35&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0.53&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;33.34&lt;/td&gt;
      &lt;td&gt;0.12&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0.02&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;We have information on 500 customers, for whom we observe whether they are &lt;code&gt;new&lt;/code&gt; customers, their &lt;code&gt;age&lt;/code&gt;, the sales they generated before the email campaign (&lt;code&gt;sales_old&lt;/code&gt;), whether they were sent the &lt;code&gt;mail&lt;/code&gt;, and the &lt;code&gt;sales&lt;/code&gt; after the email campaign.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;outcome&lt;/strong&gt; of interest is &lt;code&gt;sales&lt;/code&gt;, which we denote with the letter &lt;em&gt;Y&lt;/em&gt;. The &lt;strong&gt;treatment&lt;/strong&gt; or policy that we would like to improve is the &lt;code&gt;mail&lt;/code&gt; campaign, which we denote with the letter &lt;em&gt;W&lt;/em&gt;. We call all the remaining variables &lt;strong&gt;confounders&lt;/strong&gt; or control variables and we denote them with &lt;em&gt;X&lt;/em&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;Y = &#39;sales&#39;
W = &#39;mail&#39;
X = [&#39;age&#39;, &#39;sales_old&#39;, &#39;new&#39;]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The Dyrected Acyclic Graph (DAG) representing the causal relationships between the variables is the following. The causal relationship of interest is depicted in green.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-mermaid&#34;&gt;flowchart TD
classDef included fill:#DCDCDC,stroke:#000000,stroke-width:3px;

W((mail))
Y((sales))
X1((new))
X2((age))
X3((sales old))

W --&amp;gt; Y
X1 --&amp;gt; W
X1 --&amp;gt; Y
X2 --&amp;gt; Y
X3 --&amp;gt; Y

class W,Y,X1,X2,X3 included;

linkStyle 0 stroke:#2db88b,stroke-width:6px;
linkStyle 1,2,3,4 stroke:#003f5c,stroke-width:6px;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;From the DAG we see that the &lt;code&gt;new&lt;/code&gt; customer indicator is a confounder and needs to be controlled for in order to identify the effect of &lt;code&gt;mail&lt;/code&gt; on &lt;code&gt;sales.&lt;/code&gt; &lt;code&gt;age&lt;/code&gt; and &lt;code&gt;sales_old&lt;/code&gt; instead are not essential for estimation but could be helpful for identification. For more information on DAGs and control variables, you can check my introductory article.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://towardsdatascience.com/b63dc69e3d8c&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://towardsdatascience.com/b63dc69e3d8c&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The objective of uplift modeling is to recover the &lt;strong&gt;Individual Treatment Effects (ITE)&lt;/strong&gt; $\tau_i$, i.e. the incremental effect on &lt;code&gt;sales&lt;/code&gt; of sending the promotional &lt;code&gt;mail&lt;/code&gt;. We can express the ITE as the difference between two hypothetical quantities: the potential outcome of the customer if they had received the email, $Y_i^{(1)}$, minus the potential outcome of the customer if they had &lt;em&gt;not&lt;/em&gt; received the email, $Y_i^{(0)}$.
$$
\tau_i = Y_i^{(1)} - Y_i^{(0)}
$$&lt;/p&gt;
&lt;p&gt;Note that for each customer, we only observe one of the two realized outcomes, depending on whether they actually received the &lt;code&gt;mail&lt;/code&gt; or not. Therefore, the ITE are inherently unobservable. What can be estimated instead is the &lt;strong&gt;Conditional Average Treatment Effect (CATE)&lt;/strong&gt; i.e., the expected individual treatment effect $\tau_i$, conditional on covariates &lt;em&gt;X&lt;/em&gt;. For example, the average effect of the &lt;code&gt;mail&lt;/code&gt; on &lt;code&gt;sales&lt;/code&gt; for older customers (&lt;code&gt;age&lt;/code&gt; &amp;gt; 50).
$$
\tau(x) = \mathbb{E} \Big[ \ \tau_i \ \Big| \ X_i = x \Big]
$$&lt;/p&gt;
&lt;p&gt;In order to be able to recover the CATE, we need to make three assumptions.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Unconfoundedness&lt;/strong&gt;: $Y^{(0)}, Y^{(1)} \perp W \ | \ X$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Overlap&lt;/strong&gt;: $0 &amp;lt; e(X) &amp;lt; 1$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Consistency&lt;/strong&gt;: $Y = W \cdot Y^{(1)} + (1-W) \cdot Y^{(0)}$&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Where $e(X)$ is the &lt;strong&gt;propensity score&lt;/strong&gt; i.e., the expected probability of being treated, conditional on covariates &lt;em&gt;X&lt;/em&gt;.
$$
e(x) = \mathbb{E} \Big[ \ W_i \ \Big| \ X_i = x \Big]
$$&lt;/p&gt;
&lt;p&gt;In what follows, we will use machine learning methods to estimate the CATE $\tau(x)$, the propensity scores $e(x)$, and the conditional expectation function (CEF) of the outcome, $\mu(x)$
$$
\mu(x) = \mathbb{E} \Big[ \ Y_i \ \Big| \ X_i = x \Big]
$$&lt;/p&gt;
&lt;p&gt;We use &lt;a href=&#34;https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Random Forest Regression&lt;/a&gt; algorithms to model the CATE and the outcome CEF, while we use &lt;a href=&#34;https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Logistic Regression&lt;/a&gt; to model the propensity score.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from sklearn.ensemble import RandomForestRegressor
from sklearn.linear_model import LogisticRegressionCV

model_tau = RandomForestRegressor(max_depth=2)
model_y = RandomForestRegressor(max_depth=2)
model_e = LogisticRegressionCV()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In this article, we do not fine-tune the underlying machine learning models, but fine-tuning is strongly recommended to improve the accuracy of uplift models (for example, with auto-ml libraries like &lt;a href=&#34;https://microsoft.github.io/FLAML/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;FLAML&lt;/a&gt;).&lt;/p&gt;
&lt;h2 id=&#34;uplift-models&#34;&gt;Uplift Models&lt;/h2&gt;
&lt;p&gt;There exist &lt;strong&gt;many methods&lt;/strong&gt; to model uplift or, in other words, to estimate Conditional Average Treatment Effects (CATE). Since the objective of this article is to compare methods to &lt;em&gt;evaluate&lt;/em&gt; uplift models, we will not explain the methods in detail. For a gentle introduction, you can check &lt;a href=&#34;https://medium.com/towards-data-science/understanding-meta-learners-8a9c1e340832&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;my introductory article on meta learners&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The learners that we will consider are the following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;S-learner or single-learner, introduced by &lt;a href=&#34;https://arxiv.org/abs/1706.03461&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Kunzel, Sekhon, Bickel, Yu (2017)&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;T-learner or two-learner, introduced by &lt;a href=&#34;https://arxiv.org/abs/1706.03461&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Kunzel, Sekhon, Bickel, Yu (2017)&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;X-learner or cross-learner, introduced by &lt;a href=&#34;https://arxiv.org/abs/1706.03461&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Kunzel, Sekhon, Bickel, Yu (2017)&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;R-learner or &lt;a href=&#34;https://www.jstor.org/stable/1912705&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Robinson&lt;/a&gt;-learner introduced by &lt;a href=&#34;https://arxiv.org/abs/1712.04912&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Nie, Wager (2017)&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;DR-learner or doubly-robust-learner, introduced by &lt;a href=&#34;https://arxiv.org/abs/2004.14497&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Kennedy (2022)&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We import all the model from Microsoft&amp;rsquo;s &lt;a href=&#34;https://econml.azurewebsites.net/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;econml&lt;/a&gt; library.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from src.learners_utils import *
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from econml.metalearners import SLearner, TLearner, XLearner
from econml.dml import NonParamDML
from econml.dr import DRLearner

S_learner = SLearner(overall_model=model_y)
T_learner = TLearner(models=clone(model_y))
X_learner = XLearner(models=model_y, propensity_model=model_e, cate_models=model_tau)
R_learner = NonParamDML(model_y=model_y, model_t=model_e, model_final=model_tau, discrete_treatment=True)
DR_learner = DRLearner(model_regression=model_y, model_propensity=model_e, model_final=model_tau)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We &lt;code&gt;fit()&lt;/code&gt; the models on the data, specifying the outcome variable &lt;em&gt;Y&lt;/em&gt;, the treatment variable &lt;em&gt;W&lt;/em&gt; and covariates &lt;em&gt;X&lt;/em&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;names = [&#39;SL&#39;, &#39;TL&#39;, &#39;XL&#39;, &#39;RL&#39;, &#39;DRL&#39;]
learners = [S_learner, T_learner, X_learner, R_learner, DR_learner]
for learner in learners:
    learner.fit(df[Y], df[W], X=df[X])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We are now ready to evaluate the models! Which model should we choose?&lt;/p&gt;
&lt;h2 id=&#34;oracle-loss-functions&#34;&gt;Oracle Loss Functions&lt;/h2&gt;
&lt;p&gt;The main problem of evaluating uplift models is that, even with a validation set and even with a randomized experiment or AB test, we do &lt;strong&gt;not observe&lt;/strong&gt; our metric of interest: the Individual Treatment Effects. In fact, we only observe the realized outcomes, $Y_i^{(0)}$ for untreated customers and $Y_i^{(1)}$ for treated customers. Therefore, for no customer we can compute the individual treatment effect in the validation data, $\tau_i = Y_i^{(1)} - Y_i^{(0)}$.&lt;/p&gt;
&lt;p&gt;Can we still do something to &lt;strong&gt;evaluate&lt;/strong&gt; our estimators?&lt;/p&gt;
&lt;p&gt;The answer is yes, but before giving more details, let&amp;rsquo;s first understand what we would do if we &lt;strong&gt;could observe&lt;/strong&gt; the Individual Treatment Effects $\tau_i$.&lt;/p&gt;
&lt;h3 id=&#34;oracle-mse-loss&#34;&gt;Oracle MSE Loss&lt;/h3&gt;
&lt;p&gt;If we could observe the individual treatment effects (but we don&amp;rsquo;t, hence the &amp;ldquo;oracle&amp;rdquo; attribute), we could try to measure how far our estimates $\hat{\tau}(X_i)$ are from the true values $\tau_i$. This is what we normally do in machine learning when we want to evaluate a prediction method: we set aside a validation dataset and we compare predicted and true values on that data. There exist plenty of loss functions to evaluate prediction accurary, so let&amp;rsquo;s concentrate on the most popular one: the &lt;strong&gt;Mean Squared Error (MSE) loss&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;$$
\mathcal{L} _ {oracle-MSE}(\hat{\tau}) = \frac{1}{n} \sum _ {i=1}^{n} \left( \hat{\tau}(X_i) - \tau(X_i) \right)^2
$$&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def loss_oracle_mse(data, learner):
    tau = learner.effect(data[X])
    return np.mean((tau - data[&#39;effect_on_sales&#39;])**2)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The function &lt;code&gt;compare_methods&lt;/code&gt; prints and plots evaluation metrics computed on a separate validation dataset.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def compare_methods(learners, names, loss, title=None, subtitle=&#39;lower is better&#39;):
    data = dgp.generate_data(seed_data=1, seed_assignment=1, keep_po=True)
    results = pd.DataFrame({
        &#39;learner&#39;: names,
        &#39;loss&#39;: [loss(data.copy(), learner) for learner in learners]
    })
    fig, ax = plt.subplots(1, 1, figsize=(6, 4))
    sns.barplot(data=results, x=&amp;quot;learner&amp;quot;, y=&#39;loss&#39;).set(ylabel=&#39;&#39;)
    plt.suptitle(title, y=1.02)
    plt.title(subtitle, fontsize=12, fontweight=None, y=0.94)
    return results
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;compare_methods(learners, names, loss_oracle_mse, title=&#39;Oracle MSE Loss&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;learner&lt;/th&gt;
      &lt;th&gt;loss&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;SL&lt;/td&gt;
      &lt;td&gt;0.002932&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;TL&lt;/td&gt;
      &lt;td&gt;0.004637&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;XL&lt;/td&gt;
      &lt;td&gt;0.000936&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;RL&lt;/td&gt;
      &lt;td&gt;0.000990&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;DRL&lt;/td&gt;
      &lt;td&gt;0.000577&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;&lt;img src=&#34;img/evaluate_uplift_31_1.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;In this case, we see that the T-learner clearly performs worst, with the S-learner just behind. On the other hand, the X-, R- and DR-learners perform significantly better, with the &lt;strong&gt;DR-learner winning&lt;/strong&gt; the race.&lt;/p&gt;
&lt;p&gt;However, this might &lt;em&gt;not&lt;/em&gt; be the best loss function to evaluate our uplift model. In fact, uplift modeling is just an intermediate step towards our ultimate goal: improving revenue.&lt;/p&gt;
&lt;h3 id=&#34;oracle-policy-gain&#34;&gt;Oracle Policy Gain&lt;/h3&gt;
&lt;p&gt;Since our ultimate goal is to &lt;strong&gt;improve revenue&lt;/strong&gt;, we could evaluate estimators by how much they increase revenue, given a certain policy function. Suppose, for example, that we had a $0.01$\$ cost of sending an email. Then, our policy would be to treat each costumer that has a predicted Conditional Average Treatment Effect above $0.01$\$.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;cost = 0.01
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;How much would our revenue actually increase? Let&amp;rsquo;s define with $d(\hat{\tau})$ our policy function, such that $d=1$ if $\tau &amp;gt;= 0.1$ and $d=0$ otherwise. Then our &lt;em&gt;gain&lt;/em&gt; (higher is better) function is:
$$
\mathcal{G} _ {oracle-POLICY}(\hat{\tau}) = \frac{1}{n} \sum _ {i=1}^{n} d(\hat{\tau}) (\tau_i - c)
$$&lt;/p&gt;
&lt;p&gt;Again, this is an &amp;ldquo;oracle&amp;rdquo; loss function that &lt;strong&gt;cannot be computed&lt;/strong&gt; in reality since we do not observe the individual treatment effects.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def gain_oracle_policy(data, learner):
    tau_hat = learner.effect(data[X])
    return np.sum((data[&#39;effect_on_sales&#39;] - cost) * (tau_hat &amp;gt; cost))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;compare_methods(learners, names, gain_oracle_policy, title=&#39;Oracle Policy Gain&#39;, subtitle=&#39;higher is better&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;learner&lt;/th&gt;
      &lt;th&gt;loss&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;SL&lt;/td&gt;
      &lt;td&gt;0.00&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;TL&lt;/td&gt;
      &lt;td&gt;4.23&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;XL&lt;/td&gt;
      &lt;td&gt;10.98&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;RL&lt;/td&gt;
      &lt;td&gt;10.43&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;DRL&lt;/td&gt;
      &lt;td&gt;12.33&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;&lt;img src=&#34;img/evaluate_uplift_38_1.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;In this case, the S-learner is clearly the worst performer, leading to no effect on revenues. The T-learner leads to modest gains while the X-, R- and DR- learners all lead to aggregate gains, with the &lt;strong&gt;X-learner slightly ahead&lt;/strong&gt;.&lt;/p&gt;
&lt;h2 id=&#34;practical-loss-functions&#34;&gt;Practical Loss Functions&lt;/h2&gt;
&lt;p&gt;In the previous section, we have seen two examples of loss functions that we would like to compute if we could observe the Individual Treatment Effects $\tau_i$. However, in practice, even with a randomized experiment and even with a validation set, we do not observe the ITE,our object of interest. We will now cover some measures that try to evaluate uplift models, given this practical constraint.&lt;/p&gt;
&lt;h3 id=&#34;outcome-loss&#34;&gt;Outcome Loss&lt;/h3&gt;
&lt;p&gt;The first and simplest approach is to switch to a different loss variable. While we cannot observe the Individual Treatment Effects, $\tau_i$, we can still observe our outcome $y_i$. This is not exactly our object of interest, but we might expect an uplift model that performs well in terms of predicting $y$ to also produce good estimates of $\tau$.&lt;/p&gt;
&lt;p&gt;One such loss function could be the &lt;strong&gt;Outcome MSE loss&lt;/strong&gt;, which is the usual MSE loss function for prediction methods.
$$
\mathcal{L}_{Y}(\hat{\mu}) = \frac{1}{n} \sum _ {i=1}^{n} \Big( \hat{\mu}(X_i, W_i) - Y_i \Big)^2
$$&lt;/p&gt;
&lt;p&gt;The problem here is that not all models directly produce an estimate of $\mu(x)$ and, even when they do, it is not the object of interest.&lt;/p&gt;
&lt;h3 id=&#34;prediction-to-prediction-loss&#34;&gt;Prediction to Prediction Loss&lt;/h3&gt;
&lt;p&gt;Another very simple approach could be to compare the predictions of the model trained on the training set with the predictions of another model trained on the validation set. While intuitive, this appraoch could be &lt;strong&gt;extremely misleading&lt;/strong&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def loss_pred(data, learner):
    tau = learner.effect(data[X])
    learner2 = copy.deepcopy(learner).fit(data[Y], data[W], X=data[X])
    tau2 = learner2.effect(data[X])
    return np.mean((tau - tau2)**2)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;compare_methods(learners, names, loss_pred, &#39;Prediction to Prediction Loss&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;learner&lt;/th&gt;
      &lt;th&gt;loss&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;SL&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;TL&lt;/td&gt;
      &lt;td&gt;0.007342&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;XL&lt;/td&gt;
      &lt;td&gt;0.000366&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;RL&lt;/td&gt;
      &lt;td&gt;0.134137&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;DRL&lt;/td&gt;
      &lt;td&gt;0.000933&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;&lt;img src=&#34;img/evaluate_uplift_47_1.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Unsurprisingly, this metric performs extremely bad, and you should &lt;strong&gt;never use it&lt;/strong&gt;, since it rewards models that are consistent, irrespectively of their quality. A model that always predicts a random constant CATE for each observations would obtain a perfect score.&lt;/p&gt;
&lt;h3 id=&#34;distribution-loss&#34;&gt;Distribution Loss&lt;/h3&gt;
&lt;p&gt;A different approach is to ask: how well can we match the distribution of potential outcomes? We can do this exarcise for either the &lt;em&gt;treated&lt;/em&gt; or &lt;em&gt;untreated&lt;/em&gt; potential outcomes. Let&amp;rsquo;s take the last case. Suppose we take the observed &lt;code&gt;sales&lt;/code&gt; for customers that did &lt;em&gt;not&lt;/em&gt; receive the &lt;code&gt;mail&lt;/code&gt; and the observed &lt;code&gt;sales&lt;/code&gt; &lt;em&gt;minus&lt;/em&gt; the estimated CATE $\hat{\tau}(x)$ for customers that did receive the &lt;code&gt;mail&lt;/code&gt;. By the &lt;strong&gt;unconfoundedness&lt;/strong&gt; assumption, these two distributions of the untreated potential outcome should be similar, conditional on covariates $X$.&lt;/p&gt;
&lt;p&gt;Therefore, we expect the distance between the two distributions to be close if we correctly estimated the treatment effects.
$$
dist \ \Big( \ {Y_i, X_i | W_i=0 } \ , \ {Y_i - \hat{\tau}(X_i), X_i | W_i=1 } \ \Big)
$$&lt;/p&gt;
&lt;p&gt;We can also do the same exercise for the &lt;em&gt;treated&lt;/em&gt; potential outcome.&lt;/p&gt;
&lt;p&gt;$$
dist \ \Big( \ {Y_i + \hat{\tau}(X_i), X_i | W_i=0 } \ , \ {Y_i, X_i | W_i=1 } \ \Big)
$$&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from dcor import energy_distance

def loss_dist(data, learner):
    tau = learner.effect(data[X])
    data.loc[data.mail==1, &#39;sales&#39;] -= tau[data.mail==1]
    return energy_distance(data.loc[data.mail==0, [Y] + X], data.loc[data.mail==1, [Y] + X], exponent=2)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;compare_methods(learners, names, loss_dist, &#39;Distribution Loss&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;learner&lt;/th&gt;
      &lt;th&gt;loss&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;SL&lt;/td&gt;
      &lt;td&gt;1.728523&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;TL&lt;/td&gt;
      &lt;td&gt;1.733941&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;XL&lt;/td&gt;
      &lt;td&gt;1.733993&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;RL&lt;/td&gt;
      &lt;td&gt;1.736704&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;DRL&lt;/td&gt;
      &lt;td&gt;1.735105&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;&lt;img src=&#34;img/evaluate_uplift_52_1.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;This measure is extremely noisy and rewards the S-learner followed by the T-learner which are actually the two worst performing models.&lt;/p&gt;
&lt;h3 id=&#34;above-below-median-difference&#34;&gt;Above-below Median Difference&lt;/h3&gt;
&lt;p&gt;The above-below median loss tries to answer the question: is our uplift model detecting &lt;strong&gt;any heterogeneity&lt;/strong&gt;? In particular, if we take the validation set and we split the sample into above-median and below median predicted uplift $\hat{\tau}(x)$, how big is the actual difference in average effect, estimated with a difference-in-means estimator? We would expect better estimators to better split the sample into high-effects and low-effects.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from statsmodels.formula.api import ols 

def loss_ab(data, learner):
    tau = learner.effect(data[X]) + np.random.normal(0, 1e-8, len(data))
    data[&#39;above_median&#39;] = tau &amp;gt;= np.median(tau)
    param = ols(&#39;sales ~ mail * above_median&#39;, data=data).fit().params[-1]
    return param
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;compare_methods(learners, names, loss_ab, title=&#39;Above-below Median Difference&#39;, subtitle=&#39;higher is better&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;learner&lt;/th&gt;
      &lt;th&gt;loss&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;SL&lt;/td&gt;
      &lt;td&gt;-0.008835&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;TL&lt;/td&gt;
      &lt;td&gt;0.221423&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;XL&lt;/td&gt;
      &lt;td&gt;0.093177&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;RL&lt;/td&gt;
      &lt;td&gt;0.134629&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;DRL&lt;/td&gt;
      &lt;td&gt;0.075319&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;&lt;img src=&#34;img/evaluate_uplift_57_1.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Unfortunately, the above-below median difference rewards the T-learner, which is among the worst performing models.&lt;/p&gt;
&lt;p&gt;It&amp;rsquo;s important to note that the difference-in-means estimators in the two groups (above- and below- median $\hat{\tau}(x)$) are &lt;strong&gt;not guaranteed to be unbiased&lt;/strong&gt;, even if the data came from a randomized experiment. In fact, we have split the two groups on a variable, $\hat{\tau}(x)$, that is highly endogenous. Therefore, the method should be used with a grain of salt.&lt;/p&gt;
&lt;h3 id=&#34;uplift-curve&#34;&gt;Uplift Curve&lt;/h3&gt;
&lt;p&gt;An extension of the above-below median test is the &lt;strong&gt;uplift curve&lt;/strong&gt;. The idea is simple: instead of splitting the sample into two groups based on the median (0.5 quantile), why not split the data into more groups (more quantiles)?&lt;/p&gt;
&lt;p&gt;For each group, we compute the difference-in-means estimate, and we plot its cumulative sum against the corresponding quantile. The result is called &lt;strong&gt;uplift curve&lt;/strong&gt;. The interpretation is simple: the higher the curve, the better we are able to separate high- from low-effect observations. However, also the same &lt;strong&gt;disclaimer&lt;/strong&gt; applies: the difference-in-means estimates are not unbiased. Therefore, they should be used with a grain of salt.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def generate_uplift_curve(df):
    Q = 20
    df_q = pd.DataFrame()
    data = dgp.generate_data(seed_data=1, seed_assignment=1, keep_po=True)
    ate = np.mean(data[Y][data[W]==1]) - np.mean(data[Y][data[W]==0])
    for learner, name in zip(learners, names):
        data[&#39;tau_hat&#39;] = learner.effect(data[X])
        data[&#39;q&#39;] = pd.qcut(-data.tau_hat + np.random.normal(0, 1e-8, len(data)), q=Q, labels=False)
        for q in range(Q):
            temp = data[data.q &amp;lt;= q]
            uplift = (np.mean(temp[Y][temp[W]==1]) - np.mean(temp[Y][temp[W]==0])) * q / (Q-1)
            df_q = pd.concat([df_q, pd.DataFrame({&#39;q&#39;: [q], &#39;uplift&#39;: [uplift], &#39;learner&#39;: [name]})], ignore_index=True)
    
    fig, ax = plt.subplots(1, 1, figsize=(8, 5))
    sns.lineplot(x=range(Q), y=ate*range(Q)/(Q-1), color=&#39;k&#39;, ls=&#39;--&#39;, lw=3)
    sns.lineplot(x=&#39;q&#39;, y=&#39;uplift&#39;, hue=&#39;learner&#39;, data=df_q);
    plt.suptitle(&#39;Uplift Curve&#39;, y=1.02, fontsize=28, fontweight=&#39;bold&#39;)
    plt.title(&#39;higher is better&#39;, fontsize=14, fontweight=None, y=0.96)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;generate_uplift_curve(df)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/evaluate_uplift_62_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;While probably not the best method to &lt;em&gt;evaluate&lt;/em&gt; uplift models, the uplift curve is very important in &lt;strong&gt;understanding&lt;/strong&gt; and &lt;strong&gt;implementing&lt;/strong&gt; them. In fact, for each model, it tells us that is the expected average treatment effect (y-axis) as we increase the share of the treated population (x-axis).&lt;/p&gt;
&lt;h3 id=&#34;nearest-neighbor-match&#34;&gt;Nearest Neighbor Match&lt;/h3&gt;
&lt;p&gt;The last couple of methods we analyzed, aggregated data in order to understand whether the methods work on larger groups. The nearest neighbor match tries instead to understand how well an uplift model predicts individual treatment effects. However, since the ITEs are not observable, it tries to build a &lt;strong&gt;proxy by matching&lt;/strong&gt; treated and control observations on observable characteristics $X$.&lt;/p&gt;
&lt;p&gt;For example, if we take all treated observations ($i: W_i=1$), and we find the nearest neighbor in the control group ($NN_0(X_i)$), the corresponding MSE loss function is
$$
\mathcal{L} _ {NN}(\hat{\tau}) = \frac{1}{n} \sum _ {i: W_i=1} \Big( \hat{\tau}(X_i) - (Y_i - NN_0(X_i)) \Big)^2
$$&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from scipy.spatial import KDTree

def loss_nn(data, learner):
    tau_hat = learner.effect(data[X])
    nn0 = KDTree(data.loc[data[W]==0, X].values)
    control_index = nn0.query(data.loc[data[W]==1, X], k=1)[-1]
    tau_nn = data.loc[data[W]==1, Y].values - data.iloc[control_index, :][Y].values
    return np.mean((tau_hat[data[W]==1] - tau_nn)**2)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;compare_methods(learners, names, loss_nn, title=&#39;Nearest Neighbor Loss&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;learner&lt;/th&gt;
      &lt;th&gt;loss&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;SL&lt;/td&gt;
      &lt;td&gt;0.050478&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;TL&lt;/td&gt;
      &lt;td&gt;0.051301&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;XL&lt;/td&gt;
      &lt;td&gt;0.047102&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;RL&lt;/td&gt;
      &lt;td&gt;0.046684&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;DRL&lt;/td&gt;
      &lt;td&gt;0.046652&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;&lt;img src=&#34;img/evaluate_uplift_67_1.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;In this case, the nearest neighbor loss performs quite well, identifying the two worse performing methods, the S- and T-learner.&lt;/p&gt;
&lt;h3 id=&#34;ipw-loss&#34;&gt;IPW Loss&lt;/h3&gt;
&lt;p&gt;The Inverse Probability Weighting (IPW) loss function was first proposed by &lt;a href=&#34;https://proceedings.mlr.press/v67/gutierrez17a/gutierrez17a.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Gutierrez, Gerardy (2017)&lt;/a&gt;, and it is the first of three metrics that we are going to see that uses a &lt;strong&gt;pseudo-outcome&lt;/strong&gt; $Y^{*}$ to evaluate the estimator. Pseudo-outcomes are variables whose expected value is the Conditional Average Treatment Effect, but that are too volatile to be directly used as estimates. For a more detailed explanation of pseudo-outcomes, I suggest &lt;a href=&#34;https://towardsdatascience.com/920177462149&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;my article on causal regression trees&lt;/a&gt;. The pseudo-outcome corresponding to the IPW loss is
$$
Y^* _ {IPW} = Y_i \frac{W_i - \hat{e}(X_i)}{\hat{e}(X_i)(1 - \hat{e}(X_i))}
$$&lt;/p&gt;
&lt;p&gt;so that the corresponding loss function is
$$
\mathcal{L} _ {IPW} = \frac{1}{n} \sum_{i=1}^{n} \left( \hat{\tau}(X_i) - Y_i \ \frac{W_i - \hat{e}(X_i)}{\hat{e}(X_i)(1 - \hat{e}(X_i))} \right)^2
$$&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def loss_ipw(data, learner):
    tau_hat = learner.effect(data[X])
    e_hat = clone(model_e).fit(data[X], data[W]).predict_proba(data[X])[:,1]
    tau_gg = data[Y] * (data[W] - e_hat) / (e_hat * (1 - e_hat))
    return np.mean((tau_hat - tau_gg)**2)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;compare_methods(learners, names, loss_ipw, title=&#39;IPW Loss&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;learner&lt;/th&gt;
      &lt;th&gt;loss&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;SL&lt;/td&gt;
      &lt;td&gt;1.170917&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;TL&lt;/td&gt;
      &lt;td&gt;1.153752&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;XL&lt;/td&gt;
      &lt;td&gt;1.172517&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;RL&lt;/td&gt;
      &lt;td&gt;1.172934&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;DRL&lt;/td&gt;
      &lt;td&gt;1.171769&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;&lt;img src=&#34;img/evaluate_uplift_72_1.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;The IPW loss is extremely noisy. A solution is to use its more robust variations, the R-loss or the DR-loss which we present next.&lt;/p&gt;
&lt;h3 id=&#34;r-loss&#34;&gt;R Loss&lt;/h3&gt;
&lt;p&gt;The R-loss was introduced together with the R-learner by &lt;a href=&#34;https://arxiv.org/abs/1712.04912&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Nie, Wager (2017)&lt;/a&gt;, and it is essentially the &lt;strong&gt;objective function&lt;/strong&gt; of the R-learner. As for the IPW-loss, the idea is to try to match a pseudo outcome whose expected value is the Conditional Average Treatment Effect.
$$
Y^* _ {R} = \frac{Y_i - \hat{\mu}_W(X_i)}{W_i - \hat{e}(X_i)}
$$&lt;/p&gt;
&lt;p&gt;The corresponding loss function is
$$
\mathcal{L}_{R} = \frac{1}{n} \sum _ {i=1}^{n} \left( \hat{\tau}(X_i) -  \frac{Y_i - \hat{\mu}_W(X_i)}{W_i - \hat{e}(X_i)} \right)^2
$$&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def loss_r(data, learner):
    tau_hat = learner.effect(data[X])
    y_hat = clone(model_y).fit(df[X + [W]], df[Y]).predict(data[X + [W]])
    e_hat = clone(model_e).fit(df[X], df[W]).predict_proba(data[X])[:,1]
    tau_nw = (data[Y] - y_hat) / (data[W] - e_hat)
    return np.mean((tau_hat - tau_nw)**2)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;results = compare_methods(learners, names, loss_r, title=&#39;R Loss&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/evaluate_uplift_77_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;The R-loss is sensibly less noisy than the IPW loss and it clearly isolates the S-learner. However, it tends to favor its corresponding learner, the R-learner.&lt;/p&gt;
&lt;h3 id=&#34;dr-loss&#34;&gt;DR Loss&lt;/h3&gt;
&lt;p&gt;The DR-loss is the &lt;strong&gt;objective function&lt;/strong&gt; of the DR-learner, and it was first introduced by &lt;a href=&#34;https://arxiv.org/abs/1909.05299&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Saito, Yasui (2020)&lt;/a&gt;. As for the IPW- and the R-loss, the idea is to try to match a pseudo outcome, whose expected value is the Conditional Average Treatment Effect. The DR pseudo-outcome is strongly related to the &lt;a href=&#34;https://towardsdatascience.com/ed4097dab27a&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;AIPW estimator&lt;/a&gt;, also known as doubly-robust estimator, hence the DR name.
$$
Y^* _ {DR} = \hat{\mu}_1(X_i) - \hat{\mu}_0(X_i) + (Y_i - \hat{\mu}_W(X_i)) \ \frac{W_i - \hat{e}(X_i)}{\hat{e}(X_i)(1 - \hat{e}(X_i))}
$$&lt;/p&gt;
&lt;p&gt;The corresponding loss function is
$$
\mathcal{L} _ {DR} = \frac{1}{n} \sum _ {i=1}^{n} \left( \hat{\tau}(X_i) - \hat{\mu}_1(X_i) + \hat{\mu}_0(X_i) - (Y_i - \hat{\mu}_W(X_i)) \ \frac{W_i - \hat{e}(X_i)}{\hat{e}(X_i)(1 - \hat{e}(X_i))} \right)^2
$$&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def loss_dr(data, learner):
    tau_hat = learner.effect(data[X])
    y_hat = clone(model_y).fit(df[X + [W]], df[Y]).predict(data[X + [W]])
    mu1 = clone(model_y).fit(df[X + [W]], df[Y]).predict(data[X + [W]].assign(mail=1))
    mu0 = clone(model_y).fit(df[X + [W]], df[Y]).predict(data[X + [W]].assign(mail=0))
    e_hat = clone(model_e).fit(df[X], df[W]).predict_proba(data[X])[:,1]
    tau_nw = mu1 - mu0 + (data[Y] - y_hat) * (data[W] - e_hat) / (e_hat * (1 - e_hat))
    return np.mean((tau_hat - tau_nw)**2)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;results = compare_methods(learners, names, loss_dr, title=&#39;DR Loss&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/evaluate_uplift_82_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;As for the R-loss, the DR-loss tends to favor its corresponding learner, the DR-learner. However, it provides a more accurate ranking in terms of algorithms&amp;rsquo; accuracy.&lt;/p&gt;
&lt;h3 id=&#34;empirical-policy-gain&#34;&gt;Empirical Policy Gain&lt;/h3&gt;
&lt;p&gt;The last loss function that we are going to analyze is different from all the others we have seen so far since it does &lt;em&gt;not&lt;/em&gt; focus on how well we are able to estimate the treatment effects but rather on how well would the corresponding &lt;strong&gt;optimal treatment policy&lt;/strong&gt; performs. In particular, &lt;a href=&#34;https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3111957&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Hitsch, Misra, Zhang (2023)&lt;/a&gt; propose the following gain function:&lt;/p&gt;
&lt;p&gt;$$
\mathcal{G} _ {HMZ} = \sum _ {i=1}^{n} \left( W_i \cdot d(\hat{\tau}) \cdot \frac{Y_i - c}{\hat{e}(X_i)} + (1-W_i) \cdot (1-d(\hat{\tau})) \cdot \frac{Y_i}{1-\hat{e}(X_i)} \right)
$$&lt;/p&gt;
&lt;p&gt;where $c$ is the treatment cost and $d$ is the optimal treatment policy given the estimated CATE $\hat{\tau}(X_i)$. In our case, we assume an individual treatment cost of $c=0.01$\$, so that the optimal policy is to treat every customer with an estimated CATE larger than 0.01.&lt;/p&gt;
&lt;p&gt;The terms $W_i \cdot d(X_i)$ and $(1-W_i) \cdot (1-d(X_i))$ imply that we use for the calculation only individuals for whom the actual treatment &lt;em&gt;W&lt;/em&gt; corresponds with the optimal one, &lt;em&gt;d&lt;/em&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def gain_policy(data, learner):
    tau_hat = learner.effect(data[X])
    e_hat = clone(model_e).fit(data[X], data[W]).predict_proba(data[X])[:,1]
    d = tau_hat &amp;gt; cost
    return np.sum((d * data[W] * (data[Y] - cost)/ e_hat + (1-d) * (1-data[W]) * data[Y] / (1-e_hat)))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;results = compare_methods(learners, names, gain_policy, title=&#39;Empirical Policy Gain&#39;, subtitle=&#39;higher is better&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/evaluate_uplift_87_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;The empirical policy gain performs very well, isolating the two worst performing methods, the S- and T-learners.&lt;/p&gt;
&lt;h2 id=&#34;meta-studies&#34;&gt;Meta Studies&lt;/h2&gt;
&lt;p&gt;In this article we have introduced a wide variety of methods to evaluate uplift models, a.k.a. Conditional Average Treatment Effect estimators. We have also tested in our simulated dataset, which is a very special and limited example. How do these metrics &lt;strong&gt;perform&lt;/strong&gt; in general?&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/1804.05146&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Schuler, Baiocchi, Tibshirani, Shah (2018)&lt;/a&gt; compares the S-loss, T-loss, R-loss, on &lt;strong&gt;simulated data&lt;/strong&gt;, for the corresponding estimators. They find that the R-loss &amp;ldquo;&lt;em&gt;is the validation set metric that, when optimized, most consistently leads to the selection of a high-performing model&lt;/em&gt;&amp;rdquo;. The authors also detect the so-called &lt;strong&gt;congeniality bias&lt;/strong&gt;: metrics such as the R- or DR-loss tend to be biased towards the corresponding learner.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2302.02923&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Curth, van der Schaar (2023)&lt;/a&gt; studies a broader array of learners from a &lt;strong&gt;theoretical perspective&lt;/strong&gt;. They find that &amp;ldquo;&lt;em&gt;no existing selection criterion is globally best across all experimental conditions we consider&lt;/em&gt;&amp;rdquo;.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2211.01939&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Mahajan, Mitliagkas, Neal, Syrgkanis (2023)&lt;/a&gt; is the &lt;strong&gt;most comprehensive&lt;/strong&gt; study in terms of scope. The authors compare many metrics on 144 datasets and 415 estimators. They find that “&lt;em&gt;no metric significantly dominates the rest&lt;/em&gt;” but “&lt;em&gt;metrics that use DR elements seem to always be among the candidate winners&lt;/em&gt;”.&lt;/p&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;In this article, we have explored multiple methods to evaluate uplift models. The &lt;strong&gt;main challenge&lt;/strong&gt; is the unobservability of the variable of interest, the Individual Treatment Effects. Therefore, different methods try to evaluate uplift models either using other variables, using proxy outcomes, or approximating the effect of implied optimal policies.&lt;/p&gt;
&lt;p&gt;It is hard to recommend using a single method since there is &lt;strong&gt;no consensus&lt;/strong&gt; on which one performs best, neither from a theoretical nor from an empirical perspective. Loss functions that use R- and DR- elements tend to perform &lt;strong&gt;consistently better&lt;/strong&gt;, but are also biased towards the corresponding learners. Understanding how these metrics work, however, can help in understanding their biases and limitations in order to make the most appropriate decisions depending on the specific scenario.&lt;/p&gt;
&lt;h3 id=&#34;references&#34;&gt;References&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Curth, van der Schaar (2023), &lt;a href=&#34;https://arxiv.org/abs/2302.02923&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;ldquo;In Search of Insights, Not Magic Bullets: Towards Demystification of the Model Selection Dilemma in Heterogeneous Treatment Effect Estimation&amp;rdquo;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Gutierrez, Gerardy (2017), &lt;a href=&#34;https://proceedings.mlr.press/v67/gutierrez17a/gutierrez17a.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;ldquo;Causal Inference and Uplift Modeling: A review of the literature&amp;rdquo;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Hitsch, Misra, Zhang (2023), &lt;a href=&#34;https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3111957&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;ldquo;Heterogeneous Treatment Effects and Optimal Targeting Policy Evaluation&amp;rdquo;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Kennedy (2022), &lt;a href=&#34;https://arxiv.org/abs/2004.14497&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;ldquo;Towards optimal doubly robust estimation of heterogeneous causal effects&amp;rdquo;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Kunzel, Sekhon, Bickel, Yu (2017), &lt;a href=&#34;https://arxiv.org/abs/1706.03461&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;ldquo;Meta-learners for Estimating Heterogeneous Treatment Effects using Machine Learning&amp;rdquo;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Mahajan, Mitliagkas, Neal, Syrgkanis (2023), &lt;a href=&#34;https://arxiv.org/abs/2211.01939&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;ldquo;Empirical Analysis of Model Selection for Heterogeneous Causal Effect Estimation&amp;rdquo;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Nie, Wager (2017), &lt;a href=&#34;https://arxiv.org/abs/1712.04912&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;ldquo;Quasi-Oracle Estimation of Heterogeneous Treatment Effects&amp;rdquo;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Saito, Yasui (2020), &lt;a href=&#34;https://arxiv.org/abs/1909.05299&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;ldquo;Counterfactual Cross-Validation: Stable Model Selection Procedure for Causal Inference Models&amp;rdquo;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Schuler, Baiocchi, Tibshirani, Shah (2018), &lt;a href=&#34;https://arxiv.org/abs/1804.05146&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;ldquo;A comparison of methods for model selection when estimating individual treatment effects&amp;rdquo;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;related-articles&#34;&gt;Related Articles&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://towardsdatascience.com/8a9c1e340832&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Understanding Meta Learners&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://towardsdatascience.com/ed4097dab27a&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Understanding AIPW, the Doubly-Robust Estimator&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://towardsdatascience.com/920177462149&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Understanding Causal Trees&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://towardsdatascience.com/43c4536f1481&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;From Causal Trees to Forests&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;code&#34;&gt;Code&lt;/h3&gt;
&lt;p&gt;You can find the original Jupyter Notebook here:&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/evaluate_uplift.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/evaluate_uplift.ipynb&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Bayesian AB Testing</title>
      <link>https://matteocourthoud.github.io/post/bayesian_ab_test/</link>
      <pubDate>Tue, 11 Jul 2023 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/post/bayesian_ab_test/</guid>
      <description>&lt;p&gt;&lt;em&gt;An introduction to the Bayesian approach to randomized experiments.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Randomized experiments, a.k.a. &lt;strong&gt;AB tests&lt;/strong&gt;, are now the established gold standard in the industry to estimate causal effects. Randomly assigning the treatment (new product, feature, UI, &amp;hellip;) to a subset of the population (users, patients, customers, &amp;hellip;) we ensure that, on average, the difference in outcomes (revenue, visits, clicks, &amp;hellip;) can be attributed to the treatment. Established companies like &lt;a href=&#34;https://partner.booking.com/en-gb/click-magazine/industry-perspectives/role-experimentation-bookingcom&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Booking.com&lt;/a&gt; report constantly running thousands of AB tests at the same time. And newer growing companies like &lt;a href=&#34;https://blog.duolingo.com/improving-duolingo-one-experiment-at-a-time/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Duolingo&lt;/a&gt; attribute a large chunk of their success to their culture of experimentation at scale.&lt;/p&gt;
&lt;p&gt;With so many experiments, one question comes natural: in one specific experiment, can you leverage information from previous tests? How? In this post, I will try to answer these questions by introducing the &lt;strong&gt;Bayesian approach to AB testing&lt;/strong&gt;. The Bayesian framework is well suited for this type of task because it naturally allows for the updating of existing knowledge (the prior) using new data. However, the method is particularly sensitive to functional form assumptions and apparently innocuous model choices can translate in sensible differences in the estimates, especially when the data is very skewed.&lt;/p&gt;
&lt;h2 id=&#34;search-and-infinite-scrolling&#34;&gt;Search and Infinite Scrolling&lt;/h2&gt;
&lt;p&gt;For the rest of the article, we are going to use a toy example, loosely inspired by &lt;a href=&#34;https://www.aeaweb.org/articles?id=10.1257/pandp.20191003&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Azavedo et al. (2019)&lt;/a&gt;: a &lt;strong&gt;search engine&lt;/strong&gt; that wants to increase its &lt;strong&gt;ad revenue&lt;/strong&gt;, without sacrificing search quality. We are a company with an established experimentation culture and we continuously test new ideas on how to rank results, how to select the most relevant ads for consumers, and the user interface (UI) of the results page. Suppose that, in this specific case, we came up with a new brilliant idea: &lt;a href=&#34;https://blog.google/products/search/continuous-scrolling-mobile/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;infinite scrolling&lt;/a&gt;! Instead of having a discrete sequence of pages, we allow users to keep scrolling down if they want to see more results.&lt;/p&gt;
&lt;img src=&#34;fig/phones.jpg&#34; width=&#34;300px&#34;/&gt;
&lt;p&gt;To understand whether infinite scrolling works, we ran an &lt;strong&gt;AB test&lt;/strong&gt;: we randomize users into a treatment and a control group. We implement infinite scrolling only for users in the treatment group. I import the data generating process &lt;code&gt;dgp_infinite_scroll()&lt;/code&gt; from &lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/src/dgp.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;src.dgp&lt;/code&gt;&lt;/a&gt;. With respect to previous articles, I generated a new DGP parent class that handles randomization and data generation, while its children classes contain the specific use-cases. I also import some plotting functions and libraries from &lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/src/utils.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;src.utils&lt;/code&gt;&lt;/a&gt;.  To include not only code but also data and tables, I use &lt;a href=&#34;https://deepnote.com&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Deepnote&lt;/a&gt;, a Jupyter-like web-based collaborative notebook environment.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;%matplotlib inline
%config InlineBackend.figure_format = &#39;retina&#39;

from src.utils import *
from src.dgp import DGP, dgp_infinite_scroll
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;dgp = dgp_infinite_scroll(n=10_000)
df = dgp.generate_data(true_effect=0.14)
df.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;past_revenue&lt;/th&gt;
      &lt;th&gt;infinite_scroll&lt;/th&gt;
      &lt;th&gt;ad_revenue&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;3.76&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;3.70&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;2.40&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1.71&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;2.98&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;4.85&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;4.24&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;4.57&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;3.87&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;3.69&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;We have information on $10.000$ website visitors for which we observe the monthly &lt;code&gt;ad_revenue&lt;/code&gt; they generated, whether they were assigned to the treatment group and were using the &lt;code&gt;infinite_scroll&lt;/code&gt;, and also the average monthly &lt;code&gt;past_revenue&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;The random treatment assignment makes the &lt;strong&gt;difference-in-means&lt;/strong&gt; estimator &lt;a href=&#34;https://en.wikipedia.org/wiki/Bias_of_an_estimator&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;unbiased&lt;/strong&gt;&lt;/a&gt;: we expect the treatment and control group to be comparable on average, so we can causal attribute the average observed difference in outcomes to the treatment effect. We estimate the treatment effect by linear regression. We can interpret the coefficient of &lt;code&gt;infinite_scroll&lt;/code&gt; as the estimated treatment effect.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;smf.ols(&#39;ad_revenue ~ infinite_scroll&#39;, df).fit().summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
         &lt;td&gt;&lt;/td&gt;            &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt;       &lt;td&gt;    1.9865&lt;/td&gt; &lt;td&gt;    0.020&lt;/td&gt; &lt;td&gt;  101.320&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    1.948&lt;/td&gt; &lt;td&gt;    2.025&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;infinite_scroll&lt;/th&gt; &lt;td&gt;    0.1524&lt;/td&gt; &lt;td&gt;    0.028&lt;/td&gt; &lt;td&gt;    5.461&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    0.098&lt;/td&gt; &lt;td&gt;    0.207&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;It seems that the &lt;code&gt;infinite_scroll&lt;/code&gt; was indeed a good idea and it increase the average monthly revenue by $0.1524$$. Moreover, the effect is significantly different from zero at the 1% confidence level.&lt;/p&gt;
&lt;p&gt;We could further improve the precision of the estimator by controlling for &lt;code&gt;past_revenue&lt;/code&gt; in the regression. We do not expect a sensible change in the estimated coefficient, but the precision should improve (if you want to know more on out control variables, check my other articles on &lt;a href=&#34;https://towardsdatascience.com/understanding-cuped-a822523641af&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CUPED&lt;/a&gt; and &lt;a href=&#34;https://towardsdatascience.com/controls-b63dc69e3d8c&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;DAGs&lt;/a&gt;).&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;reg = smf.ols(&#39;ad_revenue ~ infinite_scroll + past_revenue&#39;, df).fit()
reg.summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
         &lt;td&gt;&lt;/td&gt;            &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt;       &lt;td&gt;    0.0181&lt;/td&gt; &lt;td&gt;    0.024&lt;/td&gt; &lt;td&gt;    0.741&lt;/td&gt; &lt;td&gt; 0.459&lt;/td&gt; &lt;td&gt;   -0.030&lt;/td&gt; &lt;td&gt;    0.066&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;infinite_scroll&lt;/th&gt; &lt;td&gt;    0.1571&lt;/td&gt; &lt;td&gt;    0.020&lt;/td&gt; &lt;td&gt;    7.910&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    0.118&lt;/td&gt; &lt;td&gt;    0.196&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;past_revenue&lt;/th&gt;    &lt;td&gt;    0.9922&lt;/td&gt; &lt;td&gt;    0.010&lt;/td&gt; &lt;td&gt;   98.655&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    0.972&lt;/td&gt; &lt;td&gt;    1.012&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;Indeed, &lt;code&gt;past_revenue&lt;/code&gt; is highly predictive of current &lt;code&gt;ad_revenue&lt;/code&gt; and the precision of the estimated coefficient for &lt;code&gt;infinite_scroll&lt;/code&gt; decreases by one-third.&lt;/p&gt;
&lt;p&gt;So far, everything has been very standard. However, as we said at the beginning, suppose this is not the only experiment we ran trying to improve our browser (and ultimately ad revenue). The infinite scroll is just one idea among thousands of others that we have tested in the past. Is there a way to efficiently use this additional &lt;strong&gt;information&lt;/strong&gt;?&lt;/p&gt;
&lt;h2 id=&#34;bayesian-statistics&#34;&gt;Bayesian Statistics&lt;/h2&gt;
&lt;p&gt;One of the main advantages of Bayesian statistics over the frequentist approach is that it easily allows to incorporate additional information into a model. The idea directly follows from the main results behind all Bayesian statistics: &lt;a href=&#34;https://en.wikipedia.org/wiki/Bayes%27_theorem&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Bayes Theorem&lt;/strong&gt;&lt;/a&gt;. Bayes theorem, allows you to do inference on a model by &lt;strong&gt;inverting the inference problem&lt;/strong&gt;: from the probability of the model given the data, to the probability of the data given the model, a much easier object to deal with.&lt;/p&gt;
&lt;p&gt;$$
\underbrace{ \Pr \big( \text{model} \ \big| \ \text{data} \big) }&lt;em&gt;{\text{posterior}} = \underbrace{ \Pr(\text{model}) }&lt;/em&gt;{\text{prior}} \ \underbrace{ \frac{ \Pr \big( \text{data} \ \big| \ \text{model} \big) }{ \Pr(\text{data}) } }_{\text{likelihood}}
$$&lt;/p&gt;
&lt;p&gt;We can split the right-hand side of Bayes Theorem (or Rule) into two components: the &lt;strong&gt;prior&lt;/strong&gt; and the &lt;strong&gt;likelihood&lt;/strong&gt;. The likelihood is the information about the model that comes from the data, the prior instead is any additional information about the model.&lt;/p&gt;
&lt;p&gt;First of all, let&amp;rsquo;s map Bayes theorem into our context. What is the data, what is the model and what is our object of interest?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the &lt;strong&gt;data&lt;/strong&gt; which consists in our outcome variable &lt;code&gt;ad_revenue&lt;/code&gt;, $y$, the treatment &lt;code&gt;infinite_scroll&lt;/code&gt;, $D$ and the other variables, &lt;code&gt;past_revenue&lt;/code&gt; and a constant, which we jointly denote as $X$&lt;/li&gt;
&lt;li&gt;the &lt;strong&gt;model&lt;/strong&gt; is the distribution of &lt;code&gt;ad_revenue&lt;/code&gt;, given &lt;code&gt;past_revenue&lt;/code&gt; and the &lt;code&gt;infinite_scroll&lt;/code&gt; feature, $y | D, X$&lt;/li&gt;
&lt;li&gt;our &lt;strong&gt;object of interest&lt;/strong&gt; is the posterior $\Pr \big( \text{model} \ \big| \ \text{data} \big)$, in particular the relationship between &lt;code&gt;ad_revenue&lt;/code&gt; and &lt;code&gt;infinite_scroll&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;X = sm.add_constant(df[[&#39;past_revenue&#39;]].values)
D = df[&#39;infinite_scroll&#39;].values
y = df[&#39;ad_revenue&#39;].values
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;How do we use prior information in the context of AB testing, potentially including additional covariates?&lt;/p&gt;
&lt;h3 id=&#34;bayesian-regression&#34;&gt;Bayesian Regression&lt;/h3&gt;
&lt;p&gt;Let&amp;rsquo;s use a linear model to make it directly comparable with the frequentist approach:&lt;/p&gt;
&lt;p&gt;$$
y_i = \beta X_i + \tau D_i + \varepsilon_i \qquad \text{where} \quad \varepsilon_i \sim N \big( 0, \sigma^2 \big)
$$&lt;/p&gt;
&lt;p&gt;This is a parametric model with &lt;strong&gt;two sets of parameters&lt;/strong&gt;: the linear coefficients $\beta$ and $\tau$, and the variance of the residuals $\sigma$. An equivalent, but more Bayesian, way to write the model is:&lt;/p&gt;
&lt;p&gt;$$
y \ | \ X, D; \beta, \tau, \sigma \sim N \Big( \beta X + \tau D \ , \sigma^2 \Big) ,
$$&lt;/p&gt;
&lt;p&gt;where the semi-column separates the data from the model parameters. Differently from the frequentist approach, in Bayesian regressions we do not rely on the &lt;a href=&#34;https://en.wikipedia.org/wiki/Central_limit_theorem&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;central limit theorem&lt;/a&gt; to approximate the conditional distribution of $y$, but we directly &lt;strong&gt;assume&lt;/strong&gt; it is normal. Is it just a formality? Not really, but a proper comparison between the frequentist and Bayesian approach is beyond the scope of this article.&lt;/p&gt;
&lt;p&gt;We are interested in doing inference on the model parameters, $\beta$, $\tau$, and $\sigma$. Another &lt;strong&gt;core difference&lt;/strong&gt; between the frequentist and the Bayesian approach is that the the first assumes that the model parameters are fixed (scalars), while the latter allows them to be stochastic (random variables).&lt;/p&gt;
&lt;p&gt;This assumption has a very practical &lt;strong&gt;implication&lt;/strong&gt;: you can easily incorporate previous information about the model parameters in the form of &lt;strong&gt;prior&lt;/strong&gt; distributions. As the name says, priors contain information that was available even &lt;em&gt;before&lt;/em&gt; looking at the data. This leads to one of the most relevant questions in Bayesian statistics: &lt;strong&gt;how do you chose a prior&lt;/strong&gt;?&lt;/p&gt;
&lt;h2 id=&#34;priors&#34;&gt;Priors&lt;/h2&gt;
&lt;p&gt;When choosing a prior, one analytically appealing restriction is to have a prior distribution such that the posterior belongs to the same family. These priors are called &lt;strong&gt;conjugate priors&lt;/strong&gt;. For example, before seeing the data, I assume my treatment effect is normally distributed and I would like it to be normally distributed also after incorporating the information contained in the data.&lt;/p&gt;
&lt;p&gt;In the case of Bayesian linear regression, the conjugate priors for $\beta$ and $\sigma$ are normally and inverse-gamma distributed. Let&amp;rsquo;s start a bit blindly, by taking a standard normal and inverse gamma distribution as prior.&lt;/p&gt;
&lt;p&gt;$$
\beta_i \sim N(\boldsymbol 0, \boldsymbol 1) \
\tau_i \sim N(0,1) \
\sigma^2 \sim \Gamma^{-1} (1, 1)
$$&lt;/p&gt;
&lt;p&gt;We use the package &lt;a href=&#34;https://www.pymc.io/projects/docs/en/stable/learn.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PyMC&lt;/a&gt; to do inference. First we need to specify the model: what are the distributions of the different parameters (priors) and what is the likelihood of the data.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import pymc as pm
with pm.Model() as baseline_model:

    # Priors
    beta = pm.MvNormal(&#39;beta&#39;, mu=np.ones(np.shape(X)[1]), cov=np.eye(np.shape(X)[1]))
    tau = pm.Normal(&#39;tau&#39;, mu=0, sigma=1)
    sigma = pm.InverseGamma(&#39;sigma&#39;, mu=1, sigma=1, initval=1)
    
    # Likelihood 
    Ylikelihood = pm.Normal(&#39;y&#39;, mu=(X@beta + D@tau).flatten(), sigma=sigma, observed=y)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;PyMC has an extremely nice function that allows us to visualize the model as a graph, &lt;code&gt;model_to_graphviz&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;pm.model_to_graphviz(baseline_model)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/bayesian_ab_test_25_0.svg&#34; alt=&#34;svg&#34;&gt;&lt;/p&gt;
&lt;p&gt;From the graphical representation, we can see the various model components, their distributions, and how they interact with each other.&lt;/p&gt;
&lt;p&gt;We are now ready to &lt;strong&gt;compute&lt;/strong&gt; the model posterior. How does it work? In short, we sample realizations of model parameters, we compute the likelihood of the data given those values and the compute the corresponding posterior.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;idata = pm.sample(model=baseline_model, draws=1000)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Auto-assigning NUTS sampler...
Initializing NUTS using jitter+adapt_diag...
Multiprocess sampling (4 chains in 4 jobs)
NUTS: [beta, tau, sigma]
&lt;/code&gt;&lt;/pre&gt;
&lt;style&gt;
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
&lt;/style&gt;
&lt;div&gt;
  &lt;progress value=&#39;8000&#39; class=&#39;&#39; max=&#39;8000&#39; style=&#39;width:300px; height:20px; vertical-align: middle;&#39;&gt;&lt;/progress&gt;
  100.00% [8000/8000 00:04&amp;lt;00:00 Sampling 4 chains, 0 divergences]
&lt;/div&gt;
&lt;pre&gt;&lt;code&gt;Sampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 4_000 draws total) took 19 seconds.
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The fact that Bayesian inference requires sampling, has been historically one of the main bottlenecks of Bayesian statistics, since it makes it sensibly slower than the frequentist approach. However, this is less and less of a problem with the increased computational power of model computers.&lt;/p&gt;
&lt;p&gt;We are now ready to print out results. First, with the &lt;code&gt;summary()&lt;/code&gt; method, we can print a model summary very similar to those produced by the &lt;code&gt;statsmodels&lt;/code&gt; package.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;pm.summary(idata, hdi_prob=0.95).round(4)
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;mean&lt;/th&gt;
      &lt;th&gt;sd&lt;/th&gt;
      &lt;th&gt;hdi_2.5%&lt;/th&gt;
      &lt;th&gt;hdi_97.5%&lt;/th&gt;
      &lt;th&gt;mcse_mean&lt;/th&gt;
      &lt;th&gt;mcse_sd&lt;/th&gt;
      &lt;th&gt;ess_bulk&lt;/th&gt;
      &lt;th&gt;ess_tail&lt;/th&gt;
      &lt;th&gt;r_hat&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;beta[0]&lt;/th&gt;
      &lt;td&gt;0.019&lt;/td&gt;
      &lt;td&gt;0.025&lt;/td&gt;
      &lt;td&gt;-0.031&lt;/td&gt;
      &lt;td&gt;0.068&lt;/td&gt;
      &lt;td&gt;0.001&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;1943.0&lt;/td&gt;
      &lt;td&gt;1866.0&lt;/td&gt;
      &lt;td&gt;1.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;beta[1]&lt;/th&gt;
      &lt;td&gt;0.992&lt;/td&gt;
      &lt;td&gt;0.010&lt;/td&gt;
      &lt;td&gt;0.970&lt;/td&gt;
      &lt;td&gt;1.011&lt;/td&gt;
      &lt;td&gt;0.000&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;2239.0&lt;/td&gt;
      &lt;td&gt;1721.0&lt;/td&gt;
      &lt;td&gt;1.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;tau&lt;/th&gt;
      &lt;td&gt;0.157&lt;/td&gt;
      &lt;td&gt;0.021&lt;/td&gt;
      &lt;td&gt;0.117&lt;/td&gt;
      &lt;td&gt;0.197&lt;/td&gt;
      &lt;td&gt;0.000&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;2770.0&lt;/td&gt;
      &lt;td&gt;2248.0&lt;/td&gt;
      &lt;td&gt;1.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;sigma&lt;/th&gt;
      &lt;td&gt;0.993&lt;/td&gt;
      &lt;td&gt;0.007&lt;/td&gt;
      &lt;td&gt;0.980&lt;/td&gt;
      &lt;td&gt;1.007&lt;/td&gt;
      &lt;td&gt;0.000&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;3473.0&lt;/td&gt;
      &lt;td&gt;2525.0&lt;/td&gt;
      &lt;td&gt;1.0&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;The estimated parameters are extremely close to the ones we got with the frequentist approach, with an estimated effect of the &lt;code&gt;infinite_scroll&lt;/code&gt; equal to $0.157$.&lt;/p&gt;
&lt;p&gt;If sampling had the disadvantage of being slow, it has the advantage of being very &lt;strong&gt;transparent&lt;/strong&gt;. We can directly plot the distribution of the posterior. Let&amp;rsquo;s do it for the treatment effect $\tau$. The PyMC function &lt;code&gt;plot_posterior&lt;/code&gt; plots the distribution of the posterior, with a black bar for the Bayesian equivalent of a 95% confidence interval.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;pm.plot_posterior(idata, kind=&amp;quot;hist&amp;quot;, var_names=(&#39;tau&#39;), hdi_prob=0.95, figsize=(6, 3), bins=30); 
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/bayesian_ab_test_31_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;As expected, since we chose conjugate priors, the posterior distribution looks gaussian.&lt;/p&gt;
&lt;p&gt;So far we have chosen the prior without much guidance. However, suppose we had access to past experiments. How do we incorporate this specific information?&lt;/p&gt;
&lt;h2 id=&#34;past-experiments&#34;&gt;Past Experiments&lt;/h2&gt;
&lt;p&gt;Suppose the idea of the infinite scroll, was just one among a ton of other ones that we tried and tested in the past. For each idea we have the data for the corresponding experiment, with the corresponding estimated coefficient. Suppose we had a thousand of them.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;past_experiments = [dgp.generate_data(seed_data=i) for i in range(1000)]
taus = [smf.ols(&#39;ad_revenue ~ infinite_scroll + past_revenue&#39;, pe).fit().params.values for pe in past_experiments]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;How do we use this additional information?&lt;/p&gt;
&lt;h3 id=&#34;normal-prior&#34;&gt;Normal Prior&lt;/h3&gt;
&lt;p&gt;The first idea could be to calibrate our prior to reflect the data distribution in the past. Keeping the normality assumption, we use the estimated average and standard deviations of the estimates from past experiments.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;taus_mean = np.mean(taus, axis=0)[1]
taus_mean
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;0.0009094486420266667
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;On average, had practically no effect on &lt;code&gt;ad_revenue&lt;/code&gt;, with a average effect of $0.0009$.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;taus_std = np.sqrt(np.cov(taus, rowvar=0)[1,1])
taus_std
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;0.029014447772168384
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;However, there was sensible variation across experiments, with a standard deviation of $0.029$.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s now estimate the&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;with pm.Model() as model_normal_prior:
    k = np.shape(X)[1]
    beta = pm.MvNormal(&#39;beta&#39;, mu=np.ones(k), cov=np.eye(k))
    tau = pm.Normal(&#39;tau&#39;, mu=taus_mean, sigma=taus_std)
    sigma = pm.InverseGamma(&#39;sigma&#39;, mu=1, sigma=1, initval=1)
    Ylikelihood = pm.Normal(&#39;y&#39;, mu=(X@beta + D@tau).flatten(), sigma=sigma, observed=y)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s sample from the model.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;idata_normal_prior = pm.sample(model=model_normal_prior, draws=1000)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Auto-assigning NUTS sampler...
Initializing NUTS using jitter+adapt_diag...
Multiprocess sampling (4 chains in 4 jobs)
NUTS: [beta, tau, sigma]
&lt;/code&gt;&lt;/pre&gt;
&lt;style&gt;
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
&lt;/style&gt;
&lt;div&gt;
  &lt;progress value=&#39;8000&#39; class=&#39;&#39; max=&#39;8000&#39; style=&#39;width:300px; height:20px; vertical-align: middle;&#39;&gt;&lt;/progress&gt;
  100.00% [8000/8000 00:04&amp;lt;00:00 Sampling 4 chains, 0 divergences]
&lt;/div&gt;
&lt;pre&gt;&lt;code&gt;Sampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 4_000 draws total) took 19 seconds.
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And plot the sample posterior distribution of the treatment effect parameter $\tau$.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;pm.plot_posterior(idata_normal_prior, kind=&amp;quot;hist&amp;quot;, var_names=(&#39;tau&#39;), hdi_prob=0.95, figsize=(6, 3), bins=30); 
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/bayesian_ab_test_47_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;The estimated coefficient is sensibly smaller: $0.08$ instead of the previous estimate of $0.12$. Why is it the case?&lt;/p&gt;
&lt;p&gt;The fact is that the previous coefficient of $0.12$ is extremely unlikey, given our prior. We can compute the probability of getting the same or a more extreme value, given the prior.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;1 - sp.stats.norm(taus_mean, taus_std).cdf(0.12)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;2.025724712373389e-05
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The probability of such value is almost zero. Therefore, the estimated coefficient has moved towards the prior mean of $0.0009$.&lt;/p&gt;
&lt;h3 id=&#34;student-t-prior&#34;&gt;Student t Prior&lt;/h3&gt;
&lt;p&gt;So far, we have assumed a normal distribution for all linear coefficients. Is it appropriate? Let&amp;rsquo;s check it visually (check &lt;a href=&#34;https://medium.com/towards-data-science/how-to-compare-two-or-more-distributions-9b06ee4d30bf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt; for other methods on how to compare distributions).&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sns.histplot([tau[0] for tau in taus]).set(title=r&#39;Distribution of $\hat{\beta}_0$ in past experiments&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/bayesian_ab_test_53_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;The distribution seems pretty normal. What the treatment effect paramenter $\tau$?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;fig, ax = plt.subplots()
sns.histplot([tau[1] for tau in taus], label=&#39;past experiments&#39;);
ax.axvline(reg.params[&#39;infinite_scroll&#39;], lw=2, c=&#39;C3&#39;, ls=&#39;--&#39;, label=&#39;current experiment&#39;)
plt.legend();
plt.title(r&#39;Distribution of $\hat{\tau}$ in past experiments&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/bayesian_ab_test_55_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;The distribution very &lt;strong&gt;heavy tailed&lt;/strong&gt;! While at the center it looks like a normal distributions, the tails are much &amp;ldquo;fatter&amp;rdquo; and we have a couple of very extreme values. excluding the case of measurement error, this is a setting that happens often in the industry, where most ideas have extremely small or null effects and very rarely an idea is actually a breakthrough.&lt;/p&gt;
&lt;p&gt;One way to model this distribution is a &lt;a href=&#34;&#34;&gt;student-t distribution&lt;/a&gt;. In particular, we use a t-student with mean $0.0009$, variance $0.003$ and $1.3$ degrees of freedom to match the moments of the empirical distributions of past estimates.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;with pm.Model() as model_studentt_prior:

    # Priors
    k = np.shape(X)[1]
    beta = pm.MvNormal(&#39;beta&#39;, mu=np.ones(k), cov=np.eye(k))
    tau = pm.StudentT(&#39;tau&#39;, mu=taus_mean, sigma=0.003, nu=1.3)
    sigma = pm.InverseGamma(&#39;sigma&#39;, mu=1, sigma=1, initval=1)
    
    # Likelihood 
    Ylikelihood = pm.Normal(&#39;y&#39;, mu=(X@beta + D@tau).flatten(), sigma=sigma, observed=y)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s sample from the model.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;idata_studentt_priors = pm.sample(model=model_studentt_prior, draws=1000)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Auto-assigning NUTS sampler...
Initializing NUTS using jitter+adapt_diag...
Multiprocess sampling (4 chains in 4 jobs)
NUTS: [beta, tau, sigma]
&lt;/code&gt;&lt;/pre&gt;
&lt;style&gt;
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
&lt;/style&gt;
&lt;div&gt;
  &lt;progress value=&#39;8000&#39; class=&#39;&#39; max=&#39;8000&#39; style=&#39;width:300px; height:20px; vertical-align: middle;&#39;&gt;&lt;/progress&gt;
  100.00% [8000/8000 00:04&amp;lt;00:00 Sampling 4 chains, 0 divergences]
&lt;/div&gt;
&lt;pre&gt;&lt;code&gt;Sampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 4_000 draws total) took 18 seconds.
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And plot the sample posterior distribution of the treatment effect parameter $\tau$.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;pm.plot_posterior(idata_studentt_priors, kind=&amp;quot;hist&amp;quot;, var_names=(&#39;tau&#39;), hdi_prob=0.95, figsize=(6, 3), bins=30); 
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/bayesian_ab_test_61_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;The estimated coefficient is now again similar to the one we got with the standard normal prior, $0.11$. However, the estimate is more precise since the confidence interval has shrunk from $[0.077, 0.016]$ to $[0.065, 0.015]$.&lt;/p&gt;
&lt;p&gt;What has happened?&lt;/p&gt;
&lt;h3 id=&#34;shrinking&#34;&gt;Shrinking&lt;/h3&gt;
&lt;p&gt;The answer lies in the shape of the different &lt;strong&gt;prior distributions&lt;/strong&gt; that we have used:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;standard normal, $N(0,1)$&lt;/li&gt;
&lt;li&gt;normal with matched moments, $N(0, 0.03)$&lt;/li&gt;
&lt;li&gt;t-student with matched moments, $t_{1.3}$(0, 0.003)$&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;t_hats = np.linspace(-0.3, 0.3, 1_000)
distributions = {
    &#39;N(0,1)&#39;: sp.stats.norm(0, 1).pdf(t_hats),
    &#39;N(0, 0.03)&#39;: sp.stats.norm(0, 0.03).pdf(t_hats),
    &#39;$t_{1.3}$(0, 0.003)&#39;: sp.stats.t(df=1.3).pdf(t_hats / 0.003)*300,
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s plot all of them together.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;for i, (label, y) in enumerate(distributions.items()):
    sns.lineplot(x=t_hats, y=y, color=f&#39;C{i}&#39;, label=label);
plt.legend(); 
plt.title(&#39;Prior Distributions&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/bayesian_ab_test_67_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;As we can see, all distributions are centered on zero, but they have very different shapes. The standard normal distribution is essentially flat over the $[-0.15, 0.15]$ interval. Every value has basically the same probability. The last two instead, even though they have the same mean and variance, have very different shapes.&lt;/p&gt;
&lt;p&gt;How does it translate into our estimation? We can plot the implied posterior for different estimates, for each prior distribution.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def compute_posterior(b, prior):
    likelihood = sp.stats.norm(b, taus_std).pdf(t_hats)
    return np.average(t_hats, weights=prior*likelihood)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;fig, ax = plt.subplots(figsize=(7,6))
ax.axvline(0, lw=1.5, c=&#39;k&#39;);
ax.axhline(0, lw=1.5, c=&#39;k&#39;);
ax.axvline(reg.params[&#39;infinite_scroll&#39;], lw=2, ls=&#39;--&#39;, c=&#39;darkgray&#39;);
for i, (label, y) in enumerate(distributions.items()):
    sns.lineplot(x=t_hats, y=[compute_posterior(t, y) for t in t_hats] , color=f&#39;C{i}&#39;, label=label);
ax.set_xlim(-0.17, 0.17);
ax.set_ylim(-0.17, 0.17);
plt.legend(); 
ax.set_xlabel(&#39;Experiment Estimate&#39;);
ax.set_ylabel(&#39;Posterior&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/bayesian_ab_test_70_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;As we can see, the different priors transform the experimental estimates in very different ways. The standard normal prior essentially has no effect for estimates in the $[-0.15, 0.15]$ interval. The normal prior with matched moments instead shrinks each estimate by approximately 2/3. The effect of the t-student prior is instead &lt;strong&gt;non-linear&lt;/strong&gt;: it shrinks small estimates towards zero, while it keeps large estimates as they are.&lt;/p&gt;
&lt;p&gt;My &lt;strong&gt;intuition&lt;/strong&gt; is the following. A prior distribution very skewed or with &amp;ldquo;fat tails&amp;rdquo; means that large values are rare but not impossible. In practice, it means accepting that breakthrough improvements are possible. On the other hand, for the same variance, the distribution is more concentrated around zero than a standard normal so that small values are shrunk even more.&lt;/p&gt;
&lt;img src=&#34;fig/scroll.jpg&#34; width=&#34;300px&#34;/&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;In this article we have seen how to extend the analysis of AB test to incorporate &lt;strong&gt;information from past experiments&lt;/strong&gt;. In particular, we have seen the importance of choosing a prior. Selecting the distribution function is just as important as tuning its parameters. The shape of the prior distribution can drastically affect our inference, especially in a world with skewed distributions.&lt;/p&gt;
&lt;p&gt;Despite the length of the article, this was just a glimpse in the world of &lt;strong&gt;AB testing and Bayesian statistics&lt;/strong&gt;. While being computationally more intensive and requiring additional assumptions, the Bayesian approach is often more natural, powerful and flexible than the frequentist one. Knowing pros and cons of both approaches is crucial to get the best of both worlds, picking the approach that work best or combining them efficiently.&lt;/p&gt;
&lt;h3 id=&#34;references&#34;&gt;References&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;E. Azevedo, A. Deng, J. Olea, G. Weyl, &lt;a href=&#34;https://www.aeaweb.org/articles?id=10.1257/pandp.20191003&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Empirical Bayes Estimation of Treatment Effects with Many A/B Tests: An Overview&lt;/a&gt; (2019). &lt;em&gt;AEA Papers and Proceedings&lt;/em&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;A. Deng, &lt;a href=&#34;https://dl.acm.org/doi/abs/10.1145/2740908.2742563&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Objective Bayesian Two Sample Hypothesis Testing for Online Controlled Experiments&lt;/a&gt; (2018), &lt;em&gt;WWW15&lt;/em&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;related-articles&#34;&gt;Related Articles&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://medium.com/towards-data-science/the-bayesian-bootstrap-6ca4a1d45148&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The Bayesian Bootstrap&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://towardsdatascience.com/understanding-cuped-a822523641af&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Understanding CUPED&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://towardsdatascience.com/controls-b63dc69e3d8c&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;DAGs and Control Variables&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;code&#34;&gt;Code&lt;/h3&gt;
&lt;p&gt;You can find the original Jupyter Notebook here:&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/bayes_ab.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/bayes_ab.ipynb&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Outliers, Leverage, and Influential Observations</title>
      <link>https://matteocourthoud.github.io/post/outliers_leverage/</link>
      <pubDate>Tue, 11 Jul 2023 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/post/outliers_leverage/</guid>
      <description>&lt;p&gt;&lt;em&gt;What makes an observation &amp;ldquo;unusual&amp;rdquo;?&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;In data science, one common task is outlier detection. This is a broad term that is often misused or misunderstood. More broadly, we are often interested in understanding any observation is &lt;strong&gt;&amp;ldquo;unusual&amp;rdquo;&lt;/strong&gt;. First of all, what does it mean to be unusual? In this article we are going to inspect three different ways in which an observation can be unusual: it can be unusual characteristics, it might not fit the model or it might be particularly influential in fitting the model. We will see that in linear regression the latter characteristics is a byproduct of the first two.&lt;/p&gt;
&lt;p&gt;Importantly, being unusual is &lt;strong&gt;not necessarily bad&lt;/strong&gt;. Observations that have different characteristics from all others usually carry more information. We also expect some observations not to fit the model well, otherwise the model is likely biased (overfitting). However, &amp;ldquo;unusual&amp;rdquo; observations are also more likely to be generated by a different process. Extreme cases include measurement error or fraud, but differences can be more nuanced. Domain knowledge is always kind and dropping observations only for for statistical reasons is never wise.&lt;/p&gt;
&lt;p&gt;That said, let&amp;rsquo;s have a look at some different ways in which observations can be &amp;ldquo;unusual&amp;rdquo;.&lt;/p&gt;
&lt;h1 id=&#34;example&#34;&gt;Example&lt;/h1&gt;
&lt;p&gt;Suppose we are an &lt;strong&gt;peer-to-peer online platform&lt;/strong&gt; and we are interested in understanding if there is anything suspicious going on with our business. We have information about how much time our customers spend on the platform and the total value of their transactions.&lt;/p&gt;
&lt;p&gt;First, let&amp;rsquo;s have a look at the data. I import the data generating process &lt;code&gt;dgp_p2p()&lt;/code&gt; from &lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/src/dgp.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;src.dgp&lt;/code&gt;&lt;/a&gt; and some plotting functions and libraries from &lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/src/utils.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;src.utils&lt;/code&gt;&lt;/a&gt;. I include code snippets from &lt;a href=&#34;https://deepnote.com&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Deepnote&lt;/a&gt;, a Jupyter-like web-based collaborative notebook environment. For our purpose, Deepnote is very handy because it allows me not only to include code but also output, like data and tables.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;%matplotlib inline
%config InlineBackend.figure_format = &#39;retina&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from src.utils import *
from src.dgp import dgp_p2p
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df = dgp_p2p().generate_data()
df.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;hours&lt;/th&gt;
      &lt;th&gt;transactions&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;2.6&lt;/td&gt;
      &lt;td&gt;8.30&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;2.0&lt;/td&gt;
      &lt;td&gt;8.00&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;7.0&lt;/td&gt;
      &lt;td&gt;21.00&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;6.7&lt;/td&gt;
      &lt;td&gt;18.00&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;1.2&lt;/td&gt;
      &lt;td&gt;3.82&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;We have information on 50 clients for which we observe &lt;code&gt;hours&lt;/code&gt; spent on the website and total &lt;code&gt;transactions&lt;/code&gt; amount. Since we only have two variables we can easily inspect them using a scatterplot.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sns.scatterplot(data=df, x=&#39;hours&#39;, y=&#39;transactions&#39;).set(title=&#39;Data Scatterplot&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/outliers_leverage_9_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;The relationship between &lt;code&gt;hours&lt;/code&gt; and &lt;code&gt;transactions&lt;/code&gt; seems to follow a clear linear relationship. If we fit a linear model, we observe a particularly tight fit.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;smf.ols(&#39;hours ~ transactions&#39;, data=df).fit().summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
        &lt;td&gt;&lt;/td&gt;          &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt;    &lt;td&gt;   -0.0975&lt;/td&gt; &lt;td&gt;    0.084&lt;/td&gt; &lt;td&gt;   -1.157&lt;/td&gt; &lt;td&gt; 0.253&lt;/td&gt; &lt;td&gt;   -0.267&lt;/td&gt; &lt;td&gt;    0.072&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;transactions&lt;/th&gt; &lt;td&gt;    0.3452&lt;/td&gt; &lt;td&gt;    0.009&lt;/td&gt; &lt;td&gt;   39.660&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    0.328&lt;/td&gt; &lt;td&gt;    0.363&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;Does any data point look suspiciously different from the others? How?&lt;/p&gt;
&lt;h2 id=&#34;leverage&#34;&gt;Leverage&lt;/h2&gt;
&lt;p&gt;The first metric that we are going to use to evaluate &amp;ldquo;unusual&amp;rdquo; observations is the &lt;strong&gt;leverage&lt;/strong&gt;, which was first introduced by &lt;a href=&#34;https://www.jstor.org/stable/1268249&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Cook (1980)&lt;/a&gt;. The objective of the leverage is to capture how much a single point is different with respect to other data points. These data points are often called &lt;strong&gt;outliers&lt;/strong&gt; and there exist a nearly amount of algorithms and rules of thumb to flag them.However the idea is the same: flagging observations that are unusual in terms of features.&lt;/p&gt;
&lt;p&gt;The leverage of an observation $i$ is defined as&lt;/p&gt;
&lt;p&gt;$$
h_{ii} := x_i&amp;rsquo; (X&amp;rsquo;X)^{-1} x_i
$$&lt;/p&gt;
&lt;p&gt;One interpretation of the leverage is as a &lt;strong&gt;measure of distance&lt;/strong&gt; where individual observations are compared against the average of all observations.&lt;/p&gt;
&lt;p&gt;Another interpretation of the leverage is as the influence of the outcome of observation $i$, $y_i$, on the corresponding fitted value $\hat{y_i}$.&lt;/p&gt;
&lt;p&gt;$$
h_{ii} = \frac{\partial \hat{y}_i}{\partial y_i}
$$&lt;/p&gt;
&lt;p&gt;Algebraically, the leverage of observation $i$ is the $i^{th}$ element of the &lt;strong&gt;design matrix&lt;/strong&gt; $X&amp;rsquo; (X&amp;rsquo;X)^{-1} X$. Among the many properties of the leverages, is the fact that they are non-negative and their values sum to 1.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s compute the leverage of the observations in our dataset. We also flag observations that have unusual leverages (which we arbitrarily define as more than two standard deviations away from the average leverage).&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;X = np.reshape(df[&#39;hours&#39;].values, (-1, 1))
Y = np.reshape(df[&#39;transactions&#39;].values, (-1, 1))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df[&#39;leverage&#39;] = np.diagonal(X @ np.linalg.inv(X.T @ X) @ X.T)
df[&#39;high_leverage&#39;] = df[&#39;leverage&#39;] &amp;gt; (np.mean(df[&#39;leverage&#39;]) + 2*np.std(df[&#39;leverage&#39;]))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s plot the distribution of leverage values in our data.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;fix, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))
sns.histplot(data=df, x=&#39;leverage&#39;, hue=&#39;high_leverage&#39;, alpha=1, bins=30, ax=ax1).set(title=&#39;Distribution of Leverages&#39;);
sns.scatterplot(data=df, x=&#39;hours&#39;, y=&#39;transactions&#39;, hue=&#39;high_leverage&#39;, ax=ax2).set(title=&#39;Data Scatterplot&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/outliers_leverage_18_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;As we can see, the distribution is skewed with two observations having a unusually high leverage. Indeed, in the scatterplot these two observations are slightly separated from the rest of the distribution.&lt;/p&gt;
&lt;p&gt;Is this bad news? It depends. Outliers are &lt;strong&gt;not a problem per se&lt;/strong&gt;. Actually, if they are genuine observations, they might carry much more information than other observations. On the other hand, they are also more likely &lt;em&gt;not&lt;/em&gt; to be genuine observations (e.g. fraud, measurement error, &amp;hellip;) or to be inherently different from the other ones (e.g. professional users vs amateurs). In any case, we might want to investigate further and use as much context-specific information as we can.&lt;/p&gt;
&lt;p&gt;Importantly, the fact that an observation has a high leverage tells us information about the features of the model but nothing about the model itself. Are these users just different observations or they also behave differently?&lt;/p&gt;
&lt;h2 id=&#34;residuals&#34;&gt;Residuals&lt;/h2&gt;
&lt;p&gt;So far we have only talked about unusual features, but what about &lt;strong&gt;unusual behavior&lt;/strong&gt;? This is what regression residuals measure.&lt;/p&gt;
&lt;p&gt;Regression residuals are the difference between the predicted outcome values and the observed outcome values. In a sense, they capture what the model cannot explain: the higher the residual of one observation the more it is unusual in the sense that the model cannot explain it.&lt;/p&gt;
&lt;p&gt;In the case of linear regression, residuals can be written as&lt;/p&gt;
&lt;p&gt;$$
\hat{e} = y - \hat{y} = y - \hat \beta X
$$&lt;/p&gt;
&lt;p&gt;In our case, since $X$ is one dimensional (&lt;code&gt;hours&lt;/code&gt;), we can easily visualize them.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;Y_hat = X @ np.linalg.inv(X.T @ X) @ X.T @ Y
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plt.scatter(X, Y, s=50, label=&#39;data&#39;)
plt.plot(X, Y_hat, c=&#39;k&#39;, lw=2, label=&#39;prediction&#39;)
plt.vlines(X, np.minimum(Y, Y_hat), np.maximum(Y, Y_hat), color=&#39;r&#39;, lw=3, label=&amp;quot;residuals&amp;quot;);
plt.legend()
plt.title(f&amp;quot;Regression prediction and residuals&amp;quot;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/outliers_leverage_23_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Do some observations have unusually high residuals? Let&amp;rsquo;s plot their distribution.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df[&#39;residual&#39;] = np.abs(Y - X @ np.linalg.inv(X.T @ X) @ X.T @ Y)
df[&#39;high_residual&#39;] = df[&#39;residual&#39;] &amp;gt; (np.mean(df[&#39;residual&#39;]) + 2*np.std(df[&#39;residual&#39;]))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;fix, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))
sns.histplot(data=df, x=&#39;residual&#39;, hue=&#39;high_residual&#39;, alpha=1, bins=30, ax=ax1).set(title=&#39;Distribution of Residuals&#39;);
sns.scatterplot(data=df, x=&#39;hours&#39;, y=&#39;transactions&#39;, hue=&#39;high_residual&#39;, ax=ax2).set(title=&#39;Data Scatterplot&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/outliers_leverage_26_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Two observations have particularly high residuals. This means that for these observations, the model is not good at predicting the observed outcomes.&lt;/p&gt;
&lt;p&gt;Is this bad news? Not necessarily. A model that fits the observations too well is likely to be &lt;a href=&#34;https://en.wikipedia.org/wiki/Bias_%28statistics%29&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;biased&lt;/strong&gt;&lt;/a&gt;. However, it might still be important to understand why some users have a different relationship between hours spent and total transactions. As usual, information on the specific context is key.&lt;/p&gt;
&lt;p&gt;So far we have looked at observations with &amp;ldquo;unusual&amp;rdquo; characteristics and &amp;ldquo;unusual&amp;rdquo; model fit, but what is the observation itself is distorting the model? How much our model is driven by a handful of observations?&lt;/p&gt;
&lt;h2 id=&#34;influence&#34;&gt;Influence&lt;/h2&gt;
&lt;p&gt;The concept of &lt;strong&gt;influence and influence functions&lt;/strong&gt; was developed precisely to answer this question: what are influential observations? This questions were very popular in the 80&amp;rsquo;s and lost appeal for a long time until the recent need of explaining complex machine learning and AI models.&lt;/p&gt;
&lt;p&gt;The general idea is to define an observation as &lt;strong&gt;influential&lt;/strong&gt; if removing it significantly changes the estimated model. In linear regression, we define the influence of observation $i$ as:&lt;/p&gt;
&lt;p&gt;$$
\hat{\beta} - \hat{\beta}_{-i} = (X&amp;rsquo;X)^{-1} x_i e_i
$$&lt;/p&gt;
&lt;p&gt;Where $\hat{\beta}_{-i}$ is the OLS coefficient estimated omitting observation $i$.&lt;/p&gt;
&lt;p&gt;As you can see, there is a tight connection to both leverage $h_{ii}$ and residuals $e_i$: influence is almost the product of the two. Indeed, in linear regression, observations with high leverage are observations that are both outliers and have high residuals. None of the two conditions alone is sufficient for an observation to have an influence on the model.&lt;/p&gt;
&lt;p&gt;We can see it best in the data.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df[&#39;influence&#39;] = (np.linalg.inv(X.T @ X) @ X.T).T * np.abs(Y - Y_hat)
df[&#39;high_influence&#39;] = df[&#39;influence&#39;] &amp;gt; (np.mean(df[&#39;influence&#39;]) + 2*np.std(df[&#39;influence&#39;]))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;fix, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))
sns.histplot(data=df, x=&#39;influence&#39;, hue=&#39;high_influence&#39;, alpha=1, bins=30, ax=ax1).set(title=&#39;Distribution of Influences&#39;);
sns.scatterplot(data=df, x=&#39;hours&#39;, y=&#39;transactions&#39;, hue=&#39;high_influence&#39;, ax=ax2).set(title=&#39;Data Scatterplot&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/outliers_leverage_31_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;In our dataset, there is only one observation with high influence, and it is disproportionally larger than the influence of all other observations.&lt;/p&gt;
&lt;p&gt;We can now plot all &amp;ldquo;unusual&amp;rdquo; points in the same plot. I also report residuals and leverage of each point in a separate plot.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def plot_leverage_residuals(df):

    # Hue
    df[&#39;type&#39;] = &#39;Normal&#39;
    df.loc[df[&#39;high_residual&#39;], &#39;type&#39;] = &#39;High Residual&#39;
    df.loc[df[&#39;high_leverage&#39;], &#39;type&#39;] = &#39;High Leverage&#39;
    df.loc[df[&#39;high_influence&#39;], &#39;type&#39;] = &#39;High Influence&#39;

    # Init figure
    fig, (ax1,ax2) = plt.subplots(1,2, figsize=(12,5)) 
    ax1.plot(X, Y_hat, lw=1, c=&#39;grey&#39;, zorder=0.5)
    sns.scatterplot(data=df, x=&#39;hours&#39;, y=&#39;transactions&#39;, ax=ax1, hue=&#39;type&#39;).set(title=&#39;Data&#39;)
    sns.scatterplot(data=df, x=&#39;residual&#39;, y=&#39;leverage&#39;, hue=&#39;type&#39;, ax=ax2).set(title=&#39;Metrics&#39;)
    ax1.get_legend().remove()
    sns.move_legend(ax2, &amp;quot;upper left&amp;quot;, bbox_to_anchor=(1.05, 0.8));
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plot_leverage_residuals(df)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/outliers_leverage_34_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;As we can see, we have one point with high residual and low leverage, one with high leverage and low residual and only one point with both high leverage and high residual: the only influential point.&lt;/p&gt;
&lt;p&gt;From the plot it is also clear why none of the two conditions alone is sufficient for an observation to rive the model. The orange point has high residual but it lies right in the middle of the distribution and therefore cannot tilt the line of best fit. The green point instead has high leverage and lies far from the center of the distribution but its perfectly aligned with the line of fit. Removing it would not change anything. The red dot instead is different from the others in terms of &lt;strong&gt;both characteristics and behavior&lt;/strong&gt; and therefore tilts the fit line towards itself.&lt;/p&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;In this post, we have seen some different ways in which observations can be &amp;ldquo;unusual&amp;rdquo;: they can have either unusual characteristics or unusual behavior. In linear regression, when an observation has both it is also influential: it tilts the model towards itself.&lt;/p&gt;
&lt;p&gt;In the example of the article, we concentrated on a univariate linear regression. However, research on influence functions has recently become a hot topic because of the need to make black-box machine learning algorithms understandable. With models with millions of parameters, billions of observations and wild non-linearities, it can be very hard to establish whether a single observation is influential and how.&lt;/p&gt;
&lt;h3 id=&#34;references&#34;&gt;References&lt;/h3&gt;
&lt;p&gt;[1] D. Cook, &lt;a href=&#34;https://www.jstor.org/stable/1268249&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Detection of Influential Observation in Linear Regression&lt;/a&gt; (1980), &lt;em&gt;Technometrics&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;[2] D. Cook, S. Weisberg, &lt;a href=&#34;https://www.jstor.org/stable/1268187&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Characterizations of an Empirical Influence Function for Detecting Influential Cases in
Regression&lt;/a&gt; (1980), &lt;em&gt;Technometrics&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;[2] P. W. Koh, P. Liang, &lt;a href=&#34;http://proceedings.mlr.press/v70/koh17a&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Understanding Black-box Predictions via Influence Functions&lt;/a&gt; (2017), &lt;em&gt;ICML Proceedings&lt;/em&gt;.&lt;/p&gt;
&lt;h3 id=&#34;code&#34;&gt;Code&lt;/h3&gt;
&lt;p&gt;You can find the original Jupyter Notebook here:&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/outliers.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/outliers.ipynb&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>A/B Tests, Privacy and Online Regression</title>
      <link>https://matteocourthoud.github.io/post/online_regression/</link>
      <pubDate>Mon, 10 Jul 2023 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/post/online_regression/</guid>
      <description>&lt;p&gt;&lt;em&gt;How to run experiments without storing individual data&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;AB tests, a.k.a. randomized controlled trials, are widely recognized as the gold standard technique to compute the causal effect of a treatment (a drug, ad, product, &amp;hellip;) on an outcome of interest (a disease, firm revenue, customer satisfaction, &amp;hellip;). We randomly split a set of subjects (patients, users, customers, &amp;hellip;) into a treatment and a control group and give the treatment to the treatment group. This procedure ensures that ex-ante, the only expected difference between the two groups is caused by the treatment.&lt;/p&gt;
&lt;p&gt;One potential &lt;strong&gt;privacy concern&lt;/strong&gt; is that one needs to store data about many users for the whole duration of the experiment in order to estimate the effect of the treatment. This is not a problem if we can run the experiment instantaneusly, but can become an issue when the experiment duration is long. In this post, we are going to explore one solution to this problem: &lt;strong&gt;online regression&lt;/strong&gt;. We will see how to estimate (conditional) average treatment effects and how to do inference, using both asymptotic approximations and bootstrapping.&lt;/p&gt;
&lt;p&gt;⚠️ Some parts are algebra-intense, but you can skip them if you are only interested in the intuition.&lt;/p&gt;
&lt;h2 id=&#34;credit-cards-and-donations&#34;&gt;Credit Cards and Donations&lt;/h2&gt;
&lt;p&gt;Suppose, for example, that we were a fin-tech company. We have designed a new user interface (UI) for our mobile application and we would like to understand whether it slows down our transaction. In order to estimate the causal effect of the new UI on transaction speed, we plan to run an A/B test or randomized controlled trial.&lt;/p&gt;
&lt;p&gt;We have one major problem: we should not store user-level information for privacy reasons.&lt;/p&gt;
&lt;p&gt;First, let&amp;rsquo;s have a look at the data. I import the data generating process &lt;code&gt;dgp_credit()&lt;/code&gt; from &lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/src/dgp.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;src.dgp&lt;/code&gt;&lt;/a&gt; and some plotting functions and libraries from &lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/src/utils.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;src.utils&lt;/code&gt;&lt;/a&gt;. I include code snippets from &lt;a href=&#34;https://deepnote.com&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Deepnote&lt;/a&gt;, a Jupyter-like web-based collaborative notebook environment. For our purpose, Deepnote is very handy because it allows me not only to include code but also output, like data and tables.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;%matplotlib inline
%config InlineBackend.figure_format = &#39;retina&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from src.utils import *
from src.dgp import dgp_credit
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I first generate the whole dataset in one-shot. We will then investigate how to perform the experimental analysis in case the data was arriving dynamically.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def generate_data(self, N=100, seed=0):
        np.random.seed(seed)
        
        # Connection speed
        connection = np.random.lognormal(3, 1, N)
        
        # Treatment assignment
        treated = np.random.binomial(1, 0.5, N)
        
        # Transfer speed
        #spend = np.minimum(np.random.lognormal(1 + treated + 0.1*np.sqrt(balance), 2, N), balance)
        speed = np.minimum(np.random.exponential(10 + 4*treated - 0.5*np.sqrt(connection), N), connection)
        
        # Generate the dataframe
        df = pd.DataFrame({&#39;c&#39;: [1]*N, &#39;treated&#39;: treated,  
                           &#39;connection&#39;: np.round(connection,2), 
                           &#39;speed&#39;: np.round(speed,2)})

        return df
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;N = 100
df = generate_data(N)
df.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;c&lt;/th&gt;
      &lt;th&gt;treated&lt;/th&gt;
      &lt;th&gt;connection&lt;/th&gt;
      &lt;th&gt;speed&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;117.22&lt;/td&gt;
      &lt;td&gt;0.94&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;29.97&lt;/td&gt;
      &lt;td&gt;29.97&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;53.45&lt;/td&gt;
      &lt;td&gt;7.38&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;188.84&lt;/td&gt;
      &lt;td&gt;0.76&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;130.00&lt;/td&gt;
      &lt;td&gt;24.44&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;We have information on 100 users, for whom we observe&amp;hellip;&lt;/p&gt;
&lt;p&gt;First, let&amp;rsquo;s have a look&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;model = smf.ols(&#39;speed ~ treated + connection&#39;, data=df).fit()
model.summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
       &lt;td&gt;&lt;/td&gt;         &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt;  &lt;td&gt;    6.0740&lt;/td&gt; &lt;td&gt;    1.079&lt;/td&gt; &lt;td&gt;    5.630&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    3.933&lt;/td&gt; &lt;td&gt;    8.215&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;treated&lt;/th&gt;    &lt;td&gt;    1.3939&lt;/td&gt; &lt;td&gt;    1.297&lt;/td&gt; &lt;td&gt;    1.075&lt;/td&gt; &lt;td&gt; 0.285&lt;/td&gt; &lt;td&gt;   -1.180&lt;/td&gt; &lt;td&gt;    3.968&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;connection&lt;/th&gt; &lt;td&gt;   -0.0033&lt;/td&gt; &lt;td&gt;    0.017&lt;/td&gt; &lt;td&gt;   -0.197&lt;/td&gt; &lt;td&gt; 0.844&lt;/td&gt; &lt;td&gt;   -0.037&lt;/td&gt; &lt;td&gt;    0.030&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;In order to understand how we can make linear regression one data point at the time, we first need a brief linear algebra recap.&lt;/p&gt;
&lt;p&gt;First of all, let&amp;rsquo;s define $y$ the dependent variable, &lt;code&gt;spend&lt;/code&gt; in our case, and $X$ the explanatory variable, the &lt;code&gt;treated&lt;/code&gt; indicator, the account &lt;code&gt;balance&lt;/code&gt; and a constant.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def xy_from_df(df, r0, r1):
    return df.iloc[r0:r1,:3].to_numpy(), df.iloc[r0:r1,3].to_numpy()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The  estimator is given by
$$
\hat{\beta}_{OLS} = (X&amp;rsquo;X)^{-1}X&amp;rsquo;y
$$&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from numpy.linalg import inv

X, Y = xy_from_df(df, 0, 100)
inv(X.T @ X) @ X.T @ Y
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;array([ 6.07404291e+00,  1.39385101e+00, -3.33599131e-03])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We get indeed the same exact number as with the &lt;code&gt;smf.ols&lt;/code&gt; command!&lt;/p&gt;
&lt;p&gt;Can we compute $\beta$ one observation at the time?&lt;/p&gt;
&lt;p&gt;The answer is yes! Assume we had $n$ observations and we just received the $n+1$th observation: the pair $(x_{n+1}, y_{n+1})$. In order to compute $\hat{\beta}_{n+1}$ we need to have stored only two objects in memory&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\hat{\beta}_{n}$, the previous estimate of $\beta$&lt;/li&gt;
&lt;li&gt;$(X_n&amp;rsquo; X_n)^{-1}$, the previous value of $(X&amp;rsquo; X)^{-1}$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;First of all, how do we update $(X&amp;rsquo; X)^{-1}$?
$$
\begin{align*}
(X_{n+1}&amp;rsquo; X_{n+1})^{-1} = (X_n&amp;rsquo; X_n)^{-1} - \frac{(X_n&amp;rsquo; X_n)^{-1} x_{n+1} x_{n+1}&amp;rsquo; (X_n&amp;rsquo; X_n)^{-1}}{1 + x_{n+1}&amp;rsquo; (X_n&amp;rsquo; X_n)^{-1} x_{n+1}}
\end{align*}
$$&lt;/p&gt;
&lt;p&gt;After having updated $(X&amp;rsquo; X)^{-1}$, we can update $\hat{\beta}$.
$$
\hat{\beta}&lt;em&gt;{n+1} = \hat{\beta}&lt;/em&gt;{n} + (X_n&amp;rsquo; X_n)^{-1} x_{n} (y_n - x_n&amp;rsquo; \hat{\beta}_{n})
$$&lt;/p&gt;
&lt;p&gt;Note that this procedure is not only privacy friendly but also &lt;strong&gt;memory-friendly&lt;/strong&gt;. Our dataset is a $100 \times 4$ matrix while $(X&amp;rsquo; X)^{-1}$ is $3 \times 3$ matrix and $\beta$ is a $3 \times 1$ matrix. We are storing only 12 numbers instead of up to 400!&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def update_xb(XiX, beta, x, y):
    XiX -= (XiX @ x.T @ x @ XiX) / (1 + x @ XiX @ x.T )
    beta += XiX @ x.T @ (y - x @ beta)
    return XiX, beta
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We are now ready to estimate our OLS coefficient, one data point at the time. However, we cannot really start from the first observation, because we would be unable to invert the matrix $X&amp;rsquo;X$. We need at least $k+1$ observations, where $k$ is the number of variables in $X$.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s use a warm start of 10 observations to be safe.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Initialize XiX and beta from first 10 observations
x, y = xy_from_df(df, 0, 10)
XiX = inv(x.T @ x)
beta = XiX @ x.T @ y

# Update estimate live
for n in range(10, N):
    x, y = xy_from_df(df, n, n+1)
    XiX, beta = update_xb(XiX, beta, x, y)
    
# Print result
print(beta)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[ 6.07404291e+00  1.39385101e+00 -3.33599131e-03]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We got exactly the same coefficient! Nice!&lt;/p&gt;
&lt;p&gt;How did we get there? We can plot the evolution of out estimate $\hat{\beta}$ as we accumulate data. The dynamic plotting function is a bit more cumbersome, but you can find it in &lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/src/figures.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;src.figures&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from src.figures import online_regression

online_regression(df, &amp;quot;fig/online_reg1.gif&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;fig/online_reg1.gif&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;As we can see, as the number of data points increases, the estimate seems to less and less volatile.&lt;/p&gt;
&lt;p&gt;But is it really the case? As usual, we are not just interested in the point estimate of the effect of the &lt;code&gt;coupon&lt;/code&gt; on spending, we would also like to understand how precise this estimate is.&lt;/p&gt;
&lt;h2 id=&#34;inference&#34;&gt;Inference&lt;/h2&gt;
&lt;p&gt;We have seen how to estimate the treatment effect &amp;ldquo;online&amp;rdquo;: one observation at the time. Can we also compute the variance of the estimator in the same manner?&lt;/p&gt;
&lt;p&gt;First of all, let&amp;rsquo;s review what the variance of the OLS estimator looks like. Under baseline assumptions, the variance of the OLS estimator is given by:
$$
\text{Var}(\hat{\beta}_{OLS}) = (X&amp;rsquo;X)^{-1} \hat{\sigma}^2
$$&lt;/p&gt;
&lt;p&gt;where $\hat{\sigma}^2$ is the variance of the residuals $e = (y - X&amp;rsquo;\hat{\beta})$.&lt;/p&gt;
&lt;p&gt;The regression table reports the standard errors of the coefficients, which are the squared elements on the diagonal of $\text{Var}(\hat{\beta})$.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;model.summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
       &lt;td&gt;&lt;/td&gt;         &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt;  &lt;td&gt;    6.0740&lt;/td&gt; &lt;td&gt;    1.079&lt;/td&gt; &lt;td&gt;    5.630&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    3.933&lt;/td&gt; &lt;td&gt;    8.215&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;treated&lt;/th&gt;    &lt;td&gt;    1.3939&lt;/td&gt; &lt;td&gt;    1.297&lt;/td&gt; &lt;td&gt;    1.075&lt;/td&gt; &lt;td&gt; 0.285&lt;/td&gt; &lt;td&gt;   -1.180&lt;/td&gt; &lt;td&gt;    3.968&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;connection&lt;/th&gt; &lt;td&gt;   -0.0033&lt;/td&gt; &lt;td&gt;    0.017&lt;/td&gt; &lt;td&gt;   -0.197&lt;/td&gt; &lt;td&gt; 0.844&lt;/td&gt; &lt;td&gt;   -0.037&lt;/td&gt; &lt;td&gt;    0.030&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;Let&amp;rsquo;s check that we would obtain the same numbers using matrix algebra.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;beta = inv(X.T @ X) @ X.T @ Y
np.sqrt(np.diag(inv(X.T @ X) * np.var(Y - X @ beta)))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;array([1.06261376, 1.27718352, 0.01669716])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Indeed, we get exactly the same numbers!&lt;/p&gt;
&lt;p&gt;We already have a method to part of $\text{Var}(\hat{\beta}&lt;em&gt;{OLS})$ online: $(X&amp;rsquo;X)^{-1}$ update the matrix $(X&amp;rsquo;X)^{-1}$ online. How do we update $\hat{\sigma}^2$? This is the formula to update the sum of squared residuals $S$.
$$
S&lt;/em&gt;{n+1} = S_{n} + \frac{(y_{n+1} - x_{n+1}\hat{\beta}&lt;em&gt;n)}{1 + x&lt;/em&gt;{n+1}&amp;rsquo; (X_n&amp;rsquo; X_n)^{-1} x_{n+1}}
$$&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def update_xbs(XiX, beta, S, x, y):
    S += (y - x @ beta)**2 / (1 + x @ XiX @ x.T )
    XiX -= (XiX @ x.T @ x @ XiX) / (1 + x @ XiX @ x.T )
    beta += XiX @ x.T @ (y - x @ beta)
    return XiX, beta, S[0,0]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note, the order here is very important!&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Inizialize XiX, beta, and sigma from the first 10 observations
x, y = xy_from_df(df, 0, 10)
XiX = inv(x.T @ x)
beta = XiX @ x.T @ y
S = np.sum((y - x @ beta)**2)

# Update XiX, beta, and sigma online
for n in range(10, N):
    x, y = xy_from_df(df, n, n+1)
    XiX, beta, S = update_xbs(XiX, beta, S, x, y)
    
# Print result
print(np.sqrt(np.diag(XiX * S / (N - 3))))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1.0789208  1.29678338 0.0169534 ]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We indeed got the same result! Note that to get from the sum of squared residuals $S$ to the residuals variance $\hat{\sigma}^2$ we need to divide by the degrees of freedom: $n - k = 100 - 3$.&lt;/p&gt;
&lt;p&gt;As before we have plotted the evolution of the estimate of the OLS coefficient over time, we can now augment that plot with a confidence band of +- one standard deviation.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;online_regression(df, &amp;quot;fig/online_reg2.gif&amp;quot;, ci=True)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;fig/online_reg2.gif&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;As we can see, the estimated variance of the OLS estimator indeed decreases as the sample size increases.&lt;/p&gt;
&lt;h2 id=&#34;bootstrap&#34;&gt;Bootstrap&lt;/h2&gt;
&lt;p&gt;So far we have used the asymptotic assumptions behind the Central Limit Theorem to compute the standard errors of the estimator. However, we have a particularly small sample. We further check the empirical distribution of the model residuals.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sns.histplot(model.resid, bins=30);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/online_regression_39_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;The residuals seem to be particularly &lt;strong&gt;skewed&lt;/strong&gt;! This might be a problem in such a small sample.&lt;/p&gt;
&lt;p&gt;One alternative is &lt;strong&gt;the bootstrap&lt;/strong&gt;. Instead of relying on asymptotics, we approximate the distribution of our estimator by resampling our dataset with replacement. Can we bootstrap online?&lt;/p&gt;
&lt;p&gt;The answer is once again yes! They key is to weight each observation with an integer weight drawn from a Poisson distribution with mean (and variance) equal to 1. We repeat this process multiple times, in parallel and then we&lt;/p&gt;
&lt;p&gt;The updating rules for $(X&amp;rsquo;X)^{-1}$ and $\hat{beta}$ become the following.
$$
\begin{align*}
(X_{n+1}&amp;rsquo; X_{n+1})^{-1} = (X_n&amp;rsquo; X_n)^{-1} - \frac{w (X_n&amp;rsquo; X_n)^{-1} x_{n+1} x_{n+1}&amp;rsquo; (X_n&amp;rsquo; X_n)^{-1}}{1 + w x_{n+1}&amp;rsquo; (X_n&amp;rsquo; X_n)^{-1} x_{n+1}}
\end{align*}
$$&lt;/p&gt;
&lt;p&gt;and
$$
\hat{\beta}&lt;em&gt;{n+1} = \hat{\beta}&lt;/em&gt;{n} + w (X_n&amp;rsquo; X_n)^{-1} x_{n} (y_n - x_n&amp;rsquo; \hat{\beta}_{n})
$$&lt;/p&gt;
&lt;p&gt;where $w$ are Poisson weights. First, let&amp;rsquo;s update the updating function for $(X&amp;rsquo;X)^{-1}$ and $\hat{beta}$.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def update_xbw(XiX, beta, w, x, y):
    XiX -= (w * XiX @ x.T @ x @ XiX) / (1 + w * x @ XiX @ x.T )
    beta += w * XiX @ x.T @ (y - x @ beta)
    return XiX, beta
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can now run the online estimation. We bootstrap 1000 different estimates of $\hat{\beta}$.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Inizialize a vector of XiXs and betas 
np.random.seed(0)
K = 1000
x, y = xy_from_df(df, 0, 10)
XiXs = [inv(x.T @ x) for k in range(K)]
betas = [xix @ x.T @ y for xix in XiXs]

# Update the vector of XiXs and betas online
for n in range(10, N):
    x, y = xy_from_df(df, n, n+1)
    for k in range(K):
        w = np.random.poisson(1)
        XiXs[k], betas[k] = update_xbw(XiXs[k], betas[k], w, x, y)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can compute the estimated standard deviation of the treatment effect, simply by computing the standard deviation of the vector of bootstrapped coefficients.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;np.std(betas, axis=0)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;array([0.95301002, 1.14186364, 0.01207962])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The estimated standard errors are slightly different from the previous values of $[1.275, 1.532, 0.020]$, but not very far apart.&lt;/p&gt;
&lt;p&gt;Lastly, some of you might have wondered &amp;ldquo;&lt;em&gt;why sampling discrete weights and not continuous ones?&lt;/em&gt;&amp;rdquo;. Indeed, we can. This procedure is called the &lt;strong&gt;Bayesian Bootstrap&lt;/strong&gt; and you can find a more detailed explanation &lt;a href=&#34;https://towardsdatascience.com/6ca4a1d45148&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;In this post, we have seen hot to run an experiment without storing individual-level data. How are we able to do it? In order to compute the average treatment effect, we do not need every single observation but it&amp;rsquo;s sufficient to store just a more compact representation of it.&lt;/p&gt;
&lt;h3 id=&#34;references&#34;&gt;References&lt;/h3&gt;
&lt;p&gt;[1] W. Chou, &lt;a href=&#34;https://arxiv.org/abs/2102.03316&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Randomized Controlled Trials without Data Retention&lt;/a&gt; (2021), &lt;em&gt;Working Paper&lt;/em&gt;.&lt;/p&gt;
&lt;h3 id=&#34;related-articles&#34;&gt;Related Articles&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/954506cec665&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Experiments, Peeking, and Optimal Stopping&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/6ca4a1d45148&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The Bayesian Bootstrap&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;code&#34;&gt;Code&lt;/h3&gt;
&lt;p&gt;You can find the original Jupyter Notebook here:&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/online_reg.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/online_reg.ipynb&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>DAGs and Control Variables</title>
      <link>https://matteocourthoud.github.io/post/good_bad_controls/</link>
      <pubDate>Mon, 10 Jul 2023 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/post/good_bad_controls/</guid>
      <description>&lt;p&gt;When analyzing causal relationships, it is very hard to understand which variables to &lt;strong&gt;condition the analysis on&lt;/strong&gt;, i.e. how to &amp;ldquo;split&amp;rdquo; the data so that we are &lt;strong&gt;comparing apples to apples&lt;/strong&gt;. For example, if you want to understand the effect of having a tablet in class on studenta&amp;rsquo; performance, it makes sense to compare schools where students have similar socio-economic backgrounds. Otherwise, the risk is that only wealthier students can afford a tablet and, without controlling for it, we might attribute the effect to tablets instead of the socio-economic background.&lt;/p&gt;
&lt;p&gt;When the treatment of interest comes from a proper &lt;strong&gt;randomized experiment&lt;/strong&gt;, we do not need to worry about conditioning on other variables. If tablets are distributed randomly across schools, and we have enough schools in the experiment, we do not have to worry about the socio-economic background of students. The only advantage of conditioning the analysis on some so-called &amp;ldquo;control variable&amp;rdquo; could be an increase in power. However, this is a different story.&lt;/p&gt;
&lt;p&gt;In this post, we are going to have a brief introduction to Directed Acyclic Graphs and how they can be useful to select variables to condition a causal analysis on. Not only DAGs provide visual intuition on which variables we need to &lt;em&gt;include&lt;/em&gt; in the analysis, but also on which variables we should &lt;em&gt;not include&lt;/em&gt;, and why.&lt;/p&gt;
&lt;h2 id=&#34;directed-acyclic-graphs&#34;&gt;Directed Acyclic Graphs&lt;/h2&gt;
&lt;h3 id=&#34;definitions&#34;&gt;Definitions&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Directed acyclic graphs&lt;/strong&gt; (&lt;strong&gt;DAG&lt;/strong&gt;s) provide a visual representation of the data generating process. Random variables are represented with letters (e.g. $X$) and causal relationships are represented with arrows (e.g. $\to$). For example, we interpret&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-mermaid&#34;&gt;flowchart LR
classDef white fill:#FFFFFF,stroke:#000000,stroke-width:2px
X((X)):::white --&amp;gt; Y((Y)):::white
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;as $X$ (possibly) causes $Y$. We call a &lt;strong&gt;path&lt;/strong&gt; between two variables $X$ and $Y$ any connection, &lt;em&gt;independently of the direction of the arrows&lt;/em&gt;. If all arrows point forward, we call it a &lt;strong&gt;causal path&lt;/strong&gt;, otherwise we call it a &lt;strong&gt;spurious path&lt;/strong&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-mermaid&#34;&gt;flowchart LR
classDef included fill:#DCDCDC,stroke:#000000,stroke-width:2px;
classDef excluded fill:#ffffff,stroke:#000000,stroke-width:2px;

X((X))
Y((Y))
Z1((Z1))
Z2((Z2))
Z3((Z3))

X --&amp;gt; Z1
Z1 --&amp;gt; Z2
Z3 --&amp;gt; Z2
Z3 --&amp;gt; Y

class X,Y included;
class Z1,Z2,Z3 excluded;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In the example above, we have a path between $X$ and $Y$ passing through the variables $Z_1$, $Z_2$, and $Z_3$. Since not all arrows point forward, the path is &lt;em&gt;spurious&lt;/em&gt; and there is no causal relationship of $X$ on $Y$. In fact, variable $Z_2$ is caused by both $Z_1$ and $Z_3$ and therefore &lt;strong&gt;blocks&lt;/strong&gt; the path.&lt;/p&gt;
&lt;p&gt;$Z_2$ is called a &lt;strong&gt;collider&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;The purpose of our analysis is to assess the &lt;strong&gt;causal relationship&lt;/strong&gt; between two variables $X$ and $Y$. Directed acyclic graphs are useful because they provide us instructions on which other variables $Z$ we need to &lt;strong&gt;condition&lt;/strong&gt; our analysis on. Conditioning the analysis on a variable means that we keep it fixed and we draw our conclusions &lt;em&gt;ceteris paribus&lt;/em&gt;. For example, in a linear regression framework, inserting another regressor $Z$ means that we are computing the best linear approximation of the conditional expectation function of $Y$ given $X$, &lt;em&gt;conditional&lt;/em&gt; on the observed values of $Z$.&lt;/p&gt;
&lt;h3 id=&#34;causality&#34;&gt;Causality&lt;/h3&gt;
&lt;p&gt;In order to assess causality, we want to &lt;strong&gt;close all spurious paths&lt;/strong&gt; between $X$ and $Y$. The &lt;strong&gt;questions&lt;/strong&gt; now are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;When is a path &lt;strong&gt;open&lt;/strong&gt;?
&lt;ul&gt;
&lt;li&gt;If it does not contain &lt;em&gt;colliders&lt;/em&gt;. Otherwise, it is &lt;em&gt;closed&lt;/em&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;How do you &lt;strong&gt;close an open path&lt;/strong&gt;?
&lt;ul&gt;
&lt;li&gt;You condition on &lt;em&gt;at least one&lt;/em&gt; intermediate variable.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;How do you &lt;strong&gt;open a closed path&lt;/strong&gt;?
&lt;ul&gt;
&lt;li&gt;You condition on &lt;em&gt;all&lt;/em&gt; colliders along the path.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Suppose we are again interested in the causal relationship of $X$ on $Y$. Let&amp;rsquo;s consider the following graph&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-mermaid&#34;&gt;flowchart LR
classDef included fill:#DCDCDC,stroke:#000000,stroke-width:2px;
classDef excluded fill:#ffffff,stroke:#000000,stroke-width:2px;

X((X))
Y((Y))
Z1((Z1))
Z2((Z2))
Z3((Z3))

X --&amp;gt; Y
X --&amp;gt; Z2
Z2 --&amp;gt; Y
Z1 --&amp;gt; X
Z1 --&amp;gt; Y
X --&amp;gt; Z3
Y --&amp;gt; Z3

class X,Y included;
class Z1,Z2,Z3 excluded;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In this case, apart from the direct path, there are &lt;strong&gt;three non-direct paths&lt;/strong&gt; between $X$ and $Y$ through the variables $Z_1$, $Z_2$, and $Z_3$.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s consider the case in which we analyze the relationship between $X$ and $Y$, ignoring all other variables.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;The path through $Z_1$ is &lt;strong&gt;open&lt;/strong&gt; but it is &lt;strong&gt;spurious&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;The path through $Z_2$ is &lt;strong&gt;open&lt;/strong&gt; and &lt;strong&gt;causal&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;The path through $Z_3$ is &lt;strong&gt;closed&lt;/strong&gt; since $Z_3$ is a &lt;em&gt;collider&lt;/em&gt; and it is &lt;strong&gt;spurious&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Let&amp;rsquo;s draw the same graph indicating in &lt;em&gt;grey&lt;/em&gt; variables that we are conditioning on, with &lt;em&gt;dotted lines&lt;/em&gt; closed paths, with &lt;em&gt;red lines&lt;/em&gt; spurious open paths, and with &lt;em&gt;green lines&lt;/em&gt; causal open paths.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-mermaid&#34;&gt;flowchart LR
classDef included fill:#DCDCDC,stroke:#000000,stroke-width:2px;
classDef excluded fill:#ffffff,stroke:#000000,stroke-width:2px;

X((X))
Y((Y))
Z1((Z1))
Z2((Z2))
Z3((Z3))

X --&amp;gt; Y
X --&amp;gt; Z2
Z2 --&amp;gt; Y
Z1 --&amp;gt; X
Z1 --&amp;gt; Y
X -.-&amp;gt; Z3
Y -.-&amp;gt; Z3

linkStyle 0,1,2 stroke:#00ff00,stroke-width:4px;
linkStyle 3,4 stroke:#ff0000,stroke-width:4px;
class X,Y included;
class Z1,Z2,Z3 excluded;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In this case, to assess the &lt;strong&gt;causal&lt;/strong&gt; relationship between $X$ and $Y$ we need to &lt;strong&gt;close&lt;/strong&gt; the path that passes through $Z_1$. We can do that by conditioning the analysis on $Z_1$.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-mermaid&#34;&gt;flowchart LR
classDef included fill:#DCDCDC,stroke:#000000,stroke-width:2px;
classDef excluded fill:#ffffff,stroke:#000000,stroke-width:2px;

X((X))
Y((Y))
Z1((Z1))
Z2((Z2))
Z3((Z3))

X --&amp;gt; Y
X --&amp;gt; Z2
Z2 --&amp;gt; Y
Z1 -.-&amp;gt; X
Z1 -.-&amp;gt; Y
X -.-&amp;gt; Z3
Y -.-&amp;gt; Z3

linkStyle 0,1,2 stroke:#00ff00,stroke-width:4px;
class X,Y,Z1 included;
class Z2,Z3 excluded;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we are able to recover the causal relationship between $X$ and $Y$ by conditioning on $Z_1$.&lt;/p&gt;
&lt;p&gt;What would happen if we were also &lt;strong&gt;conditioning on $Z_2$&lt;/strong&gt;? In this case, we would &lt;strong&gt;close&lt;/strong&gt; the path passing through $Z_2$ leaving only the &lt;em&gt;direct&lt;/em&gt; path between $X$ and $Y$ open. We would then recover only the &lt;strong&gt;direct effect&lt;/strong&gt; of $X$ on $Y$ and not the &lt;em&gt;indirect&lt;/em&gt; one.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-mermaid&#34;&gt;flowchart LR
classDef included fill:#DCDCDC,stroke:#000000,stroke-width:2px;
classDef excluded fill:#ffffff,stroke:#000000,stroke-width:2px;

X((X))
Y((Y))
Z1((Z1))
Z2((Z2))
Z3((Z3))

X --&amp;gt; Y
X -.-&amp;gt; Z2
Z2 -.-&amp;gt; Y
Z1 -.-&amp;gt; X
Z1 -.-&amp;gt; Y
X -.-&amp;gt; Z3
Y -.-&amp;gt; Z3

linkStyle 0 stroke:#00ff00,stroke-width:4px;
class X,Y,Z1,Z2 included;
class Z3 excluded;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What would happen if we were also &lt;strong&gt;conditioning on $Z_3$&lt;/strong&gt;? In this case, we would &lt;strong&gt;open&lt;/strong&gt; the path passing through $Z_3$ which is a &lt;strong&gt;spurious&lt;/strong&gt; path. We would then &lt;strong&gt;not&lt;/strong&gt; be able to recover the causal effect of $X$ on $Y$.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-mermaid&#34;&gt;flowchart LR
classDef included fill:#DCDCDC,stroke:#000000,stroke-width:2px;

X((X))
Y((Y))
Z1((Z1))
Z2((Z2))
Z3((Z3))

X --&amp;gt; Y
X -.-&amp;gt; Z2
Z2 -.-&amp;gt; Y
Z1 -.-&amp;gt; X
Z1 -.-&amp;gt; Y
X --&amp;gt; Z3
Y --&amp;gt; Z3

linkStyle 0 stroke:#00ff00,stroke-width:4px;
linkStyle 5,6 stroke:#ff0000,stroke-width:4px;
class X,Y,Z1,Z2,Z3 included;
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;example-class-size-and-math-scores&#34;&gt;Example: Class Size and Math Scores&lt;/h2&gt;
&lt;p&gt;Suppose you are interested in the &lt;strong&gt;effect of class size on math scores&lt;/strong&gt;. Are bigger classes better or worse for students&amp;rsquo; performance?&lt;/p&gt;
&lt;p&gt;Assume that the data generating process can be represented with the following &lt;strong&gt;DAG&lt;/strong&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-mermaid&#34;&gt;flowchart TB
classDef included fill:#DCDCDC,stroke:#000000,stroke-width:2px;
classDef excluded fill:#ffffff,stroke:#000000,stroke-width:2px;
classDef unobserved fill:#ffffff,stroke:#000000,stroke-width:2px,stroke-dasharray: 5 5;


X((class size))
Y((math score))
Z1((class year))
Z2((good school))
Z3((math hours))
Z4((hist score))
U((ability))

X --&amp;gt; Y
Z1 --&amp;gt; X
X --&amp;gt; Z4
U --&amp;gt; Y
U --&amp;gt; Z4
Z2 --&amp;gt; X
Z2 --&amp;gt; Y
Z2 --&amp;gt; Z4
Z3 --&amp;gt; Y

class X,Y included;
class Z1,Z2,Z3,Z4 excluded;
class U unobserved;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The variables of interest are highlighted. Moreover, the dotted line around &lt;code&gt;ability&lt;/code&gt; indicates that this is a variable that we do not observe in the data.&lt;/p&gt;
&lt;p&gt;We can now load the data and check what it looks like.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;%matplotlib inline
%config InlineBackend.figure_format = &#39;retina&#39;
from src.utils import *
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from src.dgp import dgp_school

df = dgp_school().generate_data()
df.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;math_hours&lt;/th&gt;
      &lt;th&gt;history_hours&lt;/th&gt;
      &lt;th&gt;good_school&lt;/th&gt;
      &lt;th&gt;class_year&lt;/th&gt;
      &lt;th&gt;class_size&lt;/th&gt;
      &lt;th&gt;math_score&lt;/th&gt;
      &lt;th&gt;hist_score&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;15&lt;/td&gt;
      &lt;td&gt;13.009309&lt;/td&gt;
      &lt;td&gt;15.167024&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;19&lt;/td&gt;
      &lt;td&gt;13.047033&lt;/td&gt;
      &lt;td&gt;13.387456&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;25&lt;/td&gt;
      &lt;td&gt;8.330311&lt;/td&gt;
      &lt;td&gt;10.824070&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;22&lt;/td&gt;
      &lt;td&gt;11.322190&lt;/td&gt;
      &lt;td&gt;14.594394&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;15&lt;/td&gt;
      &lt;td&gt;12.338458&lt;/td&gt;
      &lt;td&gt;11.871626&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;What variables should we condition our regression on, in order to estimate the causal effect of &lt;code&gt;class size&lt;/code&gt; on &lt;code&gt;math scores&lt;/code&gt;?&lt;/p&gt;
&lt;p&gt;First of all, let&amp;rsquo;s look at what happens if we do not condition our analysis on any variable and we just regress &lt;code&gt;math score&lt;/code&gt; on &lt;code&gt;class size&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;smf.ols(&#39;math_score ~ class_size&#39;, df).fit().summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
       &lt;td&gt;&lt;/td&gt;         &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt;  &lt;td&gt;   12.0421&lt;/td&gt; &lt;td&gt;    0.259&lt;/td&gt; &lt;td&gt;   46.569&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;   11.535&lt;/td&gt; &lt;td&gt;   12.550&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;class_size&lt;/th&gt; &lt;td&gt;   -0.0399&lt;/td&gt; &lt;td&gt;    0.013&lt;/td&gt; &lt;td&gt;   -3.025&lt;/td&gt; &lt;td&gt; 0.003&lt;/td&gt; &lt;td&gt;   -0.066&lt;/td&gt; &lt;td&gt;   -0.014&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;The effect of &lt;code&gt;class_size&lt;/code&gt; is negative and statistically different from zero.&lt;/p&gt;
&lt;p&gt;But should we believe this estimated effect? Without controlling for anything, this is &lt;strong&gt;DAG representation&lt;/strong&gt; of the effect we are capturing.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-mermaid&#34;&gt;flowchart TB
classDef included fill:#DCDCDC,stroke:#000000,stroke-width:2px;
classDef excluded fill:#ffffff,stroke:#000000,stroke-width:2px;
classDef unobserved fill:#ffffff,stroke:#000000,stroke-width:2px,stroke-dasharray: 5 5;


X((class size))
Y((math score))
Z1((class year))
Z2((good school))
Z3((math hours))
Z4((hist score))
U((ability))

X --&amp;gt; Y
Z1 --&amp;gt; X
X -.-&amp;gt; Z4
U --&amp;gt; Y
U -.-&amp;gt; Z4
Z2 --&amp;gt; X
Z2 --&amp;gt; Y
Z2 --&amp;gt; Z4
Z3 --&amp;gt; Y

linkStyle 0 stroke:#00ff00,stroke-width:4px;
linkStyle 5,6 stroke:#ff0000,stroke-width:4px;

class X,Y included;
class Z1,Z2,Z3,Z4 excluded;
class U unobserved;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There is a &lt;strong&gt;spurious&lt;/strong&gt; path passing through &lt;code&gt;good school&lt;/code&gt; that &lt;strong&gt;biases&lt;/strong&gt; our estimated coefficient. Intuitively, being enrolled in a better school improves the students&amp;rsquo; math scores and better schools might have smaller class sizes. We need to control for the quality of the school.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;smf.ols(&#39;math_score ~ class_size + good_school&#39;, df).fit().summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
       &lt;td&gt;&lt;/td&gt;          &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt;   &lt;td&gt;    4.7449&lt;/td&gt; &lt;td&gt;    0.247&lt;/td&gt; &lt;td&gt;   19.176&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    4.259&lt;/td&gt; &lt;td&gt;    5.230&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;class_size&lt;/th&gt;  &lt;td&gt;    0.2095&lt;/td&gt; &lt;td&gt;    0.010&lt;/td&gt; &lt;td&gt;   20.020&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    0.189&lt;/td&gt; &lt;td&gt;    0.230&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;good_school&lt;/th&gt; &lt;td&gt;    5.0807&lt;/td&gt; &lt;td&gt;    0.130&lt;/td&gt; &lt;td&gt;   39.111&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    4.826&lt;/td&gt; &lt;td&gt;    5.336&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;Now the estimate of the effect of &lt;code&gt;class size&lt;/code&gt; on &lt;code&gt;math score&lt;/code&gt; is &lt;strong&gt;unbiased&lt;/strong&gt;! Indeed, the true coefficient in the data generating process was $0.2$.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-mermaid&#34;&gt;flowchart TB
classDef included fill:#DCDCDC,stroke:#000000,stroke-width:2px;
classDef excluded fill:#ffffff,stroke:#000000,stroke-width:2px;
classDef unobserved fill:#ffffff,stroke:#000000,stroke-width:2px,stroke-dasharray: 5 5;


X((class size))
Y((math score))
Z1((class year))
Z2((good school))
Z3((math hours))
Z4((hist score))
U((ability))

X --&amp;gt; Y
Z1 --&amp;gt; X
X -.-&amp;gt; Z4
U --&amp;gt; Y
U -.-&amp;gt; Z4
Z2 -.-&amp;gt; X
Z2 -.-&amp;gt; Y
Z2 --&amp;gt; Z4
Z3 --&amp;gt; Y

linkStyle 0 stroke:#00ff00,stroke-width:4px;

class X,Y,Z2 included;
class Z1,Z3,Z4 excluded;
class U unobserved;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What would happen if we were to instead &lt;strong&gt;control for all variables&lt;/strong&gt;?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;smf.ols(&#39;math_score ~ class_size + good_school + math_hours + class_year + hist_score&#39;, df).fit().summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
       &lt;td&gt;&lt;/td&gt;          &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt;   &lt;td&gt;   -0.7847&lt;/td&gt; &lt;td&gt;    0.310&lt;/td&gt; &lt;td&gt;   -2.529&lt;/td&gt; &lt;td&gt; 0.012&lt;/td&gt; &lt;td&gt;   -1.394&lt;/td&gt; &lt;td&gt;   -0.176&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;class_size&lt;/th&gt;  &lt;td&gt;    0.1292&lt;/td&gt; &lt;td&gt;    0.010&lt;/td&gt; &lt;td&gt;   13.054&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    0.110&lt;/td&gt; &lt;td&gt;    0.149&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;good_school&lt;/th&gt; &lt;td&gt;    2.9815&lt;/td&gt; &lt;td&gt;    0.170&lt;/td&gt; &lt;td&gt;   17.533&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    2.648&lt;/td&gt; &lt;td&gt;    3.315&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;math_hours&lt;/th&gt;  &lt;td&gt;    1.0516&lt;/td&gt; &lt;td&gt;    0.048&lt;/td&gt; &lt;td&gt;   21.744&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    0.957&lt;/td&gt; &lt;td&gt;    1.147&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;class_year&lt;/th&gt;  &lt;td&gt;    0.0424&lt;/td&gt; &lt;td&gt;    0.037&lt;/td&gt; &lt;td&gt;    1.130&lt;/td&gt; &lt;td&gt; 0.259&lt;/td&gt; &lt;td&gt;   -0.031&lt;/td&gt; &lt;td&gt;    0.116&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;hist_score&lt;/th&gt;  &lt;td&gt;    0.4116&lt;/td&gt; &lt;td&gt;    0.027&lt;/td&gt; &lt;td&gt;   15.419&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    0.359&lt;/td&gt; &lt;td&gt;    0.464&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;The coefficient is again &lt;strong&gt;biased&lt;/strong&gt;. Why?&lt;/p&gt;
&lt;p&gt;We have opened a new spurious path by controlling for &lt;code&gt;hist score&lt;/code&gt;. In fact, &lt;code&gt;hist score&lt;/code&gt; is a &lt;strong&gt;collider&lt;/strong&gt; and controlling for it has opened a path through &lt;code&gt;hist score&lt;/code&gt; and &lt;code&gt;ability&lt;/code&gt; that was otherwise closed.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-mermaid&#34;&gt;flowchart TB
classDef included fill:#DCDCDC,stroke:#000000,stroke-width:2px;
classDef excluded fill:#ffffff,stroke:#000000,stroke-width:2px;
classDef unobserved fill:#ffffff,stroke:#000000,stroke-width:2px,stroke-dasharray: 5 5;


X((class size))
Y((math score))
Z1((class year))
Z2((good school))
Z3((math hours))
Z4((hist score))
U((ability))

X --&amp;gt; Y
Z1 --&amp;gt; X
X --&amp;gt; Z4
U --&amp;gt; Y
U --&amp;gt; Z4
Z2 -.-&amp;gt; X
Z2 -.-&amp;gt; Y
Z2 --&amp;gt; Z4
Z3 --&amp;gt; Y

linkStyle 0 stroke:#00ff00,stroke-width:4px;
linkStyle 2,3,4 stroke:#ff0000,stroke-width:4px;

class X,Y,Z1,Z2,Z3,Z4 included;
class U unobserved;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The example was inspired by the following tweet.&lt;/p&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;We can illustrate this with Model 16 of the &amp;quot;Crash Course in Good and Bad Controls&amp;quot; (&lt;a href=&#34;https://t.co/GcSNzhuVt2&#34;&gt;https://t.co/GcSNzhuVt2&lt;/a&gt;). Here X = class size, Y = math4, Z = read4, and U = student&amp;#39;s ability. Conditioning on Z opens the path X -&amp;gt; Z &amp;lt;- U -&amp;gt; Y and it is thus a &amp;quot;bad control.&amp;quot; &lt;a href=&#34;https://t.co/KNfqtsMWwB&#34;&gt;https://t.co/KNfqtsMWwB&lt;/a&gt; &lt;a href=&#34;https://t.co/lUSigNYSJj&#34;&gt;pic.twitter.com/lUSigNYSJj&lt;/a&gt;&lt;/p&gt;&amp;mdash; Análise Real (@analisereal) &lt;a href=&#34;https://twitter.com/analisereal/status/1502793254592401409?ref_src=twsrc%5Etfw&#34;&gt;March 12, 2022&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;

&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;In this post, we have seen how to use Directed Acyclic Graphs to select control variables in a causal analysis. DAGs are very helpful tools since they provide an intuitive graphical representation of causal relationships between random variables. Contrary to common intuition that &amp;ldquo;the more information the better&amp;rdquo;, sometimes including extra variables might bias the analysis, preventing a causal interpretation of the results. In particular, we must pay attention not to include &lt;em&gt;colliders&lt;/em&gt; that open &lt;em&gt;spurious&lt;/em&gt; paths that would otherwise be closed.&lt;/p&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;p&gt;[1] C. Cinelli, A. Forney, J. Pearl, &lt;a href=&#34;https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3689437&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;A Crash Course in Good and Bad Controls&lt;/a&gt; (2018), working paper.&lt;/p&gt;
&lt;p&gt;[2] J. Pearl, &lt;a href=&#34;http://bayes.cs.ucla.edu/BOOK-2K/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Causality&lt;/a&gt; (2009), Cambridge University Press.&lt;/p&gt;
&lt;p&gt;[3] S. Cunningham, Chapter 3 of &lt;a href=&#34;https://mixtape.scunning.com/dag.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The Causal Inference Mixtape&lt;/a&gt; (2021), Yale University Press.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Double Debiased Machine Learning (part 1)</title>
      <link>https://matteocourthoud.github.io/post/regularization_bias/</link>
      <pubDate>Mon, 10 Jul 2023 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/post/regularization_bias/</guid>
      <description>&lt;p&gt;&lt;em&gt;Causal inference, machine learning and regularization bias&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;In causal inference, we often estimate causal effects by conditioning the analysis on other variables. We usually refer to these variables as &lt;strong&gt;control variables&lt;/strong&gt; or &lt;strong&gt;confounders&lt;/strong&gt;. In randomized control trials or AB tests, conditioning can increase the power of the analysis, by reducing imbalances that have emerged despite randomization. However, conditioning is even more important in observational studies, where, absent randomization, it might be &lt;a href=&#34;https://towardsdatascience.com/b63dc69e3d8c&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;essential to recover causal effects&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;When we have many control variables, we might want to &lt;strong&gt;select the most relevant ones&lt;/strong&gt;, ppossibly capturing nonlinearities and interactions. Machine learning algorithms are perfect for this task. However, in these cases, we are introducing a bias that is called &lt;strong&gt;regularization or pre-test, or feature selection bias&lt;/strong&gt;. In this and the next blog post, I try to explain the source of the bias and a very poweful solution called &lt;strong&gt;double debiased machine learning&lt;/strong&gt;, which has been probably one of the most relevant advancement at the intersection of machine learning and causal inference of the last decade.&lt;/p&gt;
&lt;h2 id=&#34;pre-testing&#34;&gt;Pre-Testing&lt;/h2&gt;
&lt;p&gt;Since this is a complex topic, let&amp;rsquo;s start with a simple example.&lt;/p&gt;
&lt;p&gt;Suppose we were a firm and we are interested in the &lt;strong&gt;effect of advertisement spending on revenue&lt;/strong&gt;: is advertisement worth the money? There are also a lot of other things that might influence sales, therefore, we are thinking of controlling for past sales in the analysis, in order to increase the power of our analysis.&lt;/p&gt;
&lt;p&gt;Assume the data generating process can be represented with the following &lt;a href=&#34;https://towardsdatascience.com/b63dc69e3d8c&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Directed Acyclic Graph (DAG)&lt;/strong&gt;&lt;/a&gt;. If you are not familiar with DAGs, I have written a short &lt;a href=&#34;https://towardsdatascience.com/b63dc69e3d8c&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;introduction here&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-mermaid&#34;&gt;flowchart LR
classDef included fill:#DCDCDC,stroke:#000000,stroke-width:2px;
classDef excluded fill:#ffffff,stroke:#000000,stroke-width:2px;
classDef unobserved fill:#ffffff,stroke:#000000,stroke-width:2px,stroke-dasharray: 5 5;

D((ad spend))
Z((past sales))
Y((sales))

D --&amp;gt; Y
Z -- ??? --&amp;gt; Y
Z --&amp;gt; D

class D,Y included;
class Z excluded;

linkStyle 0 stroke:#00ff00,stroke-width:4px;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I import the data generating process &lt;code&gt;dgp_tbd()&lt;/code&gt; from &lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/src/dgp.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;src.dgp&lt;/code&gt;&lt;/a&gt; and some plotting functions and libraries from &lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/src/utils.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;src.utils&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;%matplotlib inline
%config InlineBackend.figure_format = &#39;retina&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from src.utils import *
from src.dgp import dgp_pretest

df = dgp_pretest().generate_data()
df.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;ads&lt;/th&gt;
      &lt;th&gt;sales&lt;/th&gt;
      &lt;th&gt;past_sales&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;16.719800&lt;/td&gt;
      &lt;td&gt;19.196620&lt;/td&gt;
      &lt;td&gt;6.624345&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;7.732222&lt;/td&gt;
      &lt;td&gt;9.287491&lt;/td&gt;
      &lt;td&gt;4.388244&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;10.923469&lt;/td&gt;
      &lt;td&gt;11.816906&lt;/td&gt;
      &lt;td&gt;4.471828&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;8.457062&lt;/td&gt;
      &lt;td&gt;9.024376&lt;/td&gt;
      &lt;td&gt;3.927031&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;13.085146&lt;/td&gt;
      &lt;td&gt;12.814823&lt;/td&gt;
      &lt;td&gt;5.865408&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;We have data on $1000$ different markets, in which we observe current &lt;code&gt;sales&lt;/code&gt;, the amount spent in &lt;code&gt;advertisement&lt;/code&gt; and &lt;code&gt;past sales&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;We want to understand &lt;code&gt;ads&lt;/code&gt; spending is effective in increasing &lt;code&gt;sales&lt;/code&gt;. One possibility is to regress the latter on the former, using the following regression model, also called the &lt;strong&gt;short model&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;$$
\text{sales} = \alpha \cdot \text{ads} + \varepsilon
$$&lt;/p&gt;
&lt;p&gt;Should we also include &lt;code&gt;past sales&lt;/code&gt; in the regression? Then the regression model would be the following, also called &lt;strong&gt;long model&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;$$
\text{sales} = \alpha \cdot \text{ads} + \beta \cdot \text{past sales} + \varepsilon
$$&lt;/p&gt;
&lt;p&gt;Since we are not sure whether to condition the analysis on &lt;code&gt;past sales&lt;/code&gt;, we could &lt;strong&gt;let the data decide&lt;/strong&gt;: we could run the second regression and, if the effect of &lt;code&gt;past sales&lt;/code&gt;, $\beta$, is statistically significant, we are good with the long model, otherwise we run the short model.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;smf.ols(&#39;sales ~ ads + past_sales&#39;, df).fit().summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
       &lt;td&gt;&lt;/td&gt;         &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt;  &lt;td&gt;    0.1405&lt;/td&gt; &lt;td&gt;    0.185&lt;/td&gt; &lt;td&gt;    0.758&lt;/td&gt; &lt;td&gt; 0.448&lt;/td&gt; &lt;td&gt;   -0.223&lt;/td&gt; &lt;td&gt;    0.504&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;ads&lt;/th&gt;        &lt;td&gt;    0.9708&lt;/td&gt; &lt;td&gt;    0.030&lt;/td&gt; &lt;td&gt;   32.545&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    0.912&lt;/td&gt; &lt;td&gt;    1.029&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;past_sales&lt;/th&gt; &lt;td&gt;    0.3381&lt;/td&gt; &lt;td&gt;    0.095&lt;/td&gt; &lt;td&gt;    3.543&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    0.151&lt;/td&gt; &lt;td&gt;    0.525&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;It seems that the effect of &lt;code&gt;past sales&lt;/code&gt; on current &lt;code&gt;sales&lt;/code&gt; is positive and significant. Therefore, we are happy with our specification and we conclude that the effect of &lt;code&gt;ads&lt;/code&gt; on &lt;code&gt;sales&lt;/code&gt; is positive and significant with a 95% confidence interval of $[0.912, 1.029]$.&lt;/p&gt;
&lt;h2 id=&#34;the-bias&#34;&gt;The Bias&lt;/h2&gt;
&lt;p&gt;There is an &lt;strong&gt;issue&lt;/strong&gt; with this procedure: we are not taking into account the fact that we have run a test to decide whether to include &lt;code&gt;past_sales&lt;/code&gt; in the regression. The fact that we have decided to include &lt;code&gt;past_sales&lt;/code&gt; because its coefficient is significant &lt;em&gt;does&lt;/em&gt; have an effect on the inference on the effect of &lt;code&gt;ads&lt;/code&gt; on &lt;code&gt;sales&lt;/code&gt;, $\alpha$.&lt;/p&gt;
&lt;p&gt;The best way to understand the problem is through &lt;strong&gt;simulations&lt;/strong&gt;. Since we have access to the data generating process &lt;code&gt;dgp_pretest()&lt;/code&gt; (unlike in real life), we can just test what would happen if we were repeating this procedure multiple times:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;We draw a new sample from the data generating process.&lt;/li&gt;
&lt;li&gt;We regress &lt;code&gt;sales&lt;/code&gt; on &lt;code&gt;ads&lt;/code&gt; and &lt;code&gt;past_sales&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;If the coefficient of &lt;code&gt;past_sales&lt;/code&gt; is significant at the 95% level, we keep $\hat \alpha_{long}$ from (2).&lt;/li&gt;
&lt;li&gt;Otherwise, we regress &lt;code&gt;sales&lt;/code&gt; on &lt;code&gt;ads&lt;/code&gt; only, and we keep that coefficient $\hat \alpha_{short}$.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;I write a &lt;code&gt;pre_test&lt;/code&gt; function to implement the procedure above. I also save the coefficients from both regressions, long and short, and the chosen one, called the &lt;strong&gt;pre-test coefficient&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Reminder&lt;/strong&gt;: we are pre-testing the effect of &lt;code&gt;past_sales&lt;/code&gt; on &lt;code&gt;sales&lt;/code&gt; but the coefficient of interest is the one of &lt;code&gt;ads&lt;/code&gt; on &lt;code&gt;sales&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def pre_testing(d=&#39;ads&#39;, y=&#39;sales&#39;, x=&#39;past_sales&#39;, K=1000, **kwargs):
    
    # Init
    alpha = {&#39;Long&#39;: np.zeros(K), &#39;Short&#39;: np.zeros(K), &#39;Pre-test&#39;: np.zeros(K)}

    # Loop over simulations
    for k in range(K):
        
        # Generate data
        df = dgp_pretest().generate_data(seed=k, **kwargs)
        
        # Compute coefficients
        alpha[&#39;Long&#39;][k] = smf.ols(f&#39;{y} ~ {d} + {x}&#39;, df).fit().params[1]
        alpha[&#39;Short&#39;][k] = smf.ols(f&#39;{y} ~ {d}&#39;, df).fit().params[1]
    
        # Compute significance of beta
        p_value = smf.ols(f&#39;{y} ~ {d} + {x}&#39;, df).fit().pvalues[2]
        
        # Select specification based on p-value
        if p_value&amp;lt;0.05:
            alpha[&#39;Pre-test&#39;][k] = alpha[&#39;Long&#39;][k]
        else:
            alpha[&#39;Pre-test&#39;][k] = alpha[&#39;Short&#39;][k]
    
    return alpha
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;alphas = pre_testing()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can now plot the distributions (over simulations) of the estimated coefficients.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def plot_alphas(alphas, true_alpha):
    
    # Init plot
    fig, axes = plt.subplots(1, len(alphas), figsize=(4*len(alphas), 5), sharey=True, sharex=True)

    # Make one plot for each set of coefficients
    for i, key in enumerate(alphas.keys()):
        axes[i].hist(alphas[key], bins=30, lw=.1)
        axes[i].set_title(key)
        axes[i].axvline(true_alpha, c=&#39;r&#39;, ls=&#39;--&#39;)
        legend_text = [r&#39;$\alpha=%.0f$&#39; % true_alpha, r&#39;$\hat \alpha=%.4f$&#39; % np.mean(alphas[key])]
        axes[i].legend(legend_text, prop={&#39;size&#39;: 10}, loc=&#39;upper right&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plot_alphas(alphas, true_alpha=1)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/regularization_bias_16_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;In the plot above, I have depicted the estimated coefficients, across simulations, for the different regression specifications.&lt;/p&gt;
&lt;p&gt;As we can see from the first plot, if we were always running the &lt;strong&gt;long regression&lt;/strong&gt;, our estimator $\hat \alpha_{long}$ would be unbiased and normally distributed. However, if we were always running the &lt;strong&gt;short regression&lt;/strong&gt; (second plot), our estimator $\hat \alpha_{short}$ would be &lt;strong&gt;biased&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;pre-testing&lt;/strong&gt; procedure generates an estimator $\hat \alpha_{pretest}$ that is a mix of the two: most of the times we select the correct specification, the long regression, but sometimes the pre-test fails to reject the null hypothesis of no effect of &lt;code&gt;past sales&lt;/code&gt; on &lt;code&gt;sales&lt;/code&gt;, $H_0 : \beta = 0$, and we select the incorrect specification, running the short regression.&lt;/p&gt;
&lt;p&gt;Importantly, the pre-testing procedure &lt;strong&gt;does not generate a biased estimator&lt;/strong&gt;. As we can see in the last plot, the estimated coefficient is very close to the true value, 1. The reason is that most of the time, the number of times we select the &lt;em&gt;short&lt;/em&gt; regression is sufficiently small not to introduce bias, but not small enough to have valid inference.&lt;/p&gt;
&lt;p&gt;Indeed, &lt;strong&gt;pre-testing distorts inference&lt;/strong&gt;: the distribution of the estimator $\hat \alpha_{pretest}$ is not normal anymore, but bimodal. The &lt;strong&gt;consequence&lt;/strong&gt; is that our confidence intervals for $\alpha$ are going to have the wrong coverage (contain the true effect with a different probability than the claimed one).&lt;/p&gt;
&lt;h2 id=&#34;when-is-pre-testing-a-problem&#34;&gt;When is pre-testing a problem?&lt;/h2&gt;
&lt;p&gt;The problem of pre-testing arises because of the bias generated by running the short regression: &lt;a href=&#34;https://towardsdatascience.com/344ac1477699&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;omitted variable bias (OVB)&lt;/strong&gt;&lt;/a&gt;. In you are not familiar with OVB, I have written a &lt;a href=&#34;https://towardsdatascience.com/344ac1477699&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;short introduction here&lt;/a&gt;. In general however, we can express the omitted variable bias introduced by regressing $Y$ on $D$ ignoring $X$ as&lt;/p&gt;
&lt;p&gt;$$
\text{OVB} = \beta \delta \qquad \text{ where } \qquad \beta := \frac{Cov(X, Y)}{Var(X)}, \quad \delta := \frac{Cov(D, X)}{Var(D)}
$$&lt;/p&gt;
&lt;p&gt;Where $\beta$ is the effect of $X$ (&lt;code&gt;past sales&lt;/code&gt; in our example) on $Y$ (&lt;code&gt;sales&lt;/code&gt;) and $\delta$ is the effect of $D$ (&lt;code&gt;ads&lt;/code&gt;) on $X$.&lt;/p&gt;
&lt;p&gt;Pre-testing is a &lt;strong&gt;problem&lt;/strong&gt; if&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;We run the short regression instead of the long one &lt;em&gt;and&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;The effect of the bias is sensible&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;What can help improving (1), i.e. the probability of correctly rejecting the null hypothesis of zero effect of &lt;code&gt;past sales&lt;/code&gt;, $H_0 : \beta = 0$? The answer is simple: a &lt;strong&gt;bigger sample size&lt;/strong&gt;. If we have more observations, we can more precisely estimate $\beta$ and it is going to be less likely that we commit a &lt;a href=&#34;https://en.wikipedia.org/wiki/Type_I_and_type_II_errors&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;type 2 error&lt;/a&gt; and run the short regression instead of the long one.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s simulate the estimated coefficient $\hat \alpha$ under different sample sizes. Remember that the sample size used until now is $N=1000$.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;Ns = [100,300,1000,3000]
alphas = {f&#39;N = {n:.0f}&#39;:  pre_testing(N=n)[&#39;Pre-test&#39;] for n in Ns}
plot_alphas(alphas, true_alpha=1)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/regularization_bias_20_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;As we can see from the plots, as the sample size increases (left to right), the bias decreases and the distribution of the estimator $\hat \alpha_{pretest}$ converges to a normal distribution.&lt;/p&gt;
&lt;p&gt;What happens instead if the value of $\beta$ was different? It is probably going to affect point (2) in the previous paragraph, but how?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;If $\beta$ is &lt;strong&gt;very small&lt;/strong&gt;, it is going to be hard to detect it, and we will often end up running the &lt;em&gt;short&lt;/em&gt; regression, introducing a bias. However, if $\beta$ is very small, it also implies that the &lt;strong&gt;magnitude of the bias&lt;/strong&gt; is small and therefore it is not going to affect our estimate of $\alpha$ much&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;If $\beta$ is &lt;strong&gt;very big&lt;/strong&gt;, it is going to be easy to detect and we will often end up running the &lt;em&gt;long&lt;/em&gt; regression, avoiding the bias (which would have been very big though).&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Let&amp;rsquo;s simulate the estimated coefficient $\hat \alpha$ under different values of $\beta$. The true value used until now was $\beta = 0.3$.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;betas = 0.3 * np.array([0.1,0.3,1,3])
alphas = {f&#39;beta = {b:.2f}&#39;:  pre_testing(b=b)[&#39;Pre-test&#39;] for b in betas}
plot_alphas(alphas, true_alpha=1)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/regularization_bias_22_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;As we can see from the plots, as the value of $\beta$ increases, the bias first appears and then disappears. When $\beta$ is small (left plot), we often choose the short regression, but the bias is small and the average estimate is very close to the true value. For intermediate values of $\beta$, the bias is sensible and it has a clear effect on inference. Lastly, for large values of $\beta$ instead (right plot), we always run the long regression and the bias disappears.&lt;/p&gt;
&lt;p&gt;But &lt;strong&gt;when is a coefficient big or small&lt;/strong&gt;? And big or small with respect to what? The answer is simple: with respect to the &lt;strong&gt;sample size&lt;/strong&gt;, or more accurately, with respect to the inverse of the square root of the sample size, $1 / \sqrt{n}$. The reason is deeply rooted in the &lt;a href=&#34;https://en.wikipedia.org/wiki/Central_limit_theorem&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Central Limit Theorem&lt;/a&gt;, but I won&amp;rsquo;t cover it here.&lt;/p&gt;
&lt;p&gt;The idea is easier to show than to explain, so let&amp;rsquo;s repeat the same simulation as above, but now we will increase both the coefficient and the sample size at the same time.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;betas = 0.3 * 30 / np.sqrt(Ns)
alphas = {f&#39;N = {n:.0f}&#39;:  pre_testing(b=b, N=n)[&#39;Pre-test&#39;] for n,b in zip(Ns,betas)}
plot_alphas(alphas, true_alpha=1)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/regularization_bias_24_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;As we can see, now that $\beta$ is proportional to $1 / \sqrt{n}$, the distortion is not going away, not matter the sample size. Therefore, inference will always be wrong.&lt;/p&gt;
&lt;p&gt;While a coefficient that depends on the sample size might sound &lt;strong&gt;not intuitive&lt;/strong&gt;, it captures well the idea of &lt;strong&gt;magnitude&lt;/strong&gt; in a world where we do inference relying on asymptotic results, first among all the &lt;a href=&#34;https://en.wikipedia.org/wiki/Central_limit_theorem&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Central Limit Theorem&lt;/a&gt;. In fact, the Central Limit Theorem relieas on an infinitely large sample size. However, with an infinite amount of data, no coefficient is small and any non-zero effect is detected with certainty.&lt;/p&gt;
&lt;h2 id=&#34;pre-testing-and-machine-learning&#34;&gt;Pre-Testing and Machine Learning&lt;/h2&gt;
&lt;p&gt;So far we talked about a linear regression with only 2 variables. Where is the &lt;strong&gt;machine learning&lt;/strong&gt; we were promised?&lt;/p&gt;
&lt;p&gt;Usually we do not have just one control variable (or confounder), but many. Moreover, we might want to be flexible with respect to the functional form through which these control variables enter the model. In general, we will assume the following model:&lt;/p&gt;
&lt;p&gt;$$
Y = \alpha D + g_0(X) + u
\newline
D = m_0(X) + v
$$&lt;/p&gt;
&lt;p&gt;Where the effect of interest is still $\alpha$, $X$ is potentially high dimensional and we do not take a stand on the functional form through which $X$ influences $D$ or $Y$.&lt;/p&gt;
&lt;p&gt;In this setting, it is natural to use a machine learning algorithm to estimate $g_0$ and $m_0$. However, machine learning algorithms usually introduce a &lt;strong&gt;regularization bias&lt;/strong&gt; that is comparable to pre-testing.&lt;/p&gt;
&lt;p&gt;Possibly, the &amp;ldquo;simplest&amp;rdquo; way to think about it is &lt;a href=&#34;https://en.wikipedia.org/wiki/Lasso_%28statistics%29&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Lasso&lt;/a&gt;. Lasso is linear in $X$, with a penalization term that effectively just performs the variable selection we discussed above. Therefore, if we were to use Lasso of $X$ and $D$ on $Y$ we would be introducing regularization bias and inference would be distorted. The same goes for more complex algorithms.&lt;/p&gt;
&lt;p&gt;Lastly, you might still wonder &amp;ldquo;why is the model linear in the treatment variable $D$?&amp;rdquo;. Doing inference is much easier in linear model, not only for computational reasons but also for interpretation. Moreover, if the treatment $D$ is binary, the linear functional form is without loss of generality. A stronger assumption is the additive separability of $D$ and $g(X)$.&lt;/p&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;In this post, I have tried to explain how does regularization bias emerges and why it can the an issue in causal inference. This problem is inherently related to settings with many control variables or where we would like to have a model-free (i.e. non-parametric) when controlling for confounders. These are exactly the settings in which machine learning algorithms can be useful.&lt;/p&gt;
&lt;p&gt;In the next post, I will cover a simple and yet incredibly powerful solution to this problem: double-debiased machine learning.&lt;/p&gt;
&lt;h3 id=&#34;references&#34;&gt;References&lt;/h3&gt;
&lt;p&gt;[1] A. Belloni, D. Chen, V. Chernozhukov, C. Hansen, &lt;a href=&#34;https://onlinelibrary.wiley.com/doi/abs/10.3982/ECTA9626&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Sparse Models and Methods for Optimal Instruments With an Application to Eminent Domain&lt;/a&gt; (2012), &lt;em&gt;Econometrica&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;[2] A. Belloni, V. Chernozhukov, C. Hansen, &lt;a href=&#34;https://academic.oup.com/restud/article-abstract/81/2/608/1523757&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Inference on treatment effects after selection among high-dimensional controls&lt;/a&gt; (2014), &lt;em&gt;The Review of Economic Studies&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;[3] V. Chernozhukov, D. Chetverikov, M. Demirer, E. Duflo, C. Hansen, W. Newey, J. Robins, &lt;a href=&#34;https://academic.oup.com/ectj/article/21/1/C1/5056401&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Double/debiased machine learning for treatment and structural parameters&lt;/a&gt; (2018), &lt;em&gt;The Econometrics Journal&lt;/em&gt;.&lt;/p&gt;
&lt;h3 id=&#34;related-articles&#34;&gt;Related Articles&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/344ac1477699&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Understanding Omitted Variable Bias&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/59f801eb3299&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Understanding The Frisch-Waugh-Lovell Theorem&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/b63dc69e3d8c&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;DAGs and Control Variables&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;code&#34;&gt;Code&lt;/h3&gt;
&lt;p&gt;You can find the original Jupyter Notebook here:&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/pretest.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/matteocourthoud/Blog-Posts/blob/main/pretest.ipynb&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Double Debiased Machine Learning (part 2)</title>
      <link>https://matteocourthoud.github.io/post/double_ml/</link>
      <pubDate>Mon, 10 Jul 2023 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/post/double_ml/</guid>
      <description>&lt;p&gt;In the &lt;a href=&#34;https://towardsdatascience.com/eb767a59975b&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;previous part of this blog post&lt;/a&gt;, we have seen how pre-testing can distort inference, i.e., how selecting control variables depending on their statistical significance results in wrong confidence intervals for the variable of interest. This bias is generally called &lt;strong&gt;regularization bias&lt;/strong&gt; and also emerges in machine learning algorithms.&lt;/p&gt;
&lt;p&gt;In blog post, we are going to explore a solution to the simple selection example, &lt;strong&gt;post-double selection&lt;/strong&gt;, and a more general approach when we have many control variables and we do not want to assume linearity, &lt;strong&gt;double-debiased machine learning&lt;/strong&gt;.&lt;/p&gt;
&lt;h2 id=&#34;recap&#34;&gt;Recap&lt;/h2&gt;
&lt;p&gt;To better understand the source of the bias, in the first part of this post, we have explored the example of a firm that is interested in testing the effectiveness of an a campaign. The firm has information on its current ad spending and on the level of sales. The problem arises because the firm is uncertain on whether it should condition its analysis on the level of past sales.&lt;/p&gt;
&lt;p&gt;The following &lt;a href=&#34;https://towardsdatascience.com/b63dc69e3d8c&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Directed Acyclic Graph (DAG)&lt;/strong&gt;&lt;/a&gt; summarizes the data generating process.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-mermaid&#34;&gt;flowchart LR
classDef included fill:#DCDCDC,stroke:#000000,stroke-width:2px;
classDef excluded fill:#ffffff,stroke:#000000,stroke-width:2px;
classDef unobserved fill:#ffffff,stroke:#000000,stroke-width:2px,stroke-dasharray: 5 5;

D((ad spend))
Z((past sales))
Y((sales))

D --&amp;gt; Y
Z -- ??? --&amp;gt; Y
Z --&amp;gt; D

class D,Y included;
class Z excluded;

linkStyle 0 stroke:#00ff00,stroke-width:4px;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I import the data generating process &lt;code&gt;dgp_tbd()&lt;/code&gt; from &lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/src/dgp.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;src.dgp&lt;/code&gt;&lt;/a&gt; and some plotting functions and libraries from &lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/src/utils.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;src.utils&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;%matplotlib inline
%config InlineBackend.figure_format = &#39;retina&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from src.utils import *
from src.dgp import dgp_pretest

df = dgp_pretest().generate_data()
df.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;ads&lt;/th&gt;
      &lt;th&gt;sales&lt;/th&gt;
      &lt;th&gt;past_sales&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;16.719800&lt;/td&gt;
      &lt;td&gt;19.196620&lt;/td&gt;
      &lt;td&gt;6.624345&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;7.732222&lt;/td&gt;
      &lt;td&gt;9.287491&lt;/td&gt;
      &lt;td&gt;4.388244&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;10.923469&lt;/td&gt;
      &lt;td&gt;11.816906&lt;/td&gt;
      &lt;td&gt;4.471828&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;8.457062&lt;/td&gt;
      &lt;td&gt;9.024376&lt;/td&gt;
      &lt;td&gt;3.927031&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;13.085146&lt;/td&gt;
      &lt;td&gt;12.814823&lt;/td&gt;
      &lt;td&gt;5.865408&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;We have data on $1000$ different markets, in which we observe current &lt;code&gt;sales&lt;/code&gt;, the amount spent in &lt;code&gt;advertisement&lt;/code&gt; and &lt;code&gt;past sales&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;We want to understand &lt;code&gt;ads&lt;/code&gt; spending is effective in increasing &lt;code&gt;sales&lt;/code&gt;. One possibility is to regress the latter on the former, using the following regression model, also called the &lt;strong&gt;short model&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;$$
\text{sales} = \alpha \cdot \text{ads} + \varepsilon
$$&lt;/p&gt;
&lt;p&gt;Should we also include &lt;code&gt;past sales&lt;/code&gt; in the regression? Then the regression model would be the following, also called &lt;strong&gt;long model&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;$$
\text{sales} = \alpha \cdot \text{ads} + \beta \cdot \text{past sales} + \varepsilon
$$&lt;/p&gt;
&lt;p&gt;One naive approach would be to &lt;strong&gt;let the data decide&lt;/strong&gt;: we could run the second regression and, if the effect of &lt;code&gt;past sales&lt;/code&gt;, $\beta$, is statistically significant, we are good with the long model, otherwise we run the short model. This procedure is called &lt;strong&gt;pre-testing&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;The problem with this procedure is that it introduces a bias that is called &lt;strong&gt;regularization or pre-test bias&lt;/strong&gt;. Pre-testing ensures that this bias is small enough not to distort the estimated coefficient. However, it does not ensure that it is small enough not to distort the confidence intervals around the estimated coefficient.&lt;/p&gt;
&lt;p&gt;Is there a solution? Yes!&lt;/p&gt;
&lt;h2 id=&#34;post-double-selection&#34;&gt;Post-Double Selection&lt;/h2&gt;
&lt;p&gt;The solution is called &lt;strong&gt;post-double selection&lt;/strong&gt;. The method was first introduced in &lt;a href=&#34;https://academic.oup.com/restud/article-abstract/81/2/608/1523757&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Belloni, Chernozhukov, Hansen (2014)&lt;/a&gt; and later expanded in a variety of papers.&lt;/p&gt;
&lt;p&gt;The authors assume the following &lt;strong&gt;data generating process&lt;/strong&gt;:&lt;/p&gt;
&lt;p&gt;$$
y = \alpha D + \beta X + u
\newline
D = \delta X + v
$$&lt;/p&gt;
&lt;p&gt;In our example, $Y$ corresponds to &lt;code&gt;sales&lt;/code&gt;, $D$ corresponds to &lt;code&gt;ads&lt;/code&gt;, $X$ corresponds to &lt;code&gt;past_sales&lt;/code&gt; and the effect of interest is $\alpha$. In our example, $X$ is 1-dimensional for simplicity, but generally we are interested in cases where X is high-dimensional, potentially even having more dimensions than the number of observations. In that case, variable selection is &lt;strong&gt;essential&lt;/strong&gt; in linear regression since we cannot have more features than variables (the OLS coefficients are not uniquely determined anymore).&lt;/p&gt;
&lt;p&gt;Post-double selection consists in the following procedure.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Reduced Form&lt;/strong&gt; selection: lasso $Y$ on $X$. Select the statistically significant variables in the set $S_{RF} \subseteq X$&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;First Stage&lt;/strong&gt; selection: regress $D$ on $X$. Select the statistically significant variables in the set $S_{FS} \subseteq X$&lt;/li&gt;
&lt;li&gt;Regress $Y$ on $D$ and the &lt;strong&gt;union&lt;/strong&gt; of the selected variables in the first two steps, $S_{FS} \cup S_{RF}$&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The authors show that this procedure produces confidence intervals for the coefficient of interest $\alpha$ that have the correct coverage, i.e. the correct probability of type 1 error.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Note (1)&lt;/strong&gt;: this procedure is always less parsimonious, in terms of variable selection, than pre-testing. In fact, we still select all the variables we would have selected with pre-testing but, in the first stage, we might select additional variables.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Note (2)&lt;/strong&gt;: the terms &lt;em&gt;first stage&lt;/em&gt; and &lt;em&gt;reduced form&lt;/em&gt; come from the &lt;a href=&#34;https://en.wikipedia.org/wiki/Instrumental_variables_estimation&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;intrumental variables&lt;/a&gt; literature in econometrics. Indeed, the first application of post-double selection was to select instrumental variables in &lt;a href=&#34;https://onlinelibrary.wiley.com/doi/abs/10.3982/ECTA9626&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Belloni, Chen, Chernozhukov, Hansen (2012)&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Note (3)&lt;/strong&gt;: the name post-double selection comes from the fact that now we are not performing variable selection once but &lt;em&gt;twice&lt;/em&gt;.&lt;/p&gt;
&lt;h3 id=&#34;intuition&#34;&gt;Intuition&lt;/h3&gt;
&lt;p&gt;The idea behind post-double selection is: bound the &lt;a href=&#34;https://towardsdatascience.com/344ac1477699&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;omitted variables bias&lt;/a&gt;. In case you are not familiar with it, I wrote a separate &lt;a href=&#34;https://towardsdatascience.com/344ac1477699&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;blog post on omitted variable bias&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;In our setting, we can express the omitted variable bias as&lt;/p&gt;
&lt;p&gt;$$
\text{OVB} = \beta \delta \qquad \text{ where } \qquad \beta := \frac{Cov(X, Y)}{Var(X)}, \quad \delta := \frac{Cov(D, X)}{Var(D)}
$$&lt;/p&gt;
&lt;p&gt;As we can see, the omitted variable bias comes from the product of two quantities related to the omitted variable $X$:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Its partial correlation with the outcome $Y$, $\beta$&lt;/li&gt;
&lt;li&gt;Its partial correlation with the variable of interest $D$, $\delta$&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;With pre-testing, we ensure that the partial correlation between $X$ the outcome $Y$, $\beta$, is &lt;strong&gt;small&lt;/strong&gt;. In fact, we omit $Z$ when we shouldn&amp;rsquo;t (i.e. we commit a type 2 error) rarely. What do &lt;em&gt;small&lt;/em&gt; and &lt;em&gt;rarely&lt;/em&gt; mean?&lt;/p&gt;
&lt;p&gt;When we are selecting a variable because of its significance, we ensure that it dimension is smaller than $\frac{c}{\sqrt{n}}$ for some number $c$, where $n$ is the sample size.&lt;/p&gt;
&lt;p&gt;Therefore, with pre-testing, we ensure that, no matter what the value of $\delta$ is, the dimension of the bias is smaller than $\frac{c}{\sqrt{n}}$ which means that it converges to zero for sufficiently large $n$. This is why the pre-testing estimator is still &lt;strong&gt;consistent&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;However, in order for our confidence intervals to have the right coverage, this is &lt;strong&gt;not enough&lt;/strong&gt;. In practice, we need the bias to converge to zero &lt;strong&gt;faster&lt;/strong&gt; than $\frac{1}{\sqrt{n}}$. Why?&lt;/p&gt;
&lt;p&gt;To get an &lt;strong&gt;intuition&lt;/strong&gt; for this result, we need to turn to the &lt;a href=&#34;https://en.wikipedia.org/wiki/Central_limit_theorem&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Central Limit Theorem&lt;/a&gt;. The CLT tells us that for large $n$ the distribution of the sample average of a random variable $X$ converges to a normal distribution with mean $\mu$ and standard deviation $\frac{\sigma}{\sqrt{n}}$, where $\mu$ and $\sigma$ are the mean and standard deviation of $X$. To do inference, we usually apply the Central Limit Theorem to our estimator to get its asymptotic distribution, which in turn allows us to build confidence intervals (using the mean and the standard deviation). Therefore, if the bias is not sensibly smaller than the standard deviation of the estimator, the confidence intervals are going to be wrong. Therefore, we need the bias to converge to zero &lt;strong&gt;faster&lt;/strong&gt; than the standard deviation, i.e. faster than $\frac{1}{\sqrt{n}}$.&lt;/p&gt;
&lt;p&gt;In our setting, the omitted variable bias is $\beta \gamma$ and we want it to converge to zero faster than $\frac{1}{\sqrt{n}}$.  Post-double selection guarantees that&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Reduced form&lt;/em&gt; selection (pre-testing): any &amp;ldquo;missing&amp;rdquo; variable $j$ has $|\beta_j| \leq \frac{c}{\sqrt{n}}$&lt;/li&gt;
&lt;li&gt;&lt;em&gt;First stage&lt;/em&gt; selection (additional): any &amp;ldquo;missing&amp;rdquo; variable $j$ has $|\delta_j| \leq \frac{c}{\sqrt{n}}$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;As a consequence, as long as the number of omitted variables is finite, the omitted variable bias is going to converge to zero at a rate $\frac{1}{n}$, which is faster than $\frac{1}{\sqrt{n}}$. &lt;strong&gt;Problem solved&lt;/strong&gt;!&lt;/p&gt;
&lt;h3 id=&#34;application&#34;&gt;Application&lt;/h3&gt;
&lt;p&gt;Let&amp;rsquo;s now go back to our example and test the post-double selection procedure. In practice, we want to do the following:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;First Stage&lt;/strong&gt; selection: regress &lt;code&gt;ads&lt;/code&gt; on &lt;code&gt;past_sales&lt;/code&gt;. Check if &lt;code&gt;past_sales&lt;/code&gt; is statistically significant&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Reduced Form&lt;/strong&gt; selection: regress &lt;code&gt;sales&lt;/code&gt; on &lt;code&gt;past_sales&lt;/code&gt;. Check if &lt;code&gt;past_sales&lt;/code&gt; is statistically significant&lt;/li&gt;
&lt;li&gt;Regress &lt;code&gt;sales&lt;/code&gt; on &lt;code&gt;ads&lt;/code&gt; and include &lt;code&gt;past_sales&lt;/code&gt; &lt;strong&gt;only if&lt;/strong&gt; it was significant in &lt;em&gt;either&lt;/em&gt; one of the two previous regressions&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;I update the &lt;code&gt;pre_test&lt;/code&gt; function from the first part of the post to compute also the post-double selection estimator.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def pre_test(d=&#39;ads&#39;, y=&#39;sales&#39;, x=&#39;past_sales&#39;, K=1000, **kwargs):
    
    # Init
    alphas = pd.DataFrame({&#39;Long&#39;: np.zeros(K), 
             &#39;Short&#39;: np.zeros(K), 
             &#39;Pre-test&#39;: np.zeros(K),
             &#39;Post-double&#39;: np.zeros(K)})

    # Loop over simulations
    for k in range(K):
        
        # Generate data
        df = dgp_pretest().generate_data(seed=k, **kwargs)
        
        # Compute coefficients
        alphas[&#39;Long&#39;][k] = smf.ols(f&#39;{y} ~ {d} + {x}&#39;, df).fit().params[1]
        alphas[&#39;Short&#39;][k] = smf.ols(f&#39;{y} ~ {d}&#39;, df).fit().params[1]
    
        # Compute significance of beta and gamma
        p_value_ydx = smf.ols(f&#39;{y} ~ {d} + {x}&#39;, df).fit().pvalues[2]
        p_value_yx = smf.ols(f&#39;{y} ~ {x}&#39;, df).fit().pvalues[1]
        p_value_dx = smf.ols(f&#39;{d} ~ {x}&#39;, df).fit().pvalues[1]
        
        # Select pre-test specification based on regression of y on d and x
        if p_value_ydx&amp;lt;0.05:
            alphas[&#39;Pre-test&#39;][k] = alphas[&#39;Long&#39;][k]
        else:
            alphas[&#39;Pre-test&#39;][k] = alphas[&#39;Short&#39;][k]
            
        # Select post-double specification based on regression of y on d and x
        if p_value_yx&amp;lt;0.05 or p_value_dx&amp;lt;0.05:
            alphas[&#39;Post-double&#39;][k] = alphas[&#39;Long&#39;][k]
        else:
            alphas[&#39;Post-double&#39;][k] = alphas[&#39;Short&#39;][k]
    
    return alphas
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;alphas = pre_test()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can now plot the distributions (over simulations) of the estimated coefficients.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def plot_alphas(alphas, true_alpha):
    
    # Init plot
    K = len(alphas.columns)
    fig, axes = plt.subplots(1, K, figsize=(4*K, 5), sharey=True, sharex=True)

    # Make one plot for each set of coefficients
    for i, key in enumerate(alphas.columns):
        axes[i].hist(alphas[key].values, bins=30, lw=.1, color=f&#39;C{int(i==3)*2}&#39;)
        axes[i].set_title(key)
        axes[i].axvline(true_alpha, c=&#39;r&#39;, ls=&#39;--&#39;)
        legend_text = [rf&#39;$\alpha=${true_alpha}&#39;, rf&#39;$\hat \alpha=${np.mean(alphas[key]):.4f}&#39;]
        axes[i].legend(legend_text, prop={&#39;size&#39;: 10}, loc=&#39;upper right&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plot_alphas(alphas, true_alpha=1)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/double_ml_17_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;As we can see, the post-double selection estimator always correctly selects the long regression and therefore has the correct distribution.&lt;/p&gt;
&lt;h3 id=&#34;double-checks&#34;&gt;Double-checks&lt;/h3&gt;
&lt;p&gt;In the last post, we ran some simulations in order to investigate when pre-testing bias emerges. We saw that pre-testing is a problem for&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Small sample sizes $n$&lt;/li&gt;
&lt;li&gt;Intermediate values of $\beta$&lt;/li&gt;
&lt;li&gt;When the value of $\beta$ depends on the sample size&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Let&amp;rsquo;s check that post-double selection removes regularization bias in &lt;strong&gt;all&lt;/strong&gt; the previous cases.&lt;/p&gt;
&lt;p&gt;First, let&amp;rsquo;s simulate the distribution of the post-double selection estimator $\hat \alpha_{postdouble}$ for different sample sizes.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;Ns = [100,300,1000,3000]
alphas = {f&#39;N = {n:.0f}&#39;:  pre_test(N=n) for n in Ns}
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def compare_alphas(alphas, true_alpha):
    
    # Init plot
    fig, axes = plt.subplots(1, len(alphas), figsize=(4*len(alphas), 5), sharey=True, sharex=True)

    # Make one plot for each set of coefficients
    for i, key in enumerate(alphas.keys()):
        axes[i].hist(alphas[key][&#39;Pre-test&#39;], bins=30, lw=.1, alpha=0.5)
        axes[i].hist(alphas[key][&#39;Post-double&#39;], bins=30, lw=.1, alpha=0.5, color=&#39;C2&#39;)
        axes[i].set_title(key)
        axes[i].axvline(true_alpha, c=&#39;r&#39;, ls=&#39;--&#39;)
        axes[i].legend([rf&#39;$\alpha=${true_alpha}&#39;, &#39;Pre-test&#39;, &#39;Post-double&#39;], 
                       prop={&#39;size&#39;: 10}, loc=&#39;upper right&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;compare_alphas(alphas, true_alpha=1)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/double_ml_23_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;For small samples, the distribution of the pre-testing estimator is not normal but rather bimodal. From the plots we can see that the post-double estimator is gaussian also in small sample sizes.&lt;/p&gt;
&lt;p&gt;Now we repeat the same exercise, but for different values of $\beta$, the coefficient of &lt;code&gt;past_sales&lt;/code&gt; on &lt;code&gt;sales&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;betas = 0.3 * np.array([0.1,0.3,1,3])
alphas = {f&#39;beta = {b:.2f}&#39;: pre_test(b=b) for b in betas}
compare_alphas(alphas, true_alpha=1)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/double_ml_25_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Again, the post-double selection estimator has a gaussian distribution irrespectively of the value of $\beta$, while he pre-testing estimator suffers from regularization bias.&lt;/p&gt;
&lt;p&gt;For the last simulation, we change both the coefficient and the sample size at the same time.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;betas = 0.3 * 30 / np.sqrt(Ns)
alphas = {f&#39;N = {n:.0f}&#39;:  pre_test(b=b, N=n) for n,b in zip(Ns,betas)}
compare_alphas(alphas, true_alpha=1)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/double_ml_27_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Also in this last case, the post-double selection estimator performs well and inference is not distorted.&lt;/p&gt;
&lt;h2 id=&#34;double-debiased-machine-learning&#34;&gt;Double Debiased Machine Learning&lt;/h2&gt;
&lt;p&gt;So far, we only have analyzed a linear, univariate example. What happens if the dimension of $X$ increases and we do not know the functional form through which $X$ affects $Y$ and $D$? In these cases, we can use machine learning algorithms to uncover these high-dimensional non-linear relationships.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://academic.oup.com/ectj/article/21/1/C1/5056401&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Chernozhukov, Chetverikov, Demirer, Duflo, Hansen, Newey, and Robins (2018)&lt;/a&gt; investigate this setting. In particular, the authors consider the following partially linear model.&lt;/p&gt;
&lt;p&gt;$$
Y = \alpha D + g(X) + u \
D = m(X) + v
$$&lt;/p&gt;
&lt;p&gt;where $Y$ is the outcome variable, $D$ is the treatment to interest and $X$ is a potentially high-dimensional set of control variables.&lt;/p&gt;
&lt;h3 id=&#34;naive-approach&#34;&gt;Naive approach&lt;/h3&gt;
&lt;p&gt;A naive approach to estimation of $\alpha$ using machine learning methods would be, for example, to construct a sophisticated machine learning estimator for learning the regression function $\alpha D$ + $g(X)$.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Split the sample in two: main sample and auxiliary sample [why? see note below]&lt;/li&gt;
&lt;li&gt;Use the auxiliary sample to estimate $\hat g(X)$&lt;/li&gt;
&lt;li&gt;Use the main sample to compute the orthogonalized component of $Y$ on $X$: $\ \hat u = Y - \hat{g} (X)$&lt;/li&gt;
&lt;li&gt;Use the main sample to estimate the residualized OLS estimator from regressing $\hat u$ on $D$&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;$$
\hat \alpha = \left( D&amp;rsquo; D \right) ^{-1} D&amp;rsquo; \hat u
$$&lt;/p&gt;
&lt;p&gt;This estimator is going to have &lt;strong&gt;two problems&lt;/strong&gt;:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Slow rate of convergence, i.e. slower than $\sqrt(n)$&lt;/li&gt;
&lt;li&gt;It will be biased because we are employing high dimensional regularized estimators (e.g. we are doing variable selection)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;Note (1)&lt;/strong&gt;: so far we have not talked about it, but variable selection procedure also introduce another type of bias: &lt;strong&gt;overfitting bias&lt;/strong&gt;. This bias emerges because of the fact that the sample used to select the variables is the same that is used to estimate the coefficient of interest. This bias is &lt;strong&gt;easily accounted for&lt;/strong&gt; with sample splitting: using different sub-samples for the selection and the estimation procedures.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Note (2)&lt;/strong&gt;: why can we use the residuals from step 3 to estimate $\alpha$ in step 4? Because of the &lt;a href=&#34;https://towardsdatascience.com/59f801eb3299&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Frisch-Waugh-Lovell theorem&lt;/a&gt;. If you are not familiar with it, I have written a &lt;a href=&#34;https://towardsdatascience.com/59f801eb3299&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;blog post on the Frisch-Waugh-Lovell theorem here&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;orthogonalization&#34;&gt;Orthogonalization&lt;/h3&gt;
&lt;p&gt;Now consider a second construction that employs an orthogonalized formulation obtained by directly partialling out the effect of $X$ from $D$ to obtain the orthogonalized regressor $v = D − m(X)$.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Split the sample in two: main sample and auxiliary sample&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Use the auxiliary sample to estimate $\hat g(X)$ from&lt;/p&gt;
&lt;p&gt;$$
Y = \alpha D + g(X) + u \
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Use the auxiliary sample to estimate $\hat m(X)$ from&lt;/p&gt;
&lt;p&gt;$$
D = m(X) + v
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Use the main sample to compute the orthogonalized component of $D$ on $X$ as&lt;/p&gt;
&lt;p&gt;$$
\hat v = D - \hat m(X)
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Use the main sample to estimate the double-residualized OLS estimator as&lt;/p&gt;
&lt;p&gt;$$
\hat \alpha = \left( \hat{v}&amp;rsquo; D \right) ^{-1} \hat{v}&amp;rsquo; \left( Y - \hat g(X) \right)
$$&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The estimator is &lt;strong&gt;root-N consistent&lt;/strong&gt;! This means that not only the estimator converges to the true value as the sample sizes increases (i.e. it&amp;rsquo;s consistent), but also its standard deviation does (i.e. it&amp;rsquo;s root-N consistent).&lt;/p&gt;
&lt;p&gt;However, the estimator still has a lower rate of convergence because of sample splitting. The problem is solved by inverting the split sample, re-estimating the coefficient and averaging the two estimates. Note that this procedure is valid since the two estimates are independent by the sample splitting procedure.&lt;/p&gt;
&lt;h3 id=&#34;a-cautionary-tale&#34;&gt;A Cautionary Tale&lt;/h3&gt;
&lt;p&gt;Before we conclude, I have to mention a recent research paper by &lt;a href=&#34;https://arxiv.org/abs/2108.11294&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Hünermund, Louw, and Caspi (2022)&lt;/a&gt;, in which the authors show that double-debiased machine learning can easily &lt;strong&gt;backfire&lt;/strong&gt;, if we apply blindly.&lt;/p&gt;
&lt;p&gt;The problem is related to &lt;strong&gt;bad control variables&lt;/strong&gt;. If you have never heard this term, I have written an introductory &lt;a href=&#34;https://towardsdatascience.com/b63dc69e3d8c&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;blog post on good and bad control variables here&lt;/a&gt;. In short, conditioning the analysis on additional features is not always good for causal inference. Depending on the setting, there might exist variables that we want to leave out of our analysis since their &lt;strong&gt;inclusion&lt;/strong&gt; can bias the coefficient of interest, preventing a causal interpretation. The simplest example is variables that are common outcomes, of both the treatment $D$ and outcome variable $Y$.&lt;/p&gt;
&lt;p&gt;The double-debiased machine learning model implicitly assumes that the control variables $X$ are (weakly) &lt;strong&gt;common causes&lt;/strong&gt; to both the outcome $Y$ and the treatment $D$. If this is the case, and no further mediated/indirect relationship exists between $X$ and $Y$, there is no problem. However, if, for example, some variable among the controls $X$ is a common effect instead of a common cause, its inclusion will bias the coefficient of interest. Moreover, this variable is likely to be highly correlated either with the outcome $Y$ or with the treatment $D$. In the latter case, this implies that post-double selection might include it in cases in which simple selection would have not. Therefore, in presence of bad control variables, doule-debiased machine learning might be &lt;strong&gt;even worse&lt;/strong&gt; than simple pre-testing.&lt;/p&gt;
&lt;p&gt;In short, as for any method, it is &lt;strong&gt;crucial&lt;/strong&gt; to have a clear understanding of the method&amp;rsquo;s assumptions and to always check for potential violations.&lt;/p&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;In this post, we have seen how to use post-double selection and, more generally, double debiased machine learning to get rid of an important source of bias: regularization bias.&lt;/p&gt;
&lt;p&gt;This contribution by Victor Chernozhukov and co-authors has been undoubtedly one of the most relevant advances in causal inferences in the last decade. It is now widely employed in the industry and included in the most used causal inference packages, such as &lt;a href=&#34;https://econml.azurewebsites.net/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;EconML&lt;/a&gt; (Microsoft) and &lt;a href=&#34;https://causalml.readthedocs.io/en/latest/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;causalml&lt;/a&gt; (Uber).&lt;/p&gt;
&lt;p&gt;If you (understandably) feel the need for more material on double-debiased machine learning, but you do not feel like reading academic papers (also very understandable), here is a good compromise.&lt;/p&gt;
&lt;br&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/eHOjmyoPCFU&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;br&gt;
&lt;p&gt;In this video lecture, Victor Chernozhukov himself presents the idea. The video lecture is relatively heavy on math and statistics, but you cannot get a more qualified and direct source than this!&lt;/p&gt;
&lt;h3 id=&#34;references&#34;&gt;References&lt;/h3&gt;
&lt;p&gt;[1] A. Belloni, D. Chen, V. Chernozhukov, C. Hansen, &lt;a href=&#34;https://onlinelibrary.wiley.com/doi/abs/10.3982/ECTA9626&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Sparse Models and Methods for Optimal Instruments With an Application to Eminent Domain&lt;/a&gt; (2012), &lt;em&gt;Econometrica&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;[2] A. Belloni, V. Chernozhukov, C. Hansen, &lt;a href=&#34;https://academic.oup.com/restud/article-abstract/81/2/608/1523757&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Inference on treatment effects after selection among high-dimensional controls&lt;/a&gt; (2014), &lt;em&gt;The Review of Economic Studies&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;[3] V. Chernozhukov, D. Chetverikov, M. Demirer, E. Duflo, C. Hansen, W. Newey, J. Robins, &lt;a href=&#34;https://academic.oup.com/ectj/article/21/1/C1/5056401&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Double/debiased machine learning for treatment and structural parameters&lt;/a&gt; (2018), &lt;em&gt;The Econometrics Journal&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;[4] P. Hünermund, B. Louw, I. Caspi, &lt;a href=&#34;https://arxiv.org/abs/2108.11294&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Double Machine Learning and Automated Confounder Selection - A Cautionary Tale&lt;/a&gt; (2022), &lt;em&gt;working paper&lt;/em&gt;.&lt;/p&gt;
&lt;h3 id=&#34;related-articles&#34;&gt;Related Articles&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/eb767a59975b&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Double Debiased Machine Learning (part 1)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/344ac1477699&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Understanding Omitted Variable Bias&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/59f801eb3299&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Understanding The Frisch-Waugh-Lovell Theorem&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/b63dc69e3d8c&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;DAGs and Control Variables&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;code&#34;&gt;Code&lt;/h3&gt;
&lt;p&gt;You can find the original Jupyter Notebook here:&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/pds.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/matteocourthoud/Blog-Posts/blob/main/pds.ipynb&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Experiments on Returns on Investment</title>
      <link>https://matteocourthoud.github.io/post/delta_method/</link>
      <pubDate>Mon, 10 Jul 2023 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/post/delta_method/</guid>
      <description>&lt;p&gt;&lt;em&gt;An introduction to the delta method for inference on ratio metrics.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;When we run an experiment, we are often not only interested in the effect of a treatment (new product, new feature, new interface, &amp;hellip;) on revenue, but in it&amp;rsquo;s &lt;strong&gt;cost-effectiveness&lt;/strong&gt;. In other words, is the investment worth the cost? Common examples include investments in computing resources, returns on advertisement, but also click-through rates and other ratio metrics.&lt;/p&gt;
&lt;p&gt;When we investigate causal effects, the gold standard is randomized control trials, a.k.a. &lt;strong&gt;AB tests&lt;/strong&gt;. Randomly assigning the treatment to a subset of the population (users, patients, customers, &amp;hellip;) we ensure that, on average, the difference in outcomes can be attributed to the treatment. However, when the object of interest is cost-effectiveness, AB tests present some additional problems since we are not just interested in one treatment effect, but in the &lt;strong&gt;ratio of two treatment effects&lt;/strong&gt;, the outcome of the investment over its cost.&lt;/p&gt;
&lt;p&gt;In this post we are going to see how to analyze randomized experiments when the object of interest is the &lt;strong&gt;return on investment (ROI)&lt;/strong&gt;. We are going to explore alternative metrics to measure whether an investment paid off. We will also introduce a very powerful tool for inference with complex metrics: the &lt;strong&gt;delta method&lt;/strong&gt;. While the algebra can be intense, the result is simple: we can compute the confidence interval for our ratio estimator using a simple linear regression.&lt;/p&gt;
&lt;h2 id=&#34;investing-in-cloud-computing&#34;&gt;Investing in Cloud Computing&lt;/h2&gt;
&lt;p&gt;To better illustrate the concepts, we are going to use a toy example throughout the article: suppose we were an &lt;strong&gt;online marketplace&lt;/strong&gt; and we wanted to &lt;strong&gt;invest in cloud computing&lt;/strong&gt;: we want to increase the computing power behind our internal search engine, by switching to a higher tier server. The idea is that the faster search will improve the user experience, potentially leading to higher sales. Therefore, the question is: is the investment worth the cost? The object of interest is the &lt;strong&gt;return on investment (ROI)&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Differently from usual AB tests or randomized experiments, we are not interested in a single causal effect, but in the &lt;strong&gt;ratio&lt;/strong&gt; of two metrics: the effect on revenue and the effect on cost. We will still use a &lt;strong&gt;randomized control trial&lt;/strong&gt; or &lt;strong&gt;AB test&lt;/strong&gt; to estimate the ROI: we randomly assign groups of users to either the treatment or the control group. The treated users will benefit from the faster cloud machines, while the control users will use the old slower machines. Randomization ensures that we can estimate the impact of the new machines on either cost or revenue by comparing users in the treatment and control group: the difference in their average is an unbiased estimator of the average treatment effect. However, things are more complicated for their ratio.&lt;/p&gt;
&lt;p&gt;I import the data generating process &lt;code&gt;dgp_cloud()&lt;/code&gt; from &lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/src/dgp.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;src.dgp&lt;/code&gt;&lt;/a&gt;. With respect to previous articles, I generated a new DGP parent class that handles randomization and data generation, while its children classes contain the specific use-cases. I also import some plotting functions and libraries from &lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/src/utils.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;src.utils&lt;/code&gt;&lt;/a&gt;.  To include not only code but also data and tables, I use &lt;a href=&#34;https://deepnote.com&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Deepnote&lt;/a&gt;, a Jupyter-like web-based collaborative notebook environment.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;%matplotlib inline
%config InlineBackend.figure_format = &#39;retina&#39;

from src.utils import *
from src.dgp import dgp_cloud, DGP
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;dgp = dgp_cloud(n=10_000)
df = dgp.generate_data(seed_assignment=6)
df.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;new_machine&lt;/th&gt;
      &lt;th&gt;cost&lt;/th&gt;
      &lt;th&gt;revenue&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;3.14&lt;/td&gt;
      &lt;td&gt;20.90&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;3.77&lt;/td&gt;
      &lt;td&gt;33.57&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;3.16&lt;/td&gt;
      &lt;td&gt;24.31&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;2.36&lt;/td&gt;
      &lt;td&gt;20.35&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1.65&lt;/td&gt;
      &lt;td&gt;12.60&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;The data contains information on the total &lt;code&gt;cost&lt;/code&gt; and &lt;code&gt;revenue&lt;/code&gt; for a set of $10.000$ users over a period of a month. We also have information on the treatment: whether the search engine was running on the old or &lt;code&gt;new machines&lt;/code&gt;.  As it often happens with business metrics, both distributions of cost and revenues are very &lt;strong&gt;skewed&lt;/strong&gt;. Moreover, most people do not buy anything and therefore generate zero revenue, even though they still use the platform, generating positive costs.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;fix, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))
sns.histplot(df.cost, ax=ax1, color=&#39;C0&#39;).set(title=&#39;Distribution of Cost&#39;)
sns.histplot(df.revenue, ax=ax2, color=&#39;C1&#39;).set(title=&#39;Distribution of Revenue&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/delta_method_8_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;We can compute the &lt;strong&gt;difference-in-means&lt;/strong&gt; estimate for &lt;code&gt;cost&lt;/code&gt; and &lt;code&gt;revenue&lt;/code&gt; by regressing the outcome on the treatment indicator.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;smf.ols(&#39;cost ~ new_machine&#39;, data=df).fit().summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
       &lt;td&gt;&lt;/td&gt;          &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt;   &lt;td&gt;    2.9617&lt;/td&gt; &lt;td&gt;    0.043&lt;/td&gt; &lt;td&gt;   69.034&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    2.878&lt;/td&gt; &lt;td&gt;    3.046&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;new_machine&lt;/th&gt; &lt;td&gt;    0.5152&lt;/td&gt; &lt;td&gt;    0.060&lt;/td&gt; &lt;td&gt;    8.563&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    0.397&lt;/td&gt; &lt;td&gt;    0.633&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;The average &lt;code&gt;cost&lt;/code&gt; has increased by $0.5152$$ per user. What about revenue?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;smf.ols(&#39;revenue ~ new_machine&#39;, data=df).fit().summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
       &lt;td&gt;&lt;/td&gt;          &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt;   &lt;td&gt;   25.9172&lt;/td&gt; &lt;td&gt;    0.425&lt;/td&gt; &lt;td&gt;   60.950&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;   25.084&lt;/td&gt; &lt;td&gt;   26.751&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;new_machine&lt;/th&gt; &lt;td&gt;    1.0664&lt;/td&gt; &lt;td&gt;    0.596&lt;/td&gt; &lt;td&gt;    1.788&lt;/td&gt; &lt;td&gt; 0.074&lt;/td&gt; &lt;td&gt;   -0.103&lt;/td&gt; &lt;td&gt;    2.235&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;The average &lt;code&gt;revenue&lt;/code&gt; per user has also increased, by $1.0664$$. So, was the investment &lt;strong&gt;profitable&lt;/strong&gt;?&lt;/p&gt;
&lt;p&gt;To answer this question, we first have to decide which metric to use as our &lt;strong&gt;outcome metric&lt;/strong&gt;. In case of ratio metrics, this is not trivial.&lt;/p&gt;
&lt;h2 id=&#34;average-return-or-return-of-the-average&#34;&gt;Average Return or Return of the Average?&lt;/h2&gt;
&lt;p&gt;It is very tempting to approach this problem saying: it is true that we have two variables, by we can just compute their ratio, and then analyze everything as usual, using a &lt;strong&gt;single variable&lt;/strong&gt;: the individual level return.&lt;/p&gt;
&lt;p&gt;$$
\rho_i = \frac{\text{individual revenue}}{\text{individual cost}} = \frac{R_i}{C_i}
$$&lt;/p&gt;
&lt;p&gt;What happens if we analyze the experiment using this single metric?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df[&amp;quot;rho&amp;quot;] = df[&amp;quot;revenue&amp;quot;] / df[&amp;quot;cost&amp;quot;]
smf.ols(&amp;quot;rho ~ new_machine&amp;quot;, df).fit().summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
       &lt;td&gt;&lt;/td&gt;          &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt;   &lt;td&gt;    6.6898&lt;/td&gt; &lt;td&gt;    0.044&lt;/td&gt; &lt;td&gt;  150.832&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    6.603&lt;/td&gt; &lt;td&gt;    6.777&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;new_machine&lt;/th&gt; &lt;td&gt;   -0.7392&lt;/td&gt; &lt;td&gt;    0.062&lt;/td&gt; &lt;td&gt;  -11.893&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;   -0.861&lt;/td&gt; &lt;td&gt;   -0.617&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;The estimated effect is &lt;strong&gt;negative and significant&lt;/strong&gt;, $-0.7392$! It seems like the the new machines were not a good investment, and the returns have decreased by $74%$.&lt;/p&gt;
&lt;p&gt;This result seems to contradict our previous estimates. We have seen before that the revenue has increased on average more than the cost ($0.9505$ vs $0.5076$). Why is it the case? The problem is that we are giving the same weight to heavy users and light users. Let&amp;rsquo;s use a simple example with two users. The first one (blue) is a light user and before was costing $1$ $ and returning $10$ $, while now is costing $4$ $ and returning $20$ $. The other user (violet) is a heavy user and before was costing $10$ $ and returning $100$ $ and now is costing $20$ $ and returning $220$ $.&lt;/p&gt;
&lt;img src=&#34;fig/return.png&#34; width=&#34;700px&#34;/&gt;
&lt;p&gt;The average return is -3x: on average the return per user has decreased by $300%$. However, the total return per user is $1000%$: the increase in cost of $13$$ has generated $130$$ in revenue! The results are wildly different and entirely driven by the weight of the two users: the effect of the heavy user is low in relative terms but high in absolute terms, while it&amp;rsquo;s the opposite for the light user. The average relative effect is therefore mostly driven by the light user, while the relative average effect is mostly driven by the heavy user.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Which metric&lt;/strong&gt; is more relevant in our setting? We talking about return on investment, we are usually interested in understanding whether we got a return on the money we spend. Therefore, the &lt;strong&gt;total return&lt;/strong&gt; is more interesting than the average return.&lt;/p&gt;
&lt;p&gt;From now on, the object of interest will be the &lt;strong&gt;return on investment (ROI)&lt;/strong&gt;, given by the expected increase in revenue over the expected increase in cost, and we will denote it with the greek letter rho, $\rho$.&lt;/p&gt;
&lt;p&gt;$$
\rho = \frac{\text{incremental revenue}}{\text{incremental cost}} = \frac{\mathbb E [\Delta R]}{\mathbb E [\Delta C]}
$$&lt;/p&gt;
&lt;p&gt;We can estimate the ROI as the ratio of the two previous estimates: the average difference in revenue between the treatment and control group, over the average difference in cost between the treatment and control group.&lt;/p&gt;
&lt;p&gt;$$
\hat{\rho} = \frac{\mathbb E_n [\Delta R]}{\mathbb E_n [\Delta C]}
$$&lt;/p&gt;
&lt;p&gt;Note a subtle but crucial difference with respect to the previous formula: we have replaced the &lt;a href=&#34;https://en.wikipedia.org/wiki/Expected_value&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;expected values&lt;/a&gt; $\mathbb E$ with the empirical expectation operators $\mathbb E_n$, also known as the &lt;a href=&#34;https://en.wikipedia.org/wiki/Arithmetic_mean&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;sample average&lt;/a&gt;. The difference in notation is minimal, but the conceptual difference is huge. The first, $\mathbb E$, is a &lt;strong&gt;theoretical&lt;/strong&gt; concept, while the second, $\mathbb E_n$, is &lt;strong&gt;empirical&lt;/strong&gt;: it is a number that depends on the actual data. I personally like the notation since it highlights the close link between the two concepts (the second is the empirical counterpart of the first), while also making it clear that the second crucially depends on the sample size $n$.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def estimate_roi(df):
    Delta_C = df.loc[df.new_machine==1, &amp;quot;cost&amp;quot;].mean() - df.loc[df.new_machine==0, &amp;quot;cost&amp;quot;].mean()
    Delta_R = df.loc[df.new_machine==1, &amp;quot;revenue&amp;quot;].mean() - df.loc[df.new_machine==0, &amp;quot;revenue&amp;quot;].mean()
    return Delta_R / Delta_C

estimate_roi(df)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;2.0698235970047887
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The estimate is $2.0698$: each additional dollar spent in the new machines translated in $2.0698$ extra dollars in revenue. Sounds great!&lt;/p&gt;
&lt;p&gt;But how much should we trust this number? Is it significantly different form one, or it is just driven by noise?&lt;/p&gt;
&lt;h2 id=&#34;inference&#34;&gt;Inference&lt;/h2&gt;
&lt;p&gt;To answer this question, we would like to compute a &lt;a href=&#34;https://en.wikipedia.org/wiki/Confidence_interval&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;confidence interval&lt;/strong&gt;&lt;/a&gt; for our estimate. How do we compute a confidence interval for a ratio metric? The first step is to compute the standard deviation of the estimator. One method that is always available is the &lt;a href=&#34;https://en.wikipedia.org/wiki/Bootstrapping_%28statistics%29&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;bootstrap&lt;/strong&gt;&lt;/a&gt;: resample the data with replacement multiple times and use the distribution of the estimates over samples to compute the standard deviation of the estimator.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s try it in our case. I compute the standard deviation over $10.000$ bootstrapped samples, using the function &lt;code&gt;pd.DataFrame().sample()&lt;/code&gt; with the options &lt;code&gt;frac=1&lt;/code&gt; to obtain a dataset of the same size and &lt;code&gt;replace=True&lt;/code&gt; to sample with replacement.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;boot_estimates = [estimate_roi(df.sample(frac=1, replace=True, random_state=i)) for i in range(10_000)]
np.std(boot_estimates)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;0.9790730538161984
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The bootstrap estimate of the standard deviation is equal to $0.979$. How good is it?&lt;/p&gt;
&lt;p&gt;Since we fully control the data generating process, we can simulate the &amp;ldquo;true&amp;rdquo; distribution of the estimator. We do that for $10.000$ simulations and we compute the resulting standard deviation of the estimator.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;np.std(dgp.evaluate_f_redrawing_outcomes(estimate_roi, 10_000))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;1.0547776958025372
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The estimated variance of the estimator using the &amp;ldquo;true&amp;rdquo; data generating process is slightly higher but very similar, around $1.055$.&lt;/p&gt;
&lt;p&gt;The issue with the bootstrap is that it is very computational intense since it requires repeating the estimating procedure thousands of times. We are now going to explore another &lt;em&gt;extremely&lt;/em&gt; powerful alternative that requires a single estimation step, the &lt;a href=&#34;https://en.wikipedia.org/wiki/Delta_method&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;delta method&lt;/strong&gt;&lt;/a&gt;. The delta method generally allows us to do inference on functions of random variable, therefore its applications are broader than ratios.&lt;/p&gt;
&lt;p&gt;⚠️ &lt;strong&gt;Warning&lt;/strong&gt;: the next section is going to be algebra-intense. If you want, you can skip it and go straight to the last section.&lt;/p&gt;
&lt;h2 id=&#34;the-delta-method&#34;&gt;The Delta Method&lt;/h2&gt;
&lt;p&gt;What is the &lt;strong&gt;delta method&lt;/strong&gt;? In short, it is an incredibly powerful &lt;strong&gt;asymptotic inference&lt;/strong&gt; method for functions of random variables, that exploits Taylor expansions. In short, the delta method requires four ingredients&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;One or more &lt;a href=&#34;https://en.wikipedia.org/wiki/Random_variable&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;random variables&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;A function&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Central_limit_theorem&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The Central Limit Theorem&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Taylor_series&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Taylor expansions&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I will assume some basic knowledge of all four concepts. Suppose we had a set of realizations $X_1$, &amp;hellip;, $X_n$ of a random variable that satisfy the requirements for the Central Limit Theorem (CLT): independence, identically distributions with expected value $\mu$, and finite variance $\sigma^2$. Under these conditions, the CLT tells us that the sample average $\mathbb E_n[X]$ converges in distribution to a normal distribution, or more precisely&lt;/p&gt;
&lt;p&gt;$$
\sqrt{n} \ \frac{ \mathbb E_n[X] - \mu}{\sigma} \ \overset{D}{\to} \ N(0, 1)
$$&lt;/p&gt;
&lt;p&gt;What does the equation mean? It reads &amp;ldquo;the normalized sample average, scaled by a factor $\sqrt{n}$, converges in distribution to a standard normal distribution, i.e. it is approximately Gaussian for a sufficiently large sample.&lt;/p&gt;
&lt;p&gt;Now, suppose we were interested in a &lt;strong&gt;function&lt;/strong&gt; of the sample average $f\big(\mathbb E_n[X]\big)$. Note that this is different from the sample average of the function $\mathbb E_n\big[f(X)\big]$. The delta method tells us what the function of the sample average converges to.&lt;/p&gt;
&lt;p&gt;$$
\sqrt{n} \ \frac{ f\big(\mathbb E_n[X]\big) - f(\mu)}{\sigma} \ \overset{D}{\to} \ N \big(0, f&amp;rsquo;(\mu)^2 \big)
$$&lt;/p&gt;
&lt;p&gt;, where $f&amp;rsquo;(\mu)^2$ is the derivative of the function $f$, evaluated at $\mu$.&lt;/p&gt;
&lt;p&gt;What is the &lt;strong&gt;intuition&lt;/strong&gt; behind this formula? We now have a new term inside the expression of the variance, the squared first derivative $f&amp;rsquo;(\mu)^2$ ($\neq$ second derivative). If the derivative of the function is low, the variance decreases since different inputs translate into similar outputs. On the contrary, if the derivative of the function is high, the variance of the distribution is amplified, since different inputs translate into even more different outputs.&lt;/p&gt;
&lt;img src=&#34;fig/delta_intuition.png&#34; width=&#34;700px&#34;/&gt;
&lt;p&gt;The result directly follows from the Taylor approximation of $f \big(\mathbb E_n[X]\big)$&lt;/p&gt;
&lt;p&gt;$$
f\big(\mathbb E_n[X]\big) = f(\mu) + f&amp;rsquo;(\mu) (\mathbb E_n[X] - \mu) + \text{residual}
$$&lt;/p&gt;
&lt;p&gt;Importantly, asymptotically, the last term disappears and the linear approximation holds exactly!&lt;/p&gt;
&lt;p&gt;How is this connected to the ratio estimator? We need a bit more math and to switch from a single dimension to two dimensions in order to understand that. In our case, we have a bivariate function of two random variables, $\Delta R$ and $\Delta C$, which returns their ratio. In the case of a multivariate function $f$, the asymptotic variance of the estimator is given by&lt;/p&gt;
&lt;p&gt;$$
\text{AVar} \big( \hat{\rho} \big) = \nabla \hat{\rho}&amp;rsquo; \Sigma_n \nabla \hat{\rho}
$$&lt;/p&gt;
&lt;p&gt;where, $\nabla$ indicates the &lt;a href=&#34;https://en.wikipedia.org/wiki/Gradient&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;gradient&lt;/a&gt; of the function, i.e. the vector of directional derivatives, and $\Sigma_n$ is the empirical variance-covariance matrix of $X$. In our case, they correspond to&lt;/p&gt;
&lt;p&gt;$$
\nabla \hat{\rho} =
\begin{bmatrix}
\frac{1}{\mathbb E_n [\Delta C]} \newline - \frac{\mathbb E_n [\Delta R]}{\mathbb E_n [\Delta C]^2}
\end{bmatrix}
$$&lt;/p&gt;
&lt;p&gt;and&lt;/p&gt;
&lt;p&gt;$$
\Sigma_n =
\begin{bmatrix}
\text{Var}_n (\Delta R) &amp;amp; \text{Cov}_n (\Delta R, \Delta C) \newline
\text{Cov}_n (\Delta R, \Delta C) &amp;amp; \text{Var}_n (\Delta C) \newline
\end{bmatrix}
$$&lt;/p&gt;
&lt;p&gt;, where the subscripts $n$ indicate the empirical counterparts, as for the expected value.&lt;/p&gt;
&lt;p&gt;Combining the previous three equations together with a little matrix algebra, we get the formula of the asymptotic variance of the return on investment estimator.&lt;/p&gt;
&lt;p&gt;$$
\begin{align*}
\text{AVar} \big( \hat{\rho} \big) &amp;amp;= \frac{1}{\mathbb E_n[\Delta C]^2} \text{Var}_n(\Delta R) - 2 \frac{\mathbb E_n[\Delta R]}{\mathbb E_n[\Delta C]^3} \text{Cov}_n(\Delta R, \Delta C) + \frac{\mathbb E_n[\Delta R]^2}{\mathbb E_n[\Delta C]^4} \text{Var}_n(\Delta C)
\end{align*}
$$&lt;/p&gt;
&lt;p&gt;Since the estimator is given by $\hat{\rho} = \frac{\mathbb E_n[\Delta R]}{\mathbb E_n[\Delta C]}$, we can rewrite the asymptotic variance as&lt;/p&gt;
&lt;p&gt;$$
\begin{align*}
\text{AVar} \big( \hat{\rho} \big) = \frac{1}{\mathbb E_n[\Delta C]^2} \text{Var}_n \Big( \Delta R - \hat{\rho} \Delta C \Big)
\end{align*}
$$&lt;/p&gt;
&lt;p&gt;The last expression is very interesting because it suggests that we can rewrite the asymptotic variance of our estimator as the &lt;strong&gt;variance of a difference-in-means estimator&lt;/strong&gt; for a new auxiliary variable. In fact, we can rewrite the above expression as&lt;/p&gt;
&lt;p&gt;$$
\begin{align*}
\text{AVar} \big( \hat{\rho} \big) = \text{Var}_n \Big( \Delta \tilde R \Big) \qquad \text{where} \quad \tilde R = \frac{R - \hat{\rho} \ C}{| \mathbb E [\Delta C] |}
\end{align*}
$$&lt;/p&gt;
&lt;p&gt;This expression is incredibly useful because it gives us intuition and allows us to estimate the standard deviation of our estimator by &lt;strong&gt;linear regression&lt;/strong&gt;.&lt;/p&gt;
&lt;h2 id=&#34;inference-with-linear-regression&#34;&gt;Inference with Linear Regression&lt;/h2&gt;
&lt;p&gt;Did you skip the previous section? No problem!&lt;/p&gt;
&lt;p&gt;After some algebra, we concluded that we can estimate the variance of a difference-in-means estimator for an &lt;strong&gt;auxiliary variable&lt;/strong&gt; defined as&lt;/p&gt;
&lt;p&gt;$$
\tilde R = \frac{R - \hat{\rho} \ C}{| \mathbb E_n [\Delta C] |}
$$&lt;/p&gt;
&lt;p&gt;This expression might seem obscure at first, but it is incredibly useful. In fact, it gives us (1) an intuitive &lt;strong&gt;interpretation&lt;/strong&gt; of the variance of the estimator and (2) a &lt;strong&gt;practical&lt;/strong&gt; way to estimate it.&lt;/p&gt;
&lt;p&gt;Interpretation first! How should we read the above expression? We can estimate the variance of the empirical estimator as the variance of a difference-in-means estimator, for a new variable $\tilde R$ that we can easily compute from the data. We just need to take the revenue $R$, subtract the cost $C$ multiplied by the estimated ROI $\rho$ and scale it down by the expected cost difference $|\mathbb E_n[\Delta C]|$. We can interpret this variable as the &lt;strong&gt;baseline revenue&lt;/strong&gt;, i.e. the revenue not affected by the investment. The fact that it is scaled by the expected cost difference tells us that its variance will be &lt;strong&gt;decreasing in the total investment&lt;/strong&gt;: the more we spend, the more precisely we can estimate the return on that expenditure.&lt;/p&gt;
&lt;p&gt;Now, let&amp;rsquo;s estimate the variance of the ROI estimator, in &lt;strong&gt;four steps&lt;/strong&gt;.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;We need to estimate the return on investment $\hat \rho$.&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;rho_hat = estimate_roi(df)
&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;The term $| \mathbb E_n[\Delta C] |$ is the absolute difference in average cost between the treatment and control group.&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;abs_Delta_C = np.abs(df.loc[df.new_machine==1, &amp;quot;cost&amp;quot;].mean() - df.loc[df.new_machine==0, &amp;quot;cost&amp;quot;].mean())
&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;3&#34;&gt;
&lt;li&gt;We now have all the ingredients to generate the auxiliary variable $\tilde R$.&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df[&#39;revenue_tilde&#39;] = (df[&#39;revenue&#39;] - rho_hat * df[&#39;cost&#39;]) / abs_Delta_C
&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;4&#34;&gt;
&lt;li&gt;The variance of the treatment-control difference $\Delta \tilde R$ can be directly computed by linear regression, as in randomized controlled trials for difference-in-means estimators (see Agrist and Pischke, 2008).&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;smf.ols(&#39;revenue_tilde ~ new_machine&#39;, df).fit().summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
       &lt;td&gt;&lt;/td&gt;          &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt;   &lt;td&gt;   38.4067&lt;/td&gt; &lt;td&gt;    0.653&lt;/td&gt; &lt;td&gt;   58.771&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;   37.126&lt;/td&gt; &lt;td&gt;   39.688&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;new_machine&lt;/th&gt; &lt;td&gt; -2.01e-14&lt;/td&gt; &lt;td&gt;    0.917&lt;/td&gt; &lt;td&gt;-2.19e-14&lt;/td&gt; &lt;td&gt; 1.000&lt;/td&gt; &lt;td&gt;   -1.797&lt;/td&gt; &lt;td&gt;    1.797&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;The estimated standard error of the ROI is $0.917$, very close to the bootstrap estimate of $0.979$ and the simulated value of $1.055$. However, with respect to bootstrapping, the delta method allowed us to compute it in a single step, making it sensibly &lt;strong&gt;faster&lt;/strong&gt; (around $1000$ times on my local machine).&lt;/p&gt;
&lt;p&gt;Note that this estimated standard deviation implies a 95% confidence interval of $2.0698 +- 1.96 \times 0.917$, equal to $[-0.2735, 3.8671]$. This might seem like good news since the confidence interval does not cover zero. However, note that in this case, a more interesting &lt;strong&gt;null hypothesis&lt;/strong&gt; is that the ROI is equal to 1: we are breaking even. A value larger than 1 implies profits, while a value lower than 1 implies losses. In our case, we cannot reject the null hypothesis that the investment in new machines was not profitable.&lt;/p&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;In this article, we have explored a very common causal inference problem: assessing the &lt;strong&gt;return on investment&lt;/strong&gt;. Whether it&amp;rsquo;s a physical investment in new hardware, a virtual cost, or advertisement expenditure, we are interested in understanding whether this incremental cost has paid off. The additional complications come from the fact that we are studying not one, but two causal quantities, intertwined.&lt;/p&gt;
&lt;p&gt;We have first explored and compared different outcome metrics to assess whether the investment paid off. Then, we have introduced an incredibly powerful method to do inference with complex random variables: the &lt;strong&gt;delta method&lt;/strong&gt;. In the particular case of ratios, the delta method delivers a very insightful and practical functional form for the asymptotic variance of the estimator that can be estimated with a simple linear regression.&lt;/p&gt;
&lt;h3 id=&#34;references&#34;&gt;References&lt;/h3&gt;
&lt;p&gt;[1] A. Deng, U. Knoblich, J. Lu, &lt;a href=&#34;https://arxiv.org/pdf/1803.06336.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Applying the Delta Method in Metric Analytics: A Practical Guide with Novel Ideas&lt;/a&gt; (2018).&lt;/p&gt;
&lt;p&gt;[2] R. Budylin, A. Drutsa, I. Katsev, V. Tsoy, &lt;a href=&#34;https://dl.acm.org/doi/abs/10.1145/3159652.3159699&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Consistent Transformation of Ratio Metrics for Efficient Online Controlled Experiments&lt;/a&gt; (2018). &lt;em&gt;ACM&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;[3] J. Angrist, J. Pischke, &lt;a href=&#34;https://www.mostlyharmlesseconometrics.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Mostly harmless econometrics: An empiricist&amp;rsquo;s companion&lt;/a&gt; (2009). &lt;em&gt;Princeton university press&lt;/em&gt;.&lt;/p&gt;
&lt;h3 id=&#34;related-articles&#34;&gt;Related Articles&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://medium.com/towards-data-science/the-bayesian-bootstrap-6ca4a1d45148&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The Bayesian Bootstrap&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://towardsdatascience.com/df3065a0388e&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Outliers, Leverage, Residuals, and Influential Observations&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://towardsdatascience.com/b07ab46aa782&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;A/B Tests, Privacy, and Online Regression&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;code&#34;&gt;Code&lt;/h3&gt;
&lt;p&gt;You can find the original Jupyter Notebook here:&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/delta.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/delta.ipynb&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Experiments, Peeking, and Optimal Stopping</title>
      <link>https://matteocourthoud.github.io/post/optimal_stopping/</link>
      <pubDate>Mon, 10 Jul 2023 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/post/optimal_stopping/</guid>
      <description>&lt;p&gt;In the decade preceding the Second World War, there was a massive increase in industrial production of war materials, so there was a need to ensure that products, especially munitions, were reliable. The &lt;strong&gt;testing&lt;/strong&gt; of war materials is not only &lt;strong&gt;expensive&lt;/strong&gt; but also &lt;strong&gt;destructive&lt;/strong&gt; since, for example, bullets need to be fired in order to be tested.&lt;/p&gt;
&lt;p&gt;Therefore, the U.S. government was presented with the following &lt;strong&gt;dilemma&lt;/strong&gt;: how many bullets should one fire out of a batch before declaring the batch reliable? Clearly, if we were to fire all the bullets, we would know the exact amount of functioning bullets in a crate. However, there would be no bullets left to use.&lt;/p&gt;
&lt;p&gt;Because of the growing relevance of these statistical problems, in 1939, a group of prominent statisticians and economists joined forces at Columbia University&amp;rsquo;s &lt;a href=&#34;https://en.wikipedia.org/wiki/The_Statistical_Research_Group_of_World_War_II&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Statistical Research Group (SGR)&lt;/strong&gt;&lt;/a&gt;. The group included, among others, &lt;a href=&#34;https://en.wikipedia.org/wiki/W._Allen_Wallis&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;W. Allen Wallis&lt;/a&gt;, &lt;a href=&#34;https://en.wikipedia.org/wiki/Jacob_Wolfowitz&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Jacob Wolfowitz&lt;/a&gt; and &lt;a href=&#34;https://en.wikipedia.org/wiki/Abraham_Wald&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Abraham Wald&lt;/a&gt;. According to Wallis himself the SGR group was &amp;ldquo;&lt;em&gt;composed of what surely must be the most extraordinary group of statisticians ever organized, taking into account both number and quality&lt;/em&gt;&amp;rdquo;[&lt;a href=&#34;#References&#34;&gt;2&lt;/a&gt;].&lt;/p&gt;
&lt;p&gt;Their work was of first order importance and &lt;strong&gt;classified&lt;/strong&gt;, to the point that Wallis reports:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;It is said that as Wald worked on sequential analysis his pages were snatched away and given a security classification. Being still an &amp;ldquo;enemy alien&amp;rdquo;, he did not have a security clearance so, the story has it, he was not allowed to know of his results.&lt;/em&gt; [&lt;a href=&#34;https://www.jstor.org/stable/2287451&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Wallis (1980)&lt;/a&gt;]&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Indeed, the group worked under the pressure from the U.S. Army to deliver &lt;strong&gt;fast practical solutions&lt;/strong&gt; that could be readily deployed on the field. For example, Wallis reports that&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;during the &lt;a href=&#34;https://en.wikipedia.org/wiki/Battle_of_the_Bulge&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Battle of the Bulge&lt;/a&gt; in December 1944, several high-ranking Army officers flew to Washington from the battle, spent a day discussing the best settings on proximity fuzes for air bursts of artillery shells against ground troops, and flew back to the battle to put into effect advice from, among others, &lt;a href=&#34;https://en.wikipedia.org/wiki/Milton_Friedman&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Milton Friedman&lt;/a&gt;, whose earlier studies of the fuzes had given him extensive and accurate knowledge of the way the fuzes actually performed.&lt;/em&gt;  [&lt;a href=&#34;https://www.jstor.org/stable/2287451&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Wallis (1980)&lt;/a&gt;]&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The most prominent &lt;strong&gt;result&lt;/strong&gt; that came out of the SGR experience was undoubtedly the &lt;a href=&#34;https://en.wikipedia.org/wiki/Sequential_probability_ratio_test&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Sequential Probability Ratio Test&lt;/a&gt;. The idea first came to Wallis and Friedman that realized that&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;it might pay to use a test which would not be as efficient as the classical tests if a sample of exactly N were to be taken, but which would more than offset this disadvantage by providing a good chance of terminating early when used sequentially.&lt;/em&gt; [&lt;a href=&#34;https://www.jstor.org/stable/2287451&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Wallis (1980)&lt;/a&gt;]&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The two economists exposed the idea to the statistician Jacob Wolfowitz who initially&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;seemed to be something distasteful about the idea of people so ignorant of mathematics as Milton and I venturing to meddle with such sacred ideas as those of most powerful statistics, etc. No doubt this antipathy was strengthened by our calling the new tests &amp;ldquo;supercolossal&amp;rdquo; on the grounds that they are more powerful than &amp;ldquo;most powerful&amp;rdquo; tests.&lt;/em&gt; [&lt;a href=&#34;https://www.jstor.org/stable/2287451&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Wallis (1980)&lt;/a&gt;]&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Ultimately, the two economists managed to draw the attention of both Wolfowitz and Wald that started to formally work on the idea. The results remained top secret until the end of the war when Wald published his &lt;a href=&#34;https://www.jstor.org/stable/2235829&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Sequential Tests of Statistical Hypotheses&lt;/a&gt; article.&lt;/p&gt;
&lt;p&gt;In this post, after a quick introduction to hypothesis testing, we are going to explore the Sequential Probability Ratio Test and implement it in Python.&lt;/p&gt;
&lt;h2 id=&#34;hypothesis-testing&#34;&gt;Hypothesis Testing&lt;/h2&gt;
&lt;p&gt;When we design an A/B test or, more generally, an experiment, the standard steps are the following&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Define a &lt;strong&gt;null hypothesis&lt;/strong&gt; $H_0$, usually a zero effect of the experiment on a metric of interest&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;for example, no effect of a drug on mortality&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Define a &lt;strong&gt;significance level&lt;/strong&gt; $\alpha$, usually equal to 0.05, it represents the maximum probability of rejecting the null hypothesis when it is true&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;for example, the probability of claiming that the drug is effective in reducing mortality, when it&amp;rsquo;s not effective&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Define an &lt;strong&gt;alternative hypothesis&lt;/strong&gt; $H_1$, usually the minimum effect size that we would like to detect&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;for example, a decrease in mortality by 1%&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Define a &lt;strong&gt;power level&lt;/strong&gt; $1-\beta$, usually equal to 0.8 ($\beta=0.2$), it represents the minimum probability of rejecting the null hypothesis $H_0$, when the alternative $H_1$ is true&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;for example, the probability of claiming that the drug is ineffective, when it&amp;rsquo;s effective&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Pick a &lt;strong&gt;test statistic&lt;/strong&gt; whose distribution is known under both hypotheses, usually the sample average of the metric of interest&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;for example, the average mortality rate of patients&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Compute the minimum &lt;strong&gt;sample size&lt;/strong&gt;, in order to achieve the desired power level $1-\beta$, given all the test parameters&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Then, we &lt;strong&gt;run the test&lt;/strong&gt; and, depending on the realized value of the test statistic, we decide whether to &lt;strong&gt;reject&lt;/strong&gt; the null hypothesis or not. In particular, we reject the null hypothesis if the &lt;strong&gt;p-value&lt;/strong&gt;, i.e. the probability of observing under the null hypothesis a statistic as or more extreme than the sample statistic, is lower than the significance level $\alpha$.&lt;/p&gt;
&lt;p&gt;Remember that rejecting the null hypothesis does not imply accepting the alternative hypothesis.&lt;/p&gt;
&lt;h2 id=&#34;peeking&#34;&gt;Peeking&lt;/h2&gt;
&lt;p&gt;Suppose that halfway through the experiment we were to &lt;strong&gt;peek at the data&lt;/strong&gt; and notice that, for that intermediate value of the test statistic, we would reject the null hypothesis. Should we stop the experiment? If we do, what happens?&lt;/p&gt;
&lt;p&gt;The answer is that we &lt;strong&gt;should not stop&lt;/strong&gt; the experiment. If we do, the test would not achieve the desired significance level or, in other terms, our confidence intervals would have the &lt;strong&gt;wrong coverage&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s see what I mean with an &lt;strong&gt;example&lt;/strong&gt;. Suppose our &lt;strong&gt;data generating process&lt;/strong&gt; is a &lt;a href=&#34;https://en.wikipedia.org/wiki/Normal_distribution&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;standard normal distribution&lt;/a&gt; with unknown mean $\mu$ and known variance $\sigma=1$: $X \sim N(\mu,1)$.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;hypothesis&lt;/strong&gt; that we wish to test is&lt;/p&gt;
&lt;p&gt;$$
\begin{align}
H_0: \quad &amp;amp; \mu = 0
\newline
H_1: \quad &amp;amp; \mu = 0.1
\end{align}
$$&lt;/p&gt;
&lt;p&gt;After each observation $n$, we compute the &lt;a href=&#34;https://en.wikipedia.org/wiki/Z-test&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;z test statistic&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;$$
z = \frac{\bar X_n - \mu_0}{\frac{\sigma}{\sqrt{n}}} = \frac{\bar X_n - 0}{\frac{1}{\sqrt{n}}} = \bar X_n * \sqrt{n}
$$&lt;/p&gt;
&lt;p&gt;where $\bar X_n$ is the &lt;a href=&#34;https://en.wikipedia.org/wiki/Sample_mean_and_covariance&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;sample mean&lt;/a&gt; from a sample $X_1, X_2, &amp;hellip;, X_n$, of size $n$, $\sigma$ is the standard deviation of the population, and $\mu_0$ is the population mean, under the null hypothesis. The term in the denominator, $\frac{\sigma}{\sqrt{n}}$, is the variance of the sample mean. Under the null hypothesis of zero mean, the test statistic is distributed as a standard normal distribution with zero mean and unit variance, $N(0,1)$.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s code the test in Python. I import some code from &lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/src/utils.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;utils&lt;/code&gt;&lt;/a&gt; to make the plots prettier.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;%matplotlib inline
%config InlineBackend.figure_format = &#39;retina&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from src.utils import *

zstat = lambda x: np.mean(x) * np.sqrt(len(x))
zstat.__name__ = &#39;z-statistic&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Suppose we want a test with significance level $\alpha=0.05$ and power $1-\beta=0.8$. What sample size $n$ do we need?&lt;/p&gt;
&lt;p&gt;We need a sample size such that&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;The probability of rejecting the null hypothesis $H_0$, when $H_0$ is &lt;em&gt;true&lt;/em&gt;, is at most $\alpha=0.05$&lt;/li&gt;
&lt;li&gt;The probability of &lt;em&gt;not&lt;/em&gt; rejecting the null hypothesis $H_0$, when $H_0$ is &lt;em&gt;false&lt;/em&gt; (i.e. $H_1$ is true), is at most $\beta=0.2$&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;I.e. we need to find a &lt;strong&gt;critical value&lt;/strong&gt; $c$ such that&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;$c = \mu_0 + z_{0.95} * \frac{\sigma}{\sqrt{n}}$&lt;/li&gt;
&lt;li&gt;$c = \mu_1 - z_{0.8} * \frac{\sigma}{\sqrt{n}}$&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;where $z_{p}$ is the CDF inverse (or &lt;a href=&#34;https://en.wikipedia.org/wiki/Quantile_function&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;percent point function&lt;/a&gt;) at $p$, and $\mu_i$ are the values of the mean under the different hypotheses.&lt;/p&gt;
&lt;p&gt;If we do not know the &lt;strong&gt;sign&lt;/strong&gt; of the unknown mean $\mu$, we have to run a &lt;strong&gt;two-sided test&lt;/strong&gt;. This means that the maximum probability of type 1 error on each side of the distribution has to be $\alpha/2 = 0.025$, implying $z_{0.975} = 1.96$.&lt;/p&gt;
&lt;p&gt;Combining the two expressions together we can solve for the &lt;strong&gt;required minimum sample size&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;$$
n : \mu_0 + z_{0.975} * \frac{\sigma}{\sqrt{n}} = \mu_1 - z_{0.8} * \frac{\sigma}{\sqrt{n}}
$$&lt;/p&gt;
&lt;p&gt;so that&lt;/p&gt;
&lt;p&gt;$$
n = \left( \sigma * \frac{z_{0.975} + z_{0.8}}{\mu_0 + \mu_1} \right)^2 = \left( 1 * \frac{1.96 + 0.84}{0 + 0.1} \right)^2 = 784.89
$$&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from scipy.stats import norm

n = ( (norm.ppf(0.975) + norm.ppf(0.8)) / 0.1 )**2
print(f&amp;quot;Minimum sample size: {n}&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Minimum sample size: 784.8879734349086
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We need at least 785 observations.&lt;/p&gt;
&lt;p&gt;We can get a better &lt;strong&gt;intuition&lt;/strong&gt; by graphically plotting the two distributions with the critical value. I wrote a function &lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/src/figures.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;plot_test&lt;/code&gt;&lt;/a&gt; to draw a standard hypothesis testing setting.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from src.figures import plot_test

plot_test(mu0=0, mu1=0.1, alpha=0.05, n=n)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/optimal_stopping_11_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;The critical value is such that, given the distributions under the two hypothesis, the &lt;strong&gt;rejection area&lt;/strong&gt; in red is equal to $\alpha$. The sample size $n$ is such that it shrinks the variance of the two distributions so that the area in green is equal to $\beta$.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s now &lt;strong&gt;simulate an experiment&lt;/strong&gt; in which we draw an ordered sequence of observations and, after each observation, we compute the value of the test statistic.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def experiment(f_stat, mu=0, n=785, seed=1):
    np.random.seed(seed) # Set seed
    I = np.arange(1, n+1) # Observation index
    x = np.random.normal(mu, 1, n) # Observation value
    stat = [f_stat(x[:i]) for i in I] # Value of the test statistic so far
    df = pd.DataFrame({&#39;i&#39;: I, &#39;x&#39;: x, f_stat.__name__: stat}) # Generate dataframe
    return df
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s have a look at what a sample looks like.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df = experiment(zstat)
df.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;i&lt;/th&gt;
      &lt;th&gt;x&lt;/th&gt;
      &lt;th&gt;z-statistic&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1.624345&lt;/td&gt;
      &lt;td&gt;1.624345&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;-0.611756&lt;/td&gt;
      &lt;td&gt;0.716009&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;-0.528172&lt;/td&gt;
      &lt;td&gt;0.279678&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;-1.072969&lt;/td&gt;
      &lt;td&gt;-0.294276&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;0.865408&lt;/td&gt;
      &lt;td&gt;0.123814&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;We can now plot the time trend of the test statistic as we accumulate observations during the sampling process. I also mark with horizontal lines the values for rejection of the null hypothesis of a test with $\alpha = 0.05$: $z_{0.025} = -1.96$ and $z_{0.975} = 1.96$.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def plot_experiment(df, ybounds, **kwargs):
    sns.lineplot(data=df, x=&#39;i&#39;, y=df.columns[2], **kwargs)
    for ybound in ybounds:
        sns.lineplot(x=df[&#39;i&#39;], y=ybound, lw=1.5, color=&#39;black&#39;)
    plt.title(f&#39;{df.columns[2]} with sequential sampling&#39;)
    plt.yticks([0, ybounds[0], ybounds[1]])
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plot_experiment(df, ybounds=[-1.96, 1.96])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/optimal_stopping_18_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;In this case, the test never crosses the critical values. Therefore, peeking does not have an effect. We would not have stopped the experiment prematurely.&lt;/p&gt;
&lt;p&gt;What would happen if we were &lt;strong&gt;repeating&lt;/strong&gt; the experiment many times? Since the data is generated under the null hypothesis, $H_0: \mu = 0$, we expect to reject it only $\alpha=5%$ of the times.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s simulate the data-generating process $K=100$ times.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def simulate_experiments(f_stat, ybounds, xmin=0, early_stop=False, mu=0, K=100, n=785, **kwargs):
    # Count experiment durations
    stops = np.zeros(K) * n
    
    # Perform K simulations
    for k in range(K):
        # Draw data
        df = experiment(f_stat, mu=mu, seed=k, n=n)
        vals = df[f_stat.__name__].values
        
        # If early stop, check early violations (during sampling)
        if early_stop:
            violations = (vals[xmin:] &amp;gt; max(ybounds)) + (vals[xmin:] &amp;lt; min(ybounds))
        if early_stop and any(violations):
            end = 1 + xmin + np.where(violations)[0][0]
            plot_experiment(df.iloc[:end, :], ybounds, **kwargs)
            stops[k] = end * np.sign(df[f_stat.__name__].values[end])
        
        # Otherwise, only check violations of last value
        elif (vals[-1] &amp;gt; max(ybounds)) or (vals[-1] &amp;lt; min(ybounds)):
            plot_experiment(df, ybounds, **kwargs)
            stops[k] = len(df) * np.sign(vals[-1])
        
        # Plot all other observations in grey
        else: 
            plot_experiment(df, ybounds, color=&#39;grey&#39;, alpha=0.1, lw=1)
    
    # Print diagnostics
    pct_up = sum(stops&amp;gt;0)/sum(stops!=0)*100
    print(f&#39;Bounds crossed: {sum(stops!=0)} ({pct_up:.0f}% upper, {100-pct_up:.0f}% lower)&#39;)
    print(f&#39;Average experiment duration: {(sum(np.abs(stops)) + n*sum(stops==0))/ len(stops) :.0f}&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We plot the distribution of the z-statistic over samples .&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;simulate_experiments(zstat, ybounds=[-1.96, 1.96], early_stop=False);
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Bounds crossed: 3 (67% upper, 33% lower)
Average experiment duration: 785
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/optimal_stopping_22_1.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;In the figure above, I have highlighted the experiments for which we reject the null hypothesis &lt;strong&gt;without peeking&lt;/strong&gt;, i.e. given the value of the z test statistic &lt;strong&gt;at the end of the sampling&lt;/strong&gt; process. Only in 3 experiments the final value lies outside the critical values, so that we reject the null hypothesis. This means a &lt;strong&gt;rejection rate&lt;/strong&gt; of 3% which is very close to the expected rejection rate of $\alpha=0.05$ (under the null).&lt;/p&gt;
&lt;p&gt;What if instead we were &lt;strong&gt;impatient&lt;/strong&gt; and, after collecting the first 100 observations, we were stopping &lt;strong&gt;as soon as&lt;/strong&gt; we saw the z-statistic crossing the boundaries?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;stops_zstat_h0 = simulate_experiments(zstat, xmin=99, ybounds=[-1.96, 1.96], early_stop=True, lw=2);
plt.vlines(100, ymin=plt.ylim()[0], ymax=plt.ylim()[1], color=&#39;k&#39;, lw=1, ls=&#39;--&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Bounds crossed: 29 (45% upper, 55% lower)
Average experiment duration: 644
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/optimal_stopping_24_1.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;In the figure above, I have highlighted the experiments in which the values of the z-statistic crosses one of the boundaries, from the 100th observation onwards. This happens in 29 simulations out of 100, which implies a &lt;strong&gt;rejection rate&lt;/strong&gt; of 25%, which is very far from the expected rejection rate of $\alpha=0.05$ (under the null hypothesis). Peaking &lt;strong&gt;distorts&lt;/strong&gt; the significance level of the test.&lt;/p&gt;
&lt;p&gt;Potential solutions are:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;sequential probability ratio tests&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;sequential triangular testing&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;group sequential testing&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Before analyzing these sequential testing procedures, we first need to introduce the &lt;strong&gt;likelihood ratio test&lt;/strong&gt;.&lt;/p&gt;
&lt;h2 id=&#34;likelihood-ratio-test&#34;&gt;Likelihood Ratio Test&lt;/h2&gt;
&lt;p&gt;The likelihood ratio test is a test that tries to assess the likelihood that the observed data was generated by either one of two competing statistical models. &lt;/p&gt;
&lt;p&gt;In order to perform the likelihood ratio test for hypothesis testing, we need the data generating process to be fully specified under both hypotheses. For example, this would be the case with the following hypotheses:&lt;/p&gt;
&lt;p&gt;$$
\begin{align}
H_0: \quad &amp;amp; \mu=0
\newline
H_1: \quad &amp;amp; \mu=0.1
\end{align}
$$&lt;/p&gt;
&lt;p&gt;In this case, we say that the statistical test is fully specified. If the alternative hypothesis was $H_1: \mu \neq 0$, then the data generating process would not be specified under the alternative hypothesis. &lt;/p&gt;
&lt;p&gt;When a statistical test is fully specified, we can compute the likelihood ratio as the the ratio of the &lt;a href=&#34;https://en.wikipedia.org/wiki/Likelihood_function&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;likelihood function&lt;/a&gt; under the two hypotheses.&lt;/p&gt;
&lt;p&gt;$$
\Lambda (X) = \frac{\mathcal L (\theta_1 \ | \ X)}{\mathcal L (\theta_0 \ | \ X)}
$$&lt;/p&gt;
&lt;p&gt;The likelihood-ratio test provides a decision rule as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;If $\Lambda&amp;gt;c$, reject $H_{0}$;&lt;/li&gt;
&lt;li&gt;If $\Lambda&amp;lt;c$, do not reject $H_{0}$;&lt;/li&gt;
&lt;li&gt;If $\Lambda =c$, reject with probability $q$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The values $c$ and $q$ are usually chosen to obtain a specified significance level $\alpha$.&lt;/p&gt;
&lt;p&gt;The &lt;a href=&#34;https://en.wikipedia.org/wiki/Neyman%E2%80%93Pearson_lemma&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Neyman–Pearson lemma&lt;/strong&gt;&lt;/a&gt; states that this likelihood-ratio test is the most powerful among all level $\alpha$ tests for this case.&lt;/p&gt;
&lt;h3 id=&#34;special-case-testing-mean-of-normal-distribution&#34;&gt;Special Case: testing mean of normal distribution&lt;/h3&gt;
&lt;p&gt;Let&amp;rsquo;s go back to our example where data is coming from a normal distribution with unknown mean $\mu$ and known variance $\sigma^2$ and we want to perform the following test&lt;/p&gt;
&lt;p&gt;$$
\begin{align}
H_0: \quad &amp;amp; \mu = 0 ,
\newline
H_1: \quad &amp;amp; \mu = 0.1
\end{align}
$$&lt;/p&gt;
&lt;p&gt;The likelihood of the normal distribution with unknown mean $\mu$ and known variance $\sigma^2$ is&lt;/p&gt;
&lt;p&gt;$$
\mathcal L(\mu) = \left( \frac{1}{\sqrt{2 \pi} \sigma } \right)^n e^{- \sum_{i=1}^{n} \frac{(X_i - \mu)^2}{2 \sigma^2}}
$$&lt;/p&gt;
&lt;p&gt;So that the likelihood ratio under the two hypotheses is&lt;/p&gt;
&lt;p&gt;$$
\Lambda(X) = \frac{\mathcal L (0.1, \sigma^2)}{\mathcal L (0, \sigma^2)} = \frac{e^{- \sum_{i=1}^{n} \frac{(X_i - 0.1)^2}{2 \sigma^2}}}{e^{- \sum_{i=1}^{n} \frac{(X_i)^2}{2 \sigma^2}}}
$$&lt;/p&gt;
&lt;p&gt;We now have all the ingredients to move on to the final purpose of this blog post: the Sequential Probability Ratio Test.&lt;/p&gt;
&lt;h2 id=&#34;sequential-probability-ratio-test&#34;&gt;Sequential Probability Ratio Test&lt;/h2&gt;
&lt;p&gt;Given a pair of fully specified hypotheses, say $H_{0}$ and $H_{1}$, the &lt;strong&gt;first step&lt;/strong&gt; of the sequential probability ratio test is to calculate the log-likelihood ratio test $\log (\Lambda_{i})$, as new data arrive: with $S_{0}=0$, then, for $i=1,2,&amp;hellip;,$&lt;/p&gt;
&lt;p&gt;$$
S_{i} = S_{i-1} + \log(\Lambda_{i})
$$&lt;/p&gt;
&lt;p&gt;The stopping rule is a simple thresholding scheme:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$S_{i}\geq b$: Accept $H_{1}$&lt;/li&gt;
&lt;li&gt;$S_{i}\leq a$: Accept $H_{0}$&lt;/li&gt;
&lt;li&gt;$a&amp;lt;S_{i}&amp;lt;b$: continue monitoring (critical inequality)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;where $a$ and $b$ ($-\infty&amp;lt;a&amp;lt;0&amp;lt;b&amp;lt;\infty$) depend on the desired type I and type II errors, $\alpha$  and $\beta$.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.jstor.org/stable/2235829&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Wald (1945)&lt;/a&gt; shows that the choice of the following boundaries delivers a test with expected probability of type 1 and 2 error not greater than $\alpha$ and $\beta$, respectively.&lt;/p&gt;
&lt;p&gt;$$
a \approx \log {\frac  {\beta }{1-\alpha }} \quad \text{and} \quad  b \approx \log {\frac  {1-\beta }{\alpha }}
$$&lt;/p&gt;
&lt;p&gt;The equations are approximations because of the discrete nature of the data generating process.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.jstor.org/stable/2235638&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Wald and Wolfowitz (1948)&lt;/a&gt; have proven that a test with these boundaries is the most powerful sequential probability ratio test, i.e. all SPR tests with the same power and significance require at least the same amount of observations.&lt;/p&gt;
&lt;h3 id=&#34;special-case-testing-null-effect&#34;&gt;Special Case: testing null effect&lt;/h3&gt;
&lt;p&gt;Let&amp;rsquo;s go back to our example where data is coming from a normal distribution with unknown mean $\mu$ and known variance $\sigma^2$ and hypotheses $H_0: \ \mu = 0$ and $H_1: \ \mu = 0.1$.&lt;/p&gt;
&lt;p&gt;We have seen that the likelihood ratio with a sample of size $n$ is&lt;/p&gt;
&lt;p&gt;$$
\Lambda(X) = \frac{\mathcal L (0.1, \sigma^2)}{\mathcal L (0, \sigma^2)} = \frac{e^{- \sum_{i=1}^{n} \frac{(X_i - 0.1)^2}{2 \sigma^2}}}{e^{- \sum_{i=1}^{n} \frac{(X_i)^2}{2 \sigma^2}}}
$$&lt;/p&gt;
&lt;p&gt;Therefore, the log-likelihood (easier to compute) is&lt;/p&gt;
&lt;p&gt;$$
\log (\Lambda(X)) = \left( \sum_{i=1}^{n} \frac{(X_i)^2}{2 \sigma^2} \right) - \left( \sum_{i=1}^{n} \frac{(X_i - 0.1)^2}{2 \sigma^2} \right)
$$&lt;/p&gt;
&lt;h3 id=&#34;simulation&#34;&gt;Simulation&lt;/h3&gt;
&lt;p&gt;We are now ready to perform some simulations. First, let&amp;rsquo;s code the &lt;strong&gt;log likelihood ratio test statistic&lt;/strong&gt; that we have just computed.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;log_lr = lambda x: (np.sum((x)**2) - np.sum((x-0.1)**2) ) / 2
log_lr.__name__ = &#39;log likelihood-ratio&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We now repeat the same experiment we did at the beginning, with one difference: we will compute the log likelihood ratio as a statistic. The data generating process has $\mu=0$, as under the null hypothesis.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df = experiment(log_lr, )
df.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;i&lt;/th&gt;
      &lt;th&gt;x&lt;/th&gt;
      &lt;th&gt;log likelihood-ratio&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1.624345&lt;/td&gt;
      &lt;td&gt;0.157435&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;-0.611756&lt;/td&gt;
      &lt;td&gt;0.091259&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;-0.528172&lt;/td&gt;
      &lt;td&gt;0.033442&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;-1.072969&lt;/td&gt;
      &lt;td&gt;-0.078855&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;0.865408&lt;/td&gt;
      &lt;td&gt;0.002686&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;Let&amp;rsquo;s now compute the optimal bounds, given significance level $\alpha=0.05$ and power $1-\beta=0.8$.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;alpha = 0.05
beta = 0.2

a = np.log( beta / (1-alpha) )
b = np.log( (1-beta) / alpha )
print(f&#39;Optimal bounds : [{a:.3f}, {b:.3f}]&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Optimal bounds : [-1.558, 2.773]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Since significance and (one minus) power are different, the bound for the null hypothesis is much &lt;strong&gt;closer&lt;/strong&gt; than the bound for the alternative hypothesis. This means that, in case of an intermediate effect of $\mu=0.05$, we will be more likely to accept the null hypothesis $H_0: \mu = 0$ than the alternative $H_1: \mu = 0.1$.&lt;/p&gt;
&lt;p&gt;We can plot the distribution of the likelihood ratio over samples drawn under the null hypothesis $H_0: \mu = 0$.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plot_experiment(df, ybounds=[a,b])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/optimal_stopping_43_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;In this particular case, the test is inconclusive within our sampling framework. We need to &lt;strong&gt;collect more data&lt;/strong&gt; in order to come to a decision.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plot_experiment(experiment(log_lr, n=789), ybounds=[a,b]);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/optimal_stopping_45_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;It takes 789 observations to reach to a conclusion, while before the sample size was 785. This test procedure can require a &lt;strong&gt;larger sample size&lt;/strong&gt; than the previous one. Is it true on average?&lt;/p&gt;
&lt;p&gt;What would happen if we were to repeat the experiment $K=100$ times?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;simulate_experiments(log_lr, ybounds=[a, b], early_stop=True, lw=1.5);
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Bounds crossed: 96 (4% upper, 96% lower)
Average experiment duration: 264
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/optimal_stopping_47_1.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;We get a decision for 96 simulations out of 100 and for 96% of them, it&amp;rsquo;s the correct decision. Therefore, our rejection rate is very close to the expected $\alpha=0.05$ (under the null hypothesis).&lt;/p&gt;
&lt;p&gt;However, for 4 experiments, the test is inconclusive. What would happen if we were to sample until we reach a conclusion in each experiment?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;simulate_experiments(log_lr, ybounds=[a,b], early_stop=True, lw=1.5, n=1900);
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Bounds crossed: 100 (4% upper, 96% lower)
Average experiment duration: 275
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/optimal_stopping_49_1.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;As we can see from the plot, in one particularly unlucky experiment, we need to collect 1900 observations before coming to a conclusion. However, despite this outlier, the &lt;strong&gt;average experiment duration&lt;/strong&gt; is an astounding 275 samples, almost a third of the original sample size of 785.&lt;/p&gt;
&lt;p&gt;What would happen if instead the alternative hypothesis $H_1: \mu = 0.1$ was true?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;simulate_experiments(log_lr, ybounds=[a,b], early_stop=True, mu=0.1, lw=1, n=2100);
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Bounds crossed: 100 (84% upper, 16% lower)
Average experiment duration: 443
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/optimal_stopping_51_1.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;In this case, we make the correct decision in only 84% of the simulations, which is very close to the expected value of 80% (under the alternative hypothesis), i.e. the power of the experiment, 1-β.&lt;/p&gt;
&lt;p&gt;Moreover, also under the alternative hypothesis we need a significantly lower sample size: just 443 observation, on average.h a conclusion in 78/100 experiments we need just 1/3 of the samples!&lt;/p&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;In this post, we have seen the dangers of &lt;strong&gt;peeking&lt;/strong&gt; during a randomized experiment. Prematurely stopping a test can be dangerous since it distorts inference, biasing the expected rejection rates.&lt;/p&gt;
&lt;p&gt;Does it mean that we always need to perform tests with a pre-specified sample size? No! There exist procedures that allow for optimal stopping. These procedures were born for a specific purpose: reducing the sample size as much as possible, without sacrificing accuracy. The first and most known is the Sequential Probability Ratio Test, defined by Wallis as &amp;ldquo;&lt;em&gt;the most powerful and
seminal statistical ideas of the past third of a century&lt;/em&gt;&amp;rdquo; (in 1980).&lt;/p&gt;
&lt;p&gt;The SPRT was not only a powerful tool during war time but keeps being used today for very practical purposes (see for example &lt;a href=&#34;https://netflixtechblog.com/improving-experimentation-efficiency-at-netflix-with-meta-analysis-and-optimal-stopping-d8ec290ae5be&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Netflix&lt;/a&gt;, &lt;a href=&#34;https://eng.uber.com/xp/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Uber&lt;/a&gt;).&lt;/p&gt;
&lt;h3 id=&#34;references&#34;&gt;References&lt;/h3&gt;
&lt;p&gt;[1] A. Wald, &lt;a href=&#34;https://www.jstor.org/stable/2235829&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Sequential tests of statistical hypotheses&lt;/a&gt; (1945), &lt;em&gt;The Annal of Mathematical Statistics&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;[2] A. Wald and J Wolfowitz, &lt;a href=&#34;https://www.jstor.org/stable/2235638&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Optimum character of the sequential probability ratio test&lt;/a&gt; (1948), &lt;em&gt;The Annals of Mathematical Statistics&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;[3] W. A. Wallis, &lt;a href=&#34;https://www.tandfonline.com/doi/abs/10.1080/01621459.1980.10477469&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The Statistical Research Group, 1942–1945&lt;/a&gt; (1980), &lt;em&gt;Journal of the American Statistical Association&lt;/em&gt;.&lt;/p&gt;
&lt;h3 id=&#34;code&#34;&gt;Code&lt;/h3&gt;
&lt;p&gt;You can find the original Jupyter Notebook here:&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/optimal_stopping.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/matteocourthoud/Blog-Posts/blob/main/optimal_stopping.ipynb&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>From Causal Trees to Forests</title>
      <link>https://matteocourthoud.github.io/post/causal_forests/</link>
      <pubDate>Mon, 10 Jul 2023 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/post/causal_forests/</guid>
      <description>&lt;p&gt;&lt;em&gt;How to use regression trees to do policy targeting.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;In my &lt;a href=&#34;https://medium.com/towards-data-science/understanding-causal-trees-920177462149&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;previous blog post&lt;/a&gt;, we have seen how to use &lt;strong&gt;causal trees&lt;/strong&gt; to estimate heterogeneous treatment effects of a policy. If you haven&amp;rsquo;t read it, I recommend starting there first, since we are going to take the content of that article for granted and start from there.&lt;/p&gt;
&lt;p&gt;Why heterogenous treatment effects (HTE)? The estimation of heterogeneous treatments effects is important because it allows us to do &lt;strong&gt;targeting&lt;/strong&gt;. Knowing which customers are more likely to react to a discount allows a company to spend less money by offering fewer but better targeted discounts. This works also for negative effects: knowing for which patients a certain drug has side effects allows a pharmaceutical company to warn or exclude them from the treatment. There is also a more subtle advantage of estimating heterogeneous treatment effects: knowing &lt;strong&gt;for whom&lt;/strong&gt; a treatment works allows us to better understand &lt;strong&gt;how&lt;/strong&gt; a treatment works. Knowing that the effect of a discount does not depend on the income of its recipient but rather by its buying habits  tells us that maybe it is not a matter of money, but rather a matter of attention or loyalty.&lt;/p&gt;
&lt;p&gt;In this article, we will explore an extention of causal trees: causal forests. Exactly as random forests extend regression trees by averaging multiple bootstrapped trees together, causal forests extend causal trees. The main difference comes from the inference perspective, which is less straighforward. We are also going to see how to compare outputs of different HTE estimation algorithms and how to use them for &lt;strong&gt;policy targeting&lt;/strong&gt;.&lt;/p&gt;
&lt;h2 id=&#34;online-discounts&#34;&gt;Online Discounts&lt;/h2&gt;
&lt;p&gt;For the rest of the article, we resume the toy example used in the &lt;a href=&#34;https://medium.com/towards-data-science/understanding-causal-trees-920177462149&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;causal trees article&lt;/a&gt;: we assume we are an &lt;strong&gt;online store&lt;/strong&gt; and we are interested in understanding whether offering discounts to new customers increases their expenditure in the store.&lt;/p&gt;
&lt;img src=&#34;fig/causal_forests1.jpg&#34; width=&#34;300px&#34;/&gt;
&lt;p&gt;To understand whether and how much the discounts are effective we run an &lt;strong&gt;A/B test&lt;/strong&gt;: whenever a new user visits our online store, we randomly decide whether to offer them the discount or not. I import the data-generating process &lt;code&gt;dgp_online_discounts()&lt;/code&gt; from &lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/src/dgp.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;src.dgp&lt;/code&gt;&lt;/a&gt;. I also import some plotting functions and libraries from &lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/src/utils.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;src.utils&lt;/code&gt;&lt;/a&gt;. To include not only code but also data and tables, I use &lt;a href=&#34;https://deepnote.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Deepnote&lt;/a&gt;, a Jupyter-like web-based collaborative notebook environment.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;%matplotlib inline
%config InlineBackend.figure_format = &#39;retina&#39;

from src.utils import *
from src.dgp import dgp_online_discounts
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;dgp = dgp_online_discounts(n=100_000)
df = dgp.generate_data()
df.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;time&lt;/th&gt;
      &lt;th&gt;device&lt;/th&gt;
      &lt;th&gt;browser&lt;/th&gt;
      &lt;th&gt;region&lt;/th&gt;
      &lt;th&gt;discount&lt;/th&gt;
      &lt;th&gt;spend&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;10.78&lt;/td&gt;
      &lt;td&gt;mobile&lt;/td&gt;
      &lt;td&gt;edge&lt;/td&gt;
      &lt;td&gt;9&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0.46&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;0.57&lt;/td&gt;
      &lt;td&gt;desktop&lt;/td&gt;
      &lt;td&gt;firefox&lt;/td&gt;
      &lt;td&gt;9&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;11.04&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;3.74&lt;/td&gt;
      &lt;td&gt;mobile&lt;/td&gt;
      &lt;td&gt;safari&lt;/td&gt;
      &lt;td&gt;7&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1.81&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;13.37&lt;/td&gt;
      &lt;td&gt;desktop&lt;/td&gt;
      &lt;td&gt;other&lt;/td&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;31.90&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;0.71&lt;/td&gt;
      &lt;td&gt;mobile&lt;/td&gt;
      &lt;td&gt;explorer&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;15.42&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;We have data on 100.000 store visitors, for whom we observe the &lt;code&gt;time&lt;/code&gt; of the day the acessed the website, the &lt;code&gt;device&lt;/code&gt; they use, their &lt;code&gt;browser&lt;/code&gt;, and their geographical &lt;code&gt;region&lt;/code&gt;. We also see whether they were offered the &lt;code&gt;discount&lt;/code&gt;, our treatment, and what is their &lt;code&gt;spend&lt;/code&gt;, the outcome of interest.&lt;/p&gt;
&lt;p&gt;Since the treatment was randomly assigned, we can use a simple &lt;strong&gt;difference-in-means&lt;/strong&gt; estimator to estimate the treatment effect. We expect the treatment and control group to be similar, except for the &lt;code&gt;discount&lt;/code&gt;, therefore we can causally attribute any difference in &lt;code&gt;spend&lt;/code&gt; to the &lt;code&gt;discount&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;smf.ols(&#39;spend ~ discount&#39;, df).fit().summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
      &lt;td&gt;&lt;/td&gt;         &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt; &lt;td&gt;    5.0306&lt;/td&gt; &lt;td&gt;    0.045&lt;/td&gt; &lt;td&gt;  110.772&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    4.942&lt;/td&gt; &lt;td&gt;    5.120&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;discount&lt;/th&gt;  &lt;td&gt;    1.9492&lt;/td&gt; &lt;td&gt;    0.064&lt;/td&gt; &lt;td&gt;   30.346&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    1.823&lt;/td&gt; &lt;td&gt;    2.075&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;The discount seems to be effective: on average the spend in the treatment group increases by 1.95$. But are all customers equally affected?&lt;/p&gt;
&lt;p&gt;To answer this question, we would like to estimate &lt;strong&gt;heterogeneous treatment effects&lt;/strong&gt;, possibly at the individual level.&lt;/p&gt;
&lt;h2 id=&#34;causal-forests&#34;&gt;Causal Forests&lt;/h2&gt;
&lt;p&gt;There are many different options to compute heterogeneous treatment effects. The simplest one is to interact the outcome of interest with a dimension of heterogeneity. The problem with this approach is which variable to pick. Sometimes we have prior information that might guide out actions; for example, we might know that &lt;code&gt;mobile&lt;/code&gt; users on average spend more than &lt;code&gt;desktop&lt;/code&gt; users. Other times, we might be interested in one dimension for business reasons; for example we might want to invest more in a certain &lt;code&gt;region&lt;/code&gt;. However, when we do not extra information we would like this process to be data-driven.&lt;/p&gt;
&lt;p&gt;In the &lt;a href=&#34;https://medium.com/towards-data-science/understanding-causal-trees-920177462149&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;previous article&lt;/a&gt; we have explored one data-drive approach to estimate heterogeneous treatment effects: &lt;strong&gt;causal trees&lt;/strong&gt;. We will now expand them to causal forests. However, before we start, we have to give an introduction to its non-causal cousing: random forests.&lt;/p&gt;
&lt;img src=&#34;fig/causal_forests2.jpg&#34; width=&#34;300px&#34;/&gt;
&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Random_forest&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Random forests&lt;/strong&gt;&lt;/a&gt;, as the name suggests, are an extension of regression trees, adding two separate sources of randomness of top of them. In particular, a random forest algorithm takes the predictions of many different regression trees, each trained on a bootstrapped sample of the data, and averages them together. This procedure is generally known as &lt;a href=&#34;https://en.wikipedia.org/wiki/Bootstrap_aggregating&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;bagging&lt;/strong&gt;&lt;/a&gt;, boostrap-aggregating, and can be applied to any prediction algorithm and is not specific to random forest. The additional source of randomness comes from feature selection since at each split, only a random subset of all the features $X$ is considered for the optimal split.&lt;/p&gt;
&lt;p&gt;These two extra sources of randomness are extremely important and controbute to a superior performance of random forests. First of all, bagging allows random forests to &lt;strong&gt;produce smoother&lt;/strong&gt; prediction than regression trees by averaging multiple discrete predictions. Random feature selection instead allows random forests to &lt;strong&gt;explore the feature space&lt;/strong&gt; more in depth, allowing it to discover more interations than simple regression trees. In fact, there might be interactions between variables that are on their own not very predictive (and therefore would not generate splits) but together very powerful.&lt;/p&gt;
&lt;p&gt;Causal Forests are the equivalent of random forests, but for the estimation of heterogeneous treatment effects, exaxtly as for causal trees and regression trees. Exactly as for Causal Trees, we have a fundamental problem: we are interested in predicting an object that we do not observe: the individua treatment effects $\tau_i$. The solution is to create an auxiliary outcome variable $Y^*$ whose expected value for each single observation is exactly the treatment effect.&lt;/p&gt;
&lt;p&gt;$$
Y_i^* = \frac{Y_i}{D_i \cdot p(X_i) - (1-D_i) \cdot (1-p(X_i))}
$$&lt;/p&gt;
&lt;p&gt;If you want to know more details on why this variable is unbiased for the individual treatment effect, have a look at my &lt;a href=&#34;https://towardsdatascience.com/920177462149&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;previous post&lt;/a&gt; where I go more in detail. In short, you can interpret $Y_i^*$ as the difference-in-means estimator for a single observation.&lt;/p&gt;
&lt;p&gt;Once we have an outcome variable, there are still a couple of things we need to do in order to use Random Forests to estimate heterogeneous treatment effects. First, we need to build trees that have an equal number of treated and control units in each leaf. Second, we need to use different samples to build the tree and evaluate it, i.e. compute the average outcome per leaf. This procedure is often referred to as &lt;strong&gt;honest trees&lt;/strong&gt; and it&amp;rsquo;s extremely helpful for inference, since we can treat the sample of each leaf as independent from the tree structure.&lt;/p&gt;
&lt;p&gt;Before we go into the estimation, let&amp;rsquo;s first generate dummy variables for our categorical variables, &lt;code&gt;device&lt;/code&gt;, &lt;code&gt;browser&lt;/code&gt; and &lt;code&gt;region&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_dummies = pd.get_dummies(df[dgp.X[1:]], drop_first=True)
df = pd.concat([df, df_dummies], axis=1)
X = [&#39;time&#39;] + list(df_dummies.columns)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can now estimate the heterogeneous treatment effects using the Random Forest algorithm. Luckily, we don&amp;rsquo;t have to do all this by hand, but there is a great implementation of Causal Trees and Forests in Microsoft&amp;rsquo;s &lt;a href=&#34;https://econml.azurewebsites.net/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;EconML&lt;/a&gt; package. We will use the &lt;code&gt;CausalForestDML&lt;/code&gt; function. We set a seed for reproducibility.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from econml.dml import CausalForestDML

np.random.seed(0)
forest_model = CausalForestDML(max_depth=3)
forest_model = forest_model.fit(Y=df[dgp.Y], X=df[X], T=df[dgp.D])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Differently from Causal Trees, Causal Forests are harder to interpret since we cannot visualize every single tree. We can use the &lt;code&gt;SingleTreeCateInterpreter&lt;/code&gt; function to plot an equivalent representation of the Causal Forest algorithm.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from econml.cate_interpreter import SingleTreeCateInterpreter
%matplotlib inline

intrp = SingleTreeCateInterpreter(max_depth=2).interpret(forest_model, df[X])
intrp.plot(feature_names=X, fontsize=12)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/causal_forests_20_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;We can interpret the tree diagram exactly as for the Causal Tree model. On the top, we can see the average $Y^*$ in the data, $1.917$. Starting from there, the data gets split into different branches, according to the rules highlighted at the top of each node. For example, the first node splits the data into two groups of size $46878$ and $53122$ depending on whether the &lt;code&gt;time&lt;/code&gt; is later than $11.295$. At the bottom, we have our final partitions, with the predicted values. For example, the leftmost leaf contains $40191$ observation with &lt;code&gt;time&lt;/code&gt; earlier than $11.295$ and non-Safari &lt;code&gt;browser&lt;/code&gt;, for which we predict a spend of $0.264$. Darker node colors indicate higher prediction values.&lt;/p&gt;
&lt;p&gt;The problem with this representation is that, differently from the case of Causal Trees, it is only an interpretation of the model. Since Causal Forests are made of many bootstrapped trees, there is no way to directly inspect each decision tree. One way to understand which feature is most important in detemining the tree split is the so-called &lt;a href=&#34;https://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_importances.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;feature importance&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;fig, ax = plt.subplots()
sns.barplot(x=X, y=forest_model.feature_importances()[0], color=&#39;C0&#39;).set(
    title=&#39;Feature Importances&#39;, ylabel=&#39;Importance&#39;)
ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha=&amp;quot;right&amp;quot;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/causal_forests_22_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Clearly &lt;code&gt;time&lt;/code&gt; is the first dimension of heterogeneity, followed by &lt;code&gt;device&lt;/code&gt; (mobile in particular) and &lt;code&gt;browser&lt;/code&gt; (safari in particular). Other dimensions do not matter much.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s now check the model performance.&lt;/p&gt;
&lt;h3 id=&#34;performance&#34;&gt;Performance&lt;/h3&gt;
&lt;p&gt;Since we control the data generating process, we can do something that is not possible with real data: check the predicted treatment effects against the true ones. The &lt;code&gt;generate_potential_outcomes()&lt;/code&gt; function loads the data with both potential outcomes for each observation, under both treatment (&lt;code&gt;outcome_t&lt;/code&gt;) and control (&lt;code&gt;outcome_c&lt;/code&gt;). Let&amp;rsquo;s start first by evaluating how well the algorithm predicts the effects along the discrete dimensions of the data.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def compute_discrete_effects(df, hte_model):
    temp_df = df.copy()
    temp_df.time = 0
    temp_df = dgp.add_treatment_effect(temp_df)
    temp_df = temp_df.rename(columns={&#39;effect_on_spend&#39;: &#39;True&#39;})
    temp_df[&#39;Predicted&#39;] = hte_model.effect(temp_df[X])
    df_effects = pd.DataFrame()
    for var in X[1:]:
        for effect in [&#39;True&#39;, &#39;Predicted&#39;]:
            v = temp_df.loc[temp_df[var]==1, effect].mean() - temp_df[effect][temp_df[var]==0].mean()
            effect_var = {&#39;Variable&#39;: [var], &#39;Effect&#39;: [effect], &#39;Value&#39;: [v]}
            df_effects = pd.concat([df_effects, pd.DataFrame(effect_var)]).reset_index(drop=True)
    return df_effects, temp_df[&#39;Predicted&#39;].mean()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_effects, avg_effect_notime = compute_discrete_effects(df, forest_model)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;fig, ax = plt.subplots()
sns.barplot(data=df_effects, x=&amp;quot;Variable&amp;quot;, y=&amp;quot;Value&amp;quot;, hue=&amp;quot;Effect&amp;quot;, ax=ax).set(
    xlabel=&#39;&#39;, ylabel=&#39;&#39;, title=&#39;Heterogeneous Treatment Effects&#39;)
ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha=&amp;quot;right&amp;quot;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/causal_forests_28_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;The Causal Forest algorithm is pretty good at predicting the treatment effects related to the categorical variables. As for Causal Trees, this is expected since the algorithm has a very discrete nature. However, differently from Causal Trees, the predictions are more nuanced.&lt;/p&gt;
&lt;p&gt;We can now do a more relevant test: how well the algorithm performs with a continuous variable such as &lt;code&gt;time&lt;/code&gt;? First, let&amp;rsquo;s again isolate the predicted treatment effects on &lt;code&gt;time&lt;/code&gt; and ignore the other covariates.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def compute_time_effect(df, hte_model, avg_effect_notime):
    df_time = df.copy()
    df_time[[X[1:]] + [&#39;device&#39;, &#39;browser&#39;, &#39;region&#39;]] = 0
    df_time = dgp.add_treatment_effect(df_time)
    df_time[&#39;predicted&#39;] = hte_model.effect(df_time[X]) + avg_effect_notime
    return df_time
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_time = compute_time_effect(df, forest_model, avg_effect_notime)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We now plot the predicted treatment effects against the true ones, along the &lt;code&gt;time&lt;/code&gt; dimension.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sns.scatterplot(x=&#39;time&#39;, y=&#39;effect_on_spend&#39;, data=df_time, label=&#39;True&#39;)
sns.scatterplot(x=&#39;time&#39;, y=&#39;predicted&#39;, data=df_time, label=&#39;Predicted&#39;).set(
    ylabel=&#39;&#39;, title=&#39;Heterogeneous Treatment Effects&#39;)
plt.legend(title=&#39;Effect&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/causal_forests_33_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;We can now fully appreciate the difference between Causal Trees and Forests: while in the case of Causal Trees the estimates were essentially a very coarse step function, we can now see how Causal Forests produce &lt;strong&gt;smoother estimates&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;We have now explored the model, it&amp;rsquo;s time to use it!&lt;/p&gt;
&lt;h2 id=&#34;policy&#34;&gt;Policy&lt;/h2&gt;
&lt;p&gt;Suppose that we were considering offering a 4$ discount to new customers that visit our online store.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;cost = 4
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For which customers is the discount effective? We have estimated an average treatment effect of 1.9492$ which means that the discount is not really profitable on average. However, we are now able to target single individuals and we can offer the discount only to a subset of the incoming customers. We will now explore how to do &lt;strong&gt;policy targeting&lt;/strong&gt; and in order to get a better understanding of the quality of the targeting, we will use the Causal Tree model as a reference point.&lt;/p&gt;
&lt;p&gt;We build a Causal Tree using the same &lt;code&gt;CausalForestDML&lt;/code&gt; function but restricting the number of estimators and the forest size to 1.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from econml.dml import CausalForestDML

np.random.seed(0)
tree_model = CausalForestDML(n_estimators=1, subforest_size=1, inference=False, max_depth=3)
tree_model = tree_model.fit(Y=df[dgp.Y], X=df[X], T=df[dgp.D])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next, we split the dataset into a train and a test set. The idea is very similar to &lt;a href=&#34;https://en.wikipedia.org/wiki/Cross-validation_%28statistics%29&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;cross-validation&lt;/strong&gt;&lt;/a&gt;: we use the training set to train the model - in our case the estimator for the heterogeneous treatment effects - and the test set to assess its quality. The main difference is that we do not observe the true outcome in the test dataset. But we can still use the train-test split to compare in-sample predictions with out-of-sample predictions.&lt;/p&gt;
&lt;p&gt;We put 80% of all observations in the training set and 20% in the test set.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_train, df_test = df.iloc[:80_000, :], df.iloc[20_000:,]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;First, let&amp;rsquo;s retrain the models only on the training sample.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;np.random.seed(0)
tree_model = tree_model.fit(Y=df_train[dgp.Y], X=df_train[X], T=df_train[dgp.D])
forest_model = forest_model.fit(Y=df_train[dgp.Y], X=df_train[X], T=df_train[dgp.D])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we can decide on a targeting policy, i.e. decide to which customers we offer the discount. The answer seems simple: we offer the discount to all the customers for whom we anticipate a treatment effect larger than the cost, 4$.&lt;/p&gt;
&lt;p&gt;A visualization tool that allows us to understand on whom the treatment is effective and how, is the so-called &lt;strong&gt;Treatment Operative Characteristic (TOC)&lt;/strong&gt; curve. The name is remindful of the much more famous &lt;a href=&#34;https://en.wikipedia.org/wiki/Receiver_operating_characteristic&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;receiver operating characteristic (ROC)&lt;/a&gt; curve that plots the true positive rate against the false positive rate for different thresholds of a binary classifier. The idea is similar: we plot the average treatment effect for different shares of the treated population. At one extreme, when all customers are treated, and the curve takes value equal to the average treatement effect, while at the other extreme, when only one customer is treated, and the curve takes value equal to the maximum treatment effect.&lt;/p&gt;
&lt;p&gt;Now let&amp;rsquo;s compute the curve.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def compute_toc(df, hte_model, cost, truth=False):
    df_toc = pd.DataFrame()
    for q in np.linspace(0, 1, 101):
        if truth:
            df = dgp.add_treatment_effect(df_test)
            effect = df[&#39;effect_on_spend&#39;]
        else:
            effect = hte_model.effect(df[X])
        ate = np.mean(effect[effect &amp;gt;= np.quantile(effect, 1-q)])
        temp = pd.DataFrame({&#39;q&#39;: [q], &#39;ate&#39;: [ate]})
        df_toc = pd.concat([df_toc, temp]).reset_index(drop=True)
    return df_toc
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_toc_tree = compute_toc(df_train, tree_model, cost)
df_toc_forest = compute_toc(df_train, forest_model, cost)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we can plot the Treatment Operating Curves for the two CATE estimators.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def plot_toc(df_toc, cost, ax, color, title):
    ax.axhline(y=cost, lw=2, c=&#39;k&#39;)
    ax.fill_between(x=df_toc.q, y1=cost, y2=df_toc.ate, where=(df_toc.ate &amp;gt; cost), color=color, alpha=0.3)
    if any(df_toc.ate &amp;gt; cost):
        q = df_toc_tree.loc[df_toc.ate &amp;gt; cost, &#39;q&#39;].values[-1]
    else: 
        q = 0
    ax.axvline(x=q, ymin=0, ymax=0.36, lw=2, c=&#39;k&#39;, ls=&#39;--&#39;)
    sns.lineplot(data=df_toc, x=&#39;q&#39;, y=&#39;ate&#39;, ax=ax, color=color).set(
        title=title, ylabel=&#39;ATT&#39;, xlabel=&#39;Share of treated&#39;, ylim=[1.5, 8.5]) 
    ax.text(0.7, cost+0.1, f&#39;Discount cost: {cost:.0f}$&#39;, fontsize=12)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;fix, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))
plot_toc(df_toc_tree, cost, ax1, &#39;C0&#39;, &#39;TOC - Causal Tree&#39;)
plot_toc(df_toc_forest, cost, ax2, &#39;C1&#39;, &#39;TOC - Causal Forest&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/causal_forests_49_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;As expected, the TOC curve is decreasing for both estimators since the average effect decreases as we increase the share of treated customers. In other words, the more selective we are in releasing discounts, the higher the effect of the coupon, per customer. I have also plotted an horizontal line with the discount cost so that we can interpret the shaded area below the TOC curve and above the cost line as the &lt;strong&gt;expected profits&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;The two algorims predict a similar share of treated, around 20%, with the Causal Forest algorithm targeting slightly more customers. However, they predict very different profits. The Causal Tree algorithm predicts a small and constant margin, while the Causal Forest algorithm predicts a larger and steeper margin. Which algorithm is more accurate?&lt;/p&gt;
&lt;p&gt;In order to compare them, we can evaluate them in the test set. We take the model trained on the training set, we predict the treatment effects and we compare them with the predictions from a model trained on the test set. Note that, differently from machine learning standard testing procedures, there is a substantial &lt;strong&gt;difference&lt;/strong&gt;: in our case, we cannot evaluate our predictions against the ground truth, since the treatment effects are not observed. We can only compare two predictions with each other.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def compute_effect_test(df_test, hte_model, cost, ax, title, truth=False):
    df_test[&#39;Treated&#39;] = hte_model.effect(df_test[X]) &amp;gt; cost
    if truth:
        df_test = dgp.add_treatment_effect(df_test)
        df_test[&#39;Effect&#39;] = df_test[&#39;effect_on_spend&#39;]
    else:
        np.random.seed(0)
        hte_model_test = copy.deepcopy(hte_model).fit(Y=df_test[dgp.Y], X=df_test[X], T=df_test[dgp.D])
        df_test[&#39;Effect&#39;] = hte_model_test.effect(df_test[X])
    df_test[&#39;Cost Effective&#39;] = df_test[&#39;Effect&#39;] &amp;gt; cost
    tot_effect = ((df_test[&#39;Effect&#39;] - cost) * df_test[&#39;Treated&#39;]).sum()
    sns.barplot(data=df_test, x=&#39;Cost Effective&#39;, y=&#39;Treated&#39;, errorbar=None, width=0.5, ax=ax, palette=[&#39;C3&#39;, &#39;C2&#39;]).set(
        title=title + &#39;\n&#39;, ylim=[0,1])
    ax.text(0.5, 1.08, f&#39;Total effect: {tot_effect:.2f}&#39;, fontsize=14, ha=&#39;center&#39;)
    return 
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;fix, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))
compute_effect_test(df_test, tree_model, cost, ax1, &#39;Causal Tree&#39;)
compute_effect_test(df_test, forest_model, cost, ax2, &#39;Causal Forest&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/causal_forests_52_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;It seems that the Causal Tree model performs better than the Causal Forest model, with a total net effect of $8386$$ against $4948$$. From the plot we can also understand the source of the discrepancy. The Causal Forest algorithm  tends to be more restrictive and treat fewer customers, making no false positives but also having a lot of false negatives. On the other hand, the Causal Tree algorithm, is much more generous and distributes the &lt;code&gt;discount&lt;/code&gt; to mamy more new customers. This translates in both more true positives but also false positives. The net effect seem to favor the causal tree algorithm.&lt;/p&gt;
&lt;p&gt;Normally, we would stop here since there is not much more we can do. However, in our case, we have access to the &lt;strong&gt;true data generating process&lt;/strong&gt;. Therefore we can check the ground-truth accuracy of the two algorithms.&lt;/p&gt;
&lt;p&gt;First, let&amp;rsquo;s compare them in terms of prediction error of the treatment effects. For each algorithm we compute the &lt;a href=&#34;https://en.wikipedia.org/wiki/Mean_squared_error&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;mean squared error&lt;/a&gt; of the treatment effects.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from sklearn.metrics import mean_squared_error as mse

def compute_mse_test(df_test, hte_model):
    df_test = dgp.add_treatment_effect(df_test)
    print(f&amp;quot;MSE = {mse(df_test[&#39;effect_on_spend&#39;], hte_model.effect(df_test[X])):.4f}&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;compute_mse_test(df_test, tree_model)
compute_mse_test(df_test, forest_model)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;MSE = 0.9035
MSE = 0.5555
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The Random Forest model better predicts the average treatment effect, with a mean squared error of $0.5555$ instead of $0.9035$.&lt;/p&gt;
&lt;p&gt;Does this map into a &lt;strong&gt;better targeting&lt;/strong&gt;? We can now replicate the same barplot we did above, to understand how well the two algorithms perform in terms of policy targeting.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;fix, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))
compute_effect_test(df_test, tree_model, cost, ax1, &#39;Causal Tree&#39;, True)
compute_effect_test(df_test, forest_model, cost, ax2, &#39;Causal Forest&#39;, True)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/causal_forests_57_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;The plot is very similar, but the result differ substantially. In fact, the Causal Forest algorithm now outperforms the Causal Tree algorithm with a total effect of $10395$$ compared to $8828$$. Why this sudden difference?&lt;/p&gt;
&lt;p&gt;To better understand the source of the discrepancy let&amp;rsquo;s plot the TOC based on the ground truth.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_toc = compute_toc(df_test, tree_model, cost, True)

fix, ax = plt.subplots(1, 1, figsize=(7, 5))
plot_toc(df_toc, cost, ax, &#39;C2&#39;, &#39;TOC - Ground Truth&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/causal_forests_59_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;As we can see, the TOC is very skewed and there exist a few customers with very high average treatment effects. The Random Forest algorothm is better able to indentify them and therefore is overall more effective, despite targeting fewer customers.&lt;/p&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;In this post, we have seen a very powerful algorithm for the estimation of heterogeneous treatment effects: &lt;strong&gt;causal forests&lt;/strong&gt;. Causal forests are built on the same principle of causal trees, but benefit from a much deeper exploration of the parameter space and bagging.&lt;/p&gt;
&lt;p&gt;We have also seen how to use the estimates of the heterogeneous treatment effects to perform policy &lt;strong&gt;targeting&lt;/strong&gt;. By identifying users with the highest treatment effects, we are able to make profitable a policy that wouldn&amp;rsquo;t be otherwise. We have also see how the objective of policy targeting might differ from the objective of heterogeneous treatment effect estimation since the tails of the distribution might be more relevant than the average.&lt;/p&gt;
&lt;h3 id=&#34;references&#34;&gt;References&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;S. Athey, G. Imbens, &lt;a href=&#34;https://www.pnas.org/doi/abs/10.1073/pnas.1510489113&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Recursive partitioning for heterogeneous causal effects&lt;/a&gt; (2016), &lt;em&gt;PNAS&lt;/em&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;S. Wager, S. Athey, &lt;a href=&#34;https://www.tandfonline.com/doi/full/10.1080/01621459.2017.1319839&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Estimation and Inference of Heterogeneous Treatment Effects using Random Forests&lt;/a&gt; (2018), &lt;em&gt;Journal of the American Statistical Association&lt;/em&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;S. Athey, J. Tibshirani, S. Wager, &lt;a href=&#34;https://projecteuclid.org/journals/annals-of-statistics/volume-47/issue-2/Generalized-random-forests/10.1214/18-AOS1709.full&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Generalized Random Forests&lt;/a&gt; (2019). &lt;em&gt;The Annals of Statistics&lt;/em&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;M. Oprescu, V. Syrgkanis, Z. Wu, &lt;a href=&#34;http://proceedings.mlr.press/v97/oprescu19a.html?ref=https://githubhelp.com&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Orthogonal Random Forest for Causal Inference&lt;/a&gt; (2019). &lt;em&gt;Proceedings of the 36th International Conference on Machine Learning&lt;/em&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;related-articles&#34;&gt;Related Articles&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://towardsdatascience.com/b63dc69e3d8c&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;DAGs and Control Variables&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://towardsdatascience.com/99bf5cffa0d9&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Matching, Weighting, or Regression?&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://towardsdatascience.com/8a9c1e340832&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Understanding Meta Learners&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://towardsdatascience.com/ed4097dab27a&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Understanding AIPW, the Doubly-Robust Estimator&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://towardsdatascience.com/920177462149&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Understanding Causal Trees&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;code&#34;&gt;Code&lt;/h3&gt;
&lt;p&gt;You can find the original Jupyter Notebook here:&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/causal_forests.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/causal_forests.ipynb&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Goodbye Scatterplot, Welcome Binned Scatterplot</title>
      <link>https://matteocourthoud.github.io/post/binned_scatterplot/</link>
      <pubDate>Mon, 10 Jul 2023 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/post/binned_scatterplot/</guid>
      <description>&lt;p&gt;When we want to visualize the relationship between two continuous variables, the go-to plot is the &lt;strong&gt;scatterplot&lt;/strong&gt;. It&amp;rsquo;s a very intuitive visualization tool that allows us to directly look at the data. However, when we have a lot of data and/or when the data is skewed, scatterplots can be too noisy to be informative.&lt;/p&gt;
&lt;p&gt;In this blog post, I am going to review a very powerful alternative to the scatterplot to visualize correlations between two variables: the &lt;strong&gt;binned scatterplot&lt;/strong&gt;. Binned scatterplots are not only a great visualization tool, but they can also be used to do inference on the conditional distribution of the dependent variable.&lt;/p&gt;
&lt;h2 id=&#34;the-scatterplot&#34;&gt;The Scatterplot&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s start with an example. Suppose we are an &lt;strong&gt;online marketplace&lt;/strong&gt; where multiple firms offer goods that consumer can efficiently browse, compare and buy. Our &lt;strong&gt;dataset&lt;/strong&gt; consists in a snapshot of the firms active on the marketplace.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s load the data and have a look at it. You can find the code for the data generating process &lt;a href=&#34;&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;%matplotlib inline
%config InlineBackend.figure_format = &#39;retina&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from src.utils import *
from src.dgp import dgp_marketplace

df = dgp_marketplace().generate_data(N=10_000)
df.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;age&lt;/th&gt;
      &lt;th&gt;sales&lt;/th&gt;
      &lt;th&gt;online&lt;/th&gt;
      &lt;th&gt;products&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;0.312777&lt;/td&gt;
      &lt;td&gt;450.858091&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;1.176221&lt;/td&gt;
      &lt;td&gt;1121.882449&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;1.764048&lt;/td&gt;
      &lt;td&gt;2698.714549&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;1.082742&lt;/td&gt;
      &lt;td&gt;1627.746386&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;3.156503&lt;/td&gt;
      &lt;td&gt;1464.593939&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;We have information on 10.000 firms. For each firm we know:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;age&lt;/code&gt;: the age of the firm&lt;/li&gt;
&lt;li&gt;&lt;code&gt;sales&lt;/code&gt;: the monthly sales from last month&lt;/li&gt;
&lt;li&gt;&lt;code&gt;online&lt;/code&gt;: whether the firm is only active online&lt;/li&gt;
&lt;li&gt;&lt;code&gt;products&lt;/code&gt;: the number of products that the firm offers&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Suppose we are interested in understanding the relationship between &lt;code&gt;age&lt;/code&gt; and &lt;code&gt;sales&lt;/code&gt;. What is the &lt;strong&gt;life-cycle&lt;/strong&gt; of sales?&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s start with a simple &lt;strong&gt;scatterplot&lt;/strong&gt; of &lt;code&gt;sales&lt;/code&gt; over &lt;code&gt;age&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sns.scatterplot(x=&#39;age&#39;, y=&#39;sales&#39;, data=df);
plt.title(&amp;quot;Sales by firm&#39;s age&amp;quot;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/binned_scatterplot_7_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;The plot is extremely &lt;strong&gt;noisy&lt;/strong&gt;. We have a lot of observations, therefore, it is very difficult to visualize them all. If we had to guess, we could say that the relationship looks negative (&lt;code&gt;sales&lt;/code&gt; decrease with &lt;code&gt;age&lt;/code&gt;), but it would be a very uninformed guess.&lt;/p&gt;
&lt;p&gt;We are now going to explore some plausible tweaks and alternatives.&lt;/p&gt;
&lt;h2 id=&#34;scatterplot-alternatives&#34;&gt;Scatterplot Alternatives&lt;/h2&gt;
&lt;p&gt;What can we do when we have an extremely dense scatterplot? One solution could be to plot the &lt;strong&gt;density&lt;/strong&gt; of the observations, instead of the observations themselves.&lt;/p&gt;
&lt;p&gt;There are multiple solutions in Python to visualize the density of a 2-dimensional distribution. A very useful one is &lt;a href=&#34;https://seaborn.pydata.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;seaborn&lt;/a&gt; &lt;a href=&#34;https://seaborn.pydata.org/generated/seaborn.jointplot.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;jointplot&lt;/a&gt;. &lt;code&gt;jointplot&lt;/code&gt; plots the joint distribution of two variables, together with the marginal distributions along the axis. The default option is the scatterplot, but one can also choose to add a regression line (&lt;code&gt;reg&lt;/code&gt;), change the plot to a histogram (&lt;code&gt;hist&lt;/code&gt;), a hexplot (&lt;code&gt;hex&lt;/code&gt;), or a &lt;a href=&#34;https://en.wikipedia.org/wiki/Kernel_density_estimation&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;kernel density estimate&lt;/a&gt; (&lt;code&gt;kde&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s try the &lt;strong&gt;hexplot&lt;/strong&gt;, which is basically a histogram of the data, where the bins are hexagons, in the 2-dimensional space.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;s = sns.jointplot(x=&#39;age&#39;, y=&#39;sales&#39;, data=df, kind=&#39;hex&#39;, );
s.ax_joint.grid(False);
s.ax_marg_y.grid(False);
s.fig.suptitle(&amp;quot;Sales by firm&#39;s age&amp;quot;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/binned_scatterplot_11_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Not much has changed. It looks like the distributions of &lt;code&gt;age&lt;/code&gt; and &lt;code&gt;sales&lt;/code&gt; are both very &lt;strong&gt;skewed&lt;/strong&gt; and, therefore, most of the action is concentrated in a very small subspace.&lt;/p&gt;
&lt;p&gt;Maybe we could remove &lt;strong&gt;outliers&lt;/strong&gt; and zoom-in on the area where most of the data is located. Let&amp;rsquo;s zoom-in on the bottom-left corner, on observations what have &lt;code&gt;age &amp;lt; 3&lt;/code&gt; and &lt;code&gt;sales &amp;lt; 3000&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;s = sns.jointplot(x=&#39;age&#39;, y=&#39;sales&#39;, data=df.query(&amp;quot;age &amp;lt; 3 &amp;amp; sales &amp;lt; 3000&amp;quot;), kind=&amp;quot;hex&amp;quot;);
s.ax_joint.grid(False);
s.ax_marg_y.grid(False);
s.fig.suptitle(&amp;quot;Sales by firm&#39;s age&amp;quot;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/binned_scatterplot_13_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Now there is much less empty space, but it does not look like we are going far. The joint distribution is &lt;strong&gt;still too skewed&lt;/strong&gt;. This is the case when the data follows some power distribution, as it&amp;rsquo;s often the case with business data.&lt;/p&gt;
&lt;p&gt;One solution is to &lt;strong&gt;transform&lt;/strong&gt; the variable, by taking the &lt;a href=&#34;https://en.wikipedia.org/wiki/Natural_logarithm&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;natural logarithm&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df[&#39;log_age&#39;] = np.log(df[&#39;age&#39;])
df[&#39;log_sales&#39;] = np.log(df[&#39;sales&#39;])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can now plot the relationship between the logarithms of &lt;code&gt;age&lt;/code&gt; and &lt;code&gt;sales&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;s = sns.jointplot(x=&#39;log_age&#39;, y=&#39;log_sales&#39;, data=df, kind=&#39;hex&#39;);
s.ax_joint.grid(False);
s.ax_marg_y.grid(False);
s.fig.suptitle(&amp;quot;Sales by firm&#39;s age&amp;quot;, y=1.02);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/binned_scatterplot_17_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;The logarithm definitely helped. Now the data is more spread across space, which means that the visualization is more informative. Moreover, it looks like there is &lt;strong&gt;no relationship&lt;/strong&gt; between the two variables.&lt;/p&gt;
&lt;p&gt;However, there is still &lt;strong&gt;too much noise&lt;/strong&gt;. Maybe data visualization alone is not sufficient do draw a conclusion.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s swap to a more structured approach: &lt;a href=&#34;https://en.wikipedia.org/wiki/Linear_regression&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;linear regression&lt;/strong&gt;&lt;/a&gt;. Let&amp;rsquo;s linearly regress &lt;code&gt;log_sales&lt;/code&gt; on &lt;code&gt;log_age&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;smf.ols(&#39;log_sales ~ log_age&#39;, df).fit().summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
      &lt;td&gt;&lt;/td&gt;         &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt; &lt;td&gt;    7.3971&lt;/td&gt; &lt;td&gt;    0.015&lt;/td&gt; &lt;td&gt;  478.948&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    7.367&lt;/td&gt; &lt;td&gt;    7.427&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;log_age&lt;/th&gt;   &lt;td&gt;    0.1690&lt;/td&gt; &lt;td&gt;    0.010&lt;/td&gt; &lt;td&gt;   16.888&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    0.149&lt;/td&gt; &lt;td&gt;    0.189&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;The regression coefficient for &lt;code&gt;log_age&lt;/code&gt; is &lt;strong&gt;positive&lt;/strong&gt; and statistically significant (i.e. different from zero). It seems that all previous visualizations were very &lt;strong&gt;misleading&lt;/strong&gt;. From none of the graphs above we could have guessed such a strong positive relationship.&lt;/p&gt;
&lt;p&gt;However, maybe this relationship is different for &lt;code&gt;online&lt;/code&gt;-only firms and the rest of the sample. We need to control for this variable in order to avoid &lt;a href=&#34;https://en.wikipedia.org/wiki/Simpson%27s_paradox&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Simpson&amp;rsquo;s Paradox&lt;/a&gt; and, more generally, bias.&lt;/p&gt;
&lt;p&gt;With linear regression, we can &lt;strong&gt;condition the analysis on covariates&lt;/strong&gt;. Let&amp;rsquo;s add the binary indicator for &lt;code&gt;online&lt;/code&gt;-only firms and the variable counting the number of &lt;code&gt;products&lt;/code&gt; to the regression.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;smf.ols(&#39;log_sales ~ log_age + online + products&#39;, df).fit().summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
      &lt;td&gt;&lt;/td&gt;         &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt; &lt;td&gt;    6.5717&lt;/td&gt; &lt;td&gt;    0.037&lt;/td&gt; &lt;td&gt;  176.893&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    6.499&lt;/td&gt; &lt;td&gt;    6.644&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;log_age&lt;/th&gt;   &lt;td&gt;    0.0807&lt;/td&gt; &lt;td&gt;    0.010&lt;/td&gt; &lt;td&gt;    7.782&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    0.060&lt;/td&gt; &lt;td&gt;    0.101&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;online&lt;/th&gt;    &lt;td&gt;    0.1447&lt;/td&gt; &lt;td&gt;    0.027&lt;/td&gt; &lt;td&gt;    5.433&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    0.092&lt;/td&gt; &lt;td&gt;    0.197&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;products&lt;/th&gt;  &lt;td&gt;    0.3456&lt;/td&gt; &lt;td&gt;    0.014&lt;/td&gt; &lt;td&gt;   24.110&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    0.317&lt;/td&gt; &lt;td&gt;    0.374&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;The coefficient for &lt;code&gt;log_age&lt;/code&gt; is still positive and statistically significant, but its &lt;strong&gt;magnitude&lt;/strong&gt; has halved.&lt;/p&gt;
&lt;p&gt;What should we conclude? It seems that &lt;code&gt;sales&lt;/code&gt; increase over age, on average. However, this pattern might be very &lt;strong&gt;non-linear&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Within the linear regression framework, one approach could be to &lt;strong&gt;add extra terms&lt;/strong&gt; such as polynomials (&lt;code&gt;age^2&lt;/code&gt;) or categorical features (e.g. &lt;code&gt;age &amp;lt; 2&lt;/code&gt;). However, it would be really cool if there was a more &lt;strong&gt;flexible&lt;/strong&gt; (i.e. &lt;a href=&#34;https://en.wikipedia.org/wiki/Nonparametric_statistics&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;non-parametric&lt;/a&gt;) approach that could inform us on the relationship between firm &lt;code&gt;age&lt;/code&gt; and &lt;code&gt;sales&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;If only&amp;hellip;&lt;/p&gt;
&lt;h2 id=&#34;the-binned-scatterplot&#34;&gt;The Binned Scatterplot&lt;/h2&gt;
&lt;p&gt;The &lt;strong&gt;binned scatterplot&lt;/strong&gt; is a very powerful tool that provides a &lt;strong&gt;flexible&lt;/strong&gt; and &lt;strong&gt;parsimonious&lt;/strong&gt; way of visualizing and summarizing conditional means (and not only) in large datasets.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;idea&lt;/strong&gt; behind the binned scatterplot is to divide the conditioning variable, &lt;code&gt;age&lt;/code&gt; in our example, into &lt;strong&gt;equally sized bins or quantiles&lt;/strong&gt;, and then plot the conditional mean of the dependent variable, &lt;code&gt;sales&lt;/code&gt; in our example, within each bin.&lt;/p&gt;
&lt;h3 id=&#34;details&#34;&gt;Details&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/1902.09608&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Cattaneo, Crump, Farrell, Feng (2021)&lt;/a&gt; have built an extremely good package for binned scatterplots in R, &lt;a href=&#34;https://cran.r-project.org/web/packages/binsreg/index.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;binsreg&lt;/a&gt;. Moreover, they have ported the package to Python. We can install &lt;code&gt;binsreg&lt;/code&gt; directly from pip using &lt;code&gt;pip install binsreg&lt;/code&gt;. You can find more information on the Python package &lt;a href=&#34;https://pypi.org/project/binsreg/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;, while the original and detailed R package documentation can be found &lt;a href=&#34;https://www.rdocumentation.org/packages/binsreg/versions/0.7/topics/binsreg&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The most important choice when building a binned scatterplot is the &lt;strong&gt;number of bins&lt;/strong&gt;. The trade-off is the usual &lt;a href=&#34;https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;bias-variance trade-off&lt;/strong&gt;&lt;/a&gt;. By picking a higher number of bins, we have more points in the graph. In the extreme, we end up having a standard &lt;strong&gt;scatterplot&lt;/strong&gt; (assuming the conditioning variable is continuous). On the other hand, by decreasing the number bins, the plot will be more stable. However, in the extreme, we will have a &lt;strong&gt;single point&lt;/strong&gt; representing the sample mean.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/1902.09608&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Cattaneo, Crump, Farrell, Feng (2021)&lt;/a&gt; prove that, in the basic binned scatterplot, the number of bins that minimizes the mean squared error is proportional to $n^{1/3}$, where $n$ is the number of observations. Therefore, in general, more observations lead to more bins.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://onlinelibrary.wiley.com/doi/abs/10.1002/smj.3199&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Starr and Goldfarb (2020)&lt;/a&gt; add the following consideration:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;&lt;em&gt;However other elements are also important. For example, holding the distribution of x constant, the more curvilinear the true relationship between x and y is, the more bins the algorithm will select (otherwise mean squared error will increase). This implies that even with large n, few bins will be chosen for relatively flat relationships. The calculation of the optimal number of bins in a basic binned scatterplot thus takes into account the amount and location of variation in the data available to identify the relationship between x and y.&lt;/em&gt;&amp;rdquo;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;It is strongly recommended to use the default optimal number of bins. However, one can also set a customized number of bins in &lt;code&gt;binsreg&lt;/code&gt; with the &lt;code&gt;nbins&lt;/code&gt; option.&lt;/p&gt;
&lt;p&gt;Binned scatterplots however, do not just compute conditional means, for optimally chosen intervals, but they can also provide &lt;strong&gt;inference&lt;/strong&gt; for these means. In particular, we can build &lt;strong&gt;confidence intervals&lt;/strong&gt; around each data point. In the &lt;code&gt;binsreg&lt;/code&gt; package, the option &lt;code&gt;ci&lt;/code&gt; adds confidence intervals to the estimation results. The option takes as input a tuple of parameters &lt;code&gt;(p, s)&lt;/code&gt; and uses a piecewise polynomial of degree &lt;code&gt;p&lt;/code&gt; with &lt;code&gt;s&lt;/code&gt; smoothness constraints to construct the confidence intervals. By default, the confidence intervals are not included in the plot. For what concerns the choice of &lt;code&gt;p&lt;/code&gt; and &lt;code&gt;s&lt;/code&gt;, the &lt;a href=&#34;https://www.rdocumentation.org/packages/binsreg/versions/0.7/topics/binsreg&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;package documentation&lt;/a&gt; reports:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;&lt;em&gt;Recommended specification is ci=c(3,3), which adds confidence intervals based on cubic B-spline estimate of the regression function of interest to the binned scatter plot.&lt;/em&gt;&amp;rdquo;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;binsreg&#34;&gt;Binsreg&lt;/h3&gt;
&lt;p&gt;One problem with the Python version of the package, is that is not very Python-ish. Therefore, I have wrapped the &lt;code&gt;binsreg&lt;/code&gt; package into a function &lt;code&gt;binscatter&lt;/code&gt; that takes care of cleaning and formatting the output in a nicely readable &lt;a href=&#34;https://pandas.pydata.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Pandas&lt;/a&gt; DataFrame.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import binsreg

def binscatter(**kwargs):
    # Estimate binsreg
    est = binsreg.binsreg(**kwargs)
    
    # Retrieve estimates
    df_est = pd.concat([d.dots for d in est.data_plot])
    df_est = df_est.rename(columns={&#39;x&#39;: kwargs.get(&amp;quot;x&amp;quot;), &#39;fit&#39;: kwargs.get(&amp;quot;y&amp;quot;)})
    
    # Add confidence intervals
    if &amp;quot;ci&amp;quot; in kwargs:
        df_est = pd.merge(df_est, pd.concat([d.ci for d in est.data_plot]))
        df_est = df_est.drop(columns=[&#39;x&#39;])
        df_est[&#39;ci&#39;] = df_est[&#39;ci_r&#39;] - df_est[&#39;ci_l&#39;]
    
    # Rename groups
    if &amp;quot;by&amp;quot; in kwargs:
        df_est[&#39;group&#39;] = df_est[&#39;group&#39;].astype(df[kwargs.get(&amp;quot;by&amp;quot;)].dtype)
        df_est = df_est.rename(columns={&#39;group&#39;: kwargs.get(&amp;quot;by&amp;quot;)})

    return df_est
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can now proceed to &lt;strong&gt;estimate&lt;/strong&gt; and &lt;strong&gt;visualize&lt;/strong&gt; the binned scatterplot for &lt;code&gt;age&lt;/code&gt; based on &lt;code&gt;sales&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Estimate binsreg
df_est = binscatter(x=&#39;age&#39;, y=&#39;sales&#39;, data=df, ci=(3,3))
df_est.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/binned_scatterplot_31_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;group&lt;/th&gt;
      &lt;th&gt;age&lt;/th&gt;
      &lt;th&gt;bin&lt;/th&gt;
      &lt;th&gt;isknot&lt;/th&gt;
      &lt;th&gt;mid&lt;/th&gt;
      &lt;th&gt;sales&lt;/th&gt;
      &lt;th&gt;ci_l&lt;/th&gt;
      &lt;th&gt;ci_r&lt;/th&gt;
      &lt;th&gt;ci&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;Full Sample&lt;/td&gt;
      &lt;td&gt;0.016048&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1653.865445&lt;/td&gt;
      &lt;td&gt;1362.722061&lt;/td&gt;
      &lt;td&gt;1893.998686&lt;/td&gt;
      &lt;td&gt;531.276626&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;Full Sample&lt;/td&gt;
      &lt;td&gt;0.049295&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1666.329034&lt;/td&gt;
      &lt;td&gt;1486.504692&lt;/td&gt;
      &lt;td&gt;1890.922562&lt;/td&gt;
      &lt;td&gt;404.417871&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;Full Sample&lt;/td&gt;
      &lt;td&gt;0.086629&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1937.095012&lt;/td&gt;
      &lt;td&gt;1727.248438&lt;/td&gt;
      &lt;td&gt;2124.811346&lt;/td&gt;
      &lt;td&gt;397.562909&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;Full Sample&lt;/td&gt;
      &lt;td&gt;0.125955&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1972.484136&lt;/td&gt;
      &lt;td&gt;1801.125187&lt;/td&gt;
      &lt;td&gt;2243.034755&lt;/td&gt;
      &lt;td&gt;441.909568&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;Full Sample&lt;/td&gt;
      &lt;td&gt;0.167636&lt;/td&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;2142.560866&lt;/td&gt;
      &lt;td&gt;1937.677738&lt;/td&gt;
      &lt;td&gt;2405.785562&lt;/td&gt;
      &lt;td&gt;468.107824&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;The &lt;code&gt;binscatter&lt;/code&gt; function outputs a dataset in which, for each bin of the conditioning variable, &lt;code&gt;age&lt;/code&gt;, we have values and confidence intervals for the outcome variable, &lt;code&gt;sales&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;We can now plot the estimates.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Plot binned scatterplot
sns.scatterplot(x=&#39;age&#39;, y=&#39;sales&#39;, data=df_est);
plt.errorbar(&#39;age&#39;, &#39;sales&#39;, yerr=&#39;ci&#39;, data=df_est, ls=&#39;&#39;, lw=3, alpha=0.2);
plt.title(&amp;quot;Sales by firm&#39;s age&amp;quot;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/binned_scatterplot_33_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;The plot is quite revealing. Now the relationship looks extremely &lt;strong&gt;non-linear&lt;/strong&gt; with a sharp increase in &lt;code&gt;sales&lt;/code&gt; at the beginning of the lifetime of a firm, followed by a plateau.&lt;/p&gt;
&lt;p&gt;Moreover, the plot is also telling us information regarding the distributions of &lt;code&gt;age&lt;/code&gt; and &lt;code&gt;sales&lt;/code&gt;. In fact, the plot is more dense on the left, where the distribution of &lt;code&gt;age&lt;/code&gt; is concentrated. Also, confidence intervals are tighter on the left, where most of the conditional distribution of &lt;code&gt;sales&lt;/code&gt; lies.&lt;/p&gt;
&lt;p&gt;As we already discussed in the previous section, it might be important to control for other variables. For example, the number of &lt;code&gt;products&lt;/code&gt;, since firms that sell more products probably survive longer in the markets and also make more sales.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;binsreg&lt;/code&gt; allows to &lt;strong&gt;condition&lt;/strong&gt; the analysis on any number of variables, with the &lt;code&gt;w&lt;/code&gt; option.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Estimate binsreg
df_est = binscatter(x=&#39;age&#39;, y=&#39;sales&#39;, w=[&#39;products&#39;], data=df, ci=(3,3))

# Plot binned scatterplot
sns.scatterplot(x=&#39;age&#39;, y=&#39;sales&#39;, data=df_est);
plt.errorbar(&#39;age&#39;, &#39;sales&#39;, yerr=&#39;ci&#39;, data=df_est, ls=&#39;&#39;, lw=3, alpha=0.2);
plt.title(&amp;quot;Sales by firm&#39;s age&amp;quot;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/binned_scatterplot_35_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;img/binned_scatterplot_35_2.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Conditional on number of &lt;code&gt;products&lt;/code&gt;, the shape of the &lt;code&gt;sales&lt;/code&gt; life-cycle changes further. Now, after an initial increase in sales, we observe a gradual decrease over time.&lt;/p&gt;
&lt;p&gt;Do &lt;code&gt;online&lt;/code&gt;-only firms have different &lt;code&gt;sales&lt;/code&gt; life-cycles with respect to mixed online-offline firms? We can produce different binned scatterplots &lt;strong&gt;by group&lt;/strong&gt; using the option &lt;code&gt;by&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Estimate binsreg
df_est = binscatter(x=&#39;age&#39;, y=&#39;sales&#39;, by=&#39;online&#39;, w=[&#39;products&#39;], data=df, ci=(3,3))

# Plot binned scatterplot
sns.scatterplot(x=&#39;age&#39;, y=&#39;sales&#39;, data=df_est, hue=&#39;online&#39;);
plt.errorbar(&#39;age&#39;, &#39;sales&#39;, yerr=&#39;ci&#39;, data=df_est.query(&amp;quot;online==0&amp;quot;), ls=&#39;&#39;, lw=3, alpha=0.2);
plt.errorbar(&#39;age&#39;, &#39;sales&#39;, yerr=&#39;ci&#39;, data=df_est.query(&amp;quot;online==1&amp;quot;), ls=&#39;&#39;, lw=3, alpha=0.2);
plt.title(&amp;quot;Sales by firm&#39;s age&amp;quot;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/binned_scatterplot_37_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;img/binned_scatterplot_37_2.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;From the binned scatterplot, we can see that &lt;code&gt;online&lt;/code&gt; products have on average shorter lifetimes, with a higher initial peak in &lt;code&gt;sales&lt;/code&gt;, followed by a sharper decline.&lt;/p&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;In this blog post, we have analyzed a very powerful data visualization tool: the &lt;strong&gt;binned scatterplot&lt;/strong&gt;. In particular, we have seen how to use the &lt;code&gt;binsreg&lt;/code&gt; package to automatically pick the optimal number of bins and perform non-parametric inference on conditional means. However, the &lt;code&gt;binsreg&lt;/code&gt; package offers much more than that and I strongly recommend checking &lt;a href=&#34;https://cran.r-project.org/web/packages/binsreg/binsreg.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;its manual&lt;/a&gt; more in depth.&lt;/p&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;p&gt;[1] E Starr, B Goldfarb, &lt;a href=&#34;https://onlinelibrary.wiley.com/doi/abs/10.1002/smj.3199&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Binned Scatterplots: A Simple Tool to Make Research Easier and Better&lt;/a&gt; (2020), Strategic Management Journal.&lt;/p&gt;
&lt;p&gt;[2] M. D. Cattaneo, R. K. Crump, M. H. Farrell, Y. Feng, &lt;a href=&#34;https://arxiv.org/abs/1902.09608&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;On Binscatter&lt;/a&gt; (2021), working paper.&lt;/p&gt;
&lt;p&gt;[3] P. Goldsmith-Pinkham, &lt;a href=&#34;https://www.youtube.com/watch?v=fg9T2gPZCIs&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Lecture 6. Linear Regression II: Semiparametrics and Visualization&lt;/a&gt;, Applied Metrics PhD Course.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>How to Compare Two or More Distributions</title>
      <link>https://matteocourthoud.github.io/post/comparing_distributions/</link>
      <pubDate>Mon, 10 Jul 2023 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/post/comparing_distributions/</guid>
      <description>&lt;p&gt;&lt;em&gt;A complete guide to comparing distributions, from visualization to statistical tests&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Comparing the empirical distribution of a variable across different groups is a common problem in data science. In particular, in causal inference the problem often arises when we have to &lt;strong&gt;assess the quality of randomization&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;When we want to assess the causal effect of a policy (or UX feature, ad campaign, drug, &amp;hellip;), the golden standard in causal inference are &lt;a href=&#34;https://en.wikipedia.org/wiki/Randomized_controlled_trial&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;randomized control trials&lt;/strong&gt;&lt;/a&gt;, also known as &lt;a href=&#34;https://de.wikipedia.org/wiki/A/B-Test&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;A/B tests&lt;/strong&gt;&lt;/a&gt;. In practice, we select a sample for the study and we randomly split it into a &lt;strong&gt;control&lt;/strong&gt; and a &lt;strong&gt;treatment&lt;/strong&gt; group, and we compare the outcomes between the two groups. Randomization ensures that only difference between the two groups is the treatment, on average, so that we can attribute outcome differences to the treatment effect.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;problem&lt;/strong&gt; is that, despite randomization, the two groups are never identical. However, sometimes, they are not even &amp;ldquo;similar&amp;rdquo;. For example, we might have more males in one group, or older people, etc.. (we usually call these characteristics, &lt;em&gt;covariates&lt;/em&gt; or &lt;em&gt;control variables&lt;/em&gt;). When it happens, we cannot be certain anymore that the difference in the outcome is only due to the treatment and cannot be attributed to the &lt;strong&gt;inbalanced covariates&lt;/strong&gt; instead. Therefore, it is always important, after randomization, to check whether all observed variables are balanced across groups and whether there are no systematic differences. Another option, to be certain ex-ante that certain covariates are balanced, is &lt;a href=&#34;https://en.wikipedia.org/wiki/Stratified_sampling&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;stratified sampling&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;In this blog post, we are going to see different ways to compare two (or more) distributions and assess the magnitude and significance of their difference. We are going to consider two different approaches, &lt;strong&gt;visual&lt;/strong&gt; and &lt;strong&gt;statistical&lt;/strong&gt;. The two approaches generally trade-off &lt;strong&gt;intuition&lt;/strong&gt; with &lt;strong&gt;rigor&lt;/strong&gt;: from plots we can quickly assess and explore differences, but it&amp;rsquo;s hard to tell whether these differences are systematic or due to noise.&lt;/p&gt;
&lt;h2 id=&#34;example&#34;&gt;Example&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s assume we need to perform an &lt;strong&gt;experiment&lt;/strong&gt; on a group of individuals and we have randomized them into a &lt;strong&gt;treatment and control&lt;/strong&gt; group. We would like them to be &lt;strong&gt;as comparable as possible&lt;/strong&gt;, in order to attribute any difference between the two groups to the treatment effect alone. We also have divided the treatment group in different &lt;em&gt;arms&lt;/em&gt; for testing different treatments (e.g. slight variations of the same drug).&lt;/p&gt;
&lt;p&gt;For this example, I have simulated a dataset of 1000 individuals, for whom we observe a set of characteristics. I import the data generating process &lt;code&gt;dgp_rnd_assignment()&lt;/code&gt; from &lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/src/dgp.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;src.dgp&lt;/code&gt;&lt;/a&gt; and some plotting functions and libraries from &lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/src/utils.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;src.utils&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;%matplotlib inline
%config InlineBackend.figure_format = &#39;retina&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from src.utils import *
from src.dgp import dgp_rnd_assignment

df = dgp_rnd_assignment().generate_data()
df.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;Group&lt;/th&gt;
      &lt;th&gt;Arm&lt;/th&gt;
      &lt;th&gt;Gender&lt;/th&gt;
      &lt;th&gt;Age&lt;/th&gt;
      &lt;th&gt;Income&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;control&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;29.0&lt;/td&gt;
      &lt;td&gt;568.44&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;control&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;32.0&lt;/td&gt;
      &lt;td&gt;596.45&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;treatment&lt;/td&gt;
      &lt;td&gt;arm 3&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;29.0&lt;/td&gt;
      &lt;td&gt;380.86&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;control&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;25.0&lt;/td&gt;
      &lt;td&gt;476.38&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;treatment&lt;/td&gt;
      &lt;td&gt;arm 4&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;32.0&lt;/td&gt;
      &lt;td&gt;628.28&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;We have information on $1000$ individuals, for which we observe &lt;code&gt;gender&lt;/code&gt;, &lt;code&gt;age&lt;/code&gt; and weekly &lt;code&gt;income&lt;/code&gt;. Each individual is assigned either to the treatment or control &lt;code&gt;group&lt;/code&gt; and treated individuals are distributed across four treatment &lt;code&gt;arms&lt;/code&gt;.&lt;/p&gt;
&lt;h2 id=&#34;two-groups---plots&#34;&gt;Two Groups - Plots&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s start with the simplest setting: we want to compare the distribution of income across the &lt;code&gt;treatment&lt;/code&gt; and &lt;code&gt;control&lt;/code&gt; group. We first explore &lt;strong&gt;visual&lt;/strong&gt; approaches and the &lt;strong&gt;statistical&lt;/strong&gt; approaches. The advantage of the first is &lt;strong&gt;intuition&lt;/strong&gt; while the advantage of the second is &lt;strong&gt;rigor&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;For most visualizations I am going to use Python&amp;rsquo;s &lt;a href=&#34;https://seaborn.pydata.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;seaborn&lt;/code&gt;&lt;/a&gt; library.&lt;/p&gt;
&lt;h3 id=&#34;boxplot&#34;&gt;Boxplot&lt;/h3&gt;
&lt;p&gt;A first visual approach is the &lt;a href=&#34;https://en.wikipedia.org/wiki/Box_plot&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;boxplot&lt;/strong&gt;&lt;/a&gt;. The boxplot is a good trade-off between summary statistics and data visualization. The center of the &lt;strong&gt;box&lt;/strong&gt; represents the &lt;em&gt;median&lt;/em&gt; while the borders represent the first (Q1) and third &lt;a href=&#34;https://en.wikipedia.org/wiki/Quartile&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;quartile&lt;/a&gt; (Q3), respectively. The &lt;strong&gt;whiskers&lt;/strong&gt; instead, extend to the first data points that are more than 1.5 times the &lt;em&gt;interquartile range&lt;/em&gt; (Q3 - Q1) outside the box. The points that fall outside of the whiskers are plotted individually and are usually considered &lt;a href=&#34;https://en.wikipedia.org/wiki/Outlier&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;outliers&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Therefore, the boxplot provides both summary statistics (the box and the whiskers) and direct data visualization (the outliers).&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sns.boxplot(data=df, x=&#39;Group&#39;, y=&#39;Income&#39;);
plt.title(&amp;quot;Boxplot&amp;quot;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/comparing_distributions_12_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;It seems that the &lt;code&gt;income&lt;/code&gt; distribution in the &lt;code&gt;treatment&lt;/code&gt; group is slightly more dispersed: the orange box is larger and its whiskers cover a wider range. However, the &lt;strong&gt;issue&lt;/strong&gt; with the boxplot is that it hides the shape of the data, telling us some summary statistics but not showing us the actual data distribution.&lt;/p&gt;
&lt;h3 id=&#34;histogram&#34;&gt;Histogram&lt;/h3&gt;
&lt;p&gt;The most intuitive way to plot a distribution is the &lt;strong&gt;histogram&lt;/strong&gt;. The histogram groups the data into equally wide &lt;strong&gt;bins&lt;/strong&gt; and plots the number of observations within each bin.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sns.histplot(data=df, x=&#39;Income&#39;, hue=&#39;Group&#39;, bins=50);
plt.title(&amp;quot;Histogram&amp;quot;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/comparing_distributions_16_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;There are multiple &lt;strong&gt;issues&lt;/strong&gt; with this plot:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Since the two groups have a different number of observations, the two histograms are not comparable&lt;/li&gt;
&lt;li&gt;The number of bins is arbitrary&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We can solve the first issue using the &lt;code&gt;stat&lt;/code&gt; option to plot the &lt;code&gt;density&lt;/code&gt; instead of the count and setting the &lt;code&gt;common_norm&lt;/code&gt; option to &lt;code&gt;False&lt;/code&gt; to use the same normalization.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sns.histplot(data=df, x=&#39;Income&#39;, hue=&#39;Group&#39;, bins=50, stat=&#39;density&#39;, common_norm=False);
plt.title(&amp;quot;Density Histogram&amp;quot;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/comparing_distributions_18_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Now the two histograms are comparable!&lt;/p&gt;
&lt;p&gt;However, an important &lt;strong&gt;issue&lt;/strong&gt; remains: the size of the bins is arbitrary. In the extreme, if we bunch the data less, we end up with bins with at most one observation, if we bunch the data more, we end up with a single bin. In both cases, if we exaggerate, the plot loses informativeness. This is a classical &lt;a href=&#34;https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;bias-variance trade-off&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;kernel-density&#34;&gt;Kernel Density&lt;/h3&gt;
&lt;p&gt;One possible solution is to use a &lt;a href=&#34;https://en.wikipedia.org/wiki/Kernel_density_estimation&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;kernel density function&lt;/strong&gt;&lt;/a&gt; that tries to approximate the histogram with a continuous function, using &lt;a href=&#34;https://en.wikipedia.org/wiki/Kernel_density_estimation&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;kernel density estimation (KDE)&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sns.kdeplot(x=&#39;Income&#39;, data=df, hue=&#39;Group&#39;, common_norm=False);
plt.title(&amp;quot;Kernel Density Function&amp;quot;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/comparing_distributions_22_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;From the plot, it seems that the estimated kernel density of &lt;code&gt;income&lt;/code&gt; has &amp;ldquo;fatter tails&amp;rdquo; (i.e. higher variance) in the &lt;code&gt;treatment&lt;/code&gt; group, while the average seems similar across groups.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;issue&lt;/strong&gt; with kernel density estimation is that it is a bit of a  black-box and might mask relevant features of the data.&lt;/p&gt;
&lt;h3 id=&#34;cumulative-distribution&#34;&gt;Cumulative Distribution&lt;/h3&gt;
&lt;p&gt;A more transparent representation of the two distribution is their &lt;a href=&#34;https://en.wikipedia.org/wiki/Cumulative_distribution_function&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;cumulative distribution function&lt;/strong&gt;&lt;/a&gt;. At each point of the x axis (&lt;code&gt;income&lt;/code&gt;) we plot the percentage of data points that have an equal or lower value. The main &lt;strong&gt;advantages&lt;/strong&gt; of the cumulative distribution function are that&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;we do not need to make any arbitrary choice (e.g. number of bins)&lt;/li&gt;
&lt;li&gt;we do not need to perform any approximation (e.g. with KDE), but we represent all data points&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sns.histplot(x=&#39;Income&#39;, data=df, hue=&#39;Group&#39;, bins=len(df), stat=&amp;quot;density&amp;quot;,
             element=&amp;quot;step&amp;quot;, fill=False, cumulative=True, common_norm=False);
plt.title(&amp;quot;Cumulative distribution function&amp;quot;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/comparing_distributions_26_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;How should we interpret the graph?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Since the two lines cross more or less at 0.5 (y axis), it means that their median is similar&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Since the orange line is above the blue line on the left and below the blue line on the left, it means that the distribution of the &lt;code&gt;treatment&lt;/code&gt; group as fatter tails&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;qq-plot&#34;&gt;QQ Plot&lt;/h3&gt;
&lt;p&gt;A related method is the &lt;strong&gt;qq-plot&lt;/strong&gt;, where &lt;em&gt;q&lt;/em&gt; stands for quantile. The qq-plot plots the quantiles of the two distributions against each other. If the distributions are the same, we should get a 45 degree line.&lt;/p&gt;
&lt;p&gt;There is no native qq-plot function in Python and, while the &lt;code&gt;statsmodels&lt;/code&gt; package provides a &lt;a href=&#34;https://www.statsmodels.org/dev/generated/statsmodels.graphics.gofplots.qqplot.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;qqplot&lt;/code&gt; function&lt;/a&gt;, it is quite cumbersome. Therefore, we will do it by hand.&lt;/p&gt;
&lt;p&gt;First, we need to compute the quartiles of the two groups, using the &lt;code&gt;percentile&lt;/code&gt; function.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;income = df[&#39;Income&#39;].values
income_t = df.loc[df.Group==&#39;treatment&#39;, &#39;Income&#39;].values
income_c = df.loc[df.Group==&#39;control&#39;, &#39;Income&#39;].values

df_pct = pd.DataFrame()
df_pct[&#39;q_treatment&#39;] = np.percentile(income_t, range(100))
df_pct[&#39;q_control&#39;] = np.percentile(income_c, range(100))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we can plot the two quantile distributions against each other, plus the 45-degree line, representing the benchmark perfect fit.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plt.figure(figsize=(8, 8))
plt.scatter(x=&#39;q_control&#39;, y=&#39;q_treatment&#39;, data=df_pct, label=&#39;Actual fit&#39;);
sns.lineplot(x=&#39;q_control&#39;, y=&#39;q_control&#39;, data=df_pct, color=&#39;r&#39;, label=&#39;Line of perfect fit&#39;);
plt.xlabel(&#39;Quantile of income, control group&#39;)
plt.ylabel(&#39;Quantile of income, treatment group&#39;)
plt.legend()
plt.title(&amp;quot;QQ plot&amp;quot;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/comparing_distributions_32_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;The qq-plot delivers a very &lt;strong&gt;similar insight&lt;/strong&gt; with respect to the cumulative distribution plot: income in the treatment group has the same median (lines cross in the center) but wider tails (dots are below the line on the left end and above on the right end).&lt;/p&gt;
&lt;h2 id=&#34;two-groups---tests&#34;&gt;Two Groups - Tests&lt;/h2&gt;
&lt;p&gt;So far, we have seen different ways to &lt;em&gt;visualize&lt;/em&gt; differences between distributions. The main advantage of visualization is &lt;strong&gt;intuition&lt;/strong&gt;: we can eyeball the differences and intuitively assess them.&lt;/p&gt;
&lt;p&gt;However, we might want to be more &lt;strong&gt;rigorous&lt;/strong&gt; and try to assess the &lt;strong&gt;statistical significance&lt;/strong&gt; of the difference between the distributions, i.e. answer the question &amp;ldquo;&lt;em&gt;is the observed difference systematic or due to sampling noise?&lt;/em&gt;&amp;rdquo;.&lt;/p&gt;
&lt;p&gt;We are now going to analyze different tests to discern two distributions from each other.&lt;/p&gt;
&lt;h3 id=&#34;t-test&#34;&gt;T-test&lt;/h3&gt;
&lt;p&gt;The first and most common test is the &lt;a href=&#34;https://en.wikipedia.org/wiki/Student%27s_t-test&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;student t-test&lt;/a&gt;. T-tests are generally used to &lt;strong&gt;compare means&lt;/strong&gt;. In this case, we want to test whether the means of the &lt;code&gt;income&lt;/code&gt; distribution is the same across the two groups. The test statistic for the two-means comparison test is given by:&lt;/p&gt;
&lt;p&gt;$$
stat = \frac{|\bar x_1 - \bar x_2|}{\sqrt{s^2 / n }}
$$&lt;/p&gt;
&lt;p&gt;Where $\bar x$ is the sample mean and $s$ is the sample standard deviation. Under mild conditions, the test statistic is asymptotically distributed as a &lt;a href=&#34;https://en.wikipedia.org/wiki/Student%27s_t-distribution&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;student t&lt;/a&gt; distribution.&lt;/p&gt;
&lt;p&gt;We use the &lt;code&gt;ttest_ind&lt;/code&gt; function from &lt;code&gt;scipy&lt;/code&gt; to perform the t-test. The function returns both the test statistic and the implied &lt;a href=&#34;https://en.wikipedia.org/wiki/P-value&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;p-value&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from scipy.stats import ttest_ind

stat, p_value = ttest_ind(income_c, income_t)
print(f&amp;quot;t-test: statistic={stat:.4f}, p-value={p_value:.4f}&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;t-test: statistic=-1.5549, p-value=0.1203
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The p-value of the test is $0.12$, therefore we do &lt;strong&gt;not reject&lt;/strong&gt; the null hypothesis of no difference in &lt;em&gt;means&lt;/em&gt; across treatment and control groups.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: the t-test assumes that the variance in the two samples is the same so that its estimate is computed on the joint sample. &lt;a href=&#34;https://en.wikipedia.org/wiki/Welch%27s_t-test&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Welch’s t-test&lt;/a&gt; allows for unequal variances in the two samples.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;standardized-mean-difference-smd&#34;&gt;Standardized Mean Difference (SMD)&lt;/h3&gt;
&lt;p&gt;In general, it is good practice to always perform a test for difference in means on &lt;strong&gt;all variables&lt;/strong&gt; across the treatment and control group, when we are running a randomized control trial or A/B test.&lt;/p&gt;
&lt;p&gt;However, since the denominator of the t-test statistic depends on the sample size, the t-test has been &lt;strong&gt;criticized&lt;/strong&gt; for making p-values hard to compare across studies. In fact, we may obtain a significant result in an experiment with very small magnitude of difference but large sample size while we may obtain a non-significant result in an experiment with large magnitude of difference but small sample size.&lt;/p&gt;
&lt;p&gt;One solution that has been proposed is the &lt;strong&gt;standardized mean difference (SMD)&lt;/strong&gt;. As the name suggests, this is not a proper test statistic, but just a standardized difference, which can be computed as:&lt;/p&gt;
&lt;p&gt;$$
SMD = \frac{|\bar x_1 - \bar x_2|}{\sqrt{(s^2_1 + s^2_2) / 2}}
$$&lt;/p&gt;
&lt;p&gt;Usually a value below $0.1$ is considered a &amp;ldquo;small&amp;rdquo; difference.&lt;/p&gt;
&lt;p&gt;It is good practice to collect average values of all variables across treatment and control group and a measure of distance between the two — either the t-test or the SMD — into a table that is called &lt;strong&gt;balance table&lt;/strong&gt;. We can use the &lt;a href=&#34;https://causalml.readthedocs.io/en/latest/causalml.html#module-causalml.match&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;create_table_one&lt;/code&gt;&lt;/a&gt; function from the &lt;a href=&#34;https://causalml.readthedocs.io/en/latest/about.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;causalml&lt;/code&gt;&lt;/a&gt; library to generate it. As the name of the function suggests, the balance table should always be the &lt;strong&gt;first table&lt;/strong&gt; you present when performing an A/B test.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from causalml.match import create_table_one

df[&#39;treatment&#39;] = df[&#39;Group&#39;]==&#39;treatment&#39;
create_table_one(df, &#39;treatment&#39;, [&#39;Gender&#39;, &#39;Age&#39;, &#39;Income&#39;])
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;Control&lt;/th&gt;
      &lt;th&gt;Treatment&lt;/th&gt;
      &lt;th&gt;SMD&lt;/th&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;Variable&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;n&lt;/th&gt;
      &lt;td&gt;704&lt;/td&gt;
      &lt;td&gt;296&lt;/td&gt;
      &lt;td&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;Age&lt;/th&gt;
      &lt;td&gt;32.40 (8.54)&lt;/td&gt;
      &lt;td&gt;36.42 (7.76)&lt;/td&gt;
      &lt;td&gt;0.4928&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;Gender&lt;/th&gt;
      &lt;td&gt;0.51 (0.50)&lt;/td&gt;
      &lt;td&gt;0.58 (0.49)&lt;/td&gt;
      &lt;td&gt;0.1419&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;Income&lt;/th&gt;
      &lt;td&gt;524.59 (117.35)&lt;/td&gt;
      &lt;td&gt;538.75 (160.15)&lt;/td&gt;
      &lt;td&gt;0.1009&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;In the first two columns, we can see the average of the different variables across the treatment and control groups, with standard errors in parenthesis. In the &lt;strong&gt;last column&lt;/strong&gt;, the values of the SMD indicate a standardized difference of more than $0.1$ for all variables, suggesting that the two groups are probably different.&lt;/p&gt;
&lt;h3 id=&#34;mannwhitney-u-test&#34;&gt;Mann–Whitney U Test&lt;/h3&gt;
&lt;p&gt;An alternative test is the &lt;a href=&#34;https://en.wikipedia.org/wiki/Mann%E2%80%93Whitney_U_test&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Mann–Whitney U test&lt;/a&gt;. The null hypothesis for this test is that the two groups have the same distribution, while the alternative hypothesis is that one group has larger (or smaller) values than the other.&lt;/p&gt;
&lt;p&gt;Differently from the other tests we have seen so far, the Mann–Whitney U test is agnostic to outliers and concentrates on the center of the distribution.&lt;/p&gt;
&lt;p&gt;The test &lt;strong&gt;procedure&lt;/strong&gt; is the following.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Combine all data points and rank them (in increasing or decreasing order)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Compute $U_1 = R_1 - n_1(n_1 + 1)/2$, where $R_1$ is the sum of the ranks for data points in the first group and $n_1$ is the number of points in the first group.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Compute $U_2$ similarly for the second group.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The test statistic is given by $stat = min(U_1, U_2)$.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Under the null hypothesis of no systematic rank differences between the two distributions (i.e. same median), the test statistic is asymptotically normally distributed with known mean and variance.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;intuition&lt;/strong&gt; behind the computation of $R$ and $U$ is the following: if the values in the first sample were all bigger than the values in the second sample, then $R_1 = n_1(n_1 + 1)/2$ and, as a consequence, $U_1$ would then be zero (minimum attainable value). Otherwise, if the two samples were similar, $U_1$ and $U_2$ would be very close to $n_1 n_2 / 2$ (maximum attainable value).&lt;/p&gt;
&lt;p&gt;We perform the test using the &lt;code&gt;mannwhitneyu&lt;/code&gt; function from &lt;code&gt;scipy&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from scipy.stats import mannwhitneyu

stat, p_value = mannwhitneyu(income_t, income_c)
print(f&amp;quot; Mann–Whitney U Test: statistic={stat:.4f}, p-value={p_value:.4f}&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt; Mann–Whitney U Test: statistic=106371.5000, p-value=0.6012
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We get a p-value of 0.6 which implies that we do &lt;strong&gt;not reject&lt;/strong&gt; the null hypothesis of no difference between the two distributions.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: as for the t-test, there exists a version of the Mann–Whitney U test for unequal variances in the two samples, the &lt;a href=&#34;https://www.statisticshowto.com/brunner-munzel-test-generalized-wilcoxon-test/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Brunner-Munzel test&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;permutation-tests&#34;&gt;Permutation Tests&lt;/h3&gt;
&lt;p&gt;A non-parametric alternative is permutation testing. The idea is that, under the null hypothesis, the two distributions should be the same, therefore &lt;strong&gt;shuffling&lt;/strong&gt; the &lt;code&gt;group&lt;/code&gt; labels should not significantly alter any statistic.&lt;/p&gt;
&lt;p&gt;We can chose any statistic and check how its value in the original sample compares with its distribution across &lt;code&gt;group&lt;/code&gt; label permutations. For example, let&amp;rsquo;s use as a test statistic the &lt;strong&gt;difference of sample means&lt;/strong&gt; between the treatment and control group.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sample_stat = np.mean(income_t) - np.mean(income_c)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;stats = np.zeros(1000)
for k in range(1000):
    labels = np.random.permutation((df[&#39;Group&#39;] == &#39;treatment&#39;).values)
    stats[k] = np.mean(income[labels]) - np.mean(income[labels==False])
p_value = np.mean(stats &amp;gt; sample_stat)

print(f&amp;quot;Permutation test: p-value={p_value:.4f}&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Permutation test: p-value=0.0530
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The permutation test gives us a p-value of $0.056$, implying a weak &lt;strong&gt;non-rejection&lt;/strong&gt; of the null hypothesis at the 5% level.&lt;/p&gt;
&lt;p&gt;How do we &lt;strong&gt;interpret&lt;/strong&gt; the p-value? It means that the difference in means in the data is larger than $1 - 0.0560 = 94.4%$ of the differences in means across the permuted samples.&lt;/p&gt;
&lt;p&gt;We can &lt;strong&gt;visualize&lt;/strong&gt; the test, by plotting the distribution of the test statistic across permutations against its sample value.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plt.hist(stats, label=&#39;Permutation Statistics&#39;, bins=30);
plt.axvline(x=sample_stat, c=&#39;r&#39;, ls=&#39;--&#39;, label=&#39;Sample Statistic&#39;);
plt.legend();
plt.xlabel(&#39;Income difference between treatment and control group&#39;)
plt.title(&#39;Permutation Test&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/comparing_distributions_55_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;As we can see, the sample statistic is quite extreme with respect to the values in the permuted samples, but not excessively.&lt;/p&gt;
&lt;h3 id=&#34;chi-squared-test&#34;&gt;Chi-Squared Test&lt;/h3&gt;
&lt;p&gt;The &lt;a href=&#34;https://matteocourthoud.github.io/post/chisquared/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;chi-squared test&lt;/a&gt; is a very powerful test that is mostly used to test differences in frequencies.&lt;/p&gt;
&lt;p&gt;One of the &lt;strong&gt;least known applications&lt;/strong&gt; of the chi-squared test, is testing the similarity between two distributions. The &lt;strong&gt;idea&lt;/strong&gt; is to bin the observations of the two groups. If the two distributions were the same, we would expect the same frequency of observations in each bin. Importantly, we need enough observations in each bin, in order for the test to be valid.&lt;/p&gt;
&lt;p&gt;I generate bins corresponding to deciles of the distribution of &lt;code&gt;income&lt;/code&gt; in the &lt;em&gt;control&lt;/em&gt; group and then I compute the expected number of observations in each bin in the &lt;em&gt;treatment&lt;/em&gt; group, if the two distributions were the same.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Init dataframe
df_bins = pd.DataFrame()

# Generate bins from control group
_, bins = pd.qcut(income_c, q=10, retbins=True)
df_bins[&#39;bin&#39;] = pd.cut(income_c, bins=bins).value_counts().index

# Apply bins to both groups
df_bins[&#39;income_c_observed&#39;] = pd.cut(income_c, bins=bins).value_counts().values
df_bins[&#39;income_t_observed&#39;] = pd.cut(income_t, bins=bins).value_counts().values

# Compute expected frequency in the treatment group
df_bins[&#39;income_t_expected&#39;] = df_bins[&#39;income_c_observed&#39;] / np.sum(df_bins[&#39;income_c_observed&#39;]) * np.sum(df_bins[&#39;income_t_observed&#39;])

df_bins
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;bin&lt;/th&gt;
      &lt;th&gt;income_c_observed&lt;/th&gt;
      &lt;th&gt;income_t_observed&lt;/th&gt;
      &lt;th&gt;income_t_expected&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;(232.26, 380.496]&lt;/td&gt;
      &lt;td&gt;70&lt;/td&gt;
      &lt;td&gt;46&lt;/td&gt;
      &lt;td&gt;29.075391&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;(380.496, 425.324]&lt;/td&gt;
      &lt;td&gt;70&lt;/td&gt;
      &lt;td&gt;30&lt;/td&gt;
      &lt;td&gt;29.075391&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;(425.324, 456.795]&lt;/td&gt;
      &lt;td&gt;70&lt;/td&gt;
      &lt;td&gt;24&lt;/td&gt;
      &lt;td&gt;29.075391&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;(456.795, 488.83]&lt;/td&gt;
      &lt;td&gt;71&lt;/td&gt;
      &lt;td&gt;26&lt;/td&gt;
      &lt;td&gt;29.490754&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;(488.83, 513.66]&lt;/td&gt;
      &lt;td&gt;70&lt;/td&gt;
      &lt;td&gt;18&lt;/td&gt;
      &lt;td&gt;29.075391&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;5&lt;/th&gt;
      &lt;td&gt;(513.66, 540.048]&lt;/td&gt;
      &lt;td&gt;70&lt;/td&gt;
      &lt;td&gt;19&lt;/td&gt;
      &lt;td&gt;29.075391&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;6&lt;/th&gt;
      &lt;td&gt;(540.048, 576.664]&lt;/td&gt;
      &lt;td&gt;71&lt;/td&gt;
      &lt;td&gt;21&lt;/td&gt;
      &lt;td&gt;29.490754&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;7&lt;/th&gt;
      &lt;td&gt;(576.664, 621.022]&lt;/td&gt;
      &lt;td&gt;70&lt;/td&gt;
      &lt;td&gt;25&lt;/td&gt;
      &lt;td&gt;29.075391&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;8&lt;/th&gt;
      &lt;td&gt;(621.022, 682.003]&lt;/td&gt;
      &lt;td&gt;70&lt;/td&gt;
      &lt;td&gt;42&lt;/td&gt;
      &lt;td&gt;29.075391&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;9&lt;/th&gt;
      &lt;td&gt;(682.003, 973.46]&lt;/td&gt;
      &lt;td&gt;71&lt;/td&gt;
      &lt;td&gt;41&lt;/td&gt;
      &lt;td&gt;29.490754&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;We can now perform the test by comparing the expected (E) and observed (O) number of observations in the treatment group, across bins. The test statistic is given by&lt;/p&gt;
&lt;p&gt;$$
stat = \sum _{i=1}^{n} \frac{(O_i - E_i)^{2}}{E_i}
$$&lt;/p&gt;
&lt;p&gt;where the bins are indexed by $i$ and $O$ is the observed number of data points in bin $i$ and $E$ is the expected number of data points in bin $i$. Since we generated the bins using deciles of the distribution of &lt;code&gt;income&lt;/code&gt; in the control group, we expect the number of observations per bin in the treatment group to be the same across bins. The test statistic is asymptocally distributed as a &lt;a href=&#34;https://en.wikipedia.org/wiki/Chi-squared_distribution&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;chi-squared&lt;/a&gt; distribution.&lt;/p&gt;
&lt;p&gt;To compute the test statistic and the p-value of the test, we use the &lt;code&gt;chisquare&lt;/code&gt; function from &lt;code&gt;scipy&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from scipy.stats import chisquare

stat, p_value = chisquare(df_bins[&#39;income_t_observed&#39;], df_bins[&#39;income_t_expected&#39;])
print(f&amp;quot;Chi-squared Test: statistic={stat:.4f}, p-value={p_value:.4f}&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Chi-squared Test: statistic=32.1432, p-value=0.0002
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Differently from all other tests so far, the chi-squared test &lt;strong&gt;strongly rejects&lt;/strong&gt; the null hypothesis that the two distributions are the same. Why?&lt;/p&gt;
&lt;p&gt;The reason lies in the fact that the two distributions have a similar center but different tails and the chi-squared test tests the similarity along the &lt;strong&gt;whole distribution&lt;/strong&gt; and not only in the center, as we were doing with the previous tests.&lt;/p&gt;
&lt;p&gt;This result tells a &lt;strong&gt;cautionary tale&lt;/strong&gt;: it is very important to understand &lt;em&gt;what&lt;/em&gt; you are actually testing before drawing blind conclusions from a p-value!&lt;/p&gt;
&lt;h3 id=&#34;kolmogorov-smirnov-test&#34;&gt;Kolmogorov-Smirnov Test&lt;/h3&gt;
&lt;p&gt;The idea of the &lt;a href=&#34;https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Kolmogorov-Smirnov test&lt;/a&gt;, is to &lt;strong&gt;compare the cumulative distributions&lt;/strong&gt; of the two groups. In particular, the Kolmogorov-Smirnov test statistic is the maximum absolute difference between the two cumulative distributions.&lt;/p&gt;
&lt;p&gt;$$
stat = \sup _{x} \ \Big| \ F_1(x) - F_2(x) \ \Big|
$$&lt;/p&gt;
&lt;p&gt;Where $F_1$ and $F_2$ are the two cumulative distribution functions and $x$ are the values of the underlying variable. The asymptotic distribution of the Kolmogorov-Smirnov test statistic is &lt;a href=&#34;https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test#Kolmogorov_distribution&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Kolmogorov distributed&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;To better understand the test, let&amp;rsquo;s plot the cumulative distribution functions and the test statistic. First, we compute the cumulative distribution functions.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_ks = pd.DataFrame()
df_ks[&#39;Income&#39;] = np.sort(df[&#39;Income&#39;].unique())
df_ks[&#39;F_control&#39;] = df_ks[&#39;Income&#39;].apply(lambda x: np.mean(income_c&amp;lt;=x))
df_ks[&#39;F_treatment&#39;] = df_ks[&#39;Income&#39;].apply(lambda x: np.mean(income_t&amp;lt;=x))
df_ks.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;Income&lt;/th&gt;
      &lt;th&gt;F_control&lt;/th&gt;
      &lt;th&gt;F_treatment&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;216.36&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.003378&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;232.26&lt;/td&gt;
      &lt;td&gt;0.001420&lt;/td&gt;
      &lt;td&gt;0.003378&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;243.15&lt;/td&gt;
      &lt;td&gt;0.001420&lt;/td&gt;
      &lt;td&gt;0.006757&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;259.88&lt;/td&gt;
      &lt;td&gt;0.002841&lt;/td&gt;
      &lt;td&gt;0.006757&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;262.82&lt;/td&gt;
      &lt;td&gt;0.002841&lt;/td&gt;
      &lt;td&gt;0.010135&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;We now need to find the point where the absolute distance between the cumulative distribution functions is largest.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;k = np.argmax( np.abs(df_ks[&#39;F_control&#39;] - df_ks[&#39;F_treatment&#39;]))
ks_stat = np.abs(df_ks[&#39;F_treatment&#39;][k] - df_ks[&#39;F_control&#39;][k])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can visualize the value of the test statistic, by plotting the two cumulative distribution functions and the value of the test statistic.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;y = (df_ks[&#39;F_treatment&#39;][k] + df_ks[&#39;F_control&#39;][k])/2
plt.plot(&#39;Income&#39;, &#39;F_control&#39;, data=df_ks, label=&#39;Control&#39;)
plt.plot(&#39;Income&#39;, &#39;F_treatment&#39;, data=df_ks, label=&#39;Treatment&#39;)
plt.errorbar(x=df_ks[&#39;Income&#39;][k], y=y, yerr=ks_stat/2, color=&#39;k&#39;,
             capsize=5, mew=3, label=f&amp;quot;Test statistic: {ks_stat:.4f}&amp;quot;)
plt.legend(loc=&#39;center right&#39;);
plt.title(&amp;quot;Kolmogorov-Smirnov Test&amp;quot;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/comparing_distributions_69_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;From the plot, we can see that the value of the test statistic corresponds to the distance between the two cumulative distributions at &lt;code&gt;income&lt;/code&gt;~650. For that value of &lt;code&gt;income&lt;/code&gt;, we have the largest imbalance between the two groups.&lt;/p&gt;
&lt;p&gt;We can now perform the actual test using the &lt;code&gt;kstest&lt;/code&gt; function from &lt;code&gt;scipy&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from scipy.stats import kstest

stat, p_value = kstest(income_t, income_c)
print(f&amp;quot; Kolmogorov-Smirnov Test: statistic={stat:.4f}, p-value={p_value:.4f}&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt; Kolmogorov-Smirnov Test: statistic=0.0974, p-value=0.0355
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The p-value is below 5%: we &lt;strong&gt;reject&lt;/strong&gt; the null hypothesis that the two distributions are the same, with 95% confidence.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Note 1&lt;/strong&gt;: The KS test is too conservative and rejects the null hypothesis too rarely. Lilliefors test corrects this bias using a different distribution for the test statistic, the Lilliefors distribution.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Note 2&lt;/strong&gt;: the KS test uses very little information since it only compares the two cumulative distributions at one point: the one of maximum distance. The &lt;a href=&#34;https://en.wikipedia.org/wiki/Anderson%E2%80%93Darling_test&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Anderson-Darling test&lt;/a&gt; and the &lt;a href=&#34;https://en.wikipedia.org/wiki/Cram%C3%A9r%E2%80%93von_Mises_criterion&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Cramér-von Mises test&lt;/a&gt; instead compare the two distributions along the whole domain, by integration (the difference between the two lies in the weighting of the squared distances).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;multiple-groups---plots&#34;&gt;Multiple Groups - Plots&lt;/h2&gt;
&lt;p&gt;So far we have only considered the case of two groups: treatment and control. But that if we had &lt;strong&gt;multiple groups&lt;/strong&gt;? Some of the methods we have seen above scale well, while others don&amp;rsquo;t.&lt;/p&gt;
&lt;p&gt;As a working example, we are now going to check whether the distribution of &lt;code&gt;income&lt;/code&gt; is the same across treatment &lt;code&gt;arms&lt;/code&gt;.&lt;/p&gt;
&lt;h3 id=&#34;boxplot-1&#34;&gt;Boxplot&lt;/h3&gt;
&lt;p&gt;The &lt;strong&gt;boxplot&lt;/strong&gt; scales very well, when we have a number of groups in the single-digits, since we can put the different boxes side-by-side.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sns.boxplot(x=&#39;Arm&#39;, y=&#39;Income&#39;, data=df.sort_values(&#39;Arm&#39;));
plt.title(&amp;quot;Boxplot, multiple groups&amp;quot;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/comparing_distributions_78_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;From the plot, it looks like the distribution of &lt;code&gt;income&lt;/code&gt; is different across treatment arms, with higher numbered arms having a higher average income.&lt;/p&gt;
&lt;h3 id=&#34;violin-plot&#34;&gt;Violin Plot&lt;/h3&gt;
&lt;p&gt;A very nice extension of the boxplot that combines summary statistics and kernel density estimation is the  &lt;strong&gt;violinplot&lt;/strong&gt;. The violinplot plots separate densities along the y axis so that they don&amp;rsquo;t overlap. By default, it also adds a miniature boxplot inside.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sns.violinplot(x=&#39;Arm&#39;, y=&#39;Income&#39;, data=df.sort_values(&#39;Arm&#39;));
plt.title(&amp;quot;Violin Plot, multiple groups&amp;quot;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/comparing_distributions_82_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;As for the boxplot, the violin plot suggests that income is different across treatment arms.&lt;/p&gt;
&lt;h3 id=&#34;ridgeline-plot&#34;&gt;Ridgeline Plot&lt;/h3&gt;
&lt;p&gt;Lastly, the &lt;strong&gt;ridgeline plot&lt;/strong&gt; plots multiple kernel density distributions along the x-axis, making them more intuitive than the violin plot but partially overlapping them. Unfortunately, there is no default ridgeline plot neither in &lt;code&gt;matplotlib&lt;/code&gt; nor in &lt;code&gt;seaborn&lt;/code&gt;. We need to import it from &lt;a href=&#34;https://github.com/leotac/joypy&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;joypy&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from joypy import joyplot

joyplot(df, by=&#39;Arm&#39;, column=&#39;Income&#39;, colormap=sns.color_palette(&amp;quot;crest&amp;quot;, as_cmap=True));
plt.xlabel(&#39;Income&#39;);
plt.title(&amp;quot;Ridgeline Plot, multiple groups&amp;quot;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/comparing_distributions_86_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Again, the ridgeline plot suggests that higher numbered treatment arms have higher income. From this plot it is also easier to appreciate the different shapes of the distributions.&lt;/p&gt;
&lt;h2 id=&#34;multiple-groups---tests&#34;&gt;Multiple Groups - Tests&lt;/h2&gt;
&lt;p&gt;Lastly, let&amp;rsquo;s consider hypothesis tests to compare multiple groups. For simplicity, we will concentrate on the most popular one: the F-test.&lt;/p&gt;
&lt;h3 id=&#34;f-test&#34;&gt;F-test&lt;/h3&gt;
&lt;p&gt;With multiple groups, the most popular test is the &lt;a href=&#34;https://en.wikipedia.org/wiki/F-test&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;F-test&lt;/strong&gt;&lt;/a&gt;. The F-test compares the variance of a variable across different groups. This analysis is also called &lt;a href=&#34;https://en.wikipedia.org/wiki/Analysis_of_variance&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;analysis of variance, or &lt;strong&gt;ANOVA&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;In practice, the F-test statistic is&lt;/p&gt;
&lt;p&gt;$$
\text{stat} = \frac{\text{between-group variance}}{\text{within-group variance}} = \frac{\sum_{g} \big( \bar x_g - \bar x \big) / (G-1)}{\sum_{g} \sum_{i \in g} \big( \bar x_i - \bar x_g \big) / (N-G)}
$$&lt;/p&gt;
&lt;p&gt;Where $G$ is the number of groups, $N$ is the number of observations, $\bar x$ is the overall mean and $\bar x_g$ is the mean within group $g$. Under the null hypothesis of group independence, the f-statistic is &lt;a href=&#34;https://en.wikipedia.org/wiki/F-distribution&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;F-distributed&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from scipy.stats import f_oneway

income_groups = [df.loc[df[&#39;Arm&#39;]==arm, &#39;Income&#39;].values for arm in df[&#39;Arm&#39;].dropna().unique()]
stat, p_value = f_oneway(*income_groups)
print(f&amp;quot;F Test: statistic={stat:.4f}, p-value={p_value:.4f}&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;F Test: statistic=9.0911, p-value=0.0000
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The test p-value is basically zero, implying a &lt;strong&gt;strong rejection&lt;/strong&gt; of the null hypothesis of no differences in the &lt;code&gt;income&lt;/code&gt; distribution across treatment arms.&lt;/p&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;In this post we have see a ton of different ways to &lt;strong&gt;compare two or more distributions&lt;/strong&gt;, both visually and statistically. This is a primary concern in many applications, but especially in causal inference where we use randomization to make treatment and control group as comparable as possible.&lt;/p&gt;
&lt;p&gt;We have also seen how different methods might be better suited for &lt;strong&gt;different situations&lt;/strong&gt;. Visual methods are great to build intuition, but statistical methods are essential for decision-making, since we need to be able to assess the magnitude and statistical significance of the differences.&lt;/p&gt;
&lt;h3 id=&#34;references&#34;&gt;References&lt;/h3&gt;
&lt;p&gt;[1] Student, &lt;a href=&#34;https://www.jstor.org/stable/2331554&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The Probable Error of a Mean&lt;/a&gt; (1908), &lt;em&gt;Biometrika&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;[2] F. Wilcoxon, &lt;a href=&#34;https://www.jstor.org/stable/3001968&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Individual Comparisons by Ranking Methods&lt;/a&gt; (1945), &lt;em&gt;Biometrics Bulletin&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;[3] B. L. Welch, &lt;a href=&#34;https://academic.oup.com/biomet/article/34/1-2/28/210174&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The generalization of &amp;ldquo;Student&amp;rsquo;s&amp;rdquo; problem when several different population variances are involved&lt;/a&gt; (1947), &lt;em&gt;Biometrika&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;[4] H. B. Mann, D. R. Whitney, &lt;a href=&#34;https://www.jstor.org/stable/2236101&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;On a Test of Whether one of Two Random Variables is Stochastically Larger than the Other&lt;/a&gt; (1947), &lt;em&gt;The Annals of Mathematical Statistics&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;[5] E. Brunner, U. Munzen, &lt;a href=&#34;https://onlinelibrary.wiley.com/doi/abs/10.1002/%28SICI%291521-4036%28200001%2942:1%3C17::AID-BIMJ17%3E3.0.CO;2-U&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The Nonparametric Behrens-Fisher Problem: Asymptotic Theory and a Small-Sample Approximation&lt;/a&gt; (2000), &lt;em&gt;Biometrical Journal&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;[6] A. N. Kolmogorov, &lt;a href=&#34;https://link.springer.com/chapter/10.1007/978-94-011-2260-3_15&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Sulla determinazione empirica di una legge di distribuzione&lt;/a&gt; (1933), &lt;em&gt;Giorn. Ist. Ital. Attuar.&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;[7] H. Cramér, &lt;a href=&#34;https://www.tandfonline.com/doi/abs/10.1080/03461238.1928.10416862&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;On the composition of elementary errors&lt;/a&gt; (1928), &lt;em&gt;Scandinavian Actuarial Journal&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;[8] R. von Mises, &lt;a href=&#34;https://www.ams.org/journals/bull/1937-43-05/S0002-9904-1937-06520-7/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Wahrscheinlichkeit statistik und wahrheit&lt;/a&gt; (1936), &lt;em&gt;Bulletin of the American Mathematical Society&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;[9] T. W. Anderson, D. A. Darling, &lt;a href=&#34;https://projecteuclid.org/journals/annals-of-mathematical-statistics/volume-23/issue-2/Asymptotic-Theory-of-Certain-Goodness-of-Fit-Criteria-Based-on/10.1214/aoms/1177729437.full&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Asymptotic Theory of Certain &amp;ldquo;Goodness of Fit&amp;rdquo; Criteria Based on Stochastic Processes&lt;/a&gt; (1953), &lt;em&gt;The Annals of Mathematical Statistics&lt;/em&gt;.&lt;/p&gt;
&lt;h3 id=&#34;related-articles&#34;&gt;Related Articles&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/b63dc69e3d8c&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Goodbye Scatterplot, Welcome Binned Scatterplot&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;code&#34;&gt;Code&lt;/h3&gt;
&lt;p&gt;You can find the original Jupyter Notebook here:&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/distr.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/distr.ipynb&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Mean vs Median Causal Effect</title>
      <link>https://matteocourthoud.github.io/post/quantile_regression/</link>
      <pubDate>Mon, 10 Jul 2023 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/post/quantile_regression/</guid>
      <description>&lt;p&gt;&lt;em&gt;An introduction to quantile regression.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;In A/B tests, a.k.a. &lt;a href=&#34;https://en.wikipedia.org/wiki/Randomized_controlled_trial&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;randomized controlled trials&lt;/a&gt;, we usually estimate the &lt;strong&gt;average treatment effect (ATE)&lt;/strong&gt;: effect of a treatment (a drug, ad, product, &amp;hellip;) on an outcome of interest (a disease, firm revenue, customer satisfaction, &amp;hellip;), where the &amp;ldquo;average&amp;rdquo; is taken over the test subjects (patients, users, customers, &amp;hellip;). The ATE is a very useful quantity since it tells us the effect that we can expect if we were to treat a new subject with the same treatment.&lt;/p&gt;
&lt;p&gt;However, sometimes we might be interested in quantities different from the average, such as the &lt;a href=&#34;https://en.wikipedia.org/wiki/Median&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;median&lt;/strong&gt;&lt;/a&gt;. The median is an alternative measure of &lt;em&gt;central tendency&lt;/em&gt; that is more robust to outliers and is often more informative with skewed distributions. More generally, we might want to estimate the effect for different &lt;a href=&#34;https://en.wikipedia.org/wiki/Quantile&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;quantiles&lt;/a&gt; of the outcome distribution. A &lt;strong&gt;common use-case&lt;/strong&gt; is studying the impact of a UI change on the loading time of a website: a slightly heavier website might translate in an imperceptible change for most users, but a big change for a few users with very slow connections. Another common use-case is studying the impact of a product change on a product that is bought by few people: do existing customers buy it more or are we attracting new customers?&lt;/p&gt;
&lt;p&gt;These questions are hard to answer with linear regression that estimates the &lt;em&gt;average treatment effect&lt;/em&gt;. A more suitable tool is &lt;strong&gt;quantile regression&lt;/strong&gt; that can instead estimate the &lt;em&gt;median treatment effect&lt;/em&gt;. In this article we are going to cover a brief introduction to quantile regression and the estimation of quantile treatment effects.&lt;/p&gt;
&lt;h2 id=&#34;loyalty-cards-and-spending&#34;&gt;Loyalty Cards and Spending&lt;/h2&gt;
&lt;p&gt;Suppose we were an &lt;strong&gt;online store&lt;/strong&gt; and we wanted to increase sales. We decide to offer our customers a &lt;strong&gt;loyalty card&lt;/strong&gt; that grants them discounts as they increase their spend in the store. We would like to assess if the loyalty card is effective in increasing sales so we run an &lt;strong&gt;A/B test&lt;/strong&gt;: we offer the loyalty card only to a subset of our customers, at random.&lt;/p&gt;
&lt;p&gt;I import the data generating process &lt;code&gt;dgp_loyalty()&lt;/code&gt; from &lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/src/dgp.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;src.dgp&lt;/code&gt;&lt;/a&gt;. I also import some plotting functions and libraries from &lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/src/utils.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;src.utils&lt;/code&gt;&lt;/a&gt;.  To include not only code but also data and tables, I use &lt;a href=&#34;https://deepnote.com&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Deepnote&lt;/a&gt;, a Jupyter-like web-based collaborative notebook environment.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;%matplotlib inline
%config InlineBackend.figure_format = &#39;retina&#39;

from src.utils import *
from src.dgp import dgp_loyalty
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, let&amp;rsquo;s have a look at the data. We have information on $10.000$ customers, for whom we observe their &lt;code&gt;spend&lt;/code&gt; and whether they were offered the &lt;code&gt;loyalty&lt;/code&gt; card. We also observe some demographics, like &lt;code&gt;age&lt;/code&gt; and &lt;code&gt;gender&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df = dgp_loyalty().generate_data()
df.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;loyalty&lt;/th&gt;
      &lt;th&gt;spend&lt;/th&gt;
      &lt;th&gt;age&lt;/th&gt;
      &lt;th&gt;gender&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;30&lt;/td&gt;
      &lt;td&gt;Male&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;26&lt;/td&gt;
      &lt;td&gt;Male&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;27&lt;/td&gt;
      &lt;td&gt;Male&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;29&lt;/td&gt;
      &lt;td&gt;Male&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;23&lt;/td&gt;
      &lt;td&gt;Female&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;Interestingly, we notice that the outcome of interest, &lt;code&gt;spend&lt;/code&gt;, seems to have a lot of zeros. Let&amp;rsquo;s dig deeper.&lt;/p&gt;
&lt;h2 id=&#34;mean-vs-median&#34;&gt;Mean vs Median&lt;/h2&gt;
&lt;p&gt;Before analyzing our experiment, let&amp;rsquo;s have a look at our outcome variable, &lt;code&gt;spend&lt;/code&gt;. We first inspect it using centrality measures. We have two main options: the &lt;strong&gt;mean&lt;/strong&gt; and the &lt;strong&gt;median&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;First of all, what are they? The mean captures the average value, while the median captures the value in the middle of the distribution. In general, the mean is mathematically more tractable and easier to interpret, while the median is more robust to outliers. You can find plenty of articles online comparing the two measures and suggesting which one is more appropriate and when. Let&amp;rsquo;s have a look at the mean and median &lt;code&gt;spend&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;np.mean(df[&#39;spend&#39;])
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;28.136224
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;np.median(df[&#39;spend&#39;])
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;0.0
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;How do we interpret these two numbers? People spend on average 28\$ on our store. However, more than 50% of people don&amp;rsquo;t spend anything. As we can see, both measures are very informative and, to a certain extent, complementary. We can better understand the distribution of &lt;code&gt;spend&lt;/code&gt; by plotting its histogram.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sns.histplot(data=df, x=&amp;quot;spend&amp;quot;).set(ylabel=&#39;&#39;, title=&#39;Spending Distribution&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/quantile_regression_14_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;As previewed by the values of the mean and the median, the distribution of &lt;code&gt;spend&lt;/code&gt; is very skewed, with more than 5000 customers (out of 10000) not spending anything.&lt;/p&gt;
&lt;p&gt;One natural question then is: are we interested in the effect of the &lt;code&gt;loyalty&lt;/code&gt; card on average &lt;code&gt;spend&lt;/code&gt; or on median &lt;code&gt;spend&lt;/code&gt;? The first would tell us if customers spend more on average, while the second would tell us if the average customer spends more.&lt;/p&gt;
&lt;p&gt;Linear regression can tell us the effect of the &lt;code&gt;loyalty&lt;/code&gt; card on average &lt;code&gt;spend&lt;/code&gt;. However, what can we do if we were interested in the effect of the &lt;code&gt;loyalty&lt;/code&gt; card on median &lt;code&gt;spend&lt;/code&gt; (or other quantiles)? The answer is &lt;strong&gt;quantile regression&lt;/strong&gt;.&lt;/p&gt;
&lt;h2 id=&#34;quantile-regression&#34;&gt;Quantile Regression&lt;/h2&gt;
&lt;p&gt;With &lt;strong&gt;linear regression&lt;/strong&gt;, we try to estimate the &lt;em&gt;conditional expectation function&lt;/em&gt; of an outcome variable $Y$ (&lt;code&gt;spend&lt;/code&gt; in our example) with respect to one or more explanatory variables $X$ (&lt;code&gt;loyalty&lt;/code&gt; in our example).&lt;/p&gt;
&lt;p&gt;$$
\mathbb E \big[ Y \big| X \big]
$$&lt;/p&gt;
&lt;p&gt;In other words, we want to find a function $f$ such that $f(X) = \mathbb E[Y|X]$. We do so, by solving the following minimization problem:&lt;/p&gt;
&lt;p&gt;$$
\hat f(X) = \arg \min_{f} \mathbb E \big[ Y - f(X) \big]^2
$$&lt;/p&gt;
&lt;p&gt;It can be shown that the function $f$ that solves this minimization is indeed the conditional expectation of $Y$, with respect to $X$.&lt;/p&gt;
&lt;p&gt;Since $f(X)$ can be infinite dimensional, we usually estimate a parametric form of $f(X)$. The most common one is the linear form $f(X) = \beta X$, where $\beta$ is estimated by solving the corresponding minimization problem:&lt;/p&gt;
&lt;p&gt;$$
\hat \beta = \arg \min_{\beta} \mathbb E \big[ Y - \beta X \big]^2
$$&lt;/p&gt;
&lt;p&gt;The linear form is not just convenient, but it can be interpreted as the best local approximation of $f(X)$, referring to Taylor&amp;rsquo;s expansion.&lt;/p&gt;
&lt;p&gt;With &lt;strong&gt;quantile regression&lt;/strong&gt;, we do the &lt;strong&gt;same&lt;/strong&gt;. The only difference is that, instead of estimating the conditional expectation of $Y$ with respect to $X$, we want to estimate the $q$-&lt;a href=&#34;https://en.wikipedia.org/wiki/Quantile&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;quantile&lt;/a&gt; of $Y$ with respect to $X$.&lt;/p&gt;
&lt;p&gt;$$
\mathbb Q_q \big[ Y \big| X \big]
$$&lt;/p&gt;
&lt;p&gt;First of all, what is a &lt;strong&gt;quantile&lt;/strong&gt;? The &lt;a href=&#34;https://en.wikipedia.org/wiki/Quantile&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Wikipedia definition&lt;/a&gt; says&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;In statistics and probability, quantiles are cut points dividing the range of a probability distribution into continuous intervals with equal probabilities, or dividing the observations in a sample in the same way. Common quantiles have special names, such as quartiles (four groups), deciles (ten groups), and percentiles (100 groups).&amp;rdquo;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;For example, the 0.1-quantile represents the value that sits on the right of 10% of the mass of the distribution. The &lt;strong&gt;median&lt;/strong&gt; is the 0.5-quantile (or, equivalently, the $50^{th}$ percentile or the $5^{th}$ decile) and corresponds with the value in the center of the distribution. Let&amp;rsquo;s see a simple example with a &lt;a href=&#34;https://en.wikipedia.org/wiki/Log-normal_distribution&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;log-normal distribution&lt;/a&gt;. I plot the three quartiles that divide the data in four equally sized bins.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;data = np.random.lognormal(0, 1, 1_000_000);
sns.histplot(data).set(title=&#39;Lognormal Distribution&#39;, xlim=(0,10))
plt.axvline(x=np.percentile(data, 25), c=&#39;C8&#39;, label=&#39;25th percentile&#39;)
plt.axvline(x=np.median(data), c=&#39;C1&#39;, label=&#39;Median (50th pct)&#39;)
plt.axvline(x=np.percentile(data, 75), c=&#39;C3&#39;, label=&#39;75th percentile&#39;)
plt.legend();
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/quantile_regression_18_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;As we can see, the three quartiles divide the data into four bins, of equal size.&lt;/p&gt;
&lt;p&gt;So, what is the &lt;strong&gt;objective&lt;/strong&gt; of quantile regression? The objective is to find a function $f$ such that $f(X) = F^{-1}(y_q)$, where $F$ is the &lt;a href=&#34;https://en.wikipedia.org/wiki/Cumulative_distribution_function&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;cumulative distribution function&lt;/a&gt; of $Y$ and $y_q$ is the $q$-quantile of the distribution of $Y$.&lt;/p&gt;
&lt;p&gt;How do we do this? It can be shown with a little linear algebra that we can obtain the conditional quantile as the solution of the following minimization problem:&lt;/p&gt;
&lt;p&gt;$$
\hat f(X) = \arg \min_{f} \mathbb E \big[ \rho_q (Y - f(X)) \big] = \arg \min_{f} \ (1-q) \int_{-\infty}^{f(x)} (y - f(x)) \text{d} F(y) + q \int_{f(x)}^{\infty} (y - f(x)) \text{d} F(y)
$$&lt;/p&gt;
&lt;p&gt;Where $\rho_q$ is an auxiliary weighting function with the following shape.&lt;/p&gt;
&lt;img src=&#34;fig/rho.png&#34; width=&#34;400px&#34;/&gt;
&lt;p&gt;What is the &lt;strong&gt;intuition&lt;/strong&gt; behind the objective function?&lt;/p&gt;
&lt;p&gt;The idea is that we can interpret the equation as follows&lt;/p&gt;
&lt;p&gt;$$
\mathbb E \big[ \rho_q (Y - f(X)) \big] = (1-q) \underset{\color{red}{\text{mass of distribution below }f(x)}}{\int_{-\infty}^{f(x)} (y - f(x)) \text{d} F(y)} + q \underset{\color{red}{\text{mass of distribution above }f(x)}}{\int_{f(x)}^{\infty} (y - f(x)) \text{d} F(y)} \overset{\color{blue}{\text{if } f(x) = y_q}}{=} - (1-q) q + q (1-q) = 0
$$&lt;/p&gt;
&lt;p&gt;So that, when $f(X)$ corresponds with the quantile $y_q$, the value of the objective function is zero.&lt;/p&gt;
&lt;p&gt;Exactly as before, we can estimate a &lt;strong&gt;parametric form&lt;/strong&gt; of $f$ and, exactly as before, we can interpret it as a best local approximation (not trivially though, see &lt;a href=&#34;https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1468-0262.2006.00671.x&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Angrist, Chernozhukov, and Fernández-Val (2006)&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;$$
\hat \beta_q = \arg \min_{\beta} \mathbb E \big[ \rho_q (Y - \beta X ) \big]
$$&lt;/p&gt;
&lt;p&gt;We wrote $\hat \beta_q$ to indicate that this is the coefficient for the best linear approximation of the conditional $q$-quantile function.&lt;/p&gt;
&lt;p&gt;How do we &lt;strong&gt;estimate&lt;/strong&gt; a quantile regression?&lt;/p&gt;
&lt;h2 id=&#34;estimation&#34;&gt;Estimation&lt;/h2&gt;
&lt;p&gt;The &lt;a href=&#34;https://www.statsmodels.org/stable/index.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;statsmodels&lt;/a&gt; package allows us to estimate quantile regression with the the &lt;a href=&#34;https://www.statsmodels.org/dev/generated/statsmodels.regression.quantile_regression.QuantReg.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;quantreg()&lt;/code&gt;&lt;/a&gt; function. We just need to specify the quantile $q$ when we fit the model. Let&amp;rsquo;s use $q=0.5$, which corresponds with the median.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;smf.quantreg(&amp;quot;spend ~ loyalty&amp;quot;, data=df).fit(q=0.5).summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
      &lt;td&gt;&lt;/td&gt;         &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt; &lt;td&gt; 8.668e-07&lt;/td&gt; &lt;td&gt;    0.153&lt;/td&gt; &lt;td&gt; 5.66e-06&lt;/td&gt; &lt;td&gt; 1.000&lt;/td&gt; &lt;td&gt;   -0.300&lt;/td&gt; &lt;td&gt;    0.300&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;loyalty&lt;/th&gt;   &lt;td&gt;    3.4000&lt;/td&gt; &lt;td&gt;    0.217&lt;/td&gt; &lt;td&gt;   15.649&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    2.974&lt;/td&gt; &lt;td&gt;    3.826&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;Quantile regression estimates a positive coefficient for &lt;code&gt;loyalty&lt;/code&gt;. How does this estimate compare with linear regression?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;smf.ols(&amp;quot;spend ~ loyalty&amp;quot;, data=df).fit().summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
      &lt;td&gt;&lt;/td&gt;         &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt; &lt;td&gt;   25.6583&lt;/td&gt; &lt;td&gt;    0.564&lt;/td&gt; &lt;td&gt;   45.465&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;   24.552&lt;/td&gt; &lt;td&gt;   26.765&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;loyalty&lt;/th&gt;   &lt;td&gt;    4.9887&lt;/td&gt; &lt;td&gt;    0.801&lt;/td&gt; &lt;td&gt;    6.230&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    3.419&lt;/td&gt; &lt;td&gt;    6.558&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;The estimated coefficient with linear regression is higher. What does it mean? We will spend more time on the &lt;em&gt;interpretation&lt;/em&gt; of quantile regression coefficients later.&lt;/p&gt;
&lt;p&gt;Can we condition the analysis on other variables? We suspect that &lt;code&gt;spend&lt;/code&gt; is also affected by other variables and we want to increase the precision of our estimate by also conditioning the analysis on &lt;code&gt;age&lt;/code&gt; and &lt;code&gt;gender&lt;/code&gt;. We can just add the variables to the &lt;code&gt;quantreg()&lt;/code&gt; model.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;smf.quantreg(&amp;quot;spend ~ loyalty + age + gender&amp;quot;, data=df).fit(q=0.5).summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
         &lt;td&gt;&lt;/td&gt;           &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt;      &lt;td&gt;  -50.5353&lt;/td&gt; &lt;td&gt;    1.053&lt;/td&gt; &lt;td&gt;  -47.977&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;  -52.600&lt;/td&gt; &lt;td&gt;  -48.471&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;gender[T.Male]&lt;/th&gt; &lt;td&gt;  -20.2963&lt;/td&gt; &lt;td&gt;    0.557&lt;/td&gt; &lt;td&gt;  -36.410&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;  -21.389&lt;/td&gt; &lt;td&gt;  -19.204&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;loyalty&lt;/th&gt;        &lt;td&gt;    4.5747&lt;/td&gt; &lt;td&gt;    0.546&lt;/td&gt; &lt;td&gt;    8.374&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    3.504&lt;/td&gt; &lt;td&gt;    5.646&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;age&lt;/th&gt;            &lt;td&gt;    2.3663&lt;/td&gt; &lt;td&gt;    0.026&lt;/td&gt; &lt;td&gt;   92.293&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    2.316&lt;/td&gt; &lt;td&gt;    2.417&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;The coefficient of &lt;code&gt;loyalty&lt;/code&gt; increases when we condition the analysis on &lt;code&gt;age&lt;/code&gt; and &lt;code&gt;gender&lt;/code&gt;. This is true also for linear regresssion.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;smf.ols(&amp;quot;spend ~ loyalty + age + gender&amp;quot;, data=df).fit().summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
         &lt;td&gt;&lt;/td&gt;           &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt;      &lt;td&gt;  -57.4466&lt;/td&gt; &lt;td&gt;    0.911&lt;/td&gt; &lt;td&gt;  -63.028&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;  -59.233&lt;/td&gt; &lt;td&gt;  -55.660&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;gender[T.Male]&lt;/th&gt; &lt;td&gt;  -26.3170&lt;/td&gt; &lt;td&gt;    0.482&lt;/td&gt; &lt;td&gt;  -54.559&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;  -27.262&lt;/td&gt; &lt;td&gt;  -25.371&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;loyalty&lt;/th&gt;        &lt;td&gt;    3.9101&lt;/td&gt; &lt;td&gt;    0.473&lt;/td&gt; &lt;td&gt;    8.272&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    2.983&lt;/td&gt; &lt;td&gt;    4.837&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;age&lt;/th&gt;            &lt;td&gt;    2.7688&lt;/td&gt; &lt;td&gt;    0.022&lt;/td&gt; &lt;td&gt;  124.800&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    2.725&lt;/td&gt; &lt;td&gt;    2.812&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;There are a couple of things that we haven&amp;rsquo;t mentioned yet. The first one is &lt;strong&gt;inference&lt;/strong&gt;. How do we compute confidence intervals and p-values for our estimates in quantile regression?&lt;/p&gt;
&lt;h2 id=&#34;inference&#34;&gt;Inference&lt;/h2&gt;
&lt;p&gt;The asymptotic variance of the estimate $a$ of the quantile $q$ of a distribution $F$ is given by&lt;/p&gt;
&lt;p&gt;$$
AVar(y) = q(1-q) f^{-2}(y)
$$&lt;/p&gt;
&lt;p&gt;where $f$ is the &lt;a href=&#34;https://en.wikipedia.org/wiki/Probability_density_function&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;density function&lt;/a&gt; of $F$. This expression can be decomposed into &lt;strong&gt;two components&lt;/strong&gt;: $q(1-q)$ and $f^{-2}(y)$.&lt;/p&gt;
&lt;p&gt;The first component, $q(1-q)$, basically tells us that the variance of a quantile is higher the more the quantile is closer to the center of the distribution. Why is that so? First, we need to think about when the quantile of a point changes in response to a change in the value of a second point. The quantile changes when the second point swaps from left to right (or viceversa) of the first point. This is intuitively very easy if the first point lies in the middle of the distribution, but very hard if it lies at the extreme.&lt;/p&gt;
&lt;p&gt;The second component, $f^{-2}(a)$, instead tells us that this side swapping is more likely if the first point is surrounded by a lot of points.&lt;/p&gt;
&lt;p&gt;Importantly, estimating the variance of a quantile requires an estimate of the whole distribution of $Y$. This is done via approximation and it can be computationally very intensive. However, alternative procedures like the bootstrap or the &lt;a href=&#34;https://towardsdatascience.com/6ca4a1d45148&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;bayesian bootstrap&lt;/a&gt; are always available.&lt;/p&gt;
&lt;p&gt;The second thing that we haven&amp;rsquo;t talked about yet is the &lt;strong&gt;interpretation&lt;/strong&gt; of the estimated coefficients. We got a lower coefficient of &lt;code&gt;loyalty&lt;/code&gt; on &lt;code&gt;spend&lt;/code&gt; with median regression. What does it mean?&lt;/p&gt;
&lt;h2 id=&#34;interpretation&#34;&gt;Interpretation&lt;/h2&gt;
&lt;p&gt;The &lt;strong&gt;interpretation&lt;/strong&gt; of linear regression coefficients is straightforward: each coefficient is the derivative of the conditional expectation function $\mathbb E[Y|X]$ with respect to one dimension of $X$. In our case, we can interpret the regression coefficient of &lt;code&gt;loyalty&lt;/code&gt; as the average &lt;code&gt;spend&lt;/code&gt; increase from being offered a loyalty card. Crucially, here &amp;ldquo;average&amp;rdquo; means that this holds true for &lt;em&gt;each customer&lt;/em&gt;, on average.&lt;/p&gt;
&lt;p&gt;However, the interpretation of quantile regression coefficients is &lt;strong&gt;tricky&lt;/strong&gt;. Before, we were tempted to say that the &lt;code&gt;loyalty&lt;/code&gt; card increases the spend of the median customer by 3.4\$. But &lt;strong&gt;what does it mean&lt;/strong&gt;? Is it the same median customer that spends more or do we have a different median customer? This might seem like a philosophical question but it has important implications on reporting of quantile regression results. In the first case, we are making a statement that, as for the interpretation of linear regression coefficients, applies to a &lt;em&gt;single individual&lt;/em&gt;. In the second case, we are making a statement about the &lt;em&gt;distribution&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1468-0262.2005.00570.x&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Chernozhukov and Hansen (2005)&lt;/a&gt; show that a strong but helpful assumption is &lt;strong&gt;rank invariance&lt;/strong&gt;: assuming that the treatment &lt;strong&gt;does not shift&lt;/strong&gt; the relative composition of the distribution. In other words, if we rank people by &lt;code&gt;spend&lt;/code&gt; before the experiment, we assume that this ranking is not affected by the introduction of the &lt;code&gt;loyalty&lt;/code&gt; card. If I was spending less than you before, I might spend more afterwards, but still less than you (for any two people).&lt;/p&gt;
&lt;p&gt;Under this assumption, we can interpret the quantile coefficients as &lt;strong&gt;marginal effects for single individuals&lt;/strong&gt; sitting at different points of the outcome distribution, as in the first interpretation provided above. Moreover, we can report the treatment effect for many quantiles and interpret each one of them as a local effect for a different individual. Let&amp;rsquo;s plot the distribution of treatment effects, for different quantiles of &lt;code&gt;spend&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def plot_quantile_TE(df, formula, q, varname):
    df_results = pd.DataFrame()
    for q in np.arange(q, 1-q, q):
        qreg = smf.quantreg(formula, data=df).fit(q=q)
        temp = pd.DataFrame({&#39;q&#39;: [q],
                             &#39;coeff&#39;: [qreg.params[varname]], 
                             &#39;std&#39;: [qreg.bse[varname]],
                             &#39;ci_lower&#39;: [qreg.conf_int()[0][varname]],
                             &#39;ci_upper&#39;: [qreg.conf_int()[1][varname]]})
        df_results = pd.concat([df_results, temp]).reset_index(drop=True)
    
    # Plot
    fig, ax = plt.subplots()
    sns.lineplot(data=df_results, x=&#39;q&#39;, y=&#39;coeff&#39;)
    ax.fill_between(data=df_results, x=&#39;q&#39;, y1=&#39;ci_lower&#39;, y2=&#39;ci_upper&#39;, alpha=0.1);
    plt.axhline(y=0, c=&amp;quot;k&amp;quot;, lw=2, zorder=1)
    ols_coeff = smf.ols(formula, data=df).fit().params[varname]
    plt.axhline(y=ols_coeff, ls=&amp;quot;--&amp;quot;, c=&amp;quot;C1&amp;quot;, label=&amp;quot;OLS coefficient&amp;quot;, zorder=1)
    plt.legend()
    plt.title(&amp;quot;Estimated coefficient, by quantile&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plot_quantile_TE(df, formula=&amp;quot;spend ~ loyalty&amp;quot;, varname=&#39;loyalty&#39;, q=0.05)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/quantile_regression_37_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;This plot is &lt;strong&gt;extremely insightful&lt;/strong&gt;: for almost half of the customers, the &lt;code&gt;loyalty&lt;/code&gt; card has no effect. On the other hand, customers that were already spending something end up spending even more (around 10/12\$ more). This is a very powerful insight that we would have missed with linear regression that estimated an average effect of 5\$.&lt;/p&gt;
&lt;p&gt;We can repeat the same exercise, conditioning the analysis on &lt;code&gt;gender&lt;/code&gt; and &lt;code&gt;age&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plot_quantile_TE(df, formula=&amp;quot;spend ~ loyalty + age + gender&amp;quot;, varname=&#39;loyalty&#39;, q=0.05)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/quantile_regression_39_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Conditioning on other covariates removes a lot of the heterogeneity in treatment effects. The &lt;code&gt;loyalty&lt;/code&gt; card increases spending for most people, it&amp;rsquo;s demographic characteristics that are responsible for no spending to begin with.&lt;/p&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;In this article, we have explored a different &lt;strong&gt;causal estimand&lt;/strong&gt;: median treatment effects. How does it compare with the average treatment effect that we usually estimate? The pros and cons are closely related to the pros and cons of the median with respect to the mean as a measure of &lt;em&gt;central tendency&lt;/em&gt;. Median treatment effects are more informative on what is the effect on the average subject and are more robust to outliers. However, they are much more computationally intensive and they require strong assumptions for identification, such as rank invariance.&lt;/p&gt;
&lt;h3 id=&#34;references&#34;&gt;References&lt;/h3&gt;
&lt;p&gt;[1] R. Koenker, &lt;a href=&#34;https://www.cambridge.org/core/books/quantile-regression/C18AE7BCF3EC43C16937390D44A328B1&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Quantile Regression&lt;/a&gt; (1996), Cambridge University Press.&lt;/p&gt;
&lt;p&gt;[1] R. Koenker, K. Hallock, &lt;a href=&#34;https://www.aeaweb.org/articles?id=10.1257/jep.15.4.143&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Quantile Regression&lt;/a&gt;, (2001), &lt;em&gt;Journal of Economic Perspectives&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;[2] V. Chernozhukov, C. Hansen, &lt;a href=&#34;https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1468-0262.2005.00570.x&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;An IV Model of Quantile Treatment Effects&lt;/a&gt; (2005), &lt;em&gt;Econometrica&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;[3] J. Angrist, V. Chernozhukov, I. Fernández-Val, &lt;a href=&#34;https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1468-0262.2006.00671.x&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Quantile Regression under Misspecification, with an Application to the U.S. Wage Structure&lt;/a&gt; (2006), &lt;em&gt;Econometrica&lt;/em&gt;.&lt;/p&gt;
&lt;h3 id=&#34;related-articles&#34;&gt;Related Articles&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/b63dc69e3d8c&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;DAGs and Control Variables&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/6ca4a1d45148&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The Bayesian Bootstrap&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/a928f67413e4&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Goodbye Scatterplot, Welcome Binned Scatterplot&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;code&#34;&gt;Code&lt;/h3&gt;
&lt;p&gt;You can find the original Jupyter Notebook here:&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/quantile_reg.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/quantile_reg.ipynb&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Omitted Variable Bias And How To Deal With It</title>
      <link>https://matteocourthoud.github.io/post/omitted_variable_bias/</link>
      <pubDate>Mon, 10 Jul 2023 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/post/omitted_variable_bias/</guid>
      <description>&lt;p&gt;In causal inference, &lt;strong&gt;bias&lt;/strong&gt; is extremely problematic because it makes inference not valid. Bias generally means that an estimator will not deliver the estimate of the true effect, on average.&lt;/p&gt;
&lt;p&gt;This is why, in general, we prefer estimators that are &lt;strong&gt;unbiased&lt;/strong&gt;, at the cost of a higher variance, i.e. more noise. Does it mean that every biased estimator is useless? Actually no. Sometimes, with domain knowledge, we can still draw causal conclusions even with a biased estimator.&lt;/p&gt;
&lt;p&gt;In this post, we are going to review a specific but frequent source of bias, &lt;strong&gt;omitted variable bias (OVB)&lt;/strong&gt;. We are going to explore the causes of the bias and leverage these insights to make causal statements, despite the bias.&lt;/p&gt;
&lt;h2 id=&#34;theory&#34;&gt;Theory&lt;/h2&gt;
&lt;p&gt;Suppose we are interested in the effect of a variable $D$ on a variable $y$. However, there is a third variable $Z$ that we do not observe and that is correlated with both $D$ and $Y$. Assume the data generating process can be represented with the following &lt;a href=&#34;https://towardsdatascience.com/b63dc69e3d8c&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Directed Acyclic Graph (DAG)&lt;/strong&gt;&lt;/a&gt;. If you are not familiar with DAGs, I have written a short &lt;a href=&#34;https://towardsdatascience.com/b63dc69e3d8c&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;introduction here&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-mermaid&#34;&gt;flowchart LR
classDef included fill:#DCDCDC,stroke:#000000,stroke-width:2px;
classDef excluded fill:#ffffff,stroke:#000000,stroke-width:2px;
classDef unobserved fill:#ffffff,stroke:#000000,stroke-width:2px,stroke-dasharray: 5 5;

D((D))
Z((Z))
Y((Y))

D --&amp;gt; Y
Z --&amp;gt; D
Z --&amp;gt; Y

class D,Y excluded;
class Z unobserved;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Since there is a &lt;a href=&#34;https://towardsdatascience.com/b63dc69e3d8c&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;backdoor path&lt;/strong&gt;&lt;/a&gt; from $D$ to $y$ passing through $Z$, we need to condition our analysis on $Z$ in order to recover the causal effect of $D$ on $y$. If we could observe $Z$, we would run a linear regression of $y$ on $D$ and $Z$ to estimate the following model:&lt;/p&gt;
&lt;p&gt;$$
y = \alpha D + \gamma Z + \varepsilon
$$&lt;/p&gt;
&lt;p&gt;where $\alpha$ is the effect of interest. This regression is usually referred to as the &lt;strong&gt;long regression&lt;/strong&gt; since it includes all variables of the model.&lt;/p&gt;
&lt;p&gt;However, since we do not observe $Z$, we have to estimate the following model:&lt;/p&gt;
&lt;p&gt;$$
y = \alpha D + u
$$&lt;/p&gt;
&lt;p&gt;The corresponding regression is usually referred to as the &lt;strong&gt;short regression&lt;/strong&gt; since it does not include all the variables of the model&lt;/p&gt;
&lt;p&gt;What is the &lt;strong&gt;consequence&lt;/strong&gt; of estimating the short regression when the true model is the long one?&lt;/p&gt;
&lt;p&gt;In that case, the OLS estimator of $\alpha$ is&lt;/p&gt;
&lt;p&gt;$$
\begin{align}
\hat \alpha &amp;amp;= \frac{Cov(D, y)}{Var(D)} =
\newline
&amp;amp;= \frac{Cov(D, \alpha D + \gamma Z + \varepsilon)}{Var(D)} =
\newline
&amp;amp;= \frac{Cov(D, \alpha D)}{Var(D)} + \frac{Cov(D, \gamma Z)}{Var(D)} + \frac{Cov(D, \varepsilon)}{Var(D)} =
\newline
&amp;amp;= \alpha + \underbrace{ \gamma \frac{Cov(D, Z)}{Var(D)} }_{\text{omitted variable bias}}
\end{align}
$$&lt;/p&gt;
&lt;p&gt;Therefore, we can write the &lt;strong&gt;omitted variable bias&lt;/strong&gt; as&lt;/p&gt;
&lt;p&gt;$$
\text{OVB} = \gamma \delta \qquad \text{ where } \qquad \delta := \frac{Cov(D, Z)}{Var(D)}
$$&lt;/p&gt;
&lt;p&gt;The beauty of this formula is its &lt;strong&gt;interpretability&lt;/strong&gt;: the omitted variable bias consists in just &lt;strong&gt;two components&lt;/strong&gt;, both extremely easy to interpret.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\gamma$: the effect of $Z$ on $y$&lt;/li&gt;
&lt;li&gt;$\delta$: the effect of $D$ on $Z$&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;additional-controls&#34;&gt;Additional Controls&lt;/h3&gt;
&lt;p&gt;What happens if we had &lt;strong&gt;additional control variables&lt;/strong&gt; in the regression? For example, assume that besides the variable of interest $D$, we also observe a vector of other variables $X$ so that the &lt;strong&gt;long regression&lt;/strong&gt; is&lt;/p&gt;
&lt;p&gt;$$
y = \alpha D + \beta X + \gamma Z + \varepsilon
$$&lt;/p&gt;
&lt;p&gt;Thanks to the &lt;a href=&#34;https://towardsdatascience.com/59f801eb3299&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Frisch-Waugh-Lowell theorem&lt;/strong&gt;&lt;/a&gt;, we can simply &lt;strong&gt;partial-out&lt;/strong&gt; $X$ and express the omitted variable bias in terms of $D$ and $Z$.&lt;/p&gt;
&lt;p&gt;$$
\text{OVB} = \gamma \times \frac{Cov(D^{\perp X}, Z^{\perp X})}{Var(D^{\perp X})}
$$&lt;/p&gt;
&lt;p&gt;where $D^{\perp X}$ are the residuals from regressing $D$ on $X$ and $Z^{\perp X}$ are the residuals from regressing $Z$ on $X$. If you are not familiar with Frisch-Waugh-Lowell theorem, I have written a short &lt;a href=&#34;https://towardsdatascience.com/59f801eb3299&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;note here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2112.13398&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Chernozhukov, Cinelli, Newey, Sharma, Syrgkanis (2022)&lt;/a&gt; further generalize to analysis the the setting in which the control variables $X$ and the unobserved variables $Z$ enter the long model with a general functional form $f$&lt;/p&gt;
&lt;p&gt;$$
y = \alpha D + f(Z, X) + \varepsilon
$$&lt;/p&gt;
&lt;p&gt;You can find more details in their paper, but the underlying idea remains the same.&lt;/p&gt;
&lt;h2 id=&#34;example&#34;&gt;Example&lt;/h2&gt;
&lt;p&gt;Suppose we were a researcher interested in the relationship between &lt;strong&gt;education&lt;/strong&gt; and &lt;strong&gt;wages&lt;/strong&gt;. Does investing in education pay off in terms of future wages? Suppose we had data on wages for people with different years of education. Why not looking at the correlation between years of education and wages?&lt;/p&gt;
&lt;p&gt;The problem is that there might be many &lt;strong&gt;unobserved variables&lt;/strong&gt; that are correlated with both education and wages. For simplicity, let&amp;rsquo;s concentrate on &lt;strong&gt;ability&lt;/strong&gt;. People of higher ability might decide to invest more in education just because they are better in school and they get more opportunities. On the other hand, they might also get higher wages afterwards, purely because of their innate ability.&lt;/p&gt;
&lt;p&gt;We can represent the data generating process with the following &lt;strong&gt;Directed Acyclic Graph&lt;/strong&gt; (DAG).&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-mermaid&#34;&gt;flowchart TD
classDef included fill:#DCDCDC,stroke:#000000,stroke-width:2px;
classDef excluded fill:#ffffff,stroke:#000000,stroke-width:2px;
classDef unobserved fill:#ffffff,stroke:#000000,stroke-width:2px,stroke-dasharray: 5 5;

D((education))
Z((ability))
Y((wage))
X1((age))
X2((gender))

D --&amp;gt; Y
Z --&amp;gt; D
Z --&amp;gt; Y
X1 --&amp;gt; Y
X2 --&amp;gt; Y

class D,Y included;
class X1,X2 excluded;
class Z unobserved;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s load and inspect the &lt;strong&gt;data&lt;/strong&gt;. I import the data generating process from &lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/src/dgp.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;src.dgp&lt;/code&gt;&lt;/a&gt; and some plotting functions and libraries from &lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/src/utils.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;src.utils&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;%matplotlib inline
%config InlineBackend.figure_format = &#39;retina&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from src.utils import *
from src.dgp import dgp_educ_wages

df = dgp_educ_wages().generate_data(N=50)
df.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;age&lt;/th&gt;
      &lt;th&gt;gender&lt;/th&gt;
      &lt;th&gt;education&lt;/th&gt;
      &lt;th&gt;wage&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;62&lt;/td&gt;
      &lt;td&gt;male&lt;/td&gt;
      &lt;td&gt;6.0&lt;/td&gt;
      &lt;td&gt;3800.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;44&lt;/td&gt;
      &lt;td&gt;male&lt;/td&gt;
      &lt;td&gt;8.0&lt;/td&gt;
      &lt;td&gt;4500.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;63&lt;/td&gt;
      &lt;td&gt;male&lt;/td&gt;
      &lt;td&gt;8.0&lt;/td&gt;
      &lt;td&gt;4700.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;33&lt;/td&gt;
      &lt;td&gt;male&lt;/td&gt;
      &lt;td&gt;7.0&lt;/td&gt;
      &lt;td&gt;3500.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;57&lt;/td&gt;
      &lt;td&gt;female&lt;/td&gt;
      &lt;td&gt;6.0&lt;/td&gt;
      &lt;td&gt;4000.0&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;We have information on &lt;strong&gt;300 individuals&lt;/strong&gt;, for which we observe their &lt;code&gt;age&lt;/code&gt;, their &lt;code&gt;gender&lt;/code&gt;, the years of &lt;code&gt;education&lt;/code&gt;, and the current monthly &lt;code&gt;wage&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Suppose we were directly regressing &lt;code&gt;wage&lt;/code&gt; on &lt;code&gt;education&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;short_model = smf.ols(&#39;wage ~ education + gender + age&#39;, df).fit()
short_model.summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
         &lt;td&gt;&lt;/td&gt;           &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt;      &lt;td&gt; 2657.8864&lt;/td&gt; &lt;td&gt;  444.996&lt;/td&gt; &lt;td&gt;    5.973&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt; 1762.155&lt;/td&gt; &lt;td&gt; 3553.618&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;gender[T.male]&lt;/th&gt; &lt;td&gt;  335.1075&lt;/td&gt; &lt;td&gt;  132.685&lt;/td&gt; &lt;td&gt;    2.526&lt;/td&gt; &lt;td&gt; 0.015&lt;/td&gt; &lt;td&gt;   68.027&lt;/td&gt; &lt;td&gt;  602.188&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;education&lt;/th&gt;      &lt;td&gt;   95.9437&lt;/td&gt; &lt;td&gt;   38.752&lt;/td&gt; &lt;td&gt;    2.476&lt;/td&gt; &lt;td&gt; 0.017&lt;/td&gt; &lt;td&gt;   17.940&lt;/td&gt; &lt;td&gt;  173.948&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;age&lt;/th&gt;            &lt;td&gt;   12.3120&lt;/td&gt; &lt;td&gt;    6.110&lt;/td&gt; &lt;td&gt;    2.015&lt;/td&gt; &lt;td&gt; 0.050&lt;/td&gt; &lt;td&gt;    0.013&lt;/td&gt; &lt;td&gt;   24.611&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;The coefficient of &lt;code&gt;education&lt;/code&gt; is positive and significant. However, we know there might be an &lt;strong&gt;omitted variable bias&lt;/strong&gt;, because we do not observe &lt;code&gt;ability&lt;/code&gt;. In terms of DAGs, there is a &lt;strong&gt;backdoor path&lt;/strong&gt; from &lt;code&gt;education&lt;/code&gt; to &lt;code&gt;wage&lt;/code&gt; passing through &lt;code&gt;ability&lt;/code&gt; that is not blocked and therefore biases our estimate.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-mermaid&#34;&gt;flowchart TD
classDef included fill:#DCDCDC,stroke:#000000,stroke-width:2px;
classDef excluded fill:#ffffff,stroke:#000000,stroke-width:2px;
classDef unobserved fill:#ffffff,stroke:#000000,stroke-width:2px,stroke-dasharray: 5 5;

D((education))
Z((ability))
Y((wage))
X1((age))
X2((gender))

D --&amp;gt; Y
Z --&amp;gt; D
Z --&amp;gt; Y
X1 --&amp;gt; Y
X2 --&amp;gt; Y

class D,Y included;
class X1,X2 excluded;
class Z unobserved;

linkStyle 0 stroke:#00ff00,stroke-width:4px;
linkStyle 1,2 stroke:#ff0000,stroke-width:4px;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Does it mean that all our analysis is &lt;strong&gt;garbage&lt;/strong&gt;? Can we still draw some causal conclusion from the regression results?&lt;/p&gt;
&lt;h2 id=&#34;direction-of-the-bias&#34;&gt;Direction of the Bias&lt;/h2&gt;
&lt;p&gt;If we knew the signs of $\gamma$ and $\delta$, we could infer the sign of the bias, since it&amp;rsquo;s the product of the two signs.&lt;/p&gt;
&lt;p&gt;$$
\text{OVB} = \gamma \delta \qquad \text{ where } \qquad \gamma := \frac{Cov(Z, y)}{Var(Z)}, \quad \delta := \frac{Cov(D, Z)}{Var(D)}
$$&lt;/p&gt;
&lt;p&gt;which in our example is&lt;/p&gt;
&lt;p&gt;$$
\text{OVB} = \gamma \delta \qquad \text{ where } \qquad \gamma := \frac{Cov(\text{ability}, \text{wage})}{Var(\text{ability})}, \quad \delta := \frac{Cov(\text{education}, \text{ability})}{Var(\text{education})}
$$&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s analyze the two correlations separately:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The correlation between &lt;code&gt;ability&lt;/code&gt; and &lt;code&gt;wage&lt;/code&gt; is most likely positive&lt;/li&gt;
&lt;li&gt;The correlation between &lt;code&gt;ability&lt;/code&gt; and &lt;code&gt;education&lt;/code&gt; is most likely positive&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Therefore, the bias is most likely &lt;strong&gt;positive&lt;/strong&gt;. From this, we can conclude that our estimate from the regression on &lt;code&gt;wage&lt;/code&gt; on &lt;code&gt;education&lt;/code&gt; is most likely an &lt;strong&gt;overestimate&lt;/strong&gt; of the true effect, which is most likely smaller.&lt;/p&gt;
&lt;p&gt;This might seem like a small insight, but it&amp;rsquo;s actually huge. Now we can say with confidence that one year of &lt;code&gt;education&lt;/code&gt; increases &lt;code&gt;wages&lt;/code&gt; by &lt;strong&gt;at most&lt;/strong&gt; 95 dollars per month, which is a much more informative statement than just saying that the estimate is biased.&lt;/p&gt;
&lt;p&gt;In general, we can summarize the different possible effects of the bias in a 2-by-2 &lt;strong&gt;table&lt;/strong&gt;.&lt;/p&gt;
&lt;img src=&#34;other/ovb.png&#34; width=80% /&gt;
&lt;h2 id=&#34;further-sensitivity-analysis&#34;&gt;Further Sensitivity Analysis&lt;/h2&gt;
&lt;p&gt;Can we say &lt;strong&gt;more&lt;/strong&gt; about the omitted variable bias without making strong assumptions?&lt;/p&gt;
&lt;p&gt;The answer is yes! In particular, we can ask ourselves: how strong should the partial correlations $\gamma$ and $\delta$ be in order to &lt;strong&gt;overturn&lt;/strong&gt; our conclusion?&lt;/p&gt;
&lt;p&gt;In our example, we found a positive correlation between &lt;code&gt;education&lt;/code&gt; and &lt;code&gt;wages&lt;/code&gt; in the data. However, we know that we are omitting &lt;code&gt;ability&lt;/code&gt; in the regression. The question is: how strong should the correlation between &lt;code&gt;ability&lt;/code&gt; and &lt;code&gt;wage&lt;/code&gt;, $\gamma$, and between &lt;code&gt;ability&lt;/code&gt; and &lt;code&gt;education&lt;/code&gt;, $\delta$, be in order to make the effect not significant or even negative?&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://rss.onlinelibrary.wiley.com/doi/full/10.1111/rssb.12348&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Cinelli and Hazlett (2020)&lt;/a&gt; show that we can transform this question in terms of residual variation explained, i.e. the &lt;a href=&#34;https://en.wikipedia.org/wiki/Coefficient_of_determination&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;coefficient of determination, $R^2$&lt;/a&gt;. The advantage of this approach is &lt;strong&gt;interpretability&lt;/strong&gt;. It is much easier to make a guess about the percentage of variance explained than to make a guess about the magnitude of a conditional correlation.&lt;/p&gt;
&lt;p&gt;The authors wrote a companion package &lt;a href=&#34;https://github.com/carloscinelli/sensemakr&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;sensemakr&lt;/code&gt;&lt;/a&gt; to conduct the sensitivity analysis. You can find a detailed description of the package &lt;a href=&#34;https://cran.r-project.org/web/packages/sensemakr/vignettes/sensemakr.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;We will now use the &lt;code&gt;Sensemakr&lt;/code&gt; function. The main &lt;strong&gt;arguments&lt;/strong&gt; of the &lt;code&gt;Sensemakr&lt;/code&gt; function are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;model&lt;/code&gt;: the regression model we want to analyze&lt;/li&gt;
&lt;li&gt;&lt;code&gt;treatment&lt;/code&gt;: the feature/covariate of interest, in our case &lt;code&gt;education&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The question we will try to answer is the following:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;How much of the residual variation in &lt;code&gt;education&lt;/code&gt; (x axis) and &lt;code&gt;wage&lt;/code&gt; (y axis) does &lt;code&gt;ability&lt;/code&gt; need to explain in order for the effect of &lt;code&gt;education&lt;/code&gt; on &lt;code&gt;wages&lt;/code&gt; to &lt;strong&gt;change sign&lt;/strong&gt;?&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import sensemakr

sensitivity = sensemakr.Sensemakr(model = short_model, treatment = &amp;quot;education&amp;quot;)
sensitivity.plot()
plt.xlabel(&amp;quot;Partial $R^2$ of ability with education&amp;quot;);
plt.ylabel(&amp;quot;Partial $R^2$ of ability with wage&amp;quot;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/omitted_variable_bias_17_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;In the &lt;strong&gt;plot&lt;/strong&gt;, we see how the partial (because conditional on &lt;code&gt;age&lt;/code&gt; and &lt;code&gt;gender&lt;/code&gt;) $R^2$ of &lt;code&gt;ability&lt;/code&gt; with &lt;code&gt;education&lt;/code&gt; and &lt;code&gt;wage&lt;/code&gt; affects the estimated coefficient of &lt;code&gt;education&lt;/code&gt; on &lt;code&gt;wage&lt;/code&gt;. The $(0,0)$ coordinate, marked with a &lt;strong&gt;triangle&lt;/strong&gt;, corresponds to the current estimate and reflects what would happen if &lt;code&gt;ability&lt;/code&gt; had no explanatory power for both &lt;code&gt;wage&lt;/code&gt; with &lt;code&gt;education&lt;/code&gt;: nothing. As the explanatory power of &lt;code&gt;ability&lt;/code&gt; grows (moving upwards and rightwards from the triangle), the estimated coefficient decreases, as marked by the &lt;strong&gt;level curves&lt;/strong&gt;, until it becomes zero at the &lt;strong&gt;dotted red line&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;How should we &lt;strong&gt;interpret&lt;/strong&gt; the plot? We can see that we need &lt;code&gt;ability&lt;/code&gt; to explain around 30% of the residual variation in both &lt;code&gt;education&lt;/code&gt; and &lt;code&gt;wage&lt;/code&gt; in order for the effect of &lt;code&gt;education&lt;/code&gt; on &lt;code&gt;wages&lt;/code&gt; to disappear, corresponding to the red line.&lt;/p&gt;
&lt;p&gt;One question that you might (legitimately) have now is: what is 30%? Is it big or is it small? We can get a sense of the &lt;strong&gt;magnitude&lt;/strong&gt; of the partial $R^2$ by &lt;strong&gt;benchmarking&lt;/strong&gt; the results with the residual variance explained by another &lt;em&gt;observed&lt;/em&gt; variable. Let&amp;rsquo;s use &lt;code&gt;age&lt;/code&gt; for example.&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;Sensemakr&lt;/code&gt; function accepts the following optional arguments:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;benchmark_covariates&lt;/code&gt;: the covariate to use as a benchmark&lt;/li&gt;
&lt;li&gt;&lt;code&gt;kd&lt;/code&gt; and &lt;code&gt;ky&lt;/code&gt;: these arguments parameterize how many times stronger the unobserved variable (&lt;code&gt;ability&lt;/code&gt;) is related to the treatment (&lt;code&gt;kd&lt;/code&gt;) and to the outcome (&lt;code&gt;ky&lt;/code&gt;) in comparison to the observed benchmark covariate (&lt;code&gt;age&lt;/code&gt;). In our example, setting &lt;code&gt;kd&lt;/code&gt; and &lt;code&gt;ky&lt;/code&gt; equal to $[0.5, 1, 2]$ means we want to investigate the maximum strength of a variable half, same, or twice as strong as &lt;code&gt;age&lt;/code&gt; (in explaining &lt;code&gt;education&lt;/code&gt; and &lt;code&gt;wage&lt;/code&gt; variation).&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sensitivity = sensemakr.Sensemakr(model = short_model, 
                                  treatment = &amp;quot;education&amp;quot;,
                                  benchmark_covariates = &amp;quot;age&amp;quot;,
                                  kd = [0.5, 1, 2],
                                  ky = [0.5, 1, 2])
sensitivity.plot()
plt.xlabel(&amp;quot;Partial $R^2$ of ability with education&amp;quot;);
plt.ylabel(&amp;quot;Partial $R^2$ of ability with wage&amp;quot;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/omitted_variable_bias_19_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;It looks like even if &lt;code&gt;ability&lt;/code&gt; had twice as much explanatory power as &lt;code&gt;age&lt;/code&gt;, the effect of &lt;code&gt;education&lt;/code&gt; on &lt;code&gt;wage&lt;/code&gt; would still be positive. But would it be &lt;strong&gt;statistically significant&lt;/strong&gt;?&lt;/p&gt;
&lt;p&gt;We can repeat the same exercise, looking at the t-statistic instead of the magnitude of the coefficient. We just need to set the &lt;code&gt;sensitivity_of&lt;/code&gt; option in the plotting function equal to &lt;code&gt;t-value&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;The question that we are trying to answer in this case is:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;How much of the residual variation in &lt;code&gt;education&lt;/code&gt; (x axis) and &lt;code&gt;wage&lt;/code&gt; (y axis) does &lt;code&gt;ability&lt;/code&gt; need to explain in order for the effect of &lt;code&gt;education&lt;/code&gt; on &lt;code&gt;wages&lt;/code&gt; to &lt;strong&gt;become not significant&lt;/strong&gt;?&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sensitivity.plot(sensitivity_of = &#39;t-value&#39;)
plt.xlabel(&amp;quot;Partial $R^2$ of ability with education&amp;quot;);
plt.ylabel(&amp;quot;Partial $R^2$ of ability with wage&amp;quot;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/omitted_variable_bias_21_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;From the plot, we can see, we need &lt;code&gt;ability&lt;/code&gt; to explain around 5% to 10% of the residual variation in both &lt;code&gt;education&lt;/code&gt; and &lt;code&gt;wage&lt;/code&gt; in order for the effect of &lt;code&gt;education&lt;/code&gt; on &lt;code&gt;wage&lt;/code&gt; not to be significant. In particular, the red line plots the level curve for the t-statistic equal to 2.01, corresponding to a 5% significance level. From the comparison with &lt;code&gt;age&lt;/code&gt;, we see that a slightly stronger explanatory power (bigger than &lt;code&gt;1.0x age&lt;/code&gt;) would be sufficient to make the coefficient of &lt;code&gt;education&lt;/code&gt; on &lt;code&gt;wage&lt;/code&gt; not statistically significant.&lt;/p&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;In this post, I have introduced the concept of &lt;strong&gt;omitted variable bias&lt;/strong&gt;. We have seen how it&amp;rsquo;s computed in a simple linear model and how we can exploit qualitative information about the variables to make inference in presence of omitted variable bias.&lt;/p&gt;
&lt;p&gt;These tools are extremely useful since omitted variable bias is essentially &lt;strong&gt;everywhere&lt;/strong&gt;. First of all, there are always factors that we do not observe, such as ability in our toy example. However, even if we could observe everything, omitted variable bias can also emerge in the form of &lt;strong&gt;model misspecification&lt;/strong&gt;. Suppose that &lt;code&gt;wages&lt;/code&gt; depended on &lt;code&gt;age&lt;/code&gt; in a quadratic way. Then, omitting the quadratic term from the regression introduces bias, which can be analyzed with the same tools we have used for &lt;code&gt;ability&lt;/code&gt;.&lt;/p&gt;
&lt;h3 id=&#34;references&#34;&gt;References&lt;/h3&gt;
&lt;p&gt;[1] C. Cinelli, C. Hazlett, &lt;a href=&#34;https://rss.onlinelibrary.wiley.com/doi/full/10.1111/rssb.12348&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Making Sense of Sensitivity: Extending Omitted Variable Bias&lt;/a&gt; (2019), &lt;em&gt;Journal of the Royal Statistical Society&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;[2] V. Chernozhukov, C. Cinelli, W. Newey, A. Sharma, V. Syrgkanis, &lt;a href=&#34;https://arxiv.org/abs/2112.13398&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Long Story Short: Omitted Variable Bias in Causal Machine Learning&lt;/a&gt; (2022), working paper.&lt;/p&gt;
&lt;h3 id=&#34;related-articles&#34;&gt;Related Articles&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/59f801eb3299&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The FWL Theorem, Or How To Make Regressions Intuitive&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/b63dc69e3d8c&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;DAGs and Control Variables&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;code&#34;&gt;Code&lt;/h3&gt;
&lt;p&gt;You can find the original Jupyter Notebook here:&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/ovb.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/matteocourthoud/Blog-Posts/blob/main/ovb.ipynb&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Outliers, Leverage, and Influential Observations</title>
      <link>https://matteocourthoud.github.io/post/outliers_levarage/</link>
      <pubDate>Mon, 10 Jul 2023 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/post/outliers_levarage/</guid>
      <description>&lt;p&gt;&lt;em&gt;What makes an observation &amp;ldquo;unusual&amp;rdquo;?&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;In data science, one common task is outlier detection. This is a broad term that is often misused or misunderstood. More broadly, we are often interested in understanding any observation is &lt;strong&gt;&amp;ldquo;unusual&amp;rdquo;&lt;/strong&gt;. First of all, what does it mean to be unusual? In this article we are going to inspect three different ways in which an observation can be unusual: it can be unusual characteristics, it might not fit the model or it might be particularly influential in fitting the model. We will see that in linear regression the latter characteristics is a byproduct of the first two.&lt;/p&gt;
&lt;p&gt;Importantly, being unusual is &lt;strong&gt;not necessarily bad&lt;/strong&gt;. Observations that have different characteristics from all others usually carry more information. We also expect some observations not to fit the model well, otherwise the model is likely biased (overfitting). However, &amp;ldquo;unusual&amp;rdquo; observations are also more likely to be generated by a different process. Extreme cases include measurement error or fraud, but differences can be more nuanced. Domain knowledge is always kind and dropping observations only for for statistical reasons is never wise.&lt;/p&gt;
&lt;p&gt;That said, let&amp;rsquo;s have a look at some different ways in which observations can be &amp;ldquo;unusual&amp;rdquo;.&lt;/p&gt;
&lt;h1 id=&#34;example&#34;&gt;Example&lt;/h1&gt;
&lt;p&gt;Suppose we are an &lt;strong&gt;peer-to-peer online platform&lt;/strong&gt; and we are interested in understanding if there is anything suspicious going on with our business. We have information about how much time our customers spend on the platform and the total value of their transactions.&lt;/p&gt;
&lt;p&gt;First, let&amp;rsquo;s have a look at the data. I import the data generating process &lt;code&gt;dgp_p2p()&lt;/code&gt; from &lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/src/dgp.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;src.dgp&lt;/code&gt;&lt;/a&gt; and some plotting functions and libraries from &lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/src/utils.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;src.utils&lt;/code&gt;&lt;/a&gt;. I include code snippets from &lt;a href=&#34;https://deepnote.com&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Deepnote&lt;/a&gt;, a Jupyter-like web-based collaborative notebook environment. For our purpose, Deepnote is very handy because it allows me not only to include code but also output, like data and tables.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;%matplotlib inline
%config InlineBackend.figure_format = &#39;retina&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from src.utils import *
from src.dgp import dgp_p2p
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df = dgp_p2p().generate_data()
df.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;hours&lt;/th&gt;
      &lt;th&gt;transactions&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;2.6&lt;/td&gt;
      &lt;td&gt;8.30&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;2.0&lt;/td&gt;
      &lt;td&gt;8.00&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;7.0&lt;/td&gt;
      &lt;td&gt;21.00&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;6.7&lt;/td&gt;
      &lt;td&gt;18.00&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;1.2&lt;/td&gt;
      &lt;td&gt;3.82&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;We have information on 50 clients for which we observe &lt;code&gt;hours&lt;/code&gt; spent on the website and total &lt;code&gt;transactions&lt;/code&gt; amount. Since we only have two variables we can easily inspect them using a scatterplot.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sns.scatterplot(data=df, x=&#39;hours&#39;, y=&#39;transactions&#39;).set(title=&#39;Data Scatterplot&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/outliers_levarage_9_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;The relationship between &lt;code&gt;hours&lt;/code&gt; and &lt;code&gt;transactions&lt;/code&gt; seems to follow a clear linear relationship. If we fit a linear model, we observe a particularly tight fit.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;smf.ols(&#39;hours ~ transactions&#39;, data=df).fit().summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
        &lt;td&gt;&lt;/td&gt;          &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt;    &lt;td&gt;   -0.0975&lt;/td&gt; &lt;td&gt;    0.084&lt;/td&gt; &lt;td&gt;   -1.157&lt;/td&gt; &lt;td&gt; 0.253&lt;/td&gt; &lt;td&gt;   -0.267&lt;/td&gt; &lt;td&gt;    0.072&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;transactions&lt;/th&gt; &lt;td&gt;    0.3452&lt;/td&gt; &lt;td&gt;    0.009&lt;/td&gt; &lt;td&gt;   39.660&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    0.328&lt;/td&gt; &lt;td&gt;    0.363&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;Does any data point look suspiciously different from the others? How?&lt;/p&gt;
&lt;h2 id=&#34;leverage&#34;&gt;Leverage&lt;/h2&gt;
&lt;p&gt;The first metric that we are going to use to evaluate &amp;ldquo;unusual&amp;rdquo; observations is the &lt;strong&gt;leverage&lt;/strong&gt;, which was first introduced by &lt;a href=&#34;https://www.jstor.org/stable/1268249&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Cook (1980)&lt;/a&gt;. The objective of the leverage is to capture how much a single point is different with respect to other data points. These data points are often called &lt;strong&gt;outliers&lt;/strong&gt; and there exist a nearly amount of algorithms and rules of thumb to flag them.However the idea is the same: flagging observations that are unusual in terms of features.&lt;/p&gt;
&lt;p&gt;The leverage of an observation $i$ is defined as&lt;/p&gt;
&lt;p&gt;$$
h_{ii} := x_i&amp;rsquo; (X&amp;rsquo;X)^{-1} x_i
$$&lt;/p&gt;
&lt;p&gt;One interpretation of the leverage is as a &lt;strong&gt;measure of distance&lt;/strong&gt; where individual observations are compared against the average of all observations.&lt;/p&gt;
&lt;p&gt;Another interpretation of the leverage is as the influence of the outcome of observation $i$, $y_i$, on the corresponding fitted value $\hat{y_i}$.&lt;/p&gt;
&lt;p&gt;$$
h_{ii} = \frac{\partial \hat{y}_i}{\partial y_i}
$$&lt;/p&gt;
&lt;p&gt;Algebraically, the leverage of observation $i$ is the $i^{th}$ element of the &lt;strong&gt;design matrix&lt;/strong&gt; $X&amp;rsquo; (X&amp;rsquo;X)^{-1} X$. Among the many properties of the leverages, is the fact that they are non-negative and their values sum to 1.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s compute the leverage of the observations in our dataset. We also flag observations that have unusual leverages (which we arbitrarily define as more than two standard deviations away from the average leverage).&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;X = np.reshape(df[&#39;hours&#39;].values, (-1, 1))
Y = np.reshape(df[&#39;transactions&#39;].values, (-1, 1))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df[&#39;leverage&#39;] = np.diagonal(X @ np.linalg.inv(X.T @ X) @ X.T)
df[&#39;high_leverage&#39;] = df[&#39;leverage&#39;] &amp;gt; (np.mean(df[&#39;leverage&#39;]) + 2*np.std(df[&#39;leverage&#39;]))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s plot the distribution of leverage values in our data.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;fix, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))
sns.histplot(data=df, x=&#39;leverage&#39;, hue=&#39;high_leverage&#39;, alpha=1, bins=30, ax=ax1).set(title=&#39;Distribution of Leverages&#39;);
sns.scatterplot(data=df, x=&#39;hours&#39;, y=&#39;transactions&#39;, hue=&#39;high_leverage&#39;, ax=ax2).set(title=&#39;Data Scatterplot&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/outliers_levarage_18_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;As we can see, the distribution is skewed with two observations having a unusually high leverage. Indeed, in the scatterplot these two observations are slightly separated from the rest of the distribution.&lt;/p&gt;
&lt;p&gt;Is this bad news? It depends. Outliers are &lt;strong&gt;not a problem per se&lt;/strong&gt;. Actually, if they are genuine observations, they might carry much more information than other observations. On the other hand, they are also more likely &lt;em&gt;not&lt;/em&gt; to be genuine observations (e.g. fraud, measurement error, &amp;hellip;) or to be inherently different from the other ones (e.g. professional users vs amateurs). In any case, we might want to investigate further and use as much context-specific information as we can.&lt;/p&gt;
&lt;p&gt;Importantly, the fact that an observation has a high leverage tells us information about the features of the model but nothing about the model itself. Are these users just different observations or they also behave differently?&lt;/p&gt;
&lt;h2 id=&#34;residuals&#34;&gt;Residuals&lt;/h2&gt;
&lt;p&gt;So far we have only talked about unusual features, but what about &lt;strong&gt;unusual behavior&lt;/strong&gt;? This is what regression residuals measure.&lt;/p&gt;
&lt;p&gt;Regression residuals are the difference between the predicted outcome values and the observed outcome values. In a sense, they capture what the model cannot explain: the higher the residual of one observation the more it is unusual in the sense that the model cannot explain it.&lt;/p&gt;
&lt;p&gt;In the case of linear regression, residuals can be written as&lt;/p&gt;
&lt;p&gt;$$
\hat{e} = y - \hat{y} = y - \hat \beta X
$$&lt;/p&gt;
&lt;p&gt;In our case, since $X$ is one dimensional (&lt;code&gt;hours&lt;/code&gt;), we can easily visualize them.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;Y_hat = X @ np.linalg.inv(X.T @ X) @ X.T @ Y
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plt.scatter(X, Y, s=50, label=&#39;data&#39;)
plt.plot(X, Y_hat, c=&#39;k&#39;, lw=2, label=&#39;prediction&#39;)
plt.vlines(X, np.minimum(Y, Y_hat), np.maximum(Y, Y_hat), color=&#39;r&#39;, lw=3, label=&amp;quot;residuals&amp;quot;);
plt.legend()
plt.title(f&amp;quot;Regression prediction and residuals&amp;quot;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/outliers_levarage_23_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Do some observations have unusually high residuals? Let&amp;rsquo;s plot their distribution.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df[&#39;residual&#39;] = np.abs(Y - X @ np.linalg.inv(X.T @ X) @ X.T @ Y)
df[&#39;high_residual&#39;] = df[&#39;residual&#39;] &amp;gt; (np.mean(df[&#39;residual&#39;]) + 2*np.std(df[&#39;residual&#39;]))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;fix, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))
sns.histplot(data=df, x=&#39;residual&#39;, hue=&#39;high_residual&#39;, alpha=1, bins=30, ax=ax1).set(title=&#39;Distribution of Residuals&#39;);
sns.scatterplot(data=df, x=&#39;hours&#39;, y=&#39;transactions&#39;, hue=&#39;high_residual&#39;, ax=ax2).set(title=&#39;Data Scatterplot&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/outliers_levarage_26_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Two observations have particularly high residuals. This means that for these observations, the model is not good at predicting the observed outcomes.&lt;/p&gt;
&lt;p&gt;Is this bad news? Not necessarily. A model that fits the observations too well is likely to be &lt;a href=&#34;https://en.wikipedia.org/wiki/Bias_%28statistics%29&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;biased&lt;/strong&gt;&lt;/a&gt;. However, it might still be important to understand why some users have a different relationship between hours spent and total transactions. As usual, information on the specific context is key.&lt;/p&gt;
&lt;p&gt;So far we have looked at observations with &amp;ldquo;unusual&amp;rdquo; characteristics and &amp;ldquo;unusual&amp;rdquo; model fit, but what is the observation itself is distorting the model? How much our model is driven by a handful of observations?&lt;/p&gt;
&lt;h2 id=&#34;influence&#34;&gt;Influence&lt;/h2&gt;
&lt;p&gt;The concept of &lt;strong&gt;influence and influence functions&lt;/strong&gt; was developed precisely to answer this question: what are influential observations? This questions were very popular in the 80&amp;rsquo;s and lost appeal for a long time until the recent need of explaining complex machine learning and AI models.&lt;/p&gt;
&lt;p&gt;The general idea is to define an observation as &lt;strong&gt;influential&lt;/strong&gt; if removing it significantly changes the estimated model. In linear regression, we define the influence of observation $i$ as:&lt;/p&gt;
&lt;p&gt;$$
\hat{\beta} - \hat{\beta}_{-i} = (X&amp;rsquo;X)^{-1} x_i e_i
$$&lt;/p&gt;
&lt;p&gt;Where $\hat{\beta}_{-i}$ is the OLS coefficient estimated omitting observation $i$.&lt;/p&gt;
&lt;p&gt;As you can see, there is a tight connection to both leverage $h_{ii}$ and residuals $e_i$: influence is almost the product of the two. Indeed, in linear regression, observations with high leverage are observations that are both outliers and have high residuals. None of the two conditions alone is sufficient for an observation to have an influence on the model.&lt;/p&gt;
&lt;p&gt;We can see it best in the data.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df[&#39;influence&#39;] = (np.linalg.inv(X.T @ X) @ X.T).T * np.abs(Y - Y_hat)
df[&#39;high_influence&#39;] = df[&#39;influence&#39;] &amp;gt; (np.mean(df[&#39;influence&#39;]) + 2*np.std(df[&#39;influence&#39;]))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;fix, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))
sns.histplot(data=df, x=&#39;influence&#39;, hue=&#39;high_influence&#39;, alpha=1, bins=30, ax=ax1).set(title=&#39;Distribution of Influences&#39;);
sns.scatterplot(data=df, x=&#39;hours&#39;, y=&#39;transactions&#39;, hue=&#39;high_influence&#39;, ax=ax2).set(title=&#39;Data Scatterplot&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/outliers_levarage_31_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;In our dataset, there is only one observation with high influence, and it is disproportionally larger than the influence of all other observations.&lt;/p&gt;
&lt;p&gt;We can now plot all &amp;ldquo;unusual&amp;rdquo; points in the same plot. I also report residuals and leverage of each point in a separate plot.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def plot_leverage_residuals(df):

    # Hue
    df[&#39;type&#39;] = &#39;Normal&#39;
    df.loc[df[&#39;high_residual&#39;], &#39;type&#39;] = &#39;High Residual&#39;
    df.loc[df[&#39;high_leverage&#39;], &#39;type&#39;] = &#39;High Leverage&#39;
    df.loc[df[&#39;high_influence&#39;], &#39;type&#39;] = &#39;High Influence&#39;

    # Init figure
    fig, (ax1,ax2) = plt.subplots(1,2, figsize=(12,5)) 
    ax1.plot(X, Y_hat, lw=1, c=&#39;grey&#39;, zorder=0.5)
    sns.scatterplot(data=df, x=&#39;hours&#39;, y=&#39;transactions&#39;, ax=ax1, hue=&#39;type&#39;).set(title=&#39;Data&#39;)
    sns.scatterplot(data=df, x=&#39;residual&#39;, y=&#39;leverage&#39;, hue=&#39;type&#39;, ax=ax2).set(title=&#39;Metrics&#39;)
    ax1.get_legend().remove()
    sns.move_legend(ax2, &amp;quot;upper left&amp;quot;, bbox_to_anchor=(1.05, 0.8));
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plot_leverage_residuals(df)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/outliers_levarage_34_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;As we can see, we have one point with high residual and low leverage, one with high leverage and low residual and only one point with both high leverage and high residual: the only influential point.&lt;/p&gt;
&lt;p&gt;From the plot it is also clear why none of the two conditions alone is sufficient for an observation to rive the model. The orange point has high residual but it lies right in the middle of the distribution and therefore cannot tilt the line of best fit. The green point instead has high leverage and lies far from the center of the distribution but its perfectly aligned with the line of fit. Removing it would not change anything. The red dot instead is different from the others in terms of &lt;strong&gt;both characteristics and behavior&lt;/strong&gt; and therefore tilts the fit line towards itself.&lt;/p&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;In this post, we have seen some different ways in which observations can be &amp;ldquo;unusual&amp;rdquo;: they can have either unusual characteristics or unusual behavior. In linear regression, when an observation has both it is also influential: it tilts the model towards itself.&lt;/p&gt;
&lt;p&gt;In the example of the article, we concentrated on a univariate linear regression. However, research on influence functions has recently become a hot topic because of the need to make black-box machine learning algorithms understandable. With models with millions of parameters, billions of observations and wild non-linearities, it can be very hard to establish whether a single observation is influential and how.&lt;/p&gt;
&lt;h3 id=&#34;references&#34;&gt;References&lt;/h3&gt;
&lt;p&gt;[1] D. Cook, &lt;a href=&#34;https://www.jstor.org/stable/1268249&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Detection of Influential Observation in Linear Regression&lt;/a&gt; (1980), &lt;em&gt;Technometrics&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;[2] D. Cook, S. Weisberg, &lt;a href=&#34;https://www.jstor.org/stable/1268187&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Characterizations of an Empirical Influence Function for Detecting Influential Cases in
Regression&lt;/a&gt; (1980), &lt;em&gt;Technometrics&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;[2] P. W. Koh, P. Liang, &lt;a href=&#34;http://proceedings.mlr.press/v70/koh17a&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Understanding Black-box Predictions via Influence Functions&lt;/a&gt; (2017), &lt;em&gt;ICML Proceedings&lt;/em&gt;.&lt;/p&gt;
&lt;h3 id=&#34;code&#34;&gt;Code&lt;/h3&gt;
&lt;p&gt;You can find the original Jupyter Notebook here:&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/outliers.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/outliers.ipynb&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Synthetic Control</title>
      <link>https://matteocourthoud.github.io/post/synthetic_control/</link>
      <pubDate>Mon, 10 Jul 2023 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/post/synthetic_control/</guid>
      <description>&lt;p&gt;INTRO&lt;/p&gt;
&lt;h2 id=&#34;self-driving-cars&#34;&gt;Self-Driving Cars&lt;/h2&gt;
&lt;p&gt;Suppose you were a &lt;strong&gt;ride-sharing platform&lt;/strong&gt; and you want to test the effect of self-driving cars in your fleet.&lt;/p&gt;
&lt;p&gt;As you can imagine, there are many &lt;strong&gt;limitations&lt;/strong&gt; to running an AB/test for this type of feature. First of all, it&amp;rsquo;s complicated to randomize individual rides. Second, it&amp;rsquo;s a very expensive intervention. Third, and statistically most important, you cannot run this intervention at the ride level. The problem is that there are &lt;strong&gt;spillover&lt;/strong&gt; effects from treated to control units: if indeed self-driving cars are more efficient, it means that they can serve more customers in the same amount of time, reducing the customers available to normal drivers (the control group). This spillover &lt;strong&gt;contaminates&lt;/strong&gt; the experiment and prevents a causal interpretation of the results.&lt;/p&gt;
&lt;p&gt;For all these reasons, we select only one city at random to run this experiment&amp;hellip; (drum roll)&amp;hellip; Miami!&lt;/p&gt;
&lt;img src=&#34;fig/miami2.jpg&#34; width=&#34;700px&#34;/&gt;
&lt;p&gt;I generate a simulated dataset in which we observe a panel of U.S. cities over time. The revenue data is made up, while the socio-economic variables are taken from the &lt;a href=&#34;https://stats.oecd.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;OECD database&lt;/a&gt;. I import the data generating process &lt;code&gt;dgp_selfdriving()&lt;/code&gt; from &lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/src/dgp.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;src.dgp&lt;/code&gt;&lt;/a&gt;. I also import some plotting functions and libraries from &lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/src/utils.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;src.utils&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;%matplotlib inline
%config InlineBackend.figure_format = &#39;retina&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from src.utils import *
from src.dgp import dgp_selfdriving

treatment_year = 2013
treated_city = &#39;Miami&#39;
df = dgp_selfdriving().generate_data(year=treatment_year, city=treated_city)
df.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;city&lt;/th&gt;
      &lt;th&gt;year&lt;/th&gt;
      &lt;th&gt;density&lt;/th&gt;
      &lt;th&gt;employment&lt;/th&gt;
      &lt;th&gt;gdp&lt;/th&gt;
      &lt;th&gt;population&lt;/th&gt;
      &lt;th&gt;treated&lt;/th&gt;
      &lt;th&gt;post&lt;/th&gt;
      &lt;th&gt;revenue&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;Atlanta&lt;/td&gt;
      &lt;td&gt;2003&lt;/td&gt;
      &lt;td&gt;290&lt;/td&gt;
      &lt;td&gt;0.629761&lt;/td&gt;
      &lt;td&gt;6.4523&lt;/td&gt;
      &lt;td&gt;4.267538&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;25.713947&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;Atlanta&lt;/td&gt;
      &lt;td&gt;2004&lt;/td&gt;
      &lt;td&gt;295&lt;/td&gt;
      &lt;td&gt;0.635595&lt;/td&gt;
      &lt;td&gt;6.5836&lt;/td&gt;
      &lt;td&gt;4.349712&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;23.852279&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;Atlanta&lt;/td&gt;
      &lt;td&gt;2005&lt;/td&gt;
      &lt;td&gt;302&lt;/td&gt;
      &lt;td&gt;0.645614&lt;/td&gt;
      &lt;td&gt;6.6998&lt;/td&gt;
      &lt;td&gt;4.455273&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;24.332397&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;Atlanta&lt;/td&gt;
      &lt;td&gt;2006&lt;/td&gt;
      &lt;td&gt;313&lt;/td&gt;
      &lt;td&gt;0.648573&lt;/td&gt;
      &lt;td&gt;6.5653&lt;/td&gt;
      &lt;td&gt;4.609096&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;23.816017&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;Atlanta&lt;/td&gt;
      &lt;td&gt;2007&lt;/td&gt;
      &lt;td&gt;321&lt;/td&gt;
      &lt;td&gt;0.650976&lt;/td&gt;
      &lt;td&gt;6.4184&lt;/td&gt;
      &lt;td&gt;4.737037&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;25.786902&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;len(df.city.unique())
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;46
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We have information on the largest 46 U.S. cities for the period 2002-2019. The panel is &lt;strong&gt;balanced&lt;/strong&gt;, which means that we observe all cities for all time periods.&lt;/p&gt;
&lt;p&gt;Is the &lt;strong&gt;treated&lt;/strong&gt; unit, Miami, comparable to the rest of the sample? Let&amp;rsquo;s use the &lt;code&gt;create_table_one&lt;/code&gt; function from Uber&amp;rsquo;s &lt;a href=&#34;https://causalml.readthedocs.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;causalml&lt;/code&gt;&lt;/a&gt; package to produce a &lt;strong&gt;covariate balance table&lt;/strong&gt;, containing the average value of our observable characteristics, across treatment and control groups. As the name suggests, this should always be the first table you present in causal inference analysis.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from causalml.match import create_table_one

create_table_one(df, &#39;treated&#39;, [&#39;density&#39;, &#39;employment&#39;, &#39;gdp&#39;, &#39;population&#39;, &#39;revenue&#39;])
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;Control&lt;/th&gt;
      &lt;th&gt;Treatment&lt;/th&gt;
      &lt;th&gt;SMD&lt;/th&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;Variable&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;n&lt;/th&gt;
      &lt;td&gt;765&lt;/td&gt;
      &lt;td&gt;17&lt;/td&gt;
      &lt;td&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;density&lt;/th&gt;
      &lt;td&gt;256.63 (172.90)&lt;/td&gt;
      &lt;td&gt;364.94 (19.61)&lt;/td&gt;
      &lt;td&gt;0.8802&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;employment&lt;/th&gt;
      &lt;td&gt;0.63 (0.05)&lt;/td&gt;
      &lt;td&gt;0.60 (0.04)&lt;/td&gt;
      &lt;td&gt;-0.5266&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;gdp&lt;/th&gt;
      &lt;td&gt;6.07 (1.16)&lt;/td&gt;
      &lt;td&gt;5.12 (0.29)&lt;/td&gt;
      &lt;td&gt;-1.1124&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;population&lt;/th&gt;
      &lt;td&gt;3.53 (3.81)&lt;/td&gt;
      &lt;td&gt;5.85 (0.31)&lt;/td&gt;
      &lt;td&gt;0.861&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;revenue&lt;/th&gt;
      &lt;td&gt;25.25 (2.45)&lt;/td&gt;
      &lt;td&gt;23.86 (2.39)&lt;/td&gt;
      &lt;td&gt;-0.5737&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;As expected, the groups are &lt;strong&gt;not balanced&lt;/strong&gt;: Miami is more densely populated, poorer, larger and has lower employment rate than the other cities in the US in our sample.&lt;/p&gt;
&lt;p&gt;We are interested in understanding the impact of the introduction of &lt;strong&gt;self-driving cars&lt;/strong&gt; on &lt;code&gt;revenue&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;One initial idea could be to analyze the data as we would in an A/B test, comparing control and treatment group. We can estimate the treatment effect as a difference in means in &lt;code&gt;revenue&lt;/code&gt; between the treatment and control group, after the introduction of self-driving cars.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;smf.ols(&#39;revenue ~ treated&#39;, data=df[df[&#39;post&#39;]==True]).fit().summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
         &lt;td&gt;&lt;/td&gt;            &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt;       &lt;td&gt;   26.6006&lt;/td&gt; &lt;td&gt;    0.127&lt;/td&gt; &lt;td&gt;  210.061&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;   26.351&lt;/td&gt; &lt;td&gt;   26.850&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;treated[T.True]&lt;/th&gt; &lt;td&gt;   -0.7156&lt;/td&gt; &lt;td&gt;    0.859&lt;/td&gt; &lt;td&gt;   -0.833&lt;/td&gt; &lt;td&gt; 0.405&lt;/td&gt; &lt;td&gt;   -2.405&lt;/td&gt; &lt;td&gt;    0.974&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;The effect of self-driving cars seems to be negative but not significant.&lt;/p&gt;
&lt;p&gt;The main &lt;strong&gt;problem&lt;/strong&gt; here is that we have a &lt;strong&gt;single treated unit&lt;/strong&gt;: Miami. It&amp;rsquo;s very hard to argue that Miami is comparable to other cities. Randomization ensures that this simple estimator is unbiased, ex-ante. However, with a single treated unit, the estimator suffers from severe small sample bias .&lt;/p&gt;
&lt;p&gt;One alternative procedure, is to compare revenue &lt;strong&gt;before and after&lt;/strong&gt; the treatment, within the city of Miami.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;smf.ols(&#39;revenue ~ post&#39;, data=df[df[&#39;city&#39;]==treated_city]).fit().summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
        &lt;td&gt;&lt;/td&gt;          &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt;    &lt;td&gt;   22.4485&lt;/td&gt; &lt;td&gt;    0.534&lt;/td&gt; &lt;td&gt;   42.044&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;   21.310&lt;/td&gt; &lt;td&gt;   23.587&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;post[T.True]&lt;/th&gt; &lt;td&gt;    3.4364&lt;/td&gt; &lt;td&gt;    0.832&lt;/td&gt; &lt;td&gt;    4.130&lt;/td&gt; &lt;td&gt; 0.001&lt;/td&gt; &lt;td&gt;    1.663&lt;/td&gt; &lt;td&gt;    5.210&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;The effect of self-driving cars seems to be positive and statistically significant.&lt;/p&gt;
&lt;p&gt;However, the &lt;strong&gt;problem&lt;/strong&gt; of this procedure is that there might have been many &lt;strong&gt;other things happening&lt;/strong&gt; after 2016. It&amp;rsquo;s a very strong stretch to attribute all differences to self-driving cars.&lt;/p&gt;
&lt;p&gt;We can better understand this concern if we plot the time trend of revenue over cities. First, we need to reshape the data into a &lt;strong&gt;wide format&lt;/strong&gt;, with one column per city and one row per year.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df = df.pivot(index=&#39;year&#39;, columns=&#39;city&#39;, values=&#39;revenue&#39;).reset_index()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, let&amp;rsquo;s plot the revenue over time for Miami and for the other cities.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;cities = [c for c in df.columns if c!=&#39;year&#39;]
df[&#39;Other Cities&#39;] = df[[c for c in cities if c != treated_city]].mean(axis=1)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def plot_lines(df, line1, line2, year, hline=True):
    sns.lineplot(x=df[&#39;year&#39;], y=df[line1].values, label=line1)
    sns.lineplot(x=df[&#39;year&#39;], y=df[line2].values, label=line2)
    plt.axvline(x=year, ls=&amp;quot;:&amp;quot;, color=&#39;C2&#39;, label=&#39;Self-Driving Cars&#39;, zorder=1)
    plt.legend();
    plt.title(&amp;quot;Average revenue per day (in M$)&amp;quot;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Since we are talking about Miami, let&amp;rsquo;s use an appropriate color palette.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sns.set_palette(sns.color_palette([&#39;#f14db3&#39;, &#39;#0dc3e2&#39;, &#39;#443a84&#39;]))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plot_lines(df, treated_city, &#39;Other Cities&#39;, treatment_year)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/synthetic_control_22_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;As we can see, revenue seems to be increasing after the treatment in Miami. But it&amp;rsquo;s a very volatile time series. And revenue was increasing also in the rest of the country. It&amp;rsquo;s very hard from this plot to attribute the change to self-driving case.&lt;/p&gt;
&lt;p&gt;Can we do better?&lt;/p&gt;
&lt;h2 id=&#34;synthetic-control&#34;&gt;Synthetic Control&lt;/h2&gt;
&lt;p&gt;The answer is yes! Synthetic control methods were first introduced by &lt;a href=&#34;https://www.tandfonline.com/doi/abs/10.1198/jasa.2009.ap08746&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Abadie, Diamond and Hainmueller (2010)&lt;/a&gt; and allow us to do causal inference when we have as few as one treated unit and many control units and we observe them over time.&lt;/p&gt;
&lt;h3 id=&#34;setting&#34;&gt;Setting&lt;/h3&gt;
&lt;p&gt;We assume that for a panel of i.i.d. subjects $i = 1, &amp;hellip;, n$ over time $t=1, &amp;hellip;,T$ we observed a set of variables $(X_{it}, D_i, Y_{it})$ that includes&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;a treatment assignment $D_i \in \lbrace 0, 1 \rbrace$ (&lt;code&gt;treated&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;a response $Y_{i,t} \in \mathbb R$ (&lt;code&gt;revenue&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;a feature vector $X_{i,t} \in \mathbb R^n$ (&lt;code&gt;population&lt;/code&gt;, &lt;code&gt;density&lt;/code&gt;, &lt;code&gt;employment&lt;/code&gt; and &lt;code&gt;GDP&lt;/code&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Moreover, one unit (Miami in our case) is treated at time $t^*$ (2016 in our case). We distinguish time periods before treatment and time periods after treatment.&lt;/p&gt;
&lt;p&gt;Crucially, treatment $D_i$ is not randomly assigned, therefore a difference in means between the treated unit(s) and the control group is not an unbiased estimator of the average treatment effect.&lt;/p&gt;
&lt;h3 id=&#34;the-problem&#34;&gt;The Problem&lt;/h3&gt;
&lt;p&gt;The problem is that, as usual, we do not observe the counterfactual outcome for treated units, i.e. we do not know what would have happened to them, if they had not been treated. This is known as the &lt;strong&gt;fundamental problem of causal inference&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;The simplest approach, would be just to compare pre and post periods. This is called the &lt;strong&gt;event study&lt;/strong&gt; approach.&lt;/p&gt;
&lt;p&gt;However, we can do better than this. In fact, even though treatment was not randomly assigned, we still have access to some units that were not treated.&lt;/p&gt;
&lt;p&gt;For the outcome variable we have the following setup&lt;/p&gt;
&lt;p&gt;$$
Y =
\begin{bmatrix}
Y_{t, post} \ &amp;amp; Y_{c, post} \newline
Y_{t, pre} \ &amp;amp; Y_{c, pre}
\end{bmatrix}
$$&lt;/p&gt;
&lt;p&gt;which we can rewrite as&lt;/p&gt;
&lt;p&gt;$$
Y =
\begin{bmatrix}
Y^{(1)} _ {t, post} \ &amp;amp; Y^{(0)} _ {c, post} \newline
Y^{(0)} _ {t, pre} \ &amp;amp; Y^{(0)} _ {c, pre}
\end{bmatrix}
$$&lt;/p&gt;
&lt;p&gt;We basically have a &lt;strong&gt;missing data problem&lt;/strong&gt; since we do not observe $Y^{(0)} _ {t, post}$.&lt;/p&gt;
&lt;h3 id=&#34;the-solution&#34;&gt;The Solution&lt;/h3&gt;
&lt;p&gt;Following &lt;a href=&#34;https://arxiv.org/pdf/1610.07748.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Doudchenko and Inbens (2018)&lt;/a&gt;, we can formulate an estimate of the counterfactual outcome for the treated unit as a linear combination of the observed outcomes for the control units.&lt;/p&gt;
&lt;p&gt;$$
\hat Y^{(0)} _ {t, post} = \alpha + \sum_{i \in c} \beta_{i} Y^{(0)} _ {i, post}
$$&lt;/p&gt;
&lt;p&gt;where&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the constant $\alpha$ allows for different averages between the two groups&lt;/li&gt;
&lt;li&gt;the weights $\beta_i$ are allowed to vary across control units $i$ (otherwise, it would be a difference-in-differences)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;How should we &lt;strong&gt;choose which weights&lt;/strong&gt; to use? We want our synthetic control to approximate the outcome as closely as possible, before the treatment. The first approach could be to define the weights as&lt;/p&gt;
&lt;p&gt;$$
\hat \beta = \arg \min_{\beta} || \boldsymbol X_{t, pre} - \boldsymbol \beta \boldsymbol X_{c, pre} || = \sqrt{ \sum_{p} \left( X_{t, p, pre}  - \sum_{i \in c} \beta_{p} X_{c, p, pre} \right)^2 }
$$&lt;/p&gt;
&lt;p&gt;I.e. the weights are such that they minimize the distance between observable characteristics of control units $X_c$ and the treated unit $X_t$ before the treatment.&lt;/p&gt;
&lt;p&gt;You might notice a very close similarity to &lt;strong&gt;linear regression&lt;/strong&gt;.  Indeed, we are doing something very similar.&lt;/p&gt;
&lt;p&gt;In linear regression, we usually have &lt;strong&gt;many units&lt;/strong&gt; (observations), &lt;strong&gt;few exogenous features&lt;/strong&gt; and &lt;strong&gt;one endogenous feature&lt;/strong&gt; and we try to express the endogenous feature as a linear combination of the endogenous features, for each unit.&lt;/p&gt;
&lt;img src=&#34;fig/synth1.png&#34; width=&#34;500px&#34;/&gt;
&lt;p&gt;With synthetic control, we instead have &lt;strong&gt;many time periods&lt;/strong&gt; (features), few &lt;strong&gt;control units&lt;/strong&gt; and a single &lt;strong&gt;treated unit&lt;/strong&gt; and we try to express the treated unit as a linear combination of the control units, for each time period&lt;/p&gt;
&lt;img src=&#34;fig/synth2.png&#34; width=&#34;500px&#34;/&gt;
&lt;p&gt;In the end, we are doing the same thing, but with a transposed dataset.&lt;/p&gt;
&lt;img src=&#34;fig/synth3.png&#34; width=&#34;500px&#34;/&gt;
&lt;h3 id=&#34;back-to-self-driving-cars&#34;&gt;Back to self-driving cars&lt;/h3&gt;
&lt;p&gt;Let&amp;rsquo;s go back to the data now! First, we write a &lt;code&gt;synth_predict&lt;/code&gt; function that takes as input a model that is trained on control cities and tries to predict the outcome of the treated city, Miami, before the introduction of self-driving cars.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def synth_predict(df, model, city, year):
    other_cities = [c for c in cities if c not in [&#39;year&#39;, city]]
    y = df.loc[df[&#39;year&#39;] &amp;lt;= year, city]
    X = df.loc[df[&#39;year&#39;] &amp;lt;= year, other_cities]
    df[f&#39;Synthetic {city}&#39;] = model.fit(X, y).predict(df[other_cities])
    return model
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s estimate the model via linear regression.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from sklearn.linear_model import LinearRegression

coef = synth_predict(df, LinearRegression(), treated_city, treatment_year).coef_
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;How well did we &lt;strong&gt;match&lt;/strong&gt; pre-self-driving cars &lt;code&gt;revenue&lt;/code&gt; in Miami? What is the implied &lt;strong&gt;effect&lt;/strong&gt; of self-driving cars?&lt;/p&gt;
&lt;p&gt;We can visually answer both questions by plotting the actual revenue in Miami against the predicted one.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plot_lines(df, treated_city, f&#39;Synthetic {treated_city}&#39;, treatment_year)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/synthetic_control_43_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;It looks like self-driving cars had a sensible &lt;strong&gt;positive effect&lt;/strong&gt; on &lt;code&gt;revenue&lt;/code&gt; in Miami: the predicted trend is lower than the actual data and diverges right after the introduction of self-driving cars.&lt;/p&gt;
&lt;p&gt;On the other hand, we are clearly &lt;strong&gt;overfitting&lt;/strong&gt;: the pre-treatment predicted &lt;code&gt;revenue&lt;/code&gt; line is perfectly overlapping with the actual data. Given the high variability of &lt;code&gt;revenue&lt;/code&gt; in Miami, this is suspicious, to say the least.&lt;/p&gt;
&lt;p&gt;Another problem concerns the &lt;strong&gt;weights&lt;/strong&gt;. Let&amp;rsquo;s plot them.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_states = pd.DataFrame({&#39;city&#39;: [c for c in cities if c!=treated_city], &#39;ols_coef&#39;: coef})
plt.figure(figsize=(10, 10))
sns.barplot(data=df_states, x=&#39;ols_coef&#39;, y=&#39;city&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/synthetic_control_45_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;We have many &lt;strong&gt;negative weights&lt;/strong&gt;, which do not make much sense from a causal inference perspective. I can understand that Miami can be expressed as a combination of 0.2 St. Louis, 0.15 Oklahoma and 0.15 Hartford. But what does it mean that Miami is -0.15 Milwaukee?&lt;/p&gt;
&lt;p&gt;Since we would like to interpret our synthetic control as a &lt;strong&gt;weighted average&lt;/strong&gt; of untreated states, all weights should be positive  and they should sum to one.&lt;/p&gt;
&lt;p&gt;To address both concerns (weighting and overfitting), we need to impose some &lt;strong&gt;restrictions on the weights&lt;/strong&gt;.&lt;/p&gt;
&lt;h2 id=&#34;extensions&#34;&gt;Extensions&lt;/h2&gt;
&lt;h3 id=&#34;weights&#34;&gt;Weights&lt;/h3&gt;
&lt;p&gt;To solve the problems of overweighting and negative weights, &lt;a href=&#34;https://www.tandfonline.com/doi/abs/10.1198/jasa.2009.ap08746&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Abadie, Diamond and Hainmueller (2010)&lt;/a&gt; propose the following weights:&lt;/p&gt;
&lt;p&gt;$$
\hat \beta = \arg \min_{\beta} || \boldsymbol X_t - \boldsymbol \beta \boldsymbol X_c || = \sqrt{ \sum_{p} \left( X_{t, p}  - \sum_{i \in c} \beta_{p} X_{c, p} \right)^2 }
\quad \text{s.t.} \quad \sum_{p} \beta_p = 1 \quad \text{and} \quad \beta_p \geq 0 \quad \forall p
$$&lt;/p&gt;
&lt;p&gt;Which means, a set of weights $\beta$ such that&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;weighted observable characteristics of the control group $X_c$, match the observable characteristics of the treatment group $X_t$, before the treatment&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;they sum to 1&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;and are not negative.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;With this approach we get an &lt;strong&gt;interpretable counterfactual&lt;/strong&gt; as a weighted avarage of untreated units.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s write now our own objective function. I create a new class &lt;code&gt;SyntheticControl()&lt;/code&gt; which has both a &lt;code&gt;loss&lt;/code&gt; function, as described above, a method to &lt;code&gt;fit&lt;/code&gt; it and &lt;code&gt;predict&lt;/code&gt; the values for the treated unit.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from toolz import partial
from scipy.optimize import fmin_slsqp

class SyntheticControl():
    
    # Loss function
    def loss(self, W, X, y) -&amp;gt; float:
        return np.sqrt(np.mean((y - X.dot(W))**2))

    # Fit model
    def fit(self, X, y):
        w_start = [1/X.shape[1]]*X.shape[1]
        self.coef_ = fmin_slsqp(partial(self.loss, X=X, y=y),
                         np.array(w_start),
                         f_eqcons=lambda x: np.sum(x) - 1,
                         bounds=[(0.0, 1.0)]*len(w_start),
                         disp=False)
        self.mse = self.loss(W=self.coef_, X=X, y=y)
        return self
    
    # Predict 
    def predict(self, X):
        return X.dot(self.coef_)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can now repeat the same procedure as before, but using the &lt;code&gt;SyntheticControl&lt;/code&gt; method instead of the simple, unconstrained &lt;code&gt;LinearRegression&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_states[&#39;coef_synth&#39;] = synth_predict(df, SyntheticControl(), treated_city, treatment_year).coef_
plot_lines(df, treated_city, f&#39;Synthetic {treated_city}&#39;, treatment_year)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/synthetic_control_52_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;As we can see, now we are &lt;strong&gt;not overfitting&lt;/strong&gt; anymore. The actual and predicted &lt;code&gt;revenue&lt;/code&gt; pre-treatment are close but not identical. The reason is that the non-negativity constraint is constraining most coefficients to be zero (as &lt;a href=&#34;https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Lasso&lt;/a&gt; does).&lt;/p&gt;
&lt;p&gt;It looks like the effect is again negative. However, let&amp;rsquo;s plot the &lt;strong&gt;difference&lt;/strong&gt; between the two lines to better visualize the magnitude.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def plot_difference(df, city, year, vline=True, hline=True, **kwargs):
    sns.lineplot(x=df[&#39;year&#39;], y=df[city] - df[f&#39;Synthetic {city}&#39;], **kwargs)
    if vline: 
        plt.axvline(x=year, ls=&amp;quot;:&amp;quot;, color=&#39;C2&#39;, lw=3, label=&#39;Self-driving Cars&#39;, zorder=100)
        plt.legend()
    if hline: sns.lineplot(x=df[&#39;year&#39;], y=0, lw=3, color=&#39;k&#39;, zorder=1)
    plt.title(&amp;quot;Estimated effect of self-driving cars&amp;quot;);
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plot_difference(df, treated_city, treatment_year)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/synthetic_control_55_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;The difference is clearly positive and slightly increasing over time.&lt;/p&gt;
&lt;p&gt;We can also visualize the &lt;strong&gt;weights&lt;/strong&gt; to interpret the estimated counterfactual (what would have happened in Miami, without self-driving cars).&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plt.figure(figsize=(10, 10))
sns.barplot(data=df_states, x=&#39;coef_synth&#39;, y=&#39;city&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/synthetic_control_57_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;As we can see, now we are expressing &lt;code&gt;revenue&lt;/code&gt; in Miami as a linear combination of just a couple of cities: Tampa, St. Louis and, to a lower extent, Las Vegas. This makes the whole procedure very &lt;strong&gt;transparent&lt;/strong&gt;.&lt;/p&gt;
&lt;h3 id=&#34;inference&#34;&gt;Inference&lt;/h3&gt;
&lt;p&gt;What about &lt;strong&gt;inference&lt;/strong&gt;? Is the estimate significantly different from zero? Or, more practically, &amp;ldquo;&lt;em&gt;how unusual is this estimate under the null hypothesis of no policy effect&lt;/em&gt;?&amp;rdquo;.&lt;/p&gt;
&lt;p&gt;We are going to perform a &lt;a href=&#34;https://en.wikipedia.org/wiki/Permutation_test&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;randomization/permutation test&lt;/strong&gt;&lt;/a&gt; in order to answer this question. The &lt;strong&gt;idea&lt;/strong&gt; is that if the policy has no effect, the effect we observe for Miami should not be significantly different from the effect we observe for any other city.&lt;/p&gt;
&lt;p&gt;Therefore, we are going to replicate the procedure above, but for all other cities and compare them with the estimate for Miami.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from matplotlib.offsetbox import OffsetImage, AnnotationBbox

fig, ax = plt.subplots()
for city in cities:
    synth_predict(df, SyntheticControl(), city, treatment_year)
    plot_difference(df, city, treatment_year, vline=False, alpha=0.2, color=&#39;C1&#39;, lw=3)
plot_difference(df, treated_city, treatment_year)
ax.add_artist(AnnotationBbox(OffsetImage(mpimg.imread(&#39;fig/miami.png&#39;), zoom=0.25), (2015, 2.7), frameon=False));
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;---------------------------------------------------------------------------

NameError                                 Traceback (most recent call last)

Input In [21], in &amp;lt;module&amp;gt;
      6     plot_difference(df, city, treatment_year, vline=False, alpha=0.2, color=&#39;C1&#39;, lw=3)
      7 plot_difference(df, treated_city, treatment_year)
----&amp;gt; 8 ax.add_artist(AnnotationBbox(OffsetImage(mpimg.imread(&#39;fig/miami.png&#39;), zoom=0.25), (2015, 2.7), frameon=False))


NameError: name &#39;mpimg&#39; is not defined
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/synthetic_control_61_1.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;From the graph we notice two things. First, the effect for California is quite extreme and therefore likely not to be driven by random noise.&lt;/p&gt;
&lt;p&gt;Second, we also notice that there are a couple of states for which we cannot fit the pre-trend very well. This is expected since, for each state, we are building the counterfactual trend as a &lt;strong&gt;convex combination&lt;/strong&gt; of all other states. States that are quite extreme in terms of cigarette consumpion are very useful to build the counterfactuals of other states, but it&amp;rsquo;s hard to build a counterfactual for them. Not to bias the analysis, let&amp;rsquo;s exclude states for which we cannot build a &amp;ldquo;good enough&amp;rdquo; counterfectual, in terms of pre-treatment MSE.&lt;/p&gt;
&lt;p&gt;$$
MSE_{pre} = \frac{1}{n} \sum_{t \in \text{pre}} \left( Y_t - \hat Y_t \right)^2
$$&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;mse_treated = synth_predict(df, SyntheticControl(), treated_city, treatment_year).mse

fig, ax = plt.subplots()
for city in cities:
    mse = synth_predict(df, SyntheticControl(), city, treatment_year).mse
    if mse &amp;lt; 2 * mse_treated:
        plot_difference(df, city, treatment_year, vline=False, alpha=0.2, color=&#39;C1&#39;, lw=3)
plot_difference(df, treated_city, treatment_year)
ax.add_artist(AnnotationBbox(OffsetImage(mpimg.imread(&#39;fig/miami.png&#39;), zoom=0.25), (2015, 2.7), frameon=False));
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;After exluding extreme observations, it looks like the effect for California is very unusual, especially if we consider a one-sided hypothesis test (it feels weird to assume that the policy could ever increase cigarette sales).&lt;/p&gt;
&lt;p&gt;One &lt;strong&gt;statistic&lt;/strong&gt; that the authors suggest to perform a randomization test is the ratio between pre-treatment MSE and post-treatment MSE.&lt;/p&gt;
&lt;p&gt;$$
\lambda = \frac{MSE_{post}}{MSE_{pre}} = \frac{\frac{1}{n} \sum_{t \in \text{post}} \left( Y_t - \hat Y_t \right)^2 }{\frac{1}{n} \sum_{t \in \text{pre}} \left( Y_t - \hat Y_t \right)^2 }
$$&lt;/p&gt;
&lt;p&gt;We can compute a p-value as the number of observations with higher ratio.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;lambdas = {}
for city in cities:
    mse_pre = synth_predict(df, SyntheticControl(), city, treatment_year).mse
    mse_tot = np.mean((df[f&#39;Synthetic {city}&#39;] - df[city])**2)
    lambdas[city] = (mse_tot - mse_pre) / mse_pre
    
print(f&amp;quot;p-value: {np.mean(np.fromiter(lambdas.values(), dtype=&#39;float&#39;) &amp;gt; lambdas[treated_city]):.4}&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It seems that only $4.3%$ of the cities had a larger MSE ratio. We can visualize the distribution of the statistic under permutation with a histogram.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;fig, ax = plt.subplots()
_, bins, _ = plt.hist(lambdas.values(), bins=20, color=&amp;quot;C1&amp;quot;);
plt.hist([lambdas[treated_city]], bins=bins)
plt.title(&#39;Ratio of $MSE_{post}$ and $MSE_{pre}$ across cities&#39;);
ax.add_artist(AnnotationBbox(OffsetImage(plt.imread(&#39;fig/miami.png&#39;), zoom=0.25), (2.7, 1.6), frameon=False));
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Indeed, the statistic for Miami is quite extreme.&lt;/p&gt;
&lt;h3 id=&#34;synthetic-control-vs-other-methods&#34;&gt;Synthetic Control vs Other Methods&lt;/h3&gt;
&lt;p&gt;What are the advantages and disadvantages of synthetic control methods with respect to other methods?&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;As long as we use positive weights that are constrained to sum to one, the method &lt;strong&gt;avoids extrapolation&lt;/strong&gt;: we will never go out of the support of the data&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;It can be &lt;strong&gt;&amp;ldquo;pre-registered&amp;rdquo;&lt;/strong&gt; in the sense that you don&amp;rsquo;t need post-treatment observations to build the method: could avoid p-hacking and cherry picking&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Weights make the counterfactual analysis &lt;strong&gt;explicit&lt;/strong&gt;: one can look at the weights and understand which comparison we are making&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;It&amp;rsquo;s a &lt;strong&gt;bridge&lt;/strong&gt; between quantitative and qualitative research: can be used to inspect single-treated unit cases&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;TBD&lt;/p&gt;
&lt;h3 id=&#34;references&#34;&gt;References&lt;/h3&gt;
&lt;p&gt;[1] A. Abadie, A. Diamond and J. Hainmueller, &lt;a href=&#34;https://www.tandfonline.com/doi/abs/10.1198/jasa.2009.ap08746&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Synthetic Control Methods for Comparative Case Studies: Estimating the Effect of California’s Tobacco Control Program&lt;/a&gt; (2010), &lt;em&gt;Journal of the American Statistical Association&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;[2] A. Abadie, J. L&amp;rsquo;Hour, &lt;a href=&#34;https://www.tandfonline.com/doi/full/10.1080/01621459.2021.1971535&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;A Penalized Synthetic Control Estimator for Disaggregated Data&lt;/a&gt; (2020), &lt;em&gt;Journal of the American Statistical Association&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;[3] N. Doudchenko, G. Imbens, &lt;a href=&#34;https://arxiv.org/pdf/1610.07748.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Balancing, Regression, Difference-In-Differences and Synthetic Control Methods: A Synthesis&lt;/a&gt; (2017), &lt;em&gt;working paper&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;[] Matrix completion methods for causal panel data models&lt;/p&gt;
&lt;p&gt;[] Using Synthetic Controls: Feasibility, Data Requirements, and Methodological Aspects&lt;/p&gt;
&lt;h3 id=&#34;code&#34;&gt;Code&lt;/h3&gt;
&lt;p&gt;You can find the original Jupyter Notebook here:&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/synthetic_control.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/synthetic_control.ipynb&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>The Bayesian Bootstrap</title>
      <link>https://matteocourthoud.github.io/post/bayesian_bootstrap/</link>
      <pubDate>Mon, 10 Jul 2023 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/post/bayesian_bootstrap/</guid>
      <description>&lt;p&gt;&lt;em&gt;A short guide to a simple and powerful alternative to the bootstrap&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;In causal inference we do not want just to compute treatment effect, we also want to do &lt;strong&gt;inference&lt;/strong&gt; (duh!). In some cases, it&amp;rsquo;s very easy to compute the asymptotic difference of an estimator, thanks to the &lt;a href=&#34;https://en.wikipedia.org/wiki/Central_limit_theorem&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;central limit theorem&lt;/strong&gt;&lt;/a&gt;. This is the case of computing the average treatment effect in AB tests or randomized controlled trials, for example. However, in other settings, inference is more &lt;strong&gt;complicated&lt;/strong&gt;. The most frequent setting is the computation of quantities that are not sums or averages, such as the median treatment effect, for example. In these cases, we cannot rely on the central limit theorem. What can we do then?&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;bootstrap&lt;/strong&gt; is the answer! It is a very powerful procedure to compute the distribution of an estimator, without needing any knowledge of the data generating process. It is also very &lt;strong&gt;intuitive and simple&lt;/strong&gt; to implement: just re-sample your data with replacement a lot of times and compute your estimator on the re-computed sample.&lt;/p&gt;
&lt;p&gt;Can we do better? The answer is yes! The &lt;strong&gt;Bayesian Bootstrap&lt;/strong&gt; is a powerful procedure that in a lot of setting performs &lt;strong&gt;better&lt;/strong&gt; than the bootstrap. In particular, it&amp;rsquo;s usually faster, can give tighter confidence intervals and prevents a lot of corner cases of the bootstrap. In this article we are going to explore this simple but powerful procedure more in detail.&lt;/p&gt;
&lt;h2 id=&#34;the-bootstrap&#34;&gt;The Bootstrap&lt;/h2&gt;
&lt;p&gt;Bootstrap is a procedure to compute properties of an estimator by random &lt;strong&gt;re-sampling with replacement&lt;/strong&gt; from the data. It was first introduced by &lt;a href=&#34;https://www.jstor.org/stable/2958830&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Efron (1979)&lt;/a&gt;. The procedure is very simple and consists in the following steps.&lt;/p&gt;
&lt;p&gt;Suppose you have access to an i.i.d. sample $\lbrace X_i \rbrace_{i=1}^n$ and you want to compute a statistic $\theta$ using an estimator $\hat \theta(X)$. You can approximate the distribution of $\hat \theta$ by&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Sample $n$ observations with replacement from your sample $\lbrace \tilde X_i \rbrace_{i=1}^n$&lt;/li&gt;
&lt;li&gt;Compute the estimator $\hat \theta_{bootstrap}(\tilde X)$&lt;/li&gt;
&lt;li&gt;Repeat steps 1 and 2 a large number of times&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The distribution of $\hat \theta_{bootstrap}$ is a good approximation of the distribution of $\hat \theta$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Why is the bootstrap so powerful?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;First of all, it&amp;rsquo;s &lt;strong&gt;easy to implement&lt;/strong&gt;. It does not require you to do anything more than what you were already doing: estimating $\theta$. You just need to do it &lt;em&gt;a lot of times&lt;/em&gt;. Indeed, the main disadvantage of the bootstrap is its &lt;strong&gt;computational speed&lt;/strong&gt;. If estimating $\theta$ once is slow, bootstrapping it is prohibitive.&lt;/p&gt;
&lt;p&gt;Second, the bootstrap makes &lt;strong&gt;no distributional assumption&lt;/strong&gt;. It only assumes a representative sample from your population, where observations are independent from each other. This assumption might be violated when observations are tightly connected with each other, such when studying social networks.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Is bootstrap just weighting?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In the end, what we are doing is assigning &lt;strong&gt;integer weights&lt;/strong&gt; to our observations, such that their sum adds up to $n$. Such distribution is the &lt;a href=&#34;https://en.wikipedia.org/wiki/Multinomial_distribution&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;multinomial distribution&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s have a look at what a multinomial distribution look like by drawing a sample of size 10.000.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;%matplotlib inline
%config InlineBackend.figure_format = &#39;retina&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from src.utils import *
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;N = 10_000
np.random.seed(1)
bootstrap_weights = np.random.multinomial(N, np.ones(N)/N)
np.sum(bootstrap_weights)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;10000
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;First of all, we check that indeed the weights sum up to 1000, or equivalently, we generated a re-sample of the same size of the data.&lt;/p&gt;
&lt;p&gt;We can now plot the &lt;strong&gt;distribution of weights&lt;/strong&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sns.countplot(bootstrap_weights, color=&#39;C0&#39;).set(title=&#39;Bootstrap Weights&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/bayesian_bootstrap_9_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;As we can see, around 3600 observations got zero weight, however a couple of observations got a weights of 6. Or equivalently, around 3600 observations did not get re-sampled while a couple of observations got samples as many as 6 times.&lt;/p&gt;
&lt;p&gt;Now you might have a spontaneous question: why not use &lt;strong&gt;continuous weights&lt;/strong&gt; instead of discrete ones?&lt;/p&gt;
&lt;p&gt;Very good question! The &lt;strong&gt;Bayesian Bootstrap&lt;/strong&gt; is the answer.&lt;/p&gt;
&lt;h2 id=&#34;the-bayesian-bootstrap&#34;&gt;The Bayesian Bootstrap&lt;/h2&gt;
&lt;p&gt;The Bayesian bootstrap was introduced by &lt;a href=&#34;https://www.jstor.org/stable/2240875&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Rubin (1981)&lt;/a&gt; and it&amp;rsquo;s based on a very simple &lt;strong&gt;idea&lt;/strong&gt;: why not draw a smoother distribution of weights? The continuous equivalent of the multinomial distribution is the &lt;a href=&#34;https://en.wikipedia.org/wiki/Dirichlet_distribution&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Dirichelet distribution&lt;/strong&gt;&lt;/a&gt;. Below I plot the probability distribution of Multinomial and Dirichelet weights for a single observation (they are Poisson and Gamma distributed, respectively).&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from scipy.stats import gamma, poisson

x1 = np.arange(0, 8, 0.001)
x2 = np.arange(0, 8, 1)
sns.barplot(x2, poisson.pmf(x2, mu=1), color=&#39;C0&#39;, label=&#39;Multinomial Weights&#39;); 
plt.plot(x1, gamma.pdf(x1, a=1.0001), color=&#39;C1&#39;, label=&#39;Dirichlet Weights&#39;);
plt.legend()
plt.title(&#39;Distribution of Bootstrap Weights&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/bayesian_bootstrap_13_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;The Bayesian Bootstrap has &lt;strong&gt;many advantages&lt;/strong&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The first and most intuitive one is that it delivers estimates that are much more &lt;strong&gt;smooth&lt;/strong&gt; than the normal bootstrap, because of its continuous weighting scheme.&lt;/li&gt;
&lt;li&gt;Moreover, the continuous weighting scheme &lt;strong&gt;prevents corner cases&lt;/strong&gt; from emerging, since no observation will ever receive zero weight. For example, in linear regression, no problem of &lt;a href=&#34;https://en.wikipedia.org/wiki/Multicollinearity&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;collinearity&lt;/a&gt; emerges, if there wasn&amp;rsquo;t one in the original sample.&lt;/li&gt;
&lt;li&gt;Lastly, being a Bayesian method, we gain &lt;strong&gt;interpretation&lt;/strong&gt;: the estimated distribution of the estimator can be interpreted as the &lt;a href=&#34;https://en.wikipedia.org/wiki/Posterior_probability&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;posterior distribution&lt;/a&gt; with an &lt;a href=&#34;https://en.wikipedia.org/wiki/Prior_probability&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;uninformative prior&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Let&amp;rsquo;s now draw a set a Dirichlet weights.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;bayesian_weights = np.random.dirichlet(alpha=np.ones(N), size=1)[0] * N
np.sum(bayesian_weights)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;10000.000000000005
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The weights naturally sum to (approximately) 1, so we have to scale them by a factor N.&lt;/p&gt;
&lt;p&gt;As before, we can plot the distribution of weights, with the difference that now we have continuous weights, so we have to approximate the distribution.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sns.histplot(bayesian_weights, color=&#39;C1&#39;).set(title=&#39;Dirichlet Weights&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/bayesian_bootstrap_17_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;As you might have noticed, the Dirichelet distirbution has a parameter $\alpha$ that we have set to 1 for all observations. What does it do?&lt;/p&gt;
&lt;p&gt;The $\alpha$ parameter essentially governs both the absolute and relative probability of being samples. Increasing $\alpha$ for all observations makes the distribution less skewed so that all observations have a more similar weight. For $\alpha \to \infty$, all observations receiver the same weight and we are back to the original sample.&lt;/p&gt;
&lt;p&gt;How should we pick $\alpha$? &lt;a href=&#34;https://link.springer.com/book/10.1007/978-1-4612-0795-5&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Shao and Tu (1995)&lt;/a&gt; suggest the following.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;The distribution of the random weight vector does not have to be restricted to the Diri(l, &amp;hellip; , 1). Later investigations found that the weights having a scaled Diri(4, &amp;hellip; ,4) distribution give better approximations (Tu and Zheng, 1987)&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Let&amp;rsquo;s have a look at how a Dirichelet distribution with $\alpha = 4$ for all observations compare to our previous distribution with $\alpha = 1$ for all observations.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;bayesian_weights2 = np.random.dirichlet(np.ones(N) * 4, 1)[0] * N
sns.histplot(bayesian_weights, color=&#39;C1&#39;)
sns.histplot(bayesian_weights2, color=&#39;C2&#39;).set(title=&#39;Comparing Dirichlet Weights&#39;);
plt.legend([r&#39;$\alpha = 1$&#39;, r&#39;$\alpha = 4$&#39;]);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/bayesian_bootstrap_19_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;The new distribution is much less skewed and more concentrated around the average value of 1.&lt;/p&gt;
&lt;h2 id=&#34;examples&#34;&gt;Examples&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s have a look at a couple of examples, where we compare both inference procedures.&lt;/p&gt;
&lt;h3 id=&#34;mean-of-a-skewed-distribution&#34;&gt;Mean of a Skewed Distribution&lt;/h3&gt;
&lt;p&gt;First, let&amp;rsquo;s have a look at one of the simplest and most common estimators: the &lt;strong&gt;sample mean&lt;/strong&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;np.random.seed(2)
X = pd.Series(np.random.pareto(2, 100))
sns.histplot(X).set(title=&#39;Sample from Pareto Distribution&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/bayesian_bootstrap_25_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def classic_boot(df, estimator, seed=1):
    df_boot = df.sample(n=len(df), replace=True, random_state=seed)
    estimate = estimator(df_boot)
    return estimate
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;classic_boot(X, np.mean)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;0.7079805545831946
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def bayes_boot(df, estimator, seed=1):
    np.random.seed(seed)
    w = np.random.dirichlet(np.ones(len(df)), 1)[0]
    result = estimator(df, weights=w)
    return result
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;bayes_boot(X, np.average)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;1.0378495251293498
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from joblib import Parallel, delayed

def bootstrap(boot_method, df, estimator, K):
    r = Parallel(n_jobs=8)(delayed(boot_method)(df, estimator, seed=i) for i in range(K))
    return r
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def compare_boot(df, boot1, boot2, estimator, title, K=1000):
    s1 = bootstrap(boot1, df, estimator, K)
    s2 = bootstrap(boot2, df, estimator, K)
    df = pd.DataFrame({&#39;Estimate&#39;: s1 + s2,
                       &#39;Estimator&#39;: [&#39;Classic&#39;]*K + [&#39;Bayes&#39;]*K})
    sns.histplot(data=df, x=&#39;Estimate&#39;, hue=&#39;Estimator&#39;)
    plt.legend([f&#39;Bayes:   {np.mean(s2):.2f} ({np.std(s2):.2f})&#39;,
                f&#39;Classic: {np.mean(s1):.2f} ({np.std(s1):.2f})&#39;])
    plt.title(f&#39;Bootstrap Estimates of {title}&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;compare_boot(X, classic_boot, bayes_boot, np.average, &#39;Sample Mean&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/bayesian_bootstrap_32_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;In this setting, both procedures give a very similar answer.&lt;/p&gt;
&lt;p&gt;Which one is faster?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import time

def compare_time(df, boot1, boot2, estimator, K=1000):
    t1, t2 = np.zeros(K), np.zeros(K)
    for k in range(K):
        
        # Classic bootstrap
        start = time.time()
        boot1(df, estimator)
        t1[k] = time.time() - start
    
        # Bayesian bootstrap
        start = time.time()
        boot2(df, estimator)
        t2[k] = time.time() - start
    
    print(f&amp;quot;Bayes wins {np.mean(t1 &amp;gt; t2)*100}% of the time (by {np.mean((t1 - t2)/t1*100):.2f}%)&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;compare_time(X, classic_boot, bayes_boot, np.average)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Bayes wins 99.8% of the time (by 82.89%)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The Bayesian bootstrap is faster than the classical bootstrap 100% of the simulations, and by an impressive 83%!&lt;/p&gt;
&lt;h3 id=&#34;no-weighting-no-problem&#34;&gt;No Weighting? No Problem&lt;/h3&gt;
&lt;p&gt;What if we have an estimator that does not accept weights, such as the median? We can do &lt;strong&gt;two-level sampling&lt;/strong&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def twolv_boot(df, estimator, seed=1):
    np.random.seed(seed)
    w = np.random.dirichlet(np.ones(len(df))*4, 1)[0]
    df_boot = df.sample(n=len(df)*10, replace=True, weights=w, random_state=seed)
    result = estimator(df_boot)
    return result
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;np.random.seed(1)
X = pd.Series(np.random.normal(0, 10, 1000))
compare_boot(X, classic_boot, twolv_boot, np.median, &#39;Sample Median&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/bayesian_bootstrap_40_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;In this setting, the Bayesian Bootstrap is also &lt;strong&gt;more precise&lt;/strong&gt; than the classical bootstrap.&lt;/p&gt;
&lt;h3 id=&#34;logistic-regression-with-rare-outcome&#34;&gt;Logistic Regression with Rare Outcome&lt;/h3&gt;
&lt;p&gt;Let&amp;rsquo;s now explore the first of two settings in which the classical bootstrap might fall into &lt;strong&gt;corner cases&lt;/strong&gt;. Suppose we observed a feature $x$, normally distributed, and a binary outcome $y$. We are interested in the relationship between the two variables.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;N = 100
np.random.seed(1)
x = np.random.normal(0, 1, N)
y = np.rint(np.random.normal(x, 1, N) &amp;gt; 2)
df = pd.DataFrame({&#39;x&#39;: x, &#39;y&#39;: y})
df.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;x&lt;/th&gt;
      &lt;th&gt;y&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;1.624345&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;-0.611756&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;-0.528172&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;-1.072969&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;0.865408&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;In this case, we observe a positive outcome only in 10 observations out of 100.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;np.sum(df[&#39;y&#39;])
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;10.0
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Since the outcome is binary, we fit a &lt;a href=&#34;https://en.wikipedia.org/wiki/Logistic_regression&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;logistic regression&lt;/strong&gt;&lt;/a&gt; model.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;smf.logit(&#39;y ~ x&#39;, data=df).fit(disp=False).summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
      &lt;td&gt;&lt;/td&gt;         &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;z&lt;/th&gt;      &lt;th&gt;P&gt;|z|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt; &lt;td&gt;   -4.0955&lt;/td&gt; &lt;td&gt;    0.887&lt;/td&gt; &lt;td&gt;   -4.618&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;   -5.834&lt;/td&gt; &lt;td&gt;   -2.357&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;x&lt;/th&gt;         &lt;td&gt;    2.7664&lt;/td&gt; &lt;td&gt;    0.752&lt;/td&gt; &lt;td&gt;    3.677&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    1.292&lt;/td&gt; &lt;td&gt;    4.241&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;Can we bootstrap the distribution of our estimator? Let&amp;rsquo;s try to compute the logistic regression coefficient over 1000 bootstrap samples.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;estimate_logit = lambda df: smf.logit(&#39;y ~ x&#39;, data=df).fit(disp=False).params[1]
for i in range(1000):
    try:
        classic_boot(df, estimate_logit, seed=i)
    except Exception as e:
        print(f&#39;Error for bootstrap number {i}: {e}&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Error for bootstrap number 92: Perfect separation detected, results not available
Error for bootstrap number 521: Perfect separation detected, results not available
Error for bootstrap number 545: Perfect separation detected, results not available
Error for bootstrap number 721: Perfect separation detected, results not available
Error for bootstrap number 835: Perfect separation detected, results not available
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For 5 samples out of 1000, we are &lt;strong&gt;unable&lt;/strong&gt; to compute the estimate. This would not have happened with then bayesian bootstrap.&lt;/p&gt;
&lt;p&gt;This might seem like an innocuous issue in this case: we can just drop those observations. Let&amp;rsquo;s conclude with a much more dangerous example.&lt;/p&gt;
&lt;p&gt;Suppose we observed a binary feature $x$ and a continuous outcome $y$. We are again interested in the relationship between the two variables.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;N = 100
np.random.seed(1)
x = np.random.binomial(1, 5/N, N)
y = np.random.normal(1 + 2*x, 1, N)
df = pd.DataFrame({&#39;x&#39;: x, &#39;y&#39;: y})
df.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;x&lt;/th&gt;
      &lt;th&gt;y&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1.315635&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;-1.022201&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0.693796&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1.827975&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1.230095&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;Let&amp;rsquo;s compare the two bootstrap estimators of the regression coefficient of $y$ on $x$.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;estimate_beta = lambda df, **kwargs: smf.wls(&#39;y ~ x&#39;, data=df, **kwargs).fit().params[1]
compare_boot(df, classic_boot, bayes_boot, estimate_beta, &#39;beta&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/bayesian_bootstrap_55_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;The classic bootstrap procedure estimates a 50% larger variance of our estimator. Why? If we look more closely, we seen that in almost 20 re-samples, we get a very unusual estimate of zero!&lt;/p&gt;
&lt;p&gt;The problem is that in some samples we might not have have &lt;strong&gt;any observations&lt;/strong&gt; with $x=1$. Therefore, in these re-samples, the estimated coefficient is zero. This does not happen with the Bayesian bootstrap, since it does not drop any observation.&lt;/p&gt;
&lt;p&gt;The problematic part here is that we are not getting any error message or warning. This bias is very sneaky and could easily go &lt;strong&gt;unnoticed&lt;/strong&gt;!&lt;/p&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;The article was inspired by the following tweet by Brown University professor &lt;a href=&#34;https://sites.google.com/site/aboutpeterhull/home&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Peter Hull&lt;/a&gt;&lt;/p&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;Ok, so I come bearing good news for ~93% of you: esp. those bootstraping complex models (e.g. w/many FEs)&lt;br&gt;&lt;br&gt;Instead of resampling, which can be seen as reweighting by a random integer W that may be zero, you can reweight by a random non-zero non-integer W &lt;a href=&#34;https://t.co/Rpm1GmomHg&#34;&gt;https://t.co/Rpm1GmomHg&lt;/a&gt;&lt;/p&gt;&amp;mdash; Peter Hull (@instrumenthull) &lt;a href=&#34;https://twitter.com/instrumenthull/status/1487469316010389516?ref_src=twsrc%5Etfw&#34;&gt;January 29, 2022&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;

&lt;p&gt;Indeed, besides being a simple and intuitive procedure, the Bayesian Bootstrap is not part of the standard econometrics curriculum in economic graduate schools.&lt;/p&gt;
&lt;h3 id=&#34;references&#34;&gt;References&lt;/h3&gt;
&lt;p&gt;[1] B. Efron &lt;a href=&#34;https://www.jstor.org/stable/2958830&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Bootstrap Methods: Another Look at the Jackknife&lt;/a&gt; (1979), &lt;em&gt;The Annals of Statistics&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;[2] D. Rubin, &lt;a href=&#34;https://www.jstor.org/stable/2240875&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The Bayesian Bootstrap&lt;/a&gt; (1981), &lt;em&gt;The Annals of Statistics&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;[3] A. Lo, &lt;a href=&#34;https://www.jstor.org/stable/2241087&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;A Large Sample Study of the Bayesian Bootstrap&lt;/a&gt; (1987), &lt;em&gt;The Annals of Statistics&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;[4] J. Shao, D. Tu, &lt;a href=&#34;https://link.springer.com/book/10.1007/978-1-4612-0795-5&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Jacknife and Bootstrap&lt;/a&gt; (1995), &lt;em&gt;Springer&lt;/em&gt;.&lt;/p&gt;
&lt;h3 id=&#34;code&#34;&gt;Code&lt;/h3&gt;
&lt;p&gt;You can find the original Jupyter Notebook here:&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/bayes_boot.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/bayes_boot.ipynb&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>The FWL Theorem, Or How To Make All Regressions Intuitive</title>
      <link>https://matteocourthoud.github.io/post/fwl_theorem/</link>
      <pubDate>Mon, 10 Jul 2023 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/post/fwl_theorem/</guid>
      <description>&lt;p&gt;&lt;em&gt;An introduction to the Frisch-Waugh-Lowell theorem and how to use it to gain intuition in linear regressions&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;The Frisch-Waugh-Lowell theorem is a &lt;strong&gt;simple&lt;/strong&gt; but yet &lt;strong&gt;powerful&lt;/strong&gt; theorem that allows us to reduce multivariate regressions to &lt;strong&gt;univariate&lt;/strong&gt; ones. This is extremely useful when we are interested in the relationship between two variables, but we still need to control for other factors, as it is often the case in &lt;strong&gt;causal inference&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;In this blog post, I am going to introduce the Frisch-Waugh-Lowell theorem and illustrate some interesting applications.&lt;/p&gt;
&lt;h2 id=&#34;the-theorem&#34;&gt;The Theorem&lt;/h2&gt;
&lt;p&gt;The theorem was first published by &lt;a href=&#34;https://www.jstor.org/stable/1907330&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Ragnar Frisch and Frederick Waugh in 1933&lt;/a&gt;. However, since its proof was lengthy and cumbersome, &lt;a href=&#34;https://www.tandfonline.com/doi/abs/10.1080/01621459.1963.10480682&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Michael Lovell in 1963&lt;/a&gt; provided a very simple and intuitive proof and his name was added to the theorem name.&lt;/p&gt;
&lt;p&gt;The theorem states that, when estimating a model of the form&lt;/p&gt;
&lt;p&gt;$$
y_i = \beta_1 x_{i,1} + \beta_2 x_{i,2} + \varepsilon_i
$$&lt;/p&gt;
&lt;p&gt;then, the following estimators of $\beta_1$ are equivalent:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the OLS estimator obtained by regressing $y$ on $x_1$ and $x_2$&lt;/li&gt;
&lt;li&gt;the OLS estimator obtained by regressing $y$ on $\tilde x_1$
&lt;ul&gt;
&lt;li&gt;where $\tilde x_1$ is the residual from the regression of $x_1$ on $x_2$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;the OLS estimator obtained by regressing $\tilde y$ on $\tilde x_1$
&lt;ul&gt;
&lt;li&gt;where $\tilde y$ is the residual from the regression of $y$ on $x_2$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;interpretation&#34;&gt;Interpretation&lt;/h3&gt;
&lt;p&gt;What did we actually &lt;strong&gt;learn&lt;/strong&gt;?&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;Frisch-Waugh-Lowell theorem&lt;/strong&gt; is telling us that there are multiple ways to estimate a single regression coefficient. One possibility is to run the full regression of $y$ on $x$, as usual.&lt;/p&gt;
&lt;p&gt;However, we can also regress $x_1$ on $x_2$, take the residuals, and regress $y$ only those residuals. The first part of this process is sometimes referred to as &lt;strong&gt;partialling-out&lt;/strong&gt; (or &lt;em&gt;orthogonalization&lt;/em&gt;, or &lt;em&gt;residualization&lt;/em&gt;) of $x_1$ with respect to $x_2$. The idea is that we are isolating the variation in $x_1$ that is &lt;em&gt;orthogonal&lt;/em&gt; to $x_2$. Note that $x_2$ can be also be multi-dimensional (i.e. include multiple variables and not just one).&lt;/p&gt;
&lt;p&gt;Why would one ever do that?&lt;/p&gt;
&lt;p&gt;This seems like a way more &lt;strong&gt;complicated&lt;/strong&gt; procedure. Instead of simply doing the regression in 1 step, now we need to do 2 or even 3 steps. It&amp;rsquo;s not intuitive at all. The main advantage comes from the fact that we have reduced a multivariate regression to a univariate one, making more tractable and more intuitive.&lt;/p&gt;
&lt;p&gt;We will later explore more in detail three &lt;strong&gt;applications&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;data visualization&lt;/li&gt;
&lt;li&gt;computational speed&lt;/li&gt;
&lt;li&gt;further applications for inference&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;However, let&amp;rsquo;s first explore the theorem more in detail with an example.&lt;/p&gt;
&lt;h2 id=&#34;example&#34;&gt;Example&lt;/h2&gt;
&lt;p&gt;Suppose we were a retail chain, owning many different stores in different locations. We come up with a brilliant &lt;strong&gt;idea to increase sales&lt;/strong&gt;: give away discounts in the form of &lt;strong&gt;coupons&lt;/strong&gt;. We print a lot of coupons and we distribute them around.&lt;/p&gt;
&lt;p&gt;To understand whether our marketing strategy worked, in each store, we check the average daily &lt;code&gt;sales&lt;/code&gt; and which percentage of shoppers used a &lt;code&gt;coupon&lt;/code&gt;. However, there is one &lt;strong&gt;problem&lt;/strong&gt;: we are worried that higher income people are less likely to use the discount, but usually they spend more. To be safe, we also record the average &lt;code&gt;income&lt;/code&gt; in the neighborhood of each store.&lt;/p&gt;
&lt;p&gt;We can represent the data generating process with a &lt;strong&gt;Directed Acyclic Graph&lt;/strong&gt; (DAG). If you are not familiar with DAGs, I have written a short introduction to &lt;a href=&#34;https://medium.com/towards-data-science/controls-b63dc69e3d8c&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Directed Acyclic Graphs here&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-mermaid&#34;&gt;flowchart LR
classDef included fill:#DCDCDC,stroke:#000000,stroke-width:2px;
classDef excluded fill:#ffffff,stroke:#000000,stroke-width:2px;
classDef unobserved fill:#ffffff,stroke:#000000,stroke-width:2px,stroke-dasharray: 5 5;

X1((coupons))
X2((income))
X3((weekday))
Y((sales))


X1 --&amp;gt; Y
X2 --&amp;gt; X1
X2 --&amp;gt; Y
X3 --&amp;gt; Y

class X1,X2,X3,Y excluded;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s load and inspect the &lt;strong&gt;data&lt;/strong&gt;. I import the data generating process from &lt;code&gt;src.dgp&lt;/code&gt; and some plotting functions and libraries from &lt;code&gt;src.utils&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;%matplotlib inline
%config InlineBackend.figure_format = &#39;retina&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from src.utils import *
from src.dgp import dgp_store_coupons

df = dgp_store_coupons().generate_data(N=50)
df.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;sales&lt;/th&gt;
      &lt;th&gt;coupons&lt;/th&gt;
      &lt;th&gt;income&lt;/th&gt;
      &lt;th&gt;dayofweek&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;821.7&lt;/td&gt;
      &lt;td&gt;0.199&lt;/td&gt;
      &lt;td&gt;66.243&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;602.3&lt;/td&gt;
      &lt;td&gt;0.245&lt;/td&gt;
      &lt;td&gt;43.882&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;655.1&lt;/td&gt;
      &lt;td&gt;0.162&lt;/td&gt;
      &lt;td&gt;44.718&lt;/td&gt;
      &lt;td&gt;5&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;625.8&lt;/td&gt;
      &lt;td&gt;0.269&lt;/td&gt;
      &lt;td&gt;39.270&lt;/td&gt;
      &lt;td&gt;4&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;696.6&lt;/td&gt;
      &lt;td&gt;0.186&lt;/td&gt;
      &lt;td&gt;58.654&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;We have information on &lt;strong&gt;50 stores&lt;/strong&gt;, for which we observe the percentage of customers that use &lt;code&gt;coupons&lt;/code&gt;, daily &lt;code&gt;sales&lt;/code&gt; (in thousand $), average &lt;code&gt;income&lt;/code&gt; of the neighborhood (in thousand $), and &lt;code&gt;day of the week&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Suppose we were directly regressing &lt;code&gt;sales&lt;/code&gt; on &lt;code&gt;coupon&lt;/code&gt; usage. What would we get? I represent the &lt;strong&gt;result&lt;/strong&gt; of the regression graphically, using &lt;code&gt;seaborn&lt;/code&gt; &lt;code&gt;regplot&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sns.regplot(x=&amp;quot;coupons&amp;quot;, y=&amp;quot;sales&amp;quot;, data=df, ci=False, line_kws={&#39;color&#39;:&#39;r&#39;, &#39;label&#39;:&#39;linear fit&#39;})
plt.legend()
plt.title(f&amp;quot;Sales and coupon usage&amp;quot;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/fwl_theorem_12_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;It looks like coupons were a &lt;strong&gt;bad idea&lt;/strong&gt;: in stores where coupons are used more, we observe lower sales.&lt;/p&gt;
&lt;p&gt;However, it might just be that people with higher income are using less coupons, while also spending more. If this was true, it could &lt;strong&gt;bias&lt;/strong&gt; our results. In terms of the DAG, it means that we have a &lt;strong&gt;backdoor path&lt;/strong&gt; passing through &lt;code&gt;income&lt;/code&gt;, generating a non-causal relationship.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-mermaid&#34;&gt;flowchart LR
classDef included fill:#DCDCDC,stroke:#000000,stroke-width:2px;
classDef excluded fill:#ffffff,stroke:#000000,stroke-width:2px;
classDef unobserved fill:#ffffff,stroke:#000000,stroke-width:2px,stroke-dasharray: 5 5;

X1((coupons))
X2((income))
X3((weekday))
Y((sales))


X1 --&amp;gt; Y
X2 --&amp;gt; X1
X2 --&amp;gt; Y
X3 --&amp;gt; Y

class X1,Y included;
class X2,X3 excluded;

linkStyle 1,2 stroke:#ff0000,stroke-width:4px;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In order to recover the causal effect of &lt;code&gt;coupons&lt;/code&gt; on &lt;code&gt;sales&lt;/code&gt; we need to &lt;strong&gt;condition&lt;/strong&gt; our analysis on &lt;code&gt;income&lt;/code&gt;. This will &lt;strong&gt;block&lt;/strong&gt; the non-causal path passing through &lt;code&gt;income&lt;/code&gt;, leaving only the direct path from &lt;code&gt;coupons&lt;/code&gt; to &lt;code&gt;sales&lt;/code&gt; open, allowing us to estimate the causal effect.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-mermaid&#34;&gt;flowchart LR
classDef included fill:#DCDCDC,stroke:#000000,stroke-width:2px;
classDef excluded fill:#ffffff,stroke:#000000,stroke-width:2px;
classDef unobserved fill:#ffffff,stroke:#000000,stroke-width:2px,stroke-dasharray: 5 5;


X1((coupons))
X2((income))
X3((weekday))
Y((sales))


X1 --&amp;gt; Y
X2 -.-&amp;gt; X1
X2 -.-&amp;gt; Y
X3 --&amp;gt; Y

class X1,X2,Y included;
class X3 excluded;

linkStyle 0 stroke:#00ff00,stroke-width:4px;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s implement this, by including &lt;code&gt;income&lt;/code&gt; in the regression.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;smf.ols(&#39;sales ~ coupons + income&#39;, df).fit().summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
      &lt;td&gt;&lt;/td&gt;         &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt; &lt;td&gt;  161.4982&lt;/td&gt; &lt;td&gt;   33.253&lt;/td&gt; &lt;td&gt;    4.857&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;   94.601&lt;/td&gt; &lt;td&gt;  228.395&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;coupons&lt;/th&gt;   &lt;td&gt;  218.7548&lt;/td&gt; &lt;td&gt;   50.058&lt;/td&gt; &lt;td&gt;    4.370&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;  118.052&lt;/td&gt; &lt;td&gt;  319.458&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;income&lt;/th&gt;    &lt;td&gt;    9.5094&lt;/td&gt; &lt;td&gt;    0.480&lt;/td&gt; &lt;td&gt;   19.818&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    8.544&lt;/td&gt; &lt;td&gt;   10.475&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;Now the estimated effect of &lt;code&gt;coupons&lt;/code&gt; on &lt;code&gt;sales&lt;/code&gt; is positive and significant. Coupons were actually a &lt;strong&gt;good idea&lt;/strong&gt; after all.&lt;/p&gt;
&lt;h3 id=&#34;verifying-the-theorem&#34;&gt;Verifying the Theorem&lt;/h3&gt;
&lt;p&gt;Let&amp;rsquo;s now verify that the Frisch-Waugh-Lowell theorem actually holds. In particular, we want to check whether we get the &lt;strong&gt;same coefficient&lt;/strong&gt; if, instead of regressing &lt;code&gt;sales&lt;/code&gt; on &lt;code&gt;coupons&lt;/code&gt; and &lt;code&gt;income&lt;/code&gt;, we were&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;regressing &lt;code&gt;coupons&lt;/code&gt; on &lt;code&gt;income&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;computing the residuals &lt;code&gt;coupons_tilde&lt;/code&gt;, i.e. the variation in &lt;code&gt;coupons&lt;/code&gt; &lt;strong&gt;not&lt;/strong&gt; explained by &lt;code&gt;income&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;regressing &lt;code&gt;sales&lt;/code&gt; on &lt;code&gt;coupons_tilde&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df[&#39;coupons_tilde&#39;] = smf.ols(&#39;coupons ~ income&#39;, df).fit().resid

smf.ols(&#39;sales ~ coupons_tilde - 1&#39;, df).fit().summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
        &lt;td&gt;&lt;/td&gt;           &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;coupons_tilde&lt;/th&gt; &lt;td&gt;  218.7548&lt;/td&gt; &lt;td&gt; 1275.236&lt;/td&gt; &lt;td&gt;    0.172&lt;/td&gt; &lt;td&gt; 0.865&lt;/td&gt; &lt;td&gt;-2343.929&lt;/td&gt; &lt;td&gt; 2781.438&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;Yes, the coefficient is the same! However, the &lt;strong&gt;standard errors&lt;/strong&gt; now have increased a lot and the estimated coefficient is not significantly different from zero anymore.&lt;/p&gt;
&lt;p&gt;A better approach is to add a further step and repeat the same procedure also for &lt;code&gt;sales&lt;/code&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;regressing &lt;code&gt;sales&lt;/code&gt; on &lt;code&gt;income&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;computing the residuals &lt;code&gt;sales_tilde&lt;/code&gt;, i.e. the variation in &lt;code&gt;sales&lt;/code&gt; &lt;strong&gt;not&lt;/strong&gt; explained by &lt;code&gt;income&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;and finally regress &lt;code&gt;sales_tilde&lt;/code&gt; on &lt;code&gt;coupons_tilde&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df[&#39;sales_tilde&#39;] = smf.ols(&#39;sales ~ income&#39;, df).fit().resid

smf.ols(&#39;sales_tilde ~ coupons_tilde - 1&#39;, df).fit().summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
        &lt;td&gt;&lt;/td&gt;           &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;coupons_tilde&lt;/th&gt; &lt;td&gt;  218.7548&lt;/td&gt; &lt;td&gt;   49.025&lt;/td&gt; &lt;td&gt;    4.462&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;  120.235&lt;/td&gt; &lt;td&gt;  317.275&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;The coefficient is still exactly the same, but now also the standard errors are almost identical.&lt;/p&gt;
&lt;h3 id=&#34;projection&#34;&gt;Projection&lt;/h3&gt;
&lt;p&gt;What is &lt;strong&gt;partialling-out&lt;/strong&gt; (or residualization, or orthogonalization) actually doing? What is happening when we take the residuals of &lt;code&gt;coupons&lt;/code&gt; with respect to &lt;code&gt;income&lt;/code&gt;?&lt;/p&gt;
&lt;p&gt;We can &lt;strong&gt;visualize&lt;/strong&gt; the procedure in a plot. First, let&amp;rsquo;s actually display the &lt;strong&gt;residuals&lt;/strong&gt; of &lt;code&gt;coupons&lt;/code&gt; with respect to income.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df[&amp;quot;coupons_hat&amp;quot;] = smf.ols(&#39;coupons ~ income&#39;, df).fit().predict()
ax = sns.regplot(x=&amp;quot;income&amp;quot;, y=&amp;quot;coupons&amp;quot;, data=df, ci=False, line_kws={&#39;color&#39;:&#39;r&#39;, &#39;label&#39;:&#39;linear fit&#39;})
ax.vlines(df[&amp;quot;income&amp;quot;], np.minimum(df[&amp;quot;coupons&amp;quot;], df[&amp;quot;coupons_hat&amp;quot;]), np.maximum(df[&amp;quot;coupons&amp;quot;], df[&amp;quot;coupons_hat&amp;quot;]), 
           linestyle=&#39;--&#39;, color=&#39;k&#39;, alpha=0.5, linewidth=1, label=&amp;quot;residuals&amp;quot;);
plt.legend()
plt.title(f&amp;quot;Coupons usage, income and residuals&amp;quot;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/fwl_theorem_24_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;residuals&lt;/strong&gt; are the vertical dotted lines between the data and the linear fit, i.e. the part of the variation in &lt;code&gt;coupons&lt;/code&gt; unexplained by &lt;code&gt;income&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;By &lt;strong&gt;partialling-out&lt;/strong&gt;, we are removing the linear fit from the data and keeping only the residuals. We can visualize this procedure with a gif. I import the code from the &lt;code&gt;src.figures&lt;/code&gt; file that you can find &lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/src/figures.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from src.figures import gif_projection

gif_projection(x=&#39;income&#39;, y=&#39;coupons&#39;, df=df, gifname=&amp;quot;gifs/fwl.gif&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;gifs/fwl.gif&#34; alt=&#34;fwl&#34;&gt;&lt;/p&gt;
&lt;p&gt;The original distribution of the data is on the left in &lt;em&gt;blue&lt;/em&gt;, the partialled-out data in on the right in &lt;em&gt;green&lt;/em&gt;. As we can see, partialling-out removes both the level and the trend in &lt;code&gt;coupons&lt;/code&gt; that is explained by &lt;code&gt;income&lt;/code&gt;.&lt;/p&gt;
&lt;h3 id=&#34;multiple-controls&#34;&gt;Multiple Controls&lt;/h3&gt;
&lt;p&gt;We can use the Frisch-Waugh-Theorem also when we have &lt;strong&gt;multiple control variables&lt;/strong&gt;. Suppose that we wanted to also include &lt;code&gt;day of the week&lt;/code&gt; in the regression, to increase precision.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;smf.ols(&#39;sales ~ coupons + income + dayofweek&#39;, df).fit().summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
         &lt;td&gt;&lt;/td&gt;           &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt;      &lt;td&gt;  124.2721&lt;/td&gt; &lt;td&gt;   28.764&lt;/td&gt; &lt;td&gt;    4.320&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;   66.182&lt;/td&gt; &lt;td&gt;  182.362&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;dayofweek[T.2]&lt;/th&gt; &lt;td&gt;    7.7703&lt;/td&gt; &lt;td&gt;   14.607&lt;/td&gt; &lt;td&gt;    0.532&lt;/td&gt; &lt;td&gt; 0.598&lt;/td&gt; &lt;td&gt;  -21.729&lt;/td&gt; &lt;td&gt;   37.270&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;dayofweek[T.3]&lt;/th&gt; &lt;td&gt;   15.0895&lt;/td&gt; &lt;td&gt;   11.678&lt;/td&gt; &lt;td&gt;    1.292&lt;/td&gt; &lt;td&gt; 0.204&lt;/td&gt; &lt;td&gt;   -8.495&lt;/td&gt; &lt;td&gt;   38.674&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;dayofweek[T.4]&lt;/th&gt; &lt;td&gt;   28.2762&lt;/td&gt; &lt;td&gt;    9.868&lt;/td&gt; &lt;td&gt;    2.866&lt;/td&gt; &lt;td&gt; 0.007&lt;/td&gt; &lt;td&gt;    8.348&lt;/td&gt; &lt;td&gt;   48.204&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;dayofweek[T.5]&lt;/th&gt; &lt;td&gt;   44.0937&lt;/td&gt; &lt;td&gt;   10.214&lt;/td&gt; &lt;td&gt;    4.317&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;   23.467&lt;/td&gt; &lt;td&gt;   64.720&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;dayofweek[T.6]&lt;/th&gt; &lt;td&gt;   50.7664&lt;/td&gt; &lt;td&gt;   13.130&lt;/td&gt; &lt;td&gt;    3.866&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;   24.249&lt;/td&gt; &lt;td&gt;   77.283&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;dayofweek[T.7]&lt;/th&gt; &lt;td&gt;   57.3142&lt;/td&gt; &lt;td&gt;   12.413&lt;/td&gt; &lt;td&gt;    4.617&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;   32.245&lt;/td&gt; &lt;td&gt;   82.383&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;coupons&lt;/th&gt;        &lt;td&gt;  192.0262&lt;/td&gt; &lt;td&gt;   39.140&lt;/td&gt; &lt;td&gt;    4.906&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;  112.981&lt;/td&gt; &lt;td&gt;  271.071&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;income&lt;/th&gt;         &lt;td&gt;    9.8152&lt;/td&gt; &lt;td&gt;    0.404&lt;/td&gt; &lt;td&gt;   24.314&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    9.000&lt;/td&gt; &lt;td&gt;   10.630&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;We can perform the same procedure as before, but instead of &lt;strong&gt;partialling-out&lt;/strong&gt; only &lt;code&gt;income&lt;/code&gt;, now we partial out both &lt;code&gt;income&lt;/code&gt; and &lt;code&gt;day of the week&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df[&#39;coupons_tilde&#39;] = smf.ols(&#39;coupons ~ income + dayofweek&#39;, df).fit().resid
df[&#39;sales_tilde&#39;] = smf.ols(&#39;sales ~ income + dayofweek&#39;, df).fit().resid
smf.ols(&#39;sales_tilde ~ coupons_tilde - 1&#39;, df).fit().summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
        &lt;td&gt;&lt;/td&gt;           &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;coupons_tilde&lt;/th&gt; &lt;td&gt;  192.0262&lt;/td&gt; &lt;td&gt;   35.803&lt;/td&gt; &lt;td&gt;    5.363&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;  120.078&lt;/td&gt; &lt;td&gt;  263.974&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;We still get exactly the same coefficient!&lt;/p&gt;
&lt;h2 id=&#34;applications&#34;&gt;Applications&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s now inspect some useful applications of the FWL theorem.&lt;/p&gt;
&lt;h3 id=&#34;data-visualization&#34;&gt;Data Visualization&lt;/h3&gt;
&lt;p&gt;One of the advantages of the Frisch-Waugh-Theorem is that it allows us to estimate the coefficient of interest from a &lt;strong&gt;univariate&lt;/strong&gt; regression, i.e. with a single explanatory variable (or feature).&lt;/p&gt;
&lt;p&gt;Therefore, we can now represent the relationship of interest &lt;strong&gt;graphically&lt;/strong&gt;. Let&amp;rsquo;s plot the residual &lt;code&gt;sales&lt;/code&gt; against the residual &lt;code&gt;coupons&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sns.regplot(x=&amp;quot;coupons_tilde&amp;quot;, y=&amp;quot;sales_tilde&amp;quot;, data=df, ci=False, line_kws={&#39;color&#39;:&#39;r&#39;, &#39;label&#39;:&#39;linear fit&#39;})
plt.legend()
plt.title(f&amp;quot;Residual sales and residual coupons&amp;quot;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/fwl_theorem_39_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Now it&amp;rsquo;s evident from the graph that the &lt;strong&gt;conditional relationship&lt;/strong&gt; between &lt;code&gt;sales&lt;/code&gt; and &lt;code&gt;coupons&lt;/code&gt; is positive.&lt;/p&gt;
&lt;p&gt;One problem with this approach is that the variables are &lt;strong&gt;hard to interpret&lt;/strong&gt;: we now have negative values for both &lt;code&gt;sales&lt;/code&gt; and &lt;code&gt;coupons&lt;/code&gt;. Weird.&lt;/p&gt;
&lt;p&gt;Why did it happen? It happened because when we partialled-out the variables, we included the &lt;strong&gt;intercept&lt;/strong&gt; in the regression, effectively de-meaning the variables (i.e. normalizing their values so that their mean is zero).&lt;/p&gt;
&lt;p&gt;We can &lt;strong&gt;solve&lt;/strong&gt; this problem by &lt;strong&gt;scaling&lt;/strong&gt; both variables, adding their mean.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df[&#39;coupons_tilde_scaled&#39;] = df[&#39;coupons_tilde&#39;] + np.mean(df[&#39;coupons&#39;])
df[&#39;sales_tilde_scaled&#39;] = df[&#39;sales_tilde&#39;] + np.mean(df[&#39;sales&#39;])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now the magnitudes of the two variables are interpretable again.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sns.regplot(x=&amp;quot;coupons_tilde_scaled&amp;quot;, y=&amp;quot;sales_tilde_scaled&amp;quot;, data=df, ci=False, line_kws={&#39;color&#39;:&#39;r&#39;, &#39;label&#39;:&#39;linear fit&#39;})
plt.legend()
plt.title(f&amp;quot;Residual sales scaled and residual coupons scaled&amp;quot;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/fwl_theorem_43_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Is this a &lt;strong&gt;valid&lt;/strong&gt; approach or did it alter our estimates? We can can check it by running the regression with the scaled partialled-out variables.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;smf.ols(&#39;sales_tilde_scaled ~ coupons_tilde_scaled&#39;, df).fit().summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
            &lt;td&gt;&lt;/td&gt;              &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt;            &lt;td&gt;  641.6486&lt;/td&gt; &lt;td&gt;   10.017&lt;/td&gt; &lt;td&gt;   64.054&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;  621.507&lt;/td&gt; &lt;td&gt;  661.790&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;coupons_tilde_scaled&lt;/th&gt; &lt;td&gt;  192.0262&lt;/td&gt; &lt;td&gt;   36.174&lt;/td&gt; &lt;td&gt;    5.308&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;  119.294&lt;/td&gt; &lt;td&gt;  264.758&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;The coefficient is exactly the same as before!&lt;/p&gt;
&lt;h3 id=&#34;computational-speed&#34;&gt;Computational Speed&lt;/h3&gt;
&lt;p&gt;Another application of the Frisch-Waugh-Lovell theorem is to increase the computational speed of linear estimators. For example it is used to compute efficient linear estimators in presence of high-dimensional fixed effects (&lt;code&gt;day of the week&lt;/code&gt; in our example).&lt;/p&gt;
&lt;p&gt;Some packages that exploit the Frisch-Waugh-Lovell theorem include&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://scorreia.com/software/reghdfe/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;reghdfe in Stata&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://pyhdfe.readthedocs.io/en/stable/index.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;pyhdfe in Python&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;However, it&amp;rsquo;s important to also mention the &lt;a href=&#34;https://cran.r-project.org/web/packages/fixest/index.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;fixest&lt;/a&gt; package in R, which is also exceptionally efficient in running regressions with high dimensional fixed effects.&lt;/p&gt;
&lt;h3 id=&#34;inference-and-machine-learning&#34;&gt;Inference and Machine Learning&lt;/h3&gt;
&lt;p&gt;Another important application of the FWL theorem sits at the intersection of &lt;strong&gt;machine learning&lt;/strong&gt; and &lt;strong&gt;causal inference&lt;/strong&gt;. I am referring to the work on post-double selection by &lt;a href=&#34;https://academic.oup.com/restud/article-abstract/81/2/608/1523757&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Belloni, Chernozhukov, Hansen (2013)&lt;/a&gt; and the follow up work on &amp;ldquo;double machine learning&amp;rdquo; by &lt;a href=&#34;https://academic.oup.com/ectj/article/21/1/C1/5056401&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Chernozhukov, Chetverikov, Demirer, Duflo, Hansen, Newey, Robins (2018)&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;I plan to cover both applications in future posts, but I wanted to start with the basics. Stay tuned!&lt;/p&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;p&gt;[1] R. Frisch and F. V. Waugh, &lt;a href=&#34;https://www.jstor.org/stable/1907330&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Partial Time Regressions as Compared with Individual Trends&lt;/a&gt; (1933), &lt;em&gt;Econometrica&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;[2] M. C. Lowell, &lt;a href=&#34;https://www.tandfonline.com/doi/abs/10.1080/01621459.1963.10480682&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Seasonal Adjustment of Economic Time Series and Multiple Regression Analysis&lt;/a&gt; (1963), &lt;em&gt;Journal of the American Statistical Association&lt;/em&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Understanding AIPW</title>
      <link>https://matteocourthoud.github.io/post/aipw/</link>
      <pubDate>Mon, 10 Jul 2023 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/post/aipw/</guid>
      <description>&lt;p&gt;&lt;em&gt;An introduction to doubly-robust estimation of conditional average treatment effects (CATE)&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;When estimating causal effects, the gold standard is &lt;strong&gt;randomized controlled trials or AB tests&lt;/strong&gt;. By randomly exposing units to a treatment we make sure that individuals in both groups are comparable, on average, and any difference we observe can be attributed to the treatment effect alone.&lt;/p&gt;
&lt;p&gt;However, often the treatment and control groups are &lt;strong&gt;not perfectly comparable&lt;/strong&gt;. This could be due to the fact that randomization was not perfect or available. Not always we can randomize a treatment, for ethical or practical reasons. And even when we can, sometimes we do not have enough individuals or units so that differences between groups are seizable. This happens often, for example, when randomization is not done at the individual level, but at a higher level of aggregation, for example zipcodes, counties or even states.&lt;/p&gt;
&lt;p&gt;In a &lt;a href=&#34;https://towardsdatascience.com/99bf5cffa0d9&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;previous post&lt;/a&gt;, I have introduced and compared a series of methods that compute &lt;strong&gt;conditional average treatment effects (CATE)&lt;/strong&gt; from observational or experimental data. Some of these methods require the researcher to specify and estimate the distribution of the outcome of interest, given the treatment and the observable characteristics (e.g. &lt;a href=&#34;https://towardsdatascience.com/8a9c1e340832&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;meta learners&lt;/a&gt;). Other methods require the researcher to specify and estimate the probability of being treated, given the observable characteristics (e.g. &lt;a href=&#34;https://towardsdatascience.com/99bf5cffa0d9&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;IPW&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;In this post, we are going to see a procedure that &lt;strong&gt;combines&lt;/strong&gt; both methods and is &lt;strong&gt;robust&lt;/strong&gt; to misspecification of either method&amp;rsquo;s model: the Augmented Inverse Probability Weighted estimator (AIPW).&lt;/p&gt;
&lt;img src=&#34;fig/fusion.gif&#34; width=&#34;400px&#34;/&gt;
&lt;p&gt;&lt;strong&gt;TLDR;&lt;/strong&gt; AIPW greatly improves both IPW and meta-learners, and you should always use it!&lt;/p&gt;
&lt;h2 id=&#34;example&#34;&gt;Example&lt;/h2&gt;
&lt;p&gt;Assume we had blog on statistics and causal inference 😇. To improve user experience, we are considering &lt;strong&gt;releasing a dark mode&lt;/strong&gt;, and we would like to understand whether this new feature increases the time users spend on our blog.&lt;/p&gt;
&lt;p&gt;This example is borrowed from my last post on the estimation of conditional average treatment effects (CATE). You can find the &lt;a href=&#34;https://towardsdatascience.com/99bf5cffa0d9&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;original post here&lt;/a&gt;. If you remember the setting, you can skip this introduction.&lt;/p&gt;
&lt;img src=&#34;fig/modes.png&#34; width=&#34;600px&#34;/&gt;
&lt;p&gt;We are not a sophisticated company, therefore we do not run an AB test but we simply release the dark mode and we observe whether users select it or not and the time they spend on the blog. We know that there might be &lt;strong&gt;selection&lt;/strong&gt;:  users that prefer the dark mode could have different reading preferences and this might complicate our causal analysis.&lt;/p&gt;
&lt;p&gt;We can represent the data generating process with the following &lt;a href=&#34;https://towardsdatascience.com/b63dc69e3d8c&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Directed Acyclic Graph (DAG)&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-mermaid&#34;&gt;flowchart TB
classDef included fill:#DCDCDC,stroke:#000000,stroke-width:2px;
classDef excluded fill:#ffffff,stroke:#000000,stroke-width:2px;
classDef unobserved fill:#ffffff,stroke:#000000,stroke-width:2px,stroke-dasharray: 5 5;

X1((gender))
X2((age))
X3((hours))
D((dark mode))
Y((read time))

D --&amp;gt; Y
X1 --&amp;gt; Y
X1 --&amp;gt; D
X2 --&amp;gt; D
X3 --&amp;gt; Y

class D,Y included;
class X1,X2,X3 excluded;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We generate the simulated data using the data generating process &lt;code&gt;dgp_darkmode()&lt;/code&gt; from &lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/src/dgp.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;src.dgp&lt;/code&gt;&lt;/a&gt;. I also import some plotting functions and libraries from &lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/src/utils.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;src.utils&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;%matplotlib inline
%config InlineBackend.figure_format = &#39;retina&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from src.utils import *
from src.dgp import dgp_darkmode
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;dgp = dgp_darkmode()
df = dgp.generate_data()
df.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;read_time&lt;/th&gt;
      &lt;th&gt;dark_mode&lt;/th&gt;
      &lt;th&gt;male&lt;/th&gt;
      &lt;th&gt;age&lt;/th&gt;
      &lt;th&gt;hours&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;14.4&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;43.0&lt;/td&gt;
      &lt;td&gt;65.6&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;15.4&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;55.0&lt;/td&gt;
      &lt;td&gt;125.4&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;20.9&lt;/td&gt;
      &lt;td&gt;True&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;23.0&lt;/td&gt;
      &lt;td&gt;642.6&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;20.0&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;41.0&lt;/td&gt;
      &lt;td&gt;129.1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;21.5&lt;/td&gt;
      &lt;td&gt;True&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;29.0&lt;/td&gt;
      &lt;td&gt;190.2&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;We have informations on 300 users for whom we observe whether they select the &lt;code&gt;dark_mode&lt;/code&gt; (the treatment), their weekly &lt;code&gt;read_time&lt;/code&gt; (the outcome of interest) and some characteristics like &lt;code&gt;gender&lt;/code&gt;, &lt;code&gt;age&lt;/code&gt; and total &lt;code&gt;hours&lt;/code&gt; previously spend on the blog.&lt;/p&gt;
&lt;p&gt;We would like to estimate the effect of the new &lt;code&gt;dark_mode&lt;/code&gt; on users&amp;rsquo; &lt;code&gt;read_time&lt;/code&gt;. As a first approach, we might naively compute the effect as a difference in means, assuming that the treatment and control sample are comparable. We can estimate the difference in means by regressing &lt;code&gt;read_time&lt;/code&gt; on &lt;code&gt;dark_mode&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;smf.ols(&amp;quot;read_time ~ dark_mode&amp;quot;, data=df).fit().summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
          &lt;td&gt;&lt;/td&gt;             &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt;         &lt;td&gt;   19.1748&lt;/td&gt; &lt;td&gt;    0.402&lt;/td&gt; &lt;td&gt;   47.661&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;   18.383&lt;/td&gt; &lt;td&gt;   19.967&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;dark_mode[T.True]&lt;/th&gt; &lt;td&gt;   -0.4446&lt;/td&gt; &lt;td&gt;    0.571&lt;/td&gt; &lt;td&gt;   -0.779&lt;/td&gt; &lt;td&gt; 0.437&lt;/td&gt; &lt;td&gt;   -1.568&lt;/td&gt; &lt;td&gt;    0.679&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;Individuals that select the &lt;code&gt;dark_mode&lt;/code&gt; spend on average 0.44 hours less on the blog, per week. Should we conclude that &lt;code&gt;dark_mode&lt;/code&gt; is a &lt;strong&gt;bad idea&lt;/strong&gt;? Is this a causal effect?&lt;/p&gt;
&lt;p&gt;The problem is that we did &lt;strong&gt;not&lt;/strong&gt; run an &lt;a href=&#34;https://de.wikipedia.org/wiki/A/B-Test&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;AB test&lt;/strong&gt;&lt;/a&gt; or randomized control trial, therefore users that selected the &lt;code&gt;dark_mode&lt;/code&gt; might not be directly &lt;strong&gt;comparable&lt;/strong&gt; with users that didn&amp;rsquo;t. Can we verify this concern? Partially. We can only check characteristics that we observe, &lt;code&gt;gender&lt;/code&gt;, &lt;code&gt;age&lt;/code&gt; and total &lt;code&gt;hours&lt;/code&gt; in our setting. We cannot check if users differ along other dimensions that we don&amp;rsquo;t observe.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s use the &lt;code&gt;create_table_one&lt;/code&gt; function from Uber&amp;rsquo;s &lt;a href=&#34;https://causalml.readthedocs.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;causalml&lt;/code&gt;&lt;/a&gt; package to produce a &lt;strong&gt;covariate balance table&lt;/strong&gt;, containing the average value of our observable characteristics, across treatment and control groups. As the name suggests, this should always be the first table you present in causal inference analysis.&lt;/p&gt;
&lt;p&gt;We did not randomize the &lt;code&gt;dark_mode&lt;/code&gt; so that users that selected it might not be directly &lt;strong&gt;comparable&lt;/strong&gt; with users that didn&amp;rsquo;t. Can we verify this concern? Partially. We can only check characteristics that we observe, &lt;code&gt;gender&lt;/code&gt;, &lt;code&gt;age&lt;/code&gt; and total &lt;code&gt;hours&lt;/code&gt; in our setting. We cannot check if users differ along other dimensions that we don&amp;rsquo;t observe.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s use the &lt;code&gt;create_table_one&lt;/code&gt; function from Uber&amp;rsquo;s &lt;a href=&#34;https://causalml.readthedocs.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;causalml&lt;/code&gt;&lt;/a&gt; package to produce a &lt;strong&gt;covariate balance table&lt;/strong&gt;, containing the average value of our observable characteristics, across treatment and control groups. As the name suggests, this should always be the first table you present in causal inference analysis.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from causalml.match import create_table_one

X = [&#39;male&#39;, &#39;age&#39;, &#39;hours&#39;]
table1 = create_table_one(df, &#39;dark_mode&#39;, X)
table1
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;Control&lt;/th&gt;
      &lt;th&gt;Treatment&lt;/th&gt;
      &lt;th&gt;SMD&lt;/th&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;Variable&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;n&lt;/th&gt;
      &lt;td&gt;151&lt;/td&gt;
      &lt;td&gt;149&lt;/td&gt;
      &lt;td&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;age&lt;/th&gt;
      &lt;td&gt;46.01 (9.79)&lt;/td&gt;
      &lt;td&gt;39.09 (11.53)&lt;/td&gt;
      &lt;td&gt;-0.6469&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;hours&lt;/th&gt;
      &lt;td&gt;337.78 (464.00)&lt;/td&gt;
      &lt;td&gt;328.57 (442.12)&lt;/td&gt;
      &lt;td&gt;-0.0203&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;male&lt;/th&gt;
      &lt;td&gt;0.34 (0.47)&lt;/td&gt;
      &lt;td&gt;0.66 (0.48)&lt;/td&gt;
      &lt;td&gt;0.6732&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;There seems to be &lt;strong&gt;some difference&lt;/strong&gt; between treatment (&lt;code&gt;dark_mode&lt;/code&gt;) and control group. In particular, users that select the &lt;code&gt;dark_mode&lt;/code&gt; are older, have spent less hours on the blog and they are more likely to be males.&lt;/p&gt;
&lt;p&gt;What can we do? If we assume that all differences between treatment and control group are &lt;strong&gt;observable&lt;/strong&gt;, we can solve the problem by performing &lt;strong&gt;conditional analysis&lt;/strong&gt;.&lt;/p&gt;
&lt;h2 id=&#34;conditional-analysis&#34;&gt;Conditional Analysis&lt;/h2&gt;
&lt;p&gt;We assume that for a set of subjects $i = 1, &amp;hellip;, n$ we observed a set of variables $(D_i, Y_i, X_i)$ that includes&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;a treatment assignment $D_i \in \lbrace 0, 1 \rbrace$ (&lt;code&gt;dark_mode&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;a response $Y_i \in \mathbb R$ (&lt;code&gt;read_time&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;a feature vector $X_i \in \mathbb R^n$ (&lt;code&gt;gender&lt;/code&gt;, &lt;code&gt;age&lt;/code&gt; and &lt;code&gt;hours&lt;/code&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We are interested in &lt;strong&gt;estimating the conditional average treatment effect (CATE)&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;$$
\tau(x) = \mathbb E \Big[ Y_i^{(1)} - Y_i^{(0)} \ \Big| \ X_i = x \Big]
$$&lt;/p&gt;
&lt;p&gt;Where $Y_i^{(d)}$ indicates the potential outcome of individual $i$ under treatment status $d$. We also make the following assumptions.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Assumption 1 : unconfoundedness&lt;/strong&gt; (or ignorability, or selection on observables)&lt;/p&gt;
&lt;p&gt;$$
\big \lbrace Y_i^{(1)} , Y_i^{(0)} \big \rbrace \ \perp \ D_i \ | \ X_i
$$&lt;/p&gt;
&lt;p&gt;i.e. conditional on observable characteristics $X$, the treatment assignment $D$ is as good as random. What we are effectively assuming is that there is no other characteristics that we do not observe that could impact both whether a user selects the &lt;code&gt;dark_mode&lt;/code&gt; and their &lt;code&gt;read_time&lt;/code&gt;. This is a &lt;strong&gt;strong assumption&lt;/strong&gt; that is more likely to be satisfied the more individual characteristics we observe.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Assumption 2: overlap&lt;/strong&gt; (or common support)&lt;/p&gt;
&lt;p&gt;$$
\exists \eta &amp;gt; 0 \ : \ \eta \leq \mathbb E \left[ D_i = 1 \ \big | \ X_i = x \right] \leq 1-\eta
$$&lt;/p&gt;
&lt;p&gt;i.e. no observation is deterministically assigned to the treatment or control group. This is a more technical assumption that basically means that for any level of &lt;code&gt;gender&lt;/code&gt;, &lt;code&gt;age&lt;/code&gt; or &lt;code&gt;hours&lt;/code&gt;, there could exist an individual that select the &lt;code&gt;dark_mode&lt;/code&gt; and one that doesn&amp;rsquo;t. Differently from the unconfoundedness assumption, the overal assumption is &lt;strong&gt;testable&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Assumption 3: stable unit treatment value (SUTVA)&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;$$
Y^{(d)} \perp D
$$&lt;/p&gt;
&lt;p&gt;i.e. the potential outcome does not depend on the treatment status. In our case, we are ruling out the fact that another user selecting &lt;code&gt;dark_mode&lt;/code&gt; might affect my effect of &lt;code&gt;dark_mode&lt;/code&gt; on &lt;code&gt;read_time&lt;/code&gt;. The most common setting where SUTVA is violated is in presence of &lt;strong&gt;network effects&lt;/strong&gt;: if a friend of mine uses a social network increases my utility from using it.&lt;/p&gt;
&lt;h3 id=&#34;ipw-and-meta-learners&#34;&gt;IPW and Meta-Learners&lt;/h3&gt;
&lt;p&gt;Two alternative ways to perform conditional analysis are&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/99bf5cffa0d9&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;IPW&lt;/strong&gt;&lt;/a&gt;: balance observations by their conditional treatment assignment probability and then estimate the treatment effect as a weighted difference in means&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/8a9c1e340832&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Meta Learners&lt;/strong&gt;&lt;/a&gt;: predict the potential outcomes from observable characteristics and estimate treatment effects as the difference between observed and counterfactual outcomes&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;These two alternative procedures exploit the fact that we observe individual characteristics $X$ in different ways:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;IPW exploits $X$ to predict the treatment assignment $D$ and estimate the &lt;strong&gt;propensity scores&lt;/strong&gt; $e(X) = \mathbb{E} [D | X]$&lt;/li&gt;
&lt;li&gt;Meta Learners exploit $X$ to predict the counterfactual outcomes $Y^{(d)}$ and estimate the &lt;strong&gt;response function&lt;/strong&gt; $\mu(X)^{(d)} = \mathbb{E} [Y | D, X]$&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Can we &lt;strong&gt;combine&lt;/strong&gt; the two procedures and get the best of both worlds?&lt;/p&gt;
&lt;p&gt;Yes, with the &lt;strong&gt;AIPW or double-robust estimator&lt;/strong&gt;.&lt;/p&gt;
&lt;h2 id=&#34;the-aipw-estimator&#34;&gt;The AIPW Estimator&lt;/h2&gt;
&lt;p&gt;The &lt;strong&gt;Augmented Inverse Propensity Weighted&lt;/strong&gt; estimator is given by&lt;/p&gt;
&lt;p&gt;$$
\hat \tau_{AIPW} = \frac{1}{n} \sum_{i=1}^{n} \left( \hat \mu^{(1)}(X_i) - \hat \mu^{(0)}(X_i) + \frac{D_i }{\hat e(X_i)} \left( Y_i - \hat \mu^{(1)}(X_i) \right) - \frac{(1-D_i) }{1-\hat e(X_i)} \left( Y_i - \hat \mu^{(0)}(X_i) \right) \right)
$$&lt;/p&gt;
&lt;p&gt;where $\mu^{(d)}(x)$ is the &lt;strong&gt;response function&lt;/strong&gt;, i.e. the expected value of the outcome, conditional on observable characteristics $x$ and treatment status $d$, and $e(X)$ is the &lt;strong&gt;propensity score&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;$$
\mu^{(d)}(x) = \mathbb E \left[ Y_i \ \big | \ X_i = x, D_i = d \right] \qquad ; \qquad e(x) = \mathbb E \left[ D_i = 1 \ \big | \ X_i = x \right]
$$&lt;/p&gt;
&lt;p&gt;The formula of the AIPW estimator seems very cryptic at first, so let&amp;rsquo;s dig deeper and try to understand it.&lt;/p&gt;
&lt;h3 id=&#34;decomposition&#34;&gt;Decomposition&lt;/h3&gt;
&lt;p&gt;The best way to understand the AIPW formula is to &lt;strong&gt;decompose&lt;/strong&gt; it into two parts.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;first way&lt;/strong&gt; is to decompose the AIPW estimator into a &lt;a href=&#34;https://towardsdatascience.com/8a9c1e340832&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;S-learner estimator&lt;/strong&gt;&lt;/a&gt; and an adjustment factor.&lt;/p&gt;
&lt;p&gt;$$
\hat \tau_{AIPW} = \hat \tau_{S-learn} + \widehat{\text{adj}}_{S-learn}
$$&lt;/p&gt;
&lt;p&gt;where&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
\hat \tau_{S-learn} =&amp;amp; \frac{1}{n} \sum_{i=1}^{n} \left( \hat \mu^{(1)}(X_i) - \hat \mu^{(0)}(X_i) \right) \newline
\widehat {adj} _ {S-learn} =&amp;amp; \frac{1}{n} \sum_{i=1}^{n} \left(\frac{D_i }{\hat e(X_i)} \left( Y_i - \hat \mu^{(1)}(X_i) \right) - \frac{(1-D_i) }{1-\hat e(X_i)} \left( Y_i - \hat \mu^{(0)}(X_i) \right) \right)
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;The adjustment is essentially an IPW estimator performed on the &lt;strong&gt;residuals&lt;/strong&gt; of the S-learner.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;second way&lt;/strong&gt; to decompose the AIPW estimator into the &lt;strong&gt;IPW estimator&lt;/strong&gt; and an adjustment factor.&lt;/p&gt;
&lt;p&gt;$$
\hat \tau_{AIPW} = \hat \tau_{IPW} + \widehat{\text{adj}}_{IPW}
$$&lt;/p&gt;
&lt;p&gt;where&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
\hat \tau_{IPW} &amp;amp;= \frac{1}{n} \sum _ {i=1}^{n} \left( \frac{D_i Y_i}{\hat e(X_i)} - \frac{(1-D_i) Y_i}{1-\hat e(X_i)} \right) \newline
\widehat {adj} _ {IPW} &amp;amp;= \frac{1}{n} \sum_{i=1}^{n} \left( \frac{\hat e(X_i) - D_i}{\hat e(X_i)} \hat \mu^{(1)}(X_i) - \frac{(1-\hat e(X_i)) - (1-D_i)}{1-\hat e(X_i)} \hat \mu^{(0)}(X_i) \right)
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;The adjustment is essentially an S-learner estimator weighted by the residual treatment probabilities.&lt;/p&gt;
&lt;h3 id=&#34;double-robustness&#34;&gt;Double Robustness&lt;/h3&gt;
&lt;p&gt;Why is the AIPW estimator so &lt;strong&gt;compelling&lt;/strong&gt;? The reason is that it just needs one of the two predictions, $\hat \mu$ or $\hat e$, to be right in order to be &lt;strong&gt;unbiased&lt;/strong&gt; (i.e. correct on average). Let&amp;rsquo;s check it.&lt;/p&gt;
&lt;p&gt;If $\hat \mu$ is correctly specified, i.e. $\mathbb E \left[ \hat \mu^{(d)}(x) \right] = \mathbb E \left[ Y_i \ \big | \ X_i = x, D_i = d \right]$, then $\hat \tau_{AIPW}$ is unbiased, &lt;strong&gt;even if&lt;/strong&gt; $\hat e$ is misspecified.&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
\hat \tau_{AIPW} &amp;amp;\overset{p}{\to} \mathbb E \Big[ \hat \tau_{S-learn} + \widehat{\text{adj}}_{S-learn} \Big] = \newline
&amp;amp;= \mathbb E \left[ \hat \mu^{(1)}(X_i) - \hat \mu^{(0)}(X_i) + \frac{D_i \left( Y_i - \hat \mu^{(1)}(X_i) \right)}{\hat e(X_i)} - \frac{(1-D_i) \left( Y_i - \hat \mu^{(0)}(X_i) \right)}{1-\hat e(X_i)} \right] = \newline
&amp;amp;= \mathbb E \left[ \hat \mu^{(1)}(X_i) - \hat \mu^{(0)}(X_i) \right] = \newline
&amp;amp;= \mathbb E \left[ Y^{(1)} - Y^{(0)} \right] = \newline
&amp;amp;= \tau
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;intuition&lt;/strong&gt; is that, if $\hat \mu$ is correctly specified, $\hat \tau_{S-learn}$ is &lt;strong&gt;unbiased&lt;/strong&gt; and the adjustment factor &lt;strong&gt;vanishes&lt;/strong&gt;, since the residuals $\left( Y_i - \hat \mu^{(t)}(X_i) \right)$ converge to zero.&lt;/p&gt;
&lt;p&gt;On the other hand, if $\hat e$ is correctly specified, i.e. $\mathbb E \left[\hat e(x) \right] = \mathbb E \left[ D_i = 1 \ \big | \ X_i = x \right]$, then $\hat \tau_{AIPW}$ is unbiased, &lt;strong&gt;even if&lt;/strong&gt; $\hat \mu$ is misspecified.&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
\hat \tau_{AIPW} &amp;amp; \overset{p}{\to} \mathbb E \Big[ \hat \tau_{IPW} + \widehat{\text{adj}}_{IPW} \Big] = \newline
&amp;amp;= \mathbb E \left[ \frac{D_i Y_i}{\hat e(X_i)} - \frac{(1-D_i) Y_i }{1-\hat e(X_i)} + \frac{\hat e(X_i) - D_i}{\hat e(X_i)} \hat \mu^{(1)}(X_i) - \frac{(1-\hat e(X_i)) - (1-D_i)}{1-\hat e(X_i)} \hat \mu^{(0)}(X_i) \right] = \newline
&amp;amp;= \mathbb E \left[ \frac{D_i Y_i}{\hat e(X_i)} - \frac{(1-D_i) Y_i }{1-\hat e(X_i)}\right] = \newline
&amp;amp;= \mathbb E \left[ Y^{(1)} - Y^{(0)} \right] = \newline
&amp;amp;= \tau
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;intuition&lt;/strong&gt; is that, if $\hat e$ is correctly specified, $\hat \tau_{IPW}$ is &lt;strong&gt;unbiased&lt;/strong&gt; and the adjustment factor &lt;strong&gt;vanishes&lt;/strong&gt;, since the residuals $\left( D_i - \hat e (X_i) \right)$ converge to zero.&lt;/p&gt;
&lt;h3 id=&#34;best-practices&#34;&gt;Best Practices&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;1. Check Covariate Balance&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Both IPW and AIPW were built for settings in which the treatment $D$ is not unconditionally randomly assigned, but might depend on some observables $X$. This information can be checked in two ways:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Produce a balance table, summarizing the covariates across treatment arms. If unconditional randomization does not hold, we expect to see significant differences across some observables&lt;/li&gt;
&lt;li&gt;Plot the estimated propensity scores. If unconditional randomization holds, we expect the propensity scores to be constant&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;2. Check the Overlap Assumption&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Another assumption that we can check is the &lt;strong&gt;overlap&lt;/strong&gt; assumption, i.e. $\exists \eta \ : \ \eta \leq \mathbb E \left[ D_i = 1 \ \big | \ X_i = x \right] \leq 1-\eta$. To check this assumption we can simply check the bounds of the predicted propensity scores. If the overlap assumption is violated, we end up dividing some term of the estimator by zero.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;3. Use Cross-Fitting&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Whenever we build a prediction, it is best practice to exclude observation $i$ when estimating $\hat \mu^{(d)} (X_i)$ or $\hat e (X_i)$. This procedure is generally known as &lt;a href=&#34;https://en.wikipedia.org/wiki/Cross-validation_%28statistics%29&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;cross-fitting&lt;/strong&gt;&lt;/a&gt; in the machine learning literature. While there are many possible ways to perform cross-fitting, the simplest one is the following:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Split the sample in two at random&lt;/li&gt;
&lt;li&gt;Use sample 1 to estimate $\hat \mu^{(d)} (X_i)$ and $\hat e (X_i)$&lt;/li&gt;
&lt;li&gt;Use sample 2 to estimate $\hat{\tau}_{AIPW, 1}$&lt;/li&gt;
&lt;li&gt;Repeat (2) and (3) swapping samples to estimate $\hat{\tau}_{AIPW, 2}$&lt;/li&gt;
&lt;li&gt;Compute $\hat{\tau}_{AIPW}$ as the average of the two estimates&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Steps (2) and (3) ensure that the estimator is &lt;strong&gt;not overfitting&lt;/strong&gt;. Steps (4) and (5) ensure that the estimator is &lt;strong&gt;efficient&lt;/strong&gt;, using all the data for all steps and not just half. &lt;a href=&#34;https://arxiv.org/abs/2004.14497&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Kennedy (2022)&lt;/a&gt; shows that this procedure produces much more precise estimates than existing methods and provide formal results on error bounds. In particular, their main result is the following:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;&lt;em&gt;The bound on the DR-Learner error given in Theorem 2 shows that it can only deviate from the oracle error by at most a (smoothed) product of errors in the propensity score and regression estimators, thus allowing faster rates for estimating the CATE even when the nui- sance estimates converge at slower rates. Importantly the result is agnostic about the methods used, and requires no special tuning or undersmoothing.&lt;/em&gt;&amp;rdquo;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;back-to-the-data&#34;&gt;Back to the Data&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s now build and explore the AIPW estimator in our dataset on blog reading time and dark mode.&lt;/p&gt;
&lt;h3 id=&#34;propensity-scores&#34;&gt;Propensity Scores&lt;/h3&gt;
&lt;p&gt;First, let&amp;rsquo;s estimate the &lt;strong&gt;propensity scores&lt;/strong&gt; $e(X)$.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def estimate_e(df, X, D, model_e):
    e = model_e.fit(df[X], df[D]).predict_proba(df[X])[:,1]
    return e
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We estimate them by &lt;a href=&#34;https://en.wikipedia.org/wiki/Logistic_regression&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;logistic regression&lt;/a&gt; using the &lt;code&gt;LogisticRegression&lt;/code&gt; methods from the &lt;code&gt;sklearn&lt;/code&gt; package.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from sklearn.linear_model import LogisticRegression

df[&#39;e&#39;] = estimate_e(df, X, &amp;quot;dark_mode&amp;quot;, LogisticRegression())
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s check if the &lt;strong&gt;bounded support&lt;/strong&gt; assumption is satisfied, by plotting the estimated propensity scores, across treatment and control groups.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sns.histplot(data=df, x=&#39;e&#39;, hue=&#39;dark_mode&#39;, bins=30, stat=&#39;density&#39;, common_norm=False).\
    set(ylabel=&amp;quot;&amp;quot;, title=&amp;quot;Distribution of Propensity Scores&amp;quot;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/aipw_38_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;The distribution of propensity scores is different between two groups, but it&amp;rsquo;s generally overlapping.&lt;/p&gt;
&lt;p&gt;We can now use the propensity scores to build the IPW estimator.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;w = 1 / (e * df[&amp;quot;dark_mode&amp;quot;] + (1-e) * (1-df[&amp;quot;dark_mode&amp;quot;]))
smf.wls(&amp;quot;read_time ~ dark_mode&amp;quot;, weights=w, data=df).fit().summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
          &lt;td&gt;&lt;/td&gt;             &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt;         &lt;td&gt;   18.6099&lt;/td&gt; &lt;td&gt;    0.412&lt;/td&gt; &lt;td&gt;   45.159&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;   17.799&lt;/td&gt; &lt;td&gt;   19.421&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;dark_mode[T.True]&lt;/th&gt; &lt;td&gt;    1.0620&lt;/td&gt; &lt;td&gt;    0.582&lt;/td&gt; &lt;td&gt;    1.826&lt;/td&gt; &lt;td&gt; 0.069&lt;/td&gt; &lt;td&gt;   -0.083&lt;/td&gt; &lt;td&gt;    2.207&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;Note that the computed standard errors are not exact, since we are ignoring the extra uncertainty that comes from the estimation of the propensity scores $e(X)$.&lt;/p&gt;
&lt;h3 id=&#34;response-function&#34;&gt;Response Function&lt;/h3&gt;
&lt;p&gt;Let&amp;rsquo;s now estimate the second building block of the AIPW estimator: the &lt;strong&gt;response function&lt;/strong&gt; $\mu(X)$.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def estimate_mu(df, X, D, y, model_mu):
    mu = model_mu.fit(df[X + [D]], df[y])
    mu0 = mu.predict(df[X + [D]].assign(dark_mode=0))
    mu1 = mu.predict(df[X + [D]].assign(dark_mode=1))
    return mu0, mu1
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s start by estimating $\mu(X)$ with linear regression.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from sklearn.linear_model import LinearRegression

mu0, mu1 = estimate_mu(df, X, &amp;quot;dark_mode&amp;quot;, &amp;quot;read_time&amp;quot;, LinearRegression())
print(np.mean(mu1-mu0))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;1.3858099131476969
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We have computed the meta learner estimate of the average treatment effect as the difference in means between the two estimated response functions, $\mu^{(1)}(X)$ and $\mu^{(0)}(X)$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt; that we can use any estimator to get the response function, I used linear regression for simplicity.&lt;/p&gt;
&lt;h3 id=&#34;estimating-aipw&#34;&gt;Estimating AIPW&lt;/h3&gt;
&lt;p&gt;We now have &lt;strong&gt;all the building blocks&lt;/strong&gt; to compute the AIPW estimator!&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;aipw = mu1 - mu0 + df[&amp;quot;dark_mode&amp;quot;] / e * (df[&amp;quot;read_time&amp;quot;] - mu1) - (1-df[&amp;quot;dark_mode&amp;quot;]) / (1-e) * (df[&amp;quot;read_time&amp;quot;] - mu0)
print(np.mean(aipw))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;1.3153774511905783
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can also compute it directly using the &lt;code&gt;LinearDRLearner&lt;/code&gt; function from Microsoft&amp;rsquo;s &lt;a href=&#34;https://econml.azurewebsites.net/index.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;EconML&lt;/code&gt;&lt;/a&gt; library.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from econml.drlearner import LinearDRLearner

model = LinearDRLearner(model_propensity=LogisticRegression(), 
                        model_regression=LinearRegression(),
                        random_state=1)
model.fit(Y=df[&amp;quot;read_time&amp;quot;], T=df[&amp;quot;dark_mode&amp;quot;], X=df[X]);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The model directly gives us the average treatment effect.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;model.ate_inference(X=df[X].values, T0=0, T1=1).summary().tables[0]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;caption&gt;Uncertainty of Mean Point Estimate&lt;/caption&gt;
&lt;tr&gt;
  &lt;th&gt;mean_point&lt;/th&gt; &lt;th&gt;stderr_mean&lt;/th&gt; &lt;th&gt;zstat&lt;/th&gt; &lt;th&gt;pvalue&lt;/th&gt; &lt;th&gt;ci_mean_lower&lt;/th&gt; &lt;th&gt;ci_mean_upper&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
     &lt;td&gt;1.417&lt;/td&gt;      &lt;td&gt;0.541&lt;/td&gt;    &lt;td&gt;2.621&lt;/td&gt;  &lt;td&gt;0.009&lt;/td&gt;     &lt;td&gt;0.358&lt;/td&gt;         &lt;td&gt;2.477&lt;/td&gt;    
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;The estimate is statistically different from zero and the confidence interval includes the true value of 2.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt; that we got a different estimate because the &lt;code&gt;LinearDRLearner&lt;/code&gt; function also performed &lt;strong&gt;cross-fitting&lt;/strong&gt; in the background, which we did not before.&lt;/p&gt;
&lt;h3 id=&#34;assessment&#34;&gt;Assessment&lt;/h3&gt;
&lt;p&gt;Let&amp;rsquo;s now assess the main property of the AIPW estimator: its &lt;strong&gt;double robustness&lt;/strong&gt;. To do so, we compare it with its two parents: the IPW estimator and the S-learner.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def compare_estimators(X_e, X_mu, D, y, seed):
    df = dgp_darkmode().generate_data(seed=seed)
    e = estimate_e(df, X_e, D, LogisticRegression())
    mu0, mu1 = estimate_mu(df, X_mu, D, y, LinearRegression())
    slearn = mu1 - mu0
    ipw = (df[D] / e - (1-df[D]) / (1-e)) * df[y]
    aipw = slearn + df[D] / e * (df[y] - mu1) - (1-df[D]) / (1-e) * (df[y] - mu0)
    return np.mean((slearn, ipw, aipw), axis=1)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We use the &lt;a href=&#34;https://joblib.readthedocs.io/en/latest/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;joblib&lt;/code&gt;&lt;/a&gt; library to run the simulations in parallel and speed up the process.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from joblib import Parallel, delayed

def simulate_estimators(X_e, X_mu, D, y):
    r = Parallel(n_jobs=8)(delayed(compare_estimators)(X_e, X_mu, D, y, i) for i in range(100))
    df_tau = pd.DataFrame(r, columns=[&#39;S-learn&#39;, &#39;IPW&#39;, &#39;AIPW&#39;])
    plot = sns.boxplot(data=pd.melt(df_tau), x=&#39;variable&#39;, y=&#39;value&#39;, linewidth=2);
    plot.set(title=&amp;quot;Distribution of $\hat τ$ and its components&amp;quot;, xlabel=&#39;&#39;, ylabel=&#39;&#39;)
    plot.axhline(2, c=&#39;r&#39;, ls=&#39;:&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;First, let&amp;rsquo;s assume that we use &lt;strong&gt;all variables&lt;/strong&gt; for both models, $\mu(X)$ and $e(X)$. In this case, both models are &lt;strong&gt;well specified&lt;/strong&gt; and we expect all estimators to perform well.&lt;/p&gt;
&lt;p&gt;We plot the distribution of the three estimators across 100 simulations.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;simulate_estimators(X_e=X, X_mu=X, D=&amp;quot;dark_mode&amp;quot;, y=&amp;quot;read_time&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/aipw_62_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Indeed, all estimator are &lt;strong&gt;unbiased&lt;/strong&gt; and deliver very similar estimates.&lt;/p&gt;
&lt;p&gt;What happens if we &lt;strong&gt;misspecify&lt;/strong&gt; one of the two models? Let&amp;rsquo;s start by (correctly) assuming that &lt;code&gt;gender&lt;/code&gt; and &lt;code&gt;age&lt;/code&gt; influence the probability of selecting &lt;code&gt;dark_mode&lt;/code&gt; and (wrongly) assuming that only previous &lt;code&gt;hours&lt;/code&gt; influence the weekly &lt;code&gt;read_time&lt;/code&gt;. In this case, the propensity score $e(X)$ is well specified, while the response function $\mu(X)$ is misspecified.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;simulate_estimators(X_e=[&#39;male&#39;, &#39;age&#39;], X_mu=[&#39;hours&#39;], D=&amp;quot;dark_mode&amp;quot;, y=&amp;quot;read_time&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/aipw_64_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;As expected, the S-learner is biased since we have misspecified $\mu(X)$, while IPW isn&amp;rsquo;t. AIPW picks the &lt;strong&gt;best of both worlds&lt;/strong&gt; and is unbiased.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s now explore the alternative &lt;strong&gt;misspecification&lt;/strong&gt;. We (wrongly) assume that only &lt;code&gt;age&lt;/code&gt; influences the probability of selecting &lt;code&gt;dark_mode&lt;/code&gt; and (correctly) assume that both &lt;code&gt;gender&lt;/code&gt; and previous &lt;code&gt;hours&lt;/code&gt; influence the weekly &lt;code&gt;read_time&lt;/code&gt;. In this case, the propensity score $e(X)$ is misspecified, while the response function $\mu(X)$ is correctly specified.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;simulate_estimators([&#39;age&#39;], [&#39;male&#39;, &#39;hours&#39;], D=&amp;quot;dark_mode&amp;quot;, y=&amp;quot;read_time&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/aipw_66_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;In this case, the S-learner is unbiased, while IPW isn&amp;rsquo;t, since we have misspecified $e(X)$. Again, AIPW picks the &lt;strong&gt;best of both worlds&lt;/strong&gt; and is unbiased.&lt;/p&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;In this article we have seen a method to estimate conditional average treatment effects (CATE), that is &lt;strong&gt;robust to model misspecification&lt;/strong&gt;: the Augmented Inverse Propensity Weighted (AIPW) estimator. The AIPW estimator takes the best out of two existing estimators: the &lt;a href=&#34;&#34;&gt;IPW estimator&lt;/a&gt; and the &lt;a href=&#34;&#34;&gt;S-learner&lt;/a&gt;. It requires the estimation of both the propensity score function $\mathbb{E} [ D | X ]$ and the response function $\mathbb{E} [ Y | D, X ]$ and it is &lt;strong&gt;unbiased&lt;/strong&gt; even if one of the two functions is misspecified.&lt;/p&gt;
&lt;p&gt;This estimator is now a standard and it is included all the most important causal inference packages such as Microsoft&amp;rsquo;s &lt;a href=&#34;https://econml.azurewebsites.net/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;EconML&lt;/a&gt;, Uber&amp;rsquo;s &lt;a href=&#34;https://causalml.readthedocs.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;causalml&lt;/a&gt; and Stanford researchers&amp;rsquo; R package &lt;a href=&#34;https://grf-labs.github.io/grf/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;grf&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;references&#34;&gt;References&lt;/h3&gt;
&lt;p&gt;[1] J. Robins, A. Rotzniski, J. P. Zhao, &lt;a href=&#34;https://www.tandfonline.com/doi/abs/10.1080/01621459.1994.10476818&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Estimation of regression coefficients when some regressors are not always observed&lt;/a&gt; (1994), &lt;em&gt;Journal of the American Statistical Associations&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;[2] A. Glyn, K. Quinn, &lt;a href=&#34;https://www.cambridge.org/core/journals/political-analysis/article/abs/an-introduction-to-the-augmented-inverse-propensity-weighted-estimator/4B1B8301E46F4432C4DCC91FE20780DB&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;An Introduction to the Augmented Inverse Propensity Weighted Estimator&lt;/a&gt; (2010), &lt;em&gt;Political Analysis&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;[3] E. Kennedy, &lt;a href=&#34;https://arxiv.org/abs/2004.14497&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Towards optimal doubly robust estimation of heterogeneous causal effects&lt;/a&gt; (2022), &lt;em&gt;working paper&lt;/em&gt;.&lt;/p&gt;
&lt;h3 id=&#34;related-articles&#34;&gt;Related Articles&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/b63dc69e3d8c&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;DAGs and Control Variables&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/99bf5cffa0d9&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Matching, Weighting, or Regression?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/8a9c1e340832&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Understanding Meta Learners&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;code&#34;&gt;Code&lt;/h3&gt;
&lt;p&gt;You can find the original Jupyter Notebook here:&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/aipw.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/aipw.ipynb&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Understanding Causal Trees</title>
      <link>https://matteocourthoud.github.io/post/causal_trees/</link>
      <pubDate>Mon, 10 Jul 2023 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/post/causal_trees/</guid>
      <description>&lt;p&gt;&lt;em&gt;How to use regression trees to estimate heterogeneous treatment effects.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;In causal inference we are usually interested in estimating the causal effect of a treatment (a drug, ad, product, &amp;hellip;) on an outcome of interest (a disease, firm revenue, customer satisfaction, &amp;hellip;). However, knowing that a treatment works on average is often not sufficient and we would like to know for which subjects (patients, users, customers, &amp;hellip;) it works better or worse, i.e. we would like to estimate &lt;strong&gt;heterogeneous treatment effects&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Estimating heterogeneous treatments effects allows us to do &lt;strong&gt;targeting&lt;/strong&gt;. Knowing which customers are more likely to react to a discount allows a company to spend less money by offering fewer but better targeted discounts. This works also for negative effects: knowing for which patients a certain drug has side effects allows a pharmaceutical company to warn or exclude them from the treatment. There is also a more subtle advantage of estimating heterogeneous treatment effects: knowing &lt;strong&gt;for whom&lt;/strong&gt; a treatment works allows us to better understand &lt;strong&gt;how&lt;/strong&gt; a treatment works. Knowing that the effect of a discount does not depend on the income of its recipient but rather by its buying habits  tells us that maybe it is not a matter of money, but rather a matter of attention or loyalty.&lt;/p&gt;
&lt;p&gt;In this article, we will explore the estimation of heterogeneous treatment effects using a modified version of regression trees (and forests). From a machine learning perspective, there are two fundamental &lt;strong&gt;differences between causal trees and predictive trees&lt;/strong&gt;. First of all, the target is the treatment effect, which is an inherently unobservable object. Second, we are interested in doing inference, which means quantifying the uncertainty of our estimates.&lt;/p&gt;
&lt;h2 id=&#34;online-discounts&#34;&gt;Online Discounts&lt;/h2&gt;
&lt;p&gt;For the rest of the article, we are going to use a toy example, for the sake of exposition: suppose we were an &lt;strong&gt;online shop&lt;/strong&gt; and we are interested in understanding whether offering discounts to new customers increases their expenditure. In particular, we would like to know if offering discounts is more effective for some customers with respect to others, since we would prefer not to give discounts to customers that would spend anyways. Moreover, it could also be that spamming customers with pop-ups could deter them from buying, having the opposite effect.&lt;/p&gt;
&lt;img src=&#34;fig/causal_trees1.jpg&#34; width=&#34;300px&#34;/&gt;
&lt;p&gt;To understand whether and how much the discounts are effective we run an &lt;strong&gt;A/B test&lt;/strong&gt;: whenever a new user visits our online shop, we randomly decide whether to offer them the discount or not. I import the data-generating process &lt;code&gt;dgp_online_discounts()&lt;/code&gt; from &lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/src/dgp.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;src.dgp&lt;/code&gt;&lt;/a&gt;. With respect to previous articles, I generated a new DGP parent class that handles randomization and data generation, while its children classes contain specific use cases. I also import some plotting functions and libraries from &lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/src/utils.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;src.utils&lt;/code&gt;&lt;/a&gt;. To include not only code but also data and tables, I use &lt;a href=&#34;https://deepnote.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Deepnote&lt;/a&gt;, a Jupyter-like web-based collaborative notebook environment.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;%matplotlib inline
%config InlineBackend.figure_format = &#39;retina&#39;

from src.utils import *
from src.dgp import dgp_online_discounts
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;dgp = dgp_online_discounts(n=100_000)
df = dgp.generate_data()
df.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;time&lt;/th&gt;
      &lt;th&gt;device&lt;/th&gt;
      &lt;th&gt;browser&lt;/th&gt;
      &lt;th&gt;region&lt;/th&gt;
      &lt;th&gt;discount&lt;/th&gt;
      &lt;th&gt;spend&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;10.78&lt;/td&gt;
      &lt;td&gt;mobile&lt;/td&gt;
      &lt;td&gt;edge&lt;/td&gt;
      &lt;td&gt;9&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0.46&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;0.57&lt;/td&gt;
      &lt;td&gt;desktop&lt;/td&gt;
      &lt;td&gt;firefox&lt;/td&gt;
      &lt;td&gt;9&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;11.04&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;3.74&lt;/td&gt;
      &lt;td&gt;mobile&lt;/td&gt;
      &lt;td&gt;safari&lt;/td&gt;
      &lt;td&gt;7&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1.81&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;13.37&lt;/td&gt;
      &lt;td&gt;desktop&lt;/td&gt;
      &lt;td&gt;other&lt;/td&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;31.90&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;0.71&lt;/td&gt;
      &lt;td&gt;mobile&lt;/td&gt;
      &lt;td&gt;explorer&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;15.42&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;We have data on 100.000 website visitors, for whom we observe the &lt;code&gt;time&lt;/code&gt; of the day, the &lt;code&gt;device&lt;/code&gt; they use, their &lt;code&gt;browser&lt;/code&gt; and their geographical &lt;code&gt;region&lt;/code&gt;. We also see whether they were offered the &lt;code&gt;discount&lt;/code&gt;, our treatment, and what is their &lt;code&gt;spend&lt;/code&gt;, the outcome of interest.&lt;/p&gt;
&lt;p&gt;Since the treatment was randomly assigned, we can use a simple &lt;strong&gt;difference-in-means&lt;/strong&gt; estimator to estimate the treatment effect. We expect the treatment and control group to be similar, except for the &lt;code&gt;discount&lt;/code&gt;, therefore we can causally attribute any difference in &lt;code&gt;spend&lt;/code&gt; to the &lt;code&gt;discount&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;smf.ols(&#39;spend ~ discount&#39;, df).fit().summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
      &lt;td&gt;&lt;/td&gt;         &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt; &lt;td&gt;    5.0306&lt;/td&gt; &lt;td&gt;    0.045&lt;/td&gt; &lt;td&gt;  110.772&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    4.942&lt;/td&gt; &lt;td&gt;    5.120&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;discount&lt;/th&gt;  &lt;td&gt;    1.9492&lt;/td&gt; &lt;td&gt;    0.064&lt;/td&gt; &lt;td&gt;   30.346&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    1.823&lt;/td&gt; &lt;td&gt;    2.075&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;The discount seems to be effective: on average the spend in the treatment group increases by 3.86$. But are all customers equally affected?&lt;/p&gt;
&lt;p&gt;To answer this question, we would like to estimate &lt;strong&gt;heterogeneous treatment effects&lt;/strong&gt;, possibly at the individual level.&lt;/p&gt;
&lt;h2 id=&#34;conditional-average-treatment-effects&#34;&gt;Conditional Average Treatment Effects&lt;/h2&gt;
&lt;p&gt;There are many possible ways to estimate heterogenous treatment effects. The most common is to split the population in groups based on some observable characteristic, which in our case could be the &lt;code&gt;device&lt;/code&gt;, the &lt;code&gt;browser&lt;/code&gt; or the geographical &lt;code&gt;region&lt;/code&gt;. Once you have decided which variable to split your data on, you can simply interact the treatment variable (&lt;code&gt;discount&lt;/code&gt;) with the dimension of treatment heterogeneity. Let&amp;rsquo;s take &lt;code&gt;device&lt;/code&gt; for example.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;smf.ols(&#39;spend ~ discount * device&#39;, df).fit().summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
              &lt;td&gt;&lt;/td&gt;                 &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt;                 &lt;td&gt;    5.0006&lt;/td&gt; &lt;td&gt;    0.064&lt;/td&gt; &lt;td&gt;   78.076&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    4.875&lt;/td&gt; &lt;td&gt;    5.126&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;device[T.mobile]&lt;/th&gt;          &lt;td&gt;    0.0602&lt;/td&gt; &lt;td&gt;    0.091&lt;/td&gt; &lt;td&gt;    0.664&lt;/td&gt; &lt;td&gt; 0.507&lt;/td&gt; &lt;td&gt;   -0.118&lt;/td&gt; &lt;td&gt;    0.238&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;discount&lt;/th&gt;                  &lt;td&gt;    1.2264&lt;/td&gt; &lt;td&gt;    0.091&lt;/td&gt; &lt;td&gt;   13.527&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    1.049&lt;/td&gt; &lt;td&gt;    1.404&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;discount:device[T.mobile]&lt;/th&gt; &lt;td&gt;    1.4447&lt;/td&gt; &lt;td&gt;    0.128&lt;/td&gt; &lt;td&gt;   11.261&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    1.193&lt;/td&gt; &lt;td&gt;    1.696&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;How do we interpret the regression results? The effect of the &lt;code&gt;discount&lt;/code&gt; on customers&amp;rsquo; &lt;code&gt;spend&lt;/code&gt; is $1.22$$ but it increases by a further $1.44$$ if the customer is accessing the website from a mobile &lt;code&gt;device&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Splitting is easy for categorical variables, but for a continuous variable like &lt;code&gt;time&lt;/code&gt; it is not intuitive where to split. Every hour? And which dimension is more informative? It would be temping to try all possible splits, but the more we split the data, the more it is likely that we find spurious results (i.e. we overfit, in machine learning lingo). It would be great if we could &lt;strong&gt;let the data speak&lt;/strong&gt; and select the minimum and most informative splits.&lt;/p&gt;
&lt;p&gt;In a &lt;a href=&#34;https://medium.com/towards-data-science/understanding-meta-learners-8a9c1e340832&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;separate post&lt;/a&gt;, I have shown how the so-called &lt;strong&gt;meta-learners&lt;/strong&gt; take this approach to causal inference. The idea is to predict the outcome conditional on the treatment status for each observation, and then compare the predicted conditional on treatment, with the predicted outcome conditional on control. The difference is the individual treatment effect.&lt;/p&gt;
&lt;p&gt;The problem with meta-learners is that they use all their &lt;a href=&#34;https://en.wikipedia.org/wiki/Degrees_of_freedom_%28statistics%29&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;degrees of freedom&lt;/a&gt; in predicting the outcome. However, we are interested to predict treatment effect heterogeneity. If most of the variation in the outcome is &lt;em&gt;not&lt;/em&gt; in the treatment dimension, we will get very poor estimates of the treatment effects.&lt;/p&gt;
&lt;p&gt;Is it possible to instead directly concentrate on the &lt;strong&gt;prediction of individual treatment effects&lt;/strong&gt;? Let&amp;rsquo;s define $Y$ the outcome of interest, &lt;code&gt;spend&lt;/code&gt; in our case, $D$ the treatment, the &lt;code&gt;discount&lt;/code&gt;, and $X$ other observable characteristics. The &lt;em&gt;ideal&lt;/em&gt; objective function is&lt;/p&gt;
&lt;p&gt;$$
\sum_i \Big [ ( \tau_i - \hat \tau_i(X))^2 \Big ]
$$&lt;/p&gt;
&lt;p&gt;where $\tau_i$ is the treatment effect of individual $i$. However, this objective function is &lt;strong&gt;unfeasible&lt;/strong&gt; since we do not observe $\tau_i$.&lt;/p&gt;
&lt;p&gt;But, turns out that there is a way to get an unbiased estimate of the &lt;strong&gt;individual treatment effect&lt;/strong&gt;. The &lt;strong&gt;idea&lt;/strong&gt; is to use an auxiliary outcome variable, whose expected value for each individual is the individual treatment effect. This variable is&lt;/p&gt;
&lt;p&gt;$$
Y_i^* = \frac{Y_i}{D_i \cdot p(X_i) - (1-D_i) \cdot (1-p(X_i))}
$$&lt;/p&gt;
&lt;p&gt;where $p(X_i)$ is the &lt;a href=&#34;https://en.wikipedia.org/wiki/Propensity_score_matching&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;propensity score&lt;/strong&gt;&lt;/a&gt; of observation $i$, i.e. its probability of being treated. A crucial assuption here is &lt;strong&gt;unconfoundedness&lt;/strong&gt;, also known as ignorability or selection on observables. In short, we will assume that, conditional on some observables $X$ the treatment assignment is as good as random.&lt;/p&gt;
&lt;p&gt;$$
\left\lbrace Y_i^{(0)}, Y_i^{(1)} \right \rbrace \ \perp \ D_i | X_i
$$&lt;/p&gt;
&lt;p&gt;where $Y_i^{(0)}$ and $Y_i^{(1)}$ denote the control and treated potential outcomes, respectively. In our case, we have randomized assignment, therefore we do not have to worry about unconfoundedness, unless the randomization went wrong.&lt;/p&gt;
&lt;p&gt;In randomized experiments, the propensity score is known since randomization is fully under control of the experimenter. For example, in our case, the probability of treatment was 50%. In quasi-experimental studies instead, when the treatment probability is not known, it has to be estimated. Even in randomized experiments, it is always better to estimate rather than inpute the propensity scores, since it guards against sampling variation in the randomization. For more details on the propensity scores and how they are used in causal inference, I have a separate post &lt;a href=&#34;https://medium.com/towards-data-science/matching-weighting-or-regression-99bf5cffa0d9&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s first generate dummy variables for our categorical variables, &lt;code&gt;device&lt;/code&gt;, &lt;code&gt;browser&lt;/code&gt; and &lt;code&gt;region&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_dummies = pd.get_dummies(df[dgp.X[1:]], drop_first=True)
df = pd.concat([df, df_dummies], axis=1)
X = [&#39;time&#39;] + list(df_dummies.columns)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We fit a &lt;code&gt;LogisticRegression&lt;/code&gt; and use it to predict the treatment probability, i.e. construct the propensity score.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from sklearn.linear_model import LogisticRegression

df[&#39;pscore&#39;] = LogisticRegression().fit(df[X], df[dgp.D]).predict_proba(df[X])[:,1]
sns.histplot(data=df, x=&#39;pscore&#39;, hue=&#39;discount&#39;).set(
    title=&#39;Predicted propensity scores&#39;, xlim=[0,1], xlabel=&#39;Propensity Score&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/causal_trees_18_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;As expected, most propensity scores are very close to 0.5, the probability of treatment used in randomization.&lt;/p&gt;
&lt;p&gt;We now have all the elements to compute our auxiliary outcome variable $Y^*$.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df[&#39;y_star&#39;] = df[dgp.Y[0]] / (df[dgp.D] * df[&#39;pscore&#39;] - (1-df[dgp.D]) * (1-df[&#39;pscore&#39;]))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As we said before, the idea is to use $Y^*$ as the target of a prediction problem, since the expected value is exactly the individual treatment effect. Let&amp;rsquo;s check its average in the data.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df[&#39;y_star&#39;].mean()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;1.94501174385229
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Indeed its average is almost identical to the previously estimated average treatment effect of 3.85.&lt;/p&gt;
&lt;p&gt;How is it possible that, with a single observation and an estimate of the propensity score, we can estimate the individual treatment effect? What are the drawbacks?&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;intuition&lt;/strong&gt; is to approach the problem from a different perspective: &lt;em&gt;ex-ante&lt;/em&gt;, before the experiment. Imagine that our dataset had a single observation, $i$. We know that the treatment probability is $p(X_i)$, the propensity score. Therefore, in expectation, our dataset has $p(X_i)$ observations in the treatment group and $1 - p(X_i)$ observations in the control group. The rest is business as usual: we estimate the treatment effect as the difference in average outcomes between the two groups! And indeed that is what we would do:&lt;/p&gt;
&lt;p&gt;$$
Y_i^* = \frac{Y_i D_i}{p(X_i)} - \frac{Y_i (1-D_i)}{1-p(X_i)}
$$&lt;/p&gt;
&lt;p&gt;The only difference is that we have a single observation.&lt;/p&gt;
&lt;p&gt;This trick comes at a cost: $Y_i^*$ is an unbiased estimator for the individual treatment effect, but has a very &lt;strong&gt;high variance&lt;/strong&gt;. This is immediately visible by plotting its distribution.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;fig, ax = plt.subplots()
sns.histplot(df[&#39;y_star&#39;], ax=ax).set(title=&#39;Distribution of Auxiliary Variable&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/causal_trees_24_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;We are now ready to estimate &lt;strong&gt;heterogeneous treatment effects&lt;/strong&gt;, by translating the causal inference problem into a prediction problem, predicting the auxiliary outcome $Y^*$, given observable characteristics $X$.&lt;/p&gt;
&lt;img src=&#34;fig/causal_trees2.jpg&#34; width=&#34;300px&#34;/&gt;
&lt;h2 id=&#34;causal-trees&#34;&gt;Causal Trees&lt;/h2&gt;
&lt;p&gt;In the previous section, we have see that we can transform the estimation of &lt;strong&gt;heterogeneous treatment effects&lt;/strong&gt; into a prediction problem, where the outcome is the auxiliary outcome variable&lt;/p&gt;
&lt;p&gt;$$
Y_i^* = \frac{Y_i}{T_i * e_i - (1-T_i) * (1-e_i)}
$$&lt;/p&gt;
&lt;p&gt;We can in principle use any machine learning algorithm at this point to estimate individual treatment effects. However, &lt;a href=&#34;https://en.wikipedia.org/wiki/Decision_tree_learning&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;regression trees&lt;/strong&gt;&lt;/a&gt; have particularly convenient characteristics.&lt;/p&gt;
&lt;p&gt;First of all, how do regression trees work? Without going too much in detail, they are an algorithm that recursively &lt;strong&gt;partitions the data in bins&lt;/strong&gt; such that the outcome $Y$ &lt;em&gt;within&lt;/em&gt; each bin is as homogeneous as possible and the outcome &lt;em&gt;across&lt;/em&gt; bins is as heterogeneous as possible. The predicted values are simply the averages within each bin.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;averaging&lt;/strong&gt; part is one of the big advantages of regression trees for inference since we know very well how to do inference with averages, with the &lt;a href=&#34;https://en.wikipedia.org/wiki/Central_limit_theorem&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Central Limit Theorem&lt;/strong&gt;&lt;/a&gt;. The second advantage is that trees are very &lt;strong&gt;interpretable&lt;/strong&gt;, since we can directly plot the data partition as a tree structure. We will see more of this later. Last but not least, regression trees are still at the core the &lt;a href=&#34;https://arxiv.org/abs/2207.08815&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;best performing predictive algorithms&lt;/a&gt; with tabular data, as of 2022.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s use the &lt;a href=&#34;https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;DecisionTreeRegressor&lt;/code&gt;&lt;/a&gt; function from &lt;code&gt;sklearn&lt;/code&gt; to fit our regression tree and estimate heterogeneous treatment effects of &lt;code&gt;discounts&lt;/code&gt; on customers&amp;rsquo; &lt;code&gt;spend&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from sklearn.tree import DecisionTreeRegressor

tree = DecisionTreeRegressor(max_depth=2).fit(df[X], df[&#39;y_star&#39;])
df[&#39;y_hat&#39;] = tree.predict(df[X])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We have restricted the tree to have a maximum depth of 2 and at least 30 observation per partition (also called &lt;em&gt;leaf&lt;/em&gt;) so that we can easily plot the tree and visualize the estimated groups and treatment effects.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from sklearn.tree import plot_tree

plot_tree(tree, filled=True, fontsize=12, feature_names=X, impurity=False, rounded=True);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/causal_trees_31_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;How should we &lt;strong&gt;interpret&lt;/strong&gt; the tree? On the top, we can see the average $Y^*$ in the data, 3.851. Starting from there, the data gets split into different branches, according to the rules highlighted at the top of each node. For example, the first node splits the data into two groups of size 42970 and 57030 depending on whether the &lt;code&gt;time&lt;/code&gt; is later than 10.365. At the bottom, we have our final partitions, with the predicted values. For example, the leftmost leaf contains 36846 observation with &lt;code&gt;time&lt;/code&gt; earlier than 10.365 and non-Safari &lt;code&gt;browser&lt;/code&gt;, for which we predict a spend of 1.078. Darker node colors indicate higher prediction values.&lt;/p&gt;
&lt;p&gt;Should we believe these estimates? Not really, because of a couple of reasons. The &lt;strong&gt;first problem&lt;/strong&gt; is that we have an unbiased estimate of the average treatment effect only if, &lt;em&gt;within each leaf&lt;/em&gt;, we have the same number of treated and control units. This is not automatically the case with an off-the-shelf &lt;code&gt;DecisionTreeRegressor()&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Moreover, we have used the &lt;strong&gt;same data&lt;/strong&gt; to generate the tree and evaluate it. This generates some bias because of overfitting. We can split the sample in 2 and use different data to generate the tree and compute the predictions. These trees are called &lt;strong&gt;honest trees&lt;/strong&gt;.&lt;/p&gt;
&lt;h3 id=&#34;generating-splits&#34;&gt;Generating Splits&lt;/h3&gt;
&lt;p&gt;Last but not least, how should the tree be generated? The default rule to generate splits with the &lt;code&gt;DecisionTreeRegressor&lt;/code&gt; function is the &lt;code&gt;squared_error&lt;/code&gt; and there is no restriction on the minimum number of observations per leaf. Other commonly used rules include, mean absolute error, Gini&amp;rsquo;s impurity, and Shannon&amp;rsquo;s information. Which one performs better depends on the specific application, but the general objective is always prediction accuracy, broadly defined.&lt;/p&gt;
&lt;p&gt;In our case instead, the objective is inference: we want to uncover heterogeneous  treatment effects that are statistically different from each other. There is no value in generating different treatment effects if they are statistically indistinguishable. Moreover (but strongly related), when building the tree and generating the data partitions, we have to take into account that, since we use honest trees, we will use different data to estimate the within-leaf treatment effects.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.pnas.org/doi/abs/10.1073/pnas.1510489113&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Athey and Imbens (2016)&lt;/a&gt; use an modified version of the &lt;a href=&#34;https://en.wikipedia.org/wiki/Mean_squared_error&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Mean Squared Error (MSE)&lt;/a&gt; as splitting criterion, the &lt;strong&gt;Expanded Mean Squared Error (EMSE)&lt;/strong&gt;:&lt;/p&gt;
&lt;p&gt;$$
EMSE = \mathbb{E} \Big[ \big( Y_i - \hat \mu(X_i)\big)^2 - Y_i^2 \Big]
$$&lt;/p&gt;
&lt;p&gt;where the main difference is given by the additional term $Y_i^2$, the squared outcome variable. In our case, we can rewrite it as&lt;/p&gt;
&lt;p&gt;$$
EMSE = \mathbb{E} \Big[ \big( Y^&lt;em&gt;_i - \hat \tau(X_i)\big)^2 - {Y^&lt;/em&gt;_i}^2 \Big]
$$&lt;/p&gt;
&lt;p&gt;Why is this a sensible error loss? Because we can rewrite it as the sum of the squared mean μ and the estimator&amp;rsquo;s variance.&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
EMSE &amp;amp;= \mathbb{E} \Big[ \big( Y^&lt;em&gt;_i - \hat \tau(X_i)\big)^2 - {Y^&lt;/em&gt;_i}^2 \Big] = \newline
&amp;amp;= \mathbb{E} \Big[ \big( Y^&lt;em&gt;_i - \tau(X_i)\big)^2 - {Y^&lt;/em&gt;_i}^2 \Big] - \mathbb{E} \Big[ \big( \hat \tau(X_i) - \tau(X_i)\big)^2 \Big] = \newline
&amp;amp;= \mathbb{E} \Big[ \mathbb{V} \big (\hat \tau(X_i)^2 \big) \Big] - \mathbb{E} \Big[ \tau(X_i)^2 \Big]
\end{aligned}
$$&lt;/p&gt;
&lt;h3 id=&#34;implementation&#34;&gt;Implementation&lt;/h3&gt;
&lt;p&gt;Luckily, there are multiple libraries where the so-called &lt;a href=&#34;https://www.pnas.org/doi/abs/10.1073/pnas.1510489113&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;causal trees&lt;/strong&gt;&lt;/a&gt; are implemented. We import &lt;code&gt;CausalForestDML&lt;/code&gt; from Microsoft&amp;rsquo;s &lt;a href=&#34;https://econml.azurewebsites.net/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;EconML&lt;/a&gt; library, one of the best libraries for causal inference.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from econml.dml import CausalForestDML

np.random.seed(0)
tree_model = CausalForestDML(n_estimators=1, subforest_size=1, inference=False, max_depth=3)
tree_model = tree_model.fit(Y=df[dgp.Y], X=df[X], T=df[dgp.D])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We have restricted the number of estimators to 1 to have a single tree instead of multiple ones, the so-called &lt;a href=&#34;https://en.wikipedia.org/wiki/Random_forest&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;random forests&lt;/strong&gt;&lt;/a&gt; that we will cover in a separate article.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from econml.cate_interpreter import SingleTreeCateInterpreter
%matplotlib inline

intrp = SingleTreeCateInterpreter(max_depth=2).interpret(tree_model, df[X])
intrp.plot(feature_names=X, fontsize=12)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/causal_trees_39_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;As we can see, the tree representation looks extremely similar to the one we got before using the &lt;code&gt;DecisionTreeRegressor&lt;/code&gt; function. However, now the model not only reports estimates of the conditional average treatment effects, but also the standard errors of the estimates (at the bottom). How were they computed?&lt;/p&gt;
&lt;h3 id=&#34;inference&#34;&gt;Inference&lt;/h3&gt;
&lt;p&gt;Honest trees, besides improving the out-of-sample prediction accuracy of the model, have another great implication: they allow us to &lt;strong&gt;compute standard errors as if the tree structure was exogenous&lt;/strong&gt;. In fact, since the data used to compute the predictions is independent from the data used to build the tree (split the data), we can just treat the tree structure as independent from the estimated treatment effects. As a consequence, we can estimate the standard errors of the the estimates as standard errors of difference between sample averages, as in a standard AB test.&lt;/p&gt;
&lt;p&gt;If we had used the same data to build the tree and estimate the treatment effects, we would have introduced &lt;strong&gt;bias&lt;/strong&gt;, because of the spurious correlation between the covariates and the outcomes. This bias usually disappears for very large sample sizes, but honest trees do not require than.&lt;/p&gt;
&lt;h3 id=&#34;performance&#34;&gt;Performance&lt;/h3&gt;
&lt;p&gt;How well does the model perform? Since we control the data generating process, we can do something that is not possible with real data: check the predicted treatment effects against the true ones. The &lt;code&gt;generate_potential_outcomes()&lt;/code&gt; function loads the data with both potential outcomes for each observation, under both treatment (&lt;code&gt;outcome_t&lt;/code&gt;) and control (&lt;code&gt;outcome_c&lt;/code&gt;).&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def compute_discrete_effects(df, hte_model):
    temp_df = df.copy()
    temp_df.time = 0
    temp_df = dgp.add_treatment_effect(temp_df)
    temp_df = temp_df.rename(columns={&#39;effect_on_spend&#39;: &#39;True&#39;})
    temp_df[&#39;Predicted&#39;] = hte_model.effect(temp_df[X])
    df_effects = pd.DataFrame()
    for var in X[1:]:
        for effect in [&#39;True&#39;, &#39;Predicted&#39;]:
            v = temp_df[effect][temp_df[var]==1].mean() - temp_df[effect][temp_df[var]==0].mean()
            effect_var = {&#39;Variable&#39;: [var], &#39;Effect&#39;: [effect], &#39;Value&#39;: [v]}
            df_effects = pd.concat([df_effects, pd.DataFrame(effect_var)]).reset_index(drop=True)
    return df_effects, temp_df[&#39;Predicted&#39;].mean()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_effects_tree, avg_effect_notime_tree = compute_discrete_effects(df, tree_model)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;fig, ax = plt.subplots()
sns.barplot(data=df_effects_tree, x=&amp;quot;Variable&amp;quot;, y=&amp;quot;Value&amp;quot;, hue=&amp;quot;Effect&amp;quot;, ax=ax).set(
    xlabel=&#39;&#39;, ylabel=&#39;&#39;, title=&#39;Heterogeneous Treatment Effects&#39;)
ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha=&amp;quot;right&amp;quot;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/causal_trees_48_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;The causal tree is pretty good at detecting the heterogeneous treatment effects for the categorical variables. It only missed the heterogeneity in the third region.&lt;/p&gt;
&lt;p&gt;However, this is also where we expect a tree model to perform particularly well: where the effects are &lt;strong&gt;discrete&lt;/strong&gt;. How well does it do on our continuous variable, time? First, let&amp;rsquo;s again isolate the predicted treatment effects on &lt;code&gt;time&lt;/code&gt; and ignore the other covariates.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def compute_time_effect(df, hte_model, avg_effect_notime):
    df_time = df.copy()
    df_time[[X[1:]] + [&#39;device&#39;, &#39;browser&#39;, &#39;region&#39;]] = 0
    df_time = dgp.add_treatment_effect(df_time)
    df_time[&#39;predicted&#39;] = hte_model.effect(df_time[X]) + avg_effect_notime
    return df_time
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_time_tree = compute_time_effect(df, tree_model, avg_effect_notime_tree)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We now plot the predicted treatment effects against the true ones, along the &lt;code&gt;time&lt;/code&gt; dimension.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sns.scatterplot(x=&#39;time&#39;, y=&#39;effect_on_spend&#39;, data=df_time_tree, label=&#39;True&#39;)
sns.scatterplot(x=&#39;time&#39;, y=&#39;predicted&#39;, data=df_time_tree, label=&#39;Predicted&#39;).set(
    ylabel=&#39;&#39;, title=&#39;Heterogeneous Treatment Effects&#39;)
plt.legend(title=&#39;Effect&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/causal_trees_53_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;From the plot, we can appreciate the discrete nature of causal trees: the model is only able to split the continuous variable into 5 bins. These bins are close to the true treatment effects, but they fail to capture a big chunk of the treatment effect heterogeneity.&lt;/p&gt;
&lt;p&gt;Can these predictions be improved? The answer is yes, and we will explore how in the next post.&lt;/p&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;In this article, we have seen how to use causal trees to estimate &lt;strong&gt;heterogeneous treatment effects&lt;/strong&gt;. The main insight comes from the definition of an auxiliary outcome variable that allows us to frame the inference problem as a prediction problem. While we can then use any algorithm to predict treatment effects, regression trees are particularly useful because of their interpretability, prediction accuracy, and feature of generating prediction as subsample averages.&lt;/p&gt;
&lt;p&gt;The work by &lt;a href=&#34;https://www.pnas.org/doi/abs/10.1073/pnas.1510489113&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Athey and Imbens (2016)&lt;/a&gt; on regression trees to compute heterogeneous treatment effects brought together two separate literatures, causal inference and machine learning in a very fruitful &lt;strong&gt;synergy&lt;/strong&gt;. The causal inference literature (re)discovered the inference benefits of sample splitting, that allows us to do correct inference even when the data partition is complex and hard to analyze. On the other hand, splitting the tree generation phase from the within-leaf prediction phase has strong benefits in terms of prediction accuracy, by safeguarding against overfitting.&lt;/p&gt;
&lt;h3 id=&#34;references&#34;&gt;References&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;S. Athey, G. Imbens, &lt;a href=&#34;https://www.pnas.org/doi/abs/10.1073/pnas.1510489113&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Recursive partitioning for heterogeneous causal effects&lt;/a&gt; (2016), &lt;em&gt;PNAS&lt;/em&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;S. Wager, S. Athey, &lt;a href=&#34;https://www.tandfonline.com/doi/full/10.1080/01621459.2017.1319839&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Estimation and Inference of Heterogeneous Treatment Effects using Random Forests&lt;/a&gt; (2018), &lt;em&gt;Journal of the American Statistical Association&lt;/em&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;S. Athey, J. Tibshirani, S. Wager, &lt;a href=&#34;https://projecteuclid.org/journals/annals-of-statistics/volume-47/issue-2/Generalized-random-forests/10.1214/18-AOS1709.full&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Generalized Random Forests&lt;/a&gt; (2019). &lt;em&gt;The Annals of Statistics&lt;/em&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;M. Oprescu, V. Syrgkanis, Z. Wu, &lt;a href=&#34;http://proceedings.mlr.press/v97/oprescu19a.html?ref=https://githubhelp.com&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Orthogonal Random Forest for Causal Inference&lt;/a&gt; (2019). &lt;em&gt;Proceedings of the 36th International Conference on Machine Learning&lt;/em&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;related-articles&#34;&gt;Related Articles&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://towardsdatascience.com/b63dc69e3d8c&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;DAGs and Control Variables&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://towardsdatascience.com/99bf5cffa0d9&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Matching, Weighting, or Regression?&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://towardsdatascience.com/8a9c1e340832&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Understanding Meta Learners&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://towardsdatascience.com/ed4097dab27a&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Understanding AIPW, the Doubly-Robust Estimator&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;code&#34;&gt;Code&lt;/h3&gt;
&lt;p&gt;You can find the original Jupyter Notebook here:&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/causal_trees.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/causal_trees.ipynb&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Understanding Contamination Bias</title>
      <link>https://matteocourthoud.github.io/post/contamination_bias/</link>
      <pubDate>Mon, 10 Jul 2023 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/post/contamination_bias/</guid>
      <description>&lt;p&gt;&lt;em&gt;Problems and solutions of linear regression with multiple treatments&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;In many causal inference settings, we might be interested in the effect of not just one treatment, but &lt;strong&gt;many mutually exclusive treatments&lt;/strong&gt;. For example, we might want to test alternative UX designs, or drugs, or policies. Depending on the context, there might be many reasons why we want to test different treatments at the same time, but generally it can help &lt;em&gt;reducing the sample size&lt;/em&gt;, as we need just a single control group. A simple way to recover the different treatment effects is a linear regression of the outcome of interest on the different treatment indicators.&lt;/p&gt;
&lt;p&gt;However, in causal inference, we often &lt;strong&gt;condition the analysis&lt;/strong&gt; on other observable variables (often called control variables), either to increase power or, especially in quasi-experimental settings, to identify a causal parameter instead of a simple correlation. There are &lt;a href=&#34;https://towardsdatascience.com/b63dc69e3d8c&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;cases in which adding control variables can backfire&lt;/a&gt;, but otherwise, we usually think that the regression framework is still able to recover the average treatment effect.&lt;/p&gt;
&lt;p&gt;In a breakthrough paper, &lt;a href=&#34;https://www.nber.org/papers/w30108&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Goldsmith-Pinkham, Hull and Kolesár (2022)&lt;/a&gt; have recently shown that in case of &lt;em&gt;multiple and mutually-exclusive&lt;/em&gt; treatments with &lt;em&gt;control variables&lt;/em&gt;, the &lt;strong&gt;regression coefficients do not identify a causal effect&lt;/strong&gt;. However, not everything is lost: the authors propose a simple solution to this problem that still makes use of linear regression.&lt;/p&gt;
&lt;p&gt;In this blog post, I am going to go through a &lt;strong&gt;simple example&lt;/strong&gt; illustrating the nature of the problem and the solution proposed by the authors.&lt;/p&gt;
&lt;h2 id=&#34;multiple-treatments-example&#34;&gt;Multiple Treatments Example&lt;/h2&gt;
&lt;p&gt;Suppose we are an online store and we are not satisfied with our current &lt;em&gt;checkout page&lt;/em&gt;. In particular, we would like to change our &lt;strong&gt;checkout button&lt;/strong&gt; to increase the probability of a purchase. Our UX designer comes up with two alternative checkout buttons, which are displayed below.&lt;/p&gt;
&lt;img src=&#34;fig/buttons.png&#34; width=&#34;800px&#34;/&gt;
&lt;p&gt;In order to understand which button to use, we run an &lt;a href=&#34;https://en.wikipedia.org/wiki/A/B_testing&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;A/B test&lt;/strong&gt;&lt;/a&gt;, or randomized control trial. In particular, when people arrive at the checkout page, we show them one of the three options, at random. Then, for each user, we record the revenue generated which is our outcome of interest.&lt;/p&gt;
&lt;p&gt;I generate a synthetic dataset using &lt;code&gt;dgp_buttons()&lt;/code&gt; from &lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/src/dgp.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;src.dgp&lt;/code&gt;&lt;/a&gt; as data generating process. I also import plotting functions and standard libraries from &lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/src/utils.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;src.utils&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;%matplotlib inline
%config InlineBackend.figure_format = &#39;retina&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from src.utils import *
from src.dgp import dgp_buttons
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;dgp = dgp_buttons()
df = dgp.generate_data()
df.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;group&lt;/th&gt;
      &lt;th&gt;revenue&lt;/th&gt;
      &lt;th&gt;mobile&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;button1&lt;/td&gt;
      &lt;td&gt;8.927335&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;default&lt;/td&gt;
      &lt;td&gt;13.613456&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;button2&lt;/td&gt;
      &lt;td&gt;4.777628&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;default&lt;/td&gt;
      &lt;td&gt;8.909049&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;default&lt;/td&gt;
      &lt;td&gt;10.160347&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;We have information on 2000 users, for which we observe their checkout button (&lt;code&gt;default&lt;/code&gt;, &lt;code&gt;button1&lt;/code&gt; or &lt;code&gt;button2&lt;/code&gt;), the &lt;code&gt;revenue&lt;/code&gt; they generate and whether they connected from desktop or &lt;code&gt;mobile&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;We notice too late that we have a &lt;strong&gt;problem with randomization&lt;/strong&gt;. We showed &lt;code&gt;button1&lt;/code&gt; more frequently to desktop users and &lt;code&gt;button2&lt;/code&gt; more frequently to mobile users. The control group that sees the &lt;code&gt;default&lt;/code&gt; button instead is balanced.&lt;/p&gt;
&lt;img src=&#34;fig/randomization.png&#34; width=&#34;600px&#34;/&gt;
&lt;p&gt;What should we do? What happens if we simply compare &lt;code&gt;revenue&lt;/code&gt; across &lt;code&gt;groups&lt;/code&gt;? Let&amp;rsquo;s do it by regressing &lt;code&gt;revenue&lt;/code&gt; on &lt;code&gt;group&lt;/code&gt; dummy variables.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;smf.ols(&#39;revenue ~ group&#39;, data=df).fit().summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
          &lt;td&gt;&lt;/td&gt;            &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt;        &lt;td&gt;   11.6553&lt;/td&gt; &lt;td&gt;    0.149&lt;/td&gt; &lt;td&gt;   78.250&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;   11.363&lt;/td&gt; &lt;td&gt;   11.948&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;group[T.button1]&lt;/th&gt; &lt;td&gt;   -0.5802&lt;/td&gt; &lt;td&gt;    0.227&lt;/td&gt; &lt;td&gt;   -2.556&lt;/td&gt; &lt;td&gt; 0.011&lt;/td&gt; &lt;td&gt;   -1.026&lt;/td&gt; &lt;td&gt;   -0.135&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;group[T.button2]&lt;/th&gt; &lt;td&gt;   -0.5958&lt;/td&gt; &lt;td&gt;    0.218&lt;/td&gt; &lt;td&gt;   -2.727&lt;/td&gt; &lt;td&gt; 0.006&lt;/td&gt; &lt;td&gt;   -1.024&lt;/td&gt; &lt;td&gt;   -0.167&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;From the regression results we estimate a negative and significant effect for both buttons. Should we believe these estimates? Are they &lt;strong&gt;causal&lt;/strong&gt;?&lt;/p&gt;
&lt;p&gt;It is unlikely that what we have estimated are the true treatment effects. In fact, there might be substantial &lt;strong&gt;differences in purchase attitudes&lt;/strong&gt; between desktop and mobile users. Since we do not have a comparable number of mobile and desktop users across treatment arms, it might be that the observed differences in &lt;code&gt;revenue&lt;/code&gt; are due to the &lt;em&gt;device&lt;/em&gt; used and not the &lt;em&gt;button design&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Because of this, we decide to &lt;strong&gt;condition&lt;/strong&gt; our analysis on the device used and we include the &lt;code&gt;mobile&lt;/code&gt; dummy variable in the regression.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;smf.ols(&#39;revenue ~ group + mobile&#39;, data=df).fit().summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
          &lt;td&gt;&lt;/td&gt;            &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt;        &lt;td&gt;    9.1414&lt;/td&gt; &lt;td&gt;    0.110&lt;/td&gt; &lt;td&gt;   82.905&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    8.925&lt;/td&gt; &lt;td&gt;    9.358&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;group[T.button1]&lt;/th&gt; &lt;td&gt;    0.3609&lt;/td&gt; &lt;td&gt;    0.141&lt;/td&gt; &lt;td&gt;    2.558&lt;/td&gt; &lt;td&gt; 0.011&lt;/td&gt; &lt;td&gt;    0.084&lt;/td&gt; &lt;td&gt;    0.638&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;group[T.button2]&lt;/th&gt; &lt;td&gt;   -1.0326&lt;/td&gt; &lt;td&gt;    0.134&lt;/td&gt; &lt;td&gt;   -7.684&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;   -1.296&lt;/td&gt; &lt;td&gt;   -0.769&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;mobile&lt;/th&gt;           &lt;td&gt;    4.7181&lt;/td&gt; &lt;td&gt;    0.116&lt;/td&gt; &lt;td&gt;   40.691&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    4.491&lt;/td&gt; &lt;td&gt;    4.946&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;Now the coefficient of &lt;code&gt;button1&lt;/code&gt; is positive and significant. Should we recommend its implementation?&lt;/p&gt;
&lt;p&gt;The answer is &lt;strong&gt;surprisingly no&lt;/strong&gt;. &lt;a href=&#34;https://www.nber.org/papers/w30108&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Goldsmith-Pinkham, Hull, Kolesár (2022)&lt;/a&gt; show that this type of regression does not identify the average treatment effect when:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;there are mutually exclusive treatment arms (in our case, &lt;code&gt;groups&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;we are controlling for some variable $X$ (in our case, &lt;code&gt;mobile&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;there treatment effects are heterogeneous in $X$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This is true &lt;strong&gt;even if&lt;/strong&gt; the treatment is &amp;ldquo;as good as random&amp;rdquo; once we condition on $X$.&lt;/p&gt;
&lt;p&gt;Indeed, in our case, the true treatment effects are the ones reported in the following table.&lt;/p&gt;
&lt;img src=&#34;fig/effects.png&#34; width=&#34;500px&#34;/&gt;
&lt;p&gt;The first button has &lt;strong&gt;no effect&lt;/strong&gt; on revenue, irrespectively of the device, while the second button has a &lt;strong&gt;positive effect&lt;/strong&gt; for mobile users and a &lt;strong&gt;negative effect&lt;/strong&gt; for desktop users. Our (wrong) regression specification instead estimates a positive effect of the first button.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s now dig more in detail into the math, to understand why this is happening.&lt;/p&gt;
&lt;h2 id=&#34;theory&#34;&gt;Theory&lt;/h2&gt;
&lt;p&gt;This section borrows heavily from &lt;a href=&#34;https://www.nber.org/papers/w30108&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Goldsmith-Pinkham, Hull, Kolesár (2022)&lt;/a&gt;. For a great summary of the paper, I recommend this excellent Twitter thread by one of the authors, Paul Goldsmith-Pinkham.&lt;/p&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;Economists love using linear regression to estimate treatment effects — it turns out that there are perils to this method, but also amazing perks&lt;br&gt;&lt;br&gt;Come with me in this 🧵 if you want to learn… &lt;a href=&#34;https://t.co/eDsRLkZFZe&#34;&gt;https://t.co/eDsRLkZFZe&lt;/a&gt;&lt;/p&gt;&amp;mdash; Paul Goldsmith-Pinkham (@paulgp) &lt;a href=&#34;https://twitter.com/paulgp/status/1534169803388293120?ref_src=twsrc%5Etfw&#34;&gt;June 7, 2022&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;

&lt;h3 id=&#34;single-treatment-arm&#34;&gt;Single Treatment Arm&lt;/h3&gt;
&lt;p&gt;Suppose we are interested in the effect of a treatment $D$ on an outcome $Y$. First, let&amp;rsquo;s consider the standard case of a &lt;strong&gt;single treatment arm&lt;/strong&gt; so that the treatment variable is binary, $D \in \lbrace 0 , 1 \rbrace$. Also consider a single &lt;strong&gt;binary control variable&lt;/strong&gt; $X \in \lbrace 0 , 1 \rbrace$. We also assume that treatment assignment is as good as random, conditionally on $X$. This means that there might be systematic differences between the treatment and control group, however, these differences are fully accounted for by $X$. Formally we write&lt;/p&gt;
&lt;p&gt;$$
\left( Y_i^{(0)}, Y_i^{(1)} \right) \ \perp \ D_i \ | \ X_i
$$&lt;/p&gt;
&lt;p&gt;Where $Y_i^{(d)}$ denotes the potential outcome of individual $i$ when its treatment status is $d$. For example, $Y_i^{(0)}$ indicates the outcome of individual $i$ in case it is not treated. This notation comes from &lt;a href=&#34;https://www.tandfonline.com/doi/abs/10.1198/016214504000001880&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Rubin&amp;rsquo;s potential outcomes framework&lt;/a&gt;. We can write the &lt;strong&gt;individual treatment effect&lt;/strong&gt; of individual $i$ as&lt;/p&gt;
&lt;p&gt;$$
\tau_i = Y_i^{(1)} - Y_i^{(0)}
$$&lt;/p&gt;
&lt;p&gt;In this setting, the &lt;strong&gt;regression of interest&lt;/strong&gt; is&lt;/p&gt;
&lt;p&gt;$$
Y_i = \alpha + \beta D_i + \gamma X_i + u_i
$$&lt;/p&gt;
&lt;p&gt;The coefficient of interest is $\beta$.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.jstor.org/stable/2998558&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Angrist (1998)&lt;/a&gt; shows that &lt;strong&gt;the regression coefficient $\beta$ identifies the average treatment effect&lt;/strong&gt;. In particular, $\beta$ identifies a weighted average of the within-group $x$ average treatment effect $\tau (x)$ with convex weights. In this particular setting, we can write it as&lt;/p&gt;
&lt;p&gt;$$
\beta = \lambda \tau(0) + (1 - \lambda) \tau(1) \qquad \text{where} \qquad \tau (x) = \mathbb E \big[ Y_i^{(1)} - Y_i^{(0)} \ \big| \ X_i = x \big]
$$&lt;/p&gt;
&lt;p&gt;The weights $\lambda$ and $(1-\lambda)$ are given by the within-group treatment variance. Hence, the OLS estimator gives &lt;strong&gt;less weight to groups where we have less treatment variance&lt;/strong&gt;, i.e., where treatment is more imbalanced. Groups where treatment is distributed 50-50 get the most weight.&lt;/p&gt;
&lt;p&gt;$$
\lambda = \frac{ \text{Var} \big(D_i \ \big| \ X_i = 0 \big) \Pr \big(X_i=0 \big)}{\sum_{x \in \lbrace 0 , 1 \rbrace} \text{Var} \big(D_i \ \big| \ X_i = x \big) \Pr \big( X_i=x \big)} \in [0, 1]
$$&lt;/p&gt;
&lt;p&gt;The weights can be derived using the &lt;a href=&#34;https://towardsdatascience.com/59f801eb3299&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Frisch-Waugh-Lowell theorem&lt;/a&gt; to express $\beta_1$ as the OLS coefficient of a univariate regression of $Y$ on $D_{i, \perp X}$, where $D_{i, \perp X}$ are the residuals from regressing $D$ on $X$. If you are not familiar with the Frisch-Waugh-Lowell theorem, I wrote an &lt;a href=&#34;https://towardsdatascience.com/59f801eb3299&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;introductory blog post here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;$$
\beta_1 = \frac{ \mathbb E \big[ D_{i, \perp X} Y_i \big] }{ \mathbb E \big[ D_{i, \perp X}^2 \big] } =
\underbrace{ \frac{\mathbb E \big[ D_{i, \perp X} Y_i(0) \big]}{\mathbb E \big[ D_{i, \perp X}^2 \big]} } _ {=0} + \frac{\mathbb E \big[ D_{i, \perp X} D_i \tau_i \big]}{\mathbb E \big[ D_{i, \perp X}^2 \big]} =
\frac{\mathbb E \big[ \text{Var} (D_i | X_i) \ \tau(X_i) \big]}{\mathbb E \big[ \text{Var}(D_i | X_i) \big]}
$$&lt;/p&gt;
&lt;p&gt;The first term of the central expression disappears because the residual $D_{i, \perp X}$ is by construction &lt;strong&gt;mean independent&lt;/strong&gt; of the control variable $X_i$, i.e.&lt;/p&gt;
&lt;p&gt;$$
\mathbb E \big[ D_{i, \perp X} | X_i \big] = 0
$$&lt;/p&gt;
&lt;p&gt;This mean independence property is crucial to obtain an unbiased estimate and its failure in the multiple-treatment case is the source of the &lt;em&gt;contamination bias&lt;/em&gt;.&lt;/p&gt;
&lt;h3 id=&#34;multiple-treatment-arms&#34;&gt;Multiple Treatment Arms&lt;/h3&gt;
&lt;p&gt;Let&amp;rsquo;s now consider the case of multiple treatment arms, $D \in \lbrace 0, 1, 2 \rbrace$, where $1$ and $2$ indicate two mutually-exclusive treatments. We still assume &lt;strong&gt;conditional ignorability&lt;/strong&gt;, i.e., treatment assignment is as good as random, conditional on $X$.&lt;/p&gt;
&lt;p&gt;$$
\left( Y_i^{(0)}, Y_i^{(1)}, Y_i^{(2)} \right) \ \perp \ D_i \ | \ X_i
$$&lt;/p&gt;
&lt;p&gt;In this case, we have two different &lt;strong&gt;individual treatment effects&lt;/strong&gt;, one per treatment.&lt;/p&gt;
&lt;p&gt;$$
\tau_{i1} = Y_i^{(1)} - Y_i^{(0)} \qquad \text{and} \qquad \tau_{i2} = Y_i^{(2)} - Y_i^{(0)}
$$&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;regression of interest&lt;/strong&gt; is&lt;/p&gt;
&lt;p&gt;$$
Y_i = \alpha + \beta_1 D_{i1} + \beta_2 D_{i2} + \gamma X_i + u_i
$$&lt;/p&gt;
&lt;p&gt;Does the OLS estimator of $\beta_1$ and $\beta_2$ &lt;strong&gt;identify&lt;/strong&gt; an average treatment effect?&lt;/p&gt;
&lt;p&gt;It would be very tempting to say yes. In fact, it looks like not much has changed with respect to the previous setup. We just have one extra treatment, but the potential outcomes are still conditionally independent of it. Where is the &lt;strong&gt;issue&lt;/strong&gt;?&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s concentrate on $\beta_1$ (the same applies to $\beta_2$). As before, can rewrite $\beta_1$ using the &lt;a href=&#34;https://towardsdatascience.com/59f801eb3299&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Frisch-Waugh-Lowell theorem&lt;/a&gt; as the OLS coefficient of a univariate regression of $Y_i$ on $D_{i1, \perp X, D_2}$, where $D_{i1, \perp X, D_2}$ are the residuals from regressing $D_1$ on $D_2$ and $X$.&lt;/p&gt;
&lt;p&gt;$$
\beta_1 = \frac{ \mathbb E \big[D_{i1, \perp X, D_2} Y_i \big] }{ \mathbb E \big[ D_{i1, \perp X, D_2}^2 \big]} = \underbrace{ \frac{ \mathbb E \big[ D_{i1, \perp X, D_2} Y_i(0) \big] }{\mathbb E \big[ D_{i1, \perp X, D_2}^2 \big]} } _ {=0} + \frac{ \mathbb E \big[ D_{i1, \perp X, D_2} D_{i1} \tau_{i1} \big] }{ \mathbb E \big[ D_{i1, \perp X, D_2}^2 \big]} + \color{red}{ \underbrace{ \color{black}{ \frac{ \mathbb E \big[ D_{i1, \perp X, D_2} D_{i2} \tau_{i2} \big] }{ \mathbb E \big[ D_{i1, \perp X, D_2}^2 \big]}} } _ { \neq 0} }
$$&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;problem&lt;/strong&gt; is the last term. Without the last term, we could still write $\beta_1$ as a convex combination of the individual treatment effects. However, the last term biases the estimator by adding a component that depends on &lt;strong&gt;the treatment effect of $D_2$&lt;/strong&gt;, $\tau_2$. Why does this term not disappear?&lt;/p&gt;
&lt;p&gt;The problem is that $D_{i1, \perp X, D_2}$ is not mean independent of $D_{i2}$, i.e.&lt;/p&gt;
&lt;p&gt;$$
\mathbb E \big[ D_{i1, \perp X, D_2} D_{i2} \ \big| \ X_i \big] \neq 0
$$&lt;/p&gt;
&lt;p&gt;The reason lies in the fact that the treatments are &lt;strong&gt;mutually exclusive&lt;/strong&gt;. This implies that when $D_{i1}=1$, $D_{i2}$ must be zero, regardless of the value of $X_i$. Therefore, the last term does not cancel out and it introduces a &lt;strong&gt;contamination bias&lt;/strong&gt;.&lt;/p&gt;
&lt;h3 id=&#34;solution&#34;&gt;Solution&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://www.nber.org/papers/w30108&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Goldsmith-Pinkham, Hull, Kolesár (2022)&lt;/a&gt; show that a &lt;strong&gt;simple estimator&lt;/strong&gt;, first proposed by &lt;a href=&#34;https://www.aeaweb.org/articles?id=10.1257/jel.47.1.5&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Imbens and Wooldridge (2009)&lt;/a&gt;, is able to remove the bias. The procedure is the following.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;De-mean the control variable: $\tilde X = X - \bar X$&lt;/li&gt;
&lt;li&gt;Regress $Y$ on the interaction between the treatment indicators $D$ and the demeaned control variable $\tilde X$&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The OLS estimators of $\beta_1$ and $\beta_2$ are &lt;strong&gt;unbiased&lt;/strong&gt; estimators of the average treatment effects. It also just requires a linear regression. Moreover, this estimator is unbiased also for continuous control variables $X$, not only for a binary one as we have considered so far.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Why&lt;/strong&gt; was this estimator initially proposed by &lt;a href=&#34;https://www.aeaweb.org/articles?id=10.1257/jel.47.1.5&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Imbens and Wooldridge (2009)&lt;/a&gt;? Let&amp;rsquo;s analyze two parts separately: the interaction term between $D$ and $X$ and the fact that $X$ is de-meaned in the interaction term.&lt;/p&gt;
&lt;p&gt;First, the &lt;strong&gt;interaction term&lt;/strong&gt; $D X$ allows us to control for different effects and/or distributions of $X$ across treatment and control group.&lt;/p&gt;
&lt;p&gt;Second, &lt;strong&gt;de-meaning&lt;/strong&gt; $X$ in the interaction term allows us to &lt;strong&gt;interpret&lt;/strong&gt; the estimated coefficient $\hat \beta$ as the average treatment effect. In fact, assume we were estimating the following linear model, where $X$ is &lt;em&gt;not&lt;/em&gt; de-meaned in the interaction term.&lt;/p&gt;
&lt;p&gt;$$
Y_i = \alpha + \beta D_i + \gamma X_i + \delta D_i X_i + u_i
$$&lt;/p&gt;
&lt;p&gt;In this case, the marginal effect of $D$ on $Y$ is $\beta + \delta X_i$ so that the &lt;em&gt;average&lt;/em&gt; marginal effect is $\beta + \delta \bar X$ which is different from $\beta$.&lt;/p&gt;
&lt;p&gt;If instead we use the de-meaned value of $X$ in the interaction term, the marginal effect of $D$ on $Y$ is $\beta + \delta (X_i - \bar X)$ so that the &lt;em&gt;average&lt;/em&gt; marginal effect is $\beta + \delta (\bar X - \bar X) = \beta$. Now we can interpret $\beta$ as the average marginal effect of $D$ on $Y$.&lt;/p&gt;
&lt;h2 id=&#34;simulations&#34;&gt;Simulations&lt;/h2&gt;
&lt;p&gt;In order to better understand both the problem and the solution, let&amp;rsquo;s run some &lt;strong&gt;simulations&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;We run an estimator over different draws from the data generating process &lt;code&gt;dgp_buttons()&lt;/code&gt;. This is only possible with synthetic data and we do not have this luxury in reality. For each sample, we record the estimated coefficient and the corresponding &lt;a href=&#34;https://en.wikipedia.org/wiki/P-value&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;p-value&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def simulate(dgp, estimator, K=1000):
    
    # Initialize coefficients
    results = pd.DataFrame({&#39;Coefficient&#39;: np.zeros(K), &#39;pvalue&#39;: np.zeros(K)})
    
    # Compute coefficients
    for k in range(K):
        df = dgp.generate_data(seed=k)
        results.Coefficient[k] = estimator(df).params[1]
        results.pvalue[k] = estimator(df).pvalues[1]
    
    results[&#39;Significant&#39;] = results[&#39;pvalue&#39;] &amp;lt; 0.05
    return results
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;First, let&amp;rsquo;s try it with the old estimator that regresses &lt;code&gt;revenue&lt;/code&gt; on both &lt;code&gt;group&lt;/code&gt; and &lt;code&gt;mobile&lt;/code&gt; dummy variables.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;ols_estimator = lambda x: smf.ols(&#39;revenue ~ group + mobile&#39;, data=x).fit()
results = simulate(dgp, ols_estimator)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I &lt;strong&gt;plot&lt;/strong&gt; the distribution of the coefficient estimates of &lt;code&gt;button1&lt;/code&gt; over 1000 simulations, highlighting the statistically significant ones at the 5% level. I also highlight the true value of the coefficient, zero, with a vertical dotted bar.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def plot_results(results):
    p_sig = sum(results[&#39;Significant&#39;]) / len(results) * 100
    sns.histplot(data=results, x=&amp;quot;Coefficient&amp;quot;, hue=&amp;quot;Significant&amp;quot;, multiple=&amp;quot;stack&amp;quot;, 
                 palette = [&#39;tab:red&#39;, &#39;tab:green&#39;]);
    plt.axvline(x=0, c=&#39;k&#39;, ls=&#39;--&#39;, label=&#39;truth&#39;)
    plt.title(rf&amp;quot;Estimated $\beta_1$ ({p_sig:.0f}% significant)&amp;quot;);
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plot_results(results)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/contamination_bias_35_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;As we can see, we reject the null hypothesis of no effect of &lt;code&gt;button1&lt;/code&gt; in 45% of the simulations. Since we set a confidence level of 5%, we would have expected at most around 5% of rejections. Our estimator is &lt;strong&gt;biased&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;As we have seen above, the problem is that the estimator is not just a convex combination of the effect of &lt;code&gt;button1&lt;/code&gt; across mobile and desktop users (it&amp;rsquo;s zero for both), but it is &lt;strong&gt;contaminated&lt;/strong&gt; by the effect of &lt;code&gt;button2&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s now try the estimator proposed from &lt;a href=&#34;https://www.aeaweb.org/articles?id=10.1257/jel.47.1.5&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Imbens and Wooldridge (2009)&lt;/a&gt;. First, we need do de-mean our control variable, &lt;code&gt;mobile&lt;/code&gt;. Then, we regress &lt;code&gt;revenue&lt;/code&gt; on the interaction between &lt;code&gt;group&lt;/code&gt; and the de-meaned control variable, &lt;code&gt;mobile_res&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df[&#39;mobile_res&#39;] = df[&#39;mobile&#39;] - np.mean(df[&#39;mobile&#39;])
smf.ols(&#39;revenue ~ group * mobile_res&#39;, data=df).fit().summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
               &lt;td&gt;&lt;/td&gt;                  &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt;                   &lt;td&gt;   11.5773&lt;/td&gt; &lt;td&gt;    0.067&lt;/td&gt; &lt;td&gt;  172.864&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;   11.446&lt;/td&gt; &lt;td&gt;   11.709&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;group[T.button1]&lt;/th&gt;            &lt;td&gt;    0.0281&lt;/td&gt; &lt;td&gt;    0.106&lt;/td&gt; &lt;td&gt;    0.266&lt;/td&gt; &lt;td&gt; 0.790&lt;/td&gt; &lt;td&gt;   -0.180&lt;/td&gt; &lt;td&gt;    0.236&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;group[T.button2]&lt;/th&gt;            &lt;td&gt;   -1.5071&lt;/td&gt; &lt;td&gt;    0.100&lt;/td&gt; &lt;td&gt;  -15.112&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;   -1.703&lt;/td&gt; &lt;td&gt;   -1.311&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;mobile_res&lt;/th&gt;                  &lt;td&gt;    2.9107&lt;/td&gt; &lt;td&gt;    0.134&lt;/td&gt; &lt;td&gt;   21.715&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    2.648&lt;/td&gt; &lt;td&gt;    3.174&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;group[T.button1]:mobile_res&lt;/th&gt; &lt;td&gt;    0.1605&lt;/td&gt; &lt;td&gt;    0.211&lt;/td&gt; &lt;td&gt;    0.760&lt;/td&gt; &lt;td&gt; 0.448&lt;/td&gt; &lt;td&gt;   -0.254&lt;/td&gt; &lt;td&gt;    0.575&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;group[T.button2]:mobile_res&lt;/th&gt; &lt;td&gt;    5.3771&lt;/td&gt; &lt;td&gt;    0.200&lt;/td&gt; &lt;td&gt;   26.905&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    4.985&lt;/td&gt; &lt;td&gt;    5.769&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;The estimated coefficients are now &lt;strong&gt;close to their true values&lt;/strong&gt;. The estimated coefficient for &lt;code&gt;button1&lt;/code&gt; is not significant, while the estimated coefficient for &lt;code&gt;button2&lt;/code&gt; is negative and significant.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s check whether this results holds &lt;strong&gt;across samples&lt;/strong&gt; by running a simulation. We repeat the estimation procedure 1000 times and we plot the distribution of estimated coefficients for &lt;code&gt;button1&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;new_estimator = lambda x: smf.ols(&#39;revenue ~ group * mobile&#39;, data=x).fit()
new_results = simulate(dgp, new_estimator)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plot_results(new_results)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/contamination_bias_40_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Now the distribution of the estimated coefficient for &lt;code&gt;button1&lt;/code&gt; is centered around the true value of zero. Moreover, we reject the null hypothesis of no effect only in 1% of the simulations, consistently with the chosen confidence level of 95%.&lt;/p&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;In this post, we have seen the dangers of running a factor regression model with multiple &lt;em&gt;mutually exclusive&lt;/em&gt; treatment arms and treatment effect heterogeneity across a control variable. In this case, because the treatments are not independent, the regression coefficients are not a convex combination of the within-group average treatment effects, but also capture the treatment effects of the other treatments introducing &lt;strong&gt;contamination bias&lt;/strong&gt;. The solution to the problem is both simple and elegant, requiring just a linear regression.&lt;/p&gt;
&lt;p&gt;However, &lt;strong&gt;the problem is more general&lt;/strong&gt; than this setting and generally concerns every setting in which (all of the following)&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;We have multiple treatments that depend on each other&lt;/li&gt;
&lt;li&gt;We need to condition the analysis on a control variable&lt;/li&gt;
&lt;li&gt;The treatment effects are heterogeneous in the control variable&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Another popular example is the case of the &lt;a href=&#34;https://arxiv.org/abs/2201.01194&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Two-Way Fixed Effects (TWFE) estimator with staggered treatments&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;references&#34;&gt;References&lt;/h3&gt;
&lt;p&gt;[1] J. Angrist, &lt;a href=&#34;https://www.jstor.org/stable/2998558&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Estimating the Labor Market Impact of Voluntary Military Service Using Social Security Data on Military Applicants&lt;/a&gt; (1998), &lt;em&gt;Econometrica&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;[2] D. Rubin, &lt;a href=&#34;https://www.tandfonline.com/doi/abs/10.1198/016214504000001880&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Causal Inference Using Potential Outcomes&lt;/a&gt; (2005), &lt;em&gt;Journal of the American Statistical Association&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;[3] G. Imbens, J. Wooldridge, &lt;a href=&#34;https://www.aeaweb.org/articles?id=10.1257/jel.47.1.5&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Recent Developments in the Econometrics of Program Evaluation&lt;/a&gt; (2009), &lt;em&gt;Journal of Economic Literature&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;[4] P. Goldsmith-Pinkham, P. Hull, M. Kolesár, &lt;a href=&#34;https://www.nber.org/papers/w30108&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Contamination Bias in Linear Regressions&lt;/a&gt; (2022), &lt;em&gt;working paper&lt;/em&gt;.&lt;/p&gt;
&lt;h3 id=&#34;related-articles&#34;&gt;Related Articles&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/344ac1477699&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Understanding Omitted Variable Bias&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/59f801eb3299&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Understanding The Frisch-Waugh-Lovell Theorem&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/b63dc69e3d8c&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;DAGs and Control Variables&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;code&#34;&gt;Code&lt;/h3&gt;
&lt;p&gt;You can find the original Jupyter Notebook here:&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/cbias.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/cbias.ipynb&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Understanding CUPED</title>
      <link>https://matteocourthoud.github.io/post/cuped/</link>
      <pubDate>Mon, 10 Jul 2023 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/post/cuped/</guid>
      <description>&lt;p&gt;&lt;em&gt;An in depth guide to the state-of-the art variance reduction technique in A/B tests&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;During my PhD, I spent a lot of time learning and applying causal inference methods to experimental and observational data. However, I was completely clueless when I first heard of &lt;strong&gt;CUPED&lt;/strong&gt; (Controlled-Experiment using Pre-Experiment Data), a technique to increase the power of randomized controlled trials in A/B tests.&lt;/p&gt;
&lt;p&gt;What really amazed me was the popularity of the algorithm in the industry. CUPED was first introduced by Microsoft researchers &lt;a href=&#34;https://dl.acm.org/doi/abs/10.1145/2433396.2433413&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Deng, Xu, Kohavi, Walker (2013)&lt;/a&gt; and has been widely used in companies such as &lt;a href=&#34;https://www.kdd.org/kdd2016/papers/files/adp0945-xieA.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Netflix&lt;/a&gt;, &lt;a href=&#34;https://booking.ai/995d186fff1d&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Booking&lt;/a&gt;, &lt;a href=&#34;https://research.facebook.com/blog/2020/10/increasing-the-sensitivity-of-a-b-tests-by-utilizing-the-variance-estimates-of-experimental-units/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Meta&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/abs/2112.13299&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Airbnb&lt;/a&gt;, &lt;a href=&#34;https://www.tripadvisor.com/engineering/reducing-a-b-test-measurement-variance-by-30/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;TripAdvisor&lt;/a&gt;, &lt;a href=&#34;https://doordash.engineering/2020/10/07/improving-experiment-capacity-by-4x/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;DoorDash&lt;/a&gt;, &lt;a href=&#34;https://craft.faire.com/how-to-speed-up-your-a-b-test-outlier-capping-and-cuped-8c9df21c76b&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Faire&lt;/a&gt;, and many others. While digging deeper into it, I noticed a similarity with some causal inference methods I was familiar with, such as &lt;a href=&#34;https://diff.healthpolicydatascience.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Difference-in-Differences&lt;/a&gt;. I was curious and decided to dig deeper.&lt;/p&gt;
&lt;p&gt;In this post, I will present CUPED and try to compare it against other causal inference methods.&lt;/p&gt;
&lt;h2 id=&#34;example&#34;&gt;Example&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s assume we are a firm that is testing an &lt;strong&gt;ad campaign&lt;/strong&gt; and we are interested in understanding whether it increases revenue or not. We randomly split a set of users into a treatment and control group and we show the ad campaign to the treatment group. Differently from the standard A/B test setting, assume we observe users also before the test.&lt;/p&gt;
&lt;p&gt;We can now generate the simulated data, using the data generating process &lt;code&gt;dgp_cuped()&lt;/code&gt; from &lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/src/dgp.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;src.dgp&lt;/code&gt;&lt;/a&gt;. I also import some plotting functions and libraries from &lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/src/utils.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;src.utils&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;%matplotlib inline
%config InlineBackend.figure_format = &#39;retina&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from src.utils import *
from src.dgp import dgp_cuped
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df = dgp_cuped().generate_data()
df.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;i&lt;/th&gt;
      &lt;th&gt;ad_campaign&lt;/th&gt;
      &lt;th&gt;revenue0&lt;/th&gt;
      &lt;th&gt;revenue1&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;5.315635&lt;/td&gt;
      &lt;td&gt;8.359304&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;2.977799&lt;/td&gt;
      &lt;td&gt;7.751485&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;4.693796&lt;/td&gt;
      &lt;td&gt;9.025253&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;5.827975&lt;/td&gt;
      &lt;td&gt;8.540667&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;5.230095&lt;/td&gt;
      &lt;td&gt;8.910165&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;We have informations on 1000 individuals indexed by &lt;code&gt;i&lt;/code&gt; for whom we observe the revenue generated pre and post treatment, &lt;code&gt;revenue0&lt;/code&gt; and &lt;code&gt;revenue1&lt;/code&gt; respectively, and whether they have been exposed to the &lt;code&gt;ad_campaign&lt;/code&gt;.&lt;/p&gt;
&lt;h3 id=&#34;difference-in-means&#34;&gt;Difference in Means&lt;/h3&gt;
&lt;p&gt;In randomized experiments or A/B tests, &lt;strong&gt;randomization&lt;/strong&gt; allows us to estimate the average treatment effect using a simple difference in means. We can just compare the average outcome post-treatment $Y_1$ (&lt;code&gt;revenue&lt;/code&gt;) across control and treated units and randomization guarantees that this difference is due to the treatment alone, in expectation.&lt;/p&gt;
&lt;p&gt;$$
\widehat {ATE}^{simple} = \bar Y_{t=1, d=1} - \bar Y_{t=1, d=0}
$$&lt;/p&gt;
&lt;p&gt;Where the bar indicates the average over individuals. In our case, we compute the average revenue post ad campaign in the treatment group, minus the average revenue post ad campaign in the control group.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;np.mean(df.loc[df.ad_campaign==True, &#39;revenue1&#39;]) - np.mean(df.loc[df.ad_campaign==False, &#39;revenue1&#39;])
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;1.7914301325347406
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The estimated treatment effect is 1.79, very close to the &lt;strong&gt;true value&lt;/strong&gt; of 2. We can obtain the same estimate by regressing the post-treatment outcome &lt;code&gt;revenue1&lt;/code&gt; on the treatment indicator &lt;code&gt;ad_campaign&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;$$
Y_{i, t=1} = \alpha + \beta D_i + \varepsilon_i
$$&lt;/p&gt;
&lt;p&gt;Where $\beta$ is the coefficient of interest.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;smf.ols(&#39;revenue1 ~ ad_campaign&#39;, data=df).fit().summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
       &lt;td&gt;&lt;/td&gt;          &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt;   &lt;td&gt;    8.2995&lt;/td&gt; &lt;td&gt;    0.211&lt;/td&gt; &lt;td&gt;   39.398&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    7.881&lt;/td&gt; &lt;td&gt;    8.718&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;ad_campaign&lt;/th&gt; &lt;td&gt;    1.7914&lt;/td&gt; &lt;td&gt;    0.301&lt;/td&gt; &lt;td&gt;    5.953&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    1.194&lt;/td&gt; &lt;td&gt;    2.389&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;This estimator is &lt;strong&gt;unbiased&lt;/strong&gt;, which means it delivers the correct estimate, on average. However, it can still be improved: we could &lt;strong&gt;decrease its variance&lt;/strong&gt;. Decreasing the variance of an estimator is extremely important since it allows us to&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;detect smaller effects&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;detect the same effect, but with a &lt;strong&gt;smaller sample size&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In general, an estimator with a smaller variance allows us to run tests with higher &lt;a href=&#34;https://en.wikipedia.org/wiki/Power_of_a_test&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;power&lt;/strong&gt;&lt;/a&gt;, i.e. ability to detect smaller effects.&lt;/p&gt;
&lt;p&gt;Can we improve the power of our AB test? Yes, with CUPED (among other methods).&lt;/p&gt;
&lt;h2 id=&#34;cuped&#34;&gt;CUPED&lt;/h2&gt;
&lt;p&gt;The &lt;strong&gt;idea&lt;/strong&gt; of CUPED is the following. Suppose you are running an AB test and $Y$ is the outcome of interest (&lt;code&gt;revenue&lt;/code&gt; in our example) and the binary variable $D$ indicates whether a single individual has been treated or not (&lt;code&gt;ad_campaign&lt;/code&gt; in our example).&lt;/p&gt;
&lt;p&gt;Suppose you have access to another random variable $X$ which is &lt;strong&gt;not affected&lt;/strong&gt; by the treatment and has known expectation $\mathbb E[X]$. Then define&lt;/p&gt;
&lt;p&gt;$$
\hat Y^{cuped}_{1} = \bar Y_1 - \theta \bar X + \theta \mathbb E [X]
$$&lt;/p&gt;
&lt;p&gt;where $\theta$ is a scalar. This estimator is an &lt;strong&gt;unbiased&lt;/strong&gt; estimator for $\mathbb E[Y]$ since in expectation the two last terms cancel out. However, the variance of $\hat Y^{cuped}_{1}$ is&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
\text{Var} \left( \hat Y^{cuped}_{1} \right) &amp;amp;= \text{Var} \left( \bar Y_1 - \theta \bar X + \theta \mathbb E [X] \right) = \newline
&amp;amp;= \text{Var} \left( Y_1 - \theta X \right) / n = \newline
&amp;amp;= \Big( \text{Var} (Y_1) + \theta^2 \text{Var} (X) - 2 \theta \text{Cov} (X,Y) \Big) / n
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;Note that the variance of $\hat Y^{cuped}_{1}$ is minimized for&lt;/p&gt;
&lt;p&gt;$$
\theta^* = \frac{\text{Cov} (X,Y)}{\text{Var} (X)}
$$&lt;/p&gt;
&lt;p&gt;Which is the &lt;strong&gt;OLS&lt;/strong&gt; estimator of a linear regression of $Y$ on $X$. Substituting $\theta^*$ into the formula of the variance of $\hat Y^{cuped}_{1}$ we obtain&lt;/p&gt;
&lt;p&gt;$$
\text{Var} \left( \hat Y^{cuped}_{1} \right) = \text{Var} (\bar Y) (1 - \rho^2)
$$&lt;/p&gt;
&lt;p&gt;where $\rho$ is the &lt;strong&gt;correlation&lt;/strong&gt; between $Y$ and $X$. Therefore, the higher the correlation between $Y$ and $X$, the higher the variance reduction of CUPED.&lt;/p&gt;
&lt;p&gt;We can then &lt;strong&gt;estimate the average treatment effect&lt;/strong&gt;as the average difference in the transformed outcome between the control and treatment group.&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
\widehat {ATE}^{cuped} &amp;amp;= \hat Y^{cuped}&lt;em&gt;{1} (D=1) - \hat Y^{cuped}&lt;/em&gt;{1}(D=0) = \newline
&amp;amp;= \big( \bar Y_1 - \theta \bar X + \theta \mathbb E [X] \ \big| \ D = 1 \big) - \big( \bar Y_1 - \theta \bar X + \theta \mathbb E [X] \ \big| \ D = 0 \big) = \newline
&amp;amp;= \big( \bar Y_1 - \theta \bar X \ \big| \ D = 1 \big) - \big( \bar Y_1 - \theta \bar X \ \big| \ D = 0 \big)
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;Note that $\mathbb E[X]$ cancels out when taking the difference. Therefore, it is sufficient to compute&lt;/p&gt;
&lt;p&gt;$$
\hat Y_{cuped,1}&amp;rsquo; = \bar Y_1 - \theta \bar X
$$&lt;/p&gt;
&lt;p&gt;This is not an unbiased estimator of $\mathbb E[Y]$ but still delivers an unbiased estimator of the average treatment effect.&lt;/p&gt;
&lt;h3 id=&#34;optimal-x&#34;&gt;Optimal X&lt;/h3&gt;
&lt;p&gt;What is the &lt;strong&gt;optimal choice&lt;/strong&gt; for the control variable $X$?&lt;/p&gt;
&lt;p&gt;We know that $X$ should have the following &lt;strong&gt;properties&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;not affected by the treatment&lt;/li&gt;
&lt;li&gt;as correlated with $Y_1$ as possible&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The authors of the paper suggest using the &lt;strong&gt;pre-treatment outcome&lt;/strong&gt; $Y_{0}$ since it gives the most reduction in variance in practice.&lt;/p&gt;
&lt;p&gt;Therefore, &lt;strong&gt;in practice&lt;/strong&gt;, we can compute the CUPED estimate of the average treatment effect as follows:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Regress $Y_1$ on $Y_0$ and estimate $\hat \theta$&lt;/li&gt;
&lt;li&gt;Compute $\hat Y^{cuped}_{1} = \bar Y_1 - \hat \theta \bar Y_0$&lt;/li&gt;
&lt;li&gt;Compute the difference of $\hat Y^{cuped}_{1}$ between treatment and control group&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Equivalently, we can compute $\hat Y^{cuped}_{1}$ at the individual level and then regress it on the treatment dummy variable $D$.&lt;/p&gt;
&lt;h3 id=&#34;back-to-the-data&#34;&gt;Back To The Data&lt;/h3&gt;
&lt;p&gt;Let&amp;rsquo;s compute the CUPED estimate for the treatment effect, one step at the time. First, let&amp;rsquo;s estimate $\theta$.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;theta = smf.ols(&#39;revenue1 ~ revenue0&#39;, data=df).fit().params[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we can compute the transformed outcome $\hat Y^{cuped}_{1}$.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df[&#39;revenue1_cuped&#39;] = df[&#39;revenue1&#39;] - theta * (df[&#39;revenue0&#39;] - np.mean(df[&#39;revenue0&#39;]))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Lastly, we estimate the treatment effect as a difference in means, with the transformed outcome $\hat Y^{cuped}_{1}$.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;smf.ols(&#39;revenue1_cuped ~ ad_campaign&#39;, data=df).fit().summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
       &lt;td&gt;&lt;/td&gt;          &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt;   &lt;td&gt;    8.2259&lt;/td&gt; &lt;td&gt;    0.143&lt;/td&gt; &lt;td&gt;   57.677&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    7.943&lt;/td&gt; &lt;td&gt;    8.509&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;ad_campaign&lt;/th&gt; &lt;td&gt;    1.9415&lt;/td&gt; &lt;td&gt;    0.204&lt;/td&gt; &lt;td&gt;    9.529&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    1.537&lt;/td&gt; &lt;td&gt;    2.346&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;The standard error is 33% smaller!&lt;/p&gt;
&lt;h3 id=&#34;equivalent-formulation&#34;&gt;Equivalent Formulation&lt;/h3&gt;
&lt;p&gt;An alternative but algebraically &lt;strong&gt;equivalent&lt;/strong&gt; way of obtaining the CUPED estimate is the folowing&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Regress $Y_1$ on $Y_0$ and compute the residuals $\tilde Y_1$&lt;/li&gt;
&lt;li&gt;Compute $\hat Y^{cuped}_{1} = \tilde Y_1 + \bar Y_1$&lt;/li&gt;
&lt;li&gt;Compute the difference of $\hat Y^{cuped}_{1}$ between treatment and control group&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Step (3) is the same as before but (1) and (2) are different. This procedure is called &lt;strong&gt;partialling out&lt;/strong&gt; and the algebraic equivalence is guaranteed by the &lt;a href=&#34;https://towardsdatascience.com/59f801eb3299&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Frisch-Waugh-Lowell Theorem&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s check if we indeed obtain the same result.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df[&#39;revenue1_tilde&#39;] = smf.ols(&#39;revenue1 ~ revenue0&#39;, data=df).fit().resid + np.mean(df[&#39;revenue1&#39;])
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;smf.ols(&#39;revenue1_tilde ~ ad_campaign&#39;, data=df).fit().summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
       &lt;td&gt;&lt;/td&gt;          &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt;   &lt;td&gt;    8.2259&lt;/td&gt; &lt;td&gt;    0.143&lt;/td&gt; &lt;td&gt;   57.677&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    7.943&lt;/td&gt; &lt;td&gt;    8.509&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;ad_campaign&lt;/th&gt; &lt;td&gt;    1.9415&lt;/td&gt; &lt;td&gt;    0.204&lt;/td&gt; &lt;td&gt;    9.529&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    1.537&lt;/td&gt; &lt;td&gt;    2.346&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;Yes! The regression table is exactly identical.&lt;/p&gt;
&lt;h2 id=&#34;cuped-vs-other&#34;&gt;CUPED vs Other&lt;/h2&gt;
&lt;p&gt;CUPED seems to be a very powerful procedure but it is remindful of at least a couple of other methods.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Autoregression or regression with control variables&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://diff.healthpolicydatascience.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Difference-in-Differences&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Are these methods the same or is there a difference? Let&amp;rsquo;s check.&lt;/p&gt;
&lt;h3 id=&#34;autoregression&#34;&gt;Autoregression&lt;/h3&gt;
&lt;p&gt;The &lt;strong&gt;first question&lt;/strong&gt; that came to my mind when I first saw CUPED was &amp;ldquo;&lt;em&gt;is CUPED just the simple difference with an additional control variable?&lt;/em&gt;&amp;rdquo;. Or equivalently, is CUPED equivalent to running the following regression&lt;/p&gt;
&lt;p&gt;$$
Y_{i, t=1} = \alpha + \beta D_i + \gamma Y_{i, t=0} + \varepsilon_i
$$&lt;/p&gt;
&lt;p&gt;and estimating $\gamma$ via least squares?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;smf.ols(&#39;revenue1 ~ revenue0 + ad_campaign&#39;, data=df).fit().summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
       &lt;td&gt;&lt;/td&gt;          &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt;   &lt;td&gt;    1.9939&lt;/td&gt; &lt;td&gt;    0.603&lt;/td&gt; &lt;td&gt;    3.304&lt;/td&gt; &lt;td&gt; 0.001&lt;/td&gt; &lt;td&gt;    0.796&lt;/td&gt; &lt;td&gt;    3.192&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;revenue0&lt;/th&gt;    &lt;td&gt;    1.2249&lt;/td&gt; &lt;td&gt;    0.114&lt;/td&gt; &lt;td&gt;   10.755&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    0.999&lt;/td&gt; &lt;td&gt;    1.451&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;ad_campaign&lt;/th&gt; &lt;td&gt;    1.9519&lt;/td&gt; &lt;td&gt;    0.205&lt;/td&gt; &lt;td&gt;    9.529&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    1.545&lt;/td&gt; &lt;td&gt;    2.358&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;The estimated coefficient is very similar to the one we obtained with CUPED and also the standard error is very close. However, they are not exactly the same.&lt;/p&gt;
&lt;p&gt;If you are familiar with the &lt;a href=&#34;https://towardsdatascience.com/59f801eb3299&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Frisch-Waugh-Lowell Theorem&lt;/a&gt;, you might wonder why the two procedures are &lt;strong&gt;not equivalent&lt;/strong&gt;. The reason is that with CUPED we are partialling out only $Y$, while the FWL theorem holds when we are partialling out either X or both X and Y.&lt;/p&gt;
&lt;h3 id=&#34;diff-in-diffs&#34;&gt;Diff-in-Diffs&lt;/h3&gt;
&lt;p&gt;The &lt;strong&gt;second question&lt;/strong&gt; that came to my mind was &amp;ldquo;i&lt;em&gt;s CUPED just difference-in-differences?&lt;/em&gt;&amp;rdquo;. &lt;a href=&#34;https://diff.healthpolicydatascience.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Difference-in-Differences&lt;/a&gt; (or diff-in-diffs, or DiD) is an estimator that computes the treatment effect as a &lt;strong&gt;double-difference&lt;/strong&gt; instead of a single one: pre-post and treatment-control instead of just treatment-control.&lt;/p&gt;
&lt;p&gt;$$
\widehat {ATE}^{DiD} = \big( \bar Y_{t=1, d=1} - \bar Y_{t=1, d=0} \big) - \big( \bar Y_{t=0, d=1} - \bar Y_{t=0, d=0} \big)
$$&lt;/p&gt;
&lt;p&gt;This method was initially introduced in the 19th century to estimate the causes of a Cholera epidemic in London. The main advantage of diff-in-diff is that it allows to estimate the average treatment effect also when randomization is not perfect and the treatment and control group are not comparable. The &lt;strong&gt;key assumption&lt;/strong&gt; that is needed is that these difference between the treatment and control group is constant over time. By taking a double difference, we difference it out.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s check how diff-in-diff works empirically. The most common way to compute the diff-in-diff estiamtor is to first reshape the data in a &lt;strong&gt;long format&lt;/strong&gt; or &lt;strong&gt;panel format&lt;/strong&gt; (one observation is an individual $i$ at time period $t$) and then to regress the outcome $Y$ on the full interaction between the post-treatment dummy $T$ and the treatment dummy $D$.&lt;/p&gt;
&lt;p&gt;$$
Y_{i,t} = \alpha + \beta D_i + \gamma \mathbb I (t=1) + \delta D_i * \mathbb I (t=1) + \varepsilon_{i,t}
$$&lt;/p&gt;
&lt;p&gt;The estimator of the average treatment effect is the coefficient of the interaction coefficient, $\delta$.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_long = pd.wide_to_long(df, stubnames=&#39;revenue&#39;, i=&#39;i&#39;, j=&#39;t&#39;).reset_index()
df_long.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;i&lt;/th&gt;
      &lt;th&gt;t&lt;/th&gt;
      &lt;th&gt;revenue1_tilde&lt;/th&gt;
      &lt;th&gt;ad_campaign&lt;/th&gt;
      &lt;th&gt;revenue1_cuped&lt;/th&gt;
      &lt;th&gt;revenue&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;8.093744&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;8.093744&lt;/td&gt;
      &lt;td&gt;5.315635&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;10.164644&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;10.164644&lt;/td&gt;
      &lt;td&gt;2.977799&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;9.472203&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;9.472203&lt;/td&gt;
      &lt;td&gt;4.693796&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;7.688063&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;7.688063&lt;/td&gt;
      &lt;td&gt;5.827975&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;8.742618&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;8.742618&lt;/td&gt;
      &lt;td&gt;5.230095&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;The long dataset is now indexed by individuals $i$ and time $t$. We can now run the diff-in-diffs regression.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;smf.ols(&#39;revenue ~ t * ad_campaign&#39;, data=df_long).fit().summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
        &lt;td&gt;&lt;/td&gt;           &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt;     &lt;td&gt;    5.1481&lt;/td&gt; &lt;td&gt;    0.174&lt;/td&gt; &lt;td&gt;   29.608&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    4.805&lt;/td&gt; &lt;td&gt;    5.491&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;t&lt;/th&gt;             &lt;td&gt;    3.1514&lt;/td&gt; &lt;td&gt;    0.246&lt;/td&gt; &lt;td&gt;   12.816&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    2.666&lt;/td&gt; &lt;td&gt;    3.636&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;ad_campaign&lt;/th&gt;   &lt;td&gt;   -0.1310&lt;/td&gt; &lt;td&gt;    0.248&lt;/td&gt; &lt;td&gt;   -0.527&lt;/td&gt; &lt;td&gt; 0.599&lt;/td&gt; &lt;td&gt;   -0.621&lt;/td&gt; &lt;td&gt;    0.359&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;t:ad_campaign&lt;/th&gt; &lt;td&gt;    1.9224&lt;/td&gt; &lt;td&gt;    0.351&lt;/td&gt; &lt;td&gt;    5.473&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    1.230&lt;/td&gt; &lt;td&gt;    2.615&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;The estimated coefficient is close to the true value, 2, but the standard errors are bigger than the ones obtained with all other methods (0.35 &amp;raquo; 0.2). What did we miss? We didn&amp;rsquo;t &lt;strong&gt;cluster the standard errors&lt;/strong&gt;!&lt;/p&gt;
&lt;p&gt;I won&amp;rsquo;t go in detail here on what standard error clustering means, but the intuition is the following. The &lt;code&gt;statsmodels&lt;/code&gt; package by default computes the standard errors assuming that outcomes are &lt;strong&gt;independent&lt;/strong&gt; across observations. This assumption is unlikely to be true in this setting where we observe individuals over time and we are trying to exploit this information. Clustering allows for &lt;strong&gt;correlation&lt;/strong&gt; of the outcome variable within clusters. In our case, it makes sense (even without knowing the data generating process) to cluster the standard errors at the individual levels, allowing the outcome to be correlated over time for an individual $i$.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;smf.ols(&#39;revenue ~ t * ad_campaign&#39;, data=df_long)\
    .fit(cov_type=&#39;cluster&#39;, cov_kwds={&#39;groups&#39;: df_long[&#39;i&#39;]})\
    .summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
        &lt;td&gt;&lt;/td&gt;           &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;z&lt;/th&gt;      &lt;th&gt;P&gt;|z|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt;     &lt;td&gt;    5.1481&lt;/td&gt; &lt;td&gt;    0.139&lt;/td&gt; &lt;td&gt;   37.056&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    4.876&lt;/td&gt; &lt;td&gt;    5.420&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;t&lt;/th&gt;             &lt;td&gt;    3.1514&lt;/td&gt; &lt;td&gt;    0.128&lt;/td&gt; &lt;td&gt;   24.707&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    2.901&lt;/td&gt; &lt;td&gt;    3.401&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;ad_campaign&lt;/th&gt;   &lt;td&gt;   -0.1310&lt;/td&gt; &lt;td&gt;    0.181&lt;/td&gt; &lt;td&gt;   -0.724&lt;/td&gt; &lt;td&gt; 0.469&lt;/td&gt; &lt;td&gt;   -0.486&lt;/td&gt; &lt;td&gt;    0.224&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;t:ad_campaign&lt;/th&gt; &lt;td&gt;    1.9224&lt;/td&gt; &lt;td&gt;    0.209&lt;/td&gt; &lt;td&gt;    9.208&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    1.513&lt;/td&gt; &lt;td&gt;    2.332&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;Clustering standard errors at the individual level we obtain standard errors that are comparable to the previous estimates ($\sim 0.2$).&lt;/p&gt;
&lt;p&gt;Note that diff-in-diffs is &lt;strong&gt;equivalent to CUPED&lt;/strong&gt; when we assume the CUPED coefficient $\theta=1$.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df[&#39;revenue1_cuped2&#39;] = df[&#39;revenue1&#39;] - 1 * (df[&#39;revenue0&#39;] - np.mean(df[&#39;revenue0&#39;]))
smf.ols(&#39;revenue1_cuped2 ~ ad_campaign&#39;, data=df).fit().summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
       &lt;td&gt;&lt;/td&gt;          &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt;   &lt;td&gt;    8.2353&lt;/td&gt; &lt;td&gt;    0.145&lt;/td&gt; &lt;td&gt;   56.756&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    7.947&lt;/td&gt; &lt;td&gt;    8.523&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;ad_campaign&lt;/th&gt; &lt;td&gt;    1.9224&lt;/td&gt; &lt;td&gt;    0.207&lt;/td&gt; &lt;td&gt;    9.274&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    1.511&lt;/td&gt; &lt;td&gt;    2.334&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;Indeed, we obtain the same exact coefficient and almost identical standard errors!&lt;/p&gt;
&lt;h3 id=&#34;comparison&#34;&gt;Comparison&lt;/h3&gt;
&lt;p&gt;Which method is better? From what we have seen so far, all methods seem to deliver an accurate estimate, but the simple difference has a larger standard deviation.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s now compare all the methods we have seen so far via &lt;strong&gt;simulation&lt;/strong&gt;. We simulate the data generating process &lt;code&gt;dgp_cuped()&lt;/code&gt; 1000 times and we save the estimated coefficient of the following methods:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Simple difference&lt;/li&gt;
&lt;li&gt;Autoregression&lt;/li&gt;
&lt;li&gt;Diff-in-diffs&lt;/li&gt;
&lt;li&gt;CUPED&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def simulate(dgp, K=300, x=&#39;revenue0&#39;):
    
    # Initialize coefficients
    results = pd.DataFrame(columns=[&#39;k&#39;, &#39;Estimator&#39;, &#39;Estimate&#39;])
    
    # Compute coefficients
    for k in range(K):
        temp = pd.DataFrame({&#39;k&#39;: [k] * 4, 
                             &#39;Estimator&#39;: [&#39;1. Diff &#39;, &#39;2. Areg &#39;, &#39;3. DiD  &#39;, &#39;4. CUPED&#39;], 
                             &#39;Estimate&#39;: [0] * 4})
        
        # Draw data
        df = dgp.generate_data(seed=k)

        # Single diff
        temp[&#39;Estimate&#39;][0] = smf.ols(&#39;revenue1 ~ ad_campaign&#39;, data=df).fit().params[1]
        
        # Autoregression
        temp[&#39;Estimate&#39;][1] = smf.ols(f&#39;revenue1 ~ ad_campaign + {x}&#39;, data=df).fit().params[1]
        
        # Double diff
        df_long = pd.wide_to_long(df.dropna(), stubnames=&#39;revenue&#39;, i=&#39;i&#39;, j=&#39;t&#39;).reset_index()
        temp[&#39;Estimate&#39;][2] = smf.ols(&#39;revenue ~ ad_campaign * t&#39;, data=df_long)\
            .fit(cov_type=&#39;cluster&#39;, cov_kwds={&#39;groups&#39;: df_long[&#39;i&#39;]}).params[3]
        
        # Cuped
        df[&#39;revenue1_tilde&#39;] = smf.ols(f&#39;revenue1 ~ {x}&#39;, data=df).fit().resid + np.mean(df[&#39;revenue1&#39;])
        temp[&#39;Estimate&#39;][3] = smf.ols(&#39;revenue1_tilde ~ ad_campaign&#39;, data=df).fit().params[1]
                
        # Combine estimates
        results = pd.concat((results, temp))
    
    return results.reset_index(drop=True)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;results = simulate(dgp=dgp_cuped())
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s plot the distribution of the estimated parameters.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sns.kdeplot(data=results, x=&amp;quot;Estimate&amp;quot;, hue=&amp;quot;Estimator&amp;quot;);
plt.axvline(x=2, c=&#39;k&#39;, ls=&#39;--&#39;);
plt.title(&#39;Simulated Distributions&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/cuped_53_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;We can also tabulate the simulated mean and standard deviation of each estimator.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;results.groupby(&#39;Estimator&#39;).agg(mean=(&amp;quot;Estimate&amp;quot;, &amp;quot;mean&amp;quot;), std=(&amp;quot;Estimate&amp;quot;, &amp;quot;std&amp;quot;))
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;mean&lt;/th&gt;
      &lt;th&gt;std&lt;/th&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;Estimator&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;1. Diff&lt;/th&gt;
      &lt;td&gt;1.999626&lt;/td&gt;
      &lt;td&gt;0.291497&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2. Areg&lt;/th&gt;
      &lt;td&gt;2.034145&lt;/td&gt;
      &lt;td&gt;1.063968&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3. DiD&lt;/th&gt;
      &lt;td&gt;1.993494&lt;/td&gt;
      &lt;td&gt;0.197638&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4. CUPED&lt;/th&gt;
      &lt;td&gt;1.971853&lt;/td&gt;
      &lt;td&gt;0.198145&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;All estimators seem &lt;strong&gt;unbiased&lt;/strong&gt;: the average values are all close to the true value of 2. Moreover, all estimators have a very similar standard deviation, apart from the single-difference estimator!&lt;/p&gt;
&lt;h3 id=&#34;always-identical&#34;&gt;Always Identical?&lt;/h3&gt;
&lt;p&gt;Are the estimators always identical, or is there some difference among them?&lt;/p&gt;
&lt;p&gt;We could check many different departures from the original data generating process. For simplicity, I will consider only one here: &lt;strong&gt;imperfect randomization&lt;/strong&gt;. Other tweaks of the data generating process that I considered are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;pre-treatment missing values&lt;/li&gt;
&lt;li&gt;additional covariates / control variables&lt;/li&gt;
&lt;li&gt;multiple pre-treatment periods&lt;/li&gt;
&lt;li&gt;heterogeneous treatment effects&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;and combinations of them. However, I found imperfect randomization to be the most informative example.&lt;/p&gt;
&lt;p&gt;Suppose now that &lt;strong&gt;randomization was not perfect&lt;/strong&gt; and two groups are not identical. In particular, if the data generating process is&lt;/p&gt;
&lt;p&gt;$$
Y_{i,t} = \alpha + \beta D_i + \gamma \mathbb I (t=1) + \delta D_i * \mathbb I (t=1) + u_i + \varepsilon_{i,t}
$$&lt;/p&gt;
&lt;p&gt;assume that $\beta \neq 0$.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;results_beta1 = simulate(dgp=dgp_cuped(beta=1))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s plot the distribution of the estimated parameters.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sns.kdeplot(data=results_beta1, x=&amp;quot;Estimate&amp;quot;, hue=&amp;quot;Estimator&amp;quot;);
plt.axvline(x=2, c=&#39;k&#39;, ls=&#39;--&#39;);
plt.title(&#39;Simulated Distributions&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/cuped_61_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;results_beta1.groupby(&#39;Estimator&#39;).agg(mean=(&amp;quot;Estimate&amp;quot;, &amp;quot;mean&amp;quot;), std=(&amp;quot;Estimate&amp;quot;, &amp;quot;std&amp;quot;))
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;mean&lt;/th&gt;
      &lt;th&gt;std&lt;/th&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;Estimator&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;1. Diff&lt;/th&gt;
      &lt;td&gt;2.999626&lt;/td&gt;
      &lt;td&gt;0.291497&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2. Areg&lt;/th&gt;
      &lt;td&gt;1.991508&lt;/td&gt;
      &lt;td&gt;0.227065&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3. DiD&lt;/th&gt;
      &lt;td&gt;1.993494&lt;/td&gt;
      &lt;td&gt;0.197638&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4. CUPED&lt;/th&gt;
      &lt;td&gt;1.577712&lt;/td&gt;
      &lt;td&gt;0.221448&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;With imperfect treatment assignment, both difference-in-differences and autoregression are unbiased for the true treatment effect, however diff-in-diffs is more efficient. Both CUPED and simple difference are &lt;strong&gt;biased&lt;/strong&gt; instead. Why?&lt;/p&gt;
&lt;p&gt;Diff-in-diffs explicily controls for &lt;strong&gt;systematic differences&lt;/strong&gt; between treatment and control group that are &lt;strong&gt;constant over time&lt;/strong&gt;. This is exactly what this estimator was built for. Autoregression performs some sort of matching on the additional covariate, $Y_{t=0}$, effectively controlling for these systematic differences as well, but less efficiently (if you want to know more, I wrote related posts on control variables &lt;a href=&#34;https://towardsdatascience.com/b63dc69e3d8c&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt; and &lt;a href=&#34;https://towardsdatascience.com/58b63d25d2ef&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;). CUPED controls for persistent heterogeneity at the individual level, but not at the treatment assignment level. Lastly, the simple difference estimator does not control for anything.&lt;/p&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;In this post, I have analyzed an estimator of average treatment effects in AB testing, very popular in the industry: CUPED. The key idea is that, by exploiting pre-treatment data, CUPED can &lt;strong&gt;achieve a lower variance&lt;/strong&gt; by controlling for individual-level variation that is persistent over time. We have also seen that CUPED is closely related but not equivalent to autoregression and difference-in-differences. The differences among the methods clearly emerge when we have imperfect randomization.&lt;/p&gt;
&lt;p&gt;An interesting avenue of future research is what happens when we have &lt;strong&gt;a lot of pre-treatment information&lt;/strong&gt;, either in terms of time periods or observable characteristics. Scientists from Meta, &lt;a href=&#34;https://proceedings.neurips.cc/paper/2021/hash/488b084119a1c7a4950f00706ec7ea16-Abstract.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Guo, Coey, Konutgan, Li, Schoener, Goldman (2021)&lt;/a&gt;, have analyzed this problem in a very recent paper that exploits machine learning techniques to efficiently use this extra information. This approach is closely related to the &lt;a href=&#34;https://academic.oup.com/ectj/article/21/1/C1/5056401&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Double/Debiased Machine Learning&lt;/a&gt; literature. If you are interested, I wrote two articles on the topic (&lt;a href=&#34;https://towardsdatascience.com/eb767a59975b&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;part 1&lt;/a&gt; and &lt;a href=&#34;https://towardsdatascience.com/bf990720a0b2&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;part 2&lt;/a&gt;) and I might write more in the future.&lt;/p&gt;
&lt;h3 id=&#34;references&#34;&gt;References&lt;/h3&gt;
&lt;p&gt;[1] A. Deng, Y. Xu, R. Kohavi, T. Walker, &lt;a href=&#34;https://dl.acm.org/doi/abs/10.1145/2433396.2433413&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Improving the Sensitivity of Online Controlled Experiments by Utilizing Pre-Experiment Data&lt;/a&gt; (2013), &lt;em&gt;WSDM&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;[2] H. Xir, J. Aurisset, &lt;a href=&#34;https://dl.acm.org/doi/abs/10.1145/2939672.2939733&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Improving the sensitivity of online controlled experiments: Case studies at Netflix&lt;/a&gt; (2013), &lt;em&gt;ACM SIGKDD&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;[3] Y. Guo, D. Coey, M. Konutgan, W. Li, C. Schoener, M. Goldman, &lt;a href=&#34;https://proceedings.neurips.cc/paper/2021/hash/488b084119a1c7a4950f00706ec7ea16-Abstract.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Machine Learning for Variance Reduction in Online Experiments&lt;/a&gt; (2021), &lt;em&gt;NeurIPS&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;[4] V. Chernozhukov, D. Chetverikov, M. Demirer, E. Duflo, C. Hansen, W. Newey, J. Robins, &lt;a href=&#34;https://academic.oup.com/ectj/article/21/1/C1/5056401&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Double/debiased machine learning for treatment and structural parameters&lt;/a&gt; (2018), &lt;em&gt;The Econometrics Journal&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;[5] M. Bertrand, E. Duflo, S. Mullainathan, &lt;a href=&#34;https://academic.oup.com/qje/article/119/1/249/1876068&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;How Much Should We Trust Differences-In-Differences Estimates?&lt;/a&gt; (2012), &lt;em&gt;The Quarterly Journal of Economics&lt;/em&gt;.&lt;/p&gt;
&lt;h3 id=&#34;related-articles&#34;&gt;Related Articles&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/eb767a59975b&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Double Debiased Machine Learning (part 1)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/bf990720a0b2&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Double Debiased Machine Learning (part 2)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/59f801eb3299&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Understanding The Frisch-Waugh-Lovell Theorem&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/58b63d25d2ef&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Understanding Contamination Bias&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/b63dc69e3d8c&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;DAGs and Control Variables&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;code&#34;&gt;Code&lt;/h3&gt;
&lt;p&gt;You can find the original Jupyter Notebook here:&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/cuped.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/cuped.ipynb&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Understanding Meta Learners</title>
      <link>https://matteocourthoud.github.io/post/meta_learners/</link>
      <pubDate>Mon, 10 Jul 2023 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/post/meta_learners/</guid>
      <description>&lt;p&gt;&lt;em&gt;How to use machine learning to estimate heterogeneous treatment effects&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;In many settings, we are not just interested in understanding a causal effect, but also whether this effect is &lt;strong&gt;different for different users&lt;/strong&gt;. We might be interested in understanding if a drug has side effects that are different for people of different age. Or we might be interested in understanding if an ad campaign is particularly effective in certain geographical areas.&lt;/p&gt;
&lt;p&gt;This knowledge is crucial because it allows us to &lt;strong&gt;target&lt;/strong&gt; the treatment. If a drug has severe side effects for kids, we might want to restrict its distribution only to adults. Or if an ad campaign is effective only in English-speaking countries it is not worth showing it elsewhere.&lt;/p&gt;
&lt;p&gt;In this blog post we are going to explore some approaches to uncover &lt;strong&gt;treatment effect heterogeneity&lt;/strong&gt;. In particular, we are going to explore methods that try to leverage the flexibility of &lt;strong&gt;machine learning&lt;/strong&gt; algorithms.&lt;/p&gt;
&lt;h2 id=&#34;example&#34;&gt;Example&lt;/h2&gt;
&lt;p&gt;Suppose we were a company interested in understanding how much a new &lt;strong&gt;premium feature&lt;/strong&gt; increases revenue. In particular, we know that users of different &lt;strong&gt;age&lt;/strong&gt; have different spending attitudes and we suspect that the impact of the premium feature could also be different depending on the age of the user.&lt;/p&gt;
&lt;p&gt;This information might be very important, for example for &lt;strong&gt;advertisement targeting&lt;/strong&gt; or &lt;strong&gt;discount design&lt;/strong&gt;. If we discover that the premium feature increases revenue for a particular set of users, we might want to target advertisement towards that group, or offer them personalized discounts.&lt;/p&gt;
&lt;p&gt;To understand the effect of the premium feature on revenue, the run an &lt;a href=&#34;https://en.wikipedia.org/wiki/A/B_testing&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;AB test&lt;/strong&gt;&lt;/a&gt; in which we randomly give access to the premium feature to 10% of the users. The feature is &lt;strong&gt;expensive&lt;/strong&gt; and we cannot afford to give it for free to more users. Hopefully a 10% treatment probability is enough.&lt;/p&gt;
&lt;p&gt;We generate the simulated data using the data generating process &lt;code&gt;dgp_premium()&lt;/code&gt; from &lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/src/dgp.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;src.dgp&lt;/code&gt;&lt;/a&gt;. I also import some plotting functions and libraries from &lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/src/utils.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;src.utils&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;%matplotlib inline
%config InlineBackend.figure_format = &#39;retina&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from src.utils import *
from src.dgp import dgp_premium
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;dgp = dgp_premium()
df = dgp.generate_data(seed=5)
df.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;revenue&lt;/th&gt;
      &lt;th&gt;premium&lt;/th&gt;
      &lt;th&gt;age&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;10.62&lt;/td&gt;
      &lt;td&gt;True&lt;/td&gt;
      &lt;td&gt;27.32&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;10.35&lt;/td&gt;
      &lt;td&gt;True&lt;/td&gt;
      &lt;td&gt;54.57&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;10.13&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;26.68&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;9.97&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;56.58&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;10.16&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;38.51&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;We have data on 300 users, for whom we observe the &lt;code&gt;revenue&lt;/code&gt; they generate and whether they were given the &lt;code&gt;premium&lt;/code&gt; feature. Moreover, we also record the &lt;code&gt;age&lt;/code&gt; of the users.&lt;/p&gt;
&lt;p&gt;To understand whether randomization worked, we use the &lt;code&gt;create_table_one&lt;/code&gt; function from Uber&amp;rsquo;s &lt;a href=&#34;https://causalml.readthedocs.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;causalml&lt;/code&gt;&lt;/a&gt; package to produce a &lt;strong&gt;covariate balance table&lt;/strong&gt;, containing the average value of our observable characteristics, across treatment and control groups. As the name suggests, this should always be the first table you present in causal inference analysis.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from causalml.match import create_table_one

create_table_one(df, &#39;premium&#39;, [&#39;age&#39;, &#39;revenue&#39;])
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;Control&lt;/th&gt;
      &lt;th&gt;Treatment&lt;/th&gt;
      &lt;th&gt;SMD&lt;/th&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;Variable&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;n&lt;/th&gt;
      &lt;td&gt;269&lt;/td&gt;
      &lt;td&gt;31&lt;/td&gt;
      &lt;td&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;age&lt;/th&gt;
      &lt;td&gt;39.01 (12.11)&lt;/td&gt;
      &lt;td&gt;38.43 (13.26)&lt;/td&gt;
      &lt;td&gt;-0.0454&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;revenue&lt;/th&gt;
      &lt;td&gt;10.04 (0.16)&lt;/td&gt;
      &lt;td&gt;10.56 (0.23)&lt;/td&gt;
      &lt;td&gt;2.5905&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;Most users are in the control group and only 31 users have received the premium feature. Average &lt;code&gt;age&lt;/code&gt; is comparable across groups (SMD&amp;lt;1), while it seems that the premium feature increases &lt;code&gt;revenue&lt;/code&gt; by 2.6$ per user, on average.&lt;/p&gt;
&lt;p&gt;Does the effect of the &lt;code&gt;premium&lt;/code&gt; feature differ by &lt;code&gt;age&lt;/code&gt;?&lt;/p&gt;
&lt;p&gt;One simple approach could me to regress &lt;code&gt;revenue&lt;/code&gt; on a full interaction of &lt;code&gt;premium&lt;/code&gt; and age.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;linear_model = smf.ols(&#39;revenue ~ premium * age&#39;, data=df).fit()
linear_model.summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
           &lt;td&gt;&lt;/td&gt;              &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt;           &lt;td&gt;   10.0244&lt;/td&gt; &lt;td&gt;    0.034&lt;/td&gt; &lt;td&gt;  292.716&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    9.957&lt;/td&gt; &lt;td&gt;   10.092&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;premium[T.True]&lt;/th&gt;     &lt;td&gt;    0.5948&lt;/td&gt; &lt;td&gt;    0.099&lt;/td&gt; &lt;td&gt;    6.007&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    0.400&lt;/td&gt; &lt;td&gt;    0.790&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;age&lt;/th&gt;                 &lt;td&gt;    0.0005&lt;/td&gt; &lt;td&gt;    0.001&lt;/td&gt; &lt;td&gt;    0.570&lt;/td&gt; &lt;td&gt; 0.569&lt;/td&gt; &lt;td&gt;   -0.001&lt;/td&gt; &lt;td&gt;    0.002&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;premium[T.True]:age&lt;/th&gt; &lt;td&gt;   -0.0021&lt;/td&gt; &lt;td&gt;    0.002&lt;/td&gt; &lt;td&gt;   -0.863&lt;/td&gt; &lt;td&gt; 0.389&lt;/td&gt; &lt;td&gt;   -0.007&lt;/td&gt; &lt;td&gt;    0.003&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;The interaction coefficient is close to zero and not significant. It seems that there is not a different effect of &lt;code&gt;premium&lt;/code&gt; by &lt;code&gt;age&lt;/code&gt;. But is it true? The interaction coefficient only captures linear relationships. What if the relationship is &lt;strong&gt;non-linear&lt;/strong&gt;?&lt;/p&gt;
&lt;p&gt;We can check it by directly &lt;strong&gt;plotting the raw data&lt;/strong&gt;. We plot revenue by age, splitting the data between &lt;code&gt;premium&lt;/code&gt; users and non-premium users.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sns.scatterplot(data=df, x=&#39;age&#39;, y=&#39;revenue&#39;, hue=&#39;premium&#39;, s=40);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/meta_learners_13_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;From the raw data, it looks like revenue is generally higher for people between 30 and 40 and &lt;code&gt;premium&lt;/code&gt; has a particularly strong effect for people between 35 and 45/50.&lt;/p&gt;
&lt;p&gt;We can visualize the estimated revenue by age with and without treatment.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def plot_TE(df, true_te=False):
    sns.scatterplot(data=df, x=&#39;age&#39;, y=&#39;revenue&#39;, hue=&#39;premium&#39;, s=40, legend=True)
    sns.lineplot(df[&#39;age&#39;], df[&#39;mu0_hat&#39;], label=&#39;$\mu_0$&#39;)
    sns.lineplot(df[&#39;age&#39;], df[&#39;mu1_hat&#39;], label=&#39;$\mu_1$&#39;)
    if true_te:
        plt.fill_between(df[&#39;age&#39;], df[&#39;y0&#39;], df[&#39;y0&#39;] + df[&#39;y1&#39;], color=&#39;grey&#39;, alpha=0.2, label=&amp;quot;True TE&amp;quot;)
    plt.title(&#39;Distribution of revenue by age and premium status&#39;)
    plt.legend(title=&#39;Treated&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We first compute the predicted revenue with ($\mu_1$) and without &lt;code&gt;premium&lt;/code&gt; subscription ($\mu_0$) and we plot them together with the raw data.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df[&#39;mu0_hat&#39;] = linear_model.predict(df.assign(premium=0))
df[&#39;mu1_hat&#39;] = linear_model.predict(df.assign(premium=1))
plot_TE(df)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/meta_learners_17_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;As we can see, the orange line is higher than the blue line, suggesting a positive effect of &lt;code&gt;premium&lt;/code&gt; on &lt;code&gt;revenue&lt;/code&gt;. However, the two lines are essentially &lt;strong&gt;parallel&lt;/strong&gt;, suggesting no heterogeneity in treatment effects.&lt;/p&gt;
&lt;p&gt;Can we be more precise? Is there a way to estimate this treatment heterogeneity in a &lt;strong&gt;flexible way&lt;/strong&gt;, without assuming functional forms?&lt;/p&gt;
&lt;p&gt;The answer is &lt;strong&gt;yes&lt;/strong&gt;! We can use machine learning methods to flexibly estimate heterogeneous treatment effects. In particular, in this blog post we are going to inspect three and popular methods that were introduced by &lt;a href=&#34;https://www.pnas.org/doi/abs/10.1073/pnas.1804597116&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Künzel, Sekhon, Bickel, Yu, (2019)&lt;/a&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;S-learner&lt;/li&gt;
&lt;li&gt;T-learner&lt;/li&gt;
&lt;li&gt;X-learner&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;setting&#34;&gt;Setting&lt;/h2&gt;
&lt;p&gt;We assume that for a set of i.i.d. subjects $i = 1, &amp;hellip;, n$ we observed a tuple $(X_i, D_i, Y_i)$ comprised of&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;a treatment assignment $T_i \in \lbrace 0, 1 \rbrace$ (&lt;code&gt;premium&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;a response $Y_i \in \mathbb R$ (&lt;code&gt;revenue&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;a feature vector $X_i \in \mathbb R^n$ (&lt;code&gt;age&lt;/code&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We are interested in &lt;strong&gt;estimating the average treatment effect&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;$$
\tau = \mathbb E \Big[ Y_i^{(1)} - Y_i^{(0)} \Big]
$$&lt;/p&gt;
&lt;p&gt;Where $Y_i^{(d)}$ indicates the potential outcome of individual $i$ under treatment status $d$. We also make the following assumptions.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Assumption 1 : unconfoundedness&lt;/strong&gt; (or ignorability, or selection on observables)&lt;/p&gt;
&lt;p&gt;$$
\big \lbrace Y_i^{(1)} , Y_i^{(0)} \big \rbrace \ \perp \ D_i \ | \ X_i
$$&lt;/p&gt;
&lt;p&gt;i.e. conditional on observable characteristics $X$, the treatment assignment $D$ is as good as random. What we are effectively assuming is that there is no other characteristics that we do not observe that could impact both whether a user selects the &lt;code&gt;dark_mode&lt;/code&gt; and their &lt;code&gt;read_time&lt;/code&gt;. This is a &lt;strong&gt;strong assumption&lt;/strong&gt; that is more likely to be satisfied the more individual characteristics we observe.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Assumption 2: stable unit treatment value (SUTVA)&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;$$
Y^{(d)} \perp D
$$&lt;/p&gt;
&lt;p&gt;i.e. the potential outcome does not depend on the treatment status. In our case, we are ruling out the fact that another user selecting the &lt;code&gt;premium&lt;/code&gt; feature might affect my effect of &lt;code&gt;premium&lt;/code&gt; on &lt;code&gt;revenue&lt;/code&gt;. The most common setting where SUTVA is violated is in presence of &lt;strong&gt;network effects&lt;/strong&gt;: if a friend of mine uses a social network increases my utility from using it.&lt;/p&gt;
&lt;h2 id=&#34;s-learner&#34;&gt;S-Learner&lt;/h2&gt;
&lt;p&gt;The simplest meta-algorithm is the &lt;strong&gt;single learner or S-learner&lt;/strong&gt;. To build the S-learner estimator, we fit a single model for all observations.&lt;/p&gt;
&lt;p&gt;$$
\mu(z) = \mathbb E \left[ Y_i \ \big | \ (X_i, D_i) = z \right]
$$&lt;/p&gt;
&lt;p&gt;the estimator is given by the difference between the predicted values evaluated with and without the treatment, $d=1$ and $d=0$.&lt;/p&gt;
&lt;p&gt;$$
\hat \tau_{S} (x) = \hat \mu(x,1) - \hat \mu(x,0)
$$&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def S_learner(dgp, model, y, D, X):
    temp = dgp.generate_data(true_te=True).sort_values(X)
    mu = model.fit(temp[X + [D]], temp[y])
    temp[&#39;mu0_hat&#39;] = mu.predict(temp[X + [D]].assign(premium=0))
    temp[&#39;mu1_hat&#39;] = mu.predict(temp[X + [D]].assign(premium=1))
    plot_TE(temp, true_te=True)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s use a &lt;a href=&#34;https://en.wikipedia.org/wiki/Decision_tree_learning&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;decision tree regression&lt;/strong&gt;&lt;/a&gt; model to build the the S-learner, using the &lt;code&gt;DecisionTreeRegressor&lt;/code&gt; function from the &lt;a href=&#34;https://scikit-learn.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;sklearn&lt;/code&gt;&lt;/a&gt; package. I won&amp;rsquo;t go into details about decision trees here, but I will just say that it&amp;rsquo;s a non-parametric estimator that uses the training data to split the state space (&lt;code&gt;premium&lt;/code&gt; and &lt;code&gt;age&lt;/code&gt; in our case) into blocks and predicts the outcome (&lt;code&gt;revenue&lt;/code&gt; in our case) as its average value within block.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from sklearn.tree import DecisionTreeRegressor

model = DecisionTreeRegressor(min_impurity_decrease=0.001)
S_learner(dgp, model, y=&amp;quot;revenue&amp;quot;, D=&amp;quot;premium&amp;quot;, X=[&amp;quot;age&amp;quot;])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/meta_learners_25_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;The plot depicts the data together with the response functions $\hat \mu(x,1)$ and $\hat \mu(x,0)$. I have also plotted in grey the area between the true response functions: the true treatment effects.&lt;/p&gt;
&lt;p&gt;As we can see, the S-learner is flexible enough to understand that there is a difference in levels between treatment and control group (we have two separate lines). It also captures well the response function for the control group, $\hat \mu(x,0)$, but not so well the control function for the treatment group, $\hat \mu(x,1)$.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;problem&lt;/strong&gt; with the S-learner is that it is learning a &lt;strong&gt;single model&lt;/strong&gt; so we have to hope that the model uncovers heterogeneity in the treatment $D$, but it might not be the case. Moreover, if the model is heavily regularized because of the high dimensionality of $X$, it &lt;strong&gt;might not recover any treatment effect&lt;/strong&gt;. For example, with decision trees, we might not split on the treatment $D$.&lt;/p&gt;
&lt;h2 id=&#34;t-learner&#34;&gt;T-learner&lt;/h2&gt;
&lt;p&gt;To build the &lt;strong&gt;two-learner or T-learner&lt;/strong&gt; estimator, we fit two different models, one for treated units and one for control units.&lt;/p&gt;
&lt;p&gt;$$
\mu^{(1)}(x) = \mathbb E \left[ Y_i \ \big | \ X_i = x, T_i = 1 \right] \qquad ; \qquad \mu^{(0)}(x) = \mathbb E \left[ Y_i \ \big | \ X_i = x, T_i = 0 \right]
$$&lt;/p&gt;
&lt;p&gt;the estimator is given by the difference between the predicted values of the two algorithms.&lt;/p&gt;
&lt;p&gt;$$
\hat \tau_{T} (x) = \hat \mu^{(1)}(x) - \hat \mu^{(0)}(x)
$$&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def T_learner(df, model, y, D, X):
    temp = dgp.generate_data(true_te=True).sort_values(X)
    mu0 = model.fit(temp.loc[temp[D]==0, X], temp.loc[temp[D]==0, y])
    temp[&#39;mu0_hat&#39;] = mu0.predict(temp[X])
    mu1 = model.fit(temp.loc[temp[D]==1, X], temp.loc[temp[D]==1, y])
    temp[&#39;mu1_hat&#39;] = mu1.predict(temp[X])
    plot_TE(temp, true_te=True)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We use a decision tree regression model as before but, this time, we fit two separate decision trees for the treatment and control group.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;T_learner(dgp, model, y=&amp;quot;revenue&amp;quot;, D=&amp;quot;premium&amp;quot;, X=[&amp;quot;age&amp;quot;])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/meta_learners_31_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;As we can see, the T-learner is much &lt;strong&gt;more flexible&lt;/strong&gt; than the S-learner because it fits two separate models. The response function for the control group, $\hat \mu(x,0)$, is still very accurate and the response function for the treatment group, $\hat \mu(x,1)$, is more flexible than before.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;problem&lt;/strong&gt; now is that we are &lt;strong&gt;using just a fraction of the data&lt;/strong&gt; for each prediction problem, while the S-learner was using all the data. By fitting two separate models we are losing some information. Moreover, by using two different models we might get &lt;strong&gt;heterogeneity where there is none&lt;/strong&gt;. For example, with decision trees, we will probably get different splits with different samples even if the data generating process is the same.&lt;/p&gt;
&lt;h3 id=&#34;x-learner&#34;&gt;X-learner&lt;/h3&gt;
&lt;p&gt;The &lt;strong&gt;cross-learner or X-learner&lt;/strong&gt; estimator is an extension of the T-learner estimator. It is built in the following way:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;As for the T-learner, compute separate models for $\mu^{(1)}(x)$ and $\mu^{(0)}(x)$ using the treated and control units, respectively&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Compute the estimated treatment effects as&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;$$
\Delta_i (x) =
\begin{cases}
Y_i - \hat \mu^{(0)}(x) &amp;amp;\quad \text{ if } D_i = 1
\newline
\hat \mu^{(1)}(x) - Y_i &amp;amp;\quad \text{ if } D_i = 0
\end{cases}
$$&lt;/p&gt;
&lt;ol start=&#34;3&#34;&gt;
&lt;li&gt;
&lt;p&gt;Predicting $\Delta$ from $X$, compute $\hat \tau^{(0)}(x)$ from treated units and  $\hat \tau^{(1)}(x)$ from control units&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Estimate the &lt;a href=&#34;https://en.wikipedia.org/wiki/Propensity_score_matching&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;propensity score&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;$$
e(x) = \mathbb E \left[ D_i = 1 \ \big | \ X_i = x \right]
$$&lt;/p&gt;
&lt;ol start=&#34;5&#34;&gt;
&lt;li&gt;Compute the treatment effects&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;$$
\hat \tau_X(x) = \hat \tau^{(0)}(x) \hat e(x) + \hat \tau^{(1)}(x) (1 - \hat e(x))
$$&lt;/p&gt;
&lt;p&gt;Can we still recover &lt;strong&gt;pseudo response functions&lt;/strong&gt;? Yes!&lt;/p&gt;
&lt;p&gt;Which we can rewrite the treatment effects as&lt;/p&gt;
&lt;p&gt;$$
\begin{align}
\hat \tau_X(x) &amp;amp; = \hat \tau^{(0)}(x) \hat e(x) + \hat \tau^{(1)}(x) (1 - \hat e(x)) = \newline
&amp;amp;= \hat e(x) \left[ \hat \mu^{(1)}(x) - Y_i^{(0)} \right] + (1 - \hat e(x)) \left[ Y_i^{(1)} - \hat \mu^{(0)}(x) \right] = \newline
&amp;amp;= \left[ \hat e(x) \hat \mu^{(1)}(x) + (1 - \hat e(x)) Y_i^{(1)} \right] - \left[ \hat e(x) Y_i^{(0)} + (1 - \hat e(x))  \hat \mu^{(0)}(x) \right]
\end{align}
$$&lt;/p&gt;
&lt;p&gt;So that the pseudo response functions estimated by the X-learner are&lt;/p&gt;
&lt;p&gt;$$
\begin{align}
\tilde \mu_i^{(1)} (x) &amp;amp;= \hat e(x) \hat \mu^{(1)}(x) + (1 - \hat e(x)) Y_i^{(1)} \newline
\tilde \mu_i^{(0)} (x) &amp;amp;=  \hat e(x) Y_i^{(0)} + (1 - \hat e(x)) \hat \mu^{(0)}(x)
\end{align}
$$&lt;/p&gt;
&lt;p&gt;As we can see, the X-learner combines the true values $Y_i^{(d)}$ with the estimated ones $\mu_i^{(d)} (x)$ weighting by the &lt;a href=&#34;https://en.wikipedia.org/wiki/Propensity_score_matching&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;propensity scores&lt;/strong&gt;&lt;/a&gt; $e_i(x)$, i.e. the estimated treatment probabilities.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;What does it mean?&lt;/strong&gt; It means that if we have many more observations for one group (in our case the control group), the control response function $\hat \mu^{(0)}(x) $ will get most of the weight. Instead, for the other group (the treatment group in our case), the actual observations $Y_i^{(1)}$ will get most of the weight.&lt;/p&gt;
&lt;p&gt;To illustrate the method, I am going to build pseudo response functions by approximating $Y_i^{(d)}$ using the nearest observation, using the &lt;code&gt;KNeighborsRegressor&lt;/code&gt; function. I estimate the propensity scores via &lt;a href=&#34;https://en.wikipedia.org/wiki/Logistic_regression&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;logistic regression&lt;/a&gt; using the &lt;code&gt;LogisticRegressionCV&lt;/code&gt; function.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from sklearn.neighbors import KNeighborsRegressor
from sklearn.linear_model import LogisticRegressionCV

def X_learner(df, model, y, D, X):
    temp = dgp.generate_data(true_te=True).sort_values(X)
    
    # Mu
    mu0 = model.fit(temp.loc[temp[D]==0, X], temp.loc[temp[D]==0, y])
    temp[&#39;mu0_hat_&#39;] = mu0.predict(temp[X])
    mu1 = model.fit(temp.loc[temp[D]==1, X], temp.loc[temp[D]==1, y])
    temp[&#39;mu1_hat_&#39;] = mu1.predict(temp[X])
    
    # Y
    y0 = KNeighborsRegressor(n_neighbors=1).fit(temp.loc[temp[D]==0, X], temp.loc[temp[D]==0, y])
    temp[&#39;y0_hat&#39;] = y0.predict(temp[X])
    y1 = KNeighborsRegressor(n_neighbors=1).fit(temp.loc[temp[D]==1, X], temp.loc[temp[D]==1, y])
    temp[&#39;y1_hat&#39;] = y1.predict(temp[X])
    
    # Weight
    e = LogisticRegressionCV().fit(y=temp[D], X=temp[X]).predict_proba(temp[X])[:,1]
    temp[&#39;mu0_hat&#39;] = e * temp[&#39;y0_hat&#39;] + (1-e) * temp[&#39;mu0_hat_&#39;]
    temp[&#39;mu1_hat&#39;] = (1-e) * temp[&#39;y1_hat&#39;] + e * temp[&#39;mu1_hat_&#39;]
    
    # Plot
    plot_TE(temp, true_te=True)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;X_learner(df, model, y=&amp;quot;revenue&amp;quot;, D=&amp;quot;premium&amp;quot;, X=[&amp;quot;age&amp;quot;])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/meta_learners_37_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;As we can clearly see from this graph, the main advantage of &lt;strong&gt;X-learners&lt;/strong&gt; is that it adapts the &lt;strong&gt;flexibility&lt;/strong&gt; of the response functions to the context. In areas of the state space where we have a lot of data, it mostly uses the estimated response function, in areas of the state space with few data, it uses the observation themselves.&lt;/p&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;In this post, we have seen different estimators introduced by &lt;a href=&#34;https://www.pnas.org/doi/abs/10.1073/pnas.1804597116&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Künzel, Sekhon, Bickel, Yu, (2019)&lt;/a&gt; that leverage flexible &lt;strong&gt;machine learning&lt;/strong&gt; algorithms to estimate &lt;strong&gt;heterogeneous treatment effects&lt;/strong&gt;. The estimators differ for their degree of sophistication: the S-learner fits a single estimator including the treatment indicator as a covariate. The T-learner fits two separate estimators for the treatment and control group. Lastly, the X-learner is an extension of the T-learner that allows for different degrees of flexibility depending on the amount of data available across treatment and control groups.&lt;/p&gt;
&lt;p&gt;Estimation of heterogeneous treatment effect is extremely important for &lt;strong&gt;treatment targeting&lt;/strong&gt;. Indeed, there is now a growing literature that exploits machine learning methods to get flexible estimates without imposing functional form assumptions. Among the many, it&amp;rsquo;s important to mention the R-learner procedure of &lt;a href=&#34;https://academic.oup.com/biomet/article/108/2/299/5911092&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Nie and Wager (2021)&lt;/a&gt; and the causal trees and forests of &lt;a href=&#34;https://www.tandfonline.com/doi/full/10.1080/01621459.2017.1319839&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Athey and Wager (2018)&lt;/a&gt;. I might write more about these procedures in the future so, stay tuned ☺️&lt;/p&gt;
&lt;h3 id=&#34;references&#34;&gt;References&lt;/h3&gt;
&lt;p&gt;[1] S. Künzel, J. Sekhon, P. Bickel, B. Yu, &lt;a href=&#34;https://www.pnas.org/doi/abs/10.1073/pnas.1804597116&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Metalearners for estimating heterogeneous treatment effects using machine learning&lt;/a&gt; (2019), &lt;em&gt;PNAS&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;[2] X. Nie, S. Wager, &lt;a href=&#34;https://academic.oup.com/biomet/article/108/2/299/5911092&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Quasi-oracle estimation of heterogeneous treatment effects&lt;/a&gt; (2021), &lt;em&gt;Biometrika&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;[3] S. Athey, S. Wager, &lt;a href=&#34;https://www.tandfonline.com/doi/full/10.1080/01621459.2017.1319839&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Estimation and Inference of Heterogeneous Treatment Effects using Random Forests&lt;/a&gt; (2018), &lt;em&gt;Journal of the American Statistical Association&lt;/em&gt;.&lt;/p&gt;
&lt;h3 id=&#34;related-articles&#34;&gt;Related Articles&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/99bf5cffa0d9&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Matching, Weighting, or Regression?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/b63dc69e3d8c&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;DAGs and Control Variables&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;code&#34;&gt;Code&lt;/h3&gt;
&lt;p&gt;You can find the original Jupyter Notebook here:&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/meta.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/meta.ipynb&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Weighting, Matching, or Regression?</title>
      <link>https://matteocourthoud.github.io/post/weighting_matching/</link>
      <pubDate>Mon, 10 Jul 2023 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/post/weighting_matching/</guid>
      <description>&lt;p&gt;&lt;em&gt;Understanding and comparing different methods for conditional causal inference analysis&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;AB tests or randomized controlled trials are the &lt;strong&gt;gold standard&lt;/strong&gt; in causal inference. By randomly exposing units to a treatment we make sure that individuals in both groups are comparable, on average, and any difference we observe can be attributed to the treatment effect alone.&lt;/p&gt;
&lt;p&gt;However, often the treatment and control groups are &lt;strong&gt;not perfectly comparable&lt;/strong&gt;. This could be due to the fact that randomization was not perfect or available. Not always we can randomize a treatment, for ethical or practical reasons. And even when we can, sometimes we do not have enough individuals or units so that differences between groups are seizable. This happens often, for example, when randomization is not done at the individual level, but at a higher level of aggregation, for example zipcodes, counties or even states.&lt;/p&gt;
&lt;p&gt;In these settings, we can still recover a causal estimate of the treatment effect if we have &lt;strong&gt;enough information&lt;/strong&gt; about individuals, by making the treatment and control group comparable, ex-post. In this blog post, we are going to introduce and compare different procedures to estimate causal effects in presence of imbalances between treatment and control groups that are &lt;strong&gt;fully observable&lt;/strong&gt;. In particular we are going to analyze weighting, matching and regression procedures.&lt;/p&gt;
&lt;h2 id=&#34;example&#34;&gt;Example&lt;/h2&gt;
&lt;p&gt;Assume we had blog on statistics and causal inference 😇. To improve user experience, we are considering &lt;strong&gt;releasing a dark mode&lt;/strong&gt;, and we would like to understand whether this new feature increases the time users spend on our blog.&lt;/p&gt;
&lt;img src=&#34;fig/modes.png&#34; width=&#34;600px&#34;/&gt;
&lt;p&gt;We are not a sophisticated company, therefore we do not run an AB test but we simply release the dark mode and we observe whether users select it or not and the time they spend on the blog. We know that there might be &lt;strong&gt;selection&lt;/strong&gt;:  users that prefer the dark mode could have different reading preferences and this might complicate our causal analysis.&lt;/p&gt;
&lt;p&gt;We can represent the data generating process with the following &lt;a href=&#34;https://towardsdatascience.com/b63dc69e3d8c&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Directed Acyclic Graph (DAG)&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-mermaid&#34;&gt;flowchart TB
classDef included fill:#DCDCDC,stroke:#000000,stroke-width:2px;
classDef excluded fill:#ffffff,stroke:#000000,stroke-width:2px;
classDef unobserved fill:#ffffff,stroke:#000000,stroke-width:2px,stroke-dasharray: 5 5;

X1((gender))
X2((age))
X3((hours))
D((dark mode))
Y((read time))

D --&amp;gt; Y
X1 --&amp;gt; Y
X1 --&amp;gt; D
X2 --&amp;gt; D
X3 --&amp;gt; Y

class D,Y included;
class X1,X2,X3 excluded;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We generate the simulated data using the data generating process &lt;code&gt;dgp_darkmode()&lt;/code&gt; from &lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/src/dgp.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;src.dgp&lt;/code&gt;&lt;/a&gt;. I also import some plotting functions and libraries from &lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/src/utils.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;src.utils&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;%matplotlib inline
%config InlineBackend.figure_format = &#39;retina&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from src.utils import *
from src.dgp import dgp_darkmode
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df = dgp_darkmode().generate_data()
df.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;read_time&lt;/th&gt;
      &lt;th&gt;dark_mode&lt;/th&gt;
      &lt;th&gt;male&lt;/th&gt;
      &lt;th&gt;age&lt;/th&gt;
      &lt;th&gt;hours&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;14.4&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;43.0&lt;/td&gt;
      &lt;td&gt;65.6&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;15.4&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;55.0&lt;/td&gt;
      &lt;td&gt;125.4&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;20.9&lt;/td&gt;
      &lt;td&gt;True&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;23.0&lt;/td&gt;
      &lt;td&gt;642.6&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;20.0&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;41.0&lt;/td&gt;
      &lt;td&gt;129.1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;21.5&lt;/td&gt;
      &lt;td&gt;True&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;29.0&lt;/td&gt;
      &lt;td&gt;190.2&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;We have informations on 300 users for whom we observe whether they select the &lt;code&gt;dark_mode&lt;/code&gt; (the treatment), their weekly &lt;code&gt;read_time&lt;/code&gt; (the outcome of interest) and some characteristics like &lt;code&gt;gender&lt;/code&gt;, &lt;code&gt;age&lt;/code&gt; and total &lt;code&gt;hours&lt;/code&gt; previously spend on the blog.&lt;/p&gt;
&lt;p&gt;We would like to estimate the effect of the new &lt;code&gt;dark_mode&lt;/code&gt; on users&amp;rsquo; &lt;code&gt;read_time&lt;/code&gt;. If we were runnig an &lt;a href=&#34;https://de.wikipedia.org/wiki/A/B-Test&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;AB test&lt;/strong&gt;&lt;/a&gt; or randomized control trial, we could just compare users with and without the dark mode and we could attribute the difference in average reading time to the &lt;code&gt;dark_mode&lt;/code&gt;. Let&amp;rsquo;s check what number we would get.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;np.mean(df.loc[df.dark_mode==True, &#39;read_time&#39;]) - np.mean(df.loc[df.dark_mode==False, &#39;read_time&#39;])
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;-0.4446330948042103
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Individuals that select the &lt;code&gt;dark_mode&lt;/code&gt; spend on average 1.37 hours less on the blog, per week. Should we conclude that &lt;code&gt;dark_mode&lt;/code&gt; is a &lt;strong&gt;bad idea&lt;/strong&gt;? Is this a causal effect?&lt;/p&gt;
&lt;p&gt;We did not randomize the &lt;code&gt;dark_mode&lt;/code&gt; so that users that selected it might not be directly &lt;strong&gt;comparable&lt;/strong&gt; with users that didn&amp;rsquo;t. Can we verify this concern? Partially. We can only check characteristics that we observe, &lt;code&gt;gender&lt;/code&gt;, &lt;code&gt;age&lt;/code&gt; and total &lt;code&gt;hours&lt;/code&gt; in our setting. We cannot check if users differ along other dimensions that we don&amp;rsquo;t observe.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s use the &lt;code&gt;create_table_one&lt;/code&gt; function from Uber&amp;rsquo;s &lt;a href=&#34;https://causalml.readthedocs.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;causalml&lt;/code&gt;&lt;/a&gt; package to produce a &lt;strong&gt;covariate balance table&lt;/strong&gt;, containing the average value of our observable characteristics, across treatment and control groups. As the name suggests, this should always be the first table you present in causal inference analysis.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from causalml.match import create_table_one

X = [&#39;male&#39;, &#39;age&#39;, &#39;hours&#39;]
table1 = create_table_one(df, &#39;dark_mode&#39;, X)
table1
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;Control&lt;/th&gt;
      &lt;th&gt;Treatment&lt;/th&gt;
      &lt;th&gt;SMD&lt;/th&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;Variable&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;n&lt;/th&gt;
      &lt;td&gt;151&lt;/td&gt;
      &lt;td&gt;149&lt;/td&gt;
      &lt;td&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;age&lt;/th&gt;
      &lt;td&gt;46.01 (9.79)&lt;/td&gt;
      &lt;td&gt;39.09 (11.53)&lt;/td&gt;
      &lt;td&gt;-0.6469&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;hours&lt;/th&gt;
      &lt;td&gt;337.78 (464.00)&lt;/td&gt;
      &lt;td&gt;328.57 (442.12)&lt;/td&gt;
      &lt;td&gt;-0.0203&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;male&lt;/th&gt;
      &lt;td&gt;0.34 (0.47)&lt;/td&gt;
      &lt;td&gt;0.66 (0.48)&lt;/td&gt;
      &lt;td&gt;0.6732&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;There seems to be &lt;strong&gt;some difference&lt;/strong&gt; between treatment (&lt;code&gt;dark_mode&lt;/code&gt;) and control group. In particular, users that select the &lt;code&gt;dark_mode&lt;/code&gt; are older, have spent less hours on the blog and they are more likely to be males.&lt;/p&gt;
&lt;p&gt;Another way to visually observe all the differences at once is with a &lt;strong&gt;paired violinplot&lt;/strong&gt;. The advantage of the paired violinplot is that it allows us to observe the full distribution of the variable (approximated via &lt;a href=&#34;https://en.wikipedia.org/wiki/Kernel_density_estimation&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;kernel density estimation&lt;/a&gt;).&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def plot_distributions(df, X, d):
    df_long = df.copy()[X + [d]]
    df_long[X] =(df_long[X] - df_long[X].mean()) / df_long[X].std()
    df_long = pd.melt(df_long, id_vars=d, value_name=&#39;value&#39;)
    sns.violinplot(y=&amp;quot;variable&amp;quot;, x=&amp;quot;value&amp;quot;, hue=d, data=df_long, split=True).\
        set(xlabel=&amp;quot;&amp;quot;, ylabel=&amp;quot;&amp;quot;, title=&amp;quot;Normalized Variable Distribution&amp;quot;);
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plot_distributions(df, X, &amp;quot;dark_mode&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/weighting_matching_16_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;The insight of the violinplot is very similar: it seems that users that select the &lt;code&gt;dark_mode&lt;/code&gt; are different from users that don&amp;rsquo;t.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Why do we care?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;If we do not control for the observable characteristics, we are unable to estimate the true treatment effect. In short, we cannot be certain that the difference in outcome, &lt;code&gt;read_time&lt;/code&gt;, can be attributed to the treatment, &lt;code&gt;dark_mode&lt;/code&gt;, instead of other characteristics. For example, it could be that males read less and also prefer the &lt;code&gt;dark_mode&lt;/code&gt;, therefore we observe a negative correlation even though &lt;code&gt;dark_mode&lt;/code&gt; has no effect on &lt;code&gt;read_time&lt;/code&gt; (or even positive).&lt;/p&gt;
&lt;p&gt;In terms of Dyrected Acyclic Graphs, this means that we have several &lt;a href=&#34;https://towardsdatascience.com/b63dc69e3d8c&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;backdoor paths&lt;/strong&gt;&lt;/a&gt; that we need to &lt;strong&gt;block&lt;/strong&gt; in order for our analysis to be &lt;strong&gt;causal&lt;/strong&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-mermaid&#34;&gt;flowchart TB
classDef included fill:#DCDCDC,stroke:#000000,stroke-width:2px;
classDef excluded fill:#ffffff,stroke:#000000,stroke-width:2px;
classDef unobserved fill:#ffffff,stroke:#000000,stroke-width:2px,stroke-dasharray: 5 5;

X1((gender))
X2((age))
X3((hours))
D((dark mode))
Y((read time))

D --&amp;gt; Y
X1 --&amp;gt; Y
X1 --&amp;gt; D
X2 --&amp;gt; D
X3 --&amp;gt; Y

linkStyle 0 stroke:#00ff00,stroke-width:4px;
linkStyle 1,2 stroke:#ff0000,stroke-width:4px;
class D,Y included;
class X1,X2,X3 excluded;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;How do we &lt;strong&gt;block backdoor paths&lt;/strong&gt;? By conditioning the analysis on those intermediate variables. The conditional analysis allows us to recover the average treatment effect of the &lt;code&gt;dark_mode&lt;/code&gt; on &lt;code&gt;read_time&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-mermaid&#34;&gt;flowchart TB
classDef included fill:#DCDCDC,stroke:#000000,stroke-width:2px;
classDef excluded fill:#ffffff,stroke:#000000,stroke-width:2px;
classDef unobserved fill:#ffffff,stroke:#000000,stroke-width:2px,stroke-dasharray: 5 5;

X1((gender))
X2((age))
X3((hours))
D((dark mode))
Y((read time))

D --&amp;gt; Y
X1 -.-&amp;gt; Y
X1 -.-&amp;gt; D
X2 --&amp;gt; D
X3 --&amp;gt; Y

linkStyle 0 stroke:#00ff00,stroke-width:4px;
class D,Y,X1,X2,X3 included;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;How do we &lt;strong&gt;condition the analysis&lt;/strong&gt; on &lt;code&gt;gender&lt;/code&gt;, &lt;code&gt;age&lt;/code&gt; and &lt;code&gt;hours&lt;/code&gt;? We have some options:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Matching&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Propensity score&lt;/strong&gt; weighting&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Regression&lt;/strong&gt; with control variables&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Let&amp;rsquo;s explore and compare them!&lt;/p&gt;
&lt;h2 id=&#34;conditional-analysis&#34;&gt;Conditional Analysis&lt;/h2&gt;
&lt;p&gt;We assume that for a set of subjects $i = 1, &amp;hellip;, n$ we observed a tuple $(D_i, Y_i, X_i)$ comprised of&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;a treatment assignment $D_i \in \lbrace 0, 1 \rbrace$ (&lt;code&gt;dark_mode&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;a response $Y_i \in \mathbb R$ (&lt;code&gt;read_time&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;a feature vector $X_i \in \mathbb R^n$ (&lt;code&gt;gender&lt;/code&gt;, &lt;code&gt;age&lt;/code&gt; and &lt;code&gt;hours&lt;/code&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Assumption 1 : unconfoundedness&lt;/strong&gt; (or ignorability, or selection on observables)&lt;/p&gt;
&lt;p&gt;$$
\big \lbrace Y_i^{(1)} , Y_i^{(0)} \big \rbrace \ \perp \ D_i \ | \ X_i
$$&lt;/p&gt;
&lt;p&gt;i.e. conditional on observable characteristics $X$, the treatment assignment $D$ is as good as random. What we are effectively assuming is that there is no other characteristics that we do not observe that could impact both whether a user selects the &lt;code&gt;dark_mode&lt;/code&gt; and their &lt;code&gt;read_time&lt;/code&gt;. This is a &lt;strong&gt;strong assumption&lt;/strong&gt; that is more likely to be satisfied the more individual characteristics we observe.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Assumption 2: overlap&lt;/strong&gt; (or common support)&lt;/p&gt;
&lt;p&gt;$$
\exists \eta &amp;gt; 0 \ : \ \eta \leq \mathbb E \left[ T_i = 1 \ \big | \ X_i = x \right] \leq 1-\eta
$$&lt;/p&gt;
&lt;p&gt;i.e. no observation is deterministically assigned to the treatment or control group. This is a more technical assumption that basically means that for any level of &lt;code&gt;gender&lt;/code&gt;, &lt;code&gt;age&lt;/code&gt; or &lt;code&gt;hours&lt;/code&gt;, there could exist an individual that select the &lt;code&gt;dark_mode&lt;/code&gt; and one that doesn&amp;rsquo;t. Differently from the unconfoundedness assumption, the overal assumption is &lt;strong&gt;testable&lt;/strong&gt;.&lt;/p&gt;
&lt;h3 id=&#34;matching&#34;&gt;Matching&lt;/h3&gt;
&lt;p&gt;The first and most intuitive method to perform conditional analysis is &lt;strong&gt;matching&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;idea&lt;/strong&gt; of matching is very simple. Since we are not sure whether, for example, male and female users are directly comparable, we do the analysis within gender. Instead of comparing &lt;code&gt;read_time&lt;/code&gt; across &lt;code&gt;dark_mode&lt;/code&gt; in the whole sample, we do it separately for male and female users.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_gender = pd.pivot_table(df, values=&#39;read_time&#39;, index=&#39;male&#39;, columns=&#39;dark_mode&#39;, aggfunc=np.mean)
df_gender[&#39;diff&#39;] = df_gender[1] - df_gender[0] 
df_gender
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;dark_mode&lt;/th&gt;
      &lt;th&gt;False&lt;/th&gt;
      &lt;th&gt;True&lt;/th&gt;
      &lt;th&gt;diff&lt;/th&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;male&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;20.318000&lt;/td&gt;
      &lt;td&gt;22.24902&lt;/td&gt;
      &lt;td&gt;1.931020&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;16.933333&lt;/td&gt;
      &lt;td&gt;16.89898&lt;/td&gt;
      &lt;td&gt;-0.034354&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;Now the effect of &lt;code&gt;dark_mode&lt;/code&gt; seems reversed: it is negative for male users (-0.79) but bigger and positive for female users (+1.38), suggesting a positive aggregate effect, 1.38 - 0.79 = 0.59 (assuming equal proportion of genders)! This sign reversal is a very classical example of the &lt;a href=&#34;https://en.wikipedia.org/wiki/Simpson%27s_paradox&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Simpson&amp;rsquo;s Paradox&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;This comparison was easy to perform for &lt;code&gt;gender&lt;/code&gt;, since it is a binary variable. With multiple variables, potentially continuous, matching becomes much more difficult. One common strategy is to &lt;strong&gt;match users&lt;/strong&gt; in the treatment group with the most similar user in the control group, using some sort of &lt;a href=&#34;https://en.wikipedia.org/wiki/Nearest_neighbour_algorithm&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;nearest neighbor algorithm&lt;/a&gt;. I won&amp;rsquo;t go into the algorithm details here, but we can perform the matching with the &lt;code&gt;NearestNeighborMatch&lt;/code&gt; function from the &lt;code&gt;causalml&lt;/code&gt; package.&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;NearestNeighborMatch&lt;/code&gt; function generates a new dataset where users in the treatment group have been matched 1:1 (option &lt;code&gt;ratio=1&lt;/code&gt;) to users in the control group.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from causalml.match import NearestNeighborMatch

psm = NearestNeighborMatch(replace=True, ratio=1, random_state=1)
df_matched = psm.match(data=df, treatment_col=&amp;quot;dark_mode&amp;quot;, score_cols=X)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Are the two groups more comparable now? We can produce a new version of the &lt;strong&gt;balance table&lt;/strong&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;table1_matched = create_table_one(df_matched, &amp;quot;dark_mode&amp;quot;, X)
table1_matched
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;Control&lt;/th&gt;
      &lt;th&gt;Treatment&lt;/th&gt;
      &lt;th&gt;SMD&lt;/th&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;Variable&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;n&lt;/th&gt;
      &lt;td&gt;104&lt;/td&gt;
      &lt;td&gt;104&lt;/td&gt;
      &lt;td&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;age&lt;/th&gt;
      &lt;td&gt;41.93 (10.05)&lt;/td&gt;
      &lt;td&gt;41.85 (10.02)&lt;/td&gt;
      &lt;td&gt;-0.0086&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;hours&lt;/th&gt;
      &lt;td&gt;206.92 (309.62)&lt;/td&gt;
      &lt;td&gt;209.48 (321.79)&lt;/td&gt;
      &lt;td&gt;0.0081&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;male&lt;/th&gt;
      &lt;td&gt;0.62 (0.49)&lt;/td&gt;
      &lt;td&gt;0.62 (0.49)&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;Now the average differences between the two groups have &lt;strong&gt;shrunk&lt;/strong&gt; by at least a couple of orders of magnitude. However, note how the sample size has slightly decreased (300 $\to$ 246) since (1) we only match treated users and (2) we are not able to find a good match for all of them.&lt;/p&gt;
&lt;p&gt;We can visually inspect distributional differences with the paired violinplot.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plot_distributions(df_matched, X, &amp;quot;dark_mode&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/weighting_matching_28_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;A popular way to visualize pre- and post-matching covariate balance is the &lt;strong&gt;balance plot&lt;/strong&gt; that essentially displays the standardized mean differences before and after matching, for each control variable.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def plot_balance(t1, t2, X):
    df_smd = pd.DataFrame({&amp;quot;Variable&amp;quot;: X + X,
                           &amp;quot;Sample&amp;quot;: [&amp;quot;Unadjusted&amp;quot; for _ in range(len(X))] + [&amp;quot;Adjusted&amp;quot; for _ in range(len(X))],
                           &amp;quot;Standardized Mean Difference&amp;quot;: t1[&amp;quot;SMD&amp;quot;][1:].to_list() + 
                                                           t2[&amp;quot;SMD&amp;quot;][1:].to_list()})

    sns.scatterplot(x=&amp;quot;Standardized Mean Difference&amp;quot;, y=&amp;quot;Variable&amp;quot;, hue=&amp;quot;Sample&amp;quot;, data=df_smd).\
        set(title=&amp;quot;Balance Plot&amp;quot;)
    plt.axvline(x=0, color=&#39;k&#39;, ls=&#39;--&#39;, zorder=-1, alpha=0.3);
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plot_balance(table1, table1_matched, X)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/weighting_matching_31_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;As we can see, now all differences in observable characteristics between the two groups are essentially zero. We could also compare the distributions using other metrics or test statistics, such as the &lt;a href=&#34;https://towardsdatascience.com/9b06ee4d30bf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Kolmogorov-Smirnov test statistic&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;How do we &lt;strong&gt;estimate the average treatment effect&lt;/strong&gt;? We can simply do a difference in means. An equivalent way that automatically provides standard errors is to run a linear regression of the outcome, &lt;code&gt;read_time&lt;/code&gt;, on the treatment, &lt;code&gt;dark_mode&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt; that, since we have performed the matching for each treated user, the treatment effect we are estimating is the &lt;strong&gt;average treatment effect on the treated (ATT)&lt;/strong&gt;, which can be different from the average treatment effect if the treated sample differs from the overall population (which is likely to be the case, since we are doing matching in the first place).&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;smf.ols(&amp;quot;read_time ~ dark_mode&amp;quot;, data=df_matched).fit().summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
          &lt;td&gt;&lt;/td&gt;             &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt;         &lt;td&gt;   17.0365&lt;/td&gt; &lt;td&gt;    0.469&lt;/td&gt; &lt;td&gt;   36.363&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;   16.113&lt;/td&gt; &lt;td&gt;   17.960&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;dark_mode[T.True]&lt;/th&gt; &lt;td&gt;    1.4490&lt;/td&gt; &lt;td&gt;    0.663&lt;/td&gt; &lt;td&gt;    2.187&lt;/td&gt; &lt;td&gt; 0.030&lt;/td&gt; &lt;td&gt;    0.143&lt;/td&gt; &lt;td&gt;    2.755&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;The effect is now positive, but not statistically significant.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt; that we might have matched multiple treated users with the same untreated user, violating the independence assumption across observations and, in turn, distorting inference.&lt;/p&gt;
&lt;p&gt;We have two solutions:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;cluster standard errors at the matched individual level&lt;/li&gt;
&lt;li&gt;compute standard errors via bootstrap&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;We implement the first and cluster the standard errors by the original individual identifiers (the dataframe index).&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;smf.ols(&amp;quot;read_time ~ dark_mode&amp;quot;, data=df_matched)\
    .fit(cov_type=&#39;cluster&#39;, cov_kwds={&#39;groups&#39;: df_matched.index})\
    .summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
          &lt;td&gt;&lt;/td&gt;             &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;z&lt;/th&gt;      &lt;th&gt;P&gt;|z|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt;         &lt;td&gt;   17.0365&lt;/td&gt; &lt;td&gt;    0.650&lt;/td&gt; &lt;td&gt;   26.217&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;   15.763&lt;/td&gt; &lt;td&gt;   18.310&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;dark_mode[T.True]&lt;/th&gt; &lt;td&gt;    1.4490&lt;/td&gt; &lt;td&gt;    0.821&lt;/td&gt; &lt;td&gt;    1.765&lt;/td&gt; &lt;td&gt; 0.078&lt;/td&gt; &lt;td&gt;   -0.160&lt;/td&gt; &lt;td&gt;    3.058&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;The effect is even less statistically significant.&lt;/p&gt;
&lt;h3 id=&#34;propensity-score&#34;&gt;Propensity Score&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://academic.oup.com/biomet/article/70/1/41/240879&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Rosenbaum and Rubin (1983)&lt;/a&gt; proved a very powerful result: if the &lt;strong&gt;strong ignorability assumption&lt;/strong&gt; holds, it is sufficient to condition the analysis on the probability ot treatment, the &lt;strong&gt;propensity score&lt;/strong&gt;, in order to have conditional independence.&lt;/p&gt;
&lt;p&gt;$$
\big \lbrace Y_i^{(1)} , Y_i^{(0)} \big \rbrace \ \perp \ D_i \ | \ X_i \quad \leftrightarrow \quad \big \lbrace Y_i^{(1)} , Y_i^{(0)} \big \rbrace \ \perp \ D_i \ | \ e(X_i)
$$&lt;/p&gt;
&lt;p&gt;Where $e(X_i)$ is the probability of treatment of individual $i$, given the observable characteristics $X_i$.&lt;/p&gt;
&lt;p&gt;$$
e(x) = \Pr \left( D_i = 1 \ \big | \ X_i = x \right)
$$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt; that in an AB test the propensity score is constant across individuals.&lt;/p&gt;
&lt;p&gt;The result from Rosenbaum and Rubin (1983) is incredibly &lt;strong&gt;powerful and practical&lt;/strong&gt;, since the propensity score is a &lt;strong&gt;one dimensional&lt;/strong&gt; variable, while $X$ might be very high dimensional.&lt;/p&gt;
&lt;p&gt;Under the &lt;strong&gt;unconfoundedness&lt;/strong&gt; assumption introduced above, we can rewrite the average treatment effect as&lt;/p&gt;
&lt;p&gt;$$
\tau(x) = \mathbb E \left[ Y^{(1)} - Y^{(0)} \ \big| \ X = x \right] = \mathbb E \left[ \frac{D_i Y_i}{e(X_i)} - \frac{(1-D_i) Y_i}{1-e(X_i)} \right]
$$&lt;/p&gt;
&lt;p&gt;Note that this formulation of the average treatment effect does not depend on the potential outcomes $Y_i^{(1)}$ and $Y_i^{(0)}$, but only on the observed outcomes $Y_i$.&lt;/p&gt;
&lt;p&gt;This formulation of the average treatment effect implies the &lt;strong&gt;Inverse Propensity Weighted (IPW)&lt;/strong&gt; estimator which is an unbiased estimator for the average treatment effect $\tau$.&lt;/p&gt;
&lt;p&gt;$$
\hat \tau^{IPW} = \frac{1}{n} \sum _ {i=1}^{n} \left( \frac{D_i Y_i}{e(X_i)} - \frac{(1-D_i) Y_i}{1-e(X_i)} \right)
$$&lt;/p&gt;
&lt;p&gt;This estimator is &lt;strong&gt;unfeasible&lt;/strong&gt; since we do not observe the propensity scores $e(X_i)$. However, we can estimate them. Actually, &lt;a href=&#34;https://onlinelibrary.wiley.com/doi/abs/10.1111/1468-0262.00442&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Imbens, Hirano, Ridder (2003)&lt;/a&gt; show that you &lt;strong&gt;should&lt;/strong&gt; use the estimated propensity scores even if you knew the true values (for example because you know the sampling procedure). The idea is that if the estimated propensity scores are different from the true ones, this can be informative in the estimation.&lt;/p&gt;
&lt;p&gt;There are several possible ways to estimate a probability, the simplest and most common one being &lt;a href=&#34;https://en.wikipedia.org/wiki/Logistic_regression&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;logistic regression&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from sklearn.linear_model import LogisticRegressionCV

df[&amp;quot;pscore&amp;quot;] = LogisticRegressionCV().fit(y=df[&amp;quot;dark_mode&amp;quot;], X=df[X]).predict_proba(df[X])[:,1]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It is best practice, whenever we fit a prediction model, to &lt;strong&gt;fit the model on a different sample&lt;/strong&gt; with respect to the one that we use for inference. This practice is usually called &lt;strong&gt;cross-validation&lt;/strong&gt; or cross-fitting. One of the best (but computationally expensive) cross-validation procedures is &lt;strong&gt;leave-one-out (LOO)&lt;/strong&gt; cross-fitting: when predicting the value of observation $i$ we use all observations except for $i$. We implement the LOO cross-fitting procedure using the &lt;code&gt;cross_val_predict&lt;/code&gt; and &lt;code&gt;LeaveOneOut&lt;/code&gt; functions from the &lt;a href=&#34;https://scikit-learn.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;sklearn&lt;/code&gt;&lt;/a&gt; package.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from sklearn.model_selection import cross_val_predict, LeaveOneOut

df[&#39;pscore&#39;] = cross_val_predict(estimator=LogisticRegressionCV(), 
                                 X=df[X], 
                                 y=df[&amp;quot;dark_mode&amp;quot;],
                                 cv=LeaveOneOut(),
                                 method=&#39;predict_proba&#39;,
                                 n_jobs=-1)[:,1]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;An &lt;strong&gt;important check&lt;/strong&gt; to perform after estimating propensity scores is plotting them, across the treatment and control groups. First of all, we can then observe whether the two groups are balanced or not, depending on how close the two distributions are. Moreover, we can also check how likely it is that the &lt;strong&gt;overlap assumption&lt;/strong&gt; is satisfied. Ideally both distributions should span the same interval.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sns.histplot(data=df, x=&#39;pscore&#39;, hue=&#39;dark_mode&#39;, bins=30, stat=&#39;density&#39;, common_norm=False).\
    set(ylabel=&amp;quot;&amp;quot;, title=&amp;quot;Distribution of Propensity Scores&amp;quot;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/weighting_matching_43_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;As expected, the distribution of propensity scores between the treatment and control group is &lt;strong&gt;significantly different&lt;/strong&gt;, suggesting that the two groups are hardly comparable. However, there is significant overlap in the support of the distributions, suggesting that the overlap assumption is likely to be satisfied.&lt;/p&gt;
&lt;p&gt;How do we estimate the average treatment effect?&lt;/p&gt;
&lt;p&gt;Once we have computed the propensity scores, we just need to re-weight observations by their respective propensity score. We can then either compute a difference between the weighted &lt;code&gt;read_time&lt;/code&gt; averages, or run a weighted regression of &lt;code&gt;read_time&lt;/code&gt; on &lt;code&gt;dark_mode&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;w = 1 / (df[&amp;quot;pscore&amp;quot;] * df[&amp;quot;dark_mode&amp;quot;] + (1-df[&amp;quot;pscore&amp;quot;]) * (1-df[&amp;quot;dark_mode&amp;quot;]))
smf.wls(&amp;quot;read_time ~ dark_mode&amp;quot;, weights=w, data=df).fit().summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
          &lt;td&gt;&lt;/td&gt;             &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt;         &lt;td&gt;   18.5859&lt;/td&gt; &lt;td&gt;    0.412&lt;/td&gt; &lt;td&gt;   45.110&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;   17.775&lt;/td&gt; &lt;td&gt;   19.397&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;dark_mode[T.True]&lt;/th&gt; &lt;td&gt;    1.1303&lt;/td&gt; &lt;td&gt;    0.582&lt;/td&gt; &lt;td&gt;    1.942&lt;/td&gt; &lt;td&gt; 0.053&lt;/td&gt; &lt;td&gt;   -0.015&lt;/td&gt; &lt;td&gt;    2.276&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;The effect of the &lt;code&gt;dark_mode&lt;/code&gt; is now positive and almost statistically significant, at the 5% level.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt; that the &lt;code&gt;wls&lt;/code&gt; function automatically normalizes weights so that they sum to 1, which greatly improves the stability of the estimator. In fact, the unnormalized IPW estimator can be very &lt;strong&gt;unstable&lt;/strong&gt; when the propensity scores approach zero or one.&lt;/p&gt;
&lt;p&gt;Also &lt;strong&gt;note&lt;/strong&gt; that the standard errors are not correct, since they do not take into account the extra uncertainty introduced in the estimation of the propensity score. This issue was noted by &lt;a href=&#34;https://onlinelibrary.wiley.com/doi/abs/10.3982/ECTA11293&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Abadie and Imbens (2016)&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;regression-with-control-variables&#34;&gt;Regression with Control Variables&lt;/h3&gt;
&lt;p&gt;The last method we are going to review today is &lt;strong&gt;linear regression with control variables&lt;/strong&gt;. This estimator is extremely easy to implement, since we just need to add the user characteristics - &lt;code&gt;gender&lt;/code&gt;, &lt;code&gt;age&lt;/code&gt; and &lt;code&gt;hours&lt;/code&gt; - to the regression of &lt;code&gt;read_time&lt;/code&gt; on &lt;code&gt;dark_mode&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;smf.ols(&amp;quot;read_time ~ dark_mode + male + age + hours&amp;quot;, data=df).fit().summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
          &lt;td&gt;&lt;/td&gt;             &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt;         &lt;td&gt;   16.8591&lt;/td&gt; &lt;td&gt;    1.082&lt;/td&gt; &lt;td&gt;   15.577&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;   14.729&lt;/td&gt; &lt;td&gt;   18.989&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;dark_mode[T.True]&lt;/th&gt; &lt;td&gt;    1.3858&lt;/td&gt; &lt;td&gt;    0.524&lt;/td&gt; &lt;td&gt;    2.646&lt;/td&gt; &lt;td&gt; 0.009&lt;/td&gt; &lt;td&gt;    0.355&lt;/td&gt; &lt;td&gt;    2.417&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;male&lt;/th&gt;              &lt;td&gt;   -4.4855&lt;/td&gt; &lt;td&gt;    0.499&lt;/td&gt; &lt;td&gt;   -8.990&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;   -5.468&lt;/td&gt; &lt;td&gt;   -3.504&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;age&lt;/th&gt;               &lt;td&gt;    0.0513&lt;/td&gt; &lt;td&gt;    0.022&lt;/td&gt; &lt;td&gt;    2.311&lt;/td&gt; &lt;td&gt; 0.022&lt;/td&gt; &lt;td&gt;    0.008&lt;/td&gt; &lt;td&gt;    0.095&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;hours&lt;/th&gt;             &lt;td&gt;    0.0043&lt;/td&gt; &lt;td&gt;    0.001&lt;/td&gt; &lt;td&gt;    8.427&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    0.003&lt;/td&gt; &lt;td&gt;    0.005&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;The average treatment effect is again positive and statistically significant at the 1% level!&lt;/p&gt;
&lt;h2 id=&#34;comparison&#34;&gt;Comparison&lt;/h2&gt;
&lt;p&gt;How do the different methods compare to each other?&lt;/p&gt;
&lt;h3 id=&#34;ipw-and-regression&#34;&gt;IPW and Regression&lt;/h3&gt;
&lt;p&gt;There is a &lt;strong&gt;tight connection&lt;/strong&gt; between the IPW estimator and linear regression with covariates. This is particularly evident when we have a one-dimensional, discrete covariate $X$.&lt;/p&gt;
&lt;p&gt;In this case, the estimand of IPW (i.e. the quantity that IPW estimates) is given by&lt;/p&gt;
&lt;p&gt;$$
\tau^{IPW} = \frac{ \sum_x \color{red}{\tau_x} \color{blue}{\Pr(D_i | X_i = x)} \Pr(X_i = x)}{\sum_x \color{blue}{\Pr(D_i | X_i = x)} \Pr(X_i = x)}
$$&lt;/p&gt;
&lt;p&gt;The IPW estimand is a weighted average of the treatment effects $\tau_x$, where the weights are given by the &lt;strong&gt;treatment probabilities&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;On the other hand, the estimand of linear regression with control variables is&lt;/p&gt;
&lt;p&gt;$$
\tau^{OLS} = \frac{ \sum_x \color{red}{\tau_x} \color{blue}{\Pr(D_i | X_i = x)(1 - \Pr(D_i | X_i = x)) } \Pr(X_i = x)}{\sum_x \color{blue}{\Pr(D_i | X_i = x)(1 - \Pr(D_i | X_i = x)) } \Pr(X_i = x)}
$$&lt;/p&gt;
&lt;p&gt;The OLS estimand is a weighted average of the treatment effects $\tau_x$, where the weights are given by the &lt;strong&gt;variances of the treatment probabilities&lt;/strong&gt;. This means that linear regression is a weighted estimator, that gives more weight to observations that have characteristics for which we observe more treatment variability. Since a binary random variable has the highest variance when its expected value is 0.5, &lt;strong&gt;OLS gives the most weight to observations that have characteristics for which we observe a 50/50 split between treatment and control group&lt;/strong&gt;. On the other hand, if for some characteristics we only observe treated or untreated individuals, those observations are going to receive zero weight. I recommend Chapter 3 of &lt;a href=&#34;https://www.mostlyharmlesseconometrics.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Angrist and Pischke (2009)&lt;/a&gt; for more details.&lt;/p&gt;
&lt;h3 id=&#34;ipw-and-matching&#34;&gt;IPW and Matching&lt;/h3&gt;
&lt;p&gt;As we have seen in the IPW section, &lt;a href=&#34;https://academic.oup.com/biomet/article/70/1/41/240879&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Rosenbaum and Rubin (1983)&lt;/a&gt; result tells us that we do not need to perform the analysis conditional on all the covariates $X$, but it is sufficient to condition on the propensity score $e(X)$.&lt;/p&gt;
&lt;p&gt;We have seed how this result implies a weighted estimator but it also extends to matching: we do not need to match observations on all the covariates $X$, but it is sufficient to &lt;strong&gt;match them on the propensity score&lt;/strong&gt; $e(X)$. This method is called propensity score matching.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;psm = NearestNeighborMatch(replace=False, random_state=1)
df_ipwmatched = psm.match(data=df, treatment_col=&amp;quot;dark_mode&amp;quot;, score_cols=[&#39;pscore&#39;])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As before, after matching, we can simply compute the estimate as a difference in means, remembering that observations are &lt;strong&gt;not independent&lt;/strong&gt; and therefore we need to be cautious when doing inference.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;smf.ols(&amp;quot;read_time ~ dark_mode&amp;quot;, data=df_ipwmatched)\
    .fit(cov_type=&#39;cluster&#39;, cov_kwds={&#39;groups&#39;: df_ipwmatched.index})\
    .summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
          &lt;td&gt;&lt;/td&gt;             &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;z&lt;/th&gt;      &lt;th&gt;P&gt;|z|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt;         &lt;td&gt;   18.4633&lt;/td&gt; &lt;td&gt;    0.505&lt;/td&gt; &lt;td&gt;   36.576&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;   17.474&lt;/td&gt; &lt;td&gt;   19.453&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;dark_mode[T.True]&lt;/th&gt; &lt;td&gt;    1.1888&lt;/td&gt; &lt;td&gt;    0.703&lt;/td&gt; &lt;td&gt;    1.692&lt;/td&gt; &lt;td&gt; 0.091&lt;/td&gt; &lt;td&gt;   -0.188&lt;/td&gt; &lt;td&gt;    2.566&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;The estimated effect of &lt;code&gt;dark_mode&lt;/code&gt; is positive, significant at the 1% level and very close to the true value of 2!&lt;/p&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;In this blog post, we have seen how to perform &lt;strong&gt;conditional analysis&lt;/strong&gt; using different approached. Matching directly matches most similar units in the treatment and control group. Weighting simply assigns different weight to different observations depending on their probability of receiving the treatment. Regression instead weights observations depending on the conditional treatment variances, giving more weight to observations that have characteristics common to both the treatment and control group.&lt;/p&gt;
&lt;p&gt;These procedures are &lt;strong&gt;extremely helpful&lt;/strong&gt; because they can either allow us to estimate causal effects from (very rich) observational data or correct experimental estimates when randomization was not perfect or we have a small sample.&lt;/p&gt;
&lt;p&gt;Last but not least, if you want to know more, I strongly recommend this &lt;strong&gt;video lecture&lt;/strong&gt; on propensity scores from &lt;a href=&#34;https://paulgp.github.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paul Goldsmith-Pinkham&lt;/a&gt; that is freely available online.&lt;/p&gt;
&lt;br&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/8gWctYvRzk4&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;br&gt;
&lt;p&gt;The whole course is a &lt;strong&gt;gem&lt;/strong&gt; and it is an incredible privilege to have such high quality material available online for free!&lt;/p&gt;
&lt;h3 id=&#34;references&#34;&gt;References&lt;/h3&gt;
&lt;p&gt;[1] P. Rosenbaum, D. Rubin, &lt;a href=&#34;https://academic.oup.com/biomet/article/70/1/41/240879&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The central role of the propensity score in observational studies for causal effects&lt;/a&gt; (1983), &lt;em&gt;Biometrika&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;[2] G. Imbens, K. Hirano, G. Ridder, &lt;a href=&#34;https://onlinelibrary.wiley.com/doi/abs/10.1111/1468-0262.00442&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Efficient Estimation of Average Treatment Effects Using the Estimated Propensity Score&lt;/a&gt; (2003), &lt;em&gt;Econometrica&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;[3] J. Angrist, J. S. Pischke, &lt;a href=&#34;https://www.mostlyharmlesseconometrics.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Mostly harmless econometrics: An Empiricist&amp;rsquo;s Companion&lt;/a&gt; (2009), &lt;em&gt;Princeton University Press&lt;/em&gt;.&lt;/p&gt;
&lt;h3 id=&#34;related-articles&#34;&gt;Related Articles&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/59f801eb3299&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Understanding The Frisch-Waugh-Lovell Theorem&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/9b06ee4d30bf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;How to Compare Two or More Distributions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/b63dc69e3d8c&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;DAGs and Control Variables&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;code&#34;&gt;Code&lt;/h3&gt;
&lt;p&gt;You can find the original Jupyter Notebook here:&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/ipw.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/ipw.ipynb&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Bayesian AB Testing</title>
      <link>https://matteocourthoud.github.io/post/bayes_ab/</link>
      <pubDate>Wed, 01 Feb 2023 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/post/bayes_ab/</guid>
      <description>&lt;p&gt;&lt;em&gt;An introduction to the Bayesian approach to randomized experiments.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Randomized experiments, a.k.a. &lt;strong&gt;AB tests&lt;/strong&gt;, are now the established gold standard in the industry to estimate causal effects. Randomly assigning the treatment (new product, feature, UI, &amp;hellip;) to a subset of the population (users, patients, customers, &amp;hellip;) we ensure that, on average, the difference in outcomes (revenue, visits, clicks, &amp;hellip;) can be attributed to the treatment. Established companies like &lt;a href=&#34;https://partner.booking.com/en-gb/click-magazine/industry-perspectives/role-experimentation-bookingcom&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Booking.com&lt;/a&gt; report constantly running thousands of AB tests at the same time. And newer growing companies like &lt;a href=&#34;https://blog.duolingo.com/improving-duolingo-one-experiment-at-a-time/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Duolingo&lt;/a&gt; attribute a large chunk of their success to their culture of experimentation at scale.&lt;/p&gt;
&lt;p&gt;With so many experiments, one question comes natural: in one specific experiment, can you leverage information from previous tests? How? In this post, I will try to answer these questions by introducing the &lt;strong&gt;Bayesian approach to AB testing&lt;/strong&gt;. The Bayesian framework is well suited for this type of task because it naturally allows for the updating of existing knowledge (the prior) using new data. However, the method is particularly sensitive to functional form assumptions and apparently innocuous model choices can translate in sensible differences in the estimates, especially when the data is very skewed.&lt;/p&gt;
&lt;h2 id=&#34;search-and-infinite-scrolling&#34;&gt;Search and Infinite Scrolling&lt;/h2&gt;
&lt;p&gt;For the rest of the article, we are going to use a toy example, loosely inspired by &lt;a href=&#34;https://www.aeaweb.org/articles?id=10.1257/pandp.20191003&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Azavedo et al. (2019)&lt;/a&gt;: a &lt;strong&gt;search engine&lt;/strong&gt; that wants to increase its &lt;strong&gt;ad revenue&lt;/strong&gt;, without sacrificing search quality. We are a company with an established experimentation culture and we continuously test new ideas on how to rank results, how to select the most relevant ads for consumers, and the user interface (UI) of the results page. Suppose that, in this specific case, we came up with a new brilliant idea: &lt;a href=&#34;https://blog.google/products/search/continuous-scrolling-mobile/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;infinite scrolling&lt;/a&gt;! Instead of having a discrete sequence of pages, we allow users to keep scrolling down if they want to see more results.&lt;/p&gt;
&lt;img src=&#34;fig/phones.jpg&#34; width=&#34;300px&#34;/&gt;
&lt;p&gt;To understand whether infinite scrolling works, we ran an &lt;strong&gt;AB test&lt;/strong&gt;: we randomize users into a treatment and a control group. We implement infinite scrolling only for users in the treatment group. I import the data generating process &lt;code&gt;dgp_infinite_scroll()&lt;/code&gt; from &lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/src/dgp.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;src.dgp&lt;/code&gt;&lt;/a&gt;. With respect to previous articles, I generated a new DGP parent class that handles randomization and data generation, while its children classes contain the specific use-cases. I also import some plotting functions and libraries from &lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/src/utils.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;src.utils&lt;/code&gt;&lt;/a&gt;.  To include not only code but also data and tables, I use &lt;a href=&#34;https://deepnote.com&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Deepnote&lt;/a&gt;, a Jupyter-like web-based collaborative notebook environment.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;%matplotlib inline
%config InlineBackend.figure_format = &#39;retina&#39;

from src.utils import *
from src.dgp import DGP, dgp_infinite_scroll
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;dgp = dgp_infinite_scroll(n=10_000)
df = dgp.generate_data(true_effect=0.14)
df.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;past_revenue&lt;/th&gt;
      &lt;th&gt;infinite_scroll&lt;/th&gt;
      &lt;th&gt;ad_revenue&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;3.76&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;3.56&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;2.40&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1.71&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;2.98&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;4.71&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;4.24&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;4.43&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;3.87&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;3.69&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;We have information on $10.000$ website visitors for which we observe the monthly &lt;code&gt;ad_revenue&lt;/code&gt; they generated, whether they were assigned to the treatment group and were using the &lt;code&gt;infinite_scroll&lt;/code&gt;, and also the average monthly &lt;code&gt;past_revenue&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;The random treatment assignment makes the &lt;strong&gt;difference-in-means&lt;/strong&gt; estimator &lt;a href=&#34;https://en.wikipedia.org/wiki/Bias_of_an_estimator&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;unbiased&lt;/strong&gt;&lt;/a&gt;: we expect the treatment and control group to be comparable on average, so we can causal attribute the average observed difference in outcomes to the treatment effect. We estimate the treatment effect by linear regression. We can interpret the coefficient of &lt;code&gt;infinite_scroll&lt;/code&gt; as the estimated treatment effect.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;smf.ols(&#39;ad_revenue ~ infinite_scroll&#39;, df).fit().summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
         &lt;td&gt;&lt;/td&gt;            &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt;       &lt;td&gt;    1.9906&lt;/td&gt; &lt;td&gt;    0.020&lt;/td&gt; &lt;td&gt;  100.783&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    1.952&lt;/td&gt; &lt;td&gt;    2.029&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;infinite_scroll&lt;/th&gt; &lt;td&gt;    0.1441&lt;/td&gt; &lt;td&gt;    0.028&lt;/td&gt; &lt;td&gt;    5.163&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    0.089&lt;/td&gt; &lt;td&gt;    0.199&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;It seems that the &lt;code&gt;infinite_scroll&lt;/code&gt; was indeed a good idea and it increase the average monthly revenue by $0.1524$$. Moreover, the effect is significantly different from zero at the 1% confidence level.&lt;/p&gt;
&lt;p&gt;We could further improve the precision of the estimator by controlling for &lt;code&gt;past_revenue&lt;/code&gt; in the regression. We do not expect a sensible change in the estimated coefficient, but the precision should improve (if you want to know more on out control variables, check my other articles on &lt;a href=&#34;https://towardsdatascience.com/understanding-cuped-a822523641af&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CUPED&lt;/a&gt; and &lt;a href=&#34;https://towardsdatascience.com/controls-b63dc69e3d8c&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;DAGs&lt;/a&gt;).&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;reg = smf.ols(&#39;ad_revenue ~ infinite_scroll + past_revenue&#39;, df).fit()
reg.summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
         &lt;td&gt;&lt;/td&gt;            &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt;       &lt;td&gt;    0.0170&lt;/td&gt; &lt;td&gt;    0.024&lt;/td&gt; &lt;td&gt;    0.696&lt;/td&gt; &lt;td&gt; 0.487&lt;/td&gt; &lt;td&gt;   -0.031&lt;/td&gt; &lt;td&gt;    0.065&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;infinite_scroll&lt;/th&gt; &lt;td&gt;    0.1588&lt;/td&gt; &lt;td&gt;    0.020&lt;/td&gt; &lt;td&gt;    7.992&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    0.120&lt;/td&gt; &lt;td&gt;    0.198&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;past_revenue&lt;/th&gt;    &lt;td&gt;    0.9923&lt;/td&gt; &lt;td&gt;    0.010&lt;/td&gt; &lt;td&gt;   98.659&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    0.973&lt;/td&gt; &lt;td&gt;    1.012&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;Indeed, &lt;code&gt;past_revenue&lt;/code&gt; is highly predictive of current &lt;code&gt;ad_revenue&lt;/code&gt; and the precision of the estimated coefficient for &lt;code&gt;infinite_scroll&lt;/code&gt; decreases by one-third.&lt;/p&gt;
&lt;p&gt;So far, everything has been very standard. However, as we said at the beginning, suppose this is not the only experiment we ran trying to improve our browser (and ultimately ad revenue). The infinite scroll is just one idea among thousands of others that we have tested in the past. Is there a way to efficiently use this additional &lt;strong&gt;information&lt;/strong&gt;?&lt;/p&gt;
&lt;h2 id=&#34;bayesian-statistics&#34;&gt;Bayesian Statistics&lt;/h2&gt;
&lt;p&gt;One of the main advantages of Bayesian statistics over the frequentist approach is that it easily allows to incorporate additional information into a model. The idea directly follows from the main results behind all Bayesian statistics: &lt;a href=&#34;https://en.wikipedia.org/wiki/Bayes%27_theorem&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Bayes Theorem&lt;/strong&gt;&lt;/a&gt;. Bayes theorem, allows you to do inference on a model by &lt;strong&gt;inverting the inference problem&lt;/strong&gt;: from the probability of the model given the data, to the probability of the data given the model, a much easier object to deal with.&lt;/p&gt;
&lt;p&gt;$$
\underbrace{ \Pr \big( \text{model} \ \big| \ \text{data} \big) }&lt;em&gt;{\text{posterior}} = \underbrace{ \Pr(\text{model}) }&lt;/em&gt;{\text{prior}} \ \underbrace{ \frac{ \Pr \big( \text{data} \ \big| \ \text{model} \big) }{ \Pr(\text{data}) } }_{\text{likelihood}}
$$&lt;/p&gt;
&lt;p&gt;We can split the right-hand side of Bayes Theorem (or Rule) into two components: the &lt;strong&gt;prior&lt;/strong&gt; and the &lt;strong&gt;likelihood&lt;/strong&gt;. The likelihood is the information about the model that comes from the data, the prior instead is any additional information about the model.&lt;/p&gt;
&lt;p&gt;First of all, let&amp;rsquo;s map Bayes theorem into our context. What is the data, what is the model and what is our object of interest?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the &lt;strong&gt;data&lt;/strong&gt; which consists in our outcome variable &lt;code&gt;ad_revenue&lt;/code&gt;, $y$, the treatment &lt;code&gt;infinite_scroll&lt;/code&gt;, $D$ and the other variables, &lt;code&gt;past_revenue&lt;/code&gt; and a constant, which we jointly denote as $X$&lt;/li&gt;
&lt;li&gt;the &lt;strong&gt;model&lt;/strong&gt; is the distribution of &lt;code&gt;ad_revenue&lt;/code&gt;, given &lt;code&gt;past_revenue&lt;/code&gt; and the &lt;code&gt;infinite_scroll&lt;/code&gt; feature, $y | D, X$&lt;/li&gt;
&lt;li&gt;our &lt;strong&gt;object of interest&lt;/strong&gt; is the posterior $\Pr \big( \text{model} \ \big| \ \text{data} \big)$, in particular the relationship between &lt;code&gt;ad_revenue&lt;/code&gt; and &lt;code&gt;infinite_scroll&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;X = sm.add_constant(df[[&#39;past_revenue&#39;]].values)
D = df[&#39;infinite_scroll&#39;].values
y = df[&#39;ad_revenue&#39;].values
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;How do we use prior information in the context of AB testing, potentially including additional covariates?&lt;/p&gt;
&lt;h3 id=&#34;bayesian-regression&#34;&gt;Bayesian Regression&lt;/h3&gt;
&lt;p&gt;Let&amp;rsquo;s use a linear model to make it directly comparable with the frequentist approach:&lt;/p&gt;
&lt;p&gt;$$
y_i = \beta X_i + \tau D_i + \varepsilon_i \qquad \text{where} \quad \varepsilon_i \sim N \big( 0, \sigma^2 \big)
$$&lt;/p&gt;
&lt;p&gt;This is a parametric model with &lt;strong&gt;two sets of parameters&lt;/strong&gt;: the linear coefficients $\beta$ and $\tau$, and the variance of the residuals $\sigma$. An equivalent, but more Bayesian, way to write the model is:&lt;/p&gt;
&lt;p&gt;$$
y \ | \ X, D; \beta, \tau, \sigma \sim N \Big( \beta X + \tau D \ , \sigma^2 \Big) ,
$$&lt;/p&gt;
&lt;p&gt;where the semi-column separates the data from the model parameters. Differently from the frequentist approach, in Bayesian regressions we do not rely on the &lt;a href=&#34;https://en.wikipedia.org/wiki/Central_limit_theorem&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;central limit theorem&lt;/a&gt; to approximate the conditional distribution of $y$, but we directly &lt;strong&gt;assume&lt;/strong&gt; it is normal. Is it just a formality? Not really, but a proper comparison between the frequentist and Bayesian approach is beyond the scope of this article.&lt;/p&gt;
&lt;p&gt;We are interested in doing inference on the model parameters, $\beta$, $\tau$, and $\sigma$. Another &lt;strong&gt;core difference&lt;/strong&gt; between the frequentist and the Bayesian approach is that the the first assumes that the model parameters are fixed (scalars), while the latter allows them to be stochastic (random variables).&lt;/p&gt;
&lt;p&gt;This assumption has a very practical &lt;strong&gt;implication&lt;/strong&gt;: you can easily incorporate previous information about the model parameters in the form of &lt;strong&gt;prior&lt;/strong&gt; distributions. As the name says, priors contain information that was available even &lt;em&gt;before&lt;/em&gt; looking at the data. This leads to one of the most relevant questions in Bayesian statistics: &lt;strong&gt;how do you chose a prior&lt;/strong&gt;?&lt;/p&gt;
&lt;h2 id=&#34;priors&#34;&gt;Priors&lt;/h2&gt;
&lt;p&gt;When choosing a prior, one analytically appealing restriction is to have a prior distribution such that the posterior belongs to the same family. These priors are called &lt;strong&gt;conjugate priors&lt;/strong&gt;. For example, before seeing the data, I assume my treatment effect is normally distributed and I would like it to be normally distributed also after incorporating the information contained in the data.&lt;/p&gt;
&lt;p&gt;In the case of Bayesian linear regression, the conjugate priors for $\beta$ and $\sigma$ are normally and inverse-gamma distributed. Let&amp;rsquo;s start a bit blindly, by taking a standard normal and inverse gamma distribution as prior.&lt;/p&gt;
&lt;p&gt;$$
\beta_i \sim N(\boldsymbol 0, \boldsymbol 1) \
\tau_i \sim N(0,1) \
\sigma^2 \sim \Gamma^{-1} (1, 1)
$$&lt;/p&gt;
&lt;p&gt;We use the package &lt;a href=&#34;https://www.pymc.io/projects/docs/en/stable/learn.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PyMC&lt;/a&gt; to do inference. First we need to specify the model: what are the distributions of the different parameters (priors) and what is the likelihood of the data.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import pymc as pm
with pm.Model() as baseline_model:

    # Priors
    beta = pm.MvNormal(&#39;beta&#39;, mu=np.ones(np.shape(X)[1]), cov=np.eye(np.shape(X)[1]))
    tau = pm.Normal(&#39;tau&#39;, mu=0, sigma=1)
    sigma = pm.InverseGamma(&#39;sigma&#39;, mu=1, sigma=1, initval=1)
    
    # Likelihood 
    Ylikelihood = pm.Normal(&#39;y&#39;, mu=(X@beta + D@tau).flatten(), sigma=sigma, observed=y)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;PyMC has an extremely nice function that allows us to visualize the model as a graph, &lt;code&gt;model_to_graphviz&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;pm.model_to_graphviz(baseline_model)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/bayes_ab_25_0.svg&#34; alt=&#34;svg&#34;&gt;&lt;/p&gt;
&lt;p&gt;From the graphical representation, we can see the various model components, their distributions, and how they interact with each other.&lt;/p&gt;
&lt;p&gt;We are now ready to &lt;strong&gt;compute&lt;/strong&gt; the model posterior. How does it work? In short, we sample realizations of model parameters, we compute the likelihood of the data given those values and the compute the corresponding posterior.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;idata = pm.sample(model=baseline_model, draws=1000)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Auto-assigning NUTS sampler...
Initializing NUTS using jitter+adapt_diag...
Multiprocess sampling (4 chains in 4 jobs)
NUTS: [beta, tau, sigma]
&lt;/code&gt;&lt;/pre&gt;
&lt;style&gt;
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
&lt;/style&gt;
&lt;div&gt;
  &lt;progress value=&#39;8000&#39; class=&#39;&#39; max=&#39;8000&#39; style=&#39;width:300px; height:20px; vertical-align: middle;&#39;&gt;&lt;/progress&gt;
  100.00% [8000/8000 00:03&amp;lt;00:00 Sampling 4 chains, 0 divergences]
&lt;/div&gt;
&lt;pre&gt;&lt;code&gt;Sampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 4_000 draws total) took 15 seconds.
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The fact that Bayesian inference requires sampling, has been historically one of the main bottlenecks of Bayesian statistics, since it makes it sensibly slower than the frequentist approach. However, this is less and less of a problem with the increased computational power of model computers.&lt;/p&gt;
&lt;p&gt;We are now ready to print out results. First, with the &lt;code&gt;summary()&lt;/code&gt; method, we can print a model summary very similar to those produced by the &lt;code&gt;statsmodels&lt;/code&gt; package.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;pm.summary(idata, hdi_prob=0.95).round(4)
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;mean&lt;/th&gt;
      &lt;th&gt;sd&lt;/th&gt;
      &lt;th&gt;hdi_2.5%&lt;/th&gt;
      &lt;th&gt;hdi_97.5%&lt;/th&gt;
      &lt;th&gt;mcse_mean&lt;/th&gt;
      &lt;th&gt;mcse_sd&lt;/th&gt;
      &lt;th&gt;ess_bulk&lt;/th&gt;
      &lt;th&gt;ess_tail&lt;/th&gt;
      &lt;th&gt;r_hat&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;beta[0]&lt;/th&gt;
      &lt;td&gt;0.017&lt;/td&gt;
      &lt;td&gt;0.025&lt;/td&gt;
      &lt;td&gt;-0.029&lt;/td&gt;
      &lt;td&gt;0.067&lt;/td&gt;
      &lt;td&gt;0.001&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;1765.0&lt;/td&gt;
      &lt;td&gt;2233.0&lt;/td&gt;
      &lt;td&gt;1.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;beta[1]&lt;/th&gt;
      &lt;td&gt;0.992&lt;/td&gt;
      &lt;td&gt;0.010&lt;/td&gt;
      &lt;td&gt;0.972&lt;/td&gt;
      &lt;td&gt;1.012&lt;/td&gt;
      &lt;td&gt;0.000&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;1810.0&lt;/td&gt;
      &lt;td&gt;1964.0&lt;/td&gt;
      &lt;td&gt;1.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;tau&lt;/th&gt;
      &lt;td&gt;0.159&lt;/td&gt;
      &lt;td&gt;0.020&lt;/td&gt;
      &lt;td&gt;0.120&lt;/td&gt;
      &lt;td&gt;0.200&lt;/td&gt;
      &lt;td&gt;0.000&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;2885.0&lt;/td&gt;
      &lt;td&gt;1792.0&lt;/td&gt;
      &lt;td&gt;1.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;sigma&lt;/th&gt;
      &lt;td&gt;0.993&lt;/td&gt;
      &lt;td&gt;0.007&lt;/td&gt;
      &lt;td&gt;0.980&lt;/td&gt;
      &lt;td&gt;1.008&lt;/td&gt;
      &lt;td&gt;0.000&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;3537.0&lt;/td&gt;
      &lt;td&gt;2692.0&lt;/td&gt;
      &lt;td&gt;1.0&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;The estimated parameters are extremely close to the ones we got with the frequentist approach, with an estimated effect of the &lt;code&gt;infinite_scroll&lt;/code&gt; equal to $0.157$.&lt;/p&gt;
&lt;p&gt;If sampling had the disadvantage of being slow, it has the advantage of being very &lt;strong&gt;transparent&lt;/strong&gt;. We can directly plot the distribution of the posterior. Let&amp;rsquo;s do it for the treatment effect $\tau$. The PyMC function &lt;code&gt;plot_posterior&lt;/code&gt; plots the distribution of the posterior, with a black bar for the Bayesian equivalent of a 95% confidence interval.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;pm.plot_posterior(idata, kind=&amp;quot;hist&amp;quot;, var_names=(&#39;tau&#39;), hdi_prob=0.95, figsize=(6, 3), bins=30); 
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/bayes_ab_31_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;As expected, since we chose conjugate priors, the posterior distribution looks gaussian.&lt;/p&gt;
&lt;p&gt;So far we have chosen the prior without much guidance. However, suppose we had access to past experiments. How do we incorporate this specific information?&lt;/p&gt;
&lt;h2 id=&#34;past-experiments&#34;&gt;Past Experiments&lt;/h2&gt;
&lt;p&gt;Suppose the idea of the infinite scroll, was just one among a ton of other ones that we tried and tested in the past. For each idea we have the data for the corresponding experiment, with the corresponding estimated coefficient. Suppose we had a thousand of them.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;past_experiments = [dgp.generate_data(seed_data=i) for i in range(1000)]
taus = [smf.ols(&#39;ad_revenue ~ infinite_scroll + past_revenue&#39;, pe).fit().params.values for pe in past_experiments]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;How do we use this additional information?&lt;/p&gt;
&lt;h3 id=&#34;normal-prior&#34;&gt;Normal Prior&lt;/h3&gt;
&lt;p&gt;The first idea could be to calibrate our prior to reflect the data distribution in the past. Keeping the normality assumption, we use the estimated average and standard deviations of the estimates from past experiments.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;taus_mean = np.mean(taus, axis=0)[1]
taus_mean
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;0.0047987091716528915
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;On average, had practically no effect on &lt;code&gt;ad_revenue&lt;/code&gt;, with a average effect of $0.0009$.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;taus_std = np.sqrt(np.cov(taus, rowvar=0)[1,1])
taus_std
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;0.15153398725701195
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;However, there was sensible variation across experiments, with a standard deviation of $0.029$.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s now estimate the&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;with pm.Model() as model_normal_prior:
    k = np.shape(X)[1]
    beta = pm.MvNormal(&#39;beta&#39;, mu=np.ones(k), cov=np.eye(k))
    tau = pm.Normal(&#39;tau&#39;, mu=taus_mean, sigma=taus_std)
    sigma = pm.InverseGamma(&#39;sigma&#39;, mu=1, sigma=1, initval=1)
    Ylikelihood = pm.Normal(&#39;y&#39;, mu=(X@beta + D@tau).flatten(), sigma=sigma, observed=y)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s sample from the model.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;idata_normal_prior = pm.sample(model=model_normal_prior, draws=1000)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Auto-assigning NUTS sampler...
Initializing NUTS using jitter+adapt_diag...
Multiprocess sampling (4 chains in 4 jobs)
NUTS: [beta, tau, sigma]
&lt;/code&gt;&lt;/pre&gt;
&lt;style&gt;
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
&lt;/style&gt;
&lt;div&gt;
  &lt;progress value=&#39;8000&#39; class=&#39;&#39; max=&#39;8000&#39; style=&#39;width:300px; height:20px; vertical-align: middle;&#39;&gt;&lt;/progress&gt;
  100.00% [8000/8000 00:03&amp;lt;00:00 Sampling 4 chains, 0 divergences]
&lt;/div&gt;
&lt;pre&gt;&lt;code&gt;Sampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 4_000 draws total) took 14 seconds.
The acceptance probability does not match the target. It is 0.9025, but should be close to 0.8. Try to increase the number of tuning steps.
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And plot the sample posterior distribution of the treatment effect parameter $\tau$.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;pm.plot_posterior(idata_normal_prior, kind=&amp;quot;hist&amp;quot;, var_names=(&#39;tau&#39;), hdi_prob=0.95, figsize=(6, 3), bins=30); 
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/bayes_ab_47_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;The estimated coefficient is sensibly smaller: $0.08$ instead of the previous estimate of $0.12$. Why is it the case?&lt;/p&gt;
&lt;p&gt;The fact is that the previous coefficient of $0.12$ is extremely unlikey, given our prior. We can compute the probability of getting the same or a more extreme value, given the prior.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;1 - sp.stats.norm(taus_mean, taus_std).cdf(0.12)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;0.22355735943737898
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The probability of such value is almost zero. Therefore, the estimated coefficient has moved towards the prior mean of $0.0009$.&lt;/p&gt;
&lt;h3 id=&#34;student-t-prior&#34;&gt;Student t Prior&lt;/h3&gt;
&lt;p&gt;So far, we have assumed a normal distribution for all linear coefficients. Is it appropriate? Let&amp;rsquo;s check it visually (check &lt;a href=&#34;https://medium.com/towards-data-science/how-to-compare-two-or-more-distributions-9b06ee4d30bf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt; for other methods on how to compare distributions).&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sns.histplot([tau[0] for tau in taus]).set(title=r&#39;Distribution of $\hat{\beta}_0$ in past experiments&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/bayes_ab_53_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;The distribution seems pretty normal. What the treatment effect paramenter $\tau$?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;fig, ax = plt.subplots()
sns.histplot([tau[1] for tau in taus], label=&#39;past experiments&#39;);
ax.axvline(reg.params[&#39;infinite_scroll&#39;], lw=2, c=&#39;C3&#39;, ls=&#39;--&#39;, label=&#39;current experiment&#39;)
plt.legend();
plt.title(r&#39;Distribution of $\hat{\tau}$ in past experiments&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/bayes_ab_55_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;The distribution very &lt;strong&gt;heavy tailed&lt;/strong&gt;! While at the center it looks like a normal distributions, the tails are much &amp;ldquo;fatter&amp;rdquo; and we have a couple of very extreme values. excluding the case of measurement error, this is a setting that happens often in the industry, where most ideas have extremely small or null effects and very rarely an idea is actually a breakthrough.&lt;/p&gt;
&lt;p&gt;One way to model this distribution is a &lt;a href=&#34;&#34;&gt;student-t distribution&lt;/a&gt;. In particular, we use a t-student with mean $0.0009$, variance $0.003$ and $1.3$ degrees of freedom to match the moments of the empirical distributions of past estimates.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;with pm.Model() as model_studentt_prior:

    # Priors
    k = np.shape(X)[1]
    beta = pm.MvNormal(&#39;beta&#39;, mu=np.ones(k), cov=np.eye(k))
    tau = pm.StudentT(&#39;tau&#39;, mu=taus_mean, sigma=0.003, nu=1.3)
    sigma = pm.InverseGamma(&#39;sigma&#39;, mu=1, sigma=1, initval=1)
    
    # Likelihood 
    Ylikelihood = pm.Normal(&#39;y&#39;, mu=(X@beta + D@tau).flatten(), sigma=sigma, observed=y)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s sample from the model.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;idata_studentt_priors = pm.sample(model=model_studentt_prior, draws=1000)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Auto-assigning NUTS sampler...
Initializing NUTS using jitter+adapt_diag...
Multiprocess sampling (4 chains in 4 jobs)
NUTS: [beta, tau, sigma]
&lt;/code&gt;&lt;/pre&gt;
&lt;style&gt;
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
&lt;/style&gt;
&lt;div&gt;
  &lt;progress value=&#39;8000&#39; class=&#39;&#39; max=&#39;8000&#39; style=&#39;width:300px; height:20px; vertical-align: middle;&#39;&gt;&lt;/progress&gt;
  100.00% [8000/8000 00:03&amp;lt;00:00 Sampling 4 chains, 0 divergences]
&lt;/div&gt;
&lt;pre&gt;&lt;code&gt;Sampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 4_000 draws total) took 15 seconds.
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And plot the sample posterior distribution of the treatment effect parameter $\tau$.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;pm.plot_posterior(idata_studentt_priors, kind=&amp;quot;hist&amp;quot;, var_names=(&#39;tau&#39;), hdi_prob=0.95, figsize=(6, 3), bins=30); 
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/bayes_ab_61_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;The estimated coefficient is now again similar to the one we got with the standard normal prior, $0.11$. However, the estimate is more precise since the confidence interval has shrunk from $[0.077, 0.016]$ to $[0.065, 0.015]$.&lt;/p&gt;
&lt;p&gt;What has happened?&lt;/p&gt;
&lt;h3 id=&#34;shrinking&#34;&gt;Shrinking&lt;/h3&gt;
&lt;p&gt;The answer lies in the shape of the different &lt;strong&gt;prior distributions&lt;/strong&gt; that we have used:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;standard normal, $N(0,1)$&lt;/li&gt;
&lt;li&gt;normal with matched moments, $N(0, 0.03)$&lt;/li&gt;
&lt;li&gt;t-student with matched moments, $t_{1.3}$(0, 0.003)$&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;t_hats = np.linspace(-0.3, 0.3, 1_000)
distributions = {
    &#39;N(0,1)&#39;: sp.stats.norm(0, 1).pdf(t_hats),
    &#39;N(0, 0.03)&#39;: sp.stats.norm(0, 0.03).pdf(t_hats),
    &#39;$t_{1.3}$(0, 0.003)&#39;: sp.stats.t(df=1.3).pdf(t_hats / 0.003)*300,
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s plot all of them together.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;for i, (label, y) in enumerate(distributions.items()):
    sns.lineplot(x=t_hats, y=y, color=f&#39;C{i}&#39;, label=label);
plt.legend(); 
plt.title(&#39;Prior Distributions&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/bayes_ab_67_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;As we can see, all distributions are centered on zero, but they have very different shapes. The standard normal distribution is essentially flat over the $[-0.15, 0.15]$ interval. Every value has basically the same probability. The last two instead, even though they have the same mean and variance, have very different shapes.&lt;/p&gt;
&lt;p&gt;How does it translate into our estimation? We can plot the implied posterior for different estimates, for each prior distribution.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def compute_posterior(b, prior):
    likelihood = sp.stats.norm(b, taus_std).pdf(t_hats)
    return np.average(t_hats, weights=prior*likelihood)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;fig, ax = plt.subplots(figsize=(7,6))
ax.axvline(0, lw=1.5, c=&#39;k&#39;);
ax.axhline(0, lw=1.5, c=&#39;k&#39;);
ax.axvline(reg.params[&#39;infinite_scroll&#39;], lw=2, ls=&#39;--&#39;, c=&#39;darkgray&#39;);
for i, (label, y) in enumerate(distributions.items()):
    sns.lineplot(x=t_hats, y=[compute_posterior(t, y) for t in t_hats] , color=f&#39;C{i}&#39;, label=label);
ax.set_xlim(-0.17, 0.17);
ax.set_ylim(-0.17, 0.17);
plt.legend(); 
ax.set_xlabel(&#39;Experiment Estimate&#39;);
ax.set_ylabel(&#39;Posterior&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/bayes_ab_70_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;As we can see, the different priors transform the experimental estimates in very different ways. The standard normal prior essentially has no effect for estimates in the $[-0.15, 0.15]$ interval. The normal prior with matched moments instead shrinks each estimate by approximately 2/3. The effect of the t-student prior is instead &lt;strong&gt;non-linear&lt;/strong&gt;: it shrinks small estimates towards zero, while it keeps large estimates as they are.&lt;/p&gt;
&lt;p&gt;My &lt;strong&gt;intuition&lt;/strong&gt; is the following. A prior distribution very skewed or with &amp;ldquo;fat tails&amp;rdquo; means that large values are rare but not impossible. In practice, it means accepting that breakthrough improvements are possible. On the other hand, for the same variance, the distribution is more concentrated around zero than a standard normal so that small values are shrunk even more.&lt;/p&gt;
&lt;img src=&#34;fig/scroll.jpg&#34; width=&#34;300px&#34;/&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;In this article we have seen how to extend the analysis of AB test to incorporate &lt;strong&gt;information from past experiments&lt;/strong&gt;. In particular, we have seen the importance of choosing a prior. Selecting the distribution function is just as important as tuning its parameters. The shape of the prior distribution can drastically affect our inference, especially in a world with skewed distributions.&lt;/p&gt;
&lt;p&gt;Despite the length of the article, this was just a glimpse in the world of &lt;strong&gt;AB testing and Bayesian statistics&lt;/strong&gt;. While being computationally more intensive and requiring additional assumptions, the Bayesian approach is often more natural, powerful and flexible than the frequentist one. Knowing pros and cons of both approaches is crucial to get the best of both worlds, picking the approach that work best or combining them efficiently.&lt;/p&gt;
&lt;h3 id=&#34;references&#34;&gt;References&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;E. Azevedo, A. Deng, J. Olea, G. Weyl, &lt;a href=&#34;https://www.aeaweb.org/articles?id=10.1257/pandp.20191003&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Empirical Bayes Estimation of Treatment Effects with Many A/B Tests: An Overview&lt;/a&gt; (2019). &lt;em&gt;AEA Papers and Proceedings&lt;/em&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;A. Deng, &lt;a href=&#34;https://dl.acm.org/doi/abs/10.1145/2740908.2742563&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Objective Bayesian Two Sample Hypothesis Testing for Online Controlled Experiments&lt;/a&gt; (2018), &lt;em&gt;WWW15&lt;/em&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;related-articles&#34;&gt;Related Articles&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://medium.com/towards-data-science/the-bayesian-bootstrap-6ca4a1d45148&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The Bayesian Bootstrap&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://towardsdatascience.com/understanding-cuped-a822523641af&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Understanding CUPED&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://towardsdatascience.com/controls-b63dc69e3d8c&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;DAGs and Control Variables&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;code&#34;&gt;Code&lt;/h3&gt;
&lt;p&gt;You can find the original Jupyter Notebook here:&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/bayes_ab.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/bayes_ab.ipynb&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Bayesian AB Testing</title>
      <link>https://matteocourthoud.github.io/post/bayes_reg/</link>
      <pubDate>Mon, 09 Jan 2023 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/post/bayes_reg/</guid>
      <description>&lt;p&gt;&lt;em&gt;An introduction to the Bayesian approach to randomized experiments.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;AB testing&lt;/strong&gt; and experimentation is now the established gold standard in the industry to estimate causal effects. Randomly assigning the treatment (new product, feature, UI, &amp;hellip;) to a subset of the population (users, patients, customers, &amp;hellip;) we ensure that, on average, the difference in outcomes (revenue, visits, clicks, &amp;hellip;) can be attributed to the treatment. Many companies, especially in tech, before implementing any major change test them to back up their decisions with numbers. Established companies like &lt;a href=&#34;https://partner.booking.com/en-gb/click-magazine/industry-perspectives/role-experimentation-bookingcom&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Booking.com&lt;/a&gt; report constantly running thousands of AB tests at the same time. And newer growing companies like &lt;a href=&#34;https://blog.duolingo.com/improving-duolingo-one-experiment-at-a-time/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Duolingo&lt;/a&gt; attribute a large chunk of their success to their culture of experimentation at scale.&lt;/p&gt;
&lt;p&gt;With so many experiments, one question comes natural: in one specific experiment, can you leverage information from previous tests? In this post, I will try to answer this question by introducing the &lt;strong&gt;Bayesian approach to AB testing&lt;/strong&gt;. The Bayesian framework is particularly well suited for this type of task because it naturally allows for the updating of existing knowledge (the prior) using new data. However, the method is particularly sensitive to functional form assumptions and apparently innocuous model choices can translate in sensible differences in the estimates, especially when the data is very skewed.&lt;/p&gt;
&lt;h2 id=&#34;search-and-infinite-scrolling&#34;&gt;Search and Infinite Scrolling&lt;/h2&gt;
&lt;p&gt;For the rest of the article, we are going to use a toy example, loosely inspired by &lt;a href=&#34;https://www.aeaweb.org/articles?id=10.1257/pandp.20191003&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Azavedo et al. (2019)&lt;/a&gt;: a &lt;strong&gt;search engine&lt;/strong&gt; that wants to increase its &lt;strong&gt;ad revenue&lt;/strong&gt;, without sacrificing search quality. We are a company with an established experimentation culture and we continuously test new ideas on how to rank results, how to select the most relevant ads for consumers, and the user interface (UI) of the results page. Suppose that, in this specific case, we came up with a new brilliant idea: &lt;a href=&#34;https://blog.google/products/search/continuous-scrolling-mobile/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;infinite scrolling&lt;/a&gt;! Instead of having a discrete sequence of pages, we allow users to keep scrolling down if they want to see more results.&lt;/p&gt;
&lt;img src=&#34;fig/phones.jpg&#34; width=&#34;300px&#34;/&gt;
&lt;p&gt;To understand whether infinite scrolling works, we ran an &lt;strong&gt;AB test&lt;/strong&gt;: we randomize users into a treatment and a control group. We implement infinite scrolling only for users in the treatment group. I import the data generating process &lt;code&gt;dgp_infinite_scroll()&lt;/code&gt; from &lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/src/dgp.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;src.dgp&lt;/code&gt;&lt;/a&gt;. With respect to previous articles, I generated a new DGP parent class that handles randomization and data generation, while its children classes contain the specific use-cases. I also import some plotting functions and libraries from &lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/src/utils.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;src.utils&lt;/code&gt;&lt;/a&gt;.  To include not only code but also data and tables, I use &lt;a href=&#34;https://deepnote.com&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Deepnote&lt;/a&gt;, a Jupyter-like web-based collaborative notebook environment.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;%matplotlib inline
%config InlineBackend.figure_format = &#39;retina&#39;

from src.utils import *
from src.dgp import DGP, dgp_infinite_scroll
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;dgp = dgp_infinite_scroll(n=10_000)
df = dgp.generate_data(true_effect=0.14)
df.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;past_revenue&lt;/th&gt;
      &lt;th&gt;infinite_scroll&lt;/th&gt;
      &lt;th&gt;ad_revenue&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;3.76&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;3.70&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;2.40&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1.71&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;2.98&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;4.85&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;4.24&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;4.57&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;3.87&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;3.69&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;We have information on $10.000$ website visitors for which we observe the monthly &lt;code&gt;ad_revenue&lt;/code&gt; they generated, whether they were assigned to the treatment group and were using the &lt;code&gt;infinite_scroll&lt;/code&gt;, and also the average monthly &lt;code&gt;past_revenue&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;The random treatment assignment makes the &lt;strong&gt;difference-in-means&lt;/strong&gt; estimator &lt;a href=&#34;https://en.wikipedia.org/wiki/Bias_of_an_estimator&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;unbiased&lt;/strong&gt;&lt;/a&gt;: we expect the treatment and control group to be comparable on average, so we can causal attribute the average observed difference in outcomes to the treatment effect. We estimate the treatment effect by linear regression. We can interpret the coefficient of &lt;code&gt;infinite_scroll&lt;/code&gt; as the estimated treatment effect.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;smf.ols(&#39;ad_revenue ~ infinite_scroll&#39;, df).fit().summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
         &lt;td&gt;&lt;/td&gt;            &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt;       &lt;td&gt;    1.9865&lt;/td&gt; &lt;td&gt;    0.020&lt;/td&gt; &lt;td&gt;  101.320&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    1.948&lt;/td&gt; &lt;td&gt;    2.025&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;infinite_scroll&lt;/th&gt; &lt;td&gt;    0.1524&lt;/td&gt; &lt;td&gt;    0.028&lt;/td&gt; &lt;td&gt;    5.461&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    0.098&lt;/td&gt; &lt;td&gt;    0.207&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;It seems that the &lt;code&gt;infinite_scroll&lt;/code&gt; was indeed a good idea and it increase the average monthly revenue by $0.1524$$. Moreover, the effect is significantly different from zero at the 1% confidence level.&lt;/p&gt;
&lt;p&gt;We could further improve the precision of the estimator by controlling for &lt;code&gt;past_revenue&lt;/code&gt; in the regression. We do not expect a sensible change in the estimated coefficient, but the precision should improve (if you want to know more on out control variables, check my other articles on &lt;a href=&#34;https://towardsdatascience.com/understanding-cuped-a822523641af&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CUPED&lt;/a&gt; and &lt;a href=&#34;https://towardsdatascience.com/controls-b63dc69e3d8c&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;DAGs&lt;/a&gt;).&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;reg = smf.ols(&#39;ad_revenue ~ infinite_scroll + past_revenue&#39;, df).fit()
reg.summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
         &lt;td&gt;&lt;/td&gt;            &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt;       &lt;td&gt;    0.0181&lt;/td&gt; &lt;td&gt;    0.024&lt;/td&gt; &lt;td&gt;    0.741&lt;/td&gt; &lt;td&gt; 0.459&lt;/td&gt; &lt;td&gt;   -0.030&lt;/td&gt; &lt;td&gt;    0.066&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;infinite_scroll&lt;/th&gt; &lt;td&gt;    0.1571&lt;/td&gt; &lt;td&gt;    0.020&lt;/td&gt; &lt;td&gt;    7.910&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    0.118&lt;/td&gt; &lt;td&gt;    0.196&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;past_revenue&lt;/th&gt;    &lt;td&gt;    0.9922&lt;/td&gt; &lt;td&gt;    0.010&lt;/td&gt; &lt;td&gt;   98.655&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    0.972&lt;/td&gt; &lt;td&gt;    1.012&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;Indeed, &lt;code&gt;past_revenue&lt;/code&gt; is highly predictive of current &lt;code&gt;ad_revenue&lt;/code&gt; and the precision of the estimated coefficient for &lt;code&gt;infinite_scroll&lt;/code&gt; decreases by one-third.&lt;/p&gt;
&lt;p&gt;So far, everything has been very standard. However, as we said at the beginning, suppose this is not the only experiment we ran trying to improve our browser (and ultimately ad revenue). The infinite scroll is just one idea among thousands of others that we have tested in the past. Is there a way to efficiently use this additional &lt;strong&gt;information&lt;/strong&gt;?&lt;/p&gt;
&lt;h2 id=&#34;bayesian-statistics&#34;&gt;Bayesian Statistics&lt;/h2&gt;
&lt;p&gt;One of the main advantages of Bayesian statistics over the frequentist approach is that it easily allows to incorporate additional information into a model. The idea directly follows from the main results behind all Bayesian statistics: &lt;a href=&#34;https://en.wikipedia.org/wiki/Bayes%27_theorem&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Bayes Theorem&lt;/strong&gt;&lt;/a&gt;. Bayes theorem, allows you to do inference on a model by &lt;strong&gt;inverting the inference problem&lt;/strong&gt;: from the probability of the model given the data, to the probability of the data given the model, a much easier object to deal with.&lt;/p&gt;
&lt;p&gt;$$
\underbrace{ \Pr \big( \text{model} \ \big| \ \text{data} \big) }&lt;em&gt;{\text{posterior}} = \underbrace{ \Pr(\text{model}) }&lt;/em&gt;{\text{prior}} \ \underbrace{ \frac{ \Pr \big( \text{data} \ \big| \ \text{model} \big) }{ \Pr(\text{data}) } }_{\text{likelihood}}
$$&lt;/p&gt;
&lt;p&gt;We can split the right-hand side of Bayes Theorem (or Rule) into two components: the &lt;strong&gt;prior&lt;/strong&gt; and the &lt;strong&gt;likelihood&lt;/strong&gt;. The likelihood is the information about the model that comes from the data, the prior instead is any additional information about the model.&lt;/p&gt;
&lt;p&gt;First of all, let&amp;rsquo;s map Bayes theorem into our context. What is the data, what is the model and what is our object of interest?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the &lt;strong&gt;data&lt;/strong&gt; which consists in our outcome variable &lt;code&gt;ad_revenue&lt;/code&gt;, $y$, the treatment &lt;code&gt;infinite_scroll&lt;/code&gt;, $D$ and the other variables, &lt;code&gt;past_revenue&lt;/code&gt; and a constant, which we jointly denote as $X$&lt;/li&gt;
&lt;li&gt;the &lt;strong&gt;model&lt;/strong&gt; is the distribution of &lt;code&gt;ad_revenue&lt;/code&gt;, given &lt;code&gt;past_revenue&lt;/code&gt; and the &lt;code&gt;infinite_scroll&lt;/code&gt; feature, $y | D, X$&lt;/li&gt;
&lt;li&gt;our &lt;strong&gt;object of interest&lt;/strong&gt; is the posterior $\Pr \big( \text{model} \ \big| \ \text{data} \big)$, in particular the relationship between &lt;code&gt;ad_revenue&lt;/code&gt; and &lt;code&gt;infinite_scroll&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;X = sm.add_constant(df[[&#39;past_revenue&#39;]].values)
D = df[&#39;infinite_scroll&#39;].values
y = df[&#39;ad_revenue&#39;].values
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;How do we use prior information in the context of AB testing, potentially including additional covariates?&lt;/p&gt;
&lt;h3 id=&#34;bayesian-regression&#34;&gt;Bayesian Regression&lt;/h3&gt;
&lt;p&gt;Let&amp;rsquo;s use a linear model to make it directly comparable with the frequentist approach:&lt;/p&gt;
&lt;p&gt;$$
y_i = \beta X_i + \tau D_i + \varepsilon_i \qquad \text{where} \quad \varepsilon_i \sim N \big( 0, \sigma^2 \big)
$$&lt;/p&gt;
&lt;p&gt;This is a parametric model with &lt;strong&gt;two sets of parameters&lt;/strong&gt;: the linear coefficients $\beta$ and $\tau$, and the variance of the residuals $\sigma$. An equivalent, but more Bayesian, way to write the model is:&lt;/p&gt;
&lt;p&gt;$$
y \ | \ X, D; \beta, \tau, \sigma \sim N \Big( \beta X + \tau D \ , \sigma^2 \Big) ,
$$&lt;/p&gt;
&lt;p&gt;where the semi-column separates the data from the model parameters. Differently from the frequentist approach, in Bayesian regressions we do not rely on the &lt;a href=&#34;https://en.wikipedia.org/wiki/Central_limit_theorem&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;central limit theorem&lt;/a&gt; to approximate the conditional distribution of $y$, but we directly &lt;strong&gt;assume&lt;/strong&gt; it is normal. Is it just a formality? Not really, but a proper comparison between the frequentist and Bayesian approach is beyond the scope of this article.&lt;/p&gt;
&lt;p&gt;We are interested in doing inference on the model parameters, $\beta$, $\tau$, and $\sigma$. Another &lt;strong&gt;core difference&lt;/strong&gt; between the frequentist and the Bayesian approach is that the the first assumes that the model parameters are fixed (scalars), while the latter allows them to be stochastic (random variables).&lt;/p&gt;
&lt;p&gt;This assumption has a very practical &lt;strong&gt;implication&lt;/strong&gt;: you can easily incorporate previous information about the model parameters in the form of &lt;strong&gt;prior&lt;/strong&gt; distributions. As the name says, priors contain information that was available even &lt;em&gt;before&lt;/em&gt; looking at the data. This leads to one of the most relevant questions in Bayesian statistics: &lt;strong&gt;how do you chose a prior&lt;/strong&gt;?&lt;/p&gt;
&lt;h2 id=&#34;priors&#34;&gt;Priors&lt;/h2&gt;
&lt;p&gt;When choosing a prior, one analytically appealing restriction is to have a prior distribution such that the posterior belongs to the same family. These priors are called &lt;strong&gt;conjugate priors&lt;/strong&gt;. For example, before seeing the data, I assume my treatment effect is normally distributed and I would like it to be normally distributed also after incorporating the information contained in the data.&lt;/p&gt;
&lt;p&gt;In the case of Bayesian linear regression, the conjugate priors for $\beta$ and $\sigma$ are normally and inverse-gamma distributed. Let&amp;rsquo;s start a bit blindly, by taking a standard normal and inverse gamma distribution as prior.&lt;/p&gt;
&lt;p&gt;$$
\beta_i \sim N(\boldsymbol 0, \boldsymbol 1) \
\tau_i \sim N(0,1) \
\sigma^2 \sim \Gamma^{-1} (1, 1)
$$&lt;/p&gt;
&lt;p&gt;We use the package &lt;a href=&#34;https://www.pymc.io/projects/docs/en/stable/learn.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PyMC&lt;/a&gt; to do inference. First we need to specify the model: what are the distributions of the different parameters (priors) and what is the likelihood of the data.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import pymc as pm
with pm.Model() as baseline_model:

    # Priors
    beta = pm.MvNormal(&#39;beta&#39;, mu=np.ones(np.shape(X)[1]), cov=np.eye(np.shape(X)[1]))
    tau = pm.Normal(&#39;tau&#39;, mu=0, sigma=1)
    sigma = pm.InverseGamma(&#39;sigma&#39;, mu=1, sigma=1, initval=1)
    
    # Likelihood 
    Ylikelihood = pm.Normal(&#39;y&#39;, mu=(X@beta + D@tau).flatten(), sigma=sigma, observed=y)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;PyMC has an extremely nice function that allows us to visualize the model as a graph, &lt;code&gt;model_to_graphviz&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;pm.model_to_graphviz(baseline_model)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/bayes_reg_25_0.svg&#34; alt=&#34;svg&#34;&gt;&lt;/p&gt;
&lt;p&gt;From the graphical representation, we can see the various model components, their distributions, and how they interact with each other.&lt;/p&gt;
&lt;p&gt;We are now ready to &lt;strong&gt;compute&lt;/strong&gt; the model posterior. How does it work? In short, we sample realizations of model parameters, we compute the likelihood of the data given those values and the compute the corresponding posterior.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;idata = pm.sample(model=baseline_model, draws=1000)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Auto-assigning NUTS sampler...
Initializing NUTS using jitter+adapt_diag...
Multiprocess sampling (4 chains in 4 jobs)
NUTS: [beta, tau, sigma]
&lt;/code&gt;&lt;/pre&gt;
&lt;style&gt;
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
&lt;/style&gt;
&lt;div&gt;
  &lt;progress value=&#39;8000&#39; class=&#39;&#39; max=&#39;8000&#39; style=&#39;width:300px; height:20px; vertical-align: middle;&#39;&gt;&lt;/progress&gt;
  100.00% [8000/8000 00:03&amp;lt;00:00 Sampling 4 chains, 0 divergences]
&lt;/div&gt;
&lt;pre&gt;&lt;code&gt;Sampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 4_000 draws total) took 15 seconds.
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The fact that Bayesian inference requires sampling, has been historically one of the main bottlenecks of Bayesian statistics, since it makes it sensibly slower than the frequentist approach. However, this is less and less of a problem with the increased computational power of model computers.&lt;/p&gt;
&lt;p&gt;We are now ready to print out results. First, with the &lt;code&gt;summary()&lt;/code&gt; method, we can print a model summary very similar to those produced by the &lt;code&gt;statsmodels&lt;/code&gt; package.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;pm.summary(idata, hdi_prob=0.95).round(4)
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;mean&lt;/th&gt;
      &lt;th&gt;sd&lt;/th&gt;
      &lt;th&gt;hdi_2.5%&lt;/th&gt;
      &lt;th&gt;hdi_97.5%&lt;/th&gt;
      &lt;th&gt;mcse_mean&lt;/th&gt;
      &lt;th&gt;mcse_sd&lt;/th&gt;
      &lt;th&gt;ess_bulk&lt;/th&gt;
      &lt;th&gt;ess_tail&lt;/th&gt;
      &lt;th&gt;r_hat&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;beta[0]&lt;/th&gt;
      &lt;td&gt;0.019&lt;/td&gt;
      &lt;td&gt;0.025&lt;/td&gt;
      &lt;td&gt;-0.031&lt;/td&gt;
      &lt;td&gt;0.068&lt;/td&gt;
      &lt;td&gt;0.001&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;1943.0&lt;/td&gt;
      &lt;td&gt;1866.0&lt;/td&gt;
      &lt;td&gt;1.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;beta[1]&lt;/th&gt;
      &lt;td&gt;0.992&lt;/td&gt;
      &lt;td&gt;0.010&lt;/td&gt;
      &lt;td&gt;0.970&lt;/td&gt;
      &lt;td&gt;1.011&lt;/td&gt;
      &lt;td&gt;0.000&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;2239.0&lt;/td&gt;
      &lt;td&gt;1721.0&lt;/td&gt;
      &lt;td&gt;1.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;tau&lt;/th&gt;
      &lt;td&gt;0.157&lt;/td&gt;
      &lt;td&gt;0.021&lt;/td&gt;
      &lt;td&gt;0.117&lt;/td&gt;
      &lt;td&gt;0.197&lt;/td&gt;
      &lt;td&gt;0.000&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;2770.0&lt;/td&gt;
      &lt;td&gt;2248.0&lt;/td&gt;
      &lt;td&gt;1.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;sigma&lt;/th&gt;
      &lt;td&gt;0.993&lt;/td&gt;
      &lt;td&gt;0.007&lt;/td&gt;
      &lt;td&gt;0.980&lt;/td&gt;
      &lt;td&gt;1.007&lt;/td&gt;
      &lt;td&gt;0.000&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;3473.0&lt;/td&gt;
      &lt;td&gt;2525.0&lt;/td&gt;
      &lt;td&gt;1.0&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;The estimated parameters are extremely close to the ones we got with the frequentist approach, with an estimated effect of the &lt;code&gt;infinite_scroll&lt;/code&gt; equal to $0.157$.&lt;/p&gt;
&lt;p&gt;If sampling had the disadvantage of being slow, it has the advantage of being very &lt;strong&gt;transparent&lt;/strong&gt;. We can directly plot the distribution of the posterior. Let&amp;rsquo;s do it for the treatment effect $\tau$. The PyMC function &lt;code&gt;plot_posterior&lt;/code&gt; plots the distribution of the posterior, with a black bar for the Bayesian equivalent of a 95% confidence interval.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;pm.plot_posterior(idata, kind=&amp;quot;hist&amp;quot;, var_names=(&#39;tau&#39;), hdi_prob=0.95, figsize=(6, 3), bins=30); 
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/bayes_reg_31_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;As expected, since we chose conjugate priors, the posterior distribution looks gaussian.&lt;/p&gt;
&lt;p&gt;So far we have chosen the prior without much guidance. However, suppose we had access to past experiments. How do we incorporate this specific information?&lt;/p&gt;
&lt;h2 id=&#34;past-experiments&#34;&gt;Past Experiments&lt;/h2&gt;
&lt;p&gt;Suppose the idea of the infinite scroll, was just one among a ton of other ones that we tried and tested in the past. For each idea we have the data for the corresponding experiment, with the corresponding estimated coefficient. Suppose we had a thousand of them.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;past_experiments = [dgp.generate_data(seed_data=i) for i in range(1000)]
taus = [smf.ols(&#39;ad_revenue ~ infinite_scroll + past_revenue&#39;, pe).fit().params.values for pe in past_experiments]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;How do we use this additional information?&lt;/p&gt;
&lt;h3 id=&#34;normal-prior&#34;&gt;Normal Prior&lt;/h3&gt;
&lt;p&gt;The first idea could be to calibrate our prior to reflect the data distribution in the past. Keeping the normality assumption, we use the estimated average and standard deviations of the estimates from past experiments.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;taus_mean = np.mean(taus, axis=0)[1]
taus_mean
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;0.0009094486420266667
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;On average, had practically no effect on &lt;code&gt;ad_revenue&lt;/code&gt;, with a average effect of $0.0009$.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;taus_std = np.sqrt(np.cov(taus, rowvar=0)[1,1])
taus_std
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;0.029014447772168384
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;However, there was sensible variation across experiments, with a standard deviation of $0.029$.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s now estimate the&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;with pm.Model() as model_normal_prior:
    k = np.shape(X)[1]
    beta = pm.MvNormal(&#39;beta&#39;, mu=np.ones(k), cov=np.eye(k))
    tau = pm.Normal(&#39;tau&#39;, mu=taus_mean, sigma=taus_std)
    sigma = pm.InverseGamma(&#39;sigma&#39;, mu=1, sigma=1, initval=1)
    Ylikelihood = pm.Normal(&#39;y&#39;, mu=(X@beta + D@tau).flatten(), sigma=sigma, observed=y)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s sample from the model.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;idata_normal_prior = pm.sample(model=model_normal_prior, draws=1000)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Auto-assigning NUTS sampler...
Initializing NUTS using jitter+adapt_diag...
Multiprocess sampling (4 chains in 4 jobs)
NUTS: [beta, tau, sigma]
&lt;/code&gt;&lt;/pre&gt;
&lt;style&gt;
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
&lt;/style&gt;
&lt;div&gt;
  &lt;progress value=&#39;8000&#39; class=&#39;&#39; max=&#39;8000&#39; style=&#39;width:300px; height:20px; vertical-align: middle;&#39;&gt;&lt;/progress&gt;
  100.00% [8000/8000 00:04&amp;lt;00:00 Sampling 4 chains, 0 divergences]
&lt;/div&gt;
&lt;pre&gt;&lt;code&gt;Sampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 4_000 draws total) took 14 seconds.
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And plot the sample posterior distribution of the treatment effect parameter $\tau$.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;pm.plot_posterior(idata_normal_prior, kind=&amp;quot;hist&amp;quot;, var_names=(&#39;tau&#39;), hdi_prob=0.95, figsize=(6, 3), bins=30); 
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/bayes_reg_47_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;The estimated coefficient is sensibly smaller: $0.08$ instead of the previous estimate of $0.12$. Why is it the case?&lt;/p&gt;
&lt;p&gt;The fact is that the previous coefficient of $0.12$ is extremely unlikey, given our prior. We can compute the probability of getting the same or a more extreme value, given the prior.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;1 - sp.stats.norm(taus_mean, taus_std).cdf(0.12)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;2.025724712373389e-05
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The probability of such value is almost zero. Therefore, the estimated coefficient has moved towards the prior mean of $0.0009$.&lt;/p&gt;
&lt;h3 id=&#34;student-t-prior&#34;&gt;Student t Prior&lt;/h3&gt;
&lt;p&gt;So far, we have assumed a normal distribution for all linear coefficients. Is it appropriate? Let&amp;rsquo;s check it visually (check &lt;a href=&#34;https://medium.com/towards-data-science/how-to-compare-two-or-more-distributions-9b06ee4d30bf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt; for other methods on how to compare distributions).&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sns.histplot([tau[0] for tau in taus]).set(title=r&#39;Distribution of $\hat{\beta}_0$ in past experiments&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/bayes_reg_53_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;The distribution seems pretty normal. What the treatment effect paramenter $\tau$?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;fig, ax = plt.subplots()
sns.histplot([tau[1] for tau in taus], label=&#39;past experiments&#39;);
ax.axvline(reg.params[&#39;infinite_scroll&#39;], lw=2, c=&#39;C3&#39;, ls=&#39;--&#39;, label=&#39;current experiment&#39;)
plt.legend();
plt.title(r&#39;Distribution of $\hat{\tau}$ in past experiments&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/bayes_reg_55_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;The distribution very &lt;strong&gt;heavy tailed&lt;/strong&gt;! While at the center it looks like a normal distributions, the tails are much &amp;ldquo;fatter&amp;rdquo; and we have a couple of very extreme values. excluding the case of measurement error, this is a setting that happens often in the industry, where most ideas have extremely small or null effects and very rarely an idea is actually a breakthrough.&lt;/p&gt;
&lt;p&gt;One way to model this distribution is a &lt;a href=&#34;&#34;&gt;student-t distribution&lt;/a&gt;. In particular, we use a t-student with mean $0.0009$, variance $0.003$ and $1.3$ degrees of freedom to match the moments of the empirical distributions of past estimates.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;with pm.Model() as model_studentt_prior:

    # Priors
    k = np.shape(X)[1]
    beta = pm.MvNormal(&#39;beta&#39;, mu=np.ones(k), cov=np.eye(k))
    tau = pm.StudentT(&#39;tau&#39;, mu=taus_mean, sigma=0.003, nu=1.3)
    sigma = pm.InverseGamma(&#39;sigma&#39;, mu=1, sigma=1, initval=1)
    
    # Likelihood 
    Ylikelihood = pm.Normal(&#39;y&#39;, mu=(X@beta + D@tau).flatten(), sigma=sigma, observed=y)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s sample from the model.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;idata_studentt_priors = pm.sample(model=model_studentt_prior, draws=1000)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Auto-assigning NUTS sampler...
Initializing NUTS using jitter+adapt_diag...
Multiprocess sampling (4 chains in 4 jobs)
NUTS: [beta, tau, sigma]
&lt;/code&gt;&lt;/pre&gt;
&lt;style&gt;
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
&lt;/style&gt;
&lt;div&gt;
  &lt;progress value=&#39;8000&#39; class=&#39;&#39; max=&#39;8000&#39; style=&#39;width:300px; height:20px; vertical-align: middle;&#39;&gt;&lt;/progress&gt;
  100.00% [8000/8000 00:03&amp;lt;00:00 Sampling 4 chains, 0 divergences]
&lt;/div&gt;
&lt;pre&gt;&lt;code&gt;Sampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 4_000 draws total) took 15 seconds.
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And plot the sample posterior distribution of the treatment effect parameter $\tau$.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;pm.plot_posterior(idata_studentt_priors, kind=&amp;quot;hist&amp;quot;, var_names=(&#39;tau&#39;), hdi_prob=0.95, figsize=(6, 3), bins=30); 
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/bayes_reg_61_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;The estimated coefficient is now again similar to the one we got with the standard normal prior, $0.11$. However, the estimate is more precise since the confidence interval has shrunk from $[0.077, 0.016]$ to $[0.065, 0.015]$.&lt;/p&gt;
&lt;p&gt;What has happened?&lt;/p&gt;
&lt;h3 id=&#34;shrinking&#34;&gt;Shrinking&lt;/h3&gt;
&lt;p&gt;The answer lies in the shape of the different &lt;strong&gt;prior distributions&lt;/strong&gt; that we have used:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;standard normal, $N(0,1)$&lt;/li&gt;
&lt;li&gt;normal with matched moments, $N(0, 0.03)$&lt;/li&gt;
&lt;li&gt;t-student with matched moments, $t_{1.3}$(0, 0.003)$&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;t_hats = np.linspace(-0.3, 0.3, 1_000)
distributions = {
    &#39;N(0,1)&#39;: sp.stats.norm(0, 1).pdf(t_hats),
    &#39;N(0, 0.03)&#39;: sp.stats.norm(0, 0.03).pdf(t_hats),
    &#39;$t_{1.3}$(0, 0.003)&#39;: sp.stats.t(df=1.3).pdf(t_hats / 0.003)*300,
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s plot all of them together.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;for i, (label, y) in enumerate(distributions.items()):
    sns.lineplot(x=t_hats, y=y, color=f&#39;C{i}&#39;, label=label);
plt.xlim(-0.15, 0.15);
plt.legend(); 
plt.title(&#39;Prior Distributions&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/bayes_reg_67_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;As we can see, all distributions are centered on zero, but they have very different shapes. The standard normal distribution is essentially flat over the $[-0.15, 0.15]$ interval. Every value has basically the same probability. The last two instead, even though they have the same mean and variance, have very different shapes.&lt;/p&gt;
&lt;p&gt;How does it translate into our estimation? We can plot the implied posterior for different estimates, for each prior distribution.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def compute_posterior(b, prior):
    likelihood = sp.stats.norm(b, taus_std).pdf(t_hats)
    return np.average(t_hats, weights=prior*likelihood)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;fig, ax = plt.subplots(figsize=(7,6))
ax.axvline(0, lw=1.5, c=&#39;k&#39;);
ax.axhline(0, lw=1.5, c=&#39;k&#39;);
ax.axvline(0.12, lw=2, ls=&#39;--&#39;, c=&#39;darkgray&#39;);
for i, (label, y) in enumerate(distributions.items()):
    sns.lineplot(x=t_hats, y=[compute_posterior(t, y) for t in t_hats] , color=f&#39;C{i}&#39;, label=label);
ax.set_xlim(-0.16, 0.16);
ax.set_ylim(-0.16, 0.16);
plt.legend(); 
ax.set_xlabel(&#39;Experiment Estimate&#39;);
ax.set_ylabel(&#39;Posterior&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/bayes_reg_70_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;As we can see, the different priors transform the experimental estimates in very different ways. The standard normal prior essentially has no effect for estimates in the $[-0.15, 0.15]$ interval. The normal prior with matched moments instead shrinks each estimate by approximately 2/3. The effect of the t-student prior is instead &lt;strong&gt;non-linear&lt;/strong&gt;: it shrinks small estimates towards zero, while it keeps large estimates as they are.&lt;/p&gt;
&lt;p&gt;My &lt;strong&gt;intuition&lt;/strong&gt; is the following. A prior distribution very skewed or with &amp;ldquo;fat tails&amp;rdquo; means that large values are rare but not impossible. In practice, it means accepting that breakthrough improvements are possible. On the other hand, for the same variance, the distribution is more concentrated around zero than a standard normal so that small values are shrunk even more.&lt;/p&gt;
&lt;img src=&#34;fig/scroll.jpg&#34; width=&#34;300px&#34;/&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;In this article we have seen how to extend the analysis of AB test to incorporate &lt;strong&gt;information from past experiments&lt;/strong&gt;. In particular, we have seen the importance of choosing a prior. Selecting the distribution function is just as important as tuning its parameters. The shape of the prior distribution can drastically affect our inference, especially in a world with skewed distributions.&lt;/p&gt;
&lt;p&gt;Despite the length of the article, this was just a glimpse in the world of &lt;strong&gt;AB testing and Bayesian statistics&lt;/strong&gt;. While being computationally more intensive and requiring additional assumptions, the Bayesian approach is often more natural, powerful and flexible than the frequentist one. Knowing pros and cons of both approaches is crucial to get the best of both worlds, picking the approach that work best or combining them efficiently.&lt;/p&gt;
&lt;h3 id=&#34;references&#34;&gt;References&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;E. Azevedo, A. Deng, J. Olea, G. Weyl, &lt;a href=&#34;https://www.aeaweb.org/articles?id=10.1257/pandp.20191003&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Empirical Bayes Estimation of Treatment Effects with Many A/B Tests: An Overview&lt;/a&gt; (2019). &lt;em&gt;AEA Papers and Proceedings&lt;/em&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;A. Deng, &lt;a href=&#34;https://dl.acm.org/doi/abs/10.1145/2740908.2742563&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Objective Bayesian Two Sample Hypothesis Testing for Online Controlled Experiments&lt;/a&gt; (2018), &lt;em&gt;WWW15&lt;/em&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;related-articles&#34;&gt;Related Articles&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://medium.com/towards-data-science/the-bayesian-bootstrap-6ca4a1d45148&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The Bayesian Bootstrap&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://towardsdatascience.com/understanding-cuped-a822523641af&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Understanding CUPED&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://towardsdatascience.com/controls-b63dc69e3d8c&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;DAGs and Control Variables&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;code&#34;&gt;Code&lt;/h3&gt;
&lt;p&gt;You can find the original Jupyter Notebook here:&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/delta.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/delta.ipynb&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Experiments on Returns on Investment</title>
      <link>https://matteocourthoud.github.io/post/delta/</link>
      <pubDate>Tue, 27 Dec 2022 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/post/delta/</guid>
      <description>&lt;p&gt;&lt;em&gt;An introduction to the delta method for inference on ratio metrics.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;When we run an experiment, we are often not only interested in the effect of a treatment (new product, new feature, new interface, &amp;hellip;) on revenue, but in it&amp;rsquo;s &lt;strong&gt;cost-effectiveness&lt;/strong&gt;. In other words, is the investment worth the cost? Common examples include investments in computing resources, returns on advertisement, but also click-through rates and other ratio metrics.&lt;/p&gt;
&lt;p&gt;When we investigate causal effects, the gold standard is randomized control trials, a.k.a. &lt;strong&gt;AB tests&lt;/strong&gt;. Randomly assigning the treatment to a subset of the population (users, patients, customers, &amp;hellip;) we ensure that, on average, the difference in outcomes can be attributed to the treatment. However, when the object of interest is cost-effectiveness, AB tests present some additional problems since we are not just interested in one treatment effect, but in the &lt;strong&gt;ratio of two treatment effects&lt;/strong&gt;, the outcome of the investment over its cost.&lt;/p&gt;
&lt;p&gt;In this post we are going to see how to analyze randomized experiments when the object of interest is the &lt;strong&gt;return on investment (ROI)&lt;/strong&gt;. We are going to explore alternative metrics to measure whether an investment paid off. We will also introduce a very powerful tool for inference with complex metrics: the &lt;strong&gt;delta method&lt;/strong&gt;. While the algebra can be intense, the result is simple: we can compute the confidence interval for our ratio estimator using a simple linear regression.&lt;/p&gt;
&lt;h2 id=&#34;investing-in-cloud-computing&#34;&gt;Investing in Cloud Computing&lt;/h2&gt;
&lt;p&gt;To better illustrate the concepts, we are going to use a toy example throughout the article: suppose we were an &lt;strong&gt;online marketplace&lt;/strong&gt; and we wanted to &lt;strong&gt;invest in cloud computing&lt;/strong&gt;: we want to increase the computing power behind our internal search engine, by switching to a higher tier server. The idea is that the faster search will improve the user experience, potentially leading to higher sales. Therefore, the question is: is the investment worth the cost? The object of interest is the &lt;strong&gt;return on investment (ROI)&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Differently from usual AB tests or randomized experiments, we are not interested in a single causal effect, but in the &lt;strong&gt;ratio&lt;/strong&gt; of two metrics: the effect on revenue and the effect on cost. We will still use a &lt;strong&gt;randomized control trial&lt;/strong&gt; or &lt;strong&gt;AB test&lt;/strong&gt; to estimate the ROI: we randomly assign groups of users to either the treatment or the control group. The treated users will benefit from the faster cloud machines, while the control users will use the old slower machines. Randomization ensures that we can estimate the impact of the new machines on either cost or revenue by comparing users in the treatment and control group: the difference in their average is an unbiased estimator of the average treatment effect. However, things are more complicated for their ratio.&lt;/p&gt;
&lt;p&gt;I import the data generating process &lt;code&gt;dgp_cloud()&lt;/code&gt; from &lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/src/dgp.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;src.dgp&lt;/code&gt;&lt;/a&gt;. With respect to previous articles, I generated a new DGP parent class that handles randomization and data generation, while its children classes contain the specific use-cases. I also import some plotting functions and libraries from &lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/src/utils.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;src.utils&lt;/code&gt;&lt;/a&gt;.  To include not only code but also data and tables, I use &lt;a href=&#34;https://deepnote.com&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Deepnote&lt;/a&gt;, a Jupyter-like web-based collaborative notebook environment.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;%matplotlib inline
%config InlineBackend.figure_format = &#39;retina&#39;

from src.utils import *
from src.dgp import dgp_cloud, DGP
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;dgp = dgp_cloud(n=10_000)
df = dgp.generate_data(seed_assignment=6)
df.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;new_machine&lt;/th&gt;
      &lt;th&gt;cost&lt;/th&gt;
      &lt;th&gt;revenue&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;3.14&lt;/td&gt;
      &lt;td&gt;20.90&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;3.77&lt;/td&gt;
      &lt;td&gt;33.57&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;3.16&lt;/td&gt;
      &lt;td&gt;24.31&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;2.36&lt;/td&gt;
      &lt;td&gt;20.35&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1.65&lt;/td&gt;
      &lt;td&gt;12.60&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;The data contains information on the total &lt;code&gt;cost&lt;/code&gt; and &lt;code&gt;revenue&lt;/code&gt; for a set of $10.000$ users over a period of a month. We also have information on the treatment: whether the search engine was running on the old or &lt;code&gt;new machines&lt;/code&gt;.  As it often happens with business metrics, both distributions of cost and revenues are very &lt;strong&gt;skewed&lt;/strong&gt;. Moreover, most people do not buy anything and therefore generate zero revenue, even though they still use the platform, generating positive costs.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;fix, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))
sns.histplot(df.cost, ax=ax1, color=&#39;C0&#39;).set(title=&#39;Distribution of Cost&#39;)
sns.histplot(df.revenue, ax=ax2, color=&#39;C1&#39;).set(title=&#39;Distribution of Revenue&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/delta_8_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;We can compute the &lt;strong&gt;difference-in-means&lt;/strong&gt; estimate for &lt;code&gt;cost&lt;/code&gt; and &lt;code&gt;revenue&lt;/code&gt; by regressing the outcome on the treatment indicator.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;smf.ols(&#39;cost ~ new_machine&#39;, data=df).fit().summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
       &lt;td&gt;&lt;/td&gt;          &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt;   &lt;td&gt;    2.9617&lt;/td&gt; &lt;td&gt;    0.043&lt;/td&gt; &lt;td&gt;   69.034&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    2.878&lt;/td&gt; &lt;td&gt;    3.046&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;new_machine&lt;/th&gt; &lt;td&gt;    0.5152&lt;/td&gt; &lt;td&gt;    0.060&lt;/td&gt; &lt;td&gt;    8.563&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    0.397&lt;/td&gt; &lt;td&gt;    0.633&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;The average &lt;code&gt;cost&lt;/code&gt; has increased by $0.5152$$ per user. What about revenue?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;smf.ols(&#39;revenue ~ new_machine&#39;, data=df).fit().summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
       &lt;td&gt;&lt;/td&gt;          &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt;   &lt;td&gt;   25.9172&lt;/td&gt; &lt;td&gt;    0.425&lt;/td&gt; &lt;td&gt;   60.950&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;   25.084&lt;/td&gt; &lt;td&gt;   26.751&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;new_machine&lt;/th&gt; &lt;td&gt;    1.0664&lt;/td&gt; &lt;td&gt;    0.596&lt;/td&gt; &lt;td&gt;    1.788&lt;/td&gt; &lt;td&gt; 0.074&lt;/td&gt; &lt;td&gt;   -0.103&lt;/td&gt; &lt;td&gt;    2.235&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;The average &lt;code&gt;revenue&lt;/code&gt; per user has also increased, by $1.0664$$. So, was the investment &lt;strong&gt;profitable&lt;/strong&gt;?&lt;/p&gt;
&lt;p&gt;To answer this question, we first have to decide which metric to use as our &lt;strong&gt;outcome metric&lt;/strong&gt;. In case of ratio metrics, this is not trivial.&lt;/p&gt;
&lt;h2 id=&#34;average-return-or-return-of-the-average&#34;&gt;Average Return or Return of the Average?&lt;/h2&gt;
&lt;p&gt;It is very tempting to approach this problem saying: it is true that we have two variables, by we can just compute their ratio, and then analyze everything as usual, using a &lt;strong&gt;single variable&lt;/strong&gt;: the individual level return.&lt;/p&gt;
&lt;p&gt;$$
\rho_i = \frac{\text{individual revenue}}{\text{individual cost}} = \frac{R_i}{C_i}
$$&lt;/p&gt;
&lt;p&gt;What happens if we analyze the experiment using this single metric?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df[&amp;quot;rho&amp;quot;] = df[&amp;quot;revenue&amp;quot;] / df[&amp;quot;cost&amp;quot;]
smf.ols(&amp;quot;rho ~ new_machine&amp;quot;, df).fit().summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
       &lt;td&gt;&lt;/td&gt;          &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt;   &lt;td&gt;    6.6898&lt;/td&gt; &lt;td&gt;    0.044&lt;/td&gt; &lt;td&gt;  150.832&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    6.603&lt;/td&gt; &lt;td&gt;    6.777&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;new_machine&lt;/th&gt; &lt;td&gt;   -0.7392&lt;/td&gt; &lt;td&gt;    0.062&lt;/td&gt; &lt;td&gt;  -11.893&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;   -0.861&lt;/td&gt; &lt;td&gt;   -0.617&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;The estimated effect is &lt;strong&gt;negative and significant&lt;/strong&gt;, $-0.7392$! It seems like the the new machines were not a good investment, and the returns have decreased by $74%$.&lt;/p&gt;
&lt;p&gt;This result seems to contradict our previous estimates. We have seen before that the revenue has increased on average more than the cost ($0.9505$ vs $0.5076$). Why is it the case? The problem is that we are giving the same weight to heavy users and light users. Let&amp;rsquo;s use a simple example with two users. The first one (blue) is a light user and before was costing $1$ $ and returning $10$ $, while now is costing $4$ $ and returning $20$ $. The other user (violet) is a heavy user and before was costing $10$ $ and returning $100$ $ and now is costing $20$ $ and returning $220$ $.&lt;/p&gt;
&lt;img src=&#34;fig/return.png&#34; width=&#34;700px&#34;/&gt;
&lt;p&gt;The average return is -3x: on average the return per user has decreased by $300%$. However, the total return per user is $1000%$: the increase in cost of $13$$ has generated $130$$ in revenue! The results are wildly different and entirely driven by the weight of the two users: the effect of the heavy user is low in relative terms but high in absolute terms, while it&amp;rsquo;s the opposite for the light user. The average relative effect is therefore mostly driven by the light user, while the relative average effect is mostly driven by the heavy user.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Which metric&lt;/strong&gt; is more relevant in our setting? We talking about return on investment, we are usually interested in understanding whether we got a return on the money we spend. Therefore, the &lt;strong&gt;total return&lt;/strong&gt; is more interesting than the average return.&lt;/p&gt;
&lt;p&gt;From now on, the object of interest will be the &lt;strong&gt;return on investment (ROI)&lt;/strong&gt;, given by the expected increase in revenue over the expected increase in cost, and we will denote it with the greek letter rho, $\rho$.&lt;/p&gt;
&lt;p&gt;$$
\rho = \frac{\text{incremental revenue}}{\text{incremental cost}} = \frac{\mathbb E [\Delta R]}{\mathbb E [\Delta C]}
$$&lt;/p&gt;
&lt;p&gt;We can estimate the ROI as the ratio of the two previous estimates: the average difference in revenue between the treatment and control group, over the average difference in cost between the treatment and control group.&lt;/p&gt;
&lt;p&gt;$$
\hat{\rho} = \frac{\mathbb E_n [\Delta R]}{\mathbb E_n [\Delta C]}
$$&lt;/p&gt;
&lt;p&gt;Note a subtle but crucial difference with respect to the previous formula: we have replaced the &lt;a href=&#34;https://en.wikipedia.org/wiki/Expected_value&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;expected values&lt;/a&gt; $\mathbb E$ with the empirical expectation operators $\mathbb E_n$, also known as the &lt;a href=&#34;https://en.wikipedia.org/wiki/Arithmetic_mean&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;sample average&lt;/a&gt;. The difference in notation is minimal, but the conceptual difference is huge. The first, $\mathbb E$, is a &lt;strong&gt;theoretical&lt;/strong&gt; concept, while the second, $\mathbb E_n$, is &lt;strong&gt;empirical&lt;/strong&gt;: it is a number that depends on the actual data. I personally like the notation since it highlights the close link between the two concepts (the second is the empirical counterpart of the first), while also making it clear that the second crucially depends on the sample size $n$.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def estimate_roi(df):
    Delta_C = df.loc[df.new_machine==1, &amp;quot;cost&amp;quot;].mean() - df.loc[df.new_machine==0, &amp;quot;cost&amp;quot;].mean()
    Delta_R = df.loc[df.new_machine==1, &amp;quot;revenue&amp;quot;].mean() - df.loc[df.new_machine==0, &amp;quot;revenue&amp;quot;].mean()
    return Delta_R / Delta_C

estimate_roi(df)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;2.0698235970047887
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The estimate is $2.0698$: each additional dollar spent in the new machines translated in $2.0698$ extra dollars in revenue. Sounds great!&lt;/p&gt;
&lt;p&gt;But how much should we trust this number? Is it significantly different form one, or it is just driven by noise?&lt;/p&gt;
&lt;h2 id=&#34;inference&#34;&gt;Inference&lt;/h2&gt;
&lt;p&gt;To answer this question, we would like to compute a &lt;a href=&#34;https://en.wikipedia.org/wiki/Confidence_interval&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;confidence interval&lt;/strong&gt;&lt;/a&gt; for our estimate. How do we compute a confidence interval for a ratio metric? The first step is to compute the standard deviation of the estimator. One method that is always available is the &lt;a href=&#34;https://en.wikipedia.org/wiki/Bootstrapping_%28statistics%29&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;bootstrap&lt;/strong&gt;&lt;/a&gt;: resample the data with replacement multiple times and use the distribution of the estimates over samples to compute the standard deviation of the estimator.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s try it in our case. I compute the standard deviation over $10.000$ bootstrapped samples, using the function &lt;code&gt;pd.DataFrame().sample()&lt;/code&gt; with the options &lt;code&gt;frac=1&lt;/code&gt; to obtain a dataset of the same size and &lt;code&gt;replace=True&lt;/code&gt; to sample with replacement.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;boot_estimates = [estimate_roi(df.sample(frac=1, replace=True, random_state=i)) for i in range(10_000)]
np.std(boot_estimates)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;0.9790730538161984
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The bootstrap estimate of the standard deviation is equal to $0.979$. How good is it?&lt;/p&gt;
&lt;p&gt;Since we fully control the data generating process, we can simulate the &amp;ldquo;true&amp;rdquo; distribution of the estimator. We do that for $10.000$ simulations and we compute the resulting standard deviation of the estimator.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;np.std(dgp.evaluate_f_redrawing_outcomes(estimate_roi, 10_000))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;1.0547776958025372
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The estimated variance of the estimator using the &amp;ldquo;true&amp;rdquo; data generating process is slightly higher but very similar, around $1.055$.&lt;/p&gt;
&lt;p&gt;The issue with the bootstrap is that it is very computational intense since it requires repeating the estimating procedure thousands of times. We are now going to explore another &lt;em&gt;extremely&lt;/em&gt; powerful alternative that requires a single estimation step, the &lt;a href=&#34;https://en.wikipedia.org/wiki/Delta_method&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;delta method&lt;/strong&gt;&lt;/a&gt;. The delta method generally allows us to do inference on functions of random variable, therefore its applications are broader than ratios.&lt;/p&gt;
&lt;p&gt;⚠️ &lt;strong&gt;Warning&lt;/strong&gt;: the next section is going to be algebra-intense. If you want, you can skip it and go straight to the last section.&lt;/p&gt;
&lt;h2 id=&#34;the-delta-method&#34;&gt;The Delta Method&lt;/h2&gt;
&lt;p&gt;What is the &lt;strong&gt;delta method&lt;/strong&gt;? In short, it is an incredibly powerful &lt;strong&gt;asymptotic inference&lt;/strong&gt; method for functions of random variables, that exploits Taylor expansions. In short, the delta method requires four ingredients&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;One or more &lt;a href=&#34;https://en.wikipedia.org/wiki/Random_variable&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;random variables&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;A function&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Central_limit_theorem&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The Central Limit Theorem&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Taylor_series&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Taylor expansions&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I will assume some basic knowledge of all four concepts. Suppose we had a set of realizations $X_1$, &amp;hellip;, $X_n$ of a random variable that satisfy the requirements for the Central Limit Theorem (CLT): independence, identically distributions with expected value $\mu$, and finite variance $\sigma^2$. Under these conditions, the CLT tells us that the sample average $\mathbb E_n[X]$ converges in distribution to a normal distribution, or more precisely&lt;/p&gt;
&lt;p&gt;$$
\sqrt{n} \ \frac{ \mathbb E_n[X] - \mu}{\sigma} \ \overset{D}{\to} \ N(0, 1)
$$&lt;/p&gt;
&lt;p&gt;What does the equation mean? It reads &amp;ldquo;the normalized sample average, scaled by a factor $\sqrt{n}$, converges in distribution to a standard normal distribution, i.e. it is approximately Gaussian for a sufficiently large sample.&lt;/p&gt;
&lt;p&gt;Now, suppose we were interested in a &lt;strong&gt;function&lt;/strong&gt; of the sample average $f\big(\mathbb E_n[X]\big)$. Note that this is different from the sample average of the function $\mathbb E_n\big[f(X)\big]$. The delta method tells us what the function of the sample average converges to.&lt;/p&gt;
&lt;p&gt;$$
\sqrt{n} \ \frac{ f\big(\mathbb E_n[X]\big) - f(\mu)}{\sigma} \ \overset{D}{\to} \ N \big(0, f&amp;rsquo;(\mu)^2 \big)
$$&lt;/p&gt;
&lt;p&gt;, where $f&amp;rsquo;(\mu)^2$ is the derivative of the function $f$, evaluated at $\mu$.&lt;/p&gt;
&lt;p&gt;What is the &lt;strong&gt;intuition&lt;/strong&gt; behind this formula? We now have a new term inside the expression of the variance, the squared first derivative $f&amp;rsquo;(\mu)^2$ ($\neq$ second derivative). If the derivative of the function is low, the variance decreases since different inputs translate into similar outputs. On the contrary, if the derivative of the function is high, the variance of the distribution is amplified, since different inputs translate into even more different outputs.&lt;/p&gt;
&lt;img src=&#34;fig/delta_intuition.png&#34; width=&#34;700px&#34;/&gt;
&lt;p&gt;The result directly follows from the Taylor approximation of $f \big(\mathbb E_n[X]\big)$&lt;/p&gt;
&lt;p&gt;$$
f\big(\mathbb E_n[X]\big) = f(\mu) + f&amp;rsquo;(\mu) (\mathbb E_n[X] - \mu) + \text{residual}
$$&lt;/p&gt;
&lt;p&gt;Importantly, asymptotically, the last term disappears and the linear approximation holds exactly!&lt;/p&gt;
&lt;p&gt;How is this connected to the ratio estimator? We need a bit more math and to switch from a single dimension to two dimensions in order to understand that. In our case, we have a bivariate function of two random variables, $\Delta R$ and $\Delta C$, which returns their ratio. In the case of a multivariate function $f$, the asymptotic variance of the estimator is given by&lt;/p&gt;
&lt;p&gt;$$
\text{AVar} \big( \hat{\rho} \big) = \nabla \hat{\rho}&amp;rsquo; \Sigma_n \nabla \hat{\rho}
$$&lt;/p&gt;
&lt;p&gt;where, $\nabla$ indicates the &lt;a href=&#34;https://en.wikipedia.org/wiki/Gradient&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;gradient&lt;/a&gt; of the function, i.e. the vector of directional derivatives, and $\Sigma_n$ is the empirical variance-covariance matrix of $X$. In our case, they correspond to&lt;/p&gt;
&lt;p&gt;$$
\nabla \hat{\rho} =
\begin{bmatrix}
\frac{1}{\mathbb E_n [\Delta C]} \newline - \frac{\mathbb E_n [\Delta R]}{\mathbb E_n [\Delta C]^2}
\end{bmatrix}
$$&lt;/p&gt;
&lt;p&gt;and&lt;/p&gt;
&lt;p&gt;$$
\Sigma_n =
\begin{bmatrix}
\text{Var}_n (\Delta R) &amp;amp; \text{Cov}_n (\Delta R, \Delta C) \newline
\text{Cov}_n (\Delta R, \Delta C) &amp;amp; \text{Var}_n (\Delta C) \newline
\end{bmatrix}
$$&lt;/p&gt;
&lt;p&gt;, where the subscripts $n$ indicate the empirical counterparts, as for the expected value.&lt;/p&gt;
&lt;p&gt;Combining the previous three equations together with a little matrix algebra, we get the formula of the asymptotic variance of the return on investment estimator.&lt;/p&gt;
&lt;p&gt;$$
\begin{align*}
\text{AVar} \big( \hat{\rho} \big) &amp;amp;= \frac{1}{\mathbb E_n[\Delta C]^2} \text{Var}_n(\Delta R) - 2 \frac{\mathbb E_n[\Delta R]}{\mathbb E_n[\Delta C]^3} \text{Cov}_n(\Delta R, \Delta C) + \frac{\mathbb E_n[\Delta R]^2}{\mathbb E_n[\Delta C]^4} \text{Var}_n(\Delta C)
\end{align*}
$$&lt;/p&gt;
&lt;p&gt;Since the estimator is given by $\hat{\rho} = \frac{\mathbb E_n[\Delta R]}{\mathbb E_n[\Delta C]}$, we can rewrite the asymptotic variance as&lt;/p&gt;
&lt;p&gt;$$
\begin{align*}
\text{AVar} \big( \hat{\rho} \big) = \frac{1}{\mathbb E_n[\Delta C]^2} \text{Var}_n \Big( \Delta R - \hat{\rho} \Delta C \Big)
\end{align*}
$$&lt;/p&gt;
&lt;p&gt;The last expression is very interesting because it suggests that we can rewrite the asymptotic variance of our estimator as the &lt;strong&gt;variance of a difference-in-means estimator&lt;/strong&gt; for a new auxiliary variable. In fact, we can rewrite the above expression as&lt;/p&gt;
&lt;p&gt;$$
\begin{align*}
\text{AVar} \big( \hat{\rho} \big) = \text{Var}_n \Big( \Delta \tilde R \Big) \qquad \text{where} \quad \tilde R = \frac{R - \hat{\rho} \ C}{| \mathbb E [\Delta C] |}
\end{align*}
$$&lt;/p&gt;
&lt;p&gt;This expression is incredibly useful because it gives us intuition and allows us to estimate the standard deviation of our estimator by &lt;strong&gt;linear regression&lt;/strong&gt;.&lt;/p&gt;
&lt;h2 id=&#34;inference-with-linear-regression&#34;&gt;Inference with Linear Regression&lt;/h2&gt;
&lt;p&gt;Did you skip the previous section? No problem!&lt;/p&gt;
&lt;p&gt;After some algebra, we concluded that we can estimate the variance of a difference-in-means estimator for an &lt;strong&gt;auxiliary variable&lt;/strong&gt; defined as&lt;/p&gt;
&lt;p&gt;$$
\tilde R = \frac{R - \hat{\rho} \ C}{| \mathbb E_n [\Delta C] |}
$$&lt;/p&gt;
&lt;p&gt;This expression might seem obscure at first, but it is incredibly useful. In fact, it gives us (1) an intuitive &lt;strong&gt;interpretation&lt;/strong&gt; of the variance of the estimator and (2) a &lt;strong&gt;practical&lt;/strong&gt; way to estimate it.&lt;/p&gt;
&lt;p&gt;Interpretation first! How should we read the above expression? We can estimate the variance of the empirical estimator as the variance of a difference-in-means estimator, for a new variable $\tilde R$ that we can easily compute from the data. We just need to take the revenue $R$, subtract the cost $C$ multiplied by the estimated ROI $\rho$ and scale it down by the expected cost difference $|\mathbb E_n[\Delta C]|$. We can interpret this variable as the &lt;strong&gt;baseline revenue&lt;/strong&gt;, i.e. the revenue not affected by the investment. The fact that it is scaled by the expected cost difference tells us that its variance will be &lt;strong&gt;decreasing in the total investment&lt;/strong&gt;: the more we spend, the more precisely we can estimate the return on that expenditure.&lt;/p&gt;
&lt;p&gt;Now, let&amp;rsquo;s estimate the variance of the ROI estimator, in &lt;strong&gt;four steps&lt;/strong&gt;.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;We need to estimate the return on investment $\hat \rho$.&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;rho_hat = estimate_roi(df)
&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;The term $| \mathbb E_n[\Delta C] |$ is the absolute difference in average cost between the treatment and control group.&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;abs_Delta_C = np.abs(df.loc[df.new_machine==1, &amp;quot;cost&amp;quot;].mean() - df.loc[df.new_machine==0, &amp;quot;cost&amp;quot;].mean())
&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;3&#34;&gt;
&lt;li&gt;We now have all the ingredients to generate the auxiliary variable $\tilde R$.&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df[&#39;revenue_tilde&#39;] = (df[&#39;revenue&#39;] - rho_hat * df[&#39;cost&#39;]) / abs_Delta_C
&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;4&#34;&gt;
&lt;li&gt;The variance of the treatment-control difference $\Delta \tilde R$ can be directly computed by linear regression, as in randomized controlled trials for difference-in-means estimators (see Agrist and Pischke, 2008).&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;smf.ols(&#39;revenue_tilde ~ new_machine&#39;, df).fit().summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
       &lt;td&gt;&lt;/td&gt;          &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt;   &lt;td&gt;   38.4067&lt;/td&gt; &lt;td&gt;    0.653&lt;/td&gt; &lt;td&gt;   58.771&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;   37.126&lt;/td&gt; &lt;td&gt;   39.688&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;new_machine&lt;/th&gt; &lt;td&gt; -2.01e-14&lt;/td&gt; &lt;td&gt;    0.917&lt;/td&gt; &lt;td&gt;-2.19e-14&lt;/td&gt; &lt;td&gt; 1.000&lt;/td&gt; &lt;td&gt;   -1.797&lt;/td&gt; &lt;td&gt;    1.797&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;The estimated standard error of the ROI is $0.917$, very close to the bootstrap estimate of $0.979$ and the simulated value of $1.055$. However, with respect to bootstrapping, the delta method allowed us to compute it in a single step, making it sensibly &lt;strong&gt;faster&lt;/strong&gt; (around $1000$ times on my local machine).&lt;/p&gt;
&lt;p&gt;Note that this estimated standard deviation implies a 95% confidence interval of $2.0698 +- 1.96 \times 0.917$, equal to $[-0.2735, 3.8671]$. This might seem like good news since the confidence interval does not cover zero. However, note that in this case, a more interesting &lt;strong&gt;null hypothesis&lt;/strong&gt; is that the ROI is equal to 1: we are breaking even. A value larger than 1 implies profits, while a value lower than 1 implies losses. In our case, we cannot reject the null hypothesis that the investment in new machines was not profitable.&lt;/p&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;In this article, we have explored a very common causal inference problem: assessing the &lt;strong&gt;return on investment&lt;/strong&gt;. Whether it&amp;rsquo;s a physical investment in new hardware, a virtual cost, or advertisement expenditure, we are interested in understanding whether this incremental cost has paid off. The additional complications come from the fact that we are studying not one, but two causal quantities, intertwined.&lt;/p&gt;
&lt;p&gt;We have first explored and compared different outcome metrics to assess whether the investment paid off. Then, we have introduced an incredibly powerful method to do inference with complex random variables: the &lt;strong&gt;delta method&lt;/strong&gt;. In the particular case of ratios, the delta method delivers a very insightful and practical functional form for the asymptotic variance of the estimator that can be estimated with a simple linear regression.&lt;/p&gt;
&lt;h3 id=&#34;references&#34;&gt;References&lt;/h3&gt;
&lt;p&gt;[1] A. Deng, U. Knoblich, J. Lu, &lt;a href=&#34;https://arxiv.org/pdf/1803.06336.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Applying the Delta Method in Metric Analytics: A Practical Guide with Novel Ideas&lt;/a&gt; (2018).&lt;/p&gt;
&lt;p&gt;[2] R. Budylin, A. Drutsa, I. Katsev, V. Tsoy, &lt;a href=&#34;https://dl.acm.org/doi/abs/10.1145/3159652.3159699&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Consistent Transformation of Ratio Metrics for Efficient Online Controlled Experiments&lt;/a&gt; (2018). &lt;em&gt;ACM&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;[3] J. Angrist, J. Pischke, &lt;a href=&#34;https://www.mostlyharmlesseconometrics.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Mostly harmless econometrics: An empiricist&amp;rsquo;s companion&lt;/a&gt; (2009). &lt;em&gt;Princeton university press&lt;/em&gt;.&lt;/p&gt;
&lt;h3 id=&#34;related-articles&#34;&gt;Related Articles&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://medium.com/towards-data-science/the-bayesian-bootstrap-6ca4a1d45148&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The Bayesian Bootstrap&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://towardsdatascience.com/df3065a0388e&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Outliers, Leverage, Residuals, and Influential Observations&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://towardsdatascience.com/b07ab46aa782&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;A/B Tests, Privacy, and Online Regression&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;code&#34;&gt;Code&lt;/h3&gt;
&lt;p&gt;You can find the original Jupyter Notebook here:&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/delta.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/delta.ipynb&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Mean vs Median Causal Effect</title>
      <link>https://matteocourthoud.github.io/post/quantile_reg/</link>
      <pubDate>Mon, 10 Oct 2022 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/post/quantile_reg/</guid>
      <description>&lt;p&gt;&lt;em&gt;An introduction to quantile regression.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;In A/B tests, a.k.a. &lt;a href=&#34;https://en.wikipedia.org/wiki/Randomized_controlled_trial&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;randomized controlled trials&lt;/a&gt;, we usually estimate the &lt;strong&gt;average treatment effect (ATE)&lt;/strong&gt;: effect of a treatment (a drug, ad, product, &amp;hellip;) on an outcome of interest (a disease, firm revenue, customer satisfaction, &amp;hellip;), where the &amp;ldquo;average&amp;rdquo; is taken over the test subjects (patients, users, customers, &amp;hellip;). The ATE is a very useful quantity since it tells us the effect that we can expect if we were to treat a new subject with the same treatment.&lt;/p&gt;
&lt;p&gt;However, sometimes we might be interested in quantities different from the average, such as the &lt;a href=&#34;https://en.wikipedia.org/wiki/Median&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;median&lt;/strong&gt;&lt;/a&gt;. The median is an alternative measure of &lt;em&gt;central tendency&lt;/em&gt; that is more robust to outliers and is often more informative with skewed distributions. More generally, we might want to estimate the effect for different &lt;a href=&#34;https://en.wikipedia.org/wiki/Quantile&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;quantiles&lt;/a&gt; of the outcome distribution. A &lt;strong&gt;common use-case&lt;/strong&gt; is studying the impact of a UI change on the loading time of a website: a slightly heavier website might translate in an imperceptible change for most users, but a big change for a few users with very slow connections. Another common use-case is studying the impact of a product change on a product that is bought by few people: do existing customers buy it more or are we attracting new customers?&lt;/p&gt;
&lt;p&gt;These questions are hard to answer with linear regression that estimates the &lt;em&gt;average treatment effect&lt;/em&gt;. A more suitable tool is &lt;strong&gt;quantile regression&lt;/strong&gt; that can instead estimate the &lt;em&gt;median treatment effect&lt;/em&gt;. In this article we are going to cover a brief introduction to quantile regression and the estimation of quantile treatment effects.&lt;/p&gt;
&lt;h2 id=&#34;loyalty-cards-and-spending&#34;&gt;Loyalty Cards and Spending&lt;/h2&gt;
&lt;p&gt;Suppose we were an &lt;strong&gt;online store&lt;/strong&gt; and we wanted to increase sales. We decide to offer our customers a &lt;strong&gt;loyalty card&lt;/strong&gt; that grants them discounts as they increase their spend in the store. We would like to assess if the loyalty card is effective in increasing sales so we run an &lt;strong&gt;A/B test&lt;/strong&gt;: we offer the loyalty card only to a subset of our customers, at random.&lt;/p&gt;
&lt;p&gt;I import the data generating process &lt;code&gt;dgp_loyalty()&lt;/code&gt; from &lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/src/dgp.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;src.dgp&lt;/code&gt;&lt;/a&gt;. I also import some plotting functions and libraries from &lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/src/utils.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;src.utils&lt;/code&gt;&lt;/a&gt;.  To include not only code but also data and tables, I use &lt;a href=&#34;https://deepnote.com&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Deepnote&lt;/a&gt;, a Jupyter-like web-based collaborative notebook environment.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;%matplotlib inline
%config InlineBackend.figure_format = &#39;retina&#39;

from src.utils import *
from src.dgp import dgp_loyalty
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, let&amp;rsquo;s have a look at the data. We have information on $10.000$ customers, for whom we observe their &lt;code&gt;spend&lt;/code&gt; and whether they were offered the &lt;code&gt;loyalty&lt;/code&gt; card. We also observe some demographics, like &lt;code&gt;age&lt;/code&gt; and &lt;code&gt;gender&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df = dgp_loyalty().generate_data()
df.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;loyalty&lt;/th&gt;
      &lt;th&gt;spend&lt;/th&gt;
      &lt;th&gt;age&lt;/th&gt;
      &lt;th&gt;gender&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;30&lt;/td&gt;
      &lt;td&gt;Male&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;26&lt;/td&gt;
      &lt;td&gt;Male&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;27&lt;/td&gt;
      &lt;td&gt;Male&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;29&lt;/td&gt;
      &lt;td&gt;Male&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;23&lt;/td&gt;
      &lt;td&gt;Female&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;Interestingly, we notice that the outcome of interest, &lt;code&gt;spend&lt;/code&gt;, seems to have a lot of zeros. Let&amp;rsquo;s dig deeper.&lt;/p&gt;
&lt;h2 id=&#34;mean-vs-median&#34;&gt;Mean vs Median&lt;/h2&gt;
&lt;p&gt;Before analyzing our experiment, let&amp;rsquo;s have a look at our outcome variable, &lt;code&gt;spend&lt;/code&gt;. We first inspect it using centrality measures. We have two main options: the &lt;strong&gt;mean&lt;/strong&gt; and the &lt;strong&gt;median&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;First of all, what are they? The mean captures the average value, while the median captures the value in the middle of the distribution. In general, the mean is mathematically more tractable and easier to interpret, while the median is more robust to outliers. You can find plenty of articles online comparing the two measures and suggesting which one is more appropriate and when. Let&amp;rsquo;s have a look at the mean and median &lt;code&gt;spend&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;np.mean(df[&#39;spend&#39;])
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;28.136224
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;np.median(df[&#39;spend&#39;])
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;0.0
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;How do we interpret these two numbers? People spend on average 28\$ on our store. However, more than 50% of people don&amp;rsquo;t spend anything. As we can see, both measures are very informative and, to a certain extent, complementary. We can better understand the distribution of &lt;code&gt;spend&lt;/code&gt; by plotting its histogram.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sns.histplot(data=df, x=&amp;quot;spend&amp;quot;).set(ylabel=&#39;&#39;, title=&#39;Spending Distribution&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/quantile_reg_14_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;As previewed by the values of the mean and the median, the distribution of &lt;code&gt;spend&lt;/code&gt; is very skewed, with more than 5000 customers (out of 10000) not spending anything.&lt;/p&gt;
&lt;p&gt;One natural question then is: are we interested in the effect of the &lt;code&gt;loyalty&lt;/code&gt; card on average &lt;code&gt;spend&lt;/code&gt; or on median &lt;code&gt;spend&lt;/code&gt;? The first would tell us if customers spend more on average, while the second would tell us if the average customer spends more.&lt;/p&gt;
&lt;p&gt;Linear regression can tell us the effect of the &lt;code&gt;loyalty&lt;/code&gt; card on average &lt;code&gt;spend&lt;/code&gt;. However, what can we do if we were interested in the effect of the &lt;code&gt;loyalty&lt;/code&gt; card on median &lt;code&gt;spend&lt;/code&gt; (or other quantiles)? The answer is &lt;strong&gt;quantile regression&lt;/strong&gt;.&lt;/p&gt;
&lt;h2 id=&#34;quantile-regression&#34;&gt;Quantile Regression&lt;/h2&gt;
&lt;p&gt;With &lt;strong&gt;linear regression&lt;/strong&gt;, we try to estimate the &lt;em&gt;conditional expectation function&lt;/em&gt; of an outcome variable $Y$ (&lt;code&gt;spend&lt;/code&gt; in our example) with respect to one or more explanatory variables $X$ (&lt;code&gt;loyalty&lt;/code&gt; in our example).&lt;/p&gt;
&lt;p&gt;$$
\mathbb E \big[ Y \big| X \big]
$$&lt;/p&gt;
&lt;p&gt;In other words, we want to find a function $f$ such that $f(X) = \mathbb E[Y|X]$. We do so, by solving the following minimization problem:&lt;/p&gt;
&lt;p&gt;$$
\hat f(X) = \arg \min_{f} \mathbb E \big[ Y - f(X) \big]^2
$$&lt;/p&gt;
&lt;p&gt;It can be shown that the function $f$ that solves this minimization is indeed the conditional expectation of $Y$, with respect to $X$.&lt;/p&gt;
&lt;p&gt;Since $f(X)$ can be infinite dimensional, we usually estimate a parametric form of $f(X)$. The most common one is the linear form $f(X) = \beta X$, where $\beta$ is estimated by solving the corresponding minimization problem:&lt;/p&gt;
&lt;p&gt;$$
\hat \beta = \arg \min_{\beta} \mathbb E \big[ Y - \beta X \big]^2
$$&lt;/p&gt;
&lt;p&gt;The linear form is not just convenient, but it can be interpreted as the best local approximation of $f(X)$, referring to Taylor&amp;rsquo;s expansion.&lt;/p&gt;
&lt;p&gt;With &lt;strong&gt;quantile regression&lt;/strong&gt;, we do the &lt;strong&gt;same&lt;/strong&gt;. The only difference is that, instead of estimating the conditional expectation of $Y$ with respect to $X$, we want to estimate the $q$-&lt;a href=&#34;https://en.wikipedia.org/wiki/Quantile&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;quantile&lt;/a&gt; of $Y$ with respect to $X$.&lt;/p&gt;
&lt;p&gt;$$
\mathbb Q_q \big[ Y \big| X \big]
$$&lt;/p&gt;
&lt;p&gt;First of all, what is a &lt;strong&gt;quantile&lt;/strong&gt;? The &lt;a href=&#34;https://en.wikipedia.org/wiki/Quantile&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Wikipedia definition&lt;/a&gt; says&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;In statistics and probability, quantiles are cut points dividing the range of a probability distribution into continuous intervals with equal probabilities, or dividing the observations in a sample in the same way. Common quantiles have special names, such as quartiles (four groups), deciles (ten groups), and percentiles (100 groups).&amp;rdquo;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;For example, the 0.1-quantile represents the value that sits on the right of 10% of the mass of the distribution. The &lt;strong&gt;median&lt;/strong&gt; is the 0.5-quantile (or, equivalently, the $50^{th}$ percentile or the $5^{th}$ decile) and corresponds with the value in the center of the distribution. Let&amp;rsquo;s see a simple example with a &lt;a href=&#34;https://en.wikipedia.org/wiki/Log-normal_distribution&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;log-normal distribution&lt;/a&gt;. I plot the three quartiles that divide the data in four equally sized bins.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;data = np.random.lognormal(0, 1, 1_000_000);
sns.histplot(data).set(title=&#39;Lognormal Distribution&#39;, xlim=(0,10))
plt.axvline(x=np.percentile(data, 25), c=&#39;C8&#39;, label=&#39;25th percentile&#39;)
plt.axvline(x=np.median(data), c=&#39;C1&#39;, label=&#39;Median (50th pct)&#39;)
plt.axvline(x=np.percentile(data, 75), c=&#39;C3&#39;, label=&#39;75th percentile&#39;)
plt.legend();
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/quantile_reg_18_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;As we can see, the three quartiles divide the data into four bins, of equal size.&lt;/p&gt;
&lt;p&gt;So, what is the &lt;strong&gt;objective&lt;/strong&gt; of quantile regression? The objective is to find a function $f$ such that $f(X) = F^{-1}(y_q)$, where $F$ is the &lt;a href=&#34;https://en.wikipedia.org/wiki/Cumulative_distribution_function&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;cumulative distribution function&lt;/a&gt; of $Y$ and $y_q$ is the $q$-quantile of the distribution of $Y$.&lt;/p&gt;
&lt;p&gt;How do we do this? It can be shown with a little linear algebra that we can obtain the conditional quantile as the solution of the following minimization problem:&lt;/p&gt;
&lt;p&gt;$$
\hat f(X) = \arg \min_{f} \mathbb E \big[ \rho_q (Y - f(X)) \big] = \arg \min_{f} \ (1-q) \int_{-\infty}^{f(x)} (y - f(x)) \text{d} F(y) + q \int_{f(x)}^{\infty} (y - f(x)) \text{d} F(y)
$$&lt;/p&gt;
&lt;p&gt;Where $\rho_q$ is an auxiliary weighting function with the following shape.&lt;/p&gt;
&lt;img src=&#34;fig/rho.png&#34; width=&#34;400px&#34;/&gt;
&lt;p&gt;What is the &lt;strong&gt;intuition&lt;/strong&gt; behind the objective function?&lt;/p&gt;
&lt;p&gt;The idea is that we can interpret the equation as follows&lt;/p&gt;
&lt;p&gt;$$
\mathbb E \big[ \rho_q (Y - f(X)) \big] = (1-q) \underset{\color{red}{\text{mass of distribution below }f(x)}}{\int_{-\infty}^{f(x)} (y - f(x)) \text{d} F(y)} + q \underset{\color{red}{\text{mass of distribution above }f(x)}}{\int_{f(x)}^{\infty} (y - f(x)) \text{d} F(y)} \overset{\color{blue}{\text{if } f(x) = y_q}}{=} - (1-q) q + q (1-q) = 0
$$&lt;/p&gt;
&lt;p&gt;So that, when $f(X)$ corresponds with the quantile $y_q$, the value of the objective function is zero.&lt;/p&gt;
&lt;p&gt;Exactly as before, we can estimate a &lt;strong&gt;parametric form&lt;/strong&gt; of $f$ and, exactly as before, we can interpret it as a best local approximation (not trivially though, see &lt;a href=&#34;https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1468-0262.2006.00671.x&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Angrist, Chernozhukov, and Fernández-Val (2006)&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;$$
\hat \beta_q = \arg \min_{\beta} \mathbb E \big[ \rho_q (Y - \beta X ) \big]
$$&lt;/p&gt;
&lt;p&gt;We wrote $\hat \beta_q$ to indicate that this is the coefficient for the best linear approximation of the conditional $q$-quantile function.&lt;/p&gt;
&lt;p&gt;How do we &lt;strong&gt;estimate&lt;/strong&gt; a quantile regression?&lt;/p&gt;
&lt;h2 id=&#34;estimation&#34;&gt;Estimation&lt;/h2&gt;
&lt;p&gt;The &lt;a href=&#34;https://www.statsmodels.org/stable/index.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;statsmodels&lt;/a&gt; package allows us to estimate quantile regression with the the &lt;a href=&#34;https://www.statsmodels.org/dev/generated/statsmodels.regression.quantile_regression.QuantReg.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;quantreg()&lt;/code&gt;&lt;/a&gt; function. We just need to specify the quantile $q$ when we fit the model. Let&amp;rsquo;s use $q=0.5$, which corresponds with the median.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;smf.quantreg(&amp;quot;spend ~ loyalty&amp;quot;, data=df).fit(q=0.5).summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
      &lt;td&gt;&lt;/td&gt;         &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt; &lt;td&gt; 8.668e-07&lt;/td&gt; &lt;td&gt;    0.153&lt;/td&gt; &lt;td&gt; 5.66e-06&lt;/td&gt; &lt;td&gt; 1.000&lt;/td&gt; &lt;td&gt;   -0.300&lt;/td&gt; &lt;td&gt;    0.300&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;loyalty&lt;/th&gt;   &lt;td&gt;    3.4000&lt;/td&gt; &lt;td&gt;    0.217&lt;/td&gt; &lt;td&gt;   15.649&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    2.974&lt;/td&gt; &lt;td&gt;    3.826&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;Quantile regression estimates a positive coefficient for &lt;code&gt;loyalty&lt;/code&gt;. How does this estimate compare with linear regression?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;smf.ols(&amp;quot;spend ~ loyalty&amp;quot;, data=df).fit().summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
      &lt;td&gt;&lt;/td&gt;         &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt; &lt;td&gt;   25.6583&lt;/td&gt; &lt;td&gt;    0.564&lt;/td&gt; &lt;td&gt;   45.465&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;   24.552&lt;/td&gt; &lt;td&gt;   26.765&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;loyalty&lt;/th&gt;   &lt;td&gt;    4.9887&lt;/td&gt; &lt;td&gt;    0.801&lt;/td&gt; &lt;td&gt;    6.230&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    3.419&lt;/td&gt; &lt;td&gt;    6.558&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;The estimated coefficient with linear regression is higher. What does it mean? We will spend more time on the &lt;em&gt;interpretation&lt;/em&gt; of quantile regression coefficients later.&lt;/p&gt;
&lt;p&gt;Can we condition the analysis on other variables? We suspect that &lt;code&gt;spend&lt;/code&gt; is also affected by other variables and we want to increase the precision of our estimate by also conditioning the analysis on &lt;code&gt;age&lt;/code&gt; and &lt;code&gt;gender&lt;/code&gt;. We can just add the variables to the &lt;code&gt;quantreg()&lt;/code&gt; model.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;smf.quantreg(&amp;quot;spend ~ loyalty + age + gender&amp;quot;, data=df).fit(q=0.5).summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
         &lt;td&gt;&lt;/td&gt;           &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt;      &lt;td&gt;  -50.5353&lt;/td&gt; &lt;td&gt;    1.053&lt;/td&gt; &lt;td&gt;  -47.977&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;  -52.600&lt;/td&gt; &lt;td&gt;  -48.471&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;gender[T.Male]&lt;/th&gt; &lt;td&gt;  -20.2963&lt;/td&gt; &lt;td&gt;    0.557&lt;/td&gt; &lt;td&gt;  -36.410&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;  -21.389&lt;/td&gt; &lt;td&gt;  -19.204&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;loyalty&lt;/th&gt;        &lt;td&gt;    4.5747&lt;/td&gt; &lt;td&gt;    0.546&lt;/td&gt; &lt;td&gt;    8.374&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    3.504&lt;/td&gt; &lt;td&gt;    5.646&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;age&lt;/th&gt;            &lt;td&gt;    2.3663&lt;/td&gt; &lt;td&gt;    0.026&lt;/td&gt; &lt;td&gt;   92.293&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    2.316&lt;/td&gt; &lt;td&gt;    2.417&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;The coefficient of &lt;code&gt;loyalty&lt;/code&gt; increases when we condition the analysis on &lt;code&gt;age&lt;/code&gt; and &lt;code&gt;gender&lt;/code&gt;. This is true also for linear regresssion.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;smf.ols(&amp;quot;spend ~ loyalty + age + gender&amp;quot;, data=df).fit().summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
         &lt;td&gt;&lt;/td&gt;           &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt;      &lt;td&gt;  -57.4466&lt;/td&gt; &lt;td&gt;    0.911&lt;/td&gt; &lt;td&gt;  -63.028&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;  -59.233&lt;/td&gt; &lt;td&gt;  -55.660&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;gender[T.Male]&lt;/th&gt; &lt;td&gt;  -26.3170&lt;/td&gt; &lt;td&gt;    0.482&lt;/td&gt; &lt;td&gt;  -54.559&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;  -27.262&lt;/td&gt; &lt;td&gt;  -25.371&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;loyalty&lt;/th&gt;        &lt;td&gt;    3.9101&lt;/td&gt; &lt;td&gt;    0.473&lt;/td&gt; &lt;td&gt;    8.272&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    2.983&lt;/td&gt; &lt;td&gt;    4.837&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;age&lt;/th&gt;            &lt;td&gt;    2.7688&lt;/td&gt; &lt;td&gt;    0.022&lt;/td&gt; &lt;td&gt;  124.800&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    2.725&lt;/td&gt; &lt;td&gt;    2.812&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;There are a couple of things that we haven&amp;rsquo;t mentioned yet. The first one is &lt;strong&gt;inference&lt;/strong&gt;. How do we compute confidence intervals and p-values for our estimates in quantile regression?&lt;/p&gt;
&lt;h2 id=&#34;inference&#34;&gt;Inference&lt;/h2&gt;
&lt;p&gt;The asymptotic variance of the estimate $a$ of the quantile $q$ of a distribution $F$ is given by&lt;/p&gt;
&lt;p&gt;$$
AVar(y) = q(1-q) f^{-2}(y)
$$&lt;/p&gt;
&lt;p&gt;where $f$ is the &lt;a href=&#34;https://en.wikipedia.org/wiki/Probability_density_function&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;density function&lt;/a&gt; of $F$. This expression can be decomposed into &lt;strong&gt;two components&lt;/strong&gt;: $q(1-q)$ and $f^{-2}(y)$.&lt;/p&gt;
&lt;p&gt;The first component, $q(1-q)$, basically tells us that the variance of a quantile is higher the more the quantile is closer to the center of the distribution. Why is that so? First, we need to think about when the quantile of a point changes in response to a change in the value of a second point. The quantile changes when the second point swaps from left to right (or viceversa) of the first point. This is intuitively very easy if the first point lies in the middle of the distribution, but very hard if it lies at the extreme.&lt;/p&gt;
&lt;p&gt;The second component, $f^{-2}(a)$, instead tells us that this side swapping is more likely if the first point is surrounded by a lot of points.&lt;/p&gt;
&lt;p&gt;Importantly, estimating the variance of a quantile requires an estimate of the whole distribution of $Y$. This is done via approximation and it can be computationally very intensive. However, alternative procedures like the bootstrap or the &lt;a href=&#34;https://towardsdatascience.com/6ca4a1d45148&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;bayesian bootstrap&lt;/a&gt; are always available.&lt;/p&gt;
&lt;p&gt;The second thing that we haven&amp;rsquo;t talked about yet is the &lt;strong&gt;interpretation&lt;/strong&gt; of the estimated coefficients. We got a lower coefficient of &lt;code&gt;loyalty&lt;/code&gt; on &lt;code&gt;spend&lt;/code&gt; with median regression. What does it mean?&lt;/p&gt;
&lt;h2 id=&#34;interpretation&#34;&gt;Interpretation&lt;/h2&gt;
&lt;p&gt;The &lt;strong&gt;interpretation&lt;/strong&gt; of linear regression coefficients is straightforward: each coefficient is the derivative of the conditional expectation function $\mathbb E[Y|X]$ with respect to one dimension of $X$. In our case, we can interpret the regression coefficient of &lt;code&gt;loyalty&lt;/code&gt; as the average &lt;code&gt;spend&lt;/code&gt; increase from being offered a loyalty card. Crucially, here &amp;ldquo;average&amp;rdquo; means that this holds true for &lt;em&gt;each customer&lt;/em&gt;, on average.&lt;/p&gt;
&lt;p&gt;However, the interpretation of quantile regression coefficients is &lt;strong&gt;tricky&lt;/strong&gt;. Before, we were tempted to say that the &lt;code&gt;loyalty&lt;/code&gt; card increases the spend of the median customer by 3.4\$. But &lt;strong&gt;what does it mean&lt;/strong&gt;? Is it the same median customer that spends more or do we have a different median customer? This might seem like a philosophical question but it has important implications on reporting of quantile regression results. In the first case, we are making a statement that, as for the interpretation of linear regression coefficients, applies to a &lt;em&gt;single individual&lt;/em&gt;. In the second case, we are making a statement about the &lt;em&gt;distribution&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1468-0262.2005.00570.x&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Chernozhukov and Hansen (2005)&lt;/a&gt; show that a strong but helpful assumption is &lt;strong&gt;rank invariance&lt;/strong&gt;: assuming that the treatment &lt;strong&gt;does not shift&lt;/strong&gt; the relative composition of the distribution. In other words, if we rank people by &lt;code&gt;spend&lt;/code&gt; before the experiment, we assume that this ranking is not affected by the introduction of the &lt;code&gt;loyalty&lt;/code&gt; card. If I was spending less than you before, I might spend more afterwards, but still less than you (for any two people).&lt;/p&gt;
&lt;p&gt;Under this assumption, we can interpret the quantile coefficients as &lt;strong&gt;marginal effects for single individuals&lt;/strong&gt; sitting at different points of the outcome distribution, as in the first interpretation provided above. Moreover, we can report the treatment effect for many quantiles and interpret each one of them as a local effect for a different individual. Let&amp;rsquo;s plot the distribution of treatment effects, for different quantiles of &lt;code&gt;spend&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def plot_quantile_TE(df, formula, q, varname):
    df_results = pd.DataFrame()
    for q in np.arange(q, 1-q, q):
        qreg = smf.quantreg(formula, data=df).fit(q=q)
        temp = pd.DataFrame({&#39;q&#39;: [q],
                             &#39;coeff&#39;: [qreg.params[varname]], 
                             &#39;std&#39;: [qreg.bse[varname]],
                             &#39;ci_lower&#39;: [qreg.conf_int()[0][varname]],
                             &#39;ci_upper&#39;: [qreg.conf_int()[1][varname]]})
        df_results = pd.concat([df_results, temp]).reset_index(drop=True)
    
    # Plot
    fig, ax = plt.subplots()
    sns.lineplot(data=df_results, x=&#39;q&#39;, y=&#39;coeff&#39;)
    ax.fill_between(data=df_results, x=&#39;q&#39;, y1=&#39;ci_lower&#39;, y2=&#39;ci_upper&#39;, alpha=0.1);
    plt.axhline(y=0, c=&amp;quot;k&amp;quot;, lw=2, zorder=1)
    ols_coeff = smf.ols(formula, data=df).fit().params[varname]
    plt.axhline(y=ols_coeff, ls=&amp;quot;--&amp;quot;, c=&amp;quot;C1&amp;quot;, label=&amp;quot;OLS coefficient&amp;quot;, zorder=1)
    plt.legend()
    plt.title(&amp;quot;Estimated coefficient, by quantile&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plot_quantile_TE(df, formula=&amp;quot;spend ~ loyalty&amp;quot;, varname=&#39;loyalty&#39;, q=0.05)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/quantile_reg_37_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;This plot is &lt;strong&gt;extremely insightful&lt;/strong&gt;: for almost half of the customers, the &lt;code&gt;loyalty&lt;/code&gt; card has no effect. On the other hand, customers that were already spending something end up spending even more (around 10/12\$ more). This is a very powerful insight that we would have missed with linear regression that estimated an average effect of 5\$.&lt;/p&gt;
&lt;p&gt;We can repeat the same exercise, conditioning the analysis on &lt;code&gt;gender&lt;/code&gt; and &lt;code&gt;age&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plot_quantile_TE(df, formula=&amp;quot;spend ~ loyalty + age + gender&amp;quot;, varname=&#39;loyalty&#39;, q=0.05)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/quantile_reg_39_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Conditioning on other covariates removes a lot of the heterogeneity in treatment effects. The &lt;code&gt;loyalty&lt;/code&gt; card increases spending for most people, it&amp;rsquo;s demographic characteristics that are responsible for no spending to begin with.&lt;/p&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;In this article, we have explored a different &lt;strong&gt;causal estimand&lt;/strong&gt;: median treatment effects. How does it compare with the average treatment effect that we usually estimate? The pros and cons are closely related to the pros and cons of the median with respect to the mean as a measure of &lt;em&gt;central tendency&lt;/em&gt;. Median treatment effects are more informative on what is the effect on the average subject and are more robust to outliers. However, they are much more computationally intensive and they require strong assumptions for identification, such as rank invariance.&lt;/p&gt;
&lt;h3 id=&#34;references&#34;&gt;References&lt;/h3&gt;
&lt;p&gt;[1] R. Koenker, &lt;a href=&#34;https://www.cambridge.org/core/books/quantile-regression/C18AE7BCF3EC43C16937390D44A328B1&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Quantile Regression&lt;/a&gt; (1996), Cambridge University Press.&lt;/p&gt;
&lt;p&gt;[1] R. Koenker, K. Hallock, &lt;a href=&#34;https://www.aeaweb.org/articles?id=10.1257/jep.15.4.143&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Quantile Regression&lt;/a&gt;, (2001), &lt;em&gt;Journal of Economic Perspectives&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;[2] V. Chernozhukov, C. Hansen, &lt;a href=&#34;https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1468-0262.2005.00570.x&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;An IV Model of Quantile Treatment Effects&lt;/a&gt; (2005), &lt;em&gt;Econometrica&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;[3] J. Angrist, V. Chernozhukov, I. Fernández-Val, &lt;a href=&#34;https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1468-0262.2006.00671.x&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Quantile Regression under Misspecification, with an Application to the U.S. Wage Structure&lt;/a&gt; (2006), &lt;em&gt;Econometrica&lt;/em&gt;.&lt;/p&gt;
&lt;h3 id=&#34;related-articles&#34;&gt;Related Articles&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/b63dc69e3d8c&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;DAGs and Control Variables&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/6ca4a1d45148&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The Bayesian Bootstrap&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/a928f67413e4&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Goodbye Scatterplot, Welcome Binned Scatterplot&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;code&#34;&gt;Code&lt;/h3&gt;
&lt;p&gt;You can find the original Jupyter Notebook here:&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/quantile_reg.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/quantile_reg.ipynb&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>A/B Tests, Privacy and Online Regression</title>
      <link>https://matteocourthoud.github.io/post/online_reg/</link>
      <pubDate>Sun, 04 Sep 2022 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/post/online_reg/</guid>
      <description>&lt;p&gt;&lt;em&gt;How to run experiments without storing individual data&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;AB tests, a.k.a. randomized controlled trials, are widely recognized as the gold standard technique to compute the causal effect of a treatment (a drug, ad, product, &amp;hellip;) on an outcome of interest (a disease, firm revenue, customer satisfaction, &amp;hellip;). We randomly split a set of subjects (patients, users, customers, &amp;hellip;) into a treatment and a control group and give the treatment to the treatment group. This procedure ensures that ex-ante, the only expected difference between the two groups is caused by the treatment.&lt;/p&gt;
&lt;p&gt;One potential &lt;strong&gt;privacy concern&lt;/strong&gt; is that one needs to store data about many users for the whole duration of the experiment in order to estimate the effect of the treatment. This is not a problem if we can run the experiment instantaneusly, but can become an issue when the experiment duration is long. In this post, we are going to explore one solution to this problem: &lt;strong&gt;online regression&lt;/strong&gt;. We will see how to estimate (conditional) average treatment effects and how to do inference, using both asymptotic approximations and bootstrapping.&lt;/p&gt;
&lt;p&gt;⚠️ Some parts are algebra-intense, but you can skip them if you are only interested in the intuition.&lt;/p&gt;
&lt;h2 id=&#34;credit-cards-and-donations&#34;&gt;Credit Cards and Donations&lt;/h2&gt;
&lt;p&gt;Suppose, for example, that we were a fin-tech company. We have designed a new user interface (UI) for our mobile application and we would like to understand whether it slows down our transaction. In order to estimate the causal effect of the new UI on transaction speed, we plan to run an A/B test or randomized controlled trial.&lt;/p&gt;
&lt;p&gt;We have one major problem: we should not store user-level information for privacy reasons.&lt;/p&gt;
&lt;p&gt;First, let&amp;rsquo;s have a look at the data. I import the data generating process &lt;code&gt;dgp_credit()&lt;/code&gt; from &lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/src/dgp.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;src.dgp&lt;/code&gt;&lt;/a&gt; and some plotting functions and libraries from &lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/src/utils.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;src.utils&lt;/code&gt;&lt;/a&gt;. I include code snippets from &lt;a href=&#34;https://deepnote.com&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Deepnote&lt;/a&gt;, a Jupyter-like web-based collaborative notebook environment. For our purpose, Deepnote is very handy because it allows me not only to include code but also output, like data and tables.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;%matplotlib inline
%config InlineBackend.figure_format = &#39;retina&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from src.utils import *
from src.dgp import dgp_credit
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I first generate the whole dataset in one-shot. We will then investigate how to perform the experimental analysis in case the data was arriving dynamically.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def generate_data(self, N=100, seed=0):
        np.random.seed(seed)
        
        # Connection speed
        connection = np.random.lognormal(3, 1, N)
        
        # Treatment assignment
        treated = np.random.binomial(1, 0.5, N)
        
        # Transfer speed
        #spend = np.minimum(np.random.lognormal(1 + treated + 0.1*np.sqrt(balance), 2, N), balance)
        speed = np.minimum(np.random.exponential(10 + 4*treated - 0.5*np.sqrt(connection), N), connection)
        
        # Generate the dataframe
        df = pd.DataFrame({&#39;c&#39;: [1]*N, &#39;treated&#39;: treated,  
                           &#39;connection&#39;: np.round(connection,2), 
                           &#39;speed&#39;: np.round(speed,2)})

        return df
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;N = 100
df = generate_data(N)
df.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;c&lt;/th&gt;
      &lt;th&gt;treated&lt;/th&gt;
      &lt;th&gt;connection&lt;/th&gt;
      &lt;th&gt;speed&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;117.22&lt;/td&gt;
      &lt;td&gt;0.94&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;29.97&lt;/td&gt;
      &lt;td&gt;29.97&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;53.45&lt;/td&gt;
      &lt;td&gt;7.38&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;188.84&lt;/td&gt;
      &lt;td&gt;0.76&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;130.00&lt;/td&gt;
      &lt;td&gt;24.44&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;We have information on 100 users, for whom we observe&amp;hellip;&lt;/p&gt;
&lt;p&gt;First, let&amp;rsquo;s have a look&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;model = smf.ols(&#39;speed ~ treated + connection&#39;, data=df).fit()
model.summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
       &lt;td&gt;&lt;/td&gt;         &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt;  &lt;td&gt;    6.0740&lt;/td&gt; &lt;td&gt;    1.079&lt;/td&gt; &lt;td&gt;    5.630&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    3.933&lt;/td&gt; &lt;td&gt;    8.215&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;treated&lt;/th&gt;    &lt;td&gt;    1.3939&lt;/td&gt; &lt;td&gt;    1.297&lt;/td&gt; &lt;td&gt;    1.075&lt;/td&gt; &lt;td&gt; 0.285&lt;/td&gt; &lt;td&gt;   -1.180&lt;/td&gt; &lt;td&gt;    3.968&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;connection&lt;/th&gt; &lt;td&gt;   -0.0033&lt;/td&gt; &lt;td&gt;    0.017&lt;/td&gt; &lt;td&gt;   -0.197&lt;/td&gt; &lt;td&gt; 0.844&lt;/td&gt; &lt;td&gt;   -0.037&lt;/td&gt; &lt;td&gt;    0.030&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;In order to understand how we can make linear regression one data point at the time, we first need a brief linear algebra recap.&lt;/p&gt;
&lt;p&gt;First of all, let&amp;rsquo;s define $y$ the dependent variable, &lt;code&gt;spend&lt;/code&gt; in our case, and $X$ the explanatory variable, the &lt;code&gt;treated&lt;/code&gt; indicator, the account &lt;code&gt;balance&lt;/code&gt; and a constant.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def xy_from_df(df, r0, r1):
    return df.iloc[r0:r1,:3].to_numpy(), df.iloc[r0:r1,3].to_numpy()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The  estimator is given by
$$
\hat{\beta}_{OLS} = (X&amp;rsquo;X)^{-1}X&amp;rsquo;y
$$&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from numpy.linalg import inv

X, Y = xy_from_df(df, 0, 100)
inv(X.T @ X) @ X.T @ Y
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;array([ 6.07404291e+00,  1.39385101e+00, -3.33599131e-03])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We get indeed the same exact number as with the &lt;code&gt;smf.ols&lt;/code&gt; command!&lt;/p&gt;
&lt;p&gt;Can we compute $\beta$ one observation at the time?&lt;/p&gt;
&lt;p&gt;The answer is yes! Assume we had $n$ observations and we just received the $n+1$th observation: the pair $(x_{n+1}, y_{n+1})$. In order to compute $\hat{\beta}_{n+1}$ we need to have stored only two objects in memory&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\hat{\beta}_{n}$, the previous estimate of $\beta$&lt;/li&gt;
&lt;li&gt;$(X_n&amp;rsquo; X_n)^{-1}$, the previous value of $(X&amp;rsquo; X)^{-1}$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;First of all, how do we update $(X&amp;rsquo; X)^{-1}$?
$$
\begin{align*}
(X_{n+1}&amp;rsquo; X_{n+1})^{-1} = (X_n&amp;rsquo; X_n)^{-1} - \frac{(X_n&amp;rsquo; X_n)^{-1} x_{n+1} x_{n+1}&amp;rsquo; (X_n&amp;rsquo; X_n)^{-1}}{1 + x_{n+1}&amp;rsquo; (X_n&amp;rsquo; X_n)^{-1} x_{n+1}}
\end{align*}
$$&lt;/p&gt;
&lt;p&gt;After having updated $(X&amp;rsquo; X)^{-1}$, we can update $\hat{\beta}$.
$$
\hat{\beta}&lt;em&gt;{n+1} = \hat{\beta}&lt;/em&gt;{n} + (X_n&amp;rsquo; X_n)^{-1} x_{n} (y_n - x_n&amp;rsquo; \hat{\beta}_{n})
$$&lt;/p&gt;
&lt;p&gt;Note that this procedure is not only privacy friendly but also &lt;strong&gt;memory-friendly&lt;/strong&gt;. Our dataset is a $100 \times 4$ matrix while $(X&amp;rsquo; X)^{-1}$ is $3 \times 3$ matrix and $\beta$ is a $3 \times 1$ matrix. We are storing only 12 numbers instead of up to 400!&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def update_xb(XiX, beta, x, y):
    XiX -= (XiX @ x.T @ x @ XiX) / (1 + x @ XiX @ x.T )
    beta += XiX @ x.T @ (y - x @ beta)
    return XiX, beta
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We are now ready to estimate our OLS coefficient, one data point at the time. However, we cannot really start from the first observation, because we would be unable to invert the matrix $X&amp;rsquo;X$. We need at least $k+1$ observations, where $k$ is the number of variables in $X$.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s use a warm start of 10 observations to be safe.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Initialize XiX and beta from first 10 observations
x, y = xy_from_df(df, 0, 10)
XiX = inv(x.T @ x)
beta = XiX @ x.T @ y

# Update estimate live
for n in range(10, N):
    x, y = xy_from_df(df, n, n+1)
    XiX, beta = update_xb(XiX, beta, x, y)
    
# Print result
print(beta)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[ 6.07404291e+00  1.39385101e+00 -3.33599131e-03]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We got exactly the same coefficient! Nice!&lt;/p&gt;
&lt;p&gt;How did we get there? We can plot the evolution of out estimate $\hat{\beta}$ as we accumulate data. The dynamic plotting function is a bit more cumbersome, but you can find it in &lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/src/figures.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;src.figures&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from src.figures import online_regression

online_regression(df, &amp;quot;fig/online_reg1.gif&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;fig/online_reg1.gif&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;As we can see, as the number of data points increases, the estimate seems to less and less volatile.&lt;/p&gt;
&lt;p&gt;But is it really the case? As usual, we are not just interested in the point estimate of the effect of the &lt;code&gt;coupon&lt;/code&gt; on spending, we would also like to understand how precise this estimate is.&lt;/p&gt;
&lt;h2 id=&#34;inference&#34;&gt;Inference&lt;/h2&gt;
&lt;p&gt;We have seen how to estimate the treatment effect &amp;ldquo;online&amp;rdquo;: one observation at the time. Can we also compute the variance of the estimator in the same manner?&lt;/p&gt;
&lt;p&gt;First of all, let&amp;rsquo;s review what the variance of the OLS estimator looks like. Under baseline assumptions, the variance of the OLS estimator is given by:
$$
\text{Var}(\hat{\beta}_{OLS}) = (X&amp;rsquo;X)^{-1} \hat{\sigma}^2
$$&lt;/p&gt;
&lt;p&gt;where $\hat{\sigma}^2$ is the variance of the residuals $e = (y - X&amp;rsquo;\hat{\beta})$.&lt;/p&gt;
&lt;p&gt;The regression table reports the standard errors of the coefficients, which are the squared elements on the diagonal of $\text{Var}(\hat{\beta})$.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;model.summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
       &lt;td&gt;&lt;/td&gt;         &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt;  &lt;td&gt;    6.0740&lt;/td&gt; &lt;td&gt;    1.079&lt;/td&gt; &lt;td&gt;    5.630&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    3.933&lt;/td&gt; &lt;td&gt;    8.215&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;treated&lt;/th&gt;    &lt;td&gt;    1.3939&lt;/td&gt; &lt;td&gt;    1.297&lt;/td&gt; &lt;td&gt;    1.075&lt;/td&gt; &lt;td&gt; 0.285&lt;/td&gt; &lt;td&gt;   -1.180&lt;/td&gt; &lt;td&gt;    3.968&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;connection&lt;/th&gt; &lt;td&gt;   -0.0033&lt;/td&gt; &lt;td&gt;    0.017&lt;/td&gt; &lt;td&gt;   -0.197&lt;/td&gt; &lt;td&gt; 0.844&lt;/td&gt; &lt;td&gt;   -0.037&lt;/td&gt; &lt;td&gt;    0.030&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;Let&amp;rsquo;s check that we would obtain the same numbers using matrix algebra.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;beta = inv(X.T @ X) @ X.T @ Y
np.sqrt(np.diag(inv(X.T @ X) * np.var(Y - X @ beta)))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;array([1.06261376, 1.27718352, 0.01669716])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Indeed, we get exactly the same numbers!&lt;/p&gt;
&lt;p&gt;We already have a method to part of $\text{Var}(\hat{\beta}&lt;em&gt;{OLS})$ online: $(X&amp;rsquo;X)^{-1}$ update the matrix $(X&amp;rsquo;X)^{-1}$ online. How do we update $\hat{\sigma}^2$? This is the formula to update the sum of squared residuals $S$.
$$
S&lt;/em&gt;{n+1} = S_{n} + \frac{(y_{n+1} - x_{n+1}\hat{\beta}&lt;em&gt;n)}{1 + x&lt;/em&gt;{n+1}&amp;rsquo; (X_n&amp;rsquo; X_n)^{-1} x_{n+1}}
$$&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def update_xbs(XiX, beta, S, x, y):
    S += (y - x @ beta)**2 / (1 + x @ XiX @ x.T )
    XiX -= (XiX @ x.T @ x @ XiX) / (1 + x @ XiX @ x.T )
    beta += XiX @ x.T @ (y - x @ beta)
    return XiX, beta, S[0,0]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note, the order here is very important!&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Inizialize XiX, beta, and sigma from the first 10 observations
x, y = xy_from_df(df, 0, 10)
XiX = inv(x.T @ x)
beta = XiX @ x.T @ y
S = np.sum((y - x @ beta)**2)

# Update XiX, beta, and sigma online
for n in range(10, N):
    x, y = xy_from_df(df, n, n+1)
    XiX, beta, S = update_xbs(XiX, beta, S, x, y)
    
# Print result
print(np.sqrt(np.diag(XiX * S / (N - 3))))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1.0789208  1.29678338 0.0169534 ]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We indeed got the same result! Note that to get from the sum of squared residuals $S$ to the residuals variance $\hat{\sigma}^2$ we need to divide by the degrees of freedom: $n - k = 100 - 3$.&lt;/p&gt;
&lt;p&gt;As before we have plotted the evolution of the estimate of the OLS coefficient over time, we can now augment that plot with a confidence band of +- one standard deviation.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;online_regression(df, &amp;quot;fig/online_reg2.gif&amp;quot;, ci=True)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;fig/online_reg2.gif&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;As we can see, the estimated variance of the OLS estimator indeed decreases as the sample size increases.&lt;/p&gt;
&lt;h2 id=&#34;bootstrap&#34;&gt;Bootstrap&lt;/h2&gt;
&lt;p&gt;So far we have used the asymptotic assumptions behind the Central Limit Theorem to compute the standard errors of the estimator. However, we have a particularly small sample. We further check the empirical distribution of the model residuals.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sns.histplot(model.resid, bins=30);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/online_reg_39_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;The residuals seem to be particularly &lt;strong&gt;skewed&lt;/strong&gt;! This might be a problem in such a small sample.&lt;/p&gt;
&lt;p&gt;One alternative is &lt;strong&gt;the bootstrap&lt;/strong&gt;. Instead of relying on asymptotics, we approximate the distribution of our estimator by resampling our dataset with replacement. Can we bootstrap online?&lt;/p&gt;
&lt;p&gt;The answer is once again yes! They key is to weight each observation with an integer weight drawn from a Poisson distribution with mean (and variance) equal to 1. We repeat this process multiple times, in parallel and then we&lt;/p&gt;
&lt;p&gt;The updating rules for $(X&amp;rsquo;X)^{-1}$ and $\hat{beta}$ become the following.
$$
\begin{align*}
(X_{n+1}&amp;rsquo; X_{n+1})^{-1} = (X_n&amp;rsquo; X_n)^{-1} - \frac{w (X_n&amp;rsquo; X_n)^{-1} x_{n+1} x_{n+1}&amp;rsquo; (X_n&amp;rsquo; X_n)^{-1}}{1 + w x_{n+1}&amp;rsquo; (X_n&amp;rsquo; X_n)^{-1} x_{n+1}}
\end{align*}
$$&lt;/p&gt;
&lt;p&gt;and
$$
\hat{\beta}&lt;em&gt;{n+1} = \hat{\beta}&lt;/em&gt;{n} + w (X_n&amp;rsquo; X_n)^{-1} x_{n} (y_n - x_n&amp;rsquo; \hat{\beta}_{n})
$$&lt;/p&gt;
&lt;p&gt;where $w$ are Poisson weights. First, let&amp;rsquo;s update the updating function for $(X&amp;rsquo;X)^{-1}$ and $\hat{beta}$.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def update_xbw(XiX, beta, w, x, y):
    XiX -= (w * XiX @ x.T @ x @ XiX) / (1 + w * x @ XiX @ x.T )
    beta += w * XiX @ x.T @ (y - x @ beta)
    return XiX, beta
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can now run the online estimation. We bootstrap 1000 different estimates of $\hat{\beta}$.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Inizialize a vector of XiXs and betas 
np.random.seed(0)
K = 1000
x, y = xy_from_df(df, 0, 10)
XiXs = [inv(x.T @ x) for k in range(K)]
betas = [xix @ x.T @ y for xix in XiXs]

# Update the vector of XiXs and betas online
for n in range(10, N):
    x, y = xy_from_df(df, n, n+1)
    for k in range(K):
        w = np.random.poisson(1)
        XiXs[k], betas[k] = update_xbw(XiXs[k], betas[k], w, x, y)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can compute the estimated standard deviation of the treatment effect, simply by computing the standard deviation of the vector of bootstrapped coefficients.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;np.std(betas, axis=0)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;array([0.95301002, 1.14186364, 0.01207962])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The estimated standard errors are slightly different from the previous values of $[1.275, 1.532, 0.020]$, but not very far apart.&lt;/p&gt;
&lt;p&gt;Lastly, some of you might have wondered &amp;ldquo;&lt;em&gt;why sampling discrete weights and not continuous ones?&lt;/em&gt;&amp;rdquo;. Indeed, we can. This procedure is called the &lt;strong&gt;Bayesian Bootstrap&lt;/strong&gt; and you can find a more detailed explanation &lt;a href=&#34;https://towardsdatascience.com/6ca4a1d45148&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;In this post, we have seen hot to run an experiment without storing individual-level data. How are we able to do it? In order to compute the average treatment effect, we do not need every single observation but it&amp;rsquo;s sufficient to store just a more compact representation of it.&lt;/p&gt;
&lt;h3 id=&#34;references&#34;&gt;References&lt;/h3&gt;
&lt;p&gt;[1] W. Chou, &lt;a href=&#34;https://arxiv.org/abs/2102.03316&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Randomized Controlled Trials without Data Retention&lt;/a&gt; (2021), &lt;em&gt;Working Paper&lt;/em&gt;.&lt;/p&gt;
&lt;h3 id=&#34;related-articles&#34;&gt;Related Articles&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/954506cec665&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Experiments, Peeking, and Optimal Stopping&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/6ca4a1d45148&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The Bayesian Bootstrap&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;code&#34;&gt;Code&lt;/h3&gt;
&lt;p&gt;You can find the original Jupyter Notebook here:&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/online_reg.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/online_reg.ipynb&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Outliers, Leverage, and Influential Observations</title>
      <link>https://matteocourthoud.github.io/post/outliers/</link>
      <pubDate>Wed, 17 Aug 2022 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/post/outliers/</guid>
      <description>&lt;p&gt;&lt;em&gt;What makes an observation &amp;ldquo;unusual&amp;rdquo;?&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;In data science, one common task is outlier detection. This is a broad term that is often misused or misunderstood. More broadly, we are often interested in understanding any observation is &lt;strong&gt;&amp;ldquo;unusual&amp;rdquo;&lt;/strong&gt;. First of all, what does it mean to be unusual? In this article we are going to inspect three different ways in which an observation can be unusual: it can be unusual characteristics, it might not fit the model or it might be particularly influential in fitting the model. We will see that in linear regression the latter characteristics is a byproduct of the first two.&lt;/p&gt;
&lt;p&gt;Importantly, being unusual is &lt;strong&gt;not necessarily bad&lt;/strong&gt;. Observations that have different characteristics from all others usually carry more information. We also expect some observations not to fit the model well, otherwise the model is likely biased (overfitting). However, &amp;ldquo;unusual&amp;rdquo; observations are also more likely to be generated by a different process. Extreme cases include measurement error or fraud, but differences can be more nuanced. Domain knowledge is always kind and dropping observations only for for statistical reasons is never wise.&lt;/p&gt;
&lt;p&gt;That said, let&amp;rsquo;s have a look at some different ways in which observations can be &amp;ldquo;unusual&amp;rdquo;.&lt;/p&gt;
&lt;h1 id=&#34;example&#34;&gt;Example&lt;/h1&gt;
&lt;p&gt;Suppose we are an &lt;strong&gt;peer-to-peer online platform&lt;/strong&gt; and we are interested in understanding if there is anything suspicious going on with our business. We have information about how much time our customers spend on the platform and the total value of their transactions.&lt;/p&gt;
&lt;p&gt;First, let&amp;rsquo;s have a look at the data. I import the data generating process &lt;code&gt;dgp_p2p()&lt;/code&gt; from &lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/src/dgp.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;src.dgp&lt;/code&gt;&lt;/a&gt; and some plotting functions and libraries from &lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/src/utils.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;src.utils&lt;/code&gt;&lt;/a&gt;. I include code snippets from &lt;a href=&#34;https://deepnote.com&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Deepnote&lt;/a&gt;, a Jupyter-like web-based collaborative notebook environment. For our purpose, Deepnote is very handy because it allows me not only to include code but also output, like data and tables.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;%matplotlib inline
%config InlineBackend.figure_format = &#39;retina&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from src.utils import *
from src.dgp import dgp_p2p
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df = dgp_p2p().generate_data()
df.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;hours&lt;/th&gt;
      &lt;th&gt;transactions&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;2.6&lt;/td&gt;
      &lt;td&gt;8.30&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;2.0&lt;/td&gt;
      &lt;td&gt;8.00&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;7.0&lt;/td&gt;
      &lt;td&gt;21.00&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;6.7&lt;/td&gt;
      &lt;td&gt;18.00&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;1.2&lt;/td&gt;
      &lt;td&gt;3.82&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;We have information on 50 clients for which we observe &lt;code&gt;hours&lt;/code&gt; spent on the website and total &lt;code&gt;transactions&lt;/code&gt; amount. Since we only have two variables we can easily inspect them using a scatterplot.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sns.scatterplot(data=df, x=&#39;hours&#39;, y=&#39;transactions&#39;).set(title=&#39;Data Scatterplot&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/outliers_9_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;The relationship between &lt;code&gt;hours&lt;/code&gt; and &lt;code&gt;transactions&lt;/code&gt; seems to follow a clear linear relationship. If we fit a linear model, we observe a particularly tight fit.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;smf.ols(&#39;hours ~ transactions&#39;, data=df).fit().summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
        &lt;td&gt;&lt;/td&gt;          &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt;    &lt;td&gt;   -0.0975&lt;/td&gt; &lt;td&gt;    0.084&lt;/td&gt; &lt;td&gt;   -1.157&lt;/td&gt; &lt;td&gt; 0.253&lt;/td&gt; &lt;td&gt;   -0.267&lt;/td&gt; &lt;td&gt;    0.072&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;transactions&lt;/th&gt; &lt;td&gt;    0.3452&lt;/td&gt; &lt;td&gt;    0.009&lt;/td&gt; &lt;td&gt;   39.660&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    0.328&lt;/td&gt; &lt;td&gt;    0.363&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;Does any data point look suspiciously different from the others? How?&lt;/p&gt;
&lt;h2 id=&#34;leverage&#34;&gt;Leverage&lt;/h2&gt;
&lt;p&gt;The first metric that we are going to use to evaluate &amp;ldquo;unusual&amp;rdquo; observations is the &lt;strong&gt;leverage&lt;/strong&gt;, which was first introduced by &lt;a href=&#34;https://www.jstor.org/stable/1268249&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Cook (1980)&lt;/a&gt;. The objective of the leverage is to capture how much a single point is different with respect to other data points. These data points are often called &lt;strong&gt;outliers&lt;/strong&gt; and there exist a nearly amount of algorithms and rules of thumb to flag them.However the idea is the same: flagging observations that are unusual in terms of features.&lt;/p&gt;
&lt;p&gt;The leverage of an observation $i$ is defined as&lt;/p&gt;
&lt;p&gt;$$
h_{ii} := x_i&amp;rsquo; (X&amp;rsquo;X)^{-1} x_i
$$&lt;/p&gt;
&lt;p&gt;One interpretation of the leverage is as a &lt;strong&gt;measure of distance&lt;/strong&gt; where individual observations are compared against the average of all observations.&lt;/p&gt;
&lt;p&gt;Another interpretation of the leverage is as the influence of the outcome of observation $i$, $y_i$, on the corresponding fitted value $\hat{y_i}$.&lt;/p&gt;
&lt;p&gt;$$
h_{ii} = \frac{\partial \hat{y}_i}{\partial y_i}
$$&lt;/p&gt;
&lt;p&gt;Algebraically, the leverage of observation $i$ is the $i^{th}$ element of the &lt;strong&gt;design matrix&lt;/strong&gt; $X&amp;rsquo; (X&amp;rsquo;X)^{-1} X$. Among the many properties of the leverages, is the fact that they are non-negative and their values sum to 1.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s compute the leverage of the observations in our dataset. We also flag observations that have unusual leverages (which we arbitrarily define as more than two standard deviations away from the average leverage).&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;X = np.reshape(df[&#39;hours&#39;].values, (-1, 1))
Y = np.reshape(df[&#39;transactions&#39;].values, (-1, 1))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df[&#39;leverage&#39;] = np.diagonal(X @ np.linalg.inv(X.T @ X) @ X.T)
df[&#39;high_leverage&#39;] = df[&#39;leverage&#39;] &amp;gt; (np.mean(df[&#39;leverage&#39;]) + 2*np.std(df[&#39;leverage&#39;]))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s plot the distribution of leverage values in our data.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;fix, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))
sns.histplot(data=df, x=&#39;leverage&#39;, hue=&#39;high_leverage&#39;, alpha=1, bins=30, ax=ax1).set(title=&#39;Distribution of Leverages&#39;);
sns.scatterplot(data=df, x=&#39;hours&#39;, y=&#39;transactions&#39;, hue=&#39;high_leverage&#39;, ax=ax2).set(title=&#39;Data Scatterplot&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/outliers_18_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;As we can see, the distribution is skewed with two observations having a unusually high leverage. Indeed, in the scatterplot these two observations are slightly separated from the rest of the distribution.&lt;/p&gt;
&lt;p&gt;Is this bad news? It depends. Outliers are &lt;strong&gt;not a problem per se&lt;/strong&gt;. Actually, if they are genuine observations, they might carry much more information than other observations. On the other hand, they are also more likely &lt;em&gt;not&lt;/em&gt; to be genuine observations (e.g. fraud, measurement error, &amp;hellip;) or to be inherently different from the other ones (e.g. professional users vs amateurs). In any case, we might want to investigate further and use as much context-specific information as we can.&lt;/p&gt;
&lt;p&gt;Importantly, the fact that an observation has a high leverage tells us information about the features of the model but nothing about the model itself. Are these users just different observations or they also behave differently?&lt;/p&gt;
&lt;h2 id=&#34;residuals&#34;&gt;Residuals&lt;/h2&gt;
&lt;p&gt;So far we have only talked about unusual features, but what about &lt;strong&gt;unusual behavior&lt;/strong&gt;? This is what regression residuals measure.&lt;/p&gt;
&lt;p&gt;Regression residuals are the difference between the predicted outcome values and the observed outcome values. In a sense, they capture what the model cannot explain: the higher the residual of one observation the more it is unusual in the sense that the model cannot explain it.&lt;/p&gt;
&lt;p&gt;In the case of linear regression, residuals can be written as&lt;/p&gt;
&lt;p&gt;$$
\hat{e} = y - \hat{y} = y - \hat \beta X
$$&lt;/p&gt;
&lt;p&gt;In our case, since $X$ is one dimensional (&lt;code&gt;hours&lt;/code&gt;), we can easily visualize them.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;Y_hat = X @ np.linalg.inv(X.T @ X) @ X.T @ Y
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plt.scatter(X, Y, s=50, label=&#39;data&#39;)
plt.plot(X, Y_hat, c=&#39;k&#39;, lw=2, label=&#39;prediction&#39;)
plt.vlines(X, np.minimum(Y, Y_hat), np.maximum(Y, Y_hat), color=&#39;r&#39;, lw=3, label=&amp;quot;residuals&amp;quot;);
plt.legend()
plt.title(f&amp;quot;Regression prediction and residuals&amp;quot;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/outliers_23_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Do some observations have unusually high residuals? Let&amp;rsquo;s plot their distribution.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df[&#39;residual&#39;] = np.abs(Y - X @ np.linalg.inv(X.T @ X) @ X.T @ Y)
df[&#39;high_residual&#39;] = df[&#39;residual&#39;] &amp;gt; (np.mean(df[&#39;residual&#39;]) + 2*np.std(df[&#39;residual&#39;]))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;fix, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))
sns.histplot(data=df, x=&#39;residual&#39;, hue=&#39;high_residual&#39;, alpha=1, bins=30, ax=ax1).set(title=&#39;Distribution of Residuals&#39;);
sns.scatterplot(data=df, x=&#39;hours&#39;, y=&#39;transactions&#39;, hue=&#39;high_residual&#39;, ax=ax2).set(title=&#39;Data Scatterplot&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/outliers_26_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Two observations have particularly high residuals. This means that for these observations, the model is not good at predicting the observed outcomes.&lt;/p&gt;
&lt;p&gt;Is this bad news? Not necessarily. A model that fits the observations too well is likely to be &lt;a href=&#34;https://en.wikipedia.org/wiki/Bias_%28statistics%29&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;biased&lt;/strong&gt;&lt;/a&gt;. However, it might still be important to understand why some users have a different relationship between hours spent and total transactions. As usual, information on the specific context is key.&lt;/p&gt;
&lt;p&gt;So far we have looked at observations with &amp;ldquo;unusual&amp;rdquo; characteristics and &amp;ldquo;unusual&amp;rdquo; model fit, but what is the observation itself is distorting the model? How much our model is driven by a handful of observations?&lt;/p&gt;
&lt;h2 id=&#34;influence&#34;&gt;Influence&lt;/h2&gt;
&lt;p&gt;The concept of &lt;strong&gt;influence and influence functions&lt;/strong&gt; was developed precisely to answer this question: what are influential observations? This questions were very popular in the 80&amp;rsquo;s and lost appeal for a long time until the recent need of explaining complex machine learning and AI models.&lt;/p&gt;
&lt;p&gt;The general idea is to define an observation as &lt;strong&gt;influential&lt;/strong&gt; if removing it significantly changes the estimated model. In linear regression, we define the influence of observation $i$ as:&lt;/p&gt;
&lt;p&gt;$$
\hat{\beta} - \hat{\beta}_{-i} = (X&amp;rsquo;X)^{-1} x_i e_i
$$&lt;/p&gt;
&lt;p&gt;Where $\hat{\beta}_{-i}$ is the OLS coefficient estimated omitting observation $i$.&lt;/p&gt;
&lt;p&gt;As you can see, there is a tight connection to both leverage $h_{ii}$ and residuals $e_i$: influence is almost the product of the two. Indeed, in linear regression, observations with high leverage are observations that are both outliers and have high residuals. None of the two conditions alone is sufficient for an observation to have an influence on the model.&lt;/p&gt;
&lt;p&gt;We can see it best in the data.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df[&#39;influence&#39;] = (np.linalg.inv(X.T @ X) @ X.T).T * np.abs(Y - Y_hat)
df[&#39;high_influence&#39;] = df[&#39;influence&#39;] &amp;gt; (np.mean(df[&#39;influence&#39;]) + 2*np.std(df[&#39;influence&#39;]))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;fix, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))
sns.histplot(data=df, x=&#39;influence&#39;, hue=&#39;high_influence&#39;, alpha=1, bins=30, ax=ax1).set(title=&#39;Distribution of Influences&#39;);
sns.scatterplot(data=df, x=&#39;hours&#39;, y=&#39;transactions&#39;, hue=&#39;high_influence&#39;, ax=ax2).set(title=&#39;Data Scatterplot&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/outliers_31_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;In our dataset, there is only one observation with high influence, and it is disproportionally larger than the influence of all other observations.&lt;/p&gt;
&lt;p&gt;We can now plot all &amp;ldquo;unusual&amp;rdquo; points in the same plot. I also report residuals and leverage of each point in a separate plot.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def plot_leverage_residuals(df):

    # Hue
    df[&#39;type&#39;] = &#39;Normal&#39;
    df.loc[df[&#39;high_residual&#39;], &#39;type&#39;] = &#39;High Residual&#39;
    df.loc[df[&#39;high_leverage&#39;], &#39;type&#39;] = &#39;High Leverage&#39;
    df.loc[df[&#39;high_influence&#39;], &#39;type&#39;] = &#39;High Influence&#39;

    # Init figure
    fig, (ax1,ax2) = plt.subplots(1,2, figsize=(12,5)) 
    ax1.plot(X, Y_hat, lw=1, c=&#39;grey&#39;, zorder=0.5)
    sns.scatterplot(data=df, x=&#39;hours&#39;, y=&#39;transactions&#39;, ax=ax1, hue=&#39;type&#39;).set(title=&#39;Data&#39;)
    sns.scatterplot(data=df, x=&#39;residual&#39;, y=&#39;leverage&#39;, hue=&#39;type&#39;, ax=ax2).set(title=&#39;Metrics&#39;)
    ax1.get_legend().remove()
    sns.move_legend(ax2, &amp;quot;upper left&amp;quot;, bbox_to_anchor=(1.05, 0.8));
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plot_leverage_residuals(df)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/outliers_34_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;As we can see, we have one point with high residual and low leverage, one with high leverage and low residual and only one point with both high leverage and high residual: the only influential point.&lt;/p&gt;
&lt;p&gt;From the plot it is also clear why none of the two conditions alone is sufficient for an observation to rive the model. The orange point has high residual but it lies right in the middle of the distribution and therefore cannot tilt the line of best fit. The green point instead has high leverage and lies far from the center of the distribution but its perfectly aligned with the line of fit. Removing it would not change anything. The red dot instead is different from the others in terms of &lt;strong&gt;both characteristics and behavior&lt;/strong&gt; and therefore tilts the fit line towards itself.&lt;/p&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;In this post, we have seen some different ways in which observations can be &amp;ldquo;unusual&amp;rdquo;: they can have either unusual characteristics or unusual behavior. In linear regression, when an observation has both it is also influential: it tilts the model towards itself.&lt;/p&gt;
&lt;p&gt;In the example of the article, we concentrated on a univariate linear regression. However, research on influence functions has recently become a hot topic because of the need to make black-box machine learning algorithms understandable. With models with millions of parameters, billions of observations and wild non-linearities, it can be very hard to establish whether a single observation is influential and how.&lt;/p&gt;
&lt;h3 id=&#34;references&#34;&gt;References&lt;/h3&gt;
&lt;p&gt;[1] D. Cook, &lt;a href=&#34;https://www.jstor.org/stable/1268249&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Detection of Influential Observation in Linear Regression&lt;/a&gt; (1980), &lt;em&gt;Technometrics&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;[2] D. Cook, S. Weisberg, &lt;a href=&#34;https://www.jstor.org/stable/1268187&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Characterizations of an Empirical Influence Function for Detecting Influential Cases in
Regression&lt;/a&gt; (1980), &lt;em&gt;Technometrics&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;[2] P. W. Koh, P. Liang, &lt;a href=&#34;http://proceedings.mlr.press/v70/koh17a&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Understanding Black-box Predictions via Influence Functions&lt;/a&gt; (2017), &lt;em&gt;ICML Proceedings&lt;/em&gt;.&lt;/p&gt;
&lt;h3 id=&#34;code&#34;&gt;Code&lt;/h3&gt;
&lt;p&gt;You can find the original Jupyter Notebook here:&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/outliers.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/outliers.ipynb&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>The Bayesian Bootstrap</title>
      <link>https://matteocourthoud.github.io/post/bayes_boot/</link>
      <pubDate>Wed, 17 Aug 2022 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/post/bayes_boot/</guid>
      <description>&lt;p&gt;&lt;em&gt;A short guide to a simple and powerful alternative to the bootstrap&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;In causal inference we do not want just to compute treatment effect, we also want to do &lt;strong&gt;inference&lt;/strong&gt; (duh!). In some cases, it&amp;rsquo;s very easy to compute the asymptotic difference of an estimator, thanks to the &lt;a href=&#34;https://en.wikipedia.org/wiki/Central_limit_theorem&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;central limit theorem&lt;/strong&gt;&lt;/a&gt;. This is the case of computing the average treatment effect in AB tests or randomized controlled trials, for example. However, in other settings, inference is more &lt;strong&gt;complicated&lt;/strong&gt;. The most frequent setting is the computation of quantities that are not sums or averages, such as the median treatment effect, for example. In these cases, we cannot rely on the central limit theorem. What can we do then?&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;bootstrap&lt;/strong&gt; is the answer! It is a very powerful procedure to compute the distribution of an estimator, without needing any knowledge of the data generating process. It is also very &lt;strong&gt;intuitive and simple&lt;/strong&gt; to implement: just re-sample your data with replacement a lot of times and compute your estimator on the re-computed sample.&lt;/p&gt;
&lt;p&gt;Can we do better? The answer is yes! The &lt;strong&gt;Bayesian Bootstrap&lt;/strong&gt; is a powerful procedure that in a lot of setting performs &lt;strong&gt;better&lt;/strong&gt; than the bootstrap. In particular, it&amp;rsquo;s usually faster, can give tighter confidence intervals and prevents a lot of corner cases of the bootstrap. In this article we are going to explore this simple but powerful procedure more in detail.&lt;/p&gt;
&lt;h2 id=&#34;the-bootstrap&#34;&gt;The Bootstrap&lt;/h2&gt;
&lt;p&gt;Bootstrap is a procedure to compute properties of an estimator by random &lt;strong&gt;re-sampling with replacement&lt;/strong&gt; from the data. It was first introduced by &lt;a href=&#34;https://www.jstor.org/stable/2958830&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Efron (1979)&lt;/a&gt;. The procedure is very simple and consists in the following steps.&lt;/p&gt;
&lt;p&gt;Suppose you have access to an i.i.d. sample $\lbrace X_i \rbrace_{i=1}^n$ and you want to compute a statistic $\theta$ using an estimator $\hat \theta(X)$. You can approximate the distribution of $\hat \theta$ by&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Sample $n$ observations with replacement from your sample $\lbrace \tilde X_i \rbrace_{i=1}^n$&lt;/li&gt;
&lt;li&gt;Compute the estimator $\hat \theta_{bootstrap}(\tilde X)$&lt;/li&gt;
&lt;li&gt;Repeat steps 1 and 2 a large number of times&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The distribution of $\hat \theta_{bootstrap}$ is a good approximation of the distribution of $\hat \theta$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Why is the bootstrap so powerful?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;First of all, it&amp;rsquo;s &lt;strong&gt;easy to implement&lt;/strong&gt;. It does not require you to do anything more than what you were already doing: estimating $\theta$. You just need to do it &lt;em&gt;a lot of times&lt;/em&gt;. Indeed, the main disadvantage of the bootstrap is its &lt;strong&gt;computational speed&lt;/strong&gt;. If estimating $\theta$ once is slow, bootstrapping it is prohibitive.&lt;/p&gt;
&lt;p&gt;Second, the bootstrap makes &lt;strong&gt;no distributional assumption&lt;/strong&gt;. It only assumes a representative sample from your population, where observations are independent from each other. This assumption might be violated when observations are tightly connected with each other, such when studying social networks.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Is bootstrap just weighting?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In the end, what we are doing is assigning &lt;strong&gt;integer weights&lt;/strong&gt; to our observations, such that their sum adds up to $n$. Such distribution is the &lt;a href=&#34;https://en.wikipedia.org/wiki/Multinomial_distribution&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;multinomial distribution&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s have a look at what a multinomial distribution look like by drawing a sample of size 10.000.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;%matplotlib inline
%config InlineBackend.figure_format = &#39;retina&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from src.utils import *
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;N = 10_000
np.random.seed(1)
bootstrap_weights = np.random.multinomial(N, np.ones(N)/N)
np.sum(bootstrap_weights)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;10000
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;First of all, we check that indeed the weights sum up to 1000, or equivalently, we generated a re-sample of the same size of the data.&lt;/p&gt;
&lt;p&gt;We can now plot the &lt;strong&gt;distribution of weights&lt;/strong&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sns.countplot(bootstrap_weights, color=&#39;C0&#39;).set(title=&#39;Bootstrap Weights&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/bayes_boot_9_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;As we can see, around 3600 observations got zero weight, however a couple of observations got a weights of 6. Or equivalently, around 3600 observations did not get re-sampled while a couple of observations got samples as many as 6 times.&lt;/p&gt;
&lt;p&gt;Now you might have a spontaneous question: why not use &lt;strong&gt;continuous weights&lt;/strong&gt; instead of discrete ones?&lt;/p&gt;
&lt;p&gt;Very good question! The &lt;strong&gt;Bayesian Bootstrap&lt;/strong&gt; is the answer.&lt;/p&gt;
&lt;h2 id=&#34;the-bayesian-bootstrap&#34;&gt;The Bayesian Bootstrap&lt;/h2&gt;
&lt;p&gt;The Bayesian bootstrap was introduced by &lt;a href=&#34;https://www.jstor.org/stable/2240875&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Rubin (1981)&lt;/a&gt; and it&amp;rsquo;s based on a very simple &lt;strong&gt;idea&lt;/strong&gt;: why not draw a smoother distribution of weights? The continuous equivalent of the multinomial distribution is the &lt;a href=&#34;https://en.wikipedia.org/wiki/Dirichlet_distribution&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Dirichelet distribution&lt;/strong&gt;&lt;/a&gt;. Below I plot the probability distribution of Multinomial and Dirichelet weights for a single observation (they are Poisson and Gamma distributed, respectively).&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from scipy.stats import gamma, poisson

x1 = np.arange(0, 8, 0.001)
x2 = np.arange(0, 8, 1)
sns.barplot(x2, poisson.pmf(x2, mu=1), color=&#39;C0&#39;, label=&#39;Multinomial Weights&#39;); 
plt.plot(x1, gamma.pdf(x1, a=1.0001), color=&#39;C1&#39;, label=&#39;Dirichlet Weights&#39;);
plt.legend()
plt.title(&#39;Distribution of Bootstrap Weights&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/bayes_boot_13_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;The Bayesian Bootstrap has &lt;strong&gt;many advantages&lt;/strong&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The first and most intuitive one is that it delivers estimates that are much more &lt;strong&gt;smooth&lt;/strong&gt; than the normal bootstrap, because of its continuous weighting scheme.&lt;/li&gt;
&lt;li&gt;Moreover, the continuous weighting scheme &lt;strong&gt;prevents corner cases&lt;/strong&gt; from emerging, since no observation will ever receive zero weight. For example, in linear regression, no problem of &lt;a href=&#34;https://en.wikipedia.org/wiki/Multicollinearity&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;collinearity&lt;/a&gt; emerges, if there wasn&amp;rsquo;t one in the original sample.&lt;/li&gt;
&lt;li&gt;Lastly, being a Bayesian method, we gain &lt;strong&gt;interpretation&lt;/strong&gt;: the estimated distribution of the estimator can be interpreted as the &lt;a href=&#34;https://en.wikipedia.org/wiki/Posterior_probability&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;posterior distribution&lt;/a&gt; with an &lt;a href=&#34;https://en.wikipedia.org/wiki/Prior_probability&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;uninformative prior&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Let&amp;rsquo;s now draw a set a Dirichlet weights.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;bayesian_weights = np.random.dirichlet(alpha=np.ones(N), size=1)[0] * N
np.sum(bayesian_weights)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;10000.000000000005
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The weights naturally sum to (approximately) 1, so we have to scale them by a factor N.&lt;/p&gt;
&lt;p&gt;As before, we can plot the distribution of weights, with the difference that now we have continuous weights, so we have to approximate the distribution.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sns.histplot(bayesian_weights, color=&#39;C1&#39;).set(title=&#39;Dirichlet Weights&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/bayes_boot_17_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;As you might have noticed, the Dirichelet distirbution has a parameter $\alpha$ that we have set to 1 for all observations. What does it do?&lt;/p&gt;
&lt;p&gt;The $\alpha$ parameter essentially governs both the absolute and relative probability of being samples. Increasing $\alpha$ for all observations makes the distribution less skewed so that all observations have a more similar weight. For $\alpha \to \infty$, all observations receiver the same weight and we are back to the original sample.&lt;/p&gt;
&lt;p&gt;How should we pick $\alpha$? &lt;a href=&#34;https://link.springer.com/book/10.1007/978-1-4612-0795-5&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Shao and Tu (1995)&lt;/a&gt; suggest the following.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;The distribution of the random weight vector does not have to be restricted to the Diri(l, &amp;hellip; , 1). Later investigations found that the weights having a scaled Diri(4, &amp;hellip; ,4) distribution give better approximations (Tu and Zheng, 1987)&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Let&amp;rsquo;s have a look at how a Dirichelet distribution with $\alpha = 4$ for all observations compare to our previous distribution with $\alpha = 1$ for all observations.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;bayesian_weights2 = np.random.dirichlet(np.ones(N) * 4, 1)[0] * N
sns.histplot(bayesian_weights, color=&#39;C1&#39;)
sns.histplot(bayesian_weights2, color=&#39;C2&#39;).set(title=&#39;Comparing Dirichlet Weights&#39;);
plt.legend([r&#39;$\alpha = 1$&#39;, r&#39;$\alpha = 4$&#39;]);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/bayes_boot_19_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;The new distribution is much less skewed and more concentrated around the average value of 1.&lt;/p&gt;
&lt;h2 id=&#34;examples&#34;&gt;Examples&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s have a look at a couple of examples, where we compare both inference procedures.&lt;/p&gt;
&lt;h3 id=&#34;mean-of-a-skewed-distribution&#34;&gt;Mean of a Skewed Distribution&lt;/h3&gt;
&lt;p&gt;First, let&amp;rsquo;s have a look at one of the simplest and most common estimators: the &lt;strong&gt;sample mean&lt;/strong&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;np.random.seed(2)
X = pd.Series(np.random.pareto(2, 100))
sns.histplot(X).set(title=&#39;Sample from Pareto Distribution&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/bayes_boot_25_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def classic_boot(df, estimator, seed=1):
    df_boot = df.sample(n=len(df), replace=True, random_state=seed)
    estimate = estimator(df_boot)
    return estimate
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;classic_boot(X, np.mean)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;0.7079805545831946
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def bayes_boot(df, estimator, seed=1):
    np.random.seed(seed)
    w = np.random.dirichlet(np.ones(len(df)), 1)[0]
    result = estimator(df, weights=w)
    return result
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;bayes_boot(X, np.average)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;1.0378495251293498
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from joblib import Parallel, delayed

def bootstrap(boot_method, df, estimator, K):
    r = Parallel(n_jobs=8)(delayed(boot_method)(df, estimator, seed=i) for i in range(K))
    return r
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def compare_boot(df, boot1, boot2, estimator, title, K=1000):
    s1 = bootstrap(boot1, df, estimator, K)
    s2 = bootstrap(boot2, df, estimator, K)
    df = pd.DataFrame({&#39;Estimate&#39;: s1 + s2,
                       &#39;Estimator&#39;: [&#39;Classic&#39;]*K + [&#39;Bayes&#39;]*K})
    sns.histplot(data=df, x=&#39;Estimate&#39;, hue=&#39;Estimator&#39;)
    plt.legend([f&#39;Bayes:   {np.mean(s2):.2f} ({np.std(s2):.2f})&#39;,
                f&#39;Classic: {np.mean(s1):.2f} ({np.std(s1):.2f})&#39;])
    plt.title(f&#39;Bootstrap Estimates of {title}&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;compare_boot(X, classic_boot, bayes_boot, np.average, &#39;Sample Mean&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/bayes_boot_32_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;In this setting, both procedures give a very similar answer.&lt;/p&gt;
&lt;p&gt;Which one is faster?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import time

def compare_time(df, boot1, boot2, estimator, K=1000):
    t1, t2 = np.zeros(K), np.zeros(K)
    for k in range(K):
        
        # Classic bootstrap
        start = time.time()
        boot1(df, estimator)
        t1[k] = time.time() - start
    
        # Bayesian bootstrap
        start = time.time()
        boot2(df, estimator)
        t2[k] = time.time() - start
    
    print(f&amp;quot;Bayes wins {np.mean(t1 &amp;gt; t2)*100}% of the time (by {np.mean((t1 - t2)/t1*100):.2f}%)&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;compare_time(X, classic_boot, bayes_boot, np.average)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Bayes wins 99.8% of the time (by 82.89%)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The Bayesian bootstrap is faster than the classical bootstrap 100% of the simulations, and by an impressive 83%!&lt;/p&gt;
&lt;h3 id=&#34;no-weighting-no-problem&#34;&gt;No Weighting? No Problem&lt;/h3&gt;
&lt;p&gt;What if we have an estimator that does not accept weights, such as the median? We can do &lt;strong&gt;two-level sampling&lt;/strong&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def twolv_boot(df, estimator, seed=1):
    np.random.seed(seed)
    w = np.random.dirichlet(np.ones(len(df))*4, 1)[0]
    df_boot = df.sample(n=len(df)*10, replace=True, weights=w, random_state=seed)
    result = estimator(df_boot)
    return result
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;np.random.seed(1)
X = pd.Series(np.random.normal(0, 10, 1000))
compare_boot(X, classic_boot, twolv_boot, np.median, &#39;Sample Median&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/bayes_boot_40_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;In this setting, the Bayesian Bootstrap is also &lt;strong&gt;more precise&lt;/strong&gt; than the classical bootstrap.&lt;/p&gt;
&lt;h3 id=&#34;logistic-regression-with-rare-outcome&#34;&gt;Logistic Regression with Rare Outcome&lt;/h3&gt;
&lt;p&gt;Let&amp;rsquo;s now explore the first of two settings in which the classical bootstrap might fall into &lt;strong&gt;corner cases&lt;/strong&gt;. Suppose we observed a feature $x$, normally distributed, and a binary outcome $y$. We are interested in the relationship between the two variables.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;N = 100
np.random.seed(1)
x = np.random.normal(0, 1, N)
y = np.rint(np.random.normal(x, 1, N) &amp;gt; 2)
df = pd.DataFrame({&#39;x&#39;: x, &#39;y&#39;: y})
df.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;x&lt;/th&gt;
      &lt;th&gt;y&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;1.624345&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;-0.611756&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;-0.528172&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;-1.072969&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;0.865408&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;In this case, we observe a positive outcome only in 10 observations out of 100.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;np.sum(df[&#39;y&#39;])
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;10.0
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Since the outcome is binary, we fit a &lt;a href=&#34;https://en.wikipedia.org/wiki/Logistic_regression&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;logistic regression&lt;/strong&gt;&lt;/a&gt; model.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;smf.logit(&#39;y ~ x&#39;, data=df).fit(disp=False).summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
      &lt;td&gt;&lt;/td&gt;         &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;z&lt;/th&gt;      &lt;th&gt;P&gt;|z|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt; &lt;td&gt;   -4.0955&lt;/td&gt; &lt;td&gt;    0.887&lt;/td&gt; &lt;td&gt;   -4.618&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;   -5.834&lt;/td&gt; &lt;td&gt;   -2.357&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;x&lt;/th&gt;         &lt;td&gt;    2.7664&lt;/td&gt; &lt;td&gt;    0.752&lt;/td&gt; &lt;td&gt;    3.677&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    1.292&lt;/td&gt; &lt;td&gt;    4.241&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;Can we bootstrap the distribution of our estimator? Let&amp;rsquo;s try to compute the logistic regression coefficient over 1000 bootstrap samples.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;estimate_logit = lambda df: smf.logit(&#39;y ~ x&#39;, data=df).fit(disp=False).params[1]
for i in range(1000):
    try:
        classic_boot(df, estimate_logit, seed=i)
    except Exception as e:
        print(f&#39;Error for bootstrap number {i}: {e}&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Error for bootstrap number 92: Perfect separation detected, results not available
Error for bootstrap number 521: Perfect separation detected, results not available
Error for bootstrap number 545: Perfect separation detected, results not available
Error for bootstrap number 721: Perfect separation detected, results not available
Error for bootstrap number 835: Perfect separation detected, results not available
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For 5 samples out of 1000, we are &lt;strong&gt;unable&lt;/strong&gt; to compute the estimate. This would not have happened with then bayesian bootstrap.&lt;/p&gt;
&lt;p&gt;This might seem like an innocuous issue in this case: we can just drop those observations. Let&amp;rsquo;s conclude with a much more dangerous example.&lt;/p&gt;
&lt;p&gt;Suppose we observed a binary feature $x$ and a continuous outcome $y$. We are again interested in the relationship between the two variables.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;N = 100
np.random.seed(1)
x = np.random.binomial(1, 5/N, N)
y = np.random.normal(1 + 2*x, 1, N)
df = pd.DataFrame({&#39;x&#39;: x, &#39;y&#39;: y})
df.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;x&lt;/th&gt;
      &lt;th&gt;y&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1.315635&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;-1.022201&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0.693796&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1.827975&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1.230095&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;Let&amp;rsquo;s compare the two bootstrap estimators of the regression coefficient of $y$ on $x$.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;estimate_beta = lambda df, **kwargs: smf.wls(&#39;y ~ x&#39;, data=df, **kwargs).fit().params[1]
compare_boot(df, classic_boot, bayes_boot, estimate_beta, &#39;beta&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/bayes_boot_55_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;The classic bootstrap procedure estimates a 50% larger variance of our estimator. Why? If we look more closely, we seen that in almost 20 re-samples, we get a very unusual estimate of zero!&lt;/p&gt;
&lt;p&gt;The problem is that in some samples we might not have have &lt;strong&gt;any observations&lt;/strong&gt; with $x=1$. Therefore, in these re-samples, the estimated coefficient is zero. This does not happen with the Bayesian bootstrap, since it does not drop any observation.&lt;/p&gt;
&lt;p&gt;The problematic part here is that we are not getting any error message or warning. This bias is very sneaky and could easily go &lt;strong&gt;unnoticed&lt;/strong&gt;!&lt;/p&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;The article was inspired by the following tweet by Brown University professor &lt;a href=&#34;https://sites.google.com/site/aboutpeterhull/home&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Peter Hull&lt;/a&gt;&lt;/p&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;Ok, so I come bearing good news for ~93% of you: esp. those bootstraping complex models (e.g. w/many FEs)&lt;br&gt;&lt;br&gt;Instead of resampling, which can be seen as reweighting by a random integer W that may be zero, you can reweight by a random non-zero non-integer W &lt;a href=&#34;https://t.co/Rpm1GmomHg&#34;&gt;https://t.co/Rpm1GmomHg&lt;/a&gt;&lt;/p&gt;&amp;mdash; Peter Hull (@instrumenthull) &lt;a href=&#34;https://twitter.com/instrumenthull/status/1487469316010389516?ref_src=twsrc%5Etfw&#34;&gt;January 29, 2022&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;

&lt;p&gt;Indeed, besides being a simple and intuitive procedure, the Bayesian Bootstrap is not part of the standard econometrics curriculum in economic graduate schools.&lt;/p&gt;
&lt;h3 id=&#34;references&#34;&gt;References&lt;/h3&gt;
&lt;p&gt;[1] B. Efron &lt;a href=&#34;https://www.jstor.org/stable/2958830&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Bootstrap Methods: Another Look at the Jackknife&lt;/a&gt; (1979), &lt;em&gt;The Annals of Statistics&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;[2] D. Rubin, &lt;a href=&#34;https://www.jstor.org/stable/2240875&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The Bayesian Bootstrap&lt;/a&gt; (1981), &lt;em&gt;The Annals of Statistics&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;[3] A. Lo, &lt;a href=&#34;https://www.jstor.org/stable/2241087&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;A Large Sample Study of the Bayesian Bootstrap&lt;/a&gt; (1987), &lt;em&gt;The Annals of Statistics&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;[4] J. Shao, D. Tu, &lt;a href=&#34;https://link.springer.com/book/10.1007/978-1-4612-0795-5&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Jacknife and Bootstrap&lt;/a&gt; (1995), &lt;em&gt;Springer&lt;/em&gt;.&lt;/p&gt;
&lt;h3 id=&#34;code&#34;&gt;Code&lt;/h3&gt;
&lt;p&gt;You can find the original Jupyter Notebook here:&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/bayes_boot.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/bayes_boot.ipynb&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Understanding Synthetic Control Methods</title>
      <link>https://matteocourthoud.github.io/post/synth/</link>
      <pubDate>Wed, 27 Jul 2022 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/post/synth/</guid>
      <description>&lt;p&gt;&lt;em&gt;A detailed introduction to one of the most popular causal inference techniques in the industry&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;It is now widely accepted that the gold standard technique to compute the &lt;strong&gt;causal effect&lt;/strong&gt; of a treatment (a drug, ad, product, &amp;hellip;) on an outcome of interest (a disease, firm revenue, customer satisfaction, &amp;hellip;) is &lt;strong&gt;AB testing&lt;/strong&gt;, a.k.a. randomized experiments. We randomly split a set of subjects (patients, users, customers, &amp;hellip;) into a treatment and a control group and give the treatment to the treatment group. This procedure ensures that, ex-ante, the only expected difference between the two groups is caused by the treatment.&lt;/p&gt;
&lt;p&gt;One of the key assumptions in AB testing is that there is &lt;strong&gt;no contamination&lt;/strong&gt; between treatment and control group. Giving a drug to one patient in the treatment group does not affect the health of patients in the control group. This might not be the case for example if we are twying to cure a contageous disease and the two groups are not isolated. In the industry, frequent violations of the contamination assumption are &lt;strong&gt;network effects&lt;/strong&gt; - my utility of using a social network increases as the number of friends on the network increases - and &lt;strong&gt;general equilibrium effects&lt;/strong&gt; - if I improve one product, it might decrease the sales of another similar product.&lt;/p&gt;
&lt;p&gt;Because of this reason, often experiments are carried out at a sufficiently large scale so that there is no contamination across groups, such as cities, states or even countries. Then another problem arises because of the larger scale: the treatment becomes &lt;strong&gt;more expensive&lt;/strong&gt;. Giving a drug to 50% of patients in a hospital is much less expensive than giving a drug to 50% of cities in a country. Therefore, often only &lt;strong&gt;few units are treated&lt;/strong&gt; but often over a longer period of time.&lt;/p&gt;
&lt;p&gt;In these settings, a very powerful method emerged around 10 years age: &lt;strong&gt;Synthetic Control&lt;/strong&gt;. The idea of synthetic control is to exploit the temporal variation in the data instead of the cross-sectional one (across time instead of across units). This method is extremely popular in the industry - e.g. in companies like &lt;a href=&#34;https://proceedings.neurips.cc/paper/2021/file/48d23e87eb98cc2227b5a8c33fa00680-Paper.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Google&lt;/a&gt;, &lt;a href=&#34;https://eng.uber.com/causal-inference-at-uber/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Uber&lt;/a&gt;, &lt;a href=&#34;https://research.facebook.com/publications/regression-adjustment-with-synthetic-controls-in-online-experiments/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Facebook&lt;/a&gt;, &lt;a href=&#34;https://github.com/Microsoft/SparseSC&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Microsoft&lt;/a&gt;, &lt;a href=&#34;https://www.amazon.science/publications/a-self-supervised-approach-to-hierarchical-forecasting-with-applications-to-groupwise-synthetic-controls&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Amazon&lt;/a&gt; - because it is easy to interpret and deals with a setting that emerges often at large scales. In this post we are going to explore this technique by means of example: we will investigate the effectiveness of self-driving cars for a ride-sharing platform.&lt;/p&gt;
&lt;h2 id=&#34;self-driving-cars&#34;&gt;Self-Driving Cars&lt;/h2&gt;
&lt;p&gt;Suppose you were a &lt;strong&gt;ride-sharing platform&lt;/strong&gt; and you wanted to test the effect of self-driving cars in your fleet.&lt;/p&gt;
&lt;p&gt;As you can imagine, there are many &lt;strong&gt;limitations&lt;/strong&gt; to running an AB/test for this type of feature. First of all, it&amp;rsquo;s complicated to randomize individual rides. Second, it&amp;rsquo;s a very expensive intervention. Third, and statistically most important, you cannot run this intervention at the ride level. The problem is that there are &lt;strong&gt;spillover&lt;/strong&gt; effects from treated to control units: if indeed self-driving cars are more efficient, it means that they can serve more customers in the same amount of time, reducing the customers available to normal drivers (the control group). This spillover &lt;strong&gt;contaminates&lt;/strong&gt; the experiment and prevents a causal interpretation of the results.&lt;/p&gt;
&lt;p&gt;For all these reasons, we select only one city. Given the synthetic vibe of the article we cannot but select&amp;hellip; (&lt;em&gt;drum roll&lt;/em&gt;)&amp;hellip; Miami!&lt;/p&gt;
&lt;img src=&#34;fig/miami_skyline_text.jpg&#34; width=&#34;700px&#34;/&gt;
&lt;p&gt;I generate a simulated dataset in which we observe a panel of U.S. cities over time. The revenue data is made up, while the socio-economic variables are taken from the &lt;a href=&#34;https://stats.oecd.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;OECD database&lt;/a&gt;. I import the data generating process &lt;code&gt;dgp_selfdriving()&lt;/code&gt; from &lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/src/dgp.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;src.dgp&lt;/code&gt;&lt;/a&gt;. I also import some plotting functions and libraries from &lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/src/utils.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;src.utils&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;%matplotlib inline
%config InlineBackend.figure_format = &#39;retina&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from src.utils import *
from src.dgp import dgp_selfdriving

treatment_year = 2013
treated_city = &#39;Miami&#39;
df = dgp_selfdriving().generate_data(year=treatment_year, city=treated_city)
df.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;city&lt;/th&gt;
      &lt;th&gt;year&lt;/th&gt;
      &lt;th&gt;density&lt;/th&gt;
      &lt;th&gt;employment&lt;/th&gt;
      &lt;th&gt;gdp&lt;/th&gt;
      &lt;th&gt;population&lt;/th&gt;
      &lt;th&gt;treated&lt;/th&gt;
      &lt;th&gt;post&lt;/th&gt;
      &lt;th&gt;revenue&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;Atlanta&lt;/td&gt;
      &lt;td&gt;2003&lt;/td&gt;
      &lt;td&gt;290&lt;/td&gt;
      &lt;td&gt;0.629761&lt;/td&gt;
      &lt;td&gt;6.4523&lt;/td&gt;
      &lt;td&gt;4.267538&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;25.713947&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;Atlanta&lt;/td&gt;
      &lt;td&gt;2004&lt;/td&gt;
      &lt;td&gt;295&lt;/td&gt;
      &lt;td&gt;0.635595&lt;/td&gt;
      &lt;td&gt;6.5836&lt;/td&gt;
      &lt;td&gt;4.349712&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;23.852279&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;Atlanta&lt;/td&gt;
      &lt;td&gt;2005&lt;/td&gt;
      &lt;td&gt;302&lt;/td&gt;
      &lt;td&gt;0.645614&lt;/td&gt;
      &lt;td&gt;6.6998&lt;/td&gt;
      &lt;td&gt;4.455273&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;24.332397&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;Atlanta&lt;/td&gt;
      &lt;td&gt;2006&lt;/td&gt;
      &lt;td&gt;313&lt;/td&gt;
      &lt;td&gt;0.648573&lt;/td&gt;
      &lt;td&gt;6.5653&lt;/td&gt;
      &lt;td&gt;4.609096&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;23.816017&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;Atlanta&lt;/td&gt;
      &lt;td&gt;2007&lt;/td&gt;
      &lt;td&gt;321&lt;/td&gt;
      &lt;td&gt;0.650976&lt;/td&gt;
      &lt;td&gt;6.4184&lt;/td&gt;
      &lt;td&gt;4.737037&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;25.786902&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;We have information on the largest 46 U.S. cities for the period 2002-2019. The panel is &lt;strong&gt;balanced&lt;/strong&gt;, which means that we observe all cities for all time periods. Self-driving cars were introduced in 2013.&lt;/p&gt;
&lt;p&gt;Is the &lt;strong&gt;treated&lt;/strong&gt; unit, Miami, comparable to the rest of the sample? Let&amp;rsquo;s use the &lt;code&gt;create_table_one&lt;/code&gt; function from Uber&amp;rsquo;s &lt;a href=&#34;https://causalml.readthedocs.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;causalml&lt;/code&gt;&lt;/a&gt; package to produce a &lt;strong&gt;covariate balance table&lt;/strong&gt;, containing the average value of our observable characteristics, across treatment and control groups. As the name suggests, this should always be the first table you present in causal inference analysis.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from causalml.match import create_table_one

create_table_one(df, &#39;treated&#39;, [&#39;density&#39;, &#39;employment&#39;, &#39;gdp&#39;, &#39;population&#39;, &#39;revenue&#39;])
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;Control&lt;/th&gt;
      &lt;th&gt;Treatment&lt;/th&gt;
      &lt;th&gt;SMD&lt;/th&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;Variable&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;n&lt;/th&gt;
      &lt;td&gt;765&lt;/td&gt;
      &lt;td&gt;17&lt;/td&gt;
      &lt;td&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;density&lt;/th&gt;
      &lt;td&gt;256.63 (172.90)&lt;/td&gt;
      &lt;td&gt;364.94 (19.61)&lt;/td&gt;
      &lt;td&gt;0.8802&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;employment&lt;/th&gt;
      &lt;td&gt;0.63 (0.05)&lt;/td&gt;
      &lt;td&gt;0.60 (0.04)&lt;/td&gt;
      &lt;td&gt;-0.5266&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;gdp&lt;/th&gt;
      &lt;td&gt;6.07 (1.16)&lt;/td&gt;
      &lt;td&gt;5.12 (0.29)&lt;/td&gt;
      &lt;td&gt;-1.1124&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;population&lt;/th&gt;
      &lt;td&gt;3.53 (3.81)&lt;/td&gt;
      &lt;td&gt;5.85 (0.31)&lt;/td&gt;
      &lt;td&gt;0.861&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;revenue&lt;/th&gt;
      &lt;td&gt;25.25 (2.45)&lt;/td&gt;
      &lt;td&gt;23.86 (2.39)&lt;/td&gt;
      &lt;td&gt;-0.5737&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;As expected, the groups are &lt;strong&gt;not balanced&lt;/strong&gt;: Miami is more densely populated, poorer, larger and has lower employment rate than the other cities in the US in our sample.&lt;/p&gt;
&lt;p&gt;We are interested in understanding the impact of the introduction of &lt;strong&gt;self-driving cars&lt;/strong&gt; on &lt;code&gt;revenue&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;One initial idea could be to analyze the data as we would in an A/B test, comparing control and treatment group. We can estimate the treatment effect as a difference in means in &lt;code&gt;revenue&lt;/code&gt; between the treatment and control group, after the introduction of self-driving cars.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;smf.ols(&#39;revenue ~ treated&#39;, data=df[df[&#39;post&#39;]==True]).fit().summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
         &lt;td&gt;&lt;/td&gt;            &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt;       &lt;td&gt;   26.6006&lt;/td&gt; &lt;td&gt;    0.127&lt;/td&gt; &lt;td&gt;  210.061&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;   26.351&lt;/td&gt; &lt;td&gt;   26.850&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;treated[T.True]&lt;/th&gt; &lt;td&gt;   -0.7156&lt;/td&gt; &lt;td&gt;    0.859&lt;/td&gt; &lt;td&gt;   -0.833&lt;/td&gt; &lt;td&gt; 0.405&lt;/td&gt; &lt;td&gt;   -2.405&lt;/td&gt; &lt;td&gt;    0.974&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;The effect of self-driving cars seems to be negative but not significant.&lt;/p&gt;
&lt;p&gt;The main &lt;strong&gt;problem&lt;/strong&gt; here is that treatment was &lt;strong&gt;not randomly assigned&lt;/strong&gt;. We have a single treated unit, Miami, and it&amp;rsquo;s hardly comparable to other cities.&lt;/p&gt;
&lt;p&gt;One alternative procedure, is to compare revenue &lt;strong&gt;before and after&lt;/strong&gt; the treatment, within the city of Miami.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;smf.ols(&#39;revenue ~ post&#39;, data=df[df[&#39;city&#39;]==treated_city]).fit().summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
        &lt;td&gt;&lt;/td&gt;          &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt;    &lt;td&gt;   22.4485&lt;/td&gt; &lt;td&gt;    0.534&lt;/td&gt; &lt;td&gt;   42.044&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;   21.310&lt;/td&gt; &lt;td&gt;   23.587&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;post[T.True]&lt;/th&gt; &lt;td&gt;    3.4364&lt;/td&gt; &lt;td&gt;    0.832&lt;/td&gt; &lt;td&gt;    4.130&lt;/td&gt; &lt;td&gt; 0.001&lt;/td&gt; &lt;td&gt;    1.663&lt;/td&gt; &lt;td&gt;    5.210&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;The effect of self-driving cars seems to be positive and statistically significant.&lt;/p&gt;
&lt;p&gt;However, the &lt;strong&gt;problem&lt;/strong&gt; of this procedure is that there might have been many &lt;strong&gt;other things happening&lt;/strong&gt; after 2013. It&amp;rsquo;s quite a stretch to attribute all differences to self-driving cars.&lt;/p&gt;
&lt;p&gt;We can better understand this concern if we plot the time trend of revenue over cities. First, we need to reshape the data into a &lt;strong&gt;wide format&lt;/strong&gt;, with one column per city and one row per year.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df = df.pivot(index=&#39;year&#39;, columns=&#39;city&#39;, values=&#39;revenue&#39;).reset_index()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, let&amp;rsquo;s plot the revenue over time for Miami and for the other cities.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;cities = [c for c in df.columns if c!=&#39;year&#39;]
df[&#39;Other Cities&#39;] = df[[c for c in cities if c != treated_city]].mean(axis=1)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def plot_lines(df, line1, line2, year, hline=True):
    sns.lineplot(x=df[&#39;year&#39;], y=df[line1].values, label=line1)
    sns.lineplot(x=df[&#39;year&#39;], y=df[line2].values, label=line2)
    plt.axvline(x=year, ls=&amp;quot;:&amp;quot;, color=&#39;C2&#39;, label=&#39;Self-Driving Cars&#39;, zorder=1)
    plt.legend();
    plt.title(&amp;quot;Average revenue per day (in M$)&amp;quot;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Since we are talking about Miami, let&amp;rsquo;s use an appropriate color palette.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sns.set_palette(sns.color_palette([&#39;#f14db3&#39;, &#39;#0dc3e2&#39;, &#39;#443a84&#39;]))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plot_lines(df, treated_city, &#39;Other Cities&#39;, treatment_year)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/synth_22_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;As we can see, revenue seems to be increasing after the treatment in Miami. But it&amp;rsquo;s a very volatile time series. And revenue was increasing also in the rest of the country. It&amp;rsquo;s very hard from this plot to attribute the change to self-driving case.&lt;/p&gt;
&lt;p&gt;Can we do better?&lt;/p&gt;
&lt;h2 id=&#34;synthetic-control&#34;&gt;Synthetic Control&lt;/h2&gt;
&lt;p&gt;The answer is yes! Synthetic control allow us to do causal inference when we have &lt;strong&gt;as few as one treated unit&lt;/strong&gt; and &lt;strong&gt;many control units&lt;/strong&gt; and we observe them &lt;strong&gt;over time&lt;/strong&gt;. The idea is simple: combine untreated units so that they mimic the behavior of the treated unit as closely as possible, without the treatment. Then use this &amp;ldquo;synthetic unit&amp;rdquo; as a control. The method first introduced by &lt;a href=&#34;https://www.tandfonline.com/doi/abs/10.1198/jasa.2009.ap08746&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Abadie, Diamond and Hainmueller (2010)&lt;/a&gt; and has been called &lt;a href=&#34;https://www.aeaweb.org/articles?id=10.1257/jep.31.2.3&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;ldquo;the most important innovation in the policy evaluation literature in the last few years&amp;rdquo;&lt;/a&gt;. Moreover, it is widely used in the industry because of its simplicity and interpretability.&lt;/p&gt;
&lt;h3 id=&#34;setting&#34;&gt;Setting&lt;/h3&gt;
&lt;p&gt;We assume that for a panel of i.i.d. subjects $i = 1, &amp;hellip;, n$ over time $t=1, &amp;hellip;,T$ we observed a set of variables $(X_{it}, D_i, Y_{it})$ that includes&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;a treatment assignment $D_i \in \lbrace 0, 1 \rbrace$ (&lt;code&gt;treated&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;a response $Y_{i,t} \in \mathbb R$ (&lt;code&gt;revenue&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;a feature vector $X_{i,t} \in \mathbb R^n$ (&lt;code&gt;population&lt;/code&gt;, &lt;code&gt;density&lt;/code&gt;, &lt;code&gt;employment&lt;/code&gt; and &lt;code&gt;GDP&lt;/code&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Moreover, one unit (Miami in our case) is treated at time $t^*$ (2013 in our case). We distinguish time periods before treatment and time periods after treatment.&lt;/p&gt;
&lt;p&gt;Crucially, treatment $D_i$ is not randomly assigned, therefore a difference in means between the treated unit(s) and the control group is not an unbiased estimator of the average treatment effect.&lt;/p&gt;
&lt;h3 id=&#34;the-problem&#34;&gt;The Problem&lt;/h3&gt;
&lt;p&gt;The problem is that, as usual, we do not observe the counterfactual outcome for treated units, i.e. we do not know what would have happened to them, if they had not been treated. This is known as the &lt;strong&gt;fundamental problem of causal inference&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;The simplest approach, would be just to compare pre and post periods. This is called the &lt;strong&gt;event study&lt;/strong&gt; approach.&lt;/p&gt;
&lt;p&gt;However, we can do better than this. In fact, even though treatment was not randomly assigned, we still have access to some units that were not treated.&lt;/p&gt;
&lt;p&gt;For the outcome variable we observe the following values&lt;/p&gt;
&lt;p&gt;$$
Y =
\begin{bmatrix}
Y^{(1)} _ {t, post} \ &amp;amp; Y^{(0)} _ {c, post} \newline
Y^{(0)} _ {t, pre} \ &amp;amp; Y^{(0)} _ {c, pre}
\end{bmatrix}
$$&lt;/p&gt;
&lt;p&gt;Where $Y^{(d)} _ {a, t}$ is the outcome of an observation at time $t$, given treatment assignment $a$ and treatment status $d$. We basically have a &lt;strong&gt;missing data problem&lt;/strong&gt; since we do not observe $Y^{(0)} _ {t, post}$: what would have happened to treated units ($a=t$) without treatment ($d=0$).&lt;/p&gt;
&lt;h3 id=&#34;the-solution&#34;&gt;The Solution&lt;/h3&gt;
&lt;p&gt;Following &lt;a href=&#34;https://arxiv.org/pdf/1610.07748.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Doudchenko and Inbens (2018)&lt;/a&gt;, we can formulate an estimate of the counterfactual outcome for the treated unit as a linear combination of the observed outcomes for the control units.&lt;/p&gt;
&lt;p&gt;$$
\hat Y^{(0)} _ {t, post} = \alpha + \sum_{i \in c} \beta_{i} Y^{(0)} _ {i, post}
$$&lt;/p&gt;
&lt;p&gt;where&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the constant $\alpha$ allows for different averages between the two groups&lt;/li&gt;
&lt;li&gt;the weights $\beta_i$ are allowed to vary across control units $i$ (otherwise, it would be a difference-in-differences)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;How should we &lt;strong&gt;choose which weights&lt;/strong&gt; to use? We want our synthetic control to approximate the outcome as closely as possible, before the treatment. The first approach could be to define the weights as&lt;/p&gt;
&lt;p&gt;$$
\hat \beta = \arg \min_{\beta} || \boldsymbol X_{t, pre} - \boldsymbol \beta \boldsymbol X_{c, pre} || = \sqrt{ \sum_{p} \left( X_{t, p, pre}  - \sum_{i \in c} \beta_{p} X_{c, p, pre} \right)^2 }
$$&lt;/p&gt;
&lt;p&gt;I.e. the weights are such that they minimize the distance between observable characteristics of control units $X_c$ and the treated unit $X_t$ before the treatment.&lt;/p&gt;
&lt;p&gt;You might notice a very close similarity to &lt;strong&gt;linear regression&lt;/strong&gt;.  Indeed, we are doing something very similar.&lt;/p&gt;
&lt;p&gt;In linear regression, we usually have &lt;strong&gt;many units&lt;/strong&gt; (observations), &lt;strong&gt;few exogenous features&lt;/strong&gt; and &lt;strong&gt;one endogenous feature&lt;/strong&gt; and we try to express the endogenous feature as a linear combination of the endogenous features, for each unit.&lt;/p&gt;
&lt;img src=&#34;fig/synth1.png&#34; width=&#34;500px&#34;/&gt;
&lt;p&gt;With synthetic control, we instead have &lt;strong&gt;many time periods&lt;/strong&gt; (features), few &lt;strong&gt;control units&lt;/strong&gt; and a single &lt;strong&gt;treated unit&lt;/strong&gt; and we try to express the treated unit as a linear combination of the control units, for each time period.&lt;/p&gt;
&lt;p&gt;To perform the same operation, we essentially need to &lt;strong&gt;transpose the data&lt;/strong&gt;.&lt;/p&gt;
&lt;img src=&#34;fig/synth3.png&#34; width=&#34;500px&#34;/&gt;
&lt;p&gt;After the swap, we compute the &lt;strong&gt;synthetic control&lt;/strong&gt; weights, exactly as we would compute regression coefficients. However now one observation is a time period and one feature is a unit.&lt;/p&gt;
&lt;img src=&#34;fig/synth2.png&#34; width=&#34;500px&#34;/&gt;
&lt;p&gt;Notice that this swap is &lt;strong&gt;not innocent&lt;/strong&gt;. In linear regression we assume that the relationship between the exogenous features and the endogenous feature is the same &lt;strong&gt;across units&lt;/strong&gt;, instead in synthetic control we assume that the relationship between the treated units and the control unit is the same &lt;strong&gt;over time&lt;/strong&gt;.&lt;/p&gt;
&lt;h3 id=&#34;back-to-self-driving-cars&#34;&gt;Back to self-driving cars&lt;/h3&gt;
&lt;p&gt;Let&amp;rsquo;s go back to the data now! First, we write a &lt;code&gt;synth_predict&lt;/code&gt; function that takes as input a model that is trained on control cities and tries to predict the outcome of the treated city, Miami, before the introduction of self-driving cars.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def synth_predict(df, model, city, year):
    other_cities = [c for c in cities if c not in [&#39;year&#39;, city]]
    y = df.loc[df[&#39;year&#39;] &amp;lt;= year, city]
    X = df.loc[df[&#39;year&#39;] &amp;lt;= year, other_cities]
    df[f&#39;Synthetic {city}&#39;] = model.fit(X, y).predict(df[other_cities])
    return model
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s estimate the model via linear regression.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from sklearn.linear_model import LinearRegression

coef = synth_predict(df, LinearRegression(), treated_city, treatment_year).coef_
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;How well did we &lt;strong&gt;match&lt;/strong&gt; pre-self-driving cars &lt;code&gt;revenue&lt;/code&gt; in Miami? What is the implied &lt;strong&gt;effect&lt;/strong&gt; of self-driving cars?&lt;/p&gt;
&lt;p&gt;We can visually answer both questions by plotting the actual revenue in Miami against the predicted one.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plot_lines(df, treated_city, f&#39;Synthetic {treated_city}&#39;, treatment_year)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/synth_44_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;It looks like self-driving cars had a sensible &lt;strong&gt;positive effect&lt;/strong&gt; on &lt;code&gt;revenue&lt;/code&gt; in Miami: the predicted trend is lower than the actual data and diverges right after the introduction of self-driving cars.&lt;/p&gt;
&lt;p&gt;On the other hand, we are clearly &lt;strong&gt;overfitting&lt;/strong&gt;: the pre-treatment predicted &lt;code&gt;revenue&lt;/code&gt; line is perfectly overlapping with the actual data. Given the high variability of &lt;code&gt;revenue&lt;/code&gt; in Miami, this is suspicious, to say the least.&lt;/p&gt;
&lt;p&gt;Another problem concerns the &lt;strong&gt;weights&lt;/strong&gt;. Let&amp;rsquo;s plot them.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_states = pd.DataFrame({&#39;city&#39;: [c for c in cities if c!=treated_city], &#39;ols_coef&#39;: coef})
plt.figure(figsize=(10, 9))
sns.barplot(data=df_states, x=&#39;ols_coef&#39;, y=&#39;city&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/synth_46_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;We have many &lt;strong&gt;negative weights&lt;/strong&gt;, which do not make much sense from a causal inference perspective. I can understand that Miami can be expressed as a combination of 0.2 St. Louis, 0.15 Oklahoma and 0.15 Hartford. But what does it mean that Miami is -0.15 Milwaukee?&lt;/p&gt;
&lt;p&gt;Since we would like to interpret our synthetic control as a &lt;strong&gt;weighted average&lt;/strong&gt; of untreated states, all weights should be positive  and they should sum to one.&lt;/p&gt;
&lt;p&gt;To address both concerns (weighting and overfitting), we need to impose some &lt;strong&gt;restrictions on the weights&lt;/strong&gt;.&lt;/p&gt;
&lt;h2 id=&#34;extensions&#34;&gt;Extensions&lt;/h2&gt;
&lt;h3 id=&#34;weights&#34;&gt;Weights&lt;/h3&gt;
&lt;p&gt;To solve the problems of overweighting and negative weights, &lt;a href=&#34;https://www.tandfonline.com/doi/abs/10.1198/jasa.2009.ap08746&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Abadie, Diamond and Hainmueller (2010)&lt;/a&gt; propose the following weights:&lt;/p&gt;
&lt;p&gt;$$
\hat \beta = \arg \min_{\beta} || \boldsymbol X_t - \boldsymbol \beta \boldsymbol X_c || = \sqrt{ \sum_{p} \left( X_{t, p}  - \sum_{i \in c} \beta_{p} X_{c, p} \right)^2 }
\quad \text{s.t.} \quad \sum_{p} \beta_p = 1 \quad \text{and} \quad \beta_p \geq 0 \quad \forall p
$$&lt;/p&gt;
&lt;p&gt;Which means, a set of weights $\beta$ such that&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;weighted observable characteristics of the control group $X_c$, match the observable characteristics of the treatment group $X_t$, before the treatment&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;they sum to 1&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;and are not negative.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;With this approach we get an &lt;strong&gt;interpretable counterfactual&lt;/strong&gt; as a weighted avarage of untreated units.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s write now our own objective function. I create a new class &lt;code&gt;SyntheticControl()&lt;/code&gt; which has both a &lt;code&gt;loss&lt;/code&gt; function, as described above, a method to &lt;code&gt;fit&lt;/code&gt; it and &lt;code&gt;predict&lt;/code&gt; the values for the treated unit.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from toolz import partial
from scipy.optimize import fmin_slsqp

class SyntheticControl():
    
    # Loss function
    def loss(self, W, X, y) -&amp;gt; float:
        return np.sqrt(np.mean((y - X.dot(W))**2))

    # Fit model
    def fit(self, X, y):
        w_start = [1/X.shape[1]]*X.shape[1]
        self.coef_ = fmin_slsqp(partial(self.loss, X=X, y=y),
                         np.array(w_start),
                         f_eqcons=lambda x: np.sum(x) - 1,
                         bounds=[(0.0, 1.0)]*len(w_start),
                         disp=False)
        self.mse = self.loss(W=self.coef_, X=X, y=y)
        return self
    
    # Predict 
    def predict(self, X):
        return X.dot(self.coef_)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can now repeat the same procedure as before, but using the &lt;code&gt;SyntheticControl&lt;/code&gt; method instead of the simple, unconstrained &lt;code&gt;LinearRegression&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_states[&#39;coef_synth&#39;] = synth_predict(df, SyntheticControl(), treated_city, treatment_year).coef_
plot_lines(df, treated_city, f&#39;Synthetic {treated_city}&#39;, treatment_year)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/synth_53_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;As we can see, now we are &lt;strong&gt;not overfitting&lt;/strong&gt; anymore. The actual and predicted &lt;code&gt;revenue&lt;/code&gt; pre-treatment are close but not identical. The reason is that the non-negativity constraint is constraining most coefficients to be zero (as &lt;a href=&#34;https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Lasso&lt;/a&gt; does).&lt;/p&gt;
&lt;p&gt;It looks like the effect is again negative. However, let&amp;rsquo;s plot the &lt;strong&gt;difference&lt;/strong&gt; between the two lines to better visualize the magnitude.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def plot_difference(df, city, year, vline=True, hline=True, **kwargs):
    sns.lineplot(x=df[&#39;year&#39;], y=df[city] - df[f&#39;Synthetic {city}&#39;], **kwargs)
    if vline: 
        plt.axvline(x=year, ls=&amp;quot;:&amp;quot;, color=&#39;C2&#39;, label=&#39;Self-driving cars&#39;, lw=3, zorder=100)
        plt.legend()
    if hline: sns.lineplot(x=df[&#39;year&#39;], y=0, lw=3, color=&#39;k&#39;, zorder=1)
    plt.title(&amp;quot;Estimated effect of self-driving cars&amp;quot;);
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plot_difference(df, treated_city, treatment_year)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/synth_56_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;The difference is clearly positive and slightly increasing over time.&lt;/p&gt;
&lt;p&gt;We can also visualize the &lt;strong&gt;weights&lt;/strong&gt; to interpret the estimated counterfactual (what would have happened in Miami, without self-driving cars).&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plt.figure(figsize=(10, 9))
sns.barplot(data=df_states, x=&#39;coef_synth&#39;, y=&#39;city&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/synth_58_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;As we can see, now we are expressing &lt;code&gt;revenue&lt;/code&gt; in Miami as a linear combination of just a couple of cities: Tampa, St. Louis and, to a lower extent, Las Vegas. This makes the whole procedure very &lt;strong&gt;transparent&lt;/strong&gt;.&lt;/p&gt;
&lt;h3 id=&#34;inference&#34;&gt;Inference&lt;/h3&gt;
&lt;p&gt;What about &lt;strong&gt;inference&lt;/strong&gt;? Is the estimate significantly different from zero? Or, more practically, &amp;ldquo;&lt;em&gt;how unusual is this estimate under the null hypothesis of no policy effect&lt;/em&gt;?&amp;rdquo;.&lt;/p&gt;
&lt;p&gt;We are going to perform a &lt;a href=&#34;https://en.wikipedia.org/wiki/Permutation_test&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;randomization/permutation test&lt;/strong&gt;&lt;/a&gt; in order to answer this question. The &lt;strong&gt;idea&lt;/strong&gt; is that if the policy has no effect, the effect we observe for Miami should not be significantly different from the effect we observe for any other city.&lt;/p&gt;
&lt;p&gt;Therefore, we are going to replicate the procedure above, but for all other cities and compare them with the estimate for Miami.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from matplotlib.offsetbox import OffsetImage, AnnotationBbox

fig, ax = plt.subplots()
for city in cities:
    synth_predict(df, SyntheticControl(), city, treatment_year)
    plot_difference(df, city, treatment_year, vline=False, alpha=0.2, color=&#39;C1&#39;, lw=3)
plot_difference(df, treated_city, treatment_year)
ax.add_artist(AnnotationBbox(OffsetImage(plt.imread(&#39;fig/miami.png&#39;), zoom=0.25), (2015, 2.7), frameon=False));
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/synth_62_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;From the graph we notice two things. First, the effect for Miami is quite &lt;strong&gt;extreme&lt;/strong&gt; and therefore likely not to be driven by random noise.&lt;/p&gt;
&lt;p&gt;Second, we also notice that there are a couple of cities for which we cannot fit the pre-trend very well. In particular, there is a line that is sensibly lower than all others. This is expected since, for each city, we are building the counterfactual trend as a &lt;strong&gt;convex combination&lt;/strong&gt; of all other cities. Cities that are quite extreme in terms of &lt;code&gt;revenue&lt;/code&gt; are very useful to build the counterfactuals of other cities, but it&amp;rsquo;s &lt;strong&gt;hard to build a counterfactual for them&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Not to bias the analysis, let&amp;rsquo;s exclude states for which we cannot build a &amp;ldquo;good enough&amp;rdquo; counterfectual, in terms of pre-treatment MSE.&lt;/p&gt;
&lt;p&gt;$$
MSE_{pre} = \frac{1}{n} \sum_{t \in \text{pre}} \left( Y_t - \hat Y_t \right)^2
$$&lt;/p&gt;
&lt;p&gt;As a rule of thumb, &lt;a href=&#34;https://www.tandfonline.com/doi/abs/10.1198/jasa.2009.ap08746&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Abadie, Diamond and Hainmueller (2010)&lt;/a&gt; suggest to exclude units for which the prediction MSE is larger than twice the MSE of the treated unit.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Reference mse
mse_treated = synth_predict(df, SyntheticControl(), treated_city, treatment_year).mse

# Other mse
fig, ax = plt.subplots()
for city in cities:
    mse = synth_predict(df, SyntheticControl(), city, treatment_year).mse
    if mse &amp;lt; 2 * mse_treated:
        plot_difference(df, city, treatment_year, vline=False, alpha=0.2, color=&#39;C1&#39;, lw=3)
plot_difference(df, treated_city, treatment_year)
ax.add_artist(AnnotationBbox(OffsetImage(plt.imread(&#39;fig/miami.png&#39;), zoom=0.25), (2015, 2.7), frameon=False));
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/synth_64_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;After exluding extreme observations, it looks like the effect for Miami is very unusual.&lt;/p&gt;
&lt;p&gt;One &lt;strong&gt;statistic&lt;/strong&gt; that &lt;a href=&#34;https://www.tandfonline.com/doi/abs/10.1198/jasa.2009.ap08746&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Abadie, Diamond and Hainmueller (2010)&lt;/a&gt; suggest to perform a randomization test is the ratio between pre-treatment MSE and post-treatment MSE.&lt;/p&gt;
&lt;p&gt;$$
\lambda = \frac{MSE_{post}}{MSE_{pre}} = \frac{\frac{1}{n} \sum_{t \in \text{post}} \left( Y_t - \hat Y_t \right)^2 }{\frac{1}{n} \sum_{t \in \text{pre}} \left( Y_t - \hat Y_t \right)^2 }
$$&lt;/p&gt;
&lt;p&gt;We can compute a p-value as the number of observations with higher ratio.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;lambdas = {}
for city in cities:
    mse_pre = synth_predict(df, SyntheticControl(), city, treatment_year).mse
    mse_tot = np.mean((df[f&#39;Synthetic {city}&#39;] - df[city])**2)
    lambdas[city] = (mse_tot - mse_pre) / mse_pre
    
print(f&amp;quot;p-value: {np.mean(np.fromiter(lambdas.values(), dtype=&#39;float&#39;) &amp;gt; lambdas[treated_city]):.4}&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;p-value: 0.04348
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It seems that only $4.3%$ of the cities had a larger MSE ratio than Miami, implying a p-value of 0.043. We can visualize the distribution of the statistic under permutation with a histogram.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;fig, ax = plt.subplots()
_, bins, _ = plt.hist(lambdas.values(), bins=20, color=&amp;quot;C1&amp;quot;);
plt.hist([lambdas[treated_city]], bins=bins)
plt.title(&#39;Ratio of $MSE_{post}$ and $MSE_{pre}$ across cities&#39;);
ax.add_artist(AnnotationBbox(OffsetImage(plt.imread(&#39;fig/miami.png&#39;), zoom=0.25), (2.7, 1.7), frameon=False));
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/synth_68_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Indeed, the statistic for Miami is quite extreme.&lt;/p&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;In this article, we have explored a very popoular method for causal inference when we have &lt;strong&gt;few treated units&lt;/strong&gt;, but many time periods. This setting emerges often in industry settings when the treatment has to be assigned at the &lt;strong&gt;aggregate level&lt;/strong&gt; and randomization might not be possible. The key idea of synthetic control is to &lt;strong&gt;combinate control units&lt;/strong&gt; into one syntetic control unit to use as counterfactual to estimate the causal effect of the treatment.&lt;/p&gt;
&lt;p&gt;One of the main &lt;strong&gt;advantages&lt;/strong&gt; of synthetic control is that, as long as we use positive weights that are constrained to sum to one, the method &lt;strong&gt;avoids extrapolation&lt;/strong&gt;: we will never go out of the support of the data. Moreover, synthetic control studies can be &lt;strong&gt;&amp;ldquo;pre-registered&amp;rdquo;&lt;/strong&gt;: you can specify the weights before the study to avoid &lt;a href=&#34;https://en.wikipedia.org/wiki/Data_dredging&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;p-hacking&lt;/a&gt; and cherry picking. Another reason why this method is so popular in the industry is that weights make the counterfactual analysis &lt;strong&gt;explicit&lt;/strong&gt;: one can look at the weights and understand which comparison we are making.&lt;/p&gt;
&lt;p&gt;This method is relatively young and many &lt;strong&gt;extensions&lt;/strong&gt; are appearing every year. Some notable ones are the generalyzed synthetic control by &lt;a href=&#34;https://www.cambridge.org/core/journals/political-analysis/article/generalized-synthetic-control-method-causal-inference-with-interactive-fixed-effects-models/B63A8BD7C239DD4141C67DA10CD0E4F3&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Xu (2017)&lt;/a&gt;, the synthetic difference-in-differences by &lt;a href=&#34;https://arxiv.org/pdf/1610.07748.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Doudchenko and Imbens (2017)&lt;/a&gt;, the penalyzed synthetic control of &lt;a href=&#34;https://www.tandfonline.com/doi/full/10.1080/01621459.2021.1971535&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Abadie e L&amp;rsquo;Hour (2020)&lt;/a&gt; and the matrix completion methods of &lt;a href=&#34;https://www.tandfonline.com/doi/abs/10.1080/01621459.2021.1891924&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Athey et al. (2021)&lt;/a&gt;. Last but not least, if you want to have the method explained by one of its inventors, there is this great lecture by Alberto Abadie at the NBER Summer Institute freely available on Youtube.&lt;/p&gt;
&lt;br&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/T2p9Wg650bY&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;br&gt;
&lt;h3 id=&#34;references&#34;&gt;References&lt;/h3&gt;
&lt;p&gt;[1] A. Abadie, A. Diamond and J. Hainmueller, &lt;a href=&#34;https://www.tandfonline.com/doi/abs/10.1198/jasa.2009.ap08746&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Synthetic Control Methods for Comparative Case Studies: Estimating the Effect of California’s Tobacco Control Program&lt;/a&gt; (2010), &lt;em&gt;Journal of the American Statistical Association&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;[2] A. Abadie, &lt;a href=&#34;https://www.aeaweb.org/articles?id=10.1257/jel.20191450&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Using Synthetic Controls: Feasibility, Data Requirements, and Methodological Aspects&lt;/a&gt; (2021), &lt;em&gt;Journal of Economic Perspectives&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;[3] N. Doudchenko, G. Imbens, &lt;a href=&#34;https://arxiv.org/pdf/1610.07748.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Balancing, Regression, Difference-In-Differences and Synthetic Control Methods: A Synthesis&lt;/a&gt; (2017), &lt;em&gt;working paper&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;[4] Y. Xu, &lt;a href=&#34;https://www.cambridge.org/core/journals/political-analysis/article/generalized-synthetic-control-method-causal-inference-with-interactive-fixed-effects-models/B63A8BD7C239DD4141C67DA10CD0E4F3&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Generalized Synthetic Control Method: Causal Inference with Interactive Fixed Effects Models&lt;/a&gt; (2018), &lt;em&gt;Political Analysis&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;[5] A. Abadie, J. L&amp;rsquo;Hour, &lt;a href=&#34;https://www.tandfonline.com/doi/full/10.1080/01621459.2021.1971535&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;A Penalized Synthetic Control Estimator for Disaggregated Data&lt;/a&gt; (2020), &lt;em&gt;Journal of the American Statistical Association&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;[6] S. Athey, M. Bayati, N. Doudchenko, G. Imbens, K. Khosravi, &lt;a href=&#34;https://www.tandfonline.com/doi/abs/10.1080/01621459.2021.1891924&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Matrix Completion Methods for Causal Panel Data Models&lt;/a&gt; (2021), &lt;em&gt;Journal of the American Statistical Association&lt;/em&gt;.&lt;/p&gt;
&lt;h3 id=&#34;related-articles&#34;&gt;Related Articles&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/b63dc69e3d8c&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;DAGs and Control Variables&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/99bf5cffa0d9&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Matching, Weighting, or Regression?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/8a9c1e340832&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Understanding Meta Learners&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;code&#34;&gt;Code&lt;/h3&gt;
&lt;p&gt;You can find the original Jupyter Notebook here:&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/synth.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/synth.ipynb&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Weighting, Matching, or Regression?</title>
      <link>https://matteocourthoud.github.io/post/ipw/</link>
      <pubDate>Mon, 18 Jul 2022 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/post/ipw/</guid>
      <description>&lt;p&gt;&lt;em&gt;Understanding and comparing different methods for conditional causal inference analysis&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;AB tests or randomized controlled trials are the &lt;strong&gt;gold standard&lt;/strong&gt; in causal inference. By randomly exposing units to a treatment we make sure that individuals in both groups are comparable, on average, and any difference we observe can be attributed to the treatment effect alone.&lt;/p&gt;
&lt;p&gt;However, often the treatment and control groups are &lt;strong&gt;not perfectly comparable&lt;/strong&gt;. This could be due to the fact that randomization was not perfect or available. Not always we can randomize a treatment, for ethical or practical reasons. And even when we can, sometimes we do not have enough individuals or units so that differences between groups are seizable. This happens often, for example, when randomization is not done at the individual level, but at a higher level of aggregation, for example zipcodes, counties or even states.&lt;/p&gt;
&lt;p&gt;In these settings, we can still recover a causal estimate of the treatment effect if we have &lt;strong&gt;enough information&lt;/strong&gt; about individuals, by making the treatment and control group comparable, ex-post. In this blog post, we are going to introduce and compare different procedures to estimate causal effects in presence of imbalances between treatment and control groups that are &lt;strong&gt;fully observable&lt;/strong&gt;. In particular we are going to analyze weighting, matching and regression procedures.&lt;/p&gt;
&lt;h2 id=&#34;example&#34;&gt;Example&lt;/h2&gt;
&lt;p&gt;Assume we had blog on statistics and causal inference 😇. To improve user experience, we are considering &lt;strong&gt;releasing a dark mode&lt;/strong&gt;, and we would like to understand whether this new feature increases the time users spend on our blog.&lt;/p&gt;
&lt;img src=&#34;fig/modes.png&#34; width=&#34;600px&#34;/&gt;
&lt;p&gt;We are not a sophisticated company, therefore we do not run an AB test but we simply release the dark mode and we observe whether users select it or not and the time they spend on the blog. We know that there might be &lt;strong&gt;selection&lt;/strong&gt;:  users that prefer the dark mode could have different reading preferences and this might complicate our causal analysis.&lt;/p&gt;
&lt;p&gt;We can represent the data generating process with the following &lt;a href=&#34;https://towardsdatascience.com/b63dc69e3d8c&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Directed Acyclic Graph (DAG)&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-mermaid&#34;&gt;flowchart TB
classDef included fill:#DCDCDC,stroke:#000000,stroke-width:2px;
classDef excluded fill:#ffffff,stroke:#000000,stroke-width:2px;
classDef unobserved fill:#ffffff,stroke:#000000,stroke-width:2px,stroke-dasharray: 5 5;

X1((gender))
X2((age))
X3((hours))
D((dark mode))
Y((read time))

D --&amp;gt; Y
X1 --&amp;gt; Y
X1 --&amp;gt; D
X2 --&amp;gt; D
X3 --&amp;gt; Y

class D,Y included;
class X1,X2,X3 excluded;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We generate the simulated data using the data generating process &lt;code&gt;dgp_darkmode()&lt;/code&gt; from &lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/src/dgp.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;src.dgp&lt;/code&gt;&lt;/a&gt;. I also import some plotting functions and libraries from &lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/src/utils.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;src.utils&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;%matplotlib inline
%config InlineBackend.figure_format = &#39;retina&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from src.utils import *
from src.dgp import dgp_darkmode
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df = dgp_darkmode().generate_data()
df.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;read_time&lt;/th&gt;
      &lt;th&gt;dark_mode&lt;/th&gt;
      &lt;th&gt;male&lt;/th&gt;
      &lt;th&gt;age&lt;/th&gt;
      &lt;th&gt;hours&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;14.4&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;43.0&lt;/td&gt;
      &lt;td&gt;65.6&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;15.4&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;55.0&lt;/td&gt;
      &lt;td&gt;125.4&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;20.9&lt;/td&gt;
      &lt;td&gt;True&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;23.0&lt;/td&gt;
      &lt;td&gt;642.6&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;20.0&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;41.0&lt;/td&gt;
      &lt;td&gt;129.1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;21.5&lt;/td&gt;
      &lt;td&gt;True&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;29.0&lt;/td&gt;
      &lt;td&gt;190.2&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;We have informations on 300 users for whom we observe whether they select the &lt;code&gt;dark_mode&lt;/code&gt; (the treatment), their weekly &lt;code&gt;read_time&lt;/code&gt; (the outcome of interest) and some characteristics like &lt;code&gt;gender&lt;/code&gt;, &lt;code&gt;age&lt;/code&gt; and total &lt;code&gt;hours&lt;/code&gt; previously spend on the blog.&lt;/p&gt;
&lt;p&gt;We would like to estimate the effect of the new &lt;code&gt;dark_mode&lt;/code&gt; on users&amp;rsquo; &lt;code&gt;read_time&lt;/code&gt;. If we were runnig an &lt;a href=&#34;https://de.wikipedia.org/wiki/A/B-Test&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;AB test&lt;/strong&gt;&lt;/a&gt; or randomized control trial, we could just compare users with and without the dark mode and we could attribute the difference in average reading time to the &lt;code&gt;dark_mode&lt;/code&gt;. Let&amp;rsquo;s check what number we would get.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;np.mean(df.loc[df.dark_mode==True, &#39;read_time&#39;]) - np.mean(df.loc[df.dark_mode==False, &#39;read_time&#39;])
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;-0.4446330948042103
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Individuals that select the &lt;code&gt;dark_mode&lt;/code&gt; spend on average 1.37 hours less on the blog, per week. Should we conclude that &lt;code&gt;dark_mode&lt;/code&gt; is a &lt;strong&gt;bad idea&lt;/strong&gt;? Is this a causal effect?&lt;/p&gt;
&lt;p&gt;We did not randomize the &lt;code&gt;dark_mode&lt;/code&gt; so that users that selected it might not be directly &lt;strong&gt;comparable&lt;/strong&gt; with users that didn&amp;rsquo;t. Can we verify this concern? Partially. We can only check characteristics that we observe, &lt;code&gt;gender&lt;/code&gt;, &lt;code&gt;age&lt;/code&gt; and total &lt;code&gt;hours&lt;/code&gt; in our setting. We cannot check if users differ along other dimensions that we don&amp;rsquo;t observe.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s use the &lt;code&gt;create_table_one&lt;/code&gt; function from Uber&amp;rsquo;s &lt;a href=&#34;https://causalml.readthedocs.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;causalml&lt;/code&gt;&lt;/a&gt; package to produce a &lt;strong&gt;covariate balance table&lt;/strong&gt;, containing the average value of our observable characteristics, across treatment and control groups. As the name suggests, this should always be the first table you present in causal inference analysis.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from causalml.match import create_table_one

X = [&#39;male&#39;, &#39;age&#39;, &#39;hours&#39;]
table1 = create_table_one(df, &#39;dark_mode&#39;, X)
table1
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;Control&lt;/th&gt;
      &lt;th&gt;Treatment&lt;/th&gt;
      &lt;th&gt;SMD&lt;/th&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;Variable&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;n&lt;/th&gt;
      &lt;td&gt;151&lt;/td&gt;
      &lt;td&gt;149&lt;/td&gt;
      &lt;td&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;age&lt;/th&gt;
      &lt;td&gt;46.01 (9.79)&lt;/td&gt;
      &lt;td&gt;39.09 (11.53)&lt;/td&gt;
      &lt;td&gt;-0.6469&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;hours&lt;/th&gt;
      &lt;td&gt;337.78 (464.00)&lt;/td&gt;
      &lt;td&gt;328.57 (442.12)&lt;/td&gt;
      &lt;td&gt;-0.0203&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;male&lt;/th&gt;
      &lt;td&gt;0.34 (0.47)&lt;/td&gt;
      &lt;td&gt;0.66 (0.48)&lt;/td&gt;
      &lt;td&gt;0.6732&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;There seems to be &lt;strong&gt;some difference&lt;/strong&gt; between treatment (&lt;code&gt;dark_mode&lt;/code&gt;) and control group. In particular, users that select the &lt;code&gt;dark_mode&lt;/code&gt; are older, have spent less hours on the blog and they are more likely to be males.&lt;/p&gt;
&lt;p&gt;Another way to visually observe all the differences at once is with a &lt;strong&gt;paired violinplot&lt;/strong&gt;. The advantage of the paired violinplot is that it allows us to observe the full distribution of the variable (approximated via &lt;a href=&#34;https://en.wikipedia.org/wiki/Kernel_density_estimation&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;kernel density estimation&lt;/a&gt;).&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def plot_distributions(df, X, d):
    df_long = df.copy()[X + [d]]
    df_long[X] =(df_long[X] - df_long[X].mean()) / df_long[X].std()
    df_long = pd.melt(df_long, id_vars=d, value_name=&#39;value&#39;)
    sns.violinplot(y=&amp;quot;variable&amp;quot;, x=&amp;quot;value&amp;quot;, hue=d, data=df_long, split=True).\
        set(xlabel=&amp;quot;&amp;quot;, ylabel=&amp;quot;&amp;quot;, title=&amp;quot;Normalized Variable Distribution&amp;quot;);
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plot_distributions(df, X, &amp;quot;dark_mode&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/ipw_16_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;The insight of the violinplot is very similar: it seems that users that select the &lt;code&gt;dark_mode&lt;/code&gt; are different from users that don&amp;rsquo;t.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Why do we care?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;If we do not control for the observable characteristics, we are unable to estimate the true treatment effect. In short, we cannot be certain that the difference in outcome, &lt;code&gt;read_time&lt;/code&gt;, can be attributed to the treatment, &lt;code&gt;dark_mode&lt;/code&gt;, instead of other characteristics. For example, it could be that males read less and also prefer the &lt;code&gt;dark_mode&lt;/code&gt;, therefore we observe a negative correlation even though &lt;code&gt;dark_mode&lt;/code&gt; has no effect on &lt;code&gt;read_time&lt;/code&gt; (or even positive).&lt;/p&gt;
&lt;p&gt;In terms of Dyrected Acyclic Graphs, this means that we have several &lt;a href=&#34;https://towardsdatascience.com/b63dc69e3d8c&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;backdoor paths&lt;/strong&gt;&lt;/a&gt; that we need to &lt;strong&gt;block&lt;/strong&gt; in order for our analysis to be &lt;strong&gt;causal&lt;/strong&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-mermaid&#34;&gt;flowchart TB
classDef included fill:#DCDCDC,stroke:#000000,stroke-width:2px;
classDef excluded fill:#ffffff,stroke:#000000,stroke-width:2px;
classDef unobserved fill:#ffffff,stroke:#000000,stroke-width:2px,stroke-dasharray: 5 5;

X1((gender))
X2((age))
X3((hours))
D((dark mode))
Y((read time))

D --&amp;gt; Y
X1 --&amp;gt; Y
X1 --&amp;gt; D
X2 --&amp;gt; D
X3 --&amp;gt; Y

linkStyle 0 stroke:#00ff00,stroke-width:4px;
linkStyle 1,2 stroke:#ff0000,stroke-width:4px;
class D,Y included;
class X1,X2,X3 excluded;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;How do we &lt;strong&gt;block backdoor paths&lt;/strong&gt;? By conditioning the analysis on those intermediate variables. The conditional analysis allows us to recover the average treatment effect of the &lt;code&gt;dark_mode&lt;/code&gt; on &lt;code&gt;read_time&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-mermaid&#34;&gt;flowchart TB
classDef included fill:#DCDCDC,stroke:#000000,stroke-width:2px;
classDef excluded fill:#ffffff,stroke:#000000,stroke-width:2px;
classDef unobserved fill:#ffffff,stroke:#000000,stroke-width:2px,stroke-dasharray: 5 5;

X1((gender))
X2((age))
X3((hours))
D((dark mode))
Y((read time))

D --&amp;gt; Y
X1 -.-&amp;gt; Y
X1 -.-&amp;gt; D
X2 --&amp;gt; D
X3 --&amp;gt; Y

linkStyle 0 stroke:#00ff00,stroke-width:4px;
class D,Y,X1,X2,X3 included;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;How do we &lt;strong&gt;condition the analysis&lt;/strong&gt; on &lt;code&gt;gender&lt;/code&gt;, &lt;code&gt;age&lt;/code&gt; and &lt;code&gt;hours&lt;/code&gt;? We have some options:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Matching&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Propensity score&lt;/strong&gt; weighting&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Regression&lt;/strong&gt; with control variables&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Let&amp;rsquo;s explore and compare them!&lt;/p&gt;
&lt;h2 id=&#34;conditional-analysis&#34;&gt;Conditional Analysis&lt;/h2&gt;
&lt;p&gt;We assume that for a set of subjects $i = 1, &amp;hellip;, n$ we observed a tuple $(D_i, Y_i, X_i)$ comprised of&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;a treatment assignment $D_i \in \lbrace 0, 1 \rbrace$ (&lt;code&gt;dark_mode&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;a response $Y_i \in \mathbb R$ (&lt;code&gt;read_time&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;a feature vector $X_i \in \mathbb R^n$ (&lt;code&gt;gender&lt;/code&gt;, &lt;code&gt;age&lt;/code&gt; and &lt;code&gt;hours&lt;/code&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Assumption 1 : unconfoundedness&lt;/strong&gt; (or ignorability, or selection on observables)&lt;/p&gt;
&lt;p&gt;$$
\big \lbrace Y_i^{(1)} , Y_i^{(0)} \big \rbrace \ \perp \ D_i \ | \ X_i
$$&lt;/p&gt;
&lt;p&gt;i.e. conditional on observable characteristics $X$, the treatment assignment $D$ is as good as random. What we are effectively assuming is that there is no other characteristics that we do not observe that could impact both whether a user selects the &lt;code&gt;dark_mode&lt;/code&gt; and their &lt;code&gt;read_time&lt;/code&gt;. This is a &lt;strong&gt;strong assumption&lt;/strong&gt; that is more likely to be satisfied the more individual characteristics we observe.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Assumption 2: overlap&lt;/strong&gt; (or common support)&lt;/p&gt;
&lt;p&gt;$$
\exists \eta &amp;gt; 0 \ : \ \eta \leq \mathbb E \left[ T_i = 1 \ \big | \ X_i = x \right] \leq 1-\eta
$$&lt;/p&gt;
&lt;p&gt;i.e. no observation is deterministically assigned to the treatment or control group. This is a more technical assumption that basically means that for any level of &lt;code&gt;gender&lt;/code&gt;, &lt;code&gt;age&lt;/code&gt; or &lt;code&gt;hours&lt;/code&gt;, there could exist an individual that select the &lt;code&gt;dark_mode&lt;/code&gt; and one that doesn&amp;rsquo;t. Differently from the unconfoundedness assumption, the overal assumption is &lt;strong&gt;testable&lt;/strong&gt;.&lt;/p&gt;
&lt;h3 id=&#34;matching&#34;&gt;Matching&lt;/h3&gt;
&lt;p&gt;The first and most intuitive method to perform conditional analysis is &lt;strong&gt;matching&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;idea&lt;/strong&gt; of matching is very simple. Since we are not sure whether, for example, male and female users are directly comparable, we do the analysis within gender. Instead of comparing &lt;code&gt;read_time&lt;/code&gt; across &lt;code&gt;dark_mode&lt;/code&gt; in the whole sample, we do it separately for male and female users.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_gender = pd.pivot_table(df, values=&#39;read_time&#39;, index=&#39;male&#39;, columns=&#39;dark_mode&#39;, aggfunc=np.mean)
df_gender[&#39;diff&#39;] = df_gender[1] - df_gender[0] 
df_gender
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;dark_mode&lt;/th&gt;
      &lt;th&gt;False&lt;/th&gt;
      &lt;th&gt;True&lt;/th&gt;
      &lt;th&gt;diff&lt;/th&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;male&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;20.318000&lt;/td&gt;
      &lt;td&gt;22.24902&lt;/td&gt;
      &lt;td&gt;1.931020&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;16.933333&lt;/td&gt;
      &lt;td&gt;16.89898&lt;/td&gt;
      &lt;td&gt;-0.034354&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;Now the effect of &lt;code&gt;dark_mode&lt;/code&gt; seems reversed: it is negative for male users (-0.79) but bigger and positive for female users (+1.38), suggesting a positive aggregate effect, 1.38 - 0.79 = 0.59 (assuming equal proportion of genders)! This sign reversal is a very classical example of the &lt;a href=&#34;https://en.wikipedia.org/wiki/Simpson%27s_paradox&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Simpson&amp;rsquo;s Paradox&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;This comparison was easy to perform for &lt;code&gt;gender&lt;/code&gt;, since it is a binary variable. With multiple variables, potentially continuous, matching becomes much more difficult. One common strategy is to &lt;strong&gt;match users&lt;/strong&gt; in the treatment group with the most similar user in the control group, using some sort of &lt;a href=&#34;https://en.wikipedia.org/wiki/Nearest_neighbour_algorithm&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;nearest neighbor algorithm&lt;/a&gt;. I won&amp;rsquo;t go into the algorithm details here, but we can perform the matching with the &lt;code&gt;NearestNeighborMatch&lt;/code&gt; function from the &lt;code&gt;causalml&lt;/code&gt; package.&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;NearestNeighborMatch&lt;/code&gt; function generates a new dataset where users in the treatment group have been matched 1:1 (option &lt;code&gt;ratio=1&lt;/code&gt;) to users in the control group.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from causalml.match import NearestNeighborMatch

psm = NearestNeighborMatch(replace=True, ratio=1, random_state=1)
df_matched = psm.match(data=df, treatment_col=&amp;quot;dark_mode&amp;quot;, score_cols=X)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Are the two groups more comparable now? We can produce a new version of the &lt;strong&gt;balance table&lt;/strong&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;table1_matched = create_table_one(df_matched, &amp;quot;dark_mode&amp;quot;, X)
table1_matched
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;Control&lt;/th&gt;
      &lt;th&gt;Treatment&lt;/th&gt;
      &lt;th&gt;SMD&lt;/th&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;Variable&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;n&lt;/th&gt;
      &lt;td&gt;104&lt;/td&gt;
      &lt;td&gt;104&lt;/td&gt;
      &lt;td&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;age&lt;/th&gt;
      &lt;td&gt;41.93 (10.05)&lt;/td&gt;
      &lt;td&gt;41.85 (10.02)&lt;/td&gt;
      &lt;td&gt;-0.0086&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;hours&lt;/th&gt;
      &lt;td&gt;206.92 (309.62)&lt;/td&gt;
      &lt;td&gt;209.48 (321.79)&lt;/td&gt;
      &lt;td&gt;0.0081&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;male&lt;/th&gt;
      &lt;td&gt;0.62 (0.49)&lt;/td&gt;
      &lt;td&gt;0.62 (0.49)&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;Now the average differences between the two groups have &lt;strong&gt;shrunk&lt;/strong&gt; by at least a couple of orders of magnitude. However, note how the sample size has slightly decreased (300 $\to$ 246) since (1) we only match treated users and (2) we are not able to find a good match for all of them.&lt;/p&gt;
&lt;p&gt;We can visually inspect distributional differences with the paired violinplot.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plot_distributions(df_matched, X, &amp;quot;dark_mode&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/ipw_28_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;A popular way to visualize pre- and post-matching covariate balance is the &lt;strong&gt;balance plot&lt;/strong&gt; that essentially displays the standardized mean differences before and after matching, for each control variable.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def plot_balance(t1, t2, X):
    df_smd = pd.DataFrame({&amp;quot;Variable&amp;quot;: X + X,
                           &amp;quot;Sample&amp;quot;: [&amp;quot;Unadjusted&amp;quot; for _ in range(len(X))] + [&amp;quot;Adjusted&amp;quot; for _ in range(len(X))],
                           &amp;quot;Standardized Mean Difference&amp;quot;: t1[&amp;quot;SMD&amp;quot;][1:].to_list() + 
                                                           t2[&amp;quot;SMD&amp;quot;][1:].to_list()})

    sns.scatterplot(x=&amp;quot;Standardized Mean Difference&amp;quot;, y=&amp;quot;Variable&amp;quot;, hue=&amp;quot;Sample&amp;quot;, data=df_smd).\
        set(title=&amp;quot;Balance Plot&amp;quot;)
    plt.axvline(x=0, color=&#39;k&#39;, ls=&#39;--&#39;, zorder=-1, alpha=0.3);
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plot_balance(table1, table1_matched, X)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/ipw_31_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;As we can see, now all differences in observable characteristics between the two groups are essentially zero. We could also compare the distributions using other metrics or test statistics, such as the &lt;a href=&#34;https://towardsdatascience.com/9b06ee4d30bf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Kolmogorov-Smirnov test statistic&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;How do we &lt;strong&gt;estimate the average treatment effect&lt;/strong&gt;? We can simply do a difference in means. An equivalent way that automatically provides standard errors is to run a linear regression of the outcome, &lt;code&gt;read_time&lt;/code&gt;, on the treatment, &lt;code&gt;dark_mode&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt; that, since we have performed the matching for each treated user, the treatment effect we are estimating is the &lt;strong&gt;average treatment effect on the treated (ATT)&lt;/strong&gt;, which can be different from the average treatment effect if the treated sample differs from the overall population (which is likely to be the case, since we are doing matching in the first place).&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;smf.ols(&amp;quot;read_time ~ dark_mode&amp;quot;, data=df_matched).fit().summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
          &lt;td&gt;&lt;/td&gt;             &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt;         &lt;td&gt;   17.0365&lt;/td&gt; &lt;td&gt;    0.469&lt;/td&gt; &lt;td&gt;   36.363&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;   16.113&lt;/td&gt; &lt;td&gt;   17.960&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;dark_mode[T.True]&lt;/th&gt; &lt;td&gt;    1.4490&lt;/td&gt; &lt;td&gt;    0.663&lt;/td&gt; &lt;td&gt;    2.187&lt;/td&gt; &lt;td&gt; 0.030&lt;/td&gt; &lt;td&gt;    0.143&lt;/td&gt; &lt;td&gt;    2.755&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;The effect is now positive, but not statistically significant.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt; that we might have matched multiple treated users with the same untreated user, violating the independence assumption across observations and, in turn, distorting inference.&lt;/p&gt;
&lt;p&gt;We have two solutions:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;cluster standard errors at the matched individual level&lt;/li&gt;
&lt;li&gt;compute standard errors via bootstrap&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;We implement the first and cluster the standard errors by the original individual identifiers (the dataframe index).&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;smf.ols(&amp;quot;read_time ~ dark_mode&amp;quot;, data=df_matched)\
    .fit(cov_type=&#39;cluster&#39;, cov_kwds={&#39;groups&#39;: df_matched.index})\
    .summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
          &lt;td&gt;&lt;/td&gt;             &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;z&lt;/th&gt;      &lt;th&gt;P&gt;|z|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt;         &lt;td&gt;   17.0365&lt;/td&gt; &lt;td&gt;    0.650&lt;/td&gt; &lt;td&gt;   26.217&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;   15.763&lt;/td&gt; &lt;td&gt;   18.310&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;dark_mode[T.True]&lt;/th&gt; &lt;td&gt;    1.4490&lt;/td&gt; &lt;td&gt;    0.821&lt;/td&gt; &lt;td&gt;    1.765&lt;/td&gt; &lt;td&gt; 0.078&lt;/td&gt; &lt;td&gt;   -0.160&lt;/td&gt; &lt;td&gt;    3.058&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;The effect is even less statistically significant.&lt;/p&gt;
&lt;h3 id=&#34;propensity-score&#34;&gt;Propensity Score&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://academic.oup.com/biomet/article/70/1/41/240879&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Rosenbaum and Rubin (1983)&lt;/a&gt; proved a very powerful result: if the &lt;strong&gt;strong ignorability assumption&lt;/strong&gt; holds, it is sufficient to condition the analysis on the probability ot treatment, the &lt;strong&gt;propensity score&lt;/strong&gt;, in order to have conditional independence.&lt;/p&gt;
&lt;p&gt;$$
\big \lbrace Y_i^{(1)} , Y_i^{(0)} \big \rbrace \ \perp \ D_i \ | \ X_i \quad \leftrightarrow \quad \big \lbrace Y_i^{(1)} , Y_i^{(0)} \big \rbrace \ \perp \ D_i \ | \ e(X_i)
$$&lt;/p&gt;
&lt;p&gt;Where $e(X_i)$ is the probability of treatment of individual $i$, given the observable characteristics $X_i$.&lt;/p&gt;
&lt;p&gt;$$
e(x) = \Pr \left( D_i = 1 \ \big | \ X_i = x \right)
$$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt; that in an AB test the propensity score is constant across individuals.&lt;/p&gt;
&lt;p&gt;The result from Rosenbaum and Rubin (1983) is incredibly &lt;strong&gt;powerful and practical&lt;/strong&gt;, since the propensity score is a &lt;strong&gt;one dimensional&lt;/strong&gt; variable, while $X$ might be very high dimensional.&lt;/p&gt;
&lt;p&gt;Under the &lt;strong&gt;unconfoundedness&lt;/strong&gt; assumption introduced above, we can rewrite the average treatment effect as&lt;/p&gt;
&lt;p&gt;$$
\tau(x) = \mathbb E \left[ Y^{(1)} - Y^{(0)} \ \big| \ X = x \right] = \mathbb E \left[ \frac{D_i Y_i}{e(X_i)} - \frac{(1-D_i) Y_i}{1-e(X_i)} \right]
$$&lt;/p&gt;
&lt;p&gt;Note that this formulation of the average treatment effect does not depend on the potential outcomes $Y_i^{(1)}$ and $Y_i^{(0)}$, but only on the observed outcomes $Y_i$.&lt;/p&gt;
&lt;p&gt;This formulation of the average treatment effect implies the &lt;strong&gt;Inverse Propensity Weighted (IPW)&lt;/strong&gt; estimator which is an unbiased estimator for the average treatment effect $\tau$.&lt;/p&gt;
&lt;p&gt;$$
\hat \tau^{IPW} = \frac{1}{n} \sum _ {i=1}^{n} \left( \frac{D_i Y_i}{e(X_i)} - \frac{(1-D_i) Y_i}{1-e(X_i)} \right)
$$&lt;/p&gt;
&lt;p&gt;This estimator is &lt;strong&gt;unfeasible&lt;/strong&gt; since we do not observe the propensity scores $e(X_i)$. However, we can estimate them. Actually, &lt;a href=&#34;https://onlinelibrary.wiley.com/doi/abs/10.1111/1468-0262.00442&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Imbens, Hirano, Ridder (2003)&lt;/a&gt; show that you &lt;strong&gt;should&lt;/strong&gt; use the estimated propensity scores even if you knew the true values (for example because you know the sampling procedure). The idea is that if the estimated propensity scores are different from the true ones, this can be informative in the estimation.&lt;/p&gt;
&lt;p&gt;There are several possible ways to estimate a probability, the simplest and most common one being &lt;a href=&#34;https://en.wikipedia.org/wiki/Logistic_regression&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;logistic regression&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from sklearn.linear_model import LogisticRegressionCV

df[&amp;quot;pscore&amp;quot;] = LogisticRegressionCV().fit(y=df[&amp;quot;dark_mode&amp;quot;], X=df[X]).predict_proba(df[X])[:,1]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It is best practice, whenever we fit a prediction model, to &lt;strong&gt;fit the model on a different sample&lt;/strong&gt; with respect to the one that we use for inference. This practice is usually called &lt;strong&gt;cross-validation&lt;/strong&gt; or cross-fitting. One of the best (but computationally expensive) cross-validation procedures is &lt;strong&gt;leave-one-out (LOO)&lt;/strong&gt; cross-fitting: when predicting the value of observation $i$ we use all observations except for $i$. We implement the LOO cross-fitting procedure using the &lt;code&gt;cross_val_predict&lt;/code&gt; and &lt;code&gt;LeaveOneOut&lt;/code&gt; functions from the &lt;a href=&#34;https://scikit-learn.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;sklearn&lt;/code&gt;&lt;/a&gt; package.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from sklearn.model_selection import cross_val_predict, LeaveOneOut

df[&#39;pscore&#39;] = cross_val_predict(estimator=LogisticRegressionCV(), 
                                 X=df[X], 
                                 y=df[&amp;quot;dark_mode&amp;quot;],
                                 cv=LeaveOneOut(),
                                 method=&#39;predict_proba&#39;,
                                 n_jobs=-1)[:,1]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;An &lt;strong&gt;important check&lt;/strong&gt; to perform after estimating propensity scores is plotting them, across the treatment and control groups. First of all, we can then observe whether the two groups are balanced or not, depending on how close the two distributions are. Moreover, we can also check how likely it is that the &lt;strong&gt;overlap assumption&lt;/strong&gt; is satisfied. Ideally both distributions should span the same interval.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sns.histplot(data=df, x=&#39;pscore&#39;, hue=&#39;dark_mode&#39;, bins=30, stat=&#39;density&#39;, common_norm=False).\
    set(ylabel=&amp;quot;&amp;quot;, title=&amp;quot;Distribution of Propensity Scores&amp;quot;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/ipw_43_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;As expected, the distribution of propensity scores between the treatment and control group is &lt;strong&gt;significantly different&lt;/strong&gt;, suggesting that the two groups are hardly comparable. However, there is significant overlap in the support of the distributions, suggesting that the overlap assumption is likely to be satisfied.&lt;/p&gt;
&lt;p&gt;How do we estimate the average treatment effect?&lt;/p&gt;
&lt;p&gt;Once we have computed the propensity scores, we just need to re-weight observations by their respective propensity score. We can then either compute a difference between the weighted &lt;code&gt;read_time&lt;/code&gt; averages, or run a weighted regression of &lt;code&gt;read_time&lt;/code&gt; on &lt;code&gt;dark_mode&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;w = 1 / (df[&amp;quot;pscore&amp;quot;] * df[&amp;quot;dark_mode&amp;quot;] + (1-df[&amp;quot;pscore&amp;quot;]) * (1-df[&amp;quot;dark_mode&amp;quot;]))
smf.wls(&amp;quot;read_time ~ dark_mode&amp;quot;, weights=w, data=df).fit().summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
          &lt;td&gt;&lt;/td&gt;             &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt;         &lt;td&gt;   18.5859&lt;/td&gt; &lt;td&gt;    0.412&lt;/td&gt; &lt;td&gt;   45.110&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;   17.775&lt;/td&gt; &lt;td&gt;   19.397&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;dark_mode[T.True]&lt;/th&gt; &lt;td&gt;    1.1303&lt;/td&gt; &lt;td&gt;    0.582&lt;/td&gt; &lt;td&gt;    1.942&lt;/td&gt; &lt;td&gt; 0.053&lt;/td&gt; &lt;td&gt;   -0.015&lt;/td&gt; &lt;td&gt;    2.276&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;The effect of the &lt;code&gt;dark_mode&lt;/code&gt; is now positive and almost statistically significant, at the 5% level.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt; that the &lt;code&gt;wls&lt;/code&gt; function automatically normalizes weights so that they sum to 1, which greatly improves the stability of the estimator. In fact, the unnormalized IPW estimator can be very &lt;strong&gt;unstable&lt;/strong&gt; when the propensity scores approach zero or one.&lt;/p&gt;
&lt;p&gt;Also &lt;strong&gt;note&lt;/strong&gt; that the standard errors are not correct, since they do not take into account the extra uncertainty introduced in the estimation of the propensity score. This issue was noted by &lt;a href=&#34;https://onlinelibrary.wiley.com/doi/abs/10.3982/ECTA11293&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Abadie and Imbens (2016)&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;regression-with-control-variables&#34;&gt;Regression with Control Variables&lt;/h3&gt;
&lt;p&gt;The last method we are going to review today is &lt;strong&gt;linear regression with control variables&lt;/strong&gt;. This estimator is extremely easy to implement, since we just need to add the user characteristics - &lt;code&gt;gender&lt;/code&gt;, &lt;code&gt;age&lt;/code&gt; and &lt;code&gt;hours&lt;/code&gt; - to the regression of &lt;code&gt;read_time&lt;/code&gt; on &lt;code&gt;dark_mode&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;smf.ols(&amp;quot;read_time ~ dark_mode + male + age + hours&amp;quot;, data=df).fit().summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
          &lt;td&gt;&lt;/td&gt;             &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt;         &lt;td&gt;   16.8591&lt;/td&gt; &lt;td&gt;    1.082&lt;/td&gt; &lt;td&gt;   15.577&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;   14.729&lt;/td&gt; &lt;td&gt;   18.989&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;dark_mode[T.True]&lt;/th&gt; &lt;td&gt;    1.3858&lt;/td&gt; &lt;td&gt;    0.524&lt;/td&gt; &lt;td&gt;    2.646&lt;/td&gt; &lt;td&gt; 0.009&lt;/td&gt; &lt;td&gt;    0.355&lt;/td&gt; &lt;td&gt;    2.417&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;male&lt;/th&gt;              &lt;td&gt;   -4.4855&lt;/td&gt; &lt;td&gt;    0.499&lt;/td&gt; &lt;td&gt;   -8.990&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;   -5.468&lt;/td&gt; &lt;td&gt;   -3.504&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;age&lt;/th&gt;               &lt;td&gt;    0.0513&lt;/td&gt; &lt;td&gt;    0.022&lt;/td&gt; &lt;td&gt;    2.311&lt;/td&gt; &lt;td&gt; 0.022&lt;/td&gt; &lt;td&gt;    0.008&lt;/td&gt; &lt;td&gt;    0.095&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;hours&lt;/th&gt;             &lt;td&gt;    0.0043&lt;/td&gt; &lt;td&gt;    0.001&lt;/td&gt; &lt;td&gt;    8.427&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    0.003&lt;/td&gt; &lt;td&gt;    0.005&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;The average treatment effect is again positive and statistically significant at the 1% level!&lt;/p&gt;
&lt;h2 id=&#34;comparison&#34;&gt;Comparison&lt;/h2&gt;
&lt;p&gt;How do the different methods compare to each other?&lt;/p&gt;
&lt;h3 id=&#34;ipw-and-regression&#34;&gt;IPW and Regression&lt;/h3&gt;
&lt;p&gt;There is a &lt;strong&gt;tight connection&lt;/strong&gt; between the IPW estimator and linear regression with covariates. This is particularly evident when we have a one-dimensional, discrete covariate $X$.&lt;/p&gt;
&lt;p&gt;In this case, the estimand of IPW (i.e. the quantity that IPW estimates) is given by&lt;/p&gt;
&lt;p&gt;$$
\tau^{IPW} = \frac{ \sum_x \color{red}{\tau_x} \color{blue}{\Pr(D_i | X_i = x)} \Pr(X_i = x)}{\sum_x \color{blue}{\Pr(D_i | X_i = x)} \Pr(X_i = x)}
$$&lt;/p&gt;
&lt;p&gt;The IPW estimand is a weighted average of the treatment effects $\tau_x$, where the weights are given by the &lt;strong&gt;treatment probabilities&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;On the other hand, the estimand of linear regression with control variables is&lt;/p&gt;
&lt;p&gt;$$
\tau^{OLS} = \frac{ \sum_x \color{red}{\tau_x} \color{blue}{\Pr(D_i | X_i = x)(1 - \Pr(D_i | X_i = x)) } \Pr(X_i = x)}{\sum_x \color{blue}{\Pr(D_i | X_i = x)(1 - \Pr(D_i | X_i = x)) } \Pr(X_i = x)}
$$&lt;/p&gt;
&lt;p&gt;The OLS estimand is a weighted average of the treatment effects $\tau_x$, where the weights are given by the &lt;strong&gt;variances of the treatment probabilities&lt;/strong&gt;. This means that linear regression is a weighted estimator, that gives more weight to observations that have characteristics for which we observe more treatment variability. Since a binary random variable has the highest variance when its expected value is 0.5, &lt;strong&gt;OLS gives the most weight to observations that have characteristics for which we observe a 50/50 split between treatment and control group&lt;/strong&gt;. On the other hand, if for some characteristics we only observe treated or untreated individuals, those observations are going to receive zero weight. I recommend Chapter 3 of &lt;a href=&#34;https://www.mostlyharmlesseconometrics.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Angrist and Pischke (2009)&lt;/a&gt; for more details.&lt;/p&gt;
&lt;h3 id=&#34;ipw-and-matching&#34;&gt;IPW and Matching&lt;/h3&gt;
&lt;p&gt;As we have seen in the IPW section, &lt;a href=&#34;https://academic.oup.com/biomet/article/70/1/41/240879&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Rosenbaum and Rubin (1983)&lt;/a&gt; result tells us that we do not need to perform the analysis conditional on all the covariates $X$, but it is sufficient to condition on the propensity score $e(X)$.&lt;/p&gt;
&lt;p&gt;We have seed how this result implies a weighted estimator but it also extends to matching: we do not need to match observations on all the covariates $X$, but it is sufficient to &lt;strong&gt;match them on the propensity score&lt;/strong&gt; $e(X)$. This method is called propensity score matching.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;psm = NearestNeighborMatch(replace=False, random_state=1)
df_ipwmatched = psm.match(data=df, treatment_col=&amp;quot;dark_mode&amp;quot;, score_cols=[&#39;pscore&#39;])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As before, after matching, we can simply compute the estimate as a difference in means, remembering that observations are &lt;strong&gt;not independent&lt;/strong&gt; and therefore we need to be cautious when doing inference.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;smf.ols(&amp;quot;read_time ~ dark_mode&amp;quot;, data=df_ipwmatched)\
    .fit(cov_type=&#39;cluster&#39;, cov_kwds={&#39;groups&#39;: df_ipwmatched.index})\
    .summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
          &lt;td&gt;&lt;/td&gt;             &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;z&lt;/th&gt;      &lt;th&gt;P&gt;|z|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt;         &lt;td&gt;   18.4633&lt;/td&gt; &lt;td&gt;    0.505&lt;/td&gt; &lt;td&gt;   36.576&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;   17.474&lt;/td&gt; &lt;td&gt;   19.453&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;dark_mode[T.True]&lt;/th&gt; &lt;td&gt;    1.1888&lt;/td&gt; &lt;td&gt;    0.703&lt;/td&gt; &lt;td&gt;    1.692&lt;/td&gt; &lt;td&gt; 0.091&lt;/td&gt; &lt;td&gt;   -0.188&lt;/td&gt; &lt;td&gt;    2.566&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;The estimated effect of &lt;code&gt;dark_mode&lt;/code&gt; is positive, significant at the 1% level and very close to the true value of 2!&lt;/p&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;In this blog post, we have seen how to perform &lt;strong&gt;conditional analysis&lt;/strong&gt; using different approached. Matching directly matches most similar units in the treatment and control group. Weighting simply assigns different weight to different observations depending on their probability of receiving the treatment. Regression instead weights observations depending on the conditional treatment variances, giving more weight to observations that have characteristics common to both the treatment and control group.&lt;/p&gt;
&lt;p&gt;These procedures are &lt;strong&gt;extremely helpful&lt;/strong&gt; because they can either allow us to estimate causal effects from (very rich) observational data or correct experimental estimates when randomization was not perfect or we have a small sample.&lt;/p&gt;
&lt;p&gt;Last but not least, if you want to know more, I strongly recommend this &lt;strong&gt;video lecture&lt;/strong&gt; on propensity scores from &lt;a href=&#34;https://paulgp.github.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paul Goldsmith-Pinkham&lt;/a&gt; that is freely available online.&lt;/p&gt;
&lt;br&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/8gWctYvRzk4&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;br&gt;
&lt;p&gt;The whole course is a &lt;strong&gt;gem&lt;/strong&gt; and it is an incredible privilege to have such high quality material available online for free!&lt;/p&gt;
&lt;h3 id=&#34;references&#34;&gt;References&lt;/h3&gt;
&lt;p&gt;[1] P. Rosenbaum, D. Rubin, &lt;a href=&#34;https://academic.oup.com/biomet/article/70/1/41/240879&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The central role of the propensity score in observational studies for causal effects&lt;/a&gt; (1983), &lt;em&gt;Biometrika&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;[2] G. Imbens, K. Hirano, G. Ridder, &lt;a href=&#34;https://onlinelibrary.wiley.com/doi/abs/10.1111/1468-0262.00442&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Efficient Estimation of Average Treatment Effects Using the Estimated Propensity Score&lt;/a&gt; (2003), &lt;em&gt;Econometrica&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;[3] J. Angrist, J. S. Pischke, &lt;a href=&#34;https://www.mostlyharmlesseconometrics.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Mostly harmless econometrics: An Empiricist&amp;rsquo;s Companion&lt;/a&gt; (2009), &lt;em&gt;Princeton University Press&lt;/em&gt;.&lt;/p&gt;
&lt;h3 id=&#34;related-articles&#34;&gt;Related Articles&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/59f801eb3299&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Understanding The Frisch-Waugh-Lovell Theorem&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/9b06ee4d30bf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;How to Compare Two or More Distributions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/b63dc69e3d8c&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;DAGs and Control Variables&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;code&#34;&gt;Code&lt;/h3&gt;
&lt;p&gt;You can find the original Jupyter Notebook here:&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/ipw.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/ipw.ipynb&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Understanding Meta Learners</title>
      <link>https://matteocourthoud.github.io/post/meta/</link>
      <pubDate>Fri, 15 Jul 2022 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/post/meta/</guid>
      <description>&lt;p&gt;&lt;em&gt;How to use machine learning to estimate heterogeneous treatment effects&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;In many settings, we are not just interested in understanding a causal effect, but also whether this effect is &lt;strong&gt;different for different users&lt;/strong&gt;. We might be interested in understanding if a drug has side effects that are different for people of different age. Or we might be interested in understanding if an ad campaign is particularly effective in certain geographical areas.&lt;/p&gt;
&lt;p&gt;This knowledge is crucial because it allows us to &lt;strong&gt;target&lt;/strong&gt; the treatment. If a drug has severe side effects for kids, we might want to restrict its distribution only to adults. Or if an ad campaign is effective only in English-speaking countries it is not worth showing it elsewhere.&lt;/p&gt;
&lt;p&gt;In this blog post we are going to explore some approaches to uncover &lt;strong&gt;treatment effect heterogeneity&lt;/strong&gt;. In particular, we are going to explore methods that try to leverage the flexibility of &lt;strong&gt;machine learning&lt;/strong&gt; algorithms.&lt;/p&gt;
&lt;h2 id=&#34;example&#34;&gt;Example&lt;/h2&gt;
&lt;p&gt;Suppose we were a company interested in understanding how much a new &lt;strong&gt;premium feature&lt;/strong&gt; increases revenue. In particular, we know that users of different &lt;strong&gt;age&lt;/strong&gt; have different spending attitudes and we suspect that the impact of the premium feature could also be different depending on the age of the user.&lt;/p&gt;
&lt;p&gt;This information might be very important, for example for &lt;strong&gt;advertisement targeting&lt;/strong&gt; or &lt;strong&gt;discount design&lt;/strong&gt;. If we discover that the premium feature increases revenue for a particular set of users, we might want to target advertisement towards that group, or offer them personalized discounts.&lt;/p&gt;
&lt;p&gt;To understand the effect of the premium feature on revenue, the run an &lt;a href=&#34;https://en.wikipedia.org/wiki/A/B_testing&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;AB test&lt;/strong&gt;&lt;/a&gt; in which we randomly give access to the premium feature to 10% of the users. The feature is &lt;strong&gt;expensive&lt;/strong&gt; and we cannot afford to give it for free to more users. Hopefully a 10% treatment probability is enough.&lt;/p&gt;
&lt;p&gt;We generate the simulated data using the data generating process &lt;code&gt;dgp_premium()&lt;/code&gt; from &lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/src/dgp.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;src.dgp&lt;/code&gt;&lt;/a&gt;. I also import some plotting functions and libraries from &lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/src/utils.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;src.utils&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;%matplotlib inline
%config InlineBackend.figure_format = &#39;retina&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from src.utils import *
from src.dgp import dgp_premium
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;dgp = dgp_premium()
df = dgp.generate_data(seed=5)
df.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;revenue&lt;/th&gt;
      &lt;th&gt;premium&lt;/th&gt;
      &lt;th&gt;age&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;10.62&lt;/td&gt;
      &lt;td&gt;True&lt;/td&gt;
      &lt;td&gt;27.32&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;10.35&lt;/td&gt;
      &lt;td&gt;True&lt;/td&gt;
      &lt;td&gt;54.57&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;10.13&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;26.68&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;9.97&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;56.58&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;10.16&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;38.51&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;We have data on 300 users, for whom we observe the &lt;code&gt;revenue&lt;/code&gt; they generate and whether they were given the &lt;code&gt;premium&lt;/code&gt; feature. Moreover, we also record the &lt;code&gt;age&lt;/code&gt; of the users.&lt;/p&gt;
&lt;p&gt;To understand whether randomization worked, we use the &lt;code&gt;create_table_one&lt;/code&gt; function from Uber&amp;rsquo;s &lt;a href=&#34;https://causalml.readthedocs.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;causalml&lt;/code&gt;&lt;/a&gt; package to produce a &lt;strong&gt;covariate balance table&lt;/strong&gt;, containing the average value of our observable characteristics, across treatment and control groups. As the name suggests, this should always be the first table you present in causal inference analysis.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from causalml.match import create_table_one

create_table_one(df, &#39;premium&#39;, [&#39;age&#39;, &#39;revenue&#39;])
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;Control&lt;/th&gt;
      &lt;th&gt;Treatment&lt;/th&gt;
      &lt;th&gt;SMD&lt;/th&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;Variable&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;n&lt;/th&gt;
      &lt;td&gt;269&lt;/td&gt;
      &lt;td&gt;31&lt;/td&gt;
      &lt;td&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;age&lt;/th&gt;
      &lt;td&gt;39.01 (12.11)&lt;/td&gt;
      &lt;td&gt;38.43 (13.26)&lt;/td&gt;
      &lt;td&gt;-0.0454&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;revenue&lt;/th&gt;
      &lt;td&gt;10.04 (0.16)&lt;/td&gt;
      &lt;td&gt;10.56 (0.23)&lt;/td&gt;
      &lt;td&gt;2.5905&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;Most users are in the control group and only 31 users have received the premium feature. Average &lt;code&gt;age&lt;/code&gt; is comparable across groups (SMD&amp;lt;1), while it seems that the premium feature increases &lt;code&gt;revenue&lt;/code&gt; by 2.6$ per user, on average.&lt;/p&gt;
&lt;p&gt;Does the effect of the &lt;code&gt;premium&lt;/code&gt; feature differ by &lt;code&gt;age&lt;/code&gt;?&lt;/p&gt;
&lt;p&gt;One simple approach could me to regress &lt;code&gt;revenue&lt;/code&gt; on a full interaction of &lt;code&gt;premium&lt;/code&gt; and age.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;linear_model = smf.ols(&#39;revenue ~ premium * age&#39;, data=df).fit()
linear_model.summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
           &lt;td&gt;&lt;/td&gt;              &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt;           &lt;td&gt;   10.0244&lt;/td&gt; &lt;td&gt;    0.034&lt;/td&gt; &lt;td&gt;  292.716&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    9.957&lt;/td&gt; &lt;td&gt;   10.092&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;premium[T.True]&lt;/th&gt;     &lt;td&gt;    0.5948&lt;/td&gt; &lt;td&gt;    0.099&lt;/td&gt; &lt;td&gt;    6.007&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    0.400&lt;/td&gt; &lt;td&gt;    0.790&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;age&lt;/th&gt;                 &lt;td&gt;    0.0005&lt;/td&gt; &lt;td&gt;    0.001&lt;/td&gt; &lt;td&gt;    0.570&lt;/td&gt; &lt;td&gt; 0.569&lt;/td&gt; &lt;td&gt;   -0.001&lt;/td&gt; &lt;td&gt;    0.002&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;premium[T.True]:age&lt;/th&gt; &lt;td&gt;   -0.0021&lt;/td&gt; &lt;td&gt;    0.002&lt;/td&gt; &lt;td&gt;   -0.863&lt;/td&gt; &lt;td&gt; 0.389&lt;/td&gt; &lt;td&gt;   -0.007&lt;/td&gt; &lt;td&gt;    0.003&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;The interaction coefficient is close to zero and not significant. It seems that there is not a different effect of &lt;code&gt;premium&lt;/code&gt; by &lt;code&gt;age&lt;/code&gt;. But is it true? The interaction coefficient only captures linear relationships. What if the relationship is &lt;strong&gt;non-linear&lt;/strong&gt;?&lt;/p&gt;
&lt;p&gt;We can check it by directly &lt;strong&gt;plotting the raw data&lt;/strong&gt;. We plot revenue by age, splitting the data between &lt;code&gt;premium&lt;/code&gt; users and non-premium users.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sns.scatterplot(data=df, x=&#39;age&#39;, y=&#39;revenue&#39;, hue=&#39;premium&#39;, s=40);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/meta_13_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;From the raw data, it looks like revenue is generally higher for people between 30 and 40 and &lt;code&gt;premium&lt;/code&gt; has a particularly strong effect for people between 35 and 45/50.&lt;/p&gt;
&lt;p&gt;We can visualize the estimated revenue by age with and without treatment.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def plot_TE(df, true_te=False):
    sns.scatterplot(data=df, x=&#39;age&#39;, y=&#39;revenue&#39;, hue=&#39;premium&#39;, s=40, legend=True)
    sns.lineplot(df[&#39;age&#39;], df[&#39;mu0_hat&#39;], label=&#39;$\mu_0$&#39;)
    sns.lineplot(df[&#39;age&#39;], df[&#39;mu1_hat&#39;], label=&#39;$\mu_1$&#39;)
    if true_te:
        plt.fill_between(df[&#39;age&#39;], df[&#39;y0&#39;], df[&#39;y0&#39;] + df[&#39;y1&#39;], color=&#39;grey&#39;, alpha=0.2, label=&amp;quot;True TE&amp;quot;)
    plt.title(&#39;Distribution of revenue by age and premium status&#39;)
    plt.legend(title=&#39;Treated&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We first compute the predicted revenue with ($\mu_1$) and without &lt;code&gt;premium&lt;/code&gt; subscription ($\mu_0$) and we plot them together with the raw data.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df[&#39;mu0_hat&#39;] = linear_model.predict(df.assign(premium=0))
df[&#39;mu1_hat&#39;] = linear_model.predict(df.assign(premium=1))
plot_TE(df)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/meta_17_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;As we can see, the orange line is higher than the blue line, suggesting a positive effect of &lt;code&gt;premium&lt;/code&gt; on &lt;code&gt;revenue&lt;/code&gt;. However, the two lines are essentially &lt;strong&gt;parallel&lt;/strong&gt;, suggesting no heterogeneity in treatment effects.&lt;/p&gt;
&lt;p&gt;Can we be more precise? Is there a way to estimate this treatment heterogeneity in a &lt;strong&gt;flexible way&lt;/strong&gt;, without assuming functional forms?&lt;/p&gt;
&lt;p&gt;The answer is &lt;strong&gt;yes&lt;/strong&gt;! We can use machine learning methods to flexibly estimate heterogeneous treatment effects. In particular, in this blog post we are going to inspect three and popular methods that were introduced by &lt;a href=&#34;https://www.pnas.org/doi/abs/10.1073/pnas.1804597116&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Künzel, Sekhon, Bickel, Yu, (2019)&lt;/a&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;S-learner&lt;/li&gt;
&lt;li&gt;T-learner&lt;/li&gt;
&lt;li&gt;X-learner&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;setting&#34;&gt;Setting&lt;/h2&gt;
&lt;p&gt;We assume that for a set of i.i.d. subjects $i = 1, &amp;hellip;, n$ we observed a tuple $(X_i, D_i, Y_i)$ comprised of&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;a treatment assignment $T_i \in \lbrace 0, 1 \rbrace$ (&lt;code&gt;premium&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;a response $Y_i \in \mathbb R$ (&lt;code&gt;revenue&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;a feature vector $X_i \in \mathbb R^n$ (&lt;code&gt;age&lt;/code&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We are interested in &lt;strong&gt;estimating the average treatment effect&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;$$
\tau = \mathbb E \Big[ Y_i^{(1)} - Y_i^{(0)} \Big]
$$&lt;/p&gt;
&lt;p&gt;Where $Y_i^{(d)}$ indicates the potential outcome of individual $i$ under treatment status $d$. We also make the following assumptions.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Assumption 1 : unconfoundedness&lt;/strong&gt; (or ignorability, or selection on observables)&lt;/p&gt;
&lt;p&gt;$$
\big \lbrace Y_i^{(1)} , Y_i^{(0)} \big \rbrace \ \perp \ D_i \ | \ X_i
$$&lt;/p&gt;
&lt;p&gt;i.e. conditional on observable characteristics $X$, the treatment assignment $D$ is as good as random. What we are effectively assuming is that there is no other characteristics that we do not observe that could impact both whether a user selects the &lt;code&gt;dark_mode&lt;/code&gt; and their &lt;code&gt;read_time&lt;/code&gt;. This is a &lt;strong&gt;strong assumption&lt;/strong&gt; that is more likely to be satisfied the more individual characteristics we observe.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Assumption 2: stable unit treatment value (SUTVA)&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;$$
Y^{(d)} \perp D
$$&lt;/p&gt;
&lt;p&gt;i.e. the potential outcome does not depend on the treatment status. In our case, we are ruling out the fact that another user selecting the &lt;code&gt;premium&lt;/code&gt; feature might affect my effect of &lt;code&gt;premium&lt;/code&gt; on &lt;code&gt;revenue&lt;/code&gt;. The most common setting where SUTVA is violated is in presence of &lt;strong&gt;network effects&lt;/strong&gt;: if a friend of mine uses a social network increases my utility from using it.&lt;/p&gt;
&lt;h2 id=&#34;s-learner&#34;&gt;S-Learner&lt;/h2&gt;
&lt;p&gt;The simplest meta-algorithm is the &lt;strong&gt;single learner or S-learner&lt;/strong&gt;. To build the S-learner estimator, we fit a single model for all observations.&lt;/p&gt;
&lt;p&gt;$$
\mu(z) = \mathbb E \left[ Y_i \ \big | \ (X_i, D_i) = z \right]
$$&lt;/p&gt;
&lt;p&gt;the estimator is given by the difference between the predicted values evaluated with and without the treatment, $d=1$ and $d=0$.&lt;/p&gt;
&lt;p&gt;$$
\hat \tau_{S} (x) = \hat \mu(x,1) - \hat \mu(x,0)
$$&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def S_learner(dgp, model, y, D, X):
    temp = dgp.generate_data(true_te=True).sort_values(X)
    mu = model.fit(temp[X + [D]], temp[y])
    temp[&#39;mu0_hat&#39;] = mu.predict(temp[X + [D]].assign(premium=0))
    temp[&#39;mu1_hat&#39;] = mu.predict(temp[X + [D]].assign(premium=1))
    plot_TE(temp, true_te=True)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s use a &lt;a href=&#34;https://en.wikipedia.org/wiki/Decision_tree_learning&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;decision tree regression&lt;/strong&gt;&lt;/a&gt; model to build the the S-learner, using the &lt;code&gt;DecisionTreeRegressor&lt;/code&gt; function from the &lt;a href=&#34;https://scikit-learn.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;sklearn&lt;/code&gt;&lt;/a&gt; package. I won&amp;rsquo;t go into details about decision trees here, but I will just say that it&amp;rsquo;s a non-parametric estimator that uses the training data to split the state space (&lt;code&gt;premium&lt;/code&gt; and &lt;code&gt;age&lt;/code&gt; in our case) into blocks and predicts the outcome (&lt;code&gt;revenue&lt;/code&gt; in our case) as its average value within block.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from sklearn.tree import DecisionTreeRegressor

model = DecisionTreeRegressor(min_impurity_decrease=0.001)
S_learner(dgp, model, y=&amp;quot;revenue&amp;quot;, D=&amp;quot;premium&amp;quot;, X=[&amp;quot;age&amp;quot;])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/meta_25_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;The plot depicts the data together with the response functions $\hat \mu(x,1)$ and $\hat \mu(x,0)$. I have also plotted in grey the area between the true response functions: the true treatment effects.&lt;/p&gt;
&lt;p&gt;As we can see, the S-learner is flexible enough to understand that there is a difference in levels between treatment and control group (we have two separate lines). It also captures well the response function for the control group, $\hat \mu(x,0)$, but not so well the control function for the treatment group, $\hat \mu(x,1)$.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;problem&lt;/strong&gt; with the S-learner is that it is learning a &lt;strong&gt;single model&lt;/strong&gt; so we have to hope that the model uncovers heterogeneity in the treatment $D$, but it might not be the case. Moreover, if the model is heavily regularized because of the high dimensionality of $X$, it &lt;strong&gt;might not recover any treatment effect&lt;/strong&gt;. For example, with decision trees, we might not split on the treatment $D$.&lt;/p&gt;
&lt;h2 id=&#34;t-learner&#34;&gt;T-learner&lt;/h2&gt;
&lt;p&gt;To build the &lt;strong&gt;two-learner or T-learner&lt;/strong&gt; estimator, we fit two different models, one for treated units and one for control units.&lt;/p&gt;
&lt;p&gt;$$
\mu^{(1)}(x) = \mathbb E \left[ Y_i \ \big | \ X_i = x, T_i = 1 \right] \qquad ; \qquad \mu^{(0)}(x) = \mathbb E \left[ Y_i \ \big | \ X_i = x, T_i = 0 \right]
$$&lt;/p&gt;
&lt;p&gt;the estimator is given by the difference between the predicted values of the two algorithms.&lt;/p&gt;
&lt;p&gt;$$
\hat \tau_{T} (x) = \hat \mu^{(1)}(x) - \hat \mu^{(0)}(x)
$$&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def T_learner(df, model, y, D, X):
    temp = dgp.generate_data(true_te=True).sort_values(X)
    mu0 = model.fit(temp.loc[temp[D]==0, X], temp.loc[temp[D]==0, y])
    temp[&#39;mu0_hat&#39;] = mu0.predict(temp[X])
    mu1 = model.fit(temp.loc[temp[D]==1, X], temp.loc[temp[D]==1, y])
    temp[&#39;mu1_hat&#39;] = mu1.predict(temp[X])
    plot_TE(temp, true_te=True)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We use a decision tree regression model as before but, this time, we fit two separate decision trees for the treatment and control group.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;T_learner(dgp, model, y=&amp;quot;revenue&amp;quot;, D=&amp;quot;premium&amp;quot;, X=[&amp;quot;age&amp;quot;])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/meta_31_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;As we can see, the T-learner is much &lt;strong&gt;more flexible&lt;/strong&gt; than the S-learner because it fits two separate models. The response function for the control group, $\hat \mu(x,0)$, is still very accurate and the response function for the treatment group, $\hat \mu(x,1)$, is more flexible than before.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;problem&lt;/strong&gt; now is that we are &lt;strong&gt;using just a fraction of the data&lt;/strong&gt; for each prediction problem, while the S-learner was using all the data. By fitting two separate models we are losing some information. Moreover, by using two different models we might get &lt;strong&gt;heterogeneity where there is none&lt;/strong&gt;. For example, with decision trees, we will probably get different splits with different samples even if the data generating process is the same.&lt;/p&gt;
&lt;h3 id=&#34;x-learner&#34;&gt;X-learner&lt;/h3&gt;
&lt;p&gt;The &lt;strong&gt;cross-learner or X-learner&lt;/strong&gt; estimator is an extension of the T-learner estimator. It is built in the following way:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;As for the T-learner, compute separate models for $\mu^{(1)}(x)$ and $\mu^{(0)}(x)$ using the treated and control units, respectively&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Compute the estimated treatment effects as&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;$$
\Delta_i (x) =
\begin{cases}
Y_i - \hat \mu^{(0)}(x) &amp;amp;\quad \text{ if } D_i = 1
\newline
\hat \mu^{(1)}(x) - Y_i &amp;amp;\quad \text{ if } D_i = 0
\end{cases}
$$&lt;/p&gt;
&lt;ol start=&#34;3&#34;&gt;
&lt;li&gt;
&lt;p&gt;Predicting $\Delta$ from $X$, compute $\hat \tau^{(0)}(x)$ from treated units and  $\hat \tau^{(1)}(x)$ from control units&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Estimate the &lt;a href=&#34;https://en.wikipedia.org/wiki/Propensity_score_matching&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;propensity score&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;$$
e(x) = \mathbb E \left[ D_i = 1 \ \big | \ X_i = x \right]
$$&lt;/p&gt;
&lt;ol start=&#34;5&#34;&gt;
&lt;li&gt;Compute the treatment effects&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;$$
\hat \tau_X(x) = \hat \tau^{(0)}(x) \hat e(x) + \hat \tau^{(1)}(x) (1 - \hat e(x))
$$&lt;/p&gt;
&lt;p&gt;Can we still recover &lt;strong&gt;pseudo response functions&lt;/strong&gt;? Yes!&lt;/p&gt;
&lt;p&gt;Which we can rewrite the treatment effects as&lt;/p&gt;
&lt;p&gt;$$
\begin{align}
\hat \tau_X(x) &amp;amp; = \hat \tau^{(0)}(x) \hat e(x) + \hat \tau^{(1)}(x) (1 - \hat e(x)) = \newline
&amp;amp;= \hat e(x) \left[ \hat \mu^{(1)}(x) - Y_i^{(0)} \right] + (1 - \hat e(x)) \left[ Y_i^{(1)} - \hat \mu^{(0)}(x) \right] = \newline
&amp;amp;= \left[ \hat e(x) \hat \mu^{(1)}(x) + (1 - \hat e(x)) Y_i^{(1)} \right] - \left[ \hat e(x) Y_i^{(0)} + (1 - \hat e(x))  \hat \mu^{(0)}(x) \right]
\end{align}
$$&lt;/p&gt;
&lt;p&gt;So that the pseudo response functions estimated by the X-learner are&lt;/p&gt;
&lt;p&gt;$$
\begin{align}
\tilde \mu_i^{(1)} (x) &amp;amp;= \hat e(x) \hat \mu^{(1)}(x) + (1 - \hat e(x)) Y_i^{(1)} \newline
\tilde \mu_i^{(0)} (x) &amp;amp;=  \hat e(x) Y_i^{(0)} + (1 - \hat e(x)) \hat \mu^{(0)}(x)
\end{align}
$$&lt;/p&gt;
&lt;p&gt;As we can see, the X-learner combines the true values $Y_i^{(d)}$ with the estimated ones $\mu_i^{(d)} (x)$ weighting by the &lt;a href=&#34;https://en.wikipedia.org/wiki/Propensity_score_matching&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;propensity scores&lt;/strong&gt;&lt;/a&gt; $e_i(x)$, i.e. the estimated treatment probabilities.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;What does it mean?&lt;/strong&gt; It means that if we have many more observations for one group (in our case the control group), the control response function $\hat \mu^{(0)}(x) $ will get most of the weight. Instead, for the other group (the treatment group in our case), the actual observations $Y_i^{(1)}$ will get most of the weight.&lt;/p&gt;
&lt;p&gt;To illustrate the method, I am going to build pseudo response functions by approximating $Y_i^{(d)}$ using the nearest observation, using the &lt;code&gt;KNeighborsRegressor&lt;/code&gt; function. I estimate the propensity scores via &lt;a href=&#34;https://en.wikipedia.org/wiki/Logistic_regression&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;logistic regression&lt;/a&gt; using the &lt;code&gt;LogisticRegressionCV&lt;/code&gt; function.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from sklearn.neighbors import KNeighborsRegressor
from sklearn.linear_model import LogisticRegressionCV

def X_learner(df, model, y, D, X):
    temp = dgp.generate_data(true_te=True).sort_values(X)
    
    # Mu
    mu0 = model.fit(temp.loc[temp[D]==0, X], temp.loc[temp[D]==0, y])
    temp[&#39;mu0_hat_&#39;] = mu0.predict(temp[X])
    mu1 = model.fit(temp.loc[temp[D]==1, X], temp.loc[temp[D]==1, y])
    temp[&#39;mu1_hat_&#39;] = mu1.predict(temp[X])
    
    # Y
    y0 = KNeighborsRegressor(n_neighbors=1).fit(temp.loc[temp[D]==0, X], temp.loc[temp[D]==0, y])
    temp[&#39;y0_hat&#39;] = y0.predict(temp[X])
    y1 = KNeighborsRegressor(n_neighbors=1).fit(temp.loc[temp[D]==1, X], temp.loc[temp[D]==1, y])
    temp[&#39;y1_hat&#39;] = y1.predict(temp[X])
    
    # Weight
    e = LogisticRegressionCV().fit(y=temp[D], X=temp[X]).predict_proba(temp[X])[:,1]
    temp[&#39;mu0_hat&#39;] = e * temp[&#39;y0_hat&#39;] + (1-e) * temp[&#39;mu0_hat_&#39;]
    temp[&#39;mu1_hat&#39;] = (1-e) * temp[&#39;y1_hat&#39;] + e * temp[&#39;mu1_hat_&#39;]
    
    # Plot
    plot_TE(temp, true_te=True)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;X_learner(df, model, y=&amp;quot;revenue&amp;quot;, D=&amp;quot;premium&amp;quot;, X=[&amp;quot;age&amp;quot;])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/meta_37_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;As we can clearly see from this graph, the main advantage of &lt;strong&gt;X-learners&lt;/strong&gt; is that it adapts the &lt;strong&gt;flexibility&lt;/strong&gt; of the response functions to the context. In areas of the state space where we have a lot of data, it mostly uses the estimated response function, in areas of the state space with few data, it uses the observation themselves.&lt;/p&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;In this post, we have seen different estimators introduced by &lt;a href=&#34;https://www.pnas.org/doi/abs/10.1073/pnas.1804597116&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Künzel, Sekhon, Bickel, Yu, (2019)&lt;/a&gt; that leverage flexible &lt;strong&gt;machine learning&lt;/strong&gt; algorithms to estimate &lt;strong&gt;heterogeneous treatment effects&lt;/strong&gt;. The estimators differ for their degree of sophistication: the S-learner fits a single estimator including the treatment indicator as a covariate. The T-learner fits two separate estimators for the treatment and control group. Lastly, the X-learner is an extension of the T-learner that allows for different degrees of flexibility depending on the amount of data available across treatment and control groups.&lt;/p&gt;
&lt;p&gt;Estimation of heterogeneous treatment effect is extremely important for &lt;strong&gt;treatment targeting&lt;/strong&gt;. Indeed, there is now a growing literature that exploits machine learning methods to get flexible estimates without imposing functional form assumptions. Among the many, it&amp;rsquo;s important to mention the R-learner procedure of &lt;a href=&#34;https://academic.oup.com/biomet/article/108/2/299/5911092&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Nie and Wager (2021)&lt;/a&gt; and the causal trees and forests of &lt;a href=&#34;https://www.tandfonline.com/doi/full/10.1080/01621459.2017.1319839&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Athey and Wager (2018)&lt;/a&gt;. I might write more about these procedures in the future so, stay tuned ☺️&lt;/p&gt;
&lt;h3 id=&#34;references&#34;&gt;References&lt;/h3&gt;
&lt;p&gt;[1] S. Künzel, J. Sekhon, P. Bickel, B. Yu, &lt;a href=&#34;https://www.pnas.org/doi/abs/10.1073/pnas.1804597116&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Metalearners for estimating heterogeneous treatment effects using machine learning&lt;/a&gt; (2019), &lt;em&gt;PNAS&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;[2] X. Nie, S. Wager, &lt;a href=&#34;https://academic.oup.com/biomet/article/108/2/299/5911092&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Quasi-oracle estimation of heterogeneous treatment effects&lt;/a&gt; (2021), &lt;em&gt;Biometrika&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;[3] S. Athey, S. Wager, &lt;a href=&#34;https://www.tandfonline.com/doi/full/10.1080/01621459.2017.1319839&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Estimation and Inference of Heterogeneous Treatment Effects using Random Forests&lt;/a&gt; (2018), &lt;em&gt;Journal of the American Statistical Association&lt;/em&gt;.&lt;/p&gt;
&lt;h3 id=&#34;related-articles&#34;&gt;Related Articles&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/99bf5cffa0d9&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Matching, Weighting, or Regression?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/b63dc69e3d8c&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;DAGs and Control Variables&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;code&#34;&gt;Code&lt;/h3&gt;
&lt;p&gt;You can find the original Jupyter Notebook here:&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/meta.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/meta.ipynb&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Comparing ATE Estimators</title>
      <link>https://matteocourthoud.github.io/post/compare/</link>
      <pubDate>Fri, 01 Jul 2022 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/post/compare/</guid>
      <description>&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;%matplotlib inline
%config InlineBackend.figure_format = &#39;retina&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from src.utils import *
from src.dgp import dgp_compare
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df = dgp_compare().generate_data(include_beta=True)
df.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;outcome&lt;/th&gt;
      &lt;th&gt;treated&lt;/th&gt;
      &lt;th&gt;male&lt;/th&gt;
      &lt;th&gt;age&lt;/th&gt;
      &lt;th&gt;income&lt;/th&gt;
      &lt;th&gt;beta&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;38.68&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;28.0&lt;/td&gt;
      &lt;td&gt;1514.0&lt;/td&gt;
      &lt;td&gt;2.726425&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;37.81&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;47.0&lt;/td&gt;
      &lt;td&gt;1524.0&lt;/td&gt;
      &lt;td&gt;2.777192&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;27.70&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;60.0&lt;/td&gt;
      &lt;td&gt;2683.0&lt;/td&gt;
      &lt;td&gt;0.247312&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;29.56&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;29.0&lt;/td&gt;
      &lt;td&gt;3021.0&lt;/td&gt;
      &lt;td&gt;2.632797&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;36.83&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;25.0&lt;/td&gt;
      &lt;td&gt;1859.0&lt;/td&gt;
      &lt;td&gt;1.724331&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;np.mean(df[&#39;beta&#39;])
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;2.0
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;simple-difference&#34;&gt;Simple difference&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;smf.ols(&#39;outcome ~ treated&#39;, data=df).fit().summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
         &lt;td&gt;&lt;/td&gt;            &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt;       &lt;td&gt;   30.8496&lt;/td&gt; &lt;td&gt;    0.091&lt;/td&gt; &lt;td&gt;  340.817&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;   30.672&lt;/td&gt; &lt;td&gt;   31.027&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;treated[T.True]&lt;/th&gt; &lt;td&gt;   -0.0060&lt;/td&gt; &lt;td&gt;    0.118&lt;/td&gt; &lt;td&gt;   -0.051&lt;/td&gt; &lt;td&gt; 0.959&lt;/td&gt; &lt;td&gt;   -0.238&lt;/td&gt; &lt;td&gt;    0.226&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;h3 id=&#34;regression-with-control&#34;&gt;Regression with control&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;smf.ols(&#39;outcome ~ treated + male + age + income&#39;, data=df).fit().summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
         &lt;td&gt;&lt;/td&gt;            &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt;       &lt;td&gt;   29.5761&lt;/td&gt; &lt;td&gt;    0.272&lt;/td&gt; &lt;td&gt;  108.833&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;   29.043&lt;/td&gt; &lt;td&gt;   30.109&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;treated[T.True]&lt;/th&gt; &lt;td&gt;    2.2682&lt;/td&gt; &lt;td&gt;    0.129&lt;/td&gt; &lt;td&gt;   17.623&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    2.016&lt;/td&gt; &lt;td&gt;    2.520&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;male&lt;/th&gt;            &lt;td&gt;    4.8568&lt;/td&gt; &lt;td&gt;    0.104&lt;/td&gt; &lt;td&gt;   46.688&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    4.653&lt;/td&gt; &lt;td&gt;    5.061&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;age&lt;/th&gt;             &lt;td&gt;   -0.1215&lt;/td&gt; &lt;td&gt;    0.006&lt;/td&gt; &lt;td&gt;  -21.752&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;   -0.132&lt;/td&gt; &lt;td&gt;   -0.111&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;income&lt;/th&gt;          &lt;td&gt;    0.0014&lt;/td&gt; &lt;td&gt;    9e-05&lt;/td&gt; &lt;td&gt;   16.032&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    0.001&lt;/td&gt; &lt;td&gt;    0.002&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;h3 id=&#34;matching&#34;&gt;Matching&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from causalml.match import NearestNeighborMatch, create_table_one

X = [&#39;male&#39;, &#39;age&#39;, &#39;income&#39;]
psm = NearestNeighborMatch(replace=True, ratio=1, random_state=42)
df_matched = psm.match(data=df, 
                       treatment_col=&amp;quot;treated&amp;quot;,
                       score_cols=X)
smf.ols(&#39;outcome ~ treated&#39;, data=df_matched).fit().summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
         &lt;td&gt;&lt;/td&gt;            &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt;       &lt;td&gt;   29.5347&lt;/td&gt; &lt;td&gt;    0.078&lt;/td&gt; &lt;td&gt;  380.105&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;   29.382&lt;/td&gt; &lt;td&gt;   29.687&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;treated[T.True]&lt;/th&gt; &lt;td&gt;    1.6918&lt;/td&gt; &lt;td&gt;    0.110&lt;/td&gt; &lt;td&gt;   15.396&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    1.476&lt;/td&gt; &lt;td&gt;    1.907&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;h3 id=&#34;ipw&#34;&gt;IPW&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df[&amp;quot;pscore&amp;quot;] = smf.logit(&amp;quot;np.rint(treated) ~ male + age + income&amp;quot;, data=df).fit(disp=False).predict()
w = 1 / (df[&amp;quot;pscore&amp;quot;] * df[&amp;quot;treated&amp;quot;] + (1-df[&amp;quot;pscore&amp;quot;]) * (1-df[&amp;quot;treated&amp;quot;]))
smf.wls(&amp;quot;outcome ~ treated&amp;quot;, weights=w, data=df).fit().summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
         &lt;td&gt;&lt;/td&gt;            &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt;       &lt;td&gt;   30.2572&lt;/td&gt; &lt;td&gt;    0.083&lt;/td&gt; &lt;td&gt;  362.783&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;   30.094&lt;/td&gt; &lt;td&gt;   30.421&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;treated[T.True]&lt;/th&gt; &lt;td&gt;    1.6806&lt;/td&gt; &lt;td&gt;    0.116&lt;/td&gt; &lt;td&gt;   14.429&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    1.452&lt;/td&gt; &lt;td&gt;    1.909&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;h3 id=&#34;r-learner&#34;&gt;R Learner&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from causalml.inference.meta import BaseRLearner
from lightgbm import LGBMRegressor

BaseRLearner(learner=LGBMRegressor()).estimate_ate(X=df[X], treatment=df[&#39;treated&#39;], y=df[&#39;outcome&#39;])
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;(array([1.6529302]), array([1.65076231]), array([1.65509809]))
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;s-learner&#34;&gt;S Learner&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from causalml.inference.meta import BaseSLearner

BaseSLearner(learner=LGBMRegressor()).estimate_ate(X=df[X], treatment=df[&#39;treated&#39;], y=df[&#39;outcome&#39;])
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;array([2.08090912])
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;t-learner&#34;&gt;T Learner&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from causalml.inference.meta import BaseTLearner

BaseTLearner(learner=LGBMRegressor()).estimate_ate(X=df[X], treatment=df[&#39;treated&#39;], y=df[&#39;outcome&#39;])
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;(array([2.05252873]), array([1.86063479]), array([2.24442267]))
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;x-learner&#34;&gt;X Learner&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from causalml.inference.meta import BaseXLearner

BaseXLearner(learner=LGBMRegressor()).estimate_ate(X=df[X], treatment=df[&#39;treated&#39;], y=df[&#39;outcome&#39;])
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;(array([2.0255193]), array([1.83456903]), array([2.21646956]))
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;dr-learner&#34;&gt;DR Learner&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from causalml.inference.meta import BaseDRLearner

BaseDRLearner(learner=LGBMRegressor()).estimate_ate(X=df[X], treatment=df[&#39;treated&#39;], y=df[&#39;outcome&#39;])
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;(array([2.08923293]), array([1.8921804]), array([2.28628547]))
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;compare-all&#34;&gt;Compare All&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def simulate(dgp, K=100):
    
    # Initialize coefficients
    results = pd.DataFrame(columns=[&#39;k&#39;, &#39;Estimator&#39;, &#39;Estimate&#39;])
    names = [&#39;1. Diff &#39;, &#39;2. Reg  &#39;, &#39;3. Match&#39;, &#39;4. IPW  &#39;, 
             &#39;5. R lrn&#39;, &#39;6. S lrn&#39;, &#39;7. T lrn&#39;, &#39;8. X lrn&#39;, &#39;9. DRlrn&#39;]
    
    # Compute coefficients
    for k in range(K):
        print(f&amp;quot;Simulation {k}/{K}&amp;quot;, end=&amp;quot;\r&amp;quot;)
        temp = pd.DataFrame({&#39;k&#39;: [k] * len(names), 
                             &#39;Estimator&#39;: names, 
                             &#39;Estimate&#39;: [0] * len(names)})
        
        # Draw data
        df = dgp.generate_data(seed=k)

        # Single diff
        temp[&#39;Estimate&#39;][0] = smf.ols(&#39;outcome ~ treated&#39;, data=df).fit().params[1]
        
        # Regression with controls
        temp[&#39;Estimate&#39;][1] = smf.ols(f&#39;outcome ~ treated + male + age + income&#39;, data=df).fit().params[1]
        
        # Matching
        psm = NearestNeighborMatch(replace=True, ratio=1)
        df_matched = psm.match(data=df, treatment_col=&amp;quot;treated&amp;quot;, score_cols=X)
        temp[&#39;Estimate&#39;][2] = smf.ols(&#39;outcome ~ treated&#39;, data=df_matched).fit().params[1]
        
        # IPW
        df[&amp;quot;pscore&amp;quot;] = smf.logit(&amp;quot;np.rint(treated) ~ male + age + income&amp;quot;, data=df).fit(disp=False).predict()
        w = 1 / (df[&amp;quot;pscore&amp;quot;] * df[&amp;quot;treated&amp;quot;] + (1-df[&amp;quot;pscore&amp;quot;]) * (1-df[&amp;quot;treated&amp;quot;]))
        temp[&#39;Estimate&#39;][3] = smf.wls(&amp;quot;outcome ~ treated&amp;quot;, weights=w, data=df).fit().params[1]
                
        # R Learner
        temp[&#39;Estimate&#39;][4] = BaseRLearner(learner=LGBMRegressor()).estimate_ate(X=df[X], treatment=df[&#39;treated&#39;], y=df[&#39;outcome&#39;])[0][0]
        
        # S Learner
        temp[&#39;Estimate&#39;][5] = BaseSLearner(learner=LGBMRegressor()).estimate_ate(X=df[X], treatment=df[&#39;treated&#39;], y=df[&#39;outcome&#39;])[0]
        
        # T Learner
        temp[&#39;Estimate&#39;][6] = BaseTLearner(learner=LGBMRegressor()).estimate_ate(X=df[X], treatment=df[&#39;treated&#39;], y=df[&#39;outcome&#39;])[0][0]
        
        # X Learner
        temp[&#39;Estimate&#39;][7] = BaseXLearner(learner=LGBMRegressor()).estimate_ate(X=df[X], treatment=df[&#39;treated&#39;], y=df[&#39;outcome&#39;])[0][0]
        
        # DR Learner
        temp[&#39;Estimate&#39;][8] = BaseDRLearner(learner=LGBMRegressor()).estimate_ate(X=df[X], treatment=df[&#39;treated&#39;], y=df[&#39;outcome&#39;])[0][0]
        
        # Combine estimates
        results = pd.concat((results, temp))
    
    return results.reset_index(drop=True)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;results = simulate(dgp=dgp_compare())
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Simulation 99/100
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s plot the distribution of the estimated parameters.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;p = sns.kdeplot(data=results, x=&amp;quot;Estimate&amp;quot;, hue=&amp;quot;Estimator&amp;quot;, legend=True);
sns.move_legend(p, &amp;quot;upper left&amp;quot;, bbox_to_anchor=(1.05, 0.8))
plt.axvline(x=2, c=&#39;k&#39;, ls=&#39;--&#39;);
plt.title(&#39;Simulated Distributions&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/compare_27_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;We can also tabulate the simulated mean and standard deviation of each estimator.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;results.groupby(&#39;Estimator&#39;).agg(mean=(&amp;quot;Estimate&amp;quot;, &amp;quot;mean&amp;quot;), std=(&amp;quot;Estimate&amp;quot;, &amp;quot;std&amp;quot;))
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;mean&lt;/th&gt;
      &lt;th&gt;std&lt;/th&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;Estimator&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;1. Diff&lt;/th&gt;
      &lt;td&gt;-0.054010&lt;/td&gt;
      &lt;td&gt;0.120718&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2. Reg&lt;/th&gt;
      &lt;td&gt;2.190934&lt;/td&gt;
      &lt;td&gt;0.126232&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3. Match&lt;/th&gt;
      &lt;td&gt;1.575239&lt;/td&gt;
      &lt;td&gt;0.211772&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4. IPW&lt;/th&gt;
      &lt;td&gt;1.596031&lt;/td&gt;
      &lt;td&gt;0.119129&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;5. R lrn&lt;/th&gt;
      &lt;td&gt;1.649703&lt;/td&gt;
      &lt;td&gt;0.263321&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;6. S lrn&lt;/th&gt;
      &lt;td&gt;1.963797&lt;/td&gt;
      &lt;td&gt;0.129638&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;7. T lrn&lt;/th&gt;
      &lt;td&gt;1.891307&lt;/td&gt;
      &lt;td&gt;0.160480&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;8. X lrn&lt;/th&gt;
      &lt;td&gt;1.975699&lt;/td&gt;
      &lt;td&gt;0.161004&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;9. DRlrn&lt;/th&gt;
      &lt;td&gt;1.876465&lt;/td&gt;
      &lt;td&gt;0.173318&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;The only unbiased estimators seems to be the S-learner and the X-learner, followed by T-learner, DR-learner and linear regression.&lt;/p&gt;
&lt;p&gt;he S-learner however is more efficient.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>How to Compare Two or More Distributions</title>
      <link>https://matteocourthoud.github.io/post/distr/</link>
      <pubDate>Wed, 22 Jun 2022 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/post/distr/</guid>
      <description>&lt;p&gt;&lt;em&gt;A complete guide to comparing distributions, from visualization to statistical tests&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Comparing the empirical distribution of a variable across different groups is a common problem in data science. In particular, in causal inference the problem often arises when we have to &lt;strong&gt;assess the quality of randomization&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;When we want to assess the causal effect of a policy (or UX feature, ad campaign, drug, &amp;hellip;), the golden standard in causal inference are &lt;a href=&#34;https://en.wikipedia.org/wiki/Randomized_controlled_trial&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;randomized control trials&lt;/strong&gt;&lt;/a&gt;, also known as &lt;a href=&#34;https://de.wikipedia.org/wiki/A/B-Test&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;A/B tests&lt;/strong&gt;&lt;/a&gt;. In practice, we select a sample for the study and we randomly split it into a &lt;strong&gt;control&lt;/strong&gt; and a &lt;strong&gt;treatment&lt;/strong&gt; group, and we compare the outcomes between the two groups. Randomization ensures that only difference between the two groups is the treatment, on average, so that we can attribute outcome differences to the treatment effect.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;problem&lt;/strong&gt; is that, despite randomization, the two groups are never identical. However, sometimes, they are not even &amp;ldquo;similar&amp;rdquo;. For example, we might have more males in one group, or older people, etc.. (we usually call these characteristics, &lt;em&gt;covariates&lt;/em&gt; or &lt;em&gt;control variables&lt;/em&gt;). When it happens, we cannot be certain anymore that the difference in the outcome is only due to the treatment and cannot be attributed to the &lt;strong&gt;inbalanced covariates&lt;/strong&gt; instead. Therefore, it is always important, after randomization, to check whether all observed variables are balanced across groups and whether there are no systematic differences. Another option, to be certain ex-ante that certain covariates are balanced, is &lt;a href=&#34;https://en.wikipedia.org/wiki/Stratified_sampling&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;stratified sampling&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;In this blog post, we are going to see different ways to compare two (or more) distributions and assess the magnitude and significance of their difference. We are going to consider two different approaches, &lt;strong&gt;visual&lt;/strong&gt; and &lt;strong&gt;statistical&lt;/strong&gt;. The two approaches generally trade-off &lt;strong&gt;intuition&lt;/strong&gt; with &lt;strong&gt;rigor&lt;/strong&gt;: from plots we can quickly assess and explore differences, but it&amp;rsquo;s hard to tell whether these differences are systematic or due to noise.&lt;/p&gt;
&lt;h2 id=&#34;example&#34;&gt;Example&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s assume we need to perform an &lt;strong&gt;experiment&lt;/strong&gt; on a group of individuals and we have randomized them into a &lt;strong&gt;treatment and control&lt;/strong&gt; group. We would like them to be &lt;strong&gt;as comparable as possible&lt;/strong&gt;, in order to attribute any difference between the two groups to the treatment effect alone. We also have divided the treatment group in different &lt;em&gt;arms&lt;/em&gt; for testing different treatments (e.g. slight variations of the same drug).&lt;/p&gt;
&lt;p&gt;For this example, I have simulated a dataset of 1000 individuals, for whom we observe a set of characteristics. I import the data generating process &lt;code&gt;dgp_rnd_assignment()&lt;/code&gt; from &lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/src/dgp.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;src.dgp&lt;/code&gt;&lt;/a&gt; and some plotting functions and libraries from &lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/src/utils.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;src.utils&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;%matplotlib inline
%config InlineBackend.figure_format = &#39;retina&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from src.utils import *
from src.dgp import dgp_rnd_assignment

df = dgp_rnd_assignment().generate_data()
df.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;Group&lt;/th&gt;
      &lt;th&gt;Arm&lt;/th&gt;
      &lt;th&gt;Gender&lt;/th&gt;
      &lt;th&gt;Age&lt;/th&gt;
      &lt;th&gt;Income&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;control&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;29.0&lt;/td&gt;
      &lt;td&gt;568.44&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;control&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;32.0&lt;/td&gt;
      &lt;td&gt;596.45&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;treatment&lt;/td&gt;
      &lt;td&gt;arm 3&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;29.0&lt;/td&gt;
      &lt;td&gt;380.86&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;control&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;25.0&lt;/td&gt;
      &lt;td&gt;476.38&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;treatment&lt;/td&gt;
      &lt;td&gt;arm 4&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;32.0&lt;/td&gt;
      &lt;td&gt;628.28&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;We have information on $1000$ individuals, for which we observe &lt;code&gt;gender&lt;/code&gt;, &lt;code&gt;age&lt;/code&gt; and weekly &lt;code&gt;income&lt;/code&gt;. Each individual is assigned either to the treatment or control &lt;code&gt;group&lt;/code&gt; and treated individuals are distributed across four treatment &lt;code&gt;arms&lt;/code&gt;.&lt;/p&gt;
&lt;h2 id=&#34;two-groups---plots&#34;&gt;Two Groups - Plots&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s start with the simplest setting: we want to compare the distribution of income across the &lt;code&gt;treatment&lt;/code&gt; and &lt;code&gt;control&lt;/code&gt; group. We first explore &lt;strong&gt;visual&lt;/strong&gt; approaches and the &lt;strong&gt;statistical&lt;/strong&gt; approaches. The advantage of the first is &lt;strong&gt;intuition&lt;/strong&gt; while the advantage of the second is &lt;strong&gt;rigor&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;For most visualizations I am going to use Python&amp;rsquo;s &lt;a href=&#34;https://seaborn.pydata.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;seaborn&lt;/code&gt;&lt;/a&gt; library.&lt;/p&gt;
&lt;h3 id=&#34;boxplot&#34;&gt;Boxplot&lt;/h3&gt;
&lt;p&gt;A first visual approach is the &lt;a href=&#34;https://en.wikipedia.org/wiki/Box_plot&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;boxplot&lt;/strong&gt;&lt;/a&gt;. The boxplot is a good trade-off between summary statistics and data visualization. The center of the &lt;strong&gt;box&lt;/strong&gt; represents the &lt;em&gt;median&lt;/em&gt; while the borders represent the first (Q1) and third &lt;a href=&#34;https://en.wikipedia.org/wiki/Quartile&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;quartile&lt;/a&gt; (Q3), respectively. The &lt;strong&gt;whiskers&lt;/strong&gt; instead, extend to the first data points that are more than 1.5 times the &lt;em&gt;interquartile range&lt;/em&gt; (Q3 - Q1) outside the box. The points that fall outside of the whiskers are plotted individually and are usually considered &lt;a href=&#34;https://en.wikipedia.org/wiki/Outlier&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;outliers&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Therefore, the boxplot provides both summary statistics (the box and the whiskers) and direct data visualization (the outliers).&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sns.boxplot(data=df, x=&#39;Group&#39;, y=&#39;Income&#39;);
plt.title(&amp;quot;Boxplot&amp;quot;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/distr_12_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;It seems that the &lt;code&gt;income&lt;/code&gt; distribution in the &lt;code&gt;treatment&lt;/code&gt; group is slightly more dispersed: the orange box is larger and its whiskers cover a wider range. However, the &lt;strong&gt;issue&lt;/strong&gt; with the boxplot is that it hides the shape of the data, telling us some summary statistics but not showing us the actual data distribution.&lt;/p&gt;
&lt;h3 id=&#34;histogram&#34;&gt;Histogram&lt;/h3&gt;
&lt;p&gt;The most intuitive way to plot a distribution is the &lt;strong&gt;histogram&lt;/strong&gt;. The histogram groups the data into equally wide &lt;strong&gt;bins&lt;/strong&gt; and plots the number of observations within each bin.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sns.histplot(data=df, x=&#39;Income&#39;, hue=&#39;Group&#39;, bins=50);
plt.title(&amp;quot;Histogram&amp;quot;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/distr_16_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;There are multiple &lt;strong&gt;issues&lt;/strong&gt; with this plot:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Since the two groups have a different number of observations, the two histograms are not comparable&lt;/li&gt;
&lt;li&gt;The number of bins is arbitrary&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We can solve the first issue using the &lt;code&gt;stat&lt;/code&gt; option to plot the &lt;code&gt;density&lt;/code&gt; instead of the count and setting the &lt;code&gt;common_norm&lt;/code&gt; option to &lt;code&gt;False&lt;/code&gt; to use the same normalization.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sns.histplot(data=df, x=&#39;Income&#39;, hue=&#39;Group&#39;, bins=50, stat=&#39;density&#39;, common_norm=False);
plt.title(&amp;quot;Density Histogram&amp;quot;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/distr_18_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Now the two histograms are comparable!&lt;/p&gt;
&lt;p&gt;However, an important &lt;strong&gt;issue&lt;/strong&gt; remains: the size of the bins is arbitrary. In the extreme, if we bunch the data less, we end up with bins with at most one observation, if we bunch the data more, we end up with a single bin. In both cases, if we exaggerate, the plot loses informativeness. This is a classical &lt;a href=&#34;https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;bias-variance trade-off&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;kernel-density&#34;&gt;Kernel Density&lt;/h3&gt;
&lt;p&gt;One possible solution is to use a &lt;a href=&#34;https://en.wikipedia.org/wiki/Kernel_density_estimation&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;kernel density function&lt;/strong&gt;&lt;/a&gt; that tries to approximate the histogram with a continuous function, using &lt;a href=&#34;https://en.wikipedia.org/wiki/Kernel_density_estimation&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;kernel density estimation (KDE)&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sns.kdeplot(x=&#39;Income&#39;, data=df, hue=&#39;Group&#39;, common_norm=False);
plt.title(&amp;quot;Kernel Density Function&amp;quot;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/distr_22_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;From the plot, it seems that the estimated kernel density of &lt;code&gt;income&lt;/code&gt; has &amp;ldquo;fatter tails&amp;rdquo; (i.e. higher variance) in the &lt;code&gt;treatment&lt;/code&gt; group, while the average seems similar across groups.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;issue&lt;/strong&gt; with kernel density estimation is that it is a bit of a  black-box and might mask relevant features of the data.&lt;/p&gt;
&lt;h3 id=&#34;cumulative-distribution&#34;&gt;Cumulative Distribution&lt;/h3&gt;
&lt;p&gt;A more transparent representation of the two distribution is their &lt;a href=&#34;https://en.wikipedia.org/wiki/Cumulative_distribution_function&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;cumulative distribution function&lt;/strong&gt;&lt;/a&gt;. At each point of the x axis (&lt;code&gt;income&lt;/code&gt;) we plot the percentage of data points that have an equal or lower value. The main &lt;strong&gt;advantages&lt;/strong&gt; of the cumulative distribution function are that&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;we do not need to make any arbitrary choice (e.g. number of bins)&lt;/li&gt;
&lt;li&gt;we do not need to perform any approximation (e.g. with KDE), but we represent all data points&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sns.histplot(x=&#39;Income&#39;, data=df, hue=&#39;Group&#39;, bins=len(df), stat=&amp;quot;density&amp;quot;,
             element=&amp;quot;step&amp;quot;, fill=False, cumulative=True, common_norm=False);
plt.title(&amp;quot;Cumulative distribution function&amp;quot;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/distr_26_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;How should we interpret the graph?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Since the two lines cross more or less at 0.5 (y axis), it means that their median is similar&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Since the orange line is above the blue line on the left and below the blue line on the left, it means that the distribution of the &lt;code&gt;treatment&lt;/code&gt; group as fatter tails&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;qq-plot&#34;&gt;QQ Plot&lt;/h3&gt;
&lt;p&gt;A related method is the &lt;strong&gt;qq-plot&lt;/strong&gt;, where &lt;em&gt;q&lt;/em&gt; stands for quantile. The qq-plot plots the quantiles of the two distributions against each other. If the distributions are the same, we should get a 45 degree line.&lt;/p&gt;
&lt;p&gt;There is no native qq-plot function in Python and, while the &lt;code&gt;statsmodels&lt;/code&gt; package provides a &lt;a href=&#34;https://www.statsmodels.org/dev/generated/statsmodels.graphics.gofplots.qqplot.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;qqplot&lt;/code&gt; function&lt;/a&gt;, it is quite cumbersome. Therefore, we will do it by hand.&lt;/p&gt;
&lt;p&gt;First, we need to compute the quartiles of the two groups, using the &lt;code&gt;percentile&lt;/code&gt; function.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;income = df[&#39;Income&#39;].values
income_t = df.loc[df.Group==&#39;treatment&#39;, &#39;Income&#39;].values
income_c = df.loc[df.Group==&#39;control&#39;, &#39;Income&#39;].values

df_pct = pd.DataFrame()
df_pct[&#39;q_treatment&#39;] = np.percentile(income_t, range(100))
df_pct[&#39;q_control&#39;] = np.percentile(income_c, range(100))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we can plot the two quantile distributions against each other, plus the 45-degree line, representing the benchmark perfect fit.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plt.figure(figsize=(8, 8))
plt.scatter(x=&#39;q_control&#39;, y=&#39;q_treatment&#39;, data=df_pct, label=&#39;Actual fit&#39;);
sns.lineplot(x=&#39;q_control&#39;, y=&#39;q_control&#39;, data=df_pct, color=&#39;r&#39;, label=&#39;Line of perfect fit&#39;);
plt.xlabel(&#39;Quantile of income, control group&#39;)
plt.ylabel(&#39;Quantile of income, treatment group&#39;)
plt.legend()
plt.title(&amp;quot;QQ plot&amp;quot;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/distr_32_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;The qq-plot delivers a very &lt;strong&gt;similar insight&lt;/strong&gt; with respect to the cumulative distribution plot: income in the treatment group has the same median (lines cross in the center) but wider tails (dots are below the line on the left end and above on the right end).&lt;/p&gt;
&lt;h2 id=&#34;two-groups---tests&#34;&gt;Two Groups - Tests&lt;/h2&gt;
&lt;p&gt;So far, we have seen different ways to &lt;em&gt;visualize&lt;/em&gt; differences between distributions. The main advantage of visualization is &lt;strong&gt;intuition&lt;/strong&gt;: we can eyeball the differences and intuitively assess them.&lt;/p&gt;
&lt;p&gt;However, we might want to be more &lt;strong&gt;rigorous&lt;/strong&gt; and try to assess the &lt;strong&gt;statistical significance&lt;/strong&gt; of the difference between the distributions, i.e. answer the question &amp;ldquo;&lt;em&gt;is the observed difference systematic or due to sampling noise?&lt;/em&gt;&amp;rdquo;.&lt;/p&gt;
&lt;p&gt;We are now going to analyze different tests to discern two distributions from each other.&lt;/p&gt;
&lt;h3 id=&#34;t-test&#34;&gt;T-test&lt;/h3&gt;
&lt;p&gt;The first and most common test is the &lt;a href=&#34;https://en.wikipedia.org/wiki/Student%27s_t-test&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;student t-test&lt;/a&gt;. T-tests are generally used to &lt;strong&gt;compare means&lt;/strong&gt;. In this case, we want to test whether the means of the &lt;code&gt;income&lt;/code&gt; distribution is the same across the two groups. The test statistic for the two-means comparison test is given by:&lt;/p&gt;
&lt;p&gt;$$
stat = \frac{|\bar x_1 - \bar x_2|}{\sqrt{s^2 / n }}
$$&lt;/p&gt;
&lt;p&gt;Where $\bar x$ is the sample mean and $s$ is the sample standard deviation. Under mild conditions, the test statistic is asymptotically distributed as a &lt;a href=&#34;https://en.wikipedia.org/wiki/Student%27s_t-distribution&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;student t&lt;/a&gt; distribution.&lt;/p&gt;
&lt;p&gt;We use the &lt;code&gt;ttest_ind&lt;/code&gt; function from &lt;code&gt;scipy&lt;/code&gt; to perform the t-test. The function returns both the test statistic and the implied &lt;a href=&#34;https://en.wikipedia.org/wiki/P-value&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;p-value&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from scipy.stats import ttest_ind

stat, p_value = ttest_ind(income_c, income_t)
print(f&amp;quot;t-test: statistic={stat:.4f}, p-value={p_value:.4f}&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;t-test: statistic=-1.5549, p-value=0.1203
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The p-value of the test is $0.12$, therefore we do &lt;strong&gt;not reject&lt;/strong&gt; the null hypothesis of no difference in &lt;em&gt;means&lt;/em&gt; across treatment and control groups.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: the t-test assumes that the variance in the two samples is the same so that its estimate is computed on the joint sample. &lt;a href=&#34;https://en.wikipedia.org/wiki/Welch%27s_t-test&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Welch’s t-test&lt;/a&gt; allows for unequal variances in the two samples.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;standardized-mean-difference-smd&#34;&gt;Standardized Mean Difference (SMD)&lt;/h3&gt;
&lt;p&gt;In general, it is good practice to always perform a test for difference in means on &lt;strong&gt;all variables&lt;/strong&gt; across the treatment and control group, when we are running a randomized control trial or A/B test.&lt;/p&gt;
&lt;p&gt;However, since the denominator of the t-test statistic depends on the sample size, the t-test has been &lt;strong&gt;criticized&lt;/strong&gt; for making p-values hard to compare across studies. In fact, we may obtain a significant result in an experiment with very small magnitude of difference but large sample size while we may obtain a non-significant result in an experiment with large magnitude of difference but small sample size.&lt;/p&gt;
&lt;p&gt;One solution that has been proposed is the &lt;strong&gt;standardized mean difference (SMD)&lt;/strong&gt;. As the name suggests, this is not a proper test statistic, but just a standardized difference, which can be computed as:&lt;/p&gt;
&lt;p&gt;$$
SMD = \frac{|\bar x_1 - \bar x_2|}{\sqrt{(s^2_1 + s^2_2) / 2}}
$$&lt;/p&gt;
&lt;p&gt;Usually a value below $0.1$ is considered a &amp;ldquo;small&amp;rdquo; difference.&lt;/p&gt;
&lt;p&gt;It is good practice to collect average values of all variables across treatment and control group and a measure of distance between the two — either the t-test or the SMD — into a table that is called &lt;strong&gt;balance table&lt;/strong&gt;. We can use the &lt;a href=&#34;https://causalml.readthedocs.io/en/latest/causalml.html#module-causalml.match&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;create_table_one&lt;/code&gt;&lt;/a&gt; function from the &lt;a href=&#34;https://causalml.readthedocs.io/en/latest/about.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;causalml&lt;/code&gt;&lt;/a&gt; library to generate it. As the name of the function suggests, the balance table should always be the &lt;strong&gt;first table&lt;/strong&gt; you present when performing an A/B test.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from causalml.match import create_table_one

df[&#39;treatment&#39;] = df[&#39;Group&#39;]==&#39;treatment&#39;
create_table_one(df, &#39;treatment&#39;, [&#39;Gender&#39;, &#39;Age&#39;, &#39;Income&#39;])
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;Control&lt;/th&gt;
      &lt;th&gt;Treatment&lt;/th&gt;
      &lt;th&gt;SMD&lt;/th&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;Variable&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;n&lt;/th&gt;
      &lt;td&gt;704&lt;/td&gt;
      &lt;td&gt;296&lt;/td&gt;
      &lt;td&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;Age&lt;/th&gt;
      &lt;td&gt;32.40 (8.54)&lt;/td&gt;
      &lt;td&gt;36.42 (7.76)&lt;/td&gt;
      &lt;td&gt;0.4928&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;Gender&lt;/th&gt;
      &lt;td&gt;0.51 (0.50)&lt;/td&gt;
      &lt;td&gt;0.58 (0.49)&lt;/td&gt;
      &lt;td&gt;0.1419&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;Income&lt;/th&gt;
      &lt;td&gt;524.59 (117.35)&lt;/td&gt;
      &lt;td&gt;538.75 (160.15)&lt;/td&gt;
      &lt;td&gt;0.1009&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;In the first two columns, we can see the average of the different variables across the treatment and control groups, with standard errors in parenthesis. In the &lt;strong&gt;last column&lt;/strong&gt;, the values of the SMD indicate a standardized difference of more than $0.1$ for all variables, suggesting that the two groups are probably different.&lt;/p&gt;
&lt;h3 id=&#34;mannwhitney-u-test&#34;&gt;Mann–Whitney U Test&lt;/h3&gt;
&lt;p&gt;An alternative test is the &lt;a href=&#34;https://en.wikipedia.org/wiki/Mann%E2%80%93Whitney_U_test&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Mann–Whitney U test&lt;/a&gt;. The null hypothesis for this test is that the two groups have the same distribution, while the alternative hypothesis is that one group has larger (or smaller) values than the other.&lt;/p&gt;
&lt;p&gt;Differently from the other tests we have seen so far, the Mann–Whitney U test is agnostic to outliers and concentrates on the center of the distribution.&lt;/p&gt;
&lt;p&gt;The test &lt;strong&gt;procedure&lt;/strong&gt; is the following.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Combine all data points and rank them (in increasing or decreasing order)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Compute $U_1 = R_1 - n_1(n_1 + 1)/2$, where $R_1$ is the sum of the ranks for data points in the first group and $n_1$ is the number of points in the first group.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Compute $U_2$ similarly for the second group.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The test statistic is given by $stat = min(U_1, U_2)$.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Under the null hypothesis of no systematic rank differences between the two distributions (i.e. same median), the test statistic is asymptotically normally distributed with known mean and variance.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;intuition&lt;/strong&gt; behind the computation of $R$ and $U$ is the following: if the values in the first sample were all bigger than the values in the second sample, then $R_1 = n_1(n_1 + 1)/2$ and, as a consequence, $U_1$ would then be zero (minimum attainable value). Otherwise, if the two samples were similar, $U_1$ and $U_2$ would be very close to $n_1 n_2 / 2$ (maximum attainable value).&lt;/p&gt;
&lt;p&gt;We perform the test using the &lt;code&gt;mannwhitneyu&lt;/code&gt; function from &lt;code&gt;scipy&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from scipy.stats import mannwhitneyu

stat, p_value = mannwhitneyu(income_t, income_c)
print(f&amp;quot; Mann–Whitney U Test: statistic={stat:.4f}, p-value={p_value:.4f}&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt; Mann–Whitney U Test: statistic=106371.5000, p-value=0.6012
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We get a p-value of 0.6 which implies that we do &lt;strong&gt;not reject&lt;/strong&gt; the null hypothesis of no difference between the two distributions.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: as for the t-test, there exists a version of the Mann–Whitney U test for unequal variances in the two samples, the &lt;a href=&#34;https://www.statisticshowto.com/brunner-munzel-test-generalized-wilcoxon-test/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Brunner-Munzel test&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;permutation-tests&#34;&gt;Permutation Tests&lt;/h3&gt;
&lt;p&gt;A non-parametric alternative is permutation testing. The idea is that, under the null hypothesis, the two distributions should be the same, therefore &lt;strong&gt;shuffling&lt;/strong&gt; the &lt;code&gt;group&lt;/code&gt; labels should not significantly alter any statistic.&lt;/p&gt;
&lt;p&gt;We can chose any statistic and check how its value in the original sample compares with its distribution across &lt;code&gt;group&lt;/code&gt; label permutations. For example, let&amp;rsquo;s use as a test statistic the &lt;strong&gt;difference of sample means&lt;/strong&gt; between the treatment and control group.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sample_stat = np.mean(income_t) - np.mean(income_c)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;stats = np.zeros(1000)
for k in range(1000):
    labels = np.random.permutation((df[&#39;Group&#39;] == &#39;treatment&#39;).values)
    stats[k] = np.mean(income[labels]) - np.mean(income[labels==False])
p_value = np.mean(stats &amp;gt; sample_stat)

print(f&amp;quot;Permutation test: p-value={p_value:.4f}&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Permutation test: p-value=0.0530
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The permutation test gives us a p-value of $0.056$, implying a weak &lt;strong&gt;non-rejection&lt;/strong&gt; of the null hypothesis at the 5% level.&lt;/p&gt;
&lt;p&gt;How do we &lt;strong&gt;interpret&lt;/strong&gt; the p-value? It means that the difference in means in the data is larger than $1 - 0.0560 = 94.4%$ of the differences in means across the permuted samples.&lt;/p&gt;
&lt;p&gt;We can &lt;strong&gt;visualize&lt;/strong&gt; the test, by plotting the distribution of the test statistic across permutations against its sample value.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plt.hist(stats, label=&#39;Permutation Statistics&#39;, bins=30);
plt.axvline(x=sample_stat, c=&#39;r&#39;, ls=&#39;--&#39;, label=&#39;Sample Statistic&#39;);
plt.legend();
plt.xlabel(&#39;Income difference between treatment and control group&#39;)
plt.title(&#39;Permutation Test&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/distr_55_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;As we can see, the sample statistic is quite extreme with respect to the values in the permuted samples, but not excessively.&lt;/p&gt;
&lt;h3 id=&#34;chi-squared-test&#34;&gt;Chi-Squared Test&lt;/h3&gt;
&lt;p&gt;The &lt;a href=&#34;https://matteocourthoud.github.io/post/chisquared/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;chi-squared test&lt;/a&gt; is a very powerful test that is mostly used to test differences in frequencies.&lt;/p&gt;
&lt;p&gt;One of the &lt;strong&gt;least known applications&lt;/strong&gt; of the chi-squared test, is testing the similarity between two distributions. The &lt;strong&gt;idea&lt;/strong&gt; is to bin the observations of the two groups. If the two distributions were the same, we would expect the same frequency of observations in each bin. Importantly, we need enough observations in each bin, in order for the test to be valid.&lt;/p&gt;
&lt;p&gt;I generate bins corresponding to deciles of the distribution of &lt;code&gt;income&lt;/code&gt; in the &lt;em&gt;control&lt;/em&gt; group and then I compute the expected number of observations in each bin in the &lt;em&gt;treatment&lt;/em&gt; group, if the two distributions were the same.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Init dataframe
df_bins = pd.DataFrame()

# Generate bins from control group
_, bins = pd.qcut(income_c, q=10, retbins=True)
df_bins[&#39;bin&#39;] = pd.cut(income_c, bins=bins).value_counts().index

# Apply bins to both groups
df_bins[&#39;income_c_observed&#39;] = pd.cut(income_c, bins=bins).value_counts().values
df_bins[&#39;income_t_observed&#39;] = pd.cut(income_t, bins=bins).value_counts().values

# Compute expected frequency in the treatment group
df_bins[&#39;income_t_expected&#39;] = df_bins[&#39;income_c_observed&#39;] / np.sum(df_bins[&#39;income_c_observed&#39;]) * np.sum(df_bins[&#39;income_t_observed&#39;])

df_bins
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;bin&lt;/th&gt;
      &lt;th&gt;income_c_observed&lt;/th&gt;
      &lt;th&gt;income_t_observed&lt;/th&gt;
      &lt;th&gt;income_t_expected&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;(232.26, 380.496]&lt;/td&gt;
      &lt;td&gt;70&lt;/td&gt;
      &lt;td&gt;46&lt;/td&gt;
      &lt;td&gt;29.075391&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;(380.496, 425.324]&lt;/td&gt;
      &lt;td&gt;70&lt;/td&gt;
      &lt;td&gt;30&lt;/td&gt;
      &lt;td&gt;29.075391&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;(425.324, 456.795]&lt;/td&gt;
      &lt;td&gt;70&lt;/td&gt;
      &lt;td&gt;24&lt;/td&gt;
      &lt;td&gt;29.075391&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;(456.795, 488.83]&lt;/td&gt;
      &lt;td&gt;71&lt;/td&gt;
      &lt;td&gt;26&lt;/td&gt;
      &lt;td&gt;29.490754&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;(488.83, 513.66]&lt;/td&gt;
      &lt;td&gt;70&lt;/td&gt;
      &lt;td&gt;18&lt;/td&gt;
      &lt;td&gt;29.075391&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;5&lt;/th&gt;
      &lt;td&gt;(513.66, 540.048]&lt;/td&gt;
      &lt;td&gt;70&lt;/td&gt;
      &lt;td&gt;19&lt;/td&gt;
      &lt;td&gt;29.075391&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;6&lt;/th&gt;
      &lt;td&gt;(540.048, 576.664]&lt;/td&gt;
      &lt;td&gt;71&lt;/td&gt;
      &lt;td&gt;21&lt;/td&gt;
      &lt;td&gt;29.490754&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;7&lt;/th&gt;
      &lt;td&gt;(576.664, 621.022]&lt;/td&gt;
      &lt;td&gt;70&lt;/td&gt;
      &lt;td&gt;25&lt;/td&gt;
      &lt;td&gt;29.075391&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;8&lt;/th&gt;
      &lt;td&gt;(621.022, 682.003]&lt;/td&gt;
      &lt;td&gt;70&lt;/td&gt;
      &lt;td&gt;42&lt;/td&gt;
      &lt;td&gt;29.075391&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;9&lt;/th&gt;
      &lt;td&gt;(682.003, 973.46]&lt;/td&gt;
      &lt;td&gt;71&lt;/td&gt;
      &lt;td&gt;41&lt;/td&gt;
      &lt;td&gt;29.490754&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;We can now perform the test by comparing the expected (E) and observed (O) number of observations in the treatment group, across bins. The test statistic is given by&lt;/p&gt;
&lt;p&gt;$$
stat = \sum _{i=1}^{n} \frac{(O_i - E_i)^{2}}{E_i}
$$&lt;/p&gt;
&lt;p&gt;where the bins are indexed by $i$ and $O$ is the observed number of data points in bin $i$ and $E$ is the expected number of data points in bin $i$. Since we generated the bins using deciles of the distribution of &lt;code&gt;income&lt;/code&gt; in the control group, we expect the number of observations per bin in the treatment group to be the same across bins. The test statistic is asymptocally distributed as a &lt;a href=&#34;https://en.wikipedia.org/wiki/Chi-squared_distribution&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;chi-squared&lt;/a&gt; distribution.&lt;/p&gt;
&lt;p&gt;To compute the test statistic and the p-value of the test, we use the &lt;code&gt;chisquare&lt;/code&gt; function from &lt;code&gt;scipy&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from scipy.stats import chisquare

stat, p_value = chisquare(df_bins[&#39;income_t_observed&#39;], df_bins[&#39;income_t_expected&#39;])
print(f&amp;quot;Chi-squared Test: statistic={stat:.4f}, p-value={p_value:.4f}&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Chi-squared Test: statistic=32.1432, p-value=0.0002
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Differently from all other tests so far, the chi-squared test &lt;strong&gt;strongly rejects&lt;/strong&gt; the null hypothesis that the two distributions are the same. Why?&lt;/p&gt;
&lt;p&gt;The reason lies in the fact that the two distributions have a similar center but different tails and the chi-squared test tests the similarity along the &lt;strong&gt;whole distribution&lt;/strong&gt; and not only in the center, as we were doing with the previous tests.&lt;/p&gt;
&lt;p&gt;This result tells a &lt;strong&gt;cautionary tale&lt;/strong&gt;: it is very important to understand &lt;em&gt;what&lt;/em&gt; you are actually testing before drawing blind conclusions from a p-value!&lt;/p&gt;
&lt;h3 id=&#34;kolmogorov-smirnov-test&#34;&gt;Kolmogorov-Smirnov Test&lt;/h3&gt;
&lt;p&gt;The idea of the &lt;a href=&#34;https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Kolmogorov-Smirnov test&lt;/a&gt;, is to &lt;strong&gt;compare the cumulative distributions&lt;/strong&gt; of the two groups. In particular, the Kolmogorov-Smirnov test statistic is the maximum absolute difference between the two cumulative distributions.&lt;/p&gt;
&lt;p&gt;$$
stat = \sup _{x} \ \Big| \ F_1(x) - F_2(x) \ \Big|
$$&lt;/p&gt;
&lt;p&gt;Where $F_1$ and $F_2$ are the two cumulative distribution functions and $x$ are the values of the underlying variable. The asymptotic distribution of the Kolmogorov-Smirnov test statistic is &lt;a href=&#34;https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test#Kolmogorov_distribution&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Kolmogorov distributed&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;To better understand the test, let&amp;rsquo;s plot the cumulative distribution functions and the test statistic. First, we compute the cumulative distribution functions.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_ks = pd.DataFrame()
df_ks[&#39;Income&#39;] = np.sort(df[&#39;Income&#39;].unique())
df_ks[&#39;F_control&#39;] = df_ks[&#39;Income&#39;].apply(lambda x: np.mean(income_c&amp;lt;=x))
df_ks[&#39;F_treatment&#39;] = df_ks[&#39;Income&#39;].apply(lambda x: np.mean(income_t&amp;lt;=x))
df_ks.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;Income&lt;/th&gt;
      &lt;th&gt;F_control&lt;/th&gt;
      &lt;th&gt;F_treatment&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;216.36&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.003378&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;232.26&lt;/td&gt;
      &lt;td&gt;0.001420&lt;/td&gt;
      &lt;td&gt;0.003378&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;243.15&lt;/td&gt;
      &lt;td&gt;0.001420&lt;/td&gt;
      &lt;td&gt;0.006757&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;259.88&lt;/td&gt;
      &lt;td&gt;0.002841&lt;/td&gt;
      &lt;td&gt;0.006757&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;262.82&lt;/td&gt;
      &lt;td&gt;0.002841&lt;/td&gt;
      &lt;td&gt;0.010135&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;We now need to find the point where the absolute distance between the cumulative distribution functions is largest.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;k = np.argmax( np.abs(df_ks[&#39;F_control&#39;] - df_ks[&#39;F_treatment&#39;]))
ks_stat = np.abs(df_ks[&#39;F_treatment&#39;][k] - df_ks[&#39;F_control&#39;][k])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can visualize the value of the test statistic, by plotting the two cumulative distribution functions and the value of the test statistic.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;y = (df_ks[&#39;F_treatment&#39;][k] + df_ks[&#39;F_control&#39;][k])/2
plt.plot(&#39;Income&#39;, &#39;F_control&#39;, data=df_ks, label=&#39;Control&#39;)
plt.plot(&#39;Income&#39;, &#39;F_treatment&#39;, data=df_ks, label=&#39;Treatment&#39;)
plt.errorbar(x=df_ks[&#39;Income&#39;][k], y=y, yerr=ks_stat/2, color=&#39;k&#39;,
             capsize=5, mew=3, label=f&amp;quot;Test statistic: {ks_stat:.4f}&amp;quot;)
plt.legend(loc=&#39;center right&#39;);
plt.title(&amp;quot;Kolmogorov-Smirnov Test&amp;quot;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/distr_69_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;From the plot, we can see that the value of the test statistic corresponds to the distance between the two cumulative distributions at &lt;code&gt;income&lt;/code&gt;~650. For that value of &lt;code&gt;income&lt;/code&gt;, we have the largest imbalance between the two groups.&lt;/p&gt;
&lt;p&gt;We can now perform the actual test using the &lt;code&gt;kstest&lt;/code&gt; function from &lt;code&gt;scipy&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from scipy.stats import kstest

stat, p_value = kstest(income_t, income_c)
print(f&amp;quot; Kolmogorov-Smirnov Test: statistic={stat:.4f}, p-value={p_value:.4f}&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt; Kolmogorov-Smirnov Test: statistic=0.0974, p-value=0.0355
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The p-value is below 5%: we &lt;strong&gt;reject&lt;/strong&gt; the null hypothesis that the two distributions are the same, with 95% confidence.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Note 1&lt;/strong&gt;: The KS test is too conservative and rejects the null hypothesis too rarely. Lilliefors test corrects this bias using a different distribution for the test statistic, the Lilliefors distribution.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Note 2&lt;/strong&gt;: the KS test uses very little information since it only compares the two cumulative distributions at one point: the one of maximum distance. The &lt;a href=&#34;https://en.wikipedia.org/wiki/Anderson%E2%80%93Darling_test&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Anderson-Darling test&lt;/a&gt; and the &lt;a href=&#34;https://en.wikipedia.org/wiki/Cram%C3%A9r%E2%80%93von_Mises_criterion&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Cramér-von Mises test&lt;/a&gt; instead compare the two distributions along the whole domain, by integration (the difference between the two lies in the weighting of the squared distances).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;multiple-groups---plots&#34;&gt;Multiple Groups - Plots&lt;/h2&gt;
&lt;p&gt;So far we have only considered the case of two groups: treatment and control. But that if we had &lt;strong&gt;multiple groups&lt;/strong&gt;? Some of the methods we have seen above scale well, while others don&amp;rsquo;t.&lt;/p&gt;
&lt;p&gt;As a working example, we are now going to check whether the distribution of &lt;code&gt;income&lt;/code&gt; is the same across treatment &lt;code&gt;arms&lt;/code&gt;.&lt;/p&gt;
&lt;h3 id=&#34;boxplot-1&#34;&gt;Boxplot&lt;/h3&gt;
&lt;p&gt;The &lt;strong&gt;boxplot&lt;/strong&gt; scales very well, when we have a number of groups in the single-digits, since we can put the different boxes side-by-side.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sns.boxplot(x=&#39;Arm&#39;, y=&#39;Income&#39;, data=df.sort_values(&#39;Arm&#39;));
plt.title(&amp;quot;Boxplot, multiple groups&amp;quot;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/distr_78_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;From the plot, it looks like the distribution of &lt;code&gt;income&lt;/code&gt; is different across treatment arms, with higher numbered arms having a higher average income.&lt;/p&gt;
&lt;h3 id=&#34;violin-plot&#34;&gt;Violin Plot&lt;/h3&gt;
&lt;p&gt;A very nice extension of the boxplot that combines summary statistics and kernel density estimation is the  &lt;strong&gt;violinplot&lt;/strong&gt;. The violinplot plots separate densities along the y axis so that they don&amp;rsquo;t overlap. By default, it also adds a miniature boxplot inside.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sns.violinplot(x=&#39;Arm&#39;, y=&#39;Income&#39;, data=df.sort_values(&#39;Arm&#39;));
plt.title(&amp;quot;Violin Plot, multiple groups&amp;quot;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/distr_82_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;As for the boxplot, the violin plot suggests that income is different across treatment arms.&lt;/p&gt;
&lt;h3 id=&#34;ridgeline-plot&#34;&gt;Ridgeline Plot&lt;/h3&gt;
&lt;p&gt;Lastly, the &lt;strong&gt;ridgeline plot&lt;/strong&gt; plots multiple kernel density distributions along the x-axis, making them more intuitive than the violin plot but partially overlapping them. Unfortunately, there is no default ridgeline plot neither in &lt;code&gt;matplotlib&lt;/code&gt; nor in &lt;code&gt;seaborn&lt;/code&gt;. We need to import it from &lt;a href=&#34;https://github.com/leotac/joypy&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;joypy&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from joypy import joyplot

joyplot(df, by=&#39;Arm&#39;, column=&#39;Income&#39;, colormap=sns.color_palette(&amp;quot;crest&amp;quot;, as_cmap=True));
plt.xlabel(&#39;Income&#39;);
plt.title(&amp;quot;Ridgeline Plot, multiple groups&amp;quot;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/distr_86_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Again, the ridgeline plot suggests that higher numbered treatment arms have higher income. From this plot it is also easier to appreciate the different shapes of the distributions.&lt;/p&gt;
&lt;h2 id=&#34;multiple-groups---tests&#34;&gt;Multiple Groups - Tests&lt;/h2&gt;
&lt;p&gt;Lastly, let&amp;rsquo;s consider hypothesis tests to compare multiple groups. For simplicity, we will concentrate on the most popular one: the F-test.&lt;/p&gt;
&lt;h3 id=&#34;f-test&#34;&gt;F-test&lt;/h3&gt;
&lt;p&gt;With multiple groups, the most popular test is the &lt;a href=&#34;https://en.wikipedia.org/wiki/F-test&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;F-test&lt;/strong&gt;&lt;/a&gt;. The F-test compares the variance of a variable across different groups. This analysis is also called &lt;a href=&#34;https://en.wikipedia.org/wiki/Analysis_of_variance&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;analysis of variance, or &lt;strong&gt;ANOVA&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;In practice, the F-test statistic is&lt;/p&gt;
&lt;p&gt;$$
\text{stat} = \frac{\text{between-group variance}}{\text{within-group variance}} = \frac{\sum_{g} \big( \bar x_g - \bar x \big) / (G-1)}{\sum_{g} \sum_{i \in g} \big( \bar x_i - \bar x_g \big) / (N-G)}
$$&lt;/p&gt;
&lt;p&gt;Where $G$ is the number of groups, $N$ is the number of observations, $\bar x$ is the overall mean and $\bar x_g$ is the mean within group $g$. Under the null hypothesis of group independence, the f-statistic is &lt;a href=&#34;https://en.wikipedia.org/wiki/F-distribution&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;F-distributed&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from scipy.stats import f_oneway

income_groups = [df.loc[df[&#39;Arm&#39;]==arm, &#39;Income&#39;].values for arm in df[&#39;Arm&#39;].dropna().unique()]
stat, p_value = f_oneway(*income_groups)
print(f&amp;quot;F Test: statistic={stat:.4f}, p-value={p_value:.4f}&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;F Test: statistic=9.0911, p-value=0.0000
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The test p-value is basically zero, implying a &lt;strong&gt;strong rejection&lt;/strong&gt; of the null hypothesis of no differences in the &lt;code&gt;income&lt;/code&gt; distribution across treatment arms.&lt;/p&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;In this post we have see a ton of different ways to &lt;strong&gt;compare two or more distributions&lt;/strong&gt;, both visually and statistically. This is a primary concern in many applications, but especially in causal inference where we use randomization to make treatment and control group as comparable as possible.&lt;/p&gt;
&lt;p&gt;We have also seen how different methods might be better suited for &lt;strong&gt;different situations&lt;/strong&gt;. Visual methods are great to build intuition, but statistical methods are essential for decision-making, since we need to be able to assess the magnitude and statistical significance of the differences.&lt;/p&gt;
&lt;h3 id=&#34;references&#34;&gt;References&lt;/h3&gt;
&lt;p&gt;[1] Student, &lt;a href=&#34;https://www.jstor.org/stable/2331554&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The Probable Error of a Mean&lt;/a&gt; (1908), &lt;em&gt;Biometrika&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;[2] F. Wilcoxon, &lt;a href=&#34;https://www.jstor.org/stable/3001968&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Individual Comparisons by Ranking Methods&lt;/a&gt; (1945), &lt;em&gt;Biometrics Bulletin&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;[3] B. L. Welch, &lt;a href=&#34;https://academic.oup.com/biomet/article/34/1-2/28/210174&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The generalization of &amp;ldquo;Student&amp;rsquo;s&amp;rdquo; problem when several different population variances are involved&lt;/a&gt; (1947), &lt;em&gt;Biometrika&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;[4] H. B. Mann, D. R. Whitney, &lt;a href=&#34;https://www.jstor.org/stable/2236101&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;On a Test of Whether one of Two Random Variables is Stochastically Larger than the Other&lt;/a&gt; (1947), &lt;em&gt;The Annals of Mathematical Statistics&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;[5] E. Brunner, U. Munzen, &lt;a href=&#34;https://onlinelibrary.wiley.com/doi/abs/10.1002/%28SICI%291521-4036%28200001%2942:1%3C17::AID-BIMJ17%3E3.0.CO;2-U&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The Nonparametric Behrens-Fisher Problem: Asymptotic Theory and a Small-Sample Approximation&lt;/a&gt; (2000), &lt;em&gt;Biometrical Journal&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;[6] A. N. Kolmogorov, &lt;a href=&#34;https://link.springer.com/chapter/10.1007/978-94-011-2260-3_15&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Sulla determinazione empirica di una legge di distribuzione&lt;/a&gt; (1933), &lt;em&gt;Giorn. Ist. Ital. Attuar.&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;[7] H. Cramér, &lt;a href=&#34;https://www.tandfonline.com/doi/abs/10.1080/03461238.1928.10416862&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;On the composition of elementary errors&lt;/a&gt; (1928), &lt;em&gt;Scandinavian Actuarial Journal&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;[8] R. von Mises, &lt;a href=&#34;https://www.ams.org/journals/bull/1937-43-05/S0002-9904-1937-06520-7/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Wahrscheinlichkeit statistik und wahrheit&lt;/a&gt; (1936), &lt;em&gt;Bulletin of the American Mathematical Society&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;[9] T. W. Anderson, D. A. Darling, &lt;a href=&#34;https://projecteuclid.org/journals/annals-of-mathematical-statistics/volume-23/issue-2/Asymptotic-Theory-of-Certain-Goodness-of-Fit-Criteria-Based-on/10.1214/aoms/1177729437.full&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Asymptotic Theory of Certain &amp;ldquo;Goodness of Fit&amp;rdquo; Criteria Based on Stochastic Processes&lt;/a&gt; (1953), &lt;em&gt;The Annals of Mathematical Statistics&lt;/em&gt;.&lt;/p&gt;
&lt;h3 id=&#34;related-articles&#34;&gt;Related Articles&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/b63dc69e3d8c&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Goodbye Scatterplot, Welcome Binned Scatterplot&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;code&#34;&gt;Code&lt;/h3&gt;
&lt;p&gt;You can find the original Jupyter Notebook here:&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/distr.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/distr.ipynb&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Understanding Contamination Bias</title>
      <link>https://matteocourthoud.github.io/post/cbias/</link>
      <pubDate>Wed, 15 Jun 2022 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/post/cbias/</guid>
      <description>&lt;p&gt;&lt;em&gt;Problems and solutions of linear regression with multiple treatments&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;In many causal inference settings, we might be interested in the effect of not just one treatment, but &lt;strong&gt;many mutually exclusive treatments&lt;/strong&gt;. For example, we might want to test alternative UX designs, or drugs, or policies. Depending on the context, there might be many reasons why we want to test different treatments at the same time, but generally it can help &lt;em&gt;reducing the sample size&lt;/em&gt;, as we need just a single control group. A simple way to recover the different treatment effects is a linear regression of the outcome of interest on the different treatment indicators.&lt;/p&gt;
&lt;p&gt;However, in causal inference, we often &lt;strong&gt;condition the analysis&lt;/strong&gt; on other observable variables (often called control variables), either to increase power or, especially in quasi-experimental settings, to identify a causal parameter instead of a simple correlation. There are &lt;a href=&#34;https://towardsdatascience.com/b63dc69e3d8c&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;cases in which adding control variables can backfire&lt;/a&gt;, but otherwise, we usually think that the regression framework is still able to recover the average treatment effect.&lt;/p&gt;
&lt;p&gt;In a breakthrough paper, &lt;a href=&#34;https://www.nber.org/papers/w30108&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Goldsmith-Pinkham, Hull and Kolesár (2022)&lt;/a&gt; have recently shown that in case of &lt;em&gt;multiple and mutually-exclusive&lt;/em&gt; treatments with &lt;em&gt;control variables&lt;/em&gt;, the &lt;strong&gt;regression coefficients do not identify a causal effect&lt;/strong&gt;. However, not everything is lost: the authors propose a simple solution to this problem that still makes use of linear regression.&lt;/p&gt;
&lt;p&gt;In this blog post, I am going to go through a &lt;strong&gt;simple example&lt;/strong&gt; illustrating the nature of the problem and the solution proposed by the authors.&lt;/p&gt;
&lt;h2 id=&#34;multiple-treatments-example&#34;&gt;Multiple Treatments Example&lt;/h2&gt;
&lt;p&gt;Suppose we are an online store and we are not satisfied with our current &lt;em&gt;checkout page&lt;/em&gt;. In particular, we would like to change our &lt;strong&gt;checkout button&lt;/strong&gt; to increase the probability of a purchase. Our UX designer comes up with two alternative checkout buttons, which are displayed below.&lt;/p&gt;
&lt;img src=&#34;fig/buttons.png&#34; width=&#34;800px&#34;/&gt;
&lt;p&gt;In order to understand which button to use, we run an &lt;a href=&#34;https://en.wikipedia.org/wiki/A/B_testing&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;A/B test&lt;/strong&gt;&lt;/a&gt;, or randomized control trial. In particular, when people arrive at the checkout page, we show them one of the three options, at random. Then, for each user, we record the revenue generated which is our outcome of interest.&lt;/p&gt;
&lt;p&gt;I generate a synthetic dataset using &lt;code&gt;dgp_buttons()&lt;/code&gt; from &lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/src/dgp.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;src.dgp&lt;/code&gt;&lt;/a&gt; as data generating process. I also import plotting functions and standard libraries from &lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/src/utils.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;src.utils&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;%matplotlib inline
%config InlineBackend.figure_format = &#39;retina&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from src.utils import *
from src.dgp import dgp_buttons
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;dgp = dgp_buttons()
df = dgp.generate_data()
df.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;group&lt;/th&gt;
      &lt;th&gt;revenue&lt;/th&gt;
      &lt;th&gt;mobile&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;button1&lt;/td&gt;
      &lt;td&gt;8.927335&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;default&lt;/td&gt;
      &lt;td&gt;13.613456&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;button2&lt;/td&gt;
      &lt;td&gt;4.777628&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;default&lt;/td&gt;
      &lt;td&gt;8.909049&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;default&lt;/td&gt;
      &lt;td&gt;10.160347&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;We have information on 2000 users, for which we observe their checkout button (&lt;code&gt;default&lt;/code&gt;, &lt;code&gt;button1&lt;/code&gt; or &lt;code&gt;button2&lt;/code&gt;), the &lt;code&gt;revenue&lt;/code&gt; they generate and whether they connected from desktop or &lt;code&gt;mobile&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;We notice too late that we have a &lt;strong&gt;problem with randomization&lt;/strong&gt;. We showed &lt;code&gt;button1&lt;/code&gt; more frequently to desktop users and &lt;code&gt;button2&lt;/code&gt; more frequently to mobile users. The control group that sees the &lt;code&gt;default&lt;/code&gt; button instead is balanced.&lt;/p&gt;
&lt;img src=&#34;fig/randomization.png&#34; width=&#34;600px&#34;/&gt;
&lt;p&gt;What should we do? What happens if we simply compare &lt;code&gt;revenue&lt;/code&gt; across &lt;code&gt;groups&lt;/code&gt;? Let&amp;rsquo;s do it by regressing &lt;code&gt;revenue&lt;/code&gt; on &lt;code&gt;group&lt;/code&gt; dummy variables.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;smf.ols(&#39;revenue ~ group&#39;, data=df).fit().summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
          &lt;td&gt;&lt;/td&gt;            &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt;        &lt;td&gt;   11.6553&lt;/td&gt; &lt;td&gt;    0.149&lt;/td&gt; &lt;td&gt;   78.250&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;   11.363&lt;/td&gt; &lt;td&gt;   11.948&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;group[T.button1]&lt;/th&gt; &lt;td&gt;   -0.5802&lt;/td&gt; &lt;td&gt;    0.227&lt;/td&gt; &lt;td&gt;   -2.556&lt;/td&gt; &lt;td&gt; 0.011&lt;/td&gt; &lt;td&gt;   -1.026&lt;/td&gt; &lt;td&gt;   -0.135&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;group[T.button2]&lt;/th&gt; &lt;td&gt;   -0.5958&lt;/td&gt; &lt;td&gt;    0.218&lt;/td&gt; &lt;td&gt;   -2.727&lt;/td&gt; &lt;td&gt; 0.006&lt;/td&gt; &lt;td&gt;   -1.024&lt;/td&gt; &lt;td&gt;   -0.167&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;From the regression results we estimate a negative and significant effect for both buttons. Should we believe these estimates? Are they &lt;strong&gt;causal&lt;/strong&gt;?&lt;/p&gt;
&lt;p&gt;It is unlikely that what we have estimated are the true treatment effects. In fact, there might be substantial &lt;strong&gt;differences in purchase attitudes&lt;/strong&gt; between desktop and mobile users. Since we do not have a comparable number of mobile and desktop users across treatment arms, it might be that the observed differences in &lt;code&gt;revenue&lt;/code&gt; are due to the &lt;em&gt;device&lt;/em&gt; used and not the &lt;em&gt;button design&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Because of this, we decide to &lt;strong&gt;condition&lt;/strong&gt; our analysis on the device used and we include the &lt;code&gt;mobile&lt;/code&gt; dummy variable in the regression.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;smf.ols(&#39;revenue ~ group + mobile&#39;, data=df).fit().summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
          &lt;td&gt;&lt;/td&gt;            &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt;        &lt;td&gt;    9.1414&lt;/td&gt; &lt;td&gt;    0.110&lt;/td&gt; &lt;td&gt;   82.905&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    8.925&lt;/td&gt; &lt;td&gt;    9.358&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;group[T.button1]&lt;/th&gt; &lt;td&gt;    0.3609&lt;/td&gt; &lt;td&gt;    0.141&lt;/td&gt; &lt;td&gt;    2.558&lt;/td&gt; &lt;td&gt; 0.011&lt;/td&gt; &lt;td&gt;    0.084&lt;/td&gt; &lt;td&gt;    0.638&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;group[T.button2]&lt;/th&gt; &lt;td&gt;   -1.0326&lt;/td&gt; &lt;td&gt;    0.134&lt;/td&gt; &lt;td&gt;   -7.684&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;   -1.296&lt;/td&gt; &lt;td&gt;   -0.769&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;mobile&lt;/th&gt;           &lt;td&gt;    4.7181&lt;/td&gt; &lt;td&gt;    0.116&lt;/td&gt; &lt;td&gt;   40.691&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    4.491&lt;/td&gt; &lt;td&gt;    4.946&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;Now the coefficient of &lt;code&gt;button1&lt;/code&gt; is positive and significant. Should we recommend its implementation?&lt;/p&gt;
&lt;p&gt;The answer is &lt;strong&gt;surprisingly no&lt;/strong&gt;. &lt;a href=&#34;https://www.nber.org/papers/w30108&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Goldsmith-Pinkham, Hull, Kolesár (2022)&lt;/a&gt; show that this type of regression does not identify the average treatment effect when:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;there are mutually exclusive treatment arms (in our case, &lt;code&gt;groups&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;we are controlling for some variable $X$ (in our case, &lt;code&gt;mobile&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;there treatment effects are heterogeneous in $X$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This is true &lt;strong&gt;even if&lt;/strong&gt; the treatment is &amp;ldquo;as good as random&amp;rdquo; once we condition on $X$.&lt;/p&gt;
&lt;p&gt;Indeed, in our case, the true treatment effects are the ones reported in the following table.&lt;/p&gt;
&lt;img src=&#34;fig/effects.png&#34; width=&#34;500px&#34;/&gt;
&lt;p&gt;The first button has &lt;strong&gt;no effect&lt;/strong&gt; on revenue, irrespectively of the device, while the second button has a &lt;strong&gt;positive effect&lt;/strong&gt; for mobile users and a &lt;strong&gt;negative effect&lt;/strong&gt; for desktop users. Our (wrong) regression specification instead estimates a positive effect of the first button.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s now dig more in detail into the math, to understand why this is happening.&lt;/p&gt;
&lt;h2 id=&#34;theory&#34;&gt;Theory&lt;/h2&gt;
&lt;p&gt;This section borrows heavily from &lt;a href=&#34;https://www.nber.org/papers/w30108&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Goldsmith-Pinkham, Hull, Kolesár (2022)&lt;/a&gt;. For a great summary of the paper, I recommend this excellent Twitter thread by one of the authors, Paul Goldsmith-Pinkham.&lt;/p&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;Economists love using linear regression to estimate treatment effects — it turns out that there are perils to this method, but also amazing perks&lt;br&gt;&lt;br&gt;Come with me in this 🧵 if you want to learn… &lt;a href=&#34;https://t.co/eDsRLkZFZe&#34;&gt;https://t.co/eDsRLkZFZe&lt;/a&gt;&lt;/p&gt;&amp;mdash; Paul Goldsmith-Pinkham (@paulgp) &lt;a href=&#34;https://twitter.com/paulgp/status/1534169803388293120?ref_src=twsrc%5Etfw&#34;&gt;June 7, 2022&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;

&lt;h3 id=&#34;single-treatment-arm&#34;&gt;Single Treatment Arm&lt;/h3&gt;
&lt;p&gt;Suppose we are interested in the effect of a treatment $D$ on an outcome $Y$. First, let&amp;rsquo;s consider the standard case of a &lt;strong&gt;single treatment arm&lt;/strong&gt; so that the treatment variable is binary, $D \in \lbrace 0 , 1 \rbrace$. Also consider a single &lt;strong&gt;binary control variable&lt;/strong&gt; $X \in \lbrace 0 , 1 \rbrace$. We also assume that treatment assignment is as good as random, conditionally on $X$. This means that there might be systematic differences between the treatment and control group, however, these differences are fully accounted for by $X$. Formally we write&lt;/p&gt;
&lt;p&gt;$$
\left( Y_i^{(0)}, Y_i^{(1)} \right) \ \perp \ D_i \ | \ X_i
$$&lt;/p&gt;
&lt;p&gt;Where $Y_i^{(d)}$ denotes the potential outcome of individual $i$ when its treatment status is $d$. For example, $Y_i^{(0)}$ indicates the outcome of individual $i$ in case it is not treated. This notation comes from &lt;a href=&#34;https://www.tandfonline.com/doi/abs/10.1198/016214504000001880&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Rubin&amp;rsquo;s potential outcomes framework&lt;/a&gt;. We can write the &lt;strong&gt;individual treatment effect&lt;/strong&gt; of individual $i$ as&lt;/p&gt;
&lt;p&gt;$$
\tau_i = Y_i^{(1)} - Y_i^{(0)}
$$&lt;/p&gt;
&lt;p&gt;In this setting, the &lt;strong&gt;regression of interest&lt;/strong&gt; is&lt;/p&gt;
&lt;p&gt;$$
Y_i = \alpha + \beta D_i + \gamma X_i + u_i
$$&lt;/p&gt;
&lt;p&gt;The coefficient of interest is $\beta$.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.jstor.org/stable/2998558&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Angrist (1998)&lt;/a&gt; shows that &lt;strong&gt;the regression coefficient $\beta$ identifies the average treatment effect&lt;/strong&gt;. In particular, $\beta$ identifies a weighted average of the within-group $x$ average treatment effect $\tau (x)$ with convex weights. In this particular setting, we can write it as&lt;/p&gt;
&lt;p&gt;$$
\beta = \lambda \tau(0) + (1 - \lambda) \tau(1) \qquad \text{where} \qquad \tau (x) = \mathbb E \big[ Y_i^{(1)} - Y_i^{(0)} \ \big| \ X_i = x \big]
$$&lt;/p&gt;
&lt;p&gt;The weights $\lambda$ and $(1-\lambda)$ are given by the within-group treatment variance. Hence, the OLS estimator gives &lt;strong&gt;less weight to groups where we have less treatment variance&lt;/strong&gt;, i.e., where treatment is more imbalanced. Groups where treatment is distributed 50-50 get the most weight.&lt;/p&gt;
&lt;p&gt;$$
\lambda = \frac{ \text{Var} \big(D_i \ \big| \ X_i = 0 \big) \Pr \big(X_i=0 \big)}{\sum_{x \in \lbrace 0 , 1 \rbrace} \text{Var} \big(D_i \ \big| \ X_i = x \big) \Pr \big( X_i=x \big)} \in [0, 1]
$$&lt;/p&gt;
&lt;p&gt;The weights can be derived using the &lt;a href=&#34;https://towardsdatascience.com/59f801eb3299&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Frisch-Waugh-Lowell theorem&lt;/a&gt; to express $\beta_1$ as the OLS coefficient of a univariate regression of $Y$ on $D_{i, \perp X}$, where $D_{i, \perp X}$ are the residuals from regressing $D$ on $X$. If you are not familiar with the Frisch-Waugh-Lowell theorem, I wrote an &lt;a href=&#34;https://towardsdatascience.com/59f801eb3299&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;introductory blog post here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;$$
\beta_1 = \frac{ \mathbb E \big[ D_{i, \perp X} Y_i \big] }{ \mathbb E \big[ D_{i, \perp X}^2 \big] } =
\underbrace{ \frac{\mathbb E \big[ D_{i, \perp X} Y_i(0) \big]}{\mathbb E \big[ D_{i, \perp X}^2 \big]} } _ {=0} + \frac{\mathbb E \big[ D_{i, \perp X} D_i \tau_i \big]}{\mathbb E \big[ D_{i, \perp X}^2 \big]} =
\frac{\mathbb E \big[ \text{Var} (D_i | X_i) \ \tau(X_i) \big]}{\mathbb E \big[ \text{Var}(D_i | X_i) \big]}
$$&lt;/p&gt;
&lt;p&gt;The second term of the central expression disappears because the residual $D_{i, \perp X}$ is by construction &lt;strong&gt;mean independent&lt;/strong&gt; of the control variable $X_i$, i.e.&lt;/p&gt;
&lt;p&gt;$$
\mathbb E \big[ D_{i, \perp X} | X_i \big] = 0
$$&lt;/p&gt;
&lt;p&gt;This mean independence property is crucial to obtain an unbiased estimate and its failure in the multiple-treatment case is the source of the &lt;em&gt;contamination bias&lt;/em&gt;.&lt;/p&gt;
&lt;h3 id=&#34;multiple-treatment-arms&#34;&gt;Multiple Treatment Arms&lt;/h3&gt;
&lt;p&gt;Let&amp;rsquo;s now consider the case of multiple treatment arms, $D \in \lbrace 0, 1, 2 \rbrace$, where $1$ and $2$ indicate two mutually-exclusive treatments. We still assume &lt;strong&gt;conditional ignorability&lt;/strong&gt;, i.e., treatment assignment is as good as random, conditional on $X$.&lt;/p&gt;
&lt;p&gt;$$
\left( Y_i^{(0)}, Y_i^{(1)}, Y_i^{(2)} \right) \ \perp \ D_i \ | \ X_i
$$&lt;/p&gt;
&lt;p&gt;In this case, we have two different &lt;strong&gt;individual treatment effects&lt;/strong&gt;, one per treatment.&lt;/p&gt;
&lt;p&gt;$$
\tau_{i1} = Y_i^{(1)} - Y_i^{(0)} \qquad \text{and} \qquad \tau_{i2} = Y_i^{(2)} - Y_i^{(0)}
$$&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;regression of interest&lt;/strong&gt; is&lt;/p&gt;
&lt;p&gt;$$
Y_i = \alpha + \beta_1 D_{i1} + \beta_2 D_{i2} + \gamma X_i + u_i
$$&lt;/p&gt;
&lt;p&gt;Does the OLS estimator of $\beta_1$ and $\beta_2$ &lt;strong&gt;identify&lt;/strong&gt; an average treatment effect?&lt;/p&gt;
&lt;p&gt;It would be very tempting to say yes. In fact, it looks like not much has changed with respect to the previous setup. We just have one extra treatment, but the potential outcomes are still conditionally independent of it. Where is the &lt;strong&gt;issue&lt;/strong&gt;?&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s concentrate on $\beta_1$ (the same applies to $\beta_2$). As before, can rewrite $\beta_1$ using the &lt;a href=&#34;https://towardsdatascience.com/59f801eb3299&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Frisch-Waugh-Lowell theorem&lt;/a&gt; as the OLS coefficient of a univariate regression of $Y_i$ on $D_{i1, \perp X, D_2}$, where $D_{i1, \perp X, D_2}$ are the residuals from regressing $D_1$ on $D_2$ and $X$.&lt;/p&gt;
&lt;p&gt;$$
\beta_1 = \frac{ \mathbb E \big[D_{i1, \perp X, D_2} Y_i \big] }{ \mathbb E \big[ D_{i1, \perp X, D_2}^2 \big]} = \underbrace{ \frac{ \mathbb E \big[ D_{i1, \perp X, D_2} Y_i(0) \big] }{\mathbb E \big[ D_{i1, \perp X, D_2}^2 \big]} } _ {=0} + \frac{ \mathbb E \big[ D_{i1, \perp X, D_2} D_{i1} \tau_{i1} \big] }{ \mathbb E \big[ D_{i1, \perp X, D_2}^2 \big]} + \color{red}{ \underbrace{ \color{black}{ \frac{ \mathbb E \big[ D_{i1, \perp X, D_2} D_{i2} \tau_{i2} \big] }{ \mathbb E \big[ D_{i1, \perp X, D_2}^2 \big]}} } _ { \neq 0} }
$$&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;problem&lt;/strong&gt; is the last term. Without the last term, we could still write $\beta_1$ as a convex combination of the individual treatment effects. However, the last term biases the estimator by adding a component that depends on &lt;strong&gt;the treatment effect of $D_2$&lt;/strong&gt;, $\tau_2$. Why does this term not disappear?&lt;/p&gt;
&lt;p&gt;The problem is that $D_{i1, \perp X, D_2}$ is not mean independent of $D_{i2}$, i.e.&lt;/p&gt;
&lt;p&gt;$$
\mathbb E \big[ D_{i1, \perp X, D_2} D_{i2} \ \big| \ X_i \big] \neq 0
$$&lt;/p&gt;
&lt;p&gt;The reason lies in the fact that the treatments are &lt;strong&gt;mutually exclusive&lt;/strong&gt;. This implies that when $D_{i1}=1$, $D_{i2}$ must be zero, regardless of the value of $X_i$. Therefore, the last term does not cancel out and it introduces a &lt;strong&gt;contamination bias&lt;/strong&gt;.&lt;/p&gt;
&lt;h3 id=&#34;solution&#34;&gt;Solution&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://www.nber.org/papers/w30108&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Goldsmith-Pinkham, Hull, Kolesár (2022)&lt;/a&gt; show that a &lt;strong&gt;simple estimator&lt;/strong&gt;, first proposed by &lt;a href=&#34;https://www.aeaweb.org/articles?id=10.1257/jel.47.1.5&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Imbens and Wooldridge (2009)&lt;/a&gt;, is able to remove the bias. The procedure is the following.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;De-mean the control variable: $\tilde X = X - \bar X$&lt;/li&gt;
&lt;li&gt;Regress $Y$ on the interaction between the treatment indicators $D$ and the demeaned control variable $\tilde X$&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The OLS estimators of $\beta_1$ and $\beta_2$ are &lt;strong&gt;unbiased&lt;/strong&gt; estimators of the average treatment effects. It also just requires a linear regression. Moreover, this estimator is unbiased also for continuous control variables $X$, not only for a binary one as we have considered so far.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Why&lt;/strong&gt; was this estimator initially proposed by &lt;a href=&#34;https://www.aeaweb.org/articles?id=10.1257/jel.47.1.5&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Imbens and Wooldridge (2009)&lt;/a&gt;? Let&amp;rsquo;s analyze two parts separately: the interaction term between $D$ and $X$ and the fact that $X$ is de-meaned in the interaction term.&lt;/p&gt;
&lt;p&gt;First, the &lt;strong&gt;interaction term&lt;/strong&gt; $D X$ allows us to control for different effects and/or distributions of $X$ across treatment and control group.&lt;/p&gt;
&lt;p&gt;Second, &lt;strong&gt;de-meaning&lt;/strong&gt; $X$ in the interaction term allows us to &lt;strong&gt;interpret&lt;/strong&gt; the estimated coefficient $\hat \beta$ as the average treatment effect. In fact, assume we were estimating the following linear model, where $X$ is &lt;em&gt;not&lt;/em&gt; de-meaned in the interaction term.&lt;/p&gt;
&lt;p&gt;$$
Y_i = \alpha + \beta D_i + \gamma X_i + \delta D_i X_i + u_i
$$&lt;/p&gt;
&lt;p&gt;In this case, the marginal effect of $D$ on $Y$ is $\beta + \delta X_i$ so that the &lt;em&gt;average&lt;/em&gt; marginal effect is $\beta + \delta \bar X$ which is different from $\beta$.&lt;/p&gt;
&lt;p&gt;If instead we use the de-meaned value of $X$ in the interaction term, the marginal effect of $D$ on $Y$ is $\beta + \delta (X_i - \bar X)$ so that the &lt;em&gt;average&lt;/em&gt; marginal effect is $\beta + \delta (\bar X - \bar X) = \beta$. Now we can interpret $\beta$ as the average marginal effect of $D$ on $Y$.&lt;/p&gt;
&lt;h2 id=&#34;simulations&#34;&gt;Simulations&lt;/h2&gt;
&lt;p&gt;In order to better understand both the problem and the solution, let&amp;rsquo;s run some &lt;strong&gt;simulations&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;We run an estimator over different draws from the data generating process &lt;code&gt;dgp_buttons()&lt;/code&gt;. This is only possible with synthetic data and we do not have this luxury in reality. For each sample, we record the estimated coefficient and the corresponding &lt;a href=&#34;https://en.wikipedia.org/wiki/P-value&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;p-value&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def simulate(dgp, estimator, K=1000):
    
    # Initialize coefficients
    results = pd.DataFrame({&#39;Coefficient&#39;: np.zeros(K), &#39;pvalue&#39;: np.zeros(K)})
    
    # Compute coefficients
    for k in range(K):
        df = dgp.generate_data(seed=k)
        results.Coefficient[k] = estimator(df).params[1]
        results.pvalue[k] = estimator(df).pvalues[1]
    
    results[&#39;Significant&#39;] = results[&#39;pvalue&#39;] &amp;lt; 0.05
    return results
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;First, let&amp;rsquo;s try it with the old estimator that regresses &lt;code&gt;revenue&lt;/code&gt; on both &lt;code&gt;group&lt;/code&gt; and &lt;code&gt;mobile&lt;/code&gt; dummy variables.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;ols_estimator = lambda x: smf.ols(&#39;revenue ~ group + mobile&#39;, data=x).fit()
results = simulate(dgp, ols_estimator)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I &lt;strong&gt;plot&lt;/strong&gt; the distribution of the coefficient estimates of &lt;code&gt;button1&lt;/code&gt; over 1000 simulations, highlighting the statistically significant ones at the 5% level. I also highlight the true value of the coefficient, zero, with a vertical dotted bar.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def plot_results(results):
    p_sig = sum(results[&#39;Significant&#39;]) / len(results) * 100
    sns.histplot(data=results, x=&amp;quot;Coefficient&amp;quot;, hue=&amp;quot;Significant&amp;quot;, multiple=&amp;quot;stack&amp;quot;, 
                 palette = [&#39;tab:red&#39;, &#39;tab:green&#39;]);
    plt.axvline(x=0, c=&#39;k&#39;, ls=&#39;--&#39;, label=&#39;truth&#39;)
    plt.title(rf&amp;quot;Estimated $\beta_1$ ({p_sig:.0f}% significant)&amp;quot;);
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plot_results(results)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/cbias_35_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;As we can see, we reject the null hypothesis of no effect of &lt;code&gt;button1&lt;/code&gt; in 45% of the simulations. Since we set a confidence level of 5%, we would have expected at most around 5% of rejections. Our estimator is &lt;strong&gt;biased&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;As we have seen above, the problem is that the estimator is not just a convex combination of the effect of &lt;code&gt;button1&lt;/code&gt; across mobile and desktop users (it&amp;rsquo;s zero for both), but it is &lt;strong&gt;contaminated&lt;/strong&gt; by the effect of &lt;code&gt;button2&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s now try the estimator proposed from &lt;a href=&#34;https://www.aeaweb.org/articles?id=10.1257/jel.47.1.5&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Imbens and Wooldridge (2009)&lt;/a&gt;. First, we need do de-mean our control variable, &lt;code&gt;mobile&lt;/code&gt;. Then, we regress &lt;code&gt;revenue&lt;/code&gt; on the interaction between &lt;code&gt;group&lt;/code&gt; and the de-meaned control variable, &lt;code&gt;res_mobile&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df[&#39;mobile_res&#39;] = df[&#39;mobile&#39;] - np.mean(df[&#39;mobile&#39;])
smf.ols(&#39;revenue ~ group * mobile_res&#39;, data=df).fit().summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
               &lt;td&gt;&lt;/td&gt;                  &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt;                   &lt;td&gt;   11.5773&lt;/td&gt; &lt;td&gt;    0.067&lt;/td&gt; &lt;td&gt;  172.864&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;   11.446&lt;/td&gt; &lt;td&gt;   11.709&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;group[T.button1]&lt;/th&gt;            &lt;td&gt;    0.0281&lt;/td&gt; &lt;td&gt;    0.106&lt;/td&gt; &lt;td&gt;    0.266&lt;/td&gt; &lt;td&gt; 0.790&lt;/td&gt; &lt;td&gt;   -0.180&lt;/td&gt; &lt;td&gt;    0.236&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;group[T.button2]&lt;/th&gt;            &lt;td&gt;   -1.5071&lt;/td&gt; &lt;td&gt;    0.100&lt;/td&gt; &lt;td&gt;  -15.112&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;   -1.703&lt;/td&gt; &lt;td&gt;   -1.311&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;mobile_res&lt;/th&gt;                  &lt;td&gt;    2.9107&lt;/td&gt; &lt;td&gt;    0.134&lt;/td&gt; &lt;td&gt;   21.715&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    2.648&lt;/td&gt; &lt;td&gt;    3.174&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;group[T.button1]:mobile_res&lt;/th&gt; &lt;td&gt;    0.1605&lt;/td&gt; &lt;td&gt;    0.211&lt;/td&gt; &lt;td&gt;    0.760&lt;/td&gt; &lt;td&gt; 0.448&lt;/td&gt; &lt;td&gt;   -0.254&lt;/td&gt; &lt;td&gt;    0.575&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;group[T.button2]:mobile_res&lt;/th&gt; &lt;td&gt;    5.3771&lt;/td&gt; &lt;td&gt;    0.200&lt;/td&gt; &lt;td&gt;   26.905&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    4.985&lt;/td&gt; &lt;td&gt;    5.769&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;The estimated coefficients are now &lt;strong&gt;close to their true values&lt;/strong&gt;. The estimated coefficient for &lt;code&gt;button1&lt;/code&gt; is not significant, while the estimated coefficient for &lt;code&gt;button2&lt;/code&gt; is negative and significant.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s check whether this results holds &lt;strong&gt;across samples&lt;/strong&gt; by running a simulation. We repeat the estimation procedure 1000 times and we plot the distribution of estimated coefficients for &lt;code&gt;button1&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;new_estimator = lambda x: smf.ols(&#39;revenue ~ group * mobile&#39;, data=x).fit()
new_results = simulate(dgp, new_estimator)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plot_results(new_results)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/cbias_40_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Now the distribution of the estimated coefficient for &lt;code&gt;button1&lt;/code&gt; is centered around the true value of zero. Moreover, we reject the null hypothesis of no effect only in 1% of the simulations, consistently with the chosen confidence level of 95%.&lt;/p&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;In this post, we have seen the dangers of running a factor regression model with multiple &lt;em&gt;mutually exclusive&lt;/em&gt; treatment arms and treatment effect heterogeneity across a control variable. In this case, because the treatments are not independent, the regression coefficients are not a convex combination of the within-group average treatment effects, but also capture the treatment effects of the other treatments introducing &lt;strong&gt;contamination bias&lt;/strong&gt;. The solution to the problem is both simple and elegant, requiring just a linear regression.&lt;/p&gt;
&lt;p&gt;However, &lt;strong&gt;the problem is more general&lt;/strong&gt; than this setting and generally concerns every setting in which (all of the following)&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;We have multiple treatments that depend on each other&lt;/li&gt;
&lt;li&gt;We need to condition the analysis on a control variable&lt;/li&gt;
&lt;li&gt;The treatment effects are heterogeneous in the control variable&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Another popular example is the case of the &lt;a href=&#34;https://arxiv.org/abs/2201.01194&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Two-Way Fixed Effects (TWFE) estimator with staggered treatments&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;references&#34;&gt;References&lt;/h3&gt;
&lt;p&gt;[1] J. Angrist, &lt;a href=&#34;https://www.jstor.org/stable/2998558&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Estimating the Labor Market Impact of Voluntary Military Service Using Social Security Data on Military Applicants&lt;/a&gt; (1998), &lt;em&gt;Econometrica&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;[2] D. Rubin, &lt;a href=&#34;https://www.tandfonline.com/doi/abs/10.1198/016214504000001880&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Causal Inference Using Potential Outcomes&lt;/a&gt; (2005), &lt;em&gt;Journal of the American Statistical Association&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;[3] G. Imbens, J. Wooldridge, &lt;a href=&#34;https://www.aeaweb.org/articles?id=10.1257/jel.47.1.5&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Recent Developments in the Econometrics of Program Evaluation&lt;/a&gt; (2009), &lt;em&gt;Journal of Economic Literature&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;[4] P. Goldsmith-Pinkham, P. Hull, M. Kolesár, &lt;a href=&#34;https://www.nber.org/papers/w30108&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Contamination Bias in Linear Regressions&lt;/a&gt; (2022), &lt;em&gt;working paper&lt;/em&gt;.&lt;/p&gt;
&lt;h3 id=&#34;related-articles&#34;&gt;Related Articles&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/344ac1477699&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Understanding Omitted Variable Bias&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/59f801eb3299&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Understanding The Frisch-Waugh-Lovell Theorem&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/b63dc69e3d8c&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;DAGs and Control Variables&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;code&#34;&gt;Code&lt;/h3&gt;
&lt;p&gt;You can find the original Jupyter Notebook here:&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/cbias.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/matteocourthoud/Blog-Posts/blob/main/cbias.ipynb&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Understanding Combinations and Permutations</title>
      <link>https://matteocourthoud.github.io/post/combperm/</link>
      <pubDate>Fri, 10 Jun 2022 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/post/combperm/</guid>
      <description>&lt;p&gt;How many times did you face questions starting with &amp;ldquo;&lt;em&gt;Suppose you have an urn with three red balls and five blue balls, &amp;hellip;&lt;/em&gt;&amp;rdquo;? The answer for me is, not often since high-school, but recently they popped up again in &lt;strong&gt;data science interviews&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Despite countless classes in statistics, I still take a deep breath and hope I won&amp;rsquo;t embarrass myself too much. My main problem is that I get crazy confused with binary labels, especially if the label itself is not too intuitive.&lt;/p&gt;
&lt;p&gt;Some non-intuitive (for me) binary labels include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Sine_and_cosine&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;sine and cosine&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Concave_function&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;concavity and convexity&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Precision_and_recall&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;precision and recall&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Type_I_and_type_II_errors&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;type 1 and type 2 error rate&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;And last but not least, &lt;strong&gt;combinations and permutations&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;So this blog post is an attempt to clarify once and for all the difference between the two and get some practice.&lt;/p&gt;
&lt;h2 id=&#34;the-math&#34;&gt;The Math&lt;/h2&gt;
&lt;h3 id=&#34;permutations-vs-combinations&#34;&gt;Permutations vs Combinations&lt;/h3&gt;
&lt;p&gt;The main &lt;strong&gt;difference&lt;/strong&gt; between combinations and permutations is &lt;strong&gt;order&lt;/strong&gt;. In particular:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Permutations: order matters&lt;/li&gt;
&lt;li&gt;Combinations: order does &lt;em&gt;not&lt;/em&gt; matter&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;What does it mean &lt;strong&gt;in practice&lt;/strong&gt;?&lt;/p&gt;
&lt;p&gt;One rule of thumb is to check whether the individual objects are &lt;strong&gt;identifiable&lt;/strong&gt;. For example:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Urn with blue and red balls: the individual ball is not identifiable, therefore we are talking about combinations&lt;/li&gt;
&lt;li&gt;Deck of cards: individual cards are identifiable, therefore it could be either way&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Also, &lt;strong&gt;note&lt;/strong&gt; that for any problem, the number of permutations is always weakly &lt;em&gt;larger&lt;/em&gt; than the number of combinations. The intuition is simple: since in permutations order matters, AB and BA are two different outcomes, while with combinations they are grouped into a single one.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Example&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;For this section, we are going to use a simple &lt;strong&gt;example&lt;/strong&gt; in which we have to order a two-scoops ice cream cone and there are three possible flavors: amarena, chocolate and pistacchio.&lt;/p&gt;
&lt;h3 id=&#34;permutations&#34;&gt;Permutations&lt;/h3&gt;
&lt;p&gt;Let&amp;rsquo;s start with permutations since they are &lt;strong&gt;mathematically simpler&lt;/strong&gt;. We have seen that in combinations &lt;strong&gt;order matters&lt;/strong&gt;. In our example, suppose we care about which flavor is on top on the ice cream. Now we are going to further distinguish between two cases:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;With replacement&lt;/li&gt;
&lt;li&gt;Without replacement&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Where replacement means that after I draw an object, I can draw it again (e.g. I put it back in the pool).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Replacement&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In the first case, we can order both scoops of the same flavor. Therefore, for each scoop (2) we have 3 options (the flavors).&lt;/p&gt;
&lt;img src=&#34;fig/perm_repl.png&#34; width=&#34;800px&#34;/&gt;
&lt;p&gt;The number of &lt;strong&gt;overall events&lt;/strong&gt; is&lt;/p&gt;
&lt;p&gt;$$
\text{from 3 permute 2, with replacement} = 3 * 3 = 3^2 = 9
$$&lt;/p&gt;
&lt;p&gt;In general, the number of possible permutations of $n$ objects in $k$ draws with replacement is $n^k$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;No Replacement&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Alternatively, we can only order different flavors in each scoop. In this case, for the first scoop we can pick any flavor, but for the second scoop we can only pick one of the two remaining flavors.&lt;/p&gt;
&lt;img src=&#34;fig/perm.png&#34; width=&#34;550px&#34;/&gt;
&lt;p&gt;The number of &lt;strong&gt;overall events&lt;/strong&gt; is&lt;/p&gt;
&lt;p&gt;$$
\text{from 3 permute 2, without replacement} = 3 * 2 = \frac{3!}{(3-2)!} = \frac{3 * 2 * 1}{1} = 6
$$&lt;/p&gt;
&lt;p&gt;where ! denotes the &lt;a href=&#34;https://en.wikipedia.org/wiki/Factorial&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;factorial operation&lt;/a&gt; which can be recursively defined as $n! = n \times (n-1)!$ with $0! = 1$.&lt;/p&gt;
&lt;p&gt;In general, the number of possible permutations of $n$ objects in $k$ draws without replacement is $\frac{n!}{(n-k)!}$.&lt;/p&gt;
&lt;h3 id=&#34;combinations&#34;&gt;Combinations&lt;/h3&gt;
&lt;p&gt;Combinations are usually &lt;strong&gt;more common&lt;/strong&gt; since in a lot of scenarios we do not care about the order or the identity of the objects. In our example, let&amp;rsquo;s assume we don&amp;rsquo;t care which flavor gets on top. As before, we are going to further distinguish between two cases:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;With replacement&lt;/li&gt;
&lt;li&gt;Without replacement&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;No Replacement&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In this case, we cannot have two scoops of the same flavor. Therefore, for each scoop (2) we have 3 options (the flavors). However, the order of the flavors does not matter, i.e. we are indifferent between getting chocolate on top or on the bottom, as long as we get it.&lt;/p&gt;
&lt;img src=&#34;fig/comb.png&#34; width=&#34;270px&#34;/&gt;
&lt;p&gt;The number of &lt;strong&gt;overall events&lt;/strong&gt; therefore is the number of permutations of 3 flavors among 2 scoops, divided by the permutations of 2 out of 2 scoops.&lt;/p&gt;
&lt;p&gt;$$
\text{from 3 choose 2, without replacement} = \frac{\text{from 3 permute 2, without replacement}}{\text{from 2 permute 2, without replacement}} = \frac{\frac{3!}{(3-2)!}}{\frac{2!}{(2-2)!}} = \frac{3!}{2!(3-2)!} = \frac{3 * 2 * 1}{2 * 1 * 1} = 3
$$&lt;/p&gt;
&lt;p&gt;In general, we define the mathematical operation &amp;ldquo;from $n$ choose $k$&amp;rdquo; as&lt;/p&gt;
&lt;p&gt;$$
\text{from n choose k} := {n \choose k} = \frac{n!}{k!(n-k)!}
$$&lt;/p&gt;
&lt;p&gt;Which corresponds to the number of possible combinations of $n$ objects in $k$ draws without replacement.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Replacement&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Alternatively, we can be allowed to order the same flavor for both scoops.&lt;/p&gt;
&lt;img src=&#34;fig/comb_repl.png&#34; width=&#34;550px&#34;/&gt;
&lt;p&gt;In this case, the number of &lt;strong&gt;overall events&lt;/strong&gt; is&lt;/p&gt;
&lt;p&gt;$$
\text{from 3 choose 2, with replacement} = {3 + 2 - 1 \choose 2} = \frac{(3 + 2 - 1)!}{2!(3-1)!} = \frac{4 * 3 * 2 * 1}{2 * 1 * 2 * 1} = 6
$$&lt;/p&gt;
&lt;p&gt;In general, the number of possible combinations of $n$ objects in $k$ draws with replacement is ${n + k - 1 \choose k}$.&lt;/p&gt;
&lt;h3 id=&#34;summary&#34;&gt;Summary&lt;/h3&gt;
&lt;p&gt;We can summarize all the possible scenarios in a simple table.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;With Replacement&lt;/th&gt;
&lt;th&gt;Without Replacement&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Permutations&lt;/strong&gt; (order matters):&lt;/td&gt;
&lt;td&gt;$n^k$&lt;/td&gt;
&lt;td&gt;$\frac{n!}{(n-k)!} $&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Combinations&lt;/strong&gt; (order doesn&amp;rsquo;t matter):&lt;/td&gt;
&lt;td&gt;$ {n + k - 1 \choose k} \text{=}\frac{(n+k-1)!}{(n-1)!k!}  $&lt;/td&gt;
&lt;td&gt;$ {n \choose k} = \frac{n!}{(n-k)!k!} $&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Where $n$ is the number of objects and $k$ is the number of draws.&lt;/p&gt;
&lt;h2 id=&#34;example&#34;&gt;Example&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s explore together a more complex example to see how we can apply permutations and combinations to compute probabilities.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;There are four people in an elevator, four floors in the building, and each person exits at random. Find the probability that:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;all people exit at different floors&lt;/li&gt;
&lt;li&gt;all people exit at the same floor&lt;/li&gt;
&lt;li&gt;two get off at one floor and two get off at another&lt;/li&gt;
&lt;/ol&gt;
&lt;/blockquote&gt;
&lt;p&gt;I use the &lt;code&gt;factorial&lt;/code&gt; and &lt;code&gt;comb&lt;/code&gt; functions from the &lt;code&gt;math&lt;/code&gt; library.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from math import factorial, comb
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;practical-advice&#34;&gt;Practical Advice&lt;/h3&gt;
&lt;p&gt;Before we start, some &lt;strong&gt;practical advice&lt;/strong&gt;. What worked best &lt;em&gt;for me&lt;/em&gt; is to approach the question in the following way:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;what are the &lt;strong&gt;overall&lt;/strong&gt; events that we are considering?&lt;/li&gt;
&lt;li&gt;what are the &lt;strong&gt;positive&lt;/strong&gt; events that we are considering?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;And for both questions, I ask myself:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;does &lt;strong&gt;order&lt;/strong&gt; matter?&lt;/li&gt;
&lt;li&gt;is there &lt;strong&gt;replacement&lt;/strong&gt;?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Moreover, it is also very useful to &lt;strong&gt;restate the problem&lt;/strong&gt; in terms of objects and draws. For example, in this case, I can restate the problem as: &amp;ldquo;I am drawing a floor for each person&amp;rdquo;. This makes it clear whether or not there is replacement, i.e. whether or not I can draw the same floor for different persons.&lt;/p&gt;
&lt;h3 id=&#34;question-1&#34;&gt;Question 1&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;What is the probability that they all get off at different floors?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;Total&lt;/strong&gt; events: (from floors 4 permute 4, with replacement)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Positive&lt;/strong&gt; events: (from floors 4 permute 4, without replacement)&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;factorial(4) / 4**4
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;0.09375
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;question-2&#34;&gt;Question 2&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;What is the probability that they all exit at the same floor?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;Total&lt;/strong&gt; events: (from 4 floors permute 4, with replacement)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Positive&lt;/strong&gt; events: (from floors 4 choose 1)&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;4 / 4**4
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;0.015625
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;question-3&#34;&gt;Question 3&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;What is the probability that two get off at one floor and two at another?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;Total&lt;/strong&gt; events: (from floors 4 permute 4, with replacement)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Positive&lt;/strong&gt; events: (from 4 people choose 2, without replacement) * (from 4 floors choose 2, without replacement)&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;comb(4, 2) * comb(4, 2) / 4**4 
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;0.140625
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;practice-questions&#34;&gt;Practice Questions&lt;/h2&gt;
&lt;p&gt;Now it&amp;rsquo;s your time to shine! Here are some practice questions with solutions&lt;/p&gt;
&lt;h3 id=&#34;problem-1&#34;&gt;Problem 1&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;Suppose that you randomly draw 4 cards from a deck of 52 cards. What is the probability of getting 2 spades and 2 clubs?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;Total&lt;/strong&gt; events: (from 52 cards choose 4, without replacement)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Positive&lt;/strong&gt; events: (from 13 cards choose 2) * (from 13 cards choose 2)&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;comb(13, 2) * comb(13, 2) / comb(52, 4)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;0.02247298919567827
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;problem-2&#34;&gt;Problem 2&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;Suppose you draw 5 cards without replacement from a standard deck of 52 playing cards. What is the probability of guessing all 5 cards in any order?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;Total&lt;/strong&gt; events: (from 52 cards choose 5, without replacement)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Positive&lt;/strong&gt; events: 1&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;1 / comb(52, 5)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;3.8476929233231754e-07
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;problem-3&#34;&gt;Problem 3&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;Suppose you draw 3 cards without replacement from a standard deck of 52 playing cards. What is the probability of guessing all 3 cards in the correct order?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;Total&lt;/strong&gt; events: (from 52 cards permute 3, without replacement)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Positive&lt;/strong&gt; events: 1&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;1 / (factorial(52) / factorial(3))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;7.43879958514289e-68
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;problem-4&#34;&gt;Problem 4&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;Suppose you draw 5 cards without replacement from a standard deck of 52 playing cards. What is the probability of guessing 3 of them (out of 3 guesses), in any order?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;Total&lt;/strong&gt; events: (from 52 cards permute 3, without replacement)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Positive&lt;/strong&gt; events: (from 5 cards permute 3, without replacement)&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;(factorial(5) / factorial(3)) / (factorial(52) / factorial(5))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;2.9755198340571564e-65
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;problem-5&#34;&gt;Problem 5&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;A 4 digit PIN is selected. What is the probability that there are no repeated digits?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;Total&lt;/strong&gt; events: (from 10 permute 4, with replacement)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Positive&lt;/strong&gt; events: (from 10 permute 4, without replacement)&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;(factorial(10) / factorial(6)) / 10**4 
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;0.504
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;problem-6&#34;&gt;Problem 6&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;In a certain state’s lottery, 48 balls numbered 1 through 48 are placed in a machine and 6 of them are drawn at random. If the 6 numbers drawn match the numbers that a player had chosen, the player wins 1,000,000. In this lottery, the order the numbers are drawn in doesn’t matter. Compute the probability that you win the million-dollar prize if you purchase a single lottery ticket.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;Total&lt;/strong&gt; events: (from 48 choose 6, without replacement)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Positive&lt;/strong&gt; events: 1&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;1 / comb(48, 6)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;8.148955075788542e-08
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;problem-7&#34;&gt;Problem 7&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;In a certain state’s lottery, 48 balls numbered 1 through 48 are placed in a machine and 6 of them are drawn at random. If five of the six numbers drawn match the numbers that a player has chosen, the player wins a second prize of 1,000. Compute the probability that you win the second prize if you purchase a single lottery ticket.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;Total&lt;/strong&gt; events: (from 48 choose 6, without replacement)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Positive&lt;/strong&gt; events: (from 6 choose 5, without replacement)&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;comb(6, 5) / comb(48, 5)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;3.504050682589073e-06
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;problem-8&#34;&gt;Problem 8&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;Compute the probability of randomly drawing five cards from a deck and getting exactly one Ace.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;Total&lt;/strong&gt; events: (from 52 choose 5, without replacement)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Positive&lt;/strong&gt; events: (from 4 aces choose 1) * (from 48 cards that are not aces choose 4)&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;4 * comb(48, 4) / comb(52, 5)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;0.2994736356080894
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;problem-9&#34;&gt;Problem 9&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;Compute the probability of randomly drawing five cards from a deck and getting exactly two Aces.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;Total&lt;/strong&gt; events: (from 52 choose 5, without replacement)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Positive&lt;/strong&gt; events: (from 4 aces choose 2) * (from 48 cards that are not aces choose 3)&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;comb(4,2) * comb(48, 3) / comb(52, 5)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;0.03992981808107859
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;problem-10&#34;&gt;Problem 10&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;Suppose you have 3 people in a room. What is the probability that there is at least one shared birthday?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;Total&lt;/strong&gt; events: (from 365 days permute 3, with replacement)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Negative&lt;/strong&gt; events: (from 365 days permute 3, without replacement)&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;1 - (365 * 364 * 363) / (365**3)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;0.008204165884781345
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;problem-11&#34;&gt;Problem 11&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;Given a class of 12 girls and 10 boys, what is the probability that a committee of five, chosen at random from
the class, consists only of girls?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;Total&lt;/strong&gt; events: (from 22 choose 5, without replacement)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Positive&lt;/strong&gt; events: (from 12 choose 5, without replacement)&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;comb(12, 5) / comb(22, 5)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;0.03007518796992481
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;Permutation and combination questions are a classic in data science questions (unfortunately).&lt;/p&gt;
&lt;h3 id=&#34;code&#34;&gt;Code&lt;/h3&gt;
&lt;p&gt;You can find the original Jupyter Notebook here:&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/combperm.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/matteocourthoud/Blog-Posts/blob/main/combperm.ipynb&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Double Debiased Machine Learning (part 2)</title>
      <link>https://matteocourthoud.github.io/post/pds/</link>
      <pubDate>Sun, 05 Jun 2022 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/post/pds/</guid>
      <description>&lt;p&gt;In the &lt;a href=&#34;https://towardsdatascience.com/eb767a59975b&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;previous part of this blog post&lt;/a&gt;, we have seen how pre-testing can distort inference, i.e., how selecting control variables depending on their statistical significance results in wrong confidence intervals for the variable of interest. This bias is generally called &lt;strong&gt;regularization bias&lt;/strong&gt; and also emerges in machine learning algorithms.&lt;/p&gt;
&lt;p&gt;In blog post, we are going to explore a solution to the simple selection example, &lt;strong&gt;post-double selection&lt;/strong&gt;, and a more general approach when we have many control variables and we do not want to assume linearity, &lt;strong&gt;double-debiased machine learning&lt;/strong&gt;.&lt;/p&gt;
&lt;h2 id=&#34;recap&#34;&gt;Recap&lt;/h2&gt;
&lt;p&gt;To better understand the source of the bias, in the first part of this post, we have explored the example of a firm that is interested in testing the effectiveness of an a campaign. The firm has information on its current ad spending and on the level of sales. The problem arises because the firm is uncertain on whether it should condition its analysis on the level of past sales.&lt;/p&gt;
&lt;p&gt;The following &lt;a href=&#34;https://towardsdatascience.com/b63dc69e3d8c&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Directed Acyclic Graph (DAG)&lt;/strong&gt;&lt;/a&gt; summarizes the data generating process.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-mermaid&#34;&gt;flowchart LR
classDef included fill:#DCDCDC,stroke:#000000,stroke-width:2px;
classDef excluded fill:#ffffff,stroke:#000000,stroke-width:2px;
classDef unobserved fill:#ffffff,stroke:#000000,stroke-width:2px,stroke-dasharray: 5 5;

D((ad spend))
Z((past sales))
Y((sales))

D --&amp;gt; Y
Z -- ??? --&amp;gt; Y
Z --&amp;gt; D

class D,Y included;
class Z excluded;

linkStyle 0 stroke:#00ff00,stroke-width:4px;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I import the data generating process &lt;code&gt;dgp_tbd()&lt;/code&gt; from &lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/src/dgp.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;src.dgp&lt;/code&gt;&lt;/a&gt; and some plotting functions and libraries from &lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/src/utils.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;src.utils&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;%matplotlib inline
%config InlineBackend.figure_format = &#39;retina&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from src.utils import *
from src.dgp import dgp_pretest

df = dgp_pretest().generate_data()
df.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;ads&lt;/th&gt;
      &lt;th&gt;sales&lt;/th&gt;
      &lt;th&gt;past_sales&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;16.719800&lt;/td&gt;
      &lt;td&gt;19.196620&lt;/td&gt;
      &lt;td&gt;6.624345&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;7.732222&lt;/td&gt;
      &lt;td&gt;9.287491&lt;/td&gt;
      &lt;td&gt;4.388244&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;10.923469&lt;/td&gt;
      &lt;td&gt;11.816906&lt;/td&gt;
      &lt;td&gt;4.471828&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;8.457062&lt;/td&gt;
      &lt;td&gt;9.024376&lt;/td&gt;
      &lt;td&gt;3.927031&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;13.085146&lt;/td&gt;
      &lt;td&gt;12.814823&lt;/td&gt;
      &lt;td&gt;5.865408&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;We have data on $1000$ different markets, in which we observe current &lt;code&gt;sales&lt;/code&gt;, the amount spent in &lt;code&gt;advertisement&lt;/code&gt; and &lt;code&gt;past sales&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;We want to understand &lt;code&gt;ads&lt;/code&gt; spending is effective in increasing &lt;code&gt;sales&lt;/code&gt;. One possibility is to regress the latter on the former, using the following regression model, also called the &lt;strong&gt;short model&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;$$
\text{sales} = \alpha \cdot \text{ads} + \varepsilon
$$&lt;/p&gt;
&lt;p&gt;Should we also include &lt;code&gt;past sales&lt;/code&gt; in the regression? Then the regression model would be the following, also called &lt;strong&gt;long model&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;$$
\text{sales} = \alpha \cdot \text{ads} + \beta \cdot \text{past sales} + \varepsilon
$$&lt;/p&gt;
&lt;p&gt;One naive approach would be to &lt;strong&gt;let the data decide&lt;/strong&gt;: we could run the second regression and, if the effect of &lt;code&gt;past sales&lt;/code&gt;, $\beta$, is statistically significant, we are good with the long model, otherwise we run the short model. This procedure is called &lt;strong&gt;pre-testing&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;The problem with this procedure is that it introduces a bias that is called &lt;strong&gt;regularization or pre-test bias&lt;/strong&gt;. Pre-testing ensures that this bias is small enough not to distort the estimated coefficient. However, it does not ensure that it is small enough not to distort the confidence intervals around the estimated coefficient.&lt;/p&gt;
&lt;p&gt;Is there a solution? Yes!&lt;/p&gt;
&lt;h2 id=&#34;post-double-selection&#34;&gt;Post-Double Selection&lt;/h2&gt;
&lt;p&gt;The solution is called &lt;strong&gt;post-double selection&lt;/strong&gt;. The method was first introduced in &lt;a href=&#34;https://academic.oup.com/restud/article-abstract/81/2/608/1523757&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Belloni, Chernozhukov, Hansen (2014)&lt;/a&gt; and later expanded in a variety of papers.&lt;/p&gt;
&lt;p&gt;The authors assume the following &lt;strong&gt;data generating process&lt;/strong&gt;:&lt;/p&gt;
&lt;p&gt;$$
y = \alpha D + \beta X + u
\newline
D = \delta X + v
$$&lt;/p&gt;
&lt;p&gt;In our example, $Y$ corresponds to &lt;code&gt;sales&lt;/code&gt;, $D$ corresponds to &lt;code&gt;ads&lt;/code&gt;, $X$ corresponds to &lt;code&gt;past_sales&lt;/code&gt; and the effect of interest is $\alpha$. In our example, $X$ is 1-dimensional for simplicity, but generally we are interested in cases where X is high-dimensional, potentially even having more dimensions than the number of observations. In that case, variable selection is &lt;strong&gt;essential&lt;/strong&gt; in linear regression since we cannot have more features than variables (the OLS coefficients are not uniquely determined anymore).&lt;/p&gt;
&lt;p&gt;Post-double selection consists in the following procedure.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Reduced Form&lt;/strong&gt; selection: lasso $Y$ on $X$. Select the statistically significant variables in the set $S_{RF} \subseteq X$&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;First Stage&lt;/strong&gt; selection: regress $D$ on $X$. Select the statistically significant variables in the set $S_{FS} \subseteq X$&lt;/li&gt;
&lt;li&gt;Regress $Y$ on $D$ and the &lt;strong&gt;union&lt;/strong&gt; of the selected variables in the first two steps, $S_{FS} \cup S_{RF}$&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The authors show that this procedure produces confidence intervals for the coefficient of interest $\alpha$ that have the correct coverage, i.e. the correct probability of type 1 error.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Note (1)&lt;/strong&gt;: this procedure is always less parsimonious, in terms of variable selection, than pre-testing. In fact, we still select all the variables we would have selected with pre-testing but, in the first stage, we might select additional variables.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Note (2)&lt;/strong&gt;: the terms &lt;em&gt;first stage&lt;/em&gt; and &lt;em&gt;reduced form&lt;/em&gt; come from the &lt;a href=&#34;https://en.wikipedia.org/wiki/Instrumental_variables_estimation&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;intrumental variables&lt;/a&gt; literature in econometrics. Indeed, the first application of post-double selection was to select instrumental variables in &lt;a href=&#34;https://onlinelibrary.wiley.com/doi/abs/10.3982/ECTA9626&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Belloni, Chen, Chernozhukov, Hansen (2012)&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Note (3)&lt;/strong&gt;: the name post-double selection comes from the fact that now we are not performing variable selection once but &lt;em&gt;twice&lt;/em&gt;.&lt;/p&gt;
&lt;h3 id=&#34;intuition&#34;&gt;Intuition&lt;/h3&gt;
&lt;p&gt;The idea behind post-double selection is: bound the &lt;a href=&#34;https://towardsdatascience.com/344ac1477699&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;omitted variables bias&lt;/a&gt;. In case you are not familiar with it, I wrote a separate &lt;a href=&#34;https://towardsdatascience.com/344ac1477699&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;blog post on omitted variable bias&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;In our setting, we can express the omitted variable bias as&lt;/p&gt;
&lt;p&gt;$$
\text{OVB} = \beta \delta \qquad \text{ where } \qquad \beta := \frac{Cov(X, Y)}{Var(X)}, \quad \delta := \frac{Cov(D, X)}{Var(D)}
$$&lt;/p&gt;
&lt;p&gt;As we can see, the omitted variable bias comes from the product of two quantities related to the omitted variable $X$:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Its partial correlation with the outcome $Y$, $\beta$&lt;/li&gt;
&lt;li&gt;Its partial correlation with the variable of interest $D$, $\delta$&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;With pre-testing, we ensure that the partial correlation between $X$ the outcome $Y$, $\beta$, is &lt;strong&gt;small&lt;/strong&gt;. In fact, we omit $Z$ when we shouldn&amp;rsquo;t (i.e. we commit a type 2 error) rarely. What do &lt;em&gt;small&lt;/em&gt; and &lt;em&gt;rarely&lt;/em&gt; mean?&lt;/p&gt;
&lt;p&gt;When we are selecting a variable because of its significance, we ensure that it dimension is smaller than $\frac{c}{\sqrt{n}}$ for some number $c$, where $n$ is the sample size.&lt;/p&gt;
&lt;p&gt;Therefore, with pre-testing, we ensure that, no matter what the value of $\delta$ is, the dimension of the bias is smaller than $\frac{c}{\sqrt{n}}$ which means that it converges to zero for sufficiently large $n$. This is why the pre-testing estimator is still &lt;strong&gt;consistent&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;However, in order for our confidence intervals to have the right coverage, this is &lt;strong&gt;not enough&lt;/strong&gt;. In practice, we need the bias to converge to zero &lt;strong&gt;faster&lt;/strong&gt; than $\frac{1}{\sqrt{n}}$. Why?&lt;/p&gt;
&lt;p&gt;To get an &lt;strong&gt;intuition&lt;/strong&gt; for this result, we need to turn to the &lt;a href=&#34;https://en.wikipedia.org/wiki/Central_limit_theorem&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Central Limit Theorem&lt;/a&gt;. The CLT tells us that for large $n$ the distribution of the sample average of a random variable $X$ converges to a normal distribution with mean $\mu$ and standard deviation $\frac{\sigma}{\sqrt{n}}$, where $\mu$ and $\sigma$ are the mean and standard deviation of $X$. To do inference, we usually apply the Central Limit Theorem to our estimator to get its asymptotic distribution, which in turn allows us to build confidence intervals (using the mean and the standard deviation). Therefore, if the bias is not sensibly smaller than the standard deviation of the estimator, the confidence intervals are going to be wrong. Therefore, we need the bias to converge to zero &lt;strong&gt;faster&lt;/strong&gt; than the standard deviation, i.e. faster than $\frac{1}{\sqrt{n}}$.&lt;/p&gt;
&lt;p&gt;In our setting, the omitted variable bias is $\beta \gamma$ and we want it to converge to zero faster than $\frac{1}{\sqrt{n}}$.  Post-double selection guarantees that&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Reduced form&lt;/em&gt; selection (pre-testing): any &amp;ldquo;missing&amp;rdquo; variable $j$ has $|\beta_j| \leq \frac{c}{\sqrt{n}}$&lt;/li&gt;
&lt;li&gt;&lt;em&gt;First stage&lt;/em&gt; selection (additional): any &amp;ldquo;missing&amp;rdquo; variable $j$ has $|\delta_j| \leq \frac{c}{\sqrt{n}}$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;As a consequence, as long as the number of omitted variables is finite, the omitted variable bias is going to converge to zero at a rate $\frac{1}{n}$, which is faster than $\frac{1}{\sqrt{n}}$. &lt;strong&gt;Problem solved&lt;/strong&gt;!&lt;/p&gt;
&lt;h3 id=&#34;application&#34;&gt;Application&lt;/h3&gt;
&lt;p&gt;Let&amp;rsquo;s now go back to our example and test the post-double selection procedure. In practice, we want to do the following:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;First Stage&lt;/strong&gt; selection: regress &lt;code&gt;ads&lt;/code&gt; on &lt;code&gt;past_sales&lt;/code&gt;. Check if &lt;code&gt;past_sales&lt;/code&gt; is statistically significant&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Reduced Form&lt;/strong&gt; selection: regress &lt;code&gt;sales&lt;/code&gt; on &lt;code&gt;past_sales&lt;/code&gt;. Check if &lt;code&gt;past_sales&lt;/code&gt; is statistically significant&lt;/li&gt;
&lt;li&gt;Regress &lt;code&gt;sales&lt;/code&gt; on &lt;code&gt;ads&lt;/code&gt; and include &lt;code&gt;past_sales&lt;/code&gt; &lt;strong&gt;only if&lt;/strong&gt; it was significant in &lt;em&gt;either&lt;/em&gt; one of the two previous regressions&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;I update the &lt;code&gt;pre_test&lt;/code&gt; function from the first part of the post to compute also the post-double selection estimator.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def pre_test(d=&#39;ads&#39;, y=&#39;sales&#39;, x=&#39;past_sales&#39;, K=1000, **kwargs):
    
    # Init
    alphas = pd.DataFrame({&#39;Long&#39;: np.zeros(K), 
             &#39;Short&#39;: np.zeros(K), 
             &#39;Pre-test&#39;: np.zeros(K),
             &#39;Post-double&#39;: np.zeros(K)})

    # Loop over simulations
    for k in range(K):
        
        # Generate data
        df = dgp_pretest().generate_data(seed=k, **kwargs)
        
        # Compute coefficients
        alphas[&#39;Long&#39;][k] = smf.ols(f&#39;{y} ~ {d} + {x}&#39;, df).fit().params[1]
        alphas[&#39;Short&#39;][k] = smf.ols(f&#39;{y} ~ {d}&#39;, df).fit().params[1]
    
        # Compute significance of beta and gamma
        p_value_ydx = smf.ols(f&#39;{y} ~ {d} + {x}&#39;, df).fit().pvalues[2]
        p_value_yx = smf.ols(f&#39;{y} ~ {x}&#39;, df).fit().pvalues[1]
        p_value_dx = smf.ols(f&#39;{d} ~ {x}&#39;, df).fit().pvalues[1]
        
        # Select pre-test specification based on regression of y on d and x
        if p_value_ydx&amp;lt;0.05:
            alphas[&#39;Pre-test&#39;][k] = alphas[&#39;Long&#39;][k]
        else:
            alphas[&#39;Pre-test&#39;][k] = alphas[&#39;Short&#39;][k]
            
        # Select post-double specification based on regression of y on d and x
        if p_value_yx&amp;lt;0.05 or p_value_dx&amp;lt;0.05:
            alphas[&#39;Post-double&#39;][k] = alphas[&#39;Long&#39;][k]
        else:
            alphas[&#39;Post-double&#39;][k] = alphas[&#39;Short&#39;][k]
    
    return alphas
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;alphas = pre_test()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can now plot the distributions (over simulations) of the estimated coefficients.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def plot_alphas(alphas, true_alpha):
    
    # Init plot
    K = len(alphas.columns)
    fig, axes = plt.subplots(1, K, figsize=(4*K, 5), sharey=True, sharex=True)

    # Make one plot for each set of coefficients
    for i, key in enumerate(alphas.columns):
        axes[i].hist(alphas[key].values, bins=30, lw=.1, color=f&#39;C{int(i==3)*2}&#39;)
        axes[i].set_title(key)
        axes[i].axvline(true_alpha, c=&#39;r&#39;, ls=&#39;--&#39;)
        legend_text = [rf&#39;$\alpha=${true_alpha}&#39;, rf&#39;$\hat \alpha=${np.mean(alphas[key]):.4f}&#39;]
        axes[i].legend(legend_text, prop={&#39;size&#39;: 10}, loc=&#39;upper right&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plot_alphas(alphas, true_alpha=1)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/pds_17_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;As we can see, the post-double selection estimator always correctly selects the long regression and therefore has the correct distribution.&lt;/p&gt;
&lt;h3 id=&#34;double-checks&#34;&gt;Double-checks&lt;/h3&gt;
&lt;p&gt;In the last post, we ran some simulations in order to investigate when pre-testing bias emerges. We saw that pre-testing is a problem for&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Small sample sizes $n$&lt;/li&gt;
&lt;li&gt;Intermediate values of $\beta$&lt;/li&gt;
&lt;li&gt;When the value of $\beta$ depends on the sample size&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Let&amp;rsquo;s check that post-double selection removes regularization bias in &lt;strong&gt;all&lt;/strong&gt; the previous cases.&lt;/p&gt;
&lt;p&gt;First, let&amp;rsquo;s simulate the distribution of the post-double selection estimator $\hat \alpha_{postdouble}$ for different sample sizes.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;Ns = [100,300,1000,3000]
alphas = {f&#39;N = {n:.0f}&#39;:  pre_test(N=n) for n in Ns}
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def compare_alphas(alphas, true_alpha):
    
    # Init plot
    fig, axes = plt.subplots(1, len(alphas), figsize=(4*len(alphas), 5), sharey=True, sharex=True)

    # Make one plot for each set of coefficients
    for i, key in enumerate(alphas.keys()):
        axes[i].hist(alphas[key][&#39;Pre-test&#39;], bins=30, lw=.1, alpha=0.5)
        axes[i].hist(alphas[key][&#39;Post-double&#39;], bins=30, lw=.1, alpha=0.5, color=&#39;C2&#39;)
        axes[i].set_title(key)
        axes[i].axvline(true_alpha, c=&#39;r&#39;, ls=&#39;--&#39;)
        axes[i].legend([rf&#39;$\alpha=${true_alpha}&#39;, &#39;Pre-test&#39;, &#39;Post-double&#39;], 
                       prop={&#39;size&#39;: 10}, loc=&#39;upper right&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;compare_alphas(alphas, true_alpha=1)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/pds_23_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;For small samples, the distribution of the pre-testing estimator is not normal but rather bimodal. From the plots we can see that the post-double estimator is gaussian also in small sample sizes.&lt;/p&gt;
&lt;p&gt;Now we repeat the same exercise, but for different values of $\beta$, the coefficient of &lt;code&gt;past_sales&lt;/code&gt; on &lt;code&gt;sales&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;betas = 0.3 * np.array([0.1,0.3,1,3])
alphas = {f&#39;beta = {b:.2f}&#39;: pre_test(b=b) for b in betas}
compare_alphas(alphas, true_alpha=1)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/pds_25_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Again, the post-double selection estimator has a gaussian distribution irrespectively of the value of $\beta$, while he pre-testing estimator suffers from regularization bias.&lt;/p&gt;
&lt;p&gt;For the last simulation, we change both the coefficient and the sample size at the same time.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;betas = 0.3 * 30 / np.sqrt(Ns)
alphas = {f&#39;N = {n:.0f}&#39;:  pre_test(b=b, N=n) for n,b in zip(Ns,betas)}
compare_alphas(alphas, true_alpha=1)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/pds_27_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Also in this last case, the post-double selection estimator performs well and inference is not distorted.&lt;/p&gt;
&lt;h2 id=&#34;double-debiased-machine-learning&#34;&gt;Double Debiased Machine Learning&lt;/h2&gt;
&lt;p&gt;So far, we only have analyzed a linear, univariate example. What happens if the dimension of $X$ increases and we do not know the functional form through which $X$ affects $Y$ and $D$? In these cases, we can use machine learning algorithms to uncover these high-dimensional non-linear relationships.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://academic.oup.com/ectj/article/21/1/C1/5056401&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Chernozhukov, Chetverikov, Demirer, Duflo, Hansen, Newey, and Robins (2018)&lt;/a&gt; investigate this setting. In particular, the authors consider the following partially linear model.&lt;/p&gt;
&lt;p&gt;$$
Y = \alpha D + g(X) + u \
D = m(X) + v
$$&lt;/p&gt;
&lt;p&gt;where $Y$ is the outcome variable, $D$ is the treatment to interest and $X$ is a potentially high-dimensional set of control variables.&lt;/p&gt;
&lt;h3 id=&#34;naive-approach&#34;&gt;Naive approach&lt;/h3&gt;
&lt;p&gt;A naive approach to estimation of $\alpha$ using machine learning methods would be, for example, to construct a sophisticated machine learning estimator for learning the regression function $\alpha D$ + $g(X)$.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Split the sample in two: main sample and auxiliary sample [why? see note below]&lt;/li&gt;
&lt;li&gt;Use the auxiliary sample to estimate $\hat g(X)$&lt;/li&gt;
&lt;li&gt;Use the main sample to compute the orthogonalized component of $Y$ on $X$: $\ \hat u = Y - \hat{g} (X)$&lt;/li&gt;
&lt;li&gt;Use the main sample to estimate the residualized OLS estimator from regressing $\hat u$ on $D$&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;$$
\hat \alpha = \left( D&amp;rsquo; D \right) ^{-1} D&amp;rsquo; \hat u
$$&lt;/p&gt;
&lt;p&gt;This estimator is going to have &lt;strong&gt;two problems&lt;/strong&gt;:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Slow rate of convergence, i.e. slower than $\sqrt(n)$&lt;/li&gt;
&lt;li&gt;It will be biased because we are employing high dimensional regularized estimators (e.g. we are doing variable selection)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;Note (1)&lt;/strong&gt;: so far we have not talked about it, but variable selection procedure also introduce another type of bias: &lt;strong&gt;overfitting bias&lt;/strong&gt;. This bias emerges because of the fact that the sample used to select the variables is the same that is used to estimate the coefficient of interest. This bias is &lt;strong&gt;easily accounted for&lt;/strong&gt; with sample splitting: using different sub-samples for the selection and the estimation procedures.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Note (2)&lt;/strong&gt;: why can we use the residuals from step 3 to estimate $\alpha$ in step 4? Because of the &lt;a href=&#34;https://towardsdatascience.com/59f801eb3299&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Frisch-Waugh-Lovell theorem&lt;/a&gt;. If you are not familiar with it, I have written a &lt;a href=&#34;https://towardsdatascience.com/59f801eb3299&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;blog post on the Frisch-Waugh-Lovell theorem here&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;orthogonalization&#34;&gt;Orthogonalization&lt;/h3&gt;
&lt;p&gt;Now consider a second construction that employs an orthogonalized formulation obtained by directly partialling out the effect of $X$ from $D$ to obtain the orthogonalized regressor $v = D − m(X)$.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Split the sample in two: main sample and auxiliary sample&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Use the auxiliary sample to estimate $\hat g(X)$ from&lt;/p&gt;
&lt;p&gt;$$
Y = \alpha D + g(X) + u \
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Use the auxiliary sample to estimate $\hat m(X)$ from&lt;/p&gt;
&lt;p&gt;$$
D = m(X) + v
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Use the main sample to compute the orthogonalized component of $D$ on $X$ as&lt;/p&gt;
&lt;p&gt;$$
\hat v = D - \hat m(X)
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Use the main sample to estimate the double-residualized OLS estimator as&lt;/p&gt;
&lt;p&gt;$$
\hat \alpha = \left( \hat{v}&amp;rsquo; D \right) ^{-1} \hat{v}&amp;rsquo; \left( Y - \hat g(X) \right)
$$&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The estimator is &lt;strong&gt;root-N consistent&lt;/strong&gt;! This means that not only the estimator converges to the true value as the sample sizes increases (i.e. it&amp;rsquo;s consistent), but also its standard deviation does (i.e. it&amp;rsquo;s root-N consistent).&lt;/p&gt;
&lt;p&gt;However, the estimator still has a lower rate of convergence because of sample splitting. The problem is solved by inverting the split sample, re-estimating the coefficient and averaging the two estimates. Note that this procedure is valid since the two estimates are independent by the sample splitting procedure.&lt;/p&gt;
&lt;h3 id=&#34;a-cautionary-tale&#34;&gt;A Cautionary Tale&lt;/h3&gt;
&lt;p&gt;Before we conclude, I have to mention a recent research paper by &lt;a href=&#34;https://arxiv.org/abs/2108.11294&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Hünermund, Louw, and Caspi (2022)&lt;/a&gt;, in which the authors show that double-debiased machine learning can easily &lt;strong&gt;backfire&lt;/strong&gt;, if we apply blindly.&lt;/p&gt;
&lt;p&gt;The problem is related to &lt;strong&gt;bad control variables&lt;/strong&gt;. If you have never heard this term, I have written an introductory &lt;a href=&#34;https://towardsdatascience.com/b63dc69e3d8c&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;blog post on good and bad control variables here&lt;/a&gt;. In short, conditioning the analysis on additional features is not always good for causal inference. Depending on the setting, there might exist variables that we want to leave out of our analysis since their &lt;strong&gt;inclusion&lt;/strong&gt; can bias the coefficient of interest, preventing a causal interpretation. The simplest example is variables that are common outcomes, of both the treatment $D$ and outcome variable $Y$.&lt;/p&gt;
&lt;p&gt;The double-debiased machine learning model implicitly assumes that the control variables $X$ are (weakly) &lt;strong&gt;common causes&lt;/strong&gt; to both the outcome $Y$ and the treatment $D$. If this is the case, and no further mediated/indirect relationship exists between $X$ and $Y$, there is no problem. However, if, for example, some variable among the controls $X$ is a common effect instead of a common cause, its inclusion will bias the coefficient of interest. Moreover, this variable is likely to be highly correlated either with the outcome $Y$ or with the treatment $D$. In the latter case, this implies that post-double selection might include it in cases in which simple selection would have not. Therefore, in presence of bad control variables, doule-debiased machine learning might be &lt;strong&gt;even worse&lt;/strong&gt; than simple pre-testing.&lt;/p&gt;
&lt;p&gt;In short, as for any method, it is &lt;strong&gt;crucial&lt;/strong&gt; to have a clear understanding of the method&amp;rsquo;s assumptions and to always check for potential violations.&lt;/p&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;In this post, we have seen how to use post-double selection and, more generally, double debiased machine learning to get rid of an important source of bias: regularization bias.&lt;/p&gt;
&lt;p&gt;This contribution by Victor Chernozhukov and co-authors has been undoubtedly one of the most relevant advances in causal inferences in the last decade. It is now widely employed in the industry and included in the most used causal inference packages, such as &lt;a href=&#34;https://econml.azurewebsites.net/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;EconML&lt;/a&gt; (Microsoft) and &lt;a href=&#34;https://causalml.readthedocs.io/en/latest/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;causalml&lt;/a&gt; (Uber).&lt;/p&gt;
&lt;p&gt;If you (understandably) feel the need for more material on double-debiased machine learning, but you do not feel like reading academic papers (also very understandable), here is a good compromise.&lt;/p&gt;
&lt;br&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/eHOjmyoPCFU&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;br&gt;
&lt;p&gt;In this video lecture, Victor Chernozhukov himself presents the idea. The video lecture is relatively heavy on math and statistics, but you cannot get a more qualified and direct source than this!&lt;/p&gt;
&lt;h3 id=&#34;references&#34;&gt;References&lt;/h3&gt;
&lt;p&gt;[1] A. Belloni, D. Chen, V. Chernozhukov, C. Hansen, &lt;a href=&#34;https://onlinelibrary.wiley.com/doi/abs/10.3982/ECTA9626&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Sparse Models and Methods for Optimal Instruments With an Application to Eminent Domain&lt;/a&gt; (2012), &lt;em&gt;Econometrica&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;[2] A. Belloni, V. Chernozhukov, C. Hansen, &lt;a href=&#34;https://academic.oup.com/restud/article-abstract/81/2/608/1523757&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Inference on treatment effects after selection among high-dimensional controls&lt;/a&gt; (2014), &lt;em&gt;The Review of Economic Studies&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;[3] V. Chernozhukov, D. Chetverikov, M. Demirer, E. Duflo, C. Hansen, W. Newey, J. Robins, &lt;a href=&#34;https://academic.oup.com/ectj/article/21/1/C1/5056401&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Double/debiased machine learning for treatment and structural parameters&lt;/a&gt; (2018), &lt;em&gt;The Econometrics Journal&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;[4] P. Hünermund, B. Louw, I. Caspi, &lt;a href=&#34;https://arxiv.org/abs/2108.11294&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Double Machine Learning and Automated Confounder Selection - A Cautionary Tale&lt;/a&gt; (2022), &lt;em&gt;working paper&lt;/em&gt;.&lt;/p&gt;
&lt;h3 id=&#34;related-articles&#34;&gt;Related Articles&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/eb767a59975b&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Double Debiased Machine Learning (part 1)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/344ac1477699&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Understanding Omitted Variable Bias&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/59f801eb3299&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Understanding The Frisch-Waugh-Lovell Theorem&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/b63dc69e3d8c&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;DAGs and Control Variables&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;code&#34;&gt;Code&lt;/h3&gt;
&lt;p&gt;You can find the original Jupyter Notebook here:&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/pds.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/matteocourthoud/Blog-Posts/blob/main/pds.ipynb&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Double Debiased Machine Learning (part 1)</title>
      <link>https://matteocourthoud.github.io/post/pretest/</link>
      <pubDate>Sat, 04 Jun 2022 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/post/pretest/</guid>
      <description>&lt;p&gt;&lt;em&gt;Causal inference, machine learning and regularization bias&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;In causal inference, we often estimate causal effects by conditioning the analysis on other variables. We usually refer to these variables as &lt;strong&gt;control variables&lt;/strong&gt; or &lt;strong&gt;confounders&lt;/strong&gt;. In randomized control trials or AB tests, conditioning can increase the power of the analysis, by reducing imbalances that have emerged despite randomization. However, conditioning is even more important in observational studies, where, absent randomization, it might be &lt;a href=&#34;https://towardsdatascience.com/b63dc69e3d8c&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;essential to recover causal effects&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;When we have many control variables, we might want to &lt;strong&gt;select the most relevant ones&lt;/strong&gt;, ppossibly capturing nonlinearities and interactions. Machine learning algorithms are perfect for this task. However, in these cases, we are introducing a bias that is called &lt;strong&gt;regularization or pre-test, or feature selection bias&lt;/strong&gt;. In this and the next blog post, I try to explain the source of the bias and a very poweful solution called &lt;strong&gt;double debiased machine learning&lt;/strong&gt;, which has been probably one of the most relevant advancement at the intersection of machine learning and causal inference of the last decade.&lt;/p&gt;
&lt;h2 id=&#34;pre-testing&#34;&gt;Pre-Testing&lt;/h2&gt;
&lt;p&gt;Since this is a complex topic, let&amp;rsquo;s start with a simple example.&lt;/p&gt;
&lt;p&gt;Suppose we were a firm and we are interested in the &lt;strong&gt;effect of advertisement spending on revenue&lt;/strong&gt;: is advertisement worth the money? There are also a lot of other things that might influence sales, therefore, we are thinking of controlling for past sales in the analysis, in order to increase the power of our analysis.&lt;/p&gt;
&lt;p&gt;Assume the data generating process can be represented with the following &lt;a href=&#34;https://towardsdatascience.com/b63dc69e3d8c&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Directed Acyclic Graph (DAG)&lt;/strong&gt;&lt;/a&gt;. If you are not familiar with DAGs, I have written a short &lt;a href=&#34;https://towardsdatascience.com/b63dc69e3d8c&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;introduction here&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-mermaid&#34;&gt;flowchart LR
classDef included fill:#DCDCDC,stroke:#000000,stroke-width:2px;
classDef excluded fill:#ffffff,stroke:#000000,stroke-width:2px;
classDef unobserved fill:#ffffff,stroke:#000000,stroke-width:2px,stroke-dasharray: 5 5;

D((ad spend))
Z((past sales))
Y((sales))

D --&amp;gt; Y
Z -- ??? --&amp;gt; Y
Z --&amp;gt; D

class D,Y included;
class Z excluded;

linkStyle 0 stroke:#00ff00,stroke-width:4px;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I import the data generating process &lt;code&gt;dgp_tbd()&lt;/code&gt; from &lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/src/dgp.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;src.dgp&lt;/code&gt;&lt;/a&gt; and some plotting functions and libraries from &lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/src/utils.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;src.utils&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;%matplotlib inline
%config InlineBackend.figure_format = &#39;retina&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from src.utils import *
from src.dgp import dgp_pretest

df = dgp_pretest().generate_data()
df.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;ads&lt;/th&gt;
      &lt;th&gt;sales&lt;/th&gt;
      &lt;th&gt;past_sales&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;16.719800&lt;/td&gt;
      &lt;td&gt;19.196620&lt;/td&gt;
      &lt;td&gt;6.624345&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;7.732222&lt;/td&gt;
      &lt;td&gt;9.287491&lt;/td&gt;
      &lt;td&gt;4.388244&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;10.923469&lt;/td&gt;
      &lt;td&gt;11.816906&lt;/td&gt;
      &lt;td&gt;4.471828&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;8.457062&lt;/td&gt;
      &lt;td&gt;9.024376&lt;/td&gt;
      &lt;td&gt;3.927031&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;13.085146&lt;/td&gt;
      &lt;td&gt;12.814823&lt;/td&gt;
      &lt;td&gt;5.865408&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;We have data on $1000$ different markets, in which we observe current &lt;code&gt;sales&lt;/code&gt;, the amount spent in &lt;code&gt;advertisement&lt;/code&gt; and &lt;code&gt;past sales&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;We want to understand &lt;code&gt;ads&lt;/code&gt; spending is effective in increasing &lt;code&gt;sales&lt;/code&gt;. One possibility is to regress the latter on the former, using the following regression model, also called the &lt;strong&gt;short model&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;$$
\text{sales} = \alpha \cdot \text{ads} + \varepsilon
$$&lt;/p&gt;
&lt;p&gt;Should we also include &lt;code&gt;past sales&lt;/code&gt; in the regression? Then the regression model would be the following, also called &lt;strong&gt;long model&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;$$
\text{sales} = \alpha \cdot \text{ads} + \beta \cdot \text{past sales} + \varepsilon
$$&lt;/p&gt;
&lt;p&gt;Since we are not sure whether to condition the analysis on &lt;code&gt;past sales&lt;/code&gt;, we could &lt;strong&gt;let the data decide&lt;/strong&gt;: we could run the second regression and, if the effect of &lt;code&gt;past sales&lt;/code&gt;, $\beta$, is statistically significant, we are good with the long model, otherwise we run the short model.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;smf.ols(&#39;sales ~ ads + past_sales&#39;, df).fit().summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
       &lt;td&gt;&lt;/td&gt;         &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt;  &lt;td&gt;    0.1405&lt;/td&gt; &lt;td&gt;    0.185&lt;/td&gt; &lt;td&gt;    0.758&lt;/td&gt; &lt;td&gt; 0.448&lt;/td&gt; &lt;td&gt;   -0.223&lt;/td&gt; &lt;td&gt;    0.504&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;ads&lt;/th&gt;        &lt;td&gt;    0.9708&lt;/td&gt; &lt;td&gt;    0.030&lt;/td&gt; &lt;td&gt;   32.545&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    0.912&lt;/td&gt; &lt;td&gt;    1.029&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;past_sales&lt;/th&gt; &lt;td&gt;    0.3381&lt;/td&gt; &lt;td&gt;    0.095&lt;/td&gt; &lt;td&gt;    3.543&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    0.151&lt;/td&gt; &lt;td&gt;    0.525&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;It seems that the effect of &lt;code&gt;past sales&lt;/code&gt; on current &lt;code&gt;sales&lt;/code&gt; is positive and significant. Therefore, we are happy with our specification and we conclude that the effect of &lt;code&gt;ads&lt;/code&gt; on &lt;code&gt;sales&lt;/code&gt; is positive and significant with a 95% confidence interval of $[0.912, 1.029]$.&lt;/p&gt;
&lt;h2 id=&#34;the-bias&#34;&gt;The Bias&lt;/h2&gt;
&lt;p&gt;There is an &lt;strong&gt;issue&lt;/strong&gt; with this procedure: we are not taking into account the fact that we have run a test to decide whether to include &lt;code&gt;past_sales&lt;/code&gt; in the regression. The fact that we have decided to include &lt;code&gt;past_sales&lt;/code&gt; because its coefficient is significant &lt;em&gt;does&lt;/em&gt; have an effect on the inference on the effect of &lt;code&gt;ads&lt;/code&gt; on &lt;code&gt;sales&lt;/code&gt;, $\alpha$.&lt;/p&gt;
&lt;p&gt;The best way to understand the problem is through &lt;strong&gt;simulations&lt;/strong&gt;. Since we have access to the data generating process &lt;code&gt;dgp_pretest()&lt;/code&gt; (unlike in real life), we can just test what would happen if we were repeating this procedure multiple times:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;We draw a new sample from the data generating process.&lt;/li&gt;
&lt;li&gt;We regress &lt;code&gt;sales&lt;/code&gt; on &lt;code&gt;ads&lt;/code&gt; and &lt;code&gt;past_sales&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;If the coefficient of &lt;code&gt;past_sales&lt;/code&gt; is significant at the 95% level, we keep $\hat \alpha_{long}$ from (2).&lt;/li&gt;
&lt;li&gt;Otherwise, we regress &lt;code&gt;sales&lt;/code&gt; on &lt;code&gt;ads&lt;/code&gt; only, and we keep that coefficient $\hat \alpha_{short}$.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;I write a &lt;code&gt;pre_test&lt;/code&gt; function to implement the procedure above. I also save the coefficients from both regressions, long and short, and the chosen one, called the &lt;strong&gt;pre-test coefficient&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Reminder&lt;/strong&gt;: we are pre-testing the effect of &lt;code&gt;past_sales&lt;/code&gt; on &lt;code&gt;sales&lt;/code&gt; but the coefficient of interest is the one of &lt;code&gt;ads&lt;/code&gt; on &lt;code&gt;sales&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def pre_testing(d=&#39;ads&#39;, y=&#39;sales&#39;, x=&#39;past_sales&#39;, K=1000, **kwargs):
    
    # Init
    alpha = {&#39;Long&#39;: np.zeros(K), &#39;Short&#39;: np.zeros(K), &#39;Pre-test&#39;: np.zeros(K)}

    # Loop over simulations
    for k in range(K):
        
        # Generate data
        df = dgp_pretest().generate_data(seed=k, **kwargs)
        
        # Compute coefficients
        alpha[&#39;Long&#39;][k] = smf.ols(f&#39;{y} ~ {d} + {x}&#39;, df).fit().params[1]
        alpha[&#39;Short&#39;][k] = smf.ols(f&#39;{y} ~ {d}&#39;, df).fit().params[1]
    
        # Compute significance of beta
        p_value = smf.ols(f&#39;{y} ~ {d} + {x}&#39;, df).fit().pvalues[2]
        
        # Select specification based on p-value
        if p_value&amp;lt;0.05:
            alpha[&#39;Pre-test&#39;][k] = alpha[&#39;Long&#39;][k]
        else:
            alpha[&#39;Pre-test&#39;][k] = alpha[&#39;Short&#39;][k]
    
    return alpha
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;alphas = pre_testing()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can now plot the distributions (over simulations) of the estimated coefficients.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def plot_alphas(alphas, true_alpha):
    
    # Init plot
    fig, axes = plt.subplots(1, len(alphas), figsize=(4*len(alphas), 5), sharey=True, sharex=True)

    # Make one plot for each set of coefficients
    for i, key in enumerate(alphas.keys()):
        axes[i].hist(alphas[key], bins=30, lw=.1)
        axes[i].set_title(key)
        axes[i].axvline(true_alpha, c=&#39;r&#39;, ls=&#39;--&#39;)
        legend_text = [r&#39;$\alpha=%.0f$&#39; % true_alpha, r&#39;$\hat \alpha=%.4f$&#39; % np.mean(alphas[key])]
        axes[i].legend(legend_text, prop={&#39;size&#39;: 10}, loc=&#39;upper right&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plot_alphas(alphas, true_alpha=1)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/pretest_16_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;In the plot above, I have depicted the estimated coefficients, across simulations, for the different regression specifications.&lt;/p&gt;
&lt;p&gt;As we can see from the first plot, if we were always running the &lt;strong&gt;long regression&lt;/strong&gt;, our estimator $\hat \alpha_{long}$ would be unbiased and normally distributed. However, if we were always running the &lt;strong&gt;short regression&lt;/strong&gt; (second plot), our estimator $\hat \alpha_{short}$ would be &lt;strong&gt;biased&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;pre-testing&lt;/strong&gt; procedure generates an estimator $\hat \alpha_{pretest}$ that is a mix of the two: most of the times we select the correct specification, the long regression, but sometimes the pre-test fails to reject the null hypothesis of no effect of &lt;code&gt;past sales&lt;/code&gt; on &lt;code&gt;sales&lt;/code&gt;, $H_0 : \beta = 0$, and we select the incorrect specification, running the short regression.&lt;/p&gt;
&lt;p&gt;Importantly, the pre-testing procedure &lt;strong&gt;does not generate a biased estimator&lt;/strong&gt;. As we can see in the last plot, the estimated coefficient is very close to the true value, 1. The reason is that most of the time, the number of times we select the &lt;em&gt;short&lt;/em&gt; regression is sufficiently small not to introduce bias, but not small enough to have valid inference.&lt;/p&gt;
&lt;p&gt;Indeed, &lt;strong&gt;pre-testing distorts inference&lt;/strong&gt;: the distribution of the estimator $\hat \alpha_{pretest}$ is not normal anymore, but bimodal. The &lt;strong&gt;consequence&lt;/strong&gt; is that our confidence intervals for $\alpha$ are going to have the wrong coverage (contain the true effect with a different probability than the claimed one).&lt;/p&gt;
&lt;h2 id=&#34;when-is-pre-testing-a-problem&#34;&gt;When is pre-testing a problem?&lt;/h2&gt;
&lt;p&gt;The problem of pre-testing arises because of the bias generated by running the short regression: &lt;a href=&#34;https://towardsdatascience.com/344ac1477699&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;omitted variable bias (OVB)&lt;/strong&gt;&lt;/a&gt;. In you are not familiar with OVB, I have written a &lt;a href=&#34;https://towardsdatascience.com/344ac1477699&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;short introduction here&lt;/a&gt;. In general however, we can express the omitted variable bias introduced by regressing $Y$ on $D$ ignoring $X$ as&lt;/p&gt;
&lt;p&gt;$$
\text{OVB} = \beta \delta \qquad \text{ where } \qquad \beta := \frac{Cov(X, Y)}{Var(X)}, \quad \delta := \frac{Cov(D, X)}{Var(D)}
$$&lt;/p&gt;
&lt;p&gt;Where $\beta$ is the effect of $X$ (&lt;code&gt;past sales&lt;/code&gt; in our example) on $Y$ (&lt;code&gt;sales&lt;/code&gt;) and $\delta$ is the effect of $D$ (&lt;code&gt;ads&lt;/code&gt;) on $X$.&lt;/p&gt;
&lt;p&gt;Pre-testing is a &lt;strong&gt;problem&lt;/strong&gt; if&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;We run the short regression instead of the long one &lt;em&gt;and&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;The effect of the bias is sensible&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;What can help improving (1), i.e. the probability of correctly rejecting the null hypothesis of zero effect of &lt;code&gt;past sales&lt;/code&gt;, $H_0 : \beta = 0$? The answer is simple: a &lt;strong&gt;bigger sample size&lt;/strong&gt;. If we have more observations, we can more precisely estimate $\beta$ and it is going to be less likely that we commit a &lt;a href=&#34;https://en.wikipedia.org/wiki/Type_I_and_type_II_errors&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;type 2 error&lt;/a&gt; and run the short regression instead of the long one.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s simulate the estimated coefficient $\hat \alpha$ under different sample sizes. Remember that the sample size used until now is $N=1000$.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;Ns = [100,300,1000,3000]
alphas = {f&#39;N = {n:.0f}&#39;:  pre_testing(N=n)[&#39;Pre-test&#39;] for n in Ns}
plot_alphas(alphas, true_alpha=1)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/pretest_20_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;As we can see from the plots, as the sample size increases (left to right), the bias decreases and the distribution of the estimator $\hat \alpha_{pretest}$ converges to a normal distribution.&lt;/p&gt;
&lt;p&gt;What happens instead if the value of $\beta$ was different? It is probably going to affect point (2) in the previous paragraph, but how?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;If $\beta$ is &lt;strong&gt;very small&lt;/strong&gt;, it is going to be hard to detect it, and we will often end up running the &lt;em&gt;short&lt;/em&gt; regression, introducing a bias. However, if $\beta$ is very small, it also implies that the &lt;strong&gt;magnitude of the bias&lt;/strong&gt; is small and therefore it is not going to affect our estimate of $\alpha$ much&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;If $\beta$ is &lt;strong&gt;very big&lt;/strong&gt;, it is going to be easy to detect and we will often end up running the &lt;em&gt;long&lt;/em&gt; regression, avoiding the bias (which would have been very big though).&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Let&amp;rsquo;s simulate the estimated coefficient $\hat \alpha$ under different values of $\beta$. The true value used until now was $\beta = 0.3$.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;betas = 0.3 * np.array([0.1,0.3,1,3])
alphas = {f&#39;beta = {b:.2f}&#39;:  pre_testing(b=b)[&#39;Pre-test&#39;] for b in betas}
plot_alphas(alphas, true_alpha=1)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/pretest_22_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;As we can see from the plots, as the value of $\beta$ increases, the bias first appears and then disappears. When $\beta$ is small (left plot), we often choose the short regression, but the bias is small and the average estimate is very close to the true value. For intermediate values of $\beta$, the bias is sensible and it has a clear effect on inference. Lastly, for large values of $\beta$ instead (right plot), we always run the long regression and the bias disappears.&lt;/p&gt;
&lt;p&gt;But &lt;strong&gt;when is a coefficient big or small&lt;/strong&gt;? And big or small with respect to what? The answer is simple: with respect to the &lt;strong&gt;sample size&lt;/strong&gt;, or more accurately, with respect to the inverse of the square root of the sample size, $1 / \sqrt{n}$. The reason is deeply rooted in the &lt;a href=&#34;https://en.wikipedia.org/wiki/Central_limit_theorem&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Central Limit Theorem&lt;/a&gt;, but I won&amp;rsquo;t cover it here.&lt;/p&gt;
&lt;p&gt;The idea is easier to show than to explain, so let&amp;rsquo;s repeat the same simulation as above, but now we will increase both the coefficient and the sample size at the same time.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;betas = 0.3 * 30 / np.sqrt(Ns)
alphas = {f&#39;N = {n:.0f}&#39;:  pre_testing(b=b, N=n)[&#39;Pre-test&#39;] for n,b in zip(Ns,betas)}
plot_alphas(alphas, true_alpha=1)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/pretest_24_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;As we can see, now that $\beta$ is proportional to $1 / \sqrt{n}$, the distortion is not going away, not matter the sample size. Therefore, inference will always be wrong.&lt;/p&gt;
&lt;p&gt;While a coefficient that depends on the sample size might sound &lt;strong&gt;not intuitive&lt;/strong&gt;, it captures well the idea of &lt;strong&gt;magnitude&lt;/strong&gt; in a world where we do inference relying on asymptotic results, first among all the &lt;a href=&#34;https://en.wikipedia.org/wiki/Central_limit_theorem&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Central Limit Theorem&lt;/a&gt;. In fact, the Central Limit Theorem relieas on an infinitely large sample size. However, with an infinite amount of data, no coefficient is small and any non-zero effect is detected with certainty.&lt;/p&gt;
&lt;h2 id=&#34;pre-testing-and-machine-learning&#34;&gt;Pre-Testing and Machine Learning&lt;/h2&gt;
&lt;p&gt;So far we talked about a linear regression with only 2 variables. Where is the &lt;strong&gt;machine learning&lt;/strong&gt; we were promised?&lt;/p&gt;
&lt;p&gt;Usually we do not have just one control variable (or confounder), but many. Moreover, we might want to be flexible with respect to the functional form through which these control variables enter the model. In general, we will assume the following model:&lt;/p&gt;
&lt;p&gt;$$
Y = \alpha D + g_0(X) + u
\newline
D = m_0(X) + v
$$&lt;/p&gt;
&lt;p&gt;Where the effect of interest is still $\alpha$, $X$ is potentially high dimensional and we do not take a stand on the functional form through which $X$ influences $D$ or $Y$.&lt;/p&gt;
&lt;p&gt;In this setting, it is natural to use a machine learning algorithm to estimate $g_0$ and $m_0$. However, machine learning algorithms usually introduce a &lt;strong&gt;regularization bias&lt;/strong&gt; that is comparable to pre-testing.&lt;/p&gt;
&lt;p&gt;Possibly, the &amp;ldquo;simplest&amp;rdquo; way to think about it is &lt;a href=&#34;https://en.wikipedia.org/wiki/Lasso_%28statistics%29&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Lasso&lt;/a&gt;. Lasso is linear in $X$, with a penalization term that effectively just performs the variable selection we discussed above. Therefore, if we were to use Lasso of $X$ and $D$ on $Y$ we would be introducing regularization bias and inference would be distorted. The same goes for more complex algorithms.&lt;/p&gt;
&lt;p&gt;Lastly, you might still wonder &amp;ldquo;why is the model linear in the treatment variable $D$?&amp;rdquo;. Doing inference is much easier in linear model, not only for computational reasons but also for interpretation. Moreover, if the treatment $D$ is binary, the linear functional form is without loss of generality. A stronger assumption is the additive separability of $D$ and $g(X)$.&lt;/p&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;In this post, I have tried to explain how does regularization bias emerges and why it can the an issue in causal inference. This problem is inherently related to settings with many control variables or where we would like to have a model-free (i.e. non-parametric) when controlling for confounders. These are exactly the settings in which machine learning algorithms can be useful.&lt;/p&gt;
&lt;p&gt;In the next post, I will cover a simple and yet incredibly powerful solution to this problem: double-debiased machine learning.&lt;/p&gt;
&lt;h3 id=&#34;references&#34;&gt;References&lt;/h3&gt;
&lt;p&gt;[1] A. Belloni, D. Chen, V. Chernozhukov, C. Hansen, &lt;a href=&#34;https://onlinelibrary.wiley.com/doi/abs/10.3982/ECTA9626&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Sparse Models and Methods for Optimal Instruments With an Application to Eminent Domain&lt;/a&gt; (2012), &lt;em&gt;Econometrica&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;[2] A. Belloni, V. Chernozhukov, C. Hansen, &lt;a href=&#34;https://academic.oup.com/restud/article-abstract/81/2/608/1523757&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Inference on treatment effects after selection among high-dimensional controls&lt;/a&gt; (2014), &lt;em&gt;The Review of Economic Studies&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;[3] V. Chernozhukov, D. Chetverikov, M. Demirer, E. Duflo, C. Hansen, W. Newey, J. Robins, &lt;a href=&#34;https://academic.oup.com/ectj/article/21/1/C1/5056401&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Double/debiased machine learning for treatment and structural parameters&lt;/a&gt; (2018), &lt;em&gt;The Econometrics Journal&lt;/em&gt;.&lt;/p&gt;
&lt;h3 id=&#34;related-articles&#34;&gt;Related Articles&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/344ac1477699&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Understanding Omitted Variable Bias&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/59f801eb3299&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Understanding The Frisch-Waugh-Lovell Theorem&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/b63dc69e3d8c&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;DAGs and Control Variables&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;code&#34;&gt;Code&lt;/h3&gt;
&lt;p&gt;You can find the original Jupyter Notebook here:&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/pretest.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/matteocourthoud/Blog-Posts/blob/main/pretest.ipynb&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Understanding The Chi-Squared Test</title>
      <link>https://matteocourthoud.github.io/post/chisquared/</link>
      <pubDate>Tue, 31 May 2022 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/post/chisquared/</guid>
      <description>&lt;p&gt;If you search the Wikipedia definition of Chi-Squared test, you get the following definition:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Pearson&amp;rsquo;s chi-squared test $\chi^2$ is a statistical test applied to sets of categorical data to evaluate how likely it is that any observed difference between the sets arose by chance.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;What does it mean? Let&amp;rsquo;s see it together.&lt;/p&gt;
&lt;h2 id=&#34;test-1-discrete-distribution&#34;&gt;Test 1: Discrete Distribution&lt;/h2&gt;
&lt;p&gt;Suppose you want to &lt;strong&gt;test whether a dice is fair&lt;/strong&gt;. You throw the dice 60 times and you count the number of times you get each outcome.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s simulate some data (from a fair dice).&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;%matplotlib inline
%config InlineBackend.figure_format = &#39;retina&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from src.utils import *
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def generate_data_dice(N=60, seed=1):
    np.random.seed(seed) # Set seed for replicability
    dice_numbers = [1,2,3,4,5,6]  # Dice numbers
    dice_throws = np.random.choice(dice_numbers, size=N)  # Actual dice throws
    data = pd.DataFrame({&amp;quot;dice number&amp;quot;: dice_numbers,
                         &amp;quot;observed&amp;quot;: [sum(dice_throws==n) for n in dice_numbers],
                         &amp;quot;expected&amp;quot;: int(N / 6)})
    return data
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;data_dice = generate_data_dice()
data_dice
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;dice number&lt;/th&gt;
      &lt;th&gt;observed&lt;/th&gt;
      &lt;th&gt;expected&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;10&lt;/td&gt;
      &lt;td&gt;10&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;14&lt;/td&gt;
      &lt;td&gt;10&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;6&lt;/td&gt;
      &lt;td&gt;10&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;8&lt;/td&gt;
      &lt;td&gt;10&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;12&lt;/td&gt;
      &lt;td&gt;10&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;5&lt;/th&gt;
      &lt;td&gt;6&lt;/td&gt;
      &lt;td&gt;10&lt;/td&gt;
      &lt;td&gt;10&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;If we were throwing the dice &lt;strong&gt;a lot&lt;/strong&gt; of times, we would expect the same number of observations for each outcome. However, there is inherent noise in the process. How can we tell whether the fact that we didn&amp;rsquo;t get exactly 10 observations for each outcome is just due to randomness or it&amp;rsquo;s because the dice is unfair?&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;idea&lt;/strong&gt; is to compute some statistic whose distribution is known under the assumption that the dice is fair, and then check if its value is &lt;strong&gt;&amp;ldquo;unusual&amp;rdquo;&lt;/strong&gt; or not. If the value is particularly &amp;ldquo;unusual&amp;rdquo;, we reject the null hypothesis that the dice is fair.&lt;/p&gt;
&lt;p&gt;In our case, the statistic we choose is the chi-squared $\chi^{2}$ test-statistic.&lt;/p&gt;
&lt;p&gt;The value of the &lt;a href=&#34;https://en.wikipedia.org/wiki/Pearson%27s_chi-squared_test&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Pearson&amp;rsquo;s chi-squared test-statistic&lt;/strong&gt;&lt;/a&gt; is&lt;/p&gt;
&lt;p&gt;$$
T_{\chi^2} = \sum _{i=1}^{n} \frac{(O_i - E_i)^{2}}{E_i} = N \sum _{i=1}^{n} \frac{\left(O_i/N - p_i \right)^2 }{p_i}
$$&lt;/p&gt;
&lt;p&gt;where&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$O_{i}$ = the number of observations of type i.&lt;/li&gt;
&lt;li&gt;$N$ = total number of observations&lt;/li&gt;
&lt;li&gt;$E_{i}=N * p_{i}$ = the expected (theoretical) count of type $i$, asserted by the null hypothesis that the fraction of type $i$ in the population is $p_{i}$&lt;/li&gt;
&lt;li&gt;$n$ = the number of cells in the table.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def compute_chi2_stat(data):
    return sum( (data.observed - data.expected)**2 / data.expected )
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;chi2_stat = compute_chi2_stat(data_dice)
chi2_stat
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;4.0
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What do we make of this number? Is it &lt;strong&gt;unusual&lt;/strong&gt;?&lt;/p&gt;
&lt;p&gt;If the dice were fair, the test Pearson&amp;rsquo;s chi-squared test-statistic $T_{\chi^2}$ would be distributed as a &lt;strong&gt;chi-squared distribution&lt;/strong&gt; with $k-1$ degrees of freedom, $\chi^2_{k-1}$. For the moment, take this claim at face value, we will verify it later, both empirically and theoretically. We will also discuss the degrees of freedom in detail later on.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Important!&lt;/strong&gt; Do not confuse the chi-squared test statistic (a number) with the chi-squared distribution (a distribution).&lt;/p&gt;
&lt;p&gt;What does a chi-squared distribution with $n-1$ degrees of freedom, $\chi^2_{k-1}$, look like?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from scipy.stats import chi2

x = np.arange(0, 30, 0.001) # x-axis ranges from 0 to 30 with .001 steps
chi2_5_pdf = chi2.pdf(x, df=5) # Chi-square distribution with 5 degrees of freedom
plt.plot(x, chi2_5_pdf); 
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/chisquared_13_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;How does the value of the statistic we have observed compares with its the distribution under the null hypothesis of a fair dice?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plt.plot(x, chi2.pdf(x, df=5), label=&#39;chi2 distribution&#39;);
plt.axvline(chi2_stat, color=&#39;k&#39;, label=&#39;chi2 statistic&#39;)
plt.legend();
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/chisquared_15_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;The test statistic seems to fall well within the distribution, i.e. it does not seem to be an unusual event. Indeed, the question we want to answer is: &amp;ldquo;&lt;em&gt;under the null hypothesis that the dice is fair, how unlikely is the statistic we have observed?&lt;/em&gt;&amp;rdquo;.&lt;/p&gt;
&lt;p&gt;The last component we need in order to build a hypothesis test is a level of &lt;strong&gt;confidence&lt;/strong&gt;, i.e. a threshold of &amp;ldquo;unlikeliness&amp;rdquo; of an event, below which we declare that the event is too unlikely under the model, for the model to be true. Let&amp;rsquo;s say we decide to set that threshold at 5%.&lt;/p&gt;
&lt;p&gt;If the likelihood of observing an even that (or more) extreme than the one we have actually observed is less than 5%, we reject the null hypothesis that the dice is fair.&lt;/p&gt;
&lt;p&gt;What is this value for a chi-squared distribution with 5 degrees of freedom? We can compute the percent point function (ppf) of 95% for the chi-squared distribution, which is essentially the inverse of the &lt;a href=&#34;https://en.wikipedia.org/wiki/Cumulative_distribution_function&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;cumulative distribution function&lt;/a&gt;. This value is often called the &lt;strong&gt;critical value&lt;/strong&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;z95 = chi2.ppf(0.95, df=5)
z95
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;11.070497693516351
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Since our value is smaller than the critical value, we do not reject the null. The critical value is indeed critical because it splits the domain of the test statistic into two areas: the &lt;strong&gt;rejection area&lt;/strong&gt;, where we reject the null hypothesis, and the non-rejection area, where we don&amp;rsquo;t.&lt;/p&gt;
&lt;p&gt;We can plot the rejection and non-rejection areas in a plot.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def plot_test(x, stat, df):
    z95 = chi2.ppf(0.95, df=df)
    chi2_pdf = chi2.pdf(x, df=df)
    plt.plot(x, chi2_pdf);
    plt.fill_between(x[x&amp;gt;z95], chi2_pdf[x&amp;gt;z95], color=&#39;r&#39;, alpha=0.4, label=&#39;rejection area&#39;)
    plt.fill_between(x[x&amp;lt;z95], chi2_pdf[x&amp;lt;z95], color=&#39;g&#39;, alpha=0.4, label=&#39;non-rejection area&#39;)
    plt.axvline(chi2_stat, color=&#39;k&#39;, label=&#39;chi2 statistic&#39;)
    plt.ylim(0, plt.ylim()[1])
    plt.legend();
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plot_test(x, chi2_stat, df=5)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/chisquared_20_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;From the plot, we can see that we do not reject the null hypothesis that the dice is fair for our value of the $\chi^2$ test statistic.&lt;/p&gt;
&lt;h2 id=&#34;why-the-chi-squared-distribution&#34;&gt;Why the Chi-squared Distribution?&lt;/h2&gt;
&lt;p&gt;How do we know that that particular statistic has that particular distribution?&lt;/p&gt;
&lt;p&gt;Before digging into the math, we can check this claim via &lt;strong&gt;simulation&lt;/strong&gt;. Since we have access to the data generating process, we can repeat the procedure above many times, i.e.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;roll a (fair) dice 60 times&lt;/li&gt;
&lt;li&gt;compute the chi-square statistic&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;and then plot the distribution of chi square statistics.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def simulate_chi2stats(K, N, dgp):
    chi2_stats = [compute_chi2_stat(dgp(seed=k)) for k in range(K)]
    return np.array(chi2_stats)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;chi2_stats = simulate_chi2stats(K=100, N=60, dgp=generate_data_dice)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plt.hist(chi2_stats, density=True, bins=30, alpha=0.3, color=&#39;C0&#39;);
plt.plot(x, chi2_5_pdf);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/chisquared_26_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Since we only did it 100 times, the distribution looks pretty coarse but vaguely close to its theoretical counterpart. Let&amp;rsquo;s try 1000 times.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;chi2_stats = simulate_chi2stats(K=1000, N=60, dgp=generate_data_dice)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plt.hist(chi2_stats, density=True, bins=30, alpha=0.3, color=&#39;C0&#39;);
plt.plot(x, chi2_5_pdf);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/chisquared_29_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;The empirical distribution of the test statistic is indeed very close to its theoretical counterpart.&lt;/p&gt;
&lt;h2 id=&#34;some-statistics&#34;&gt;Some Statistics&lt;/h2&gt;
&lt;p&gt;Why does the distribution of the test statistic look like that? Let&amp;rsquo;s now dig deeper into the math.&lt;/p&gt;
&lt;p&gt;There are two things we need to know in order to understand the answer:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the Central Limit Theorem&lt;/li&gt;
&lt;li&gt;the relationship between a chi-squared and a normal distribution&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The Wikipedia definition of the &lt;a href=&#34;https://en.wikipedia.org/wiki/Central_limit_theorem&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Central Limit Theorem&lt;/a&gt; says that&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;&lt;em&gt;In probability theory, the central limit theorem (CLT) establishes that, in many situations, when independent random variables are summed up, their properly normalized sum tends toward a normal distribution (informally a bell curve) even if the original variables themselves are not normally distributed.&lt;/em&gt;&amp;rdquo;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Where does a normal distribution come up in our case? If we look at a single row in our data, i.e. the occurrences of a specific dice throw, it can be interpreted as the sum of realization from a &lt;a href=&#34;https://en.wikipedia.org/wiki/Bernoulli_distribution&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Bernoulli distribution&lt;/a&gt; with probability 1/6.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;&lt;em&gt;In probability theory and statistics, the Bernoulli distribution is the discrete probability distribution of a random variable which takes the value $1$ with probability $p$ and the value $0$ with probability $q=1-p$.&lt;/em&gt;&amp;rdquo;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;In our case, the probability of getting a particular number is exactly 1/6. What is the distribution of the sum of its realizations? The Central Limit Theorem also tells us that:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;&lt;em&gt;If $X_1, X_2, \dots , X_n, \dots$ are random samples drawn from a population with overall mean $\mu$ and finite variance $\sigma^2$, and if $\bar X_n$ is the sample mean of the first $n$ samples, then the limiting form of the distribution,&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;$$
Z = \lim_{n \to \infty} \sqrt{n} \left( \frac{\bar X_n - \mu }{\sigma} \right)
$$&lt;/p&gt;
&lt;p&gt;&lt;em&gt;is a standard normal distribution.&lt;/em&gt;&amp;rdquo;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Therefore, in our case, the distribution of the sum of Bernoulli distributions with mean $p$ is distributed as a normal distribution with&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;mean $p$&lt;/li&gt;
&lt;li&gt;variance $p * (1-p)$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Therefore, we can obtain a random variable that is asymptotically standard normal distributed as&lt;/p&gt;
&lt;p&gt;$$
\lim_{n \to \infty} \ \sqrt{n} \left( \frac {\bar X_n - p}{\sqrt{p * (1-p)}} \right) \sim N(0,1)
$$&lt;/p&gt;
&lt;p&gt;Our last piece: what is a &lt;a href=&#34;https://en.wikipedia.org/wiki/Chi-squared_distribution&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;chi-squared distribution&lt;/a&gt;? The Wikipedia definition says&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;&lt;em&gt;If $Z_1, &amp;hellip;, Z_k$ are independent, standard normal random variables, then the sum of their squares,&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;$$
Q = \sum_{i=1}^k Z_i^2
$$
&lt;em&gt;is distributed according to the chi-squared distribution with $k$ degrees of freedom.&lt;/em&gt;&amp;rdquo;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;I.e. the sum of standard normal distributions is a chi-squared distribution, where the &lt;strong&gt;degrees of freedom&lt;/strong&gt; indicate the number of normal distributions we are summing over. Since the normalized sum of realizations of each dice number should converge to a standard normal distribution, their sum of squares should converge to a chi-squared distribution. I.e.&lt;/p&gt;
&lt;p&gt;$$
\lim_{n \to \infty} \ \sum_k n \frac{(\bar X_n - p)^2}{p * (1-p)} \sim \chi^2_k
$$&lt;/p&gt;
&lt;p&gt;There is just one issue: the last distribution is not really independent from the others. In fact, as soon as we know that we have thrown 60 dices and how many 1s, 2s, 3s, 4s, and 5s we got, we can compute the number of 6s. Therefore, we should exclude one distribution since only 5 (or, in general, $k-1$) are truly independent.&lt;/p&gt;
&lt;p&gt;In practice, however, we sum all distributions, but then we scale them down by multiplying them by $(1-p)$ so that we have&lt;/p&gt;
&lt;p&gt;$$
\lim_{n \to \infty} \ \sum_k n \frac{(\bar X_n - p)^2}{p} \sim \chi^2_{k-1}
$$&lt;/p&gt;
&lt;p&gt;which is exactly the formula we used to compute the test statistic:&lt;/p&gt;
&lt;p&gt;$$
T_{\chi^2} = \sum _{i=1}^{n} \frac{(O_i - E_i)^{2}}{E_i} = N \sum _{i=1}^{n} \frac{\left(O_i/N - p_i \right)^2 }{p_i}
$$&lt;/p&gt;
&lt;h2 id=&#34;test-2-independence&#34;&gt;Test 2: Independence&lt;/h2&gt;
&lt;p&gt;Chi-squared tests can also be used to &lt;strong&gt;test independence between two variables&lt;/strong&gt;. The idea is fundamentally the same as the test in the previous section: checking systematic differences between observed and expected values, across different variables.&lt;/p&gt;
&lt;p&gt;Suppose you have &lt;strong&gt;data on grades in a classroom, by gender&lt;/strong&gt;. Grades go from $1$ to $4$. Assuming males and females are equally prepared for the test, you want to test whether there has been discrimination in grading.&lt;/p&gt;
&lt;p&gt;The problem is again asserting whether the observed differences are random or systematic.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s generate some data (under the no discrimination assumption).&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def generate_data_grades(N_male=60, N_female=40, seed=1):
    np.random.seed(seed)
    grade_scale = [1,2,3,4]
    p = [0.1, 0.2, 0.5, 0.2]
    grades_male = np.random.choice(grade_scale, size=N_male, p=p)
    grades_female = np.random.choice(grade_scale, size=N_female, p=p)
    data = pd.DataFrame({&amp;quot;grade&amp;quot;: grade_scale + grade_scale,
                          &amp;quot;gender&amp;quot;: [&amp;quot;male&amp;quot; for i in grade_scale] + [&amp;quot;female&amp;quot; for i in grade_scale],
                          &amp;quot;observed&amp;quot;: [sum(grades_male==n) for n in grade_scale] + [sum(grades_female==n) for n in grade_scale],
                        })  
    data[&#39;expected gender&#39;] = data.groupby(&amp;quot;gender&amp;quot;)[&amp;quot;observed&amp;quot;].transform(&amp;quot;mean&amp;quot;) 
    data[&#39;expected grade&#39;] = data.groupby(&amp;quot;grade&amp;quot;)[&amp;quot;observed&amp;quot;].transform(&amp;quot;mean&amp;quot;) 
    data[&#39;expected&#39;] = data[&#39;expected gender&#39;] * data[&#39;expected grade&#39;] / data[&#39;observed&#39;].mean()
    return data
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;data_grades = generate_data_grades()
data_grades
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;grade&lt;/th&gt;
      &lt;th&gt;gender&lt;/th&gt;
      &lt;th&gt;observed&lt;/th&gt;
      &lt;th&gt;expected gender&lt;/th&gt;
      &lt;th&gt;expected grade&lt;/th&gt;
      &lt;th&gt;expected&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;male&lt;/td&gt;
      &lt;td&gt;9&lt;/td&gt;
      &lt;td&gt;15.0&lt;/td&gt;
      &lt;td&gt;5.5&lt;/td&gt;
      &lt;td&gt;6.6&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;male&lt;/td&gt;
      &lt;td&gt;14&lt;/td&gt;
      &lt;td&gt;15.0&lt;/td&gt;
      &lt;td&gt;10.5&lt;/td&gt;
      &lt;td&gt;12.6&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;male&lt;/td&gt;
      &lt;td&gt;27&lt;/td&gt;
      &lt;td&gt;15.0&lt;/td&gt;
      &lt;td&gt;24.5&lt;/td&gt;
      &lt;td&gt;29.4&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;male&lt;/td&gt;
      &lt;td&gt;10&lt;/td&gt;
      &lt;td&gt;15.0&lt;/td&gt;
      &lt;td&gt;9.5&lt;/td&gt;
      &lt;td&gt;11.4&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;female&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;10.0&lt;/td&gt;
      &lt;td&gt;5.5&lt;/td&gt;
      &lt;td&gt;4.4&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;5&lt;/th&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;female&lt;/td&gt;
      &lt;td&gt;7&lt;/td&gt;
      &lt;td&gt;10.0&lt;/td&gt;
      &lt;td&gt;10.5&lt;/td&gt;
      &lt;td&gt;8.4&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;6&lt;/th&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;female&lt;/td&gt;
      &lt;td&gt;22&lt;/td&gt;
      &lt;td&gt;10.0&lt;/td&gt;
      &lt;td&gt;24.5&lt;/td&gt;
      &lt;td&gt;19.6&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;7&lt;/th&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;female&lt;/td&gt;
      &lt;td&gt;9&lt;/td&gt;
      &lt;td&gt;10.0&lt;/td&gt;
      &lt;td&gt;9.5&lt;/td&gt;
      &lt;td&gt;7.6&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;Has there been discrimination? We again compare observed and expected grades, where expected grades are computed under the independence assumption: as the product of the marginal distributions of &lt;code&gt;grade&lt;/code&gt; and &lt;code&gt;gender&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;The value of the test-statistic is&lt;/p&gt;
&lt;p&gt;$$
T_{\chi^2} = \sum_{i=1}^r \sum_{j=1}^c \frac{(O_{i,j} - E_{i,j})^2 }{ E_{i,j} } = N \sum_{i,j} p_{i \cdot} p_{\cdot j} \left( \frac{O_{i,j}/N - p_{i \cdot} p_{\cdot j} }{ p_{i \cdot} p_{\cdot j}} \right)^2
$$&lt;/p&gt;
&lt;p&gt;where&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;$N$ is the total sample size (the sum of all cells in the table)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$p_{i \cdot} = \frac{O_{i\cdot }}{N} = \sum_{j=1}^{c} \frac{O_{i,j}}{N}$ is the fraction of observations of type i ignoring the column attribute (fraction of row totals), and&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$p_{\cdot j} = \frac{O_{\cdot j}}{N} = \sum_{i=1}^{r} \frac{O_{i,j}}{N}$ is the fraction of observations of type j ignoring the row attribute (fraction of column totals).&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;So the formula for the test statistic is the same&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;chi2_stat = compute_chi2_stat(data_grades)
chi2_stat
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;3.490327550477927
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As before, we can double-check whether the statistic is indeed distributed as a chi-squared with $k-1$ degrees of freedom by simulating the data generating process.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;chi2_stats = simulate_chi2stats(K=1000, N=60, dgp=generate_data_grades)
plt.hist(chi2_stats, density=True, bins=30, alpha=0.3, color=&#39;C0&#39;);
plt.plot(x, chi2_5_pdf);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/chisquared_41_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;The two distributions do not look similar anymore.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;What happened?&lt;/strong&gt; We forgot to change the degrees of freedom! The general formula for the degrees of freedom when testing the independence of variables is $(N_i - 1) \times (N_j - 1)$. So in our case, it&amp;rsquo;s $(4-1) \times (2-1) = 3$.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;chi2_3_pdf = chi2.pdf(x, df=3)
chi2_stats = simulate_chi2stats(K=1000, N=60, dgp=generate_data_grades)
plt.hist(chi2_stats, density=True, bins=30, alpha=0.3, color=&#39;C0&#39;);
plt.plot(x, chi2_3_pdf);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/chisquared_43_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Now the empirical distribution is close to its theoretical counterpart.&lt;/p&gt;
&lt;p&gt;Do we &lt;strong&gt;reject&lt;/strong&gt; the null hypothesis of independent distributions of gender and grades? We can visualize the value of the test statistic together with the rejection areas.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plot_test(x, chi2_stat, df=3)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/chisquared_45_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;We &lt;strong&gt;do not reject&lt;/strong&gt; the null hypothesis of independend distributions of gender and grades.&lt;/p&gt;
&lt;h2 id=&#34;test-3-continuous-distributions&#34;&gt;Test 3: Continuous Distributions&lt;/h2&gt;
&lt;p&gt;As we have seen, the chi-square test can be used to compare observed means/frequencies against a null hypothesis. How can we use this statistic to &lt;strong&gt;test a distributional assumption&lt;/strong&gt;?&lt;/p&gt;
&lt;p&gt;The answer is simple: we can construct conditional means. The easiest way to do it is to &lt;strong&gt;bin the data&lt;/strong&gt; into intervals and then check if the observed frequencies match the expected probabilities, within each bin.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: a good practice is to have equally sized bins, in terms of expected probabilities, since it ensures that we have as many observations in each bin as possible.&lt;/p&gt;
&lt;p&gt;As an example, let&amp;rsquo;s assume we have to analyze the customer service of a firm. We would like to understand if the number of complains follows an &lt;a href=&#34;https://en.wikipedia.org/wiki/Exponential_distribution&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;exponential distribution&lt;/a&gt; with paramenter $\lambda=1$.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from scipy.stats import expon
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def generate_complaints_data(N=100, cuts=4, seed=2):
    np.random.seed(seed)
    complaints = np.random.exponential(size=N)
    cat, bins = pd.qcut(complaints, cuts, retbins=True)
    p = [expon.cdf(bins[n+1]) - expon.cdf(bins[n]) for n in range(len(bins)-1)]
    data = pd.DataFrame({&amp;quot;bin&amp;quot;: cat.unique(),
                         &amp;quot;observed&amp;quot;: [sum(cat==n) for n in cat.unique()],
                         &amp;quot;expected&amp;quot;: np.dot(p, N)})
    return data, complaints
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;data_complaints, complaints = generate_complaints_data()
data_complaints
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;bin&lt;/th&gt;
      &lt;th&gt;observed&lt;/th&gt;
      &lt;th&gt;expected&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;(0.297, 0.573]&lt;/td&gt;
      &lt;td&gt;25&lt;/td&gt;
      &lt;td&gt;24.360534&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;(0.0121, 0.297]&lt;/td&gt;
      &lt;td&gt;25&lt;/td&gt;
      &lt;td&gt;17.974853&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;(0.573, 1.025]&lt;/td&gt;
      &lt;td&gt;25&lt;/td&gt;
      &lt;td&gt;20.489741&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;(1.025, 5.092]&lt;/td&gt;
      &lt;td&gt;25&lt;/td&gt;
      &lt;td&gt;35.258339&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;We have split the data into equally sized bins of size 25 and we have computed the expected number of observations within each bin, if the data was indeed exponentially distributed with parameter $\lambda=1$.&lt;/p&gt;
&lt;p&gt;We can &lt;strong&gt;plot&lt;/strong&gt; the observed and realized distribution of the data.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Plot data
plt.hist(complaints, density=True, bins=30, alpha=0.3, color=&#39;C0&#39;);
exp_pdf = expon.pdf(x)
plt.plot(x, exp_pdf);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/chisquared_53_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;The two distributions seem close but we need a test statistic in order to assess whether the differences are random or systematic.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;chi2_stat = compute_chi2_stat(data_complaints)
chi2_stat
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;6.739890904809741
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Do we reject the null hypothesis that the data is drawn from an exponential distribution? We decide by comparing the value of the test statistic with the rejection areas.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plot_test(x, chi2_stat, df=3)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/chisquared_57_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Since the test statistic falls outside of the rejection area, we &lt;strong&gt;do not reject&lt;/strong&gt; the null hypothesis that the data is drawn from an exponential distribution.&lt;/p&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;In this tutorial, we have seen how to perform 3 hypoteses tests&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;testing if a set of means or sums is coming from the expected distribution&lt;/li&gt;
&lt;li&gt;testing if two distributions are independent or not&lt;/li&gt;
&lt;li&gt;testing a specific data generating process&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The underlying principle is the same: testing discrepancies between expected and observed count data.&lt;/p&gt;
&lt;p&gt;The key statistic is Pearson&amp;rsquo;s chi-square statistic and the key distribution is the chi-squared distribution. We have seen how to compute the statistic, why it has a chi-squared distribution, and how to use this information to perform a statistical hypothesis test.&lt;/p&gt;
&lt;h3 id=&#34;code&#34;&gt;Code&lt;/h3&gt;
&lt;p&gt;You can find the original Jupyter Notebook here:&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/chisquared.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/chisquared.ipynb&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Comparing Distributions, From Zero to Hero</title>
      <link>https://matteocourthoud.github.io/post/distr_compare/</link>
      <pubDate>Sun, 29 May 2022 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/post/distr_compare/</guid>
      <description>&lt;p&gt;The problem of comparing distributions often arises in causal inference when we have to &lt;strong&gt;assess the quality of randomization&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;When we want to assess the causal effect of a policy (or, feature, campaign, drug, &amp;hellip;), the golden standard in causal inference are &lt;a href=&#34;https://en.wikipedia.org/wiki/Randomized_controlled_trial&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;randomized control trials&lt;/strong&gt;&lt;/a&gt;, also known in the industry as &lt;a href=&#34;https://de.wikipedia.org/wiki/A/B-Test&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;A/B tests&lt;/strong&gt;&lt;/a&gt;. In practice, we select a sample for the study and we randomly split it into a &lt;strong&gt;control&lt;/strong&gt; and a &lt;strong&gt;treatment&lt;/strong&gt; group, to compare the outcomes between the two groups. The idea is that, under a set of assumption, randomization assures that only difference between the two groups is the treatment, on average. Therefore, we can attribute the differences in outcomes to the treatment effect alone.&lt;/p&gt;
&lt;p&gt;The problem is that, despite randomization, the two groups are never identical. However, sometimes, they are not even &amp;ldquo;similar&amp;rdquo;. For example, we might have more females in one group, or older people, etc.. (we usually call these characteristics, &lt;em&gt;covariates&lt;/em&gt; or &lt;em&gt;control variables&lt;/em&gt;). When it happens, we cannot be certain anymore that the differences in the outcome is only due to the treatment and cannot be attributed to the &lt;strong&gt;inbalanced covariates&lt;/strong&gt; instead. Therefore, it is always important, after randomization, to check whether all observed variables are balance across groups and whether there are no systematic differences. Another option, to be certain that certain covariates are balanced, is &lt;a href=&#34;https://en.wikipedia.org/wiki/Stratified_sampling&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;em&gt;stratified sampling&lt;/em&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;In this blog post, we are going to see different ways to compare two (or more) distributions and assess the magnitude and significance of their difference. We are going to consider two different approaches, &lt;strong&gt;graphical&lt;/strong&gt; and &lt;strong&gt;numerical&lt;/strong&gt;. The two approaches generally trade-off &lt;strong&gt;intuition&lt;/strong&gt; with &lt;strong&gt;rigour&lt;/strong&gt;: from plots we can assess subtle differences but it&amp;rsquo;s hard to assess whether these differences are systematic or due to noise.&lt;/p&gt;
&lt;h2 id=&#34;the-data&#34;&gt;The Data&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s assume we need to perform an experiment on a group of individuals and we have randomized them into a treatment and control group. We would like them to be &lt;strong&gt;as comparable as possible&lt;/strong&gt;, in order to attribute any difference between the two groups to the treatment effect alone. We also have divided the treatment group in different &lt;em&gt;arms&lt;/em&gt; for testing different treatments.&lt;/p&gt;
&lt;p&gt;For this example, I have simulated a dataset of 1000 individuals, for whom we observe a set of characteristics. I import the data generating process &lt;code&gt;dgp_rnd_assignment()&lt;/code&gt; from &lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/src/dgp.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;src.dgp&lt;/code&gt;&lt;/a&gt; and some plotting functions and libraries from &lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/src/utils.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;src.utils&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;%matplotlib inline
%config InlineBackend.figure_format = &#39;retina&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from src.utils import *
from src.dgp import dgp_rnd_assignment

df = dgp_rnd_assignment().generate_data()
df.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;group&lt;/th&gt;
      &lt;th&gt;arm&lt;/th&gt;
      &lt;th&gt;gender&lt;/th&gt;
      &lt;th&gt;age&lt;/th&gt;
      &lt;th&gt;income&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;control&lt;/td&gt;
      &lt;td&gt;arm 2&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;29.0&lt;/td&gt;
      &lt;td&gt;3967.16&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;control&lt;/td&gt;
      &lt;td&gt;arm 4&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;32.0&lt;/td&gt;
      &lt;td&gt;2927.77&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;treatment&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;29.0&lt;/td&gt;
      &lt;td&gt;1642.66&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;control&lt;/td&gt;
      &lt;td&gt;arm 4&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;25.0&lt;/td&gt;
      &lt;td&gt;1867.64&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;treatment&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;32.0&lt;/td&gt;
      &lt;td&gt;3202.35&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;We have information on $1000$ individuals, for which we observe &lt;code&gt;gender&lt;/code&gt;, &lt;code&gt;age&lt;/code&gt; and &lt;code&gt;income&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;We now want to understand whether the treatment and control &lt;code&gt;groups&lt;/code&gt; are comparable or if there are systematic difference between them.&lt;/p&gt;
&lt;h2 id=&#34;plots&#34;&gt;Plots&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s concentrate on one variable: &lt;code&gt;income&lt;/code&gt;. Does the income distribution differ between the two groups?&lt;/p&gt;
&lt;p&gt;A first approach could be the &lt;a href=&#34;https://en.wikipedia.org/wiki/Box_plot&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;boxplot&lt;/strong&gt;&lt;/a&gt;. The boxplot is a good trade-off between summary statistics and data visualization. The center and the borders of the &lt;strong&gt;box&lt;/strong&gt; represent the &lt;em&gt;median&lt;/em&gt; and the first (Q1) and third &lt;em&gt;quartile&lt;/em&gt; (Q3), respectively. The &lt;strong&gt;whiskers&lt;/strong&gt; instead, extend to the first data points that are more than 1.5 times the &lt;em&gt;interquartile range&lt;/em&gt; (Q3 - Q1) outside the box. The points that fall outside of the whiskers are plotted individually.&lt;/p&gt;
&lt;p&gt;Therefore, the boxplot provides both summary statistics (the box and the whiskers) and direct data visualization (the extreme data points).&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sns.boxplot(x=&#39;group&#39;, y=&#39;income&#39;, data=df);
plt.title(&amp;quot;Boxplot&amp;quot;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/distr_compare_9_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;It seems that the &lt;code&gt;income&lt;/code&gt; distribution in the &lt;code&gt;treatment&lt;/code&gt; group is slightly more dispersed: the orange box is larger and the extreme &lt;code&gt;treatment&lt;/code&gt; points cover a wider range. However, the &lt;strong&gt;issue&lt;/strong&gt; with the boxplot is that it hides the shape of the data, telling us some summary statistics but not showing us the actual data distribution.&lt;/p&gt;
&lt;p&gt;The most intuitive way to plot a distribution is the &lt;strong&gt;histogram&lt;/strong&gt;. The histogram groups the data into equally spaced &lt;strong&gt;bins&lt;/strong&gt; and plots the number of observations within each bin.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sns.histplot(x=&#39;income&#39;, data=df, hue=&#39;group&#39;, bins=50);
plt.title(&amp;quot;Histogram&amp;quot;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/distr_compare_11_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;There are multiple &lt;strong&gt;issues&lt;/strong&gt; with this plot:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The two histograms are not comparable: we would like a density, not a count&lt;/li&gt;
&lt;li&gt;The number of bins is arbitrary&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We can solve the first issue using the &lt;code&gt;stat&lt;/code&gt; option to plot the &lt;code&gt;density&lt;/code&gt; instead of the count and setting the &lt;code&gt;common_norm&lt;/code&gt; option to &lt;code&gt;False&lt;/code&gt; to use the same normalization.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sns.histplot(x=&#39;income&#39;, data=df, hue=&#39;group&#39;, bins=50, stat=&#39;density&#39;, common_norm=False);
plt.title(&amp;quot;Density Histogram&amp;quot;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/distr_compare_13_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Now the two histograms are comparable!&lt;/p&gt;
&lt;p&gt;However, an important &lt;strong&gt;issue&lt;/strong&gt; remains: the size of the bins is arbitrary. If we bunch the data less, we end up with bins with one observation at most, if we bunch the data more, we lose information. This is a classical &lt;a href=&#34;https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;bias-variance trade-off&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;One possible solution is to use a &lt;a href=&#34;https://en.wikipedia.org/wiki/Kernel_density_estimation&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;kernel density function&lt;/strong&gt;&lt;/a&gt; that tries to approximate the histogram with a continuous function, using &lt;a href=&#34;https://en.wikipedia.org/wiki/Kernel_density_estimation&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;kernel density estimation (KDE)&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sns.kdeplot(x=&#39;income&#39;, data=df, hue=&#39;group&#39;, common_norm=False);
plt.title(&amp;quot;Kernel Density Function&amp;quot;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/distr_compare_15_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;We can now visualize both distributions very intuitively.&lt;/p&gt;
&lt;p&gt;If we had multiple categories, a very similar plot is the &lt;strong&gt;violinplot&lt;/strong&gt;. The violinplot also performs kernel density estimation, but plots separate densities along the y axis so that they don&amp;rsquo;t overlap. By default, it also adds a miniature boxplot inside.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sns.violinplot(x=&#39;arm&#39;, y=&#39;income&#39;, data=df);
plt.title(&amp;quot;Violin Plot&amp;quot;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/distr_compare_17_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;From the plot, it seems that the estimated kernel density of &lt;code&gt;income&lt;/code&gt; is very similar across treatment arms.&lt;/p&gt;
&lt;p&gt;However, the &lt;strong&gt;issue&lt;/strong&gt; with kernel density estimation is that it is somehow a black-box and might mask relevant features of the data.&lt;/p&gt;
&lt;p&gt;A much more transparent representation of the two distribution is their &lt;a href=&#34;https://en.wikipedia.org/wiki/Cumulative_distribution_function&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;cumulative distribution function&lt;/strong&gt;&lt;/a&gt;. At each point of the x axis (&lt;code&gt;income&lt;/code&gt;) we plot the percentage of data points that have an equal or lower value. The main advantages of the cumulative distribution function are that&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;we do not need to make any arbitrary choice (e.g. number of bins)&lt;/li&gt;
&lt;li&gt;we do not need to perform any approximation (e.g. with KDE)&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sns.histplot(x=&#39;income&#39;, data=df, hue=&#39;group&#39;, bins=len(df), stat=&amp;quot;density&amp;quot;,
             element=&amp;quot;step&amp;quot;, fill=False, cumulative=True, common_norm=False);
plt.title(&amp;quot;Cumulative distribution function&amp;quot;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/distr_compare_19_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;We can now clearly see that there are relatively more observations with low income in the treatment group than in the control group. In fact, the blue line is above the orange line on the right and below the orange line on the left.&lt;/p&gt;
&lt;p&gt;A related alternative method is the &lt;strong&gt;qq-plot&lt;/strong&gt;, where &lt;em&gt;q&lt;/em&gt; stands for quantile. The qq-plot plots the quantiles of the two distributions against each other. If the distributions are the same, we should get the 45 degree line.&lt;/p&gt;
&lt;p&gt;There is no native qq-plot function in Python and, while the &lt;code&gt;statsmodels&lt;/code&gt; package provides a &lt;a href=&#34;https://www.statsmodels.org/dev/generated/statsmodels.graphics.gofplots.qqplot.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;qqplot&lt;/code&gt; function&lt;/a&gt;, it is quite cumbersome. Therefore, we will do it by hand.&lt;/p&gt;
&lt;p&gt;First, we need to compute the quartiles of the two groups, using the &lt;code&gt;percentile&lt;/code&gt; function.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;income = df[&#39;income&#39;].values
income_t = df.loc[df.group==&#39;treatment&#39;, &#39;income&#39;].values
income_c = df.loc[df.group==&#39;control&#39;, &#39;income&#39;].values

df_pct = pd.DataFrame()
df_pct[&#39;q_treatment&#39;] = np.percentile(income_t, range(100))
df_pct[&#39;q_control&#39;] = np.percentile(income_c, range(100))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we can plot the two quantile distributions against each other, plus the 45-degree line, representing the benchmark perfect fit.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plt.scatter(x=&#39;q_control&#39;, y=&#39;q_treatment&#39;, data=df_pct, label=&#39;Actual fit&#39;);
sns.lineplot(x=&#39;q_control&#39;, y=&#39;q_control&#39;, data=df_pct, color=&#39;r&#39;, label=&#39;Line of perfect fit&#39;);
plt.xlabel(&#39;Quantile of income, control group&#39;)
plt.ylabel(&#39;Quantile of income, treatment group&#39;)
plt.legend()
plt.title(&amp;quot;QQ plot&amp;quot;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/distr_compare_23_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;The qq-plot delivers a very similar insight with respect to the cumulative distribution plot: income in the treatment group is generally lower.&lt;/p&gt;
&lt;h2 id=&#34;tests&#34;&gt;Tests&lt;/h2&gt;
&lt;p&gt;So far, we have seen different ways to visualize differences between distributions. The main advantage of visualization is &lt;strong&gt;intuition&lt;/strong&gt;: we can eyeball the differences and intuitively assess them.&lt;/p&gt;
&lt;p&gt;However, we might want to be more &lt;strong&gt;rigorous&lt;/strong&gt; and try to assess the &lt;strong&gt;statistical significance&lt;/strong&gt; of the difference between the distributions, i.e. answer the question &amp;ldquo;&lt;em&gt;is the observed difference systematic or due to sampling variation?&lt;/em&gt;&amp;rdquo;.&lt;/p&gt;
&lt;p&gt;We are now going to analyze different tests to discern two distributions from each other.&lt;/p&gt;
&lt;h3 id=&#34;t-test&#34;&gt;T-test&lt;/h3&gt;
&lt;p&gt;The first and most common test is the &lt;a href=&#34;https://en.wikipedia.org/wiki/Student%27s_t-test&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;student t-test&lt;/a&gt;. T-tests are generally used to &lt;strong&gt;compare means&lt;/strong&gt;. In this case, we want to test whether the means of the &lt;code&gt;income&lt;/code&gt; distribution is the same across the two groups. The test statistic for the two-means comparison test is given by:&lt;/p&gt;
&lt;p&gt;$$
stat = \frac{|\bar x_1 - \bar x_2|}{\sqrt{s_1 / n_1 + s_2 / n_2}}
$$&lt;/p&gt;
&lt;p&gt;Where $\bar x$ is the sample mean and $s$ is the sample standard deviation. Under mild conditions, the test statistic is asymptotically distributed as a &lt;a href=&#34;https://en.wikipedia.org/wiki/Student%27s_t-distribution&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;student t&lt;/a&gt; distribution.&lt;/p&gt;
&lt;p&gt;We use the &lt;code&gt;ttest_ind&lt;/code&gt; function from &lt;code&gt;scipy&lt;/code&gt; to perform the t-test. The function returns both the test statistic and the implied &lt;a href=&#34;https://en.wikipedia.org/wiki/P-value&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;p-value&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from scipy.stats import ttest_ind

stat, p_value = ttest_ind(income_c, income_t)
print(f&amp;quot;t-test: statistic={stat:.4f}, p-value={p_value:.4f}&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;t-test: statistic=-1.3192, p-value=0.1874
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In general, it is common practice to always perform this test on all variables, when we are running a randomized control trial or A/B test. The results of these tests are usually collected into a table that is called &lt;strong&gt;balance table&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;We can use the &lt;a href=&#34;https://causalml.readthedocs.io/en/latest/causalml.html#module-causalml.match&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;create_table_one&lt;/code&gt;&lt;/a&gt; function from the &lt;a href=&#34;https://causalml.readthedocs.io/en/latest/about.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;causalml&lt;/code&gt;&lt;/a&gt; library to generate the balance table. As the name of the function suggests, the balance table should always be the &lt;strong&gt;first table&lt;/strong&gt; you present when performing an A/B test.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from causalml.match import create_table_one

df[&#39;treatment&#39;] = df[&#39;group&#39;]==&#39;treatment&#39;
create_table_one(df, &#39;treatment&#39;, [&#39;gender&#39;, &#39;age&#39;, &#39;income&#39;])
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;Control&lt;/th&gt;
      &lt;th&gt;Treatment&lt;/th&gt;
      &lt;th&gt;SMD&lt;/th&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;Variable&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;n&lt;/th&gt;
      &lt;td&gt;704&lt;/td&gt;
      &lt;td&gt;296&lt;/td&gt;
      &lt;td&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;age&lt;/th&gt;
      &lt;td&gt;31.94 (8.53)&lt;/td&gt;
      &lt;td&gt;35.88 (7.78)&lt;/td&gt;
      &lt;td&gt;0.4822&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;gender&lt;/th&gt;
      &lt;td&gt;0.51 (0.50)&lt;/td&gt;
      &lt;td&gt;0.58 (0.49)&lt;/td&gt;
      &lt;td&gt;0.1419&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;income&lt;/th&gt;
      &lt;td&gt;3166.07 (1321.89)&lt;/td&gt;
      &lt;td&gt;3296.32 (1645.58)&lt;/td&gt;
      &lt;td&gt;0.0873&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;In the first two columns, we can see the average of the different variables across the treatment and control groups, with standard errors in parenthesis. In the &lt;strong&gt;last column&lt;/strong&gt;, we have the p-values of the t-test for the null hypothesis of zero difference in means.&lt;/p&gt;
&lt;p&gt;From the table, we observe that we cannot reject the null hypothesis of zero difference in mean for any variable, at the 95% confidence level.&lt;/p&gt;
&lt;h3 id=&#34;chi-squared-test&#34;&gt;Chi-Squared Test&lt;/h3&gt;
&lt;p&gt;The &lt;a href=&#34;https://matteocourthoud.github.io/post/chisquared/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;chi-squared test&lt;/a&gt; is a very powerful test that can be used in many different settings. If you want to find out more about it, I have written a very comprehensive &lt;a href=&#34;https://matteocourthoud.github.io/post/chisquared/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;blog post here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;One of the &lt;strong&gt;least known applications&lt;/strong&gt; of the chi-squared test, is testing the similarity between two distributions. The &lt;strong&gt;idea&lt;/strong&gt; is to bin the observations of the two groups. If the two distributions were the same, we would expect the same frequency of observations in each bin. Importantly, we need enough observations in each bin, in order for the test to be valid. I generate bins corresponding to deciles of the distribution of &lt;code&gt;income&lt;/code&gt; in the control group.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_bins = pd.DataFrame()
_, bins = pd.qcut(income_c, q=10, retbins=True)
df_bins[&#39;bin&#39;] = pd.cut(income_c, bins=bins).value_counts().index
df_bins[&#39;income_c&#39;] = pd.cut(income_c, bins=bins).value_counts().values
df_bins[&#39;income_t&#39;] = pd.cut(income_t, bins=bins).value_counts().values

df_bins
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;bin&lt;/th&gt;
      &lt;th&gt;income_c&lt;/th&gt;
      &lt;th&gt;income_t&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;(808.96, 1730.77]&lt;/td&gt;
      &lt;td&gt;70&lt;/td&gt;
      &lt;td&gt;42&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;(1730.77, 2127.948]&lt;/td&gt;
      &lt;td&gt;70&lt;/td&gt;
      &lt;td&gt;36&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;(2127.948, 2411.451]&lt;/td&gt;
      &lt;td&gt;70&lt;/td&gt;
      &lt;td&gt;25&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;(2411.451, 2670.312]&lt;/td&gt;
      &lt;td&gt;71&lt;/td&gt;
      &lt;td&gt;20&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;(2670.312, 2935.05]&lt;/td&gt;
      &lt;td&gt;70&lt;/td&gt;
      &lt;td&gt;26&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;5&lt;/th&gt;
      &lt;td&gt;(2935.05, 3208.662]&lt;/td&gt;
      &lt;td&gt;70&lt;/td&gt;
      &lt;td&gt;13&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;6&lt;/th&gt;
      &lt;td&gt;(3208.662, 3542.805]&lt;/td&gt;
      &lt;td&gt;71&lt;/td&gt;
      &lt;td&gt;25&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;7&lt;/th&gt;
      &lt;td&gt;(3542.805, 3972.996]&lt;/td&gt;
      &lt;td&gt;70&lt;/td&gt;
      &lt;td&gt;26&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;8&lt;/th&gt;
      &lt;td&gt;(3972.996, 4912.782]&lt;/td&gt;
      &lt;td&gt;70&lt;/td&gt;
      &lt;td&gt;41&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;9&lt;/th&gt;
      &lt;td&gt;(4912.782, 11634.39]&lt;/td&gt;
      &lt;td&gt;71&lt;/td&gt;
      &lt;td&gt;42&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;We can now perform the test by comparing the frequencies of the two distributions, across bins. The test statistic is given by:&lt;/p&gt;
&lt;p&gt;$$
stat = \sum _{i=1}^{n} \frac{(O_i - E_i)^{2}}{E_i}
$$&lt;/p&gt;
&lt;p&gt;Where the bins are indexed by $i$ and $O$ is the observed number of data points in bin $i$ and $E$ is the expected number of data points in bin $i$. Since we generated the bins using deciles of the distribution of &lt;code&gt;income&lt;/code&gt; in the control group, we expect the number of observations per bin in the treatment group to be the same across bins. Under mild assumptions, the test statistic is asymptocally distributed as a &lt;a href=&#34;https://en.wikipedia.org/wiki/Chi-squared_distribution&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;chi-squared&lt;/a&gt; distribution.&lt;/p&gt;
&lt;p&gt;To compute the test statistic and the p-value of the test, we use the &lt;code&gt;chisquare&lt;/code&gt; function from &lt;code&gt;scipy&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from scipy.stats import chisquare

df_bins[&#39;income_t_norm&#39;] = df_bins[&#39;income_t&#39;] / np.sum(df_bins[&#39;income_t&#39;]) * np.sum(df_bins[&#39;income_c&#39;])
stat, p_value = chisquare(df_bins[&#39;income_c&#39;], df_bins[&#39;income_t_norm&#39;])
print(f&amp;quot;Chi-squared Test: statistic={stat:.4f}, p-value={p_value:.4f}&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Chi-squared Test: statistic=95.2526, p-value=0.0000
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The p-value is practically zero, implying that we reject the null hypothesis of no difference between the two distributions.&lt;/p&gt;
&lt;h3 id=&#34;kolmogorov-smirnov-test&#34;&gt;Kolmogorov-Smirnov Test&lt;/h3&gt;
&lt;p&gt;The idea of the &lt;a href=&#34;https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Kolmogorov-Smirnov test&lt;/a&gt;, is to &lt;strong&gt;compare the cumulative distributions&lt;/strong&gt; of the two groups. In particular, the Kolmogorov-Smirnov test statistic is the maximum absolute difference between the two cumulative distributions.&lt;/p&gt;
&lt;p&gt;$$
stat = \sup _{x} \ \Big| \ F_1(x) - F_2(x) \ \Big|
$$&lt;/p&gt;
&lt;p&gt;Where $F_1$ and $F_2$ are the two cumulative distribution functions and $x$ are the values of the underlying variable. Under mild conditions, the asymptotic distribution of the Kolmogorov-Smirnov test statistic is known.&lt;/p&gt;
&lt;p&gt;To better understand the test, let&amp;rsquo;s plot the cumulative distribution functions and the test statistic. First, we compute the cumulative distribution functions.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_ks = pd.DataFrame()
df_ks[&#39;income&#39;] = np.sort(df[&#39;income&#39;].unique())
df_ks[&#39;F_control&#39;] = df_ks[&#39;income&#39;].apply(lambda x: np.mean(income_c&amp;lt;=x))
df_ks[&#39;F_treatment&#39;] = df_ks[&#39;income&#39;].apply(lambda x: np.mean(income_t&amp;lt;=x))
df_ks.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;income&lt;/th&gt;
      &lt;th&gt;F_control&lt;/th&gt;
      &lt;th&gt;F_treatment&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;808.96&lt;/td&gt;
      &lt;td&gt;0.001420&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;831.93&lt;/td&gt;
      &lt;td&gt;0.002841&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;893.28&lt;/td&gt;
      &lt;td&gt;0.002841&lt;/td&gt;
      &lt;td&gt;0.003378&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;925.08&lt;/td&gt;
      &lt;td&gt;0.004261&lt;/td&gt;
      &lt;td&gt;0.003378&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;951.21&lt;/td&gt;
      &lt;td&gt;0.004261&lt;/td&gt;
      &lt;td&gt;0.006757&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;We now need to find the point where the absolute distance between the cumulative distribution functions is largest.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;k = np.argmax( np.abs(df_ks[&#39;F_control&#39;] - df_ks[&#39;F_treatment&#39;]))
tstat = np.abs(df_ks[&#39;F_treatment&#39;][k] - df_ks[&#39;F_control&#39;][k])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can visualize the value of the test statistic, by plotting the two cumulative distribution functions and the value of the test statistic.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;y = (df_ks[&#39;F_treatment&#39;][k] + df_ks[&#39;F_control&#39;][k])/2
plt.plot(&#39;income&#39;, &#39;F_control&#39;, data=df_ks, label=&#39;Control&#39;)
plt.plot(&#39;income&#39;, &#39;F_treatment&#39;, data=df_ks, label=&#39;Treatment&#39;)
plt.errorbar(x=df_ks[&#39;income&#39;][k], y=y, yerr=tstat/2, color=&#39;k&#39;,
             capsize=5, mew=3, label=f&amp;quot;Test statistic: {tstat:.4f}&amp;quot;)
plt.legend(loc=&#39;center right&#39;);
plt.title(&amp;quot;Kolmogorov-Smirnov Test&amp;quot;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/distr_compare_45_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;From the plot, we can see that the value of the test statistic corresponds to the distance between the two cumulative distributions at &lt;code&gt;income&lt;/code&gt;=4000. For that value of &lt;code&gt;income&lt;/code&gt;, we have the largest imbalance between the two groups.&lt;/p&gt;
&lt;p&gt;We can now perform the actual test using the &lt;code&gt;kstest&lt;/code&gt; function from &lt;code&gt;scipy&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from scipy.stats import kstest

stat, p_value = kstest(income_t, income_c)
print(f&amp;quot; Kolmogorov-Smirnov Test: statistic={stat:.4f}, p-value={p_value:.4f}&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt; Kolmogorov-Smirnov Test: statistic=0.0881, p-value=0.0730
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The p-value is still above 5%: we do not reject the null hypothesis that the two distributions are the same, with 95% confidence.&lt;/p&gt;
&lt;h3 id=&#34;permutation-testing&#34;&gt;Permutation Testing&lt;/h3&gt;
&lt;p&gt;A non-parametric alternative is permutation testing. The idea is that, under the null hypothesis, the two distributions should be the same, therefore &lt;strong&gt;shuffling&lt;/strong&gt; the observations across groups, should not significantly alter any statistic.&lt;/p&gt;
&lt;p&gt;We can then chose any statistic and compute how much more extreme it is for different permutations, with respect to its value in the original sample. For example, let&amp;rsquo;s use as a test statistic the sample mean of the treatment group.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;stats = [np.mean(np.random.choice(income, size=len(income_t), replace=False)) for _ in range(1000)]
p_value = np.mean(stats &amp;gt; np.mean(income_t))

print(f&amp;quot;Permutation test: p-value={p_value:.4f}&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Permutation test: p-value=0.0820
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The permutation test gives us a p-value very similar to the ones obtained with the other tests.&lt;/p&gt;
&lt;p&gt;How do we &lt;strong&gt;interpret&lt;/strong&gt; the p-value? It means that he sample mean of the treatment group in the data is larger than $1 - 0.082 = 91.8%$ of the sample means of the treatment group across the permuted samples.&lt;/p&gt;
&lt;p&gt;We can &lt;strong&gt;visualize&lt;/strong&gt; the test, by plotting the distribution of the test statistics against its sample value.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plt.hist(stats, label=&#39;Permutation Statistics&#39;);
plt.axvline(x=np.mean(income_t), c=&#39;r&#39;, ls=&#39;--&#39;, label=&#39;Sample Statistic&#39;);
plt.legend();
plt.title(&#39;Permutation Test&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/distr_compare_53_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;As we can see, the sample statistic is quite extreme with respect to the values in the permuted samples, but not excessively.&lt;/p&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;TBD&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Omitted Variable Bias And What Can We Do About It</title>
      <link>https://matteocourthoud.github.io/post/ovb/</link>
      <pubDate>Wed, 25 May 2022 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/post/ovb/</guid>
      <description>&lt;p&gt;In causal inference, &lt;strong&gt;bias&lt;/strong&gt; is extremely problematic because it makes inference not valid. Bias generally means that an estimator will not deliver the estimate of the true effect, on average.&lt;/p&gt;
&lt;p&gt;This is why, in general, we prefer estimators that are &lt;strong&gt;unbiased&lt;/strong&gt;, at the cost of a higher variance, i.e. more noise. Does it mean that every biased estimator is useless? Actually no. Sometimes, with domain knowledge, we can still draw causal conclusions even with a biased estimator.&lt;/p&gt;
&lt;p&gt;In this post, we are going to review a specific but frequent source of bias, &lt;strong&gt;omitted variable bias (OVB)&lt;/strong&gt;. We are going to explore the causes of the bias and leverage these insights to make causal statements, despite the bias.&lt;/p&gt;
&lt;h2 id=&#34;theory&#34;&gt;Theory&lt;/h2&gt;
&lt;p&gt;Suppose we are interested in the effect of a variable $D$ on a variable $y$. However, there is a third variable $Z$ that we do not observe and that is correlated with both $D$ and $Y$. Assume the data generating process can be represented with the following &lt;a href=&#34;https://towardsdatascience.com/b63dc69e3d8c&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Directed Acyclic Graph (DAG)&lt;/strong&gt;&lt;/a&gt;. If you are not familiar with DAGs, I have written a short &lt;a href=&#34;https://towardsdatascience.com/b63dc69e3d8c&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;introduction here&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-mermaid&#34;&gt;flowchart LR
classDef included fill:#DCDCDC,stroke:#000000,stroke-width:2px;
classDef excluded fill:#ffffff,stroke:#000000,stroke-width:2px;
classDef unobserved fill:#ffffff,stroke:#000000,stroke-width:2px,stroke-dasharray: 5 5;

D((D))
Z((Z))
Y((Y))

D --&amp;gt; Y
Z --&amp;gt; D
Z --&amp;gt; Y

class D,Y excluded;
class Z unobserved;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Since there is a &lt;a href=&#34;https://towardsdatascience.com/b63dc69e3d8c&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;backdoor path&lt;/strong&gt;&lt;/a&gt; from $D$ to $y$ passing through $Z$, we need to condition our analysis on $Z$ in order to recover the causal effect of $D$ on $y$. If we could observe $Z$, we would run a linear regression of $y$ on $D$ and $Z$ to estimate the following model:&lt;/p&gt;
&lt;p&gt;$$
y = \alpha D + \gamma Z + \varepsilon
$$&lt;/p&gt;
&lt;p&gt;where $\alpha$ is the effect of interest. This regression is usually referred to as the &lt;strong&gt;long regression&lt;/strong&gt; since it includes all variables of the model.&lt;/p&gt;
&lt;p&gt;However, since we do not observe $Z$, we have to estimate the following model:&lt;/p&gt;
&lt;p&gt;$$
y = \alpha D + u
$$&lt;/p&gt;
&lt;p&gt;The corresponding regression is usually referred to as the &lt;strong&gt;short regression&lt;/strong&gt; since it does not include all the variables of the model&lt;/p&gt;
&lt;p&gt;What is the &lt;strong&gt;consequence&lt;/strong&gt; of estimating the short regression when the true model is the long one?&lt;/p&gt;
&lt;p&gt;In that case, the OLS estimator of $\alpha$ is&lt;/p&gt;
&lt;p&gt;$$
\begin{align}
\hat \alpha &amp;amp;= \frac{Cov(D, y)}{Var(D)} =
\newline
&amp;amp;= \frac{Cov(D, \alpha D + \gamma Z + \varepsilon)}{Var(D)} =
\newline
&amp;amp;= \frac{Cov(D, \alpha D)}{Var(D)} + \frac{Cov(D, \gamma Z)}{Var(D)} + \frac{Cov(D, \varepsilon)}{Var(D)} =
\newline
&amp;amp;= \alpha + \underbrace{ \gamma \frac{Cov(D, Z)}{Var(D)} }_{\text{omitted variable bias}}
\end{align}
$$&lt;/p&gt;
&lt;p&gt;Therefore, we can write the &lt;strong&gt;omitted variable bias&lt;/strong&gt; as&lt;/p&gt;
&lt;p&gt;$$
\text{OVB} = \gamma \delta \qquad \text{ where } \qquad \delta := \frac{Cov(D, Z)}{Var(D)}
$$&lt;/p&gt;
&lt;p&gt;The beauty of this formula is its &lt;strong&gt;interpretability&lt;/strong&gt;: the omitted variable bias consists in just &lt;strong&gt;two components&lt;/strong&gt;, both extremely easy to interpret.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\gamma$: the effect of $Z$ on $y$&lt;/li&gt;
&lt;li&gt;$\delta$: the effect of $D$ on $Z$&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;additional-controls&#34;&gt;Additional Controls&lt;/h3&gt;
&lt;p&gt;What happens if we had &lt;strong&gt;additional control variables&lt;/strong&gt; in the regression? For example, assume that besides the variable of interest $D$, we also observe a vector of other variables $X$ so that the &lt;strong&gt;long regression&lt;/strong&gt; is&lt;/p&gt;
&lt;p&gt;$$
y = \alpha D + \beta X + \gamma Z + \varepsilon
$$&lt;/p&gt;
&lt;p&gt;Thanks to the &lt;a href=&#34;https://towardsdatascience.com/59f801eb3299&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Frisch-Waugh-Lowell theorem&lt;/strong&gt;&lt;/a&gt;, we can simply &lt;strong&gt;partial-out&lt;/strong&gt; $X$ and express the omitted variable bias in terms of $D$ and $Z$.&lt;/p&gt;
&lt;p&gt;$$
\text{OVB} = \gamma \times \frac{Cov(D^{\perp X}, Z^{\perp X})}{Var(D^{\perp X})}
$$&lt;/p&gt;
&lt;p&gt;where $D^{\perp X}$ are the residuals from regressing $D$ on $X$ and $Z^{\perp X}$ are the residuals from regressing $Z$ on $X$. If you are not familiar with Frisch-Waugh-Lowell theorem, I have written a short &lt;a href=&#34;https://towardsdatascience.com/59f801eb3299&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;note here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2112.13398&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Chernozhukov, Cinelli, Newey, Sharma, Syrgkanis (2022)&lt;/a&gt; further generalize to analysis the the setting in which the control variables $X$ and the unobserved variables $Z$ enter the long model with a general functional form $f$&lt;/p&gt;
&lt;p&gt;$$
y = \alpha D + f(Z, X) + \varepsilon
$$&lt;/p&gt;
&lt;p&gt;You can find more details in their paper, but the underlying idea remains the same.&lt;/p&gt;
&lt;h2 id=&#34;example&#34;&gt;Example&lt;/h2&gt;
&lt;p&gt;Suppose we were a researcher interested in the relationship between &lt;strong&gt;education&lt;/strong&gt; and &lt;strong&gt;wages&lt;/strong&gt;. Does investing in education pay off in terms of future wages? Suppose we had data on wages for people with different years of education. Why not looking at the correlation between years of education and wages?&lt;/p&gt;
&lt;p&gt;The problem is that there might be many &lt;strong&gt;unobserved variables&lt;/strong&gt; that are correlated with both education and wages. For simplicity, let&amp;rsquo;s concentrate on &lt;strong&gt;ability&lt;/strong&gt;. People of higher ability might decide to invest more in education just because they are better in school and they get more opportunities. On the other hand, they might also get higher wages afterwards, purely because of their innate ability.&lt;/p&gt;
&lt;p&gt;We can represent the data generating process with the following &lt;strong&gt;Directed Acyclic Graph&lt;/strong&gt; (DAG).&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-mermaid&#34;&gt;flowchart TD
classDef included fill:#DCDCDC,stroke:#000000,stroke-width:2px;
classDef excluded fill:#ffffff,stroke:#000000,stroke-width:2px;
classDef unobserved fill:#ffffff,stroke:#000000,stroke-width:2px,stroke-dasharray: 5 5;

D((education))
Z((ability))
Y((wage))
X1((age))
X2((gender))

D --&amp;gt; Y
Z --&amp;gt; D
Z --&amp;gt; Y
X1 --&amp;gt; Y
X2 --&amp;gt; Y

class D,Y included;
class X1,X2 excluded;
class Z unobserved;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s load and inspect the &lt;strong&gt;data&lt;/strong&gt;. I import the data generating process from &lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/src/dgp.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;src.dgp&lt;/code&gt;&lt;/a&gt; and some plotting functions and libraries from &lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/src/utils.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;src.utils&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;%matplotlib inline
%config InlineBackend.figure_format = &#39;retina&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from src.utils import *
from src.dgp import dgp_educ_wages

df = dgp_educ_wages().generate_data(N=50)
df.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;age&lt;/th&gt;
      &lt;th&gt;gender&lt;/th&gt;
      &lt;th&gt;education&lt;/th&gt;
      &lt;th&gt;wage&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;62&lt;/td&gt;
      &lt;td&gt;male&lt;/td&gt;
      &lt;td&gt;6.0&lt;/td&gt;
      &lt;td&gt;3800.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;44&lt;/td&gt;
      &lt;td&gt;male&lt;/td&gt;
      &lt;td&gt;8.0&lt;/td&gt;
      &lt;td&gt;4500.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;63&lt;/td&gt;
      &lt;td&gt;male&lt;/td&gt;
      &lt;td&gt;8.0&lt;/td&gt;
      &lt;td&gt;4700.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;33&lt;/td&gt;
      &lt;td&gt;male&lt;/td&gt;
      &lt;td&gt;7.0&lt;/td&gt;
      &lt;td&gt;3500.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;57&lt;/td&gt;
      &lt;td&gt;female&lt;/td&gt;
      &lt;td&gt;6.0&lt;/td&gt;
      &lt;td&gt;4000.0&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;We have information on &lt;strong&gt;300 individuals&lt;/strong&gt;, for which we observe their &lt;code&gt;age&lt;/code&gt;, their &lt;code&gt;gender&lt;/code&gt;, the years of &lt;code&gt;education&lt;/code&gt;, and the current monthly &lt;code&gt;wage&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Suppose we were directly regressing &lt;code&gt;wage&lt;/code&gt; on &lt;code&gt;education&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;short_model = smf.ols(&#39;wage ~ education + gender + age&#39;, df).fit()
short_model.summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
         &lt;td&gt;&lt;/td&gt;           &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt;      &lt;td&gt; 2657.8864&lt;/td&gt; &lt;td&gt;  444.996&lt;/td&gt; &lt;td&gt;    5.973&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt; 1762.155&lt;/td&gt; &lt;td&gt; 3553.618&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;gender[T.male]&lt;/th&gt; &lt;td&gt;  335.1075&lt;/td&gt; &lt;td&gt;  132.685&lt;/td&gt; &lt;td&gt;    2.526&lt;/td&gt; &lt;td&gt; 0.015&lt;/td&gt; &lt;td&gt;   68.027&lt;/td&gt; &lt;td&gt;  602.188&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;education&lt;/th&gt;      &lt;td&gt;   95.9437&lt;/td&gt; &lt;td&gt;   38.752&lt;/td&gt; &lt;td&gt;    2.476&lt;/td&gt; &lt;td&gt; 0.017&lt;/td&gt; &lt;td&gt;   17.940&lt;/td&gt; &lt;td&gt;  173.948&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;age&lt;/th&gt;            &lt;td&gt;   12.3120&lt;/td&gt; &lt;td&gt;    6.110&lt;/td&gt; &lt;td&gt;    2.015&lt;/td&gt; &lt;td&gt; 0.050&lt;/td&gt; &lt;td&gt;    0.013&lt;/td&gt; &lt;td&gt;   24.611&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;The coefficient of &lt;code&gt;education&lt;/code&gt; is positive and significant. However, we know there might be an &lt;strong&gt;omitted variable bias&lt;/strong&gt;, because we do not observe &lt;code&gt;ability&lt;/code&gt;. In terms of DAGs, there is a &lt;strong&gt;backdoor path&lt;/strong&gt; from &lt;code&gt;education&lt;/code&gt; to &lt;code&gt;wage&lt;/code&gt; passing through &lt;code&gt;ability&lt;/code&gt; that is not blocked and therefore biases our estimate.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-mermaid&#34;&gt;flowchart TD
classDef included fill:#DCDCDC,stroke:#000000,stroke-width:2px;
classDef excluded fill:#ffffff,stroke:#000000,stroke-width:2px;
classDef unobserved fill:#ffffff,stroke:#000000,stroke-width:2px,stroke-dasharray: 5 5;

D((education))
Z((ability))
Y((wage))
X1((age))
X2((gender))

D --&amp;gt; Y
Z --&amp;gt; D
Z --&amp;gt; Y
X1 --&amp;gt; Y
X2 --&amp;gt; Y

class D,Y included;
class X1,X2 excluded;
class Z unobserved;

linkStyle 0 stroke:#00ff00,stroke-width:4px;
linkStyle 1,2 stroke:#ff0000,stroke-width:4px;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Does it mean that all our analysis is &lt;strong&gt;garbage&lt;/strong&gt;? Can we still draw some causal conclusion from the regression results?&lt;/p&gt;
&lt;h2 id=&#34;direction-of-the-bias&#34;&gt;Direction of the Bias&lt;/h2&gt;
&lt;p&gt;If we knew the signs of $\gamma$ and $\delta$, we could infer the sign of the bias, since it&amp;rsquo;s the product of the two signs.&lt;/p&gt;
&lt;p&gt;$$
\text{OVB} = \gamma \delta \qquad \text{ where } \qquad \gamma := \frac{Cov(Z, y)}{Var(Z)}, \quad \delta := \frac{Cov(D, Z)}{Var(D)}
$$&lt;/p&gt;
&lt;p&gt;which in our example is&lt;/p&gt;
&lt;p&gt;$$
\text{OVB} = \gamma \delta \qquad \text{ where } \qquad \gamma := \frac{Cov(\text{ability}, \text{wage})}{Var(\text{ability})}, \quad \delta := \frac{Cov(\text{education}, \text{ability})}{Var(\text{education})}
$$&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s analyze the two correlations separately:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The correlation between &lt;code&gt;ability&lt;/code&gt; and &lt;code&gt;wage&lt;/code&gt; is most likely positive&lt;/li&gt;
&lt;li&gt;The correlation between &lt;code&gt;ability&lt;/code&gt; and &lt;code&gt;education&lt;/code&gt; is most likely positive&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Therefore, the bias is most likely &lt;strong&gt;positive&lt;/strong&gt;. From this, we can conclude that our estimate from the regression on &lt;code&gt;wage&lt;/code&gt; on &lt;code&gt;education&lt;/code&gt; is most likely an &lt;strong&gt;overestimate&lt;/strong&gt; of the true effect, which is most likely smaller.&lt;/p&gt;
&lt;p&gt;This might seem like a small insight, but it&amp;rsquo;s actually huge. Now we can say with confidence that one year of &lt;code&gt;education&lt;/code&gt; increases &lt;code&gt;wages&lt;/code&gt; by &lt;strong&gt;at most&lt;/strong&gt; 95 dollars per month, which is a much more informative statement than just saying that the estimate is biased.&lt;/p&gt;
&lt;p&gt;In general, we can summarize the different possible effects of the bias in a 2-by-2 &lt;strong&gt;table&lt;/strong&gt;.&lt;/p&gt;
&lt;img src=&#34;other/ovb.png&#34; width=80% /&gt;
&lt;h2 id=&#34;further-sensitivity-analysis&#34;&gt;Further Sensitivity Analysis&lt;/h2&gt;
&lt;p&gt;Can we say &lt;strong&gt;more&lt;/strong&gt; about the omitted variable bias without making strong assumptions?&lt;/p&gt;
&lt;p&gt;The answer is yes! In particular, we can ask ourselves: how strong should the partial correlations $\gamma$ and $\delta$ be in order to &lt;strong&gt;overturn&lt;/strong&gt; our conclusion?&lt;/p&gt;
&lt;p&gt;In our example, we found a positive correlation between &lt;code&gt;education&lt;/code&gt; and &lt;code&gt;wages&lt;/code&gt; in the data. However, we know that we are omitting &lt;code&gt;ability&lt;/code&gt; in the regression. The question is: how strong should the correlation between &lt;code&gt;ability&lt;/code&gt; and &lt;code&gt;wage&lt;/code&gt;, $\gamma$, and between &lt;code&gt;ability&lt;/code&gt; and &lt;code&gt;education&lt;/code&gt;, $\delta$, be in order to make the effect not significant or even negative?&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://rss.onlinelibrary.wiley.com/doi/full/10.1111/rssb.12348&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Cinelli and Hazlett (2020)&lt;/a&gt; show that we can transform this question in terms of residual variation explained, i.e. the &lt;a href=&#34;https://en.wikipedia.org/wiki/Coefficient_of_determination&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;coefficient of determination, $R^2$&lt;/a&gt;. The advantage of this approach is &lt;strong&gt;interpretability&lt;/strong&gt;. It is much easier to make a guess about the percentage of variance explained than to make a guess about the magnitude of a conditional correlation.&lt;/p&gt;
&lt;p&gt;The authors wrote a companion package &lt;a href=&#34;https://github.com/carloscinelli/sensemakr&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;sensemakr&lt;/code&gt;&lt;/a&gt; to conduct the sensitivity analysis. You can find a detailed description of the package &lt;a href=&#34;https://cran.r-project.org/web/packages/sensemakr/vignettes/sensemakr.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;We will now use the &lt;code&gt;Sensemakr&lt;/code&gt; function. The main &lt;strong&gt;arguments&lt;/strong&gt; of the &lt;code&gt;Sensemakr&lt;/code&gt; function are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;model&lt;/code&gt;: the regression model we want to analyze&lt;/li&gt;
&lt;li&gt;&lt;code&gt;treatment&lt;/code&gt;: the feature/covariate of interest, in our case &lt;code&gt;education&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The question we will try to answer is the following:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;How much of the residual variation in &lt;code&gt;education&lt;/code&gt; (x axis) and &lt;code&gt;wage&lt;/code&gt; (y axis) does &lt;code&gt;ability&lt;/code&gt; need to explain in order for the effect of &lt;code&gt;education&lt;/code&gt; on &lt;code&gt;wages&lt;/code&gt; to &lt;strong&gt;change sign&lt;/strong&gt;?&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import sensemakr

sensitivity = sensemakr.Sensemakr(model = short_model, treatment = &amp;quot;education&amp;quot;)
sensitivity.plot()
plt.xlabel(&amp;quot;Partial $R^2$ of ability with education&amp;quot;);
plt.ylabel(&amp;quot;Partial $R^2$ of ability with wage&amp;quot;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/ovb_17_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;In the &lt;strong&gt;plot&lt;/strong&gt;, we see how the partial (because conditional on &lt;code&gt;age&lt;/code&gt; and &lt;code&gt;gender&lt;/code&gt;) $R^2$ of &lt;code&gt;ability&lt;/code&gt; with &lt;code&gt;education&lt;/code&gt; and &lt;code&gt;wage&lt;/code&gt; affects the estimated coefficient of &lt;code&gt;education&lt;/code&gt; on &lt;code&gt;wage&lt;/code&gt;. The $(0,0)$ coordinate, marked with a &lt;strong&gt;triangle&lt;/strong&gt;, corresponds to the current estimate and reflects what would happen if &lt;code&gt;ability&lt;/code&gt; had no explanatory power for both &lt;code&gt;wage&lt;/code&gt; with &lt;code&gt;education&lt;/code&gt;: nothing. As the explanatory power of &lt;code&gt;ability&lt;/code&gt; grows (moving upwards and rightwards from the triangle), the estimated coefficient decreases, as marked by the &lt;strong&gt;level curves&lt;/strong&gt;, until it becomes zero at the &lt;strong&gt;dotted red line&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;How should we &lt;strong&gt;interpret&lt;/strong&gt; the plot? We can see that we need &lt;code&gt;ability&lt;/code&gt; to explain around 30% of the residual variation in both &lt;code&gt;education&lt;/code&gt; and &lt;code&gt;wage&lt;/code&gt; in order for the effect of &lt;code&gt;education&lt;/code&gt; on &lt;code&gt;wages&lt;/code&gt; to disappear, corresponding to the red line.&lt;/p&gt;
&lt;p&gt;One question that you might (legitimately) have now is: what is 30%? Is it big or is it small? We can get a sense of the &lt;strong&gt;magnitude&lt;/strong&gt; of the partial $R^2$ by &lt;strong&gt;benchmarking&lt;/strong&gt; the results with the residual variance explained by another &lt;em&gt;observed&lt;/em&gt; variable. Let&amp;rsquo;s use &lt;code&gt;age&lt;/code&gt; for example.&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;Sensemakr&lt;/code&gt; function accepts the following optional arguments:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;benchmark_covariates&lt;/code&gt;: the covariate to use as a benchmark&lt;/li&gt;
&lt;li&gt;&lt;code&gt;kd&lt;/code&gt; and &lt;code&gt;ky&lt;/code&gt;: these arguments parameterize how many times stronger the unobserved variable (&lt;code&gt;ability&lt;/code&gt;) is related to the treatment (&lt;code&gt;kd&lt;/code&gt;) and to the outcome (&lt;code&gt;ky&lt;/code&gt;) in comparison to the observed benchmark covariate (&lt;code&gt;age&lt;/code&gt;). In our example, setting &lt;code&gt;kd&lt;/code&gt; and &lt;code&gt;ky&lt;/code&gt; equal to $[0.5, 1, 2]$ means we want to investigate the maximum strength of a variable half, same, or twice as strong as &lt;code&gt;age&lt;/code&gt; (in explaining &lt;code&gt;education&lt;/code&gt; and &lt;code&gt;wage&lt;/code&gt; variation).&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sensitivity = sensemakr.Sensemakr(model = short_model, 
                                  treatment = &amp;quot;education&amp;quot;,
                                  benchmark_covariates = &amp;quot;age&amp;quot;,
                                  kd = [0.5, 1, 2],
                                  ky = [0.5, 1, 2])
sensitivity.plot()
plt.xlabel(&amp;quot;Partial $R^2$ of ability with education&amp;quot;);
plt.ylabel(&amp;quot;Partial $R^2$ of ability with wage&amp;quot;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/ovb_19_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;It looks like even if &lt;code&gt;ability&lt;/code&gt; had twice as much explanatory power as &lt;code&gt;age&lt;/code&gt;, the effect of &lt;code&gt;education&lt;/code&gt; on &lt;code&gt;wage&lt;/code&gt; would still be positive. But would it be &lt;strong&gt;statistically significant&lt;/strong&gt;?&lt;/p&gt;
&lt;p&gt;We can repeat the same exercise, looking at the t-statistic instead of the magnitude of the coefficient. We just need to set the &lt;code&gt;sensitivity_of&lt;/code&gt; option in the plotting function equal to &lt;code&gt;t-value&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;The question that we are trying to answer in this case is:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;How much of the residual variation in &lt;code&gt;education&lt;/code&gt; (x axis) and &lt;code&gt;wage&lt;/code&gt; (y axis) does &lt;code&gt;ability&lt;/code&gt; need to explain in order for the effect of &lt;code&gt;education&lt;/code&gt; on &lt;code&gt;wages&lt;/code&gt; to &lt;strong&gt;become not significant&lt;/strong&gt;?&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sensitivity.plot(sensitivity_of = &#39;t-value&#39;)
plt.xlabel(&amp;quot;Partial $R^2$ of ability with education&amp;quot;);
plt.ylabel(&amp;quot;Partial $R^2$ of ability with wage&amp;quot;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/ovb_21_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;From the plot, we can see, we need &lt;code&gt;ability&lt;/code&gt; to explain around 5% to 10% of the residual variation in both &lt;code&gt;education&lt;/code&gt; and &lt;code&gt;wage&lt;/code&gt; in order for the effect of &lt;code&gt;education&lt;/code&gt; on &lt;code&gt;wage&lt;/code&gt; not to be significant. In particular, the red line plots the level curve for the t-statistic equal to 2.01, corresponding to a 5% significance level. From the comparison with &lt;code&gt;age&lt;/code&gt;, we see that a slightly stronger explanatory power (bigger than &lt;code&gt;1.0x age&lt;/code&gt;) would be sufficient to make the coefficient of &lt;code&gt;education&lt;/code&gt; on &lt;code&gt;wage&lt;/code&gt; not statistically significant.&lt;/p&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;In this post, I have introduced the concept of &lt;strong&gt;omitted variable bias&lt;/strong&gt;. We have seen how it&amp;rsquo;s computed in a simple linear model and how we can exploit qualitative information about the variables to make inference in presence of omitted variable bias.&lt;/p&gt;
&lt;p&gt;These tools are extremely useful since omitted variable bias is essentially &lt;strong&gt;everywhere&lt;/strong&gt;. First of all, there are always factors that we do not observe, such as ability in our toy example. However, even if we could observe everything, omitted variable bias can also emerge in the form of &lt;strong&gt;model misspecification&lt;/strong&gt;. Suppose that &lt;code&gt;wages&lt;/code&gt; depended on &lt;code&gt;age&lt;/code&gt; in a quadratic way. Then, omitting the quadratic term from the regression introduces bias, which can be analyzed with the same tools we have used for &lt;code&gt;ability&lt;/code&gt;.&lt;/p&gt;
&lt;h3 id=&#34;references&#34;&gt;References&lt;/h3&gt;
&lt;p&gt;[1] C. Cinelli, C. Hazlett, &lt;a href=&#34;https://rss.onlinelibrary.wiley.com/doi/full/10.1111/rssb.12348&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Making Sense of Sensitivity: Extending Omitted Variable Bias&lt;/a&gt; (2019), &lt;em&gt;Journal of the Royal Statistical Society&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;[2] V. Chernozhukov, C. Cinelli, W. Newey, A. Sharma, V. Syrgkanis, &lt;a href=&#34;https://arxiv.org/abs/2112.13398&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Long Story Short: Omitted Variable Bias in Causal Machine Learning&lt;/a&gt; (2022), working paper.&lt;/p&gt;
&lt;h3 id=&#34;related-articles&#34;&gt;Related Articles&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/59f801eb3299&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The FWL Theorem, Or How To Make Regressions Intuitive&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/b63dc69e3d8c&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;DAGs and Control Variables&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;code&#34;&gt;Code&lt;/h3&gt;
&lt;p&gt;You can find the original Jupyter Notebook here:&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/ovb.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/matteocourthoud/Blog-Posts/blob/main/ovb.ipynb&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Goodbye Scatterplot, Welcome Binned Scatterplot</title>
      <link>https://matteocourthoud.github.io/post/binscatter/</link>
      <pubDate>Sat, 21 May 2022 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/post/binscatter/</guid>
      <description>&lt;p&gt;When we want to visualize the relationship between two continuous variables, the go-to plot is the &lt;strong&gt;scatterplot&lt;/strong&gt;. It&amp;rsquo;s a very intuitive visualization tool that allows us to directly look at the data. However, when we have a lot of data and/or when the data is skewed, scatterplots can be too noisy to be informative.&lt;/p&gt;
&lt;p&gt;In this blog post, I am going to review a very powerful alternative to the scatterplot to visualize correlations between two variables: the &lt;strong&gt;binned scatterplot&lt;/strong&gt;. Binned scatterplots are not only a great visualization tool, but they can also be used to do inference on the conditional distribution of the dependent variable.&lt;/p&gt;
&lt;h2 id=&#34;the-scatterplot&#34;&gt;The Scatterplot&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s start with an example. Suppose we are an &lt;strong&gt;online marketplace&lt;/strong&gt; where multiple firms offer goods that consumer can efficiently browse, compare and buy. Our &lt;strong&gt;dataset&lt;/strong&gt; consists in a snapshot of the firms active on the marketplace.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s load the data and have a look at it. You can find the code for the data generating process &lt;a href=&#34;&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;%matplotlib inline
%config InlineBackend.figure_format = &#39;retina&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from src.utils import *
from src.dgp import dgp_marketplace

df = dgp_marketplace().generate_data(N=10_000)
df.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;age&lt;/th&gt;
      &lt;th&gt;sales&lt;/th&gt;
      &lt;th&gt;online&lt;/th&gt;
      &lt;th&gt;products&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;0.312777&lt;/td&gt;
      &lt;td&gt;450.858091&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;1.176221&lt;/td&gt;
      &lt;td&gt;1121.882449&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;1.764048&lt;/td&gt;
      &lt;td&gt;2698.714549&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;1.082742&lt;/td&gt;
      &lt;td&gt;1627.746386&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;3.156503&lt;/td&gt;
      &lt;td&gt;1464.593939&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;We have information on 10.000 firms. For each firm we know:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;age&lt;/code&gt;: the age of the firm&lt;/li&gt;
&lt;li&gt;&lt;code&gt;sales&lt;/code&gt;: the monthly sales from last month&lt;/li&gt;
&lt;li&gt;&lt;code&gt;online&lt;/code&gt;: whether the firm is only active online&lt;/li&gt;
&lt;li&gt;&lt;code&gt;products&lt;/code&gt;: the number of products that the firm offers&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Suppose we are interested in understanding the relationship between &lt;code&gt;age&lt;/code&gt; and &lt;code&gt;sales&lt;/code&gt;. What is the &lt;strong&gt;life-cycle&lt;/strong&gt; of sales?&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s start with a simple &lt;strong&gt;scatterplot&lt;/strong&gt; of &lt;code&gt;sales&lt;/code&gt; over &lt;code&gt;age&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sns.scatterplot(x=&#39;age&#39;, y=&#39;sales&#39;, data=df);
plt.title(&amp;quot;Sales by firm&#39;s age&amp;quot;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/binscatter_7_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;The plot is extremely &lt;strong&gt;noisy&lt;/strong&gt;. We have a lot of observations, therefore, it is very difficult to visualize them all. If we had to guess, we could say that the relationship looks negative (&lt;code&gt;sales&lt;/code&gt; decrease with &lt;code&gt;age&lt;/code&gt;), but it would be a very uninformed guess.&lt;/p&gt;
&lt;p&gt;We are now going to explore some plausible tweaks and alternatives.&lt;/p&gt;
&lt;h2 id=&#34;scatterplot-alternatives&#34;&gt;Scatterplot Alternatives&lt;/h2&gt;
&lt;p&gt;What can we do when we have an extremely dense scatterplot? One solution could be to plot the &lt;strong&gt;density&lt;/strong&gt; of the observations, instead of the observations themselves.&lt;/p&gt;
&lt;p&gt;There are multiple solutions in Python to visualize the density of a 2-dimensional distribution. A very useful one is &lt;a href=&#34;https://seaborn.pydata.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;seaborn&lt;/a&gt; &lt;a href=&#34;https://seaborn.pydata.org/generated/seaborn.jointplot.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;jointplot&lt;/a&gt;. &lt;code&gt;jointplot&lt;/code&gt; plots the joint distribution of two variables, together with the marginal distributions along the axis. The default option is the scatterplot, but one can also choose to add a regression line (&lt;code&gt;reg&lt;/code&gt;), change the plot to a histogram (&lt;code&gt;hist&lt;/code&gt;), a hexplot (&lt;code&gt;hex&lt;/code&gt;), or a &lt;a href=&#34;https://en.wikipedia.org/wiki/Kernel_density_estimation&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;kernel density estimate&lt;/a&gt; (&lt;code&gt;kde&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s try the &lt;strong&gt;hexplot&lt;/strong&gt;, which is basically a histogram of the data, where the bins are hexagons, in the 2-dimensional space.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;s = sns.jointplot(x=&#39;age&#39;, y=&#39;sales&#39;, data=df, kind=&#39;hex&#39;, );
s.ax_joint.grid(False);
s.ax_marg_y.grid(False);
s.fig.suptitle(&amp;quot;Sales by firm&#39;s age&amp;quot;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/binscatter_11_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Not much has changed. It looks like the distributions of &lt;code&gt;age&lt;/code&gt; and &lt;code&gt;sales&lt;/code&gt; are both very &lt;strong&gt;skewed&lt;/strong&gt; and, therefore, most of the action is concentrated in a very small subspace.&lt;/p&gt;
&lt;p&gt;Maybe we could remove &lt;strong&gt;outliers&lt;/strong&gt; and zoom-in on the area where most of the data is located. Let&amp;rsquo;s zoom-in on the bottom-left corner, on observations what have &lt;code&gt;age &amp;lt; 3&lt;/code&gt; and &lt;code&gt;sales &amp;lt; 3000&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;s = sns.jointplot(x=&#39;age&#39;, y=&#39;sales&#39;, data=df.query(&amp;quot;age &amp;lt; 3 &amp;amp; sales &amp;lt; 3000&amp;quot;), kind=&amp;quot;hex&amp;quot;);
s.ax_joint.grid(False);
s.ax_marg_y.grid(False);
s.fig.suptitle(&amp;quot;Sales by firm&#39;s age&amp;quot;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/binscatter_13_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Now there is much less empty space, but it does not look like we are going far. The joint distribution is &lt;strong&gt;still too skewed&lt;/strong&gt;. This is the case when the data follows some power distribution, as it&amp;rsquo;s often the case with business data.&lt;/p&gt;
&lt;p&gt;One solution is to &lt;strong&gt;transform&lt;/strong&gt; the variable, by taking the &lt;a href=&#34;https://en.wikipedia.org/wiki/Natural_logarithm&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;natural logarithm&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df[&#39;log_age&#39;] = np.log(df[&#39;age&#39;])
df[&#39;log_sales&#39;] = np.log(df[&#39;sales&#39;])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can now plot the relationship between the logarithms of &lt;code&gt;age&lt;/code&gt; and &lt;code&gt;sales&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;s = sns.jointplot(x=&#39;log_age&#39;, y=&#39;log_sales&#39;, data=df, kind=&#39;hex&#39;);
s.ax_joint.grid(False);
s.ax_marg_y.grid(False);
s.fig.suptitle(&amp;quot;Sales by firm&#39;s age&amp;quot;, y=1.02);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/binscatter_17_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;The logarithm definitely helped. Now the data is more spread across space, which means that the visualization is more informative. Moreover, it looks like there is &lt;strong&gt;no relationship&lt;/strong&gt; between the two variables.&lt;/p&gt;
&lt;p&gt;However, there is still &lt;strong&gt;too much noise&lt;/strong&gt;. Maybe data visualization alone is not sufficient do draw a conclusion.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s swap to a more structured approach: &lt;a href=&#34;https://en.wikipedia.org/wiki/Linear_regression&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;linear regression&lt;/strong&gt;&lt;/a&gt;. Let&amp;rsquo;s linearly regress &lt;code&gt;log_sales&lt;/code&gt; on &lt;code&gt;log_age&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;smf.ols(&#39;log_sales ~ log_age&#39;, df).fit().summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
      &lt;td&gt;&lt;/td&gt;         &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt; &lt;td&gt;    7.3971&lt;/td&gt; &lt;td&gt;    0.015&lt;/td&gt; &lt;td&gt;  478.948&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    7.367&lt;/td&gt; &lt;td&gt;    7.427&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;log_age&lt;/th&gt;   &lt;td&gt;    0.1690&lt;/td&gt; &lt;td&gt;    0.010&lt;/td&gt; &lt;td&gt;   16.888&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    0.149&lt;/td&gt; &lt;td&gt;    0.189&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;The regression coefficient for &lt;code&gt;log_age&lt;/code&gt; is &lt;strong&gt;positive&lt;/strong&gt; and statistically significant (i.e. different from zero). It seems that all previous visualizations were very &lt;strong&gt;misleading&lt;/strong&gt;. From none of the graphs above we could have guessed such a strong positive relationship.&lt;/p&gt;
&lt;p&gt;However, maybe this relationship is different for &lt;code&gt;online&lt;/code&gt;-only firms and the rest of the sample. We need to control for this variable in order to avoid &lt;a href=&#34;https://en.wikipedia.org/wiki/Simpson%27s_paradox&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Simpson&amp;rsquo;s Paradox&lt;/a&gt; and, more generally, bias.&lt;/p&gt;
&lt;p&gt;With linear regression, we can &lt;strong&gt;condition the analysis on covariates&lt;/strong&gt;. Let&amp;rsquo;s add the binary indicator for &lt;code&gt;online&lt;/code&gt;-only firms and the variable counting the number of &lt;code&gt;products&lt;/code&gt; to the regression.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;smf.ols(&#39;log_sales ~ log_age + online + products&#39;, df).fit().summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
      &lt;td&gt;&lt;/td&gt;         &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt; &lt;td&gt;    6.5717&lt;/td&gt; &lt;td&gt;    0.037&lt;/td&gt; &lt;td&gt;  176.893&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    6.499&lt;/td&gt; &lt;td&gt;    6.644&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;log_age&lt;/th&gt;   &lt;td&gt;    0.0807&lt;/td&gt; &lt;td&gt;    0.010&lt;/td&gt; &lt;td&gt;    7.782&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    0.060&lt;/td&gt; &lt;td&gt;    0.101&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;online&lt;/th&gt;    &lt;td&gt;    0.1447&lt;/td&gt; &lt;td&gt;    0.027&lt;/td&gt; &lt;td&gt;    5.433&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    0.092&lt;/td&gt; &lt;td&gt;    0.197&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;products&lt;/th&gt;  &lt;td&gt;    0.3456&lt;/td&gt; &lt;td&gt;    0.014&lt;/td&gt; &lt;td&gt;   24.110&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    0.317&lt;/td&gt; &lt;td&gt;    0.374&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;The coefficient for &lt;code&gt;log_age&lt;/code&gt; is still positive and statistically significant, but its &lt;strong&gt;magnitude&lt;/strong&gt; has halved.&lt;/p&gt;
&lt;p&gt;What should we conclude? It seems that &lt;code&gt;sales&lt;/code&gt; increase over age, on average. However, this pattern might be very &lt;strong&gt;non-linear&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Within the linear regression framework, one approach could be to &lt;strong&gt;add extra terms&lt;/strong&gt; such as polynomials (&lt;code&gt;age^2&lt;/code&gt;) or categorical features (e.g. &lt;code&gt;age &amp;lt; 2&lt;/code&gt;). However, it would be really cool if there was a more &lt;strong&gt;flexible&lt;/strong&gt; (i.e. &lt;a href=&#34;https://en.wikipedia.org/wiki/Nonparametric_statistics&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;non-parametric&lt;/a&gt;) approach that could inform us on the relationship between firm &lt;code&gt;age&lt;/code&gt; and &lt;code&gt;sales&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;If only&amp;hellip;&lt;/p&gt;
&lt;h2 id=&#34;the-binned-scatterplot&#34;&gt;The Binned Scatterplot&lt;/h2&gt;
&lt;p&gt;The &lt;strong&gt;binned scatterplot&lt;/strong&gt; is a very powerful tool that provides a &lt;strong&gt;flexible&lt;/strong&gt; and &lt;strong&gt;parsimonious&lt;/strong&gt; way of visualizing and summarizing conditional means (and not only) in large datasets.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;idea&lt;/strong&gt; behind the binned scatterplot is to divide the conditioning variable, &lt;code&gt;age&lt;/code&gt; in our example, into &lt;strong&gt;equally sized bins or quantiles&lt;/strong&gt;, and then plot the conditional mean of the dependent variable, &lt;code&gt;sales&lt;/code&gt; in our example, within each bin.&lt;/p&gt;
&lt;h3 id=&#34;details&#34;&gt;Details&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/1902.09608&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Cattaneo, Crump, Farrell, Feng (2021)&lt;/a&gt; have built an extremely good package for binned scatterplots in R, &lt;a href=&#34;https://cran.r-project.org/web/packages/binsreg/index.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;binsreg&lt;/a&gt;. Moreover, they have ported the package to Python. We can install &lt;code&gt;binsreg&lt;/code&gt; directly from pip using &lt;code&gt;pip install binsreg&lt;/code&gt;. You can find more information on the Python package &lt;a href=&#34;https://pypi.org/project/binsreg/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;, while the original and detailed R package documentation can be found &lt;a href=&#34;https://www.rdocumentation.org/packages/binsreg/versions/0.7/topics/binsreg&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The most important choice when building a binned scatterplot is the &lt;strong&gt;number of bins&lt;/strong&gt;. The trade-off is the usual &lt;a href=&#34;https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;bias-variance trade-off&lt;/strong&gt;&lt;/a&gt;. By picking a higher number of bins, we have more points in the graph. In the extreme, we end up having a standard &lt;strong&gt;scatterplot&lt;/strong&gt; (assuming the conditioning variable is continuous). On the other hand, by decreasing the number bins, the plot will be more stable. However, in the extreme, we will have a &lt;strong&gt;single point&lt;/strong&gt; representing the sample mean.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/1902.09608&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Cattaneo, Crump, Farrell, Feng (2021)&lt;/a&gt; prove that, in the basic binned scatterplot, the number of bins that minimizes the mean squared error is proportional to $n^{1/3}$, where $n$ is the number of observations. Therefore, in general, more observations lead to more bins.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://onlinelibrary.wiley.com/doi/abs/10.1002/smj.3199&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Starr and Goldfarb (2020)&lt;/a&gt; add the following consideration:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;&lt;em&gt;However other elements are also important. For example, holding the distribution of x constant, the more curvilinear the true relationship between x and y is, the more bins the algorithm will select (otherwise mean squared error will increase). This implies that even with large n, few bins will be chosen for relatively flat relationships. The calculation of the optimal number of bins in a basic binned scatterplot thus takes into account the amount and location of variation in the data available to identify the relationship between x and y.&lt;/em&gt;&amp;rdquo;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;It is strongly recommended to use the default optimal number of bins. However, one can also set a customized number of bins in &lt;code&gt;binsreg&lt;/code&gt; with the &lt;code&gt;nbins&lt;/code&gt; option.&lt;/p&gt;
&lt;p&gt;Binned scatterplots however, do not just compute conditional means, for optimally chosen intervals, but they can also provide &lt;strong&gt;inference&lt;/strong&gt; for these means. In particular, we can build &lt;strong&gt;confidence intervals&lt;/strong&gt; around each data point. In the &lt;code&gt;binsreg&lt;/code&gt; package, the option &lt;code&gt;ci&lt;/code&gt; adds confidence intervals to the estimation results. The option takes as input a tuple of parameters &lt;code&gt;(p, s)&lt;/code&gt; and uses a piecewise polynomial of degree &lt;code&gt;p&lt;/code&gt; with &lt;code&gt;s&lt;/code&gt; smoothness constraints to construct the confidence intervals. By default, the confidence intervals are not included in the plot. For what concerns the choice of &lt;code&gt;p&lt;/code&gt; and &lt;code&gt;s&lt;/code&gt;, the &lt;a href=&#34;https://www.rdocumentation.org/packages/binsreg/versions/0.7/topics/binsreg&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;package documentation&lt;/a&gt; reports:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;&lt;em&gt;Recommended specification is ci=c(3,3), which adds confidence intervals based on cubic B-spline estimate of the regression function of interest to the binned scatter plot.&lt;/em&gt;&amp;rdquo;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;binsreg&#34;&gt;Binsreg&lt;/h3&gt;
&lt;p&gt;One problem with the Python version of the package, is that is not very Python-ish. Therefore, I have wrapped the &lt;code&gt;binsreg&lt;/code&gt; package into a function &lt;code&gt;binscatter&lt;/code&gt; that takes care of cleaning and formatting the output in a nicely readable &lt;a href=&#34;https://pandas.pydata.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Pandas&lt;/a&gt; DataFrame.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import binsreg

def binscatter(**kwargs):
    # Estimate binsreg
    est = binsreg.binsreg(**kwargs)
    
    # Retrieve estimates
    df_est = pd.concat([d.dots for d in est.data_plot])
    df_est = df_est.rename(columns={&#39;x&#39;: kwargs.get(&amp;quot;x&amp;quot;), &#39;fit&#39;: kwargs.get(&amp;quot;y&amp;quot;)})
    
    # Add confidence intervals
    if &amp;quot;ci&amp;quot; in kwargs:
        df_est = pd.merge(df_est, pd.concat([d.ci for d in est.data_plot]))
        df_est = df_est.drop(columns=[&#39;x&#39;])
        df_est[&#39;ci&#39;] = df_est[&#39;ci_r&#39;] - df_est[&#39;ci_l&#39;]
    
    # Rename groups
    if &amp;quot;by&amp;quot; in kwargs:
        df_est[&#39;group&#39;] = df_est[&#39;group&#39;].astype(df[kwargs.get(&amp;quot;by&amp;quot;)].dtype)
        df_est = df_est.rename(columns={&#39;group&#39;: kwargs.get(&amp;quot;by&amp;quot;)})

    return df_est
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can now proceed to &lt;strong&gt;estimate&lt;/strong&gt; and &lt;strong&gt;visualize&lt;/strong&gt; the binned scatterplot for &lt;code&gt;age&lt;/code&gt; based on &lt;code&gt;sales&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Estimate binsreg
df_est = binscatter(x=&#39;age&#39;, y=&#39;sales&#39;, data=df, ci=(3,3))
df_est.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;group&lt;/th&gt;
      &lt;th&gt;age&lt;/th&gt;
      &lt;th&gt;bin&lt;/th&gt;
      &lt;th&gt;isknot&lt;/th&gt;
      &lt;th&gt;mid&lt;/th&gt;
      &lt;th&gt;sales&lt;/th&gt;
      &lt;th&gt;ci_l&lt;/th&gt;
      &lt;th&gt;ci_r&lt;/th&gt;
      &lt;th&gt;ci&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;Full Sample&lt;/td&gt;
      &lt;td&gt;0.012556&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1624.779616&lt;/td&gt;
      &lt;td&gt;1312.439124&lt;/td&gt;
      &lt;td&gt;1905.535412&lt;/td&gt;
      &lt;td&gt;593.096288&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;Full Sample&lt;/td&gt;
      &lt;td&gt;0.037015&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1664.078013&lt;/td&gt;
      &lt;td&gt;1435.438411&lt;/td&gt;
      &lt;td&gt;1893.888819&lt;/td&gt;
      &lt;td&gt;458.450408&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;Full Sample&lt;/td&gt;
      &lt;td&gt;0.065813&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1779.657894&lt;/td&gt;
      &lt;td&gt;1555.909281&lt;/td&gt;
      &lt;td&gt;1968.681960&lt;/td&gt;
      &lt;td&gt;412.772679&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;Full Sample&lt;/td&gt;
      &lt;td&gt;0.094486&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1976.464837&lt;/td&gt;
      &lt;td&gt;1740.530049&lt;/td&gt;
      &lt;td&gt;2216.800005&lt;/td&gt;
      &lt;td&gt;476.269956&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;Full Sample&lt;/td&gt;
      &lt;td&gt;0.125363&lt;/td&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;2015.833752&lt;/td&gt;
      &lt;td&gt;1796.489393&lt;/td&gt;
      &lt;td&gt;2280.237320&lt;/td&gt;
      &lt;td&gt;483.747927&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;The &lt;code&gt;binscatter&lt;/code&gt; function outputs a dataset in which, for each bin of the conditioning variable, &lt;code&gt;age&lt;/code&gt;, we have values and confidence intervals for the outcome variable, &lt;code&gt;sales&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;We can now plot the estimates.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Plot binned scatterplot
sns.scatterplot(x=&#39;age&#39;, y=&#39;sales&#39;, data=df_est);
plt.errorbar(&#39;age&#39;, &#39;sales&#39;, yerr=&#39;ci&#39;, data=df_est, ls=&#39;&#39;, lw=3, alpha=0.2);
plt.title(&amp;quot;Sales by firm&#39;s age&amp;quot;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/binscatter_33_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;The plot is quite revealing. Now the relationship looks extremely &lt;strong&gt;non-linear&lt;/strong&gt; with a sharp increase in &lt;code&gt;sales&lt;/code&gt; at the beginning of the lifetime of a firm, followed by a plateau.&lt;/p&gt;
&lt;p&gt;Moreover, the plot is also telling us information regarding the distributions of &lt;code&gt;age&lt;/code&gt; and &lt;code&gt;sales&lt;/code&gt;. In fact, the plot is more dense on the left, where the distribution of &lt;code&gt;age&lt;/code&gt; is concentrated. Also, confidence intervals are tighter on the left, where most of the conditional distribution of &lt;code&gt;sales&lt;/code&gt; lies.&lt;/p&gt;
&lt;p&gt;As we already discussed in the previous section, it might be important to control for other variables. For example, the number of &lt;code&gt;products&lt;/code&gt;, since firms that sell more products probably survive longer in the markets and also make more sales.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;binsreg&lt;/code&gt; allows to &lt;strong&gt;condition&lt;/strong&gt; the analysis on any number of variables, with the &lt;code&gt;w&lt;/code&gt; option.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Estimate binsreg
df_est = binscatter(x=&#39;age&#39;, y=&#39;sales&#39;, w=[&#39;products&#39;], data=df, ci=(3,3))

# Plot binned scatterplot
sns.scatterplot(x=&#39;age&#39;, y=&#39;sales&#39;, data=df_est);
plt.errorbar(&#39;age&#39;, &#39;sales&#39;, yerr=&#39;ci&#39;, data=df_est, ls=&#39;&#39;, lw=3, alpha=0.2);
plt.title(&amp;quot;Sales by firm&#39;s age&amp;quot;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/binscatter_35_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Conditional on number of &lt;code&gt;products&lt;/code&gt;, the shape of the &lt;code&gt;sales&lt;/code&gt; life-cycle changes further. Now, after an initial increase in sales, we observe a gradual decrease over time.&lt;/p&gt;
&lt;p&gt;Do &lt;code&gt;online&lt;/code&gt;-only firms have different &lt;code&gt;sales&lt;/code&gt; life-cycles with respect to mixed online-offline firms? We can produce different binned scatterplots &lt;strong&gt;by group&lt;/strong&gt; using the option &lt;code&gt;by&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Estimate binsreg
df_est = binscatter(x=&#39;age&#39;, y=&#39;sales&#39;, by=&#39;online&#39;, w=[&#39;products&#39;], data=df, ci=(3,3))

# Plot binned scatterplot
sns.scatterplot(x=&#39;age&#39;, y=&#39;sales&#39;, data=df_est, hue=&#39;online&#39;);
plt.errorbar(&#39;age&#39;, &#39;sales&#39;, yerr=&#39;ci&#39;, data=df_est.query(&amp;quot;online==0&amp;quot;), ls=&#39;&#39;, lw=3, alpha=0.2);
plt.errorbar(&#39;age&#39;, &#39;sales&#39;, yerr=&#39;ci&#39;, data=df_est.query(&amp;quot;online==1&amp;quot;), ls=&#39;&#39;, lw=3, alpha=0.2);
plt.title(&amp;quot;Sales by firm&#39;s age&amp;quot;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/binscatter_37_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;From the binned scatterplot, we can see that &lt;code&gt;online&lt;/code&gt; products have on average shorter lifetimes, with a higher initial peak in &lt;code&gt;sales&lt;/code&gt;, followed by a sharper decline.&lt;/p&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;In this blog post, we have analyzed a very powerful data visualization tool: the &lt;strong&gt;binned scatterplot&lt;/strong&gt;. In particular, we have seen how to use the &lt;code&gt;binsreg&lt;/code&gt; package to automatically pick the optimal number of bins and perform non-parametric inference on conditional means. However, the &lt;code&gt;binsreg&lt;/code&gt; package offers much more than that and I strongly recommend checking &lt;a href=&#34;https://cran.r-project.org/web/packages/binsreg/binsreg.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;its manual&lt;/a&gt; more in depth.&lt;/p&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;p&gt;[1] E Starr, B Goldfarb, &lt;a href=&#34;https://onlinelibrary.wiley.com/doi/abs/10.1002/smj.3199&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Binned Scatterplots: A Simple Tool to Make Research Easier and Better&lt;/a&gt; (2020), Strategic Management Journal.&lt;/p&gt;
&lt;p&gt;[2] M. D. Cattaneo, R. K. Crump, M. H. Farrell, Y. Feng, &lt;a href=&#34;https://arxiv.org/abs/1902.09608&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;On Binscatter&lt;/a&gt; (2021), working paper.&lt;/p&gt;
&lt;p&gt;[3] P. Goldsmith-Pinkham, &lt;a href=&#34;https://www.youtube.com/watch?v=fg9T2gPZCIs&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Lecture 6. Linear Regression II: Semiparametrics and Visualization&lt;/a&gt;, Applied Metrics PhD Course.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>The FWL Theorem, Or How To Make All Regressions Intuitive</title>
      <link>https://matteocourthoud.github.io/post/fwl/</link>
      <pubDate>Mon, 16 May 2022 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/post/fwl/</guid>
      <description>&lt;p&gt;&lt;em&gt;An introduction to the Frisch-Waugh-Lowell theorem and how to use it to gain intuition in linear regressions&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;The Frisch-Waugh-Lowell theorem is a &lt;strong&gt;simple&lt;/strong&gt; but yet &lt;strong&gt;powerful&lt;/strong&gt; theorem that allows us to reduce multivariate regressions to &lt;strong&gt;univariate&lt;/strong&gt; ones. This is extremely useful when we are interested in the relationship between two variables, but we still need to control for other factors, as it is often the case in &lt;strong&gt;causal inference&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;In this blog post, I am going to introduce the Frisch-Waugh-Lowell theorem and illustrate some interesting applications.&lt;/p&gt;
&lt;h2 id=&#34;the-theorem&#34;&gt;The Theorem&lt;/h2&gt;
&lt;p&gt;The theorem was first published by &lt;a href=&#34;https://www.jstor.org/stable/1907330&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Ragnar Frisch and Frederick Waugh in 1933&lt;/a&gt;. However, since its proof was lengthy and cumbersome, &lt;a href=&#34;https://www.tandfonline.com/doi/abs/10.1080/01621459.1963.10480682&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Michael Lovell in 1963&lt;/a&gt; provided a very simple and intuitive proof and his name was added to the theorem name.&lt;/p&gt;
&lt;p&gt;The theorem states that, when estimating a model of the form&lt;/p&gt;
&lt;p&gt;$$
y_i = \beta_1 x_{i,1} + \beta_2 x_{i,2} + \varepsilon_i
$$&lt;/p&gt;
&lt;p&gt;then, the following estimators of $\beta_1$ are equivalent:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the OLS estimator obtained by regressing $y$ on $x_1$ and $x_2$&lt;/li&gt;
&lt;li&gt;the OLS estimator obtained by regressing $y$ on $\tilde x_1$
&lt;ul&gt;
&lt;li&gt;where $\tilde x_1$ is the residual from the regression of $x_1$ on $x_2$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;the OLS estimator obtained by regressing $\tilde y$ on $\tilde x_1$
&lt;ul&gt;
&lt;li&gt;where $\tilde y$ is the residual from the regression of $y$ on $x_2$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;interpretation&#34;&gt;Interpretation&lt;/h3&gt;
&lt;p&gt;What did we actually &lt;strong&gt;learn&lt;/strong&gt;?&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;Frisch-Waugh-Lowell theorem&lt;/strong&gt; is telling us that there are multiple ways to estimate a single regression coefficient. One possibility is to run the full regression of $y$ on $x$, as usual.&lt;/p&gt;
&lt;p&gt;However, we can also regress $x_1$ on $x_2$, take the residuals, and regress $y$ only those residuals. The first part of this process is sometimes referred to as &lt;strong&gt;partialling-out&lt;/strong&gt; (or &lt;em&gt;orthogonalization&lt;/em&gt;, or &lt;em&gt;residualization&lt;/em&gt;) of $x_1$ with respect to $x_2$. The idea is that we are isolating the variation in $x_1$ that is &lt;em&gt;orthogonal&lt;/em&gt; to $x_2$. Note that $x_2$ can be also be multi-dimensional (i.e. include multiple variables and not just one).&lt;/p&gt;
&lt;p&gt;Why would one ever do that?&lt;/p&gt;
&lt;p&gt;This seems like a way more &lt;strong&gt;complicated&lt;/strong&gt; procedure. Instead of simply doing the regression in 1 step, now we need to do 2 or even 3 steps. It&amp;rsquo;s not intuitive at all. The main advantage comes from the fact that we have reduced a multivariate regression to a univariate one, making more tractable and more intuitive.&lt;/p&gt;
&lt;p&gt;We will later explore more in detail three &lt;strong&gt;applications&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;data visualization&lt;/li&gt;
&lt;li&gt;computational speed&lt;/li&gt;
&lt;li&gt;further applications for inference&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;However, let&amp;rsquo;s first explore the theorem more in detail with an example.&lt;/p&gt;
&lt;h2 id=&#34;example&#34;&gt;Example&lt;/h2&gt;
&lt;p&gt;Suppose we were a retail chain, owning many different stores in different locations. We come up with a brilliant &lt;strong&gt;idea to increase sales&lt;/strong&gt;: give away discounts in the form of &lt;strong&gt;coupons&lt;/strong&gt;. We print a lot of coupons and we distribute them around.&lt;/p&gt;
&lt;p&gt;To understand whether our marketing strategy worked, in each store, we check the average daily &lt;code&gt;sales&lt;/code&gt; and which percentage of shoppers used a &lt;code&gt;coupon&lt;/code&gt;. However, there is one &lt;strong&gt;problem&lt;/strong&gt;: we are worried that higher income people are less likely to use the discount, but usually they spend more. To be safe, we also record the average &lt;code&gt;income&lt;/code&gt; in the neighborhood of each store.&lt;/p&gt;
&lt;p&gt;We can represent the data generating process with a &lt;strong&gt;Directed Acyclic Graph&lt;/strong&gt; (DAG). If you are not familiar with DAGs, I have written a short introduction to &lt;a href=&#34;https://medium.com/towards-data-science/controls-b63dc69e3d8c&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Directed Acyclic Graphs here&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-mermaid&#34;&gt;flowchart LR
classDef included fill:#DCDCDC,stroke:#000000,stroke-width:2px;
classDef excluded fill:#ffffff,stroke:#000000,stroke-width:2px;
classDef unobserved fill:#ffffff,stroke:#000000,stroke-width:2px,stroke-dasharray: 5 5;

X1((coupons))
X2((income))
X3((weekday))
Y((sales))


X1 --&amp;gt; Y
X2 --&amp;gt; X1
X2 --&amp;gt; Y
X3 --&amp;gt; Y

class X1,X2,X3,Y excluded;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s load and inspect the &lt;strong&gt;data&lt;/strong&gt;. I import the data generating process from &lt;code&gt;src.dgp&lt;/code&gt; and some plotting functions and libraries from &lt;code&gt;src.utils&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;%matplotlib inline
%config InlineBackend.figure_format = &#39;retina&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from src.utils import *
from src.dgp import dgp_store_coupons

df = dgp_store_coupons().generate_data(N=50)
df.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;sales&lt;/th&gt;
      &lt;th&gt;coupons&lt;/th&gt;
      &lt;th&gt;income&lt;/th&gt;
      &lt;th&gt;dayofweek&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;821.7&lt;/td&gt;
      &lt;td&gt;0.199&lt;/td&gt;
      &lt;td&gt;66.243&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;602.3&lt;/td&gt;
      &lt;td&gt;0.245&lt;/td&gt;
      &lt;td&gt;43.882&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;655.1&lt;/td&gt;
      &lt;td&gt;0.162&lt;/td&gt;
      &lt;td&gt;44.718&lt;/td&gt;
      &lt;td&gt;5&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;625.8&lt;/td&gt;
      &lt;td&gt;0.269&lt;/td&gt;
      &lt;td&gt;39.270&lt;/td&gt;
      &lt;td&gt;4&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;696.6&lt;/td&gt;
      &lt;td&gt;0.186&lt;/td&gt;
      &lt;td&gt;58.654&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;We have information on &lt;strong&gt;50 stores&lt;/strong&gt;, for which we observe the percentage of customers that use &lt;code&gt;coupons&lt;/code&gt;, daily &lt;code&gt;sales&lt;/code&gt; (in thousand $), average &lt;code&gt;income&lt;/code&gt; of the neighborhood (in thousand $), and &lt;code&gt;day of the week&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Suppose we were directly regressing &lt;code&gt;sales&lt;/code&gt; on &lt;code&gt;coupon&lt;/code&gt; usage. What would we get? I represent the &lt;strong&gt;result&lt;/strong&gt; of the regression graphically, using &lt;code&gt;seaborn&lt;/code&gt; &lt;code&gt;regplot&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sns.regplot(x=&amp;quot;coupons&amp;quot;, y=&amp;quot;sales&amp;quot;, data=df, ci=False, line_kws={&#39;color&#39;:&#39;r&#39;, &#39;label&#39;:&#39;linear fit&#39;})
plt.legend()
plt.title(f&amp;quot;Sales and coupon usage&amp;quot;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/fwl_12_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;It looks like coupons were a &lt;strong&gt;bad idea&lt;/strong&gt;: in stores where coupons are used more, we observe lower sales.&lt;/p&gt;
&lt;p&gt;However, it might just be that people with higher income are using less coupons, while also spending more. If this was true, it could &lt;strong&gt;bias&lt;/strong&gt; our results. In terms of the DAG, it means that we have a &lt;strong&gt;backdoor path&lt;/strong&gt; passing through &lt;code&gt;income&lt;/code&gt;, generating a non-causal relationship.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-mermaid&#34;&gt;flowchart LR
classDef included fill:#DCDCDC,stroke:#000000,stroke-width:2px;
classDef excluded fill:#ffffff,stroke:#000000,stroke-width:2px;
classDef unobserved fill:#ffffff,stroke:#000000,stroke-width:2px,stroke-dasharray: 5 5;

X1((coupons))
X2((income))
X3((weekday))
Y((sales))


X1 --&amp;gt; Y
X2 --&amp;gt; X1
X2 --&amp;gt; Y
X3 --&amp;gt; Y

class X1,Y included;
class X2,X3 excluded;

linkStyle 1,2 stroke:#ff0000,stroke-width:4px;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In order to recover the causal effect of &lt;code&gt;coupons&lt;/code&gt; on &lt;code&gt;sales&lt;/code&gt; we need to &lt;strong&gt;condition&lt;/strong&gt; our analysis on &lt;code&gt;income&lt;/code&gt;. This will &lt;strong&gt;block&lt;/strong&gt; the non-causal path passing through &lt;code&gt;income&lt;/code&gt;, leaving only the direct path from &lt;code&gt;coupons&lt;/code&gt; to &lt;code&gt;sales&lt;/code&gt; open, allowing us to estimate the causal effect.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-mermaid&#34;&gt;flowchart LR
classDef included fill:#DCDCDC,stroke:#000000,stroke-width:2px;
classDef excluded fill:#ffffff,stroke:#000000,stroke-width:2px;
classDef unobserved fill:#ffffff,stroke:#000000,stroke-width:2px,stroke-dasharray: 5 5;


X1((coupons))
X2((income))
X3((weekday))
Y((sales))


X1 --&amp;gt; Y
X2 -.-&amp;gt; X1
X2 -.-&amp;gt; Y
X3 --&amp;gt; Y

class X1,X2,Y included;
class X3 excluded;

linkStyle 0 stroke:#00ff00,stroke-width:4px;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s implement this, by including &lt;code&gt;income&lt;/code&gt; in the regression.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;smf.ols(&#39;sales ~ coupons + income&#39;, df).fit().summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
      &lt;td&gt;&lt;/td&gt;         &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt; &lt;td&gt;  161.4982&lt;/td&gt; &lt;td&gt;   33.253&lt;/td&gt; &lt;td&gt;    4.857&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;   94.601&lt;/td&gt; &lt;td&gt;  228.395&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;coupons&lt;/th&gt;   &lt;td&gt;  218.7548&lt;/td&gt; &lt;td&gt;   50.058&lt;/td&gt; &lt;td&gt;    4.370&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;  118.052&lt;/td&gt; &lt;td&gt;  319.458&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;income&lt;/th&gt;    &lt;td&gt;    9.5094&lt;/td&gt; &lt;td&gt;    0.480&lt;/td&gt; &lt;td&gt;   19.818&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    8.544&lt;/td&gt; &lt;td&gt;   10.475&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;Now the estimated effect of &lt;code&gt;coupons&lt;/code&gt; on &lt;code&gt;sales&lt;/code&gt; is positive and significant. Coupons were actually a &lt;strong&gt;good idea&lt;/strong&gt; after all.&lt;/p&gt;
&lt;h3 id=&#34;verifying-the-theorem&#34;&gt;Verifying the Theorem&lt;/h3&gt;
&lt;p&gt;Let&amp;rsquo;s now verify that the Frisch-Waugh-Lowell theorem actually holds. In particular, we want to check whether we get the &lt;strong&gt;same coefficient&lt;/strong&gt; if, instead of regressing &lt;code&gt;sales&lt;/code&gt; on &lt;code&gt;coupons&lt;/code&gt; and &lt;code&gt;income&lt;/code&gt;, we were&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;regressing &lt;code&gt;coupons&lt;/code&gt; on &lt;code&gt;income&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;computing the residuals &lt;code&gt;coupons_tilde&lt;/code&gt;, i.e. the variation in &lt;code&gt;coupons&lt;/code&gt; &lt;strong&gt;not&lt;/strong&gt; explained by &lt;code&gt;income&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;regressing &lt;code&gt;sales&lt;/code&gt; on &lt;code&gt;coupons_tilde&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df[&#39;coupons_tilde&#39;] = smf.ols(&#39;coupons ~ income&#39;, df).fit().resid

smf.ols(&#39;sales ~ coupons_tilde - 1&#39;, df).fit().summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
        &lt;td&gt;&lt;/td&gt;           &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;coupons_tilde&lt;/th&gt; &lt;td&gt;  218.7548&lt;/td&gt; &lt;td&gt; 1275.236&lt;/td&gt; &lt;td&gt;    0.172&lt;/td&gt; &lt;td&gt; 0.865&lt;/td&gt; &lt;td&gt;-2343.929&lt;/td&gt; &lt;td&gt; 2781.438&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;Yes, the coefficient is the same! However, the &lt;strong&gt;standard errors&lt;/strong&gt; now have increased a lot and the estimated coefficient is not significantly different from zero anymore.&lt;/p&gt;
&lt;p&gt;A better approach is to add a further step and repeat the same procedure also for &lt;code&gt;sales&lt;/code&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;regressing &lt;code&gt;sales&lt;/code&gt; on &lt;code&gt;income&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;computing the residuals &lt;code&gt;sales_tilde&lt;/code&gt;, i.e. the variation in &lt;code&gt;sales&lt;/code&gt; &lt;strong&gt;not&lt;/strong&gt; explained by &lt;code&gt;income&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;and finally regress &lt;code&gt;sales_tilde&lt;/code&gt; on &lt;code&gt;coupons_tilde&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df[&#39;sales_tilde&#39;] = smf.ols(&#39;sales ~ income&#39;, df).fit().resid

smf.ols(&#39;sales_tilde ~ coupons_tilde - 1&#39;, df).fit().summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
        &lt;td&gt;&lt;/td&gt;           &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;coupons_tilde&lt;/th&gt; &lt;td&gt;  218.7548&lt;/td&gt; &lt;td&gt;   49.025&lt;/td&gt; &lt;td&gt;    4.462&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;  120.235&lt;/td&gt; &lt;td&gt;  317.275&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;The coefficient is still exactly the same, but now also the standard errors are almost identical.&lt;/p&gt;
&lt;h3 id=&#34;projection&#34;&gt;Projection&lt;/h3&gt;
&lt;p&gt;What is &lt;strong&gt;partialling-out&lt;/strong&gt; (or residualization, or orthogonalization) actually doing? What is happening when we take the residuals of &lt;code&gt;coupons&lt;/code&gt; with respect to &lt;code&gt;income&lt;/code&gt;?&lt;/p&gt;
&lt;p&gt;We can &lt;strong&gt;visualize&lt;/strong&gt; the procedure in a plot. First, let&amp;rsquo;s actually display the &lt;strong&gt;residuals&lt;/strong&gt; of &lt;code&gt;coupons&lt;/code&gt; with respect to income.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df[&amp;quot;coupons_hat&amp;quot;] = smf.ols(&#39;coupons ~ income&#39;, df).fit().predict()
ax = sns.regplot(x=&amp;quot;income&amp;quot;, y=&amp;quot;coupons&amp;quot;, data=df, ci=False, line_kws={&#39;color&#39;:&#39;r&#39;, &#39;label&#39;:&#39;linear fit&#39;})
ax.vlines(df[&amp;quot;income&amp;quot;], np.minimum(df[&amp;quot;coupons&amp;quot;], df[&amp;quot;coupons_hat&amp;quot;]), np.maximum(df[&amp;quot;coupons&amp;quot;], df[&amp;quot;coupons_hat&amp;quot;]), 
           linestyle=&#39;--&#39;, color=&#39;k&#39;, alpha=0.5, linewidth=1, label=&amp;quot;residuals&amp;quot;);
plt.legend()
plt.title(f&amp;quot;Coupons usage, income and residuals&amp;quot;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/fwl_24_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;residuals&lt;/strong&gt; are the vertical dotted lines between the data and the linear fit, i.e. the part of the variation in &lt;code&gt;coupons&lt;/code&gt; unexplained by &lt;code&gt;income&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;By &lt;strong&gt;partialling-out&lt;/strong&gt;, we are removing the linear fit from the data and keeping only the residuals. We can visualize this procedure with a gif. I import the code from the &lt;code&gt;src.figures&lt;/code&gt; file that you can find &lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/src/figures.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from src.figures import gif_projection

gif_projection(x=&#39;income&#39;, y=&#39;coupons&#39;, df=df, gifname=&amp;quot;gifs/fwl.gif&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;gifs/fwl.gif&#34; alt=&#34;fwl&#34;&gt;&lt;/p&gt;
&lt;p&gt;The original distribution of the data is on the left in &lt;em&gt;blue&lt;/em&gt;, the partialled-out data in on the right in &lt;em&gt;green&lt;/em&gt;. As we can see, partialling-out removes both the level and the trend in &lt;code&gt;coupons&lt;/code&gt; that is explained by &lt;code&gt;income&lt;/code&gt;.&lt;/p&gt;
&lt;h3 id=&#34;multiple-controls&#34;&gt;Multiple Controls&lt;/h3&gt;
&lt;p&gt;We can use the Frisch-Waugh-Theorem also when we have &lt;strong&gt;multiple control variables&lt;/strong&gt;. Suppose that we wanted to also include &lt;code&gt;day of the week&lt;/code&gt; in the regression, to increase precision.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;smf.ols(&#39;sales ~ coupons + income + dayofweek&#39;, df).fit().summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
         &lt;td&gt;&lt;/td&gt;           &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt;      &lt;td&gt;  124.2721&lt;/td&gt; &lt;td&gt;   28.764&lt;/td&gt; &lt;td&gt;    4.320&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;   66.182&lt;/td&gt; &lt;td&gt;  182.362&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;dayofweek[T.2]&lt;/th&gt; &lt;td&gt;    7.7703&lt;/td&gt; &lt;td&gt;   14.607&lt;/td&gt; &lt;td&gt;    0.532&lt;/td&gt; &lt;td&gt; 0.598&lt;/td&gt; &lt;td&gt;  -21.729&lt;/td&gt; &lt;td&gt;   37.270&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;dayofweek[T.3]&lt;/th&gt; &lt;td&gt;   15.0895&lt;/td&gt; &lt;td&gt;   11.678&lt;/td&gt; &lt;td&gt;    1.292&lt;/td&gt; &lt;td&gt; 0.204&lt;/td&gt; &lt;td&gt;   -8.495&lt;/td&gt; &lt;td&gt;   38.674&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;dayofweek[T.4]&lt;/th&gt; &lt;td&gt;   28.2762&lt;/td&gt; &lt;td&gt;    9.868&lt;/td&gt; &lt;td&gt;    2.866&lt;/td&gt; &lt;td&gt; 0.007&lt;/td&gt; &lt;td&gt;    8.348&lt;/td&gt; &lt;td&gt;   48.204&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;dayofweek[T.5]&lt;/th&gt; &lt;td&gt;   44.0937&lt;/td&gt; &lt;td&gt;   10.214&lt;/td&gt; &lt;td&gt;    4.317&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;   23.467&lt;/td&gt; &lt;td&gt;   64.720&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;dayofweek[T.6]&lt;/th&gt; &lt;td&gt;   50.7664&lt;/td&gt; &lt;td&gt;   13.130&lt;/td&gt; &lt;td&gt;    3.866&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;   24.249&lt;/td&gt; &lt;td&gt;   77.283&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;dayofweek[T.7]&lt;/th&gt; &lt;td&gt;   57.3142&lt;/td&gt; &lt;td&gt;   12.413&lt;/td&gt; &lt;td&gt;    4.617&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;   32.245&lt;/td&gt; &lt;td&gt;   82.383&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;coupons&lt;/th&gt;        &lt;td&gt;  192.0262&lt;/td&gt; &lt;td&gt;   39.140&lt;/td&gt; &lt;td&gt;    4.906&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;  112.981&lt;/td&gt; &lt;td&gt;  271.071&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;income&lt;/th&gt;         &lt;td&gt;    9.8152&lt;/td&gt; &lt;td&gt;    0.404&lt;/td&gt; &lt;td&gt;   24.314&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    9.000&lt;/td&gt; &lt;td&gt;   10.630&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;We can perform the same procedure as before, but instead of &lt;strong&gt;partialling-out&lt;/strong&gt; only &lt;code&gt;income&lt;/code&gt;, now we partial out both &lt;code&gt;income&lt;/code&gt; and &lt;code&gt;day of the week&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df[&#39;coupons_tilde&#39;] = smf.ols(&#39;coupons ~ income + dayofweek&#39;, df).fit().resid
df[&#39;sales_tilde&#39;] = smf.ols(&#39;sales ~ income + dayofweek&#39;, df).fit().resid
smf.ols(&#39;sales_tilde ~ coupons_tilde - 1&#39;, df).fit().summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
        &lt;td&gt;&lt;/td&gt;           &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;coupons_tilde&lt;/th&gt; &lt;td&gt;  192.0262&lt;/td&gt; &lt;td&gt;   35.803&lt;/td&gt; &lt;td&gt;    5.363&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;  120.078&lt;/td&gt; &lt;td&gt;  263.974&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;We still get exactly the same coefficient!&lt;/p&gt;
&lt;h2 id=&#34;applications&#34;&gt;Applications&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s now inspect some useful applications of the FWL theorem.&lt;/p&gt;
&lt;h3 id=&#34;data-visualization&#34;&gt;Data Visualization&lt;/h3&gt;
&lt;p&gt;One of the advantages of the Frisch-Waugh-Theorem is that it allows us to estimate the coefficient of interest from a &lt;strong&gt;univariate&lt;/strong&gt; regression, i.e. with a single explanatory variable (or feature).&lt;/p&gt;
&lt;p&gt;Therefore, we can now represent the relationship of interest &lt;strong&gt;graphically&lt;/strong&gt;. Let&amp;rsquo;s plot the residual &lt;code&gt;sales&lt;/code&gt; against the residual &lt;code&gt;coupons&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sns.regplot(x=&amp;quot;coupons_tilde&amp;quot;, y=&amp;quot;sales_tilde&amp;quot;, data=df, ci=False, line_kws={&#39;color&#39;:&#39;r&#39;, &#39;label&#39;:&#39;linear fit&#39;})
plt.legend()
plt.title(f&amp;quot;Residual sales and residual coupons&amp;quot;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/fwl_39_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Now it&amp;rsquo;s evident from the graph that the &lt;strong&gt;conditional relationship&lt;/strong&gt; between &lt;code&gt;sales&lt;/code&gt; and &lt;code&gt;coupons&lt;/code&gt; is positive.&lt;/p&gt;
&lt;p&gt;One problem with this approach is that the variables are &lt;strong&gt;hard to interpret&lt;/strong&gt;: we now have negative values for both &lt;code&gt;sales&lt;/code&gt; and &lt;code&gt;coupons&lt;/code&gt;. Weird.&lt;/p&gt;
&lt;p&gt;Why did it happen? It happened because when we partialled-out the variables, we included the &lt;strong&gt;intercept&lt;/strong&gt; in the regression, effectively de-meaning the variables (i.e. normalizing their values so that their mean is zero).&lt;/p&gt;
&lt;p&gt;We can &lt;strong&gt;solve&lt;/strong&gt; this problem by &lt;strong&gt;scaling&lt;/strong&gt; both variables, adding their mean.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df[&#39;coupons_tilde_scaled&#39;] = df[&#39;coupons_tilde&#39;] + np.mean(df[&#39;coupons&#39;])
df[&#39;sales_tilde_scaled&#39;] = df[&#39;sales_tilde&#39;] + np.mean(df[&#39;sales&#39;])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now the magnitudes of the two variables are interpretable again.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sns.regplot(x=&amp;quot;coupons_tilde_scaled&amp;quot;, y=&amp;quot;sales_tilde_scaled&amp;quot;, data=df, ci=False, line_kws={&#39;color&#39;:&#39;r&#39;, &#39;label&#39;:&#39;linear fit&#39;})
plt.legend()
plt.title(f&amp;quot;Residual sales scaled and residual coupons scaled&amp;quot;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/fwl_43_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Is this a &lt;strong&gt;valid&lt;/strong&gt; approach or did it alter our estimates? We can can check it by running the regression with the scaled partialled-out variables.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;smf.ols(&#39;sales_tilde_scaled ~ coupons_tilde_scaled&#39;, df).fit().summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
            &lt;td&gt;&lt;/td&gt;              &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt;            &lt;td&gt;  641.6486&lt;/td&gt; &lt;td&gt;   10.017&lt;/td&gt; &lt;td&gt;   64.054&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;  621.507&lt;/td&gt; &lt;td&gt;  661.790&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;coupons_tilde_scaled&lt;/th&gt; &lt;td&gt;  192.0262&lt;/td&gt; &lt;td&gt;   36.174&lt;/td&gt; &lt;td&gt;    5.308&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;  119.294&lt;/td&gt; &lt;td&gt;  264.758&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;The coefficient is exactly the same as before!&lt;/p&gt;
&lt;h3 id=&#34;computational-speed&#34;&gt;Computational Speed&lt;/h3&gt;
&lt;p&gt;Another application of the Frisch-Waugh-Lovell theorem is to increase the computational speed of linear estimators. For example it is used to compute efficient linear estimators in presence of high-dimensional fixed effects (&lt;code&gt;day of the week&lt;/code&gt; in our example).&lt;/p&gt;
&lt;p&gt;Some packages that exploit the Frisch-Waugh-Lovell theorem include&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://scorreia.com/software/reghdfe/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;reghdfe in Stata&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://pyhdfe.readthedocs.io/en/stable/index.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;pyhdfe in Python&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;However, it&amp;rsquo;s important to also mention the &lt;a href=&#34;https://cran.r-project.org/web/packages/fixest/index.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;fixest&lt;/a&gt; package in R, which is also exceptionally efficient in running regressions with high dimensional fixed effects.&lt;/p&gt;
&lt;h3 id=&#34;inference-and-machine-learning&#34;&gt;Inference and Machine Learning&lt;/h3&gt;
&lt;p&gt;Another important application of the FWL theorem sits at the intersection of &lt;strong&gt;machine learning&lt;/strong&gt; and &lt;strong&gt;causal inference&lt;/strong&gt;. I am referring to the work on post-double selection by &lt;a href=&#34;https://academic.oup.com/restud/article-abstract/81/2/608/1523757&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Belloni, Chernozhukov, Hansen (2013)&lt;/a&gt; and the follow up work on &amp;ldquo;double machine learning&amp;rdquo; by &lt;a href=&#34;https://academic.oup.com/ectj/article/21/1/C1/5056401&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Chernozhukov, Chetverikov, Demirer, Duflo, Hansen, Newey, Robins (2018)&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;I plan to cover both applications in future posts, but I wanted to start with the basics. Stay tuned!&lt;/p&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;p&gt;[1] R. Frisch and F. V. Waugh, &lt;a href=&#34;https://www.jstor.org/stable/1907330&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Partial Time Regressions as Compared with Individual Trends&lt;/a&gt; (1933), &lt;em&gt;Econometrica&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;[2] M. C. Lowell, &lt;a href=&#34;https://www.tandfonline.com/doi/abs/10.1080/01621459.1963.10480682&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Seasonal Adjustment of Economic Time Series and Multiple Regression Analysis&lt;/a&gt; (1963), &lt;em&gt;Journal of the American Statistical Association&lt;/em&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>DAGs and Control Variables</title>
      <link>https://matteocourthoud.github.io/post/controls/</link>
      <pubDate>Mon, 25 Apr 2022 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/post/controls/</guid>
      <description>&lt;p&gt;When analyzing causal relationships, it is very hard to understand which variables to &lt;strong&gt;condition the analysis on&lt;/strong&gt;, i.e. how to &amp;ldquo;split&amp;rdquo; the data so that we are &lt;strong&gt;comparing apples to apples&lt;/strong&gt;. For example, if you want to understand the effect of having a tablet in class on studenta&amp;rsquo; performance, it makes sense to compare schools where students have similar socio-economic backgrounds. Otherwise, the risk is that only wealthier students can afford a tablet and, without controlling for it, we might attribute the effect to tablets instead of the socio-economic background.&lt;/p&gt;
&lt;p&gt;When the treatment of interest comes from a proper &lt;strong&gt;randomized experiment&lt;/strong&gt;, we do not need to worry about conditioning on other variables. If tablets are distributed randomly across schools, and we have enough schools in the experiment, we do not have to worry about the socio-economic background of students. The only advantage of conditioning the analysis on some so-called &amp;ldquo;control variable&amp;rdquo; could be an increase in power. However, this is a different story.&lt;/p&gt;
&lt;p&gt;In this post, we are going to have a brief introduction to Directed Acyclic Graphs and how they can be useful to select variables to condition a causal analysis on. Not only DAGs provide visual intuition on which variables we need to &lt;em&gt;include&lt;/em&gt; in the analysis, but also on which variables we should &lt;em&gt;not include&lt;/em&gt;, and why.&lt;/p&gt;
&lt;h2 id=&#34;directed-acyclic-graphs&#34;&gt;Directed Acyclic Graphs&lt;/h2&gt;
&lt;h3 id=&#34;definitions&#34;&gt;Definitions&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Directed acyclic graphs&lt;/strong&gt; (&lt;strong&gt;DAG&lt;/strong&gt;s) provide a visual representation of the data generating process. Random variables are represented with letters (e.g. $X$) and causal relationships are represented with arrows (e.g. $\to$). For example, we interpret&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-mermaid&#34;&gt;flowchart LR
classDef white fill:#FFFFFF,stroke:#000000,stroke-width:2px
X((X)):::white --&amp;gt; Y((Y)):::white
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;as $X$ (possibly) causes $Y$. We call a &lt;strong&gt;path&lt;/strong&gt; between two variables $X$ and $Y$ any connection, &lt;em&gt;independently of the direction of the arrows&lt;/em&gt;. If all arrows point forward, we call it a &lt;strong&gt;causal path&lt;/strong&gt;, otherwise we call it a &lt;strong&gt;spurious path&lt;/strong&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-mermaid&#34;&gt;flowchart LR
classDef included fill:#DCDCDC,stroke:#000000,stroke-width:2px;
classDef excluded fill:#ffffff,stroke:#000000,stroke-width:2px;

X((X))
Y((Y))
Z1((Z1))
Z2((Z2))
Z3((Z3))

X --&amp;gt; Z1
Z1 --&amp;gt; Z2
Z3 --&amp;gt; Z2
Z3 --&amp;gt; Y

class X,Y included;
class Z1,Z2,Z3 excluded;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In the example above, we have a path between $X$ and $Y$ passing through the variables $Z_1$, $Z_2$, and $Z_3$. Since not all arrows point forward, the path is &lt;em&gt;spurious&lt;/em&gt; and there is no causal relationship of $X$ on $Y$. In fact, variable $Z_2$ is caused by both $Z_1$ and $Z_3$ and therefore &lt;strong&gt;blocks&lt;/strong&gt; the path.&lt;/p&gt;
&lt;p&gt;$Z_2$ is called a &lt;strong&gt;collider&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;The purpose of our analysis is to assess the &lt;strong&gt;causal relationship&lt;/strong&gt; between two variables $X$ and $Y$. Directed acyclic graphs are useful because they provide us instructions on which other variables $Z$ we need to &lt;strong&gt;condition&lt;/strong&gt; our analysis on. Conditioning the analysis on a variable means that we keep it fixed and we draw our conclusions &lt;em&gt;ceteris paribus&lt;/em&gt;. For example, in a linear regression framework, inserting another regressor $Z$ means that we are computing the best linear approximation of the conditional expectation function of $Y$ given $X$, &lt;em&gt;conditional&lt;/em&gt; on the observed values of $Z$.&lt;/p&gt;
&lt;h3 id=&#34;causality&#34;&gt;Causality&lt;/h3&gt;
&lt;p&gt;In order to assess causality, we want to &lt;strong&gt;close all spurious paths&lt;/strong&gt; between $X$ and $Y$. The &lt;strong&gt;questions&lt;/strong&gt; now are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;When is a path &lt;strong&gt;open&lt;/strong&gt;?
&lt;ul&gt;
&lt;li&gt;If it does not contain &lt;em&gt;colliders&lt;/em&gt;. Otherwise, it is &lt;em&gt;closed&lt;/em&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;How do you &lt;strong&gt;close an open path&lt;/strong&gt;?
&lt;ul&gt;
&lt;li&gt;You condition on &lt;em&gt;at least one&lt;/em&gt; intermediate variable.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;How do you &lt;strong&gt;open a closed path&lt;/strong&gt;?
&lt;ul&gt;
&lt;li&gt;You condition on &lt;em&gt;all&lt;/em&gt; colliders along the path.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Suppose we are again interested in the causal relationship of $X$ on $Y$. Let&amp;rsquo;s consider the following graph&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-mermaid&#34;&gt;flowchart LR
classDef included fill:#DCDCDC,stroke:#000000,stroke-width:2px;
classDef excluded fill:#ffffff,stroke:#000000,stroke-width:2px;

X((X))
Y((Y))
Z1((Z1))
Z2((Z2))
Z3((Z3))

X --&amp;gt; Y
X --&amp;gt; Z2
Z2 --&amp;gt; Y
Z1 --&amp;gt; X
Z1 --&amp;gt; Y
X --&amp;gt; Z3
Y --&amp;gt; Z3

class X,Y included;
class Z1,Z2,Z3 excluded;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In this case, apart from the direct path, there are &lt;strong&gt;three non-direct paths&lt;/strong&gt; between $X$ and $Y$ through the variables $Z_1$, $Z_2$, and $Z_3$.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s consider the case in which we analyze the relationship between $X$ and $Y$, ignoring all other variables.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;The path through $Z_1$ is &lt;strong&gt;open&lt;/strong&gt; but it is &lt;strong&gt;spurious&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;The path through $Z_2$ is &lt;strong&gt;open&lt;/strong&gt; and &lt;strong&gt;causal&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;The path through $Z_3$ is &lt;strong&gt;closed&lt;/strong&gt; since $Z_3$ is a &lt;em&gt;collider&lt;/em&gt; and it is &lt;strong&gt;spurious&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Let&amp;rsquo;s draw the same graph indicating in &lt;em&gt;grey&lt;/em&gt; variables that we are conditioning on, with &lt;em&gt;dotted lines&lt;/em&gt; closed paths, with &lt;em&gt;red lines&lt;/em&gt; spurious open paths, and with &lt;em&gt;green lines&lt;/em&gt; causal open paths.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-mermaid&#34;&gt;flowchart LR
classDef included fill:#DCDCDC,stroke:#000000,stroke-width:2px;
classDef excluded fill:#ffffff,stroke:#000000,stroke-width:2px;

X((X))
Y((Y))
Z1((Z1))
Z2((Z2))
Z3((Z3))

X --&amp;gt; Y
X --&amp;gt; Z2
Z2 --&amp;gt; Y
Z1 --&amp;gt; X
Z1 --&amp;gt; Y
X -.-&amp;gt; Z3
Y -.-&amp;gt; Z3

linkStyle 0,1,2 stroke:#00ff00,stroke-width:4px;
linkStyle 3,4 stroke:#ff0000,stroke-width:4px;
class X,Y included;
class Z1,Z2,Z3 excluded;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In this case, to assess the &lt;strong&gt;causal&lt;/strong&gt; relationship between $X$ and $Y$ we need to &lt;strong&gt;close&lt;/strong&gt; the path that passes through $Z_1$. We can do that by conditioning the analysis on $Z_1$.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-mermaid&#34;&gt;flowchart LR
classDef included fill:#DCDCDC,stroke:#000000,stroke-width:2px;
classDef excluded fill:#ffffff,stroke:#000000,stroke-width:2px;

X((X))
Y((Y))
Z1((Z1))
Z2((Z2))
Z3((Z3))

X --&amp;gt; Y
X --&amp;gt; Z2
Z2 --&amp;gt; Y
Z1 -.-&amp;gt; X
Z1 -.-&amp;gt; Y
X -.-&amp;gt; Z3
Y -.-&amp;gt; Z3

linkStyle 0,1,2 stroke:#00ff00,stroke-width:4px;
class X,Y,Z1 included;
class Z2,Z3 excluded;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we are able to recover the causal relationship between $X$ and $Y$ by conditioning on $Z_1$.&lt;/p&gt;
&lt;p&gt;What would happen if we were also &lt;strong&gt;conditioning on $Z_2$&lt;/strong&gt;? In this case, we would &lt;strong&gt;close&lt;/strong&gt; the path passing through $Z_2$ leaving only the &lt;em&gt;direct&lt;/em&gt; path between $X$ and $Y$ open. We would then recover only the &lt;strong&gt;direct effect&lt;/strong&gt; of $X$ on $Y$ and not the &lt;em&gt;indirect&lt;/em&gt; one.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-mermaid&#34;&gt;flowchart LR
classDef included fill:#DCDCDC,stroke:#000000,stroke-width:2px;
classDef excluded fill:#ffffff,stroke:#000000,stroke-width:2px;

X((X))
Y((Y))
Z1((Z1))
Z2((Z2))
Z3((Z3))

X --&amp;gt; Y
X -.-&amp;gt; Z2
Z2 -.-&amp;gt; Y
Z1 -.-&amp;gt; X
Z1 -.-&amp;gt; Y
X -.-&amp;gt; Z3
Y -.-&amp;gt; Z3

linkStyle 0 stroke:#00ff00,stroke-width:4px;
class X,Y,Z1,Z2 included;
class Z3 excluded;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What would happen if we were also &lt;strong&gt;conditioning on $Z_3$&lt;/strong&gt;? In this case, we would &lt;strong&gt;open&lt;/strong&gt; the path passing through $Z_3$ which is a &lt;strong&gt;spurious&lt;/strong&gt; path. We would then &lt;strong&gt;not&lt;/strong&gt; be able to recover the causal effect of $X$ on $Y$.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-mermaid&#34;&gt;flowchart LR
classDef included fill:#DCDCDC,stroke:#000000,stroke-width:2px;

X((X))
Y((Y))
Z1((Z1))
Z2((Z2))
Z3((Z3))

X --&amp;gt; Y
X -.-&amp;gt; Z2
Z2 -.-&amp;gt; Y
Z1 -.-&amp;gt; X
Z1 -.-&amp;gt; Y
X --&amp;gt; Z3
Y --&amp;gt; Z3

linkStyle 0 stroke:#00ff00,stroke-width:4px;
linkStyle 5,6 stroke:#ff0000,stroke-width:4px;
class X,Y,Z1,Z2,Z3 included;
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;example-class-size-and-math-scores&#34;&gt;Example: Class Size and Math Scores&lt;/h2&gt;
&lt;p&gt;Suppose you are interested in the &lt;strong&gt;effect of class size on math scores&lt;/strong&gt;. Are bigger classes better or worse for students&amp;rsquo; performance?&lt;/p&gt;
&lt;p&gt;Assume that the data generating process can be represented with the following &lt;strong&gt;DAG&lt;/strong&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-mermaid&#34;&gt;flowchart TB
classDef included fill:#DCDCDC,stroke:#000000,stroke-width:2px;
classDef excluded fill:#ffffff,stroke:#000000,stroke-width:2px;
classDef unobserved fill:#ffffff,stroke:#000000,stroke-width:2px,stroke-dasharray: 5 5;


X((class size))
Y((math score))
Z1((class year))
Z2((good school))
Z3((math hours))
Z4((hist score))
U((ability))

X --&amp;gt; Y
Z1 --&amp;gt; X
X --&amp;gt; Z4
U --&amp;gt; Y
U --&amp;gt; Z4
Z2 --&amp;gt; X
Z2 --&amp;gt; Y
Z2 --&amp;gt; Z4
Z3 --&amp;gt; Y

class X,Y included;
class Z1,Z2,Z3,Z4 excluded;
class U unobserved;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The variables of interest are highlighted. Moreover, the dotted line around &lt;code&gt;ability&lt;/code&gt; indicates that this is a variable that we do not observe in the data.&lt;/p&gt;
&lt;p&gt;We can now load the data and check what it looks like.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;%matplotlib inline
%config InlineBackend.figure_format = &#39;retina&#39;
from src.utils import *
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from src.dgp import dgp_school

df = dgp_school().generate_data()
df.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;math_hours&lt;/th&gt;
      &lt;th&gt;history_hours&lt;/th&gt;
      &lt;th&gt;good_school&lt;/th&gt;
      &lt;th&gt;class_year&lt;/th&gt;
      &lt;th&gt;class_size&lt;/th&gt;
      &lt;th&gt;math_score&lt;/th&gt;
      &lt;th&gt;hist_score&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;15&lt;/td&gt;
      &lt;td&gt;13.009309&lt;/td&gt;
      &lt;td&gt;15.167024&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;19&lt;/td&gt;
      &lt;td&gt;13.047033&lt;/td&gt;
      &lt;td&gt;13.387456&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;25&lt;/td&gt;
      &lt;td&gt;8.330311&lt;/td&gt;
      &lt;td&gt;10.824070&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;22&lt;/td&gt;
      &lt;td&gt;11.322190&lt;/td&gt;
      &lt;td&gt;14.594394&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;15&lt;/td&gt;
      &lt;td&gt;12.338458&lt;/td&gt;
      &lt;td&gt;11.871626&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;What variables should we condition our regression on, in order to estimate the causal effect of &lt;code&gt;class size&lt;/code&gt; on &lt;code&gt;math scores&lt;/code&gt;?&lt;/p&gt;
&lt;p&gt;First of all, let&amp;rsquo;s look at what happens if we do not condition our analysis on any variable and we just regress &lt;code&gt;math score&lt;/code&gt; on &lt;code&gt;class size&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;smf.ols(&#39;math_score ~ class_size&#39;, df).fit().summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
       &lt;td&gt;&lt;/td&gt;         &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt;  &lt;td&gt;   12.0421&lt;/td&gt; &lt;td&gt;    0.259&lt;/td&gt; &lt;td&gt;   46.569&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;   11.535&lt;/td&gt; &lt;td&gt;   12.550&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;class_size&lt;/th&gt; &lt;td&gt;   -0.0399&lt;/td&gt; &lt;td&gt;    0.013&lt;/td&gt; &lt;td&gt;   -3.025&lt;/td&gt; &lt;td&gt; 0.003&lt;/td&gt; &lt;td&gt;   -0.066&lt;/td&gt; &lt;td&gt;   -0.014&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;The effect of &lt;code&gt;class_size&lt;/code&gt; is negative and statistically different from zero.&lt;/p&gt;
&lt;p&gt;But should we believe this estimated effect? Without controlling for anything, this is &lt;strong&gt;DAG representation&lt;/strong&gt; of the effect we are capturing.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-mermaid&#34;&gt;flowchart TB
classDef included fill:#DCDCDC,stroke:#000000,stroke-width:2px;
classDef excluded fill:#ffffff,stroke:#000000,stroke-width:2px;
classDef unobserved fill:#ffffff,stroke:#000000,stroke-width:2px,stroke-dasharray: 5 5;


X((class size))
Y((math score))
Z1((class year))
Z2((good school))
Z3((math hours))
Z4((hist score))
U((ability))

X --&amp;gt; Y
Z1 --&amp;gt; X
X -.-&amp;gt; Z4
U --&amp;gt; Y
U -.-&amp;gt; Z4
Z2 --&amp;gt; X
Z2 --&amp;gt; Y
Z2 --&amp;gt; Z4
Z3 --&amp;gt; Y

linkStyle 0 stroke:#00ff00,stroke-width:4px;
linkStyle 5,6 stroke:#ff0000,stroke-width:4px;

class X,Y included;
class Z1,Z2,Z3,Z4 excluded;
class U unobserved;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There is a &lt;strong&gt;spurious&lt;/strong&gt; path passing through &lt;code&gt;good school&lt;/code&gt; that &lt;strong&gt;biases&lt;/strong&gt; our estimated coefficient. Intuitively, being enrolled in a better school improves the students&amp;rsquo; math scores and better schools might have smaller class sizes. We need to control for the quality of the school.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;smf.ols(&#39;math_score ~ class_size + good_school&#39;, df).fit().summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
       &lt;td&gt;&lt;/td&gt;          &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt;   &lt;td&gt;    4.7449&lt;/td&gt; &lt;td&gt;    0.247&lt;/td&gt; &lt;td&gt;   19.176&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    4.259&lt;/td&gt; &lt;td&gt;    5.230&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;class_size&lt;/th&gt;  &lt;td&gt;    0.2095&lt;/td&gt; &lt;td&gt;    0.010&lt;/td&gt; &lt;td&gt;   20.020&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    0.189&lt;/td&gt; &lt;td&gt;    0.230&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;good_school&lt;/th&gt; &lt;td&gt;    5.0807&lt;/td&gt; &lt;td&gt;    0.130&lt;/td&gt; &lt;td&gt;   39.111&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    4.826&lt;/td&gt; &lt;td&gt;    5.336&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;Now the estimate of the effect of &lt;code&gt;class size&lt;/code&gt; on &lt;code&gt;math score&lt;/code&gt; is &lt;strong&gt;unbiased&lt;/strong&gt;! Indeed, the true coefficient in the data generating process was $0.2$.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-mermaid&#34;&gt;flowchart TB
classDef included fill:#DCDCDC,stroke:#000000,stroke-width:2px;
classDef excluded fill:#ffffff,stroke:#000000,stroke-width:2px;
classDef unobserved fill:#ffffff,stroke:#000000,stroke-width:2px,stroke-dasharray: 5 5;


X((class size))
Y((math score))
Z1((class year))
Z2((good school))
Z3((math hours))
Z4((hist score))
U((ability))

X --&amp;gt; Y
Z1 --&amp;gt; X
X -.-&amp;gt; Z4
U --&amp;gt; Y
U -.-&amp;gt; Z4
Z2 -.-&amp;gt; X
Z2 -.-&amp;gt; Y
Z2 --&amp;gt; Z4
Z3 --&amp;gt; Y

linkStyle 0 stroke:#00ff00,stroke-width:4px;

class X,Y,Z2 included;
class Z1,Z3,Z4 excluded;
class U unobserved;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What would happen if we were to instead &lt;strong&gt;control for all variables&lt;/strong&gt;?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;smf.ols(&#39;math_score ~ class_size + good_school + math_hours + class_year + hist_score&#39;, df).fit().summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
       &lt;td&gt;&lt;/td&gt;          &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt;   &lt;td&gt;   -0.7847&lt;/td&gt; &lt;td&gt;    0.310&lt;/td&gt; &lt;td&gt;   -2.529&lt;/td&gt; &lt;td&gt; 0.012&lt;/td&gt; &lt;td&gt;   -1.394&lt;/td&gt; &lt;td&gt;   -0.176&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;class_size&lt;/th&gt;  &lt;td&gt;    0.1292&lt;/td&gt; &lt;td&gt;    0.010&lt;/td&gt; &lt;td&gt;   13.054&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    0.110&lt;/td&gt; &lt;td&gt;    0.149&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;good_school&lt;/th&gt; &lt;td&gt;    2.9815&lt;/td&gt; &lt;td&gt;    0.170&lt;/td&gt; &lt;td&gt;   17.533&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    2.648&lt;/td&gt; &lt;td&gt;    3.315&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;math_hours&lt;/th&gt;  &lt;td&gt;    1.0516&lt;/td&gt; &lt;td&gt;    0.048&lt;/td&gt; &lt;td&gt;   21.744&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    0.957&lt;/td&gt; &lt;td&gt;    1.147&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;class_year&lt;/th&gt;  &lt;td&gt;    0.0424&lt;/td&gt; &lt;td&gt;    0.037&lt;/td&gt; &lt;td&gt;    1.130&lt;/td&gt; &lt;td&gt; 0.259&lt;/td&gt; &lt;td&gt;   -0.031&lt;/td&gt; &lt;td&gt;    0.116&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;hist_score&lt;/th&gt;  &lt;td&gt;    0.4116&lt;/td&gt; &lt;td&gt;    0.027&lt;/td&gt; &lt;td&gt;   15.419&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    0.359&lt;/td&gt; &lt;td&gt;    0.464&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;The coefficient is again &lt;strong&gt;biased&lt;/strong&gt;. Why?&lt;/p&gt;
&lt;p&gt;We have opened a new spurious path by controlling for &lt;code&gt;hist score&lt;/code&gt;. In fact, &lt;code&gt;hist score&lt;/code&gt; is a &lt;strong&gt;collider&lt;/strong&gt; and controlling for it has opened a path through &lt;code&gt;hist score&lt;/code&gt; and &lt;code&gt;ability&lt;/code&gt; that was otherwise closed.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-mermaid&#34;&gt;flowchart TB
classDef included fill:#DCDCDC,stroke:#000000,stroke-width:2px;
classDef excluded fill:#ffffff,stroke:#000000,stroke-width:2px;
classDef unobserved fill:#ffffff,stroke:#000000,stroke-width:2px,stroke-dasharray: 5 5;


X((class size))
Y((math score))
Z1((class year))
Z2((good school))
Z3((math hours))
Z4((hist score))
U((ability))

X --&amp;gt; Y
Z1 --&amp;gt; X
X --&amp;gt; Z4
U --&amp;gt; Y
U --&amp;gt; Z4
Z2 -.-&amp;gt; X
Z2 -.-&amp;gt; Y
Z2 --&amp;gt; Z4
Z3 --&amp;gt; Y

linkStyle 0 stroke:#00ff00,stroke-width:4px;
linkStyle 2,3,4 stroke:#ff0000,stroke-width:4px;

class X,Y,Z1,Z2,Z3,Z4 included;
class U unobserved;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The example was inspired by the following tweet.&lt;/p&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;We can illustrate this with Model 16 of the &amp;quot;Crash Course in Good and Bad Controls&amp;quot; (&lt;a href=&#34;https://t.co/GcSNzhuVt2&#34;&gt;https://t.co/GcSNzhuVt2&lt;/a&gt;). Here X = class size, Y = math4, Z = read4, and U = student&amp;#39;s ability. Conditioning on Z opens the path X -&amp;gt; Z &amp;lt;- U -&amp;gt; Y and it is thus a &amp;quot;bad control.&amp;quot; &lt;a href=&#34;https://t.co/KNfqtsMWwB&#34;&gt;https://t.co/KNfqtsMWwB&lt;/a&gt; &lt;a href=&#34;https://t.co/lUSigNYSJj&#34;&gt;pic.twitter.com/lUSigNYSJj&lt;/a&gt;&lt;/p&gt;&amp;mdash; Análise Real (@analisereal) &lt;a href=&#34;https://twitter.com/analisereal/status/1502793254592401409?ref_src=twsrc%5Etfw&#34;&gt;March 12, 2022&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;

&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;In this post, we have seen how to use Directed Acyclic Graphs to select control variables in a causal analysis. DAGs are very helpful tools since they provide an intuitive graphical representation of causal relationships between random variables. Contrary to common intuition that &amp;ldquo;the more information the better&amp;rdquo;, sometimes including extra variables might bias the analysis, preventing a causal interpretation of the results. In particular, we must pay attention not to include &lt;em&gt;colliders&lt;/em&gt; that open &lt;em&gt;spurious&lt;/em&gt; paths that would otherwise be closed.&lt;/p&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;p&gt;[1] C. Cinelli, A. Forney, J. Pearl, &lt;a href=&#34;https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3689437&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;A Crash Course in Good and Bad Controls&lt;/a&gt; (2018), working paper.&lt;/p&gt;
&lt;p&gt;[2] J. Pearl, &lt;a href=&#34;http://bayes.cs.ucla.edu/BOOK-2K/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Causality&lt;/a&gt; (2009), Cambridge University Press.&lt;/p&gt;
&lt;p&gt;[3] S. Cunningham, Chapter 3 of &lt;a href=&#34;https://mixtape.scunning.com/dag.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The Causal Inference Mixtape&lt;/a&gt; (2021), Yale University Press.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>How To Make A Personal Website with Hugo</title>
      <link>https://matteocourthoud.github.io/post/website/</link>
      <pubDate>Mon, 18 Apr 2022 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/post/website/</guid>
      <description>&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;During the second year of my PhD, I decided that I wanted to have a personal website. After (too many) hours of research, I decided to build it using &lt;a href=&#34;https://gohugo.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Hugo&lt;/a&gt;, and I picked the &lt;a href=&#34;https://wowchemy.com/docs/getting-started/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Wowchemy&lt;/strong&gt;&lt;/a&gt; theme, also known as &lt;a href=&#34;https://themes.gohugo.io/academic/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Hugo Academic&lt;/a&gt;. In this tutorial, I am going to share my guide to building a website on &lt;a href=&#34;https://pages.github.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github Pages&lt;/a&gt; so that you don’t have to go through all the pain I went through 😁.&lt;/p&gt;
&lt;p&gt;Before we start, I have to warn you. If you don’t care about &lt;strong&gt;personalization&lt;/strong&gt; or if you have very &lt;strong&gt;little time&lt;/strong&gt; to spend on building a website, I strongly recommend &lt;a href=&#34;https://sites.google.com/new&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Google Sites&lt;/a&gt;, which is, in my opinion, the fastest and easiest way to build an academic website. However, if you enjoy customizing your website, or if you like &lt;a href=&#34;https://github.com/matteocourthoud/custom-wowchemy-theme&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;my template&lt;/a&gt;, then this guide might be useful.&lt;/p&gt;
&lt;p&gt;Also, note that Hugo offers &lt;a href=&#34;https://themes.gohugo.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;many other website templates&lt;/a&gt;. I suggest checking them out. Some interesting &lt;strong&gt;alternatives&lt;/strong&gt; are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://themes.gohugo.io/themes/hugo-resume/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Resume&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://themes.gohugo.io/themes/somrat/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Somrat&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://themes.gohugo.io/themes/hugo-uilite/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;UILite (paid)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;However, in this guide, I will concentrate on the Hugo Academic theme, since it’s the one I used for &lt;a href=&#34;https://matteocourthoud.github.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;my website&lt;/a&gt; and I believe it’s the best one for building academic profile pages. But the first part of this guide is general and it works for any Hugo theme.&lt;/p&gt;
&lt;h2 id=&#34;create-website&#34;&gt;Create Website&lt;/h2&gt;
&lt;h3 id=&#34;0-prerequisites&#34;&gt;0. Prerequisites&lt;/h3&gt;
&lt;p&gt;Before we start, I will take for granted the following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;that you have an account on &lt;a href=&#34;https://www.github.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;that you have &lt;a href=&#34;https://www.r-project.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;R&lt;/a&gt; installed&lt;/li&gt;
&lt;li&gt;that you have &lt;a href=&#34;https://www.rstudio.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;RStudio&lt;/a&gt; installed&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;1-create-github-repository&#34;&gt;1. Create Github Repository&lt;/h3&gt;
&lt;p&gt;First, go to your &lt;a href=&#34;https://www.github.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt; page and create a new repository (&lt;code&gt;+&lt;/code&gt; button in the top-right corner).&lt;/p&gt;
&lt;img src=&#34;img/new_repo.png&#34; alt=&#34;new_repo&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;Name the repository &lt;code&gt;username.github.io&lt;/code&gt; where &lt;code&gt;username&lt;/code&gt; is your Github username.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;img/name_repo.png&#34; alt=&#34;name_repo&#34;&gt;&lt;/p&gt;
&lt;p&gt;In my case, my github username is &lt;code&gt;matteocourthoud&lt;/code&gt;, therefore the repository is &lt;code&gt;matteocourthoud.github.io&lt;/code&gt; and my personal website is &lt;a href=&#34;https://matteocourthoud.github.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://matteocourthoud.github.io&lt;/a&gt;. Use the default settings when creating the repository.&lt;/p&gt;
&lt;h3 id=&#34;2-install-blogdown-and-hugo&#34;&gt;2. Install Blogdown and Hugo&lt;/h3&gt;
&lt;p&gt;Now you need to install Blogdown, which is the program what will allow you to build and deploy your website, and Hugo, which is the template generator.&lt;/p&gt;
&lt;p&gt;Switch to RStudio and type the following commands&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Install blogdown
install.packages(&amp;quot;blogdown&amp;quot;)

# Install Hugo
blogdown::install_hugo()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now everything should be ready!&lt;/p&gt;
&lt;h3 id=&#34;3-setup-folder&#34;&gt;3. Setup folder&lt;/h3&gt;
&lt;p&gt;Open RStudio and select &lt;code&gt;New Project&lt;/code&gt;.&lt;/p&gt;
&lt;img src=&#34;img/new_project.png&#34; alt=&#34;new_project&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;Select &lt;code&gt;New Directory&lt;/code&gt; when asked where to create the project.&lt;/p&gt;
&lt;img src=&#34;img/new_project2.png&#34; alt=&#34;new_project2&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;Then select &lt;code&gt;Website using blogdown&lt;/code&gt; as project type.&lt;/p&gt;
&lt;img src=&#34;img/new_project3.png&#34; alt=&#34;new_project3&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;Now you have to select a couple of options:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;Directory name&lt;/code&gt;: here input the name of the folder which will contain all the website files. The name is irrelevant. I called mine &lt;code&gt;website&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Create project as a subdirectory of&lt;/code&gt;: select the directory in which you want to put the website folder.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Theme&lt;/code&gt;: input &lt;code&gt;wowchemy/starter-academic&lt;/code&gt; instead of the default theme.&lt;/li&gt;
&lt;/ul&gt;
&lt;img src=&#34;img/new_project4.png&#34; alt=&#34;new_project4&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: if you want to install a different theme, just go on the corresponding Github page (for example &lt;a href=&#34;https://github.com/caressofsteel/hugo-story&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/caressofsteel/hugo-story&lt;/a&gt;) and instead of &lt;code&gt;gcushen/hugo-academic&lt;/code&gt;, insert the corresponding Github repository (for example &lt;code&gt;caressofsteel/hugo-story&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;If you go into the website folder, it should look something like&lt;/p&gt;
&lt;img src=&#34;img/folder.png&#34; alt=&#34;folder&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;h3 id=&#34;4-build-website&#34;&gt;4. Build website&lt;/h3&gt;
&lt;p&gt;To build the website, open the RProject file &lt;code&gt;website.Rproj&lt;/code&gt; in RStudio and type&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;blogdown::hugo_build(local=TRUE)
&lt;/code&gt;&lt;/pre&gt;
&lt;img src=&#34;img/hugo_build.png&#34; alt=&#34;hugo_build&#34;&gt;
&lt;p&gt;This command will generate a &lt;code&gt;public/&lt;/code&gt; subfolder in which the actual code of the website is stored.&lt;/p&gt;
&lt;p&gt;Don’t ask me why, but the option &lt;code&gt;local=TRUE&lt;/code&gt; seems to make a difference. Updating without it sometimes does not change the content in the &lt;code&gt;public/&lt;/code&gt; subfolder.&lt;/p&gt;
&lt;img src=&#34;img/folder2.png&#34; alt=&#34;folder2&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;To preview the website, type in RStudio&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;blogdown::serve_site()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The following preview should automatically open in your browser.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;img/prevew.png&#34; alt=&#34;preview&#34;&gt;&lt;/p&gt;
&lt;p&gt;Previewing the website is very useful as it allows you to see live changes locally inside RStudio, before publishing them. This is the &lt;strong&gt;main advantage of working in RStudio&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;If the preview does &lt;em&gt;not automatically&lt;/em&gt; open in your browser, and instead it previews inside RStudio Viewer panel, you can preview it in your browser using the upper left right-most button.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;img/preview2.png&#34; alt=&#34;preview2&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;5-publish-website&#34;&gt;5. Publish website&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Importantly&lt;/strong&gt;, before pushing the code online, you need to open the file &lt;code&gt;config.yaml&lt;/code&gt; and change the &lt;code&gt;baseurl&lt;/code&gt; to your future website url, which will be &lt;code&gt;https://username.github.io/&lt;/code&gt;, where &lt;code&gt;username&lt;/code&gt; is your Github username.&lt;/p&gt;
&lt;img src=&#34;img/username.png&#34; alt=&#34;username&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;Now that you have set the correct url, you have to push the changes from the &lt;code&gt;public/&lt;/code&gt; folder to your &lt;code&gt;username.github.io&lt;/code&gt; repository on Github.&lt;/p&gt;
&lt;p&gt;To do that, you need to get to the website folder. Let’s assume that the path to your folder is &lt;code&gt;Documents/website&lt;/code&gt;. Open the Terminal and type&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;cd Documents/website/public
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The following code will link the &lt;code&gt;public/&lt;/code&gt; folder, containing the actual code of the website, to your &lt;code&gt;username.github.io&lt;/code&gt; repository.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Init git in the /website/public/ folder
git init

# Add and commit the changes
git add .
git commit -m &amp;quot;first version of the website&amp;quot;

# Set origin
git remote add origin https://github.com/username/username.github.io.git

# Rename local branch
git branch -M main

# And push your updates online
git push -u origin main
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Wait a few seconds (or minutes for heavy changes) and your website should be online!&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;If the website is not working&lt;/strong&gt;, you can check the following:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Is there anything in your &lt;code&gt;public/&lt;/code&gt; folder? (does it even exist?) If not, something went wrong when compiling the website with &lt;code&gt;blogdown::hugo_build()&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Inside your &lt;code&gt;public/&lt;/code&gt; folder, there should be an &lt;code&gt;index.html&lt;/code&gt; file. If you double-click on it, you should see a local preview of your website in your browser. If not, something in the website code is wrong.&lt;/li&gt;
&lt;li&gt;Is the content of your &lt;code&gt;public/&lt;/code&gt; folder exactly the same as the content of your Github repository? If not, something went wrong when pushing to Github.&lt;/li&gt;
&lt;li&gt;Did you name your Github repository &lt;code&gt;username.github.io&lt;/code&gt;, where &lt;code&gt;username&lt;/code&gt; is your Github username?&lt;/li&gt;
&lt;li&gt;Did you change the &lt;code&gt;baseurl&lt;/code&gt; option in the file &lt;code&gt;config.yaml&lt;/code&gt; to &lt;code&gt;https://username.github.io/&lt;/code&gt;, where &lt;code&gt;username&lt;/code&gt; is your Github username?&lt;/li&gt;
&lt;li&gt;You can check the list of websites deployments at &lt;code&gt;https://github.com/username/username.github.io/deployments&lt;/code&gt;. Control that they correspond with your commits.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;If all the conditions are satisfied, but the website is still not online, maybe it’s just a matter of time. Have some patience.&lt;/p&gt;
&lt;h2 id=&#34;basic-customization&#34;&gt;Basic Customization&lt;/h2&gt;
&lt;p&gt;The basic files that you want to modify to customize your website are the following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;config/_default/config.yaml&lt;/code&gt;: general website information&lt;/li&gt;
&lt;li&gt;&lt;code&gt;config/_default/params.yaml&lt;/code&gt;: website customization&lt;/li&gt;
&lt;li&gt;&lt;code&gt;config/_default/menus.yaml&lt;/code&gt;: top bar / menu customization&lt;/li&gt;
&lt;li&gt;&lt;code&gt;content/authors/admin/_index.md&lt;/code&gt;: personal information&lt;/li&gt;
&lt;/ul&gt;
&lt;img src=&#34;img/files.png&#34; alt=&#34;files&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;For what concerns &lt;strong&gt;images&lt;/strong&gt;, there are two main things you might want to modify:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Profile picture: change the &lt;code&gt;content/authors/admin/avatar.jpg&lt;/code&gt; picture&lt;/li&gt;
&lt;li&gt;Website icon: change the &lt;code&gt;assets/media/icon.png&lt;/code&gt; picture&lt;/li&gt;
&lt;/ul&gt;
&lt;img src=&#34;img/images.png&#34; alt=&#34;images&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;In order to modify the &lt;strong&gt;widgets&lt;/strong&gt; on your homepage, go to &lt;code&gt;content/home/&lt;/code&gt; and modify the files inside. If you want to remove a section, just open the corresponding file and select &lt;code&gt;active: false&lt;/code&gt;. If there is no &lt;code&gt;active&lt;/code&gt; option, just copy the line &lt;code&gt;active: false&lt;/code&gt; in the corresponding file.&lt;/p&gt;
&lt;img src=&#34;img/widgets.png&#34; alt=&#34;widgets&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;On my website, I have only the following sections set to true:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;about&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;projects&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;posts&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;contact&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;To change the &lt;strong&gt;color palette&lt;/strong&gt; of the website, go to &lt;code&gt;data\theme&lt;/code&gt; and generate a &lt;code&gt;custom_theme.toml&lt;/code&gt; file with the following content:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-toml&#34;&gt;# Theme metadata
name = &amp;quot;My custom theme&amp;quot;

# Is theme light or dark?
light = true

# Primary
primary = &amp;quot;#284f7a&amp;quot;

# Menu
menu_primary = &amp;quot;#fff&amp;quot;
menu_text = &amp;quot;#34495e&amp;quot;
menu_text_active = &amp;quot;#284f7a&amp;quot;
menu_title = &amp;quot;#2b2b2b&amp;quot;

# Home sections
home_section_odd = &amp;quot;rgb(255, 255, 255)&amp;quot;
home_section_even = &amp;quot;rgb(247, 247, 247)&amp;quot;

[dark]
  link = &amp;quot;#bbdefb&amp;quot;
  link_hover = &amp;quot;#bbdefb&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then go to the &lt;code&gt;config/_default/params.yaml&lt;/code&gt; file and set the &lt;code&gt;theme&lt;/code&gt; to &lt;code&gt;custom_theme&lt;/code&gt;.&lt;/p&gt;
&lt;img src=&#34;img/custom_theme.png&#34; alt=&#34;custom_theme&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;You can get more information on how to personalize it &lt;a href=&#34;https://wowchemy.com/docs/getting-started/customization/#custom-theme&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;To change the &lt;strong&gt;font&lt;/strong&gt;, go to &lt;code&gt;data\fonts&lt;/code&gt; and generate a &lt;code&gt;custom_font.toml&lt;/code&gt; file with the following content:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-toml&#34;&gt;# Font style metadata
name = &amp;quot;My custom font&amp;quot;

# Optional Google font URL
google_fonts = &amp;quot;family=Roboto+Mono&amp;amp;family=Source+Sans+Pro:wght@200;300;400;700&amp;quot;

# Font families
heading_font = &amp;quot;Source Sans Pro&amp;quot;
body_font = &amp;quot;Source Sans Pro&amp;quot;
nav_font = &amp;quot;Source Sans Pro&amp;quot;
mono_font = &amp;quot;Roboto Mono&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then go to the &lt;code&gt;config/_default/params.yaml&lt;/code&gt; file and set the &lt;code&gt;font&lt;/code&gt; to &lt;code&gt;custom_font&lt;/code&gt;.&lt;/p&gt;
&lt;img src=&#34;img/custom_font.png&#34; alt=&#34;custom_font&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;You can get more information on how to personalize it &lt;a href=&#34;https://wowchemy.com/docs/getting-started/customization/#custom-font&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;. Importantly, by default, the website supports only fonts of weight 400 and 700. If you want a lighter font, like the &lt;a href=&#34;https://fonts.google.com/specimen/Source&amp;#43;Sans&amp;#43;Pro&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Source Sans Pro&lt;/a&gt; I use for my website, you have to dig into the advanced customization (which requires HTML and CSS skills).&lt;/p&gt;
&lt;h2 id=&#34;advanced-customization&#34;&gt;Advanced Customization&lt;/h2&gt;
&lt;p&gt;Advanced customization is possible but &lt;strong&gt;it’s a pain&lt;/strong&gt;. You basically want to go inside &lt;code&gt;themes\github.com\wowchemy\wowchemy-hugo-modules\wowchemy&lt;/code&gt; and start digging. Tip: you want to start digging in the following places:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;In &lt;code&gt;layouts\partials&lt;/code&gt; to edit the HTML files&lt;/li&gt;
&lt;li&gt;In &lt;code&gt;assets\scss&lt;/code&gt; to edit the SCSS code&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If you want to copy my exact theme, I have published my custom theme here: &lt;a href=&#34;https://github.com/matteocourthoud/custom-wowchemy-settings&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/matteocourthoud/custom-wowchemy-settings&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;You have to do the following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;go inside the &lt;code&gt;theme&lt;/code&gt; folder&lt;/li&gt;
&lt;li&gt;copy the content of the &lt;code&gt;custom-wowchemy-theme&lt;/code&gt; repository in a folder there&lt;/li&gt;
&lt;li&gt;go to the &lt;code&gt;config.yaml&lt;/code&gt; file into the MODULES section&lt;/li&gt;
&lt;/ul&gt;
&lt;img src=&#34;img/config_before.png&#34; alt=&#34;config_before&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;ul&gt;
&lt;li&gt;change the second link to the folder with the custom settings&lt;/li&gt;
&lt;/ul&gt;
&lt;img src=&#34;img/config_after.png&#34; alt=&#34;config_after&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;Now your website should look quite similar to mine! :)&lt;/p&gt;
&lt;h3 id=&#34;examples&#34;&gt;Examples&lt;/h3&gt;
&lt;p&gt;Here are some examples of &lt;strong&gt;advanced customizations&lt;/strong&gt; you can do. For all the examples the baseline directory is you theme directory, &lt;code&gt;themes/custom-wowchemy-theme&lt;/code&gt; if you renamed it as in the previous paragraph.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Example 1&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;What to have your section titles fixed on top of the screen?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Go to &lt;code&gt;assets/scss/wowchemy/widgets/_base.scss&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Search for &lt;code&gt;.section-heading h1&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;It should look like this&lt;/p&gt;
&lt;img src=&#34;img/section_heading_before.png&#34; alt=&#34;section_heading_before&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Add a couple of lines as follows&lt;/p&gt;
&lt;img src=&#34;img/section_heading_after.png&#34; alt=&#34;section_heading_after&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Now the section titles should stay anchored at the top of the page&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Example 2&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Do you want to put a &lt;strong&gt;background image&lt;/strong&gt; in your home page?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Put the selected background image, for example &lt;code&gt;image.png&lt;/code&gt;,  into the &lt;code&gt;static/img&lt;/code&gt; folder (the location itself does not matter)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Go to &lt;code&gt;assets/scss/wowchemy/widgets/_about.scss&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Add the following lines anywhere in the code&lt;/p&gt;
&lt;img src=&#34;img/background_widget.png&#34; alt=&#34;background_widget&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Now go to &lt;code&gt;layouts/partials/widgets/about.html&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Add the following lines after &lt;code&gt;&amp;lt;!-- About widget --&amp;gt;&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;img/background_widget2.png&#34; alt=&#34;background_widget2&#34;&gt;`&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Now &lt;code&gt;image.png&lt;/code&gt; should appear as background image in your homepage.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;google-analytics&#34;&gt;Google Analytics&lt;/h2&gt;
&lt;p&gt;In order for the website to be displayed in Google searches, you need to ask Google to track it.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Go to the &lt;a href=&#34;https://search.google.com/search-console&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Google Search Console website&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Use the &lt;a href=&#34;https://search.google.com/search-console?action=inspect&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;URL Inspection tool&lt;/a&gt; to inspect the URL of your personal website: &lt;code&gt;https://username.github.io&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Use &lt;strong&gt;Request indexing&lt;/strong&gt; to request Google to index your website so that it will apprear in Google searches.&lt;/li&gt;
&lt;li&gt;Under &lt;strong&gt;Sitemap&lt;/strong&gt; provide the link to your website sitemap to Google. It should be &lt;code&gt;https://username.github.io/sitemap.xml&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In order to receive statistics on your website, you first need to get your associated tracking code.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Go to the &lt;a href=&#34;https://www.google.com/analytics/web/#home/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Google Analytics website&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Click &lt;a href=&#34;https://support.google.com/analytics/answer/6132368&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Admin&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Select an account from the menu in the &lt;strong&gt;ACCOUNT&lt;/strong&gt; column.&lt;/li&gt;
&lt;li&gt;Select a property from the menu in the &lt;strong&gt;PROPERTY&lt;/strong&gt; column.&lt;/li&gt;
&lt;li&gt;Under &lt;strong&gt;PROPERTY&lt;/strong&gt;, click Tracking Info &amp;gt; Tracking Code.&lt;/li&gt;
&lt;li&gt;Your tracking ID and property number are displayed at the top of the page. It should have the form &lt;code&gt;UA-xxxxxxxxx-1&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Now that we have the website tracking code, we need to insert it into the &lt;code&gt;googleAnalytics&lt;/code&gt; section of the &lt;code&gt;config/_default/params.yaml&lt;/code&gt; file.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;marketing:
  google_analytics: &#39;UA-xxxxxxxxx-1&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The mobile application of &lt;a href=&#34;https://analytics.google.com/analytics/web/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Google Analytics&lt;/a&gt; is particular intuitive and allows you to monitor your website traffic in detail. You just need to link the website from the &lt;a href=&#34;https://search.google.com/search-console&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Google Sesarch Console&lt;/a&gt; and then you can motitor you website from this platform. There is also a very nice mobile app for both &lt;a href=&#34;https://play.google.com/store/apps/details?id=com.google.android.apps.giant&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Android&lt;/a&gt; and &lt;a href=&#34;https://apps.apple.com/us/app/google-analytics/id881599038&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;iOS&lt;/a&gt; to monitor your website from your smartphone.&lt;/p&gt;
&lt;p&gt;Another good free tool to analyze the “quality” of your website is &lt;a href=&#34;https://www.seomechanic.com/seo-analyzer/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;SEO Mechanic&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;p&gt;Here are the main resources I used to write this guide:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Wowchemy website: &lt;a href=&#34;https://wowchemy.com/docs/getting-started/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://wowchemy.com/docs/getting-started/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Old Academic website: &lt;a href=&#34;https://sourcethemes.com/academic/docs/install/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://sourcethemes.com/academic/docs/install/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Guide for the Terminal: &lt;a href=&#34;https://github.com/fliptanedo/FlipWebsite2017&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/fliptanedo/FlipWebsite2017&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Difference in Differences</title>
      <link>https://matteocourthoud.github.io/post/diff_in_diffs/</link>
      <pubDate>Fri, 15 Apr 2022 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/post/diff_in_diffs/</guid>
      <description>&lt;p&gt;In this tutorial, we are going to see how to estimate the causal effect of a treatment on an outcome when treatment assignment is not random, but we observe both treated and untreated units before and after treatment. Under certain structural assumptions, especially parallel outcome trends in the absence of treatment, we can recover the average treatment effect.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Requisites&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;For this tutorial, I assume you are familiar with the following concepts:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Rubin&amp;rsquo;s potential outcome framework&lt;/li&gt;
&lt;li&gt;Ordinary least squares regression&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Academic Application&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;As an academic application, we are going to replicate &lt;a href=&#34;http://sims.princeton.edu/yftp/emet04/ck/CardKruegerMinWage.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Minimum Wages and Employment: A Case Study of the Fast-Food Industry in New Jersey and Pennsylvania&lt;/a&gt; (1994) by Card and Krueger. The authors study the effect of a minimum wage policy in New Jersey on emplyment, by using Pennsylvania as a control state.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Business Case&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;As a business case, we are going to study a firm that has run a TV ad campaign. The firm would like to understand the impact of the campaign on revenue and has randomized the campaign over municipalities.&lt;/p&gt;
&lt;h2 id=&#34;setting&#34;&gt;Setting&lt;/h2&gt;
&lt;p&gt;We assume that for a set of i.i.d. subjects $i = 1, &amp;hellip;, n$ over $T$ time periods $t = 1 ,  &amp;hellip; , T$, we observed a tuple $(X_{it}, D_{it}, Y_{it})$ comprised of&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;a feature vector $X_{it} \in \mathbb R^{p}$&lt;/li&gt;
&lt;li&gt;a treatment assignment $D_i \in \lbrace 0, 1 \rbrace$&lt;/li&gt;
&lt;li&gt;a response $Y_{it} \in \mathbb R$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We assume that treatment occurs between time $t=0$ and time $t=1$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Assumption 1: parallel trends&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In the absence of treatment, the outcome $Y_{it}$ &lt;strong&gt;evolve in parallel&lt;/strong&gt; across units, i.e. and their $\gamma_{t}$ are the same.&lt;/p&gt;
&lt;p&gt;$$
Y_{it}^{(0)} - Y_{j,t}^{(0)} = \alpha \quad \forall \ t
$$&lt;/p&gt;
&lt;h2 id=&#34;diff-in-diffs&#34;&gt;Diff-in-diffs&lt;/h2&gt;
&lt;p&gt;In this setting, we cannot estimate any causal parameter with any other &lt;strong&gt;further assumption&lt;/strong&gt;. What is the minimal number of assumptions that we could make in order to estimate a causal parameter?&lt;/p&gt;
&lt;p&gt;If we were to assume that treatment was randomly assigned, we could retrieve the average treatment effect as a difference in means.&lt;/p&gt;
&lt;p&gt;$$
\mathbb E[\tau_t] = \mathbb E \big[ Y_{it} \ \big| \ D_i = 1 \big] - \mathbb E \big[ Y_{it} \big| \ D_i = 0 \big]
$$&lt;/p&gt;
&lt;p&gt;However, it would be a very strong assumption, and it would ignore some information that we possess: the time dimension (pre-post).&lt;/p&gt;
&lt;p&gt;If we were to assume instead that no other shocks affected the treated units between period $t=0$ and $t=1$, we could retrieve the average treatment effect on the treated as a pre-post difference.&lt;/p&gt;
&lt;p&gt;$$
\mathbb E[\tau | D_i=1] = \mathbb E \big[ Y_{i1} \ \big| \ D_i = 1 \big] - \mathbb E \big[ Y_{i0} \ \big| \ D_i = 1 \big]
$$&lt;/p&gt;
&lt;p&gt;However, it also this would be a very strong assumption, and it would ignore the fact that we have control units.&lt;/p&gt;
&lt;p&gt;Can we make less stringent assumption and still recover a causal parameter using both the availability of a (non-random) control group and the time dimension?&lt;/p&gt;
&lt;h3 id=&#34;did-model&#34;&gt;DiD Model&lt;/h3&gt;
&lt;p&gt;The model that is commonly assumed in diff-ind-diff settings, is the following&lt;/p&gt;
&lt;p&gt;$$
Y_{it} (D_{it}) = \alpha_{i} + \gamma_{t} + \tau_{i} D_{it}
$$&lt;/p&gt;
&lt;p&gt;First, let&amp;rsquo;s summarize the potential outcome values $Y^{(d)}_{it}$ in the simple $2 \times 2$ setting.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;$t=0$&lt;/th&gt;
&lt;th&gt;$t=1$&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;$D=0$&lt;/td&gt;
&lt;td&gt;$\gamma_0 + \alpha_i$&lt;/td&gt;
&lt;td&gt;$\gamma_1 + \alpha_i$&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;$D=1$&lt;/td&gt;
&lt;td&gt;$\gamma_0 + \alpha_i + \tau_i$&lt;/td&gt;
&lt;td&gt;$\gamma_1 + \alpha_i + \tau_i$&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;For a single unit, $i$, the pre-post outcome difference is given by&lt;/p&gt;
&lt;p&gt;$$
Y_{i1} - Y_{i0} = (\gamma_1 - \gamma_0) + \tau_i (D_{i1} - D_{i0})
$$&lt;/p&gt;
&lt;p&gt;If we take the difference of the expression above between treated and untreated units, we get&lt;/p&gt;
&lt;p&gt;$$
\mathbb E \Big[ Y_{i1} - Y_{i0} \ \Big| \ D_{i1} - D_{i0} = 1 \Big] - \mathbb E \Big[ Y_{i1} - Y_{i0} \ \Big| \ D_{i1} - D_{i0} = 0 \Big] = \mathbb E \Big[ \tau_i \ \Big| \ D_{i1} - D_{i0} = 1 \Big] = ATT
$$&lt;/p&gt;
&lt;p&gt;which is the average treatment effect on the treated (ATT).&lt;/p&gt;
&lt;p&gt;We can get this double difference with the folowing regressio model&lt;/p&gt;
&lt;p&gt;$$
Y_{it} (D_{it}) = \alpha_{i} + \gamma_{t} + \beta D_{it} + \varepsilon_{it}
$$&lt;/p&gt;
&lt;p&gt;where the OLS estimator $\hat \beta$ will be unbiased for the ATT.&lt;/p&gt;
&lt;h3 id=&#34;multiple-time-periods&#34;&gt;Multiple Time Periods&lt;/h3&gt;
&lt;p&gt;What if we didn&amp;rsquo;t just have one pre-treatment period and one post-treatment period? Great! We can actually do more things.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;We can partially test assumptions&lt;/li&gt;
&lt;li&gt;We can estimate dynamic effects&lt;/li&gt;
&lt;li&gt;We can run placebo tests&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;How do we implement it? Run a regression with multiple interactions&lt;/p&gt;
&lt;p&gt;$$
Y_{it} (D_{it}) = \alpha_{i} + \gamma_{t} + \sum_{t=1}^{T} \beta_t D_{it} + \varepsilon_{it}
$$&lt;/p&gt;
&lt;h3 id=&#34;comments&#34;&gt;Comments&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Parametric Assumption&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The diff-in-diffs method makes a lot of parametric assumptions that are is easy to forget.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Inference&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Bertrand, Duflo, and Mullainathan (2004) point out that conventional robust standard errors usually overestimate the actual standard deviation of the estimator. The authors recommend &lt;strong&gt;clustering&lt;/strong&gt; the standard errors at the level of randomization (e.g. classes, counties, villages, &amp;hellip;).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Testing pre-trends&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Having multiple pre-treatment time periods is helpful for testing the parallel trends assumption. However, this practice can lead to pre-testing bias. In particular, if one selects results based on a pre-treatment parallel trend test, inference on the ATT gets distorderd.&lt;/p&gt;
&lt;h2 id=&#34;academic-application&#34;&gt;Academic Application&lt;/h2&gt;
&lt;p&gt;As an academic application, we are going to replicate &lt;a href=&#34;http://sims.princeton.edu/yftp/emet04/ck/CardKruegerMinWage.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Minimum Wages and Employment: A Case Study of the Fast-Food Industry in New Jersey and Pennsylvania&lt;/a&gt; (1994) by Card and Krueger. The authors study the effect of a minimum wage policy in New Jersey on emplyment, by using Pennsylvania as a control state.&lt;/p&gt;
&lt;p&gt;The authors describe the setting as follows&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;On April 1, 1992, New Jersey&amp;rsquo;s minimum wage rose from $4.25 to $5.05 per hour. To evaluate the impact of the law, the authors surveyed 410 fast-food restaurants in New Jersey and eastern Pennsylvania before and after the rise. Comparisons of employment growth at stores in New Jersey and Pennsylvania (where the minimum wage was constant) provide simple estimates of the effect of the higher minimum wage.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Let&amp;rsquo;s start by loading and inspecting the data.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;%matplotlib inline
%config InlineBackend.figure_format = &#39;retina&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from src.utils import *
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from src.data import import_ck94

df = pd.read_csv(&#39;data/ck94.csv&#39;)
df.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;id&lt;/th&gt;
      &lt;th&gt;after&lt;/th&gt;
      &lt;th&gt;new_jersey&lt;/th&gt;
      &lt;th&gt;chain&lt;/th&gt;
      &lt;th&gt;employment&lt;/th&gt;
      &lt;th&gt;hrsopen&lt;/th&gt;
      &lt;th&gt;wage&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;wendys&lt;/td&gt;
      &lt;td&gt;34.0&lt;/td&gt;
      &lt;td&gt;12.0&lt;/td&gt;
      &lt;td&gt;5.00&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;wendys&lt;/td&gt;
      &lt;td&gt;24.0&lt;/td&gt;
      &lt;td&gt;12.0&lt;/td&gt;
      &lt;td&gt;5.50&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;6&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;burgerking&lt;/td&gt;
      &lt;td&gt;70.5&lt;/td&gt;
      &lt;td&gt;18.0&lt;/td&gt;
      &lt;td&gt;5.00&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;7&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;burgerking&lt;/td&gt;
      &lt;td&gt;23.5&lt;/td&gt;
      &lt;td&gt;24.0&lt;/td&gt;
      &lt;td&gt;5.00&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;8&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;kfc&lt;/td&gt;
      &lt;td&gt;11.0&lt;/td&gt;
      &lt;td&gt;10.0&lt;/td&gt;
      &lt;td&gt;5.25&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;We have information on fast food restaurants, indexed by &lt;code&gt;i&lt;/code&gt;, at time &lt;code&gt;t&lt;/code&gt;. We distinguish between before and faster treatment and between New Jersey &lt;code&gt;nj&lt;/code&gt; and Pennsylvania restaurants. We also know the &lt;code&gt;chain&lt;/code&gt; of the restaurant, the &lt;code&gt;employment&lt;/code&gt;, the hours open &lt;code&gt;hrsopen&lt;/code&gt; and the &lt;code&gt;wage&lt;/code&gt;. We are interested on the effect on the policy on wages.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s start by producing the $2 \times 2$ table of treatment-control before-after average outcomes.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df.pivot_table(index=&#39;new_jersey&#39;, columns=&#39;after&#39;, values=&#39;employment&#39;, aggfunc=&#39;mean&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;after&lt;/th&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;th&gt;1&lt;/th&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;new_jersey&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;23.704545&lt;/td&gt;
      &lt;td&gt;21.825758&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;20.657746&lt;/td&gt;
      &lt;td&gt;21.048415&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;From the table we can see that a simple &lt;strong&gt;before-after comparison&lt;/strong&gt; would give a small posive effect of $21.05 - 20.66 = 0.39$.&lt;/p&gt;
&lt;p&gt;On the other hand, if one was doing an ex-post &lt;strong&gt;treated-control comparison&lt;/strong&gt;, would get a negative effect of $21.05 - 21.83 = - 0.78$.&lt;/p&gt;
&lt;p&gt;The difference-in-differences estimator takes into account the fact that&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;There is a pre-treatment level difference between New Jersey and Pennsylvania&lt;/li&gt;
&lt;li&gt;Employment was falling in Pennsylvania even without treatment&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The &lt;strong&gt;double difference&lt;/strong&gt; in means gives a positive effect, significantly larger than any of the two previous estimates.&lt;/p&gt;
&lt;p&gt;$$
\hat \tau_{DiD} = \Big( 21.05 - 20.66 \Big) - \Big( 21.83 - 23.70 \Big) = 0.39 + 1.87 = 2.26
$$&lt;/p&gt;
&lt;p&gt;We can replicate the result with a linear regression.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;smf.ols(&#39;employment ~ new_jersey * after&#39;, df).fit().summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
          &lt;td&gt;&lt;/td&gt;            &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt;        &lt;td&gt;   23.7045&lt;/td&gt; &lt;td&gt;    1.149&lt;/td&gt; &lt;td&gt;   20.627&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;   21.448&lt;/td&gt; &lt;td&gt;   25.961&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;new_jersey&lt;/th&gt;       &lt;td&gt;   -3.0468&lt;/td&gt; &lt;td&gt;    1.276&lt;/td&gt; &lt;td&gt;   -2.388&lt;/td&gt; &lt;td&gt; 0.017&lt;/td&gt; &lt;td&gt;   -5.552&lt;/td&gt; &lt;td&gt;   -0.542&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;after&lt;/th&gt;            &lt;td&gt;   -1.8788&lt;/td&gt; &lt;td&gt;    1.625&lt;/td&gt; &lt;td&gt;   -1.156&lt;/td&gt; &lt;td&gt; 0.248&lt;/td&gt; &lt;td&gt;   -5.070&lt;/td&gt; &lt;td&gt;    1.312&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;new_jersey:after&lt;/th&gt; &lt;td&gt;    2.2695&lt;/td&gt; &lt;td&gt;    1.804&lt;/td&gt; &lt;td&gt;    1.258&lt;/td&gt; &lt;td&gt; 0.209&lt;/td&gt; &lt;td&gt;   -1.273&lt;/td&gt; &lt;td&gt;    5.812&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;The effect is $2.26$, but it is not significantly different from zero.&lt;/p&gt;
&lt;h2 id=&#34;business-case&#34;&gt;Business Case&lt;/h2&gt;
&lt;p&gt;We are given the following problem:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;A firm wants to test the impact of a TV advertisement campaign on revenue. The firm releases the ad on a random sample of municipalities and track the revenue over time, before and after the ad campaign.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;We start by drawing a sample from the data generating process.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from src.dgp import dgp_did

dgp = dgp_did()
df = dgp.generate_data()
df.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;day&lt;/th&gt;
      &lt;th&gt;id&lt;/th&gt;
      &lt;th&gt;treated&lt;/th&gt;
      &lt;th&gt;post&lt;/th&gt;
      &lt;th&gt;revenue&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;3.599341&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;-0.146912&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;0.696527&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;1.445169&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;1.659696&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;We have &lt;code&gt;revenue&lt;/code&gt; data on a set of customers over time. We also know to which &lt;code&gt;group&lt;/code&gt; they were assigned and whether the time is before or after the intervention.&lt;/p&gt;
&lt;p&gt;Since we do not have any control variable, we can directly visualize the revenue dynamics, distinguishing between treatment and control group.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sns.lineplot(x=df[&#39;day&#39;], y=df[&#39;revenue&#39;], hue=df[&#39;treated&#39;]);
plt.axvline(x=10, ls=&amp;quot;:&amp;quot;, color=&#39;C2&#39;);
plt.title(&#39;Revenue over time&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/diff_in_diffs_26_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;It seems like the treatment group was producing higher revenues before treatment and the gap has increased with treatment but it is closing over time.&lt;/p&gt;
&lt;p&gt;To assess the magnitude of the effect and perform inference, we can regress revenue on a post-treatment dummy, a treatment dummy and their interaction.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;smf.ols(&#39;revenue ~ post * treated&#39;, df).fit().summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
            &lt;td&gt;&lt;/td&gt;              &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt;            &lt;td&gt;    4.6357&lt;/td&gt; &lt;td&gt;    0.078&lt;/td&gt; &lt;td&gt;   59.428&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    4.483&lt;/td&gt; &lt;td&gt;    4.789&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;post[T.True]&lt;/th&gt;         &lt;td&gt;    0.8928&lt;/td&gt; &lt;td&gt;    0.110&lt;/td&gt; &lt;td&gt;    8.093&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    0.676&lt;/td&gt; &lt;td&gt;    1.109&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;treated&lt;/th&gt;              &lt;td&gt;    1.0558&lt;/td&gt; &lt;td&gt;    0.110&lt;/td&gt; &lt;td&gt;    9.571&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    0.839&lt;/td&gt; &lt;td&gt;    1.272&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;post[T.True]:treated&lt;/th&gt; &lt;td&gt;    0.1095&lt;/td&gt; &lt;td&gt;    0.156&lt;/td&gt; &lt;td&gt;    0.702&lt;/td&gt; &lt;td&gt; 0.483&lt;/td&gt; &lt;td&gt;   -0.196&lt;/td&gt; &lt;td&gt;    0.415&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;While the coefficient for the interaction term is positive, it does not seem to be statistically significant. However, this might be due to the fact that the treatment effect is fading away over time.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s fit the same regression, with a linear time trend.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;smf.ols(&#39;revenue ~ post * treated * day&#39;, df).fit().summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
              &lt;td&gt;&lt;/td&gt;                &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt;                &lt;td&gt;    4.1144&lt;/td&gt; &lt;td&gt;    0.168&lt;/td&gt; &lt;td&gt;   24.501&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    3.785&lt;/td&gt; &lt;td&gt;    4.444&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;post[T.True]&lt;/th&gt;             &lt;td&gt;    0.2871&lt;/td&gt; &lt;td&gt;    0.458&lt;/td&gt; &lt;td&gt;    0.626&lt;/td&gt; &lt;td&gt; 0.531&lt;/td&gt; &lt;td&gt;   -0.612&lt;/td&gt; &lt;td&gt;    1.186&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;treated&lt;/th&gt;                  &lt;td&gt;    1.0788&lt;/td&gt; &lt;td&gt;    0.237&lt;/td&gt; &lt;td&gt;    4.543&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    0.613&lt;/td&gt; &lt;td&gt;    1.544&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;post[T.True]:treated&lt;/th&gt;     &lt;td&gt;    1.5910&lt;/td&gt; &lt;td&gt;    0.648&lt;/td&gt; &lt;td&gt;    2.454&lt;/td&gt; &lt;td&gt; 0.014&lt;/td&gt; &lt;td&gt;    0.320&lt;/td&gt; &lt;td&gt;    2.862&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;day&lt;/th&gt;                      &lt;td&gt;    0.0948&lt;/td&gt; &lt;td&gt;    0.027&lt;/td&gt; &lt;td&gt;    3.502&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    0.042&lt;/td&gt; &lt;td&gt;    0.148&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;post[T.True]:day&lt;/th&gt;         &lt;td&gt;   -0.0221&lt;/td&gt; &lt;td&gt;    0.038&lt;/td&gt; &lt;td&gt;   -0.576&lt;/td&gt; &lt;td&gt; 0.564&lt;/td&gt; &lt;td&gt;   -0.097&lt;/td&gt; &lt;td&gt;    0.053&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;treated:day&lt;/th&gt;              &lt;td&gt;   -0.0042&lt;/td&gt; &lt;td&gt;    0.038&lt;/td&gt; &lt;td&gt;   -0.109&lt;/td&gt; &lt;td&gt; 0.913&lt;/td&gt; &lt;td&gt;   -0.079&lt;/td&gt; &lt;td&gt;    0.071&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;post[T.True]:treated:day&lt;/th&gt; &lt;td&gt;   -0.0929&lt;/td&gt; &lt;td&gt;    0.054&lt;/td&gt; &lt;td&gt;   -1.716&lt;/td&gt; &lt;td&gt; 0.086&lt;/td&gt; &lt;td&gt;   -0.199&lt;/td&gt; &lt;td&gt;    0.013&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;Now the treatment effect is positive and significant at the 5% level. And indeed, we estimate a decreasing trend, post treatment, for the treated. However, it is not statistically significant.&lt;/p&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=mbYTZ0w-QTw&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Video lecture on Difference-in-Differences&lt;/a&gt; by Paul Goldsmith-Pinkham (Yale)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://matheusfacure.github.io/python-causality-handbook/13-Difference-in-Differences.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Chapter 13&lt;/a&gt; of Causal Inference for The Brave and The True by Matheus Facure&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://mixtape.scunning.com/difference-in-differences.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Chapter 9&lt;/a&gt; of The Causal Inference Mixtape by Scott Cunningham&lt;/li&gt;
&lt;li&gt;Chapter 5 of &lt;a href=&#34;https://www.mostlyharmlesseconometrics.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Mostly Harmless Econometrics&lt;/a&gt; by Agrist and Pischke&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Instrumental Variables</title>
      <link>https://matteocourthoud.github.io/post/iv/</link>
      <pubDate>Fri, 15 Apr 2022 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/post/iv/</guid>
      <description>&lt;p&gt;In this tutorial, we are going to see how to estimate causal effects when the treatment is not randomly assigned, but we have access to a third variable that is as good as randomly assigned and is correlated (only) with the treatment. These variables are called instrumental variables and are a powerful tool for causal inference, especially in observational studies.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Requisites&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;For this tutorial, I assume you are familiar with the following concepts:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Rubin&amp;rsquo;s potential outcome framework&lt;/li&gt;
&lt;li&gt;Ordinary least squares regression&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Academic Application 1&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;As a first academic application, we are going to replicate &lt;a href=&#34;https://www.jstor.org/stable/2937954&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Does Compulsory School Attendance Affect Schooling and Earnings?&lt;/a&gt; (1991) by Angrist and Krueger. The authors study the effect of education on wages.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Academic Application 2&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;As a further academic application, we are going to replicate &lt;a href=&#34;https://economics.mit.edu/files/4123&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The Colonial Origins of Comparative Development&lt;/a&gt; (2002) by Acemoglu, Johnson, Robinson. The authors study the effect of institutions on economic development.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Business Case&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;As a business case, we are going to study a company that wants to find out whether subscribing to its newsletter has an effect on revenues. Since the travel agency cannot force customers to subscribing to the newsletter, it randomly sends reminder emails to infer the effect of the newsletter on revenues.&lt;/p&gt;
&lt;h2 id=&#34;setting&#34;&gt;Setting&lt;/h2&gt;
&lt;p&gt;We assume that for a set of i.i.d. subjects $i = 1, &amp;hellip;, n$ we observed a tuple $(X_i, T_i, Y_i)$ comprised of&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;a feature vector $X_i \in \mathbb R^n$&lt;/li&gt;
&lt;li&gt;a treatment variable $T_i \in \lbrace 0, 1 \rbrace$&lt;/li&gt;
&lt;li&gt;a response $Y_i \in \mathbb R$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Crucially, we do not assume unconfoundedness / strong ignorability hence&lt;/p&gt;
&lt;p&gt;$$
\big \lbrace Y_i^{(1)} , Y_i^{(0)} \big \rbrace \ \not \perp \ T_i \ | \ X_i
$$&lt;/p&gt;
&lt;h2 id=&#34;instrumental-variables&#34;&gt;Instrumental Variables&lt;/h2&gt;
&lt;p&gt;The &lt;strong&gt;standard linear IV model&lt;/strong&gt; is the following&lt;/p&gt;
&lt;p&gt;$$
Y_i = T_i \alpha + X_i \beta_1 + \varepsilon_i
\newline
T_i = Z_i \gamma + X_i \beta_2 + u_i
$$&lt;/p&gt;
&lt;p&gt;We assume there exists an &lt;strong&gt;instrumental variable&lt;/strong&gt; $Z_i \in \mathbb R^k$ that satisfies the following assumptions.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Assumption 1: Exclusion&lt;/strong&gt;: $\mathbb E [Z \varepsilon] = 0$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Assumption 2: Relevance&lt;/strong&gt;: $\mathbb E [Z T] \neq 0$&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The model can be represented by a DAG.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from src.plots import dag_iv
dag_iv()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/iv_6_0.svg&#34; alt=&#34;svg&#34;&gt;&lt;/p&gt;
&lt;p&gt;The IV estimator is instead unbiased&lt;/p&gt;
&lt;p&gt;$$
\hat \beta_{IV} = (Z&amp;rsquo;X)^{-1}(Z&amp;rsquo;Y)
$$&lt;/p&gt;
&lt;h3 id=&#34;potential-outcomes-perspective&#34;&gt;Potential Outcomes Perspective&lt;/h3&gt;
&lt;p&gt;We need to extend the potential outcomes framework in order to allow for the instrumental variable $Z$. First we define the potential outcomes as $Y^{(D(Z_i))}(Z_i)$&lt;/p&gt;
&lt;p&gt;The assumptions become&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Exclusion: $Y^{(D(Z_i))}(Z_i) = Y^{(T(Z_i))}$&lt;/li&gt;
&lt;li&gt;Relevance: $P(z) = \mathbb E [T, Z=z]$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We assume that $Z$ is fully randomly assigned (while $T$ is not).&lt;/p&gt;
&lt;p&gt;What does IV estimate?&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
\mathbb E[Y_i | Z_i = 1] - \mathbb E[Y_i | Z_i = 0] &amp;amp;= \Pr (D_i^{(1)} - D_i^{(0)} = 1) \times \mathbb E \Big[ Y_i^{(1)} - Y_i^{(0)} = 1 \ \Big | \ D_i^{(1)} - D_i^{(0)} = 1 \Big] -
\newline
&amp;amp;- \Pr (D_i^{(1)} - D_i^{(0)} = -1) \times \mathbb E \Big[ Y_i^{(1)} - Y_i^{(0)} = 1 \ \Big | \ D_i^{(1)} - D_i^{(0)} = -1 \Big]
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;Is this a quantity of interest? Almost. There are &lt;strong&gt;two issues&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;First, the first term is the treatment effect, but only for those individuals for whom $D_i^{(1)} - D_i^{(0)} = 1$, i.e. those that are induced into treatment by $Z_i$. These individuals are referred to as &lt;strong&gt;compliers&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Second, the second term is problematic since it removes from the first effect, the local effect of another subpopulation: $D_i^{(1)} - D_i^{(0)} = -1$, i.e. those that are induced out of treatment by $Z_i$. These individuals are referred to as &lt;strong&gt;defiers&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;We can get rid of defiers with a simple assumption.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Assumption 3: monotonocity&lt;/strong&gt;: $D_i^{(1)} \geq D_i^{(0)}$ (or viceversa)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;All effects must be monotone in the same direction&lt;/li&gt;
&lt;li&gt;Fundamentally untestable&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Then, the IV estimator can be expressed as a ration between two differences in means&lt;/p&gt;
&lt;p&gt;$$
\hat \beta_{IV} = \frac{\mathbb E[Y_i | Z_i = 1] - \mathbb E[Y_i | Z_i = 0]}{\mathbb E[T_i | Z_i = 1] - \mathbb E[T_i | Z_i = 0]}
$$&lt;/p&gt;
&lt;h3 id=&#34;structural-perspective&#34;&gt;Structural Perspective&lt;/h3&gt;
&lt;p&gt;One can interpret the IV estimator as a GMM estimator that uses the exclusion restriction as estimating equation.&lt;/p&gt;
&lt;p&gt;$$
\hat \beta_{GMM} = \arg \min_{\beta} \mathbb E \Big[ Z (Y - \alpha T - \beta X) \Big]^2
$$&lt;/p&gt;
&lt;h2 id=&#34;the-algebra-of-iv&#34;&gt;The Algebra of IV&lt;/h2&gt;
&lt;p&gt;TBD&lt;/p&gt;
&lt;h2 id=&#34;demand-and-supply&#34;&gt;Demand and Supply&lt;/h2&gt;
&lt;p&gt;TBD&lt;/p&gt;
&lt;h2 id=&#34;academic-application-1&#34;&gt;Academic Application 1&lt;/h2&gt;
&lt;p&gt;As an research paper replication, we are going to replicate &lt;a href=&#34;https://www.jstor.org/stable/2937954&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Does compulsory school attendance affect schooling and earnings?&lt;/a&gt; (1991) by Angrist and Krueger. The authors study the effect of education on wages.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;problem&lt;/strong&gt; of studying the relationship of education on wages is that there might be factors that influence both education and wages but we do not observe, for example ability. Students that have higher ability might decide to stay longer in school and also get higher wages afterwards.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;idea&lt;/strong&gt; of the authors is to use the quarter of birth as an instrument for education. In fact, quarter of birth is plausibly exogenous with respect to wages while, on the other hand, is correlated with education. Why? Students that are both in the last quarter of the year cannot drop out as early as other students and therefore are exposed to more eduction.&lt;/p&gt;
&lt;p&gt;We can represent the DAG of their model as follows.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;%matplotlib inline
%config InlineBackend.figure_format = &#39;retina&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from src.utils import *
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;dag_iv(Y=&amp;quot;wage&amp;quot;, T=&amp;quot;education&amp;quot;, Z=&amp;quot;quarter of birth&amp;quot;, U=&amp;quot;ability&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/iv_20_0.svg&#34; alt=&#34;svg&#34;&gt;&lt;/p&gt;
&lt;p&gt;A shortcoming of this instrument comes out of the fact that the population of &lt;strong&gt;compliers&lt;/strong&gt; is students that drop out of school as soon as possible, we will know the treatment effect only for this population. It&amp;rsquo;s important to keep this in mind when interpreting the results.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s load the data, freely available &lt;a href=&#34;https://economics.mit.edu/faculty/angrist/data1/data/angkru1991&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df = pd.read_csv(&#39;data/ak91.csv&#39;)
df.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;log_wage&lt;/th&gt;
      &lt;th&gt;years_of_schooling&lt;/th&gt;
      &lt;th&gt;date_of_birth&lt;/th&gt;
      &lt;th&gt;year_of_birth&lt;/th&gt;
      &lt;th&gt;quarter_of_birth&lt;/th&gt;
      &lt;th&gt;state_of_birth&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;5.790019&lt;/td&gt;
      &lt;td&gt;12.0&lt;/td&gt;
      &lt;td&gt;1930.0&lt;/td&gt;
      &lt;td&gt;1930.0&lt;/td&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;45.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;5.952494&lt;/td&gt;
      &lt;td&gt;11.0&lt;/td&gt;
      &lt;td&gt;1930.0&lt;/td&gt;
      &lt;td&gt;1930.0&lt;/td&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;45.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;5.315949&lt;/td&gt;
      &lt;td&gt;12.0&lt;/td&gt;
      &lt;td&gt;1930.0&lt;/td&gt;
      &lt;td&gt;1930.0&lt;/td&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;45.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;5.595926&lt;/td&gt;
      &lt;td&gt;12.0&lt;/td&gt;
      &lt;td&gt;1930.0&lt;/td&gt;
      &lt;td&gt;1930.0&lt;/td&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;45.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;6.068915&lt;/td&gt;
      &lt;td&gt;12.0&lt;/td&gt;
      &lt;td&gt;1930.0&lt;/td&gt;
      &lt;td&gt;1930.0&lt;/td&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;37.0&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;We have the variables of interest, &lt;code&gt;log_wage&lt;/code&gt;, &lt;code&gt;years_of_schooling&lt;/code&gt; and &lt;code&gt;quarter_of_birth&lt;/code&gt;, together with a set of controls.&lt;/p&gt;
&lt;h3 id=&#34;ols&#34;&gt;OLS&lt;/h3&gt;
&lt;p&gt;If we were to ignore the endogeneity problem we would estimate a linear regression of &lt;code&gt;log_wage&lt;/code&gt; on &lt;code&gt;years_of_schooling&lt;/code&gt;, plus control dummy variables for the &lt;code&gt;state_of_birth&lt;/code&gt; and &lt;code&gt;year_of_birth&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;smf.ols(&#39;log_wage ~ years_of_schooling&#39;, df).fit().summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
           &lt;td&gt;&lt;/td&gt;             &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt;          &lt;td&gt;    4.9952&lt;/td&gt; &lt;td&gt;    0.004&lt;/td&gt; &lt;td&gt; 1118.882&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    4.986&lt;/td&gt; &lt;td&gt;    5.004&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;years_of_schooling&lt;/th&gt; &lt;td&gt;    0.0709&lt;/td&gt; &lt;td&gt;    0.000&lt;/td&gt; &lt;td&gt;  209.243&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    0.070&lt;/td&gt; &lt;td&gt;    0.072&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;h3 id=&#34;iv&#34;&gt;IV&lt;/h3&gt;
&lt;p&gt;We now use &lt;code&gt;quarter_of_birth&lt;/code&gt; as an instrument for &lt;code&gt;years_of_schooling&lt;/code&gt;. We cannot check the exclusion restriction condition, but we can check the &lt;strong&gt;relevance&lt;/strong&gt; condition.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s start first by plotting average &lt;code&gt;years_of_schooling&lt;/code&gt; by date of birth.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;group_df = df.groupby(&amp;quot;date_of_birth&amp;quot;).mean().reset_index()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plt.figure(figsize=(15,6))
sns.lineplot(data=group_df, x=&amp;quot;date_of_birth&amp;quot;, y=&amp;quot;years_of_schooling&amp;quot;, zorder=1)\
.set(title=&amp;quot;First Stage&amp;quot;, xlabel=&amp;quot;Year of Birth&amp;quot;, ylabel=&amp;quot;Years of Schooling&amp;quot;);

for q in range(1, 5):
    x = group_df.loc[group_df[&#39;quarter_of_birth&#39;]==q, &amp;quot;date_of_birth&amp;quot;]
    y = group_df.loc[group_df[&#39;quarter_of_birth&#39;]==q, &amp;quot;years_of_schooling&amp;quot;]
    plt.scatter(x, y, marker=&amp;quot;s&amp;quot;, s=200, c=f&amp;quot;C{q}&amp;quot;)
    plt.scatter(x, y, marker=f&amp;quot;${q}$&amp;quot;, s=100, c=f&amp;quot;white&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/iv_31_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;As we can see, there is an upward trend but, within each year, people both in the last quarter usually have more years of schooling than people born in other quarters of the year.&lt;/p&gt;
&lt;p&gt;We can check this correlation more formally by regressing &lt;code&gt;years_of_schooling&lt;/code&gt; of a set of dummies for &lt;code&gt;quarter_of_birth&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;smf.ols(&#39;years_of_schooling ~ C(quarter_of_birth)&#39;, df).fit().summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
               &lt;td&gt;&lt;/td&gt;                 &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt;                  &lt;td&gt;   12.6881&lt;/td&gt; &lt;td&gt;    0.011&lt;/td&gt; &lt;td&gt; 1105.239&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;   12.666&lt;/td&gt; &lt;td&gt;   12.711&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;C(quarter_of_birth)[T.2.0]&lt;/th&gt; &lt;td&gt;    0.0566&lt;/td&gt; &lt;td&gt;    0.016&lt;/td&gt; &lt;td&gt;    3.473&lt;/td&gt; &lt;td&gt; 0.001&lt;/td&gt; &lt;td&gt;    0.025&lt;/td&gt; &lt;td&gt;    0.089&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;C(quarter_of_birth)[T.3.0]&lt;/th&gt; &lt;td&gt;    0.1173&lt;/td&gt; &lt;td&gt;    0.016&lt;/td&gt; &lt;td&gt;    7.338&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    0.086&lt;/td&gt; &lt;td&gt;    0.149&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;C(quarter_of_birth)[T.4.0]&lt;/th&gt; &lt;td&gt;    0.1514&lt;/td&gt; &lt;td&gt;    0.016&lt;/td&gt; &lt;td&gt;    9.300&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    0.119&lt;/td&gt; &lt;td&gt;    0.183&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;The relationship between &lt;code&gt;years_of_schooling&lt;/code&gt; and &lt;code&gt;quarter_of_birth&lt;/code&gt; is indeed statistically significant.&lt;/p&gt;
&lt;p&gt;Does it translate it into higher wages? We can have a first glimpse of potential IV effects by plotting wages against the date of birth.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plt.figure(figsize=(15,6))
sns.lineplot(data=group_df, x=&amp;quot;date_of_birth&amp;quot;, y=&amp;quot;log_wage&amp;quot;, zorder=1)\
.set(title=&amp;quot;Reduced Form&amp;quot;, xlabel=&amp;quot;Year of Birth&amp;quot;, ylabel=&amp;quot;Log Wage&amp;quot;);

for q in range(1, 5):
    x = group_df.loc[group_df[&#39;quarter_of_birth&#39;]==q, &amp;quot;date_of_birth&amp;quot;]
    y = group_df.loc[group_df[&#39;quarter_of_birth&#39;]==q, &amp;quot;log_wage&amp;quot;]
    plt.scatter(x, y, marker=&amp;quot;s&amp;quot;, s=200, c=f&amp;quot;C{q}&amp;quot;)
    plt.scatter(x, y, marker=f&amp;quot;${q}$&amp;quot;, s=100, c=f&amp;quot;white&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/iv_35_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;It seems that indeed people both in later quarters earn higher wages later in life.&lt;/p&gt;
&lt;p&gt;We now turn into the estimation of the causal effect of education on wages.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df[[&#39;q1&#39;, &#39;q2&#39;, &#39;q3&#39;, &#39;q4&#39;]] = pd.get_dummies(df[&#39;quarter_of_birth&#39;])
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from linearmodels.iv import IV2SLS

IV2SLS.from_formula(&#39;log_wage ~ 1 + [years_of_schooling ~ q1 + q2 + q3]&#39;, data=df).fit().summary.tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;caption&gt;Parameter Estimates&lt;/caption&gt;
&lt;tr&gt;
           &lt;td&gt;&lt;/td&gt;          &lt;th&gt;Parameter&lt;/th&gt; &lt;th&gt;Std. Err.&lt;/th&gt; &lt;th&gt;T-stat&lt;/th&gt; &lt;th&gt;P-value&lt;/th&gt; &lt;th&gt;Lower CI&lt;/th&gt; &lt;th&gt;Upper CI&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt;           &lt;td&gt;4.5898&lt;/td&gt;    &lt;td&gt;0.2494&lt;/td&gt;   &lt;td&gt;18.404&lt;/td&gt; &lt;td&gt;0.0000&lt;/td&gt;   &lt;td&gt;4.1010&lt;/td&gt;   &lt;td&gt;5.0786&lt;/td&gt; 
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;years_of_schooling&lt;/th&gt;  &lt;td&gt;0.1026&lt;/td&gt;    &lt;td&gt;0.0195&lt;/td&gt;   &lt;td&gt;5.2539&lt;/td&gt; &lt;td&gt;0.0000&lt;/td&gt;   &lt;td&gt;0.0643&lt;/td&gt;   &lt;td&gt;0.1409&lt;/td&gt; 
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;The coefficient is slightly higher than the OLS coefficient. It&amp;rsquo;s important to remember that the estimated effect is specific to the subpopulation of people that drop out of school as soon as they can.&lt;/p&gt;
&lt;h2 id=&#34;research-paper-replication-2&#34;&gt;Research Paper Replication 2&lt;/h2&gt;
&lt;p&gt;In &lt;a href=&#34;https://economics.mit.edu/files/4123&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The Colonial Origins of Comparative Development&lt;/a&gt; (2002) by Acemoglu, Johnson, Robinson, the authors wish to determine whether or not differences in institutions can help to explain observed economic outcomes.&lt;/p&gt;
&lt;p&gt;How do we measure &lt;em&gt;institutional differences&lt;/em&gt; and &lt;em&gt;economic outcomes&lt;/em&gt;?&lt;/p&gt;
&lt;p&gt;In this paper,&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;economic outcomes are proxied by log GDP per capita in 1995, adjusted for exchange rates.&lt;/li&gt;
&lt;li&gt;institutional differences are proxied by an index of protection against expropriation on average over 1985-95, constructed by the &lt;a href=&#34;https://www.prsgroup.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Political Risk Services Group&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The &lt;strong&gt;problem&lt;/strong&gt; is that there might exist other factors that affects both the quality of institutions and GDP. The authors suggest the following problems as sources of endogeneity:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;richer countries may be able to afford or prefer better institutions&lt;/li&gt;
&lt;li&gt;variables that affect income may also be correlated with institutional differences&lt;/li&gt;
&lt;li&gt;the construction of the index may be biased; analysts may be biased towards seeing countries with higher income having better institutions&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The &lt;strong&gt;idea&lt;/strong&gt; of the authors is to use settler&amp;rsquo;s mortality during the colonization period as an instrument for the quality of institutions. They hypothesize that higher mortality rates of colonizers led to the establishment of institutions that were more extractive in nature (less protection against expropriation), and these institutions still persist today.&lt;/p&gt;
&lt;p&gt;We can represent their DAG as follows.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;dag_iv(Y=&amp;quot;GDP&amp;quot;, T=&amp;quot;institutions&amp;quot;, Z=&amp;quot;settlers&#39; mortality&amp;quot;, U=&amp;quot;tons of stuff&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/iv_42_0.svg&#34; alt=&#34;svg&#34;&gt;&lt;/p&gt;
&lt;p&gt;First, let&amp;rsquo;s load the data (available &lt;a href=&#34;https://economics.mit.edu/faculty/acemoglu/data/ajr2001&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;) and have a look at it.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df = pd.read_csv(&#39;data/ajr02.csv&#39;,index_col=0)
df.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;GDP&lt;/th&gt;
      &lt;th&gt;Exprop&lt;/th&gt;
      &lt;th&gt;Mort&lt;/th&gt;
      &lt;th&gt;Latitude&lt;/th&gt;
      &lt;th&gt;Neo&lt;/th&gt;
      &lt;th&gt;Africa&lt;/th&gt;
      &lt;th&gt;Asia&lt;/th&gt;
      &lt;th&gt;Namer&lt;/th&gt;
      &lt;th&gt;Samer&lt;/th&gt;
      &lt;th&gt;logMort&lt;/th&gt;
      &lt;th&gt;Latitude2&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;8.39&lt;/td&gt;
      &lt;td&gt;6.50&lt;/td&gt;
      &lt;td&gt;78.20&lt;/td&gt;
      &lt;td&gt;0.3111&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;4.359270&lt;/td&gt;
      &lt;td&gt;0.096783&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;7.77&lt;/td&gt;
      &lt;td&gt;5.36&lt;/td&gt;
      &lt;td&gt;280.00&lt;/td&gt;
      &lt;td&gt;0.1367&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;5.634790&lt;/td&gt;
      &lt;td&gt;0.018687&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;9.13&lt;/td&gt;
      &lt;td&gt;6.39&lt;/td&gt;
      &lt;td&gt;68.90&lt;/td&gt;
      &lt;td&gt;0.3778&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;4.232656&lt;/td&gt;
      &lt;td&gt;0.142733&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;9.90&lt;/td&gt;
      &lt;td&gt;9.32&lt;/td&gt;
      &lt;td&gt;8.55&lt;/td&gt;
      &lt;td&gt;0.3000&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;2.145931&lt;/td&gt;
      &lt;td&gt;0.090000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;5&lt;/th&gt;
      &lt;td&gt;9.29&lt;/td&gt;
      &lt;td&gt;7.50&lt;/td&gt;
      &lt;td&gt;85.00&lt;/td&gt;
      &lt;td&gt;0.2683&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;4.442651&lt;/td&gt;
      &lt;td&gt;0.071985&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;The data contains the main variables, &lt;code&gt;DGP&lt;/code&gt;, &lt;code&gt;Exprop&lt;/code&gt; and &lt;code&gt;Mort&lt;/code&gt;, plus some geographical information.&lt;/p&gt;
&lt;h3 id=&#34;ols-1&#34;&gt;OLS&lt;/h3&gt;
&lt;p&gt;What would we get if we were to ignore the endogeneity problem? We estimate the following misspecified model by OLS&lt;/p&gt;
&lt;p&gt;$$
{GDP}_i = \beta_0 + \beta_1 {Exprop}_i + \varepsilon_i
$$&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;reg1 = smf.ols(&#39;GDP ~ Exprop&#39;, df).fit()
reg1.summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
      &lt;td&gt;&lt;/td&gt;         &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt; &lt;td&gt;    4.6609&lt;/td&gt; &lt;td&gt;    0.409&lt;/td&gt; &lt;td&gt;   11.402&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    3.844&lt;/td&gt; &lt;td&gt;    5.478&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Exprop&lt;/th&gt;    &lt;td&gt;    0.5220&lt;/td&gt; &lt;td&gt;    0.061&lt;/td&gt; &lt;td&gt;    8.527&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    0.400&lt;/td&gt; &lt;td&gt;    0.644&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;The coefficient of &lt;code&gt;Exprop&lt;/code&gt; is positive and significant but we know it is a biased estimate of the causal effect.&lt;/p&gt;
&lt;p&gt;One direction we could take in addressing the endogeneity problem could be to control for any factor that affects both &lt;code&gt;GDP&lt;/code&gt; and &lt;code&gt;Exprop&lt;/code&gt;. In particular, the authors consider the following sets of variables:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;climat; proxied by latitude&lt;/li&gt;
&lt;li&gt;differences that affect both economic performance and institutions, eg. cultural, historical, etc.; controlled for with the use of continent dummies&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;reg2 = smf.ols(&#39;GDP ~ Exprop + Latitude + Latitude2&#39;, df).fit()
reg3 = smf.ols(&#39;GDP ~ Exprop + Latitude + Latitude2 + Asia + Africa + Namer + Samer&#39;, df).fit()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from statsmodels.iolib.summary2 import summary_col

summary_col(results=[reg1,reg2,reg3],
            float_format=&#39;%0.2f&#39;,
            stars = True,
            info_dict={&#39;No. observations&#39; : lambda x: f&amp;quot;{int(x.nobs):d}&amp;quot;},
            regressor_order=[&#39;Intercept&#39;,&#39;Exprop&#39;,&#39;Latitude&#39;,&#39;Latitude2&#39;])
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
          &lt;td&gt;&lt;/td&gt;          &lt;th&gt;GDP I&lt;/th&gt;  &lt;th&gt;GDP II&lt;/th&gt;  &lt;th&gt;GDP III&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt;        &lt;td&gt;4.66***&lt;/td&gt; &lt;td&gt;4.55***&lt;/td&gt; &lt;td&gt;5.95***&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;&lt;/th&gt;                 &lt;td&gt;(0.41)&lt;/td&gt;  &lt;td&gt;(0.45)&lt;/td&gt;  &lt;td&gt;(0.68)&lt;/td&gt; 
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Exprop&lt;/th&gt;           &lt;td&gt;0.52***&lt;/td&gt; &lt;td&gt;0.49***&lt;/td&gt; &lt;td&gt;0.40***&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;&lt;/th&gt;                 &lt;td&gt;(0.06)&lt;/td&gt;  &lt;td&gt;(0.07)&lt;/td&gt;  &lt;td&gt;(0.06)&lt;/td&gt; 
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Latitude&lt;/th&gt;            &lt;td&gt;&lt;/td&gt;      &lt;td&gt;2.16&lt;/td&gt;    &lt;td&gt;0.42&lt;/td&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;&lt;/th&gt;                    &lt;td&gt;&lt;/td&gt;     &lt;td&gt;(1.68)&lt;/td&gt;  &lt;td&gt;(1.47)&lt;/td&gt; 
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Latitude2&lt;/th&gt;           &lt;td&gt;&lt;/td&gt;      &lt;td&gt;-2.12&lt;/td&gt;   &lt;td&gt;0.44&lt;/td&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;&lt;/th&gt;                    &lt;td&gt;&lt;/td&gt;     &lt;td&gt;(2.86)&lt;/td&gt;  &lt;td&gt;(2.48)&lt;/td&gt; 
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Africa&lt;/th&gt;              &lt;td&gt;&lt;/td&gt;        &lt;td&gt;&lt;/td&gt;     &lt;td&gt;-1.06**&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;&lt;/th&gt;                    &lt;td&gt;&lt;/td&gt;        &lt;td&gt;&lt;/td&gt;     &lt;td&gt;(0.41)&lt;/td&gt; 
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Asia&lt;/th&gt;                &lt;td&gt;&lt;/td&gt;        &lt;td&gt;&lt;/td&gt;     &lt;td&gt;-0.74*&lt;/td&gt; 
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;&lt;/th&gt;                    &lt;td&gt;&lt;/td&gt;        &lt;td&gt;&lt;/td&gt;     &lt;td&gt;(0.42)&lt;/td&gt; 
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Namer&lt;/th&gt;               &lt;td&gt;&lt;/td&gt;        &lt;td&gt;&lt;/td&gt;      &lt;td&gt;-0.17&lt;/td&gt; 
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;&lt;/th&gt;                    &lt;td&gt;&lt;/td&gt;        &lt;td&gt;&lt;/td&gt;     &lt;td&gt;(0.40)&lt;/td&gt; 
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Samer&lt;/th&gt;               &lt;td&gt;&lt;/td&gt;        &lt;td&gt;&lt;/td&gt;      &lt;td&gt;-0.12&lt;/td&gt; 
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;&lt;/th&gt;                    &lt;td&gt;&lt;/td&gt;        &lt;td&gt;&lt;/td&gt;     &lt;td&gt;(0.42)&lt;/td&gt; 
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;R-squared&lt;/th&gt;         &lt;td&gt;0.54&lt;/td&gt;    &lt;td&gt;0.56&lt;/td&gt;    &lt;td&gt;0.71&lt;/td&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;R-squared Adj.&lt;/th&gt;    &lt;td&gt;0.53&lt;/td&gt;    &lt;td&gt;0.54&lt;/td&gt;    &lt;td&gt;0.67&lt;/td&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;No. observations&lt;/th&gt;   &lt;td&gt;64&lt;/td&gt;      &lt;td&gt;64&lt;/td&gt;      &lt;td&gt;64&lt;/td&gt;   
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;The coefficient of &lt;code&gt;Expropr&lt;/code&gt; decreases in magnitude but remains positive and significant after the addition of geographical control variables. This might suggest that the endogeneity problem is not very pronounced. However, it&amp;rsquo;s hard to say given the large number of factors that could affect both institutions and GDP.&lt;/p&gt;
&lt;h3 id=&#34;iv-1&#34;&gt;IV&lt;/h3&gt;
&lt;p&gt;In order for &lt;code&gt;Mort&lt;/code&gt; to be a valid instrument it needs to satisfy the two IV conditions:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Exclusion&lt;/strong&gt;: &lt;code&gt;Mort&lt;/code&gt; must be correlated to &lt;code&gt;GDP&lt;/code&gt; only through &lt;code&gt;Exprop&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Relevance&lt;/strong&gt;: &lt;code&gt;Mort&lt;/code&gt; must be correlated with &lt;code&gt;Exprop&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The &lt;strong&gt;exclusion restriction&lt;/strong&gt; condition is untestable, however, we may not be satisfied if settler mortality rates in the 17th to 19th centuries have a direct effect on current GDP (in addition to their indirect effect through institutions).&lt;/p&gt;
&lt;p&gt;For example, settler mortality rates may be related to the current disease environment in a country, which could affect current economic performance.&lt;/p&gt;
&lt;p&gt;The authors argue this is unlikely because:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The majority of settler deaths were due to malaria and yellow fever and had a limited effect on local people.&lt;/li&gt;
&lt;li&gt;The disease burden on local people in Africa or India, for example, did not appear to be higher than average, supported by relatively high population densities in these areas before colonization.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The &lt;strong&gt;relevance&lt;/strong&gt; condition is testable and we can check it by computing the partial correlation between &lt;code&gt;Mort&lt;/code&gt; and &lt;code&gt;Exprop&lt;/code&gt;. Let&amp;rsquo;s start by visual inspection first.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sns.scatterplot(data=df, x=&#39;Mort&#39;, y=&#39;Exprop&#39;)\
.set(title=&#39;First Stage&#39;,
    xlabel=&#39;Settler mortality&#39;,
    ylabel=&#39;Risk of expropriation&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/iv_57_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Visually, the first stage seems weak, at best. However, a regression of &lt;code&gt;Exprop&lt;/code&gt; on &lt;code&gt;Mort&lt;/code&gt; can help us better assess whether the relationship is significant or not.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;smf.ols(&#39;Exprop ~ Mort&#39;, df).fit().summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
      &lt;td&gt;&lt;/td&gt;         &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt; &lt;td&gt;    6.7094&lt;/td&gt; &lt;td&gt;    0.202&lt;/td&gt; &lt;td&gt;   33.184&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    6.305&lt;/td&gt; &lt;td&gt;    7.114&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Mort&lt;/th&gt;      &lt;td&gt;   -0.0008&lt;/td&gt; &lt;td&gt;    0.000&lt;/td&gt; &lt;td&gt;   -2.059&lt;/td&gt; &lt;td&gt; 0.044&lt;/td&gt; &lt;td&gt;   -0.002&lt;/td&gt; &lt;td&gt;-2.28e-05&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;The coefficient is negative, as expected, and statistically significant.&lt;/p&gt;
&lt;p&gt;The second-stage regression results give us an unbiased and consistent estimate of the effect of institutions on economic outcomes.&lt;/p&gt;
&lt;p&gt;$$
{GDP}_i = \beta_0 + \beta_1 {Exprop}_i + \varepsilon_i \
{Exprop}_i = \delta_0 + \delta_1 {logMort}_i + v_i
$$&lt;/p&gt;
&lt;p&gt;Note that while our parameter estimates are correct, our standard errors
are not and for this reason, computing 2SLS ‘manually’ (in stages with
OLS) is not recommended.&lt;/p&gt;
&lt;p&gt;We can correctly estimate a 2SLS regression in one step using the
&lt;a href=&#34;https://github.com/bashtage/linearmodels&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;linearmodels&lt;/a&gt; package, an extension of &lt;code&gt;statsmodels&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Note that when using &lt;code&gt;IV2SLS&lt;/code&gt;, the exogenous and instrument variables
are split up in the function arguments (whereas before the instrument
included exogenous variables)&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;IV2SLS.from_formula(&#39;GDP ~ 1 + [Exprop ~ logMort]&#39;, data=df).fit().summary.tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;caption&gt;Parameter Estimates&lt;/caption&gt;
&lt;tr&gt;
      &lt;td&gt;&lt;/td&gt;      &lt;th&gt;Parameter&lt;/th&gt; &lt;th&gt;Std. Err.&lt;/th&gt; &lt;th&gt;T-stat&lt;/th&gt; &lt;th&gt;P-value&lt;/th&gt; &lt;th&gt;Lower CI&lt;/th&gt; &lt;th&gt;Upper CI&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt;  &lt;td&gt;2.0448&lt;/td&gt;    &lt;td&gt;1.1273&lt;/td&gt;   &lt;td&gt;1.8139&lt;/td&gt; &lt;td&gt;0.0697&lt;/td&gt;   &lt;td&gt;-0.1647&lt;/td&gt;  &lt;td&gt;4.2542&lt;/td&gt; 
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Exprop&lt;/th&gt;     &lt;td&gt;0.9235&lt;/td&gt;    &lt;td&gt;0.1691&lt;/td&gt;   &lt;td&gt;5.4599&lt;/td&gt; &lt;td&gt;0.0000&lt;/td&gt;   &lt;td&gt;0.5920&lt;/td&gt;   &lt;td&gt;1.2550&lt;/td&gt; 
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;The result suggests a stronger positive relationship than what the OLS results indicated.&lt;/p&gt;
&lt;h2 id=&#34;business-case&#34;&gt;Business Case&lt;/h2&gt;
&lt;p&gt;We are given the following problem:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;A firm would like to understand whether its newsletter is working to increase revenue. However, it cannot force customers to subscribe to the newsletter. Instead, the firm sends a reminder email to a random sample of customers for the newsletter. Estimate the effect of the newsletter on revenue.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;We start by drawing a sample from the data generating process.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from src.dgp import dgp_newsletter

dgp = dgp_newsletter()
df = dgp.generate_data()
df.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;reminder&lt;/th&gt;
      &lt;th&gt;subscribe&lt;/th&gt;
      &lt;th&gt;revenue&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0.582809&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;3.427162&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1.953731&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;2.902038&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0.826724&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;From the data, we know the &lt;code&gt;revenue&lt;/code&gt; per customer, whether it was sent a &lt;code&gt;reminder&lt;/code&gt; for the newsletter and whether it actually decided to &lt;code&gt;subscribe&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;If we to estimate the effect of &lt;code&gt;subscribe&lt;/code&gt; on &lt;code&gt;revenue&lt;/code&gt;, we might get a biased estimate because the decision of subscribing is endogenous. For example, we can imagine that wealthier customers are generating more revenue but are also less likely to subscribe.&lt;/p&gt;
&lt;p&gt;We can represent the model with a DAG.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;dag_iv(Y=&amp;quot;revenue&amp;quot;, T=&amp;quot;subscribe&amp;quot;, Z=&amp;quot;reminder&amp;quot;, U=&amp;quot;income&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/iv_68_0.svg&#34; alt=&#34;svg&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;ols-2&#34;&gt;OLS&lt;/h3&gt;
&lt;p&gt;By directly inspecting the data, it seems that subscribed members actually generate less revenue than normal customers.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sns.barplot(x=&#39;subscribe&#39;, y=&#39;revenue&#39;, data=df);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/iv_71_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;A linear regression confirms the graphical intuition.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;smf.ols(&#39;revenue ~ 1 + subscribe&#39;, df).fit().summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
      &lt;td&gt;&lt;/td&gt;         &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt; &lt;td&gt;    1.7752&lt;/td&gt; &lt;td&gt;    0.086&lt;/td&gt; &lt;td&gt;   20.697&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    1.607&lt;/td&gt; &lt;td&gt;    1.943&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;subscribe&lt;/th&gt; &lt;td&gt;   -0.7441&lt;/td&gt; &lt;td&gt;    0.140&lt;/td&gt; &lt;td&gt;   -5.334&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;   -1.018&lt;/td&gt; &lt;td&gt;   -0.470&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;However, if indeed wealthier customers generate more revenue and are less likely to subscribe, we have a negative omitted variable bias and we can expect the true effect of the newsletter to be bigger than the OLS estimate.&lt;/p&gt;
&lt;h3 id=&#34;iv-2&#34;&gt;IV&lt;/h3&gt;
&lt;p&gt;Let&amp;rsquo;s now exploit the random variation induced by the discount. In order for our instrument to be valid, we need it to be exogenous (untestable) and relevant. We can test the relevance with the &lt;strong&gt;first stage&lt;/strong&gt; regresssion of &lt;code&gt;reminder&lt;/code&gt; on &lt;code&gt;subscribe&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;smf.ols(&#39;subscribe ~ 1 + reminder&#39;, df).fit().summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
      &lt;td&gt;&lt;/td&gt;         &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt; &lt;td&gt;    0.2368&lt;/td&gt; &lt;td&gt;    0.021&lt;/td&gt; &lt;td&gt;   11.324&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    0.196&lt;/td&gt; &lt;td&gt;    0.278&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;reminder&lt;/th&gt;  &lt;td&gt;    0.2790&lt;/td&gt; &lt;td&gt;    0.029&lt;/td&gt; &lt;td&gt;    9.488&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    0.221&lt;/td&gt; &lt;td&gt;    0.337&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;It seems that the instrument is relevant. We can now estimate the IV regression.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;IV2SLS.from_formula(&#39;revenue ~ 1 + [subscribe ~ reminder]&#39;, data=df).fit().summary.tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;caption&gt;Parameter Estimates&lt;/caption&gt;
&lt;tr&gt;
      &lt;td&gt;&lt;/td&gt;      &lt;th&gt;Parameter&lt;/th&gt; &lt;th&gt;Std. Err.&lt;/th&gt; &lt;th&gt;T-stat&lt;/th&gt; &lt;th&gt;P-value&lt;/th&gt; &lt;th&gt;Lower CI&lt;/th&gt; &lt;th&gt;Upper CI&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt;  &lt;td&gt;0.9485&lt;/td&gt;    &lt;td&gt;0.2147&lt;/td&gt;   &lt;td&gt;4.4184&lt;/td&gt; &lt;td&gt;0.0000&lt;/td&gt;   &lt;td&gt;0.5278&lt;/td&gt;   &lt;td&gt;1.3693&lt;/td&gt; 
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;subscribe&lt;/th&gt;  &lt;td&gt;1.4428&lt;/td&gt;    &lt;td&gt;0.5406&lt;/td&gt;   &lt;td&gt;2.6689&lt;/td&gt; &lt;td&gt;0.0076&lt;/td&gt;   &lt;td&gt;0.3832&lt;/td&gt;   &lt;td&gt;2.5023&lt;/td&gt; 
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;The estimated coefficient has now flipped sign and turned positive! Ignoring the endogeneity problem would have lead us to the wrong conclusion.&lt;/p&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=LEAx0He_KBI&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Instrumental Variables&lt;/a&gt; video lecture by Paul Goldsmith-Pinkham (Yale)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://matheusfacure.github.io/python-causality-handbook/08-Instrumental-Variables.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Instrumental Variables&lt;/a&gt; section from Matheus Facure&amp;rsquo;s &lt;a href=&#34;https://matheusfacure.github.io/python-causality-handbook/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Causal Inference for The Brave and The True&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.jstor.org/stable/2937954&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Does compulsory school attendance affect schooling and earnings?&lt;/a&gt; (1991) by Angrist and Krueger&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://economics.mit.edu/files/4123&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The Colonial Origins of Comparative Development&lt;/a&gt; (2002) by Acemoglu, Johnson, Robinson&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Permutation Tests for Dummies</title>
      <link>https://matteocourthoud.github.io/post/permutation_test/</link>
      <pubDate>Fri, 15 Apr 2022 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/post/permutation_test/</guid>
      <description>&lt;p&gt;If you search &amp;ldquo;permutation test&amp;rdquo; on Wikipedia, you get the following definition:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;A permutation test (also called re-randomization test) is an exact statistical hypothesis test making use of the proof by contradiction in which the distribution of the test statistic under the null hypothesis is obtained by calculating all possible values of the test statistic under possible rearrangements of the observed data.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;What does it mean? In this tutorial we are going to see in detail what this definition means, how to implement permutation tests, and their pitfalls.&lt;/p&gt;
&lt;h2 id=&#34;example-1-is-a-coin-fair&#34;&gt;Example 1: is a coin fair?&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s start with an example: suppose you wanted to test whether a coin is fair. You throw the coin 10 times and you count the number of times you get heads. Let&amp;rsquo;s simulate the outcome.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import numpy as np

np.random.seed(1)
np.random.binomial(1, 0.5, 10)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;array([0, 1, 0, 0, 0, 0, 0, 0, 0, 1])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Out of 10 coin throws, we got only 2 heads. Does it mean that the coin is not fair?&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;question&lt;/strong&gt; that permutation testing is trying to answer is &amp;ldquo;&lt;em&gt;how unlikely is the observed outcome under the null hypothesis that the coin is fair?&lt;/em&gt;&amp;rdquo;.&lt;/p&gt;
&lt;p&gt;In this case we can directly compute this answer since we have a very little number of throws. The total number of outcomes is $2^{10}$. The number of as or more extreme outcomes, under the assumption that the coin is fair (50-50) is&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;0 heads: ${10 \choose 0} = 1$&lt;/li&gt;
&lt;li&gt;1 head: ${10 \choose 1} = 10$&lt;/li&gt;
&lt;li&gt;2 heads: ${10 \choose 2} = 45$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;So that the probability of getting the same or a more extreme outcome is&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from scipy.special import comb

(comb(10, 0) + comb(10, 1) + comb(10, 2)) / 2**10
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;0.0546875
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This probability seems low but not too low.&lt;/p&gt;
&lt;p&gt;However, we have forgot one thing. We want to test whether the coin is fair in &lt;strong&gt;either&lt;/strong&gt; direction. We would suspect that the coin is unfair if we were getting few heads (as we did), but also if we were getting many heads. Therefore, we should account for both extremes.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sum([comb(10, i) for i in [0, 1, 2, 8, 9, 10]]) / 2**10
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;0.109375
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This number should not be surprising since it&amp;rsquo;s exactly double the previous one.&lt;/p&gt;
&lt;p&gt;It is common in statistics to say that an event is unusual if its probability is less than 1 in 20, i.e. $5%$. If we were adopting that threshold, we would not conclude that getting 2 heads in 10 trows is so unusual. However, getting just one, would be.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sum([comb(10, i) for i in [0, 1, 9, 10]]) / 2**10
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;0.021484375
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;hypothesis-testing&#34;&gt;Hypothesis Testing&lt;/h2&gt;
&lt;p&gt;The process we just went through is called &lt;strong&gt;hypothesis testing&lt;/strong&gt;. The components of an hypothesis test are:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A null hypothesis $H_0$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;in our case, that the coin war fair&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;A test statistic $t$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;in our case, the number of zeros&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;A level of significance $\alpha$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;it is common to choose 5%&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The &lt;strong&gt;idea&lt;/strong&gt; behind &lt;strong&gt;permutation testing&lt;/strong&gt; is the following: in a setting in which we are checking whether one variable has an effect on another variable, the two variables should not be correlated, under the null hypothesis . Therefore, we could re-shuffle the treatment variable and re-compute the test statistic. Lastly, we can compute the p-value as the fraction of as or more extremes outcomes under re-shuffling of the data.&lt;/p&gt;
&lt;h2 id=&#34;example-2-are-women-smarter&#34;&gt;Example 2: are women smarter?&lt;/h2&gt;
&lt;p&gt;Suppose now we were interested in knowing whether females perform better in a test than men. Let&amp;rsquo;s start by writing the data generating process under the assumption of no difference in scores. However, only 30% of the sample will be female.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import pandas as pd

# Data generating process
def generate_data_gender(N=100, seed=1):
    np.random.seed(seed) # Set seed for replicability
    data = pd.DataFrame({&amp;quot;female&amp;quot;: np.random.binomial(1, 0.3, N),
                         &amp;quot;test_score&amp;quot;: np.random.exponential(3, N)})
    return data
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s now generate a sample of size 100.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Generate data
data_gender = generate_data_gender()
data_gender.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;female&lt;/th&gt;
      &lt;th&gt;test_score&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1.186447&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;2.246348&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;6.513147&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1.326091&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;7.175402&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;We can compute the treatment effect by computing the difference in mean outcomes between male and females.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def compute_score_diff(data):
    T = np.mean(data.loc[data[&#39;female&#39;]==1, &#39;test_score&#39;]) - np.mean(data.loc[data[&#39;female&#39;]==0, &#39;test_score&#39;])
    return T
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;T = compute_score_diff(data_gender)
print(f&amp;quot;The estimated treatment effect is {T}&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;The estimated treatment effect is -1.3612262580563321
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It looks that females actually did worse than males. But is the difference statistically significant? We can perform a randomization test and compute the probability of observing a more extreme outcome.&lt;/p&gt;
&lt;p&gt;First, let&amp;rsquo;s write the permutation routine that takes a variable in the data and permutes it.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def permute(data, var, r):
    temp_data = data.copy()
    temp_data[var] = np.random.choice(data[var], size=len(data), replace=r)
    return temp_data
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can now write the permutation test. It spits out a vector of statistics and prints the implied p-value.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def permutation_test(data, permute, var, compute_stat, K=1000, r=False):
    T = compute_stat(data)
    T_perm = []
    for k in range(K):
        temp_data = permute(data, var, r)
        T_perm += [compute_stat(temp_data)]
    print(f&amp;quot;The p-value is {sum(np.abs(T_perm) &amp;gt;= np.abs(T))/K}&amp;quot;)
    return T_perm
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;Ts = permutation_test(data_gender, permute, &#39;test_score&#39;, compute_score_diff)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;The p-value is 0.063
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Apparently the result we have observed was quite unusual, but not at the 5% level. We can plot the distribution of statistics to visualize this result.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;%matplotlib inline
%config InlineBackend.figure_format = &#39;retina&#39;
from src.utils import *
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def plot_test(T, Ts, title):
    plt.hist(Ts, density=True, bins=30, alpha=0.7, color=&#39;C0&#39;)
    plt.vlines([-T, T], ymin=plt.ylim()[0], ymax=plt.ylim()[1], color=&#39;C2&#39;)
    plt.title(title);
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plot_test(T, Ts, &#39;Distribution of score differences under permutation&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/permutation_test_29_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;As we can see, the observed difference in scores is quite extreme with respect the distribution generate by the permutation.&lt;/p&gt;
&lt;p&gt;One &lt;strong&gt;issue&lt;/strong&gt; with the permutation test we just ran is that it is computationally expensive to draw without replacement. The standard and much faster procedure is to draw without replacement.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;Ts_repl = permutation_test(data_gender, permute, &#39;test_score&#39;, compute_score_diff, r=True)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;The p-value is 0.052
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The p-value is virtually the same.&lt;/p&gt;
&lt;p&gt;How &lt;strong&gt;accurate&lt;/strong&gt; is the test? Since we have access to the data generating process, we can compute the true p-value via simulation. We draw many samples from the true data generating process and, for each, compute the difference in scores. The simulated p-value is going to be the frequency of more extreme statistics.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Function to simulate data and compute pvalue
def simulate_stat(dgp, compute_stat, K=1000):
    T = compute_stat(dgp())
    T_sim = []
    for k in range(K):
        data = dgp(seed=k)
        T_sim += [compute_stat(data)]
    print(f&amp;quot;The p-value is {sum(np.abs(T_sim) &amp;gt;= np.abs(T))/K}&amp;quot;)
    return np.array(T_sim)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;T_sim = simulate_stat(generate_data_gender, compute_score_diff)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;The p-value is 0.038
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Again, we can plot the distribution of simulated statistics to understand the computed p-value.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plot_test(T, T_sim, &#39;Distribution of score differences under simulation&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/permutation_test_36_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;As expected, most of the mass lies within the interval, indicating a relatively extreme result. We have just been &amp;ldquo;unlucky&amp;rdquo; with the draw, but the permutation test was accurate.&lt;/p&gt;
&lt;h2 id=&#34;permutation-tests-vs-t-tests&#34;&gt;Permutation tests vs t-tests&lt;/h2&gt;
&lt;p&gt;What is the difference between a t-test and a permutation test?&lt;/p&gt;
&lt;p&gt;Permutation test &lt;strong&gt;advantages&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;does not make distributional assumptions&lt;/li&gt;
&lt;li&gt;not sensible to outliers&lt;/li&gt;
&lt;li&gt;can be computed also for statistics whose distribution is not known&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Permutation test &lt;strong&gt;disadvantages&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;computationally intense&lt;/li&gt;
&lt;li&gt;very sample-dependent&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;example-3-is-university-worth&#34;&gt;Example 3: is university worth?&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s now switch to a new example to compare t-tests and permutation tests.&lt;/p&gt;
&lt;p&gt;Assume we want to check whether university is a worthy investment. We have information about whether individuals attended university and their future salary. The problem here is that income is a particularly &lt;strong&gt;skewed&lt;/strong&gt; variable.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Data generating process
def generate_data_income(N=1000, seed=1):
    np.random.seed(seed) # Set seed for replicability
    university = np.random.binomial(1, 0.5, N) # Treatment
    data = pd.DataFrame({&amp;quot;university&amp;quot;: university,
                         &amp;quot;income&amp;quot;: np.random.lognormal(university, 2.3, N)})
    return data
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;data_income = generate_data_income()
data_income.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;university&lt;/th&gt;
      &lt;th&gt;income&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;5.305618&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1.289598&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;6.507720&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;6.019961&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0.034482&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;The distribution of income is very heavy tailed. Let&amp;rsquo;s plot its density across the two groups.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sns.kdeplot(data=data_income, x=&amp;quot;income&amp;quot;, hue=&amp;quot;university&amp;quot;)\
.set(title=&#39;Income density by group&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/permutation_test_45_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;The distribution is so skewed that we cannot actually visually perceive differences between the two groups. Let&amp;rsquo;s compute the expected difference.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def compute_income_diff(data):
    T = np.mean(data.loc[data[&#39;university&#39;]==1, &#39;income&#39;]) - np.mean(data.loc[data[&#39;university&#39;]==0, &#39;income&#39;])
    return T
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;T = compute_income_diff(data_income)
T
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;23.546974435985444
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It looks like university graduates have higher income. Is this difference statistically different from zero? Let&amp;rsquo;s perform a permutation test.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;T_perm = permutation_test(data_income, permute, &#39;university&#39;, compute_income_diff)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;The p-value is 0.011
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The permutation test is telling us that the difference is extremely unusual under the null hypothesis. In other words, it is very unlikely that university graduates earn the same income of non-university graduates.&lt;/p&gt;
&lt;p&gt;What would be the outcome of a standard t-test?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from scipy.stats import ttest_ind

ttest_ind(data_income.query(&#39;university==1&#39;)[&#39;income&#39;], data_income.query(&#39;university==0&#39;)[&#39;income&#39;])
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Ttest_indResult(statistic=1.5589492598056494, pvalue=0.1193254252009701)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As we can see, the two tests provide extremely different results. The t-test is much more conservative, telling us that the unlikeliness of the data is just $12%$ compared to the $1.1%$ of the permutation test.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;reason&lt;/strong&gt; is that we have extremely skewed data. The t-test is very sensible to extreme observation and will therefore compute a very high variance because of very few data points.&lt;/p&gt;
&lt;p&gt;The permutation test can further address the problem of a skewed outcome distribution by using a test statistic that is more &lt;strong&gt;sensible to outliers&lt;/strong&gt;. Let&amp;rsquo;s perform the permutation test using the &lt;strong&gt;trimmed mean&lt;/strong&gt; instead of the mean.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from scipy.stats import trim_mean

def compute_income_mediandiff(data):
    T = np.median(data.loc[data[&#39;university&#39;]==1, &#39;income&#39;]) - np.median(data.loc[data[&#39;university&#39;]==0, &#39;income&#39;])
    return T
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;T_perm = permutation_test(data_income, permute, &#39;university&#39;, compute_income_mediandiff)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;The p-value is 0.0
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In this case, the permutation test is extremely confident that the trimmed mean of the two groups is different.&lt;/p&gt;
&lt;p&gt;However, an advantage of the t-test is &lt;strong&gt;speed&lt;/strong&gt;. Let&amp;rsquo;s compare the two tests by computing their execution time. Note that this is just a rough approximation since the permutation test could be sensible optimized.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import time

# No replacement
start = time.time()
permutation_test(data_income, permute, &#39;university&#39;, compute_income_diff)
print(f&amp;quot;Elapsed time without replacement: {time.time() - start}&amp;quot;)

# Replacement
start = time.time()
ttest_ind(data_income.query(&#39;university==1&#39;)[&#39;income&#39;], data_income.query(&#39;university==0&#39;)[&#39;income&#39;])
print(f&amp;quot;Elapsed time with replacement: {time.time() - start}&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;The p-value is 0.016
Elapsed time without replacement: 0.28911614418029785
Elapsed time with replacement: 0.00125885009765625
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The permutation test is 300 times slower. This can be a particularly relevant difference for larger sample sizes.&lt;/p&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;In this tutorial, we have seen how to perform permutation tests across different data generating processes.&lt;/p&gt;
&lt;p&gt;The underlying principle is the same: permute an variable that is assumed to be random under the null hypothesis and re-compute the test statistic. Then check how unusual was the test statistic in the original dataset.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Policy Learning</title>
      <link>https://matteocourthoud.github.io/post/policy_learning/</link>
      <pubDate>Fri, 15 Apr 2022 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/post/policy_learning/</guid>
      <description>&lt;p&gt;In this tutorial, we are going to see how to design the most welfare-improving policy in presence of treatment effect heterogeneity and treatment costs or budget constraints.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Requisites&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;For this tutorial, I assume you are familiar with the following concepts:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Rubin&amp;rsquo;s potential outcome framework&lt;/li&gt;
&lt;li&gt;Propensity weighting or uplifting&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://matteocourthoud.github.io/post/aipw/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;AIPW or Double Robust Estimators&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://matteocourthoud.github.io/post/causal_trees/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Causal Trees&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Academic Replication&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;We are going to replicate the paper by &lt;a href=&#34;https://www.aeaweb.org/articles?id=10.1257/jep.32.4.201&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Hanna and Olken (2018)&lt;/a&gt; in which the authors study the optimal allocation of a cash transfer to households in Peru. We slightly twist the original paper by actually assigning the transfer and assuming 100% consumption.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Business Case&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;We are going to study a company that has to decide which consumers to target with ads.&lt;/p&gt;
&lt;h2 id=&#34;setting&#34;&gt;Setting&lt;/h2&gt;
&lt;p&gt;We assume that for a set of i.i.d. subjects $i = 1, &amp;hellip;, n$ we observed a tuple $(X_i, T_i, Y_i)$ comprised of&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;a feature vector $X_i \in \mathbb R^n$&lt;/li&gt;
&lt;li&gt;a treatment variable $T_i \in \lbrace 0, 1 \rbrace$&lt;/li&gt;
&lt;li&gt;a response $Y_i \in \mathbb R$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Assumption 1 : unconfoundedness&lt;/strong&gt; (or ignorability, or selection on observables)&lt;/p&gt;
&lt;p&gt;$$
\big \lbrace Y_i^{(1)} , Y_i^{(0)} \big \rbrace \ \perp \ T_i \ | \ X_i
$$&lt;/p&gt;
&lt;p&gt;i.e. conditional on observable characteristics $X$, the treatment assignment $T$ is as good as random.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Assumption 2: overlap&lt;/strong&gt; (or bounded support)&lt;/p&gt;
&lt;p&gt;$$
\exists \eta &amp;gt; 0 \ : \ \eta \leq \mathbb E \left[ T_i = 1 \ \big | \ X_i = x \right] \leq 1-\eta
$$&lt;/p&gt;
&lt;p&gt;i.e. no observation is deterministically assigned to the treatment or control group.&lt;/p&gt;
&lt;h2 id=&#34;policy-learning&#34;&gt;Policy Learning&lt;/h2&gt;
&lt;p&gt;The objective of policy learning is to decide which people to treat. More explicitly, we want to learn a map from observable characteristics to a (usually binary) policy space.&lt;/p&gt;
&lt;p&gt;$$
\pi : \mathcal X \to \lbrace 0, 1 \rbrace
$$&lt;/p&gt;
&lt;p&gt;Policy learning is closely related to the &lt;strong&gt;estimation of heterogeneous treatment effects&lt;/strong&gt;. In fact, in both settings, we want to investigate how the treatment affects different individuals in different ways.&lt;/p&gt;
&lt;p&gt;The main &lt;strong&gt;difference&lt;/strong&gt; between policy learning and the estimation of heterogeneous treatment effects is the objective function. In policy learning, we are acting in a limited resources setting where providing treatment is costly and the cost could depend on individual characteristics. For example, it might be more costly to vaccinate individuals that live in remote areas. Therefore, one might not just want to treat individuals with the largest expected treatment effect, but the ones for whom treatment is most cost-effective.&lt;/p&gt;
&lt;p&gt;The utilitarian &lt;strong&gt;value&lt;/strong&gt; of a policy $\pi$&lt;/p&gt;
&lt;p&gt;$$
V(\pi) = \mathbb E \Big[ Y_i(\pi(X_i)) \Big] = \mathbb E \big[ Y^{(0)}_i \big] + \mathbb E \big[ \tau(X_i) \pi(X_i) \big]
$$&lt;/p&gt;
&lt;p&gt;measures the expectation of the potential outcome $Y$ if we were to &lt;strong&gt;assign&lt;/strong&gt; treatment $T$ according to policy $\pi$. This expectation can be split into &lt;strong&gt;two parts&lt;/strong&gt;:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;The baseline expected potential outcome $\mathbb E \big[ Y^{(0)}_i \big]$&lt;/li&gt;
&lt;li&gt;The expected effect of the policy $\mathbb E \big[ \tau(X_i) \pi(X_i) \big]$&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The &lt;strong&gt;objective&lt;/strong&gt; of policy learning is to learn a policy with high value $V(\pi)$. As part (2) of the formula makes clear, you get a higher value if you treat the people with a high treatment effect $\tau(x)$.&lt;/p&gt;
&lt;p&gt;A &lt;strong&gt;simple approach&lt;/strong&gt; could be to assign treatment according to a &lt;strong&gt;thresholding rule&lt;/strong&gt; $\tau(x) &amp;gt; c$, where $c$ is some cost below which is not worth treating individuals (or there is not enough budget).&lt;/p&gt;
&lt;p&gt;However, estimating the conditional average treatment effect (CATE) function $\tau(x)$ and learning a good policy $\pi(x)$ are different &lt;strong&gt;problems&lt;/strong&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the correct loss function for policy learning is not the mean squared error (MSE) on $\tau(x)$
&lt;ul&gt;
&lt;li&gt;we want to &lt;strong&gt;maximize welfare&lt;/strong&gt;!&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;the CATE function $\tau(x)$ might not use some features for targeting
&lt;ul&gt;
&lt;li&gt;e.g. &lt;strong&gt;cannot discriminate&lt;/strong&gt; based on race or gender&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;you don&amp;rsquo;t want to have feature that people can influence
&lt;ul&gt;
&lt;li&gt;e.g. use a self-reported measure that people can distort&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We would like to find a &lt;strong&gt;loss function&lt;/strong&gt; $L(\pi ; Y_i, X_i, T_i)$ such that&lt;/p&gt;
&lt;p&gt;$$
\mathbb E \big[ L(\pi ; Y_i, X_i, T_i) \big] = - V(\pi)
$$&lt;/p&gt;
&lt;h3 id=&#34;ipw-loss&#34;&gt;IPW Loss&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://onlinelibrary.wiley.com/doi/abs/10.3982/ECTA13288&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Kitagawa and Tenenov (2018)&lt;/a&gt; propose to learn an empirical estimate of the value function using inverse propensity weighting (IPW).&lt;/p&gt;
&lt;p&gt;$$
\hat \pi = \arg \max_{\pi} \Big\lbrace \hat V(\pi) : \pi \in \Pi \Big\rbrace
$$&lt;/p&gt;
&lt;p&gt;where&lt;/p&gt;
&lt;p&gt;$$
\hat V(\pi) = \frac{ \mathbb I \big(\lbrace T_i = \pi(X_i) \rbrace \big) }{ \mathbb P \big[ \lbrace T_i = \pi(X_i) \rbrace \ \big| \ X_i \big] } Y_i
$$&lt;/p&gt;
&lt;p&gt;The authors show that under &lt;strong&gt;unconfoundedness&lt;/strong&gt;, if the propensity score $e(x)$ is known and $\Pi$ is not too complex, the value of the estimated policy converges to the optimal value.&lt;/p&gt;
&lt;p&gt;Note that this is a &lt;strong&gt;very different problem&lt;/strong&gt; from the normal optimization problem with a MSE loss. In fact, we now have a binary argument in the loss function which makes the problem similar to a classification problem, in which we want to classify people into &lt;em&gt;high gain&lt;/em&gt; and &lt;em&gt;low gain&lt;/em&gt; categories.&lt;/p&gt;
&lt;h3 id=&#34;aipw-loss&#34;&gt;AIPW Loss&lt;/h3&gt;
&lt;p&gt;If propensity score $e(x)$ is not known, we can use a &lt;strong&gt;doubly robust estimator&lt;/strong&gt;, exactly as for the average treatment effect.&lt;/p&gt;
&lt;p&gt;$$
\hat V = \frac{1}{n} \sum_{i=1}^{n}
\begin{cases}
\hat \Gamma_i \quad &amp;amp;\text{if} \quad \pi(X_i) = 1
\newline&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;\hat \Gamma_i \quad &amp;amp;\text{if} \quad \pi(X_i) = 0
\end{cases}
$$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;where&lt;/p&gt;
&lt;p&gt;$$
\hat \Gamma_i = \hat \mu^{(1)}(X_i) - \hat \mu^{(0)}(X_i) + \frac{T_i }{\hat e(X_i)} \left( Y_i - \hat \mu^{(1)}(X_i) \right) - \frac{(1-T_i) }{1-\hat e(X_i)} \left( Y_i - \hat \mu^{(0)}(X_i) \right)
$$&lt;/p&gt;
&lt;p&gt;The relationship with AIPW is that $\hat \tau_{AIPW} = \frac{1}{n} \sum_{i=1}^{n} \hat \Gamma_i$. Therefore, the objective function $V(\pi)$ is build so that when we assign treatment to a unit we &amp;ldquo;gain&amp;rdquo; the double-robust score $\hat \tau_{AIPW}$, while, if we do not assign treatment, we &amp;ldquo;pay&amp;rdquo; the double-robust score $\hat \tau_{AIPW}$.&lt;/p&gt;
&lt;h2 id=&#34;academic-application&#34;&gt;Academic Application&lt;/h2&gt;
&lt;p&gt;For the academic applicaiton, we are going to replicate the paper by &lt;a href=&#34;https://www.aeaweb.org/articles?id=10.1257/jep.32.4.201&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Hanna and Olken (2018)&lt;/a&gt; in which the authors study the optimal allocation of a cash transfer to households in Peru. We slightly twist the original paper by actually assigning the transfer and assuming 100% consumption.&lt;/p&gt;
&lt;p&gt;First, let&amp;rsquo;s load the modified dataset.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from src.utils import *
from src.dgp import dgp_ao18
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;%matplotlib inline
%config InlineBackend.figure_format = &#39;retina&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;dgp = dgp_ao18()
df = dgp.import_data()
df.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;Unnamed: 0&lt;/th&gt;
      &lt;th&gt;d_fuel_other&lt;/th&gt;
      &lt;th&gt;d_fuel_wood&lt;/th&gt;
      &lt;th&gt;d_fuel_coal&lt;/th&gt;
      &lt;th&gt;d_fuel_kerosene&lt;/th&gt;
      &lt;th&gt;d_fuel_gas&lt;/th&gt;
      &lt;th&gt;d_fuel_electric&lt;/th&gt;
      &lt;th&gt;d_fuel_none&lt;/th&gt;
      &lt;th&gt;d_water_other&lt;/th&gt;
      &lt;th&gt;d_water_river&lt;/th&gt;
      &lt;th&gt;...&lt;/th&gt;
      &lt;th&gt;d_lux_1&lt;/th&gt;
      &lt;th&gt;d_lux_2&lt;/th&gt;
      &lt;th&gt;d_lux_3&lt;/th&gt;
      &lt;th&gt;d_lux_4&lt;/th&gt;
      &lt;th&gt;d_lux_5&lt;/th&gt;
      &lt;th&gt;training&lt;/th&gt;
      &lt;th&gt;h_hhsize&lt;/th&gt;
      &lt;th&gt;cash_transfer&lt;/th&gt;
      &lt;th&gt;consumption&lt;/th&gt;
      &lt;th&gt;welfare&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;211.0000&lt;/td&gt;
      &lt;td&gt;5.351858&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;420.1389&lt;/td&gt;
      &lt;td&gt;6.040585&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;390.8318&lt;/td&gt;
      &lt;td&gt;5.968277&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;9&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;285.6018&lt;/td&gt;
      &lt;td&gt;5.654599&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;8&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;118.0713&lt;/td&gt;
      &lt;td&gt;4.771289&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;5 rows × 78 columns&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;As we can see, we have a lot of information about individuals in Peru. Crucially for the research question, we observe&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;whether the household received a cash transfer, &lt;code&gt;cash_transfer&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;the household&amp;rsquo;s welfare afterwards, &lt;code&gt;welfare_post&lt;/code&gt;
&lt;ul&gt;
&lt;li&gt;assuming
$$
\text{welfare} = \log (\text{consumption})
$$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We would like to understand which individuals should be given a transfer, given that the transfer is costly. Let&amp;rsquo;s assume the transfer costs $0.3$ units of welfare.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from econml.policy import DRPolicyForest

cost = 0.3
policy = DRPolicyForest(random_state=1).fit(Y=df[dgp.Y] - cost*df[dgp.T], T=df[dgp.T], X=df[dgp.X])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can partially visualize the policy by plotting a regression tree for the most important features.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;%matplotlib inline
policy.plot(tree_id=1, max_depth=2, feature_names=dgp.X, fontsize=8)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/policy_learning_18_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;To understand if the estimated policy was effective, we can load the oracle dataset, with the potential outcomes.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_oracle = dgp.import_data(oracle=True)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;From the oracle dataset, we can compute the actual value of the policy.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;T_hat = policy.predict(df[dgp.X])
V_policy = (df_oracle[&#39;welfare_1&#39;].values - cost - df_oracle[&#39;welfare_0&#39;].values) * T_hat
print(f&#39;Estimated policy value (N_T={sum(T_hat)}): {np.mean(V_policy) :.4}&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Estimated policy value (N_T=21401): 0.05897
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The value is positive, indicating that the treatment was effective. But how well did we do? We can compare the estimated policy with the oracle policy that assign treatment to each cost-effective unit.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;T_oracle = (df_oracle[&#39;welfare_1&#39;] - df_oracle[&#39;welfare_0&#39;]) &amp;gt; cost
V_oracle = (df_oracle[&#39;welfare_1&#39;] - cost - df_oracle[&#39;welfare_0&#39;] ) * T_oracle
print(f&#39;Oracle policy value (N_T={sum(T_oracle)}): {np.mean(V_oracle) :.4}&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Oracle policy value (N_T=17630): 0.07494
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We actually achieved 79% of the potential policy gains! Also note that our policy is too generous, treating more units than optimal. But how well would we have done if the same amount of cash transfers were given at random?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;T_rand = np.random.binomial(1, sum(T_hat)/len(df), len(df))
V_rand = (df_oracle[&#39;welfare_1&#39;] - cost - df_oracle[&#39;welfare_0&#39;] ) * T_rand
print(f&#39;Random policy value (N_T={sum(T_rand)}): {np.mean(V_rand) :.4}&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Random policy value (N_T=21359): 0.0002698
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;A random assignment of the same amount of cash transfers would not achieve any effect. However, this assumes that we already know the optimal amount of funds to distribute. What if instead we had treated everyone?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;V_all = (df_oracle[&#39;welfare_1&#39;] - cost - df_oracle[&#39;welfare_0&#39;] )
print(f&#39;All-treated policy value (N_T={len(df)}): {np.mean(V_all) :.4}&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;All-treated policy value (N_T=45378): 0.0004019
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Indiscriminate treatment would again not achieve any effect. Lastly, what if we had just estimated the treatment effect using AIPW and used it as a threshold?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from econml.dr import LinearDRLearner

model = LinearDRLearner(random_state=1).fit(Y=df[dgp.Y], T=df[dgp.T], X=df[dgp.X])
T_ipw = model.effect(X=df[dgp.X], T0=0, T1=1) &amp;gt; cost
V_ipw = (df_oracle[&#39;welfare_1&#39;] - cost - df_oracle[&#39;welfare_0&#39;] ) * T_ipw
print(f&#39;IPW policy value (N_T={sum(T_ipw)}): {np.mean(V_ipw) :.4}&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;IPW policy value (N_T=21003): 0.06293
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We are actually doing better! Weird&amp;hellip;&lt;/p&gt;
&lt;h2 id=&#34;business-case&#34;&gt;Business Case&lt;/h2&gt;
&lt;p&gt;We are given the following problem:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;A firm would like to understand which customers to show an ad, in order to increase revenue. The firm ran a A/B test showing a random sample of customers an ad. First, try to understand if there is heterogeneity in treatment. Then, decide which customers to show the ad, given that ads are costly (1$ each). Further suppose that you cannot discriminate on gender. How do the results change?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;We start by drawing a sample from the data generating process.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from src.utils import *
from src.dgp import dgp_ad
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;dgp = dgp_ad()
df = dgp.generate_data()
df.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;male&lt;/th&gt;
      &lt;th&gt;black&lt;/th&gt;
      &lt;th&gt;age&lt;/th&gt;
      &lt;th&gt;educ&lt;/th&gt;
      &lt;th&gt;ad&lt;/th&gt;
      &lt;th&gt;revenue&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;55.0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;-0.327221&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;47.0&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;0.659393&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;31.0&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;True&lt;/td&gt;
      &lt;td&gt;2.805178&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;51.0&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;-0.508548&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;48.0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;True&lt;/td&gt;
      &lt;td&gt;0.762280&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;We have information on the number of pages visited in the previous month, whether the user is located in the US, whether it connects by mobile and the revenue pre-intervention.&lt;/p&gt;
&lt;p&gt;We are going to use the &lt;a href=&#34;https://econml.azurewebsites.net&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;econml&lt;/code&gt;&lt;/a&gt; library to estimate the treatment effects. First, we use the &lt;code&gt;DRLearner&lt;/code&gt; library to estimate heterogeneous treatment effects using a double robust estimator. We can specify both the &lt;code&gt;model_propensity&lt;/code&gt; for $e(x)$ and the &lt;code&gt;model_regression&lt;/code&gt; for $\mu(x)$.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from econml.dr import DRLearner

model = DRLearner(random_state=1).fit(Y=df[dgp.Y], T=df[dgp.T], X=df[dgp.X]);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can plot a visual representation of the treatment effect heterogeneity using the &lt;code&gt;SingleTreePolicyInterpreter&lt;/code&gt; function, which infers a tree representation of the treatment effects learned from another model.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from econml.cate_interpreter import SingleTreeCateInterpreter

SingleTreeCateInterpreter(max_depth=2, random_state=1).interpret(model, X=df[dgp.X]).plot(feature_names=dgp.X)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/policy_learning_39_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;It seems that the most relevant dimension of treatment heterogeneity is &lt;code&gt;education&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;We can now use policy learning to estimate a treatment policy. We use the &lt;code&gt;DRPolicyTree&lt;/code&gt; from the &lt;code&gt;econml&lt;/code&gt; package.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from econml.policy import DRPolicyTree

policy = DRPolicyTree(random_state=1, max_depth=2).fit(Y=df[dgp.Y], T=df[dgp.T], X=df[dgp.X])
policy.plot(feature_names=dgp.X)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/policy_learning_41_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;We will now assume that the treatment is costly.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;cost = 1
policy = DRPolicyTree(random_state=1, max_depth=2).fit(Y=df[dgp.Y]-cost*df[dgp.T], T=df[dgp.T], X=df[dgp.X])
policy.plot(feature_names=dgp.X)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/policy_learning_43_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;As we can see, the model decides to use race to discriminate treatment. However, let&amp;rsquo;s now suppose we cannot discriminate on race and gender.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;X_short = [&#39;age&#39;, &#39;educ&#39;]
policy = DRPolicyTree(random_state=1, max_depth=2).fit(Y=df[dgp.Y]-cost*df[dgp.T], T=df[dgp.T], X=df[X_short])
policy.plot(feature_names=X_short)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/policy_learning_45_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;In this case, the model uses education instead of race in order to assign treatment.&lt;/p&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://onlinelibrary.wiley.com/doi/abs/10.3982/ECTA13288&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Who Should Be Treated? Empirical Welfare Maximization Methods for Treatment Choice&lt;/a&gt; (2018) by Kitagawa and Tetenov&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://ideas.repec.org/p/ecl/stabus/3506.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Efficient Policy Learning&lt;/a&gt; (2017) by Athey and Wager&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=YQXRwvFQOPk&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Policy Learning&lt;/a&gt; video lecture by Stefan Wager (Stanford)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/microsoft/EconML/blob/main/notebooks/CustomerScenarios/Case%20Study%20-%20Customer%20Segmentation%20at%20An%20Online%20Media%20Company.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Customer Segmentation&lt;/a&gt; case study by EconML&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Propensity Score Matching</title>
      <link>https://matteocourthoud.github.io/post/propensity_score/</link>
      <pubDate>Fri, 15 Apr 2022 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/post/propensity_score/</guid>
      <description>&lt;p&gt;In this tutorial, we are going to see how to estimate causal effects when the treatment is not &lt;em&gt;unconditionally&lt;/em&gt; randomly assigned, but we need to condition on observable features in order to assume treatment exogeneity. This might happen either when an experiment is stratified or in observational studies.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Requisites&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;For this tutorial, I assume you are familiar with the following concepts:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Rubin&amp;rsquo;s potential outcome framework&lt;/li&gt;
&lt;li&gt;Ordinary least squares regression&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Academic Application&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;As an academic application, we are going to replicate &lt;a href=&#34;https://www.jstor.org/stable/2937954&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Evaluating the Econometric Evaluations of Training Programs with Experimental Data&lt;/a&gt; (1986) by Lalonde and the followup paper &lt;a href=&#34;https://www.tandfonline.com/doi/abs/10.1080/01621459.1999.10473858&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Causal Effects in Non-Experimental Studies: Reevaluating the Evaluation of Training Programs&lt;/a&gt; (1999) by Dahejia and Wahba. These papers study a randomized intervention providing work experienced to improve labor market outcomes.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Business Case&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;TBD&lt;/p&gt;
&lt;h2 id=&#34;setting&#34;&gt;Setting&lt;/h2&gt;
&lt;p&gt;We assume that for a set of i.i.d. subjects $i = 1, &amp;hellip;, n$ we observed a tuple $(X_i, D_i, Y_i)$ comprised of&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;a feature vector $X_i \in \mathbb R^n$&lt;/li&gt;
&lt;li&gt;a treatment assignment $D_i \in \lbrace 0, 1 \rbrace$&lt;/li&gt;
&lt;li&gt;a response $Y_i \in \mathbb R$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Assumption 1 : unconfoundedness&lt;/strong&gt; (or ignorability, or selection on observables, or conditional independence)&lt;/p&gt;
&lt;p&gt;$$
\big \lbrace Y_i^{(1)} , Y_i^{(0)} \big \rbrace \ \perp \ D_i \ | \ X_i
$$&lt;/p&gt;
&lt;p&gt;i.e. conditional on observable characteristics $X$, the treatment assignment $T$ is as good as random. What this assumption rules out is &lt;em&gt;selection on unobservables&lt;/em&gt;. Moreover, it&amp;rsquo;s &lt;em&gt;untestable&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Assumption 2: overlap&lt;/strong&gt; (or common support)&lt;/p&gt;
&lt;p&gt;$$
\exists \eta &amp;gt; 0 \ : \ \eta \leq \mathbb E \left[ D_i = 1 \ \big | \ X_i = x \right] \leq 1-\eta
$$&lt;/p&gt;
&lt;p&gt;i.e. no observation is deterministically assigned to the treatment or control group. We need this assumption for counterfactual statements to make sense. If some observations had zero probability of (not) being treated, it would make no sense to try to estimate their counterfactual outcome in case they would have (not) being treated. Also this assumption is &lt;em&gt;untestable&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Assumption 3: stable unit treatment value (SUTVA)&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;$$
Y_i^{(D_i)} \perp D_j \quad \forall j \neq i
$$&lt;/p&gt;
&lt;p&gt;i.e. the potential outcome of one individual is independent from the treatment status of any other individual. Common &lt;em&gt;violations&lt;/em&gt; of this assumption include&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;general equilibrium effects&lt;/li&gt;
&lt;li&gt;spillover effects&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This assumption is &lt;em&gt;untestable&lt;/em&gt;.&lt;/p&gt;
&lt;h2 id=&#34;propensity-scores&#34;&gt;Propensity Scores&lt;/h2&gt;
&lt;h3 id=&#34;exogenous-treatment&#34;&gt;Exogenous Treatment&lt;/h3&gt;
&lt;p&gt;The &lt;strong&gt;fundamental problem of causal inference&lt;/strong&gt; is that we do not observe counterfactual outcomes, i.e. we do not observe what would have happened to treated units if they had not received the treatment and viceversa.&lt;/p&gt;
&lt;p&gt;If treatment is exogenous, we know that the difference in means identifies the average treatment effect $\mathbb E[\tau]$.&lt;/p&gt;
&lt;p&gt;$$
\mathbb E[\tau] = \mathbb E \big[ Y_i \ \big| \ D_i = 1 \big] - \mathbb E \big[ Y_i \ \big| \ D_i = 0 \big] = \mathbb E \big[ Y_{i}^{(1)} - Y_{i}^{(0)} \big]
$$&lt;/p&gt;
&lt;p&gt;Therefore, we can build an unbiased estimator of the average treatment effect as the empirical counterpart of the expression above&lt;/p&gt;
&lt;p&gt;$$
\hat \tau(Y, D) = \frac{1}{n} \sum_{i=1}^{n} \big( D_i Y_i - (1-D_i) Y_i \big)
$$&lt;/p&gt;
&lt;p&gt;In case treatment is not randomly assigned, we use the Thompson Horowitz (1952) estimator&lt;/p&gt;
&lt;p&gt;$$
\hat \tau(Y, D) = \frac{1}{n} \sum_{i=1}^{n} \left( \frac{D_i Y_i}{\pi_{i}} - \frac{(1-D_i) Y_i}{1 - \pi_{i}} \right)
$$&lt;/p&gt;
&lt;p&gt;where $\pi_{i} = \Pr(D_i=1)$ is the probability of being treated, also known as &lt;strong&gt;propensity score&lt;/strong&gt;. Sometimes the propensity score is known, for example when treatment is stratified. However, in general, it is not.&lt;/p&gt;
&lt;h3 id=&#34;conditionally-exogenous-treatment&#34;&gt;Conditionally Exogenous Treatment&lt;/h3&gt;
&lt;p&gt;In many cases and especially in observational studies, treatment $D$ is not unconditionally exogenous, but it&amp;rsquo;s exogenous only after we condition on some characteristic $X$. If these characteristics are observables, we have the &lt;strong&gt;unconfoundedness&lt;/strong&gt; assumption.&lt;/p&gt;
&lt;p&gt;Under unconfoundedness, we can still identify the average treatment effect, as a &lt;em&gt;conditional&lt;/em&gt; difference in means:&lt;/p&gt;
&lt;p&gt;$$
\mathbb E[\tau] = \mathbb E \big[ Y_{i}^{(1)} - Y_{i}^{(0)} \ \big| \ X_i \big]
$$&lt;/p&gt;
&lt;p&gt;The main problem is that we need to condition of the observables that actually make the unconfoundedness assumption hold. This might be tricky in two cases:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;when we have many observables&lt;/li&gt;
&lt;li&gt;when we do not know the functional form of the observables that we need to condition on&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The main contribution of Rosenbaum and Rubin (1983) is to show that if &lt;strong&gt;unconfoundedness&lt;/strong&gt; holds, then&lt;/p&gt;
&lt;p&gt;$$
\big \lbrace Y_i^{(1)} , Y_i^{(0)} \big \rbrace \ \perp \ D_i \ | \ \pi(X_i)
$$&lt;/p&gt;
&lt;p&gt;i.e. you only need to condition on $\pi(X)$ in order to recover the average treatment effect.&lt;/p&gt;
&lt;p&gt;$$
\mathbb E[\tau] = \mathbb E \big[ Y_{i}^{(1)} - Y_{i}^{(0)} \ \big| \ \pi(X_i) \big]
$$&lt;/p&gt;
&lt;p&gt;This implies the following &lt;strong&gt;inverse propensity-weighted&lt;/strong&gt; estimator:&lt;/p&gt;
&lt;p&gt;$$
\hat \tau^{IPW} = \frac{1}{n} \sum_{i=1}^{n} \left( \frac{D_i Y_i}{\hat \pi(X_i)} - \frac{(1-D_i) Y_i}{1 - \hat \pi(X_i)} \right)
$$&lt;/p&gt;
&lt;p&gt;which, under &lt;em&gt;unconfoundedness&lt;/em&gt; is an &lt;strong&gt;unbiased&lt;/strong&gt; estimator of the average treatment effect, $\mathbb E \left[\hat \tau^{IPW} \right] = \tau$.&lt;/p&gt;
&lt;p&gt;This is a very practically relevant result since it tells us that we need to condition on a single variable instead of a potentially infinite dimensional array. The only thing we need to do is to estimate $\pi(X_i)$.&lt;/p&gt;
&lt;h3 id=&#34;comments&#34;&gt;Comments&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Actual vs Estimated Scores&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Hirano and Ridder (2002) show that even when you know the true propensity score $\pi(X)$, it&amp;rsquo;s better to plug in the estimated propensity score $\hat \pi(X)$. Why? The idea is that the deviation between the actual and the estimated propensity score is providing some additional information. Therefore, it is best to use the actual fraction of treated rather than the theoretical one.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Propensity Scores and Regression&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;What is the difference between running a regression with controls vs doing propensity score matching?&lt;/p&gt;
&lt;p&gt;Aranow and Miller (2015) investigate this comparison in depth. First of all, whenever you are inserting &lt;strong&gt;control variables&lt;/strong&gt; in a regression, you are implicitly thinking about propensity scores. Both approaches are implicitly estimating counterfactual outcomes. Usually OLS extrapolates further away from the actual support than propensity score does.&lt;/p&gt;
&lt;p&gt;In the tweet (and its comments) below you can find further discussion and comments.&lt;/p&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;Thank you for tolerating such a vague poll question. &lt;br&gt;&lt;br&gt;Let me explain why I think this is a useful thing to bring front and certain, and highlight what I think is a flaw in how much of econometrics is taught, currently. &lt;br&gt;&lt;br&gt;1/n &lt;a href=&#34;https://t.co/Wm2jFereYO&#34;&gt;pic.twitter.com/Wm2jFereYO&lt;/a&gt;&lt;/p&gt;&amp;mdash; Paul Goldsmith-Pinkham (@paulgp) &lt;a href=&#34;https://twitter.com/paulgp/status/1470787510091632651?ref_src=twsrc%5Etfw&#34;&gt;December 14, 2021&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;

&lt;h2 id=&#34;academic-application&#34;&gt;Academic Application&lt;/h2&gt;
&lt;p&gt;As an academic application, we are going to replicate &lt;a href=&#34;https://www.tandfonline.com/doi/abs/10.1080/01621459.1999.10473858&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Causal Effects in Non-Experimental Studies: Reevaluating the Evaluation of Training Programs&lt;/a&gt; (1999) by Dahejia and Wahba.&lt;/p&gt;
&lt;p&gt;This study builds on a previous study: &lt;a href=&#34;https://www.jstor.org/stable/2937954&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Evaluating the Econometric Evaluations of Training Programs with Experimental Data&lt;/a&gt; (1986) by Lalonde. In this study, the author compares observational and experimental methods. In particular, he studies an experimental intervention called the NSW (National Supported Work demonstration). The NSW is a temporary training program to give work experience to unemployed people.&lt;/p&gt;
&lt;p&gt;The exogenous variation allows us to estimate the treatment effect as a difference in means. The author then asks: what if we didn&amp;rsquo;t have access to an experiment? In particular, what if we did not have information on the control group? He takes a sample of untreated people from the PSID panel and use them as a control group.&lt;/p&gt;
&lt;h3 id=&#34;experimental-data&#34;&gt;Experimental Data&lt;/h3&gt;
&lt;p&gt;Let&amp;rsquo;s start by loading the NSW data.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;%matplotlib inline
%config InlineBackend.figure_format = &#39;retina&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from src.utils import *
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_nsw = pd.read_csv(&#39;data/l86_nsw.csv&#39;)
df_nsw.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;treat&lt;/th&gt;
      &lt;th&gt;age&lt;/th&gt;
      &lt;th&gt;educ&lt;/th&gt;
      &lt;th&gt;black&lt;/th&gt;
      &lt;th&gt;hisp&lt;/th&gt;
      &lt;th&gt;marr&lt;/th&gt;
      &lt;th&gt;nodegree&lt;/th&gt;
      &lt;th&gt;re74&lt;/th&gt;
      &lt;th&gt;re75&lt;/th&gt;
      &lt;th&gt;re78&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;37&lt;/td&gt;
      &lt;td&gt;11&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;9930.045898&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;22&lt;/td&gt;
      &lt;td&gt;9&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;3595.894043&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;30&lt;/td&gt;
      &lt;td&gt;12&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;24909.449219&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;27&lt;/td&gt;
      &lt;td&gt;11&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;7506.145996&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;33&lt;/td&gt;
      &lt;td&gt;8&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;289.789886&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;The treatment variable is &lt;code&gt;treat&lt;/code&gt; and the outcome of interest is &lt;code&gt;re78&lt;/code&gt;, the income in 1978. We also have access to a bunch of covariates.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;y = &#39;re78&#39;
T = &#39;treat&#39;
X = df_nsw.columns[2:9]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Was there selection on observables? Let&amp;rsquo;s summarize the data, according to treatment status.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_nsw.groupby(&#39;treat&#39;).agg([&#39;mean&#39;, &#39;std&#39;]).T.unstack(1)
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead tr th {
    text-align: left;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;treat&lt;/th&gt;
      &lt;th colspan=&#34;2&#34; halign=&#34;left&#34;&gt;0&lt;/th&gt;
      &lt;th colspan=&#34;2&#34; halign=&#34;left&#34;&gt;1&lt;/th&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;mean&lt;/th&gt;
      &lt;th&gt;std&lt;/th&gt;
      &lt;th&gt;mean&lt;/th&gt;
      &lt;th&gt;std&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;age&lt;/th&gt;
      &lt;td&gt;25.053846&lt;/td&gt;
      &lt;td&gt;7.057745&lt;/td&gt;
      &lt;td&gt;25.816216&lt;/td&gt;
      &lt;td&gt;7.155019&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;educ&lt;/th&gt;
      &lt;td&gt;10.088462&lt;/td&gt;
      &lt;td&gt;1.614325&lt;/td&gt;
      &lt;td&gt;10.345946&lt;/td&gt;
      &lt;td&gt;2.010650&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;black&lt;/th&gt;
      &lt;td&gt;0.826923&lt;/td&gt;
      &lt;td&gt;0.379043&lt;/td&gt;
      &lt;td&gt;0.843243&lt;/td&gt;
      &lt;td&gt;0.364558&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;hisp&lt;/th&gt;
      &lt;td&gt;0.107692&lt;/td&gt;
      &lt;td&gt;0.310589&lt;/td&gt;
      &lt;td&gt;0.059459&lt;/td&gt;
      &lt;td&gt;0.237124&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;marr&lt;/th&gt;
      &lt;td&gt;0.153846&lt;/td&gt;
      &lt;td&gt;0.361497&lt;/td&gt;
      &lt;td&gt;0.189189&lt;/td&gt;
      &lt;td&gt;0.392722&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;nodegree&lt;/th&gt;
      &lt;td&gt;0.834615&lt;/td&gt;
      &lt;td&gt;0.372244&lt;/td&gt;
      &lt;td&gt;0.708108&lt;/td&gt;
      &lt;td&gt;0.455867&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;re74&lt;/th&gt;
      &lt;td&gt;2107.026651&lt;/td&gt;
      &lt;td&gt;5687.905639&lt;/td&gt;
      &lt;td&gt;2095.573693&lt;/td&gt;
      &lt;td&gt;4886.620354&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;re75&lt;/th&gt;
      &lt;td&gt;1266.909015&lt;/td&gt;
      &lt;td&gt;3102.982088&lt;/td&gt;
      &lt;td&gt;1532.055313&lt;/td&gt;
      &lt;td&gt;3219.250879&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;re78&lt;/th&gt;
      &lt;td&gt;4554.801120&lt;/td&gt;
      &lt;td&gt;5483.836001&lt;/td&gt;
      &lt;td&gt;6349.143502&lt;/td&gt;
      &lt;td&gt;7867.402183&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;It seems that covariates are balanced across treatment arms. Nothing seems to point towards selection on observables. Therefore, we can compute the average treatment effect as a simple difference in means&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_nsw.loc[df_nsw[T]==1, y].mean() - df_nsw.loc[df_nsw[T]==0, y].mean()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;1794.3423818501024
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Or equivalently in a regression&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;est = smf.ols(&#39;re78 ~ treat&#39;, df_nsw).fit()
est.summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
      &lt;td&gt;&lt;/td&gt;         &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt; &lt;td&gt; 4554.8011&lt;/td&gt; &lt;td&gt;  408.046&lt;/td&gt; &lt;td&gt;   11.162&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt; 3752.855&lt;/td&gt; &lt;td&gt; 5356.747&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;treat&lt;/th&gt;     &lt;td&gt; 1794.3424&lt;/td&gt; &lt;td&gt;  632.853&lt;/td&gt; &lt;td&gt;    2.835&lt;/td&gt; &lt;td&gt; 0.005&lt;/td&gt; &lt;td&gt;  550.574&lt;/td&gt; &lt;td&gt; 3038.110&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;It looks like the effect is positive and significant.&lt;/p&gt;
&lt;h3 id=&#34;observational-data&#34;&gt;Observational Data&lt;/h3&gt;
&lt;p&gt;Let&amp;rsquo;s now load a different dataset in which we have replaced the true control units with observations from the PSID sample.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_psid = pd.read_csv(&#39;data/l86_psid.csv&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Is this dataset balanced?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_psid.groupby(&#39;treat&#39;).agg([&#39;mean&#39;, &#39;std&#39;]).T.unstack(1)
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead tr th {
    text-align: left;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;treat&lt;/th&gt;
      &lt;th colspan=&#34;2&#34; halign=&#34;left&#34;&gt;0&lt;/th&gt;
      &lt;th colspan=&#34;2&#34; halign=&#34;left&#34;&gt;1&lt;/th&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;mean&lt;/th&gt;
      &lt;th&gt;std&lt;/th&gt;
      &lt;th&gt;mean&lt;/th&gt;
      &lt;th&gt;std&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;age&lt;/th&gt;
      &lt;td&gt;36.094862&lt;/td&gt;
      &lt;td&gt;12.081030&lt;/td&gt;
      &lt;td&gt;25.816216&lt;/td&gt;
      &lt;td&gt;7.155019&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;educ&lt;/th&gt;
      &lt;td&gt;10.766798&lt;/td&gt;
      &lt;td&gt;3.176827&lt;/td&gt;
      &lt;td&gt;10.345946&lt;/td&gt;
      &lt;td&gt;2.010650&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;black&lt;/th&gt;
      &lt;td&gt;0.391304&lt;/td&gt;
      &lt;td&gt;0.489010&lt;/td&gt;
      &lt;td&gt;0.843243&lt;/td&gt;
      &lt;td&gt;0.364558&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;hisp&lt;/th&gt;
      &lt;td&gt;0.067194&lt;/td&gt;
      &lt;td&gt;0.250853&lt;/td&gt;
      &lt;td&gt;0.059459&lt;/td&gt;
      &lt;td&gt;0.237124&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;marr&lt;/th&gt;
      &lt;td&gt;0.735178&lt;/td&gt;
      &lt;td&gt;0.442113&lt;/td&gt;
      &lt;td&gt;0.189189&lt;/td&gt;
      &lt;td&gt;0.392722&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;nodegree&lt;/th&gt;
      &lt;td&gt;0.486166&lt;/td&gt;
      &lt;td&gt;0.500799&lt;/td&gt;
      &lt;td&gt;0.708108&lt;/td&gt;
      &lt;td&gt;0.455867&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;re74&lt;/th&gt;
      &lt;td&gt;11027.303390&lt;/td&gt;
      &lt;td&gt;10814.670751&lt;/td&gt;
      &lt;td&gt;2095.573693&lt;/td&gt;
      &lt;td&gt;4886.620354&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;re75&lt;/th&gt;
      &lt;td&gt;7569.222058&lt;/td&gt;
      &lt;td&gt;9041.944403&lt;/td&gt;
      &lt;td&gt;1532.055313&lt;/td&gt;
      &lt;td&gt;3219.250879&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;re78&lt;/th&gt;
      &lt;td&gt;9995.949977&lt;/td&gt;
      &lt;td&gt;11184.450050&lt;/td&gt;
      &lt;td&gt;6349.143502&lt;/td&gt;
      &lt;td&gt;7867.402183&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;People in the PSID control group are older, more educated, white, married and generally have considerably higher pre-intervention earnings (&lt;code&gt;re74&lt;/code&gt;). This makes sense since the people selected for the NSW program are people that are younger, less experienced and unemployed.&lt;/p&gt;
&lt;p&gt;Lalonde (1986) argues in favor of experimental approaches by showing that using a non-experimental setting, one would not be able to estimate the true treatment effect. Actually, one could even get statistically significant results of the opposite sign.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s repeat the regression exercise for the PSID data.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;smf.ols(&#39;re78 ~ treat&#39;, df_psid).fit().summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
      &lt;td&gt;&lt;/td&gt;         &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt; &lt;td&gt; 9995.9500&lt;/td&gt; &lt;td&gt;  623.715&lt;/td&gt; &lt;td&gt;   16.026&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt; 8770.089&lt;/td&gt; &lt;td&gt; 1.12e+04&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;treat&lt;/th&gt;     &lt;td&gt;-3646.8065&lt;/td&gt; &lt;td&gt;  959.704&lt;/td&gt; &lt;td&gt;   -3.800&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;-5533.027&lt;/td&gt; &lt;td&gt;-1760.586&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;The estimated coefficient is negative and significant. The conclusion from Lalonde (1986) is&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;This comparison shows that many of the econometric procedures d not replicate the experimentally determined results&amp;rdquo;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Dahejia and Wahba (1999) argue that with appropriate matching one would still be able to get a relatively precise estimate of the treatment effect. In particular, the argue in favor of controlling for pre-intervention income, &lt;code&gt;re74&lt;/code&gt; and &lt;code&gt;re75&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s just linearly insert the control variables in the regression.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;smf.ols(&#39;re78 ~ treat + &#39; + &#39; + &#39;.join(X), df_psid).fit().summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
      &lt;td&gt;&lt;/td&gt;         &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt; &lt;td&gt;-1089.9263&lt;/td&gt; &lt;td&gt; 2913.224&lt;/td&gt; &lt;td&gt;   -0.374&lt;/td&gt; &lt;td&gt; 0.708&lt;/td&gt; &lt;td&gt;-6815.894&lt;/td&gt; &lt;td&gt; 4636.042&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;treat&lt;/th&gt;     &lt;td&gt; 2642.1456&lt;/td&gt; &lt;td&gt; 1039.655&lt;/td&gt; &lt;td&gt;    2.541&lt;/td&gt; &lt;td&gt; 0.011&lt;/td&gt; &lt;td&gt;  598.694&lt;/td&gt; &lt;td&gt; 4685.597&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;educ&lt;/th&gt;      &lt;td&gt;  521.5869&lt;/td&gt; &lt;td&gt;  208.751&lt;/td&gt; &lt;td&gt;    2.499&lt;/td&gt; &lt;td&gt; 0.013&lt;/td&gt; &lt;td&gt;  111.284&lt;/td&gt; &lt;td&gt;  931.890&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;black&lt;/th&gt;     &lt;td&gt;-1026.6762&lt;/td&gt; &lt;td&gt; 1006.433&lt;/td&gt; &lt;td&gt;   -1.020&lt;/td&gt; &lt;td&gt; 0.308&lt;/td&gt; &lt;td&gt;-3004.830&lt;/td&gt; &lt;td&gt;  951.478&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;hisp&lt;/th&gt;      &lt;td&gt; -903.1023&lt;/td&gt; &lt;td&gt; 1726.419&lt;/td&gt; &lt;td&gt;   -0.523&lt;/td&gt; &lt;td&gt; 0.601&lt;/td&gt; &lt;td&gt;-4296.394&lt;/td&gt; &lt;td&gt; 2490.189&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;marr&lt;/th&gt;      &lt;td&gt; 1026.6143&lt;/td&gt; &lt;td&gt;  943.788&lt;/td&gt; &lt;td&gt;    1.088&lt;/td&gt; &lt;td&gt; 0.277&lt;/td&gt; &lt;td&gt; -828.410&lt;/td&gt; &lt;td&gt; 2881.639&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;nodegree&lt;/th&gt;  &lt;td&gt;-1469.3712&lt;/td&gt; &lt;td&gt; 1166.114&lt;/td&gt; &lt;td&gt;   -1.260&lt;/td&gt; &lt;td&gt; 0.208&lt;/td&gt; &lt;td&gt;-3761.379&lt;/td&gt; &lt;td&gt;  822.637&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;re74&lt;/th&gt;      &lt;td&gt;    0.1928&lt;/td&gt; &lt;td&gt;    0.058&lt;/td&gt; &lt;td&gt;    3.329&lt;/td&gt; &lt;td&gt; 0.001&lt;/td&gt; &lt;td&gt;    0.079&lt;/td&gt; &lt;td&gt;    0.307&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;re75&lt;/th&gt;      &lt;td&gt;    0.4976&lt;/td&gt; &lt;td&gt;    0.070&lt;/td&gt; &lt;td&gt;    7.068&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    0.359&lt;/td&gt; &lt;td&gt;    0.636&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;The treatment effect is now positive, borderline significant, and close to the experimental estimate of $1794$$. Moreover, it&amp;rsquo;s hard to tell whether this is the correct functional form for the control variables.&lt;/p&gt;
&lt;h3 id=&#34;inverse-propensity-score-weighting&#34;&gt;Inverse propensity score weighting&lt;/h3&gt;
&lt;p&gt;Another option is to use &lt;strong&gt;inverse propensity score weighting&lt;/strong&gt;. First, we need to estimate the treatment probability. Let&amp;rsquo;s start with a very simple standard model to predict binary outcomes.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from sklearn.linear_model import LogisticRegressionCV

pi = LogisticRegressionCV().fit(y=df_psid[T], X=df_psid[X])
df_psid[&#39;pscore&#39;] = pi.predict_proba(df_psid[X])[:,1]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;How does the distribution of the propensity scores look like?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sns.histplot(data=df_psid, x=&#39;pscore&#39;, hue=T, bins=20)\
.set(title=&#39;Distribution of propensity scores, PSID data&#39;, xlabel=&#39;&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/propensity_score_41_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;It seems that indeed we predict higher propensity scores for treated people, and viceversa, indicating a strong selection on observable. However, there is also a considerable amount of overlap.&lt;/p&gt;
&lt;p&gt;We can now estimate the treatment effect by weighting by the inverse of the propensity score. First, let&amp;rsquo;s exclude observations with a very extreme predicted score.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_psid1 = df_psid[(df_psid[&#39;pscore&#39;]&amp;lt;0.9) &amp;amp; (df_psid[&#39;pscore&#39;]&amp;gt;0.1)]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we can need to construct the weights.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_psid1[&#39;weight&#39;] = df_psid1[&#39;treat&#39;] / df_psid1[&#39;pscore&#39;] + (1-df_psid1[&#39;treat&#39;]) / (1-df_psid1[&#39;pscore&#39;])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally, we run a weighted regression of income on the treatment program.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;est = smf.wls(&#39;re78 ~ treat&#39;, df_psid1, weights=df_psid1[&#39;weight&#39;]).fit()
est.summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
      &lt;td&gt;&lt;/td&gt;         &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt; &lt;td&gt; 4038.1507&lt;/td&gt; &lt;td&gt;  512.268&lt;/td&gt; &lt;td&gt;    7.883&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt; 3030.227&lt;/td&gt; &lt;td&gt; 5046.074&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;treat&lt;/th&gt;     &lt;td&gt; 2166.8750&lt;/td&gt; &lt;td&gt;  730.660&lt;/td&gt; &lt;td&gt;    2.966&lt;/td&gt; &lt;td&gt; 0.003&lt;/td&gt; &lt;td&gt;  729.250&lt;/td&gt; &lt;td&gt; 3604.500&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;The effect is positive, statistically significant and very close to the experimental estimate of $1794$$.&lt;/p&gt;
&lt;p&gt;What would have been the propensity scores if we had used the NSW experimental sample? If it&amp;rsquo;s a well done experiment with a sufficiently large sample, we would expect the propensity scores to concentrate around the percentage of people treated, $0.41$ in our data.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;pi = LogisticRegressionCV().fit(y=df_nsw[T], X=df_nsw[X])
df_nsw[&#39;pscore&#39;] = pi.predict_proba(df_nsw[X])[:,1]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sns.histplot(data=df_nsw, x=&#39;pscore&#39;, hue=T, bins=20)\
.set(title=&#39;Distribution of propensity scores, NSW data&#39;, xlabel=&#39;&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/propensity_score_50_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Indeed, now the distribution of the p-scores is concentrated around the treatment frequency in the data. Remarkably, the standard deviation is extremely tight.&lt;/p&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://academic.oup.com/biomet/article/70/1/41/240879&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The central role of the propensity score in observational studies for causal effects&lt;/a&gt; (1983) by Rosenbaum and Rubin&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=8gWctYvRzk4&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Propensity Scores&lt;/a&gt; video lecture by Paul Goldsmith-Pinkham (Yale)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=m3Y8heXoDxE&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Propensity Scores&lt;/a&gt; video lecture by Stefan Wager (Stanford)&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Regression Discontinuity</title>
      <link>https://matteocourthoud.github.io/post/regression_discontinuity/</link>
      <pubDate>Fri, 15 Apr 2022 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/post/regression_discontinuity/</guid>
      <description>&lt;p&gt;In this tutorial, we are going to see how to estimate causal effects when treatment assignment is not random, but determined by a &lt;em&gt;forcing variable&lt;/em&gt; such as a test or a requirement. In this case, we can get a local estimate of the treatment effect by comparing units just above and just below the threshold by assuming that there is no sorting/gaming around it.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Requisites&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;For this tutorial, I assume you are familiar with the following concepts:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Rubin&amp;rsquo;s potential outcome framework&lt;/li&gt;
&lt;li&gt;Ordinary least squares regression&lt;/li&gt;
&lt;li&gt;Non-parametric regression&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://matteocourthoud.github.io/post/iv/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Instrumental variables&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Academic Application&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;As an academic application, we are going to replicate &lt;a href=&#34;https://academic.oup.com/qje/article/119/3/807/1938834&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Do voters affect or elect policies? Evidence from the US House&lt;/a&gt; (2004) by Lee, Moretti, Butler. The authors study whether electoral strength has an effect on policies. To identify the effect, they the advantage that is given by incumbency status, and the quasi-exogeneity given by close elections.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Business Case&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;TBD&lt;/p&gt;
&lt;h2 id=&#34;setting&#34;&gt;Setting&lt;/h2&gt;
&lt;p&gt;We assume that for a set of i.i.d. subjects $i = 1, &amp;hellip;, n$ we observed a tuple $(X_i, D_i, Y_i, Z_i)$ comprised of&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;a feature vector $X_i \in \mathbb R^n$&lt;/li&gt;
&lt;li&gt;a treatment assignment $D_i \in \lbrace 0, 1 \rbrace$&lt;/li&gt;
&lt;li&gt;a response $Y_i \in \mathbb R$
&lt;ul&gt;
&lt;li&gt;outcome of interest that depends on both $X_i$ and $D_i$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;a &lt;strong&gt;forcing variable&lt;/strong&gt; $Z_i \in \mathbb R$
&lt;ul&gt;
&lt;li&gt;variable that determines treatment assignment $D_i$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We normalize the forcing variable $Z_i$ such that $Z_i=0$ corresponds to the cutoff for treatment assignment. We will distinguish two cases for the effect of $Z_i$ on $D_i$:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Sharp RD&lt;/strong&gt;: $D_i = (Z_i \geq 0)$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;treatment is exactly determined by the cutoff&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Fuzzy RD&lt;/strong&gt;: $\lim_{z \to 0_{-}} \mathbb E[D_i | Z_i=z] \neq \lim_{z \to 0_{+}} \mathbb E[D_i | Z_i=z]$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;treatment probability changes at the cutoff&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;Assumption 1 : CE smoothness&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Assumption 2: no sorting&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id=&#34;regression-discontinuity&#34;&gt;Regression Discontinuity&lt;/h2&gt;
&lt;p&gt;The key behind regression discontinuity is what is called a &lt;strong&gt;forcing&lt;/strong&gt; variable that determines treatment assignment. Common examples include test scores for university enrollment (you need a certain test score to get access university) or income for some policy eligibility (you need to be below a certain income threshold to be eligible for a subsidy).&lt;/p&gt;
&lt;p&gt;Clearly, in this setting, treatment is not exogenous. However, the &lt;strong&gt;idea&lt;/strong&gt; behind regression discontinuity is that units &lt;em&gt;sufficiently&lt;/em&gt; close to the discontinuity $Z_i=0$ are &lt;em&gt;sufficiently&lt;/em&gt; similar so that we can attribute differences in the outcome $Y_i$ to the treatment $T_i$.&lt;/p&gt;
&lt;p&gt;What does &lt;em&gt;sufficiently&lt;/em&gt; exactly mean?&lt;/p&gt;
&lt;p&gt;In practice, we are assuming a certain degree of &lt;strong&gt;smoothness&lt;/strong&gt; of the conditional expectation function $\mathbb E[D_i | Z_i=z]$. If this assumption holds, we can estimate the &lt;strong&gt;local average treatment effect&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;$$
\tau^{LATE} = \lim_{z \to 0_{+}} \mathbb E[Y_i | Z_i=z] - \lim_{z \to 0_{-}} \mathbb E[Y_i | Z_i=z] = \mathbb E \big[ Y_{i}^{(1)} - Y_{i}^{(0)} | Z_i=0 \big]
$$&lt;/p&gt;
&lt;p&gt;Note that this is the average treatment effect for a very narrow set of individuals: those that are extremely close to the cutoff.&lt;/p&gt;
&lt;h3 id=&#34;data-challenge&#34;&gt;Data Challenge&lt;/h3&gt;
&lt;p&gt;Regression discontinuity design is a particularly &lt;strong&gt;data hungry&lt;/strong&gt; procedure. In fact, we need to&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;have a very good flexible approximation of the conditional expectation of the outcome $Y_i$ at the cutoff $Z_i=0$&lt;/li&gt;
&lt;li&gt;while also accounting for the effect of the forcing variable $Z$ on the outcome $Y$&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;If we knew the functional form of $\mathbb E[Y_i | Z_i]$, it would be easy.&lt;/p&gt;
&lt;h3 id=&#34;mccrary-test&#34;&gt;McCrary Test&lt;/h3&gt;
&lt;h3 id=&#34;regression-kink-design&#34;&gt;Regression Kink Design&lt;/h3&gt;
&lt;h2 id=&#34;academic-application&#34;&gt;Academic Application&lt;/h2&gt;
&lt;p&gt;As an academic application, we are going to replicate &lt;a href=&#34;https://academic.oup.com/qje/article/119/3/807/1938834&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Do voters affect or elect policies? Evidence from the US House&lt;/a&gt; (2004) by Lee, Moretti, Butler. The authors study whether electoral strength has an effect on policies. To identify the effect, they the advantage that is given by incumbency status, and the quasi-exogeneity given by close elections.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;%matplotlib inline
%config InlineBackend.figure_format = &#39;retina&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from src.utils import *
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df = sm.datasets.get_rdataset(&#39;close_elections_lmb&#39;, package=&#39;causaldata&#39;).data
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df = pd.read_csv(&#39;data/l08.csv&#39;)
df.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;state&lt;/th&gt;
      &lt;th&gt;district&lt;/th&gt;
      &lt;th&gt;id&lt;/th&gt;
      &lt;th&gt;score&lt;/th&gt;
      &lt;th&gt;year&lt;/th&gt;
      &lt;th&gt;demvoteshare&lt;/th&gt;
      &lt;th&gt;democrat&lt;/th&gt;
      &lt;th&gt;lagdemocrat&lt;/th&gt;
      &lt;th&gt;lagdemvoteshare&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;64.339996&lt;/td&gt;
      &lt;td&gt;1948&lt;/td&gt;
      &lt;td&gt;0.553026&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0.469256&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;60.279999&lt;/td&gt;
      &lt;td&gt;1948&lt;/td&gt;
      &lt;td&gt;0.553026&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0.469256&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;57.060001&lt;/td&gt;
      &lt;td&gt;1950&lt;/td&gt;
      &lt;td&gt;0.582441&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0.553026&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;6&lt;/td&gt;
      &lt;td&gt;73.830002&lt;/td&gt;
      &lt;td&gt;1950&lt;/td&gt;
      &lt;td&gt;0.582441&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0.553026&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;7&lt;/td&gt;
      &lt;td&gt;42.959999&lt;/td&gt;
      &lt;td&gt;1954&lt;/td&gt;
      &lt;td&gt;0.569626&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0.539680&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;The first thing we would like to inspect, is the distribution of democratic vote shares &lt;code&gt;demvoteshare&lt;/code&gt;, against their lagged values &lt;code&gt;lagdemvoteshare&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sns.scatterplot(df[&#39;lagdemvoteshare&#39;], df[&#39;demvoteshare&#39;])\
.set(title=&#39;Vote share and incumbency status&#39;, xlabel=&#39;Dem Vote Share (t-1)&#39;, ylabel=&#39;Dem Vote Share (t)&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/regression_discontinuity_19_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;The plot is extremely messy. However we can already see some discontinuity at the threshold: it seems that incumbents do not get vote shares below 0.35.&lt;/p&gt;
&lt;p&gt;To have a more transparent representation of the data, we can use a binscatterplot. Binscatterplots are very similar to histograms with a main difference: instead of having a fixed width, they have a fixed number of observations per bin.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from scipy.stats import binned_statistic

def binscatter(x, y, bins=30, area=True, **kwargs):
    y_bins, x_edges, _ = binned_statistic(x, y, statistic=&#39;mean&#39;, bins=bins)
    x_bins = (x_edges[:-1] + x_edges[1:]) / 2
    p = sns.scatterplot(x_bins, y_bins, **kwargs)
    if area:
        y_std, _, _ = binned_statistic(x, y, statistic=&#39;std&#39;, bins=bins)
        plt.fill_between(x_bins, y_bins-y_std, y_bins+y_std, alpha=0.2, color=&#39;C0&#39;)
    return p
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can now plot the average vote share by previous vote share. The shades represent one standard deviation, at the bin level.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;binscatter(df[&#39;lagdemvoteshare&#39;], df[&#39;demvoteshare&#39;], bins=100)\
.set(title=&#39;Vote share and incumbency status&#39;, xlabel=&#39;Dem Vote Share (t-1)&#39;, ylabel=&#39;Dem Vote Share (t)&#39;);
plt.axvline(x=0.5, ls=&amp;quot;:&amp;quot;, color=&#39;C2&#39;);
plt.title(&#39;Vote share and incumbency status&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/regression_discontinuity_23_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Now it seems quite clear that there exist a discontinuity at $0.5$. We can get a first estimate of the local average treatment effect by assuming a linear model and running a linear regression.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;smf.ols(&#39;demvoteshare ~ lagdemvoteshare + (lagdemvoteshare&amp;gt;0.5)&#39;, df).fit().summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
                &lt;td&gt;&lt;/td&gt;                   &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt;                     &lt;td&gt;    0.2173&lt;/td&gt; &lt;td&gt;    0.005&lt;/td&gt; &lt;td&gt;   46.829&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    0.208&lt;/td&gt; &lt;td&gt;    0.226&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;lagdemvoteshare &gt; 0.5[T.True]&lt;/th&gt; &lt;td&gt;    0.0956&lt;/td&gt; &lt;td&gt;    0.003&lt;/td&gt; &lt;td&gt;   33.131&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    0.090&lt;/td&gt; &lt;td&gt;    0.101&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;lagdemvoteshare&lt;/th&gt;               &lt;td&gt;    0.4865&lt;/td&gt; &lt;td&gt;    0.011&lt;/td&gt; &lt;td&gt;   42.539&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    0.464&lt;/td&gt; &lt;td&gt;    0.509&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;The effect is positive and statistically significant. We can also allow the slope of the line to differ on the two sides of the discontinuity.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df = df.sort_values(&#39;lagdemvoteshare&#39;)
model = smf.ols(&#39;demvoteshare ~ lagdemvoteshare * (lagdemvoteshare&amp;gt;0.5)&#39;, df).fit()
model.summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
                        &lt;td&gt;&lt;/td&gt;                           &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt;                                     &lt;td&gt;    0.2256&lt;/td&gt; &lt;td&gt;    0.007&lt;/td&gt; &lt;td&gt;   34.588&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    0.213&lt;/td&gt; &lt;td&gt;    0.238&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;lagdemvoteshare &gt; 0.5[T.True]&lt;/th&gt;                 &lt;td&gt;    0.0747&lt;/td&gt; &lt;td&gt;    0.012&lt;/td&gt; &lt;td&gt;    6.334&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    0.052&lt;/td&gt; &lt;td&gt;    0.098&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;lagdemvoteshare&lt;/th&gt;                               &lt;td&gt;    0.4653&lt;/td&gt; &lt;td&gt;    0.016&lt;/td&gt; &lt;td&gt;   28.547&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    0.433&lt;/td&gt; &lt;td&gt;    0.497&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;lagdemvoteshare:lagdemvoteshare &gt; 0.5[T.True]&lt;/th&gt; &lt;td&gt;    0.0418&lt;/td&gt; &lt;td&gt;    0.023&lt;/td&gt; &lt;td&gt;    1.827&lt;/td&gt; &lt;td&gt; 0.068&lt;/td&gt; &lt;td&gt;   -0.003&lt;/td&gt; &lt;td&gt;    0.087&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;Let&amp;rsquo;s plot the predicted vote share over the previous graph.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;binscatter(df[&#39;lagdemvoteshare&#39;], df[&#39;demvoteshare&#39;], bins=100, alpha=0.5)\
.set(title=&#39;Vote share and incumbency status&#39;, xlabel=&#39;Dem Vote Share (t-1)&#39;, ylabel=&#39;Dem Vote Share (t)&#39;);
plt.plot(df[&#39;lagdemvoteshare&#39;], model.fittedvalues, color=&#39;C1&#39;)
plt.axvline(x=0.5, ls=&amp;quot;:&amp;quot;, color=&#39;C2&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/regression_discontinuity_29_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Now that we have established a discontinuity at the cutoff, we need to check the RD &lt;strong&gt;assumptions&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;First, is there &lt;strong&gt;sorting&lt;/strong&gt; across the cutoff? In this case, are democratic politicians more or less likely to lose close elections than republicans? We can plot the distribution of (lagged) vote shares and inspect its shape at the cutoff.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sns.histplot(df[&#39;lagdemvoteshare&#39;], bins=100)\
.set(title=&#39;Distribution of lagged dem vote share&#39;, xlabel=&#39;&#39;)
plt.axvline(x=0.5, ls=&amp;quot;:&amp;quot;, color=&#39;C2&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/regression_discontinuity_31_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;If looks pretty smooth. If anything, there is a loss of density at the cutoff, plausibly indicating stronger competition when the competition is close. However, if does not seem particularly asymmetric.&lt;/p&gt;
&lt;p&gt;A &lt;strong&gt;placebo&lt;/strong&gt; test that we can run is to check if the forcing variable has an effect on variables on which we do not expect to have an effect. In this setting, the most intuitive placebo outcome is previous elections: we do not expect that being on either side of the cutoff today is related to any past outcome.&lt;/p&gt;
&lt;p&gt;In our case, we can simply swap the two variables to run the test.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;binscatter(df[&#39;demvoteshare&#39;], df[&#39;lagdemvoteshare&#39;], bins=100)\
.set(title=&#39;Vote share and incumbency status&#39;, xlabel=&#39;Dem Vote Share (t)&#39;, ylabel=&#39;Dem Vote Share (t-1)&#39;);
plt.axvline(x=0.5, ls=&amp;quot;:&amp;quot;, color=&#39;C2&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/regression_discontinuity_33_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;The distribution of vote shares in the past period does not seem to be discontinuous in the incumbency status today, as expected.&lt;/p&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=72KFY8beH0w&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Regression discontinuity video lecture by Paul Goldsmith-Pinkham (Yale)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Free Courses in Economics</title>
      <link>https://matteocourthoud.github.io/post/courses/</link>
      <pubDate>Thu, 10 Feb 2022 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/post/courses/</guid>
      <description>&lt;p&gt;In this page, I collect lectures and materials for graduate courses in Economics and Social Sciences.&lt;/p&gt;
&lt;p&gt;I will only link to lectures and materials that are freely available. I will not link to courses hosted on MOOC websites or that require university credentials to access.&lt;/p&gt;
&lt;p&gt;A special mention goes to the following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The &lt;strong&gt;NBER&lt;/strong&gt; that during each Summer Institute has a &lt;a href=&#34;https://www.nber.org/research/lectures&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;lecture series&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;The &lt;strong&gt;Chamberlain Seminar&lt;/strong&gt; that since 2021 started hosting and recording &lt;a href=&#34;https://www.chamberlainseminar.org/past-seminars&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;tutorial sessions&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;video-lectures&#34;&gt;Video Lectures&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Course Title&lt;/th&gt;
&lt;th&gt;Author&lt;/th&gt;
&lt;th&gt;University&lt;/th&gt;
&lt;th&gt;Year&lt;/th&gt;
&lt;th&gt;Material&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://www.youtube.com/playlist?list=PLxq_lXOUlvQAoWZEqhRqHNezS30lI49G-&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Machine Learning and Causal Inference&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Susan Athey et al.&lt;/td&gt;
&lt;td&gt;Stanford&lt;/td&gt;
&lt;td&gt;2022&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://www.gsb.stanford.edu/faculty-research/centers-initiatives/sil/research/methods/ai-machine-learning/short-course&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Yes&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://www.youtube.com/playlist?list=PLo0lw6BstMGZQqx_r1GnOETkFYihCgve9&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Causal Inference with Panel Data&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Yiqing Xu&lt;/td&gt;
&lt;td&gt;Washington U&lt;/td&gt;
&lt;td&gt;2021&lt;/td&gt;
&lt;td&gt;No&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://chrisconlon.github.io/gradio.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Industrial Organization&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Chris Conlon&lt;/td&gt;
&lt;td&gt;NYU&lt;/td&gt;
&lt;td&gt;2021&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://chrisconlon.github.io/gradio.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Yes&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://chrisconlon.github.io/metrics.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Panel Data Econometrics&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Chris Conlon&lt;/td&gt;
&lt;td&gt;NYU&lt;/td&gt;
&lt;td&gt;2021&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://chrisconlon.github.io/metrics.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Yes&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://www.youtube.com/playlist?list=PLoROMvodv4rPLKxIpqhjhPgdQy7imNkDn&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Machine Learning with Graphs&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Yure Leskovec&lt;/td&gt;
&lt;td&gt;Stanford&lt;/td&gt;
&lt;td&gt;2021&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;http://web.stanford.edu/class/cs224w/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Yes&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://www.youtube.com/playlist?list=PLWWcL1M3lLlojLTSVf2gGYQ_9TlPyPbiJ&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Applied Methods&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Paul Goldsmith-Pinkham&lt;/td&gt;
&lt;td&gt;Yale&lt;/td&gt;
&lt;td&gt;2021&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://github.com/paulgp/applied-methods-phd&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Yes&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://taylorjwright.github.io/did-reading-group/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;DiD Reading Group&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;misc&lt;/td&gt;
&lt;td&gt;misc&lt;/td&gt;
&lt;td&gt;2021&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://taylorjwright.github.io/did-reading-group/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Yes&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://vimeo.com/user108848900&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Computational Economics&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Kenneth Judd&lt;/td&gt;
&lt;td&gt;Stanford&lt;/td&gt;
&lt;td&gt;2020&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://github.com/KennethJudd/CompEcon2020&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Yes&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://www.youtube.com/playlist?list=PLoROMvodv4rOSOPzutgyCTapiGlY2Nd8u&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Reinforcement Learning&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Emma Brunskill&lt;/td&gt;
&lt;td&gt;Stanford&lt;/td&gt;
&lt;td&gt;2020&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;http://web.stanford.edu/class/cs234/index.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Yes&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://www.youtube.com/playlist?list=PLoazKTcS0RzZ1SUgeOgc6SWt51gfT80N0&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Causal Inference&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Brady Neal&lt;/td&gt;
&lt;td&gt;Quebec AI Institute&lt;/td&gt;
&lt;td&gt;2020&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://www.bradyneal.com/causal-inference-course&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Yes&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://www.youtube.com/playlist?list=PLoROMvodv4rObpMCir6rNNUlFAn56Js20&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Natural Language Understanding&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Christopher Potts&lt;/td&gt;
&lt;td&gt;Stanford&lt;/td&gt;
&lt;td&gt;2019&lt;/td&gt;
&lt;td&gt;No&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;material&#34;&gt;Material&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Course Title&lt;/th&gt;
&lt;th&gt;Author&lt;/th&gt;
&lt;th&gt;University&lt;/th&gt;
&lt;th&gt;Year&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://floswald.github.io/NumericalMethods/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Computational Economics&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://floswald.github.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Florial Oswald&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Bocconi&lt;/td&gt;
&lt;td&gt;2021&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://github.com/uo-ec607/lectures&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Data Science for Economists&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://grantmcdermott.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Grant McDermott&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Oregon&lt;/td&gt;
&lt;td&gt;2020&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;http://www.johnasker.com/IO.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Industrial Organization&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;John Asker&lt;/td&gt;
&lt;td&gt;UCLA&lt;/td&gt;
&lt;td&gt;2020&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://kohei-kawaguchi.github.io/EmpiricalIO/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Topics in Empirical Industrial Organization&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Kohei Kawaguchi&lt;/td&gt;
&lt;td&gt;Hong Kong&lt;/td&gt;
&lt;td&gt;2020&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;http://individual.utoronto.ca/vaguirre/courses/eco2901/teaching_io_toronto.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Industrial Organization&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Victor Aguirregabiria&lt;/td&gt;
&lt;td&gt;Toronto&lt;/td&gt;
&lt;td&gt;2019&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://github.com/OU-PhD-Econometrics/fall-2020&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Econometrics&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Tyler Ransom&lt;/td&gt;
&lt;td&gt;Oklahoma&lt;/td&gt;
&lt;td&gt;2020&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://github.com/MartinSpindler/Machine-Learning-in-Econometrics&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Machine Learning in Econometrics&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Martin Spindler&lt;/td&gt;
&lt;td&gt;Munich&lt;/td&gt;
&lt;td&gt;2020&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;http://comlabgames.com/structuraleconometrics/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Structural Econometrics&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Robert Miller&lt;/td&gt;
&lt;td&gt;Carnegie Mellon&lt;/td&gt;
&lt;td&gt;2019&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
</description>
    </item>
    
    <item>
      <title>Economics Conferences</title>
      <link>https://matteocourthoud.github.io/post/conferences/</link>
      <pubDate>Mon, 10 Jan 2022 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/post/conferences/</guid>
      <description>&lt;p&gt;In this page, I collect information about conferences in Economics and Finance.&lt;/p&gt;
&lt;p&gt;If you know about public conferences or meetings that are missing from this list, please either &lt;a href=&#34;mailto:matteo.courthoud@econ.uzh.ch&#34;&gt;contact me&lt;/a&gt; or &lt;a href=&#34;https://github.com/matteocourthoud/website/blob/master/content/post/conferences/index.md&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;edit the table on Github&lt;/a&gt;!&lt;/p&gt;
&lt;p&gt;Note that conferences are ordered by deadline and not by conference date.&lt;/p&gt;
&lt;h2 id=&#34;january&#34;&gt;January&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Conference&lt;/th&gt;
&lt;th&gt;Organizer&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Field&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Deadline&lt;/th&gt;
&lt;th&gt;Target&lt;/th&gt;
&lt;th&gt;Date&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://www.sgvs.ch/conferences/sses2022&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Annual Congress of the Swiss Society of Economics and Statistics&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;SSES&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;All&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://www.sgvs.ch/conferences/sses2022/call_for_papers&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;January 31&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;23/06/21&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;february&#34;&gt;February&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Conference&lt;/th&gt;
&lt;th&gt;Organizer&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Field&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Deadline&lt;/th&gt;
&lt;th&gt;Target&lt;/th&gt;
&lt;th&gt;Date&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://beccle.no/event/bergen-competition-policy-conference-2020/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Bergen Competition Policy Conference&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;NHH&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Comp policy&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://beccle.no/event/bergen-competition-policy-conference-2020/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;February 03&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;23/04/20&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://cepr.org/6754/cfp-mainconference&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CEPR/JIE Conference on Applied IO&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;CEPR&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;IO&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://matteocourthoud.github.io/post/conferences/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;February 10&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Junior&lt;/td&gt;
&lt;td&gt;08/06/21&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;http://ec22.sigecom.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Economics and Computation&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;ACM SIGecom&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Theory&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://ec22.sigecom.org/call-for-contributions-acm/papers/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;February 10&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;11/07/21&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://www.econometricsociety.org/meetings/schedule/2022/06/16/2022-north-america-summer-meeting&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ES North American Summer Meeting&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Econometric Society&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;All&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://www.econometricsociety.org/meetings/schedule/2022/06/16/2022-north-america-summer-meeting&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;February 12&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Senior&lt;/td&gt;
&lt;td&gt;16/06/21&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://www.eea-esem-congresses.org/index.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;EEA Summer Meeting&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;European Economic Association&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;All&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://www.eea-esem-congresses.org/important-dates.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;February 15&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;23/08/21&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://www.eea-esem-congresses.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ES European Summer Meeting&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Econometric Society&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;All&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://www.eea-esem-congresses.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;February 15&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Senior&lt;/td&gt;
&lt;td&gt;23/08/21&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://www.economicdynamics.org/sedam_2021/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Annual Meeting of the Society for Economic Dynamics&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;SED&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Macro&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://www.economicdynamics.org/sedam_2021/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;February 15&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;01/07/21&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://games2020.hu/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GAMES 2020&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Game Theory Society&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Game Theory&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://games2020.hu/registration/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;February 20&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;19/07/21&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://www.nottingham.ac.uk/gep/news-events/conferences/2020-21/postgrad-conference-2021.aspx&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Annual GEP/CEPR Postgraduate Conference&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;University of Nottingham&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Policy&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://www.nottingham.ac.uk/gep/documents/conferences/2020-21/pg-conf-cfp-2021.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;February 26&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Junior&lt;/td&gt;
&lt;td&gt;06/05/21&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://www.digital-economics.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Doctoral Workshop on the Economics of Digitization&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;TSE&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Digitalization&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://www.digital-economics.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;February 28&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Junior&lt;/td&gt;
&lt;td&gt;12/05/22&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;march&#34;&gt;March&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Conference&lt;/th&gt;
&lt;th&gt;Organizer&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Field&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Deadline&lt;/th&gt;
&lt;th&gt;Target&lt;/th&gt;
&lt;th&gt;Date&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://cssh.northeastern.edu/economics/iioc/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Annual IIOC&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Northeastern University&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;IO&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://cssh.northeastern.edu/economics/iioc/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;March 01&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;30/04/21&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://yem2020.econ.muni.cz/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Young Economists&amp;rsquo; Meeting&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;University fo Munich&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;All&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://yem2020.econ.muni.cz/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;March 08&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Junior&lt;/td&gt;
&lt;td&gt;01/10/20&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://www.barcelonagse.eu/summer-forum&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GSE Summer Forum&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Barcelona GSE&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;All&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://www.barcelonagse.eu/summer-forum&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;March 14&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;07/06/21&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://www.nhh.no/en/calendar/conferences/2021/earie-2021/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;EARIE&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;NHH&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;IO&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://www.nhh.no/en/calendar/conferences/2021/earie-2021/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;March 15&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;27/08/21&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://www.sioe.org/news/economics-media-workshop-call-paper-poster-presentations&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Economics of Media Workshop&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Queen’s University&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;IO&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://www.sioe.org/news/economics-media-workshop-call-paper-poster-presentations&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;March 15&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Junior&lt;/td&gt;
&lt;td&gt;12/06/20&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://www.qmul.ac.uk/sef/events/conferences/items/3rd-qmul-economics-and-finance-workshop-for-phd--post-doctoral-students.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;QMUL Economics and Finance Workshop&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Queen Mary University&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;All&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://econ.columbia.edu/call-for-papers-3rd-qm-phd-workshop/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;March 20&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Junior&lt;/td&gt;
&lt;td&gt;26/05/20&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://static1.squarespace.com/static/56086d00e4b0fb7874bc2d42/t/5e753140c2225859fa93ba1e/1584738624656/callforpapers.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Virtual Finance and Economics Conference&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Yale University&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Finecon&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://static1.squarespace.com/static/56086d00e4b0fb7874bc2d42/t/5e753140c2225859fa93ba1e/1584738624656/callforpapers.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;March 25&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;17/04/20&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://sites.google.com/view/dc-io-day&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;DC IO Day 2020&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Georgetown University&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;IO&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://sites.google.com/view/dc-io-day&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;March 31&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;15/05/20&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;april&#34;&gt;April&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Conference&lt;/th&gt;
&lt;th&gt;Organizer&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Field&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Deadline&lt;/th&gt;
&lt;th&gt;Target&lt;/th&gt;
&lt;th&gt;Date&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://economics.stanford.edu/site/site-2021&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;SITE&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Stanford University&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Theory&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://economics.stanford.edu/site/site-2021&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;April 01&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;12/06/21&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://www.nber.org/conferences/summer-institute&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;NBER Summer Meeting Workshop&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;NBER&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;All&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://www.nber.org/conferences/summer-institute&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;April 05&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;12/07/21&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://www.aeaweb.org/conference/submissions&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;AEA Annual Meeting&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;AEA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;All&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://www.aeaweb.org/conference/submissions&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;April 15&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;07/01/22&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://matteocourthoud.github.io/post/conferences/files/swissIOday2021_CallForPapers.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Swiss IO Day&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;University of Bern&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;IO&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://matteocourthoud.github.io/post/conferences/files/swissIOday2021_CallForPapers.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;April 16&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;11/06/21&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://www.econometricsociety.org/meetings/schedule/2022/01/06/2022-north-american-winter-meeting&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Econometric Society - North American Winter Meetings&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;ES&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;All&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://www.econometricsociety.org/meetings/schedule/2022/01/06/2022-north-american-winter-meeting&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;April 21&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;06/01/22&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://www.cresse.info/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CRESSE&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;CRESSE&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;IO&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://www.cresse.info/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;April 30&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;26/07/21&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;may&#34;&gt;May&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Conference&lt;/th&gt;
&lt;th&gt;Organizer&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Field&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Deadline&lt;/th&gt;
&lt;th&gt;Target&lt;/th&gt;
&lt;th&gt;Date&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://mailchi.mp/cepr/european-research-workshop-in-international-trade-erwit-506353&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;European Research Workshop in International Trade&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;CEPR&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Trade&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://mailchi.mp/cepr/european-research-workshop-in-international-trade-erwit-506353&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;May 02&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;22/10/21&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;http://coin.wne.uw.edu.pl/wiem/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Warsaw International Economic Meeting&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Warsaw University&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;All&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;http://coin.wne.uw.edu.pl/wiem/wiem2020-cfp.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;May 03&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;01/07/20&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://warwick.ac.uk/fac/soc/economics/events/2021/6/economics_phd_conference/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Warwick Economics PhD Conference&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;University of Warwick&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;All&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://warwick.ac.uk/fac/soc/economics/events/2021/6/economics_phd_conference/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;May 09&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;PhD&lt;/td&gt;
&lt;td&gt;24/06/21&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://www.law.northwestern.edu/research-faculty/clbe/events/antitrust/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Annual Conference on Antitrust Economics and Competition Policy&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Northwestern University&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Comp policy&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://www.law.northwestern.edu/research-faculty/clbe/events/antitrust/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;May 17&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;17/09/21&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://www.economicsofai.com/blog/2021/2/2/2021-nber-economics-of-ai-conference-call-for-papers&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;NBER Economics of AI Conference&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;NBER&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;AI&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://www.economicsofai.com/blog/2021/2/2/2021-nber-economics-of-ai-conference-call-for-papers&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;May 31&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;23/09/21&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;june&#34;&gt;June&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Conference&lt;/th&gt;
&lt;th&gt;Organizer&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Field&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Deadline&lt;/th&gt;
&lt;th&gt;Target&lt;/th&gt;
&lt;th&gt;Date&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://eaamo.org/cfp/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Equity and Access in Algorithms, Mechanisms, and Optimization (EAAMO)&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;ACM&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;AI&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://eaamo.org/cfp/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;June 14&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;05/10/21&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://www.ftc.gov/news-events/events-calendar/fourteenth-annual-federal-trade-commission-microeconomics-conference&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;FTC Micro Conference&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;FTC&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;IO&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://www.ftc.gov/system/files/documents/public_events/1588356/20210326_-_micro_conf_call_for_papers.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;June 23&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Senior&lt;/td&gt;
&lt;td&gt;04/11/21&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;july&#34;&gt;July&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Conference&lt;/th&gt;
&lt;th&gt;Organizer&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Field&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Deadline&lt;/th&gt;
&lt;th&gt;Target&lt;/th&gt;
&lt;th&gt;Date&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://www.emconference.net/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Empirics and Methods in Economics Conference&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Northwestern &amp;amp; Chicago&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Empirical&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://www.emconference.net/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;July 30&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Junior&lt;/td&gt;
&lt;td&gt;22/10/20&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;august&#34;&gt;August&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Conference&lt;/th&gt;
&lt;th&gt;Organizer&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Field&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Deadline&lt;/th&gt;
&lt;th&gt;Target&lt;/th&gt;
&lt;th&gt;Date&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://docs.google.com/document/d/1BjHI_6HyL7pk2UYUNDlDY971B9AO83wD8DfEF6KNDfs/edit&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;AI Policy Conference&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;ETH Zurich&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;AI&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://docs.google.com/document/d/1BjHI_6HyL7pk2UYUNDlDY971B9AO83wD8DfEF6KNDfs/edit&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;August 1&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;14/09/21&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://sites.google.com/site/uscfom/finance-organizations-and-markets-fom-research-group&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Finance, Organizations and Markets (FOM) Conference&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Dartmouth College&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Finance, IO&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://sites.google.com/site/uscfom/finance-organizations-and-markets-fom-research-group&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;August 14&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;28/10/21&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;september&#34;&gt;September&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Conference&lt;/th&gt;
&lt;th&gt;Organizer&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Field&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Deadline&lt;/th&gt;
&lt;th&gt;Target&lt;/th&gt;
&lt;th&gt;Date&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://why21.causalai.net/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Causal Inference &amp;amp; Machine Learning: Why now?&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;NeurIPS&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Econometrics&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://why21.causalai.net/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;September 18&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;13/12/21&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://www.ub.edu/school-economics/ewmes&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ES European Winter Meeting&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Econometricc Society&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;All&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://www.ub.edu/school-economics/ewmes&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;September 19&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Senior&lt;/td&gt;
&lt;td&gt;13/12/21&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://www.causalscience.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Causal Data Science Conference&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;causalscience&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;All&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://www.causalscience.org/blog/causal-data-science-meeting-2021-call-for-papers/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;September 30&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;15/11/21&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;october&#34;&gt;October&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Conference&lt;/th&gt;
&lt;th&gt;Organizer&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Field&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Deadline&lt;/th&gt;
&lt;th&gt;Target&lt;/th&gt;
&lt;th&gt;Date&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://www.tse-fr.eu/conferences/2022-15th-digital-economics-conference&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Digital Economics Conference&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;TSE&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Digital&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://www.tse-fr.eu/sites/default/files/TSE/documents/conf/2022/Digital_Economics/call_for_papers_digital_conf_2022.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;October 3&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;13/01/22&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://apios.org.au/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Asia-Pacific IO Conference&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Asia-Pacific IO Society&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;IO&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://apios.org.au/submission/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;October 22&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;13/12/21&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://www.sgvs.ch/conferences/ysem2021&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Young Swiss Economists Meeting&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;SSES&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;All&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://www.sgvs.ch/files/Call_for_Papers_YSEM_2021.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;October 25&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Junior&lt;/td&gt;
&lt;td&gt;11/02/21&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;november&#34;&gt;November&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Conference&lt;/th&gt;
&lt;th&gt;Organizer&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Field&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Deadline&lt;/th&gt;
&lt;th&gt;Target&lt;/th&gt;
&lt;th&gt;Date&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://www.law.nyu.edu/conferences/2022-NextGen-Antitrust-Conference&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Next Generation of Antitrust, Data Privacy and Data Protection Scholars Conference&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;NYU&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;IO&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://www.law.nyu.edu/conferences/2022-NextGen-Antitrust-Conference&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;November 1&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;28/01/22&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://smye2021.weebly.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Spring Meeting of Young Economists&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;University of Bologna&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;All&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://smye2021.weebly.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;November 12&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Junior&lt;/td&gt;
&lt;td&gt;17/06/21&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://www.nber.org/conferences/industrial-organization-program-meeting-spring-2021&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;NBER IO Winter Meeting&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;NBER&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;IO&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://conference.nber.org/confsubmit/backend/cfp?id=IOs21&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;November 20&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Senior&lt;/td&gt;
&lt;td&gt;12/02/21&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://www.zew.de/en/events-and-professional-training/detail/2021-macci-annual-conference/3320&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;MaCCI Annual Conference&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;University of Mannheim&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;IO&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://www.zew.de/en/events-and-professional-training/detail/2021-macci-annual-conference/3320&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;November 30&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;12/03/21&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://www.tse-fr.eu/sites/default/files/TSE/documents/conf/2022/postal/call_postal_2022_v2.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Postal Economics Conference&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;TSE&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Digital&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://www.tse-fr.eu/sites/default/files/TSE/documents/conf/2022/postal/call_postal_2022_v2.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;November 30&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;07/04/22&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;december&#34;&gt;December&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Conference&lt;/th&gt;
&lt;th&gt;Organizer&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Field&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Deadline&lt;/th&gt;
&lt;th&gt;Target&lt;/th&gt;
&lt;th&gt;Date&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://sites.google.com/site/ecbeconference&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Early-Career Behavioral Economics Conference&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Princeton University&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Behavioral&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://sites.google.com/site/ecbeconference/call&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;December 15&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;junior&lt;/td&gt;
&lt;td&gt;03/06/21&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;undefined&#34;&gt;Undefined&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Conference&lt;/th&gt;
&lt;th&gt;Organizer&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Field&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Deadline&lt;/th&gt;
&lt;th&gt;Target&lt;/th&gt;
&lt;th&gt;Date&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://www.law.northwestern.edu/research-faculty/clbe/events/innovation/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Annual Conference on Innovation Economics&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Northwestern University&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Innovation&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://www.law.northwestern.edu/research-faculty/clbe/callforpapers/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;forthcoming&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;27/08/20&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://conference2.aau.at/event/4/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Conference on Mechanism and Institution Design&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Universität Klagenfurt&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Market Design&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;closed&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;11/06/20&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://sites.google.com/site/dteaworkshop/home&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;D-TEA Conference&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;HEC Paris&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Theory&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;closed&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;16/06/20&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://sites.wustl.edu/egsc/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Economics Graduate Student Conference&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Washington University&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;All&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;closed&lt;/td&gt;
&lt;td&gt;Junior&lt;/td&gt;
&lt;td&gt;07/11/20&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;http://conference.nber.org/confer/2020/SI2020/SI2020.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;NBER Summer Institute&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;NBER&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;All&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;invitation&lt;/td&gt;
&lt;td&gt;Senior&lt;/td&gt;
&lt;td&gt;06/07/20&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;http://competitionpolicy.ac.uk/events/annual-conferences&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CCP Annual Conference&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Centre for Competition Policy&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Comp policy&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;closed&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;24/06/21&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://www.res.org.uk/event-listing/2021-annual-conference.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;RES Annual Conference&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Royal Economics Society&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;All&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;closed&lt;/td&gt;
&lt;td&gt;Senior&lt;/td&gt;
&lt;td&gt;12/04/21&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://www.emerginginvestigators.org/conference&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;JEI Student Conference&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Harvard University&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;All&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;canceled&lt;/td&gt;
&lt;td&gt;Junior&lt;/td&gt;
&lt;td&gt;20/06/20&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://bfi.uchicago.edu/event/sixth-annual-conference-on-network-science-and-economics/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Annual Conference on Network Science and Economics&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Becker Friedman Institute&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Networks&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;canceled&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;27/03/20&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://saet.uiowa.edu/2021-annual-saet-conference/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Annual SAET Conference&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Society for the Advancement of Economic Theory&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Theory&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;closed&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;13/06/21&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
</description>
    </item>
    
    <item>
      <title>How to Work on a Remote Machine via SSH</title>
      <link>https://matteocourthoud.github.io/post/ssh/</link>
      <pubDate>Mon, 06 Dec 2021 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/post/ssh/</guid>
      <description>&lt;p&gt;Welcome to my tutorial on how to set up a remote machine and deploy your code there. I will first analyze SSH and then look at two specific applications: coding in Python and Julia.&lt;/p&gt;
&lt;h2 id=&#34;setup&#34;&gt;Setup&lt;/h2&gt;
&lt;p&gt;In order to start working on a remote server you need&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the server&lt;/li&gt;
&lt;li&gt;local shell&lt;/li&gt;
&lt;li&gt;SSH installed&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;SSH, or Secure Shell, is a protocol designed to transfer data between a client and a server (two computers basically) over an untrusted network.&lt;/p&gt;
&lt;p&gt;The way SSH works is it encrypts the connection using a pair of keys and the server, which is the computer you would connect to, is usually waiting for an SSH connection on Port 22.&lt;/p&gt;
&lt;p&gt;SSH is normally installed by default. To check if you have SSH installed, open the terminal and write &lt;code&gt;ssh&lt;/code&gt;. You should receive a message that looks like this&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;usage: ssh [-1246AaCfGgKkMNnqsTtVvXxYy] [-b bind_address] [-c cipher_spec]
[-D [bind_address:]port] [-E log_file] [-e escape_char]
[-F configfile] [-I pkcs11] [-i identity_file]
[-J [user@]host[:port]] [-L address] [-l login_name] [-m mac_spec] [-O ctl_cmd] [-o option] [-p port] [-Q query_option] [-R address] [-S ctl_path] [-W host:port] [-w local_tun[:remote_tun]]
[user@]hostname [command]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If SSH is not installed, you can install it using the following commands.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;sudo apt-get install openssh-server
sudo systemctl enable ssh
sudo systemctl start ssh
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now that you have installed SSH, we are ready to setup a remote connection.&lt;/p&gt;
&lt;p&gt;From the computer you want to access remotey, generate the public key.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;ssh-keygen -t rsa
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You will be asked for a location. If you decide to enter one manually then that will be the pair’s location, if you leave the default one it will be inside the &lt;code&gt;.ssh&lt;/code&gt; hidden folder in your home directory.&lt;/p&gt;
&lt;p&gt;Now you will be prompted for a password. If you enter one you will be asked for it every time you use the key, this works for added security. If you don’t want a password just press enter and continue without one.&lt;/p&gt;
&lt;p&gt;Two files were created. One file ends with the ‘.pub’ extension and the other one doesn’t. The file that ends with ‘.pub’ is your public key. This key needs to be in the computer you want to connect to (the server) inside a file called &lt;code&gt;authorized_keys&lt;/code&gt; . You can accomplish this with the following command:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;ssh-copy-id username@ip
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For example in my case to send the key to my computer it would be:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;ssh-copy-id sergiop@132.132.132.132
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you have MacOS there’s a chance you don’t have ssh-copy-id installed, in that case you can install it using&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;brew install ssh-copy-id
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you haven’t installed &lt;code&gt;brew&lt;/code&gt;, you can install it by following &lt;a href=&#34;https://brew.sh/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;this guide&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;connect&#34;&gt;Connect&lt;/h2&gt;
&lt;p&gt;To permanently add the SSH key, you can use the follwing command&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;ssh-add directory\key.pem
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Lastly, to connect, just type the following command.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;ssh username@ip
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Where &lt;code&gt;username&lt;/code&gt; is the server name and &lt;code&gt;ip&lt;/code&gt; is the public IP adress, e.g. 132.132.132.132.&lt;/p&gt;
&lt;p&gt;If your server is not public, you will not be able to access it.&lt;/p&gt;
&lt;p&gt;If your server is password protected, you will be prompted to insert a password when you connect. If not, you should protect it with a password.&lt;/p&gt;
&lt;h2 id=&#34;managing-screens&#34;&gt;Managing screens&lt;/h2&gt;
&lt;p&gt;While you are connected to the remote terminal, any disturbance to your connection will interrupt the code. In order to avoid that, you want to create separate screens. This will allow your code to run remotely undisturbed, irrespectively of your connection.&lt;/p&gt;
&lt;p&gt;First, you need to install &lt;code&gt;screen&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;brew install screen
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To create a new screen, just type&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;screen
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now you can lunch your code.&lt;/p&gt;
&lt;p&gt;After that, you want to detach from that screen so that the code can run remotely undisturbed.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;screen -d
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Another option is to use &lt;code&gt;ctrl+a&lt;/code&gt; followed by &lt;code&gt;ctrl+d&lt;/code&gt;. This will detach the screen without the need to type anythin in the terminal, in case the terminal is busy (most likely).&lt;/p&gt;
&lt;p&gt;To list the current active screens type&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;screen -ls
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you want to check at any time that your code is running, without re-attaching to the screen, you can just type&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;top
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;which is the general command to check active processes. To exit, use &lt;code&gt;ctrl+z&lt;/code&gt;, which generally terminates processes in the terminal.&lt;/p&gt;
&lt;p&gt;To reattach to your screen, type&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;screen -r
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In case you have multiple screens (you can check with &lt;code&gt;screen -ls&lt;/code&gt;), you can reattach to a specific one by typing&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;screen -r 12345
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;where &lt;code&gt;12345&lt;/code&gt; is the id of the screen.&lt;/p&gt;
&lt;p&gt;To kill a screen, type&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;screen -XS 12345 quit
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;where again &lt;code&gt;12345&lt;/code&gt; is the id of the screen.&lt;/p&gt;
&lt;h2 id=&#34;python-and-pycharm&#34;&gt;Python and Pycharm&lt;/h2&gt;
&lt;p&gt;If you are coding in Python, &lt;a href=&#34;https://www.jetbrains.com/pycharm/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PyCharm&lt;/a&gt; is one of the best IDEs. Among many features, it offers the possibility to set a remote compiler for your pthon console and to sync input and output files automatically.&lt;/p&gt;
&lt;p&gt;First, you need to have setup a remote SSH connection following the steps above. Importantly, you need to have added the public key to your machine using the &lt;code&gt;ssh-add&lt;/code&gt; command, as explained above.&lt;/p&gt;
&lt;p&gt;Then open Pytharm, go to the lower-right corner, where the current interpreter is listed (e.g. Pytohn 3.8), click it and select &lt;code&gt;interpreter settings&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;img/interpreter_settings.png&#34; alt=&#34;interpreter_settings&#34;&gt;&lt;/p&gt;
&lt;p&gt;Click on the gear icon ⚙️ on the top-right corner and select &lt;code&gt;add&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;img/add.png&#34; alt=&#34;add&#34;&gt;&lt;/p&gt;
&lt;p&gt;Insert the server &lt;code&gt;host&lt;/code&gt; (IP address, e.g. 132.132.132.132) and &lt;code&gt;username&lt;/code&gt; (e.g. sergiop).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;img/configuration.png&#34; alt=&#34;configuration&#34;&gt;&lt;/p&gt;
&lt;p&gt;Next, you have to insert your credentials. If you have a password, insert it, otherwise you have to insert the path to your SSH key file.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;img/password.png&#34; alt=&#34;configuration&#34;&gt;&lt;/p&gt;
&lt;p&gt;Lastly, select the remote interpreter. If you are using a python version that is not default, browse to the preferred python installation folder. Also, check the box for &lt;code&gt;execute code giving this interpreter with root privileges via sudo&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;You can also select which remote folder to sync with your local project. By default, you are given a &lt;code&gt;tmp/pycharm_project_XX&lt;/code&gt; folder. You can change it if you want. I recommend also to have the last option checked: &lt;code&gt;automatically sync project files to the server&lt;/code&gt;. This will automatically synch all remote changes with your local machine, in your local project folder.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;img/folder.png&#34; alt=&#34;configuration&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;julia-and-juno&#34;&gt;Julia and Juno&lt;/h2&gt;
&lt;p&gt;If you are coding in Julia, &lt;a href=&#34;https://junolab.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Juno&lt;/a&gt; is the best IDE around. It’s an integration with &lt;a href=&#34;https://atom.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Atom&lt;/a&gt; with a dedicated compiler, local variables, syntax highlight, autocompletion.&lt;/p&gt;
&lt;p&gt;On Atom, you first need to install the &lt;a href=&#34;https://github.com/h3imdall/ftp-remote-edit&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;ftp-remote-edit&lt;/code&gt;&lt;/a&gt; package.&lt;/p&gt;
&lt;p&gt;Then go to the menu item &lt;code&gt;Packages &amp;gt; Ftp-Remote-Edit &amp;gt; Toggle&lt;/code&gt;.&lt;/p&gt;
&lt;img src=&#34;img/toggle.png&#34; alt=&#34;julia&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;A new &lt;code&gt;Remote&lt;/code&gt; panel will open with the default button to &lt;code&gt;Edit a new server&lt;/code&gt;.&lt;/p&gt;
&lt;img src=&#34;img/edit.png&#34; alt=&#34;julia&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;Click it and you will be able to set up your remote connection.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Press &lt;code&gt;New&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Insert your username in &lt;code&gt;The name of the server&lt;/code&gt;, for example &lt;code&gt;sergiop&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Insert your ip adress in &lt;code&gt;The hostname or IP adress of the server&lt;/code&gt;, for example &lt;code&gt;123.123.123.123&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Select &lt;code&gt;SFTP - SSH File Transfer Protocol&lt;/code&gt; under &lt;code&gt;Protocol&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Select your &lt;code&gt;Logon&lt;/code&gt; option. You can either insert your password every time, just once, or use a keyfile.&lt;/li&gt;
&lt;li&gt;Insert again your username in &lt;code&gt;Username for autentication&lt;/code&gt;, again for example &lt;code&gt;sergiop&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;If you don’t want to start from the root folder, you can change the &lt;code&gt;Initial Directory&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;img/julia.png&#34; alt=&#34;julia&#34;&gt;&lt;/p&gt;
&lt;p&gt;Now you will be able to see your remote directory (named for example &lt;code&gt;sergiop&lt;/code&gt;) in the &lt;code&gt;Remote&lt;/code&gt; panel.&lt;/p&gt;
&lt;p&gt;To start using Julia remotely, just start a new remote Julia process from the menu on the left.&lt;/p&gt;
&lt;img src=&#34;img/remote.png&#34; alt=&#34;julia&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;Now you are ready to deploy your Julia code on your remote server!&lt;/p&gt;
&lt;h2 id=&#34;jupyter-notebooks&#34;&gt;Jupyter Notebooks&lt;/h2&gt;
&lt;p&gt;If you want to have a &lt;a href=&#34;https://jupyter.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Jupyter Notebook&lt;/a&gt; running remotely, the steps are the following. The main advantage of a Jupyter Notebook is that it allows you to mix text and code in a single file, similarly to &lt;a href=&#34;https://rmarkdown.rstudio.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;RMarkdown&lt;/a&gt;, with the advantage of not being contrained to use a R (or Python) kernel. For example, I often use Jupyter Notebook with Julia or Matlab Kernels. Moreover, you can also make nice slides out of it!&lt;/p&gt;
&lt;p&gt;First, connect to the remote machine. Look at &lt;a href=&#34;https://matteocourthoud.github.io/post/remote/#setup&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;section 1&lt;/a&gt; to set up your SSH connection.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;ssh username@ip
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Start a Jupyter Notebook in the remote machine.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;jupyter notebook --no-browser
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The command will open a jupyter notebook in the remote machine. To connect to it, we need to know which port it used. The default port is &lt;code&gt;8888&lt;/code&gt;. If that port is busy, it will look for another available one. We can see the port from the output in terminal.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Jupyter Notebook is running at: http://localhost:XXXX/…&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Where &lt;code&gt;XXXX&lt;/code&gt; is the repote port used.&lt;/p&gt;
&lt;p&gt;Now we need to forward the remote port &lt;code&gt;XXXX&lt;/code&gt; to our local &lt;code&gt;YYYY&lt;/code&gt; port.&lt;/p&gt;
&lt;p&gt;Open a new &lt;em&gt;local&lt;/em&gt; shell. Type&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;ssh -L localhost:YYYY:localhost:XXXX username@ip
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Where &lt;code&gt;YYYY&lt;/code&gt; can be anything. I’d use the default port &lt;code&gt;8888&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;ssh -L localhost:8889:localhost:8888 username@ip
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now go to your browser and type&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;localhost:YYYY
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Which in my case is&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;localhost:8889
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This will open the remote Jupyter Notebook.&lt;/p&gt;
&lt;p&gt;Done!&lt;/p&gt;
&lt;p&gt;In case you want to check which Jupiter notebooks are running, type&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;jupyter notebook list
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To kill a notebook use&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;jupyter notebook stop XXXX
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;sources&#34;&gt;Sources&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://medium.com/@SergioPietri/how-to-setup-and-use-ssh-for-remote-connections-e86556d804dd&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;How To Setup And Use SSH For Remote Connections&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.junolab.org/stable/man/remote/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Connecting to a Julia session on a remote machine&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://ljvmiranda921.github.io/notebook/2018/01/31/running-a-jupyter-notebook/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Running a Jupyter notebook from a remote server&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Curriculum Vitae</title>
      <link>https://matteocourthoud.github.io/cv/</link>
      <pubDate>Wed, 10 Nov 2021 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/cv/</guid>
      <description>&lt;h2 id=&#34;work-experience&#34;&gt;Work Experience&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Senior Applied Scientist&lt;/strong&gt; for &lt;a href=&#34;https://en.zalando.de/?_rfl=de&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Zalando&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;From August 2023&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Technical Writer&lt;/strong&gt; for &lt;a href=&#34;https://medium.com/@matteo.courthoud&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Towards Data Science&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;From April 2022&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Data Science Intern&lt;/strong&gt; for &lt;a href=&#34;https://www.google.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Google&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;August 2022 - December 2022&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;External Consultant&lt;/strong&gt; for &lt;a href=&#34;https://www.amazon.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Amazon&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;2021 - 2022&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Intern&lt;/strong&gt; at EU Commission, Brussels (BE), DG-COMP, Chief Economist Office
&lt;ul&gt;
&lt;li&gt;European Commission, spring 2016&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;education&#34;&gt;Education&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Ph.D.&lt;/strong&gt; Economics, &lt;em&gt;University of Zurich&lt;/em&gt;, 2023&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Visiting Assistant in Research, &lt;em&gt;Yale University&lt;/em&gt;, fall winter 2021-2022&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;M.S.&lt;/strong&gt; Economics and Social Sciences, &lt;em&gt;Bocconi University&lt;/em&gt;, 2016&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;B.S.&lt;/strong&gt; Economics and Social Sciences, &lt;em&gt;Bocconi University&lt;/em&gt;, 2014&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Primary interests&lt;/strong&gt;: Industrial Organization, Econometrics, Causal Inference &lt;br&gt;
&lt;strong&gt;Secondary interests&lt;/strong&gt;: Competition Policy, Machine Learning, Market Design, Behavioral Economics&lt;/p&gt;
&lt;h2 id=&#34;working-papers&#34;&gt;Working Papers&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://matteocourthoud.github.io/project/foreclosure/&#34;&gt;Foreclosure Complementarities: Exclusionary Bundling and Predatory Pricing&lt;/a&gt; (&lt;em&gt;with G. Crawford&lt;/em&gt;) &lt;br&gt;
We use a computational model of industry dynamics to study exclusionary bundling and predatory pricing. We show that the two foreclosure practices are complementary and we investigate policies.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://matteocourthoud.github.io/project/alg_platform/&#34;&gt;Algorithmic Collusion Detection&lt;/a&gt; &lt;br&gt;
I show that pricing algorithms can learn reward-punishment schemes without observing rival’s actions and I propose a model-free test for algorithmic collusion based on observational data.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://matteocourthoud.github.io/project/alg_detection/&#34;&gt;Algorithmic Collusion in Online Marketplaces&lt;/a&gt; &lt;br&gt;
I show that profit maximizing online marketplaces, by algorithmically controlling consumers&amp;rsquo; attention, have the incentives and ability to influence algorithmic pricing collusion.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://matteocourthoud.github.io/project/approximations/&#34;&gt;Approximation Methods for Large Dynamic Stochastic Games&lt;/a&gt; &lt;br&gt;
I compare existing approximation methods to compute Markow Perfect Equilibrium in dynamic stochastic games with large state spaces and propose a new one: games with random order.&lt;/p&gt;
&lt;h2 id=&#34;teaching&#34;&gt;Teaching&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Instructor &lt;a href=&#34;https://matteocourthoud.github.io/course/empirical-io/&#34;&gt;Empirical Industrial Organization&lt;/a&gt; (PhD)
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;University of Zurich&lt;/em&gt;, fall 2021&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;T.A. for &lt;a href=&#34;https://matteocourthoud.github.io/course/ml-econ/&#34;&gt;Machine Learning for Economic Analysis&lt;/a&gt; (MSc)
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;University of Zurich&lt;/em&gt;, fall 2020-21&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Instructor &lt;a href=&#34;https://pp4rs.github.io/2020-uzh/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Programming Practices for Research Economists&lt;/a&gt; (PhD)
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;University of Zurich&lt;/em&gt;, winter 2019&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;T.A. &lt;a href=&#34;https://matteocourthoud.github.io/course/metrics/&#34;&gt;Econometrics for Research Students&lt;/a&gt; (PhD)
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;University of Zurich&lt;/em&gt;, fall 2018-19&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;T.A. Markets, Organizations and Incentives (BSc)
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Bocconi University&lt;/em&gt;, fall 2016&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;T.A. Industrial Organization (BSc)
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Bocconi University&lt;/em&gt;, spring 2017&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;presentations&#34;&gt;Presentations&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;2021&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Asia and Pacific IO Conference (14/12)&lt;/li&gt;
&lt;li&gt;European Winter Meeting of the Econometric Society (13/12)&lt;/li&gt;
&lt;li&gt;Industrial Organization Prospectus Workshop &lt;em&gt;at Yale University&lt;/em&gt; (14/10)&lt;/li&gt;
&lt;li&gt;2nd AI and Policy Conference (14/09)&lt;/li&gt;
&lt;li&gt;Swiss IO Day (10/06)&lt;/li&gt;
&lt;li&gt;Annual Congress of the Swiss Society of Economics and Statistics Microeconomics (14/05)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;2020/19&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Applied Microeconomics Seminar &lt;em&gt;at University of Zürich&lt;/em&gt; (x4)&lt;/li&gt;
&lt;li&gt;Theory Seminar &lt;em&gt;at University of Zürich&lt;/em&gt; (x3)&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;academic-activities&#34;&gt;Academic Activities&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Referee&lt;/strong&gt; for the Journal of Competition Law and Economics&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Research Assistant&lt;/strong&gt; for Paolo Pinotti, Eliana La Ferrara, Anna Gibert
&lt;ul&gt;
&lt;li&gt;Bocconi University, 2016-17&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;honors-awards-and-fellowships&#34;&gt;Honors, Awards, and Fellowships&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Doc.Mobility Scholarship (09/2021)
&lt;ul&gt;
&lt;li&gt;Swiss National Science Foundation&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;1st place, &lt;a href=&#34;https://analytics-club.org/wordpress/datathon/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Machine Learning Datathlon&lt;/a&gt; (05/2021)
&lt;ul&gt;
&lt;li&gt;ETH Zurich, Analytics Club&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Doctoral Scholarship (08/2017)
&lt;ul&gt;
&lt;li&gt;University of Zurich&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;thesis-supervisions&#34;&gt;Thesis Supervisions&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Jannick Sicher (MSc), &amp;ldquo;&lt;em&gt;Adversarial Approach to Algorithmic Collusion Detection&lt;/em&gt;&amp;rdquo;&amp;quot;&lt;/li&gt;
&lt;li&gt;Tim Herter (MSc), &amp;ldquo;&lt;em&gt;The Effect of U.S. Food Aid on Underweight&lt;/em&gt;&amp;rdquo;&lt;/li&gt;
&lt;li&gt;Andrin Pluss (BSc), &amp;ldquo;&lt;em&gt;Merger Simulation and Competition Models&lt;/em&gt;&amp;rdquo;&lt;/li&gt;
&lt;li&gt;Yuming Pan (MSc), &amp;ldquo;&lt;em&gt;The Impact of Free Legal Aid on Criminal Activities&lt;/em&gt;&amp;rdquo;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;skills&#34;&gt;Skills&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Computational&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Computing:&lt;/strong&gt; Python, Julia, R, MATLAB, Stata&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Design:&lt;/strong&gt; LaTeX, Markdown, Tableau, CSS&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Cloud&lt;/strong&gt;: AWS, Redshift&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Other&lt;/strong&gt;: Git, SQL, Unix, Docker&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Languages&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Italian (native)&lt;/li&gt;
&lt;li&gt;English (fluent)&lt;/li&gt;
&lt;li&gt;French (conversational)&lt;/li&gt;
&lt;li&gt;German (beginner)&lt;/li&gt;
&lt;li&gt;Spanish (beginner)&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.econ.uzh.ch/en/people/faculty/crawford.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Gregory Crawford&lt;/a&gt;, University of Zurich&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.econ.uzh.ch/en/people/faculty/schmutzler.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Armin Schmutzler&lt;/a&gt;, University of Zurich&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.econ.uzh.ch/en/people/faculty/kozbur.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Damian Kozbur&lt;/a&gt;, University of Zurich&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://faculty.unibocconi.eu/chiarafumagalli/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Chiara Fumagalli&lt;/a&gt;, Bocconi University&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;contact&#34;&gt;Contact&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;website&lt;/strong&gt; &lt;a href=&#34;https://matteocourthoud.github.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;matteocourthoud.github.io&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;linkedin&lt;/strong&gt; &lt;a href=&#34;https://www.linkedin.com/in/matteo-courthoud/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;matteocourthoud&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;twitter&lt;/strong&gt; &lt;a href=&#34;https://twitter.com/MatteoCourthoud/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;@matteocourthoud&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;github&lt;/strong&gt; &lt;a href=&#34;https://github.com/matteocourthoud&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;matteocourthoud&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Algorithmic Collusion on Online Marketplaces</title>
      <link>https://matteocourthoud.github.io/project/alg_platform/</link>
      <pubDate>Mon, 25 Oct 2021 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/project/alg_platform/</guid>
      <description>&lt;p&gt;The use of algorithms to set prices is particularly popular in online marketplaces, where sellers need to take quick decisions in complex dynamic environments. In this article, I investigate the role of online marketplaces in facilitating or preventing collusion among sellers that use pricing algorithms. In particular, I investigate a platform that has the ability to give prominence to certain products and automates this decision through a reinforcement learning algorithm, that maximizes the platform&amp;rsquo;s profits. Depending on whether the business model of the platform is more aligned with consumer welfare or with sellers&amp;rsquo; profits (e.g., if it collects quantity or profit fees), the platform either prevents or facilitates collusion among algorithmic sellers. If the platform is also active as a seller, the so-called dual role, it is able to both induce sellers to set high prices and appropriate most of the profits. Importantly, self-preferencing only happens during the learning phase and not in equilibrium. I investigate a potential solution: separating the sales and marketplace divisions. The policy is effective but does not fully restore the competitive outcome when the fee is distortive, as in the case of a revenue fee.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>My Color Palette</title>
      <link>https://matteocourthoud.github.io/post/palette/</link>
      <pubDate>Sun, 24 Oct 2021 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/post/palette/</guid>
      <description>&lt;p&gt;Ok, this is a fun post. I am choosing… &lt;strong&gt;my color palette&lt;/strong&gt;!&lt;/p&gt;
&lt;p&gt;I have decided to unify all the color palettes I have on my website, slides, graphs, etc… into a unique universal color palette.&lt;/p&gt;
&lt;h2 id=&#34;main-color&#34;&gt;Main Color&lt;/h2&gt;
&lt;p&gt;First of all, I have to choose my main color.&lt;/p&gt;
&lt;script src=&#34;https://coolors.co/palette-widget/widget.js&#34;&gt;&lt;/script&gt;
&lt;script data-id=&#34;09673912029165208&#34;&gt;new CoolorsPaletteWidget(&#34;09673912029165208&#34;, [&#34;003f5c&#34;,&#34;003f5c&#34;]); &lt;/script&gt;
&lt;p&gt;Here are some shades of it.&lt;/p&gt;
&lt;script src=&#34;https://coolors.co/palette-widget/widget.js&#34;&gt;&lt;/script&gt;
&lt;script data-id=&#34;0683428549461768&#34;&gt;new CoolorsPaletteWidget(&#34;0683428549461768&#34;, [&#34;002637&#34;,&#34;00324a&#34;,&#34;003f5c&#34;,&#34;17506b&#34;,&#34;2c6078&#34;]); &lt;/script&gt;
&lt;h2 id=&#34;related-palettes&#34;&gt;Related Palettes&lt;/h2&gt;
&lt;p&gt;Now I will build a couple of colors palettes based on it.&lt;/p&gt;
&lt;p&gt;The first one, is red oriented.&lt;/p&gt;
&lt;script src=&#34;https://coolors.co/palette-widget/widget.js&#34;&gt;&lt;/script&gt;
&lt;script data-id=&#34;06164154396260932&#34;&gt;new CoolorsPaletteWidget(&#34;06164154396260932&#34;, [&#34;003f5c&#34;,&#34;444e86&#34;,&#34;955196&#34;,&#34;dd5182&#34;,&#34;ff6e54&#34;,&#34;ffa600&#34;]); &lt;/script&gt;
&lt;p&gt;Second one, is green oriented.&lt;/p&gt;
&lt;script src=&#34;https://coolors.co/palette-widget/widget.js&#34;&gt;&lt;/script&gt;
&lt;script data-id=&#34;033203286745601424&#34;&gt;new CoolorsPaletteWidget(&#34;033203286745601424&#34;, [&#34;003f5c&#34;,&#34;00677f&#34;,&#34;00908f&#34;,&#34;2db88b&#34;,&#34;94dc7b&#34;,&#34;f9f871&#34;]); &lt;/script&gt;
&lt;h2 id=&#34;color-sequence&#34;&gt;Color Sequence&lt;/h2&gt;
&lt;p&gt;Now I need a high contrast scheme for graphs. I add one color at the time to check that contrast is always maximized.&lt;/p&gt;
&lt;script src=&#34;https://coolors.co/palette-widget/widget.js&#34;&gt;&lt;/script&gt;       &lt;script data-id=&#34;0605466695047113&#34;&gt;new CoolorsPaletteWidget(&#34;0605466695047113&#34;, [&#34;003f5c&#34;,&#34;ff6e54&#34;,&#34;f9f871&#34;,&#34;2db88b&#34;,&#34;955196&#34;]); &lt;/script&gt;https://coolors.co/003f5c-ff6e54-f9f871-2db88b-955196)
&lt;p&gt;A milder version of the same palette is:&lt;/p&gt;
&lt;script src=&#34;https://coolors.co/palette-widget/widget.js&#34;&gt;&lt;/script&gt;       &lt;script data-id=&#34;007538072748725222&#34;&gt;new CoolorsPaletteWidget(&#34;007538072748725222&#34;, [&#34;00798c&#34;,&#34;d1495b&#34;,&#34;edae49&#34;,&#34;52a369&#34;,&#34;756ab2&#34;]); &lt;/script&gt;https://coolors.co/00798c-d1495b-edae49-52a369-756ab2)</description>
    </item>
    
    <item>
      <title>Foreclosure Complementarities</title>
      <link>https://matteocourthoud.github.io/project/foreclosure/</link>
      <pubDate>Fri, 01 Oct 2021 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/project/foreclosure/</guid>
      <description>&lt;p&gt;In recent merger cases across complementary markets, antitrust authorities have expressed foreclosure concerns. In particular, the presence of scale economies in one market might propagate to the complementary market, ultimately leading to the monopolization of both. In this paper, we investigate the interplay between two foreclosure practices: exclusionary bundling and predatory pricing in the setting of complementary markets with economies of scale. We show that the two practices are complementary when markets display economies of scale, exclusionary bundling is more likely and, when bundling is allowed, predatory pricing is more likely. We show that this outcome is due to exit-inducing behavior of dominant firms: shutting down predatory incentives restores competition in both markets. We investigate different policies: banning mergers between market leaders, allowing product bundling only when more than one firm is integrated and able to offer the bundle, and lastly knowledge sharing across firms in order to limit the economies of scale. All policies are effective, each for a different reason.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>PhD Frequently Asked Questions</title>
      <link>https://matteocourthoud.github.io/post/phd_faq/</link>
      <pubDate>Fri, 01 Oct 2021 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/post/phd_faq/</guid>
      <description>&lt;p&gt;In this page, I collect anquestions that I frequently asked myself during my PhD, possibly with answers.&lt;/p&gt;
&lt;p&gt;Personally, the article for PhD students that helped me the most is &lt;a href=&#34;https://medium.com/@paul.niehaus/doing-research-18cb310529e0&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;“Doing research”&lt;/a&gt; by Paul Niehaus. But beware, it might not work for everyone.&lt;/p&gt;
&lt;h2 id=&#34;starting-the-phd&#34;&gt;Starting the PhD&lt;/h2&gt;
&lt;h3 id=&#34;information&#34;&gt;Information&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.nature.com/articles/d41586-019-03459-7?sf223557541=1&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;“PhDs: the tortuous truth”&lt;/a&gt;, Chris Woolston, 2019.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://medium.economist.com/why-doing-a-phd-is-often-a-waste-of-time-349206f9addb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;“Why doing a PhD is often a waste of time”&lt;/a&gt;, The Economist, 2016&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.theguardian.com/careers/phd-right-career-option&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;“Should you do a PhD?&amp;quot;&lt;/a&gt;, Daniel K. Sokol, 2012.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://tertilt.vwl.uni-mannheim.de/bachelor/GradSchoolGuide.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;“So, you want to go to a grad school in economics?&amp;quot;&lt;/a&gt;, Ceyhun Elgin and Mario Solis-Garcia, 2007.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;applying&#34;&gt;Applying&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://james-tierney.medium.com/how-to-ask-your-professor-for-a-letter-of-recommendation-f06e8b2f2c64&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;“How to Ask Your Professor for a Letter of Recommendation”&lt;/a&gt;, James Tierney, 2020.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://drive.google.com/file/d/16eUvtahziPyBTpX_ZeyXjPck2OyinfHH/view&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;“Pre-Doc Guide”&lt;/a&gt;, Alvin Christian, 2019.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://athey.people.stanford.edu/professional-advice&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;“Advice for Applying to Grad School in Economics”&lt;/a&gt;, Susan Athey, 2016.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://qz.com/116081/the-complete-guide-to-getting-into-an-economics-phd-program/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;“The complete guide to getting into an economics PhD program”&lt;/a&gt;, Miles Kimball, 2013.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.ericzwick.com/public_goods/twelve_steps.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;“The 12 Step Program for Grad School”&lt;/a&gt;, Erik Zwick.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;starting&#34;&gt;Starting&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/hagertynw/grad-school-reflections/blob/master/grad_school_reflections.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;“Reflections on Grad School in Economics”&lt;/a&gt;, Nick Hagerty, 2020.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://law.vanderbilt.edu/phd/How_to_Survive_1st_Year.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;“How to survive your first year of graduate school in economics”&lt;/a&gt;, Matthew Pearson, 2005.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;during-the-phd&#34;&gt;During the PhD&lt;/h2&gt;
&lt;h3 id=&#34;mental-health&#34;&gt;Mental Health&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://scholar.harvard.edu/files/bolotnyy/files/bbb_mentalhealth_paper.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;“Graduate Student Mental Health: Lessons from American Economics Departments”&lt;/a&gt;, Bolotnyy, Valentin, Matthew Basilico, and Paul Barreira, 2021.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.insidehighered.com/news/2019/11/14/phd-student-poll-finds-mental-health-bullying-and-career-uncertainty-are-top&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;“Mental Health, Bullying, Career Uncertainty”&lt;/a&gt;, Colleen Flahert, 2019.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.sciencemag.org/careers/2019/03/how-mindfulness-can-help-phd-students-deal-mental-health-challenges&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;“How mindfulness can help Ph.D. students deal with mental health challenges”&lt;/a&gt;, Katie Langin, 2019.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.phdstudies.com/article/managing-your-mental-health-as-a-phd-student/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;“Managing Your Mental Health as a PhD Student”&lt;/a&gt;, Joanna Hughes, 2019.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.psychologytoday.com/us/blog/emotional-mastery/201904/what-makes-it-so-hard-ask-help&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;“What Makes It So Hard to Ask for Help?&amp;quot;&lt;/a&gt;, Joan Rosenberg, 2019.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.sciencemag.org/careers/2018/11/grad-school-depression-almost-took-me-end-road-i-found-new-start&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;“Grad school depression almost took me to the end of the road—but I found a new start”&lt;/a&gt;, Francis Aguisanda, 2018.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.nature.com/articles/nj7587-555a.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;“Faking it”&lt;/a&gt;, Chris Woolston, 2016.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://blogs.nature.com/naturejobs/2016/09/14/panic-and-a-phd/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;“Panic and a PhD”&lt;/a&gt;, Jack Leeming, 2016.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://qz.com/547641/theres-an-awful-cost-to-getting-a-phd-that-no-one-talks-about/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;“There’s an awful cost to getting a PhD that no one talks about”&lt;/a&gt;, Jennifer Walker, 2015.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;research-and-ideas&#34;&gt;Research and Ideas&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://www.ricardodahis.com/files/papers/Dahis_Advice_Research.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;“Advice for Academic Research”&lt;/a&gt;, Ricardo Dahis, 2021.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.aeaweb.org/articles?id=10.1257/jel.20191573&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;“Sins of Omission and the Practice of Economics”&lt;/a&gt;, George A. Akerlof, 2020.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://medium.com/@paul.niehaus/doing-research-18cb310529e0&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;“Doing research”&lt;/a&gt;, Paul Niehaus, 2019.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://static1.squarespace.com/static/55c143d9e4b0cb07521c6d17/t/5b4f409f575d1ff83c2f12d8/1531920545061/PhDGuidebook.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;“An unofficial guidebook for PhD students in economics and education”&lt;/a&gt;, Alex Eble, 2018.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://iu.box.com/s/0ha9gcq0t22kyyy1rqv15mkmauw1py18&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;“The Research Productivity of New PhDs in Economics: The Surprisingly High Non-Success of the Successful”&lt;/a&gt;, John P. Conley and Ali Sina Önder, 2014.&lt;/li&gt;
&lt;li&gt;[“How to get started on research in economics?&amp;quot;](&lt;a href=&#34;http://econ.lse.ac.uk/staff/spischke/phds/How&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://econ.lse.ac.uk/staff/spischke/phds/How&lt;/a&gt; to start.pdf), Steve Pischke, 2009.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://iu.box.com/s/km7cxhcxgfcdpk4cp38b47x7is7lum11&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;“The Importance of Stupidity in Scientific Research”&lt;/a&gt;, Martin A. Schwartz, 2008.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://stevepavlina.com/blog/2007/01/7-rules-for-maximizing-your-creative-output/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;“7 Rules for Maximizing Your Creative Output”&lt;/a&gt;, Steve Pavlina, 2007.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://people.ischool.berkeley.edu/~hal/Papers/how.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;“How To Build An Economic Model in Your Spare Time”&lt;/a&gt;, Hal. R. Varian, 1998.&lt;/li&gt;
&lt;li&gt;[“Ph.D. Thesis Research: Where do I Start?&amp;quot;](&lt;a href=&#34;http://www.columbia.edu/~drd28/Thesis&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://www.columbia.edu/~drd28/Thesis&lt;/a&gt; Research.pdf), Don Davis.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;presenting&#34;&gt;Presenting&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://david-schindler.de/unfair-questions/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;“Unfair Questions”&lt;/a&gt;, David Schindler, 2021.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/paulgp/beamer-tips/blob/master/slides.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;“Beamer Tips for Presentations”&lt;/a&gt;, Paul Goldsmith-Pinkham, 2020.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.princeton.edu/~reddings/tradephd/public_speaking_for_academic_economists.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;“Public Speaking for Academic Economists”&lt;/a&gt;, Rachel Meager, 2017.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.europeanjobmarketofeconomists.org/uploads/HowToPresent_LaFerrara.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;“How to present your job market paper”&lt;/a&gt;, Eliana La Ferrara, 2018.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://people.bu.edu/guren/Guren_HowToGiveALunchTalk.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;“How To Give a Lunch Talk”&lt;/a&gt;, Adam Guren, 2018.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://chrisblattman.com/2010/02/22/the-discussants-art/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;“The Discussant’s Art”&lt;/a&gt;, Chris Blattman, 2010.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://papers.ssrn.com/sol3/papers.cfm?abstract_id=1332144&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;“How to be a Great Conference Participants”&lt;/a&gt;, Art Carden, 2009.&lt;/li&gt;
&lt;li&gt;[“The “Big 5” and Other Ideas For Presentations”](&lt;a href=&#34;http://econ.lse.ac.uk/staff/spischke/phds/The&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://econ.lse.ac.uk/staff/spischke/phds/The&lt;/a&gt; Big 5.pdf), Cox, Donald, 2000.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://iu.box.com/s/aw92d7kl7xh5s4zsub8jq3qnknq9zcsi&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;“How to Give an Applied Micro Talk”&lt;/a&gt;, Jesse M. Shapiro.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://iu.box.com/s/37j3eip7x9fdg30n4eeepu92228eb999&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;“Tips on How to Avoid Disaster in Presentations”&lt;/a&gt;, Monika Piazzesi.&lt;/li&gt;
&lt;li&gt;[“Seminar Slides “](&lt;a href=&#34;https://www.ssc.wisc.edu/~bhansen/placement/Seminar&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.ssc.wisc.edu/~bhansen/placement/Seminar&lt;/a&gt; Slides.pdf), Bruce Hansen.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;writing&#34;&gt;Writing&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;[“5 Steps Toward a Paper”](&lt;a href=&#34;https://www.google.com/url?q=https%3A%2F%2Fwww.dropbox.com%2Fs%2Fq7wjaidl5w91srt%2FGuest&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.google.com/url?q=https%3A%2F%2Fwww.dropbox.com%2Fs%2Fq7wjaidl5w91srt%2FGuest&lt;/a&gt; lecture FS.pdf%3Fdl%3D0&amp;amp;sa=D&amp;amp;sntz=1&amp;amp;usg=AFQjCNG_nRs6QlkZzWBHAy0PjF4jfEYBAw), Frank Schilbach, 2019.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.nature.com/articles/d41586-019-02918-5&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;“Novelist Cormac McCarthy’s tips on how to write a great science paper”&lt;/a&gt;, Van Savage and Pamela Yeh, 2019.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://marcfbellemare.com/wordpress/12797&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;“The “Middle Bits” Formula for Applied Papers”&lt;/a&gt;, Marc Bellamare, 2018.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://marcfbellemare.com/wordpress/12060&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;“The Conclusion Formula”&lt;/a&gt;, Marc Bellamare, 2018.&lt;/li&gt;
&lt;li&gt;[“The Introduction Formula”](&lt;a href=&#34;https://www.albany.edu/spatial/training/5-The&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.albany.edu/spatial/training/5-The&lt;/a&gt; Introduction Formula.pdf), Keith Head, 2015.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.people.fas.harvard.edu/~pnikolov/resources/writingtips.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;“Writing Tips For Economics Research Papers”&lt;/a&gt;, Plamen Nikolov, 2013.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://economics.harvard.edu/files/economics/files/tenruleswriting.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;“The Ten Most Important Rules of Writing Your Job Market Paper”&lt;/a&gt;, Goldin, Claudia and Lawrence Katz, 2008.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://schwert.ssb.rochester.edu/aec510/phd_paper_writing.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;“Writing Tips for Ph.D. Students”&lt;/a&gt;, John Cochrane, 2005.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://qed.econ.queensu.ca/pub/faculty/sumon/mkremer_checklist_paper.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;“Writing Papers: A Checklist”&lt;/a&gt;, Michael Kremer.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;referiing&#34;&gt;Referiing&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://blog.academicsequitur.com/2019/06/30/how-to-write-a-good-referee-report/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;“How To Write A Good Referee Report”&lt;/a&gt;, Tatyana Deryugina, 2019.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://iu.box.com/s/lgmhqw5uxvrb7qdrhxxskzki9pcwx7o6&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;“How to Review Manuscripts”&lt;/a&gt;, Elsevier, 2015.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://marcfbellemare.com/wordpress/5542&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;“Contributing to Public Goods: My 20 Rules for Refereeing”&lt;/a&gt;, Marc F. Bellemare, 2012&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;finishing-the-phd&#34;&gt;Finishing the PhD&lt;/h2&gt;
&lt;h3 id=&#34;the-job-market&#34;&gt;The Job Market&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.aeaweb.org/content/file?id=869&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;“A Guide and Advice for Economists on the U.S. Junior Academic Job Market 2018-2019 Edition”&lt;/a&gt;, John Cawley, 2018.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://chrisblattman.com/job-market/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;“Academic job market advice for economics, political science, public policy, and other professional schools”&lt;/a&gt;, Blattman, Christopher, 2015.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.ericzwick.com/public_goods/love_the_market.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;“How I Learned to Stop Worrying and Love the Job Market”&lt;/a&gt;, Erik Zwick, 2014.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;the-private-sector&#34;&gt;The Private Sector&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.linkedin.com/pulse/my-journey-from-economics-phd-data-scientist-tech-rose-tan&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;“My Journey from Economics PhD to Data Scientist in Tech”&lt;/a&gt;, Rose Tan, 2021.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://scarlet-chen.medium.com/tech-industry-jobs-for-econ-phds-54a276dda80b&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;“Tech Industry Jobs for Econ PhDs”&lt;/a&gt;, Scarlet Chen, 2020.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://scarlet-chen.medium.com/my-journey-from-econ-phd-to-tech-part-1-interview-prep-networking-d256918410a2&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;“My Journey from Econ PhD to Tech”&lt;/a&gt;, Scarlet Chen, 2020.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.nature.com/articles/d41586-018-05838-y&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;“Why it is not a ‘failure’ to leave academia”&lt;/a&gt;, Philipp Kruger, 2018.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;the-tenure-track&#34;&gt;The Tenure Track&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://blogs.scientificamerican.com/guest-blog/the-awesomest-7-year-postdoc-or-how-i-learned-to-stop-worrying-and-love-the-tenure-track-faculty-life/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;“The Awesomest 7-Year Postdoc or: How I Learned to Stop Worrying and Love the Tenure-Track Faculty Life”&lt;/a&gt;, Radhika Nagpal, 2013.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;more&#34;&gt;More&lt;/h2&gt;
&lt;p&gt;You can find more resources here:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;AEA &lt;a href=&#34;https://www.aeaweb.org/about-aea/committees/cswep/mentoring/reading&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Mentoring Reading Materials&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Johannes Pfeifer &lt;a href=&#34;https://sites.google.com/site/pfeiferecon/job-market-resources&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Job Market Resources&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Kristoph Kronenberg &lt;a href=&#34;https://sites.google.com/view/christoph-kronenberg/home/resources&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Resources&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Patrick Button &lt;a href=&#34;https://www.patrickbutton.com/resources&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Resources&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Ryan Edwards &lt;a href=&#34;http://www.ryanbedwards.com/resources&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Resources&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Jennifer Doleac &lt;a href=&#34;http://jenniferdoleac.com/resources/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Resources&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Amanda Agan &lt;a href=&#34;https://sites.google.com/site/amandayagan/writingadvice&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Writing and Presentation Advice&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Random forum &lt;a href=&#34;http://www.inhe365.com/thread-17506-1-1.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Resource Collection&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Algorithmic Collusion Detection</title>
      <link>https://matteocourthoud.github.io/project/alg_detection/</link>
      <pubDate>Sun, 12 Sep 2021 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/project/alg_detection/</guid>
      <description>&lt;p&gt;Reinforcement learning algorithms are gradually replacing humans in many decision-making processes, such as pricing in high-frequency markets. Recent studies on algorithmic pricing have shown that algorithms can learn sophisticated grim-trigger strategies with the intent of keeping supra-competitive prices. This paper focuses on algorithmic collusion detection. One frequent suggestion is to look at the inputs of the strategies, for example at whether the algorithms condition their prices on previous competitors&amp;rsquo; prices. The first part of the paper shows that this approach might not be sufficient to detect collusion since the algorithms can learn reward-punishment schemes that are fully independent of the rival’s actions. The mechanism that ensures the stability of supra-competitive prices is self-punishment.&lt;/p&gt;
&lt;p&gt;The second part of the paper explores a novel test for algorithmic collusion detection. The test builds on the intuition that as algorithms are able to learn to collude, they might be able to learn to exploit collusive strategies. In fact, since they are not designed to learn sub-game perfect equilibrium strategies, there is the possibility that their strategies could be exploited. When one algorithm is unilaterally retrained, keeping the collusive strategies of its competitor fixed, it learns more profitable strategies. Usually, these strategies are more competitive, but not always. Since this change in strategies happens only when algorithms are colluding, retraining can be used as a test to detect algorithmic collusion.&lt;/p&gt;
&lt;p&gt;To make the test implementable, the last part of the paper studies whether one could get the same insights on collusive behavior using only observational data, from a single algorithm. The result is a unilateral empirical test for algorithmic collusion that does not require any assumptions neither on the algorithms themselves nor on the underlying environment. The key insight is that algorithms, during their learning phase, produce natural experiments that allow an observer to estimate their behavior in counterfactual scenarios. The simulations show that, at least in a controlled experimental setting, the test is extremely successful in detecting algorithmic collusion.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Coding Resources for Social Sciences</title>
      <link>https://matteocourthoud.github.io/post/coding/</link>
      <pubDate>Tue, 24 Aug 2021 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/post/coding/</guid>
      <description>&lt;p&gt;In this page, I collect useful resources for coding for researchers in social sciences. A mention goes to &lt;a href=&#34;https://maxkasy.github.io/home/computationlinks/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Maximilian Kasy&lt;/a&gt; that inspired me to build this page.&lt;/p&gt;
&lt;p&gt;A quick legend:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;📗 book&lt;/li&gt;
&lt;li&gt;🌐 webpage&lt;/li&gt;
&lt;li&gt;📈 charts&lt;/li&gt;
&lt;li&gt;🎥 videos&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;econometrics-and-statistics&#34;&gt;Econometrics and Statistics&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;📗&lt;a href=&#34;https://www.ssc.wisc.edu/~bhansen/econometrics/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Bruce Hansen’s Econometrics&lt;/strong&gt;&lt;/a&gt;: By far the best freely available and regularly updated resource for Econometrics&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;machine-learning&#34;&gt;Machine Learning&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;📗&lt;a href=&#34;https://web.stanford.edu/~hastie/Papers/ESLII.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;The Elements of Statistical Learning&lt;/strong&gt;&lt;/a&gt;: General introduction to machine learning&lt;/li&gt;
&lt;li&gt;📗&lt;a href=&#34;http://www.gaussianprocess.org/gpml/chapters/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Gaussian Processes for Machine Learning&lt;/strong&gt;&lt;/a&gt;: Extremely useful tools for nonparametric Bayesian modeling&lt;/li&gt;
&lt;li&gt;📗&lt;a href=&#34;https://www.deeplearningbook.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Deep Learning&lt;/strong&gt;&lt;/a&gt;: The theory and implementation of neural nets&lt;/li&gt;
&lt;li&gt;📗&lt;a href=&#34;https://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning/understanding-machine-learning-theory-algorithms.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Understanding Machine Learning: From Theory to Algorithms&lt;/strong&gt;&lt;/a&gt;: An introduction to statistical learning theory in the tradition of Vapnik&lt;/li&gt;
&lt;li&gt;📗&lt;a href=&#34;http://www.incompleteideas.net/book/RLbook2018.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Reinforcement Learning - An Introduction&lt;/strong&gt;&lt;/a&gt;: Adaptive learning for Markov decision problems&lt;/li&gt;
&lt;li&gt;📗&lt;a href=&#34;http://jeffe.cs.illinois.edu/teaching/algorithms/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Algorithms&lt;/strong&gt;&lt;/a&gt;: Introduction to the theory of algorithms&lt;/li&gt;
&lt;li&gt;🌐&lt;a href=&#34;https://playground.tensorflow.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Tensorflow Playground&lt;/strong&gt;&lt;/a&gt;: Visualisation tool for neural networks&lt;/li&gt;
&lt;li&gt;🌐&lt;a href=&#34;https://www.fast.ai/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Artificial Intelligence&lt;/strong&gt;&lt;/a&gt;: Online lectures on AI&lt;/li&gt;
&lt;li&gt;🌐&lt;a href=&#34;https://global.oup.com/academic/product/the-ethical-algorithm-9780190948207&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;The Ethical Algorithm&lt;/strong&gt;&lt;/a&gt;: How to impose normative constraints on ML and other algorithms&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;python&#34;&gt;Python&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;🌐&lt;a href=&#34;https://realpython.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;RealPython&lt;/strong&gt;&lt;/a&gt;: Collection of Python tutorials, from introductory to advanced. Also contains &lt;a href=&#34;https://realpython.com/learning-paths/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;learning paths&lt;/a&gt; for specific topics&lt;/li&gt;
&lt;li&gt;🌐&lt;a href=&#34;https://python.quantecon.org/intro.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;QuantEcon Python&lt;/strong&gt;&lt;/a&gt; Tutorials and economic applications in Python, especially for macroeconomics&lt;/li&gt;
&lt;li&gt;🌐&lt;a href=&#34;https://blog.finxter.com/python-cheat-sheets/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Cheat Sheets&lt;/strong&gt;&lt;/a&gt;: Collection of cheat sheets for python&lt;/li&gt;
&lt;li&gt;🌐&lt;a href=&#34;https://docs.python-guide.org/writing/structure/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Structuring a Python project&lt;/strong&gt;&lt;/a&gt;: Advanced tutorial on how to structure a Python program&lt;/li&gt;
&lt;li&gt;🌐&lt;a href=&#34;https://www.softwaretestinghelp.com/python-ide-code-editors/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;IDE Guide&lt;/strong&gt;&lt;/a&gt;: Comparison of IDEs for Python. Suggested: &lt;a href=&#34;https://www.jetbrains.com/pycharm/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PyCharm&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;🌐&lt;a href=&#34;https://www.jetbrains.com/help/pycharm/configuring-remote-interpreters-via-ssh.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Configuring remote interpreters via SSH&lt;/strong&gt;&lt;/a&gt;: How to use Python remotely via SSH via PyCharm&lt;/li&gt;
&lt;li&gt;📈&lt;a href=&#34;https://www.kaggle.com/maheshdadhich/strength-of-visualization-python-visuals-tutorial/notebook&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Visualization in Python&lt;/strong&gt;&lt;/a&gt;: How to make nice graphs in Python, with a dedicated jupyter notebook&lt;/li&gt;
&lt;li&gt;📈&lt;a href=&#34;https://python-graph-gallery.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Python Graph Gallery&lt;/strong&gt;&lt;/a&gt;: Graph examples in Python&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;matlab&#34;&gt;Matlab&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;🌐&lt;a href=&#34;https://mathworks.com/help/matlab/matlab_oop/user-defined-classes.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;User defined classes in Matlab&lt;/strong&gt;&lt;/a&gt;: How to work with classes in Matlab&lt;/li&gt;
&lt;li&gt;🌐&lt;a href=&#34;https://am111.readthedocs.io/en/latest/jmatlab_use.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Julyter Notebooks&lt;/strong&gt;&lt;/a&gt;: How to run a jupyter notebook with Matlab kernel&lt;/li&gt;
&lt;li&gt;📈&lt;a href=&#34;https://www.bradleymonk.com/wp/how-to-make-professional-looking-plots-for-journal-publication-using-matlab-r2014a-and-r2014b/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Graph Tips in Matlab&lt;/strong&gt;&lt;/a&gt; and &lt;a href=&#34;https://mathworks.com/matlabcentral/answers/133372-how-to-make-nice-plots&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;link2&lt;/a&gt;: Suggestions on how to make pretty graphs in Matlab&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;julia&#34;&gt;Julia&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;🌐&lt;a href=&#34;https://docs.julialang.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Julia Manual&lt;/strong&gt;&lt;/a&gt;: Julia unfortunately lacks a big community and tutorials, but it has a very good manual&lt;/li&gt;
&lt;li&gt;🌐&lt;a href=&#34;https://julia.quantecon.org/index_toc.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;QuantEcon Julia&lt;/strong&gt;&lt;/a&gt; Tutorials and economic applications in Julia, especially for macroeconomics&lt;/li&gt;
&lt;li&gt;🌐&lt;a href=&#34;https://medium.com/dev-genius/what-is-the-best-ide-for-developing-in-the-programming-language-julia-484c913f07bc&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;IDE Guide&lt;/strong&gt;&lt;/a&gt;: Guide for IDEs for Julia. Suggested: Juno for Atom.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;r&#34;&gt;R&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;📗&lt;a href=&#34;https://cran.r-project.org/doc/manuals/r-release/R-intro.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;An Introduction to R&lt;/strong&gt;&lt;/a&gt;: Complete introduction to base R&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;📗&lt;a href=&#34;http://r4ds.had.co.nz/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;R for Data Science&lt;/strong&gt;&lt;/a&gt; Introduction to data analysis using R, focused on the tidyverse packages&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;📗&lt;a href=&#34;https://adv-r.hadley.nz/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Advanced R&lt;/strong&gt;&lt;/a&gt;: In depth discussion of programming in R&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;📗&lt;a href=&#34;https://bradleyboehmke.github.io/HOML/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Hands-On Machine Learning with R&lt;/strong&gt;&lt;/a&gt;: Fitting ML models in R&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;🌐&lt;a href=&#34;https://github.com/stan-dev/rstan/wiki/RStan-Getting-Started&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Bayesian statistics using Stan&lt;/strong&gt;&lt;/a&gt; and &lt;a href=&#34;https://mc-stan.org/docs/2_20/stan-users-guide/index.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;link&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;🌐&lt;a href=&#34;https://www.rstudio.com/resources/cheatsheets/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;RStudio Cheat Sheets&lt;/strong&gt;&lt;/a&gt; for various extensions, including data processing, visualization, writing web apps, …&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;📈&lt;a href=&#34;https://www.r-graph-gallery.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;R Graph Gallery&lt;/strong&gt;&lt;/a&gt;: Graph examples in R&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;📈&lt;a href=&#34;https://www.christophenicault.com/pages/visualizations/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Nice Graphs with code&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;A collection of elaborate graphs with code in R&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;others&#34;&gt;Others&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;📗&lt;a href=&#34;https://git-scm.com/book/en/v2&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Github Advanced&lt;/strong&gt;&lt;/a&gt;: Advanced guide for version control with Github&lt;/li&gt;
&lt;li&gt;🎥&lt;a href=&#34;https://missing.csail.mit.edu/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;The Missing Semester of Your CS Education&lt;/strong&gt;&lt;/a&gt; Video lectures and notes on tools for computer scientists (version control, debugging, …)&lt;/li&gt;
&lt;li&gt;📈&lt;a href=&#34;http://pgfplots.sourceforge.net/gallery.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;PGF plots in Latex&lt;/strong&gt;&lt;/a&gt;: Gallery and examples to make plots directly in Latex&lt;/li&gt;
&lt;li&gt;🌐&lt;a href=&#34;https://medium.com/@SergioPietri/how-to-setup-and-use-ssh-for-remote-connections-e86556d804dd&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Work remotely from server&lt;/strong&gt;&lt;/a&gt;: How to setup SSH for remote computing&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>How to access WRDS in Python</title>
      <link>https://matteocourthoud.github.io/post/wrds/</link>
      <pubDate>Tue, 09 Mar 2021 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/post/wrds/</guid>
      <description>&lt;p&gt;In this page, I explain how to work with the WRDS database using Python.&lt;/p&gt;
&lt;h2 id=&#34;setup&#34;&gt;Setup&lt;/h2&gt;
&lt;p&gt;The first thing we need to do, is to set up a connection to the &lt;a href=&#34;https://wrds-www.wharton.upenn.edu/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;WRDS database&lt;/a&gt;. I am assuming you have credentials to log in. Check the &lt;a href=&#34;https://wrds-www.wharton.upenn.edu/login/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;log in page&lt;/a&gt; to make sure.&lt;/p&gt;
&lt;p&gt;The second requirement is the &lt;a href=&#34;https://pypi.org/project/wrds/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;wrds&lt;/a&gt; Python package.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pip3 install wrds
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, in order to connect to the WRDS database, you just need to run the following commang in Python.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import wrds
db = wrds.Connection() 
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then, you will be propted to input your WRDS username and password.&lt;/p&gt;
&lt;p&gt;However, if you are using a Python IDE such as &lt;a href=&#34;https://www.jetbrains.com/pycharm/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PyCharm&lt;/a&gt;, you cannot run the command from the Python Console. Moreover, you might want to save your credentials once and for all, so that you don’t have to log in every time.&lt;/p&gt;
&lt;p&gt;First, walk to your home directory from the Terminal (&lt;code&gt;/Users/username&lt;/code&gt;).&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;cd
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now create an empty &lt;code&gt;.pgpass&lt;/code&gt; file.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;touch .pgpass
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now you write &lt;code&gt;your_username&lt;/code&gt; and &lt;code&gt;your_password&lt;/code&gt; into the &lt;code&gt;.pgpass&lt;/code&gt; file.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;echo &amp;quot;wrds-pgdata.wharton.upenn.edu:9737:wrds:your_username:your_password&amp;quot; &amp;gt;&amp;gt; .pgpass
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You also need to restrict permissions to the file.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;chmod 600 ~/.pgpass
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now you can go back to your Python IDE and access the database by just inputing your username.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import wrds
db = wrds.Connection(wrds_username=&#39;your_username&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If everything works, you should see the following output.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Loading library list...
Done
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;query&#34;&gt;Query&lt;/h2&gt;
&lt;p&gt;The available functions are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;db.connection()&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;db.list_libraries()&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;db.list_tables()&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;db.get_table()&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;db.describe_table()&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;db.raw_sql()&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;db.close()&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I make a simple example of how they work. Suppose first you want to list all the libraries in the WRDS database.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;db.list_libraries()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then you can list all the datasets within a given library.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;db.list_tables(library=&#39;comp&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Before downloading a table, you can describe it.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;df = db.describe_table(library=&#39;comp&#39;, table=&#39;funda&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To download the dataset you can use the &lt;code&gt;get_table()&lt;/code&gt; function.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;df = db.get_table(library=&#39;comp&#39;, table=&#39;funda&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can restrict both the rows and the columns you want to query.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;df_short = db.get_table(library=&#39;comp&#39;, table=&#39;funda&#39;, columns = [&#39;conm&#39;, &#39;gvkey&#39;, &#39;cik&#39;], obs=5)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can also query the database directly using SQL.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;df_sql = db.raw_sql(&#39;&#39;&#39;select conm, gvkey, cik FROM comp.funda WHERE fyear&amp;gt;2010 AND (indfmt=&#39;INDL&#39;)&#39;&#39;&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;sources&#34;&gt;Sources&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://wrds-www.wharton.upenn.edu/pages/support/programming-wrds/programming-python/querying-wrds-data-python/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Querying WRDS Data using Python&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://wrds-www.wharton.upenn.edu/documents/1443/wrds_connection.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Using Python on WRDS Platform&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://sites.duke.edu/kevinstandridge/2020/03/07/introduction-to-the-wrds-python-package/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Introduction to the WRDS Python Package&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://wizardkingz.github.io/wrdsdataaccesspython-tutorial/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;WRDS Data Access Via Python API&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Approximation Methods for Large Dynamic Stochastic Games</title>
      <link>https://matteocourthoud.github.io/project/approximations/</link>
      <pubDate>Fri, 01 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/project/approximations/</guid>
      <description>&lt;p&gt;Dynamic stochastic games notoriously suffer from a curse of dimensionality that makes computing the Markov Perfect Equilibrium of large games infeasible. This article compares the existing approximation methods and alternative equilibrium concepts that have been proposed in the literature to overcome this problem. No method clearly dominates the others but some are dominated in all dimensions. In general, alternative equilibrium concepts outperform sampling-based approximation methods. I propose a new game structure, games with random order, in which players move sequentially and the order of play is unknown. The Markov Perfect equilibrium of this game consistently outperforms all existing approximation methods in terms of approximation accuracy while still being extremely efficient in terms of computational time.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Summer Schools in Economics</title>
      <link>https://matteocourthoud.github.io/post/summer_schools/</link>
      <pubDate>Tue, 10 Mar 2020 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/post/summer_schools/</guid>
      <description>&lt;p&gt;In this page, I collect information about summer schools in Economics.&lt;/p&gt;
&lt;p&gt;If you know about summer schools that are missing from this list, please either &lt;a href=&#34;mailto:matteo.courthoud@econ.uzh.ch&#34;&gt;contact me&lt;/a&gt; or &lt;a href=&#34;https://github.com/matteocourthoud/website/blob/master/content/post/summerschools/index.md&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;edit the table on Github&lt;/a&gt;!&lt;/p&gt;
&lt;h2 id=&#34;2020&#34;&gt;2020&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Name and Link&lt;/th&gt;
&lt;th&gt;Field&lt;/th&gt;
&lt;th&gt;Organizer&lt;/th&gt;
&lt;th&gt;Location&lt;/th&gt;
&lt;th&gt;Instructor(s)&lt;/th&gt;
&lt;th&gt;Dates&lt;/th&gt;
&lt;th&gt;Deadline&lt;/th&gt;
&lt;th&gt;Fee&lt;/th&gt;
&lt;th&gt;Aid&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://dseconf.org/dse2020&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Dynamic Structural Econometrics&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Econometrics, IO&lt;/td&gt;
&lt;td&gt;Econometrics Society&lt;/td&gt;
&lt;td&gt;Zurich&lt;/td&gt;
&lt;td&gt;John Rust et al.&lt;/td&gt;
&lt;td&gt;June 15-21&lt;/td&gt;
&lt;td&gt;March 15&lt;/td&gt;
&lt;td&gt;500$&lt;/td&gt;
&lt;td&gt;no&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://www.cresse.info/default.aspx?articleID=3398&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CRESSE&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Comp. Policy, IO&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;Crete&lt;/td&gt;
&lt;td&gt;various&lt;/td&gt;
&lt;td&gt;June 20 - July 02&lt;/td&gt;
&lt;td&gt;FCFS&lt;/td&gt;
&lt;td&gt;3200€&lt;/td&gt;
&lt;td&gt;-30%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://www.barcelonagse.eu/study/summer-school/digital-economy&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Digital Economy&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Comp. Policy, IO&lt;/td&gt;
&lt;td&gt;Barcelona GSE&lt;/td&gt;
&lt;td&gt;Barcelona&lt;/td&gt;
&lt;td&gt;Martin Peitz&lt;/td&gt;
&lt;td&gt;July 13-17&lt;/td&gt;
&lt;td&gt;March 10&lt;/td&gt;
&lt;td&gt;550€&lt;/td&gt;
&lt;td&gt;maybe&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://www.parisschoolofeconomics.eu/en/teaching/pse-summer-school/social-networks-platforms/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Social Networks, Platforms…&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Comp. Policy, IO&lt;/td&gt;
&lt;td&gt;PSE&lt;/td&gt;
&lt;td&gt;Paris&lt;/td&gt;
&lt;td&gt;from Paris&lt;/td&gt;
&lt;td&gt;June 15-19&lt;/td&gt;
&lt;td&gt;March 31&lt;/td&gt;
&lt;td&gt;1200€&lt;/td&gt;
&lt;td&gt;no&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;2019&#34;&gt;2019&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Name and Link&lt;/th&gt;
&lt;th&gt;Field&lt;/th&gt;
&lt;th&gt;Organizer&lt;/th&gt;
&lt;th&gt;Location&lt;/th&gt;
&lt;th&gt;Instructor(s)&lt;/th&gt;
&lt;th&gt;Dates&lt;/th&gt;
&lt;th&gt;Deadline&lt;/th&gt;
&lt;th&gt;Fee&lt;/th&gt;
&lt;th&gt;Aid&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://dseconf.org/dse2019&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Dynamic Structural Econometrics&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Econometrics, IO&lt;/td&gt;
&lt;td&gt;Econometrics Society&lt;/td&gt;
&lt;td&gt;Chicago&lt;/td&gt;
&lt;td&gt;John Rust et al.&lt;/td&gt;
&lt;td&gt;July 08-14&lt;/td&gt;
&lt;td&gt;March 15&lt;/td&gt;
&lt;td&gt;500$&lt;/td&gt;
&lt;td&gt;no&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;CRESSE&lt;/td&gt;
&lt;td&gt;Competition Policy, IO&lt;/td&gt;
&lt;td&gt;various&lt;/td&gt;
&lt;td&gt;Crete&lt;/td&gt;
&lt;td&gt;various&lt;/td&gt;
&lt;td&gt;June 20 - July 02&lt;/td&gt;
&lt;td&gt;FCFS&lt;/td&gt;
&lt;td&gt;3200€&lt;/td&gt;
&lt;td&gt;-30%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://www.cemfi.es/studies/css/course.asp?cu=10&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Empirical Analysis of Firm Performance&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;IO, Trade&lt;/td&gt;
&lt;td&gt;CEMFI&lt;/td&gt;
&lt;td&gt;Madrid&lt;/td&gt;
&lt;td&gt;Jan de Loecker&lt;/td&gt;
&lt;td&gt;August 19-23&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://www.cemfi.es/studies/css/course.asp?cu=16&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Panel Data Econometrics&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Econometrics&lt;/td&gt;
&lt;td&gt;CEMFI&lt;/td&gt;
&lt;td&gt;Madrid&lt;/td&gt;
&lt;td&gt;Steve Bond&lt;/td&gt;
&lt;td&gt;September 02-06&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;2018&#34;&gt;2018&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Name and Link&lt;/th&gt;
&lt;th&gt;Field&lt;/th&gt;
&lt;th&gt;Organizer&lt;/th&gt;
&lt;th&gt;Location&lt;/th&gt;
&lt;th&gt;Instructor(s)&lt;/th&gt;
&lt;th&gt;Dates&lt;/th&gt;
&lt;th&gt;Deadline&lt;/th&gt;
&lt;th&gt;Fee&lt;/th&gt;
&lt;th&gt;Aid&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://www.econ.ku.dk/cce/events/summerschool/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Dynamic Structural Models&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Econometrics, IO&lt;/td&gt;
&lt;td&gt;University of Copenhagen&lt;/td&gt;
&lt;td&gt;Copenhagen&lt;/td&gt;
&lt;td&gt;John Rust et al.&lt;/td&gt;
&lt;td&gt;May 28 - Jun 03&lt;/td&gt;
&lt;td&gt;March 15&lt;/td&gt;
&lt;td&gt;600€&lt;/td&gt;
&lt;td&gt;no&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://www.cemfi.es/studies/css/course_previous_years.asp?c=12&amp;amp;y=2018&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Empirical Analysis of Innovation in Oligopoly Industries&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
</description>
    </item>
    
    <item>
      <title>Ownership and Product Similarity</title>
      <link>https://matteocourthoud.github.io/project/ownership/</link>
      <pubDate>Sat, 01 Feb 2020 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/project/ownership/</guid>
      <description>&lt;p&gt;I generate a time-varying measure of S&amp;amp;P500 firm similarity using a zero-shot clustering model. The model takes as input BERT embeddings of product descriptions and is trained on market definitions from the EU commission. The objective is to estimate the causal effect of common ownership on product similarity.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Slides</title>
      <link>https://matteocourthoud.github.io/slides/example/</link>
      <pubDate>Tue, 05 Feb 2019 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/slides/example/</guid>
      <description>&lt;h1 id=&#34;create-slides-in-markdown-with-wowchemy&#34;&gt;Create slides in Markdown with Wowchemy&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;https://wowchemy.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Wowchemy&lt;/a&gt; | &lt;a href=&#34;https://owchemy.com/docs/managing-content/#create-slides&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Documentation&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;features&#34;&gt;Features&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Efficiently write slides in Markdown&lt;/li&gt;
&lt;li&gt;3-in-1: Create, Present, and Publish your slides&lt;/li&gt;
&lt;li&gt;Supports speaker notes&lt;/li&gt;
&lt;li&gt;Mobile friendly slides&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;controls&#34;&gt;Controls&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Next: &lt;code&gt;Right Arrow&lt;/code&gt; or &lt;code&gt;Space&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Previous: &lt;code&gt;Left Arrow&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Start: &lt;code&gt;Home&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Finish: &lt;code&gt;End&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Overview: &lt;code&gt;Esc&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Speaker notes: &lt;code&gt;S&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Fullscreen: &lt;code&gt;F&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Zoom: &lt;code&gt;Alt + Click&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/hakimel/reveal.js#pdf-export&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PDF Export&lt;/a&gt;: &lt;code&gt;E&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;code-highlighting&#34;&gt;Code Highlighting&lt;/h2&gt;
&lt;p&gt;Inline code: &lt;code&gt;variable&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Code block:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;porridge = &amp;quot;blueberry&amp;quot;
if porridge == &amp;quot;blueberry&amp;quot;:
    print(&amp;quot;Eating...&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h2 id=&#34;math&#34;&gt;Math&lt;/h2&gt;
&lt;p&gt;In-line math: $x + y = z$&lt;/p&gt;
&lt;p&gt;Block math:&lt;/p&gt;
&lt;p&gt;$$
f\left( x \right) = ;\frac{{2\left( {x + 4} \right)\left( {x - 4} \right)}}{{\left( {x + 4} \right)\left( {x + 1} \right)}}
$$&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;fragments&#34;&gt;Fragments&lt;/h2&gt;
&lt;p&gt;Make content appear incrementally&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;{{% fragment %}} One {{% /fragment %}}
{{% fragment %}} **Two** {{% /fragment %}}
{{% fragment %}} Three {{% /fragment %}}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Press &lt;code&gt;Space&lt;/code&gt; to play!&lt;/p&gt;
&lt;span class=&#34;fragment &#34; &gt;
   One 
&lt;/span&gt;
&lt;span class=&#34;fragment &#34; &gt;
   **Two** 
&lt;/span&gt;
&lt;span class=&#34;fragment &#34; &gt;
   Three 
&lt;/span&gt;
&lt;hr&gt;
&lt;p&gt;A fragment can accept two optional parameters:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;class&lt;/code&gt;: use a custom style (requires definition in custom CSS)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;weight&lt;/code&gt;: sets the order in which a fragment appears&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;speaker-notes&#34;&gt;Speaker Notes&lt;/h2&gt;
&lt;p&gt;Add speaker notes to your presentation&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-markdown&#34;&gt;{{% speaker_note %}}
- Only the speaker can read these notes
- Press `S` key to view
{{% /speaker_note %}}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Press the &lt;code&gt;S&lt;/code&gt; key to view the speaker notes!&lt;/p&gt;
&lt;aside class=&#34;notes&#34;&gt;
  &lt;ul&gt;
&lt;li&gt;Only the speaker can read these notes&lt;/li&gt;
&lt;li&gt;Press &lt;code&gt;S&lt;/code&gt; key to view&lt;/li&gt;
&lt;/ul&gt;

&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;themes&#34;&gt;Themes&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;black: Black background, white text, blue links (default)&lt;/li&gt;
&lt;li&gt;white: White background, black text, blue links&lt;/li&gt;
&lt;li&gt;league: Gray background, white text, blue links&lt;/li&gt;
&lt;li&gt;beige: Beige background, dark text, brown links&lt;/li&gt;
&lt;li&gt;sky: Blue background, thin dark text, blue links&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;ul&gt;
&lt;li&gt;night: Black background, thick white text, orange links&lt;/li&gt;
&lt;li&gt;serif: Cappuccino background, gray text, brown links&lt;/li&gt;
&lt;li&gt;simple: White background, black text, blue links&lt;/li&gt;
&lt;li&gt;solarized: Cream-colored background, dark green text, blue links&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;/media/boards.jpg&#34;
  &gt;

&lt;h2 id=&#34;custom-slide&#34;&gt;Custom Slide&lt;/h2&gt;
&lt;p&gt;Customize the slide style and background&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-markdown&#34;&gt;{{&amp;lt; slide background-image=&amp;quot;/media/boards.jpg&amp;quot; &amp;gt;}}
{{&amp;lt; slide background-color=&amp;quot;#0000FF&amp;quot; &amp;gt;}}
{{&amp;lt; slide class=&amp;quot;my-style&amp;quot; &amp;gt;}}
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h2 id=&#34;custom-css-example&#34;&gt;Custom CSS Example&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s make headers navy colored.&lt;/p&gt;
&lt;p&gt;Create &lt;code&gt;assets/css/reveal_custom.css&lt;/code&gt; with:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-css&#34;&gt;.reveal section h1,
.reveal section h2,
.reveal section h3 {
  color: navy;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h1 id=&#34;questions&#34;&gt;Questions?&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/wowchemy/wowchemy-hugo-modules/discussions&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Ask&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://wowchemy.com/docs/managing-content/#create-slides&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Documentation&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Privacy Policy</title>
      <link>https://matteocourthoud.github.io/privacy/</link>
      <pubDate>Thu, 28 Jun 2018 00:00:00 +0100</pubDate>
      <guid>https://matteocourthoud.github.io/privacy/</guid>
      <description>&lt;p&gt;&lt;strong&gt;My website does not host third-party cookies&lt;/strong&gt; and hosts three first-party cookies just to generally understand the audience of the website. The cookies are from Google Analytics.&lt;/p&gt;
&lt;p&gt;You do not know what cookies are? You would like to learn more details? More information below.&lt;/p&gt;
&lt;h2 id=&#34;cookies-intro&#34;&gt;Cookies Intro&lt;/h2&gt;
&lt;h3 id=&#34;what-is-a-cookie&#34;&gt;What is a cookie?&lt;/h3&gt;
&lt;p&gt;A cookie is a small text file that a server (website) passes to a client (your computer) through your web browser (most likely Chrome). This small text file is then stored on your computer. The browser may &lt;strong&gt;store&lt;/strong&gt; the cookie and &lt;strong&gt;send it&lt;/strong&gt; with later requests.&lt;/p&gt;
&lt;p&gt;A cookie &lt;strong&gt;is not a program&lt;/strong&gt;. It doesn’t perform a function. It’s just text. You can open and read cookies with a basic word processor. The two most common types of cookies are &lt;strong&gt;first-party cookies&lt;/strong&gt; and &lt;strong&gt;third-party&lt;/strong&gt; &lt;strong&gt;cookies&lt;/strong&gt;. Before arriving to that, we first need to understand how are cookies generated, and how they are shared and with whom.&lt;/p&gt;
&lt;h3 id=&#34;how-are-cookies-generated&#34;&gt;How are cookies generated?&lt;/h3&gt;
&lt;p&gt;When you open a website, you send an HTTP request to a server, which replies with a response. After receiving an HTTP request, a server can send one or more &lt;code&gt;Set-Cookie&lt;/code&gt; headers with the response. The browser usually stores the cookie and sends it with requests made to the same server inside a &lt;code&gt;Cookie&lt;/code&gt; HTTP header.&lt;/p&gt;
&lt;p&gt;For example, this instructs the server sending headers to tell the client to store a pair of cookies&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;HTTP/2.0 200 OK
Content-Type: text/html
Set-Cookie: yummy_cookie=choco
Set-Cookie: tasty_cookie=strawberry

[page content]
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;what-do-cookies-do&#34;&gt;What do cookies do?&lt;/h3&gt;
&lt;p&gt;Once a cookies is stored in your browser, with every subsequent request to the server, the browser sends all previously stored cookies back to the server using the &lt;code&gt;Cookie&lt;/code&gt; header.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;GET /sample_page.html HTTP/2.0
Host: www.example.org
Cookie: yummy_cookie=choco; tasty_cookie=strawberry
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There are restrictions to which cookies get sent to which servers. More details below.&lt;/p&gt;
&lt;h2 id=&#34;cookies-content&#34;&gt;Cookies Content&lt;/h2&gt;
&lt;p&gt;A cookie has the following attributes:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;Expires&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Max-Age&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;SameSite&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Domain&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Path&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;HTTPOnly&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Secure&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;how-long-are-cookies-stored&#34;&gt;How long are cookies stored?&lt;/h3&gt;
&lt;p&gt;The &lt;code&gt;Expires&lt;/code&gt; and &lt;code&gt;Max-Age&lt;/code&gt; attributes define the lifetime of a cookie: how long a cookie is stored in your browser.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;Expires&lt;/code&gt; indicates the maximum lifetime of the cookie. If unspecified, the cookie becomes a session cookie. A session finishes when the client shuts down, after which the session cookie is removed.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Max-Age&lt;/code&gt; indicates the number of seconds until the cookie expires. A zero or negative number will expire the cookie immediately. If both &lt;code&gt;Expires&lt;/code&gt; and &lt;code&gt;Max-Age&lt;/code&gt; are set, &lt;code&gt;Max-Age&lt;/code&gt; has precedence.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;who-are-my-cookes-sent-to&#34;&gt;Who are my cookes sent to?&lt;/h3&gt;
&lt;p&gt;The &lt;code&gt;SameSite&lt;/code&gt;, &lt;code&gt;Domain&lt;/code&gt; and &lt;code&gt;Path&lt;/code&gt; attributes define the &lt;em&gt;scope&lt;/em&gt; of a cookie: what URLs the cookies should be sent to.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;The &lt;code&gt;SameSite&lt;/code&gt; attribute controls whether or not a cookie is sent with cross-origin requests, providing some protection against cross-site request forgery attacks.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The &lt;code&gt;Domain&lt;/code&gt; attribute specifies which hosts can receive a cookie. If unspecified, the attribute defaults to the same &lt;a href=&#34;https://developer.mozilla.org/en-US/docs/Glossary/Host&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;host&lt;/a&gt; that set the cookie, &lt;em&gt;excluding subdomains&lt;/em&gt;. If &lt;code&gt;Domain&lt;/code&gt; &lt;em&gt;is&lt;/em&gt; specified, then subdomains are always included. Therefore, specifying &lt;code&gt;Domain&lt;/code&gt; is less restrictive than omitting it. However, it can be helpful when subdomains need to share information about a user. For example, if you set &lt;code&gt;Domain=mozilla.org&lt;/code&gt;, cookies are available on subdomains like &lt;code&gt;developer.mozilla.org&lt;/code&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The &lt;code&gt;Path&lt;/code&gt; attribute indicates a URL path that must exist in the requested URL in order to send the &lt;code&gt;Cookie&lt;/code&gt; header.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;are-my-cookies-safe&#34;&gt;Are my cookies safe?&lt;/h3&gt;
&lt;p&gt;The &lt;code&gt;HTTPOnly&lt;/code&gt; and &lt;code&gt;Secure&lt;/code&gt; attributes help protecting your cookies from malicious server and client attacks, respectively.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Setting the &lt;code&gt;HTTPOnly&lt;/code&gt; attribute makes sure that no client-side scripts are not allowed to access the cookie, i.e. no app or program on your machine can read your cookies. This protects cookies from Cross Site Scripting attacks that can be used to steal cookies with the help of client-side scripts.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The &lt;code&gt;Secure&lt;/code&gt; attribute makes sure that the cookie will only be sent with requests made over an encrypted connection and an attacker won&amp;rsquo;t be able to steal cookies by &lt;em&gt;sniffing&lt;/em&gt;. Sniffing can be defined as passively reading data that is being transmitted. In order to overcome this problem, we encrypt data before transmission. Encryption of data ensures that any potential attacker who sniffs traffic will not be able to steal clear text data, thus ensuring their safety.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;cookies-purpose&#34;&gt;Cookies Purpose&lt;/h2&gt;
&lt;p&gt;Cookies are mainly used for three purposes:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Session management&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;Logins, shopping carts, game scores, or anything else the server should remember&lt;/li&gt;
&lt;li&gt;E.g. you add something to che cart (without logging in!) and when you come back the cart is not empty? That&amp;rsquo;s a session management cookie!&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Personalization&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;User preferences, themes, and other settings&lt;/li&gt;
&lt;li&gt;E.g. you put a website in dark mode and when you come back is still dark? That&amp;rsquo;s a personalization cookie!&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Tracking&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;Recording and analyzing user behavior&lt;/li&gt;
&lt;li&gt;You search for a product and on a different website you see an ad for it? That&amp;rsquo;s a tracking cookie!&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;first-party-cookies&#34;&gt;First-Party Cookies&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Session management and personalization cookies&lt;/strong&gt; cookies are always first-party cookies. Few people have problems with first-party cookies because they’re typically used to store information that could be useful to &lt;strong&gt;enhance the user experience&lt;/strong&gt; later on. Some example of information and its purpose are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Logins&lt;/strong&gt;: so that you don&amp;rsquo;t have to log in every website every time you visit them&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Searches&lt;/strong&gt;: so that you get suggesions for past searches&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Settings&lt;/strong&gt;: so that you don&amp;rsquo;t have to change them every time&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Game scores&lt;/strong&gt;: so that you can compare your new scores to the old ones&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;third-party-cookies&#34;&gt;Third Party Cookies&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Tracking cookies&lt;/strong&gt; are almost always third-party cookies. They collect and share user data through networks of websites, often without the user’s consent. These networks aggregate and sync countless data points. In the end, they know more about you than you expect.&lt;/p&gt;
&lt;p&gt;Look at it like this: You visit three websites - A, B, and C. On website A, you take some action that signals you want to buy running shoes. On website B, you do something that indicates you are a man (maybe you browse the men’s section). On website C, you see an ad for men’s running shoes, even though you haven’t given that site any information yet. You wouldn’t expect Website C to know anything about you, but the cookies saved on your computer from other websites provide it with plenty of information.&lt;/p&gt;
&lt;p&gt;As cookie use became sophisticated, users became less comfortable. The first time you browse for a product on Amazon and then see an ad for it on Facebook is unsettling. It’s a clear sign that your Internet habits aren’t as anonymous as you thought.&lt;/p&gt;
&lt;h2 id=&#34;regulation&#34;&gt;Regulation&lt;/h2&gt;
&lt;p&gt;Legislation or regulations that cover the use of cookies include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The General Data Privacy Regulation (GDPR) in the European Union&lt;/li&gt;
&lt;li&gt;The ePrivacy Directive in the EU&lt;/li&gt;
&lt;li&gt;The California Consumer Privacy Act&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;These regulations have global reach. They apply to any site on the &lt;em&gt;World Wide&lt;/em&gt; Web that users from these jurisdictions access (the EU and California, with the caveat that California&amp;rsquo;s law applies only to entities with gross revenue over 25 million USD, among things).&lt;/p&gt;
&lt;h3 id=&#34;gdpr&#34;&gt;GDPR&lt;/h3&gt;
&lt;p&gt;The &lt;a href=&#34;https://www.osano.com/articles/gdpr-guide&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;General Data Protection Regulation (GDPR)&lt;/a&gt; and &lt;a href=&#34;https://www.osano.com/articles/eprivacy-guide&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ePrivacy &lt;/a&gt;Directive are the strongest examples of this. These EU laws treat cookies as “personal data,” which makes them subject to regulation. Any website that serves EU residents must collect consent from users before serving any non-essential cookies to the user’s device.&lt;/p&gt;
&lt;h3 id=&#34;others&#34;&gt;Others&lt;/h3&gt;
&lt;p&gt;The EU’s efforts to protect personal data ignited a global trend, and that’s changing the data privacy landscape. Other jurisdictions have passed or are working on passing their own data privacy initiatives.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The &lt;a href=&#34;https://www.osano.com/articles/ccpa-guide&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;California Consumer Privacy Act (CCPA)&lt;/a&gt; gives California residents the right to know the types of personal information organizations collect about them and the right to prohibit the sale of their personal information to other parties. (It’s a big law with other data security measures as well.)&lt;/li&gt;
&lt;li&gt;The &lt;a href=&#34;https://www.osano.com/articles/lgpd-enforcement-begins&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Brazilian General Data Protection Law (LGPD)&lt;/a&gt; is an entirely new legal framework in Brazil to protect personal information. Users must consent to the use of third-party cookies when data is transferred.&lt;/li&gt;
&lt;li&gt;The Vermont Act 171 of 2018 Data Broker Regulation requires data brokers (businesses that collect and sell data on individuals they don’t have a relationship with) to register with the state, provide users with an opt-out mechanism, and comply with a list of security requirements.&lt;/li&gt;
&lt;li&gt;The &lt;a href=&#34;https://www.osano.com/articles/new-york-shield-law&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Stop Hacks and Improve Electronic Data Security (SHIELD) Act&lt;/a&gt; creates a definition for privacy information, encompassing many of the data points typically stored within cookies.&lt;/li&gt;
&lt;li&gt;India, Chile, and New Zealand are working on similar data privacy laws.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;sources&#34;&gt;Sources&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://developer.mozilla.org/en-US/docs/Web/HTTP/Cookies&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Using HTTP cookies&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.osano.com/articles/how-cookies-work&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;How cookies work, and how to conduct a cookie audit&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Terms</title>
      <link>https://matteocourthoud.github.io/terms/</link>
      <pubDate>Thu, 28 Jun 2018 00:00:00 +0100</pubDate>
      <guid>https://matteocourthoud.github.io/terms/</guid>
      <description>&lt;p&gt;&amp;hellip;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title></title>
      <link>https://matteocourthoud.github.io/admin/config.yml</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/admin/config.yml</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
