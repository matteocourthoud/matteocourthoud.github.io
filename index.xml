<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Matteo Courthoud</title>
    <link>https://matteocourthoud.github.io/</link>
      <atom:link href="https://matteocourthoud.github.io/index.xml" rel="self" type="application/rss+xml" />
    <description>Matteo Courthoud</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><copyright>Theme edited by Matteo Courthoud© - Want to have a similar website? [Guide here](https://matteocourthoud.github.io/post/website/).</copyright><lastBuildDate>Mon, 28 Feb 2022 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://matteocourthoud.github.io/media/icon_hu03e9b3967b83fd39296ec9da5ff1ea05_201175_512x512_fill_lanczos_center_3.png</url>
      <title>Matteo Courthoud</title>
      <link>https://matteocourthoud.github.io/</link>
    </image>
    
    <item>
      <title>Linear Regression</title>
      <link>https://matteocourthoud.github.io/course/ml-econ/01_regression/</link>
      <pubDate>Mon, 28 Feb 2022 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/course/ml-econ/01_regression/</guid>
      <description>&lt;p&gt;This chapter follows closely Chapter 3 of &lt;a href=&#34;https://hastie.su.domains/ISLR2/ISLRv2_website.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;An Introduction to Statistical Learning&lt;/a&gt; by James, Witten, Tibshirani, Friedman.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Remove warnings
import warnings
warnings.filterwarnings(&#39;ignore&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Import everything
import pandas as pd
import numpy as np
import seaborn as sns
import statsmodels.api as sm

from sklearn.linear_model import LinearRegression
from numpy.linalg import inv
from numpy.random import normal as rnorm
from statsmodels.stats.outliers_influence import OLSInfluence
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Setup matplotlib for graphs
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import axes3d

# Set global parameters
%matplotlib inline
plt.style.use(&#39;seaborn-white&#39;)
plt.rcParams[&#39;lines.linewidth&#39;] = 3
plt.rcParams[&#39;figure.figsize&#39;] = (10,6)
plt.rcParams[&#39;figure.titlesize&#39;] = 20
plt.rcParams[&#39;axes.titlesize&#39;] = 18
plt.rcParams[&#39;axes.labelsize&#39;] = 14
plt.rcParams[&#39;legend.fontsize&#39;] = 14
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can inspect all the available global parameter options &lt;a href=&#34;https://matplotlib.org/3.3.2/tutorials/introductory/customizing.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;11-simple-linear-regression&#34;&gt;1.1 Simple Linear Regression&lt;/h2&gt;
&lt;p&gt;First, let&amp;rsquo;s load the Advertising dataset. It contains information on displays sales (in thousands of units) for a particular product and a list of advertising budgets (in thousands of dollars) for TV, radio, and newspaper media.&lt;/p&gt;
&lt;p&gt;We open the dataset using the &lt;code&gt;pandas&lt;/code&gt; library which is &lt;strong&gt;the&lt;/strong&gt; library for handling datasets and data analysis in Python.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Advertisement spending data
advertising = pd.read_csv(&#39;data/Advertising.csv&#39;, usecols=[1,2,3,4])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s have a look at the content. We can have a glance at the first rows by using the function &lt;code&gt;head&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Preview of the data
advertising.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;TV&lt;/th&gt;
      &lt;th&gt;Radio&lt;/th&gt;
      &lt;th&gt;Newspaper&lt;/th&gt;
      &lt;th&gt;Sales&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;230.1&lt;/td&gt;
      &lt;td&gt;37.8&lt;/td&gt;
      &lt;td&gt;69.2&lt;/td&gt;
      &lt;td&gt;22.1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;44.5&lt;/td&gt;
      &lt;td&gt;39.3&lt;/td&gt;
      &lt;td&gt;45.1&lt;/td&gt;
      &lt;td&gt;10.4&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;17.2&lt;/td&gt;
      &lt;td&gt;45.9&lt;/td&gt;
      &lt;td&gt;69.3&lt;/td&gt;
      &lt;td&gt;9.3&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;151.5&lt;/td&gt;
      &lt;td&gt;41.3&lt;/td&gt;
      &lt;td&gt;58.5&lt;/td&gt;
      &lt;td&gt;18.5&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;180.8&lt;/td&gt;
      &lt;td&gt;10.8&lt;/td&gt;
      &lt;td&gt;58.4&lt;/td&gt;
      &lt;td&gt;12.9&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;We can have a general overview of the dataset using the function &lt;code&gt;info&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Overview of all variables
advertising.info()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;class &#39;pandas.core.frame.DataFrame&#39;&amp;gt;
RangeIndex: 200 entries, 0 to 199
Data columns (total 4 columns):
 #   Column     Non-Null Count  Dtype  
---  ------     --------------  -----  
 0   TV         200 non-null    float64
 1   Radio      200 non-null    float64
 2   Newspaper  200 non-null    float64
 3   Sales      200 non-null    float64
dtypes: float64(4)
memory usage: 6.4 KB
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can have more information on the single variables using the function &lt;code&gt;describe&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Summary of all variables
advertising.describe()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;TV&lt;/th&gt;
      &lt;th&gt;Radio&lt;/th&gt;
      &lt;th&gt;Newspaper&lt;/th&gt;
      &lt;th&gt;Sales&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;count&lt;/th&gt;
      &lt;td&gt;200.000000&lt;/td&gt;
      &lt;td&gt;200.000000&lt;/td&gt;
      &lt;td&gt;200.000000&lt;/td&gt;
      &lt;td&gt;200.000000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;mean&lt;/th&gt;
      &lt;td&gt;147.042500&lt;/td&gt;
      &lt;td&gt;23.264000&lt;/td&gt;
      &lt;td&gt;30.554000&lt;/td&gt;
      &lt;td&gt;14.022500&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;std&lt;/th&gt;
      &lt;td&gt;85.854236&lt;/td&gt;
      &lt;td&gt;14.846809&lt;/td&gt;
      &lt;td&gt;21.778621&lt;/td&gt;
      &lt;td&gt;5.217457&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;min&lt;/th&gt;
      &lt;td&gt;0.700000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.300000&lt;/td&gt;
      &lt;td&gt;1.600000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;25%&lt;/th&gt;
      &lt;td&gt;74.375000&lt;/td&gt;
      &lt;td&gt;9.975000&lt;/td&gt;
      &lt;td&gt;12.750000&lt;/td&gt;
      &lt;td&gt;10.375000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;50%&lt;/th&gt;
      &lt;td&gt;149.750000&lt;/td&gt;
      &lt;td&gt;22.900000&lt;/td&gt;
      &lt;td&gt;25.750000&lt;/td&gt;
      &lt;td&gt;12.900000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;75%&lt;/th&gt;
      &lt;td&gt;218.825000&lt;/td&gt;
      &lt;td&gt;36.525000&lt;/td&gt;
      &lt;td&gt;45.100000&lt;/td&gt;
      &lt;td&gt;17.400000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;max&lt;/th&gt;
      &lt;td&gt;296.400000&lt;/td&gt;
      &lt;td&gt;49.600000&lt;/td&gt;
      &lt;td&gt;114.000000&lt;/td&gt;
      &lt;td&gt;27.000000&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;If you just want to call a variable in &lt;code&gt;pandas&lt;/code&gt;, you have 3 options:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;use squared brackets as if the varaible was a component of a dictionary&lt;/li&gt;
&lt;li&gt;use or dot subscripts as if the variable was a function of the data&lt;/li&gt;
&lt;li&gt;use the &lt;code&gt;loc&lt;/code&gt; function (best practice)&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# 1. Brackets
advertising[&#39;TV&#39;]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;0      230.1
1       44.5
2       17.2
3      151.5
4      180.8
       ...  
195     38.2
196     94.2
197    177.0
198    283.6
199    232.1
Name: TV, Length: 200, dtype: float64
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# 2. Brackets
advertising.TV
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;0      230.1
1       44.5
2       17.2
3      151.5
4      180.8
       ...  
195     38.2
196     94.2
197    177.0
198    283.6
199    232.1
Name: TV, Length: 200, dtype: float64
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# The loc function
advertising.loc[:,&#39;TV&#39;]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;0      230.1
1       44.5
2       17.2
3      151.5
4      180.8
       ...  
195     38.2
196     94.2
197    177.0
198    283.6
199    232.1
Name: TV, Length: 200, dtype: float64
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that the &lt;code&gt;loc&lt;/code&gt; function is more powerful and is generally used to subset lines and columns.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Select multiple columns and subset of rows
advertising.loc[0:5,[&#39;Sales&#39;,&#39;TV&#39;]]
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;Sales&lt;/th&gt;
      &lt;th&gt;TV&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;22.1&lt;/td&gt;
      &lt;td&gt;230.1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;10.4&lt;/td&gt;
      &lt;td&gt;44.5&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;9.3&lt;/td&gt;
      &lt;td&gt;17.2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;18.5&lt;/td&gt;
      &lt;td&gt;151.5&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;12.9&lt;/td&gt;
      &lt;td&gt;180.8&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;5&lt;/th&gt;
      &lt;td&gt;7.2&lt;/td&gt;
      &lt;td&gt;8.7&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;Suppose we are interested in the (linear) relationship between sales and tv advertisement.&lt;/p&gt;
&lt;p&gt;$$
sales ≈ \beta_0 + \beta_1 TV.
$$&lt;/p&gt;
&lt;p&gt;How are the two two variables related? Visual inspection: scatterplot.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Figure 3.1
def make_fig_3_1a():
    
    # Init figure
    fig, ax = plt.subplots(1,1)
    ax.set_title(&#39;Figure 3.1&#39;);

    # Plot scatter and best fit line
    sns.regplot(x=advertising.TV, y=advertising.Sales, ax=ax, order=1, ci=None, scatter_kws={&#39;color&#39;:&#39;r&#39;, &#39;s&#39;:20})
    ax.set_xlim(-10,310); ax.set_ylim(ymin=0)
    ax.legend([&#39;Least Squares Fit&#39;,&#39;Data&#39;]);
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;make_fig_3_1a()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/01_regression_23_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;estimating-the-coefficients&#34;&gt;Estimating the Coefficients&lt;/h3&gt;
&lt;p&gt;How do we estimate the best fit line? Minimize the Residual Sum of Squares (RSS).&lt;/p&gt;
&lt;p&gt;First, suppose we have a dataset $\mathcal D = {x_i, y_i}_{i=1}^N$. We define the prediction of $y$ based on $X$ as&lt;/p&gt;
&lt;p&gt;$$
\hat y_i = \hat \beta X_i
$$&lt;/p&gt;
&lt;p&gt;The residuals are the unexplained component of $y$&lt;/p&gt;
&lt;p&gt;$$
e_i = y_i - \hat y_i
$$&lt;/p&gt;
&lt;p&gt;Our objective function (to be minimized) is the Resdual Sum of Squares (RSS):&lt;/p&gt;
&lt;p&gt;$$
RSS := \sum_{n=1}^N e_i^2
$$&lt;/p&gt;
&lt;p&gt;And the OLS coefficient is defined as its minimizer:&lt;/p&gt;
&lt;p&gt;$$
\hat \beta_{OLS} := \arg\min_{\beta} \sum_{n=1}^N e_i^2 = \arg\min_{\beta} \sum_{n=1}^N (y_i - X_i \beta)^2
$$&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s use the &lt;code&gt;sklearn&lt;/code&gt; library to fit a linear regression model of &lt;em&gt;Sales&lt;/em&gt; on &lt;em&gt;TV&lt;/em&gt; advertisement.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Define X and y
X = advertising.TV.values.reshape(-1,1)
y = advertising.Sales.values

# Fit linear regressions
reg = LinearRegression().fit(X,y)
print(reg.intercept_)
print(reg.coef_)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;7.0325935491276885
[0.04753664]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can visualize the residuals as the vertical distances between the data and the prediction line. The objective function RSS is the sum of the squares of the lengths of vertical lines.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Compute predicted values
y_hat = reg.predict(X)

# Figure 3.1
def make_figure_3_1b():

    # Init figure
    fig, ax = plt.subplots(1,1)
    ax.set_title(&#39;Figure 3.1&#39;);

    # Add residuals
    sns.regplot(x=advertising.TV, y=advertising.Sales, ax=ax, order=1, ci=None, scatter_kws={&#39;color&#39;:&#39;r&#39;, &#39;s&#39;:20})
    ax.vlines(X, np.minimum(y,y_hat), np.maximum(y,y_hat), linestyle=&#39;--&#39;, color=&#39;k&#39;, alpha=0.5, linewidth=1)
    plt.legend([&#39;Least Squares Fit&#39;,&#39;Data&#39;,&#39;Residuals&#39;]);
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;make_figure_3_1b()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/01_regression_29_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;The closed form solution in matrix algebra is
$$
\hat \beta_{OLS} = (X&amp;rsquo;X)^{-1}(X&amp;rsquo;y)
$$&lt;/p&gt;
&lt;p&gt;Python has a series of shortcuts to make the syntax less verbose. However, we still need to import the &lt;code&gt;inv&lt;/code&gt; function from &lt;code&gt;numpy&lt;/code&gt;. In Matlab it would be &lt;code&gt;(X&#39;*X)^{-1}*(X&#39;*y)&lt;/code&gt;, almost literal.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Compute OLS coefficient with matrix algebra
beta = inv(X.T @ X) @ X.T @ y

print(beta)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[0.08324961]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Why is the result different?&lt;/p&gt;
&lt;p&gt;We are missing one coefficient: the intercept. Our regression now looks like this&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# New figure 1
def make_new_figure_1():

    # Init figure
    fig, ax = plt.subplots(1,1)
    fig.suptitle(&#39;Role of the Intercept&#39;)

    # Add new line on the previous plot
    sns.regplot(x=advertising.TV, y=advertising.Sales, ax=ax, order=1, ci=None, scatter_kws={&#39;color&#39;:&#39;r&#39;, &#39;s&#39;:10})
    ax.plot(X, beta*X, color=&#39;g&#39;)
    plt.xlim(-10,310); plt.ylim(ymin=0);
    ax.legend([&#39;With Intercept&#39;, &#39;Without intercept&#39;]);
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;make_new_figure_1()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/01_regression_34_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;How do we insert an intercept using matrix algebra? We add a column of ones.&lt;/p&gt;
&lt;p&gt;$$
X_1 = [\boldsymbol{1}, X]
$$&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# How to insert intercept? Add constant: column of ones
one = np.ones(np.shape(X))
X1 = np.concatenate([one,X],axis=1)

print(np.shape(X1))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;(200, 2)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we compute again the coefficients as before.&lt;/p&gt;
&lt;p&gt;$$
\hat \beta_{OLS} = (X_1&amp;rsquo;X_1)^{-1}(X_1&amp;rsquo;y)
$$&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Compute beta OLS with intercept
beta_OLS = inv(X1.T @ X1) @ X1.T @ y

print(beta_OLS)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[7.03259355 0.04753664]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we have indeed obtained the same exact coefficients.&lt;/p&gt;
&lt;p&gt;What does minimizing the Residual Sum of Squares means in practice? How does the objective function looks like?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from sklearn.preprocessing import scale

# First, scale the data
X = scale(advertising.TV, with_mean=True, with_std=False).reshape(-1,1)
y = advertising.Sales
regr = LinearRegression().fit(X,y)

# Create grid coordinates for plotting
B0 = np.linspace(regr.intercept_-2, regr.intercept_+2, 50)
B1 = np.linspace(regr.coef_-0.02, regr.coef_+0.02, 50)
xx, yy = np.meshgrid(B0, B1, indexing=&#39;xy&#39;)
Z = np.zeros((B0.size,B1.size))

# Calculate Z-values (RSS) based on grid of coefficients
for (i,j),v in np.ndenumerate(Z):
    Z[i,j] =((y - (xx[i,j]+X.ravel()*yy[i,j]))**2).sum()/1000

# Minimized RSS
min_RSS = r&#39;$\beta_0$, $\beta_1$ for minimized RSS&#39;
min_rss = np.sum((regr.intercept_+regr.coef_*X - y.values.reshape(-1,1))**2)/1000
min_rss
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;2.1025305831313514
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Figure 3.2 - Regression coefficients - RSS
def make_fig_3_2():
    fig = plt.figure(figsize=(15,6))
    fig.suptitle(&#39;RSS - Regression coefficients&#39;)

    ax1 = fig.add_subplot(121)
    ax2 = fig.add_subplot(122, projection=&#39;3d&#39;)

    # Left plot
    CS = ax1.contour(xx, yy, Z, cmap=plt.cm.Set1, levels=[2.15, 2.2, 2.3, 2.5, 3])
    ax1.scatter(regr.intercept_, regr.coef_[0], c=&#39;r&#39;, label=min_RSS)
    ax1.clabel(CS, inline=True, fontsize=10, fmt=&#39;%1.1f&#39;)

    # Right plot
    ax2.plot_surface(xx, yy, Z, rstride=3, cstride=3, alpha=0.3)
    ax2.contour(xx, yy, Z, zdir=&#39;z&#39;, offset=Z.min(), cmap=plt.cm.Set1,
                alpha=0.4, levels=[2.15, 2.2, 2.3, 2.5, 3])
    ax2.scatter3D(regr.intercept_, regr.coef_[0], min_rss, c=&#39;r&#39;, label=min_RSS)
    ax2.set_zlabel(&#39;RSS&#39;)
    ax2.set_zlim(Z.min(),Z.max())
    ax2.set_ylim(0.02,0.07)

    # settings common to both plots
    for ax in fig.axes:
        ax.set_xlabel(r&#39;$\beta_0$&#39;)
        ax.set_ylabel(r&#39;$\beta_1$&#39;)
        ax.set_yticks([0.03,0.04,0.05,0.06])
        ax.legend()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;make_fig_3_2()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/01_regression_43_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;assessing-the-accuracy-of-the-coefficient-estimates&#34;&gt;Assessing the Accuracy of the Coefficient Estimates&lt;/h3&gt;
&lt;p&gt;How accurate is our regression fit? Suppose we were drawing different (small) samples from the same data generating process, for example&lt;/p&gt;
&lt;p&gt;$$
y_i = 2 + 3x_i + \varepsilon_i
$$&lt;/p&gt;
&lt;p&gt;where $x_i \sim N(0,1)$ and $\varepsilon \sim N(0,3)$.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Init
N = 30;    # Sample size
K = 100;   # Number of simulations
beta_hat = np.zeros((2,K))
x = np.linspace(-4,4,N)

# Set seed
np.random.seed(1)

# K simulations
for i in range(K):
    # Simulate data
    x1 = np.random.normal(0,1,N).reshape([-1,1])
    X = np.concatenate([np.ones(np.shape(x1)), x1], axis=1)
    epsilon = np.random.normal(0,5,N)
    beta0 = [2,3]
    y = X @ beta0 + epsilon

    # Estimate coefficients
    beta_hat[:,i] = inv(X.T @ X) @ X.T @ y
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# new figure 2
def make_new_fig_2():

    # Init figure
    fig, ax = plt.subplots(1,1)
    
    for i in range(K):
        # Plot line
        ax.plot(x, beta_hat[0,i] + x*beta_hat[1,i], color=&#39;blue&#39;, alpha=0.2, linewidth=1)
        if i==K-1:
            ax.plot(x, beta_hat[0,i] + x*beta_hat[1,i], color=&#39;blue&#39;, alpha=0.2, linewidth=1, label=&#39;Estimated Lines&#39;)

    # Plot true line
    ax.plot(x, 2 + 3*x, color=&#39;red&#39;, linewidth=3, label=&#39;True Line&#39;);
    ax.set_xlabel(&#39;X&#39;); ax.set_ylabel(&#39;Y&#39;); ax.legend();
    ax.set_xlim(-4,4);
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;make_new_fig_2()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/01_regression_48_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;regplot&lt;/code&gt; command lets us automatically draw confidence intervals. Let&amp;rsquo;s draw the last simulated dataset with conficence intervals.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;fig, ax = plt.subplots(1,1)

# Plot last simulation scatterplot with confidence interval
sns.regplot(x=x1, y=y, ax=ax, order=1, scatter_kws={&#39;color&#39;:&#39;r&#39;, &#39;s&#39;:20});
ax.set_xlabel(&#39;X&#39;); ax.set_ylabel(&#39;Y&#39;); 
ax.legend([&#39;Best fit&#39;,&#39;Data&#39;, &#39;Confidence Intervals&#39;]);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/01_regression_50_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;As we can see, depending on the sample, we get a different estimate of the linear relationship between $x$ and $y$. However, there estimates are on average correct. Indeed, we can visualize their distribution.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Plot distribution of coefficients
plot = sns.jointplot(x=beta_hat[0,:], y=beta_hat[1,:], color=&#39;red&#39;, edgecolor=&amp;quot;white&amp;quot;);
plot.ax_joint.axvline(x=2);
plot.ax_joint.axhline(y=3);
plot.set_axis_labels(&#39;beta_0&#39;, &#39;beta_1&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/01_regression_52_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;How do we compute confidence intervals by hand?&lt;/p&gt;
&lt;p&gt;$$
Var(\hat \beta_{OLS}) = \sigma^2 (X&amp;rsquo;X)^{-1}
$$&lt;/p&gt;
&lt;p&gt;where $\sigma^2 = Var(\varepsilon)$. Since we do not know $Var(\varepsilon)$, we estimate it as $Var(e)$.&lt;/p&gt;
&lt;p&gt;$$
\hat Var(\hat \beta_{OLS}) = \hat \sigma^2 (X&amp;rsquo;X)^{-1}
$$&lt;/p&gt;
&lt;p&gt;If we assume the standard errors are normally distributed (or we apply the Central Limit Theorem, assuming $n \to \infty$), a 95% confidence interval for the OLS coefficient takes the form&lt;/p&gt;
&lt;p&gt;$$
CI(\hat \beta_{OLS}) = \Big[ \hat \beta_{OLS} - 1.96 \times \hat SE(\hat \beta_{OLS}) \ , \ \hat \beta_{OLS} + 1.96 \times \hat SE(\hat \beta_{OLS}) \Big]
$$&lt;/p&gt;
&lt;p&gt;where $\hat SE(\hat \beta_{OLS}) = \sqrt{\hat Var(\hat \beta_{OLS})}$.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Import again X and y from example above
X = advertising.TV.values.reshape(-1,1)
X1 = np.concatenate([np.ones(np.shape(X)), X], axis=1)
y = advertising.Sales.values

# Compute residual variance
X_hat = X1 @ beta_OLS
e = y - X_hat
sigma_hat = np.var(e)
var_beta_OLS = sigma_hat * inv(X1.T @ X1)

# Take elements on the diagonal and square them
std_beta_OLS = [var_beta_OLS[0,0]**.5, var_beta_OLS[1,1]**.5]

print(std_beta_OLS)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[0.4555479737400674, 0.0026771203500466564]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;statsmodels&lt;/code&gt; library allows us to produce nice tables with parameter estimates and standard errors.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Table 3.1 &amp;amp; 3.2
est = sm.OLS.from_formula(&#39;Sales ~ TV&#39;, advertising).fit()
est.summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
      &lt;td&gt;&lt;/td&gt;         &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt; &lt;td&gt;    7.0326&lt;/td&gt; &lt;td&gt;    0.458&lt;/td&gt; &lt;td&gt;   15.360&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    6.130&lt;/td&gt; &lt;td&gt;    7.935&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;TV&lt;/th&gt;        &lt;td&gt;    0.0475&lt;/td&gt; &lt;td&gt;    0.003&lt;/td&gt; &lt;td&gt;   17.668&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    0.042&lt;/td&gt; &lt;td&gt;    0.053&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;h3 id=&#34;assessing-the-accuracy-of-the-model&#34;&gt;Assessing the Accuracy of the Model&lt;/h3&gt;
&lt;p&gt;What metrics can we use to assess whether the model is a good model, in terms of capturing the relationship between the variables?&lt;/p&gt;
&lt;p&gt;First, we can compute our objective function: the Residual Sum of Squares (&lt;em&gt;RSS&lt;/em&gt;). Lower values of our objective function imply that we got a better fit.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# RSS with regression coefficients
RSS = sum(e**2)

print(RSS)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;2102.530583131351
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The problem with &lt;em&gt;RSS&lt;/em&gt; as a metric is that it&amp;rsquo;s hard to compare different regressions since its scale depends on the magnitude of the variables.&lt;/p&gt;
&lt;p&gt;One measure of fit that does not depend on the magnitude of the variables is $R^2$: the percentage of our explanatory variable explained by the model&lt;/p&gt;
&lt;p&gt;$$
R^2 = 1 - \frac{\text{RSS}}{\text{TSS}}
$$&lt;/p&gt;
&lt;p&gt;where&lt;/p&gt;
&lt;p&gt;$$
TSS = \sum_{i=1}^N (y_i - \bar y)^2
$$&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# TSS
TSS = sum( (y-np.mean(y))**2 )

# R2
R2 = 1 - RSS/TSS

print(R2)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;0.6118750508500709
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Can the $R^2$ metric be negative? When?&lt;/p&gt;
&lt;h2 id=&#34;22-multiple-linear-regression&#34;&gt;2.2 Multiple Linear Regression&lt;/h2&gt;
&lt;p&gt;What if we have more than one explanatory variable? Spoiler: we already did, but one was a constant.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s have a look at the regression of &lt;em&gt;Sales&lt;/em&gt; on &lt;em&gt;Radio&lt;/em&gt; and &lt;em&gt;TV&lt;/em&gt; advertisement expenditure separately.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Table 3.3 (1)
est = sm.OLS.from_formula(&#39;Sales ~ Radio&#39;, advertising).fit()
est.summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
      &lt;td&gt;&lt;/td&gt;         &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt; &lt;td&gt;    9.3116&lt;/td&gt; &lt;td&gt;    0.563&lt;/td&gt; &lt;td&gt;   16.542&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    8.202&lt;/td&gt; &lt;td&gt;   10.422&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Radio&lt;/th&gt;     &lt;td&gt;    0.2025&lt;/td&gt; &lt;td&gt;    0.020&lt;/td&gt; &lt;td&gt;    9.921&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    0.162&lt;/td&gt; &lt;td&gt;    0.243&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Table 3.3 (2)
est = sm.OLS.from_formula(&#39;Sales ~ Newspaper&#39;, advertising).fit()
est.summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
      &lt;td&gt;&lt;/td&gt;         &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt; &lt;td&gt;   12.3514&lt;/td&gt; &lt;td&gt;    0.621&lt;/td&gt; &lt;td&gt;   19.876&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;   11.126&lt;/td&gt; &lt;td&gt;   13.577&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Newspaper&lt;/th&gt; &lt;td&gt;    0.0547&lt;/td&gt; &lt;td&gt;    0.017&lt;/td&gt; &lt;td&gt;    3.300&lt;/td&gt; &lt;td&gt; 0.001&lt;/td&gt; &lt;td&gt;    0.022&lt;/td&gt; &lt;td&gt;    0.087&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;It seems that both Radio and Newspapers are positively correlated with &lt;em&gt;Sales&lt;/em&gt;. Why don&amp;rsquo;t we estimate a unique regression with both dependent variables?&lt;/p&gt;
&lt;h3 id=&#34;estimating-the-regression-coefficients&#34;&gt;Estimating the Regression Coefficients&lt;/h3&gt;
&lt;p&gt;Suppose now we enrich our previous model adding all different forms of advertisement:&lt;/p&gt;
&lt;p&gt;$$
\text{Sales} = \beta_0 + \beta_1 \text{TV} + \beta_2 \text{Radio} + \beta_3 \text{Newspaper} + \varepsilon
$$&lt;/p&gt;
&lt;p&gt;We estimate it using the &lt;code&gt;statsmodels&lt;/code&gt; &lt;code&gt;ols&lt;/code&gt; library.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Table 3.4
est = sm.OLS.from_formula(&#39;Sales ~ TV + Radio + Newspaper&#39;, advertising).fit()
est.summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
      &lt;td&gt;&lt;/td&gt;         &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt; &lt;td&gt;    2.9389&lt;/td&gt; &lt;td&gt;    0.312&lt;/td&gt; &lt;td&gt;    9.422&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    2.324&lt;/td&gt; &lt;td&gt;    3.554&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;TV&lt;/th&gt;        &lt;td&gt;    0.0458&lt;/td&gt; &lt;td&gt;    0.001&lt;/td&gt; &lt;td&gt;   32.809&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    0.043&lt;/td&gt; &lt;td&gt;    0.049&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Radio&lt;/th&gt;     &lt;td&gt;    0.1885&lt;/td&gt; &lt;td&gt;    0.009&lt;/td&gt; &lt;td&gt;   21.893&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    0.172&lt;/td&gt; &lt;td&gt;    0.206&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Newspaper&lt;/th&gt; &lt;td&gt;   -0.0010&lt;/td&gt; &lt;td&gt;    0.006&lt;/td&gt; &lt;td&gt;   -0.177&lt;/td&gt; &lt;td&gt; 0.860&lt;/td&gt; &lt;td&gt;   -0.013&lt;/td&gt; &lt;td&gt;    0.011&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;Why now it seems that there is no relationship between Sales and Newspaper while the univariate regression told us the opposite?&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s explore the correlation between those variables.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Table 3.5 - Correlation Matrix
advertising.corr()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;TV&lt;/th&gt;
      &lt;th&gt;Radio&lt;/th&gt;
      &lt;th&gt;Newspaper&lt;/th&gt;
      &lt;th&gt;Sales&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;TV&lt;/th&gt;
      &lt;td&gt;1.000000&lt;/td&gt;
      &lt;td&gt;0.054809&lt;/td&gt;
      &lt;td&gt;0.056648&lt;/td&gt;
      &lt;td&gt;0.782224&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;Radio&lt;/th&gt;
      &lt;td&gt;0.054809&lt;/td&gt;
      &lt;td&gt;1.000000&lt;/td&gt;
      &lt;td&gt;0.354104&lt;/td&gt;
      &lt;td&gt;0.576223&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;Newspaper&lt;/th&gt;
      &lt;td&gt;0.056648&lt;/td&gt;
      &lt;td&gt;0.354104&lt;/td&gt;
      &lt;td&gt;1.000000&lt;/td&gt;
      &lt;td&gt;0.228299&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;Sales&lt;/th&gt;
      &lt;td&gt;0.782224&lt;/td&gt;
      &lt;td&gt;0.576223&lt;/td&gt;
      &lt;td&gt;0.228299&lt;/td&gt;
      &lt;td&gt;1.000000&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;Let&amp;rsquo;s try to inspect the relationship visually. Note that now the linear best fit is going to be 3-dimensional. In order to make it visually accessible, we consider only on &lt;em&gt;TV&lt;/em&gt; and &lt;em&gt;Radio&lt;/em&gt; advertisement expediture as dependent variables. The best fit will be a plane instead of a line.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Fit regression
est = sm.OLS.from_formula(&#39;Sales ~ Radio + TV&#39;, advertising).fit()
print(est.params)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Intercept    2.921100
Radio        0.187994
TV           0.045755
dtype: float64
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Create a coordinate grid
Radio = np.arange(0,50)
TV = np.arange(0,300)
B1, B2 = np.meshgrid(Radio, TV, indexing=&#39;xy&#39;)

# Compute predicted plane
Z = np.zeros((TV.size, Radio.size))
for (i,j),v in np.ndenumerate(Z):
        Z[i,j] =(est.params[0] + B1[i,j]*est.params[1] + B2[i,j]*est.params[2])
        
# Compute residuals
e = est.predict() - advertising.Sales
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Figure 3.5 - Multiple Linear Regression
def make_fig_3_5():

    # Init figure
    fig = plt.figure()
    ax = axes3d.Axes3D(fig, auto_add_to_figure=False)
    fig.add_axes(ax)
    fig.suptitle(&#39;Figure 3.5&#39;);


    # Plot best fit plane
    ax.plot_surface(B1, B2, Z, color=&#39;k&#39;, alpha=0.3)
    points = ax.scatter3D(advertising.Radio, advertising.TV, advertising.Sales, c=e, cmap=&amp;quot;seismic&amp;quot;, vmin=-5, vmax=5)
    plt.colorbar(points, cax=fig.add_axes([0.9, 0.1, 0.03, 0.8]))
    ax.set_xlabel(&#39;Radio&#39;); ax.set_xlim(0,50)
    ax.set_ylabel(&#39;TV&#39;); ax.set_ylim(bottom=0)
    ax.set_zlabel(&#39;Sales&#39;);
    ax.view_init(20, 20)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;make_fig_3_5()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/01_regression_79_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;some-important-questions&#34;&gt;Some Important Questions&lt;/h3&gt;
&lt;p&gt;How do you check whether the model fit well the data with multiple regressors? &lt;code&gt;statmodels&lt;/code&gt; and most regression packages automatically outputs more information about the least squares model.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Measires of fit
est.summary().tables[0]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;caption&gt;OLS Regression Results&lt;/caption&gt;
&lt;tr&gt;
  &lt;th&gt;Dep. Variable:&lt;/th&gt;          &lt;td&gt;Sales&lt;/td&gt;      &lt;th&gt;  R-squared:         &lt;/th&gt; &lt;td&gt;   0.897&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Model:&lt;/th&gt;                   &lt;td&gt;OLS&lt;/td&gt;       &lt;th&gt;  Adj. R-squared:    &lt;/th&gt; &lt;td&gt;   0.896&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Method:&lt;/th&gt;             &lt;td&gt;Least Squares&lt;/td&gt;  &lt;th&gt;  F-statistic:       &lt;/th&gt; &lt;td&gt;   859.6&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Date:&lt;/th&gt;             &lt;td&gt;Mon, 03 Jan 2022&lt;/td&gt; &lt;th&gt;  Prob (F-statistic):&lt;/th&gt; &lt;td&gt;4.83e-98&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Time:&lt;/th&gt;                 &lt;td&gt;18:28:21&lt;/td&gt;     &lt;th&gt;  Log-Likelihood:    &lt;/th&gt; &lt;td&gt; -386.20&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;No. Observations:&lt;/th&gt;      &lt;td&gt;   200&lt;/td&gt;      &lt;th&gt;  AIC:               &lt;/th&gt; &lt;td&gt;   778.4&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Df Residuals:&lt;/th&gt;          &lt;td&gt;   197&lt;/td&gt;      &lt;th&gt;  BIC:               &lt;/th&gt; &lt;td&gt;   788.3&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Df Model:&lt;/th&gt;              &lt;td&gt;     2&lt;/td&gt;      &lt;th&gt;                     &lt;/th&gt;     &lt;td&gt; &lt;/td&gt;   
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Covariance Type:&lt;/th&gt;      &lt;td&gt;nonrobust&lt;/td&gt;    &lt;th&gt;                     &lt;/th&gt;     &lt;td&gt; &lt;/td&gt;   
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;First measure: the &lt;strong&gt;F-test&lt;/strong&gt;. The F-test tries to answe the question &amp;ldquo;&lt;em&gt;Is There a Relationship Between the Response and Predictors?&lt;/em&gt;&amp;rdquo;&lt;/p&gt;
&lt;p&gt;In particular, it tests the following hypothesis&lt;/p&gt;
&lt;p&gt;$$
H_1: \text{is at least one coefficient different from zero?}
$$&lt;/p&gt;
&lt;p&gt;against the null hypothesis&lt;/p&gt;
&lt;p&gt;$$
H_0: \beta_0 = \beta_1 = &amp;hellip; = 0
$$&lt;/p&gt;
&lt;p&gt;This hypothesis test is performed by computing the F-statistic,&lt;/p&gt;
&lt;p&gt;$$
F=\frac{(\mathrm{TSS}-\mathrm{RSS}) / p}{\operatorname{RSS} /(n-p-1)}
$$&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s try to compute it by hand.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Init
X = advertising[[&#39;Radio&#39;, &#39;TV&#39;]]
y = advertising.Sales
e = y - est.predict(X)
RSS = np.sum(e**2)
TSS = np.sum((y - np.mean(y))**2)
(n,p) = np.shape(X)

# Compute F
F = ((TSS - RSS)/p) / (RSS/(n-p-1))
print(&#39;F = %.4f&#39; % F)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;F = 859.6177
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;A rule of thumb is to reject $H_0$ if $F &amp;gt; 10$.&lt;/p&gt;
&lt;p&gt;We can also test that a particular subset of coefficients are equal to zero. In that case, we just substitute the Total Sum of Squares (TSS) with the Residual Sum of Squares under the null.&lt;/p&gt;
&lt;p&gt;$$
F=\frac{(\mathrm{RSS_0}-\mathrm{RSS}) / p}{\operatorname{RSS} /(n-p-1)}
$$&lt;/p&gt;
&lt;p&gt;i.e. we perfome the regression under the null hypothesis and we compute&lt;/p&gt;
&lt;p&gt;$$
RSS_0 = \sum_{n=1}^N (y_i - X_i \beta)^2 \quad s.t. \quad  H_0
$$&lt;/p&gt;
&lt;h2 id=&#34;23-other-considerations-in-the-regression-model&#34;&gt;2.3 Other Considerations in the Regression Model&lt;/h2&gt;
&lt;h3 id=&#34;qualitative-predictors&#34;&gt;Qualitative Predictors&lt;/h3&gt;
&lt;p&gt;What if some variables are qualitative instead of quantitative? Let&amp;rsquo;s change dataset and use the &lt;code&gt;credit&lt;/code&gt; dataset.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Credit ratings dataset
credit = pd.read_csv(&#39;data/Credit.csv&#39;, usecols=list(range(1,12)))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This dataset contains information on credit ratings, i.e. each person is assigned a &lt;code&gt;Rating&lt;/code&gt; score based on his/her own individual characteristics.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s have a look at data types.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Summary
credit.info()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;class &#39;pandas.core.frame.DataFrame&#39;&amp;gt;
RangeIndex: 400 entries, 0 to 399
Data columns (total 11 columns):
 #   Column     Non-Null Count  Dtype  
---  ------     --------------  -----  
 0   Income     400 non-null    float64
 1   Limit      400 non-null    int64  
 2   Rating     400 non-null    int64  
 3   Cards      400 non-null    int64  
 4   Age        400 non-null    int64  
 5   Education  400 non-null    int64  
 6   Gender     400 non-null    object 
 7   Student    400 non-null    object 
 8   Married    400 non-null    object 
 9   Ethnicity  400 non-null    object 
 10  Balance    400 non-null    int64  
dtypes: float64(1), int64(6), object(4)
memory usage: 34.5+ KB
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As we can see, some variables like &lt;code&gt;Gender&lt;/code&gt;, &lt;code&gt;Student&lt;/code&gt; or &lt;code&gt;Married&lt;/code&gt; are not numeric.&lt;/p&gt;
&lt;p&gt;We can have a closer look at what these variables look like.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Look at data
credit.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;Income&lt;/th&gt;
      &lt;th&gt;Limit&lt;/th&gt;
      &lt;th&gt;Rating&lt;/th&gt;
      &lt;th&gt;Cards&lt;/th&gt;
      &lt;th&gt;Age&lt;/th&gt;
      &lt;th&gt;Education&lt;/th&gt;
      &lt;th&gt;Gender&lt;/th&gt;
      &lt;th&gt;Student&lt;/th&gt;
      &lt;th&gt;Married&lt;/th&gt;
      &lt;th&gt;Ethnicity&lt;/th&gt;
      &lt;th&gt;Balance&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;14.891&lt;/td&gt;
      &lt;td&gt;3606&lt;/td&gt;
      &lt;td&gt;283&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;34&lt;/td&gt;
      &lt;td&gt;11&lt;/td&gt;
      &lt;td&gt;Male&lt;/td&gt;
      &lt;td&gt;No&lt;/td&gt;
      &lt;td&gt;Yes&lt;/td&gt;
      &lt;td&gt;Caucasian&lt;/td&gt;
      &lt;td&gt;333&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;106.025&lt;/td&gt;
      &lt;td&gt;6645&lt;/td&gt;
      &lt;td&gt;483&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;82&lt;/td&gt;
      &lt;td&gt;15&lt;/td&gt;
      &lt;td&gt;Female&lt;/td&gt;
      &lt;td&gt;Yes&lt;/td&gt;
      &lt;td&gt;Yes&lt;/td&gt;
      &lt;td&gt;Asian&lt;/td&gt;
      &lt;td&gt;903&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;104.593&lt;/td&gt;
      &lt;td&gt;7075&lt;/td&gt;
      &lt;td&gt;514&lt;/td&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;71&lt;/td&gt;
      &lt;td&gt;11&lt;/td&gt;
      &lt;td&gt;Male&lt;/td&gt;
      &lt;td&gt;No&lt;/td&gt;
      &lt;td&gt;No&lt;/td&gt;
      &lt;td&gt;Asian&lt;/td&gt;
      &lt;td&gt;580&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;148.924&lt;/td&gt;
      &lt;td&gt;9504&lt;/td&gt;
      &lt;td&gt;681&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;36&lt;/td&gt;
      &lt;td&gt;11&lt;/td&gt;
      &lt;td&gt;Female&lt;/td&gt;
      &lt;td&gt;No&lt;/td&gt;
      &lt;td&gt;No&lt;/td&gt;
      &lt;td&gt;Asian&lt;/td&gt;
      &lt;td&gt;964&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;55.882&lt;/td&gt;
      &lt;td&gt;4897&lt;/td&gt;
      &lt;td&gt;357&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;68&lt;/td&gt;
      &lt;td&gt;16&lt;/td&gt;
      &lt;td&gt;Male&lt;/td&gt;
      &lt;td&gt;No&lt;/td&gt;
      &lt;td&gt;Yes&lt;/td&gt;
      &lt;td&gt;Caucasian&lt;/td&gt;
      &lt;td&gt;331&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;Let&amp;rsquo;s consider the variable &lt;code&gt;Student&lt;/code&gt;. From a quick inspection it looks like it&amp;rsquo;s a binary &lt;em&gt;Yes/No&lt;/em&gt; variable. Let&amp;rsquo;s check by listing all its values.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# What values does the Student variable take?
credit[&#39;Student&#39;].unique()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;array([&#39;No&#39;, &#39;Yes&#39;], dtype=object)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What happens if you pass a binary varaible to &lt;code&gt;statsmodel&lt;/code&gt;? It automatically generates a dummy out of it.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Table 3.7
est = sm.OLS.from_formula(&#39;Balance ~ Student&#39;, credit).fit()
est.summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
         &lt;td&gt;&lt;/td&gt;           &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt;      &lt;td&gt;  480.3694&lt;/td&gt; &lt;td&gt;   23.434&lt;/td&gt; &lt;td&gt;   20.499&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;  434.300&lt;/td&gt; &lt;td&gt;  526.439&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Student[T.Yes]&lt;/th&gt; &lt;td&gt;  396.4556&lt;/td&gt; &lt;td&gt;   74.104&lt;/td&gt; &lt;td&gt;    5.350&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;  250.771&lt;/td&gt; &lt;td&gt;  542.140&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;If a variable takes more than one value, &lt;code&gt;statsmodel&lt;/code&gt; automatically generates a uniqe dummy for each level (-1).&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Table 3.8
est = sm.OLS.from_formula(&#39;Balance ~ Ethnicity&#39;, credit).fit()
est.summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
             &lt;td&gt;&lt;/td&gt;               &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt;              &lt;td&gt;  531.0000&lt;/td&gt; &lt;td&gt;   46.319&lt;/td&gt; &lt;td&gt;   11.464&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;  439.939&lt;/td&gt; &lt;td&gt;  622.061&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Ethnicity[T.Asian]&lt;/th&gt;     &lt;td&gt;  -18.6863&lt;/td&gt; &lt;td&gt;   65.021&lt;/td&gt; &lt;td&gt;   -0.287&lt;/td&gt; &lt;td&gt; 0.774&lt;/td&gt; &lt;td&gt; -146.515&lt;/td&gt; &lt;td&gt;  109.142&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Ethnicity[T.Caucasian]&lt;/th&gt; &lt;td&gt;  -12.5025&lt;/td&gt; &lt;td&gt;   56.681&lt;/td&gt; &lt;td&gt;   -0.221&lt;/td&gt; &lt;td&gt; 0.826&lt;/td&gt; &lt;td&gt; -123.935&lt;/td&gt; &lt;td&gt;   98.930&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;h3 id=&#34;relaxing-the-additive-assumption&#34;&gt;Relaxing the Additive Assumption&lt;/h3&gt;
&lt;p&gt;We have seen that both TV and Radio advertisement are positively associated with Sales. What if there is a synergy? For example it might be that if someone sees an ad &lt;em&gt;both&lt;/em&gt; on TV and on the radio, s/he is much more likely to buy the product.&lt;/p&gt;
&lt;p&gt;Consider the following model&lt;/p&gt;
&lt;p&gt;$$
\text{Sales} ≈ \beta_0 + \beta_1 \text{TV} + \beta_2 \text{Radio} + \beta_3 \text{TV} \times \text{Radio}
$$&lt;/p&gt;
&lt;p&gt;which can be rewritten as&lt;/p&gt;
&lt;p&gt;$$
\text{Sales} ≈ \beta_0 + (\beta_1 + \beta_3 \text{Radio}) \times \text{TV} + \beta_2 \text{Radio}
$$&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s estimate the linear regression model, with the intercept.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Table 3.9 - Interaction Variables
est = sm.OLS.from_formula(&#39;Sales ~ TV + Radio + TV*Radio&#39;, advertising).fit()
est.summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
      &lt;td&gt;&lt;/td&gt;         &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt; &lt;td&gt;    6.7502&lt;/td&gt; &lt;td&gt;    0.248&lt;/td&gt; &lt;td&gt;   27.233&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    6.261&lt;/td&gt; &lt;td&gt;    7.239&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;TV&lt;/th&gt;        &lt;td&gt;    0.0191&lt;/td&gt; &lt;td&gt;    0.002&lt;/td&gt; &lt;td&gt;   12.699&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    0.016&lt;/td&gt; &lt;td&gt;    0.022&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Radio&lt;/th&gt;     &lt;td&gt;    0.0289&lt;/td&gt; &lt;td&gt;    0.009&lt;/td&gt; &lt;td&gt;    3.241&lt;/td&gt; &lt;td&gt; 0.001&lt;/td&gt; &lt;td&gt;    0.011&lt;/td&gt; &lt;td&gt;    0.046&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;TV:Radio&lt;/th&gt;  &lt;td&gt;    0.0011&lt;/td&gt; &lt;td&gt; 5.24e-05&lt;/td&gt; &lt;td&gt;   20.727&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    0.001&lt;/td&gt; &lt;td&gt;    0.001&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;A positive and significant interaction term indicates a hint of a sinergy effect.&lt;/p&gt;
&lt;h3 id=&#34;heterogeneous-effects&#34;&gt;Heterogeneous Effects&lt;/h3&gt;
&lt;p&gt;We can do interactions with qualitative variables as well. Conside the credit rating dataset.&lt;/p&gt;
&lt;p&gt;What if &lt;code&gt;Balance&lt;/code&gt; depends by &lt;code&gt;Income&lt;/code&gt; differently, depending on whether one is a &lt;code&gt;Student&lt;/code&gt; or not?&lt;/p&gt;
&lt;p&gt;Consider the following model:&lt;/p&gt;
&lt;p&gt;$$
\text{Balance} ≈ \beta_0 + \beta_1 \text{Income} + \beta_2 \text{Student} + \beta_3 \text{Income} \times \text{Student}
$$&lt;/p&gt;
&lt;p&gt;The last coefficient $\beta_3$ should tell us how much &lt;code&gt;Balance&lt;/code&gt; increases in &lt;code&gt;Income&lt;/code&gt; for &lt;code&gt;Students&lt;/code&gt; with respect to non-Students.&lt;/p&gt;
&lt;p&gt;Indeed, we can decompose the regression in the following equivalent way:&lt;/p&gt;
&lt;p&gt;$$
\text{Balance} ≈ \beta_0 + \beta_1 \text{Income} + \beta_2 \text{Student} + \beta_3 \text{Income} \times \text{Student}
$$&lt;/p&gt;
&lt;p&gt;which can be interpreted in the following way since &lt;code&gt;Student&lt;/code&gt; is a binary variable&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;If the person is &lt;em&gt;not&lt;/em&gt; a student
$$
\text{Balance} ≈ \beta_0 + \beta_1 \text{Income}
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;If the person is a student
$$
\text{Balance} ≈ (\beta_0 + \beta_2) + (\beta_1 + \beta_3 ) \text{Income}
$$&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;We are allowing not only for a different intercept for &lt;code&gt;Students&lt;/code&gt;, $\beta_0 \to \beta_0 + \beta_2$,  but also for a different impact of &lt;code&gt;Income&lt;/code&gt;, $\beta_1 \to \beta_1 + \beta_3$.&lt;/p&gt;
&lt;p&gt;We can visually inspect the distribution of &lt;code&gt;Income&lt;/code&gt; across the two groups.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Divide data into students and non-students
x_student = credit.loc[credit.Student==&#39;Yes&#39;,&#39;Income&#39;]
y_student = credit.loc[credit.Student==&#39;Yes&#39;,&#39;Balance&#39;]
x_nonstudent = credit.loc[credit.Student==&#39;No&#39;,&#39;Income&#39;]
y_nonstudent = credit.loc[credit.Student==&#39;No&#39;,&#39;Balance&#39;]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Make figure 3.8
def make_fig_3_8():
    
    # Init figure
    fig, ax = plt.subplots(1,1)
    fig.suptitle(&#39;Figure 3.8&#39;)

    # Relationship betweeen income and balance for students and non-students
    ax.scatter(x=x_nonstudent, y=y_nonstudent, facecolors=&#39;None&#39;, edgecolors=&#39;k&#39;, alpha=0.5);
    ax.scatter(x=x_student, y=y_student, facecolors=&#39;r&#39;, edgecolors=&#39;r&#39;, alpha=0.7);
    ax.legend([&#39;non-student&#39;, &#39;student&#39;]);
    ax.set_xlabel(&#39;Income&#39;); ax.set_ylabel(&#39;Balance&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;make_fig_3_8()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/01_regression_114_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;It is hard from the scatterplot to see whether there is a different relationship between income and balance for students and non-students.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s fit two separate regressions.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Interaction between qualitative and quantative variables
est1 = sm.OLS.from_formula(&#39;Balance ~ Income + Student&#39;, credit).fit()
reg1 = est1.params
est2 = sm.OLS.from_formula(&#39;Balance ~ Income + Student + Income*Student&#39;, credit).fit()
reg2 = est2.params

print(&#39;Regression 1 - without interaction term&#39;)
print(reg1)
print(&#39;\nRegression 2 - with interaction term&#39;)
print(reg2)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Regression 1 - without interaction term
Intercept         211.142964
Student[T.Yes]    382.670539
Income              5.984336
dtype: float64

Regression 2 - with interaction term
Intercept                200.623153
Student[T.Yes]           476.675843
Income                     6.218169
Income:Student[T.Yes]     -1.999151
dtype: float64
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Without the interaction term, the two lines have different levels but the same slope. Introducing an interaction term allows the two groups to have different responses to Income.&lt;/p&gt;
&lt;p&gt;We can visualize the relationship in a graph.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Income (x-axis)
income = np.linspace(0,150)

# Balance without interaction term (y-axis)
student1 = np.linspace(reg1[&#39;Intercept&#39;]+reg1[&#39;Student[T.Yes]&#39;],
                       reg1[&#39;Intercept&#39;]+reg1[&#39;Student[T.Yes]&#39;]+150*reg1[&#39;Income&#39;])
non_student1 =  np.linspace(reg1[&#39;Intercept&#39;], reg1[&#39;Intercept&#39;]+150*reg1[&#39;Income&#39;])

# Balance with iteraction term (y-axis)
student2 = np.linspace(reg2[&#39;Intercept&#39;]+reg2[&#39;Student[T.Yes]&#39;],
                       reg2[&#39;Intercept&#39;]+reg2[&#39;Student[T.Yes]&#39;]+
                       150*(reg2[&#39;Income&#39;]+reg2[&#39;Income:Student[T.Yes]&#39;]))
non_student2 =  np.linspace(reg2[&#39;Intercept&#39;], reg2[&#39;Intercept&#39;]+150*reg2[&#39;Income&#39;])
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Figure 3.7
def make_fig_3_7():
    
    fig, (ax1,ax2) = plt.subplots(1,2, figsize=(12,5))
    fig.suptitle(&#39;Figure 3.7&#39;)

    # Plot best fit with and without interaction
    ax1.plot(income, student1, &#39;r&#39;, income, non_student1, &#39;k&#39;)
    ax2.plot(income, student2, &#39;r&#39;, income, non_student2, &#39;k&#39;)
    
    titles = [&#39;Dummy&#39;, &#39;Dummy + Interaction&#39;]
    for ax, t in zip(fig.axes, titles):
        ax.legend([&#39;student&#39;, &#39;non-student&#39;], loc=2)
        ax.set_xlabel(&#39;Income&#39;)
        ax.set_ylabel(&#39;Balance&#39;)
        ax.set_ylim(ymax=1550)
        ax.set_title(t)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;make_fig_3_7()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/01_regression_120_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;non-linear-relationships&#34;&gt;Non-Linear Relationships&lt;/h3&gt;
&lt;p&gt;What if we allow for further non-linearities? Let&amp;rsquo;s change dataset again and use the &lt;code&gt;car&lt;/code&gt; dataset.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Automobile dataset (dropping missing values)
auto = pd.read_csv(&#39;data/Auto.csv&#39;, na_values=&#39;?&#39;).dropna()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This dataset contains information of a wide variety of car models.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;auto.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;mpg&lt;/th&gt;
      &lt;th&gt;cylinders&lt;/th&gt;
      &lt;th&gt;displacement&lt;/th&gt;
      &lt;th&gt;horsepower&lt;/th&gt;
      &lt;th&gt;weight&lt;/th&gt;
      &lt;th&gt;acceleration&lt;/th&gt;
      &lt;th&gt;year&lt;/th&gt;
      &lt;th&gt;origin&lt;/th&gt;
      &lt;th&gt;name&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;18.0&lt;/td&gt;
      &lt;td&gt;8&lt;/td&gt;
      &lt;td&gt;307.0&lt;/td&gt;
      &lt;td&gt;130&lt;/td&gt;
      &lt;td&gt;3504&lt;/td&gt;
      &lt;td&gt;12.0&lt;/td&gt;
      &lt;td&gt;70&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;chevrolet chevelle malibu&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;15.0&lt;/td&gt;
      &lt;td&gt;8&lt;/td&gt;
      &lt;td&gt;350.0&lt;/td&gt;
      &lt;td&gt;165&lt;/td&gt;
      &lt;td&gt;3693&lt;/td&gt;
      &lt;td&gt;11.5&lt;/td&gt;
      &lt;td&gt;70&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;buick skylark 320&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;18.0&lt;/td&gt;
      &lt;td&gt;8&lt;/td&gt;
      &lt;td&gt;318.0&lt;/td&gt;
      &lt;td&gt;150&lt;/td&gt;
      &lt;td&gt;3436&lt;/td&gt;
      &lt;td&gt;11.0&lt;/td&gt;
      &lt;td&gt;70&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;plymouth satellite&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;16.0&lt;/td&gt;
      &lt;td&gt;8&lt;/td&gt;
      &lt;td&gt;304.0&lt;/td&gt;
      &lt;td&gt;150&lt;/td&gt;
      &lt;td&gt;3433&lt;/td&gt;
      &lt;td&gt;12.0&lt;/td&gt;
      &lt;td&gt;70&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;amc rebel sst&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;17.0&lt;/td&gt;
      &lt;td&gt;8&lt;/td&gt;
      &lt;td&gt;302.0&lt;/td&gt;
      &lt;td&gt;140&lt;/td&gt;
      &lt;td&gt;3449&lt;/td&gt;
      &lt;td&gt;10.5&lt;/td&gt;
      &lt;td&gt;70&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;ford torino&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;Suppose we wanted to understand which car caracteristics are correlated with higher efficiency, i.e. &lt;code&gt;mpg&lt;/code&gt; (miles per gallon).&lt;/p&gt;
&lt;p&gt;Consider in particular the relationship between &lt;code&gt;mpg&lt;/code&gt; and &lt;code&gt;horsepower&lt;/code&gt;. It might be a highly non-linear relationship.&lt;/p&gt;
&lt;p&gt;$$
\text{mpg} ≈ \beta_0 + \beta_1 \text{horsepower} + \beta_2 \text{horsepower}^2 + &amp;hellip; ???
$$&lt;/p&gt;
&lt;p&gt;How many terms should we include?&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s look at the data to understand if it naturally suggests non-linearities.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;fig, ax = plt.subplots(1,1)

# Plot polinomials of different degree
plt.scatter(x=auto.horsepower, y=auto.mpg, facecolors=&#39;None&#39;, edgecolors=&#39;k&#39;, alpha=.3) 
plt.ylim(5,55); plt.xlim(40,240); 
plt.xlabel(&#39;horsepower&#39;); plt.ylabel(&#39;mpg&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/01_regression_128_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;The relationship looks non-linear but in which way exactly? Let&amp;rsquo;s try to fit polinomials of different degrees.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def make_fig_38():
    
    # Figure 3.8 
    fig, ax = plt.subplots(1,1)
    ax.set_title(&#39;Figure 3.8&#39;)

    # Plot polinomials of different degree
    plt.scatter(x=auto.horsepower, y=auto.mpg, facecolors=&#39;None&#39;, edgecolors=&#39;k&#39;, alpha=.3) 
    sns.regplot(x=auto.horsepower, y=auto.mpg, ci=None, label=&#39;Linear&#39;, scatter=False, color=&#39;orange&#39;)
    sns.regplot(x=auto.horsepower, y=auto.mpg, ci=None, label=&#39;Degree 2&#39;, order=2, scatter=False, color=&#39;lightblue&#39;)
    sns.regplot(x=auto.horsepower, y=auto.mpg, ci=None, label=&#39;Degree 5&#39;, order=5, scatter=False, color=&#39;g&#39;)
    plt.legend()
    plt.ylim(5,55)
    plt.xlim(40,240);
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;make_fig_38()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/01_regression_131_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;As we can see, the tails are highly unstable depending on the specification.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s add a quadratic term&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Table 3.10
auto[&#39;horsepower2&#39;] = auto.horsepower**2
auto.head(3)
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;mpg&lt;/th&gt;
      &lt;th&gt;cylinders&lt;/th&gt;
      &lt;th&gt;displacement&lt;/th&gt;
      &lt;th&gt;horsepower&lt;/th&gt;
      &lt;th&gt;weight&lt;/th&gt;
      &lt;th&gt;acceleration&lt;/th&gt;
      &lt;th&gt;year&lt;/th&gt;
      &lt;th&gt;origin&lt;/th&gt;
      &lt;th&gt;name&lt;/th&gt;
      &lt;th&gt;horsepower2&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;18.0&lt;/td&gt;
      &lt;td&gt;8&lt;/td&gt;
      &lt;td&gt;307.0&lt;/td&gt;
      &lt;td&gt;130&lt;/td&gt;
      &lt;td&gt;3504&lt;/td&gt;
      &lt;td&gt;12.0&lt;/td&gt;
      &lt;td&gt;70&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;chevrolet chevelle malibu&lt;/td&gt;
      &lt;td&gt;16900&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;15.0&lt;/td&gt;
      &lt;td&gt;8&lt;/td&gt;
      &lt;td&gt;350.0&lt;/td&gt;
      &lt;td&gt;165&lt;/td&gt;
      &lt;td&gt;3693&lt;/td&gt;
      &lt;td&gt;11.5&lt;/td&gt;
      &lt;td&gt;70&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;buick skylark 320&lt;/td&gt;
      &lt;td&gt;27225&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;18.0&lt;/td&gt;
      &lt;td&gt;8&lt;/td&gt;
      &lt;td&gt;318.0&lt;/td&gt;
      &lt;td&gt;150&lt;/td&gt;
      &lt;td&gt;3436&lt;/td&gt;
      &lt;td&gt;11.0&lt;/td&gt;
      &lt;td&gt;70&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;plymouth satellite&lt;/td&gt;
      &lt;td&gt;22500&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;How does the regression change?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;est = sm.OLS.from_formula(&#39;mpg ~ horsepower + horsepower2&#39;, auto).fit()
est.summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
       &lt;td&gt;&lt;/td&gt;          &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt;   &lt;td&gt;   56.9001&lt;/td&gt; &lt;td&gt;    1.800&lt;/td&gt; &lt;td&gt;   31.604&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;   53.360&lt;/td&gt; &lt;td&gt;   60.440&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;horsepower&lt;/th&gt;  &lt;td&gt;   -0.4662&lt;/td&gt; &lt;td&gt;    0.031&lt;/td&gt; &lt;td&gt;  -14.978&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;   -0.527&lt;/td&gt; &lt;td&gt;   -0.405&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;horsepower2&lt;/th&gt; &lt;td&gt;    0.0012&lt;/td&gt; &lt;td&gt;    0.000&lt;/td&gt; &lt;td&gt;   10.080&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    0.001&lt;/td&gt; &lt;td&gt;    0.001&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;h3 id=&#34;non-linearities&#34;&gt;Non-Linearities&lt;/h3&gt;
&lt;p&gt;How can we assess if there are non-linearities and of which kind? We can look at the residuals.&lt;/p&gt;
&lt;p&gt;If the residuals show some kind of pattern, probably we could have fit the line better. Moreover, we can use the pattern itself to understand how.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Linear fit
X = auto.horsepower.values.reshape(-1,1)
y = auto.mpg
regr = LinearRegression().fit(X, y)

auto[&#39;pred1&#39;] = regr.predict(X)
auto[&#39;resid1&#39;] = auto.mpg - auto.pred1

# Quadratic fit
X2 = auto[[&#39;horsepower&#39;, &#39;horsepower2&#39;]]
regr.fit(X2, y)

auto[&#39;pred2&#39;] = regr.predict(X2)
auto[&#39;resid2&#39;] = auto.mpg - auto.pred2
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Figure 3.9
def make_fig_39():
    
    fig, (ax1,ax2) = plt.subplots(1,2, figsize=(12,5))
    fig.suptitle(&#39;Figure 3.9&#39;)

    # Left plot
    sns.regplot(x=auto.pred1, y=auto.resid1, lowess=True, 
                ax=ax1, line_kws={&#39;color&#39;:&#39;r&#39;, &#39;lw&#39;:1},
                scatter_kws={&#39;facecolors&#39;:&#39;None&#39;, &#39;edgecolors&#39;:&#39;k&#39;, &#39;alpha&#39;:0.5})
    ax1.hlines(0,xmin=ax1.xaxis.get_data_interval()[0],
               xmax=ax1.xaxis.get_data_interval()[1], linestyles=&#39;dotted&#39;)
    ax1.set_title(&#39;Residual Plot for Linear Fit&#39;)

    # Right plot
    sns.regplot(x=auto.pred2, y=auto.resid2, lowess=True,
                line_kws={&#39;color&#39;:&#39;r&#39;, &#39;lw&#39;:1}, ax=ax2,
                scatter_kws={&#39;facecolors&#39;:&#39;None&#39;, &#39;edgecolors&#39;:&#39;k&#39;, &#39;alpha&#39;:0.5})
    ax2.hlines(0,xmin=ax2.xaxis.get_data_interval()[0],
               xmax=ax2.xaxis.get_data_interval()[1], linestyles=&#39;dotted&#39;)
    ax2.set_title(&#39;Residual Plot for Quadratic Fit&#39;)

    for ax in fig.axes:
        ax.set_xlabel(&#39;Fitted values&#39;)
        ax.set_ylabel(&#39;Residuals&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt; make_fig_39()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/01_regression_141_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;It looks like the residuals from the linear fit (on the left) exibit a pattern:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;positive values at the tails&lt;/li&gt;
&lt;li&gt;negative values in the center&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This suggests a quadratic fit. Indeed, the residuals when we include &lt;code&gt;horsepower^2&lt;/code&gt; (on the right) seem more uniformly centered around zero.&lt;/p&gt;
&lt;h3 id=&#34;outliers&#34;&gt;Outliers&lt;/h3&gt;
&lt;p&gt;Observations with high residuals have a good chance of being highly influentials. However, they do not have to be.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s use the following data generating process:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$X \sim N(0,1)$&lt;/li&gt;
&lt;li&gt;$\varepsilon \sim N(0,0.5)$&lt;/li&gt;
&lt;li&gt;$\beta_0 = 3$&lt;/li&gt;
&lt;li&gt;$y = \beta_0 X + \varepsilon$&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;np.random.seed(1)

# Generate random y
n = 50
X = rnorm(1,1,(n,1))
e = rnorm(0,0.5,(n,1))
b0 = 3
y = X*b0 + e
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now let&amp;rsquo;s change observation &lt;code&gt;20&lt;/code&gt; so that it becomes an outlier, i.e. it has a high residual.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Generate outlier
X[20] = 1
y[20] = 7

# Short regression without observation number 41
X_small = np.delete(X, 20)
y_small = np.delete(y, 20)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s now plot the data and the residuals&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Figure 3.12
def make_fig_3_12():
    
    # Init figure
    fig, (ax1,ax2) = plt.subplots(1,2, figsize=(12,5)) 
    fig.suptitle(&#39;Figure 3.12&#39;)

    # Plot 1
    ax1.scatter(x=X, y=y, facecolors=&#39;None&#39;, edgecolors=&#39;k&#39;, alpha=.5) 
    sns.regplot(x=X, y=y, ax=ax1, order=1, ci=None, scatter=False, line_kws={&#39;color&#39;:&#39;r&#39;, &#39;lw&#39;:1})
    sns.regplot(x=X_small, y=y_small, ax=ax1, order=1, ci=None, scatter=False, line_kws={&#39;color&#39;:&#39;b&#39;, &#39;lw&#39;:1})
    ax1.set_xlabel(&#39;X&#39;); ax1.set_ylabel(&#39;Y&#39;); 
    ax1.legend([&#39;With obs. 20&#39;, &#39;Without obs. 20&#39;], fontsize=12);

    # Hihglight outliers
    ax1.scatter(x=X[20], y=y[20], facecolors=&#39;None&#39;, edgecolors=&#39;r&#39;, alpha=1) 
    ax1.annotate(&amp;quot;20&amp;quot;, (1.1, 7), color=&#39;r&#39;)

    # Compute fitted values and residuals
    r = regr.fit(X, y)
    y_hat = r.predict(X)
    e = np.abs(y - y_hat)

    # Plot 2
    ax2.scatter(x=y_hat, y=e, facecolors=&#39;None&#39;, edgecolors=&#39;k&#39;, alpha=.5)
    ax2.set_xlabel(&#39;Fitted Values&#39;); ax2.set_ylabel(&#39;Residuals&#39;);
    ax2.hlines(0,xmin=ax2.xaxis.get_data_interval()[0],
               xmax=ax2.xaxis.get_data_interval()[1], linestyles=&#39;dotted&#39;,color=&#39;k&#39;)

    # Highlight outlier
    ax2.scatter(x=y_hat[20], y=e[20], facecolors=&#39;None&#39;, edgecolors=&#39;r&#39;, alpha=1) 
    ax2.annotate(&amp;quot;20&amp;quot;, (2.2, 3.6), color=&#39;r&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;make_fig_3_12()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/01_regression_150_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;high-leverage-points&#34;&gt;High Leverage Points&lt;/h3&gt;
&lt;p&gt;A better concept of &amp;ldquo;influential observation&amp;rdquo; is the Leverage, which represents how much an observation is distant from the others in terms of observables.&lt;/p&gt;
&lt;p&gt;The leverage formula of observation $i$ is&lt;/p&gt;
&lt;p&gt;$$
h_i = x_i (X&#39; X)^{-1} x_i&#39;
$$&lt;/p&gt;
&lt;p&gt;However, leverage alone is not necessarily enough for an observation to being highly influential.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s modify observation &lt;code&gt;41&lt;/code&gt; so that it has a high leverage.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Generate observation with high leverage
X[41] = 4
y[41] = 12

# Short regression without observation number 41
X_small = np.delete(X_small, 41)
y_small = np.delete(y_small, 41)

# Compute leverage
H = X @ inv(X.T @ X) @ X.T
h = np.diagonal(H)

# Compute fitted values and residuals
y_hat = X @ inv(X.T @ X) @ X.T @ y
e = np.abs(y - y_hat) 
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What happens now that we have added an observation with high leverage? How does the levarage look like?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Figure 3.13
def make_fig_3_13():

    # Init figure
    fig, (ax1,ax2) = plt.subplots(1,2, figsize=(12,5)) 
    fig.suptitle(&#39;Figure 3.12&#39;)

    # Plot 1
    ax1.scatter(x=X, y=y, facecolors=&#39;None&#39;, edgecolors=&#39;k&#39;, alpha=.5) 
    ax1.scatter(x=X[[20,41]], y=y[[20,41]], facecolors=&#39;None&#39;, edgecolors=&#39;r&#39;, alpha=1) 
    sns.regplot(x=X, y=y, ax=ax1, order=1, ci=None, scatter=False, line_kws={&#39;color&#39;:&#39;r&#39;, &#39;lw&#39;:1})
    sns.regplot(x=X_small, y=y_small, ax=ax1, order=1, ci=None, scatter=False, line_kws={&#39;color&#39;:&#39;b&#39;, &#39;lw&#39;:1})
    ax1.set_xlabel(&#39;X&#39;); ax1.set_ylabel(&#39;Y&#39;); ax1.axis(xmax=4.5);
    ax1.legend([&#39;With obs. 20,41&#39;, &#39;Without obs. 20,41&#39;]);

    # Highlight points
    ax1.annotate(&amp;quot;20&amp;quot;, (1.1, 7), color=&#39;r&#39;)
    ax1.annotate(&amp;quot;41&amp;quot;, (3.6, 12), color=&#39;r&#39;);



    # Plot 2
    ax2.scatter(x=h, y=e, facecolors=&#39;None&#39;, edgecolors=&#39;k&#39;, alpha=.5)
    ax2.set_xlabel(&#39;Leverage&#39;); ax2.set_ylabel(&#39;Residuals&#39;); 
    ax2.hlines(0,xmin=ax2.xaxis.get_data_interval()[0],
               xmax=ax2.xaxis.get_data_interval()[1], linestyles=&#39;dotted&#39;,color=&#39;k&#39;)
    # Highlight outlier
    ax2.scatter(x=h[[20,41]], y=e[[20,41]], facecolors=&#39;None&#39;, edgecolors=&#39;r&#39;, alpha=1);

    # Highlight points
    ax2.annotate(&amp;quot;20&amp;quot;, (0, 3.7), color=&#39;r&#39;)
    ax2.annotate(&amp;quot;41&amp;quot;, (0.14, 0.4), color=&#39;r&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;make_fig_3_13()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/01_regression_156_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;influential-observations&#34;&gt;Influential Observations&lt;/h3&gt;
&lt;p&gt;As we have seen, being an outliers or having high leverage alone might be not enough to conclude that an observation is influential.&lt;/p&gt;
&lt;p&gt;What really matters is a combination of both: observations with high leverage and high residuals, i.e. observations that are not only different in terms of observables (high leverage) but are also different in terms of their relationship between observables and dependent variable (high residual).&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s now modify observation &lt;code&gt;7&lt;/code&gt; so that it is an outlier and has high leverage.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Generate outlier with high leverage
X[7] = 4
y[7] = 7
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Short regression without observation number 41
X_small = np.delete(X, 7)
y_small = np.delete(y, 7)

# Compute leverage
H = X @ inv(X.T @ X) @ X.T
h = np.diagonal(H)

# Compute fitted values and residuals
r = regr.fit(X, y)
y_hat = r.predict(X)
e = np.abs(y - y_hat)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now the best linear fit line has noticeably moved.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def make_fig_extra_3():

    fig, (ax1,ax2) = plt.subplots(1,2, figsize=(12,5)) 

    # Plot 1
    ax1.scatter(x=X, y=y, facecolors=&#39;None&#39;, edgecolors=&#39;k&#39;, alpha=.5) 
    ax1.scatter(x=X[[7,20,41]], y=y[[7,20,41]], facecolors=&#39;None&#39;, edgecolors=&#39;r&#39;, alpha=1) 
    sns.regplot(x=X, y=y, ax=ax1, order=1, ci=None, scatter=False, line_kws={&#39;color&#39;:&#39;r&#39;, &#39;lw&#39;:1})
    sns.regplot(x=X_small, y=y_small, ax=ax1, order=1, ci=None, scatter=False, line_kws={&#39;color&#39;:&#39;b&#39;, &#39;lw&#39;:1})
    ax1.set_xlabel(&#39;X&#39;); ax1.set_ylabel(&#39;Y&#39;); ax1.axis(xmax=4.5);
    ax1.legend([&#39;With obs. 7,20,41&#39;, &#39;Without obs. 7,20,41&#39;]);

    # Highlight points
    ax1.annotate(&amp;quot;7&amp;quot;, (3.7, 7), color=&#39;r&#39;)
    ax1.annotate(&amp;quot;20&amp;quot;, (1.15, 7.05), color=&#39;r&#39;)
    ax1.annotate(&amp;quot;41&amp;quot;, (3.6, 12), color=&#39;r&#39;);



    # Plot 2
    ax2.scatter(x=h, y=e, facecolors=&#39;None&#39;, edgecolors=&#39;k&#39;, alpha=.5)
    ax2.set_xlabel(&#39;Leverage&#39;); ax2.set_ylabel(&#39;Residuals&#39;); 
    ax2.hlines(0,xmin=ax2.xaxis.get_data_interval()[0],
               xmax=ax2.xaxis.get_data_interval()[1], linestyles=&#39;dotted&#39;,color=&#39;k&#39;)
    # Highlight outlier
    ax2.scatter(x=h[[7,20,41]], y=e[[7,20,41]], facecolors=&#39;None&#39;, edgecolors=&#39;r&#39;, alpha=1);

    # Highlight points
    ax2.annotate(&amp;quot;7&amp;quot;, (0.12, 4.0), color=&#39;r&#39;);
    ax2.annotate(&amp;quot;20&amp;quot;, (0, 3.8), color=&#39;r&#39;)
    ax2.annotate(&amp;quot;41&amp;quot;, (0.12, 0.9), color=&#39;r&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;make_fig_extra_3()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/01_regression_164_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;collinearity&#34;&gt;Collinearity&lt;/h3&gt;
&lt;p&gt;Collinearity is the situation in which two dependent varaibles are higly correlated with each other. Algebraically, this is a problem because the $X&amp;rsquo;X$ matrix becomes almost-non-invertible.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s have a look at the &lt;code&gt;ratings&lt;/code&gt; dataset.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Inspect dataset
sns.pairplot(credit[[&#39;Age&#39;, &#39;Balance&#39;, &#39;Limit&#39;, &#39;Rating&#39;]], height=1.8);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/01_regression_167_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;If we zoom into the variable &lt;code&gt;Limit&lt;/code&gt;, we see that for example it is not very correlated with &lt;code&gt;Age&lt;/code&gt; but is very correlated with &lt;code&gt;Rating&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Figure 3.14
def make_fig_3_14():
    
    # Init figure
    fig, (ax1,ax2) = plt.subplots(1,2, figsize=(12,5)) 
    fig.suptitle(&#39;Figure 3.14&#39;)

    # Left plot
    ax1.scatter(credit.Limit, credit.Age, facecolor=&#39;None&#39;, edgecolor=&#39;brown&#39;)
    ax1.set_ylabel(&#39;Age&#39;)

    # Right plot
    ax2.scatter(credit.Limit, credit.Rating, facecolor=&#39;None&#39;, edgecolor=&#39;brown&#39;)
    ax2.set_ylabel(&#39;Rating&#39;)

    for ax in fig.axes:
        ax.set_xlabel(&#39;Limit&#39;)
        ax.set_xticks([2000,4000,6000,8000,12000])
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;make_fig_3_14()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/01_regression_170_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;If we regress &lt;code&gt;Balance&lt;/code&gt; on &lt;code&gt;Limit&lt;/code&gt; and &lt;code&gt;Age&lt;/code&gt;, the coefficient of &lt;code&gt;Limit&lt;/code&gt; is positive and highly significant.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Regress balance on limit and age
reg1 = sm.OLS.from_formula(&#39;Balance ~ Limit + Age&#39;, credit).fit()
reg1.summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
      &lt;td&gt;&lt;/td&gt;         &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt; &lt;td&gt; -173.4109&lt;/td&gt; &lt;td&gt;   43.828&lt;/td&gt; &lt;td&gt;   -3.957&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt; -259.576&lt;/td&gt; &lt;td&gt;  -87.246&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Limit&lt;/th&gt;     &lt;td&gt;    0.1734&lt;/td&gt; &lt;td&gt;    0.005&lt;/td&gt; &lt;td&gt;   34.496&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    0.163&lt;/td&gt; &lt;td&gt;    0.183&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Age&lt;/th&gt;       &lt;td&gt;   -2.2915&lt;/td&gt; &lt;td&gt;    0.672&lt;/td&gt; &lt;td&gt;   -3.407&lt;/td&gt; &lt;td&gt; 0.001&lt;/td&gt; &lt;td&gt;   -3.614&lt;/td&gt; &lt;td&gt;   -0.969&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;However, if we regress &lt;code&gt;Balance&lt;/code&gt; on &lt;code&gt;Limit&lt;/code&gt; and &lt;code&gt;Rating&lt;/code&gt;, the coefficient of &lt;code&gt;Limit&lt;/code&gt; is now not significant anymore.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Regress balance on limit and rating
reg2 = sm.OLS.from_formula(&#39;Balance ~ Limit + Rating&#39;, credit).fit()
reg2.summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
      &lt;td&gt;&lt;/td&gt;         &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt; &lt;td&gt; -377.5368&lt;/td&gt; &lt;td&gt;   45.254&lt;/td&gt; &lt;td&gt;   -8.343&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt; -466.505&lt;/td&gt; &lt;td&gt; -288.569&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Limit&lt;/th&gt;     &lt;td&gt;    0.0245&lt;/td&gt; &lt;td&gt;    0.064&lt;/td&gt; &lt;td&gt;    0.384&lt;/td&gt; &lt;td&gt; 0.701&lt;/td&gt; &lt;td&gt;   -0.101&lt;/td&gt; &lt;td&gt;    0.150&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Rating&lt;/th&gt;    &lt;td&gt;    2.2017&lt;/td&gt; &lt;td&gt;    0.952&lt;/td&gt; &lt;td&gt;    2.312&lt;/td&gt; &lt;td&gt; 0.021&lt;/td&gt; &lt;td&gt;    0.330&lt;/td&gt; &lt;td&gt;    4.074&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;Looking at the objective function, the Residual Sum of Squares, helps understanding what is the problem.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# First scale variables
y = credit.Balance
regr1 = LinearRegression().fit(scale(credit[[&#39;Age&#39;, &#39;Limit&#39;]].astype(&#39;float&#39;), with_std=False), y)
regr2 = LinearRegression().fit(scale(credit[[&#39;Rating&#39;, &#39;Limit&#39;]], with_std=False), y)

# Create grid coordinates for plotting
B_Age = np.linspace(regr1.coef_[0]-3, regr1.coef_[0]+3, 100)
B_Limit = np.linspace(regr1.coef_[1]-0.02, regr1.coef_[1]+0.02, 100)

B_Rating = np.linspace(regr2.coef_[0]-3, regr2.coef_[0]+3, 100)
B_Limit2 = np.linspace(regr2.coef_[1]-0.2, regr2.coef_[1]+0.2, 100)

X1, Y1 = np.meshgrid(B_Limit, B_Age, indexing=&#39;xy&#39;)
X2, Y2 = np.meshgrid(B_Limit2, B_Rating, indexing=&#39;xy&#39;)
Z1 = np.zeros((B_Age.size,B_Limit.size))
Z2 = np.zeros((B_Rating.size,B_Limit2.size))

Limit_scaled = scale(credit.Limit.astype(&#39;float&#39;), with_std=False)
Age_scaled = scale(credit.Age.astype(&#39;float&#39;), with_std=False)
Rating_scaled = scale(credit.Rating.astype(&#39;float&#39;), with_std=False)

# Calculate Z-values (RSS) based on grid of coefficients
for (i,j),v in np.ndenumerate(Z1):
    Z1[i,j] =((y - (regr1.intercept_ + X1[i,j]*Limit_scaled +
                    Y1[i,j]*Age_scaled))**2).sum()/1000000
    
for (i,j),v in np.ndenumerate(Z2):
    Z2[i,j] =((y - (regr2.intercept_ + X2[i,j]*Limit_scaled +
                    Y2[i,j]*Rating_scaled))**2).sum()/1000000
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Figure 3.15
def make_fig_3_15():

    # Init figure
    fig, (ax1,ax2) = plt.subplots(1,2, figsize=(12,5)) 
    fig.suptitle(&#39;Figure 3.15&#39;)

    # Minimum
    min_RSS = r&#39;$\beta_0$, $\beta_1$ for minimized RSS&#39;

    # Left plot
    CS = ax1.contour(X1, Y1, Z1, cmap=plt.cm.Set1, levels=[21.25, 21.5, 21.8])
    ax1.scatter(reg1.params[1], reg1.params[2], c=&#39;r&#39;, label=min_RSS)
    ax1.clabel(CS, inline=True, fontsize=10, fmt=&#39;%1.1f&#39;)
    ax1.set_ylabel(r&#39;$\beta_{Age}$&#39;)

    # Right plot
    CS = ax2.contour(X2, Y2, Z2, cmap=plt.cm.Set1, levels=[21.5, 21.8])
    ax2.scatter(reg2.params[1], reg2.params[2], c=&#39;r&#39;, label=min_RSS)
    ax2.clabel(CS, inline=True, fontsize=10, fmt=&#39;%1.1f&#39;)
    ax2.set_ylabel(r&#39;$\beta_{Rating}$&#39;)
    #ax2.set_xticks([-0.1, 0, 0.1, 0.2])

    for ax in fig.axes:
        ax.set_xlabel(r&#39;$\beta_{Limit}$&#39;)
        ax.legend()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;make_fig_3_15()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/01_regression_178_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;As we can see, in the left plot the minimum is much better defined than in the right plot.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Data Exploration</title>
      <link>https://matteocourthoud.github.io/course/data-science/01_data_exploration/</link>
      <pubDate>Sun, 27 Feb 2022 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/course/data-science/01_data_exploration/</guid>
      <description>&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import numpy as np
import pandas as pd
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;setup&#34;&gt;Setup&lt;/h2&gt;
&lt;p&gt;For the scope of this tutorial we are going to use AirBnb Scraped data for the city of Bologna. The data is freely available at &lt;strong&gt;Inside AirBnb&lt;/strong&gt;: &lt;a href=&#34;http://insideairbnb.com/get-the-data.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://insideairbnb.com/get-the-data.html&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;A description of all variables in all datasets is avaliable &lt;a href=&#34;https://docs.google.com/spreadsheets/d/1iWCNJcSutYqpULSQHlNyGInUvHg2BoUGoNRIGa6Szc4/edit#gid=982310896&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;We are going to use 2 datasets:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;listing dataset: contains listing-level information&lt;/li&gt;
&lt;li&gt;pricing dataset: contains pricing data, over time&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;importing-data&#34;&gt;Importing Data&lt;/h2&gt;
&lt;p&gt;Pandas has a variety of function to import data&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;pd.read_csv()&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;pd.read_html()&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;pd.read_parquet()&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Importatly for our purpose, &lt;code&gt;pd.read_csv()&lt;/code&gt; can directly import data from the web.&lt;/p&gt;
&lt;p&gt;The first dataset that we are going to import is the dataset of Airbnb listings in Bologna. It contains listing-level information.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;url_listings = &amp;quot;http://data.insideairbnb.com/italy/emilia-romagna/bologna/2021-12-17/visualisations/listings.csv&amp;quot;
df_listings = pd.read_csv(url_listings)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The second dataset that we are going to use is the dataset of calendar prices. This time the dataset is compressed but we can use the &lt;code&gt;compression&lt;/code&gt; option to import it directly.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;url_prices = &amp;quot;http://data.insideairbnb.com/italy/emilia-romagna/bologna/2021-12-17/data/calendar.csv.gz&amp;quot;
df_prices = pd.read_csv(url_prices, compression=&amp;quot;gzip&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;inspecting-data&#34;&gt;Inspecting Data&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Methods&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;info()&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;head()&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;describe()&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;p&gt;The first way yo have a quick look at the data is the &lt;code&gt;info()&lt;/code&gt; method. If called with the option &lt;code&gt;verbose=False&lt;/code&gt;, it gives a quick overview of the dimensions of the data.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_listings.info(verbose=False)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;class &#39;pandas.core.frame.DataFrame&#39;&amp;gt;
RangeIndex: 3453 entries, 0 to 3452
Columns: 18 entries, id to license
dtypes: float64(4), int64(8), object(6)
memory usage: 485.7+ KB
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If we want to know how the data looks like, we can use the &lt;code&gt;head()&lt;/code&gt; method. It prints the first 5 lines of the data by default.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_listings.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;id&lt;/th&gt;
      &lt;th&gt;name&lt;/th&gt;
      &lt;th&gt;host_id&lt;/th&gt;
      &lt;th&gt;host_name&lt;/th&gt;
      &lt;th&gt;neighbourhood_group&lt;/th&gt;
      &lt;th&gt;neighbourhood&lt;/th&gt;
      &lt;th&gt;latitude&lt;/th&gt;
      &lt;th&gt;longitude&lt;/th&gt;
      &lt;th&gt;room_type&lt;/th&gt;
      &lt;th&gt;price&lt;/th&gt;
      &lt;th&gt;minimum_nights&lt;/th&gt;
      &lt;th&gt;number_of_reviews&lt;/th&gt;
      &lt;th&gt;last_review&lt;/th&gt;
      &lt;th&gt;reviews_per_month&lt;/th&gt;
      &lt;th&gt;calculated_host_listings_count&lt;/th&gt;
      &lt;th&gt;availability_365&lt;/th&gt;
      &lt;th&gt;number_of_reviews_ltm&lt;/th&gt;
      &lt;th&gt;license&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;42196&lt;/td&gt;
      &lt;td&gt;50 sm Studio in the historic centre&lt;/td&gt;
      &lt;td&gt;184487&lt;/td&gt;
      &lt;td&gt;Carlo&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;Santo Stefano&lt;/td&gt;
      &lt;td&gt;44.48507&lt;/td&gt;
      &lt;td&gt;11.34786&lt;/td&gt;
      &lt;td&gt;Entire home/apt&lt;/td&gt;
      &lt;td&gt;68&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;180&lt;/td&gt;
      &lt;td&gt;2021-11-12&lt;/td&gt;
      &lt;td&gt;1.32&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;161&lt;/td&gt;
      &lt;td&gt;6&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;46352&lt;/td&gt;
      &lt;td&gt;A room in Pasolini&#39;s house&lt;/td&gt;
      &lt;td&gt;467810&lt;/td&gt;
      &lt;td&gt;Eleonora&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;Porto - Saragozza&lt;/td&gt;
      &lt;td&gt;44.49168&lt;/td&gt;
      &lt;td&gt;11.33514&lt;/td&gt;
      &lt;td&gt;Private room&lt;/td&gt;
      &lt;td&gt;29&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;300&lt;/td&gt;
      &lt;td&gt;2021-11-30&lt;/td&gt;
      &lt;td&gt;2.20&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;248&lt;/td&gt;
      &lt;td&gt;37&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;59697&lt;/td&gt;
      &lt;td&gt;COZY LARGE BEDROOM in the city center&lt;/td&gt;
      &lt;td&gt;286688&lt;/td&gt;
      &lt;td&gt;Paolo&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;Santo Stefano&lt;/td&gt;
      &lt;td&gt;44.48817&lt;/td&gt;
      &lt;td&gt;11.34124&lt;/td&gt;
      &lt;td&gt;Private room&lt;/td&gt;
      &lt;td&gt;50&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;240&lt;/td&gt;
      &lt;td&gt;2020-10-04&lt;/td&gt;
      &lt;td&gt;2.18&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;327&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;85368&lt;/td&gt;
      &lt;td&gt;Garden House Bologna&lt;/td&gt;
      &lt;td&gt;467675&lt;/td&gt;
      &lt;td&gt;Anna Maria&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;Santo Stefano&lt;/td&gt;
      &lt;td&gt;44.47834&lt;/td&gt;
      &lt;td&gt;11.35672&lt;/td&gt;
      &lt;td&gt;Entire home/apt&lt;/td&gt;
      &lt;td&gt;126&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;40&lt;/td&gt;
      &lt;td&gt;2019-11-03&lt;/td&gt;
      &lt;td&gt;0.34&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;332&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;145779&lt;/td&gt;
      &lt;td&gt;SINGLE ROOM&lt;/td&gt;
      &lt;td&gt;705535&lt;/td&gt;
      &lt;td&gt;Valerio&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;Porto - Saragozza&lt;/td&gt;
      &lt;td&gt;44.49306&lt;/td&gt;
      &lt;td&gt;11.33786&lt;/td&gt;
      &lt;td&gt;Private room&lt;/td&gt;
      &lt;td&gt;50&lt;/td&gt;
      &lt;td&gt;10&lt;/td&gt;
      &lt;td&gt;69&lt;/td&gt;
      &lt;td&gt;2021-12-05&lt;/td&gt;
      &lt;td&gt;0.55&lt;/td&gt;
      &lt;td&gt;9&lt;/td&gt;
      &lt;td&gt;365&lt;/td&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;We can print a description of the data using &lt;code&gt;describe()&lt;/code&gt;. If we have many variables, it&amp;rsquo;s best to print it transposed using the &lt;code&gt;.T&lt;/code&gt; attribute.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_listings.describe().T[:5]
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;count&lt;/th&gt;
      &lt;th&gt;mean&lt;/th&gt;
      &lt;th&gt;std&lt;/th&gt;
      &lt;th&gt;min&lt;/th&gt;
      &lt;th&gt;25%&lt;/th&gt;
      &lt;th&gt;50%&lt;/th&gt;
      &lt;th&gt;75%&lt;/th&gt;
      &lt;th&gt;max&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;id&lt;/th&gt;
      &lt;td&gt;3453.0&lt;/td&gt;
      &lt;td&gt;2.950218e+07&lt;/td&gt;
      &lt;td&gt;1.523988e+07&lt;/td&gt;
      &lt;td&gt;42196.0000&lt;/td&gt;
      &lt;td&gt;1.748597e+07&lt;/td&gt;
      &lt;td&gt;3.078707e+07&lt;/td&gt;
      &lt;td&gt;4.220094e+07&lt;/td&gt;
      &lt;td&gt;5.385496e+07&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;host_id&lt;/th&gt;
      &lt;td&gt;3453.0&lt;/td&gt;
      &lt;td&gt;1.236424e+08&lt;/td&gt;
      &lt;td&gt;1.160756e+08&lt;/td&gt;
      &lt;td&gt;38468.0000&lt;/td&gt;
      &lt;td&gt;2.550007e+07&lt;/td&gt;
      &lt;td&gt;8.845438e+07&lt;/td&gt;
      &lt;td&gt;2.005926e+08&lt;/td&gt;
      &lt;td&gt;4.354316e+08&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;neighbourhood_group&lt;/th&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;latitude&lt;/th&gt;
      &lt;td&gt;3453.0&lt;/td&gt;
      &lt;td&gt;4.449756e+01&lt;/td&gt;
      &lt;td&gt;1.173569e-02&lt;/td&gt;
      &lt;td&gt;44.4236&lt;/td&gt;
      &lt;td&gt;4.449186e+01&lt;/td&gt;
      &lt;td&gt;4.449699e+01&lt;/td&gt;
      &lt;td&gt;4.450271e+01&lt;/td&gt;
      &lt;td&gt;4.455093e+01&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;longitude&lt;/th&gt;
      &lt;td&gt;3453.0&lt;/td&gt;
      &lt;td&gt;1.134509e+01&lt;/td&gt;
      &lt;td&gt;1.986071e-02&lt;/td&gt;
      &lt;td&gt;11.2320&lt;/td&gt;
      &lt;td&gt;1.133732e+01&lt;/td&gt;
      &lt;td&gt;1.134519e+01&lt;/td&gt;
      &lt;td&gt;1.135406e+01&lt;/td&gt;
      &lt;td&gt;1.142027e+01&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;We can get the list of columns using the &lt;code&gt;.columns&lt;/code&gt; attribute.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_listings.columns
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Index([&#39;id&#39;, &#39;name&#39;, &#39;host_id&#39;, &#39;host_name&#39;, &#39;neighbourhood_group&#39;,
       &#39;neighbourhood&#39;, &#39;latitude&#39;, &#39;longitude&#39;, &#39;room_type&#39;, &#39;price&#39;,
       &#39;minimum_nights&#39;, &#39;number_of_reviews&#39;, &#39;last_review&#39;,
       &#39;reviews_per_month&#39;, &#39;calculated_host_listings_count&#39;,
       &#39;availability_365&#39;, &#39;number_of_reviews_ltm&#39;, &#39;license&#39;],
      dtype=&#39;object&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can get the index using the &lt;code&gt;.index&lt;/code&gt; attribute,&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_listings.index
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;RangeIndex(start=0, stop=3453, step=1)
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;data-selection&#34;&gt;Data Selection&lt;/h2&gt;
&lt;p&gt;We can access single columns as if the DataFrame was a dictionary.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_listings[&#39;price&#39;]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;0        68
1        29
2        50
3       126
4        50
       ... 
3448     32
3449     45
3450     50
3451    134
3452    115
Name: price, Length: 3453, dtype: int64
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can select rows and columns by index, using the &lt;code&gt;.iloc&lt;/code&gt; attribute.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_listings.iloc[:7, 5:9]
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;neighbourhood&lt;/th&gt;
      &lt;th&gt;latitude&lt;/th&gt;
      &lt;th&gt;longitude&lt;/th&gt;
      &lt;th&gt;room_type&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;Santo Stefano&lt;/td&gt;
      &lt;td&gt;44.48507&lt;/td&gt;
      &lt;td&gt;11.34786&lt;/td&gt;
      &lt;td&gt;Entire home/apt&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;Porto - Saragozza&lt;/td&gt;
      &lt;td&gt;44.49168&lt;/td&gt;
      &lt;td&gt;11.33514&lt;/td&gt;
      &lt;td&gt;Private room&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;Santo Stefano&lt;/td&gt;
      &lt;td&gt;44.48817&lt;/td&gt;
      &lt;td&gt;11.34124&lt;/td&gt;
      &lt;td&gt;Private room&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;Santo Stefano&lt;/td&gt;
      &lt;td&gt;44.47834&lt;/td&gt;
      &lt;td&gt;11.35672&lt;/td&gt;
      &lt;td&gt;Entire home/apt&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;Porto - Saragozza&lt;/td&gt;
      &lt;td&gt;44.49306&lt;/td&gt;
      &lt;td&gt;11.33786&lt;/td&gt;
      &lt;td&gt;Private room&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;5&lt;/th&gt;
      &lt;td&gt;Navile&lt;/td&gt;
      &lt;td&gt;44.51628&lt;/td&gt;
      &lt;td&gt;11.33074&lt;/td&gt;
      &lt;td&gt;Private room&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;6&lt;/th&gt;
      &lt;td&gt;Santo Stefano&lt;/td&gt;
      &lt;td&gt;44.48787&lt;/td&gt;
      &lt;td&gt;11.35392&lt;/td&gt;
      &lt;td&gt;Entire home/apt&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;If we want to condition only on rows or columns, we have use &lt;code&gt;:&lt;/code&gt; for the unrestricted dimesion, otherwise we get an error.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_listings.iloc[:, 5:9].head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;neighbourhood&lt;/th&gt;
      &lt;th&gt;latitude&lt;/th&gt;
      &lt;th&gt;longitude&lt;/th&gt;
      &lt;th&gt;room_type&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;Santo Stefano&lt;/td&gt;
      &lt;td&gt;44.48507&lt;/td&gt;
      &lt;td&gt;11.34786&lt;/td&gt;
      &lt;td&gt;Entire home/apt&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;Porto - Saragozza&lt;/td&gt;
      &lt;td&gt;44.49168&lt;/td&gt;
      &lt;td&gt;11.33514&lt;/td&gt;
      &lt;td&gt;Private room&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;Santo Stefano&lt;/td&gt;
      &lt;td&gt;44.48817&lt;/td&gt;
      &lt;td&gt;11.34124&lt;/td&gt;
      &lt;td&gt;Private room&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;Santo Stefano&lt;/td&gt;
      &lt;td&gt;44.47834&lt;/td&gt;
      &lt;td&gt;11.35672&lt;/td&gt;
      &lt;td&gt;Entire home/apt&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;Porto - Saragozza&lt;/td&gt;
      &lt;td&gt;44.49306&lt;/td&gt;
      &lt;td&gt;11.33786&lt;/td&gt;
      &lt;td&gt;Private room&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;Instead, the &lt;code&gt;.loc&lt;/code&gt; attribute allows us to use row and column names.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_listings.loc[:, [&#39;neighbourhood&#39;, &#39;latitude&#39;, &#39;longitude&#39;]].head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;neighbourhood&lt;/th&gt;
      &lt;th&gt;latitude&lt;/th&gt;
      &lt;th&gt;longitude&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;Santo Stefano&lt;/td&gt;
      &lt;td&gt;44.48507&lt;/td&gt;
      &lt;td&gt;11.34786&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;Porto - Saragozza&lt;/td&gt;
      &lt;td&gt;44.49168&lt;/td&gt;
      &lt;td&gt;11.33514&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;Santo Stefano&lt;/td&gt;
      &lt;td&gt;44.48817&lt;/td&gt;
      &lt;td&gt;11.34124&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;Santo Stefano&lt;/td&gt;
      &lt;td&gt;44.47834&lt;/td&gt;
      &lt;td&gt;11.35672&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;Porto - Saragozza&lt;/td&gt;
      &lt;td&gt;44.49306&lt;/td&gt;
      &lt;td&gt;11.33786&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;We can also select ranges.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_listings.loc[:, &#39;neighbourhood&#39;:&#39;room_type&#39;].head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;neighbourhood&lt;/th&gt;
      &lt;th&gt;latitude&lt;/th&gt;
      &lt;th&gt;longitude&lt;/th&gt;
      &lt;th&gt;room_type&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;Santo Stefano&lt;/td&gt;
      &lt;td&gt;44.48507&lt;/td&gt;
      &lt;td&gt;11.34786&lt;/td&gt;
      &lt;td&gt;Entire home/apt&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;Porto - Saragozza&lt;/td&gt;
      &lt;td&gt;44.49168&lt;/td&gt;
      &lt;td&gt;11.33514&lt;/td&gt;
      &lt;td&gt;Private room&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;Santo Stefano&lt;/td&gt;
      &lt;td&gt;44.48817&lt;/td&gt;
      &lt;td&gt;11.34124&lt;/td&gt;
      &lt;td&gt;Private room&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;Santo Stefano&lt;/td&gt;
      &lt;td&gt;44.47834&lt;/td&gt;
      &lt;td&gt;11.35672&lt;/td&gt;
      &lt;td&gt;Entire home/apt&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;Porto - Saragozza&lt;/td&gt;
      &lt;td&gt;44.49306&lt;/td&gt;
      &lt;td&gt;11.33786&lt;/td&gt;
      &lt;td&gt;Private room&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;There is an easy way to &lt;strong&gt;select numerical columns&lt;/strong&gt;, the &lt;code&gt;.select_dtypes()&lt;/code&gt; function.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_listings.select_dtypes(include=[&#39;number&#39;]).head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;id&lt;/th&gt;
      &lt;th&gt;host_id&lt;/th&gt;
      &lt;th&gt;neighbourhood_group&lt;/th&gt;
      &lt;th&gt;latitude&lt;/th&gt;
      &lt;th&gt;longitude&lt;/th&gt;
      &lt;th&gt;price&lt;/th&gt;
      &lt;th&gt;minimum_nights&lt;/th&gt;
      &lt;th&gt;number_of_reviews&lt;/th&gt;
      &lt;th&gt;reviews_per_month&lt;/th&gt;
      &lt;th&gt;calculated_host_listings_count&lt;/th&gt;
      &lt;th&gt;availability_365&lt;/th&gt;
      &lt;th&gt;number_of_reviews_ltm&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;42196&lt;/td&gt;
      &lt;td&gt;184487&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;44.48507&lt;/td&gt;
      &lt;td&gt;11.34786&lt;/td&gt;
      &lt;td&gt;68&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;180&lt;/td&gt;
      &lt;td&gt;1.32&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;161&lt;/td&gt;
      &lt;td&gt;6&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;46352&lt;/td&gt;
      &lt;td&gt;467810&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;44.49168&lt;/td&gt;
      &lt;td&gt;11.33514&lt;/td&gt;
      &lt;td&gt;29&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;300&lt;/td&gt;
      &lt;td&gt;2.20&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;248&lt;/td&gt;
      &lt;td&gt;37&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;59697&lt;/td&gt;
      &lt;td&gt;286688&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;44.48817&lt;/td&gt;
      &lt;td&gt;11.34124&lt;/td&gt;
      &lt;td&gt;50&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;240&lt;/td&gt;
      &lt;td&gt;2.18&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;327&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;85368&lt;/td&gt;
      &lt;td&gt;467675&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;44.47834&lt;/td&gt;
      &lt;td&gt;11.35672&lt;/td&gt;
      &lt;td&gt;126&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;40&lt;/td&gt;
      &lt;td&gt;0.34&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;332&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;145779&lt;/td&gt;
      &lt;td&gt;705535&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;44.49306&lt;/td&gt;
      &lt;td&gt;11.33786&lt;/td&gt;
      &lt;td&gt;50&lt;/td&gt;
      &lt;td&gt;10&lt;/td&gt;
      &lt;td&gt;69&lt;/td&gt;
      &lt;td&gt;0.55&lt;/td&gt;
      &lt;td&gt;9&lt;/td&gt;
      &lt;td&gt;365&lt;/td&gt;
      &lt;td&gt;5&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;Other types include&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;object&lt;/code&gt; for strings&lt;/li&gt;
&lt;li&gt;&lt;code&gt;bool&lt;/code&gt; for booleans&lt;/li&gt;
&lt;li&gt;&lt;code&gt;int&lt;/code&gt; for integers&lt;/li&gt;
&lt;li&gt;&lt;code&gt;float&lt;/code&gt; for floats (numbers that are not integers)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We can also use logical operators to selet rows.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_listings.loc[df_listings[&#39;number_of_reviews&#39;]&amp;gt;500, :].head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;id&lt;/th&gt;
      &lt;th&gt;name&lt;/th&gt;
      &lt;th&gt;host_id&lt;/th&gt;
      &lt;th&gt;host_name&lt;/th&gt;
      &lt;th&gt;neighbourhood_group&lt;/th&gt;
      &lt;th&gt;neighbourhood&lt;/th&gt;
      &lt;th&gt;latitude&lt;/th&gt;
      &lt;th&gt;longitude&lt;/th&gt;
      &lt;th&gt;room_type&lt;/th&gt;
      &lt;th&gt;price&lt;/th&gt;
      &lt;th&gt;minimum_nights&lt;/th&gt;
      &lt;th&gt;number_of_reviews&lt;/th&gt;
      &lt;th&gt;last_review&lt;/th&gt;
      &lt;th&gt;reviews_per_month&lt;/th&gt;
      &lt;th&gt;calculated_host_listings_count&lt;/th&gt;
      &lt;th&gt;availability_365&lt;/th&gt;
      &lt;th&gt;number_of_reviews_ltm&lt;/th&gt;
      &lt;th&gt;license&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;52&lt;/th&gt;
      &lt;td&gt;884148&lt;/td&gt;
      &lt;td&gt;APOSA FLAT / CITY CENTER - BO&lt;/td&gt;
      &lt;td&gt;4664996&lt;/td&gt;
      &lt;td&gt;Vie D&#39;Acqua Di Sandra Maria&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;Santo Stefano&lt;/td&gt;
      &lt;td&gt;44.49945&lt;/td&gt;
      &lt;td&gt;11.34566&lt;/td&gt;
      &lt;td&gt;Entire home/apt&lt;/td&gt;
      &lt;td&gt;46&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;668&lt;/td&gt;
      &lt;td&gt;2021-12-11&lt;/td&gt;
      &lt;td&gt;6.24&lt;/td&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;252&lt;/td&gt;
      &lt;td&gt;20&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;92&lt;/th&gt;
      &lt;td&gt;1435627&lt;/td&gt;
      &lt;td&gt;heart of Bologna Piazza Maggiore&lt;/td&gt;
      &lt;td&gt;7714013&lt;/td&gt;
      &lt;td&gt;Carlotta&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;Porto - Saragozza&lt;/td&gt;
      &lt;td&gt;44.49321&lt;/td&gt;
      &lt;td&gt;11.33569&lt;/td&gt;
      &lt;td&gt;Entire home/apt&lt;/td&gt;
      &lt;td&gt;56&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;508&lt;/td&gt;
      &lt;td&gt;2021-12-12&lt;/td&gt;
      &lt;td&gt;5.08&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;131&lt;/td&gt;
      &lt;td&gt;69&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;98&lt;/th&gt;
      &lt;td&gt;1566003&lt;/td&gt;
      &lt;td&gt;&#34;i portici di via Piella &#34;&lt;/td&gt;
      &lt;td&gt;8325248&lt;/td&gt;
      &lt;td&gt;Massimo&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;Santo Stefano&lt;/td&gt;
      &lt;td&gt;44.49855&lt;/td&gt;
      &lt;td&gt;11.34411&lt;/td&gt;
      &lt;td&gt;Entire home/apt&lt;/td&gt;
      &lt;td&gt;51&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;764&lt;/td&gt;
      &lt;td&gt;2021-12-14&lt;/td&gt;
      &lt;td&gt;7.62&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;119&lt;/td&gt;
      &lt;td&gt;120&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;131&lt;/th&gt;
      &lt;td&gt;2282623&lt;/td&gt;
      &lt;td&gt;S.Orsola zone,parking for free and self check-in&lt;/td&gt;
      &lt;td&gt;11658074&lt;/td&gt;
      &lt;td&gt;Cecilia&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;San Donato - San Vitale&lt;/td&gt;
      &lt;td&gt;44.49328&lt;/td&gt;
      &lt;td&gt;11.36650&lt;/td&gt;
      &lt;td&gt;Entire home/apt&lt;/td&gt;
      &lt;td&gt;38&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;689&lt;/td&gt;
      &lt;td&gt;2021-10-24&lt;/td&gt;
      &lt;td&gt;7.20&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;72&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;175&lt;/th&gt;
      &lt;td&gt;3216486&lt;/td&gt;
      &lt;td&gt;Stanza Privata&lt;/td&gt;
      &lt;td&gt;16289536&lt;/td&gt;
      &lt;td&gt;Fabio&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;Navile&lt;/td&gt;
      &lt;td&gt;44.50903&lt;/td&gt;
      &lt;td&gt;11.34200&lt;/td&gt;
      &lt;td&gt;Private room&lt;/td&gt;
      &lt;td&gt;82&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;569&lt;/td&gt;
      &lt;td&gt;2021-12-05&lt;/td&gt;
      &lt;td&gt;6.93&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;7&lt;/td&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;We can use logical operations as well. But remember to use paranthesis.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: the &lt;code&gt;and&lt;/code&gt; and &lt;code&gt;or&lt;/code&gt; expressions do not work in this setting. We have to use &lt;code&gt;&amp;amp;&lt;/code&gt; and &lt;code&gt;|&lt;/code&gt; instead.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_listings.loc[(df_listings[&#39;number_of_reviews&#39;]&amp;gt;300) &amp;amp;
                (df_listings[&#39;reviews_per_month&#39;]&amp;gt;7), 
                :].head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;id&lt;/th&gt;
      &lt;th&gt;name&lt;/th&gt;
      &lt;th&gt;host_id&lt;/th&gt;
      &lt;th&gt;host_name&lt;/th&gt;
      &lt;th&gt;neighbourhood_group&lt;/th&gt;
      &lt;th&gt;neighbourhood&lt;/th&gt;
      &lt;th&gt;latitude&lt;/th&gt;
      &lt;th&gt;longitude&lt;/th&gt;
      &lt;th&gt;room_type&lt;/th&gt;
      &lt;th&gt;price&lt;/th&gt;
      &lt;th&gt;minimum_nights&lt;/th&gt;
      &lt;th&gt;number_of_reviews&lt;/th&gt;
      &lt;th&gt;last_review&lt;/th&gt;
      &lt;th&gt;reviews_per_month&lt;/th&gt;
      &lt;th&gt;calculated_host_listings_count&lt;/th&gt;
      &lt;th&gt;availability_365&lt;/th&gt;
      &lt;th&gt;number_of_reviews_ltm&lt;/th&gt;
      &lt;th&gt;license&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;98&lt;/th&gt;
      &lt;td&gt;1566003&lt;/td&gt;
      &lt;td&gt;&#34;i portici di via Piella &#34;&lt;/td&gt;
      &lt;td&gt;8325248&lt;/td&gt;
      &lt;td&gt;Massimo&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;Santo Stefano&lt;/td&gt;
      &lt;td&gt;44.498550&lt;/td&gt;
      &lt;td&gt;11.344110&lt;/td&gt;
      &lt;td&gt;Entire home/apt&lt;/td&gt;
      &lt;td&gt;51&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;764&lt;/td&gt;
      &lt;td&gt;2021-12-14&lt;/td&gt;
      &lt;td&gt;7.62&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;119&lt;/td&gt;
      &lt;td&gt;120&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;131&lt;/th&gt;
      &lt;td&gt;2282623&lt;/td&gt;
      &lt;td&gt;S.Orsola zone,parking for free and self check-in&lt;/td&gt;
      &lt;td&gt;11658074&lt;/td&gt;
      &lt;td&gt;Cecilia&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;San Donato - San Vitale&lt;/td&gt;
      &lt;td&gt;44.493280&lt;/td&gt;
      &lt;td&gt;11.366500&lt;/td&gt;
      &lt;td&gt;Entire home/apt&lt;/td&gt;
      &lt;td&gt;38&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;689&lt;/td&gt;
      &lt;td&gt;2021-10-24&lt;/td&gt;
      &lt;td&gt;7.20&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;72&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;204&lt;/th&gt;
      &lt;td&gt;4166793&lt;/td&gt;
      &lt;td&gt;Centralissimo a Bologna&lt;/td&gt;
      &lt;td&gt;8325248&lt;/td&gt;
      &lt;td&gt;Massimo&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;Santo Stefano&lt;/td&gt;
      &lt;td&gt;44.500920&lt;/td&gt;
      &lt;td&gt;11.344560&lt;/td&gt;
      &lt;td&gt;Entire home/apt&lt;/td&gt;
      &lt;td&gt;71&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;750&lt;/td&gt;
      &lt;td&gt;2021-12-10&lt;/td&gt;
      &lt;td&gt;9.21&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;233&lt;/td&gt;
      &lt;td&gt;84&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;751&lt;/th&gt;
      &lt;td&gt;15508481&lt;/td&gt;
      &lt;td&gt;Monolocale in zona fiera /centro&lt;/td&gt;
      &lt;td&gt;99632788&lt;/td&gt;
      &lt;td&gt;Walid&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;Navile&lt;/td&gt;
      &lt;td&gt;44.514462&lt;/td&gt;
      &lt;td&gt;11.353731&lt;/td&gt;
      &lt;td&gt;Entire home/apt&lt;/td&gt;
      &lt;td&gt;64&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;475&lt;/td&gt;
      &lt;td&gt;2021-12-01&lt;/td&gt;
      &lt;td&gt;7.56&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;48&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;773&lt;/th&gt;
      &lt;td&gt;15886516&lt;/td&gt;
      &lt;td&gt;Monolocale nel cuore del ghetto ebraico di Bol...&lt;/td&gt;
      &lt;td&gt;103024123&lt;/td&gt;
      &lt;td&gt;Catia&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;Santo Stefano&lt;/td&gt;
      &lt;td&gt;44.495080&lt;/td&gt;
      &lt;td&gt;11.347220&lt;/td&gt;
      &lt;td&gt;Entire home/apt&lt;/td&gt;
      &lt;td&gt;58&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;428&lt;/td&gt;
      &lt;td&gt;2021-12-15&lt;/td&gt;
      &lt;td&gt;7.88&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;285&lt;/td&gt;
      &lt;td&gt;17&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;For a single column (i.e. a Series), we can get the unique values using the &lt;code&gt;unique()&lt;/code&gt; function.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_listings[&#39;neighbourhood&#39;].unique()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;array([&#39;Santo Stefano&#39;, &#39;Porto - Saragozza&#39;, &#39;Navile&#39;,
       &#39;San Donato - San Vitale&#39;, &#39;Savena&#39;, &#39;Borgo Panigale - Reno&#39;],
      dtype=object)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For multiple columns, we can use the &lt;code&gt;drop_duplicates&lt;/code&gt; function.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_listings[[&#39;neighbourhood&#39;, &#39;room_type&#39;]].drop_duplicates()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;neighbourhood&lt;/th&gt;
      &lt;th&gt;room_type&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;Santo Stefano&lt;/td&gt;
      &lt;td&gt;Entire home/apt&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;Porto - Saragozza&lt;/td&gt;
      &lt;td&gt;Private room&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;Santo Stefano&lt;/td&gt;
      &lt;td&gt;Private room&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;5&lt;/th&gt;
      &lt;td&gt;Navile&lt;/td&gt;
      &lt;td&gt;Private room&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;7&lt;/th&gt;
      &lt;td&gt;Navile&lt;/td&gt;
      &lt;td&gt;Entire home/apt&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;8&lt;/th&gt;
      &lt;td&gt;Porto - Saragozza&lt;/td&gt;
      &lt;td&gt;Entire home/apt&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;19&lt;/th&gt;
      &lt;td&gt;San Donato - San Vitale&lt;/td&gt;
      &lt;td&gt;Private room&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;24&lt;/th&gt;
      &lt;td&gt;Savena&lt;/td&gt;
      &lt;td&gt;Private room&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;36&lt;/th&gt;
      &lt;td&gt;Borgo Panigale - Reno&lt;/td&gt;
      &lt;td&gt;Entire home/apt&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;41&lt;/th&gt;
      &lt;td&gt;San Donato - San Vitale&lt;/td&gt;
      &lt;td&gt;Entire home/apt&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;70&lt;/th&gt;
      &lt;td&gt;Porto - Saragozza&lt;/td&gt;
      &lt;td&gt;Hotel room&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;75&lt;/th&gt;
      &lt;td&gt;Borgo Panigale - Reno&lt;/td&gt;
      &lt;td&gt;Private room&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;110&lt;/th&gt;
      &lt;td&gt;Santo Stefano&lt;/td&gt;
      &lt;td&gt;Hotel room&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;111&lt;/th&gt;
      &lt;td&gt;Savena&lt;/td&gt;
      &lt;td&gt;Entire home/apt&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;388&lt;/th&gt;
      &lt;td&gt;Porto - Saragozza&lt;/td&gt;
      &lt;td&gt;Shared room&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;678&lt;/th&gt;
      &lt;td&gt;Navile&lt;/td&gt;
      &lt;td&gt;Shared room&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1393&lt;/th&gt;
      &lt;td&gt;Savena&lt;/td&gt;
      &lt;td&gt;Shared room&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1416&lt;/th&gt;
      &lt;td&gt;San Donato - San Vitale&lt;/td&gt;
      &lt;td&gt;Shared room&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1572&lt;/th&gt;
      &lt;td&gt;San Donato - San Vitale&lt;/td&gt;
      &lt;td&gt;Hotel room&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1637&lt;/th&gt;
      &lt;td&gt;Santo Stefano&lt;/td&gt;
      &lt;td&gt;Shared room&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1751&lt;/th&gt;
      &lt;td&gt;Navile&lt;/td&gt;
      &lt;td&gt;Hotel room&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;h2 id=&#34;aggregation-and-pivot-tables&#34;&gt;Aggregation and Pivot Tables&lt;/h2&gt;
&lt;p&gt;We can compute statistics by group using &lt;code&gt;.groupby()&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_listings.groupby(&#39;neighbourhood&#39;)[[&#39;price&#39;, &#39;reviews_per_month&#39;]].mean()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;price&lt;/th&gt;
      &lt;th&gt;reviews_per_month&lt;/th&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;neighbourhood&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;Borgo Panigale - Reno&lt;/th&gt;
      &lt;td&gt;83.020548&lt;/td&gt;
      &lt;td&gt;0.983488&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;Navile&lt;/th&gt;
      &lt;td&gt;142.200993&lt;/td&gt;
      &lt;td&gt;1.156745&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;Porto - Saragozza&lt;/th&gt;
      &lt;td&gt;129.908312&lt;/td&gt;
      &lt;td&gt;1.340325&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;San Donato - San Vitale&lt;/th&gt;
      &lt;td&gt;91.618138&lt;/td&gt;
      &lt;td&gt;0.933011&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;Santo Stefano&lt;/th&gt;
      &lt;td&gt;119.441841&lt;/td&gt;
      &lt;td&gt;1.344810&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;Savena&lt;/th&gt;
      &lt;td&gt;69.626016&lt;/td&gt;
      &lt;td&gt;0.805888&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;If you want to perform more than one function, maybe on different columns, you can use &lt;code&gt;.aggregate()&lt;/code&gt; which can be shortened to &lt;code&gt;.agg()&lt;/code&gt;. It takes as argument a dictionary with variables as keys and lists of functions as values.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_listings.groupby(&#39;neighbourhood&#39;).agg({&amp;quot;reviews_per_month&amp;quot;: [&amp;quot;mean&amp;quot;],
                                          &amp;quot;price&amp;quot;: [&amp;quot;min&amp;quot;, np.max]}).reset_index()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead tr th {
    text-align: left;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;neighbourhood&lt;/th&gt;
      &lt;th&gt;reviews_per_month&lt;/th&gt;
      &lt;th colspan=&#34;2&#34; halign=&#34;left&#34;&gt;price&lt;/th&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;mean&lt;/th&gt;
      &lt;th&gt;min&lt;/th&gt;
      &lt;th&gt;amax&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;Borgo Panigale - Reno&lt;/td&gt;
      &lt;td&gt;0.983488&lt;/td&gt;
      &lt;td&gt;9&lt;/td&gt;
      &lt;td&gt;1429&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;Navile&lt;/td&gt;
      &lt;td&gt;1.156745&lt;/td&gt;
      &lt;td&gt;14&lt;/td&gt;
      &lt;td&gt;5000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;Porto - Saragozza&lt;/td&gt;
      &lt;td&gt;1.340325&lt;/td&gt;
      &lt;td&gt;7&lt;/td&gt;
      &lt;td&gt;9999&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;San Donato - San Vitale&lt;/td&gt;
      &lt;td&gt;0.933011&lt;/td&gt;
      &lt;td&gt;10&lt;/td&gt;
      &lt;td&gt;1600&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;Santo Stefano&lt;/td&gt;
      &lt;td&gt;1.344810&lt;/td&gt;
      &lt;td&gt;11&lt;/td&gt;
      &lt;td&gt;9999&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;5&lt;/th&gt;
      &lt;td&gt;Savena&lt;/td&gt;
      &lt;td&gt;0.805888&lt;/td&gt;
      &lt;td&gt;9&lt;/td&gt;
      &lt;td&gt;680&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;The problem with this syntax is that it generates a hierarchical structure for variable names, which might not be so easy to work with. In the example above, to access the mean price, you have to use &lt;code&gt;df.price[&amp;quot;min&amp;quot;]&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;To perform variable naming and aggregation and the same time, you can ise the following syntax: &lt;code&gt;agg(output_var = (&amp;quot;input_var&amp;quot;, function))&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_listings.groupby(&#39;neighbourhood&#39;).agg(mean_reviews=(&amp;quot;reviews_per_month&amp;quot;, &amp;quot;mean&amp;quot;),
                                         min_price=(&amp;quot;price&amp;quot;, &amp;quot;min&amp;quot;),
                                         max_price=(&amp;quot;price&amp;quot;, np.max)).reset_index()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;neighbourhood&lt;/th&gt;
      &lt;th&gt;mean_reviews&lt;/th&gt;
      &lt;th&gt;min_price&lt;/th&gt;
      &lt;th&gt;max_price&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;Borgo Panigale - Reno&lt;/td&gt;
      &lt;td&gt;0.983488&lt;/td&gt;
      &lt;td&gt;9&lt;/td&gt;
      &lt;td&gt;1429&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;Navile&lt;/td&gt;
      &lt;td&gt;1.156745&lt;/td&gt;
      &lt;td&gt;14&lt;/td&gt;
      &lt;td&gt;5000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;Porto - Saragozza&lt;/td&gt;
      &lt;td&gt;1.340325&lt;/td&gt;
      &lt;td&gt;7&lt;/td&gt;
      &lt;td&gt;9999&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;San Donato - San Vitale&lt;/td&gt;
      &lt;td&gt;0.933011&lt;/td&gt;
      &lt;td&gt;10&lt;/td&gt;
      &lt;td&gt;1600&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;Santo Stefano&lt;/td&gt;
      &lt;td&gt;1.344810&lt;/td&gt;
      &lt;td&gt;11&lt;/td&gt;
      &lt;td&gt;9999&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;5&lt;/th&gt;
      &lt;td&gt;Savena&lt;/td&gt;
      &lt;td&gt;0.805888&lt;/td&gt;
      &lt;td&gt;9&lt;/td&gt;
      &lt;td&gt;680&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;We can make pivot tables with the &lt;code&gt;.pivot_table()&lt;/code&gt; function. It takes the folling arguments:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;index&lt;/code&gt;: rows&lt;/li&gt;
&lt;li&gt;&lt;code&gt;columns&lt;/code&gt;: columns&lt;/li&gt;
&lt;li&gt;&lt;code&gt;values&lt;/code&gt;: values&lt;/li&gt;
&lt;li&gt;&lt;code&gt;aggfunc&lt;/code&gt;: aggregation function&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_listings.pivot_table(index=&#39;neighbourhood&#39;, columns=&#39;room_type&#39;, values=&#39;price&#39;, aggfunc=&#39;mean&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;room_type&lt;/th&gt;
      &lt;th&gt;Entire home/apt&lt;/th&gt;
      &lt;th&gt;Hotel room&lt;/th&gt;
      &lt;th&gt;Private room&lt;/th&gt;
      &lt;th&gt;Shared room&lt;/th&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;neighbourhood&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;Borgo Panigale - Reno&lt;/th&gt;
      &lt;td&gt;96.700935&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;45.487179&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;Navile&lt;/th&gt;
      &lt;td&gt;172.140000&lt;/td&gt;
      &lt;td&gt;1350.000000&lt;/td&gt;
      &lt;td&gt;68.416107&lt;/td&gt;
      &lt;td&gt;28.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;Porto - Saragozza&lt;/th&gt;
      &lt;td&gt;148.410926&lt;/td&gt;
      &lt;td&gt;102.375000&lt;/td&gt;
      &lt;td&gt;83.070234&lt;/td&gt;
      &lt;td&gt;16.5&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;San Donato - San Vitale&lt;/th&gt;
      &lt;td&gt;106.775000&lt;/td&gt;
      &lt;td&gt;55.000000&lt;/td&gt;
      &lt;td&gt;61.194030&lt;/td&gt;
      &lt;td&gt;59.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;Santo Stefano&lt;/th&gt;
      &lt;td&gt;129.990260&lt;/td&gt;
      &lt;td&gt;103.827586&lt;/td&gt;
      &lt;td&gt;80.734177&lt;/td&gt;
      &lt;td&gt;95.4&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;Savena&lt;/th&gt;
      &lt;td&gt;86.301370&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;46.229167&lt;/td&gt;
      &lt;td&gt;22.5&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Matrix Algebra</title>
      <link>https://matteocourthoud.github.io/course/metrics/01_matrices/</link>
      <pubDate>Fri, 29 Oct 2021 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/course/metrics/01_matrices/</guid>
      <description>&lt;h2 id=&#34;basics&#34;&gt;Basics&lt;/h2&gt;
&lt;h3 id=&#34;matrix-definition&#34;&gt;Matrix Definition&lt;/h3&gt;
&lt;p&gt;A real $n \times m$ matrix $A$ is an array&lt;/p&gt;
&lt;p&gt;$$
A=
\begin{bmatrix}
a_{11} &amp;amp; a_{12} &amp;amp; \dots  &amp;amp; a_{1m} \newline
a_{21} &amp;amp; a_{22} &amp;amp; \dots  &amp;amp; a_{2m} \newline
\vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots \newline
a_{n1} &amp;amp; a_{n2} &amp;amp; \dots  &amp;amp; a_{nm}
\end{bmatrix}
$$&lt;/p&gt;
&lt;p&gt;We write $[A]_ {ij} = a_ {ij}$ to indicate the $(i,j)$-element of $A$.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;We will usually take the convention that a real vector
$x \in \mathbb R^n$ is identified with an $n \times 1$ matrix.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The $n \times n$ &lt;strong&gt;identity matrix&lt;/strong&gt; $I_n$ is given by&lt;br&gt;
$$
[I_n] _ {ij} = \begin{cases} 1 \ \ \ \text{if} \ i=j \newline
0 \ \ \ \text{if} \ i \neq j \end{cases}
$$&lt;/p&gt;
&lt;h3 id=&#34;fundamental-operations&#34;&gt;Fundamental Operations&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;Two $n \times m$ matrices, $A,B$, are added element-wise so that
$[A+B]_{ij} = [A] _{ij} + [B] _{ij}$.&lt;/li&gt;
&lt;li&gt;A matrix $A$ can be multiplied by a scalar $c\in \mathbb{R}$ in
which case we set $[cA]_{ij} = c[A] _{ij}$.&lt;/li&gt;
&lt;li&gt;An $n \times m$ matrix $A$ can be multiplied with an $m \times p$
matrix $B$.&lt;/li&gt;
&lt;li&gt;The product $AB$ is defined according to the rule
$[AB] _ {ij} = \sum_{k=1}^m [A] _{ik} [B] _{kj}$.&lt;/li&gt;
&lt;li&gt;An $n \times n$ matrix is invertible if there exists a matrix $B$
such that $AB=I$. In this case, we use the notational convention of
writing $B = A^{-1}$.&lt;/li&gt;
&lt;li&gt;Matrix transposition is defined by $[A&#39;] _{ij} = [A] _{ji}$.&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;trace-and-determinant&#34;&gt;Trace and Determinant&lt;/h3&gt;
&lt;p&gt;The &lt;strong&gt;trace&lt;/strong&gt; of a square matrix $A$ with dimension $n \times n$ is
$\text{tr}(A) = \sum_{i=1}^n a_{ii}$.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;determinant&lt;/strong&gt; of a square $n \times n$ matrix A is defined
according to one of the following three (equivalent) definitions.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Recursively as
$det(A) = \sum_{i=1}^n a_{ij} (-1)^{i+j} det([A]&lt;em&gt;{-i,-j})$ where
$[A]&lt;/em&gt;{-i,-j}$ is the matrix obtained by deleting the $i$th row and
the $j$th column.&lt;/li&gt;
&lt;li&gt;$A \mapsto det(A)$ under the unique alternating multilinear map on
$n \times n$ matrices such that $I \mapsto 1$.&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;linear-independence&#34;&gt;Linear Independence&lt;/h3&gt;
&lt;p&gt;Vectors $x_1,&amp;hellip;,x_k$ are &lt;strong&gt;linearly independent&lt;/strong&gt; if the only solution
to the equation $b_1x_1 + &amp;hellip; + b_k x_k=0, \ b_j \in \mathbb R$, is
$b_1=b_2=&amp;hellip;=b_k=0$.&lt;/p&gt;
&lt;h3 id=&#34;useful-identities&#34;&gt;Useful Identities&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;$(A+B)&#39; =A&#39;+B&#39;$&lt;/li&gt;
&lt;li&gt;$(AB)C = A(BC)$&lt;/li&gt;
&lt;li&gt;$A(B+C) = AB+AC$&lt;/li&gt;
&lt;li&gt;$(AB&#39;) = B&amp;rsquo;A&#39;$&lt;/li&gt;
&lt;li&gt;$(A^{-1})&#39; = (A&#39;)^{-1}$&lt;/li&gt;
&lt;li&gt;$(AB)^{-1} = B^{-1}A^{-1}$&lt;/li&gt;
&lt;li&gt;$\text{tr}(cA) = c\text{tr}(A)$&lt;/li&gt;
&lt;li&gt;$\text{tr}(A+B) = \text{tr}(A) + \text{tr}(B)$&lt;/li&gt;
&lt;li&gt;$\text{tr}(AB) =\text{tr}(BA)$&lt;/li&gt;
&lt;li&gt;$det(I)=1$&lt;/li&gt;
&lt;li&gt;$det(cA) = c^ndet(A)$ if $A$ is $n \times n$ and $c \in \mathbb R$&lt;/li&gt;
&lt;li&gt;$det(A) = det(A&#39;)$&lt;/li&gt;
&lt;li&gt;$det(AB) = det(A)det(B)$&lt;/li&gt;
&lt;li&gt;$det(A^{-1}) = (det(A))^{-1}$&lt;/li&gt;
&lt;li&gt;$A^{-1}$ exists iff $det(A) \neq 0$&lt;/li&gt;
&lt;li&gt;$rank(A) = rank(A&#39;) = rank(A&amp;rsquo;A) = rank(AA&#39;)$&lt;/li&gt;
&lt;li&gt;$A^{-1}$ exists iff $rank(A)=n$ for $A$ $n \times n$&lt;/li&gt;
&lt;li&gt;$rank(AB) \leq \min \lbrace rank(A), rank(B) \rbrace$&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;matrix-rank&#34;&gt;Matrix Rank&lt;/h3&gt;
&lt;p&gt;The &lt;strong&gt;rank&lt;/strong&gt; of a matrix, $rank(A)$ is equal to the maximal number of
linearly independent rows for $A$.&lt;/p&gt;
&lt;p&gt;Let $A$ be an $n \times n$ matrix. The $n \times 1$ vector $x \neq 0$ is
an &lt;strong&gt;eigenvector&lt;/strong&gt; of $A$ with corresponding &lt;strong&gt;eigenvalue&lt;/strong&gt; $\lambda$ is
$Ax = \lambda x$.&lt;/p&gt;
&lt;h3 id=&#34;definitions&#34;&gt;Definitions&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;A matrix $A$ is diagonal if $[A]_ {ij} \neq 0$ only if $i=j$.&lt;/li&gt;
&lt;li&gt;An $n \times n$ matrix $A$ is orthogonal if $A&amp;rsquo;A = I$&lt;/li&gt;
&lt;li&gt;A matrix $A$ is symmetric if $[A]_ {ij} = [A]_ {ji}$.&lt;/li&gt;
&lt;li&gt;An $n \times n$ matrix $A$ is idempotent if $A^2=A$.&lt;/li&gt;
&lt;li&gt;The matrix of zeros ($[A]_ {ij} =0$ for each $i,j$) is simply
denoted 0.&lt;/li&gt;
&lt;li&gt;An $n \times n$ matrix $A$ is nilpotent if $A^k=0$ for some integer
$k&amp;gt;0$.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;spectral-decomposition&#34;&gt;Spectral Decomposition&lt;/h2&gt;
&lt;h3 id=&#34;spectral-theorem&#34;&gt;Spectral Theorem&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt;: Let $A$ be an $n \times n$ symmetric matrix. Then $A$ can
be factored as $A = C \Lambda C&#39;$ where $C$ is orthogonal and $\Lambda$
is diagonal.&lt;/p&gt;
&lt;p&gt;If we postmultiply $A$ by $C$, we get&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$AC = C \Lambda C&amp;rsquo;C$ and&lt;/li&gt;
&lt;li&gt;$AC = C \Lambda$.&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;This is a matrix equation which can be split into columns. The $i$th
column of the equation reads $A c_i = \lambda_i c_i$ which corresponds
to the definition of eigenvalues and eigenvectors. So if the
decomposition exists, then $C$ is the eigenvector matrix and $\Lambda$
contains the eigenvalues.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;rank-and-trace&#34;&gt;Rank and Trace&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt;: The rank of a symmetric matrix equals the number of non
zero eigenvalues.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;:
$rank(A) = rank(C\Lambda C&#39;) = rank(\Lambda) = | \lbrace i: \lambda_i \neq 0 \rbrace |$.
$$\tag*{$\blacksquare$}$$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt;: The nonzero eigenvalues of $AA&#39;$ and $A&amp;rsquo;A$ are identical.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt;: The trace of a symmetric matrix equals the sum of its
eignevalues.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;:
$tr(A) = tr(C \Lambda C&#39;) = tr((C \Lambda)C&#39;) = tr(C&amp;rsquo;C \Lambda) = tr(\Lambda) = \sum_ {i=1}^n \lambda_i.$
$$\tag*{$\blacksquare$}$$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt;: The determinant of a symmetric matrix equals the product of
its eignevalues.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;:
$det(A) = det(C \Lambda C&#39;) = det(C)det(\Lambda)det(C&#39;) = det(C)det(C&#39;)det(\Lambda) = det(CC&#39;) det(\Lambda) = det(I)det(\Lambda) = det(\Lambda) = \prod_ {i=1}^n \lambda_i.$
$$\tag*{$\blacksquare$}$$&lt;/p&gt;
&lt;h3 id=&#34;eigenvalues&#34;&gt;Eigenvalues&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt;: For any symmetric matrix $A$, the eigenvalues of $A^2$ are
the square of the eignevalues of $A$, and the eigenvectors are the same.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;:
$A = C \Lambda C&#39; \implies A^2 = C \Lambda C&#39; C \Lambda C&#39; = C \Lambda I \Lambda C&#39; = C \Lambda^2 C&#39;$
$$\tag*{$\blacksquare$}$$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt;: For any symmetric matrix $A$, and any integer $k&amp;gt;0$, the
eigenvalues of $A^k$ are the $k$th power of the eigenvalues of $A$, and
the eigenvectors are the same.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt;: Any square symmetric matrix $A$ with positive eigenvalues
can be written as the product of a lower triangular matrix $L$ and its
(upper triangular) transpose $L&#39; = U$. That is $A = LU = LL&#39;$&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Note that $$
A = LL&#39; = LU = U&amp;rsquo;U  = (L&#39;)^{-1}L^{-1} = U^{-1}(U&#39;)^{-1}
$$ where $L^{-1}$ is lower triangular and $U^{ -1}$ is upper
trianguar. You can check this for the $2 \times 2$ case. Also note
that the validity of the theorem can be extended to symmetric matrices
with non- negative eigenvalues by a limiting argument. However, then
the proof is not constructive anymore.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;quadratic-forms-and-definite-matrices&#34;&gt;Quadratic Forms and Definite Matrices&lt;/h2&gt;
&lt;h3 id=&#34;definition&#34;&gt;Definition&lt;/h3&gt;
&lt;p&gt;A &lt;strong&gt;quadratic form&lt;/strong&gt; in the $n \times n$ matrix $A$ and $n \times 1$
vector $x$ is defined by the scalar $x&amp;rsquo;Ax$.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;$A$ is negative definite (ND) if for each $x \neq 0$, $x&amp;rsquo;Ax &amp;lt; 0$&lt;/li&gt;
&lt;li&gt;$A$ is negative semidefinite (NSD) if for each $x \neq 0$,
$x&amp;rsquo;Ax \leq 0$&lt;/li&gt;
&lt;li&gt;$A$ is positive definite (PD) if for each $x \neq 0$, $x&amp;rsquo;Ax &amp;gt; 0$&lt;/li&gt;
&lt;li&gt;$A$ is positive semidefinite (PSD) if for each $x \neq 0$,
$x&amp;rsquo;Ax \geq 0$&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;equivalence&#34;&gt;Equivalence&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt;: Let $A$ be a symmetric matrix. Then $A$ is PD(ND) $\iff$
all of its eigenvalues are positive (negative).&lt;/p&gt;
&lt;p&gt;Some more results:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;If a symmetric matrix $A$ is PD (PSD, ND, NSD), then
$\text{det}(A) &amp;gt;(\geq,&amp;lt;,\leq) 0$.&lt;/li&gt;
&lt;li&gt;If symmetric matrix $A$ is PD (ND) then $A^{-1}$ is symmetric PD
(ND).&lt;/li&gt;
&lt;li&gt;The identity matrix is PD (since all eigenvalues are equal to 1).&lt;/li&gt;
&lt;li&gt;Every symmetric idempotent matrix is PSD (since the eigenvalues are
only 0 or 1).&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt;: If $A$ is $n\times k$ with $n&amp;gt;k$ and $rank(A)=k$, then
$A&amp;rsquo;A$ is PD and $AA&#39;$ is PSD.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;semidefinite partial order&lt;/strong&gt; is defined by $A \geq B$ iff $A-B$ is
PSD.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt;: Let $A$, $B$ be symmetric,square , PD, conformable. Then
$A-B$ is PD iff $A^{-1}-B^{-1}$ is PD.&lt;/p&gt;
&lt;h2 id=&#34;matrix-calculus&#34;&gt;Matrix Calculus&lt;/h2&gt;
&lt;h3 id=&#34;comformable-matrices&#34;&gt;Comformable Matrices&lt;/h3&gt;
&lt;p&gt;We first define matrices blockwise when they are conformable. In
particular, we assume that if $A_1, A_2, A_3, A_4$ are matrices with
appropriate dimensions then the matrix $$
A = \begin{bmatrix} A_1 &amp;amp; A_1 \newline
A_3 &amp;amp; A_4 \end{bmatrix}
$$ is defined in the obvious way.&lt;/p&gt;
&lt;h3 id=&#34;matrix-functions&#34;&gt;Matrix Functions&lt;/h3&gt;
&lt;p&gt;Let
$F: \mathbb R^m \times \mathbb R^n \rightarrow \mathbb R^p \times \mathbb R^q$
be a matrix valued function. More precisely, given a real $m \times n$
matrix $X$, $F(X)$ returns the $p \times q$ matrix&lt;br&gt;
$$
\begin{bmatrix}
f_ {11}(X) &amp;amp; &amp;hellip; &amp;amp; f_ {1q}(X) \newline \vdots &amp;amp; \ddots &amp;amp; \vdots \newline
f_ {p1}(X)&amp;amp; &amp;hellip; &amp;amp; f_ {pq}(X)
\end{bmatrix}
$$&lt;/p&gt;
&lt;h3 id=&#34;matrix-derivatives&#34;&gt;Matrix Derivatives&lt;/h3&gt;
&lt;p&gt;The derivative of $F$ with respect to the matrix $X$ is the
$mp \times nq$ matrix $$
\frac{\partial F(X)}{\partial X} = \begin{bmatrix}
\frac{\partial F(X)}{\partial x_ {11}} &amp;amp; &amp;hellip; &amp;amp; \frac{\partial F(X)}{\partial x_ {1n}} \newline \vdots &amp;amp; \ddots &amp;amp; \vdots \newline
\frac{\partial F(X)}{\partial x_ {m1}} &amp;amp; &amp;hellip; &amp;amp; \frac{\partial F(X)}{\partial x_ {mn}}
\end{bmatrix}
$$ where each $\frac{\partial F(X)}{\partial x_ {ij}}$ is a $p\times q$
matrix given by&lt;br&gt;
$$
\frac{\partial F(X)}{\partial x_ {ij}} = \begin{bmatrix}
\frac{\partial f_ {11}(X)}{\partial x_ {ij}} &amp;amp; &amp;hellip; &amp;amp; \frac{\partial f_ {1q}(X)}{\partial x_ {ij}} \newline
\vdots &amp;amp; \ddots &amp;amp; \vdots \newline
\frac{\partial f_ {p1}(X)}{\partial x_ {ij}} &amp;amp; &amp;hellip; &amp;amp; \frac{\partial f_ {pq}(X)}{\partial x_ {ij}}
\end{bmatrix}
$$ The most important case is when
$F: \mathbb R^n \rightarrow \mathbb R$ since this simplifies the
derivation of the least squares estimator. Also, the trickiest thing is
to make sure that dimensions are correct.&lt;/p&gt;
&lt;h3 id=&#34;useful-results-in-matrix-calculus&#34;&gt;Useful Results in Matrix Calculus&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;$\frac{\partial b&amp;rsquo;x}{\partial x}= b$ for $dim(b) = dim(x)$&lt;/li&gt;
&lt;li&gt;$\frac{\partial B&amp;rsquo;x}{\partial x}= B$ for arbitrary, conformable $B$&lt;/li&gt;
&lt;li&gt;$\frac{\partial B&amp;rsquo;x}{\partial x&#39;}= B&#39;$ for arbitrary, conformable
$B$&lt;/li&gt;
&lt;li&gt;$\frac{\partial x&amp;rsquo;Ax}{\partial x} = (A + A&#39;)x$&lt;/li&gt;
&lt;li&gt;$\frac{\partial x&amp;rsquo;Ax}{\partial A} = xx&#39;$&lt;/li&gt;
&lt;li&gt;$\frac{\partial x&amp;rsquo;Ax}{\partial x} = det(A) (A^{-1})&#39;$&lt;/li&gt;
&lt;li&gt;$\frac{\partial \ln det(A)}{\partial A} = (A^{-1})&#39;$&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>Instrumental Variables</title>
      <link>https://matteocourthoud.github.io/course/ml-econ/02_iv/</link>
      <pubDate>Mon, 28 Feb 2022 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/course/ml-econ/02_iv/</guid>
      <description>&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Remove warnings
import warnings
warnings.filterwarnings(&#39;ignore&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Import everything
import pandas as pd
import numpy as np
import seaborn as sns
import statsmodels.api as sm

from numpy.linalg import inv
from statsmodels.iolib.summary2 import summary_col
from linearmodels.iv import IV2SLS
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Import matplotlib for graphs
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import axes3d

# Set global parameters
%matplotlib inline
plt.style.use(&#39;seaborn-white&#39;)
plt.rcParams[&#39;lines.linewidth&#39;] = 3
plt.rcParams[&#39;figure.figsize&#39;] = (10,6)
plt.rcParams[&#39;figure.titlesize&#39;] = 20
plt.rcParams[&#39;axes.titlesize&#39;] = 18
plt.rcParams[&#39;axes.labelsize&#39;] = 14
plt.rcParams[&#39;legend.fontsize&#39;] = 14
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;21-simple-linear-regression&#34;&gt;2.1 Simple Linear Regression&lt;/h2&gt;
&lt;p&gt;In &lt;a href=&#34;https://economics.mit.edu/files/4123&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Acemoglu, Johnson, Robinson (2002), &amp;ldquo;&lt;em&gt;The Colonial Origins of Comparative Development&lt;/em&gt;&amp;quot;&lt;/a&gt; the authors wish to determine whether or not differences in institutions can help to explain observed economic outcomes.&lt;/p&gt;
&lt;p&gt;How do we measure &lt;em&gt;institutional differences&lt;/em&gt; and &lt;em&gt;economic outcomes&lt;/em&gt;?&lt;/p&gt;
&lt;p&gt;In this paper,&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;economic outcomes are proxied by log GDP per capita in 1995, adjusted for exchange rates.&lt;/li&gt;
&lt;li&gt;institutional differences are proxied by an index of protection against expropriation on average over 1985-95, constructed by the &lt;a href=&#34;https://www.prsgroup.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Political Risk Services Group&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;These variables and other data used in the paper are available for download on Daron Acemoglu’s &lt;a href=&#34;https://economics.mit.edu/faculty/acemoglu/data/ajr2001&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;webpage&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The original dataset in in Stata &lt;code&gt;.dta&lt;/code&gt; format but has been converted to &lt;code&gt;.csv&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;First, let&amp;rsquo;s load the data and have a look at it.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Load Acemoglu Johnson Robinson Dataset
df = pd.read_csv(&#39;data/AJR02.csv&#39;,index_col=0)
df.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;GDP&lt;/th&gt;
      &lt;th&gt;Exprop&lt;/th&gt;
      &lt;th&gt;Mort&lt;/th&gt;
      &lt;th&gt;Latitude&lt;/th&gt;
      &lt;th&gt;Neo&lt;/th&gt;
      &lt;th&gt;Africa&lt;/th&gt;
      &lt;th&gt;Asia&lt;/th&gt;
      &lt;th&gt;Namer&lt;/th&gt;
      &lt;th&gt;Samer&lt;/th&gt;
      &lt;th&gt;logMort&lt;/th&gt;
      &lt;th&gt;Latitude2&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;8.39&lt;/td&gt;
      &lt;td&gt;6.50&lt;/td&gt;
      &lt;td&gt;78.20&lt;/td&gt;
      &lt;td&gt;0.3111&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;4.359270&lt;/td&gt;
      &lt;td&gt;0.096783&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;7.77&lt;/td&gt;
      &lt;td&gt;5.36&lt;/td&gt;
      &lt;td&gt;280.00&lt;/td&gt;
      &lt;td&gt;0.1367&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;5.634790&lt;/td&gt;
      &lt;td&gt;0.018687&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;9.13&lt;/td&gt;
      &lt;td&gt;6.39&lt;/td&gt;
      &lt;td&gt;68.90&lt;/td&gt;
      &lt;td&gt;0.3778&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;4.232656&lt;/td&gt;
      &lt;td&gt;0.142733&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;9.90&lt;/td&gt;
      &lt;td&gt;9.32&lt;/td&gt;
      &lt;td&gt;8.55&lt;/td&gt;
      &lt;td&gt;0.3000&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;2.145931&lt;/td&gt;
      &lt;td&gt;0.090000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;5&lt;/th&gt;
      &lt;td&gt;9.29&lt;/td&gt;
      &lt;td&gt;7.50&lt;/td&gt;
      &lt;td&gt;85.00&lt;/td&gt;
      &lt;td&gt;0.2683&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;4.442651&lt;/td&gt;
      &lt;td&gt;0.071985&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;Let’s use a scatterplot to see whether any obvious relationship exists between GDP per capita and the protection against expropriation.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Plot relationship between GDP and expropriation rate
fig, ax = plt.subplots(1,1)
ax.set_title(&#39;Figure 1: joint distribution of GDP and expropriation&#39;)
df.plot(x=&#39;Exprop&#39;, y=&#39;GDP&#39;, kind=&#39;scatter&#39;, s=50, ax=ax);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/02_iv_9_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;The plot shows a fairly strong positive relationship between
protection against expropriation and log GDP per capita.&lt;/p&gt;
&lt;p&gt;Specifically, if higher protection against expropriation is a measure of
institutional quality, then better institutions appear to be positively
correlated with better economic outcomes (higher GDP per capita).&lt;/p&gt;
&lt;p&gt;Given the plot, choosing a linear model to describe this relationship
seems like a reasonable assumption.&lt;/p&gt;
&lt;p&gt;We can write our model as&lt;/p&gt;
&lt;p&gt;$$
{GDP}_i = \beta_0 + \beta_1 {Exprop}_i + \varepsilon_i
$$&lt;/p&gt;
&lt;p&gt;where:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$ \beta_0 $ is the intercept of the linear trend line on the
y-axis&lt;/li&gt;
&lt;li&gt;$ \beta_1 $ is the slope of the linear trend line, representing
the &lt;em&gt;marginal effect&lt;/em&gt; of protection against risk on log GDP per
capita&lt;/li&gt;
&lt;li&gt;$ \varepsilon_i $ is a random error term (deviations of observations from
the linear trend due to factors not included in the model)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The most common technique to estimate the parameters ($ \beta $’s)
of the linear model is Ordinary Least Squares (OLS).&lt;/p&gt;
&lt;p&gt;As the name implies, an OLS model is solved by finding the parameters
that minimize &lt;em&gt;the sum of squared residuals&lt;/em&gt;, i.e.&lt;/p&gt;
&lt;p&gt;$$
\underset{\hat{\beta}}{\min} \sum^N_{i=1}{\hat{u}^2_i}
$$&lt;/p&gt;
&lt;p&gt;where $ \hat{u}_i $ is the difference between the observation and
the predicted value of the dependent variable.&lt;/p&gt;
&lt;p&gt;To estimate the constant term $ \beta_0 $, we need to add a column
of 1’s to our dataset (consider the equation if $ \beta_0 $ was
replaced with $ \beta_0 x_i $ and $ x_i = 1 $)&lt;/p&gt;
&lt;p&gt;Now we can construct our model in &lt;code&gt;statsmodels&lt;/code&gt; using the OLS function.&lt;/p&gt;
&lt;p&gt;We will use &lt;code&gt;pandas&lt;/code&gt; dataframes with &lt;code&gt;statsmodels&lt;/code&gt;, however standard arrays can also be used as arguments&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Regress GDP on Expropriation Rate
reg1 = sm.OLS.from_formula(&#39;GDP ~ Exprop&#39;, df)
type(reg1)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;statsmodels.regression.linear_model.OLS
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So far we have simply constructed our model.&lt;/p&gt;
&lt;p&gt;We need to use &lt;code&gt;.fit()&lt;/code&gt; to obtain parameter estimates
$ \hat{\beta}_0 $ and $ \hat{\beta}_1 $&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Fit regression
results = reg1.fit()
type(results)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;statsmodels.regression.linear_model.RegressionResultsWrapper
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We now have the fitted regression model stored in &lt;code&gt;results&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;To view the OLS regression results, we can call the &lt;code&gt;.summary()&lt;/code&gt;
method.&lt;/p&gt;
&lt;p&gt;Note that an observation was mistakenly dropped from the results in the
original paper (see the note located in maketable2.do from Acemoglu’s webpage), and thus the
coefficients differ slightly.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;results.summary()
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;caption&gt;OLS Regression Results&lt;/caption&gt;
&lt;tr&gt;
  &lt;th&gt;Dep. Variable:&lt;/th&gt;           &lt;td&gt;GDP&lt;/td&gt;       &lt;th&gt;  R-squared:         &lt;/th&gt; &lt;td&gt;   0.540&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Model:&lt;/th&gt;                   &lt;td&gt;OLS&lt;/td&gt;       &lt;th&gt;  Adj. R-squared:    &lt;/th&gt; &lt;td&gt;   0.532&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Method:&lt;/th&gt;             &lt;td&gt;Least Squares&lt;/td&gt;  &lt;th&gt;  F-statistic:       &lt;/th&gt; &lt;td&gt;   72.71&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Date:&lt;/th&gt;             &lt;td&gt;Mon, 03 Jan 2022&lt;/td&gt; &lt;th&gt;  Prob (F-statistic):&lt;/th&gt; &lt;td&gt;4.84e-12&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Time:&lt;/th&gt;                 &lt;td&gt;18:31:09&lt;/td&gt;     &lt;th&gt;  Log-Likelihood:    &lt;/th&gt; &lt;td&gt; -68.214&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;No. Observations:&lt;/th&gt;      &lt;td&gt;    64&lt;/td&gt;      &lt;th&gt;  AIC:               &lt;/th&gt; &lt;td&gt;   140.4&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Df Residuals:&lt;/th&gt;          &lt;td&gt;    62&lt;/td&gt;      &lt;th&gt;  BIC:               &lt;/th&gt; &lt;td&gt;   144.7&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Df Model:&lt;/th&gt;              &lt;td&gt;     1&lt;/td&gt;      &lt;th&gt;                     &lt;/th&gt;     &lt;td&gt; &lt;/td&gt;   
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Covariance Type:&lt;/th&gt;      &lt;td&gt;nonrobust&lt;/td&gt;    &lt;th&gt;                     &lt;/th&gt;     &lt;td&gt; &lt;/td&gt;   
&lt;/tr&gt;
&lt;/table&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
      &lt;td&gt;&lt;/td&gt;         &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt; &lt;td&gt;    4.6609&lt;/td&gt; &lt;td&gt;    0.409&lt;/td&gt; &lt;td&gt;   11.402&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    3.844&lt;/td&gt; &lt;td&gt;    5.478&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Exprop&lt;/th&gt;    &lt;td&gt;    0.5220&lt;/td&gt; &lt;td&gt;    0.061&lt;/td&gt; &lt;td&gt;    8.527&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    0.400&lt;/td&gt; &lt;td&gt;    0.644&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
  &lt;th&gt;Omnibus:&lt;/th&gt;       &lt;td&gt; 7.134&lt;/td&gt; &lt;th&gt;  Durbin-Watson:     &lt;/th&gt; &lt;td&gt;   2.081&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Prob(Omnibus):&lt;/th&gt; &lt;td&gt; 0.028&lt;/td&gt; &lt;th&gt;  Jarque-Bera (JB):  &lt;/th&gt; &lt;td&gt;   6.698&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Skew:&lt;/th&gt;          &lt;td&gt;-0.784&lt;/td&gt; &lt;th&gt;  Prob(JB):          &lt;/th&gt; &lt;td&gt;  0.0351&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Kurtosis:&lt;/th&gt;      &lt;td&gt; 3.234&lt;/td&gt; &lt;th&gt;  Cond. No.          &lt;/th&gt; &lt;td&gt;    31.2&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;&lt;br/&gt;&lt;br/&gt;Warnings:&lt;br/&gt;[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
&lt;p&gt;From our results, we see that&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The intercept $ \hat{\beta}_0 = 4.63 $.&lt;/li&gt;
&lt;li&gt;The slope $ \hat{\beta}_1 = 0.53 $.&lt;/li&gt;
&lt;li&gt;The positive $ \hat{\beta}_1 $ parameter estimate implies that.
institutional quality has a positive effect on economic outcomes, as
we saw in the figure.&lt;/li&gt;
&lt;li&gt;The p-value of 0.000 for $ \hat{\beta}_1 $ implies that the
effect of institutions on GDP is statistically significant (using p &amp;lt;
0.05 as a rejection rule).&lt;/li&gt;
&lt;li&gt;The R-squared value of 0.611 indicates that around 61% of variation
in log GDP per capita is explained by protection against
expropriation.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Using our parameter estimates, we can now write our estimated
relationship as&lt;/p&gt;
&lt;p&gt;$$
\widehat{GDP}_i = 4.63 + 0.53 \ {Exprop}_i
$$&lt;/p&gt;
&lt;p&gt;This equation describes the line that best fits our data, as shown in
Figure 2.&lt;/p&gt;
&lt;p&gt;We can use this equation to predict the level of log GDP per capita for
a value of the index of expropriation protection.&lt;/p&gt;
&lt;p&gt;For example, for a country with an index value of 6.51 (the average for
the dataset), we find that their predicted level of log GDP per capita
in 1995 is 8.09.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;mean_expr = np.mean(df[&#39;Exprop&#39;])
mean_expr
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;6.5160937500000005
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;predicted_logpdp95 = results.params[0] + results.params[1] * mean_expr
predicted_logpdp95
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;8.062499999999995
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;An easier (and more accurate) way to obtain this result is to use
&lt;code&gt;.predict()&lt;/code&gt; and set $ constant = 1 $ and
$ {Exprop}_i = mean_expr $&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;results.predict(exog=[1, mean_expr])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can obtain an array of predicted $ {GDP}_i $ for every value
of $ {Exprop}_i $ in our dataset by calling &lt;code&gt;.predict()&lt;/code&gt; on our
results.&lt;/p&gt;
&lt;p&gt;Plotting the predicted values against $ {Exprop}_i $ shows that the
predicted values lie along the linear line that we fitted above.&lt;/p&gt;
&lt;p&gt;The observed values of $ {GDP}_i $ are also plotted for
comparison purposes&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Make first new figure
def make_new_fig_2():

    # Init figure
    fig, ax = plt.subplots(1,1)
    ax.set_title(&#39;Figure 2: OLS predicted values&#39;)

    # Drop missing observations from whole sample
    df_plot = df.dropna(subset=[&#39;GDP&#39;, &#39;Exprop&#39;])
    sns.regplot(x=df_plot[&#39;Exprop&#39;], y=df_plot[&#39;GDP&#39;], ax=ax, order=1, ci=None, line_kws={&#39;color&#39;:&#39;r&#39;})

    ax.legend([&#39;predicted&#39;, &#39;observed&#39;])
    ax.set_xlabel(&#39;Exprop&#39;)
    ax.set_ylabel(&#39;GDP&#39;)
    plt.show()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;make_new_fig_2()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;ERROR! Session/line number was not unique in database. History logging moved to new session 305
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/02_iv_28_1.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;22-extending-the-linear-regression-model&#34;&gt;2.2 Extending the Linear Regression Model&lt;/h2&gt;
&lt;p&gt;So far we have only accounted for institutions affecting economic performance - almost certainly there are numerous other factors affecting GDP that are not included in our model.&lt;/p&gt;
&lt;p&gt;Leaving out variables that affect $ GDP_i $ will result in &lt;strong&gt;omitted variable bias&lt;/strong&gt;, yielding biased and inconsistent parameter estimates.&lt;/p&gt;
&lt;p&gt;We can extend our bivariate regression model to a &lt;strong&gt;multivariate regression model&lt;/strong&gt; by adding in other factors that may affect $ GDP_i $.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://python-programming.quantecon.org/zreferences.html#acemoglu2001&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[AJR01]&lt;/a&gt; consider other factors such as:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the effect of climate on economic outcomes; latitude is used to proxy
this&lt;/li&gt;
&lt;li&gt;differences that affect both economic performance and institutions,
eg. cultural, historical, etc.; controlled for with the use of
continent dummies&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Let’s estimate some of the extended models considered in the paper
(Table 2) using data from &lt;code&gt;maketable2.dta&lt;/code&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Add constant term to dataset
df[&#39;const&#39;] = 1

# Create lists of variables to be used in each regression
X1 = df[[&#39;const&#39;, &#39;Exprop&#39;]]
X2 = df[[&#39;const&#39;, &#39;Exprop&#39;, &#39;Latitude&#39;, &#39;Latitude2&#39;]]
X3 = df[[&#39;const&#39;, &#39;Exprop&#39;, &#39;Latitude&#39;, &#39;Latitude2&#39;, &#39;Asia&#39;, &#39;Africa&#39;, &#39;Namer&#39;, &#39;Samer&#39;]]

# Estimate an OLS regression for each set of variables
reg1 = sm.OLS(df[&#39;GDP&#39;], X1, missing=&#39;drop&#39;).fit()
reg2 = sm.OLS(df[&#39;GDP&#39;], X2, missing=&#39;drop&#39;).fit()
reg3 = sm.OLS(df[&#39;GDP&#39;], X3, missing=&#39;drop&#39;).fit()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now that we have fitted our model, we will use &lt;code&gt;summary_col&lt;/code&gt; to
display the results in a single table (model numbers correspond to those
in the paper)&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;info_dict={&#39;No. observations&#39; : lambda x: f&amp;quot;{int(x.nobs):d}&amp;quot;}

results_table = summary_col(results=[reg1,reg2,reg3],
                            float_format=&#39;%0.2f&#39;,
                            stars = True,
                            model_names=[&#39;Model 1&#39;,&#39;Model 2&#39;,&#39;Model 3&#39;],
                            info_dict=info_dict,
                            regressor_order=[&#39;const&#39;,&#39;Exprop&#39;,&#39;Latitude&#39;,&#39;Latitude2&#39;])

results_table
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
          &lt;td&gt;&lt;/td&gt;         &lt;th&gt;Model 1&lt;/th&gt; &lt;th&gt;Model 2&lt;/th&gt; &lt;th&gt;Model 3&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;const&lt;/th&gt;            &lt;td&gt;4.66***&lt;/td&gt; &lt;td&gt;4.55***&lt;/td&gt; &lt;td&gt;5.95***&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;&lt;/th&gt;                 &lt;td&gt;(0.41)&lt;/td&gt;  &lt;td&gt;(0.45)&lt;/td&gt;  &lt;td&gt;(0.68)&lt;/td&gt; 
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Exprop&lt;/th&gt;           &lt;td&gt;0.52***&lt;/td&gt; &lt;td&gt;0.49***&lt;/td&gt; &lt;td&gt;0.40***&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;&lt;/th&gt;                 &lt;td&gt;(0.06)&lt;/td&gt;  &lt;td&gt;(0.07)&lt;/td&gt;  &lt;td&gt;(0.06)&lt;/td&gt; 
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Latitude&lt;/th&gt;            &lt;td&gt;&lt;/td&gt;      &lt;td&gt;2.16&lt;/td&gt;    &lt;td&gt;0.42&lt;/td&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;&lt;/th&gt;                    &lt;td&gt;&lt;/td&gt;     &lt;td&gt;(1.68)&lt;/td&gt;  &lt;td&gt;(1.47)&lt;/td&gt; 
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Latitude2&lt;/th&gt;           &lt;td&gt;&lt;/td&gt;      &lt;td&gt;-2.12&lt;/td&gt;   &lt;td&gt;0.44&lt;/td&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;&lt;/th&gt;                    &lt;td&gt;&lt;/td&gt;     &lt;td&gt;(2.86)&lt;/td&gt;  &lt;td&gt;(2.48)&lt;/td&gt; 
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Africa&lt;/th&gt;              &lt;td&gt;&lt;/td&gt;        &lt;td&gt;&lt;/td&gt;     &lt;td&gt;-1.06**&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;&lt;/th&gt;                    &lt;td&gt;&lt;/td&gt;        &lt;td&gt;&lt;/td&gt;     &lt;td&gt;(0.41)&lt;/td&gt; 
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Asia&lt;/th&gt;                &lt;td&gt;&lt;/td&gt;        &lt;td&gt;&lt;/td&gt;     &lt;td&gt;-0.74*&lt;/td&gt; 
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;&lt;/th&gt;                    &lt;td&gt;&lt;/td&gt;        &lt;td&gt;&lt;/td&gt;     &lt;td&gt;(0.42)&lt;/td&gt; 
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Namer&lt;/th&gt;               &lt;td&gt;&lt;/td&gt;        &lt;td&gt;&lt;/td&gt;      &lt;td&gt;-0.17&lt;/td&gt; 
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;&lt;/th&gt;                    &lt;td&gt;&lt;/td&gt;        &lt;td&gt;&lt;/td&gt;     &lt;td&gt;(0.40)&lt;/td&gt; 
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Samer&lt;/th&gt;               &lt;td&gt;&lt;/td&gt;        &lt;td&gt;&lt;/td&gt;      &lt;td&gt;-0.12&lt;/td&gt; 
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;&lt;/th&gt;                    &lt;td&gt;&lt;/td&gt;        &lt;td&gt;&lt;/td&gt;     &lt;td&gt;(0.42)&lt;/td&gt; 
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;No. observations&lt;/th&gt;   &lt;td&gt;64&lt;/td&gt;      &lt;td&gt;64&lt;/td&gt;      &lt;td&gt;64&lt;/td&gt;   
&lt;/tr&gt;
&lt;/table&gt;
&lt;h2 id=&#34;23-endogeneity&#34;&gt;2.3 Endogeneity&lt;/h2&gt;
&lt;p&gt;As &lt;a href=&#34;https://python-programming.quantecon.org/zreferences.html#acemoglu2001&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[AJR01]&lt;/a&gt; discuss, the OLS models likely suffer from &lt;strong&gt;endogeneity&lt;/strong&gt; issues, resulting in biased and inconsistent model estimates.&lt;/p&gt;
&lt;p&gt;Namely, there is likely a two-way relationship between institutions an economic outcomes:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;richer countries may be able to afford or prefer better institutions&lt;/li&gt;
&lt;li&gt;variables that affect income may also be correlated with institutional differences&lt;/li&gt;
&lt;li&gt;the construction of the index may be biased; analysts may be biased towards seeing countries with higher income having better institutions&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;To deal with endogeneity, we can use &lt;strong&gt;two-stage least squares (2SLS) regression&lt;/strong&gt;, which is an extension of OLS regression.&lt;/p&gt;
&lt;p&gt;This method requires replacing the endogenous variable $ {Exprop}_i $ with a variable that is:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;correlated with $ {Exprop}_i $&lt;/li&gt;
&lt;li&gt;not correlated with the error term (ie. it should not directly affect the dependent variable, otherwise it would be correlated with $ u_i $ due to omitted variable bias)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;We can write our model as&lt;/p&gt;
&lt;p&gt;$$
{GDP}_i = \beta_0 + \beta_1 {Exprop}_i + \varepsilon_i \
{Exprop}_i = \delta_0 + \delta_1 {logMort}_i + v_i
$$&lt;/p&gt;
&lt;p&gt;The new set of regressors &lt;code&gt;logMort&lt;/code&gt; is called an &lt;strong&gt;instrument&lt;/strong&gt;, which aims to remove endogeneity in our proxy of institutional differences.&lt;/p&gt;
&lt;p&gt;The main contribution of &lt;a href=&#34;https://python-programming.quantecon.org/zreferences.html#acemoglu2001&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[AJR01]&lt;/a&gt; is the use of settler mortality rates to instrument for institutional differences.&lt;/p&gt;
&lt;p&gt;They hypothesize that higher mortality rates of colonizers led to the establishment of institutions that were more extractive in nature (less protection against expropriation), and these institutions still persist today.&lt;/p&gt;
&lt;p&gt;Using a scatterplot (Figure 3 in &lt;a href=&#34;https://python-programming.quantecon.org/zreferences.html#acemoglu2001&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[AJR01]&lt;/a&gt;), we can see protection against expropriation is negatively correlated with settler mortality rates, coinciding with the authors’ hypothesis and satisfying the first condition of a valid instrument.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Dropping NA&#39;s is required to use numpy&#39;s polyfit
df2 = df.dropna(subset=[&#39;logMort&#39;, &#39;Exprop&#39;])
X = df2[&#39;logMort&#39;]
y = df2[&#39;Exprop&#39;]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Make new figure 2
def make_new_figure_2():

    # Init figure
    fig, ax = plt.subplots(1,1)
    ax.set_title(&#39;Figure 3: First-stage&#39;)

    # Fit a linear trend line
    sns.regplot(x=X, y=y, ax=ax, order=1, scatter=True, ci=None, line_kws={&amp;quot;color&amp;quot;: &amp;quot;r&amp;quot;})

    ax.set_xlim([1.8,8.4])
    ax.set_ylim([3.3,10.4])
    ax.set_xlabel(&#39;Log of Settler Mortality&#39;)
    ax.set_ylabel(&#39;Average Expropriation Risk 1985-95&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;make_new_figure_2()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/02_iv_43_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;The second condition may not be satisfied if settler mortality rates in the 17th to 19th centuries have a direct effect on current GDP (in addition to their indirect effect through institutions).&lt;/p&gt;
&lt;p&gt;For example, settler mortality rates may be related to the current disease environment in a country, which could affect current economic performance.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://python-programming.quantecon.org/zreferences.html#acemoglu2001&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[AJR01]&lt;/a&gt; argue this is unlikely because:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The majority of settler deaths were due to malaria and yellow fever
and had a limited effect on local people.&lt;/li&gt;
&lt;li&gt;The disease burden on local people in Africa or India, for example,
did not appear to be higher than average, supported by relatively
high population densities in these areas before colonization.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;As we appear to have a valid instrument, we can use 2SLS regression to
obtain consistent and unbiased parameter estimates.&lt;/p&gt;
&lt;h3 id=&#34;first-stage&#34;&gt;First stage&lt;/h3&gt;
&lt;p&gt;The first stage involves regressing the endogenous variable
($ {Exprop}_i $) on the instrument.&lt;/p&gt;
&lt;p&gt;The instrument is the set of all exogenous variables in our model (and
not just the variable we have replaced).&lt;/p&gt;
&lt;p&gt;Using model 1 as an example, our instrument is simply a constant and
settler mortality rates $ {logMort}_i $.&lt;/p&gt;
&lt;p&gt;Therefore, we will estimate the first-stage regression as&lt;/p&gt;
&lt;p&gt;$$
{Exprop}_i = \delta_0 + \delta_1 {logMort}_i + v_i
$$&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Add a constant variable
df[&#39;const&#39;] = 1

# Fit the first stage regression and print summary
results_fs = sm.OLS(df[&#39;Exprop&#39;],
                    df.loc[:,[&#39;const&#39;, &#39;logMort&#39;]],
                    missing=&#39;drop&#39;).fit()
results_fs.summary()
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;caption&gt;OLS Regression Results&lt;/caption&gt;
&lt;tr&gt;
  &lt;th&gt;Dep. Variable:&lt;/th&gt;         &lt;td&gt;Exprop&lt;/td&gt;      &lt;th&gt;  R-squared:         &lt;/th&gt; &lt;td&gt;   0.274&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Model:&lt;/th&gt;                   &lt;td&gt;OLS&lt;/td&gt;       &lt;th&gt;  Adj. R-squared:    &lt;/th&gt; &lt;td&gt;   0.262&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Method:&lt;/th&gt;             &lt;td&gt;Least Squares&lt;/td&gt;  &lt;th&gt;  F-statistic:       &lt;/th&gt; &lt;td&gt;   23.34&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Date:&lt;/th&gt;             &lt;td&gt;Mon, 03 Jan 2022&lt;/td&gt; &lt;th&gt;  Prob (F-statistic):&lt;/th&gt; &lt;td&gt;9.27e-06&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Time:&lt;/th&gt;                 &lt;td&gt;18:31:10&lt;/td&gt;     &lt;th&gt;  Log-Likelihood:    &lt;/th&gt; &lt;td&gt; -104.69&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;No. Observations:&lt;/th&gt;      &lt;td&gt;    64&lt;/td&gt;      &lt;th&gt;  AIC:               &lt;/th&gt; &lt;td&gt;   213.4&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Df Residuals:&lt;/th&gt;          &lt;td&gt;    62&lt;/td&gt;      &lt;th&gt;  BIC:               &lt;/th&gt; &lt;td&gt;   217.7&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Df Model:&lt;/th&gt;              &lt;td&gt;     1&lt;/td&gt;      &lt;th&gt;                     &lt;/th&gt;     &lt;td&gt; &lt;/td&gt;   
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Covariance Type:&lt;/th&gt;      &lt;td&gt;nonrobust&lt;/td&gt;    &lt;th&gt;                     &lt;/th&gt;     &lt;td&gt; &lt;/td&gt;   
&lt;/tr&gt;
&lt;/table&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
     &lt;td&gt;&lt;/td&gt;        &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;const&lt;/th&gt;   &lt;td&gt;    9.3659&lt;/td&gt; &lt;td&gt;    0.611&lt;/td&gt; &lt;td&gt;   15.339&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    8.145&lt;/td&gt; &lt;td&gt;   10.586&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;logMort&lt;/th&gt; &lt;td&gt;   -0.6133&lt;/td&gt; &lt;td&gt;    0.127&lt;/td&gt; &lt;td&gt;   -4.831&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;   -0.867&lt;/td&gt; &lt;td&gt;   -0.360&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
  &lt;th&gt;Omnibus:&lt;/th&gt;       &lt;td&gt; 0.047&lt;/td&gt; &lt;th&gt;  Durbin-Watson:     &lt;/th&gt; &lt;td&gt;   1.592&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Prob(Omnibus):&lt;/th&gt; &lt;td&gt; 0.977&lt;/td&gt; &lt;th&gt;  Jarque-Bera (JB):  &lt;/th&gt; &lt;td&gt;   0.154&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Skew:&lt;/th&gt;          &lt;td&gt; 0.060&lt;/td&gt; &lt;th&gt;  Prob(JB):          &lt;/th&gt; &lt;td&gt;   0.926&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Kurtosis:&lt;/th&gt;      &lt;td&gt; 2.792&lt;/td&gt; &lt;th&gt;  Cond. No.          &lt;/th&gt; &lt;td&gt;    19.4&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;&lt;br/&gt;&lt;br/&gt;Warnings:&lt;br/&gt;[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
&lt;p&gt;We need to retrieve the predicted values of $ {Exprop}_i $ using
&lt;code&gt;.predict()&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;We then replace the endogenous variable $ {Exprop}_i $ with the
predicted values $ \widehat{Exprop}_i $ in the original linear model.&lt;/p&gt;
&lt;p&gt;Our second stage regression is thus&lt;/p&gt;
&lt;p&gt;$$
{GDP}_i = \beta_0 + \beta_1 \widehat{Exprop}_i + u_i
$$&lt;/p&gt;
&lt;h3 id=&#34;second-stage&#34;&gt;Second stage&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Second stage
df[&#39;predicted_Exprop&#39;] = results_fs.predict()
results_ss = sm.OLS.from_formula(&#39;GDP ~ predicted_Exprop&#39;, df).fit()

# Print
results_ss.summary()
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;caption&gt;OLS Regression Results&lt;/caption&gt;
&lt;tr&gt;
  &lt;th&gt;Dep. Variable:&lt;/th&gt;           &lt;td&gt;GDP&lt;/td&gt;       &lt;th&gt;  R-squared:         &lt;/th&gt; &lt;td&gt;   0.462&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Model:&lt;/th&gt;                   &lt;td&gt;OLS&lt;/td&gt;       &lt;th&gt;  Adj. R-squared:    &lt;/th&gt; &lt;td&gt;   0.453&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Method:&lt;/th&gt;             &lt;td&gt;Least Squares&lt;/td&gt;  &lt;th&gt;  F-statistic:       &lt;/th&gt; &lt;td&gt;   53.24&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Date:&lt;/th&gt;             &lt;td&gt;Mon, 03 Jan 2022&lt;/td&gt; &lt;th&gt;  Prob (F-statistic):&lt;/th&gt; &lt;td&gt;6.58e-10&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Time:&lt;/th&gt;                 &lt;td&gt;18:31:10&lt;/td&gt;     &lt;th&gt;  Log-Likelihood:    &lt;/th&gt; &lt;td&gt; -73.208&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;No. Observations:&lt;/th&gt;      &lt;td&gt;    64&lt;/td&gt;      &lt;th&gt;  AIC:               &lt;/th&gt; &lt;td&gt;   150.4&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Df Residuals:&lt;/th&gt;          &lt;td&gt;    62&lt;/td&gt;      &lt;th&gt;  BIC:               &lt;/th&gt; &lt;td&gt;   154.7&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Df Model:&lt;/th&gt;              &lt;td&gt;     1&lt;/td&gt;      &lt;th&gt;                     &lt;/th&gt;     &lt;td&gt; &lt;/td&gt;   
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Covariance Type:&lt;/th&gt;      &lt;td&gt;nonrobust&lt;/td&gt;    &lt;th&gt;                     &lt;/th&gt;     &lt;td&gt; &lt;/td&gt;   
&lt;/tr&gt;
&lt;/table&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
          &lt;td&gt;&lt;/td&gt;            &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt;        &lt;td&gt;    2.0448&lt;/td&gt; &lt;td&gt;    0.830&lt;/td&gt; &lt;td&gt;    2.463&lt;/td&gt; &lt;td&gt; 0.017&lt;/td&gt; &lt;td&gt;    0.385&lt;/td&gt; &lt;td&gt;    3.705&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;predicted_Exprop&lt;/th&gt; &lt;td&gt;    0.9235&lt;/td&gt; &lt;td&gt;    0.127&lt;/td&gt; &lt;td&gt;    7.297&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    0.671&lt;/td&gt; &lt;td&gt;    1.177&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
  &lt;th&gt;Omnibus:&lt;/th&gt;       &lt;td&gt;10.463&lt;/td&gt; &lt;th&gt;  Durbin-Watson:     &lt;/th&gt; &lt;td&gt;   2.052&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Prob(Omnibus):&lt;/th&gt; &lt;td&gt; 0.005&lt;/td&gt; &lt;th&gt;  Jarque-Bera (JB):  &lt;/th&gt; &lt;td&gt;  10.693&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Skew:&lt;/th&gt;          &lt;td&gt;-0.806&lt;/td&gt; &lt;th&gt;  Prob(JB):          &lt;/th&gt; &lt;td&gt; 0.00476&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Kurtosis:&lt;/th&gt;      &lt;td&gt; 4.188&lt;/td&gt; &lt;th&gt;  Cond. No.          &lt;/th&gt; &lt;td&gt;    57.8&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;&lt;br/&gt;&lt;br/&gt;Warnings:&lt;br/&gt;[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
&lt;p&gt;The second-stage regression results give us an unbiased and consistent
estimate of the effect of institutions on economic outcomes.&lt;/p&gt;
&lt;p&gt;The result suggests a stronger positive relationship than what the OLS
results indicated.&lt;/p&gt;
&lt;p&gt;Note that while our parameter estimates are correct, our standard errors
are not and for this reason, computing 2SLS ‘manually’ (in stages with
OLS) is not recommended.&lt;/p&gt;
&lt;p&gt;We can correctly estimate a 2SLS regression in one step using the
&lt;a href=&#34;https://github.com/bashtage/linearmodels&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;linearmodels&lt;/a&gt; package, an extension of &lt;code&gt;statsmodels&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Note that when using &lt;code&gt;IV2SLS&lt;/code&gt;, the exogenous and instrument variables
are split up in the function arguments (whereas before the instrument
included exogenous variables)&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# IV regression
iv = IV2SLS(dependent=df[&#39;GDP&#39;],
            exog=df[&#39;const&#39;],
            endog=df[&#39;Exprop&#39;],
            instruments=df[&#39;logMort&#39;]).fit()

# Print
iv.summary
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;caption&gt;IV-2SLS Estimation Summary&lt;/caption&gt;
&lt;tr&gt;
  &lt;th&gt;Dep. Variable:&lt;/th&gt;           &lt;td&gt;GDP&lt;/td&gt;       &lt;th&gt;  R-squared:         &lt;/th&gt; &lt;td&gt;0.2205&lt;/td&gt; 
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Estimator:&lt;/th&gt;             &lt;td&gt;IV-2SLS&lt;/td&gt;     &lt;th&gt;  Adj. R-squared:    &lt;/th&gt; &lt;td&gt;0.2079&lt;/td&gt; 
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;No. Observations:&lt;/th&gt;        &lt;td&gt;64&lt;/td&gt;        &lt;th&gt;  F-statistic:       &lt;/th&gt; &lt;td&gt;29.811&lt;/td&gt; 
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Date:&lt;/th&gt;             &lt;td&gt;Mon, Jan 03 2022&lt;/td&gt; &lt;th&gt;  P-value (F-stat)   &lt;/th&gt; &lt;td&gt;0.0000&lt;/td&gt; 
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Time:&lt;/th&gt;                 &lt;td&gt;18:31:10&lt;/td&gt;     &lt;th&gt;  Distribution:      &lt;/th&gt; &lt;td&gt;chi2(1)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Cov. Estimator:&lt;/th&gt;        &lt;td&gt;robust&lt;/td&gt;      &lt;th&gt;                     &lt;/th&gt;    &lt;td&gt;&lt;/td&gt;    
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;&lt;/th&gt;                          &lt;td&gt;&lt;/td&gt;         &lt;th&gt;                     &lt;/th&gt;    &lt;td&gt;&lt;/td&gt;    
&lt;/tr&gt;
&lt;/table&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;caption&gt;Parameter Estimates&lt;/caption&gt;
&lt;tr&gt;
     &lt;td&gt;&lt;/td&gt;    &lt;th&gt;Parameter&lt;/th&gt; &lt;th&gt;Std. Err.&lt;/th&gt; &lt;th&gt;T-stat&lt;/th&gt; &lt;th&gt;P-value&lt;/th&gt; &lt;th&gt;Lower CI&lt;/th&gt; &lt;th&gt;Upper CI&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;const&lt;/th&gt;   &lt;td&gt;2.0448&lt;/td&gt;    &lt;td&gt;1.1273&lt;/td&gt;   &lt;td&gt;1.8139&lt;/td&gt; &lt;td&gt;0.0697&lt;/td&gt;   &lt;td&gt;-0.1647&lt;/td&gt;  &lt;td&gt;4.2542&lt;/td&gt; 
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Exprop&lt;/th&gt;  &lt;td&gt;0.9235&lt;/td&gt;    &lt;td&gt;0.1691&lt;/td&gt;   &lt;td&gt;5.4599&lt;/td&gt; &lt;td&gt;0.0000&lt;/td&gt;   &lt;td&gt;0.5920&lt;/td&gt;   &lt;td&gt;1.2550&lt;/td&gt; 
&lt;/tr&gt;
&lt;/table&gt;&lt;br/&gt;&lt;br/&gt;Endogenous: Exprop&lt;br/&gt;Instruments: logMort&lt;br/&gt;Robust Covariance (Heteroskedastic)&lt;br/&gt;Debiased: False
&lt;p&gt;Given that we now have consistent and unbiased estimates, we can infer
from the model we have estimated that institutional differences
(stemming from institutions set up during colonization) can help
to explain differences in income levels across countries today.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://python-programming.quantecon.org/zreferences.html#acemoglu2001&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[AJR01]&lt;/a&gt; use a marginal effect of 0.94 to calculate that the
difference in the index between Chile and Nigeria (ie. institutional
quality) implies up to a 7-fold difference in income, emphasizing the
significance of institutions in economic development.&lt;/p&gt;
&lt;h2 id=&#34;24-matrix-algebra&#34;&gt;2.4 Matrix Algebra&lt;/h2&gt;
&lt;p&gt;The OLS parameter $ \beta $ can also be estimated using matrix
algebra and &lt;code&gt;numpy&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;The linear equation we want to estimate is (written in matrix form)&lt;/p&gt;
&lt;p&gt;$$
y = X\beta + \varepsilon
$$&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Init 
X = df[[&#39;const&#39;, &#39;Exprop&#39;]].values
Z = df[[&#39;const&#39;, &#39;logMort&#39;]].values
y = df[&#39;GDP&#39;].values
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To solve for the unknown parameter $ \beta $, we want to minimize
the sum of squared residuals&lt;/p&gt;
&lt;p&gt;$$
\underset{\hat{\beta}}{\min} \ \hat{\varepsilon}&#39;\hat{\varepsilon}
$$&lt;/p&gt;
&lt;p&gt;Rearranging the first equation and substituting into the second
equation, we can write&lt;/p&gt;
&lt;p&gt;$$
\underset{\hat{\beta}}{\min} \ (Y - X\hat{\beta})&#39; (Y - X\hat{\beta})
$$&lt;/p&gt;
&lt;p&gt;Solving this optimization problem gives the solution for the
$ \hat{\beta} $ coefficients&lt;/p&gt;
&lt;p&gt;$$
\hat{\beta} = (X&amp;rsquo;X)^{-1}X&amp;rsquo;y
$$&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Compute beta OLS
beta_OLS = inv(X.T @ X) @ X.T @ y

print(beta_OLS)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[4.66087966 0.52203367]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As we as see above, the OLS coefficient might suffer from endogeneity bias. We can solve the issue by instrumenting the predicted average expropriation rate with the average settler mortality.&lt;/p&gt;
&lt;p&gt;If we define settler mortality as $Z$, our full model is&lt;/p&gt;
&lt;p&gt;$$
y = X\beta + \varepsilon \
X = Z\gamma + \mu
$$&lt;/p&gt;
&lt;p&gt;Where we refer to the second equation as second stage and to the first equation as the reduced form equation. In our case, since the number of endogenous varaibles is equal to the number of insturments, there are two equivalent estimators that do not suffer from endogeneity bias: 2SLS and IV.&lt;/p&gt;
&lt;p&gt;IV, the one stage estimator&lt;/p&gt;
&lt;p&gt;$$
\hat \beta_{IV} = (Z&amp;rsquo;X)^{-1} Z&#39; y
$$&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Compute beta IV
beta_IV = inv(Z.T @ X) @ Z.T @ y

print(beta_IV)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[2.0447613  0.92351936]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;One of the hypothesis behind the IV estimator is the &lt;em&gt;relevance&lt;/em&gt; of the instrument, i.e. we have a strong predictor in the first stage. This is the only hypothesis that we can empirically assess by checking the significance of the first stage coefficient.&lt;/p&gt;
&lt;p&gt;$$
\hat \gamma = (Z&#39; Z)^{-1} Z&amp;rsquo;X \
\hat Var (\hat \gamma) = \sigma_u^2 (Z&#39; Z)^{-1}
$$&lt;/p&gt;
&lt;p&gt;where&lt;/p&gt;
&lt;p&gt;$$
u = X - Z \hat \gamma
$$&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Estimate first stage coefficient
gamma_hat = (inv(Z.T @ Z) @ Z.T @ X)

print(gamma_hat[1,1])
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;-0.613289272386864
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Compute variance of the estimator
u = X - Z @ gamma_hat
var_gamma_hat = np.var(u) * inv(Z.T @ Z)

# Compute standard errors
std_gamma_hat = var_gamma_hat[1,1]**.5
print(std_gamma_hat)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;0.08834733362858548
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Compute 95% confidence interval
CI = [gamma_hat[1,1] - 1.96*std_gamma_hat, gamma_hat[1,1] + 1.96*std_gamma_hat]

print(CI)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[-0.7864500462988916, -0.4401284984748365]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The first stage coefficient is negative and significant, i.e. settler mortality is negatively correlated with the expropriation rate.&lt;/p&gt;
&lt;p&gt;How does it work when we have more instruments than endogenous variables? Two-State Least Squares.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Regress $X$ on $Z$ and obtain $\hat X$:
$$
\hat X = Z (Z&#39; Z)^{-1} Z&amp;rsquo;X
$$&lt;/li&gt;
&lt;li&gt;Regress $Y$ on $\hat X$ and obtain $\hat \beta_{2SLS}$
$$
\hat \beta_{2SLS} = (\hat X&#39; \hat X)^{-1} \hat X&#39; y
$$&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In our case, just for the sake of exposition, let&amp;rsquo;s generate a second instrument: the settler mortality squared, &lt;code&gt;logMort_2&lt;/code&gt; = &lt;code&gt;logMort&lt;/code&gt;^2.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df[&#39;logMort_2&#39;] = df[&#39;logMort&#39;]**2
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Define Z
Z1 = df[[&#39;const&#39;, &#39;logMort&#39;, &#39;logMort_2&#39;]].values

# Compute beta 2SLS in two steps
X_hat = Z1 @ inv(Z1.T @ Z1) @ Z1.T @ X
beta_2SLS = inv(X_hat.T @ X_hat) @ X_hat.T @ y

print(beta_2SLS)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[3.08817432 0.76339075]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The 2SLS estimator does not have to be actually estimated in two stages. Combining the two formulas above, we get&lt;/p&gt;
&lt;p&gt;$$
\hat{\beta} _ {2SLS} =  \Big( X&amp;rsquo;Z (Z&amp;rsquo;Z)^{-1} Z&amp;rsquo;X \Big)^{-1} \Big( X&amp;rsquo;Z (Z&amp;rsquo;Z)^{-1} Z&amp;rsquo;y \Big)
$$&lt;/p&gt;
&lt;p&gt;which can be computed in one step.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Compute beta 2SLS in one step
beta_2SLS = inv(X_hat.T @ Z1 @ inv(Z1.T @ Z1) @ Z1.T @ X_hat) @ X_hat.T @ Z1 @ inv(Z1.T @ Z1) @ Z1.T @ y
    
print(beta_2SLS)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[3.08817432 0.76339075]
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Data Types</title>
      <link>https://matteocourthoud.github.io/course/data-science/02_data_types/</link>
      <pubDate>Sun, 27 Feb 2022 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/course/data-science/02_data_types/</guid>
      <description>&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import numpy as np
import pandas as pd
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;setup&#34;&gt;Setup&lt;/h2&gt;
&lt;p&gt;For the scope of this tutorial we are going to use AirBnb Scraped data for the city of Bologna. The data is freely available at &lt;strong&gt;Inside AirBnb&lt;/strong&gt;: &lt;a href=&#34;http://insideairbnb.com/get-the-data.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://insideairbnb.com/get-the-data.html&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;A description of all variables in all datasets is avaliable &lt;a href=&#34;https://docs.google.com/spreadsheets/d/1iWCNJcSutYqpULSQHlNyGInUvHg2BoUGoNRIGa6Szc4/edit#gid=982310896&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;We are going to use 2 datasets:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;listing dataset: contains listing-level information&lt;/li&gt;
&lt;li&gt;pricing dataset: contains pricing data, over time&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Import listings data
url_listings = &amp;quot;http://data.insideairbnb.com/italy/emilia-romagna/bologna/2021-12-17/visualisations/listings.csv&amp;quot;
df_listings = pd.read_csv(url_listings)

# Import pricing data
url_prices = &amp;quot;http://data.insideairbnb.com/italy/emilia-romagna/bologna/2021-12-17/data/calendar.csv.gz&amp;quot;
df_prices = pd.read_csv(url_prices, compression=&amp;quot;gzip&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;numerical-data&#34;&gt;Numerical Data&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Methods&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;pd.cut()&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;p&gt;Standard mathematical operations between columns are done row-wise.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_prices[&#39;maximum_nights&#39;] - df_prices[&#39;minimum_nights&#39;]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;0           148
1           357
2           357
3           357
4           357
           ... 
1260340    1124
1260341    1124
1260342    1124
1260343    1124
1260344    1124
Length: 1260345, dtype: int64
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can use most &lt;code&gt;numpy&lt;/code&gt; operations element-wise on a single column.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;np.log(df_listings[&#39;price&#39;])
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;0       4.219508
1       3.367296
2       3.912023
3       4.836282
4       3.912023
          ...   
3448    3.465736
3449    3.806662
3450    3.912023
3451    4.897840
3452    4.744932
Name: price, Length: 3453, dtype: float64
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can create a categorical variables from a numerical one using the &lt;code&gt;pd.cut()&lt;/code&gt; function.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;pd.cut(df_listings[&#39;price&#39;], 
       bins = [0, 50, 100, np.inf], 
       labels=[&#39;cheap&#39;, &#39;ok&#39;, &#39;expensive&#39;])
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;0              ok
1           cheap
2           cheap
3       expensive
4           cheap
          ...    
3448        cheap
3449        cheap
3450        cheap
3451    expensive
3452    expensive
Name: price, Length: 3453, dtype: category
Categories (3, object): [&#39;cheap&#39; &amp;lt; &#39;ok&#39; &amp;lt; &#39;expensive&#39;]
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;string-data&#34;&gt;String Data&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Methods&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;.str()&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;p&gt;We can use the &lt;code&gt;+&lt;/code&gt; operator between columns, to do pairwise append.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: we cannot do it with strings.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_listings[&#39;host_name&#39;] + df_listings[&#39;neighbourhood&#39;]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;0                     CarloSanto Stefano
1              EleonoraPorto - Saragozza
2                     PaoloSanto Stefano
3                Anna MariaSanto Stefano
4               ValerioPorto - Saragozza
                      ...               
3448                        IleanaNavile
3449           FernandaPorto - Saragozza
3450                        IleanaNavile
3451        Wonderful ItalySanto Stefano
3452    Wonderful ItalyPorto - Saragozza
Length: 3453, dtype: object
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Pandas Series have a lot of vectorized string functions you can call on them.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;[i for i in dir(pd.Series.str) if &#39;_&#39; not in i]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[&#39;capitalize&#39;,
 &#39;casefold&#39;,
 &#39;cat&#39;,
 &#39;center&#39;,
 &#39;contains&#39;,
 &#39;count&#39;,
 &#39;decode&#39;,
 &#39;encode&#39;,
 &#39;endswith&#39;,
 &#39;extract&#39;,
 &#39;extractall&#39;,
 &#39;find&#39;,
 &#39;findall&#39;,
 &#39;fullmatch&#39;,
 &#39;get&#39;,
 &#39;index&#39;,
 &#39;isalnum&#39;,
 &#39;isalpha&#39;,
 &#39;isdecimal&#39;,
 &#39;isdigit&#39;,
 &#39;islower&#39;,
 &#39;isnumeric&#39;,
 &#39;isspace&#39;,
 &#39;istitle&#39;,
 &#39;isupper&#39;,
 &#39;join&#39;,
 &#39;len&#39;,
 &#39;ljust&#39;,
 &#39;lower&#39;,
 &#39;lstrip&#39;,
 &#39;match&#39;,
 &#39;normalize&#39;,
 &#39;pad&#39;,
 &#39;partition&#39;,
 &#39;repeat&#39;,
 &#39;replace&#39;,
 &#39;rfind&#39;,
 &#39;rindex&#39;,
 &#39;rjust&#39;,
 &#39;rpartition&#39;,
 &#39;rsplit&#39;,
 &#39;rstrip&#39;,
 &#39;slice&#39;,
 &#39;split&#39;,
 &#39;startswith&#39;,
 &#39;strip&#39;,
 &#39;swapcase&#39;,
 &#39;title&#39;,
 &#39;translate&#39;,
 &#39;upper&#39;,
 &#39;wrap&#39;,
 &#39;zfill&#39;]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For example, we want to remove the dollar symbol from the &lt;code&gt;price&lt;/code&gt; variable in the &lt;code&gt;df_prices&lt;/code&gt; dataset.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_prices[&#39;price&#39;].str.replace(&#39;$&#39;, &#39;&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;/var/folders/xy/szm3b7211cj16ldwcywbvfzdqv7yhd/T/ipykernel_74833/117745113.py:1: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will*not* be treated as literal strings when regex=True.
  df_prices[&#39;price&#39;].str.replace(&#39;$&#39;, &#39;&#39;)





0           70.00
1           68.00
2           68.00
3           68.00
4           68.00
            ...  
1260340    115.00
1260341    115.00
1260342    115.00
1260343    115.00
1260344    115.00
Name: price, Length: 1260345, dtype: object
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Some of these functions use regular expressions.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;match()&lt;/code&gt;: Call re.match() on each element, returning a boolean.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;extract()&lt;/code&gt;: Call re.match() on each element, returning matched groups as strings.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;findall()&lt;/code&gt;: Call re.findall() on each element&lt;/li&gt;
&lt;li&gt;&lt;code&gt;replace()&lt;/code&gt;: Replace occurrences of pattern with some other string&lt;/li&gt;
&lt;li&gt;&lt;code&gt;contains()&lt;/code&gt;: Call re.search() on each element, returning a boolean&lt;/li&gt;
&lt;li&gt;&lt;code&gt;count()&lt;/code&gt;: Count occurrences of pattern&lt;/li&gt;
&lt;li&gt;&lt;code&gt;split()&lt;/code&gt;: Equivalent to str.split(), but accepts regexps
rsplit()&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For example, the next code checks whether in the word &lt;code&gt;centre&lt;/code&gt; or &lt;code&gt;center&lt;/code&gt; are contained in the text description.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_listings[&#39;name&#39;].str.contains(&#39;centre|center&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;0        True
1       False
2        True
3       False
4       False
        ...  
3448    False
3449    False
3450    False
3451    False
3452    False
Name: name, Length: 3453, dtype: bool
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Lastly, we can (try to) convert string variables to numeric using &lt;code&gt;astype(float)&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_prices[&#39;price&#39;].str.replace(&#39;[$|,]&#39;, &#39;&#39;, regex=True).astype(float)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;0           70.0
1           68.0
2           68.0
3           68.0
4           68.0
           ...  
1260340    115.0
1260341    115.0
1260342    115.0
1260343    115.0
1260344    115.0
Name: price, Length: 1260345, dtype: float64
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can also use it to convert numerics to strings using &lt;code&gt;astype(str)&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_listings[&#39;id&#39;].astype(str)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;0          42196
1          46352
2          59697
3          85368
4         145779
          ...   
3448    53810648
3449    53820830
3450    53837098
3451    53837654
3452    53854962
Name: id, Length: 3453, dtype: object
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;time-data&#34;&gt;Time Data&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Methods&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;pd.to_datetime()&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;.dt.year()&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;pd.to_timedelta()&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;p&gt;In the &lt;code&gt;df_prices&lt;/code&gt; we have a date variable, &lt;code&gt;date&lt;/code&gt;. Which format is it in? We can check it with the &lt;code&gt;.dtypes&lt;/code&gt; attribute.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_prices[&#39;date&#39;].dtypes
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;dtype(&#39;O&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can convert a variable into a date using the&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_prices[&#39;datetime&#39;] = pd.to_datetime(df_prices[&#39;date&#39;])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Indeed, if we now check the format of the &lt;code&gt;datetime&lt;/code&gt; variable, it&amp;rsquo;s &lt;code&gt;datetime&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_prices[&#39;datetime&#39;].dtypes
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;dtype(&#39;&amp;lt;M8[ns]&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Once we have a variable in &lt;code&gt;datetime&lt;/code&gt; format, we gain plenty of datetime operations through the &lt;code&gt;dt&lt;/code&gt; library. For example, we can extract the year using &lt;code&gt;.dt.year&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_prices[&#39;datetime&#39;].dt.year
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;0          2021
1          2021
2          2021
3          2021
4          2021
           ... 
1260340    2022
1260341    2022
1260342    2022
1260343    2022
1260344    2022
Name: datetime, Length: 1260345, dtype: int64
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can add or subtract time periods from a date using the &lt;code&gt;pd.to_timedelta()&lt;/code&gt; function. We need to specify the unit of measurement with the &lt;code&gt;unit&lt;/code&gt; option.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_prices[&#39;datetime&#39;] -  pd.to_timedelta(3, unit=&#39;d&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;0         2021-12-14
1         2021-12-14
2         2021-12-15
3         2021-12-16
4         2021-12-17
             ...    
1260340   2022-12-09
1260341   2022-12-10
1260342   2022-12-11
1260343   2022-12-12
1260344   2022-12-13
Name: datetime, Length: 1260345, dtype: datetime64[ns]
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;missing-data&#34;&gt;Missing Data&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Methods&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;.isna()&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;.dropna()&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;.fillna()&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;p&gt;The function &lt;code&gt;isna()&lt;/code&gt; reports missing values.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_listings.isna().head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;id&lt;/th&gt;
      &lt;th&gt;name&lt;/th&gt;
      &lt;th&gt;host_id&lt;/th&gt;
      &lt;th&gt;host_name&lt;/th&gt;
      &lt;th&gt;neighbourhood_group&lt;/th&gt;
      &lt;th&gt;neighbourhood&lt;/th&gt;
      &lt;th&gt;latitude&lt;/th&gt;
      &lt;th&gt;longitude&lt;/th&gt;
      &lt;th&gt;room_type&lt;/th&gt;
      &lt;th&gt;price&lt;/th&gt;
      &lt;th&gt;minimum_nights&lt;/th&gt;
      &lt;th&gt;number_of_reviews&lt;/th&gt;
      &lt;th&gt;last_review&lt;/th&gt;
      &lt;th&gt;reviews_per_month&lt;/th&gt;
      &lt;th&gt;calculated_host_listings_count&lt;/th&gt;
      &lt;th&gt;availability_365&lt;/th&gt;
      &lt;th&gt;number_of_reviews_ltm&lt;/th&gt;
      &lt;th&gt;license&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;True&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;True&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;True&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;True&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;True&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;True&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;True&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;True&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;True&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;True&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;To get a quick description of the amount of missing data in the dataset, we can use&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_listings.isna().sum()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;id                                   0
name                                 0
host_id                              0
host_name                            9
neighbourhood_group               3453
neighbourhood                        0
latitude                             0
longitude                            0
room_type                            0
price                                0
minimum_nights                       0
number_of_reviews                    0
last_review                        409
reviews_per_month                  409
calculated_host_listings_count       0
availability_365                     0
number_of_reviews_ltm                0
license                           3318
dtype: int64
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can drop missing values using &lt;code&gt;dropna()&lt;/code&gt;. It drops all rows with at least one missing value.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_listings.dropna().shape
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;(0, 18)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In this case unfortunately, it drops all the rows. If we wa to drop only rows with all missing values, we can use the parameter &lt;code&gt;how=&#39;all&#39;&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_listings.dropna(how=&#39;all&#39;).shape
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;(3453, 18)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If we want to drop only missing values for one particular value, we can use the &lt;code&gt;subset&lt;/code&gt; option.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_listings.dropna(subset=[&#39;reviews_per_month&#39;]).shape
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;(3044, 18)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can also fill the missing values instead of dropping them, using &lt;code&gt;fillna()&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_listings.fillna(&#39; -- This was NA  -- &#39;).head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;id&lt;/th&gt;
      &lt;th&gt;name&lt;/th&gt;
      &lt;th&gt;host_id&lt;/th&gt;
      &lt;th&gt;host_name&lt;/th&gt;
      &lt;th&gt;neighbourhood_group&lt;/th&gt;
      &lt;th&gt;neighbourhood&lt;/th&gt;
      &lt;th&gt;latitude&lt;/th&gt;
      &lt;th&gt;longitude&lt;/th&gt;
      &lt;th&gt;room_type&lt;/th&gt;
      &lt;th&gt;price&lt;/th&gt;
      &lt;th&gt;minimum_nights&lt;/th&gt;
      &lt;th&gt;number_of_reviews&lt;/th&gt;
      &lt;th&gt;last_review&lt;/th&gt;
      &lt;th&gt;reviews_per_month&lt;/th&gt;
      &lt;th&gt;calculated_host_listings_count&lt;/th&gt;
      &lt;th&gt;availability_365&lt;/th&gt;
      &lt;th&gt;number_of_reviews_ltm&lt;/th&gt;
      &lt;th&gt;license&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;42196&lt;/td&gt;
      &lt;td&gt;50 sm Studio in the historic centre&lt;/td&gt;
      &lt;td&gt;184487&lt;/td&gt;
      &lt;td&gt;Carlo&lt;/td&gt;
      &lt;td&gt;-- This was NA  --&lt;/td&gt;
      &lt;td&gt;Santo Stefano&lt;/td&gt;
      &lt;td&gt;44.48507&lt;/td&gt;
      &lt;td&gt;11.34786&lt;/td&gt;
      &lt;td&gt;Entire home/apt&lt;/td&gt;
      &lt;td&gt;68&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;180&lt;/td&gt;
      &lt;td&gt;2021-11-12&lt;/td&gt;
      &lt;td&gt;1.32&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;161&lt;/td&gt;
      &lt;td&gt;6&lt;/td&gt;
      &lt;td&gt;-- This was NA  --&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;46352&lt;/td&gt;
      &lt;td&gt;A room in Pasolini&#39;s house&lt;/td&gt;
      &lt;td&gt;467810&lt;/td&gt;
      &lt;td&gt;Eleonora&lt;/td&gt;
      &lt;td&gt;-- This was NA  --&lt;/td&gt;
      &lt;td&gt;Porto - Saragozza&lt;/td&gt;
      &lt;td&gt;44.49168&lt;/td&gt;
      &lt;td&gt;11.33514&lt;/td&gt;
      &lt;td&gt;Private room&lt;/td&gt;
      &lt;td&gt;29&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;300&lt;/td&gt;
      &lt;td&gt;2021-11-30&lt;/td&gt;
      &lt;td&gt;2.2&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;248&lt;/td&gt;
      &lt;td&gt;37&lt;/td&gt;
      &lt;td&gt;-- This was NA  --&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;59697&lt;/td&gt;
      &lt;td&gt;COZY LARGE BEDROOM in the city center&lt;/td&gt;
      &lt;td&gt;286688&lt;/td&gt;
      &lt;td&gt;Paolo&lt;/td&gt;
      &lt;td&gt;-- This was NA  --&lt;/td&gt;
      &lt;td&gt;Santo Stefano&lt;/td&gt;
      &lt;td&gt;44.48817&lt;/td&gt;
      &lt;td&gt;11.34124&lt;/td&gt;
      &lt;td&gt;Private room&lt;/td&gt;
      &lt;td&gt;50&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;240&lt;/td&gt;
      &lt;td&gt;2020-10-04&lt;/td&gt;
      &lt;td&gt;2.18&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;327&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;-- This was NA  --&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;85368&lt;/td&gt;
      &lt;td&gt;Garden House Bologna&lt;/td&gt;
      &lt;td&gt;467675&lt;/td&gt;
      &lt;td&gt;Anna Maria&lt;/td&gt;
      &lt;td&gt;-- This was NA  --&lt;/td&gt;
      &lt;td&gt;Santo Stefano&lt;/td&gt;
      &lt;td&gt;44.47834&lt;/td&gt;
      &lt;td&gt;11.35672&lt;/td&gt;
      &lt;td&gt;Entire home/apt&lt;/td&gt;
      &lt;td&gt;126&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;40&lt;/td&gt;
      &lt;td&gt;2019-11-03&lt;/td&gt;
      &lt;td&gt;0.34&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;332&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;-- This was NA  --&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;145779&lt;/td&gt;
      &lt;td&gt;SINGLE ROOM&lt;/td&gt;
      &lt;td&gt;705535&lt;/td&gt;
      &lt;td&gt;Valerio&lt;/td&gt;
      &lt;td&gt;-- This was NA  --&lt;/td&gt;
      &lt;td&gt;Porto - Saragozza&lt;/td&gt;
      &lt;td&gt;44.49306&lt;/td&gt;
      &lt;td&gt;11.33786&lt;/td&gt;
      &lt;td&gt;Private room&lt;/td&gt;
      &lt;td&gt;50&lt;/td&gt;
      &lt;td&gt;10&lt;/td&gt;
      &lt;td&gt;69&lt;/td&gt;
      &lt;td&gt;2021-12-05&lt;/td&gt;
      &lt;td&gt;0.55&lt;/td&gt;
      &lt;td&gt;9&lt;/td&gt;
      &lt;td&gt;365&lt;/td&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;-- This was NA  --&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;We can also make missing values if we want.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_listings.iloc[2, 2] = np.nan
df_listings.iloc[:3, :3]
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;id&lt;/th&gt;
      &lt;th&gt;name&lt;/th&gt;
      &lt;th&gt;host_id&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;42196&lt;/td&gt;
      &lt;td&gt;50 sm Studio in the historic centre&lt;/td&gt;
      &lt;td&gt;184487.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;46352&lt;/td&gt;
      &lt;td&gt;A room in Pasolini&#39;s house&lt;/td&gt;
      &lt;td&gt;467810.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;59697&lt;/td&gt;
      &lt;td&gt;COZY LARGE BEDROOM in the city center&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Demand Estimation</title>
      <link>https://matteocourthoud.github.io/course/empirical-io/02_demand_estimation/</link>
      <pubDate>Fri, 29 Oct 2021 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/course/empirical-io/02_demand_estimation/</guid>
      <description>&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;h3 id=&#34;setting&#34;&gt;Setting&lt;/h3&gt;
&lt;p&gt;Oligopoly Supply&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;firms produce differentiated goods/products&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;selling to consumers with heterogeneous preferences&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;static model, complete information&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;products are given&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;equilibrium: NE for each product/market&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;cost-function&#34;&gt;Cost Function&lt;/h3&gt;
&lt;p&gt;Variable cost of product $j$:
$C_j (Q_j , w_{jt} , \mathbb \omega_{jt}, \gamma)$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;$Q_j$: total quantity of good $j$ sold&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$w_{jt}$ observable cost shifters; may include product
characteristics $x_{jt}$ that will affect demand (later)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$\omega_{jt}$ unobserved cost shifters (“cost shocks”); may be
correlated with latent demand shocks (later)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$\gamma$: parameters&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Notes&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;for multi-product firms, we’ll assume variable cost additive across
products for simplicity&lt;/li&gt;
&lt;li&gt;we ignore fixed costs: these affect entry/exit/innovation but not
pricing, &lt;em&gt;conditional on these things&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;notation&#34;&gt;Notation&lt;/h3&gt;
&lt;p&gt;Some other variables&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$J_t$: products/goods/choices in market $t$ (for now $J_t = J$)&lt;/li&gt;
&lt;li&gt;$P_t = (p_{1t},&amp;hellip;,p_{Jt})$: prices of all goods&lt;/li&gt;
&lt;li&gt;$\boldsymbol X_t = ( \boldsymbol x_{1t} , … , \boldsymbol x_{Jt})$ :
other characteristics of goods affecting demand (observed and
unobserved to us)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In general&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;I use &lt;strong&gt;bold&lt;/strong&gt; for arrays in dimensions that are not $i$
(consumers), $j$ (firms) or $t$ (markets)
&lt;ul&gt;
&lt;li&gt;For example product characteristics
$\boldsymbol x_{jt} = \lbrace x_{jt}^1,, &amp;hellip;, x_{jt}^K \rbrace$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;I use CAPS for variables aggregated over $j$ (firms)
&lt;ul&gt;
&lt;li&gt;For example vector of prices in market $t$:
$P_t = (p_{1t},&amp;hellip;,p_{Jt})$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;equilibrium-pricing&#34;&gt;Equilibrium Pricing&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Demand system:&lt;/p&gt;
&lt;p&gt;$$
q_{jt} = Q_j ( P_t, \boldsymbol X_t) \quad \text{for} \quad j = 1,&amp;hellip;,J.
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Profit function&lt;/p&gt;
&lt;p&gt;$$
\pi_{jt} = Q_j (P_t, \boldsymbol X_t) \Big[p_{jt} − mc_j (w_{jt}, \omega_{jt}, \gamma) \Big]
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;FOC wrt to $p_{jt}$:&lt;/p&gt;
&lt;p&gt;$$
p_{jt} = mc_{jt} - Q_j (P_t, \boldsymbol X_t) \left(\frac{\partial Q_j}{\partial p_{jt}}\right)^{-1}
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Inverse elasticity pricing (i.e., monopoly pricing) against the
“residual demand curve” $Q_j (P_t, \boldsymbol X_t)$:&lt;/p&gt;
&lt;p&gt;$$
\frac{p_{jt} - mc_{jt}}{p_{jt}} = - \frac{Q_j (P_t, \boldsymbol X_t)}{p_{jt}} \left(\frac{\partial Q_j}{\partial p_{jt}}\right)^{-1}
$$&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;what-do-we-get&#34;&gt;What do we get?&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Holding all else fixed, markups/prices depend on the own-price
elasticities of residual demand. Equilibrium depends, further, on
how a change in price of one good affects the quantities sold of
others, i.e., on cross-price demand elasticities&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;If we known demand, we can also perform a &lt;strong&gt;small miracle&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Re-arrange FOC&lt;/p&gt;
&lt;p&gt;$$
mc_{jt} = p_{jt} + Q_j (P_t, \boldsymbol X_t)\left(\frac{\partial Q_j}{\partial p_{jt}}\right)^{-1}
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Supply model + estimated demand $\to$ estimates of marginal
costs!&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;If we know demand and marginal costs, we can”predict” a lot of
stuff - i.e., give the quantitative implications of the model for
counterfactual worlds&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;issues&#34;&gt;Issues&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Typically we need to know levels/elasticities of demand at
particular points; i.e., effects of one price change holding all
else fixed&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The main challenge: unobserved demand shifters (“demand shocks”) at
the level of the good×market (e.g., unobserved product char or
market-specific variation in mean tastes for products)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;demand shocks are among the things that must be held fixed to
measure the relevant demand elasticities etc.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;explicit modeling of these demand shocks central in the applied IO
literature following S. Berry, Levinsohn, and Pakes
(&lt;a href=&#34;#ref-berry1995automobile&#34;&gt;1995&lt;/a&gt;) (often ignored outside this
literature).&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;key-challenge&#34;&gt;Key Challenge&lt;/h3&gt;
&lt;p&gt;The &lt;strong&gt;demand&lt;/strong&gt; of product $j$&lt;/p&gt;
&lt;p&gt;$$
q_{jt} (\boldsymbol X_{t}, P_t, \Xi_t)
$$&lt;/p&gt;
&lt;p&gt;depends on:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;$P_t$: $J$-vector of &lt;em&gt;all&lt;/em&gt; goods’ prices in market $t$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$\boldsymbol X_t$: $J \times k$ matrix of &lt;em&gt;all&lt;/em&gt; non-price
observables in market $t$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$\Xi_t$: J-vector of demand shocks for &lt;em&gt;all&lt;/em&gt; goods in market $t$&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Key insight&lt;/strong&gt;: we have an endogeneity problem even if prices were
exogenous!&lt;/p&gt;
&lt;h3 id=&#34;price-endogeneity-adds-to-the-challenge&#34;&gt;Price Endogeneity Adds to the Challenge&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;all $J$ endogenous prices are on RHS of demand for each good&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;equilibrium pricing implies that each price depends on all demand
shocks and all cost shocks&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;prices endogenous&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;control function generally is not a valid solution&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;clear that we need sources of exogenous price variation, but&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;what exactly is required?&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;how do we proceed?&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;blp-model&#34;&gt;BLP: Model&lt;/h2&gt;
&lt;h3 id=&#34;goals-of-blp&#34;&gt;Goals of BLP&lt;/h3&gt;
&lt;p&gt;Model of S. Berry, Levinsohn, and Pakes
(&lt;a href=&#34;#ref-berry1995automobile&#34;&gt;1995&lt;/a&gt;)&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;parsimonious specification to generate the distribution
$F_U (\cdot| P, \Xi)$ of random utilities&lt;/li&gt;
&lt;li&gt;sufficiently rich heterogeneity in preferences to permit
reasonable/flexible substitution patterns&lt;/li&gt;
&lt;li&gt;be explicit about unobservables, including the nature of endogeneity
“problem(s)”&lt;/li&gt;
&lt;li&gt;use the model to reveal solutions to the identification problem,
including appropriate instruments&lt;/li&gt;
&lt;li&gt;computationally feasible (in early 1990s!) algorithm for consistent
estimation of the model and standard errors.&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;utility-specification&#34;&gt;Utility Specification&lt;/h3&gt;
&lt;p&gt;Utility of consumer $i$ for product $j$&lt;/p&gt;
&lt;p&gt;$$
u_{ijt} = \boldsymbol x_{jt} \boldsymbol \beta_{it} - \alpha p_{jt} + \xi_{jt} + \epsilon_{ijt}
$$&lt;/p&gt;
&lt;p&gt;Where&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;$\boldsymbol x_{jt}$: $K$-vector of characteristics of product $j$
in market $t$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$\boldsymbol \beta_{it} = (\beta_{it}^{1}, &amp;hellip;, \beta_{it}^K)$:
vector of tastes for characteristics $1,…,K$ in market $t$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;$\beta_{it}^k = \beta_0^k + \sigma_k \zeta_{it}^k$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;$\beta_0^k$: fixed taste for characteristic $k$ (the usual
$\beta$)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$\zeta_{it}^k$: random taste, i.i.d. across consumers and
markets $t$&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$\alpha$: price elasticity&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$p_{jt}$ price of product $j$ in market $t$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$\xi_{jt}$: unobservable product shock at the level of products $j$
$\times$ market $t$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$\epsilon_{ijt}$: idiosyncratic (and latent) taste&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;exogenous-and-endogenous-product-characteristics&#34;&gt;Exogenous and Endogenous Product Characteristics&lt;/h3&gt;
&lt;p&gt;Utility of consumer $i$ for product $j$&lt;/p&gt;
&lt;p&gt;$$
u_{ijt} = \boldsymbol x_{jt} \beta_{it} - \alpha p_{jt} + \xi_{jt} + \epsilon_{ijt}
$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;exogenous characteristics: $\boldsymbol x_{jt} \perp \xi_{jt}$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;endogenous characteristics: $p_{jt}$ (usually a scalar, price)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;typically each $p_{jt}$ will depend on whole vector
$\Xi_t = (\xi_{1t} , . . . , \xi_{Jt} )$
&lt;ul&gt;
&lt;li&gt;and on own costs $mc_{jt}$ and others’ costs $mc_{-jt}$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;we need to distinguish true effects of prices on demand from the&lt;/li&gt;
&lt;li&gt;effects of $\Xi_t$ ; this will require &lt;strong&gt;instruments&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;of course the equation above is not an estimating equation
($u_{ijt}$ not observed)&lt;/li&gt;
&lt;li&gt;because prices and quantities are all endogenous - indeed
determined - simultaneously, you may suspect (correctly) that
instruments for prices alone may not suffice.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;utility-specification-rewritten&#34;&gt;Utility Specification, Rewritten&lt;/h3&gt;
&lt;p&gt;Rewrite&lt;/p&gt;
&lt;p&gt;$$
\begin{align}
u_{ijt} &amp;amp;= \boldsymbol x_{jt} \boldsymbol \beta_{it} - \alpha p_{jt} + \xi_{jt} + \epsilon_{ijt} = \newline
&amp;amp;= \delta_{jt} + \nu_{ijt}
\end{align}
$$&lt;/p&gt;
&lt;p&gt;where&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\delta_{jt} = \boldsymbol x_{jt} \boldsymbol \beta_0 - \alpha p_{jt} + \xi_{jt}$
&lt;ul&gt;
&lt;li&gt;mean utility of good $j$ in market $t$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;$\nu_{ijt} = \sum_{k} x_{jt}^{k} \sigma^{k} \zeta_{i t}^{k} + \epsilon_{ijt} \equiv \boldsymbol x_{jt} \tilde{\boldsymbol \beta}&lt;em&gt;{it} + \epsilon&lt;/em&gt;{ijt}$
&lt;ul&gt;
&lt;li&gt;We split $\beta_{it}$ into its &lt;strong&gt;random&lt;/strong&gt; ($\tilde{\beta}_{it}$)
and &lt;strong&gt;non-random&lt;/strong&gt; ($\beta_0$) part&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;from-consumer-utility-to-demand&#34;&gt;From Consumer Utility to Demand&lt;/h3&gt;
&lt;p&gt;With a &lt;strong&gt;continuum of consumers in each market&lt;/strong&gt;: market shares = choice
probabilities&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;P.S. continuum not needed, enough that sampling error on choice
probs negligible compared to that of moments based on variation
across products/markets&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$
s_{jt} (\Delta_t, \boldsymbol X_t, \boldsymbol \sigma) = \Pr (y_{it} = j) = \int_{\mathcal A_j (\Delta_t)} \text d F_{\nu} \Big(\nu_{i0t}, \nu_{i1t}, &amp;hellip; , \nu_{iJt} \ \Big| \ \boldsymbol X_t, \boldsymbol \sigma \Big)
$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;where $$
\mathcal A_j(\Delta_t) = \Big\lbrace (\nu_{i0t}, \nu_{i1t}, &amp;hellip; , \nu_{iJt} ) \in \mathbb{R}^{J+1}: \delta_{jt} + \nu_{ijt} \geq \delta_{kt} + \nu_{ikt} \ , \ \forall k \Big\rbrace
$$ &lt;strong&gt;In words&lt;/strong&gt;: market share of firm $j$ is the frequency of
consumers buying good $j$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Demand&lt;/strong&gt; is just shares $s_{jt}$ per market size $M_t$ $$
q_{jt} = M_t \times s_j (\Delta_t, \boldsymbol X_t, \boldsymbol \sigma)
$$&lt;/p&gt;
&lt;h3 id=&#34;why-random-coefficients&#34;&gt;Why Random Coefficients?&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Without&lt;/strong&gt; random coefficients $$
\begin{aligned}
u_{ijt} &amp;amp;= \underbrace{\boldsymbol x_{jt} \boldsymbol \beta_0 - \alpha p_{jt} + \xi_{jt}} + \epsilon_{ijt} \newline
&amp;amp;= \hspace{3.4em} \delta_{jt} \hspace{3.4em} + \epsilon_{ijt}
\end{aligned}
$$ If $\epsilon_{ijt}$ are iid and independent of
$(\boldsymbol X_t, P_t)$, e.g. as in the multinomial logit or probit
models,&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;products differ only in mean utilities $\delta_{jt}$&lt;/li&gt;
&lt;li&gt;$\to$ market shares depend only on the mean utilities&lt;/li&gt;
&lt;li&gt;$\to$ price elasticities (own and cross) depend only on mean
utilities too&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Implication&lt;/strong&gt;: two products with the same market shares have the same
cross elasticities w.r.t. &lt;strong&gt;all&lt;/strong&gt; other products&lt;/p&gt;
&lt;h3 id=&#34;does-this-matter&#34;&gt;Does this matter?&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Yes&lt;/strong&gt;!&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Mercedes class-A&lt;/em&gt; and &lt;em&gt;Fiat Panda&lt;/em&gt; might both have low market
shares&lt;/li&gt;
&lt;li&gt;But realistically should have very different cross-price
elasticities w.r.t. &lt;em&gt;BMW series-2&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;What is the issue?&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Models (like MNL) that have only &lt;strong&gt;iid additive taste shocks&lt;/strong&gt;
impose very &lt;strong&gt;restrictive relationships&lt;/strong&gt; between the &lt;strong&gt;levels&lt;/strong&gt; of
market shares and the matrix of own and cross-price &lt;strong&gt;derivatives&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Impact on &lt;strong&gt;counterfactuals&lt;/strong&gt;!&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Restrictions only coming from model assumptions (analytical
convenience)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Models always imporse restrictions&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;necessary for estimation&lt;/li&gt;
&lt;li&gt;but must allow flexibility in the relevant dimensions&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;how-do-random-coefficients-help&#34;&gt;How do random coefficients help?&lt;/h3&gt;
&lt;p&gt;In &lt;strong&gt;reality&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;goods differ in multiple dimensions&lt;/li&gt;
&lt;li&gt;consumers have (heterogeneous) preferences over these differences&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;How do &lt;strong&gt;random coefficients&lt;/strong&gt; capture it?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;large $\beta_i^k$ $\leftrightarrow$ strong taste for characteristic
$k$
&lt;ul&gt;
&lt;li&gt;e.g., maximum speed for sport car&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Consumer $i$’s first choice likely to have high value of $x^k$&lt;/li&gt;
&lt;li&gt;$i$’s second choice too!
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Mark&lt;/strong&gt;: cross elasticities are always about 1st vs. 2nd
choices&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Incorporating this allows more sensible substitution patterns&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;competition is mostly “local”&lt;/li&gt;
&lt;li&gt;i.e., between firms offering products appealing to the same
consumers.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;which-random-coefficients&#34;&gt;Which random coefficients?&lt;/h3&gt;
&lt;p&gt;Which characteristics have random coefficients?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;dummies for subsets of products?
&lt;ul&gt;
&lt;li&gt;S. T. Berry (&lt;a href=&#34;#ref-berry1994estimating&#34;&gt;1994&lt;/a&gt;): covers the
nested logit as a special case&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;certain horizontal or vertical characteristics?
&lt;ul&gt;
&lt;li&gt;parts of $(\boldsymbol X_t, P_t)$?&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;In practice&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Choice depends on the application and data set, including
instruments&lt;/li&gt;
&lt;li&gt;Too many RC’s (w.r.t quantity of data available) $\to$ imprecise
estimates of $\boldsymbol \sigma$&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;blp-estimation&#34;&gt;BLP: Estimation&lt;/h2&gt;
&lt;h3 id=&#34;setting-1&#34;&gt;Setting&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Observables&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\boldsymbol X_t$: product characteristics&lt;/li&gt;
&lt;li&gt;$P_t$: prices&lt;/li&gt;
&lt;li&gt;$S_t$: observed market shares&lt;/li&gt;
&lt;li&gt;$\boldsymbol W_t$: observable cost shifters&lt;/li&gt;
&lt;li&gt;$\boldsymbol Z_t$: excluded instruments&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Sketch of procedure&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;start with demand model alone&lt;/li&gt;
&lt;li&gt;suppose $ F_{&lt;code&gt;\nu&lt;/code&gt;{=tex}} (&lt;code&gt;\cdot  &lt;/code&gt;{=tex} |
 &lt;code&gt;\boldsymbol &lt;/code&gt;{=tex}X, &lt;code&gt;\boldsymbol &lt;/code&gt;{=tex}&lt;code&gt;\sigma &lt;/code&gt;{=tex})$ is
known (i.e., $\sigma$ known)&lt;/li&gt;
&lt;li&gt;for each market $t$, find mean utilities $\Delta_t \in \mathbb R$
such that
$s_{jt} (\Delta_t, \boldsymbol X_t, \boldsymbol \sigma) = s^{obs}_{jt} \ \forall j$
&lt;ul&gt;
&lt;li&gt;i.e.,“invert” model at observed market shares to find mean
utilities $\boldsymbol \delta$&lt;/li&gt;
&lt;li&gt;where $s^{obs}_{jt}$ are the observed market shares&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;using IV ,e.g. $\mathbb E [\boldsymbol z_{jt} | \xi_{jt} ] = 0$,
estimate the equation&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;issues-1&#34;&gt;Issues&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;What instruments?&lt;/li&gt;
&lt;li&gt;Will the “inversion” step actually work?&lt;/li&gt;
&lt;li&gt;What about $\boldsymbol \sigma$??&lt;/li&gt;
&lt;li&gt;Formal estimator?&lt;/li&gt;
&lt;li&gt;Computational algorithm(s)?&lt;/li&gt;
&lt;li&gt;Supply side
&lt;ul&gt;
&lt;li&gt;additional restrictions (moment conditions)
&lt;ul&gt;
&lt;li&gt;help estimation of demand&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;additional parameters: marginal cost function
&lt;ul&gt;
&lt;li&gt;why? may care directly&lt;/li&gt;
&lt;li&gt;and needed for counterfactuals that change equilibrium
quantities unless $mc$ is constant&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;instruments&#34;&gt;Instruments&lt;/h3&gt;
&lt;p&gt;We need intruments for all endogenous variables—&lt;strong&gt;prices and
quantities&lt;/strong&gt;—&lt;strong&gt;independently&lt;/strong&gt;.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Excluded cost shifters $\boldsymbol W_t$ (classic)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Usually: wages, material costs, shipping cost to market $t$,
taxes/tariffs, demand shifters from other markets&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Or proxies for them&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Usually: price of same good in another mkt (“&lt;em&gt;Hausman
instruments&lt;/em&gt;”)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Markup shifters:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Usually: characteristics of “nearby” markets (“&lt;em&gt;Waldfogel
instruments&lt;/em&gt;”)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Logic: income/age/education in San Francisco might affect prices
in Oakland but might be independent fo Oakland preferences&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Product characteristics of other firms in the same market
$\boldsymbol X_{-jt}$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;“&lt;em&gt;BLP instruments&lt;/em&gt;”&lt;/li&gt;
&lt;li&gt;affect quantities directly; affect prices (markups) via
equilibrium only&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;inversion&#34;&gt;Inversion&lt;/h3&gt;
&lt;p&gt;How do we get from market shares to prices??&lt;/p&gt;
&lt;p&gt;Given x,σ and any positive shares sh, define the following &lt;strong&gt;mapping&lt;/strong&gt;
$\Phi : \mathbb R^j \to \mathbb R^j$ $$
\Phi (\Delta_t) = \Delta_t + \log\Big( \hat S^{obs}_t \Big) - \log \Big( S_t (\Delta_t, \boldsymbol X_t, \boldsymbol \sigma) \Big)
$$ S. T. Berry (&lt;a href=&#34;#ref-berry1994estimating&#34;&gt;1994&lt;/a&gt;): for any nonzero
shares sh, Φ is a &lt;strong&gt;contraction&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;under mild conditions on the linear random coefficients random
utility model&lt;/li&gt;
&lt;li&gt;extreme value and normal random coeff not necessary&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;What does it imply?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;It has a unique fixed point: we can compute
$\delta_{jt} = \delta (S_t, \boldsymbol X_t, \boldsymbol \sigma)$&lt;/li&gt;
&lt;li&gt;We can compute the fixed point iterating the contraction from any
initial guess $\Delta_{0t}$&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;what-about-sigma&#34;&gt;What about $\sigma$?&lt;/h3&gt;
&lt;p&gt;What we we got?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;inversion result: for any market shares and any
$\boldsymbol \sigma$, we can find a vector of mean utilities
$\Delta_t$ that rationalizes the data with the BLP model&lt;/li&gt;
&lt;li&gt;a non-identification result? there is no information about
$\boldsymbol \sigma$ from market shares?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;What are we forgetting?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Cross-market variation!&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;We can get the mean utilities
$\delta_{jt} = \boldsymbol x_{jt} \boldsymbol \beta_0 - \alpha p_{jt} + \xi_{jt}$&lt;/li&gt;
&lt;li&gt;As in OLS, use $\boldsymbol z_{jt} \perp \xi_{jt}$ to get
identification of
$(\alpha, \boldsymbol \beta_0, \boldsymbol \sigma)$&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;identification-of-sigma&#34;&gt;Identification of $\sigma$&lt;/h3&gt;
&lt;p&gt;We are trying to estimate
$(\alpha, \boldsymbol \beta_0, \boldsymbol \sigma)$ from $$
\mathbb E \Big[ \xi_{jt} (\alpha, \boldsymbol \beta_0, \boldsymbol \sigma) \cdot \boldsymbol z_{jt} \Big] = \mathbb E \Big[ \big( \delta_{jt}(\boldsymbol \sigma) - \boldsymbol x_{jt} \boldsymbol \beta_0 + \alpha p_{jt} \big) \cdot \boldsymbol z_{jt} \Big]
$$ What kind of &lt;strong&gt;intruments&lt;/strong&gt; $\boldsymbol z_{jt}$ do we need?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\boldsymbol x_{jt}$ (for $\boldsymbol \beta_0$)&lt;/li&gt;
&lt;li&gt;intruments for $p_{jt}$ (for $\alpha$)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;but also&lt;/strong&gt; something for $\boldsymbol \sigma$!&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;blp-estimation-1&#34;&gt;BLP Estimation&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Steps&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Take guess of parameters
$(\alpha, \boldsymbol \beta_0, \boldsymbol \sigma)$&lt;/li&gt;
&lt;li&gt;From observed market shared $S^{obs}&lt;em&gt;{t}$ and $\boldsymbol \sigma$
get mean utilities $\delta&lt;/em&gt;{jt} (\boldsymbol \sigma)$&lt;/li&gt;
&lt;li&gt;Use also $(\alpha, \boldsymbol \beta_0)$ to get
$\xi_{jt} (\alpha, \boldsymbol \beta_0, \boldsymbol \sigma)$&lt;/li&gt;
&lt;li&gt;Compute empirical moments
$\frac{1}{JT} \xi_{jt} (\alpha, \boldsymbol \beta_0, \boldsymbol \sigma) \cdot \boldsymbol z_{jt}$&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The GMM estimator is
$(\hat \alpha, \boldsymbol{\hat{\beta}_0}, \boldsymbol{\hat{\sigma}})$
that get the empirical moments as close to $0$ as possible.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Issues&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Computing $S_t (\Delta_t, \boldsymbol X_t, \boldsymbol \sigma)$
involves a &lt;strong&gt;high dimensional integral&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;Use simulation to approximate distribution of random tastes
$\zeta_{it}^k$&lt;/li&gt;
&lt;li&gt;P.S. recall that we have decomposed random coefficients
$\beta_{it}^k$ as
$\beta_{it}^k = \beta_0^k + \sigma_k \zeta_{it}^k$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;$\xi_{jt} (\alpha, \boldsymbol \beta_0, \boldsymbol \sigma)$ has
&lt;strong&gt;no closed form solution&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;Compute it via contraction&lt;/li&gt;
&lt;li&gt;MPEC?&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;computation&#34;&gt;Computation&lt;/h2&gt;
&lt;h3 id=&#34;nested-fixed-point-algorithm&#34;&gt;Nested fixed point algorithm&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Sketch of the algorithm&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Draw a vector of consumer tastes&lt;/li&gt;
&lt;li&gt;Until you have found a minimum for
$\mathbb E \Big[ \xi_{jt} (\alpha, \boldsymbol \beta_0, \boldsymbol \sigma) \cdot \boldsymbol z_{jt} \Big]$
do
&lt;ul&gt;
&lt;li&gt;Pick a vector of parameter values
$(\alpha, \boldsymbol \beta_0, \boldsymbol \sigma)$&lt;/li&gt;
&lt;li&gt;Initialize mean utilities $\delta_{jt}^0$&lt;/li&gt;
&lt;li&gt;Until
$\big|\big| \Delta_{t}^{n+1} - \Delta_{t}^{n} \big|\big| &amp;lt; tolerance$
do
&lt;ul&gt;
&lt;li&gt;Compute implied shares:
$s_{jt} (\Delta_{t}^{n}, \boldsymbol X_t, \boldsymbol \sigma) = \int \frac{\exp \left[ \boldsymbol x_{j t} \boldsymbol{\tilde{\beta}}&lt;em&gt;{it}+\delta&lt;/em&gt;{j t}\right]}{1+\sum_{j^{\prime}} \exp \left[\boldsymbol x_{j^{\prime} t} \boldsymbol{\tilde{\beta}}&lt;em&gt;{it}+\delta&lt;/em&gt;{j&#39; t}\right]} f\left( \boldsymbol{\tilde{\beta}}&lt;em&gt;{it} \mid \theta\right) d \tilde{\beta}&lt;/em&gt;{i t}$&lt;/li&gt;
&lt;li&gt;Update mean utilities:
$\Delta_{t}^{n+1} = \Delta_{t}^{n} + \log\Big( \hat S^{obs}&lt;em&gt;t \Big) - \log \Big( S_t (\Delta&lt;/em&gt;{t}^{n}, \boldsymbol X_t, \boldsymbol \sigma) \Big)$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Compute
$\xi_{jt} = \delta_{jt} - \boldsymbol x_{jt} \boldsymbol \beta_0 + \alpha p_{jt}$&lt;/li&gt;
&lt;li&gt;Compute
$\mathbb E \Big[ \xi_{jt} (\alpha, \boldsymbol \beta_0, \boldsymbol \sigma) \cdot \boldsymbol z_{jt} \Big]$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;notes&#34;&gt;Notes&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Important to draw shocks outside the optimization routine!&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;appendix&#34;&gt;Appendix&lt;/h2&gt;
&lt;h3 id=&#34;references-references&#34;&gt;References [references]&lt;/h3&gt;
&lt;div id=&#34;refs&#34; class=&#34;references csl-bib-body hanging-indent&#34;
markdown=&#34;1&#34;&gt;
&lt;div id=&#34;ref-berry1994estimating&#34; class=&#34;csl-entry&#34; markdown=&#34;1&#34;&gt;
&lt;p&gt;Berry, Steven T. 1994. “Estimating Discrete-Choice Models of Product
Differentiation.” &lt;em&gt;The RAND Journal of Economics&lt;/em&gt;, 242–62.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-berry1995automobile&#34; class=&#34;csl-entry&#34; markdown=&#34;1&#34;&gt;
&lt;p&gt;Berry, Steven, James Levinsohn, and Ariel Pakes. 1995. “Automobile
Prices in Market Equilibrium.” &lt;em&gt;Econometrica: Journal of the Econometric
Society&lt;/em&gt;, 841–90.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Probability Theory</title>
      <link>https://matteocourthoud.github.io/course/metrics/02_probability/</link>
      <pubDate>Fri, 29 Oct 2021 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/course/metrics/02_probability/</guid>
      <description>&lt;h2 id=&#34;probability&#34;&gt;Probability&lt;/h2&gt;
&lt;h3 id=&#34;probability-space&#34;&gt;Probability Space&lt;/h3&gt;
&lt;p&gt;A &lt;strong&gt;probability space&lt;/strong&gt; is a triple $(\Omega, \mathcal A, P)$ where&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\Omega$ is the sample space.&lt;/li&gt;
&lt;li&gt;$\mathcal A$ is the $\sigma$-algebra on $\Omega$.&lt;/li&gt;
&lt;li&gt;$P$ is a probability measure.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The &lt;strong&gt;sample space&lt;/strong&gt; $\Omega$ is the space of all possible events.&lt;/p&gt;
&lt;p&gt;What is a $\sigma$-algebra and a probability measure?&lt;/p&gt;
&lt;h3 id=&#34;sigma-algebra&#34;&gt;Sigma Algebra&lt;/h3&gt;
&lt;p&gt;A nonempty set (of subsets of $\Omega$) $\mathcal A \in 2^\Omega$ is a
&lt;strong&gt;sigma algebra&lt;/strong&gt; ($\sigma$-algebra) of $\Omega$ if the following
conditions hold:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;$\Omega \in \mathcal A$&lt;/li&gt;
&lt;li&gt;If $A \in \mathcal A$, then $(\Omega - A) \in \mathcal A$&lt;/li&gt;
&lt;li&gt;If $A_1, A_2, &amp;hellip; \in \mathcal A$, then
$\bigcup _ {i=1}^{\infty} A_i \in \mathcal A$&lt;/li&gt;
&lt;/ol&gt;
&lt;blockquote&gt;
&lt;p&gt;The smallest $\sigma$-algebra is $\lbrace \emptyset, \Omega \rbrace$
and the largest one is $2^\Omega$ (in cardinality terms).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Suppose $\Omega = \mathbb R$. Let
$\mathcal{C} = \lbrace (a, b],-\infty \leq a&amp;lt;b&amp;lt;\infty \rbrace$. Then the
&lt;strong&gt;Borel&lt;/strong&gt; $\sigma$&lt;strong&gt;- algebra&lt;/strong&gt; on $\mathbb R$ is defined by $$
\mathcal B (\mathbb R) = \sigma (\mathcal C)
$$&lt;/p&gt;
&lt;h3 id=&#34;probability-measure&#34;&gt;Probability Measure&lt;/h3&gt;
&lt;p&gt;A &lt;strong&gt;probability measure&lt;/strong&gt; $P: \mathcal A \to [0,1]$ is a set function
with domain $\mathcal A$ and codomain $[0,1]$ such that&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;$P(A) \geq 0 \ \forall A \in \mathcal A$&lt;/li&gt;
&lt;li&gt;$P$ is $\sigma$-additive: is $A_n \in \mathcal A$ are pairwise
disjoint events ($A_j \cap A_k = \emptyset$ for $j \neq k$), then $$
P\left(\bigcup _ {n=1}^{\infty} A_{n} \right)=\sum _ {n=1}^{\infty} P\left(A_{n}\right)
$$&lt;/li&gt;
&lt;li&gt;$P(\Omega) = 1$&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;properties&#34;&gt;Properties&lt;/h3&gt;
&lt;p&gt;Some properties of probability measures&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$P\left(A^{c}\right)=1-P(A)$&lt;/li&gt;
&lt;li&gt;$P(\emptyset)=0$&lt;/li&gt;
&lt;li&gt;For $A, B \in \mathcal{A}$, $P(A \cup B)=P(A)+P(B)-P(A \cap B)$&lt;/li&gt;
&lt;li&gt;For $A, B \in \mathcal{A}$, if $A \subset B$ then $P(A) \leq P(B)$&lt;/li&gt;
&lt;li&gt;For $A_n \in \mathcal{A}$,
$P \left(\cup _ {n=1}^\infty A_{n} \right) \leq \sum _ {n=1}^\infty P(A_n)$&lt;/li&gt;
&lt;li&gt;For $A_n \in \mathcal{A}$, if $A_n \uparrow A$ then
$\lim _ {n \to \infty} P(A_n) = P(A)$&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;conditional-probability&#34;&gt;Conditional Probability&lt;/h3&gt;
&lt;p&gt;Let $A, B \in \mathcal A$ and $P(B) &amp;gt; 0$, the &lt;strong&gt;conditional
probability&lt;/strong&gt; of $A$ given $B$ is $$
P(A | B)=\frac{P(A \cap B)}{P(B)}
$$&lt;/p&gt;
&lt;p&gt;Two events $A$ and $B$ are &lt;strong&gt;independent&lt;/strong&gt; if $P(A \cap B)=P(A) P(B)$.&lt;/p&gt;
&lt;h3 id=&#34;law-of-total-probability&#34;&gt;Law of Total Probability&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt; (Law of Total Probability)&lt;/p&gt;
&lt;p&gt;Let $(E_n) _ {n \geq 1}$ be a finite or countable partition of $\Omega$.
Then, if $A \in \mathcal A$, $$
P(A) = \sum_n P(A | E_n ) P(E_n)
$$&lt;/p&gt;
&lt;h3 id=&#34;bayes-theorem&#34;&gt;Bayes Theorem&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt; (Bayes Theorem)&lt;/p&gt;
&lt;p&gt;Let $(E_n) _ {n \geq 1}$ be a finite or countable partition of $\Omega$,
and suppose $P(A) &amp;gt; 0$. Then, $$
P(E_n | A) = \frac{P(A | E_n) P(E_n)}{\sum_m P(A | E_m) P(E_m)}
$$&lt;/p&gt;
&lt;p&gt;For a single event $E \in \Omega$, $$
P(E|A) = \frac{P(A|E) P(E)}{P(A)}
$$&lt;/p&gt;
&lt;h2 id=&#34;random-variables&#34;&gt;Random Variables&lt;/h2&gt;
&lt;h3 id=&#34;definition&#34;&gt;Definition&lt;/h3&gt;
&lt;p&gt;A &lt;strong&gt;random variable&lt;/strong&gt; $X$ on a probability space
$(\Omega,\mathcal A, P)$ is a (measurable) mapping
$X : \Omega \to \mathbb{R}$ such that $$
\forall B \in \mathcal{B}(\mathbb{R}), \quad X^{-1}(B) \in \mathcal{A}
$$&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The measurability condition states that the inverse image is a
measurable set of $\Omega$ i.e. $X^{-1}(B) \in \mathcal A$. This is
essential since probabilities are defined only on $\mathcal A$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;In words, a random variable it’s a mapping from events to real numbers
such that each interval on the real line can be mapped back into an
element of the sigma algebra (it can be the empty set).&lt;/p&gt;
&lt;h3 id=&#34;distribution-function&#34;&gt;Distribution Function&lt;/h3&gt;
&lt;p&gt;Let $X$ be a real valued random variable. The &lt;strong&gt;distribution function&lt;/strong&gt;
(also called cumulative distribution function) of $X$, commonly denoted
$F_X(x)$ is defined by $$
F_X(x) = \Pr(X \leq x)
$$&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Properties&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$F$ is monotone non-decreasing&lt;/li&gt;
&lt;li&gt;$F$ is right continuous&lt;/li&gt;
&lt;li&gt;$\lim _ {x \to - \infty} F(x)=0$ and
$\lim _ {x \to + \infty} F(x)=1$&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;p&gt;The random variables $(X_1, .. , X_n)$ are independent if and only if $$
F _ {(X_1, &amp;hellip; , X_n)} (x) = \prod _ {i=1}^n F_{X_i} (x_i) \quad \forall x \in \mathbb R^n
$$&lt;/p&gt;
&lt;h3 id=&#34;density-function&#34;&gt;Density Function&lt;/h3&gt;
&lt;p&gt;Let $X$ be a real valued random variable. $X$ has a &lt;strong&gt;probability
density function&lt;/strong&gt; if there exists $f_X(x)$ such that for all measurable
$A \subset \mathbb{R}$, $$
P(X \in A) = \int_A f_X(x) \mathrm{d} x
$$&lt;/p&gt;
&lt;h2 id=&#34;moments&#34;&gt;Moments&lt;/h2&gt;
&lt;h3 id=&#34;expected-value&#34;&gt;Expected Value&lt;/h3&gt;
&lt;p&gt;The &lt;strong&gt;expected value&lt;/strong&gt; of a random variable, when it exists, is given by
$$
\mathbb{E}[ X ] = \int_ \Omega X(\omega) \mathrm{d} P
$$ When $X$ has a density, then $$
\mathbb{E} [ X ] = \int_ \mathbb{R} x f_X (x) \mathrm{d} x = \int _ \mathbb{R} x \mathrm{d} F_X (x)
$$&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;empirical expectation&lt;/strong&gt; (or &lt;strong&gt;sample average&lt;/strong&gt;) is given by $$
\mathbb{E}_n [x_i] = \frac{1}{n} \sum _ {i=1}^N x_i
$$&lt;/p&gt;
&lt;h3 id=&#34;variance-and-covariance&#34;&gt;Variance and Covariance&lt;/h3&gt;
&lt;p&gt;The &lt;strong&gt;covariance&lt;/strong&gt; of two random variables $X$, $Y$ defined on $\Omega$
is $$
Cov(X, Y ) = \mathbb{E}[ (X - \mathbb{E}[ X ]) (Y - \mathbb{E}[ Y ]) ]  = \mathbb{E}[XY ] - \mathbb{E}[ X ]E[ Y ]
$$ In vector notation,
$Cov(X, Y) = \mathbb{E}[XY&#39;] - \mathbb{E}[ X ]\mathbb{E}[Y&#39;]$.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;variance&lt;/strong&gt; of a random variable $X$, when it exists, is given by
$$
Var(X) = \mathbb{E}[ (X - \mathbb{E}[ X ])^2 ] = \mathbb{E}[X^2] - \mathbb{E}[ X ]^2
$$ In vector notation,
$Var(X) = \mathbb{E}[XX&#39;] - \mathbb{E}[ X ]\mathbb{E}[X&#39;]$.&lt;/p&gt;
&lt;h3 id=&#34;properties-1&#34;&gt;Properties&lt;/h3&gt;
&lt;p&gt;Let $X, Y, Z, T \in \mathcal{L}^{2}$ and $a, b, c, d \in \mathbb{R}$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$Cov(X, X) = Var(X)$&lt;/li&gt;
&lt;li&gt;$Cov(X, Y) = Cov(Y, X)$&lt;/li&gt;
&lt;li&gt;$Cov(aX + b, Y) = a \ Cov(X,Y)$&lt;/li&gt;
&lt;li&gt;$Cov(X+Z, Y) = Cov(X,Y) + Cov(Z,Y)$&lt;/li&gt;
&lt;li&gt;$Cov(aX + bZ, cY + dT) = ac * Cov(X,Y) + ad * Cov(X,T) + bc * Cov(Z,Y) + bd * Cov(Z,T)$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Let $X, Y \in \mathcal L^1$ be independent. Then,
$\mathbb E[XY] = \mathbb E[ X ] \mathbb E[ Y ]$.&lt;/p&gt;
&lt;p&gt;If $X$ and $Y$ are independent, then $Cov(X,Y) = 0$.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Note that the converse does not hold:
$Cov(X,Y) = 0 \not \to X \perp Y$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;sample-variance&#34;&gt;Sample Variance&lt;/h3&gt;
&lt;p&gt;The &lt;strong&gt;sample variance&lt;/strong&gt; is given by $$
Var_n (x_i) = \frac{1}{n} \sum _ {i=1}^N (x_i - \bar{x})^2
$$ where
$\bar{x_i} = \mathbb{E}_n [x_i] = \frac{1}{n} \sum _ {i=1}^N x_i$.&lt;/p&gt;
&lt;h3 id=&#34;finite-sample-bias-theorem&#34;&gt;Finite Sample Bias Theorem&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt;: The expected sample variance
$\mathbb{E} [\sigma^2_n] = \mathbb{E} \left[ \frac{1}{n} \sum _ {i=1}^N \left(y_i - \mathbb{E}_n[ Y ] \right)^2 \right]$
gives an estimate of the population variance that is biased by a factor
of $\frac{1}{n}$ and is therefore referred to as &lt;strong&gt;biased sample
variance&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;: $$
\begin{aligned}
&amp;amp;\mathbb{E}[\sigma^2_n] =  \mathbb{E} \left[ \frac{1}{n} \sum _ {i=1}^n \left( y_i - \mathbb{E}_n [ Y ] \right)^2 \right] =
\newline
&amp;amp;= \mathbb{E} \left[ \frac{1}{n} \sum _ {i=1}^n \left( y_i - \frac{1}{n} \sum _ {i=1}^n y_i \right )^2 \right] =
\newline
&amp;amp;= \frac{1}{n} \sum _ {i=1}^n \mathbb{E} \left[ y_i^2 - \frac{2}{n} y_i \sum _ {j=1}^n y_j + \frac{1}{n^2} \sum _ {j=1}^n y_j \sum _ {k=1}^{n}y_k  \right] =
\newline
&amp;amp;= \frac{1}{n} \sum _ {i=1}^n  \left[ \frac{n-2}{n} \mathbb{E}[y_i^2]  - \frac{2}{n} \sum _ {j\neq i} \mathbb{E}[y_i y_j] + \frac{1}{n^2} \sum _ {j=1}^n \sum _ {k\neq j} \mathbb{E}[y_j y_k] + \frac{1}{n^2} \sum _ {j=1}^n \mathbb{E}[y_j^2] \right] =
\newline
&amp;amp;= \frac{1}{n} \sum _ {i=1}^n  \left[ \frac{n-2}{n}(\mu^2 + \sigma^2) - \frac{2}{n} (n-1) \mu^2 + \frac{1}{n^2} n(n-1)\mu^2 + \frac{1}{n^2} n (\mu^2 + \sigma^2)]\right] =
\newline
&amp;amp;= \frac{n-1}{n} \sigma^2
\end{aligned}
$$ $$\tag*{$\blacksquare$}$$&lt;/p&gt;
&lt;h3 id=&#34;inequalities&#34;&gt;Inequalities&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Triangle Inequality&lt;/strong&gt;: if $\mathbb{E} [ X ] &amp;lt; \infty$, then $$
|\mathbb{E} [ X ] | \leq \mathbb{E} [|X|]
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Markov’s Inequality&lt;/strong&gt;: if $\mathbb{E}[ X ] &amp;lt; \infty$, then $$
\Pr(|X| &amp;gt; t) \leq \frac{1}{t} \mathbb{E}[|X|]
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Chebyshev’s Inequality&lt;/strong&gt;: if $\mathbb{E}[X^2] &amp;lt; \infty$, then $$
\Pr(|X- \mu|&amp;gt; t \sigma) \leq \frac{1}{t^2}\Leftrightarrow \Pr(|X- \mu|&amp;gt; t ) \leq \frac{\sigma^2}{t^2}
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Cauchy-Schwarz’s Inequality&lt;/strong&gt;: $$
\mathbb{E} [|XY|] \leq \sqrt{\mathbb{E}[X^2] \mathbb{E}[Y^2]}
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Minkowski Inequality&lt;/strong&gt;: $$
\left( \sum _ {k=1}^n | x_k + y_k |^p \right) ^ {\frac{1}{p}} \leq \left( \sum _ {k=1}^n | x_k |^p \right) ^ {\frac{1}{p}} + \left( \sum _ {k=1}^n | y_k | ^p \right) ^ { \frac{1}{p} }
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Jensen’s Inequality&lt;/strong&gt;: if $g( \cdot)$ is concave (e.g. logarithmic
function), then $$
\mathbb{E}[g(x)] \leq g(\mathbb{E}[ X ])
$$ Similarly, if $g(\cdot)$ is convex (e.g. exponential function),
then $$
\mathbb{E}[g(x)] \geq g(\mathbb{E}[ X ])
$$&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;law-of-iterated-expectations&#34;&gt;Law of Iterated Expectations&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt; (Law of Iterated Expectations) $$
\mathbb{E}(Y) = \mathbb{E}_X [\mathbb{E}(Y|X)]
$$ &amp;gt; This states that the expectation of the conditional expectation is
the unconditional expectation. &amp;gt; &amp;gt; In other words the average of the
conditional averages is the unconditional average.&lt;/p&gt;
&lt;h3 id=&#34;law-of-total-variance&#34;&gt;Law of Total Variance&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt; (Law of Total Variance) $$
Var(Y) = Var_X (\mathbb{E}[Y |X]) + \mathbb{E}_X [Var(Y|X)]
$$&lt;/p&gt;
&lt;p&gt;Since variances are always non-negative, the law of total variance
implies $$
Var(Y) \geq Var_X (\mathbb{E}[Y |X])
$$&lt;/p&gt;
&lt;h2 id=&#34;distributions&#34;&gt;Distributions&lt;/h2&gt;
&lt;h3 id=&#34;normal-distribution&#34;&gt;Normal Distribution&lt;/h3&gt;
&lt;p&gt;We say that a random variable $Z$ has the &lt;strong&gt;standard normal
distribution&lt;/strong&gt;, or &lt;strong&gt;Gaussian&lt;/strong&gt;, written $Z \sim N(0,1)$, if it has the
density $$
\phi(x)=\frac{1}{\sqrt{2 \pi}} \exp \left(-\frac{x^{2}}{2}\right), \quad-\infty&amp;lt;x&amp;lt;\infty
$$ If $Z \sim N(0, 1)$ and $X = \mu + \sigma Z$ for $\mu \in \mathbb R$
and $\sigma \geq 0$, then $X$ has a &lt;strong&gt;univariate normal distribution&lt;/strong&gt;,
written $X \sim N(\mu, \sigma^2)$. By change-of-variables &lt;em&gt;X&lt;/em&gt; has the
density $$
f(x)=\frac{1}{\sqrt{2 \pi \sigma^{2}}} \exp \left(-\frac{(x-\mu)^{2}}{2 \sigma^{2}}\right), \quad-\infty&amp;lt;x&amp;lt;\infty
$$&lt;/p&gt;
&lt;h3 id=&#34;multinomial-normal-distribution&#34;&gt;Multinomial Normal Distribution&lt;/h3&gt;
&lt;p&gt;We say that the &lt;em&gt;k&lt;/em&gt; -vector &lt;em&gt;Z&lt;/em&gt; has a &lt;strong&gt;multivariate standard normal
distribution&lt;/strong&gt;, written $Z \sim N(0, I_k)$ if it has the joint density
$$
f(x)=\frac{1}{(2 \pi)^{k / 2}} \exp \left(-\frac{x^{\prime} x}{2}\right), \quad x \in \mathbb{R}^{k}
$$ If $Z \sim N(0, I_k)$ and $X = \mu + B Z$, then the &lt;em&gt;k&lt;/em&gt;-vector $X$
has a &lt;strong&gt;multivariate normal distribution&lt;/strong&gt;, written
$X \sim N(\mu, \Sigma)$ where $\Sigma = BB&#39; \geq 0$. If $\sigma &amp;gt; 0$,
then by change-of-variables $X$ has the joint density function $$
f(x)=\frac{1}{(2 \pi)^{k / 2} \operatorname{det}(\Sigma)^{1 / 2}} \exp \left(-\frac{(x-\mu)^{\prime} \Sigma^{-1}(x-\mu)}{2}\right), \quad x \in \mathbb{R}^{k}
$$&lt;/p&gt;
&lt;h3 id=&#34;properties-2&#34;&gt;Properties&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;The expectation and covariance matrix of $X \sim N(\mu, \Sigma)$ are
$\mathbb E &lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; = \mu$ and $Var&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; =\Sigma$.&lt;/li&gt;
&lt;li&gt;If $(X,Y)$ are multivariate normal, $X$ and $Y$ are uncorrelated if
and only if they are independent.&lt;/li&gt;
&lt;li&gt;If $X \sim N(\mu, \Sigma)$ and $Y = a + bB$, then
$X \sim N(a + B\mu, B \Sigma B&#39;)$.&lt;/li&gt;
&lt;li&gt;If $X \sim N(0, I_k)$, then $X&amp;rsquo;X \sim \chi^2_k$, chi-square with $k$
degrees of freedom.&lt;/li&gt;
&lt;li&gt;If $X \sim N(0, \Sigma)$ with $\Sigma&amp;gt;0$, then
$X&#39; \Sigma X \sim \chi_k$ where $k = \dim (X)$.&lt;/li&gt;
&lt;li&gt;If $Z \sim N(0,1)$ and $Q \sim \chi^2_k$ are independent then
$\frac{Z}{\sqrt{Q/k}} \sim t_k$, student t with &lt;em&gt;k&lt;/em&gt; degrees of
freedom.&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;normal-distribution-relatives&#34;&gt;Normal Distribution Relatives&lt;/h3&gt;
&lt;p&gt;These distributions are relatives of the normal distribution&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;$\chi^2_q \sim \sum _ {i=1}^q Z_i^2$ where $Z_i \sim N(0,1)$&lt;/li&gt;
&lt;li&gt;$t_n \sim \frac{Z}{\sqrt{\chi^2 _ n}/n }$&lt;/li&gt;
&lt;li&gt;$F(n_1 , n_2) \sim \frac{\chi^2 _ {n_1} / n_1}{\chi^2 _ {n_2}/n_2}$&lt;/li&gt;
&lt;/ol&gt;
&lt;blockquote&gt;
&lt;p&gt;The $t$ distribution is approximately standard normal but has heavier
tails. The approximation is good for $n \geq 30$:
$t_{n\geq 30} \sim N(0,1)$&lt;/p&gt;
&lt;/blockquote&gt;
</description>
    </item>
    
    <item>
      <title>Non-Parametric Regression</title>
      <link>https://matteocourthoud.github.io/course/ml-econ/03_nonparametric/</link>
      <pubDate>Mon, 28 Feb 2022 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/course/ml-econ/03_nonparametric/</guid>
      <description>&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Setup
%matplotlib inline
from utils.lecture03 import *
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;dataset&#34;&gt;Dataset&lt;/h3&gt;
&lt;p&gt;For this session, we are mostly going to work with the wage dataset.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df = pd.read_csv(&#39;data/Wage.csv&#39;, index_col=0)
df.head(3)
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;year&lt;/th&gt;
      &lt;th&gt;age&lt;/th&gt;
      &lt;th&gt;maritl&lt;/th&gt;
      &lt;th&gt;race&lt;/th&gt;
      &lt;th&gt;education&lt;/th&gt;
      &lt;th&gt;region&lt;/th&gt;
      &lt;th&gt;jobclass&lt;/th&gt;
      &lt;th&gt;health&lt;/th&gt;
      &lt;th&gt;health_ins&lt;/th&gt;
      &lt;th&gt;logwage&lt;/th&gt;
      &lt;th&gt;wage&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;231655&lt;/th&gt;
      &lt;td&gt;2006&lt;/td&gt;
      &lt;td&gt;18&lt;/td&gt;
      &lt;td&gt;1. Never Married&lt;/td&gt;
      &lt;td&gt;1. White&lt;/td&gt;
      &lt;td&gt;1. &amp;lt; HS Grad&lt;/td&gt;
      &lt;td&gt;2. Middle Atlantic&lt;/td&gt;
      &lt;td&gt;1. Industrial&lt;/td&gt;
      &lt;td&gt;1. &amp;lt;=Good&lt;/td&gt;
      &lt;td&gt;2. No&lt;/td&gt;
      &lt;td&gt;4.318063&lt;/td&gt;
      &lt;td&gt;75.043154&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;86582&lt;/th&gt;
      &lt;td&gt;2004&lt;/td&gt;
      &lt;td&gt;24&lt;/td&gt;
      &lt;td&gt;1. Never Married&lt;/td&gt;
      &lt;td&gt;1. White&lt;/td&gt;
      &lt;td&gt;4. College Grad&lt;/td&gt;
      &lt;td&gt;2. Middle Atlantic&lt;/td&gt;
      &lt;td&gt;2. Information&lt;/td&gt;
      &lt;td&gt;2. &amp;gt;=Very Good&lt;/td&gt;
      &lt;td&gt;2. No&lt;/td&gt;
      &lt;td&gt;4.255273&lt;/td&gt;
      &lt;td&gt;70.476020&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;161300&lt;/th&gt;
      &lt;td&gt;2003&lt;/td&gt;
      &lt;td&gt;45&lt;/td&gt;
      &lt;td&gt;2. Married&lt;/td&gt;
      &lt;td&gt;1. White&lt;/td&gt;
      &lt;td&gt;3. Some College&lt;/td&gt;
      &lt;td&gt;2. Middle Atlantic&lt;/td&gt;
      &lt;td&gt;1. Industrial&lt;/td&gt;
      &lt;td&gt;1. &amp;lt;=Good&lt;/td&gt;
      &lt;td&gt;1. Yes&lt;/td&gt;
      &lt;td&gt;4.875061&lt;/td&gt;
      &lt;td&gt;130.982177&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;This dataset contains information on wages and individual characteristics.&lt;/p&gt;
&lt;p&gt;Our main objective is going to be to explain wages using the observables contained in the dataset.&lt;/p&gt;
&lt;h2 id=&#34;polynomial-regression-and-step-functions&#34;&gt;Polynomial Regression and Step Functions&lt;/h2&gt;
&lt;p&gt;As we have seen in the first lecture, the most common way to introduce linearities is to replace the standard linear model&lt;/p&gt;
&lt;p&gt;$$
y_i = \beta_0 + \beta_1 x_i + \varepsilon_i
$$&lt;/p&gt;
&lt;p&gt;with a polynomial function&lt;/p&gt;
&lt;p&gt;$$
y_i = \beta_0 + \beta_1 x_i + \beta_2 x_i^2 + \beta_2 x_i^3 + &amp;hellip; + \varepsilon_i
$$&lt;/p&gt;
&lt;h3 id=&#34;explore-the-data&#34;&gt;Explore the Data&lt;/h3&gt;
&lt;p&gt;Suppose we want to investigate the relationship between &lt;code&gt;wage&lt;/code&gt; and &lt;code&gt;age&lt;/code&gt;. Let&amp;rsquo;s first plot the two variables.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Scatterplot of the data
df.plot.scatter(&#39;age&#39;,&#39;wage&#39;,color=&#39;w&#39;, edgecolors=&#39;k&#39;, alpha=0.3);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/03_nonparametric_10_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;polynomials-of-different-degrees&#34;&gt;Polynomials of different degrees&lt;/h3&gt;
&lt;p&gt;The relationship is highly complex and non-linear. Let&amp;rsquo;s expand our linear regression polynomials of different degrees: 1 to 5.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;X_poly1 = PolynomialFeatures(1).fit_transform(df.age.values.reshape(-1,1))
X_poly2 = PolynomialFeatures(2).fit_transform(df.age.values.reshape(-1,1))
X_poly3 = PolynomialFeatures(3).fit_transform(df.age.values.reshape(-1,1))
X_poly4 = PolynomialFeatures(4).fit_transform(df.age.values.reshape(-1,1))
X_poly5 = PolynomialFeatures(5).fit_transform(df.age.values.reshape(-1,1))
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;variables&#34;&gt;Variables&lt;/h3&gt;
&lt;p&gt;Our dependent varaible is going to be a dummy for income above 250.000 USD.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Get X and y
X = df.age
y = df.wage
y01 = (df.wage &amp;gt; 250).map({False:0, True:1}).values
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;polynomia-regression&#34;&gt;Polynomia Regression&lt;/h3&gt;
&lt;p&gt;If we run a linear regression on a 4-degree polinomial expansion of &lt;code&gt;age&lt;/code&gt;, this is what it looks like`:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Fit ols on 4th degree polynomial
fit = sm.OLS(y, X_poly4).fit()
fit.summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
    &lt;td&gt;&lt;/td&gt;       &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;const&lt;/th&gt; &lt;td&gt; -184.1542&lt;/td&gt; &lt;td&gt;   60.040&lt;/td&gt; &lt;td&gt;   -3.067&lt;/td&gt; &lt;td&gt; 0.002&lt;/td&gt; &lt;td&gt; -301.879&lt;/td&gt; &lt;td&gt;  -66.430&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;x1&lt;/th&gt;    &lt;td&gt;   21.2455&lt;/td&gt; &lt;td&gt;    5.887&lt;/td&gt; &lt;td&gt;    3.609&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    9.703&lt;/td&gt; &lt;td&gt;   32.788&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;x2&lt;/th&gt;    &lt;td&gt;   -0.5639&lt;/td&gt; &lt;td&gt;    0.206&lt;/td&gt; &lt;td&gt;   -2.736&lt;/td&gt; &lt;td&gt; 0.006&lt;/td&gt; &lt;td&gt;   -0.968&lt;/td&gt; &lt;td&gt;   -0.160&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;x3&lt;/th&gt;    &lt;td&gt;    0.0068&lt;/td&gt; &lt;td&gt;    0.003&lt;/td&gt; &lt;td&gt;    2.221&lt;/td&gt; &lt;td&gt; 0.026&lt;/td&gt; &lt;td&gt;    0.001&lt;/td&gt; &lt;td&gt;    0.013&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;x4&lt;/th&gt;    &lt;td&gt;-3.204e-05&lt;/td&gt; &lt;td&gt; 1.64e-05&lt;/td&gt; &lt;td&gt;   -1.952&lt;/td&gt; &lt;td&gt; 0.051&lt;/td&gt; &lt;td&gt;-6.42e-05&lt;/td&gt; &lt;td&gt; 1.45e-07&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;h3 id=&#34;measures-of-fit&#34;&gt;Measures of Fit&lt;/h3&gt;
&lt;p&gt;In this case, the single coefficients are not of particular interest. We are mostly interested in the best capturing the relationship between &lt;code&gt;age&lt;/code&gt; and &lt;code&gt;wage&lt;/code&gt;. How can we pick among thedifferent polynomials?&lt;/p&gt;
&lt;p&gt;We compare different polynomial degrees. For each regression, we are going to look at a series of metrics:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;absolute residuals&lt;/li&gt;
&lt;li&gt;sum of squared residuals&lt;/li&gt;
&lt;li&gt;the difference in SSR w.r (SSR).t the 0-degree case&lt;/li&gt;
&lt;li&gt;F statistic&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Run regressions
fit_1 = sm.OLS(y, X_poly1).fit()
fit_2 = sm.OLS(y, X_poly2).fit()
fit_3 = sm.OLS(y, X_poly3).fit()
fit_4 = sm.OLS(y, X_poly4).fit()
fit_5 = sm.OLS(y, X_poly5).fit()

# Compare fit
sm.stats.anova_lm(fit_1, fit_2, fit_3, fit_4, fit_5, typ=1)
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;df_resid&lt;/th&gt;
      &lt;th&gt;ssr&lt;/th&gt;
      &lt;th&gt;df_diff&lt;/th&gt;
      &lt;th&gt;ss_diff&lt;/th&gt;
      &lt;th&gt;F&lt;/th&gt;
      &lt;th&gt;Pr(&amp;gt;F)&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;2998.0&lt;/td&gt;
      &lt;td&gt;5.022216e+06&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;2997.0&lt;/td&gt;
      &lt;td&gt;4.793430e+06&lt;/td&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;228786.010128&lt;/td&gt;
      &lt;td&gt;143.593107&lt;/td&gt;
      &lt;td&gt;2.363850e-32&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;2996.0&lt;/td&gt;
      &lt;td&gt;4.777674e+06&lt;/td&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;15755.693664&lt;/td&gt;
      &lt;td&gt;9.888756&lt;/td&gt;
      &lt;td&gt;1.679202e-03&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;2995.0&lt;/td&gt;
      &lt;td&gt;4.771604e+06&lt;/td&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;6070.152124&lt;/td&gt;
      &lt;td&gt;3.809813&lt;/td&gt;
      &lt;td&gt;5.104620e-02&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;2994.0&lt;/td&gt;
      &lt;td&gt;4.770322e+06&lt;/td&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;1282.563017&lt;/td&gt;
      &lt;td&gt;0.804976&lt;/td&gt;
      &lt;td&gt;3.696820e-01&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;The polynomial degree 4 seems best.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Set polynomial X to 4th degree
X_poly = X_poly4
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;binary-dependent-variable&#34;&gt;Binary Dependent Variable&lt;/h3&gt;
&lt;p&gt;Since we have a binary dependent variable, it would be best to account for it in our regression framework. One way to do so, is to run a logistic regression.&lt;/p&gt;
&lt;p&gt;How to interpret a Logistic Regression?&lt;/p&gt;
&lt;p&gt;$$
y = \mathbb I \ \Big( \beta_0 + \beta_1 x_i + \beta_2 x_i^2 + \beta_2 x_i^3 + &amp;hellip; + \varepsilon_i \Big)
$$&lt;/p&gt;
&lt;p&gt;where $\mathbb I(\cdot)$ is an indicator function and now $\varepsilon_i$ is the error term.&lt;/p&gt;
&lt;h3 id=&#34;binomial-link-functions&#34;&gt;Binomial Link Functions&lt;/h3&gt;
&lt;p&gt;Depending on the assumed distribution of the error term, we get different results. I list below the error types supported by the &lt;code&gt;Binomial&lt;/code&gt; family.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# List link functions for the Binomial family
sm.families.family.Binomial.links
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[statsmodels.genmod.families.links.logit,
 statsmodels.genmod.families.links.probit,
 statsmodels.genmod.families.links.cauchy,
 statsmodels.genmod.families.links.log,
 statsmodels.genmod.families.links.cloglog,
 statsmodels.genmod.families.links.identity]
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;logit-link-function&#34;&gt;Logit Link Function&lt;/h3&gt;
&lt;p&gt;We are going to pick the &lt;code&gt;logit&lt;/code&gt; link, i.e. we are going to assume that the error term is Type 1 Extreme Value (or Gumbel) distributed. It instead we take the usual standard normal distribution assumption for $\varepsilon_i$, we get &lt;code&gt;probit&lt;/code&gt; regression.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Pick the logit link for the Binomial family
logit_link = sm.families.Binomial(sm.genmod.families.links.logit())
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Given the error distribution, we can write the probability that $y=1$ as&lt;/p&gt;
&lt;p&gt;$$
\Pr(y=1) = \frac{e^{ \beta_0 + \beta_1 x_i + \beta_2 x_i^2 + \beta_2 x_i^3 + &amp;hellip; + \varepsilon_i }}{1 + e^{ \beta_0 + \beta_1 x_i + \beta_2 x_i^2 + \beta_2 x_i^3 + &amp;hellip; + \varepsilon_i } }
$$&lt;/p&gt;
&lt;h3 id=&#34;logistic-regression&#34;&gt;Logistic Regression&lt;/h3&gt;
&lt;p&gt;We now estimate the regression and plot the estimated relationship between &lt;code&gt;age&lt;/code&gt; and &lt;code&gt;wage&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Run logistic regression
logit_poly = sm.GLM(y01, X_poly, family=logit_link).fit()
logit_poly.summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
    &lt;td&gt;&lt;/td&gt;       &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;z&lt;/th&gt;      &lt;th&gt;P&gt;|z|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;const&lt;/th&gt; &lt;td&gt; -109.5530&lt;/td&gt; &lt;td&gt;   47.655&lt;/td&gt; &lt;td&gt;   -2.299&lt;/td&gt; &lt;td&gt; 0.022&lt;/td&gt; &lt;td&gt; -202.956&lt;/td&gt; &lt;td&gt;  -16.150&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;x1&lt;/th&gt;    &lt;td&gt;    8.9950&lt;/td&gt; &lt;td&gt;    4.187&lt;/td&gt; &lt;td&gt;    2.148&lt;/td&gt; &lt;td&gt; 0.032&lt;/td&gt; &lt;td&gt;    0.789&lt;/td&gt; &lt;td&gt;   17.201&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;x2&lt;/th&gt;    &lt;td&gt;   -0.2816&lt;/td&gt; &lt;td&gt;    0.135&lt;/td&gt; &lt;td&gt;   -2.081&lt;/td&gt; &lt;td&gt; 0.037&lt;/td&gt; &lt;td&gt;   -0.547&lt;/td&gt; &lt;td&gt;   -0.016&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;x3&lt;/th&gt;    &lt;td&gt;    0.0039&lt;/td&gt; &lt;td&gt;    0.002&lt;/td&gt; &lt;td&gt;    2.022&lt;/td&gt; &lt;td&gt; 0.043&lt;/td&gt; &lt;td&gt;    0.000&lt;/td&gt; &lt;td&gt;    0.008&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;x4&lt;/th&gt;    &lt;td&gt;-1.949e-05&lt;/td&gt; &lt;td&gt; 9.91e-06&lt;/td&gt; &lt;td&gt;   -1.966&lt;/td&gt; &lt;td&gt; 0.049&lt;/td&gt; &lt;td&gt;-3.89e-05&lt;/td&gt; &lt;td&gt;-6.41e-08&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;h3 id=&#34;linear-model-comparison&#34;&gt;Linear Model Comparison&lt;/h3&gt;
&lt;p&gt;What is the difference with the linear model?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Run OLS regression with binary outcome
ols_poly = sm.OLS(y01, X_poly).fit()
ols_poly.summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
    &lt;td&gt;&lt;/td&gt;       &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;const&lt;/th&gt; &lt;td&gt;   -0.1126&lt;/td&gt; &lt;td&gt;    0.240&lt;/td&gt; &lt;td&gt;   -0.468&lt;/td&gt; &lt;td&gt; 0.640&lt;/td&gt; &lt;td&gt;   -0.584&lt;/td&gt; &lt;td&gt;    0.359&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;x1&lt;/th&gt;    &lt;td&gt;    0.0086&lt;/td&gt; &lt;td&gt;    0.024&lt;/td&gt; &lt;td&gt;    0.363&lt;/td&gt; &lt;td&gt; 0.717&lt;/td&gt; &lt;td&gt;   -0.038&lt;/td&gt; &lt;td&gt;    0.055&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;x2&lt;/th&gt;    &lt;td&gt;   -0.0002&lt;/td&gt; &lt;td&gt;    0.001&lt;/td&gt; &lt;td&gt;   -0.270&lt;/td&gt; &lt;td&gt; 0.787&lt;/td&gt; &lt;td&gt;   -0.002&lt;/td&gt; &lt;td&gt;    0.001&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;x3&lt;/th&gt;    &lt;td&gt; 3.194e-06&lt;/td&gt; &lt;td&gt; 1.23e-05&lt;/td&gt; &lt;td&gt;    0.260&lt;/td&gt; &lt;td&gt; 0.795&lt;/td&gt; &lt;td&gt;-2.09e-05&lt;/td&gt; &lt;td&gt; 2.73e-05&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;x4&lt;/th&gt;    &lt;td&gt;-1.939e-08&lt;/td&gt; &lt;td&gt; 6.57e-08&lt;/td&gt; &lt;td&gt;   -0.295&lt;/td&gt; &lt;td&gt; 0.768&lt;/td&gt; &lt;td&gt;-1.48e-07&lt;/td&gt; &lt;td&gt; 1.09e-07&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;The magnitude of the coefficients is different, but the signs are the same.&lt;/p&gt;
&lt;h3 id=&#34;plot-data-and-predictions&#34;&gt;Plot data and predictions&lt;/h3&gt;
&lt;p&gt;Let&amp;rsquo;s plot the estimated curves against the data distribution.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Generate predictions
x_grid = np.arange(df.age.min(), df.age.max()).reshape(-1,1)
X_poly_test = PolynomialFeatures(4).fit_transform(x_grid)
y_hat1 = sm.OLS(y, X_poly).fit().predict(X_poly_test)
y01_hat1 = logit_poly.predict(X_poly_test)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plot_predictions(X, y, x_grid, y01, y_hat1, y01_hat1, &#39;Figure 7.1: Degree-4 Polynomial&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/03_nonparametric_44_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Which is remindful of
&lt;img src=&#34;../figures/nonlinearities.jpg&#34; alt=&#34;Le Petit Prince - Elephant figure&#34; title=&#34;Nonlinearities&#34;&gt;&lt;/p&gt;
&lt;p&gt;Using polynomial functions of the features as predictors in a linear model imposes a global structure on the non-linear function of &lt;code&gt;age&lt;/code&gt;. We can instead use step functions in order to avoid imposing such a global structure.&lt;/p&gt;
&lt;p&gt;For example, we could break the range of &lt;code&gt;age&lt;/code&gt; into bins, and fit a different constant in each bin.&lt;/p&gt;
&lt;h2 id=&#34;step-functions&#34;&gt;Step Functions&lt;/h2&gt;
&lt;p&gt;Building a step function means first picking $K$ cutpoints $c_1 , c_2 , . . . , c_K$ in the range of &lt;code&gt;age&lt;/code&gt;,
and then construct $K + 1$ new variables&lt;/p&gt;
&lt;p&gt;$$
C_0(&lt;code&gt;age&lt;/code&gt;) = \mathbb I ( &lt;code&gt;age&lt;/code&gt; &amp;lt; c_1) \
C_1(&lt;code&gt;age&lt;/code&gt;) = \mathbb I ( c_1 &amp;lt; &lt;code&gt;age&lt;/code&gt; &amp;lt; c_2) \
C_2(&lt;code&gt;age&lt;/code&gt;) = \mathbb I ( c_2 &amp;lt; &lt;code&gt;age&lt;/code&gt; &amp;lt; c_3) \
&amp;hellip; \
C_{K-1}(&lt;code&gt;age&lt;/code&gt;) = \mathbb I ( c_{K-1} &amp;lt; &lt;code&gt;age&lt;/code&gt; &amp;lt; c_K) \
C_K(&lt;code&gt;age&lt;/code&gt;) = \mathbb I ( c_K &amp;lt; &lt;code&gt;age&lt;/code&gt;) \
$$&lt;/p&gt;
&lt;p&gt;where $\mathbb I(\cdot)$ is the indicator function.&lt;/p&gt;
&lt;h3 id=&#34;binning&#34;&gt;Binning&lt;/h3&gt;
&lt;p&gt;First, we generate the cuts.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Generate cuts for the variable age
df_cut, bins = pd.cut(df.age, 4, retbins=True, right=True)
df_cut.value_counts(sort=False)
type(df_cut)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;pandas.core.series.Series
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s generate a DataFrame out of this series.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Generate bins for &amp;quot;age&amp;quot; from the cuts
df_steps = pd.concat([df.age, df_cut, df.wage], keys=[&#39;age&#39;,&#39;age_cuts&#39;,&#39;wage&#39;], axis=1)
df_steps.head(5)
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;age&lt;/th&gt;
      &lt;th&gt;age_cuts&lt;/th&gt;
      &lt;th&gt;wage&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;231655&lt;/th&gt;
      &lt;td&gt;18&lt;/td&gt;
      &lt;td&gt;(17.938, 33.5]&lt;/td&gt;
      &lt;td&gt;75.043154&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;86582&lt;/th&gt;
      &lt;td&gt;24&lt;/td&gt;
      &lt;td&gt;(17.938, 33.5]&lt;/td&gt;
      &lt;td&gt;70.476020&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;161300&lt;/th&gt;
      &lt;td&gt;45&lt;/td&gt;
      &lt;td&gt;(33.5, 49.0]&lt;/td&gt;
      &lt;td&gt;130.982177&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;155159&lt;/th&gt;
      &lt;td&gt;43&lt;/td&gt;
      &lt;td&gt;(33.5, 49.0]&lt;/td&gt;
      &lt;td&gt;154.685293&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;11443&lt;/th&gt;
      &lt;td&gt;50&lt;/td&gt;
      &lt;td&gt;(49.0, 64.5]&lt;/td&gt;
      &lt;td&gt;75.043154&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;h3 id=&#34;dummy-variables&#34;&gt;Dummy Variables&lt;/h3&gt;
&lt;p&gt;Now we can generate different dummy variables out of each bin.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Create dummy variables for the age groups
df_steps_dummies = pd.get_dummies(df_steps[&#39;age_cuts&#39;])

# Statsmodels requires explicit adding of a constant (intercept)
df_steps_dummies = sm.add_constant(df_steps_dummies)
df_steps_dummies.head(5)
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;const&lt;/th&gt;
      &lt;th&gt;(17.938, 33.5]&lt;/th&gt;
      &lt;th&gt;(33.5, 49.0]&lt;/th&gt;
      &lt;th&gt;(49.0, 64.5]&lt;/th&gt;
      &lt;th&gt;(64.5, 80.0]&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;231655&lt;/th&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;86582&lt;/th&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;161300&lt;/th&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;155159&lt;/th&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;11443&lt;/th&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;h3 id=&#34;stepwise-regression&#34;&gt;Stepwise Regression&lt;/h3&gt;
&lt;p&gt;We are now ready to run our regression&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Generate our new X variable
X_step = df_steps_dummies.drop(df_steps_dummies.columns[1], axis=1)

# OLS Regression on step functions
ols_step = sm.OLS(y, X_step).fit()
ols_step.summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
        &lt;td&gt;&lt;/td&gt;          &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;const&lt;/th&gt;        &lt;td&gt;   94.1584&lt;/td&gt; &lt;td&gt;    1.476&lt;/td&gt; &lt;td&gt;   63.790&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;   91.264&lt;/td&gt; &lt;td&gt;   97.053&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;(33.5, 49.0]&lt;/th&gt; &lt;td&gt;   24.0535&lt;/td&gt; &lt;td&gt;    1.829&lt;/td&gt; &lt;td&gt;   13.148&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;   20.466&lt;/td&gt; &lt;td&gt;   27.641&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;(49.0, 64.5]&lt;/th&gt; &lt;td&gt;   23.6646&lt;/td&gt; &lt;td&gt;    2.068&lt;/td&gt; &lt;td&gt;   11.443&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;   19.610&lt;/td&gt; &lt;td&gt;   27.719&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;(64.5, 80.0]&lt;/th&gt; &lt;td&gt;    7.6406&lt;/td&gt; &lt;td&gt;    4.987&lt;/td&gt; &lt;td&gt;    1.532&lt;/td&gt; &lt;td&gt; 0.126&lt;/td&gt; &lt;td&gt;   -2.139&lt;/td&gt; &lt;td&gt;   17.420&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;From the regression outcome we can see that most bin coefficients are significant, except for the last one.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Put the test data in the same bins as the training data.
bin_mapping = np.digitize(x_grid.ravel(), bins)
bin_mapping
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2,
       2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
       3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4])
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Get dummies, drop first dummy category, add constant
X_step_test = sm.add_constant(pd.get_dummies(bin_mapping).drop(1, axis=1))
X_step_test.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;const&lt;/th&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;th&gt;4&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Step prediction
y_hat2 = ols_step.predict(X_step_test)
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;logistic-step-regression&#34;&gt;Logistic Step Regression&lt;/h3&gt;
&lt;p&gt;We are going again to run a logistic regression, given that our outcome is binary.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Logistic regression on step functions
logit_step = sm.GLM(y01, X_step, family=logit_link).fit()
y01_hat2 = logit_step.predict(X_step_test)
logit_step.summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
        &lt;td&gt;&lt;/td&gt;          &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;z&lt;/th&gt;      &lt;th&gt;P&gt;|z|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;const&lt;/th&gt;        &lt;td&gt;   -5.0039&lt;/td&gt; &lt;td&gt;    0.449&lt;/td&gt; &lt;td&gt;  -11.152&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;   -5.883&lt;/td&gt; &lt;td&gt;   -4.124&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;(33.5, 49.0]&lt;/th&gt; &lt;td&gt;    1.5998&lt;/td&gt; &lt;td&gt;    0.474&lt;/td&gt; &lt;td&gt;    3.378&lt;/td&gt; &lt;td&gt; 0.001&lt;/td&gt; &lt;td&gt;    0.672&lt;/td&gt; &lt;td&gt;    2.528&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;(49.0, 64.5]&lt;/th&gt; &lt;td&gt;    1.7147&lt;/td&gt; &lt;td&gt;    0.488&lt;/td&gt; &lt;td&gt;    3.512&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    0.758&lt;/td&gt; &lt;td&gt;    2.672&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;(64.5, 80.0]&lt;/th&gt; &lt;td&gt;    0.7413&lt;/td&gt; &lt;td&gt;    1.102&lt;/td&gt; &lt;td&gt;    0.672&lt;/td&gt; &lt;td&gt; 0.501&lt;/td&gt; &lt;td&gt;   -1.420&lt;/td&gt; &lt;td&gt;    2.902&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;h3 id=&#34;plotting&#34;&gt;Plotting&lt;/h3&gt;
&lt;p&gt;How does the predicted function looks like?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plot_predictions(X, y, x_grid, y01, y_hat2, y01_hat2, &#39;Figure 7.2: Piecewise Constant&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/03_nonparametric_69_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;regression-splines&#34;&gt;Regression Splines&lt;/h2&gt;
&lt;p&gt;Spline regression, or piece-wise polynomial regression, involves fitting separate low-degree polynomials
over different regions of $X$. The idea is to have one regression specification but with different coefficients in different parts of the $X$ range. The points where the coefficients change are called knots.&lt;/p&gt;
&lt;p&gt;For example, we could have a third degree polynomial &lt;em&gt;and&lt;/em&gt; splitting the sample in two.&lt;/p&gt;
&lt;p&gt;$$
y_{i}=\left{\begin{array}{ll}
\beta_{01}+\beta_{11} x_{i}+\beta_{21} x_{i}^{2}+\beta_{31} x_{i}^{3}+\epsilon_{i} &amp;amp; \text { if } x_{i}&amp;lt;c \
\beta_{02}+\beta_{12} x_{i}+\beta_{22} x_{i}^{2}+\beta_{32} x_{i}^{3}+\epsilon_{i} &amp;amp; \text { if } x_{i} \geq c
\end{array}\right.
$$&lt;/p&gt;
&lt;p&gt;We have now two sets of coefficients, one for each subsample.&lt;/p&gt;
&lt;p&gt;Generally, using more knots leads to a more flexible piecewise polynomial. Also increasing the degree of the polynomial increases the degree of flexibility.&lt;/p&gt;
&lt;h3 id=&#34;example&#34;&gt;Example&lt;/h3&gt;
&lt;p&gt;We are now going to plot 4 different examples for the &lt;code&gt;age&lt;/code&gt; &lt;code&gt;wage&lt;/code&gt; relationship:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Discontinuous piecewise cubic&lt;/li&gt;
&lt;li&gt;Continuous piecewise cubic&lt;/li&gt;
&lt;li&gt;Quadratic (continuous)&lt;/li&gt;
&lt;li&gt;Continuous piecewise linear&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Cut dataset
df_short = df.iloc[:80,:]
X_short = df_short.age
y_short = df_short.wage
x_grid_short = np.arange(df_short.age.min(), df_short.age.max()+1).reshape(-1,1)

# 1. Discontinuous piecewise cubic
spline1 = &amp;quot;bs(x, knots=(50,50,50,50), degree=3, include_intercept=False)&amp;quot;

# 2. Continuous piecewise cubic
spline2 = &amp;quot;bs(x, knots=(50,50,50), degree=3, include_intercept=False)&amp;quot;

# 3. Quadratic (continuous)
spline3 = &amp;quot;bs(x, knots=(%s,%s), degree=2, include_intercept=False)&amp;quot; % (min(df.age), min(df.age))

# 4. Continuous piecewise linear
spline4 = &amp;quot;bs(x, knots=(%s,50), degree=1, include_intercept=False)&amp;quot; % min(df.age)
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;generate-predictions&#34;&gt;Generate Predictions&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Generate spline predictions
def fit_predict_spline(spline, X, y, x_grid):
    transformed_x = dmatrix(spline, {&amp;quot;x&amp;quot;: X}, return_type=&#39;dataframe&#39;)
    fit = sm.GLM(y, transformed_x).fit()
    y_hat = fit.predict(dmatrix(spline, {&amp;quot;x&amp;quot;: x_grid}, return_type=&#39;dataframe&#39;))
    return y_hat

y_hats = [fit_predict_spline(s, X_short, y_short, x_grid_short) for s in [spline1, spline2, spline3, spline4]]
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;plotting-1&#34;&gt;Plotting&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plot_splines(df_short, x_grid_short, y_hats)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/03_nonparametric_78_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;comment&#34;&gt;Comment&lt;/h3&gt;
&lt;p&gt;The first example makes us think on why would we want out function to be discontinuous. Unless we expect a sudden wage jump at a certain age, we would like the function to be continuous. However, if for example we split &lt;code&gt;age&lt;/code&gt; around the retirement age, we might expect a discontinuity.&lt;/p&gt;
&lt;p&gt;The second example (top right) makes us think on why would we want out function not to be differentiable. Unless we have some specific mechanism in mind, ususally there is a trade-off between making the function non-differentiable or increasing the degree of the polynomial, as the last two examples show us. We get a similar fit with a quadratic fit or a discontinuous linear fit. The main difference is that in the second case we are picking the discontinuity point by hand instead of letting the data choose how to change the slope of the curve.&lt;/p&gt;
&lt;h3 id=&#34;the-spline-basis-representation&#34;&gt;The Spline Basis Representation&lt;/h3&gt;
&lt;p&gt;How can we fit a piecewise degree-d polynomial under the constraint that it (and possibly its first d − 1 derivatives) be continuous?&lt;/p&gt;
&lt;p&gt;The most direct way to represent a cubic spline is to start off with a basis for a cubic polynomial—namely, x,x2,x3—and then add one truncated power basis function per knot. A truncated power basis function is defined as&lt;/p&gt;
&lt;p&gt;$$
h(x, c)=(x-c)_{+}^{3} = \Bigg{\begin{array}{cl}
(x-c)^{3} &amp;amp; \text { if } x&amp;gt;c \
0 &amp;amp; \text { otherwise }
\end{array}
$$&lt;/p&gt;
&lt;p&gt;One can show that adding a term of the form $\beta_4 h(x, c)$ to the model for a cubic polynomial will lead to a discontinuity in only the third derivative at $c$; the function will remain continuous, with continuous first and second derivatives, at each of the knots.&lt;/p&gt;
&lt;h3 id=&#34;cubic-splines&#34;&gt;Cubic Splines&lt;/h3&gt;
&lt;p&gt;One way to specify the spline is using nodes and degrees of freedom.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Specifying 3 knots and 3 degrees of freedom
spline5 = &amp;quot;bs(x, knots=(25,40,60), degree=3, include_intercept=False)&amp;quot;
pred5 = fit_predict_spline(spline5, X, y, x_grid)
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;no-knots&#34;&gt;No Knots&lt;/h3&gt;
&lt;p&gt;When we fit a spline, where should we place the knots?&lt;/p&gt;
&lt;p&gt;The regression spline is most flexible in regions that contain a lot of knots, because in those regions the polynomial coefficients can change rapidly. Hence, one option is to place more knots in places where we feel the function might vary most rapidly, and to place fewer knots where it seems more stable. While this option can work well, in practice it is common to place knots in a uniform fashion. One way to do this is to specify the desired degrees of freedom, and then have the software automatically place the corresponding number of knots at uniform quantiles of the data.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Specifying degree 3 and 6 degrees of freedom 
spline6 = &amp;quot;bs(x, df=6, degree=3, include_intercept=False)&amp;quot;
pred6 = fit_predict_spline(spline6, X, y, x_grid)
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;natural-splines&#34;&gt;Natural Splines&lt;/h3&gt;
&lt;p&gt;A natural spline is a regression spline with additional boundary constraints: the function is required to be linear at the boundary (in the region where X is smaller than the smallest knot, or larger than the largest knot). This addi- tional constraint means that natural splines generally produce more stable estimates at the boundaries.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Natural spline with 4 degrees of freedom
spline7 = &amp;quot;cr(x, df=4)&amp;quot;
pred7 = fit_predict_spline(spline7, X, y, x_grid)
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;comparison&#34;&gt;Comparison&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Compare predictons
preds = [pred5, pred6, pred7]
labels = [&#39;degree 3, knots 3&#39;, &#39;degree 3, degrees of freedom 3&#39;, &#39;natural, degrees of freedom 4&#39;]
compare_predictions(X, y, x_grid, preds, labels)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/03_nonparametric_93_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;comparison-to-polynomial-regression&#34;&gt;Comparison to Polynomial Regression&lt;/h3&gt;
&lt;p&gt;Regression splines often give superior results to polynomial regression. This is because unlike polynomials, which must use a high degree to produce flexible fits, splines introduce flexibility by increasing the number of knots but keeping the degree fixed.&lt;/p&gt;
&lt;p&gt;We are now fitting a polynomial of degree 15 and a spline with 15 degrees of freedom.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Polynomial of degree 15
X_poly15 = PolynomialFeatures(15).fit_transform(df.age.values.reshape(-1,1))
ols_poly_15 = sm.OLS(y, X_poly15).fit()
pred8 = ols_poly_15.predict(PolynomialFeatures(15).fit_transform(x_grid))

# Spline with 15 degrees of freedon
spline9 = &amp;quot;bs(x, df=15, degree=3, include_intercept=False)&amp;quot;
pred9 = fit_predict_spline(spline9, X, y, x_grid)
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;plotting-2&#34;&gt;Plotting&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Compare predictons
preds = [pred8, pred9]
labels = [&#39;Polynomial&#39;, &#39;Spline&#39;]
compare_predictions(X, y, x_grid, preds, labels)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/03_nonparametric_98_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;As we can see, despite the two regressions having the same degrees of freedom, the polynomial fit is much more volatile. We can compare them along some dimensions.&lt;/p&gt;
&lt;h2 id=&#34;local-regression&#34;&gt;Local Regression&lt;/h2&gt;
&lt;p&gt;So far we have looked at so-called &amp;ldquo;&lt;em&gt;global methods&lt;/em&gt;&amp;quot;: methods that try to fit a unique function specification over the whole data. The function specification can be complex, as in the case of splines, but can be expressed globally.&lt;/p&gt;
&lt;p&gt;Local regression is a different approach for fitting flexible non-linear functions, which involves computing the fit at a target point $x_0$ using only the nearby training observations.&lt;/p&gt;
&lt;h3 id=&#34;details&#34;&gt;Details&lt;/h3&gt;
&lt;p&gt;How does local regression work?&lt;/p&gt;
&lt;p&gt;Ingredients: $X$, $y$.&lt;/p&gt;
&lt;p&gt;How to you output a prediction $\hat y_i$ at a new point $x_i$?&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Take a number of points in $X$ close to $x_i$: $X_{\text{close-to-i}}$&lt;/li&gt;
&lt;li&gt;Assign a weight to each of there points&lt;/li&gt;
&lt;li&gt;Fit a weigthed least squares regression of $X_{\text{close-to-i}}$ on $y_{\text{close-to-i}}$&lt;/li&gt;
&lt;li&gt;Use the estimated coefficients $\hat \beta$ to predict $\hat y_i = \hat \beta_0 + \hat \beta_1 x_i$&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;generate-data&#34;&gt;Generate Data&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Set seed
np.random.seed(1)

# Generate data
X_sim = np.sort(np.random.uniform(0,1,100))
e = np.random.uniform(-.5,.5,100)
y_sim = -4*X_sim**2 + 3*X_sim + e

# True Generating process without noise
X_grid = np.linspace(0,1,100)
y_grid = -4*X_grid**2 + 3*X_grid
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;plotting-3&#34;&gt;Plotting&lt;/h3&gt;
&lt;p&gt;Let&amp;rsquo;s visualize the simulated data and the curve without noise.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plot_simulated_data(X_sim, y_sim, X_grid, y_grid);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/03_nonparametric_108_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;fit-ll-regression&#34;&gt;Fit LL Regression&lt;/h3&gt;
&lt;p&gt;Now we fit a local linear regression.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Settings
spec = &#39;ll&#39;
bandwidth = 0.1
kernel = &#39;gaussian&#39;

# Locally linear regression
local_reg = KernelReg(y_sim, X_sim.reshape(-1,1), 
                      var_type=&#39;c&#39;, 
                      reg_type=spec, 
                      bw=[bandwidth])
y_hat = KernelReg.fit(local_reg)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What do the parameters mean?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;var_type&lt;/code&gt;: dependent variable type (&lt;code&gt;c&lt;/code&gt; i.e. &lt;em&gt;continuous&lt;/em&gt;)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;reg_type&lt;/code&gt;: local regression specification (&lt;code&gt;ll&lt;/code&gt; i.e. &lt;em&gt;locally linear&lt;/em&gt;)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;bw&lt;/code&gt;      : bandwidth length (&lt;em&gt;0.1&lt;/em&gt;)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;ckertype&lt;/code&gt;: kernel type (&lt;em&gt;gaussian&lt;/em&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;prediction&#34;&gt;Prediction&lt;/h3&gt;
&lt;p&gt;What does the prediction looks like?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;fig, ax = plot_simulated_data(X_sim, y_sim, X_grid, y_grid);
make_figure_7_9a(fig, ax, X_sim, y_hat);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/03_nonparametric_115_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;details-1&#34;&gt;Details&lt;/h3&gt;
&lt;p&gt;How exactly was the prediction generated? It was generated pointwise. We are now going to look at the prediction at one particular point: $x_i=0.5$.&lt;/p&gt;
&lt;p&gt;We proceed as follows:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;We select the focal point: $x_i=0.5$&lt;/li&gt;
&lt;li&gt;We select observations close to $\ x_i$, i.e. $x_{\text{close to i}} = { x \in X : |x_i - x| &amp;lt; 0.1 } \ $ and $ \ y_{\text{close to i}} = { y \in Y : |x_i - x| &amp;lt; 0.1 }$&lt;/li&gt;
&lt;li&gt;We apply gaussian weights&lt;/li&gt;
&lt;li&gt;We run a weighted linear regression of $y_{\text{close to i}}$ on $x_{\text{close to i}}$&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Get local X and y
x_i = 0.5
close_to_i = (x_i-bandwidth &amp;lt; X_sim) &amp;amp; (X_sim &amp;lt; x_i+bandwidth)
X_tilde = X_sim[close_to_i]
y_tilde = y_sim[close_to_i]

# Get local estimates
local_estimate = KernelReg.fit(local_reg, data_predict=[x_i])
y_i_hat = local_estimate[0]
beta_i_hat = local_estimate[1]
alpha_i_hat = y_i_hat - beta_i_hat*x_i
print(&#39;Estimates: alpha=%1.4f, beta=%1.4f&#39; % (alpha_i_hat, beta_i_hat))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Estimates: alpha=0.7006, beta=-0.6141
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;visualization&#34;&gt;Visualization&lt;/h3&gt;
&lt;p&gt;Now we can use the locally estimated coefficients to predict the value of $\hat y_i(x_i)$ for $x_i = 0.5$.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Build local predictions
close_to_i_grid = (x_i-bandwidth &amp;lt; X_grid) &amp;amp; (X_grid &amp;lt; x_i+bandwidth)
X_grid_tilde = X_grid[close_to_i_grid].reshape(-1,1)
y_grid_tilde = alpha_i_hat + X_grid_tilde*beta_i_hat
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;fig, ax = plot_simulated_data(X_sim, y_sim, X_grid, y_grid);
make_figure_7_9a(fig, ax, X_sim, y_hat);
make_figure_7_9b(fig, ax, X_tilde, y_tilde, X_grid_tilde, y_grid_tilde, x_i, y_i_hat)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/03_nonparametric_122_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;zooming-in&#34;&gt;Zooming in&lt;/h3&gt;
&lt;p&gt;We can zoom in and look only at the &amp;ldquo;&lt;em&gt;close to i&lt;/em&gt;&amp;rdquo; sample.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sns.regplot(X_tilde, y_tilde);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/03_nonparametric_125_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Why is the line upward sloped? We forgot the gaussian weights.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Weights
w = norm.pdf((X_sim-x_i)/bandwidth)

# Estimate LWS
mod_wls = sm.WLS(y_sim, sm.add_constant(X_sim), weights=w)
results = mod_wls.fit()

print(&#39;Estimates: alpha=%1.4f, beta=%1.4f&#39; % tuple(results.params))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Estimates: alpha=0.7006, beta=-0.6141
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We indeed got the same estimates as before. Note two things:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;the badwidth defines the scale parameter of the gaussian weights&lt;/li&gt;
&lt;li&gt;our locally linear regression is acqually global&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;plotting-4&#34;&gt;Plotting&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;make_figure_7_9d(X_sim, y_sim, w, results, X_grid, x_i, y_i_hat)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/03_nonparametric_130_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Now the slope is indeed negative, as in the locally linear regression.&lt;/p&gt;
&lt;h2 id=&#34;generalized-additive-models&#34;&gt;Generalized Additive Models&lt;/h2&gt;
&lt;p&gt;Generalized additive models (GAMs) provide a general framework for extending a standard linear model by allowing non-linear functions of each of the variables, while maintaining additivity.&lt;/p&gt;
&lt;h3 id=&#34;gam-for-regression-problems&#34;&gt;GAM for Regression Problems&lt;/h3&gt;
&lt;p&gt;Imagine to extend the general regression framework to some separabily additive model of the form&lt;/p&gt;
&lt;p&gt;$$
y_i = \beta_0 + \sum_{k=1}^K \beta_k f_k(x_{ik}) + \varepsilon_i
$$&lt;/p&gt;
&lt;p&gt;It is called an additive model because we calculate a separate $f_k$ for each $X_k$, and then add together all of their contributions.&lt;/p&gt;
&lt;p&gt;Consider for example the following model&lt;/p&gt;
&lt;p&gt;$$
\text{wage} = \beta_0 + f_1(\text{year}) + f_2(\text{age}) + f_3(\text{education}) + \varepsilon
$$&lt;/p&gt;
&lt;h3 id=&#34;example-1&#34;&gt;Example&lt;/h3&gt;
&lt;p&gt;We are going to use the following functions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$f_1$: natural spline with 8 degrees of freedom&lt;/li&gt;
&lt;li&gt;$f_2$: natural spline with 10 degrees of freedom&lt;/li&gt;
&lt;li&gt;$f_3$: step function&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Set X and y
df[&#39;education_&#39;] = LabelEncoder().fit_transform(df[&amp;quot;education&amp;quot;])
X = df[[&#39;year&#39;,&#39;age&#39;,&#39;education_&#39;]].to_numpy()
y = df[[&#39;wage&#39;]].to_numpy()

## model
linear_gam = LinearGAM(s(0, n_splines=8) + s(1, n_splines=10) + f(2))
linear_gam.gridsearch(X, y);
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;100% (11 of 11) |########################| Elapsed Time: 0:00:00 Time:  0:00:00
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;plotting-5&#34;&gt;Plotting&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plot_gam(linear_gam)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/03_nonparametric_140_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;pros-and-cons&#34;&gt;Pros and Cons&lt;/h3&gt;
&lt;p&gt;Before we move on, let us summarize the &lt;strong&gt;advantages&lt;/strong&gt; of a GAM.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;GAMs allow us to fit a non-linear $f_k$ to each $X_k$, so that we can automatically model non-linear relationships that standard linear regression will miss&lt;/li&gt;
&lt;li&gt;The non-linear fits can potentially make more accurate predictions&lt;/li&gt;
&lt;li&gt;Because the model is additive, we can still examine the effect of each $X_k$ on $Y$ separately&lt;/li&gt;
&lt;li&gt;The smoothness of the function $f_k$ for the variable $X_k$ can be summarized via degrees of freedom.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The main &lt;strong&gt;limitation&lt;/strong&gt; of GAMs is that the model is restricted to be additive. With many variables, important interactions can be missed. However, as with linear regression, we can manually add interaction terms to the GAM model by including additional predictors of the form $X_j \times X_k$.&lt;/p&gt;
&lt;h3 id=&#34;gams-for-classification-problems&#34;&gt;GAMs for Classification Problems&lt;/h3&gt;
&lt;p&gt;We can use GAMs also with a binary dependent variable.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Binary dependent variable
y_binary = (y&amp;gt;250)

## Logit link function
logit_gam = LogisticGAM(s(0, n_splines=8) + s(1, n_splines=10) + f(2), fit_intercept=True)
logit_gam.gridsearch(X, y_binary);
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;100% (11 of 11) |########################| Elapsed Time: 0:00:00 Time:  0:00:00
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;plotting-6&#34;&gt;Plotting&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plot_gam(logit_gam)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/03_nonparametric_147_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;The results are qualitatively similar to the non-binary case.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Data Wrangling</title>
      <link>https://matteocourthoud.github.io/course/data-science/03_data_wrangling/</link>
      <pubDate>Sun, 27 Feb 2022 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/course/data-science/03_data_wrangling/</guid>
      <description>&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import numpy as np
import pandas as pd
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;setup&#34;&gt;Setup&lt;/h2&gt;
&lt;p&gt;For the scope of this tutorial we are going to use AirBnb Scraped data for the city of Bologna. The data is freely available at &lt;strong&gt;Inside AirBnb&lt;/strong&gt;: &lt;a href=&#34;http://insideairbnb.com/get-the-data.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://insideairbnb.com/get-the-data.html&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;A description of all variables in all datasets is avaliable &lt;a href=&#34;https://docs.google.com/spreadsheets/d/1iWCNJcSutYqpULSQHlNyGInUvHg2BoUGoNRIGa6Szc4/edit#gid=982310896&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;We are going to use 2 datasets:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;listing dataset: contains listing-level information&lt;/li&gt;
&lt;li&gt;pricing dataset: contains pricing data, over time&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Import listings data
url_listings = &amp;quot;http://data.insideairbnb.com/italy/emilia-romagna/bologna/2021-12-17/visualisations/listings.csv&amp;quot;
df_listings = pd.read_csv(url_listings)

# Import pricing data
url_prices = &amp;quot;http://data.insideairbnb.com/italy/emilia-romagna/bologna/2021-12-17/data/calendar.csv.gz&amp;quot;
df_prices = pd.read_csv(url_prices, compression=&amp;quot;gzip&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;editing-the-dataframe&#34;&gt;Editing the Dataframe&lt;/h2&gt;
&lt;p&gt;You can &lt;strong&gt;sort&lt;/strong&gt; the data using the &lt;code&gt;sort_values&lt;/code&gt; function.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Options&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;ascending&lt;/code&gt;: bool or list of bool, default True&lt;/li&gt;
&lt;li&gt;&lt;code&gt;na_position&lt;/code&gt;: {‘first’, ‘last’}, default ‘last’&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_listings.sort_values(by=[&#39;name&#39;, &#39;price&#39;], 
                        ascending=[False, True], 
                        na_position=&#39;last&#39;).head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;id&lt;/th&gt;
      &lt;th&gt;name&lt;/th&gt;
      &lt;th&gt;host_id&lt;/th&gt;
      &lt;th&gt;host_name&lt;/th&gt;
      &lt;th&gt;neighbourhood_group&lt;/th&gt;
      &lt;th&gt;neighbourhood&lt;/th&gt;
      &lt;th&gt;latitude&lt;/th&gt;
      &lt;th&gt;longitude&lt;/th&gt;
      &lt;th&gt;room_type&lt;/th&gt;
      &lt;th&gt;price&lt;/th&gt;
      &lt;th&gt;minimum_nights&lt;/th&gt;
      &lt;th&gt;number_of_reviews&lt;/th&gt;
      &lt;th&gt;last_review&lt;/th&gt;
      &lt;th&gt;reviews_per_month&lt;/th&gt;
      &lt;th&gt;calculated_host_listings_count&lt;/th&gt;
      &lt;th&gt;availability_365&lt;/th&gt;
      &lt;th&gt;number_of_reviews_ltm&lt;/th&gt;
      &lt;th&gt;license&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;2280&lt;/th&gt;
      &lt;td&gt;38601411&lt;/td&gt;
      &lt;td&gt;🏡Giardino di Annabella-relax in città-casa intera&lt;/td&gt;
      &lt;td&gt;240803020&lt;/td&gt;
      &lt;td&gt;Annabella&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;Porto - Saragozza&lt;/td&gt;
      &lt;td&gt;44.49303&lt;/td&gt;
      &lt;td&gt;11.31986&lt;/td&gt;
      &lt;td&gt;Entire home/apt&lt;/td&gt;
      &lt;td&gt;90&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;53&lt;/td&gt;
      &lt;td&gt;2021-12-13&lt;/td&gt;
      &lt;td&gt;1.96&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;76&lt;/td&gt;
      &lt;td&gt;27&lt;/td&gt;
      &lt;td&gt;392901&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2988&lt;/th&gt;
      &lt;td&gt;48177313&lt;/td&gt;
      &lt;td&gt;❤ Romantic Suite with SPA Bath ❤ 4starbologna.com&lt;/td&gt;
      &lt;td&gt;239491712&lt;/td&gt;
      &lt;td&gt;4 Star Bologna&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;Santo Stefano&lt;/td&gt;
      &lt;td&gt;44.50271&lt;/td&gt;
      &lt;td&gt;11.34998&lt;/td&gt;
      &lt;td&gt;Entire home/apt&lt;/td&gt;
      &lt;td&gt;309&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;2021-03-14&lt;/td&gt;
      &lt;td&gt;0.11&lt;/td&gt;
      &lt;td&gt;14&lt;/td&gt;
      &lt;td&gt;344&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3302&lt;/th&gt;
      &lt;td&gt;52367336&lt;/td&gt;
      &lt;td&gt;✨House of Alchemy✨&lt;/td&gt;
      &lt;td&gt;140013413&lt;/td&gt;
      &lt;td&gt;Greta&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;Porto - Saragozza&lt;/td&gt;
      &lt;td&gt;44.49072&lt;/td&gt;
      &lt;td&gt;11.30890&lt;/td&gt;
      &lt;td&gt;Entire home/apt&lt;/td&gt;
      &lt;td&gt;96&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;7&lt;/td&gt;
      &lt;td&gt;2021-11-28&lt;/td&gt;
      &lt;td&gt;3.18&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;88&lt;/td&gt;
      &lt;td&gt;7&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2039&lt;/th&gt;
      &lt;td&gt;34495335&lt;/td&gt;
      &lt;td&gt;♥ Romantic for Couple in Love ♥ | 4 Star Boutique&lt;/td&gt;
      &lt;td&gt;239491712&lt;/td&gt;
      &lt;td&gt;4 Star Bologna&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;Santo Stefano&lt;/td&gt;
      &lt;td&gt;44.50368&lt;/td&gt;
      &lt;td&gt;11.34972&lt;/td&gt;
      &lt;td&gt;Entire home/apt&lt;/td&gt;
      &lt;td&gt;143&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;25&lt;/td&gt;
      &lt;td&gt;2021-08-20&lt;/td&gt;
      &lt;td&gt;0.79&lt;/td&gt;
      &lt;td&gt;14&lt;/td&gt;
      &lt;td&gt;262&lt;/td&gt;
      &lt;td&gt;6&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2964&lt;/th&gt;
      &lt;td&gt;47866124&lt;/td&gt;
      &lt;td&gt;♡Amazing Suite with Private SPA ♡ 4starbologna...&lt;/td&gt;
      &lt;td&gt;239491712&lt;/td&gt;
      &lt;td&gt;4 Star Bologna&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;Santo Stefano&lt;/td&gt;
      &lt;td&gt;44.50381&lt;/td&gt;
      &lt;td&gt;11.34951&lt;/td&gt;
      &lt;td&gt;Entire home/apt&lt;/td&gt;
      &lt;td&gt;347&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;2021-10-17&lt;/td&gt;
      &lt;td&gt;0.72&lt;/td&gt;
      &lt;td&gt;14&lt;/td&gt;
      &lt;td&gt;337&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;You can remane columns using the &lt;code&gt;rename()&lt;/code&gt; function. It takes a dictionary as &lt;code&gt;column&lt;/code&gt; argument in the form &lt;code&gt;{&amp;quot;old_name&amp;quot;: &amp;quot;new_name&amp;quot;}&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_listings.rename(columns={&#39;name&#39;: &#39;listing_name&#39;, 
                            &#39;id&#39;: &#39;listing_id&#39;}).head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;listing_id&lt;/th&gt;
      &lt;th&gt;listing_name&lt;/th&gt;
      &lt;th&gt;host_id&lt;/th&gt;
      &lt;th&gt;host_name&lt;/th&gt;
      &lt;th&gt;neighbourhood_group&lt;/th&gt;
      &lt;th&gt;neighbourhood&lt;/th&gt;
      &lt;th&gt;latitude&lt;/th&gt;
      &lt;th&gt;longitude&lt;/th&gt;
      &lt;th&gt;room_type&lt;/th&gt;
      &lt;th&gt;price&lt;/th&gt;
      &lt;th&gt;minimum_nights&lt;/th&gt;
      &lt;th&gt;number_of_reviews&lt;/th&gt;
      &lt;th&gt;last_review&lt;/th&gt;
      &lt;th&gt;reviews_per_month&lt;/th&gt;
      &lt;th&gt;calculated_host_listings_count&lt;/th&gt;
      &lt;th&gt;availability_365&lt;/th&gt;
      &lt;th&gt;number_of_reviews_ltm&lt;/th&gt;
      &lt;th&gt;license&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;42196&lt;/td&gt;
      &lt;td&gt;50 sm Studio in the historic centre&lt;/td&gt;
      &lt;td&gt;184487&lt;/td&gt;
      &lt;td&gt;Carlo&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;Santo Stefano&lt;/td&gt;
      &lt;td&gt;44.48507&lt;/td&gt;
      &lt;td&gt;11.34786&lt;/td&gt;
      &lt;td&gt;Entire home/apt&lt;/td&gt;
      &lt;td&gt;68&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;180&lt;/td&gt;
      &lt;td&gt;2021-11-12&lt;/td&gt;
      &lt;td&gt;1.32&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;161&lt;/td&gt;
      &lt;td&gt;6&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;46352&lt;/td&gt;
      &lt;td&gt;A room in Pasolini&#39;s house&lt;/td&gt;
      &lt;td&gt;467810&lt;/td&gt;
      &lt;td&gt;Eleonora&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;Porto - Saragozza&lt;/td&gt;
      &lt;td&gt;44.49168&lt;/td&gt;
      &lt;td&gt;11.33514&lt;/td&gt;
      &lt;td&gt;Private room&lt;/td&gt;
      &lt;td&gt;29&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;300&lt;/td&gt;
      &lt;td&gt;2021-11-30&lt;/td&gt;
      &lt;td&gt;2.20&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;248&lt;/td&gt;
      &lt;td&gt;37&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;59697&lt;/td&gt;
      &lt;td&gt;COZY LARGE BEDROOM in the city center&lt;/td&gt;
      &lt;td&gt;286688&lt;/td&gt;
      &lt;td&gt;Paolo&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;Santo Stefano&lt;/td&gt;
      &lt;td&gt;44.48817&lt;/td&gt;
      &lt;td&gt;11.34124&lt;/td&gt;
      &lt;td&gt;Private room&lt;/td&gt;
      &lt;td&gt;50&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;240&lt;/td&gt;
      &lt;td&gt;2020-10-04&lt;/td&gt;
      &lt;td&gt;2.18&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;327&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;85368&lt;/td&gt;
      &lt;td&gt;Garden House Bologna&lt;/td&gt;
      &lt;td&gt;467675&lt;/td&gt;
      &lt;td&gt;Anna Maria&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;Santo Stefano&lt;/td&gt;
      &lt;td&gt;44.47834&lt;/td&gt;
      &lt;td&gt;11.35672&lt;/td&gt;
      &lt;td&gt;Entire home/apt&lt;/td&gt;
      &lt;td&gt;126&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;40&lt;/td&gt;
      &lt;td&gt;2019-11-03&lt;/td&gt;
      &lt;td&gt;0.34&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;332&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;145779&lt;/td&gt;
      &lt;td&gt;SINGLE ROOM&lt;/td&gt;
      &lt;td&gt;705535&lt;/td&gt;
      &lt;td&gt;Valerio&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;Porto - Saragozza&lt;/td&gt;
      &lt;td&gt;44.49306&lt;/td&gt;
      &lt;td&gt;11.33786&lt;/td&gt;
      &lt;td&gt;Private room&lt;/td&gt;
      &lt;td&gt;50&lt;/td&gt;
      &lt;td&gt;10&lt;/td&gt;
      &lt;td&gt;69&lt;/td&gt;
      &lt;td&gt;2021-12-05&lt;/td&gt;
      &lt;td&gt;0.55&lt;/td&gt;
      &lt;td&gt;9&lt;/td&gt;
      &lt;td&gt;365&lt;/td&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;h2 id=&#34;aggregating&#34;&gt;Aggregating&lt;/h2&gt;
&lt;p&gt;We can compute statistics by group using &lt;code&gt;groupby()&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_listings.groupby(&#39;neighbourhood&#39;)[[&#39;price&#39;, &#39;reviews_per_month&#39;]].mean()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;price&lt;/th&gt;
      &lt;th&gt;reviews_per_month&lt;/th&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;neighbourhood&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;Borgo Panigale - Reno&lt;/th&gt;
      &lt;td&gt;83.020548&lt;/td&gt;
      &lt;td&gt;0.983488&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;Navile&lt;/th&gt;
      &lt;td&gt;142.200993&lt;/td&gt;
      &lt;td&gt;1.156745&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;Porto - Saragozza&lt;/th&gt;
      &lt;td&gt;129.908312&lt;/td&gt;
      &lt;td&gt;1.340325&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;San Donato - San Vitale&lt;/th&gt;
      &lt;td&gt;91.618138&lt;/td&gt;
      &lt;td&gt;0.933011&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;Santo Stefano&lt;/th&gt;
      &lt;td&gt;119.441841&lt;/td&gt;
      &lt;td&gt;1.344810&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;Savena&lt;/th&gt;
      &lt;td&gt;69.626016&lt;/td&gt;
      &lt;td&gt;0.805888&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;If you want to perform more than one function, maybe on different columns, you can use &lt;code&gt;aggregate()&lt;/code&gt; which can be shortened to &lt;code&gt;agg()&lt;/code&gt;. The sintax is &lt;code&gt;agg(output_var = (&amp;quot;input_var&amp;quot;, function))&lt;/code&gt; and it accepts also &lt;code&gt;numpy&lt;/code&gt; functions.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_listings.groupby(&#39;neighbourhood&#39;).agg(mean_reviews=(&amp;quot;reviews_per_month&amp;quot;, &amp;quot;mean&amp;quot;),
                                         min_price=(&amp;quot;price&amp;quot;, &amp;quot;min&amp;quot;),
                                         max_price=(&amp;quot;price&amp;quot;, np.max)).reset_index()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;neighbourhood&lt;/th&gt;
      &lt;th&gt;mean_reviews&lt;/th&gt;
      &lt;th&gt;min_price&lt;/th&gt;
      &lt;th&gt;max_price&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;Borgo Panigale - Reno&lt;/td&gt;
      &lt;td&gt;0.983488&lt;/td&gt;
      &lt;td&gt;9&lt;/td&gt;
      &lt;td&gt;1429&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;Navile&lt;/td&gt;
      &lt;td&gt;1.156745&lt;/td&gt;
      &lt;td&gt;14&lt;/td&gt;
      &lt;td&gt;5000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;Porto - Saragozza&lt;/td&gt;
      &lt;td&gt;1.340325&lt;/td&gt;
      &lt;td&gt;7&lt;/td&gt;
      &lt;td&gt;9999&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;San Donato - San Vitale&lt;/td&gt;
      &lt;td&gt;0.933011&lt;/td&gt;
      &lt;td&gt;10&lt;/td&gt;
      &lt;td&gt;1600&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;Santo Stefano&lt;/td&gt;
      &lt;td&gt;1.344810&lt;/td&gt;
      &lt;td&gt;11&lt;/td&gt;
      &lt;td&gt;9999&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;5&lt;/th&gt;
      &lt;td&gt;Savena&lt;/td&gt;
      &lt;td&gt;0.805888&lt;/td&gt;
      &lt;td&gt;9&lt;/td&gt;
      &lt;td&gt;680&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;If we want to build a new column by group, we can use &lt;code&gt;transform()&lt;/code&gt; on the grouped data. Unfortunately, it does not work as nicely as &lt;code&gt;aggregate()&lt;/code&gt; and we have to do one column at the time.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_listings.groupby(&#39;neighbourhood&#39;)[[&#39;price&#39;, &#39;reviews_per_month&#39;]].transform(&#39;mean&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;price&lt;/th&gt;
      &lt;th&gt;reviews_per_month&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;119.441841&lt;/td&gt;
      &lt;td&gt;1.344810&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;129.908312&lt;/td&gt;
      &lt;td&gt;1.340325&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;119.441841&lt;/td&gt;
      &lt;td&gt;1.344810&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;119.441841&lt;/td&gt;
      &lt;td&gt;1.344810&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;129.908312&lt;/td&gt;
      &lt;td&gt;1.340325&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;...&lt;/th&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3448&lt;/th&gt;
      &lt;td&gt;142.200993&lt;/td&gt;
      &lt;td&gt;1.156745&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3449&lt;/th&gt;
      &lt;td&gt;129.908312&lt;/td&gt;
      &lt;td&gt;1.340325&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3450&lt;/th&gt;
      &lt;td&gt;142.200993&lt;/td&gt;
      &lt;td&gt;1.156745&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3451&lt;/th&gt;
      &lt;td&gt;119.441841&lt;/td&gt;
      &lt;td&gt;1.344810&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3452&lt;/th&gt;
      &lt;td&gt;129.908312&lt;/td&gt;
      &lt;td&gt;1.340325&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;3453 rows × 2 columns&lt;/p&gt;
&lt;/div&gt;
&lt;h2 id=&#34;reshaping&#34;&gt;Reshaping&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;combining-datasets&#34;&gt;Combining Datasets&lt;/h2&gt;
&lt;p&gt;We can &lt;strong&gt;concatenate&lt;/strong&gt; datasets using &lt;code&gt;pd.concat()&lt;/code&gt;. It takes as argument a list of dataframes. By default, &lt;code&gt;pd.concat()&lt;/code&gt; performs the outer join. We can change it using the &lt;code&gt;join&lt;/code&gt; option.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_listings1 = df_listings[:2000]
np.shape(df_listings1)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;(2000, 18)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_listings2 = df_listings[1000:]
np.shape(df_listings2)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;(2453, 18)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;np.shape(
    pd.concat([df_listings1, df_listings2], join=&#39;inner&#39;)
)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;(4453, 18)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To instead merge dataframes, we can use the &lt;code&gt;pd.merge&lt;/code&gt; function.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Options&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;how&lt;/code&gt;: {‘left’, ‘right’, ‘outer’, ‘inner’, ‘cross’}, default ‘inner’&lt;/li&gt;
&lt;li&gt;&lt;code&gt;on&lt;/code&gt;: label or list&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;pd.merge(df_listings, df_prices, left_on=&#39;id&#39;, right_on=&#39;listing_id&#39;, how=&#39;inner&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;id&lt;/th&gt;
      &lt;th&gt;name&lt;/th&gt;
      &lt;th&gt;host_id&lt;/th&gt;
      &lt;th&gt;host_name&lt;/th&gt;
      &lt;th&gt;neighbourhood_group&lt;/th&gt;
      &lt;th&gt;neighbourhood&lt;/th&gt;
      &lt;th&gt;latitude&lt;/th&gt;
      &lt;th&gt;longitude&lt;/th&gt;
      &lt;th&gt;room_type&lt;/th&gt;
      &lt;th&gt;price_x&lt;/th&gt;
      &lt;th&gt;...&lt;/th&gt;
      &lt;th&gt;availability_365&lt;/th&gt;
      &lt;th&gt;number_of_reviews_ltm&lt;/th&gt;
      &lt;th&gt;license&lt;/th&gt;
      &lt;th&gt;listing_id&lt;/th&gt;
      &lt;th&gt;date&lt;/th&gt;
      &lt;th&gt;available&lt;/th&gt;
      &lt;th&gt;price_y&lt;/th&gt;
      &lt;th&gt;adjusted_price&lt;/th&gt;
      &lt;th&gt;minimum_nights_y&lt;/th&gt;
      &lt;th&gt;maximum_nights&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;42196&lt;/td&gt;
      &lt;td&gt;50 sm Studio in the historic centre&lt;/td&gt;
      &lt;td&gt;184487&lt;/td&gt;
      &lt;td&gt;Carlo&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;Santo Stefano&lt;/td&gt;
      &lt;td&gt;44.485070&lt;/td&gt;
      &lt;td&gt;11.347860&lt;/td&gt;
      &lt;td&gt;Entire home/apt&lt;/td&gt;
      &lt;td&gt;68&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;161&lt;/td&gt;
      &lt;td&gt;6&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;42196&lt;/td&gt;
      &lt;td&gt;2021-12-17&lt;/td&gt;
      &lt;td&gt;f&lt;/td&gt;
      &lt;td&gt;$68.00&lt;/td&gt;
      &lt;td&gt;$68.00&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;360&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;42196&lt;/td&gt;
      &lt;td&gt;50 sm Studio in the historic centre&lt;/td&gt;
      &lt;td&gt;184487&lt;/td&gt;
      &lt;td&gt;Carlo&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;Santo Stefano&lt;/td&gt;
      &lt;td&gt;44.485070&lt;/td&gt;
      &lt;td&gt;11.347860&lt;/td&gt;
      &lt;td&gt;Entire home/apt&lt;/td&gt;
      &lt;td&gt;68&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;161&lt;/td&gt;
      &lt;td&gt;6&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;42196&lt;/td&gt;
      &lt;td&gt;2021-12-18&lt;/td&gt;
      &lt;td&gt;f&lt;/td&gt;
      &lt;td&gt;$68.00&lt;/td&gt;
      &lt;td&gt;$68.00&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;360&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;42196&lt;/td&gt;
      &lt;td&gt;50 sm Studio in the historic centre&lt;/td&gt;
      &lt;td&gt;184487&lt;/td&gt;
      &lt;td&gt;Carlo&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;Santo Stefano&lt;/td&gt;
      &lt;td&gt;44.485070&lt;/td&gt;
      &lt;td&gt;11.347860&lt;/td&gt;
      &lt;td&gt;Entire home/apt&lt;/td&gt;
      &lt;td&gt;68&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;161&lt;/td&gt;
      &lt;td&gt;6&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;42196&lt;/td&gt;
      &lt;td&gt;2021-12-19&lt;/td&gt;
      &lt;td&gt;f&lt;/td&gt;
      &lt;td&gt;$68.00&lt;/td&gt;
      &lt;td&gt;$68.00&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;360&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;42196&lt;/td&gt;
      &lt;td&gt;50 sm Studio in the historic centre&lt;/td&gt;
      &lt;td&gt;184487&lt;/td&gt;
      &lt;td&gt;Carlo&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;Santo Stefano&lt;/td&gt;
      &lt;td&gt;44.485070&lt;/td&gt;
      &lt;td&gt;11.347860&lt;/td&gt;
      &lt;td&gt;Entire home/apt&lt;/td&gt;
      &lt;td&gt;68&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;161&lt;/td&gt;
      &lt;td&gt;6&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;42196&lt;/td&gt;
      &lt;td&gt;2021-12-20&lt;/td&gt;
      &lt;td&gt;f&lt;/td&gt;
      &lt;td&gt;$68.00&lt;/td&gt;
      &lt;td&gt;$68.00&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;360&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;42196&lt;/td&gt;
      &lt;td&gt;50 sm Studio in the historic centre&lt;/td&gt;
      &lt;td&gt;184487&lt;/td&gt;
      &lt;td&gt;Carlo&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;Santo Stefano&lt;/td&gt;
      &lt;td&gt;44.485070&lt;/td&gt;
      &lt;td&gt;11.347860&lt;/td&gt;
      &lt;td&gt;Entire home/apt&lt;/td&gt;
      &lt;td&gt;68&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;161&lt;/td&gt;
      &lt;td&gt;6&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;42196&lt;/td&gt;
      &lt;td&gt;2021-12-21&lt;/td&gt;
      &lt;td&gt;f&lt;/td&gt;
      &lt;td&gt;$68.00&lt;/td&gt;
      &lt;td&gt;$68.00&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;360&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;...&lt;/th&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1260340&lt;/th&gt;
      &lt;td&gt;53854962&lt;/td&gt;
      &lt;td&gt;A pochi passi dal Mercato delle Erbe by Wonder...&lt;/td&gt;
      &lt;td&gt;13036400&lt;/td&gt;
      &lt;td&gt;Wonderful Italy&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;Porto - Saragozza&lt;/td&gt;
      &lt;td&gt;44.497456&lt;/td&gt;
      &lt;td&gt;11.337492&lt;/td&gt;
      &lt;td&gt;Entire home/apt&lt;/td&gt;
      &lt;td&gt;115&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;98&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;53854962&lt;/td&gt;
      &lt;td&gt;2022-12-12&lt;/td&gt;
      &lt;td&gt;f&lt;/td&gt;
      &lt;td&gt;$115.00&lt;/td&gt;
      &lt;td&gt;$115.00&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1125&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1260341&lt;/th&gt;
      &lt;td&gt;53854962&lt;/td&gt;
      &lt;td&gt;A pochi passi dal Mercato delle Erbe by Wonder...&lt;/td&gt;
      &lt;td&gt;13036400&lt;/td&gt;
      &lt;td&gt;Wonderful Italy&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;Porto - Saragozza&lt;/td&gt;
      &lt;td&gt;44.497456&lt;/td&gt;
      &lt;td&gt;11.337492&lt;/td&gt;
      &lt;td&gt;Entire home/apt&lt;/td&gt;
      &lt;td&gt;115&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;98&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;53854962&lt;/td&gt;
      &lt;td&gt;2022-12-13&lt;/td&gt;
      &lt;td&gt;f&lt;/td&gt;
      &lt;td&gt;$115.00&lt;/td&gt;
      &lt;td&gt;$115.00&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1125&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1260342&lt;/th&gt;
      &lt;td&gt;53854962&lt;/td&gt;
      &lt;td&gt;A pochi passi dal Mercato delle Erbe by Wonder...&lt;/td&gt;
      &lt;td&gt;13036400&lt;/td&gt;
      &lt;td&gt;Wonderful Italy&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;Porto - Saragozza&lt;/td&gt;
      &lt;td&gt;44.497456&lt;/td&gt;
      &lt;td&gt;11.337492&lt;/td&gt;
      &lt;td&gt;Entire home/apt&lt;/td&gt;
      &lt;td&gt;115&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;98&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;53854962&lt;/td&gt;
      &lt;td&gt;2022-12-14&lt;/td&gt;
      &lt;td&gt;f&lt;/td&gt;
      &lt;td&gt;$115.00&lt;/td&gt;
      &lt;td&gt;$115.00&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1125&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1260343&lt;/th&gt;
      &lt;td&gt;53854962&lt;/td&gt;
      &lt;td&gt;A pochi passi dal Mercato delle Erbe by Wonder...&lt;/td&gt;
      &lt;td&gt;13036400&lt;/td&gt;
      &lt;td&gt;Wonderful Italy&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;Porto - Saragozza&lt;/td&gt;
      &lt;td&gt;44.497456&lt;/td&gt;
      &lt;td&gt;11.337492&lt;/td&gt;
      &lt;td&gt;Entire home/apt&lt;/td&gt;
      &lt;td&gt;115&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;98&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;53854962&lt;/td&gt;
      &lt;td&gt;2022-12-15&lt;/td&gt;
      &lt;td&gt;f&lt;/td&gt;
      &lt;td&gt;$115.00&lt;/td&gt;
      &lt;td&gt;$115.00&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1125&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1260344&lt;/th&gt;
      &lt;td&gt;53854962&lt;/td&gt;
      &lt;td&gt;A pochi passi dal Mercato delle Erbe by Wonder...&lt;/td&gt;
      &lt;td&gt;13036400&lt;/td&gt;
      &lt;td&gt;Wonderful Italy&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;Porto - Saragozza&lt;/td&gt;
      &lt;td&gt;44.497456&lt;/td&gt;
      &lt;td&gt;11.337492&lt;/td&gt;
      &lt;td&gt;Entire home/apt&lt;/td&gt;
      &lt;td&gt;115&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;98&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;53854962&lt;/td&gt;
      &lt;td&gt;2022-12-16&lt;/td&gt;
      &lt;td&gt;f&lt;/td&gt;
      &lt;td&gt;$115.00&lt;/td&gt;
      &lt;td&gt;$115.00&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1125&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;1260345 rows × 25 columns&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;As you can see, since the variable &lt;code&gt;price&lt;/code&gt; was present in both datasets, we now have a &lt;code&gt;price.x&lt;/code&gt; and a &lt;code&gt;price.y&lt;/code&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Asymptotic Theory</title>
      <link>https://matteocourthoud.github.io/course/metrics/03_asymptotics/</link>
      <pubDate>Fri, 29 Oct 2021 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/course/metrics/03_asymptotics/</guid>
      <description>&lt;h2 id=&#34;convergence&#34;&gt;Convergence&lt;/h2&gt;
&lt;h3 id=&#34;sequences&#34;&gt;Sequences&lt;/h3&gt;
&lt;p&gt;A sequence of nonrandom numbers $\lbrace a_n \rbrace$ &lt;strong&gt;converges&lt;/strong&gt; to
$a$ (has limit $a$) if for all $\varepsilon&amp;gt;0$, there exists
$n _ \varepsilon$ such that if $n &amp;gt; n_ \varepsilon$, then
$|a_n - a| &amp;lt; \varepsilon$. We write $a_n \to a$ as $n \to \infty$.&lt;/p&gt;
&lt;p&gt;A sequence of nonrandom numbers $\lbrace a_n \rbrace$ is &lt;strong&gt;bounded&lt;/strong&gt; if
and only if there is some $B &amp;lt; \infty$ such that $|a_n| \leq B$ for all
$n=1,2,&amp;hellip;$ Otherwise, we say that $\lbrace a_n \rbrace$ is unbounded.&lt;/p&gt;
&lt;h3 id=&#34;big-o-and-small-o-notation&#34;&gt;Big-O and Small-o Notation&lt;/h3&gt;
&lt;p&gt;A sequence of nonrandom numbers $\lbrace a_n \rbrace$ is $O(N^\delta)$
(at most of order $N^\delta$) if $N^{-\delta} a_n$ is bounded. When
$\delta=0$, $a_n$ is bounded, and we also write $a_n = O(1)$ (big oh
one).&lt;/p&gt;
&lt;p&gt;A sequence of nonrandom numbers $\lbrace a_n \rbrace$ is $o(N^\delta)$
if $N^{-\delta} a_n \to 0$. When $\delta=0$, $a_n$ converges to zero,
and we also write $a_n = o(1)$ (little oh one).&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Properties&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;if $a_n = o(N^{\delta})$, then $a_n = O(N^\delta)$&lt;/li&gt;
&lt;li&gt;if $a_n = o(1)$, then $a_n = O(1)$&lt;/li&gt;
&lt;li&gt;if each element of a sequence of vectors or matrices is
$O(N^\delta)$, we say the sequence of vectors or matrices is
$O(N^\delta)$&lt;/li&gt;
&lt;li&gt;similarly for $o(N^\delta)$.&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;convergence-in-probability&#34;&gt;Convergence in Probability&lt;/h3&gt;
&lt;p&gt;A sequence of random variables $\lbrace X_n \rbrace$ &lt;strong&gt;converges in
probability&lt;/strong&gt; to a constant $c \in \mathbb R$ if for all $\varepsilon&amp;gt;0$
$$
\Pr \big( |X_n - c| &amp;gt; \varepsilon \big) \to 0 \qquad \text{ as } n \to \infty
$$ We write $X_n \overset{p}{\to} c$ and say that $a$ is the probability
limit (&lt;em&gt;plim&lt;/em&gt;) of $X_n$: $\mathrm{plim} X_n = c$. In the special case
where $c=0$, we also say that $\lbrace X_n \rbrace$ is $o_p(1)$ (little
oh p one). We also write $X_n = o_p(1)$ or $X_n \overset{p}{\to} 0$.&lt;/p&gt;
&lt;p&gt;A sequence of random variables $\lbrace X_n \rbrace$ is bounded in
probability if for every $\varepsilon&amp;gt;0$, there exists a
$B _ \varepsilon &amp;lt; \infty$ and an integer $n_ \varepsilon$ such that $$
\Pr \big( |x_ n| &amp;gt; B_ \varepsilon \big) &amp;lt; \varepsilon \qquad \text{ for all } n &amp;gt; n_ \varepsilon
$$ We write $X_n = O_p(1)$ ($\lbrace X_n \rbrace$ is big oh p one).&lt;/p&gt;
&lt;p&gt;A sequence of random variables $\lbrace X_n \rbrace$ is $o_p(a_n)$ where
$\lbrace a_n \rbrace$ is a nonrandom positive sequence, if
$X_n/a_n = o_p(1)$. We write $X_n = o_p(a_n)$.&lt;/p&gt;
&lt;p&gt;A sequence of random variables $\lbrace X_n \rbrace$ is $O_p(a_n)$ where
$\lbrace a_n \rbrace$ is a nonrandom positive sequence, if
$X_n/a_n = O_p(1)$. We write $X_n = O_p(a_n)$.&lt;/p&gt;
&lt;h3 id=&#34;other-convergences&#34;&gt;Other Convergences&lt;/h3&gt;
&lt;p&gt;A sequence of random variables $\lbrace X_n \rbrace$ &lt;strong&gt;converges almost
surely&lt;/strong&gt; to a constant $c \in \mathbb R$ if $$
\Pr \big( X_n \overset{p}{\to} c \big) = 1
$$ We write $X_n \overset{as}{\to} c$.&lt;/p&gt;
&lt;p&gt;A sequence of random variables $\lbrace X_n \rbrace$ &lt;strong&gt;converges in mean
square&lt;/strong&gt; to a constant $c \in \mathbb R$ if $$
\mathbb E [(X_n - c)^2] \to 0  \qquad \text{ as } n \to \infty
$$ We write $X_n \overset{ms}{\to} c$.&lt;/p&gt;
&lt;p&gt;Let $\lbrace X_n \rbrace$ be a sequence of random variables and $F_n$ be
the cumulative distribution function (cdf) of $X_n$. We say that $X_n$
&lt;strong&gt;converges in distribution&lt;/strong&gt; to a random variable $x$ with cdf $F$ if
the cdf $F_n$ of $X_n$ converges to the cdf $F$ of $x$ &lt;em&gt;at every
continuity point&lt;/em&gt; of $F$. We write $X_n \overset{d}{\to} x$ and we call
$F$ the &lt;strong&gt;asymptotic distribution&lt;/strong&gt; of $X_n$.&lt;/p&gt;
&lt;h3 id=&#34;compare-convergences&#34;&gt;Compare Convergences&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Lemma&lt;/strong&gt;: Let $\lbrace X_n \rbrace$ be a sequence of random variables
and $c \in \mathbb R$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$X_n \overset{ms}{\to} c \ \Rightarrow \ X_n \overset{p}{\to} c$&lt;/li&gt;
&lt;li&gt;$X_n \overset{as}{\to} c \ \Rightarrow \ X_n \overset{p}{\to} c$&lt;/li&gt;
&lt;li&gt;$X_n \overset{p}{\to} c \ \Rightarrow \ X_n \overset{d}{\to} c$&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;Note that all the above definitions naturally extend to a sequence of
random vectors by requiring element-by-element convergence. For
example, a sequence of $K \times 1$ random vectors
$\lbrace X_n \rbrace$ &lt;strong&gt;converges in probability&lt;/strong&gt; to a constant
$c \in \mathbb R^K$ if for all $\varepsilon&amp;gt;0$ $$
\Pr \big( |X _ {nk} - c_k| &amp;gt; \varepsilon \big) \to 0 \qquad \text{ as } n \to \infty \quad \forall k = 1&amp;hellip;K
$$&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;theorems&#34;&gt;Theorems&lt;/h2&gt;
&lt;h3 id=&#34;slutsky-theorem&#34;&gt;Slutsky Theorem&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Let $\lbrace X_n \rbrace$ and $\lbrace Y_n \rbrace$ be two sequences of
random variables, $x$ a random variable and $c \in \mathbb R$ a constant
such that $\lbrace X_n \rbrace \overset{d}{\to} X$ and
$\lbrace Y_n \rbrace \overset{p}{\to} c$. Then&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$X_n + Y_n \overset{d}{\to} X + c$&lt;/li&gt;
&lt;li&gt;$X_n \cdot Y_n \overset{d}{\to} X \cdot c$&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;continuous-mapping-theorem&#34;&gt;Continuous Mapping Theorem&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Let $\lbrace X_n \rbrace$ be sequence of $K \times 1$ random vectors and
$g: \mathbb{R}^K \to \mathbb{R}^J$ a continuous function that does not
depend on $n$.Then&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$x _n \overset{as}{\to} x \ \Rightarrow \ g(X_n) \overset{as}{\to} g(x)$&lt;/li&gt;
&lt;li&gt;$x _n \overset{p}{\to} x \ \Rightarrow \ g(X_n) \overset{p}{\to} g(x)$&lt;/li&gt;
&lt;li&gt;$x _n \overset{d}{\to} x \ \Rightarrow \ g(X_n) \overset{d}{\to} g(x)$&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;weak-law-of-large-numbers&#34;&gt;Weak Law of Large Numbers&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Let $\lbrace x_i \rbrace _ {i=1}^n$ be a sequence of independent,
identically distributed random variables such that
$\mathbb{E}[|x_i|] &amp;lt; \infty$. Then the sequence satisfies the &lt;strong&gt;weak law
of large numbers (WLLN)&lt;/strong&gt;: $$
\mathbb{E}_n[x_i] = \frac{1}{n} \sum _ {i=1}^n x_i \overset{p}{\to} \mu \qquad \text{ where } \mu \equiv \mathbb{E}[x_i]
$$&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Intuitions&lt;/strong&gt; for the law of large numbers:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Cancellation with high probability.&lt;/li&gt;
&lt;li&gt;Re-visiting regions of the sample space over and over again.&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;wlln-proof&#34;&gt;WLLN Proof&lt;/h3&gt;
&lt;p&gt;The independence of the random variables implies no correlation between
them, and we have that $$
Var \left( \mathbb{E}_n[x_i] \right) = Var \left( \frac{1}{n} \sum _ {i=1}^n x_i \right) = \frac{1}{n^2} Var\left( \sum _ {i=1}^n x_i \right) = \frac{n \sigma^2}{n^2} = \frac{\sigma^2}{n}
$$ Using Chebyshev’s inequality on $\mathbb{E}_n[x_i]$ results in $$
\Pr \big( \left|\mathbb{E}_n[x_i]-\mu \right| &amp;gt; \varepsilon \big) \leq {\frac {\sigma ^{2}}{n\varepsilon ^{2}}}
$$ As $n$ approaches infinity, the right hand side approaches $0$. And
by definition of convergence in probability, we have obtained
$\mathbb{E}_n[x_i] \overset{p}{\to} \mu$ as $n \to \infty$.
$$\tag*{$\blacksquare$}$$&lt;/p&gt;
&lt;h3 id=&#34;central-limit-theorem&#34;&gt;Central Limit Theorem&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Lindberg-Levy Central Limit Theorem&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Let $\lbrace x_i \rbrace _ {i=1}^n$ be a sequence of independent,
identically distributed random variables such that
$\mathbb{E}[x_i^2] &amp;lt; \infty$, and $\mathbb{E}[x_i] = \mu$. Then
$\lbrace x_i \rbrace$ satisfies the &lt;strong&gt;central limit theorem (CLT)&lt;/strong&gt;;
that is, $$
\frac{1}{\sqrt{n}} \sum _ {i=1}^{n} (x_i - \mu) \overset{d}{\to} N(0,\sigma^2)
$$ where $\sigma^2 = Var(x_i) = \mathbb{E}[x_i x_i&#39;]$ is necessarily
positive semidefinite.&lt;/p&gt;
&lt;h3 id=&#34;clt-proof-1&#34;&gt;CLT Proof (1)&lt;/h3&gt;
&lt;p&gt;Suppose $\lbrace x_i \rbrace$ are independent and identically
distributed random variables, each with mean $\mu$ and finite variance
$\sigma^2$. The sum $x_1 + &amp;hellip; + X_n$ has mean $n \mu$ and variance
$n \sigma^2$.&lt;/p&gt;
&lt;p&gt;Consider the random variable $$
Z_n = \frac{x_1 + &amp;hellip; + X_n - n\mu}{\sqrt{n \sigma^2}} = \sum _ {i=1}^n \frac{x_i - \mu}{\sqrt{n \sigma^2}} = \sum _ {i=1}^n \frac{1}{\sqrt{n}} \tilde x_i
$$&lt;/p&gt;
&lt;p&gt;where in the last step we defined the new random variables
$\tilde x_i = \frac{x_i - \mu}{\sigma}$ each with zero mean and unit
variance. The characteristic function of $Z_n$ is given by $$
\varphi _ {Z_n} (t) = \varphi _ { \sum _ {i=1}^n \frac{1}{\sqrt{n} } \tilde{x}_i}(t) = \varphi _ {\tilde x_1} \left( \frac{t}{\sqrt{n}} \right) \times &amp;hellip; \times \varphi _ {Y_n} \left( \frac{t}{\sqrt{n}} \right) = \left[ \varphi _ {\tilde x_1} \left( \frac{t}{\sqrt{n}} \right) \right]^n
$$&lt;/p&gt;
&lt;p&gt;where in the last step we used the fact that all of the $\tilde{x}_i$
are identically distributed.&lt;/p&gt;
&lt;h3 id=&#34;clt-proof-2&#34;&gt;CLT Proof (2)&lt;/h3&gt;
&lt;p&gt;The characteristic function of $\tilde{x}_1$ is, by Taylor’s theorem, $$
\varphi _ {\tilde{x}_1} \left( \frac{t}{\sqrt{n}} \right) = 1 - \frac{t^2}{2n} + o \left( \frac{t^2}{n} \right) \qquad \text{ for } n \to \infty
$$&lt;/p&gt;
&lt;p&gt;where $o(t^2)$ is “little o notation” for some function of $t$ that goes
to zero more rapidly than $t^2$. By the limit of the exponential
function, the characteristic function of $Z_n$ equals $$
\varphi _ {Z_ n}(t) = \left[  1 - \frac{t^2}{2n} + o \left( \frac{t^2}{n} \right) \right]^n \to e^{ -\frac{1}{2}t^2 } \qquad \text{ for } n \to \infty
$$&lt;/p&gt;
&lt;p&gt;Note that all of the higher order terms vanish in the limit
$n \to \infty$. The right hand side equals the characteristic function
of a standard normal distribution $N(0,1)$, which implies through Lévy’s
continuity theorem that the distribution of $Z_ n$ will approach
$N(0,1)$ as $n \to \infty$. Therefore, the sum $x_1 + &amp;hellip; + x_n$ will
approach that of the normal distribution $N(n_{\mu}, n\sigma^2)$, and
the sample average $$
\mathbb{E}_n [x_i] = \frac{1}{n} \sum _ {i=1}^n x_i
$$&lt;/p&gt;
&lt;p&gt;converges to the normal distribution $N(\mu, \sigma^2)$, from which the
central limit theorem follows. $$\tag*{$\blacksquare$}$$&lt;/p&gt;
&lt;h3 id=&#34;delta-method&#34;&gt;Delta Method&lt;/h3&gt;
&lt;p&gt;Let $\lbrace X_n \rbrace$ be a sequence of independent, identically
distributed $K \times 1$ random vectors such that&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\sqrt{n} (X_n - c) \overset{d}{\to} Z$ for some fixed
$c \in \mathbb{R}^K$&lt;/li&gt;
&lt;li&gt;and $\Sigma$ a $K \times K$ positive definite matrix.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Suppose $g : \mathbb{R}^K \to \mathbb{R}^J$ with $J \leq K$ is
continuously differentiable and full rank at $c$, then $$
\sqrt{n} \Big[ g(X_n) - g( c ) \Big] \overset{d}{\to} G Z
$$&lt;/p&gt;
&lt;p&gt;where $G = \frac{\partial g( c )}{\partial x}$ is the $J \times K$
matrix of partial derivatives evaluated at $c$.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Note that the most common utilization is with the random variable
$\mathbb E_n [x_i]$. In fact, under the assumptions of the CLT, we
have that $$
\sqrt{n} \Big[ g \big( \mathbb E_n [x_i] \big) - g(\mu) \Big] \overset{d}{\to} N(0, G \Sigma G&#39;)
$$&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;ergodic-theory&#34;&gt;Ergodic Theory&lt;/h2&gt;
&lt;h3 id=&#34;ppt&#34;&gt;PPT&lt;/h3&gt;
&lt;p&gt;Let $(\Omega, \mathcal{B}, P)$ be a probability space and
$T: \Omega \rightarrow \Omega$ a measurable map. $T$ is a &lt;strong&gt;probability
preserving transformation&lt;/strong&gt; if the probability of the pre-image of every
set is the same as the probability of the set itself,
i.e. $\forall G, \Pr(T^{-1}(G)) = \Pr(G)$.&lt;/p&gt;
&lt;p&gt;Let $(\Omega, \mathcal{B}, P)$ be a probability space and
$T: \Omega \rightarrow \Omega$ a PPT. A set $G \in \mathcal{B}$ is
&lt;strong&gt;invariant&lt;/strong&gt; if $T^{-1}(G)=G$.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Note that it does not have to work the other way around:
$G \neq T(G)$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Let $(\Omega, \mathcal{B}, P)$ be a probability space and
$T: \Omega \rightarrow \Omega$ a PPT. $T$ is &lt;strong&gt;ergodic&lt;/strong&gt; if every
invariant set $G \in \mathcal{B}$ has probability zero or one,
i.e. $\Pr(G) = 0 \lor \Pr(G) = 1$.&lt;/p&gt;
&lt;h3 id=&#34;poincarè-recurrence&#34;&gt;Poincarè Recurrence&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Let $(\Omega, \mathcal{B}, P)$ be a probability space and
$T: \Omega \rightarrow \Omega$ a PPT. Suppose $A \in \mathcal{B}$ is
measurable. Then, for almost every $\omega \in A$, $T^n(\omega)\in A$
for infinitely many $n$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;We follow 5 steps:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Let
$G = \lbrace \omega \in A : T^K(\omega) \notin A \quad \forall k &amp;gt;0 \rbrace$:
the set of all points of A that never ``return” in A.&lt;/li&gt;
&lt;li&gt;Note that $\forall j \geq 1$, $T^{-j}(G) \cap G = \emptyset$. In
fact, suppose $\omega \in T^{-j}(G)$. Then $\omega \notin G$ since
otherwise we would have $\omega \in G \subseteq A$ and
$\omega \in T^J(G) \subseteq A$ which contradicts the definition of
$G$.&lt;/li&gt;
&lt;li&gt;It follows that $\forall l,n \geq 1$,
$T^{-l}(G) \cap T^{-n}(G) = \emptyset$&lt;/li&gt;
&lt;li&gt;Since $T$ is a PPT, $\Pr(T^{-j}(G)) = \Pr(G)$ $\forall j$&lt;/li&gt;
&lt;li&gt;Then $$
\Pr (T^{-1}(G) \cup T^{-2}(G) \cup &amp;hellip; \cup T^{-l}(G)) = l \cdot \Pr(G) \leq 1 \Rightarrow \Pr(G) \leq \frac{1}{l} \quad \Rightarrow \quad \lim_ {l \to \infty} \Pr(G) = 0
$$ $$\tag*{$\blacksquare$}$$&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;comment&#34;&gt;Comment&lt;/h3&gt;
&lt;p&gt;Halmos: “&lt;em&gt;The recurrence theorem says that under the appropriate
conditions on a transformation T almost every point of each measurable
set $A$ returns to $A$ infinitely often. It is natural to ask: exactly
how long a time do the images of such recurrent points spend in $A$? The
precise formulation of the problem runs as follows: given a point $x$
(for present purposes it does not matter whether $x$ is in $A$ or not),
and given a positive integer $n$, form the ratio of the number of these
points that belong to $A$ to the total number (i.e., to $n$), and
evaluate the limit of these ratios as $n$ tends to infinity. It is, of
course, not at all obvious in what sense, if any, that limit exists. If
$f$ is the characteristic function of $A$ then the ratio just discussed
is&lt;/em&gt;” $$
\frac{1}{n} \sum _ {i=1}^n f(T^{i}x) = \frac{1}{n} \sum _ {i=1}^n x_i
$$&lt;/p&gt;
&lt;h3 id=&#34;ergodic-theorem&#34;&gt;Ergodic Theorem&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Let $T$ be an ergodic PPT on $\Omega$. Let $x$ be a random variable on
$\Omega$ with $\mathbb{E}[x] &amp;lt; \infty$. Let $x_i = x \circ T^i$. Then,
$$
\frac{1}{n} \sum _ {i=1}^n x_i \overset{as}{\to} \mathbb{E}[x]
$$&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;To figure out whether a PPT is ergodic, it’s useful to draw a graph
with $T^{-1}(G)$ on the y-axis and $G$ on the x-axis.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;comment-1&#34;&gt;Comment&lt;/h3&gt;
&lt;p&gt;From the ergodic theorem, we have that $$
\lim _ {n \to \infty} \frac{1}{n} \sum _ {i=1}^n f(T^{i}x) g(x) = f^* (x)g(x) \quad \Rightarrow \quad  \lim _ {n \to \infty} \Pr(T^{-n}G \cap H) = \Pr(G)\Pr(H)
$$ where $f^* (x) = \int f(x) dx = \mathbb{E}[f]$.&lt;/p&gt;
&lt;p&gt;[Halmos]: &lt;em&gt;We have seen that if a transformation $T$ is ergodic, then
$\Pr(T^{-n}G \cap H)$ converges in the sense of Cesaro to
$\Pr(G)\Pr(H)$. The validity of this condition for all $G$ and $H$ is,
in fact, equivalent to ergodicity. To prove this, suppose that $A$ is a
measurable invariant set, and take both $G$ and $H$ equal to $A$. It
follows that $\Pr(A) = (\Pr(A))^2$, and hence that $\Pr(A)$ is either 0
or 1.&lt;/em&gt;&lt;/p&gt;
&lt;h3 id=&#34;comment-2&#34;&gt;Comment 2&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;The Cesaro convergence condition has a natural intuitive
interpretation. We may visualize the transformation $T$ as a particular
way of stirring the contents of a vessel (of total volume 1) full of an
incompressible fluid, which may be thought of as 90 per cent gin ($G$)
and 10 per cent vermouth ($H$). If $H$ is the region originally occupied
by the vermouth, then, for any part $G$ of the vessel, the relative
amount of vermouth in $G$, after $n$ repetitions of the act of stirring,
is given by $\Pr(T^{-n}G \cap H)/\Pr(H)$. The ergodicity of $T$ implies
therefore that on the average this relative amount is exactly equal to
10 per cent. In general, in physical situations like this one, one
expects to be justified in making a much stronger statement, namely
that, after the liquid has been stirred sufficiently often
($n \to \infty$), every part $G$ of the container will contain
approximately 10 per cent vermouth. In mathematical language this
expectation amounts to replacing Cesaro convergence by ordinary
convergence, i.e., to the condition
$\lim_ {n\to \infty} \Pr(T^{-n}G \cap H) = \Pr(G)\Pr(H)$. If a
transformation $T$ satisfies this condition for every pair $G$ and $H$
of measurable sets, it is called mixing, or, in distinction from a
related but slightly weaker concept, strongly mixing.&lt;/em&gt;”&lt;/p&gt;
&lt;h3 id=&#34;mixing&#34;&gt;Mixing&lt;/h3&gt;
&lt;p&gt;Let $\lbrace\Omega, \mathcal{B}, P \rbrace$ be a probability space. Let
$T$ be a probability preserving transform. Then $T$ is &lt;strong&gt;strongly
mixing&lt;/strong&gt; if for every invariant sets $G$,$H \in \mathcal{B}$ $$
P(G \cap T^{-k}H) \to P(G)P(H) \quad \text{ as } k \to \infty
$$ where $T^{-k}H$ is defined as
$T^{-k}H = T^{-1}(&amp;hellip;T^{-1}(T^{-1} H)&amp;hellip;)$ repeated $k$ times.&lt;/p&gt;
&lt;p&gt;Let $\lbrace X_i\rbrace _ {i=-\infty}^{\infty}$ be a two sided sequence
of random variables. Let $\mathcal{B}_ {-\infty}^n$ be the sigma algebra
generated by $\lbrace X_i\rbrace _ {i=-\infty}^{n}$ and
$\mathcal{B}_ {n+k}^\infty$ the sigma algebra generated by
$\lbrace X_i \rbrace _ {i=n+k}^{\infty}$. Define the mixing coefficient
$$
\alpha(k) = \sup_ {n \in \mathbb{Z}} \sup_ {G \in \mathcal{B}_ {-\infty}^n} \sup_ {H \in \mathcal{B}_ {n+k}^\infty} | \Pr(G \cap H) - \Pr(G) \Pr(H)|
$$ $\lbrace X_i \rbrace$ is $\mathbb{\alpha}$&lt;strong&gt;-mixing&lt;/strong&gt; if
$\alpha(k) \to 0$ if $k \to \infty$.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Note that mixing implies ergodicity.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;stationarity&#34;&gt;Stationarity&lt;/h3&gt;
&lt;p&gt;Let $X_i : \Omega \to \mathbb{R}$ be a (two sided) sequence of random
variables with $i \in \mathbb{Z}$. $X_i$ is &lt;strong&gt;strongly stationary&lt;/strong&gt; or
simply stationary if $$
\Pr (X _ {i_ 1} \leq a_ 1 , &amp;hellip; , X _ {i_ k} \leq a_ k ) = \Pr (X _ { i _ {1-s}} \leq a_ 1 , &amp;hellip; , X _ {i _ {k-s}} \leq a_ k)  \quad \text{ for every } i_ 1, &amp;hellip;, i_ k, a_ 1, &amp;hellip;, a_ k, s \in \mathbb{R}.
$$&lt;/p&gt;
&lt;p&gt;Let $X_i : \Omega \to \mathbb{R}$ be a (two sided) sequence of random
variables with $i \in \mathbb{Z}$. $X_i$ is &lt;strong&gt;covariance stationary&lt;/strong&gt; if
$\mathbb{E}[X_i] = \mathbb{E}[X_j]$ for every $i,j$ and
$\mathbb{E}[X_i X_j] = \mathbb{E}[X _ {i+k} X _ {j+k}]$ for all $i,j,k$.
All of the second moments above are assumed to exist.&lt;/p&gt;
&lt;p&gt;Let $X_t : \Omega \to \mathbb{R}$ be a sequence of random variables
indexed by $t \in \mathbb{Z}$ such that $\mathbb{E}[|X_t|] &amp;lt; 1$ for each
$t$. $X_t$ is a &lt;strong&gt;martingale&lt;/strong&gt; if
$\mathbb{E} [X _ t |X _ {t-1} , X _ {t-2} , &amp;hellip;] = X _ t$. $X_t$ is a
&lt;strong&gt;martingale difference&lt;/strong&gt; if
$\mathbb{E} [X _ t | X _ {t-1} , X _ {t-2} ,&amp;hellip;] = 0$.&lt;/p&gt;
&lt;h3 id=&#34;gordins-central-limit-theorem&#34;&gt;Gordin’s Central Limit Theorem&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Let $\lbrace z_i \rbrace$ be a stationary, $\alpha$-mixing sequence of
random variables. If moreover&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\sum_ {m=1}^\infty \alpha(m)^{\frac{\delta}{2 + \delta}} &amp;lt; \infty$&lt;/li&gt;
&lt;li&gt;$\mathbb{E}[z_i] = 0$&lt;/li&gt;
&lt;li&gt;$\mathbb{E}\Big[ ||z_i || ^ {2+\delta} \Big] &amp;lt; \infty$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Then $$
\sqrt{n} \mathbb{E}_n [z_i] \overset{d}{\to} N(0,\Omega) \quad  \text{ where } \quad  \Omega = \lim _ {n \to \infty} Var(\sqrt{n} \mathbb{E}_n [z_i])
$$&lt;/p&gt;
&lt;p&gt;Let $\Omega_k = \mathbb{E}[ z_i z _ {i+k}&#39;]$. Then a necessary condition
for Gordin’s CLT is covariance summability:
$\sum _ {k=1}^\infty \Omega_k &amp;lt; \infty$.&lt;/p&gt;
&lt;h3 id=&#34;ergodic-central-limit-theorem&#34;&gt;Ergodic Central Limit Theorem&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Let $\lbrace z_i \rbrace$ be a stationary, ergodic, martingale
difference sequence. Then $$
\sqrt{n} \mathbb{E}_n [z_i] \overset{d}{\to} N(0,\Omega) \quad  \text{ where } \quad  \Omega = \lim _ {n \to \infty} Var(\sqrt{n}\mathbb{E}_n[z_i])
$$&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Resampling Methods</title>
      <link>https://matteocourthoud.github.io/course/ml-econ/04_crossvalidation/</link>
      <pubDate>Mon, 28 Feb 2022 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/course/ml-econ/04_crossvalidation/</guid>
      <description>&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Remove warnings
import warnings
warnings.filterwarnings(&#39;ignore&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Import
import pandas as pd
import numpy as np
import seaborn as sns
import time

from numpy.linalg import inv
from numpy.random import normal
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import train_test_split, LeaveOneOut, KFold, cross_val_score
from sklearn.preprocessing import PolynomialFeatures
from sklearn.utils import resample
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Import matplotlib for graphs
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import axes3d

# Set global parameters
%matplotlib inline
plt.style.use(&#39;seaborn-white&#39;)
plt.rcParams[&#39;lines.linewidth&#39;] = 3
plt.rcParams[&#39;figure.figsize&#39;] = (10,6)
plt.rcParams[&#39;figure.titlesize&#39;] = 20
plt.rcParams[&#39;axes.titlesize&#39;] = 18
plt.rcParams[&#39;axes.labelsize&#39;] = 14
plt.rcParams[&#39;legend.fontsize&#39;] = 14
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Resampling methods involve repeatedly drawing samples from a training set and refitting a model of interest on each sample in order to obtain additional information about the fitted model.&lt;/p&gt;
&lt;h2 id=&#34;41-cross-validation&#34;&gt;4.1 Cross-Validation&lt;/h2&gt;
&lt;p&gt;Cross-validation can be used to estimate the test error associated with a given statistical learning method in order to evaluate its performance, or to select the appropriate level of flexibility. The process of evaluating a model’s performance is known as &lt;strong&gt;model assessment&lt;/strong&gt;, whereas the process of selecting the proper level of flexibility for a model is known as &lt;strong&gt;model selection&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s use the &lt;code&gt;auto&lt;/code&gt; dataset we have used for nonparametric models.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Load car dataset
df1 = pd.read_csv(&#39;data/Auto.csv&#39;, na_values=&#39;?&#39;).dropna()
df1.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;mpg&lt;/th&gt;
      &lt;th&gt;cylinders&lt;/th&gt;
      &lt;th&gt;displacement&lt;/th&gt;
      &lt;th&gt;horsepower&lt;/th&gt;
      &lt;th&gt;weight&lt;/th&gt;
      &lt;th&gt;acceleration&lt;/th&gt;
      &lt;th&gt;year&lt;/th&gt;
      &lt;th&gt;origin&lt;/th&gt;
      &lt;th&gt;name&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;18.0&lt;/td&gt;
      &lt;td&gt;8&lt;/td&gt;
      &lt;td&gt;307.0&lt;/td&gt;
      &lt;td&gt;130&lt;/td&gt;
      &lt;td&gt;3504&lt;/td&gt;
      &lt;td&gt;12.0&lt;/td&gt;
      &lt;td&gt;70&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;chevrolet chevelle malibu&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;15.0&lt;/td&gt;
      &lt;td&gt;8&lt;/td&gt;
      &lt;td&gt;350.0&lt;/td&gt;
      &lt;td&gt;165&lt;/td&gt;
      &lt;td&gt;3693&lt;/td&gt;
      &lt;td&gt;11.5&lt;/td&gt;
      &lt;td&gt;70&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;buick skylark 320&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;18.0&lt;/td&gt;
      &lt;td&gt;8&lt;/td&gt;
      &lt;td&gt;318.0&lt;/td&gt;
      &lt;td&gt;150&lt;/td&gt;
      &lt;td&gt;3436&lt;/td&gt;
      &lt;td&gt;11.0&lt;/td&gt;
      &lt;td&gt;70&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;plymouth satellite&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;16.0&lt;/td&gt;
      &lt;td&gt;8&lt;/td&gt;
      &lt;td&gt;304.0&lt;/td&gt;
      &lt;td&gt;150&lt;/td&gt;
      &lt;td&gt;3433&lt;/td&gt;
      &lt;td&gt;12.0&lt;/td&gt;
      &lt;td&gt;70&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;amc rebel sst&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;17.0&lt;/td&gt;
      &lt;td&gt;8&lt;/td&gt;
      &lt;td&gt;302.0&lt;/td&gt;
      &lt;td&gt;140&lt;/td&gt;
      &lt;td&gt;3449&lt;/td&gt;
      &lt;td&gt;10.5&lt;/td&gt;
      &lt;td&gt;70&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;ford torino&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;h3 id=&#34;the-validation-set-approach&#34;&gt;The Validation Set Approach&lt;/h3&gt;
&lt;p&gt;Suppose that we would like to estimate the test error associated with fitting a particular statistical learning method on a set of observations. The validation set approach is a very simple strategy for this task. It involves randomly dividing the available set of observations into two parts&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;a &lt;strong&gt;training set&lt;/strong&gt; and&lt;/li&gt;
&lt;li&gt;a &lt;strong&gt;validation set&lt;/strong&gt; or hold-out set&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The model is fit on the training set, and the fitted model is used to predict the responses for the observations in the validation set. The resulting validation set error rate-typically assessed using MSE in the case of a quantitative response—provides an estimate of the test error rate.&lt;/p&gt;
&lt;p&gt;In the following example we are are going to compute the MSE fit polynomial of different order (one to ten). We are going to split the data 50-50 across training and test set.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Cross-validation function for polynomials
def cv_poly(X, y, p_order, r_states, t_prop):
    start = time.time()
    
    # Init scores
    scores = np.zeros((p_order.size,r_states.size))
    
    # Generate 10 random splits of the dataset
    for j in r_states:
        
        # Split sample in train and test
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=t_prop, random_state=j)
        
            
        # For every polynomial degree
        for i in p_order:

            # Generate polynomial
            X_train_poly = PolynomialFeatures(i+1).fit_transform(X_train)
            X_test_poly = PolynomialFeatures(i+1).fit_transform(X_test)

            # Fit regression                                                                    
            ols = LinearRegression().fit(X_train_poly, y_train)
            pred = ols.predict(X_test_poly)
            scores[i,j]= mean_squared_error(y_test, pred)
    print(&#39;Time elapsed: %.4f seconds&#39; % (time.time()-start))
    return scores
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Init
t_prop = 0.5
p_order = np.arange(10)
r_states = np.arange(10)

# Get X,y 
X = df1.horsepower.values.reshape(-1,1)
y = df1.mpg.ravel()

# Compute scores
cv_scores = cv_poly(X, y, p_order, r_states, t_prop)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Time elapsed: 0.0277 seconds
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s test the score for polynomials of different orders.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Figure 5.2
def make_figure_5_2():
    
    # Init
    fig, (ax1, ax2) = plt.subplots(1,2,figsize=(14,6))
    fig.suptitle(&#39;Figure 5.2&#39;)

    # Left plot (first split)
    ax1.plot(p_order+1,cv_scores[:,0], &#39;-o&#39;)
    ax1.set_title(&#39;Random split of the data set&#39;)

    # Right plot (all splits)
    ax2.plot(p_order+1,cv_scores)
    ax2.set_title(&#39;10 random splits of the data set&#39;)

    for ax in fig.axes:
        ax.set_ylabel(&#39;Mean Squared Error&#39;)
        ax.set_ylim(15,30)
        ax.set_xlabel(&#39;Degree of Polynomial&#39;)
        ax.set_xlim(0.5,10.5)
        ax.set_xticks(range(2,11,2));
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;make_figure_5_2()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/04_crossvalidation_15_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;This figure illustrates a &lt;strong&gt;first drawback&lt;/strong&gt; of the validation approach: the estimate of the test error rate can be highly variable, depending on precisely which observations are included in the training set and which observations are included in the validation set.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;second drawback&lt;/strong&gt; of the validation approach is that only a subset of the observations—those that are included in the training set rather than in the validation set—are used to fit the model. Since statistical methods tend to per- form worse when trained on fewer observations, this suggests that the validation set error rate may tend to overestimate the test error rate for the model fit on the entire data set.&lt;/p&gt;
&lt;h3 id=&#34;leave-one-out-cross-validation&#34;&gt;Leave-One-Out Cross-Validation&lt;/h3&gt;
&lt;p&gt;Leave-one-out cross-validation (LOOCV) attempts to address that method’s drawbacks.&lt;/p&gt;
&lt;p&gt;Like the validation set approach, LOOCV involves splitting the set of observations into two parts. However, instead of creating two subsets of comparable size, a single observation $i$ is used for the validation set, and the remaining $n-1$ observations make up the training set. The statistical learning method is fit on the $n−1$ training observations and the MSE is computed using the excluded observation $i$. The procedure is repeated $n$ times, for $i=1,&amp;hellip;,n$.&lt;/p&gt;
&lt;p&gt;The LOOCV estimate for the test MSE is the average of these $n$ test error estimates:&lt;/p&gt;
&lt;p&gt;$$
\mathrm{CV}&lt;em&gt;{(n)}=\frac{1}{n} \sum&lt;/em&gt;{i=1}^{n} \mathrm{MSE}_{i}
$$&lt;/p&gt;
&lt;p&gt;LOOCV has a couple of major &lt;strong&gt;advantages&lt;/strong&gt; over the validation set approach.&lt;/p&gt;
&lt;p&gt;First, it has far less bias. In LOOCV, we repeatedly fit the statistical learning method using training sets that contain $n − 1$ observations, almost as many as are in the entire data set. However, this also means that LOOCV is more computationally intense.&lt;/p&gt;
&lt;p&gt;Second, in contrast to the validation approach which will yield different results when applied repeatedly due to randomness in the training/validation set splits, performing LOOCV multiple times will always yield the same results: there is no randomness in the training/validation set splits.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# LeaveOneOut CV function for polynomials
def loo_cv_poly(X, y, p_order):
    start = time.time()
    
    # Init
    loo = LeaveOneOut().get_n_splits(y)
    loo_scores = np.zeros((p_order.size,1))
    
    # For every polynomial degree
    for i in p_order:
        # Generate polynomial
        X_poly = PolynomialFeatures(i+1).fit_transform(X)

        # Get score
        loo_scores[i] = cross_val_score(LinearRegression(), X_poly, y, cv=loo, scoring=&#39;neg_mean_squared_error&#39;).mean()
        
    print(&#39;Time elapsed: %.4f seconds&#39; % (time.time()-start))
    return loo_scores
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s compare the validation set approach against LOO in terms of computational time.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Validation set approach
cv_scores = cv_poly(X, y, p_order, r_states, t_prop)
    
# Leave One Out CV
loo_scores = loo_cv_poly(X, y, p_order)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Time elapsed: 0.0270 seconds
Time elapsed: 1.1495 seconds
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As expected, LOOCV is much more computationally intense. Even accounting for the fact that we repeat every the validation set approach 10 times.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s now compare them in terms of accuracy in minimizing the MSE.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Make new figure 1
def make_new_figure_1():

    # Init
    fig, ax = plt.subplots(1,1, figsize=(7,6))

    # Left plot
    ax.plot(p_order+1, np.array(loo_scores)*-1, &#39;-o&#39;, label=&#39;LOOCV&#39;)
    ax.plot(p_order+1, np.mean(cv_scores, axis=1), &#39;-o&#39;, c=&#39;orange&#39;, label=&#39;Standard CV&#39;)
    ax.set_ylabel(&#39;Mean Squared Error&#39;); ax.set_xlabel(&#39;Degree of Polynomial&#39;);
    ax.set_ylim(15,30); ax.set_xlim(0.5,10.5);
    ax.set_xticks(range(2,11,2));
    ax.legend();
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;make_new_figure_1()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/04_crossvalidation_26_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;With least squares linear or polynomial regression, an amazing shortcut makes the cost of LOOCV the same as that of a single model fit! The following formula holds:&lt;/p&gt;
&lt;p&gt;$$
\mathrm{CV}&lt;em&gt;{(n)}=\frac{1}{n} \sum&lt;/em&gt;{i=1}^{n}\left(\frac{y_{i}-\hat{y}&lt;em&gt;{i}}{1-h&lt;/em&gt;{i}}\right)^{2}
$$&lt;/p&gt;
&lt;p&gt;where $\hat y_i$ is the $i^{th}$ fitted value from the original least squares fit, and $h_i$ is the leverage of observation $i$.&lt;/p&gt;
&lt;h3 id=&#34;k-fold-cross-validation&#34;&gt;k-Fold Cross-Validation&lt;/h3&gt;
&lt;p&gt;An alternative to LOOCV is k-fold CV. This approach involves the following steps:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Randomly dividing the set of observations into $k$ groups, or folds, of approximately equal size.&lt;/li&gt;
&lt;li&gt;The first fold is treated as a validation set, and the method is fit on the remaining $k − 1$ folds.&lt;/li&gt;
&lt;li&gt;The mean squared error, MSE1, is then computed on the observations in the held-out fold.&lt;/li&gt;
&lt;li&gt;Steps (1)-(3) are repeated $k$ times; each time, a different group of observations is treated as a validation set.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The k-fold CV estimate is computed by averaging these values&lt;/p&gt;
&lt;p&gt;$$
\mathrm{CV}&lt;em&gt;{(k)}=\frac{1}{k} \sum&lt;/em&gt;{i=1}^{k} \mathrm{MSE}_{i}
$$&lt;/p&gt;
&lt;p&gt;LOOCV is a special case of k-fold CV in which $k$ is set to equal $n$. In practice, one typically performs k-fold CV using $k = 5$ or $k = 10$.&lt;/p&gt;
&lt;p&gt;The most obvious &lt;strong&gt;advantage&lt;/strong&gt; is computational. LOOCV requires fitting the statistical learning method $n$ times, while k-fold CV only requires $k$ splits.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# 10fold CV function for polynomials
def k10_cv_poly(X, y, p_order, r_states, folds):
    start = time.time()
    
    # Init
    k10_scores = np.zeros((p_order.size,r_states.size))

    # Generate 10 random splits of the dataset
    for j in r_states:

        # For every polynomial degree
        for i in p_order:

            # Generate polynomial
            X_poly = PolynomialFeatures(i+1).fit_transform(X)

            # Split sample in train and test
            kf10 = KFold(n_splits=folds, shuffle=True, random_state=j)
            k10_scores[i,j] = cross_val_score(LinearRegression(), X_poly, y, cv=kf10, 
                                               scoring=&#39;neg_mean_squared_error&#39;).mean()  
    
    print(&#39;Time elapsed: %.4f seconds&#39; % (time.time()-start))
    return k10_scores
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s now compare 10 fold cross-validation with LOO in terms of computational time.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Leave One Out CV
loo_scores = loo_cv_poly(X, y, p_order)
    
# 10-fold CV
folds = 10
k10_scores = k10_cv_poly(X, y, p_order, r_states, folds)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Time elapsed: 1.1153 seconds
Time elapsed: 0.3078 seconds
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Indeed we see that the LOOCV approach is more computationally intense. Even accounting for the fact that we repeat every 10-fold cross-validation 10 times.&lt;/p&gt;
&lt;p&gt;We can now compare all the methods in terms of accuracy.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Figure 5.4
def make_figure_5_4():

    fig, (ax1, ax2, ax3) = plt.subplots(1,3,figsize=(17,5))
    fig.suptitle(&#39;Figure 5.4&#39;)

    # Left plot
    ax1.plot(p_order+1, np.array(loo_scores)*-1, &#39;-o&#39;)
    ax1.set_title(&#39;LOOCV&#39;, fontsize=12)

    # Center plot
    ax2.plot(p_order+1,k10_scores*-1)
    ax2.set_title(&#39;10-fold CV&#39;, fontsize=12)

    # Right plot
    ax3.plot(p_order+1, np.array(loo_scores)*-1, &#39;-o&#39;, label=&#39;LOOCV&#39;)
    ax3.plot(p_order+1, np.mean(cv_scores, axis=1), label=&#39;Standard CV&#39;)
    ax3.plot(p_order+1,np.mean(k10_scores,axis=1)*-1, label=&#39;10-fold CV&#39;)
    ax3.set_title(&#39;Comparison&#39;, fontsize=12);
    ax3.legend();

    for ax in fig.axes:
        ax.set_ylabel(&#39;Mean Squared Error&#39;)
        ax.set_ylim(15,30)
        ax.set_xlabel(&#39;Degree of Polynomial&#39;)
        ax.set_xlim(0.5,10.5)
        ax.set_xticks(range(2,11,2));
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;make_figure_5_4()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/04_crossvalidation_37_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;10-fold cross-validation outputs a very similar MSE with respect to LOOCV, but with considerably less computational time.&lt;/p&gt;
&lt;h2 id=&#34;42-the-bootstrap&#34;&gt;4.2 The Bootstrap&lt;/h2&gt;
&lt;p&gt;The bootstrap is a widely applicable and extremely powerful statistical tool that can be used to &lt;strong&gt;quantify the uncertainty&lt;/strong&gt; associated with a given estimator or statistical learning method. In the specific case of linear regression, this is not particularly useful since there exist a formula for the standard errors. However, there are many models (almost all actually) for which there exists no closed for solution to the estimator variance.&lt;/p&gt;
&lt;p&gt;In pricinple, we would like to draw independent samples from the true data generating process and assessing the uncertainty of an estimator by comparing its values across the different samples. However, this is clearly unfeasible since we do not know the true data generating process.&lt;/p&gt;
&lt;p&gt;With the bootstrap, rather than repeatedly obtaining independent data sets from the population, we instead obtain distinct data sets by repeatedly sampling observations from the original data set. The power of the bootstrap lies in the fact that it can be easily applied to a wide range of statistical learning methods, including some for which a measure of variability is otherwise difficult to obtain and is not automatically output by statistical software.&lt;/p&gt;
&lt;p&gt;We are now going to assess its usefulness through simulation. Take the following model:&lt;/p&gt;
&lt;p&gt;$$
y_i = \beta_0 \cdot x_i + \varepsilon_i
$$&lt;/p&gt;
&lt;p&gt;where $\beta_0 = 0.6$ and $\varepsilon \sim N(0,1)$. We are now going to assess the variance of the OLS estimator $\hat \beta$ with the standard formula, simulating different samples and with bootstrap.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Set seed
np.random.seed(1)

# Init
simulations = 1000
N = 1000
beta_0 = 0.6
beta_sim = np.zeros((simulations,1))

# Generate X
X = normal(0,3,N).reshape(-1,1)

# Loop over simulations
for i in range(simulations):
    
    # Generate y
    e = normal(0,1,N).reshape(-1,1)
    y = beta_0*X + e
    
    # Estimate beta OLS
    beta_sim[i] = inv(X.T @ X) @ X.T @ y
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Init Bootstrap
beta_boot = np.zeros((simulations,1))

# Loop over simulations
for i in range(simulations):
    
    # Sample y
    X_sample, y_sample = resample(X, y, random_state=i)
    
    # Estimate beta OLS
    beta_boot[i] = inv(X_sample.T @ X_sample) @ X_sample.T @ y_sample
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can first compare the means.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Print means
print(&#39;True value      : %.4f&#39; % beta_0)
print(&#39;Mean Simulations: %.4f&#39; % np.mean(beta_sim))
print(&#39;Mean One Sample : %.4f&#39; % beta_sim[-1])
print(&#39;Mean Boostrap   : %.4f&#39; % np.mean(beta_boot))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;True value      : 0.6000
Mean Simulations: 0.6003
Mean One Sample : 0.5815
Mean Boostrap   : 0.5816
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The mean of the bootstrap estimtor is quite off. But this is not its actual purpose: it is designed to assess the uncertainty of an estimator, not its value.&lt;/p&gt;
&lt;p&gt;Now we compare the variances.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Print variances
print(&#39;True std       : %.6f&#39; % np.sqrt(inv(X.T @ X)))
print(&#39;Std Simulations: %.6f&#39; % np.std(beta_sim))
print(&#39;Std One Sample : %.6f&#39; % np.sqrt(inv(X.T @ X) * np.var(y - beta_sim[-1]*X)))
print(&#39;Std Boostrap   : %.6f&#39; % np.std(beta_boot))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;True std       : 0.010737
Std Simulations: 0.010830
Std One Sample : 0.010536
Std Boostrap   : 0.010812
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Bootstrap gets as close to the true standard deviation of the estimator as the simulation with the true data generating process. Impressive!&lt;/p&gt;
&lt;p&gt;We can now have a visual inspection.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Figure 5.10
def make_figure_5_10():

    fig, (ax1, ax2, ax3) = plt.subplots(1,3,figsize=(14,6))
    fig.suptitle(&#39;Figure 5.10&#39;)

    # Left plot
    ax1.hist(beta_sim, bins=10, edgecolor=&#39;black&#39;);
    ax1.axvline(x=beta_0, color=&#39;r&#39;, label=&#39;beta_0&#39;)
    ax1.set_xlabel(&#39;beta simulated&#39;);

    # Center plot
    ax2.hist(beta_boot, bins=10, color=&#39;orange&#39;, edgecolor=&#39;black&#39;);
    ax2.axvline(x=beta_0, color=&#39;r&#39;, label=&#39;beta_0&#39;)
    ax2.set_xlabel(&#39;beta bootstrap&#39;);

    # Right plot
    df_bootstrap = pd.DataFrame({&#39;simulated&#39;: beta_sim.ravel(), &#39;bootstrap&#39;:beta_boot.ravel()}, 
                                index=range(simulations))
    ax3 = sns.boxplot(data=df_bootstrap, width=0.5, linewidth=2);
    ax3.axhline(y=beta_0, color=&#39;r&#39;, label=&#39;beta_0&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;make_figure_5_10()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/04_crossvalidation_52_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;As we can see, the bootstrap is a powerful tool to assess the uncertainty of an estimator.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Plotting</title>
      <link>https://matteocourthoud.github.io/course/data-science/04_plotting/</link>
      <pubDate>Sun, 27 Feb 2022 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/course/data-science/04_plotting/</guid>
      <description>&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import numpy as np
import pandas as pd

import folium
import geopandas
import contextily
import matplotlib as mpl
import matplotlib.pyplot as plt
import seaborn as sns

from src.import_data import import_data
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For the scope of this tutorial we are going to use AirBnb Scraped data for the city of Bologna. The data is freely available at &lt;strong&gt;Inside AirBnb&lt;/strong&gt;: &lt;a href=&#34;http://insideairbnb.com/get-the-data.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://insideairbnb.com/get-the-data.html&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;A description of all variables in all datasets is avaliable &lt;a href=&#34;https://docs.google.com/spreadsheets/d/1iWCNJcSutYqpULSQHlNyGInUvHg2BoUGoNRIGa6Szc4/edit#gid=982310896&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;We are going to use 2 datasets:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;listing dataset: contains listing-level information&lt;/li&gt;
&lt;li&gt;pricing dataset: contains pricing data, over time&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We import and clean them with a script. If you want more details, have a look at the &lt;a href=&#34;https://matteocourthoud.github.io/course/data-science/01_data_exploration/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;data exploration&lt;/a&gt; and &lt;a href=&#34;https://matteocourthoud.github.io/course/data-science/03_data_wrangling&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;data wrangling&lt;/a&gt; sections.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_listings, df_prices, df = import_data()
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;intro&#34;&gt;Intro&lt;/h2&gt;
&lt;p&gt;The default library for plotting in python is &lt;code&gt;matplotlib&lt;/code&gt;. However, a more modern package that builds on top of it, is &lt;code&gt;seaborn&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;We start by setting the theme and by telling the notebook to display the plots inline.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sns.set()
%matplotlib inline
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can set some global paramters for all plots. You can find a list of all the options &lt;a href=&#34;https://matplotlib.org/stable/tutorials/introductory/customizing.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;. If you want to customize all plots in a project in the samy way, you can create a &lt;code&gt;filename.mplstyle&lt;/code&gt; file and call it at the beginning of each file as &lt;code&gt;plt.style.use(&#39;filename.mplstyle&#39;)&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;mpl.rcParams[&#39;figure.figsize&#39;] = (10,6)
mpl.rcParams[&#39;axes.labelsize&#39;] = 16
mpl.rcParams[&#39;axes.titlesize&#39;] = 18
mpl.rcParams[&#39;axes.titleweight&#39;] = &#39;bold&#39;
mpl.rcParams[&#39;figure.titlesize&#39;] = 18
mpl.rcParams[&#39;figure.titleweight&#39;] = &#39;bold&#39;
mpl.rcParams[&#39;axes.titlepad&#39;] = 20
mpl.rcParams[&#39;legend.facecolor&#39;] = &#39;w&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;distributions&#34;&gt;Distributions&lt;/h2&gt;
&lt;p&gt;Suppose you have a numerical variable and you want to see how it&amp;rsquo;s distributed. The best option is to use an &lt;strong&gt;histogram&lt;/strong&gt;. Seaborn function is &lt;code&gt;sns.histplot&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_listings[&#39;log_price&#39;] = np.log(1+df_listings[&#39;mean_price&#39;])
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sns.histplot(df_listings[&#39;log_price&#39;], bins=50)\
.set(title=&#39;Distribution of log-prices&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/04_plotting_13_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;We can add a smooth kernel density approximation with the &lt;code&gt;kde&lt;/code&gt; option.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sns.histplot(df_listings[&#39;log_price&#39;], bins=50, kde=True)\
.set(title=&#39;Distribution of log-prices with density&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/04_plotting_15_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;If we have a categorical variable, we might want to plot the distribution of the data across its values. We can use a &lt;strong&gt;barplot&lt;/strong&gt;. Seaborn function is &lt;code&gt;sns.countplot()&lt;/code&gt; for count data.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sns.countplot(x=&amp;quot;neighborhood&amp;quot;, data=df_listings)\
.set(title=&#39;Number of observations by neighborhood&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/04_plotting_17_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;If instead we want to see the distribution of another variable across some group, we can use the &lt;code&gt;sns.barplot()&lt;/code&gt; function.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sns.barplot(x=&amp;quot;neighborhood&amp;quot;, y=&amp;quot;mean_price&amp;quot;, data=df_listings)\
.set(title=&#39;Average price by neighborhood&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/04_plotting_19_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;We can also use other metrics besides the mean with the &lt;code&gt;estimator&lt;/code&gt; option.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sns.barplot(x=&amp;quot;neighborhood&amp;quot;, y=&amp;quot;mean_price&amp;quot;, data=df_listings, estimator=np.median)\
.set(title=&#39;Median price by neighborhood&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/04_plotting_21_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;We can also plot the full distribution using, for example &lt;strong&gt;boxplots&lt;/strong&gt; with &lt;code&gt;sns.boxplot()&lt;/code&gt;. Boxplots display quartiles and outliers.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sns.boxplot(x=&amp;quot;neighborhood&amp;quot;, y=&amp;quot;log_price&amp;quot;, data=df_listings)\
.set(title=&#39;Price distribution across neighborhoods&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/04_plotting_23_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;If we want to see the full distribution, we can use the &lt;code&gt;sns.violinplot()&lt;/code&gt; function.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sns.violinplot(x=&amp;quot;neighborhood&amp;quot;, y=&amp;quot;log_price&amp;quot;, data=df_listings)\
.set(title=&#39;Price distribution across neighborhoods&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/04_plotting_25_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;time-series&#34;&gt;Time Series&lt;/h2&gt;
&lt;p&gt;If the dataset has a time dimension, we might want to explore how a variable evolves over time. Seaborn function is &lt;code&gt;sns.lineplot()&lt;/code&gt;. If the data has multiple observations for each time period, it will also display a 95% confidence interval around the mean.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sns.lineplot(data=df, x=&#39;date&#39;, y=&#39;price&#39;)\
.set(title=&amp;quot;Price distribution over time&amp;quot;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/04_plotting_28_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;We can do the samy by group, with the &lt;code&gt;hue&lt;/code&gt; option. We can suppress confidence intervals setting &lt;code&gt;ci=None&lt;/code&gt; (making the code much faster).&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sns.lineplot(data=df, x=&#39;date&#39;, y=&#39;price&#39;, hue=&#39;neighborhood&#39;, ci=None)\
.set(title=&amp;quot;Price distribution over time&amp;quot;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/04_plotting_30_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;correlations&#34;&gt;Correlations&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_listings[&amp;quot;log_reviews&amp;quot;] = np.log(1 + df_listings[&amp;quot;number_of_reviews&amp;quot;])
df_listings[&amp;quot;log_rpm&amp;quot;] = np.log(1 + df_listings[&amp;quot;reviews_per_month&amp;quot;])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The most intuitive way to plot a correlation between two variables is a &lt;strong&gt;scatterplot&lt;/strong&gt;. Seaborn function is &lt;code&gt;sns.scatterplot()&lt;/code&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sns.scatterplot(data=df_listings, x=&amp;quot;log_rpm&amp;quot;, y=&amp;quot;log_price&amp;quot;, alpha=0.3)\
.set(title=&#39;Prices and Ratings&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/04_plotting_34_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;As usual, we can split the data by group with the &lt;code&gt;hue&lt;/code&gt; option.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sns.scatterplot(data=df_listings, x=&amp;quot;log_rpm&amp;quot;, y=&amp;quot;log_price&amp;quot;, 
                hue=&amp;quot;room_type&amp;quot;, alpha=0.3)\
.set(title=&amp;quot;Prices and Reviews, by room type&amp;quot;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/04_plotting_36_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;We can also add the marginal distributions using the &lt;code&gt;sns.jointplot()&lt;/code&gt; function.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sns.jointplot(data=df_listings, x=&amp;quot;log_rpm&amp;quot;, y=&amp;quot;log_price&amp;quot;, kind=&amp;quot;hex&amp;quot;)\
.fig.suptitle(&amp;quot;Prices and Reviews, with marginals&amp;quot;)  
plt.subplots_adjust(top=0.9);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/04_plotting_38_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;If we want to plot correlations (and marginals) of multiple variables, we can use the &lt;code&gt;sns.pairplot()&lt;/code&gt; function.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sns.pairplot(data=df_listings,
             vars=[&amp;quot;log_rpm&amp;quot;, &amp;quot;log_reviews&amp;quot;, &amp;quot;log_price&amp;quot;],
             plot_kws={&#39;s&#39;:2})\
.fig.suptitle(&amp;quot;Correlations&amp;quot;);
plt.subplots_adjust(top=0.9)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/04_plotting_40_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;We can distinguish across groups with the &lt;code&gt;hue&lt;/code&gt; option.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sns.pairplot(data=df_listings,
             vars=[&amp;quot;log_rpm&amp;quot;, &amp;quot;log_reviews&amp;quot;, &amp;quot;log_price&amp;quot;],
             kind=&amp;quot;reg&amp;quot;,
             hue=&#39;room_type&#39;,
             plot_kws={&#39;scatter_kws&#39;:{&#39;s&#39;:.1}})\
.fig.suptitle(&amp;quot;Correlations, by room type&amp;quot;);
plt.subplots_adjust(top=0.9)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/04_plotting_42_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;If we want to plot all the correlations in the data, we can use the &lt;code&gt;sns.relplot()&lt;/code&gt; function together with a correlation matrix.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Compute a correlation matrix and convert to long-form
corr_mat = df.corr().stack().reset_index(name=&amp;quot;correlation&amp;quot;)

# Draw each cell as a scatter point with varying size and color
g = sns.relplot(data=corr_mat,
            x=&amp;quot;level_0&amp;quot;, y=&amp;quot;level_1&amp;quot;, hue=&amp;quot;correlation&amp;quot;, size=&amp;quot;correlation&amp;quot;,
            palette=&amp;quot;vlag&amp;quot;, hue_norm=(-1, 1), edgecolor=&amp;quot;.7&amp;quot;,
            height=10, sizes=(50, 250), size_norm=(-.2, .8));

# Extra cleaning
for label in g.ax.get_xticklabels():
    label.set_rotation(90)
g.set(xlabel=&amp;quot;&amp;quot;, ylabel=&amp;quot;&amp;quot;, aspect=&amp;quot;equal&amp;quot;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/04_plotting_44_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;For univariate linear regression, we can display data, best fit and uncertainty in the same plot using &lt;code&gt;sns.regplot()&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sns.regplot(x=&amp;quot;log_rpm&amp;quot;, y=&amp;quot;log_price&amp;quot;, data=df_listings,
            scatter_kws={&#39;s&#39;:5, &#39;alpha&#39;:.1})\
.set(title=&#39;Price Elasticity to Reviews&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/04_plotting_46_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;geographical-data&#34;&gt;Geographical data&lt;/h2&gt;
&lt;p&gt;We can in principle plot geographical data as a simple scatterplot.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sns.scatterplot(data=df_listings, x=&amp;quot;longitude&amp;quot;, y=&amp;quot;latitude&amp;quot;)\
.set(title=&#39;Listing coordinates&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/04_plotting_49_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;However, we can do better and do the scatterplot over a map layer.&lt;/p&gt;
&lt;p&gt;First, we neeed to convert the &lt;code&gt;latitude&lt;/code&gt; and &lt;code&gt;longitude&lt;/code&gt; variables into coordinates. We use the library &lt;code&gt;geopandas&lt;/code&gt;. Note that the original coordinate system is &lt;code&gt;4326&lt;/code&gt; (3D) and we need to &lt;code&gt;3857&lt;/code&gt; (2D).&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;geom = geopandas.points_from_xy(df_listings.longitude, df_listings.latitude)
gdf = geopandas.GeoDataFrame(
    df_listings, 
    geometry=geom,
    crs=4326).to_crs(3857)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We import a map of Bologna using the library &lt;code&gt;contextily&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;bologna = contextily.Place(&amp;quot;Bologna&amp;quot;, source=contextily.providers.Stamen.TonerLite)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We are now ready to plot it with the airbnb listings.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;ax = bologna.plot()
gdf.plot(ax=ax, c=df_listings[&#39;mean_price&#39;], cmap=&#39;viridis&#39;, alpha=0.8);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/04_plotting_55_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Inference</title>
      <link>https://matteocourthoud.github.io/course/metrics/04_inference/</link>
      <pubDate>Fri, 29 Oct 2021 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/course/metrics/04_inference/</guid>
      <description>&lt;h2 id=&#34;statistical-models&#34;&gt;Statistical Models&lt;/h2&gt;
&lt;h3 id=&#34;definition&#34;&gt;Definition&lt;/h3&gt;
&lt;p&gt;A &lt;strong&gt;statistical model&lt;/strong&gt; is a set of probability distributions
$\lbrace P \rbrace$.&lt;/p&gt;
&lt;p&gt;More precisely, a &lt;strong&gt;statistical model over data&lt;/strong&gt; $D \in \mathcal{D}$ is
a set of probability distribution over datasets $D$ which takes values
in $\mathcal{D}$.&lt;/p&gt;
&lt;p&gt;Suppose you have regression data $\lbrace x_i , y_i \rbrace _ {i=1}^N$
with $x_i \in \mathbb{R}^p$ and $y_i \in \mathbb{R}$. The statistical
model is&lt;/p&gt;
&lt;p&gt;$$
\Big\lbrace   P : y_i = f(x_i) + \varepsilon_i, \ x_i \sim F_x , \ \varepsilon_i \sim F _\varepsilon , \ \varepsilon_i \perp x_i , \ f \in C^2 (\mathbb{R}^p) \Big\rbrace
$$&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;In words&lt;/strong&gt;: the statistical model is the set of distributions $P$
such that an additive decomposition of $y_i$ as
$f(x_i) + \varepsilon_i$ exists for some $x_i$; where $f$ is twice
continuously differentiable.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;A &lt;strong&gt;data generating process&lt;/strong&gt; (DGP) is a single statistical distribution
over&lt;/p&gt;
&lt;h3 id=&#34;parametrization&#34;&gt;Parametrization&lt;/h3&gt;
&lt;p&gt;A statistical model parameterized by $\theta \in \Theta$ is &lt;strong&gt;well
specified&lt;/strong&gt; if the data generating process corresponds to some
$\theta_0$ and $\theta_0 \in \Theta$. Otherwise, the statistical model
is &lt;strong&gt;misspecified&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;A statistical model can be parametrized as
$\mathcal{F} = \lbrace P_\theta \rbrace _ {\lbrace \theta \in \Theta \rbrace }$.&lt;/p&gt;
&lt;p&gt;We can divide statistical models into 3 classes&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;h2 id=&#34;parametric-the-stochastic-features-of-the-model-are-completly-specified-up-to-a-finite-dimensional-parameter-lbrace-p_theta-rbrace-_--lbrace-theta-in-theta-rbrace--with-theta-subseteq-mathbbrk-kinfty&#34;&gt;&lt;strong&gt;Parametric&lt;/strong&gt;: the stochastic features of the model are completly specified up to a finite dimensional parameter: $\lbrace P_\theta \rbrace _ { \lbrace \theta \in \Theta \rbrace }$ with $\Theta \subseteq \mathbb{R}^k, k&amp;lt;\infty$;&lt;/h2&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Semiparametric&lt;/strong&gt;: it is a partially specified model, e.g.,
$\lbrace P_\theta \rbrace _ { \lbrace \theta \in \Theta, \gamma \in \Gamma \rbrace }$
with $\Theta$ of finite dimension and $\Gamma$ of infinite
dimension;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Non parametric&lt;/strong&gt;: there is no finite dimensional component of the
model.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;estimation&#34;&gt;Estimation&lt;/h3&gt;
&lt;p&gt;Let $\mathcal{D}$ be the set of possible data realizations. Let
$D \in \mathcal{D}$ be your data. Let $\mathcal{F}$ be a statistical
model indexed by some parameter $\theta \in \Theta$. An &lt;strong&gt;estimator&lt;/strong&gt; is
a map $$
\mathcal{D} \to \mathcal{F} \quad , \quad  D \mapsto \hat{\theta}
$$&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;In words&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;An estimator is a map from the set of data realizations to the set
of statistical models.&lt;/li&gt;
&lt;li&gt;It takes as inputs a dataset $D$ and outputs a parameter estimate
$\hat \theta$.&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;inference&#34;&gt;Inference&lt;/h3&gt;
&lt;p&gt;Let $\alpha &amp;gt; 0$ be a small tolerance. Statistical &lt;strong&gt;inference&lt;/strong&gt; is a
map into subsets of $\mathcal{F}$ given by $$
\mathcal{D} \to \mathcal{G} \subseteq \mathcal{F}: \min _ \theta P_\theta (\mathcal{G} | \theta \in \mathcal{G}) \geq 1-\alpha
$$&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;In words&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Inference maps datasets into sets of models&lt;/li&gt;
&lt;li&gt;The set contains only models that generate the observed data with
high probability&lt;/li&gt;
&lt;li&gt;I.e. at least $1-\alpha$&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;hypotesis-testing&#34;&gt;Hypotesis Testing&lt;/h2&gt;
&lt;h3 id=&#34;hypothesis&#34;&gt;Hypothesis&lt;/h3&gt;
&lt;p&gt;A &lt;strong&gt;statistical hypothesis&lt;/strong&gt; $H_0$, is a subset of a statistical model,
$\mathcal K \subset \mathcal F$.&lt;/p&gt;
&lt;p&gt;If $\mathcal F$ is the statistical model and $\mathcal K$ is the
statistical hypothesis, we use the notation $H_0 : P \in \mathcal K$.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Example&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Common hypothesis are&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A single coefficient being equal to zero,
$\beta_k = c \in \mathbb R$&lt;/li&gt;
&lt;li&gt;Multiple linear combination of coefficients being equal to some
values: $\boldsymbol R&#39; \beta = r \in \mathbb R^p$&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;test&#34;&gt;Test&lt;/h3&gt;
&lt;p&gt;A &lt;strong&gt;hypothesis test&lt;/strong&gt; $T$ is a map from the space of datasets to a
decision, rejection (0) or acceptance (1) $$
\mathcal D \to \lbrace 0, 1 \rbrace \quad, \quad D \mapsto T
$$&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Generally, we are interested in understanding whether it is likely
that data $D$ are drawn from a model $\mathcal K$ or not.&lt;/p&gt;
&lt;p&gt;A hypothesis test, $T$ is our tool for deciding whether the hypothesis
is consistent with the data.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$T(D) = 0 \to$ fail to reject $H_0$ and test inconclusive&lt;/li&gt;
&lt;li&gt;$T (D) = 1 \to$ reject $H_0$ and D is inconsistent with any
$P \in \mathcal K$&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;errors&#34;&gt;Errors&lt;/h3&gt;
&lt;p&gt;Let $\mathcal K \subset \mathcal F$ be a statistical hypothesis and $T$
a hypothesis test.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;A &lt;strong&gt;Type I error&lt;/strong&gt; is an event $T(D)=1$ under $P \in \mathcal K$.
&lt;ul&gt;
&lt;li&gt;In words: rejecting the null hypothesis, when it is is true&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;A &lt;strong&gt;Type II error&lt;/strong&gt; is an event $T(D)=0$ under $P \in \mathcal K^C$.
&lt;ul&gt;
&lt;li&gt;In words: not rejecting the null hypothesis, when it is false&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The corresponding probability of a type I error is called &lt;strong&gt;size&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;The corresponding probability of a type II error is called &lt;strong&gt;power&lt;/strong&gt;
(against the alternative P).&lt;/p&gt;
&lt;h3 id=&#34;type-i-error-and-test-size&#34;&gt;Type I Error and Test Size&lt;/h3&gt;
&lt;p&gt;Test &lt;strong&gt;size&lt;/strong&gt; is the probability of a Type I error, i.e. $$
\Pr \Big[ \text{ Reject } H_0 \Big| H_0 \text{ is true } \Big] = \Pr \Big[ T(D)=1 \Big| P \in \mathcal K \Big]
$$ A primary goal of test construction is to limit the incidence of Type
I error by bounding the size of the test.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;In the dominant approach to hypothesis testing the researcher
pre-selects a &lt;strong&gt;significance level&lt;/strong&gt; $\alpha \in (0,1)$ and then
selects the test so that its size is no larger than $\alpha$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;type-ii-error-and-power&#34;&gt;Type II Error and Power&lt;/h3&gt;
&lt;p&gt;Test &lt;strong&gt;power&lt;/strong&gt; is the probability of a Type II error, i.e. $$
\Pr \Big[ \text{ Not Reject } H_0 \Big| H_0 \text{ is false } \Big] = \Pr \Big[ T(D)=0 \Big| P \in \mathcal K^C \Big]
$$&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;In the dominant approach to hypothesis testing the goal of test
construction is to have high power subject to the constraint that the
size of the test is lower than the pre-specified significance level.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;statistical-significance&#34;&gt;Statistical Significance&lt;/h3&gt;
&lt;p&gt;TBD&lt;/p&gt;
&lt;h3 id=&#34;p-values&#34;&gt;P-Values&lt;/h3&gt;
&lt;h3 id=&#34;recap&#34;&gt;Recap&lt;/h3&gt;
&lt;p&gt;We now summarize the main features of hypothesis testing.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Select a significance level $\alpha$.&lt;/li&gt;
&lt;li&gt;Select a test statistic $T$ with asymptotic distribution $T\to \xi$
under $H_0$.&lt;/li&gt;
&lt;li&gt;Set the asymptotic critical value $c$ so that 1−&lt;em&gt;G&lt;/em&gt;(&lt;em&gt;c&lt;/em&gt;)=α, where
&lt;em&gt;G&lt;/em&gt; is the distribution function of $\xi$.&lt;/li&gt;
&lt;li&gt;Calculate the asymptotic p-value &lt;em&gt;p&lt;/em&gt;=1−&lt;em&gt;G&lt;/em&gt;(&lt;em&gt;T&lt;/em&gt;).&lt;/li&gt;
&lt;li&gt;Reject $H_0$ if &lt;em&gt;T&lt;/em&gt; &amp;gt; &lt;em&gt;c&lt;/em&gt;, or equivalently &lt;em&gt;p&lt;/em&gt; &amp;lt; α.&lt;/li&gt;
&lt;li&gt;Accept $H_0$ if &lt;em&gt;T&lt;/em&gt; ≤ &lt;em&gt;c&lt;/em&gt;, or equivalently &lt;em&gt;p&lt;/em&gt; ≥ α.&lt;/li&gt;
&lt;li&gt;Report $p$ to summarize the evidence concerning $H_0$ versus $H_1$.&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;examples&#34;&gt;Examples&lt;/h3&gt;
&lt;p&gt;Let’s focus two hypotheses:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;$\beta_k = c \in \mathbb R$&lt;/li&gt;
&lt;li&gt;$\boldsymbol R&#39; \beta = r \in \mathbb R^p$&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;t-test-with-known-variance&#34;&gt;t-test with Known Variance&lt;/h3&gt;
&lt;p&gt;Consider the testing problem $H_0 : \beta_k = c$, where $c$ is a
pre-specified value under the null. Suppose the variance of the esimator
$\hat \beta_k$ is &lt;strong&gt;known&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;The t-statistic for this problem is defined by $$
n_{k}:=\frac{\hat \beta_{k} - c}{\sigma_{\hat \beta_{k}}}
$$ In the testing procedure above, the sampling distribution under the
null $H_0$ is given by $$
n_k \sim N(0,1)
$$ Where $N(0,1)$ the standard normal distribution.&lt;/p&gt;
&lt;h3 id=&#34;t-test-with-unknown-variance&#34;&gt;t-test with Unknown Variance&lt;/h3&gt;
&lt;p&gt;Consider the testing problem $H_0 : \beta_k = c$, where $c$ is a
presepecified value under the null. In case the variance of the
estimator $\hat \beta_k$ is &lt;strong&gt;not known&lt;/strong&gt;, we have to replace it with a
consistent estimate $\hat \sigma^2_{\hat \beta}$&lt;/p&gt;
&lt;p&gt;The t-statistic for this problem is defined by $$
t_{k}:=\frac{\hat \beta_{k} - c}{\hat \sigma_{\hat \beta_{k}}}
$$ In the testing procedure above, the sampling distribution under the
null $H_0$ is given by $$
t_k \sim t_{n-K}
$$ Where $t_{n-K}$ denotes the t-distribution with $n-K$ degress of
freedom.&lt;/p&gt;
&lt;h3 id=&#34;wald-test&#34;&gt;Wald-test&lt;/h3&gt;
&lt;p&gt;Consider the testing problem $\boldsymbol R&#39; \beta = r$, where
$\boldsymbol R \in \mathbb R^{p+K}$ is a pre-specified set of linear
combinations and $r \in \mathbb R^p$ is a restriction vector. Suppose
the variance of the esimator $\hat \beta$ is &lt;strong&gt;known&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;The Wald statistic for this problem is given by $$
W := \frac{(R \hat \beta-r)^{\prime}(R \hat \beta-r) }{R&#39; \sigma^{2}&lt;em&gt;{\hat \beta} R}
$$ In the testing procedure above, the sampling distribution under the
null $H_0$ is given by $$
W \sim \chi^2&lt;/em&gt;{n-K}
$$ Where $\chi^2_{n-K}$ denotes the chi-squared distribution with $n-K$
degress of freedom.&lt;/p&gt;
&lt;h3 id=&#34;comments-on-the-wald-test&#34;&gt;Comments on the Wald test&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;The Wald statistic $W$ is a weighted Euclidean measure of the length
of the vector $R \hat \beta-r$&lt;/li&gt;
&lt;li&gt;The Wald test is intrinsecally 2-sided&lt;/li&gt;
&lt;li&gt;When $p=1$ then $W = |T|$ , the square of the t-statistic, so
hypothesis tests based on $W$ and $|T|$ are equivalent.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;f-test&#34;&gt;F-test&lt;/h3&gt;
&lt;p&gt;Consider the testing problem $\boldsymbol R&#39; \beta = r$, where
$\boldsymbol R \in \mathbb R^{p+K}$ is a pre-specified set of linear
combinations and $r \in \mathbb R^p$ is a restriction vector. In case
the variance of the estimator $\hat \beta$ is &lt;strong&gt;not known&lt;/strong&gt;, we have to
replace it with a consistent estimate $\hat \sigma^2_{\hat \beta}$.&lt;/p&gt;
&lt;p&gt;The F-statistic for this problem is given by $$
F := \frac{(R \hat \beta-r)^{\prime}(R \hat \beta-r) / p }{R&#39; \hat \sigma^{2} _ {\hat \beta} R}
$$ In the testing procedure above, the sampling distribution under the
null $H_0$ is given by $$
F \sim F_{p, n-K}
$$ Where $F_{p, n-K}$ denotes the F-distribution with $n-K$ degress of
freedom, with $p$ restrictions.&lt;/p&gt;
&lt;h3 id=&#34;f-test-equivalence&#34;&gt;F-test Equivalence&lt;/h3&gt;
&lt;p&gt;Consider the testing problem $\boldsymbol R&#39; \beta = r$, where
$\boldsymbol R \in \mathbb R^{p+K}$ is a pre-specified set of linear
combinations and $r \in \mathbb R^p$ is a restriction vector. Consider
two estimators&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\hat \beta_U = \arg \min_b \frac{1}{n} (y - X \beta)&#39; (y - X\beta)$&lt;/li&gt;
&lt;li&gt;$\hat \beta_R = \arg \min_{b : \boldsymbol R&#39; \beta = r} \frac{1}{n} (y - X \beta)&#39; (y - X\beta)$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Then the F statistic is numerically equivalent to the following
expression $$
F = \frac{\left(S S R_{R}-S S R_{U}\right) / p}{S S R_{U} /(n-K)}
$$ where SSR is the sum of squared residuals.&lt;/p&gt;
&lt;h3 id=&#34;confidence-intervals&#34;&gt;Confidence Intervals&lt;/h3&gt;
&lt;p&gt;TBD&lt;/p&gt;
&lt;h3 id=&#34;minimum-distance-tests&#34;&gt;Minimum Distance Tests&lt;/h3&gt;
&lt;p&gt;TBD&lt;/p&gt;
&lt;h2 id=&#34;asymptotics&#34;&gt;Asymptotics&lt;/h2&gt;
&lt;h3 id=&#34;estimator-properties&#34;&gt;Estimator Properties&lt;/h3&gt;
&lt;p&gt;Given a sequence of well specified data generating processes
$\mathcal F_n$, each indexed by the same parameter space $\Theta$, with
$\theta_0$ a component of the true parameter for each $n$.&lt;/p&gt;
&lt;p&gt;Then estimator $\hat \theta$ is&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;unbiased&lt;/strong&gt; if $\mathbb E [\hat \theta] = \theta_0$&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;consistent&lt;/strong&gt; if $\hat \theta \overset{p}{\to} \theta_0$&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;asymptotically normal&lt;/strong&gt;
$\sqrt{n} (\hat \theta - \theta_0) \overset{d}{\to} N(0, V)$ for
some positive definite $V$&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;test-consistency&#34;&gt;Test Consistency&lt;/h3&gt;
&lt;p&gt;The &lt;strong&gt;asymptotic size&lt;/strong&gt; of a testing procedure is defined as the
limiting probability of rejecting $H_0$ when $H_0$ is true.
Mathematically, we can write this as
$\lim _ {n \to \infty} \Pr_n ( \text{reject } H_0 | H_0)$, where the $n$
subscript indexes the sample size.&lt;/p&gt;
&lt;p&gt;A test is said to be &lt;strong&gt;consistent&lt;/strong&gt; against the alternative $H_1$ if the
null hypothesis is rejected with probability approaching $1$ when $H_1$
is true:
$\lim _ {N \to \infty} \Pr_N (\text{reject } H_0 | H_1) \overset{p}{\to} 1$.&lt;/p&gt;
&lt;h3 id=&#34;convergence&#34;&gt;Convergence&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt;: Suppose that
$\sqrt{n}(\hat{\theta} - \theta_0) \overset{d}{\to} N(0, V)$, where $V$
is positive definite. Then for any non-stochastic $Q\times P$ matrix
$R$, $Q \leq P$, with rank$( R ) = Q$ $$
\sqrt{n} R (\hat{\theta} - \theta_0) \sim N(0, R VR&#39;)
$$ and $$
[\sqrt{n}R(\hat{\theta} - \theta_0)]&#39;[RVR&#39;]^{-1}[\sqrt{n}R(\hat{\theta} - \theta_0)] \overset{d}{\to} \chi^2_Q
$$ In addition, if $\text{plim} \hat{V} _n = V$, then $$
(\hat{\theta} - \theta_0)&#39; R&#39;[R (\hat{V} _n/n) R&#39;]^{-1}R (\hat{\theta} - \theta_0) \overset{d}{\to} \chi^2_Q
$$&lt;/p&gt;
&lt;h3 id=&#34;wald-statistic&#34;&gt;Wald Statistic&lt;/h3&gt;
&lt;p&gt;For testing the null hypothesis $H_0: R\theta_0 = r$, where $r$ is a
$Q\times1$ random vector, define the &lt;strong&gt;Wald statistic&lt;/strong&gt; for testing
$H_0$ against $H_1 : R\theta_0 \neq r$ as $$
W_n = (R\hat{\theta} - r)&#39;[R (\hat{V} _n/n) R&#39;]^{-1} (R\hat{\theta} - r)
$$ Under $H_0$, $W_n \overset{d}{\to} \chi^2_Q$. If we abuse the
asymptotics and we treat $\hat{\theta}$ as being distributed as Normal
we get the equation exactly.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Model Selection and Regularization</title>
      <link>https://matteocourthoud.github.io/course/ml-econ/05_regularization/</link>
      <pubDate>Mon, 28 Feb 2022 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/course/ml-econ/05_regularization/</guid>
      <description>&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Remove warnings
import warnings
warnings.filterwarnings(&#39;ignore&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Import
import pandas as pd
import numpy as np
import time
import itertools
import statsmodels.api as sm
import seaborn as sns

from numpy.random import normal, uniform
from itertools import combinations
from statsmodels.api import add_constant
from statsmodels.regression.linear_model import OLS
from sklearn.linear_model import LinearRegression, Ridge, Lasso, RidgeCV, LassoCV
from sklearn.cross_decomposition import PLSRegression, PLSSVD
from sklearn.model_selection import KFold, cross_val_score, train_test_split, LeaveOneOut, ShuffleSplit
from sklearn.preprocessing import scale
from sklearn.decomposition import PCA
from sklearn.metrics import mean_squared_error
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Import matplotlib for graphs
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import axes3d

# Set global parameters
%matplotlib inline
plt.style.use(&#39;seaborn-white&#39;)
plt.rcParams[&#39;lines.linewidth&#39;] = 3
plt.rcParams[&#39;figure.figsize&#39;] = (12,5)
plt.rcParams[&#39;figure.titlesize&#39;] = 20
plt.rcParams[&#39;axes.titlesize&#39;] = 18
plt.rcParams[&#39;axes.labelsize&#39;] = 14
plt.rcParams[&#39;legend.fontsize&#39;] = 14
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;When we talk about big data, we do not only talk about bigger sample size, $n$, but also about a larger number of explanatory variables, $p$. However, with ordinary least squares, we are limited by the identification constraint that $p &amp;lt; n$. Moreover, for inference and prediction accuracy, we would actually like to have $k &amp;laquo; n$.&lt;/p&gt;
&lt;p&gt;This session adresses methods to use a least squares fit in a setting in which the number of regressors, $p$, is large with respect to the sample size, $n$&lt;/p&gt;
&lt;h2 id=&#34;51-subset-selection&#34;&gt;5.1 Subset Selection&lt;/h2&gt;
&lt;p&gt;The Subset Selection approach involves identifying a subset of the $p$ predictors that we believe to be related to the response. We then fit a model using least squares on the reduced set of variables.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s load the &lt;code&gt;credit rating&lt;/code&gt; dataset.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Credit ratings dataset
credit = pd.read_csv(&#39;data/Credit.csv&#39;, usecols=list(range(1,12)))
credit.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;Income&lt;/th&gt;
      &lt;th&gt;Limit&lt;/th&gt;
      &lt;th&gt;Rating&lt;/th&gt;
      &lt;th&gt;Cards&lt;/th&gt;
      &lt;th&gt;Age&lt;/th&gt;
      &lt;th&gt;Education&lt;/th&gt;
      &lt;th&gt;Gender&lt;/th&gt;
      &lt;th&gt;Student&lt;/th&gt;
      &lt;th&gt;Married&lt;/th&gt;
      &lt;th&gt;Ethnicity&lt;/th&gt;
      &lt;th&gt;Balance&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;14.891&lt;/td&gt;
      &lt;td&gt;3606&lt;/td&gt;
      &lt;td&gt;283&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;34&lt;/td&gt;
      &lt;td&gt;11&lt;/td&gt;
      &lt;td&gt;Male&lt;/td&gt;
      &lt;td&gt;No&lt;/td&gt;
      &lt;td&gt;Yes&lt;/td&gt;
      &lt;td&gt;Caucasian&lt;/td&gt;
      &lt;td&gt;333&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;106.025&lt;/td&gt;
      &lt;td&gt;6645&lt;/td&gt;
      &lt;td&gt;483&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;82&lt;/td&gt;
      &lt;td&gt;15&lt;/td&gt;
      &lt;td&gt;Female&lt;/td&gt;
      &lt;td&gt;Yes&lt;/td&gt;
      &lt;td&gt;Yes&lt;/td&gt;
      &lt;td&gt;Asian&lt;/td&gt;
      &lt;td&gt;903&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;104.593&lt;/td&gt;
      &lt;td&gt;7075&lt;/td&gt;
      &lt;td&gt;514&lt;/td&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;71&lt;/td&gt;
      &lt;td&gt;11&lt;/td&gt;
      &lt;td&gt;Male&lt;/td&gt;
      &lt;td&gt;No&lt;/td&gt;
      &lt;td&gt;No&lt;/td&gt;
      &lt;td&gt;Asian&lt;/td&gt;
      &lt;td&gt;580&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;148.924&lt;/td&gt;
      &lt;td&gt;9504&lt;/td&gt;
      &lt;td&gt;681&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;36&lt;/td&gt;
      &lt;td&gt;11&lt;/td&gt;
      &lt;td&gt;Female&lt;/td&gt;
      &lt;td&gt;No&lt;/td&gt;
      &lt;td&gt;No&lt;/td&gt;
      &lt;td&gt;Asian&lt;/td&gt;
      &lt;td&gt;964&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;55.882&lt;/td&gt;
      &lt;td&gt;4897&lt;/td&gt;
      &lt;td&gt;357&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;68&lt;/td&gt;
      &lt;td&gt;16&lt;/td&gt;
      &lt;td&gt;Male&lt;/td&gt;
      &lt;td&gt;No&lt;/td&gt;
      &lt;td&gt;Yes&lt;/td&gt;
      &lt;td&gt;Caucasian&lt;/td&gt;
      &lt;td&gt;331&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;We are going to look at the relationship between individual characteristics and account &lt;code&gt;Balance&lt;/code&gt; in the &lt;code&gt;Credit&lt;/code&gt; dataset.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# X and y
X = credit.loc[:, credit.columns != &#39;Balance&#39;]
y = credit.loc[:,&#39;Balance&#39;]
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;best-subset-selection&#34;&gt;Best Subset Selection&lt;/h3&gt;
&lt;p&gt;To perform best subset selection, we fit a separate least squares regression for each possible combination of the $p$ predictors. That is, we fit all $p$ models that contain exactly one predictor, all $p = p(p−1)/2$ models that contain 2 exactly two predictors, and so forth. We then look at all of the resulting models, with the goal of identifying the one that is best.&lt;/p&gt;
&lt;p&gt;Clearly the &lt;strong&gt;main disadvantage&lt;/strong&gt; of &lt;em&gt;best subset selection&lt;/em&gt; is computational power.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def model_selection(X, y, *args):
    
    # Init 
    scores = list(itertools.repeat(np.zeros((0,2)), len(args)))

    # Categorical variables 
    categ_cols = {&amp;quot;Gender&amp;quot;, &amp;quot;Student&amp;quot;, &amp;quot;Married&amp;quot;, &amp;quot;Ethnicity&amp;quot;}

    # Loop over all admissible number of regressors
    K = np.shape(X)[1]
    for k in range(K+1):
        print(&amp;quot;Computing k=%1.0f&amp;quot; % k, end =&amp;quot;&amp;quot;)
        
        # Loop over all combinations
        for i in combinations(range(K), k):

            # Subset X
            X_subset = X.iloc[:,list(i)]

            # Get dummies for categorical variables
            if k&amp;gt;0:
                categ_subset = list(categ_cols &amp;amp; set(X_subset.columns))
                X_subset = pd.get_dummies(X_subset, columns=categ_subset, drop_first=True)

            # Regress
            reg = OLS(y,add_constant(X_subset)).fit()

            # Metrics
            for i,metric in enumerate(args):
                score = np.reshape([k,metric(reg)], (1,-1))
                scores[i] = np.append(scores[i], score, axis=0)
        print(&amp;quot;&amp;quot;, end=&amp;quot;\r&amp;quot;)
    return scores
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We are going to consider 10 variables and two difference metrics: the Sum of Squares Residuals and $R^2$.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Set metrics
rss = lambda reg : reg.ssr
r2 = lambda reg : reg.rsquared

# Compute scores
scores = model_selection(X, y, rss, r2)
ms_RSS = scores[0]
ms_R2 = scores[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Computing k=10
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Save best scores
K = np.shape(X)[1]
ms_RSS_best = [np.min(ms_RSS[ms_RSS[:,0]==k,1]) for k in range(K+1)]
ms_R2_best = [np.max(ms_R2[ms_R2[:,0]==k,1]) for k in range(K+1)]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s plot the best scores.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Figure 6.1
def make_figure_6_1():

    fig, (ax1,ax2) = plt.subplots(1,2)
    fig.suptitle(&#39;Figure 6.1: Best Model Selection&#39;)

    # RSS
    ax1.scatter(x=ms_RSS[:,0], y=ms_RSS[:,1], facecolors=&#39;None&#39;, edgecolors=&#39;k&#39;, alpha=0.5);
    ax1.plot(range(K+1), ms_RSS_best, c=&#39;r&#39;);
    ax1.scatter(np.argmin(ms_RSS_best), np.min(ms_RSS_best), marker=&#39;x&#39;, s=300)
    ax1.set_ylabel(&#39;RSS&#39;);

    # R2
    ax2.scatter(x=ms_R2[:,0], y=ms_R2[:,1], facecolors=&#39;None&#39;, edgecolors=&#39;k&#39;, alpha=0.5);
    ax2.plot(range(K+1), ms_R2_best, c=&#39;r&#39;);
    ax2.scatter(np.argmax(ms_R2_best), np.max(ms_R2_best), marker=&#39;x&#39;, s=300)
    ax2.set_ylabel(&#39;R2&#39;);

    # All axes;
    for ax in fig.axes:
        ax.set_xlabel(&#39;Number of Predictors&#39;); 
        ax.set_yticks([]);
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;make_figure_6_1()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/05_regularization_19_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;The figure shows that, as expected, both metrics improve as the number of variables increases; however, from the three-variable model on, there is little improvement in RSS and $R^2$ as a result of including additional predictors.&lt;/p&gt;
&lt;h3 id=&#34;forward-stepwise-selection&#34;&gt;Forward Stepwise Selection&lt;/h3&gt;
&lt;p&gt;For computational reasons, best subset selection cannot be applied with very large $p$.&lt;/p&gt;
&lt;p&gt;While the best subset selection procedure considers all $2^p$ possible models containing subsets of the p predictors, forward step-wise considers a much smaller set of models. Forward stepwise selection begins with a model containing no predictors, and then adds predictors to the model, one-at-a-time, until all of the predictors are in the model. In particular, at each step the variable that gives the greatest additional improvement to the fit is added to the model.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def forward_selection(X, y, f):

    # Init RSS and R2
    K = np.shape(X)[1]
    fms_scores = np.zeros((K,1))
    
    # Categorical variables 
    categ_cols = {&amp;quot;Gender&amp;quot;, &amp;quot;Student&amp;quot;, &amp;quot;Married&amp;quot;, &amp;quot;Ethnicity&amp;quot;}

    # Loop over p
    selected_cols = []
    for k in range(1,K+1):

        # Loop over selected columns
        remaining_cols = [col for col in X.columns if col not in selected_cols]
        temp_scores = np.zeros((0,1))

        # Loop on remaining columns    
        for col in remaining_cols:
            # Subset X
            X_subset = X.loc[:,selected_cols + [col]]
            if k&amp;gt;0:
                categ_subset = list(categ_cols &amp;amp; set(X_subset.columns))
                X_subset = pd.get_dummies(X_subset, columns=categ_subset, drop_first=True)

            # Regress
            reg = OLS(y,add_constant(X_subset).values).fit()

            # Metrics
            temp_scores = np.append(temp_scores, f(reg))

        # Pick best variable
        best_col = remaining_cols[np.argmin(temp_scores)]
        print(best_col)
        selected_cols += [best_col]
        fms_scores[k-1] = np.min(temp_scores)
        
    return fms_scores
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s select the best model according, using the sum of squared residuals as a metric.&lt;/p&gt;
&lt;p&gt;What are the most important variables?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Forward selection by RSS
rss = lambda reg : reg.ssr
fms_RSS = forward_selection(X, y, rss)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Rating
Income
Student
Limit
Cards
Age
Ethnicity
Gender
Married
Education
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What happens if we use $R^2$ instead?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Forward selection by R2
r2 = lambda reg : -reg.rsquared
fms_R2 = -forward_selection(X, y, r2)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Rating
Income
Student
Limit
Cards
Age
Ethnicity
Gender
Married
Education
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Unsurprisingly, both methods select the same models. Why? In the end $R^2$ is just a normalized version of RSS.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s plot the scores of the two methods, for different number of predictors.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# New figure 1
def make_new_figure_1():

    # Init
    fig, (ax1,ax2) = plt.subplots(1,2)
    fig.suptitle(&#39;Forward Model Selection&#39;)

    # RSS
    ax1.plot(range(1,K+1), fms_RSS, c=&#39;r&#39;);
    ax1.scatter(np.argmin(fms_RSS)+1, np.min(fms_RSS), marker=&#39;x&#39;, s=300)
    ax1.set_ylabel(&#39;RSS&#39;);

    # R2
    ax2.plot(range(1,K+1), fms_R2, c=&#39;r&#39;);
    ax2.scatter(np.argmax(fms_R2)+1, np.max(fms_R2), marker=&#39;x&#39;, s=300)
    ax2.set_ylabel(&#39;R2&#39;);

    # All axes;
    for ax in fig.axes:
        ax.set_xlabel(&#39;Number of Predictors&#39;); 
        ax.set_yticks([]);
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;make_new_figure_1()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/05_regularization_31_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;backward-stepwise-selection&#34;&gt;Backward Stepwise Selection&lt;/h3&gt;
&lt;p&gt;Like forward stepwise selection, backward stepwise selection provides an efficient alternative to best subset selection. However, unlike forward stepwise selection, it begins with the full least squares model containing all p predictors, and then iteratively removes the least useful predictor, one-at-a-time.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def backward_selection(X, y, f):

    # Init RSS and R2
    K = np.shape(X)[1]
    fms_scores = np.zeros((K,1))
    
    # Categorical variables 
    categ_cols = {&amp;quot;Gender&amp;quot;, &amp;quot;Student&amp;quot;, &amp;quot;Married&amp;quot;, &amp;quot;Ethnicity&amp;quot;}

    # Loop over p
    selected_cols = list(X.columns)
    for k in range(K,0,-1):

        # Loop over selected columns
        temp_scores = np.zeros((0,1))

        # Loop on remaining columns    
        for col in selected_cols:
            # Subset X
            X_subset = X.loc[:,[x for x in selected_cols if x != col]]
            if k&amp;gt;1:
                categ_subset = list(categ_cols &amp;amp; set(X_subset.columns))
                X_subset = pd.get_dummies(X_subset, columns=categ_subset, drop_first=True)

            # Regress
            reg = OLS(y,add_constant(X_subset).values).fit()

            # Metrics
            temp_scores = np.append(temp_scores, f(reg))

        # Pick best variable
        worst_col = selected_cols[np.argmin(temp_scores)]
        print(worst_col)
        selected_cols.remove(worst_col)
        fms_scores[k-1] = np.min(temp_scores)
        
    return fms_scores
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s select the best model according, using the sum of squared residuals as a metric.&lt;/p&gt;
&lt;p&gt;What are the most important variables?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Backward selection by RSS
rss = lambda reg : reg.ssr
bms_RSS = backward_selection(X, y, rss)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Education
Married
Gender
Ethnicity
Age
Rating
Cards
Student
Income
Limit
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What if we use $R^2$ instead?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Backward selection by R2
r2 = lambda reg : -reg.rsquared
bms_R2 = -backward_selection(X, y, r2)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Education
Married
Gender
Ethnicity
Age
Rating
Cards
Student
Income
Limit
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The interesting part here is that the the variable &lt;code&gt;Rating&lt;/code&gt; that was selected first by forward model selection, is now dropped $5^{th}$ to last. Why? It&amp;rsquo;s probably because it contains a lot of information by itself (hence first in FMS) but it&amp;rsquo;s highly correlated with &lt;code&gt;Student&lt;/code&gt;, &lt;code&gt;Income&lt;/code&gt; and &lt;code&gt;Limit&lt;/code&gt; while these variables are more ortogonal to each other, and hence it gets dropped before them in BMS.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Plot correlations
sns.pairplot(credit[[&#39;Rating&#39;,&#39;Student&#39;,&#39;Income&#39;,&#39;Limit&#39;]], height=1.8);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/05_regularization_40_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;If is indeed what we see: &lt;code&gt;Rating&lt;/code&gt; and &lt;code&gt;Limit&lt;/code&gt; are highly correlated.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s plot the scores for different number of predictors.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# New figure 2
def make_new_figure_2():

    # Init
    fig, (ax1,ax2) = plt.subplots(1,2)
    fig.suptitle(&#39;Backward Model Selection&#39;)

    # RSS
    ax1.plot(range(1,K+1), bms_RSS, c=&#39;r&#39;);
    ax1.scatter(np.argmin(bms_RSS)+1, np.min(bms_RSS), marker=&#39;x&#39;, s=300)
    ax1.set_ylabel(&#39;RSS&#39;);

    # R2
    ax2.plot(range(1,K+1), bms_R2, c=&#39;r&#39;);
    ax2.scatter(np.argmax(bms_R2)+1, np.max(bms_R2), marker=&#39;x&#39;, s=300)
    ax2.set_ylabel(&#39;R2&#39;);

    # All axes;
    for ax in fig.axes:
        ax.set_xlabel(&#39;Number of Predictors&#39;); 
        ax.set_yticks([]);
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;make_new_figure_2()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/05_regularization_44_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;choosing-the-optimal-model&#34;&gt;Choosing the Optimal Model&lt;/h3&gt;
&lt;p&gt;So far we have use the trainint error in order to select the model. However, the training error can be a poor estimate of the test error. Therefore, RSS and R2 are not suitable for selecting the best model among a collection of models with different numbers of predictors.&lt;/p&gt;
&lt;p&gt;In order to select the best model with respect to test error, we need to estimate this test error. There are two common approaches:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;We can indirectly estimate test error by making an adjustment to the training error to account for the bias due to overfitting.&lt;/li&gt;
&lt;li&gt;We can directly estimate the test error, using either a validation set approach or a cross-validation approach.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Some metrics that account for the trainint error are&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Akaike Information Criterium (AIC)&lt;/li&gt;
&lt;li&gt;Bayesian Information Criterium (BIC)&lt;/li&gt;
&lt;li&gt;Adjusted $R^2$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The idea behind all these varaibles is to insert a penalty for the number of parameters used in the model. All these measure have theoretical fundations which are beyond the scope of this session.&lt;/p&gt;
&lt;p&gt;We are now going to test the three metrics&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Set metrics
aic = lambda reg : reg.aic
bic = lambda reg : reg.bic
r2a = lambda reg : reg.rsquared_adj

# Compute best model selection scores
scores = model_selection(X, y, aic, bic, r2a)
ms_AIC = scores[0]
ms_BIC = scores[1]
ms_R2a = scores[2]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Computing k=10
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Save best scores
ms_AIC_best = [np.min(ms_AIC[ms_AIC[:,0]==k,1]) for k in range(K+1)]
ms_BIC_best = [np.min(ms_BIC[ms_BIC[:,0]==k,1]) for k in range(K+1)]
ms_R2a_best = [np.max(ms_R2a[ms_R2a[:,0]==k,1]) for k in range(K+1)]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We plot the scores for different model selection methods.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Figure 6.2
def make_figure_6_2():

    # Init
    fig, (ax1,ax2,ax3) = plt.subplots(1,3, figsize=(16,5))
    fig.suptitle(&#39;Figure 6.2&#39;)

    # AIC
    ax1.scatter(x=ms_AIC[:,0], y=ms_AIC[:,1], facecolors=&#39;None&#39;, edgecolors=&#39;k&#39;, alpha=0.5);
    ax1.plot(range(K+1),ms_AIC_best, c=&#39;r&#39;);
    ax1.scatter(np.argmin(ms_AIC_best), np.min(ms_AIC_best), marker=&#39;x&#39;, s=300)
    ax1.set_ylabel(&#39;AIC&#39;);

    # BIC
    ax2.scatter(x=ms_BIC[:,0], y=ms_BIC[:,1], facecolors=&#39;None&#39;, edgecolors=&#39;k&#39;, alpha=0.5);
    ax2.plot(range(K+1), ms_BIC_best, c=&#39;r&#39;);
    ax2.scatter(np.argmin(ms_BIC_best), np.min(ms_BIC_best), marker=&#39;x&#39;, s=300)
    ax2.set_ylabel(&#39;BIC&#39;);

    # R2 adj
    ax3.scatter(x=ms_R2a[:,0], y=ms_R2a[:,1], facecolors=&#39;None&#39;, edgecolors=&#39;k&#39;, alpha=0.5);
    ax3.plot(range(K+1), ms_R2a_best, c=&#39;r&#39;);
    ax3.scatter(np.argmax(ms_R2a_best), np.max(ms_R2a_best), marker=&#39;x&#39;, s=300)
    ax3.set_ylabel(&#39;R2_adj&#39;);

    # All axes;
    for ax in fig.axes:
        ax.set_xlabel(&#39;Number of Predictors&#39;); 
        ax.set_yticks([]);
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;make_figure_6_2()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/05_regularization_52_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;As we can see, all three metrics select more parsimonious models, with BIC being particularly conservative with only 4 variables and $R^2_{adj}$ selecting the larger model with 7 variables.&lt;/p&gt;
&lt;h3 id=&#34;validation-and-cross-validation&#34;&gt;Validation and Cross-Validation&lt;/h3&gt;
&lt;p&gt;As an alternative to the approaches just discussed, we can directly estimate the test error using the validation set and cross-validation methods discussed in the previous session.&lt;/p&gt;
&lt;p&gt;The main problem with cross-validation is the computational burden. We are now going to perform &lt;em&gt;best model selection&lt;/em&gt; using the following cross-validation algorithms:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Validation set approach, 50-50 split, repeated 10 times&lt;/li&gt;
&lt;li&gt;5-fold cross-validation&lt;/li&gt;
&lt;li&gt;10-fold cross-validation&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We are not going to perform Leave-One-Out cross-validation for computational reasons.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def cv_scores(X, y, *args):

    # Init 
    scores = list(itertools.repeat(np.zeros((0,2)), len(args)))

    # Categorical variables 
    categ_cols = {&amp;quot;Gender&amp;quot;, &amp;quot;Student&amp;quot;, &amp;quot;Married&amp;quot;, &amp;quot;Ethnicity&amp;quot;}

    # Loop over all possible combinations of regressions
    K = np.shape(X)[1]
    for k in range(K+1):
        print(&amp;quot;Computing k=%1.0f&amp;quot; % k, end =&amp;quot;&amp;quot;)
        for i in combinations(range(K), k):

            # Subset X
            X_subset = X.iloc[:,list(i)]

            # Get dummies for categorical variables
            if k&amp;gt;0:
                categ_subset = list(categ_cols &amp;amp; set(X_subset.columns))
                X_subset = pd.get_dummies(X_subset, columns=categ_subset, drop_first=True)

            # Metrics
            for i,cv_method in enumerate(args):
                score = cross_val_score(LinearRegression(), add_constant(X_subset), y, 
                                        cv=cv_method, scoring=&#39;neg_mean_squared_error&#39;).mean()
                score_pair = np.reshape([k,score], (1,-1))
                scores[i] = np.append(scores[i], score_pair, axis=0)
        print(&amp;quot;&amp;quot;, end=&amp;quot;\r&amp;quot;)
                
    return scores
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s compute the scores for different model selection methods.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Define cv methods
vset = ShuffleSplit(n_splits=10, test_size=0.5)
kf5 = KFold(n_splits=5, shuffle=True)
kf10 = KFold(n_splits=10, shuffle=True)

# Get best model selection scores
scores = cv_scores(X, y, vset, kf5, kf10)
ms_vset = scores[0]
ms_kf5 = scores[1]
ms_kf10 = scores[2]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Computing k=10
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Save best scores
ms_vset_best = [np.max(ms_vset[ms_vset[:,0]==k,1]) for k in range(K+1)]
ms_kf5_best = [np.max(ms_kf5[ms_kf5[:,0]==k,1]) for k in range(K+1)]
ms_kf10_best = [np.max(ms_kf10[ms_kf10[:,0]==k,1]) for k in range(K+1)]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We not plot the scores.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Figure 6.3
def make_figure_6_3():

    # Init
    fig, (ax1,ax2,ax3) = plt.subplots(1,3, figsize=(16,5))
    fig.suptitle(&#39;Figure 6.3&#39;)

    # Validation Set
    ax1.scatter(x=ms_vset[:,0], y=ms_vset[:,1], facecolors=&#39;None&#39;, edgecolors=&#39;k&#39;, alpha=0.5);
    ax1.plot(range(K+1),ms_vset_best, c=&#39;r&#39;);
    ax1.scatter(np.argmax(ms_vset_best), np.max(ms_vset_best), marker=&#39;x&#39;, s=300)
    ax1.set_ylabel(&#39;Validation Set&#39;);


    # 5-Fold Cross Validation
    ax2.scatter(x=ms_kf5[:,0], y=ms_kf5[:,1], facecolors=&#39;None&#39;, edgecolors=&#39;k&#39;, alpha=0.5);
    ax2.plot(range(K+1), ms_kf5_best, c=&#39;r&#39;);
    ax2.scatter(np.argmax(ms_kf5_best), np.max(ms_kf5_best), marker=&#39;x&#39;, s=300)
    ax2.set_ylabel(&#39;5-Fold Cross Validation&#39;);


    # 10-Fold Cross-Validation
    ax3.scatter(x=ms_kf10[:,0], y=ms_kf10[:,1], facecolors=&#39;None&#39;, edgecolors=&#39;k&#39;, alpha=0.5);
    ax3.plot(range(K+1), ms_kf10_best, c=&#39;r&#39;);
    ax3.scatter(np.argmax(ms_kf10_best), np.max(ms_kf10_best), marker=&#39;x&#39;, s=300)
    ax3.set_ylabel(&#39;10-Fold Cross-Validation&#39;);

    # All axes;
    for ax in fig.axes:
        ax.set_xlabel(&#39;Number of Predictors&#39;); 
        ax.set_yticks([]);
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;make_figure_6_3()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/05_regularization_62_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;From the figure we see that each cross-validation method selects a different model and the most accurate one, K-fold CV, select 5 predictors.&lt;/p&gt;
&lt;h2 id=&#34;52-shrinkage-methods&#34;&gt;5.2 Shrinkage Methods&lt;/h2&gt;
&lt;p&gt;Model selection methods constrained the number of varaibles &lt;em&gt;before&lt;/em&gt; running a linear regression. Shrinkage methods attempt to do the two things simultaneously. In particular they &lt;em&gt;constrain&lt;/em&gt; or &lt;em&gt;shrink&lt;/em&gt; coefficients by imposing penalties in the objective functions for high values of the parameters.&lt;/p&gt;
&lt;h3 id=&#34;ridge-regression&#34;&gt;Ridge Regression&lt;/h3&gt;
&lt;p&gt;The Least Squares Regression minimizes the Residual Sum of Squares&lt;/p&gt;
&lt;p&gt;$$
\mathrm{RSS}=\sum_{i=1}^{n}\left(y_{i}-\beta_{0}-\sum_{j=1}^{p} \beta_{j} x_{i j}\right)^{2}
$$&lt;/p&gt;
&lt;p&gt;The Ridge Regression objective function instead is&lt;/p&gt;
&lt;p&gt;$$
\sum_{i=1}^{n}\left(y_{i}-\beta_{0}-\sum_{j=1}^{p} \beta_{j} x_{i j}\right)^{2}+\lambda \sum_{j=1}^{p} \beta_{j}^{2}=\mathrm{RSS}+\lambda \sum_{j=1}^{p} \beta_{j}^{2}
$$&lt;/p&gt;
&lt;p&gt;where $\lambda&amp;gt;0$ is a tuning parameter that regulates the extent to which large parameters are penalized.&lt;/p&gt;
&lt;p&gt;In matrix notation, the objective function is&lt;/p&gt;
&lt;p&gt;$$
||X\beta - y||^2_2 + \alpha ||\beta||^2_2
$$&lt;/p&gt;
&lt;p&gt;which is equivalent to optimizing&lt;/p&gt;
&lt;p&gt;$$
\frac{1}{N}||X\beta - y||^2_2 + \frac{\alpha}{N} ||\beta||^2_2
$$&lt;/p&gt;
&lt;p&gt;We are now going to run Ridge Regression on the &lt;code&gt;Credit&lt;/code&gt; dataset trying to explain account &lt;code&gt;Balance&lt;/code&gt; with a set of observable individual characteristics.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# X and y
categ_cols = [&amp;quot;Gender&amp;quot;, &amp;quot;Student&amp;quot;, &amp;quot;Married&amp;quot;, &amp;quot;Ethnicity&amp;quot;]
X = credit.loc[:, credit.columns != &#39;Balance&#39;]
X = pd.get_dummies(X, columns=categ_cols, drop_first=True)
y = credit.loc[:,&#39;Balance&#39;]
n = len(credit)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We run ridge regression over a range of values for the penalty paramenter $\lambda$.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Init alpha grid
n_grid = 100
alphas = 10**np.linspace(-2,5,n_grid).reshape(-1,1)
ridge = Ridge()
ridge_coefs = []

# Loop over values of alpha
for a in alphas:
    ridge.set_params(alpha=a)
    ridge.fit(scale(X), y)
    ridge_coefs.append(ridge.coef_)
ridge_coefs = np.reshape(ridge_coefs,(n_grid,-1))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We use linear regression as a comparison.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# OLS regression
ols = LinearRegression().fit(scale(X),y)
ols_coefs = ols.coef_;
mod_ols = np.linalg.norm(ols_coefs)

# Relative magnitude
rel_beta = [np.linalg.norm(ridge_coefs[k,:])/mod_ols for k in range(n_grid)]
rel_beta = np.reshape(rel_beta, (-1,1))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We plot the results&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Figure 6.4
def make_figure_6_4():

    # Init
    fig, (ax1,ax2) = plt.subplots(1,2)
    fig.suptitle(&#39;Figure 6.4: Ridge Regression Coefficients&#39;)

    highlight = [0,1,2,7];

    # Plot coefficients - absolute
    ax1.plot(alphas, ridge_coefs[:,highlight], alpha=1)
    ax1.plot(alphas, ridge_coefs, c=&#39;grey&#39;, alpha=0.3)
    ax1.set_xscale(&#39;log&#39;)
    ax1.set_xlabel(&#39;lambda&#39;); ax1.set_ylabel(&#39;Standardized coefficients&#39;);
    ax1.legend([&#39;Income&#39;, &#39;Limit&#39;, &#39;Rating&#39;, &#39;Student&#39;])

    # Plot coefficients - relative
    ax2.plot(rel_beta, ridge_coefs[:,highlight], alpha=1)
    ax2.plot(rel_beta, ridge_coefs, c=&#39;grey&#39;, alpha=0.3)
    ax2.set_xlabel(&#39;Relative Beta&#39;); ax2.set_ylabel(&#39;Standardized coefficients&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;make_figure_6_4()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/05_regularization_76_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;As we decrease $\lambda$, the Ridge coefficients get larger. Moreover, the variables with the consistently largest coefficients are &lt;code&gt;Income&lt;/code&gt;, &lt;code&gt;Limit&lt;/code&gt;, &lt;code&gt;Rating&lt;/code&gt; and &lt;code&gt;Student&lt;/code&gt;.&lt;/p&gt;
&lt;h3 id=&#34;bias-variance-trade-off&#34;&gt;Bias-Variance Trade-off&lt;/h3&gt;
&lt;p&gt;Ridge regression’s advantage over least squares is rooted in the bias-variance trade-off. As $\lambda$ increases, the flexibility of the ridge regression fit decreases, leading to decreased variance but increased bias.&lt;/p&gt;
&lt;p&gt;$$
y_0 = f(x_0) + \varepsilon
$$&lt;/p&gt;
&lt;p&gt;Recap: we can decompose the Mean Squared Error of an estimator into two components: the &lt;em&gt;variance&lt;/em&gt; and the squared &lt;em&gt;bias&lt;/em&gt;:&lt;/p&gt;
&lt;p&gt;$$
\mathbb E\left(y_{0}-\hat{f}\left(x_{0}\right)\right)^{2} = \mathbb E\left(f(x_0) + \varepsilon - \hat f(x_{0})\right)^{2} = \
= \mathbb E\left(f(x_0) - \mathbb E[\hat f(x_{0})] + \varepsilon - \hat f(x_{0}) + \mathbb E[\hat f(x_{0})] \right)^{2} = \
= \mathbb E \left[ \mathbb E [\hat{f} (x_{0}) ] - f(x_0) \right]^2 + \mathbb E \left[ \left( \hat{f} (x_{0}) - \mathbb E [\hat{f} (x_{0})] \right)^2 \right] + \mathbb E[\varepsilon^2] \
= \operatorname{Bias} \left( \hat{f} (x_{0}) \right)^2 + \operatorname{Var}\left(\hat{f}\left(x_{0}\right)\right) + \operatorname{Var}(\varepsilon)
$$&lt;/p&gt;
&lt;p&gt;The last term is the variance of the error term, sometimes also called the &lt;em&gt;irreducible error&lt;/em&gt; since it&amp;rsquo;s pure noise, and we cannot account for it.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Compute var-bias
def compute_var_bias(X_train, b0, x0, a, k, n, sim, f):
    
    # Init 
    y_hat = np.zeros(sim)
    coefs = np.zeros((sim, k))
    
    # Loop over simulations
    for s in range(sim):
        e_train = normal(0,1,(n,1))
        y_train = X_train @ b0 + e_train
        fit = f(a).fit(X_train, y_train)
        y_hat[s] = fit.predict(x0)
        coefs[s,:] = fit.coef_
        
    # Compute MSE, Var and Bias2   
    e_test = normal(0,1,(sim,1))
    y_test = x0 @ b0 + e_test
    mse = np.mean((y_test - y_hat)**2)
    var = np.var(y_hat)
    bias2 = np.mean(x0 @ b0 - y_hat)**2
    
    return [mse, var, bias2], np.mean(coefs, axis=0)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;np.random.seed(1)

# Generate random data
n = 50
k = 45
N = 50000
X_train = normal(0.2,1,(n,k))
x0 = normal(0.2,1,(1,k))
e_train = normal(0,1,(n,1))
b0 = uniform(0,1,(k,1))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Init alpha grid
sim = 1000
n_grid = 30
df = pd.DataFrame({&#39;alpha&#39;:10**np.linspace(-5,5,n_grid)})
ridge_coefs2 = []

# Init simulations
sim = 1000
ridge = lambda a: Ridge(alpha=a, fit_intercept=False)

# Loop over values of alpha
for i in range(len(df)):
    print(&amp;quot;Alpha %1.0f/%1.0f&amp;quot; % (i+1,len(df)), end =&amp;quot;&amp;quot;)
    a = df.loc[i,&#39;alpha&#39;]
    df.loc[i,[&#39;mse&#39;,&#39;var&#39;,&#39;bias2&#39;]], c = compute_var_bias(X_train, b0, x0, a, k, n, sim, ridge)
    ridge_coefs2.append(c)
    print(&amp;quot;&amp;quot;, end=&amp;quot;\r&amp;quot;)
ridge_coefs2 = np.reshape(ridge_coefs2,(n_grid,-1))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Alpha 30/30
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# OLS regression
y_train = X_train @ b0 + e_train
ols = LinearRegression().fit(X_train,y_train)
ols_coefs = ols.coef_;
mod_ols = np.linalg.norm(ols_coefs)

# Relative magnitude
rel_beta = [np.linalg.norm(ridge_coefs2[i,:])/mod_ols for i in range(n_grid)]
rel_beta = np.reshape(rel_beta, (-1,1))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Figure 6.5
def make_figure_6_5():
    
    # Init
    fig, (ax1,ax2) = plt.subplots(1,2)
    fig.suptitle(&#39;Figure 6.5: Ridge Bias-Var decomposition&#39;)

    # MSE
    ax1.plot(df[&#39;alpha&#39;], df[[&#39;bias2&#39;,&#39;var&#39;,&#39;mse&#39;]]);
    ax1.set_xscale(&#39;log&#39;);
    ax1.set_xlabel(&#39;lambda&#39;); ax1.set_ylabel(&#39;Mean Squared Error&#39;);
    ax1.legend([&#39;Bias2&#39;,&#39;Variance&#39;,&#39;MSE&#39;], fontsize=12);

    # MSE
    ax2.plot(rel_beta, df[[&#39;bias2&#39;,&#39;var&#39;,&#39;mse&#39;]]);
    ax2.set_xlabel(&#39;Relative Beta&#39;); ax2.set_ylabel(&#39;Mean Squared Error&#39;);
    ax2.legend([&#39;Bias2&#39;,&#39;Variance&#39;,&#39;MSE&#39;], fontsize=12);
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;make_figure_6_5()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/05_regularization_86_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Ridge regression has the advantage of shrinking coefficients. However, unlike best subset, forward stepwise, and backward stepwise selection, which will generally select models that involve just a subset of the variables, ridge regression will include all $p$ predictors in the final model.&lt;/p&gt;
&lt;p&gt;Lasso solves that problem by using a different penalty function.&lt;/p&gt;
&lt;h3 id=&#34;lasso&#34;&gt;Lasso&lt;/h3&gt;
&lt;p&gt;The lasso coefficients minimize the following objective function:&lt;/p&gt;
&lt;p&gt;$$
\sum_{i=1}^{n}\left(y_{i}-\beta_{0}-\sum_{j=1}^{p} \beta_{j} x_{i j}\right)^{2}+\lambda \sum_{j=1}^{p}\left|\beta_{j}\right| = \mathrm{RSS} + \lambda \sum_{j=1}^p|\beta_j|
$$&lt;/p&gt;
&lt;p&gt;so that the main difference with respect to ridge regression is the penalty function $\lambda \sum_{j=1}^{p}\left|\beta_{j}\right|$ instead of $\lambda \sum_{j=1}^p (\beta_j)^2$.&lt;/p&gt;
&lt;p&gt;A consequence of this objective function is that Lasso is much more likely to shrink coefficients to exactly zero, while Ridge only decreases their magnitude. The reason why lies in the shape of the objective function. You can rewrite the Ridge and Lasso minimization problems as constrained optimization:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Ridge
$$
\underset{\beta}{\operatorname{min}} \ \left{\sum_{i=1}^{n}\left(y_{i}-\beta_{0}-\sum_{j=1}^{p} \beta_{j} x_{i j}\right)^{2}\right} \quad \text { subject to } \quad \sum_{j=1}^{p}\left|\beta_{j}\right| \leq s
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Lasso
$$
\underset{\beta}{\operatorname{min}} \ \left{\sum_{i=1}^{n}\left(y_{i}-\beta_{0}-\sum_{j=1}^{p} \beta_{j} x_{i j}\right)^{2}\right} \quad \text { subject to } \quad \sum_{j=1}^{p} \beta_{j}^{2} \leq s
$$&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In pictures, constrained optimization problem lookes like this.&lt;/p&gt;
&lt;img src=&#34;figures/ridgelasso.png&#34; alt=&#34;Drawing&#34; style=&#34;width: 600px;&#34;/&gt;
&lt;p&gt;The red curves represents the contour sets of the RSS. They are elliptical since the objective function is quadratic. The blue area represents the admissible set, i.e. the constraints. As we can see, it is much easier with Lasso to have the constrained optimum on one of the edges of the rhombus.&lt;/p&gt;
&lt;p&gt;We are now going to repeat the same exercise on the &lt;code&gt;Credit&lt;/code&gt; dataset, trying to predict account &lt;code&gt;Balance&lt;/code&gt; with a set of obsevable induvidual characteristics, for different values of the penalty paramenter $\lambda$.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# X and y
categ_cols = [&amp;quot;Gender&amp;quot;, &amp;quot;Student&amp;quot;, &amp;quot;Married&amp;quot;, &amp;quot;Ethnicity&amp;quot;]
X = credit.loc[:, credit.columns != &#39;Balance&#39;]
X = pd.get_dummies(X, columns=categ_cols, drop_first=True)
y = credit.loc[:,&#39;Balance&#39;]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The $\lambda$ grid is going to be slightly different now.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Init alpha grid
n_grid = 100
alphas = 10**np.linspace(0,3,n_grid).reshape(-1,1)
lasso = Lasso()
lasso_coefs = []

# Loop over values of alpha
for a in alphas:
    lasso.set_params(alpha=a)
    lasso.fit(scale(X), y)
    lasso_coefs.append(lasso.coef_)
lasso_coefs = np.reshape(lasso_coefs,(n_grid,-1))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We run OLS to plot the relative magnitude of the Lasso coefficients.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Relative magnitude
mod_ols = np.linalg.norm(ols_coefs)
rel_beta = [np.linalg.norm(lasso_coefs[i,:])/mod_ols for i in range(n_grid)]
rel_beta = np.reshape(rel_beta, (-1,1))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We plot the magnitude of the coefficients $\beta$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;for different values of $\lambda$&lt;/li&gt;
&lt;li&gt;for different values of of $||\beta||$&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Figure 6.6
def make_figure_6_6():

    # Init
    fig, (ax1,ax2) = plt.subplots(1,2)
    fig.suptitle(&#39;Figure 6.6&#39;)

    highlight = [0,1,2,7];

    # Plot coefficients - absolute
    ax1.plot(alphas, lasso_coefs[:,highlight], alpha=1)
    ax1.plot(alphas, lasso_coefs, c=&#39;grey&#39;, alpha=0.3)
    ax1.set_xscale(&#39;log&#39;)
    ax1.set_xlabel(&#39;lambda&#39;); ax1.set_ylabel(&#39;Standardized coefficients&#39;);
    ax1.legend([&#39;Income&#39;, &#39;Limit&#39;, &#39;Rating&#39;, &#39;Student&#39;], fontsize=12)

    # Plot coefficients - relative
    ax2.plot(rel_beta, lasso_coefs[:,highlight], alpha=1)
    ax2.plot(rel_beta, lasso_coefs, c=&#39;grey&#39;, alpha=0.3)
    ax2.set_xlabel(&#39;relative mod beta&#39;); ax2.set_ylabel(&#39;Standardized coefficients&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;make_figure_6_6()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/05_regularization_100_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Rating&lt;/code&gt; seems to be the most important variable, followed by &lt;code&gt;Limit&lt;/code&gt; and &lt;code&gt;Student&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;As with ridge regression, the lasso shrinks the coefficient estimates towards zero. However, in the case of the lasso, the $l_1$ penalty has the effect of forcing some of the coefficient estimates to be exactly equal to zero when the tuning parameter $\lambda$ is sufficiently large. Hence, much like best subset selection, the lasso performs variable selection.&lt;/p&gt;
&lt;p&gt;We say that the lasso yields &lt;strong&gt;sparse&lt;/strong&gt; models — that is, models that involve only a subset of the variable&lt;/p&gt;
&lt;p&gt;We now plot how the choice of $\lambda$ affects the bias-variance trade-off.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Init alpha grid
sim = 1000
n_grid = 30
df = pd.DataFrame({&#39;alpha&#39;:10**np.linspace(-1,1,n_grid)})
lasso_coefs2 = []

# Init simulations
sim = 1000
lasso = lambda a: Lasso(alpha=a, fit_intercept=False)

# Loop over values of alpha
for i in range(len(df)):
    print(&amp;quot;Alpha %1.0f/%1.0f&amp;quot; % (i+1,len(df)), end =&amp;quot;&amp;quot;)
    a = df.loc[i,&#39;alpha&#39;]
    df.loc[i,[&#39;mse&#39;,&#39;var&#39;,&#39;bias2&#39;]], c = compute_var_bias(X_train, b0, x0, a, k, n, sim, lasso)
    lasso_coefs2.append(c)
    print(&amp;quot;&amp;quot;, end=&amp;quot;\r&amp;quot;)
lasso_coefs2 = np.reshape(lasso_coefs2,(n_grid,-1))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Alpha 30/30
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Relative magnitude
mod_ols = np.linalg.norm(ols_coefs)
rel_beta = [np.linalg.norm(lasso_coefs2[k,:])/mod_ols for k in range(n_grid)]
rel_beta = np.reshape(rel_beta, (-1,1))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# OLS regression
y_train = X_train @ b0 + e_train
ols = LinearRegression().fit(X_train,y_train)
ols_coefs = ols.coef_;
mod_ols = np.linalg.norm(ols_coefs)

# Relative magnitude
mod_ols = np.linalg.norm(ols_coefs)
rel_beta = [np.linalg.norm(lasso_coefs2[k,:])/mod_ols for k in range(n_grid)]
rel_beta = np.reshape(rel_beta, (-1,1))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Figure 6.8
def make_figure_6_8():

    fig, (ax1,ax2) = plt.subplots(1,2, figsize=(12,5))
    fig.suptitle(&#39;Figure 6.8: Lasso Bias-Var decomposition&#39;)

    # MSE
    ax1.plot(df[&#39;alpha&#39;], df[[&#39;bias2&#39;,&#39;var&#39;,&#39;mse&#39;]]);
    ax1.set_xscale(&#39;log&#39;);
    ax1.set_xlabel(&#39;lambda&#39;); ax1.set_ylabel(&#39;Mean Squared Error&#39;);
    ax1.legend([&#39;Bias2&#39;,&#39;Variance&#39;,&#39;MSE&#39;], fontsize=12);

    # MSE
    ax2.plot(rel_beta, df[[&#39;bias2&#39;,&#39;var&#39;,&#39;mse&#39;]]);
    ax2.set_xlabel(&#39;Relative Beta&#39;); ax1.set_ylabel(&#39;Mean Squared Error&#39;);
    ax2.legend([&#39;Bias2&#39;,&#39;Variance&#39;,&#39;MSE&#39;], fontsize=12);
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;make_figure_6_8()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/05_regularization_108_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;As $\lambda$ increases the squared bias increases and the variance decreases.&lt;/p&gt;
&lt;h3 id=&#34;comparing-the-lasso-and-ridge-regression&#34;&gt;Comparing the Lasso and Ridge Regression&lt;/h3&gt;
&lt;p&gt;In order to obtain a better intuition about the behavior of ridge regression and the lasso, consider a simple special case with $n = p$, and $X$ a diagonal matrix with $1$’s on the diagonal and $0$’s in all off-diagonal elements. To simplify the problem further, assume also that we are performing regression without an intercept.&lt;/p&gt;
&lt;p&gt;With these assumptions, the usual least squares problem simplifies to the coefficients that minimize&lt;/p&gt;
&lt;p&gt;$$
\sum_{j=1}^{p}\left(y_{j}-\beta_{j}\right)^{2}
$$&lt;/p&gt;
&lt;p&gt;In this case, the least squares solution is given by&lt;/p&gt;
&lt;p&gt;$$
\hat \beta_j = y_j
$$&lt;/p&gt;
&lt;p&gt;One can show that in this setting, the ridge regression estimates take the form&lt;/p&gt;
&lt;p&gt;$$
\hat \beta_j^{RIDGE} = \frac{y_j}{1+\lambda}
$$&lt;/p&gt;
&lt;p&gt;and the lasso estimates take the form&lt;/p&gt;
&lt;p&gt;$$
\hat{\beta}&lt;em&gt;{j}^{LASSO}=\left{\begin{array}{ll}
y&lt;/em&gt;{j}-\lambda / 2 &amp;amp; \text { if } y_{j}&amp;gt;\lambda / 2 \
y_{j}+\lambda / 2 &amp;amp; \text { if } y_{j}&amp;lt;-\lambda / 2 \
0 &amp;amp; \text { if }\left|y_{j}\right| \leq \lambda / 2
\end{array}\right.
$$&lt;/p&gt;
&lt;p&gt;We plot the relationship visually.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;np.random.seed(3)

# Generate random data
n = 100
k = n
X = np.eye(k)
e = normal(0,1,(n,1))
b0 = uniform(-1,1,(k,1))
y = X @ b0 + e
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# OLS regression
reg = LinearRegression().fit(X,y)
ols_coefs = reg.coef_;

# Ridge regression
ridge = Ridge(alpha=1).fit(X,y)
ridge_coefs = ridge.coef_;

# Ridge regression
lasso = Lasso(alpha=0.01).fit(X,y)
lasso_coefs = lasso.coef_.reshape(1,-1);

# sort
order = np.argsort(y.reshape(1,-1), axis=1)
y_sorted = np.take_along_axis(ols_coefs, order, axis=1) 
ols_coefs = np.take_along_axis(ols_coefs, order, axis=1) 
ridge_coefs = np.take_along_axis(ridge_coefs, order, axis=1) 
lasso_coefs = np.take_along_axis(lasso_coefs, order, axis=1) 
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Figure 6.10
def make_figure_6_10():
    
    # Init
    fig, (ax1,ax2) = plt.subplots(1,2)
    fig.suptitle(&#39;Figure 6.10&#39;)

    # Ridge
    ax1.plot(y_sorted.T, ols_coefs.T)
    ax1.plot(y_sorted.T, ridge_coefs.T)
    ax1.set_xlabel(&#39;True Coefficient&#39;); ax1.set_ylabel(&#39;Estimated Coefficient&#39;);
    ax1.legend([&#39;OLS&#39;,&#39;Ridge&#39;], fontsize=12);

    # Lasso
    ax2.plot(y_sorted.T, ols_coefs.T)
    ax2.plot(y_sorted.T, lasso_coefs.T)
    ax2.set_xlabel(&#39;True Coefficient&#39;); ax2.set_ylabel(&#39;Estimated Coefficient&#39;);
    ax2.legend([&#39;OLS&#39;,&#39;Lasso&#39;], fontsize=12);
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt; make_figure_6_10()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/05_regularization_117_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;We see that ridge regression shrinks every dimension of the data by the same proportion, whereas the lasso hrinks all coefficients toward zero by a similar amount, and sufficiently small coefficients are shrunken all the way to zero.&lt;/p&gt;
&lt;h3 id=&#34;selecting-the-tuning-parameter&#34;&gt;Selecting the Tuning Parameter&lt;/h3&gt;
&lt;p&gt;Implementing ridge regression and the lasso requires a method for selecting a value for the tuning parameter $\lambda$.&lt;/p&gt;
&lt;p&gt;Cross-validation provides a simple way to tackle this problem. We choose a grid of $\lambda$ values, and compute the cross-validation error for each value of $\lambda$. We then select the tuning parameter value for which the cross-validation error is smallest. Finally, the model is re-fit using all of the available observations and the selected value of the tuning parameter.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# X and y
categ_cols = [&amp;quot;Gender&amp;quot;, &amp;quot;Student&amp;quot;, &amp;quot;Married&amp;quot;, &amp;quot;Ethnicity&amp;quot;]
X = credit.loc[:, credit.columns != &#39;Balance&#39;]
X = pd.get_dummies(X, columns=categ_cols, drop_first=True).values
y = credit.loc[:,&#39;Balance&#39;]
n = len(credit)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We are going to use 10-fold CV as cross-validation algorithm.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Get MSE
def cv_lasso(X,y,a):
    # Init mse
    mse = []
    
    # Generate splits
    kf10 = KFold(n_splits=10, random_state=None, shuffle=False)
    kf10.get_n_splits(X)
    
    # Loop over splits
    for train_index, test_index in kf10.split(X):
        X_train, X_test = X[train_index], X[test_index]
        y_train, y_test = y[train_index], y[test_index]
        lasso = Lasso(alpha=a).fit(X_train, y_train)
        y_hat = lasso.predict(X_test)
        mse.append(mean_squared_error(y_test, y_hat))
    return np.mean(mse)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Compute MSE over grid of alphas
n_grid = 30
alphas = 10**np.linspace(0,3,n_grid).reshape(-1,1)
MSE = [cv_lasso(X,y,a) for a in alphas]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What is the optimal $\lambda$?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Find minimum alpha
alpha_min = alphas[np.argmin(MSE)]
print(&#39;Best alpha by 10fold CV:&#39;,alpha_min[0])
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Best alpha by 10fold CV: 2.592943797404667
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We now plot the objective function and the implied coefficients at the optimal $\lambda$.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Get coefficients
coefs = []

# Loop over values of alpha
for a in alphas:
    lasso = Lasso(alpha=a).fit(scale(X), y)
    coefs.append(lasso.coef_)
coefs = np.reshape(coefs,(n_grid,-1))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;np.shape(coefs)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;(30, 11)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Figure 6.12
def make_figure_6_12():
    
    # Init
    fig, (ax1,ax2) = plt.subplots(1,2)
    fig.suptitle(&#39;Figure 6.12: Lasso 10-fold CV&#39;)

    # MSE by LOO CV
    ax1.plot(alphas, MSE, alpha=1);
    ax1.axvline(alpha_min, c=&#39;k&#39;, ls=&#39;--&#39;)
    ax1.set_xscale(&#39;log&#39;)
    ax1.set_xlabel(&#39;lambda&#39;); ax1.set_ylabel(&#39;MSE&#39;);

    highlight = [0,1,2,7];

    # Plot coefficients - absolute
    ax2.plot(alphas, coefs[:,highlight], alpha=1)
    ax2.plot(alphas, coefs, c=&#39;grey&#39;, alpha=0.3)
    ax2.axvline(alpha_min, c=&#39;k&#39;, ls=&#39;--&#39;)
    ax2.set_xscale(&#39;log&#39;)
    ax2.set_xlabel(&#39;lambda&#39;); ax2.set_ylabel(&#39;Standardized coefficients&#39;);
    ax2.legend([&#39;Income&#39;, &#39;Limit&#39;, &#39;Rating&#39;, &#39;Student&#39;], fontsize=10);
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;make_figure_6_12()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/05_regularization_131_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Machine Learning Pipeline</title>
      <link>https://matteocourthoud.github.io/course/data-science/05_ml_pipeline/</link>
      <pubDate>Sun, 27 Feb 2022 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/course/data-science/05_ml_pipeline/</guid>
      <description>&lt;p&gt;In this notebook, we are going to build a pipeline for a general prediction problem.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Standard Imports
from src.utils import *
from src.get_feature_names import get_feature_names
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Set inline graphs
sns.set()
%matplotlib inline
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Usually, in machine learning prediction tasks, the data consists in 3 files:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;X_train.csv&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;y_train.csv&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;X_test.csv&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The purpose of the exercise is to produce a &lt;em&gt;y_test.csv&lt;/em&gt; file, with the predicted values corresponding to the &lt;em&gt;X_test.csv&lt;/em&gt; observations.&lt;/p&gt;
&lt;p&gt;The functions we will write are going to be general and will adapt to any type of dataset, and we will test them on the &lt;a href=&#34;https://www.kaggle.com/c/house-prices-advanced-regression-techniques&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;House Prices Dataset&lt;/a&gt; which is a standard dataset for these kind of tasks. The data consists of 2 files:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;train.csv&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;test.csv&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The target variable that we want to predict is &lt;code&gt;SalePrice&lt;/code&gt;.&lt;/p&gt;
&lt;h2 id=&#34;setup&#34;&gt;Setup&lt;/h2&gt;
&lt;p&gt;First we want to import the data.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Import data
df_train = pd.read_csv(&amp;quot;data/train.csv&amp;quot;)
df_test = pd.read_csv(&amp;quot;data/test.csv&amp;quot;)

print(f&amp;quot;Training data: {np.shape(df_train)} \n Testing data: {np.shape(df_test)}&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Training data: (1460, 81) 
 Testing data: (1459, 80)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The training data also includes the target variable &lt;code&gt;SalePrice&lt;/code&gt;, while, as usual, the testing data does not. We need to separate the training data into two parts:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;X&lt;/code&gt;: the &lt;strong&gt;features&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;y&lt;/code&gt;: the &lt;strong&gt;target&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Select the features
X_train = df_train.drop([&#39;SalePrice&#39;], axis=1)
X_test = df_test

# Check size
print(f&amp;quot;Training features: {np.shape(X_train)} \n Testing features: {np.shape(X_test)}&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Training features: (1460, 80) 
 Testing features: (1459, 80)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Select the target
y_train = df_train[&#39;SalePrice&#39;]

# Check size
print(f&amp;quot;Training target: {np.shape(y_train)}&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Training target: (1460,)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It&amp;rsquo;s good practice to immediately set aside a &lt;strong&gt;validation&lt;/strong&gt; sample with 20% of the observations. The purpose of the validation sample is to give us unbiased estimate of the prediction score. Therefore, we want to set it aside as soon as possible, not to be conditioned in any way by it. Possibly, set it away even before data exploration.&lt;/p&gt;
&lt;p&gt;The more we tune the algorithm based on the feedback received from the validation sample, the more biased our estimate is going to be. Ideally, one would use only cross-validation on the training data and tune only a couple of times using the validation data.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Set aside the validation sample
X_train, X_validation, y_train, y_validation = train_test_split(X_train, y_train, test_size=0.2)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we are ready to build and test our pipeline.&lt;/p&gt;
&lt;h2 id=&#34;data-exploration&#34;&gt;Data Exploration&lt;/h2&gt;
&lt;p&gt;First, let&amp;rsquo;s have a quick look at the data.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;X_train.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;Id&lt;/th&gt;
      &lt;th&gt;MSSubClass&lt;/th&gt;
      &lt;th&gt;MSZoning&lt;/th&gt;
      &lt;th&gt;LotFrontage&lt;/th&gt;
      &lt;th&gt;LotArea&lt;/th&gt;
      &lt;th&gt;Street&lt;/th&gt;
      &lt;th&gt;Alley&lt;/th&gt;
      &lt;th&gt;LotShape&lt;/th&gt;
      &lt;th&gt;LandContour&lt;/th&gt;
      &lt;th&gt;Utilities&lt;/th&gt;
      &lt;th&gt;...&lt;/th&gt;
      &lt;th&gt;ScreenPorch&lt;/th&gt;
      &lt;th&gt;PoolArea&lt;/th&gt;
      &lt;th&gt;PoolQC&lt;/th&gt;
      &lt;th&gt;Fence&lt;/th&gt;
      &lt;th&gt;MiscFeature&lt;/th&gt;
      &lt;th&gt;MiscVal&lt;/th&gt;
      &lt;th&gt;MoSold&lt;/th&gt;
      &lt;th&gt;YrSold&lt;/th&gt;
      &lt;th&gt;SaleType&lt;/th&gt;
      &lt;th&gt;SaleCondition&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;563&lt;/th&gt;
      &lt;td&gt;564&lt;/td&gt;
      &lt;td&gt;50&lt;/td&gt;
      &lt;td&gt;RL&lt;/td&gt;
      &lt;td&gt;66.0&lt;/td&gt;
      &lt;td&gt;21780&lt;/td&gt;
      &lt;td&gt;Pave&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;Reg&lt;/td&gt;
      &lt;td&gt;Lvl&lt;/td&gt;
      &lt;td&gt;AllPub&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;144&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;7&lt;/td&gt;
      &lt;td&gt;2008&lt;/td&gt;
      &lt;td&gt;WD&lt;/td&gt;
      &lt;td&gt;Normal&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;48&lt;/th&gt;
      &lt;td&gt;49&lt;/td&gt;
      &lt;td&gt;190&lt;/td&gt;
      &lt;td&gt;RM&lt;/td&gt;
      &lt;td&gt;33.0&lt;/td&gt;
      &lt;td&gt;4456&lt;/td&gt;
      &lt;td&gt;Pave&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;Reg&lt;/td&gt;
      &lt;td&gt;Lvl&lt;/td&gt;
      &lt;td&gt;AllPub&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;6&lt;/td&gt;
      &lt;td&gt;2009&lt;/td&gt;
      &lt;td&gt;New&lt;/td&gt;
      &lt;td&gt;Partial&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;148&lt;/th&gt;
      &lt;td&gt;149&lt;/td&gt;
      &lt;td&gt;20&lt;/td&gt;
      &lt;td&gt;RL&lt;/td&gt;
      &lt;td&gt;63.0&lt;/td&gt;
      &lt;td&gt;7500&lt;/td&gt;
      &lt;td&gt;Pave&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;Reg&lt;/td&gt;
      &lt;td&gt;Lvl&lt;/td&gt;
      &lt;td&gt;AllPub&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;2008&lt;/td&gt;
      &lt;td&gt;WD&lt;/td&gt;
      &lt;td&gt;Normal&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;801&lt;/th&gt;
      &lt;td&gt;802&lt;/td&gt;
      &lt;td&gt;30&lt;/td&gt;
      &lt;td&gt;RM&lt;/td&gt;
      &lt;td&gt;40.0&lt;/td&gt;
      &lt;td&gt;4800&lt;/td&gt;
      &lt;td&gt;Pave&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;Reg&lt;/td&gt;
      &lt;td&gt;Lvl&lt;/td&gt;
      &lt;td&gt;AllPub&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;7&lt;/td&gt;
      &lt;td&gt;2007&lt;/td&gt;
      &lt;td&gt;WD&lt;/td&gt;
      &lt;td&gt;Normal&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1374&lt;/th&gt;
      &lt;td&gt;1375&lt;/td&gt;
      &lt;td&gt;60&lt;/td&gt;
      &lt;td&gt;FV&lt;/td&gt;
      &lt;td&gt;85.0&lt;/td&gt;
      &lt;td&gt;10625&lt;/td&gt;
      &lt;td&gt;Pave&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;Reg&lt;/td&gt;
      &lt;td&gt;Lvl&lt;/td&gt;
      &lt;td&gt;AllPub&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;7&lt;/td&gt;
      &lt;td&gt;2008&lt;/td&gt;
      &lt;td&gt;WD&lt;/td&gt;
      &lt;td&gt;Normal&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;5 rows × 80 columns&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;The &lt;code&gt;Id&lt;/code&gt; column is clearly not useful for prediction, let&amp;rsquo;s drop it from both datasets.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Drop Id 
X_train.drop([&amp;quot;Id&amp;quot;], axis=1, inplace=True)
X_test.drop([&amp;quot;Id&amp;quot;], axis=1, inplace=True)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we want to identify categorical and numerical variables.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Save column types
numerical_cols = list(X_train.describe().columns)
categorical_cols = list(X_train.describe(include=object).columns)
print(&amp;quot;There are %i numerical and %i categorical variables&amp;quot; % (len(numerical_cols), len(categorical_cols)))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;There are 36 numerical and 43 categorical variables
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s start by analyzing the numerical variables.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;X_numerical = X_train.loc[:, numerical_cols]
corr = X_numerical.corr()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;fig, ax = plt.subplots(1, 1, figsize=(10,10))
fig.suptitle(&amp;quot;Correlation between categorical variables&amp;quot;, fontsize=16)
cbar_ax = fig.add_axes([.95, .12, .05, .76])
sns.heatmap(corr, vmin=-1, vmax=1, center=0, cmap=sns.diverging_palette(20, 220, n=20), 
            square=True, ax=ax, cbar_ax = cbar_ax)
plt.show()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/05_ml_pipeline_24_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;For the non/numeric columns, we need a further option.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;unique_values = X_train.describe(include=object).T.unique
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Plot
fig, ax = plt.subplots(1, 1, figsize=(10,6))
fig.suptitle(&amp;quot;Distribution of unique values for categorical variables&amp;quot;, fontsize=16)
sns.histplot(data=unique_values)
plt.show();
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/05_ml_pipeline_27_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s save the identity of the numerical and categorical columns.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Save column types
numerical_cols = list(X_train.describe().columns)
categorical_cols = list(X_train.describe(include=object).columns)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;How many missing values are there in the dataset?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;missing_values = X_train.isnull().sum().sort_values(ascending=True)[-20:] / len(X_train)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;fig, ax = plt.subplots(figsize=(10,8))
ax.set_title(&amp;quot;Variables with most missing values&amp;quot;, fontsize=16)
ax.barh(np.arange(len(missing_values)), missing_values)
ax.set_yticks(np.arange(len(missing_values)))
ax.set_yticklabels(missing_values.index)
ax.set_xlabel(&#39;Percentage of missig values&#39;)
plt.show()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/05_ml_pipeline_32_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Around 10% of each feature is missing. We will have to deal with that.&lt;/p&gt;
&lt;h2 id=&#34;pre-processing&#34;&gt;Pre-processing&lt;/h2&gt;
&lt;p&gt;First, let&amp;rsquo;s process &lt;strong&gt;numerical variables&lt;/strong&gt;. We want to do two things:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;inpute missing values&lt;/li&gt;
&lt;li&gt;standardize all variables&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Which imputer should to use? It depends on the &lt;strong&gt;type of missing data&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Missing absolutely at random&lt;/strong&gt;: as the name says, in this case we believe that missing values are distributed uniformly at random, independently across variables.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;In this case, the only information on missing values comes from the distribution of non-missing values of the same variable.&lt;/li&gt;
&lt;li&gt;No information on missing values is contained in other variables.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Missing at random&lt;/strong&gt;: in this case, missing values are random, conditional on values of other observed variables.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;In this case, information in other variables might help filling missing values.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Missing non at random&lt;/strong&gt;: in this last case, missing values depend on information that we do not observe.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;This is the most tricky category of missing values since data alone does not tell us which values might be missing. For example, we might have that older women might be less likely to report the age.&lt;/li&gt;
&lt;li&gt;If we consider the data missing at random (absolutely or not), we would underestimate the missing ages.&lt;/li&gt;
&lt;li&gt;External information such as the sample population might help. For example, we could estimate the probability of not reporting the age and fill the missing values with the expected age, &lt;em&gt;conditional&lt;/em&gt; on age not being reported.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;So, which imputers are readily available in &lt;code&gt;sklearn&lt;/code&gt; for numerical data?&lt;/p&gt;
&lt;p&gt;For data &lt;strong&gt;missing absolutely at random&lt;/strong&gt;, there is one standard &lt;code&gt;sklearn&lt;/code&gt; library: &lt;code&gt;SimpleImputer()&lt;/code&gt;. It allows different &lt;code&gt;strategy&lt;/code&gt; options such as&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;&amp;quot;mean&amp;quot;&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;&amp;quot;median&amp;quot;&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;&amp;quot;most_frequent&amp;quot;&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For data &lt;strong&gt;missing at random&lt;/strong&gt;, there are multiple &lt;code&gt;sklearn&lt;/code&gt; libraries:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;KNNImputer()&lt;/code&gt;: uses KNN&lt;/li&gt;
&lt;li&gt;&lt;code&gt;IterativeImputer()&lt;/code&gt;: uses a variety of ML algorithms
&lt;ul&gt;
&lt;li&gt;see comparison &lt;a href=&#34;https://scikit-learn.org/stable/auto_examples/impute/plot_iterative_imputer_variants_comparison.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;After we have inputed missing values, we want to standardize numerical variables to make the algorithm more efficient and robust to outliers.&lt;/p&gt;
&lt;p&gt;The two main options for standardization are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;StandardScaler()&lt;/code&gt;: which normalizes each variable to mean zero and unit variance&lt;/li&gt;
&lt;li&gt;&lt;code&gt;MinMaxScaler()&lt;/code&gt;: which normalizes each variable to an interval between zero an one&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Inputer for numerical variables
num = Pipeline(steps=[
    (&#39;ii&#39;, IterativeImputer()),
    (&#39;ss&#39;, StandardScaler())
    ])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For &lt;strong&gt;categorical variables&lt;/strong&gt;, we do not have to worry about scaling. However, we still need to impute missing values and, crucially, we need to transform them into numerical variables. This process is called &lt;strong&gt;encoding&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Which imputer should to use?&lt;/p&gt;
&lt;p&gt;For data &lt;strong&gt;missing absolutely at random&lt;/strong&gt;, the only available &lt;code&gt;strategy&lt;/code&gt; option for &lt;code&gt;SimpleImputer()&lt;/code&gt; is&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;&amp;quot;most_frequent&amp;quot;&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For data &lt;strong&gt;missing at random&lt;/strong&gt;, we can still use both&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;KNNImputer()&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;IterativeImputer()&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For &lt;strong&gt;encoding&lt;/strong&gt; categorical variables, the standard option is &lt;code&gt;OneHotEncoder()&lt;/code&gt; which generates unique binary variables out of every values of the categorical variable.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# One Hot Encoder for categorical data
cat = Pipeline(steps=[
    (&#39;si&#39;, SimpleImputer(strategy=&amp;quot;most_frequent&amp;quot;)),
    (&#39;ohe&#39;, OneHotEncoder(handle_unknown=&amp;quot;ignore&amp;quot;)),
    ])
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Preprocess column transformer for preprocessing data
preprocess = ColumnTransformer(
                    transformers=[
                        (&#39;num&#39;, num, numerical_cols),
                        (&#39;cat&#39;, cat, categorical_cols),
                    ])
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;information-and-components&#34;&gt;Information and components&lt;/h2&gt;
&lt;p&gt;How much information is contained in our dataset? It is a dense or sparse dataset?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;X_clean = num.fit_transform(X_numerical)
pca = PCA().fit(X_clean)
explained_variance = pca.explained_variance_ratio_
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;fig, (ax1, ax2) = plt.subplots(1,2, figsize=(15,6))
fig.suptitle(&#39;Principal Component Analysis&#39;, fontsize=16);

# Relative 
ax1.plot(range(len(explained_variance)), explained_variance)
ax1.set_ylabel(&#39;Prop. Variance Explained&#39;)
ax1.set_xlabel(&#39;Principal Component&#39;);

# Cumulative
ax2.plot(range(len(explained_variance)), np.cumsum(explained_variance))
ax2.set_ylabel(&#39;Cumulative Variance Explained&#39;);
ax2.set_xlabel(&#39;Principal Component&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/05_ml_pipeline_48_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;feature-importance&#34;&gt;Feature Importance&lt;/h2&gt;
&lt;p&gt;Before starting our prediction analysis, we would like to understand which variables are most important for our prediction problem.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def plot_featureimportance(importance, preprocess):
    df = pd.DataFrame({&amp;quot;names&amp;quot;: get_feature_names(preprocess), &amp;quot;values&amp;quot;: importance})
    df = df.sort_values(&amp;quot;values&amp;quot;).iloc[:20, :]
    # plot
    fig, ax = plt.subplots(figsize=(10,8))
    ax.set_title(&amp;quot;Feature importance&amp;quot;, fontsize=16)
    sns.barplot(y=&amp;quot;names&amp;quot;, x=&amp;quot;values&amp;quot;, data=df)
    ax.barh(np.arange(len(df)), df[&amp;quot;values&amp;quot;])
    plt.show()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We start with linear regression feature importance: we standardize all variables to be mean vero and unit variance, and we run a linear regression over the test set.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def featureimportance_lr(X, y):
    X_clean = preprocess.fit_transform(X)
    # fit the model
    model = LinearRegression()
    model.fit(X_clean, y)
    # get importance
    importance = np.abs(model.coef_)
    plot_featureimportance(importance, preprocess)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Plot linear feature importance
featureimportance_lr(X_train, y_train)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/05_ml_pipeline_54_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;We now look at regression tree feature importance.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def featureimportance_forest(X, y):
    X_clean = preprocess.fit_transform(X)
    # fit the model
    model = RandomForestRegressor()
    model.fit(X_clean, y)
    # get importance
    importance = model.feature_importances_
    plot_featureimportance(importance, preprocess)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Plot tree feature importance
featureimportance_forest(X_train, y_train)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/05_ml_pipeline_57_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;weighting&#34;&gt;Weighting&lt;/h2&gt;
&lt;p&gt;Another important check to perform concerns weighting. Is the distribution of our objective variable the same in the training and in the test sample? If it is not the case, we might get a poor performance just because our training sample is not representative of our testing sample.&lt;/p&gt;
&lt;p&gt;This is something that usually &lt;strong&gt;we cannot test&lt;/strong&gt;, since we do not have access to the distribution of the target variable in the test data. However, we might be given the information ex-ante as a warning.&lt;/p&gt;
&lt;p&gt;In this case, we perform the analysis on the validation set. Since we have selected the validation set at random, we do not expect significant differences.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15,6))

# Plot 1
sns.histplot(data=y_train, kde=True, ax=ax1)
sns.histplot(data=y_validation, kde=True, ax=ax1, color=&#39;orange&#39;)
ax1.set_title(&amp;quot;Density Function of y&amp;quot;, fontsize=16);
ax1.legend([&#39;y train&#39;, &#39;y validation&#39;])

# Plot 2
sns.histplot(data=y_train,  element=&amp;quot;step&amp;quot;, fill=False,
    cumulative=True, stat=&amp;quot;density&amp;quot;, common_norm=False, ax=ax2)
sns.histplot(data=y_validation, element=&amp;quot;step&amp;quot;, fill=False,
    cumulative=True, stat=&amp;quot;density&amp;quot;, common_norm=False, ax=ax2, color=&#39;orange&#39;)
ax2.set_title(&amp;quot;Cumulative Distribution of y&amp;quot;, fontsize=16);
ax2.legend([&#39;y train&#39;, &#39;y validation&#39;]);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/05_ml_pipeline_60_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Since the size of the test sample is smaller than the size of the training sample, the two densities are different. However, the distributions indicate that the standardized distributions are the same.&lt;/p&gt;
&lt;h2 id=&#34;model&#34;&gt;Model&lt;/h2&gt;
&lt;p&gt;There are many models to choose among.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# prepare models
models = {&amp;quot;Lasso&amp;quot;: Lasso(alpha=100),
          &amp;quot;Ridge&amp;quot;: BayesianRidge(),
          &amp;quot;KNN&amp;quot;: KNeighborsRegressor(),
          &amp;quot;Kernel&amp;quot;: KernelRidge(),
          &amp;quot;Naive&amp;quot;: GaussianNB(),
          &amp;quot;SVM&amp;quot;: SVR(),
          &amp;quot;Ada&amp;quot;: AdaBoostRegressor(),
          &amp;quot;Tree&amp;quot;: DecisionTreeRegressor(),
          &amp;quot;Forest&amp;quot;: RandomForestRegressor(),
          &amp;quot;GBoost&amp;quot;: GradientBoostingRegressor(),
          &amp;quot;XGBoost&amp;quot;: XGBRegressor(),
          &amp;quot;LGBoost&amp;quot;: LGBMRegressor()}
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def evaluate_model(model, name, X, y, cv, scoring):
    X_clean = preprocess.fit_transform(X)
    start = time.perf_counter()
    cv_results = cross_val_score(model, X_clean, y, cv=cv, scoring=scoring)
    t = time.perf_counter()-start
    score = {&amp;quot;model&amp;quot;:name, &amp;quot;mean&amp;quot;:-np.mean(cv_results), &amp;quot;std&amp;quot;:np.std(cv_results), &amp;quot;time&amp;quot;:t}
    print(&amp;quot;%s: %f (%f) in %f seconds&amp;quot; % (name, -np.mean(cv_results), np.std(cv_results), t))
    return score
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def plot_model_scores(scores):
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15,6))
    fig.suptitle(&amp;quot;Comparing algorithms&amp;quot;, fontsize=16)
    
    # Plot 1
    scores.sort_values(&amp;quot;mean&amp;quot;, ascending=False, inplace=True)
    ax1.set_title(&amp;quot;Mean squared error&amp;quot;, fontsize=16)
    ax1.barh(range(len(scores)), scores[&amp;quot;mean&amp;quot;], xerr=scores[&amp;quot;std&amp;quot;])
    ax1.set_yticks(range(len(scores)))
    ax1.set_yticklabels([s for s in scores[&amp;quot;model&amp;quot;]])
    
    # Plot 2
    scores.sort_values(&amp;quot;time&amp;quot;, ascending=False, inplace=True)
    ax2.set_title(&amp;quot;Time&amp;quot;, fontsize=16)
    ax2.barh(range(len(scores)), scores[&amp;quot;time&amp;quot;], color=&#39;tab:orange&#39;)
    ax2.set_yticks(range(len(scores)))
    ax2.set_yticklabels([s for s in scores[&amp;quot;model&amp;quot;]])
    plt.show()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def compare_models(models):
    scores = pd.DataFrame()
    cv = KFold(n_splits=5)
    scoring = &#39;neg_mean_squared_error&#39;
    for name, model in models.items():
        score = evaluate_model(model, name, X_validation, y_validation, cv, scoring)
        scores = scores.append(score, ignore_index=True)
    return scores
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;scores = compare_models(models)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Lasso: 1309794414.859426 (1313338809.546610) in 0.126276 seconds
Ridge: 715278312.108788 (217652788.836596) in 0.582577 seconds
KNN: 1413048071.753012 (430146510.332671) in 0.016970 seconds
Kernel: 1477920100.374237 (1401354461.497433) in 0.083176 seconds
Naive: 3649023680.704793 (1021600484.370466) in 0.041219 seconds
SVM: 5817131318.486010 (1454335763.390944) in 0.044825 seconds
Ada: 1022540011.988822 (407906261.283972) in 0.302022 seconds
Tree: 1709270763.177791 (294314053.687804) in 0.017814 seconds
Forest: 865948793.183670 (397233796.258241) in 1.088624 seconds
GBoost: 778348874.875873 (314733456.217197) in 0.492180 seconds
XGBoost: 1055015638.687999 (377350793.401314) in 0.586652 seconds
LGBoost: 877619178.062008 (311547130.916242) in 0.271566 seconds
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plot_model_scores(scores)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/05_ml_pipeline_69_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;pipeline&#34;&gt;Pipeline&lt;/h2&gt;
&lt;p&gt;We are now ready to pick a model.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Set model
model = LGBMRegressor()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We need to choose a cross-validation procedure to test our model.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;cv = KFold()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally, we can combine all the parts into a single pipeline.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;final_pipeline = Pipeline(steps=[
        (&#39;preprocess&#39;, preprocess),
        (&#39;model&#39;, model)
        ])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we can decide which parts of the pipeline to test.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Select parameters to explore
param_grid = {&#39;preprocess__num__ii&#39;: [SimpleImputer(), KNNImputer(), IterativeImputer()],
              &#39;preprocess__cat__si__strategy&#39;: [&amp;quot;most_frequent&amp;quot;, &amp;quot;constant&amp;quot;],
              &#39;model__learning_rate&#39;: [0.1, 0.2],
              &#39;model__subsample&#39;: [1.0, 0.5],
              &#39;model__max_depth&#39;: [30, -1]}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We now generate a grid of parameters we want to search over.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Save pipeline
grid_search = GridSearchCV(final_pipeline, 
                           param_grid, 
                           cv=cv,
                           n_jobs=-1, 
                           scoring=&#39;neg_mean_squared_error&#39;,
                           verbose=3)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We fit the pipeline and pick the best estimator, from the cross-validation score.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Fit pipeline
grid_search.fit(X_train, y_train)
grid_search.best_estimator_
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Fitting 5 folds for each of 48 candidates, totalling 240 fits





Pipeline(steps=[(&#39;preprocess&#39;,
                 ColumnTransformer(transformers=[(&#39;num&#39;,
                                                  Pipeline(steps=[(&#39;ii&#39;,
                                                                   SimpleImputer()),
                                                                  (&#39;ss&#39;,
                                                                   StandardScaler())]),
                                                  [&#39;MSSubClass&#39;, &#39;LotFrontage&#39;,
                                                   &#39;LotArea&#39;, &#39;OverallQual&#39;,
                                                   &#39;OverallCond&#39;, &#39;YearBuilt&#39;,
                                                   &#39;YearRemodAdd&#39;, &#39;MasVnrArea&#39;,
                                                   &#39;BsmtFinSF1&#39;, &#39;BsmtFinSF2&#39;,
                                                   &#39;BsmtUnfSF&#39;, &#39;TotalBsmtSF&#39;,
                                                   &#39;1stFlrSF&#39;, &#39;2ndFlrSF&#39;,
                                                   &#39;LowQualFinSF&#39;, &#39;GrLivArea&#39;,
                                                   &#39;BsmtFull...
                                                   &#39;LotConfig&#39;, &#39;LandSlope&#39;,
                                                   &#39;Neighborhood&#39;, &#39;Condition1&#39;,
                                                   &#39;Condition2&#39;, &#39;BldgType&#39;,
                                                   &#39;HouseStyle&#39;, &#39;RoofStyle&#39;,
                                                   &#39;RoofMatl&#39;, &#39;Exterior1st&#39;,
                                                   &#39;Exterior2nd&#39;, &#39;MasVnrType&#39;,
                                                   &#39;ExterQual&#39;, &#39;ExterCond&#39;,
                                                   &#39;Foundation&#39;, &#39;BsmtQual&#39;,
                                                   &#39;BsmtCond&#39;, &#39;BsmtExposure&#39;,
                                                   &#39;BsmtFinType1&#39;,
                                                   &#39;BsmtFinType2&#39;, &#39;Heating&#39;,
                                                   &#39;HeatingQC&#39;, &#39;CentralAir&#39;,
                                                   &#39;Electrical&#39;, ...])])),
                (&#39;model&#39;, LGBMRegressor(max_depth=30))])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We have three ways of testing the quality of fit of our model:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;score on the training data&lt;/li&gt;
&lt;li&gt;score on the validation data&lt;/li&gt;
&lt;li&gt;score on the test data&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Score on the training data&lt;/strong&gt;: this is a biased score since we have picked the model that was best fitting the training data. Kfold cross-validation is efficient in terms of data use, but still evaluates the model over the same data it was trained.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Cross/validation score
y_train_hat = grid_search.best_estimator_.predict(X_train)
train_rmse = mean_squared_error(y_train, y_train_hat, squared=False)
print(&#39;RMSE on training data :&#39;, train_rmse)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;RMSE on training data : 12343.48585393485
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Score on the validation data&lt;/strong&gt;: this is an unbiased score since we have left out this sample exactly for this purpose. However, be aware that the validation score is unbiased on on the first run. Once we change the grid and pick the algorithm based on previous validation data scores, also this score becomes biased.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Validation set score
y_validation_hat = grid_search.best_estimator_.predict(X_validation)
validation_rmse = mean_squared_error(y_validation, y_validation_hat, squared=False)
print(&#39;RMSE on validation data :&#39;, validation_rmse)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;RMSE on validation data : 26213.18659495431
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Final predictions&lt;/strong&gt;: we can now use our model to output the predictions.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Validation score
y_test_hat = grid_search.best_estimator_.predict(X_test)
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>OLS Algebra</title>
      <link>https://matteocourthoud.github.io/course/metrics/05_ols_algebra/</link>
      <pubDate>Fri, 29 Oct 2021 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/course/metrics/05_ols_algebra/</guid>
      <description>&lt;h2 id=&#34;the-gauss-markov-model&#34;&gt;The Gauss Markov Model&lt;/h2&gt;
&lt;h3 id=&#34;definition&#34;&gt;Definition&lt;/h3&gt;
&lt;p&gt;A statistical model for regression data is the &lt;strong&gt;Gauss Markov Model&lt;/strong&gt; if
each of its distributions satisfies the conditions&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Linearity&lt;/strong&gt;: a statistical model $\mathcal{F}$ over data
$\mathcal{D}$ satisfies linearity if for each element of
$\mathcal{F}$, the data can be decomposed in $$
\begin{aligned}
y_ i &amp;amp;= \beta_ 1 x _ {i1} + \dots + \beta_ k x _ {ik} + \varepsilon_ i = x_ i&#39;\beta + \varepsilon_ i \newline
\underset{n \times 1}{\vphantom{\beta_ \beta} y} &amp;amp;= \underset{n \times k}{\vphantom{\beta}X} \cdot \underset{k \times 1}{\beta} + \underset{n \times 1}{\vphantom{\beta}\varepsilon}
\end{aligned}
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Strict Exogeneity&lt;/strong&gt;:
$\mathbb E [\varepsilon_i|x_1, \dots, x_n] = 0, \forall i$.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;No Multicollinerity&lt;/strong&gt;: $\mathbb E_n [x_i x_i&#39;]$ is strictly
positive definite almost surely. Equivalent to require $rank(X)=k$
with probability $p \to 1$. Intuition: no regressor is a linear
combination of other regressors.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Spherical Error Variance&lt;/strong&gt;:
-$\mathbb E[\varepsilon_i^2 | x] = \sigma^2 &amp;gt; 0, \ \forall i$
-$\mathbb E [\varepsilon_i \varepsilon_j |x ] = 0, \ \forall$
$1 \leq i &amp;lt; j \leq n$&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The &lt;strong&gt;Extended Gauss Markov Model&lt;/strong&gt; also satisfies assumption&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Normal error term&lt;/strong&gt;: $\varepsilon|X \sim N(0, \sigma^2 I_n)$ and
$\varepsilon \perp X$.&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;implications&#34;&gt;Implications&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Note that by (2) and (4) you get &lt;strong&gt;homoskedasticity&lt;/strong&gt;:&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$
Var(\varepsilon_i|x) = \mathbb E[\varepsilon_i^2|x]- \mathbb E[\varepsilon_i|x]^2 = \sigma^2 I \qquad \forall i
$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Strict exogeneity is not restrictive since it is sufficient to
include a constant in the regression to enforce it $$
y_i = \alpha + x_i&#39;\beta + (\varepsilon_i - \alpha) \quad \Rightarrow \quad \mathbb E[\varepsilon_i] = \mathbb E_x [ \mathbb E[ \varepsilon_i | x]] = 0
$$&lt;/li&gt;
&lt;li&gt;This implies $\mathbb E[x _ {jk} \varepsilon_i ] = 0$ by the LIE.&lt;/li&gt;
&lt;li&gt;These two conditions together imply
$Cov (x _ {jk} \varepsilon_i ) = 0$.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;projection&#34;&gt;Projection&lt;/h3&gt;
&lt;p&gt;A map $\Pi: V \to V$ is a &lt;strong&gt;projection&lt;/strong&gt; if $\Pi \circ \Pi = \Pi$.&lt;/p&gt;
&lt;p&gt;The Gauss Markov Model assumes that the &lt;strong&gt;conditional expectation
function (CEF)&lt;/strong&gt; $f(X) = \mathbb E[Y|X]$ and the &lt;strong&gt;linear projection&lt;/strong&gt;
$g(X) = X \beta$ coincide.&lt;/p&gt;
&lt;h3 id=&#34;code---dgp&#34;&gt;Code - DGP&lt;/h3&gt;
&lt;p&gt;This code draws 100 observations from the model
$y = 2 x_1 - x_2 + \varepsilon$ where $x_1, x_2 \sim U[0,1]$ and
$\varepsilon \sim N(0,1)$.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# Set seed
Random.seed!(123);

# Set the number of observations
n = 100;

# Set the dimension of X
k = 2;

# Draw a sample of explanatory variables
X = rand(Uniform(0,1), n, k);

# Draw the error term
σ = 1;
ε = rand(Normal(0,1), n, 1) * sqrt(σ);

# Set the parameters
β = [2; -1];

# Calculate the dependent variable
y = X*β + ε;
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;the-ols-estimator&#34;&gt;The OLS estimator&lt;/h2&gt;
&lt;h3 id=&#34;definition-1&#34;&gt;Definition&lt;/h3&gt;
&lt;p&gt;The &lt;strong&gt;sum of squared residuals (SSR)&lt;/strong&gt; is given by $$
Q_n (\beta) \equiv   \frac{1}{n} \sum _ {i=1}^n \left( y_i - x_i&#39;\beta \right)^2 = \frac{1}{n} (y - X\beta)&#39; (y - X \beta)
$$&lt;/p&gt;
&lt;p&gt;Consider a dataset $\mathcal{D}$ and define
$Q_n(\beta) = \mathbb E_n[(y_i - x_i&#39;\beta )^2 ]$. Then the &lt;strong&gt;ordinary
least squares (OLS)&lt;/strong&gt; estimator $\hat \beta _ {OLS}$ is the value of
$\beta$ that minimizes $Q_n(\beta)$.&lt;/p&gt;
&lt;p&gt;When we can write $D = (y, X)$ in matrix form, then $$
\hat \beta _ {OLS} = \arg \min_\beta \frac{1}{n} (y - X \beta)&#39; (y - X\beta)
$$&lt;/p&gt;
&lt;h3 id=&#34;derivation&#34;&gt;Derivation&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Under the assumption that $X$ has full rank, the OLS estimator is unique
and it is determined by the normal equations. More explicitly,
$\hat \beta$ is the OLS estimate precisely when $X&amp;rsquo;X \hat \beta = X&amp;rsquo;y$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Taking the FOC: $$
\frac{\partial Q_n (\beta)}{\partial \beta} = -\frac{2}{n} X&#39; y  + \frac{2}{n} X&amp;rsquo;X\beta = 0 \quad \Leftrightarrow \quad X&amp;rsquo;X \beta = X&amp;rsquo;y
$$ Since $(X&amp;rsquo;X)^{-1}$ exists by assumption,&lt;/p&gt;
&lt;p&gt;Finally,
$\frac{\partial^2 Q_n (\beta)}{\partial \beta \partial \beta&#39;} = X&amp;rsquo;X/n$
is positive definite since $X&amp;rsquo;X$ is positive semi-definite and
$(X&amp;rsquo;X)^{-1}$ exists because $X$ is full rank. Therefore, $Q_n(\beta)$
minimized at $\hat \beta_n$. $$\tag*{$\blacksquare$}$$&lt;/p&gt;
&lt;p&gt;The $k$ equations $X&amp;rsquo;X \hat \beta = X&amp;rsquo;y$ are called &lt;strong&gt;normal
equations&lt;/strong&gt;.&lt;/p&gt;
&lt;h3 id=&#34;futher-objects&#34;&gt;Futher Objects&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Fitted coefficient:
$\hat \beta _ {OLS} = (X&amp;rsquo;X)^{-1} X&amp;rsquo;y = \mathbb E_n [x_i x_i&#39;] \mathbb E_n [x_i y_i]$&lt;/li&gt;
&lt;li&gt;Fitted residual: $\hat \varepsilon_i = y_i - x_i&#39;\hat \beta$&lt;/li&gt;
&lt;li&gt;Fitted value: $\hat y_i = x_i&#39; \hat \beta$&lt;/li&gt;
&lt;li&gt;Predicted coefficient:
$\hat \beta _ {-i} = \mathbb E_n [x _ {-i} x&#39; _ {-i}] \mathbb E_n [x _ {-i} y _ {-i}]$&lt;/li&gt;
&lt;li&gt;Prediction error:
$\hat \varepsilon _ {-i} = y_i - x_i&#39;\hat \beta _ {-i}$&lt;/li&gt;
&lt;li&gt;Predicted value: $\hat y_i = x_i&#39; \hat \beta _ {-i}$&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;notes-on-orthogonality-conditions&#34;&gt;Notes on Orthogonality Conditions&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;The normal equations are equivalent to the moment condition
$\mathbb E_n [x_i \varepsilon_i]= 0$.&lt;/li&gt;
&lt;li&gt;The algebraic result $\mathbb E_n [x_i \hat \varepsilon_i]= 0$ is
called &lt;strong&gt;ortogonality property&lt;/strong&gt; of the OLS residual
$\hat \varepsilon_i$.&lt;/li&gt;
&lt;li&gt;If we have included a constant in the regression,
$\mathbb E_n [\hat \varepsilon_i] = 0$.&lt;/li&gt;
&lt;li&gt;$\mathbb E \Big[\mathbb E_n [x_i \varepsilon_i ] \Big] = 0$ by
strict exogeneity (assumed in GM), but
$\mathbb E_n [x_i \varepsilon_i] \ne \mathbb E [x_i \varepsilon_i] = 0$.
This is why $\hat \beta _ {OLS}$ is just an estimate of $\beta_0$.&lt;/li&gt;
&lt;li&gt;Calculating OLS is like replacing the $j$ equations
$\mathbb E [x _ {ij} \varepsilon_i] = 0$ $\forall j$ with
$\mathbb E_n [x _ {ij} \varepsilon_i] = 0$ $\forall j$ and forcing
them to hold (remindful of GMM).&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;the-projection-matrix&#34;&gt;The Projection Matrix&lt;/h3&gt;
&lt;p&gt;The &lt;strong&gt;projection matrix&lt;/strong&gt; is given by $P = X(X&amp;rsquo;X)^{-1} X&#39;$. It has the
following properties: - $PX = X$ - $P \hat \varepsilon = 0 \quad$ ($P$,
$\varepsilon$ orthogonal) -
$P y = X(X&amp;rsquo;X)^{-1} X&amp;rsquo;y = X\hat \beta = \hat y$ - Symmetric: $P=P&#39;$,
Idempotent: $PP = P$ -
$tr(P) = tr( X(X&amp;rsquo;X)^{-1} X&#39;) = tr( X&amp;rsquo;X(X&amp;rsquo;X)^{-1}) = tr(I_k) = k$ - Its
diagonal elements are $h_{ii} = x_i (X&amp;rsquo;X)^{-1} x_i&#39;$ and are called
&lt;strong&gt;leverage&lt;/strong&gt;.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;$h _ {ii} \in [0,1]$ is a normalized length of the observed regressor
vector $x_i$. In the OLS regression framework it captures the relative
influence of observation $i$ on the estimated coefficient. Note that
$\sum _ n h_{ii} = k$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;the-annihilator-matrix&#34;&gt;The Annihilator Matrix&lt;/h3&gt;
&lt;p&gt;The &lt;strong&gt;annihilator matrix&lt;/strong&gt; is given by $M = I_n - P$. It has the
following properties: - $MX = 0 \quad$ ($M$, $X$ orthogonal) -
$M \hat \varepsilon = \hat \varepsilon$ - $M y = \hat \varepsilon$ -
Symmetric: $M=M&#39;$, idempotent: $MM = M$ - $tr(M) = n - k$ - Its diagonal
elements are $1 - h_{ii} \in [0,1]$&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Then we can equivalently write $\hat y$ (defined by stacking
$\hat y_i$ into a vector) as $\hat y = Py$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;estimating-beta&#34;&gt;Estimating Beta&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# Estimate beta
β_hat = inv(X&#39;*X)*(X&#39;*y)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 2×1 Array{Float64,2}:
##   1.8821600407711814
##  -0.9429354944506099
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# Equivalent but faster formulation
β_hat = (X&#39;*X)\(X&#39;*y)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 2×1 Array{Float64,2}:
##   1.8821600407711816
##  -0.9429354944506098
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# Even faster (but less intuitive) formulation
β_hat = X\y
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 2×1 Array{Float64,2}:
##   1.8821600407711807
##  -0.9429354944506088
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;equivalent-formulation&#34;&gt;Equivalent Formulation?&lt;/h3&gt;
&lt;p&gt;Generally it’s not true that $$
\hat \beta_{OLS} = \frac{Var(X)}{Cov(X,y)}
$$&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# Wrong formulation
β_wrong = inv(cov(X)) * cov(X, y)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 2×1 Array{Float64,2}:
##   1.8490257777704475
##  -0.9709213554007003
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;equivalent-formulation-correct&#34;&gt;Equivalent Formulation (correct)&lt;/h3&gt;
&lt;p&gt;But it’s true if you include a constant, $\alpha$ $$
y = \alpha + X \beta  + \varepsilon
$$&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# Correct, with constant
α = 3;
y1 = α .+ X*β + ε;
β_hat1 = [ones(n,1) X] \ y1
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 3×1 Array{Float64,2}:
##   3.0362313477745615
##   1.8490257777704477
##  -0.9709213554007007
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;β_correct1 = inv(cov(X)) * cov(X, y1)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 2×1 Array{Float64,2}:
##   1.8490257777704477
##  -0.9709213554007006
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;some-more-objects&#34;&gt;Some More Objects&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# Predicted y
y_hat = X*β_hat;

# Residuals
ε_hat = y - X*β_hat;

# Projection matrix
P = X * inv(X&#39;*X) * X&#39;;

# Annihilator matrix
M = I - P;

# Leverage
h = diag(P);
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;ols-residuals&#34;&gt;OLS Residuals&lt;/h2&gt;
&lt;h3 id=&#34;homoskedasticity&#34;&gt;Homoskedasticity&lt;/h3&gt;
&lt;p&gt;The error is &lt;strong&gt;homoskedastic&lt;/strong&gt; if
$\mathbb E [\varepsilon^2 | x] = \sigma^2$ does not depend on $x$. $$
Var(\varepsilon) = I \sigma^2 = \begin{bmatrix}
\sigma^2 &amp;amp; \dots &amp;amp; 0 \newline\newline&lt;br&gt;
\vdots &amp;amp; \ddots &amp;amp; \vdots \newline
0 &amp;amp; \dots &amp;amp; \sigma^2
\end{bmatrix}
$$&lt;/p&gt;
&lt;p&gt;The error is &lt;strong&gt;heteroskedastic&lt;/strong&gt; if
$\mathbb E [\varepsilon^2 | x] = \sigma^2(x)$ does depend on $x$. $$
Var(\varepsilon) = I \sigma_i^2 =
\begin{bmatrix}
\sigma_1^2 &amp;amp; \dots &amp;amp; 0 \newline
\vdots &amp;amp; \ddots &amp;amp; \vdots \newline
0 &amp;amp; \dots &amp;amp; \sigma_n^2
\end{bmatrix}
$$&lt;/p&gt;
&lt;h3 id=&#34;residual-variance&#34;&gt;Residual Variance&lt;/h3&gt;
&lt;p&gt;The OLS &lt;strong&gt;residual variance&lt;/strong&gt; can be an object of interest even in a
heteroskedastic regression. Its method of moments estimator is given by
$$
\hat \sigma^2 = \frac{1}{n} \sum _ {i=1}^n \hat \varepsilon_i^2
$$&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Note that $\hat \sigma^2$ can be rewritten as $$
\hat \sigma^2 = \frac{1}{n} \varepsilon&#39; M&#39; M \varepsilon = \frac{1}{n} tr(\varepsilon&#39; M \varepsilon) = \frac{1}{n} tr(M \varepsilon&#39; \varepsilon)
$$&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;However, the method of moments estimator is a biesed estimator. In fact
$$
\mathbb E[\hat \sigma^2 | X] = \frac{1}{n} \mathbb E [ tr(M \varepsilon&#39; \varepsilon) | X] =  \frac{1}{n} tr( M\mathbb E[\varepsilon&#39; \varepsilon |X]) = \frac{1}{n} \sum _ {i=1}^n (1-h_{ii}) \sigma^2_i
$$&lt;/p&gt;
&lt;p&gt;Under conditional homoskedasticity, the above expression simplifies to
$$
\mathbb E[\hat \sigma^2 | X] = \frac{1}{n} tr(M) \sigma^2 = \frac{n-k}{n} \sigma^2
$$&lt;/p&gt;
&lt;h3 id=&#34;sample-variance&#34;&gt;Sample Variance&lt;/h3&gt;
&lt;p&gt;The OLS &lt;strong&gt;residual sample variance&lt;/strong&gt; is denoted by $s^2$ and is given by
$$
s^2 = \frac{SSR}{n-k} = \frac{\hat \varepsilon&#39;\hat \varepsilon}{n-k} = \frac{1}{n-k}\sum _ {i=1}^n \hat \varepsilon_i^2
$$ Furthermore, the square root of $s^2$, denoted $s$, is called the
standard error of the regression (SER) or the standard error of the
equation (SEE). Not to be confused with other notions of standard error
to be defined later in the course.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The sum of squared residuals can be rewritten as:
$SSR = \hat \varepsilon&#39; \hat \varepsilon = \varepsilon&#39; M \varepsilon$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The OLS residual sample variance is an unbiased estimator of the error
variance $\sigma^2$.&lt;/p&gt;
&lt;p&gt;Another unbiased estimator of $\sigma^2$ is given by $$
\bar \sigma^2 = \frac{1}{n} \sum _ {i=1}^n (1-h_{ii})^{-1} \hat \varepsilon_i^2
$$&lt;/p&gt;
&lt;h3 id=&#34;uncentered-r2&#34;&gt;Uncentered R^2&lt;/h3&gt;
&lt;p&gt;One measure of the variability of the dependent variable $y_i$ is the
sum of squares $\sum _ {i=1}^n y_i^2 = y&amp;rsquo;y$. There is a decomposition:
$$
\begin{aligned}
y&amp;rsquo;y &amp;amp;= (\hat y + e)&#39; (\hat y + \hat \varepsilon) \newline
&amp;amp;= \hat y&#39; \hat y + 2 \hat y&#39; \hat \varepsilon + \hat \varepsilon&#39; \hat \varepsilon e \newline
&amp;amp;= \hat y&#39; \hat y + 2 b&amp;rsquo;X&#39;\hat \varepsilon + \hat \varepsilon&#39; \hat \varepsilon \ \ (\text{since} \ \hat y = Xb) \newline
&amp;amp;= \hat y&#39; \hat y + \hat \varepsilon&#39;\hat \varepsilon \ \ (\text{since} \ X&#39;\hat \varepsilon =0)
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;uncentered&lt;/strong&gt; $\mathbf{R^2}$ is defined as: $$
R^2 _ {uc} \equiv 1 - \frac{\hat \varepsilon&#39;\hat \varepsilon}{y&amp;rsquo;y} = 1 - \frac{\mathbb E_n[\hat \varepsilon_i^2]}{\mathbb E_n[y_i^2]} = \frac{ \mathbb E [\hat y_i^2]}{ \mathbb E [y_i^2]}
$$&lt;/p&gt;
&lt;h3 id=&#34;centered-r2&#34;&gt;Centered R^2&lt;/h3&gt;
&lt;p&gt;A more natural measure of variability is the sum of centered squares
$\sum _ {i=1}^n (y_i - \bar y)^2,$ where
$\bar y := \frac{1}{n}\sum _ {i=1}^n y_i$. If the regressors include a
constant, it can be decomposed as $$
\sum _ {i=1}^n (y_i - \bar y)^2 = \sum _ {i=1}^n (\hat y_i - \bar y)^2 + \sum _ {i=1}^n \hat \varepsilon_i^2
$$&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;coefficient of determination&lt;/strong&gt;, $\mathbf{R^2}$, is defined as $$
R^2 \equiv 1 - \frac{\sum _ {i=1}^n \hat \varepsilon_i^2}{\sum _ {i=1}^n (y_i - \bar y)^2 }= \frac{  \sum _ {i=1}^n (\hat y_i - \bar y)^2 } { \sum _ {i=1}^n (y_i - \bar y)^2} = \frac{\mathbb E_n[(\hat y_i - \bar y)^2]}{\mathbb E_n[(y_i - \bar y)^2]}
$$&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Always use the centered $R^2$ unless you really know what you are
doing.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;code---variance&#34;&gt;Code - Variance&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# Biased variance estimator
σ_hat = ε_hat&#39;*ε_hat / n;

# Unbiased estimator 1
σ_hat_2 = ε_hat&#39;*ε_hat / (n-k);

# Unbiased estimator 2
σ_hat_3 = mean( ε_hat.^2 ./ (1 .- h) );
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;code---r2&#34;&gt;Code - R^2&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# R squared - uncentered
R2_uc = (y_hat&#39;*y_hat)/ (y&#39;*y);

# R squared
y_bar = mean(y);
R2 = ((y_hat .- y_bar)&#39;*(y_hat .- y_bar))/ ((y .- y_bar)&#39;*(y .- y_bar));
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;finite-sample-properties-of-ols&#34;&gt;Finite Sample Properties of OLS&lt;/h2&gt;
&lt;h3 id=&#34;conditional-unbiasedness&#34;&gt;Conditional Unbiasedness&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Under the GM assumptions (1)-(3), the OLS estimator is &lt;strong&gt;conditionally
unbiased&lt;/strong&gt;, i.e. the distribution of $\hat \beta _ {OLS}$ is centered at
$\beta_0$: $\mathbb E [\hat \beta | X] = \beta_0$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt; $$
\begin{aligned}
\mathbb E [\hat \beta  | X] &amp;amp;= \mathbb E [ (X&amp;rsquo;X)^{-1} X&amp;rsquo;y | X] = \newline
&amp;amp;= (X&amp;rsquo;X)^{-1} X &#39; \mathbb E  [y | X] = \newline
&amp;amp;= (X&amp;rsquo;X)^{-1} X&#39; \mathbb E  [X \beta + \varepsilon | X] = \newline
&amp;amp;= (X&amp;rsquo;X)^{-1} X&amp;rsquo;X \beta + (X&amp;rsquo;X)^{-1} X&#39; \mathbb E  [\varepsilon | X] = \newline
&amp;amp;= \beta
\end{aligned}
$$ $$\tag*{$\blacksquare$}$$&lt;/p&gt;
&lt;h3 id=&#34;ols-variance&#34;&gt;OLS Variance&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Under the GM assumptions (1)-(3),
$Var(\hat \beta |X) = \sigma^2 (X&amp;rsquo;X)^{-1}$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;: $$
\begin{aligned}
Var(\hat \beta |X) &amp;amp;= Var( (X&amp;rsquo;X)^{-1} X&amp;rsquo;y|X) = \newline
&amp;amp;= ((X&amp;rsquo;X)^{-1} X&#39; ) Var(y|X) ((X&amp;rsquo;X)^{-1} X&#39; )&#39; = \newline
&amp;amp;= ((X&amp;rsquo;X)^{-1} X&#39; ) Var(X\beta + \varepsilon|X) ((X&amp;rsquo;X)^{-1} X&#39; )&#39; = \newline
&amp;amp;= ((X&amp;rsquo;X)^{-1} X&#39; ) Var(\varepsilon|X) ((X&amp;rsquo;X)^{-1} X&#39; )&#39; = \newline
&amp;amp;= ((X&amp;rsquo;X)^{-1} X&#39; ) \sigma^2 I ((X&amp;rsquo;X)^{-1} X&#39; )&#39; =  \newline
&amp;amp;= \sigma^2 (X&amp;rsquo;X)^{-1}
\end{aligned}
$$ $$\tag*{$\blacksquare$}$$&lt;/p&gt;
&lt;p&gt;Higher correlation of the $X$ implies higher variance of the OLS
estimator.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Intuition: individual observations carry less information. You are
exploring a smaller region of the $X$ space.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;blue&#34;&gt;BLUE&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Under the GM assumptions (1)-(3),
$Cov (\hat \beta, \hat \varepsilon ) = 0$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Under the GM assumptions (1)-(3), $\hat \beta _ {OLS}$ is the best (most
efficient) linear, unbiased estimator (&lt;strong&gt;BLUE&lt;/strong&gt;), i.e., for any unbiased
linear estimator $b$: $Var (b|X) \geq Var (\hat \beta |X)$.&lt;/p&gt;
&lt;h3 id=&#34;blue-proof&#34;&gt;BLUE Proof&lt;/h3&gt;
&lt;p&gt;Consider four steps:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Define three objects: (i) $b= Cy$, (ii) $A = (X&amp;rsquo;X)^{-1} X&#39;$ such
that $\hat \beta = A y$, and (iii) $D = C-A$.&lt;/li&gt;
&lt;li&gt;Decompose $b$ as $$
\begin{aligned}
b &amp;amp;= (D + A) y = \newline
&amp;amp;=  Dy + Ay = \newline&lt;br&gt;
&amp;amp;= D (X\beta + \varepsilon) + \hat \beta = \newline
&amp;amp;= DX\beta + D \varepsilon + \hat \beta
\end{aligned}
$$&lt;/li&gt;
&lt;li&gt;By assumption, $b$ must be unbiased: $$
\begin{aligned}
\mathbb E [b|X] &amp;amp;= \mathbb E [D(X\beta + \varepsilon) + Ay |X] = \newline
&amp;amp;= \mathbb E [DX\beta|X] + \mathbb E [D\varepsilon |X] + \mathbb E [\hat \beta |X] = \newline
&amp;amp;= DX\beta + D \mathbb E [\varepsilon |X] +\beta \newline&lt;br&gt;
&amp;amp;= DX\beta + \beta
\end{aligned}
$$ Hence, it must be that $DX = 0$&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;blue-proof-2&#34;&gt;BLUE Proof (2)&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;We know by (2)-(3) that $b = D \varepsilon + \hat \beta$. We can now
calculate its variance. $$
\begin{aligned}
Var (b|X) &amp;amp;= Var (\hat \beta + D\varepsilon|X) = \newline
&amp;amp;= Var (Ay + D\varepsilon|X) = \newline
&amp;amp;= Var (AX\beta + (D + A)\varepsilon|X) = \newline
&amp;amp;= Var((D+A)\varepsilon |X) = \newline
&amp;amp;= (D+A)\sigma^2 I (D+A)&#39; = \newline
&amp;amp;= \sigma^2 I (DD&#39; + AA&#39; + DA&#39; + AD&#39;) = \newline
&amp;amp;= \sigma^2 I (DD&#39; + AA&#39;) \geq \newline
&amp;amp;\geq \sigma^2 AA&#39;= \newline
&amp;amp;= \sigma^2 (X&amp;rsquo;X)^{-1} = \newline
&amp;amp;= Var (\hat \beta|X)
\end{aligned}
$$ since $DA&#39;= AD&#39; = 0$, $DX = 0$ and $AA&#39; = (X&amp;rsquo;X)^{-1}$.
$$\tag*{$\blacksquare$}$$&lt;/li&gt;
&lt;/ol&gt;
&lt;blockquote&gt;
&lt;p&gt;$Var(b | X) \geq Var (\hat{\beta} | X)$ is meant in a positive
definite sense.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;code---variance-1&#34;&gt;Code - Variance&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# Ideal variance of the OLS estimator
var_β = σ * inv(X&#39;*X)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 2×2 Array{Float64,2}:
##   0.0609402  -0.0467732
##  -0.0467732   0.0656808
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# Standard errors
std_β = sqrt.(diag(var_β))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 2-element Array{Float64,1}:
##  0.24686077212177054
##  0.25628257446345265
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Convexity and Optimization</title>
      <link>https://matteocourthoud.github.io/course/ml-econ/06_convexity/</link>
      <pubDate>Mon, 28 Feb 2022 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/course/ml-econ/06_convexity/</guid>
      <description>&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Remove warnings
import warnings
warnings.filterwarnings(&#39;ignore&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Import
import autograd.numpy as np
from autograd import grad
import seaborn as sns
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Import matplotlib for graphs
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import axes3d

# Set global parameters
%matplotlib inline
plt.style.use(&#39;seaborn-white&#39;)
plt.rcParams[&#39;lines.linewidth&#39;] = 3
plt.rcParams[&#39;figure.figsize&#39;] = (10,6)
plt.rcParams[&#39;figure.titlesize&#39;] = 20
plt.rcParams[&#39;axes.titlesize&#39;] = 18
plt.rcParams[&#39;axes.labelsize&#39;] = 14
plt.rcParams[&#39;legend.fontsize&#39;] = 14
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Function to plot errors
def error_plot(ys, yscale=&#39;log&#39;):
    plt.figure()
    plt.xlabel(&#39;Step&#39;)
    plt.ylabel(&#39;Error&#39;)
    plt.yscale(yscale)
    plt.plot(range(len(ys)), ys)
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;61-gradient-descent&#34;&gt;6.1 Gradient Descent&lt;/h2&gt;
&lt;p&gt;We start with a basic implementation of projected gradient descent.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def gradient_descent(init, steps, grad, proj=lambda x: x):
    &amp;quot;&amp;quot;&amp;quot;Projected gradient descent.
    
    Inputs:
        initial: starting point
        steps: list of scalar step sizes
        grad: function mapping points to gradients
        proj (optional): function mapping points to points
        
    Returns:
        List of all points computed by projected gradient descent.
    &amp;quot;&amp;quot;&amp;quot;
    xs = [init]
    for step in steps:
        xs.append(proj(xs[-1] - step * grad(xs[-1])))
    return xs
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that this implementation keeps around all points computed along the way. This is clearly not what you would do on large instances. We do this for illustrative purposes to be able to easily inspect the computed sequence of points.&lt;/p&gt;
&lt;h3 id=&#34;warm-up-optimizing-a-quadratic&#34;&gt;Warm-up: Optimizing a quadratic&lt;/h3&gt;
&lt;p&gt;As a toy example, let&amp;rsquo;s optimize $$f(x)=\frac12|x|^2,$$ which has the gradient map $\nabla f(x)=x.$&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def quadratic(x):
    return 0.5*x.dot(x)

def quadratic_gradient(x):
    return x
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note the function is $1$-smooth and $1$-strongly convex. Our theorems would then suggest that we use a constant step size of $1.$ If you think about it, for this step size the algorithm will actually find the optimal solution in just one step.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;x0 = np.random.normal(0, 1, (1000))
_, x1 = gradient_descent(x0, [1.0], quadratic_gradient)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Indeed, it does.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;x1.all() == 0
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;True
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s say we don&amp;rsquo;t have the right learning rate.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;xs = gradient_descent(x0, [0.1]*50, quadratic_gradient)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Plot errors along steps
error_plot([quadratic(x) for x in xs])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/06_convexity_18_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;constrained-optimization&#34;&gt;Constrained Optimization&lt;/h3&gt;
&lt;p&gt;Let&amp;rsquo;s say we want to optimize the function inside some affine subspace. Recall that affine subspaces are convex sets. Below we pick a random low dimensional affine subspace $b+U$ and define the corresponding linear projection operator.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# U is an orthonormal basis of a random 100-dimensional subspace.
U = np.linalg.qr(np.random.normal(0, 1, (1000, 100)))[0]
b = np.random.normal(0, 1, 1000)

def proj(x):
    &amp;quot;&amp;quot;&amp;quot;Projection of x onto an affine subspace&amp;quot;&amp;quot;&amp;quot;
    return b + U.dot(U.T).dot(x-b)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;x0 = np.random.normal(0, 1, (1000))
xs = gradient_descent(x0, [0.1]*50, quadratic_gradient, proj)
# the optimal solution is the projection of the origin
x_opt = proj(0)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s plot the results.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;error_plot([quadratic(x) for x in xs])
plt.plot(range(len(xs)), [quadratic(x_opt)]*len(xs),
        label=&#39;$\\frac{1}{2}|\!|x_{\mathrm{opt}}|\!|^2$&#39;)
plt.legend();
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/06_convexity_24_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;The orangle line shows the optimal error, which the algorithm reaches quickly.&lt;/p&gt;
&lt;p&gt;The iterates also converge to the optimal solution in domain as the following plot shows.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;error_plot([np.linalg.norm(x_opt-x)**2 for x in xs])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/06_convexity_27_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;least-squares&#34;&gt;Least Squares&lt;/h3&gt;
&lt;p&gt;One of the most fundamental data analysis tools is &lt;em&gt;linear least squares&lt;/em&gt;. Given an $m\times n$ matrix $A$ and a vector $b$ our goal is to find a vector $x\in\mathbb{R}^n$ that minimizes the following objective:&lt;/p&gt;
&lt;p&gt;
$$f(x) = \frac 1{2m}\sum_{i=1}^m (a_i^\top x - b_j)^2 
=\frac1{2m}\|Ax-b\|^2$$
&lt;/p&gt;
&lt;p&gt;We can verify that $\nabla f(x) = A^\top(Ax-b)$ and
$\nabla^2 f(x) = A^\top A.$&lt;/p&gt;
&lt;p&gt;Hence, the objective is $\beta$-smooth with
$\beta=\lambda_{\mathrm{max}}(A^\top A)$, and $\alpha$-strongly convex with $\alpha=\lambda_{\mathrm{min}}(A^\top A)$.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def least_squares(A, b, x):
    &amp;quot;&amp;quot;&amp;quot;Least squares objective.&amp;quot;&amp;quot;&amp;quot;
    return (0.5/m) * np.linalg.norm(A.dot(x)-b)**2

def least_squares_gradient(A, b, x):
    &amp;quot;&amp;quot;&amp;quot;Gradient of least squares objective at x.&amp;quot;&amp;quot;&amp;quot;
    return A.T.dot(A.dot(x)-b)/m
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;overdetermined-case-mge-n&#34;&gt;Overdetermined case $m\ge n$&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;m, n = 1000, 100
A = np.random.normal(0, 1, (m, n))
x_opt = np.random.normal(0, 1, n)
noise = np.random.normal(0, 0.1, m)
b = A.dot(x_opt) + noise
objective = lambda x: least_squares(A, b, x)
gradient = lambda x: least_squares_gradient(A, b, x)
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;convergence-in-objective&#34;&gt;Convergence in Objective&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;x0 = np.random.normal(0, 1, n)
xs = gradient_descent(x0, [0.1]*100, gradient)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;error_plot([objective(x) for x in xs])
plt.plot(range(len(xs)), [np.linalg.norm(noise)**2]*len(xs),
        label=&#39;noise level&#39;)
plt.plot(range(len(xs)), [least_squares(A,b,x_opt)]*len(xs),
        label=&#39;optimal&#39;)
plt.legend();
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/06_convexity_35_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;convergence-in-domain&#34;&gt;Convergence in Domain&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;error_plot([np.linalg.norm(x-x_opt)**2 for x in xs])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/06_convexity_37_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;underdetermined-case-m--n&#34;&gt;Underdetermined Case $m &amp;lt; n$&lt;/h3&gt;
&lt;p&gt;In the underdetermined case, the least squares objective is inevitably not strongly convex, since $A^\top A$ is a rank deficient matrix and hence $\lambda_{\mathrm{min}}(A^\top A)=0.$&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;m, n = 100, 1000
A = np.random.normal(0, 1, (m, n))
b = np.random.normal(0, 1, m)
# The least norm solution is given by the pseudo-inverse
x_opt = np.linalg.pinv(A).dot(b)
objective = lambda x: least_squares(A, b, x)
gradient = lambda x: least_squares_gradient(A, b, x)
x0 = np.random.normal(0, 1, n)
xs = gradient_descent(x0, [0.1]*100, gradient)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Results.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;error_plot([objective(x) for x in xs])
plt.plot(range(len(xs)), [least_squares(A,b,x_opt)]*len(xs),
        label=&#39;optimal&#39;)
plt.legend();
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/06_convexity_41_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;While we quickly reduce the error, we don&amp;rsquo;t actually converge in domain to the least norm solution. This is just because the function is no longer strongly convex in the underdetermined case.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;error_plot([np.linalg.norm(x-x_opt)**2 for x in xs], yscale=&#39;linear&#39;)
plt.plot(range(len(xs)), [np.linalg.norm(x_opt)**2]*len(xs),
         label=&#39;$|\!|x_{\mathrm{opt}}|\!|^2$&#39;)
plt.legend();
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/06_convexity_43_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;ell_2-regularized-least-squares&#34;&gt;$\ell_2$-regularized least squares&lt;/h2&gt;
&lt;p&gt;In the underdetermined case, it is often desirable to restore strong convexity of the objective function by adding an $\ell_2^2$-penality, also known as &lt;em&gt;Tikhonov regularization&lt;/em&gt;, $\ell_2$-regularization, or &lt;em&gt;weight decay&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;
$$\frac1{2m}\|Ax-b\|^2 + \frac{\alpha}2\|x\|^2$$
&lt;/p&gt;
&lt;p&gt;Note: With this modification the objective is $\alpha$-strongly convex again.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def least_squares_l2(A, b, x, alpha=0.1):
    return least_squares(A, b, x) + (alpha/2) * x.dot(x)

def least_squares_l2_gradient(A, b, x, alpha=0.1):
    return least_squares_gradient(A, b, x) + alpha * x
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s create a least squares instance.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;m, n = 100, 1000
A = np.random.normal(0, 1, (m, n))
b = A.dot(np.random.normal(0, 1, n))
objective = lambda x: least_squares_l2(A, b, x)
gradient = lambda x: least_squares_l2_gradient(A, b, x)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that we can find the optimal solution to the optimization problem in closed form without even running gradient descent by computing $x_{\mathrm{opt}}=(A^\top+\alpha I)^{-1}A^\top b.$ Please verify that this point is indeed optimal.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;x_opt = np.linalg.inv(A.T.dot(A) + 0.1*np.eye(1000)).dot(A.T).dot(b)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here&amp;rsquo;s how gradient descent fares.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;x0 = np.random.normal(0, 1, n)
xs = gradient_descent(x0, [0.1]*500, gradient)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We plot the descent.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;error_plot([objective(x) for x in xs])
plt.plot(range(len(xs)), [least_squares_l2(A,b,x_opt)]*len(xs),
        label=&#39;optimal&#39;)
plt.legend();
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/06_convexity_54_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;You see that the error doesn&amp;rsquo;t decrease below a certain level due to the regularization term. This is not a bad thing. In fact, the regularization term gives as &lt;em&gt;strong convexity&lt;/em&gt; which leads to convergence in domain again:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;xs = gradient_descent(x0, [0.1]*500, gradient)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;error_plot([np.linalg.norm(x-x_opt)**2 for x in xs])
plt.plot(range(len(xs)), [np.linalg.norm(x_opt)**2]*len(xs),
        label=&#39;squared norm of $x_{\mathrm{opt}}$&#39;)
plt.legend();
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/06_convexity_57_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;the-magic-of-implicit-regularization&#34;&gt;The Magic of Implicit Regularization&lt;/h2&gt;
&lt;p&gt;Sometimes simply running gradient descent from a suitable initial point has a regularizing effect on its own &lt;strong&gt;without introducing an explicit regularization term&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;We will see this below where we revisit the unregularized least squares objective, but initialize gradient descent from the origin rather than a random gaussian point.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# We initialize from 0
x0 = np.zeros(n)
# Note this is the gradient w.r.t. the unregularized objective!
gradient = lambda x: least_squares_gradient(A, b, x)
xs = gradient_descent(x0, [0.1]*50, gradient)
error_plot([np.linalg.norm(x_opt-x)**2 for x in xs], yscale=&#39;linear&#39;)
plt.plot(range(len(xs)), [np.linalg.norm(x_opt)**2]*len(xs),
         label=&#39;$|\!|x_{\mathrm{opt}}|\!|^2$&#39;)
plt.legend();
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/06_convexity_60_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Incredible!&lt;/em&gt; We converge to the minimum norm solution!&lt;/p&gt;
&lt;p&gt;Implicit regularization is a deep phenomenon that&amp;rsquo;s an active research topic in learning and optimization. It&amp;rsquo;s exciting that we see it play out in this simple least squares problem already!&lt;/p&gt;
&lt;h2 id=&#34;lasso&#34;&gt;LASSO&lt;/h2&gt;
&lt;p&gt;LASSO is the name for $\ell_1$-regularized least squares regression:&lt;/p&gt;
&lt;p&gt;
$$\frac1{2m}\|Ax-b\|^2 + \alpha\|x\|_1$$
&lt;/p&gt;
&lt;p&gt;We will see that LASSO is able to fine &lt;em&gt;sparse&lt;/em&gt; solutions if they exist. This is a common motivation for using an $\ell_1$-regularizer.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def lasso(A, b, x, alpha=0.1):
    return least_squares(A, b, x) + alpha * np.linalg.norm(x, 1)

def ell1_subgradient(x):
    &amp;quot;&amp;quot;&amp;quot;Subgradient of the ell1-norm at x.&amp;quot;&amp;quot;&amp;quot;
    g = np.ones(x.shape)
    g[x &amp;lt; 0.] = -1.0
    return g

def lasso_subgradient(A, b, x, alpha=0.1):
    &amp;quot;&amp;quot;&amp;quot;Subgradient of the lasso objective at x&amp;quot;&amp;quot;&amp;quot;
    return least_squares_gradient(A, b, x) + alpha*ell1_subgradient(x)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;m, n = 100, 1000
A = np.random.normal(0, 1, (m, n))
x_opt = np.zeros(n)
x_opt[:10] = 1.0
b = A.dot(x_opt)
x0 = np.random.normal(0, 1, n)
xs = gradient_descent(x0, [0.1]*500, lambda x: lasso_subgradient(A, b, x))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;error_plot([lasso(A, b, x) for x in xs])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/06_convexity_66_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plt.figure()
plt.title(&#39;Comparison of initial, optimal, and computed point&#39;)
idxs = range(50)
plt.plot(idxs, x0[idxs], &#39;--&#39;, color=&#39;#aaaaaa&#39;, label=&#39;initial&#39;)
plt.plot(idxs, x_opt[idxs], &#39;r-&#39;, label=&#39;optimal&#39;)
plt.plot(idxs, xs[-1][idxs], &#39;g-&#39;, label=&#39;final&#39;)
plt.xlabel(&#39;Coordinate&#39;)
plt.ylabel(&#39;Value&#39;)
plt.legend();
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/06_convexity_67_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;As promised, LASSO correctly identifies the significant coordinates of the optimal solution. This is why, in practice, LASSO is a popular tool for feature selection.&lt;/p&gt;
&lt;p&gt;Play around with this plot to inspect other points along the way, e.g., the point that achieves lowest objective value. Why does the objective value go up even though we continue to get better solutions?&lt;/p&gt;
&lt;h2 id=&#34;support-vector-machines&#34;&gt;Support Vector Machines&lt;/h2&gt;
&lt;p&gt;In a linear classification problem, we&amp;rsquo;re given $m$ labeled points $(a_i, y_i)$ and we wish to find a hyperplane given by a point $x$ that separates them so that&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\langle a_i, x\rangle \ge 1$ when $y_i=1$, and&lt;/li&gt;
&lt;li&gt;$\langle a_i, x\rangle \le -1$ when $y_i = -1$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The smaller the norm $|x|$ the larger the &lt;em&gt;margin&lt;/em&gt; between positive and negative instances. Therefore, it makes sense to throw in a regularizer that penalizes large norms. This leads to the objective.&lt;/p&gt;
&lt;p&gt;
$$\frac 1m \sum_{i=1}^m \max\{1-y_i(a_i^\top x), 0\} + \frac{\alpha}2\|x\|^2$$
&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def hinge_loss(z):
    return np.maximum(1.-z, np.zeros(z.shape))

def svm_objective(A, y, x, alpha=0.1):
    &amp;quot;&amp;quot;&amp;quot;SVM objective.&amp;quot;&amp;quot;&amp;quot;
    m, _ = A.shape
    return np.mean(hinge_loss(np.diag(y).dot(A.dot(x))))+(alpha/2)*x.dot(x)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;z = np.linspace(-2, 2, 100)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plt.figure()
plt.plot(z, hinge_loss(z));
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/06_convexity_73_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def hinge_subgradient(z):
    g = np.zeros(z.shape)
    g[z &amp;lt; 1] = -1.
    return g

def svm_subgradient(A, y, x, alpha=0.1):
    g1 = hinge_subgradient(np.diag(y).dot(A.dot(x)))
    g2 = np.diag(y).dot(A)
    return g1.dot(g2) + alpha*x
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plt.figure()
plt.plot(z, hinge_subgradient(z));
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/06_convexity_75_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;m, n = 1000, 100
A = np.vstack([np.random.normal(0.1, 1, (m//2, n)),
               np.random.normal(-0.1, 1, (m//2, n))])
y = np.hstack([np.ones(m//2), -1.*np.ones(m//2)])
x0 = np.random.normal(0, 1, n)
xs = gradient_descent(x0, [0.01]*100, 
                      lambda x: svm_subgradient(A, y, x, 0.05))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;error_plot([svm_objective(A, y, x) for x in xs])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/06_convexity_77_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s see if averaging out the solutions gives us an improved function value.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;xavg = 0.0
for x in xs:
    xavg += x
svm_objective(A, y, xs[-1]), svm_objective(A, y, xavg/len(xs))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;(1.0710162653835846, 0.9069593413738611)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can also look at the accuracy of our linear model for predicting the labels. From how we defined the data, we can see that the all ones vector is the highest accuracy classifier in the limit of infinite data (very large $m$). For a finite data set, the accuracy could be even higher due to random fluctuations.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def accuracy(A, y, x):
    return np.mean(np.diag(y).dot(A.dot(x))&amp;gt;0)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plt.figure()
plt.ylabel(&#39;Accuracy&#39;)
plt.xlabel(&#39;Step&#39;)
plt.plot(range(len(xs)), [accuracy(A, y, x) for x in xs])
plt.plot(range(len(xs)), [accuracy(A, y, np.ones(n))]*len(xs),
        label=&#39;Population optimum&#39;)
plt.legend();
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/06_convexity_82_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;We see that the accuracy spikes pretty early and drops a bit as we train for too long.&lt;/p&gt;
&lt;h2 id=&#34;sparse-inverse-covariance-estimation&#34;&gt;Sparse Inverse Covariance Estimation&lt;/h2&gt;
&lt;p&gt;Given a positive semidefinite matrix $S\in\mathbb{R}^{n\times n}$ the objective function in sparse inverse covariance estimation is as follows:&lt;/p&gt;
&lt;p&gt;
$$ \min_{X\in\mathbb{R}^{n\times n}, X\succeq 0} 
\langle S, X\rangle - \log\det(X) + \alpha\|X\|_1$$
&lt;/p&gt;
&lt;p&gt;Here, we define
$$\langle S, X\rangle = \mathrm{trace}(S^\top X)$$
and
$$|X|&lt;em&gt;1 = \sum&lt;/em&gt;{ij}|X_{ij}|.$$&lt;/p&gt;
&lt;p&gt;Typically, we think of the matrix $S$ as a sample covariance matrix of a set of vectors $a_1,\dots, a_m,$ defined as:
$$
S = \frac1{m-1}\sum_{i=1}^n a_ia_i^\top
$$
The example also highlights the utility of automatic differentiation as provided by the &lt;code&gt;autograd&lt;/code&gt; package that we&amp;rsquo;ll regularly use. In a later lecture we will understand exactly how automatic differentiation works. For now we just treat it as a blackbox that gives us gradients.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;np.random.seed(1337)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def sparse_inv_cov(S, X, alpha=0.1):
    return (np.trace(S.T.dot(X))
            - np.log(np.linalg.det(X))
            + alpha * np.sum(np.abs(X)))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;n = 5
A = np.random.normal(0, 1, (n, n))
S = A.dot(A.T)
objective = lambda X: sparse_inv_cov(S, X)
# autograd provides a &amp;quot;gradient&amp;quot;, yay!
gradient = grad(objective)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We also need to worry about the projection onto the positive semidefinite cone, which corresponds to truncating eigenvalues.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def projection(X):
    &amp;quot;&amp;quot;&amp;quot;Projection onto positive semidefinite cone.&amp;quot;&amp;quot;&amp;quot;
    es, U = np.linalg.eig(X)
    es[es&amp;lt;0] = 0.
    return U.dot(np.diag(es).dot(U.T))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;A0 = np.random.normal(0, 1, (n,n))
X0 = A0.dot(A0.T)
Xs = gradient_descent(X0, [0.01]*500, gradient, projection)
error_plot([objective(X) for X in Xs])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/06_convexity_91_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;going-crazy-with-autograd&#34;&gt;Going crazy with autograd&lt;/h2&gt;
&lt;p&gt;Just for fun, we&amp;rsquo;ll go through a crazy example below. We can use &lt;code&gt;autograd&lt;/code&gt; not just for getting gradients for natural objectives, we can in principle also use it to tune hyperparameters of our optimizer, like the step size schedulde.&lt;/p&gt;
&lt;p&gt;Below we see how we can find a better 10-step learning rate schedules for optimizing a quadratic. This is mostly just for illustrative purposes (although some researchers are exploring these kinds of ideas more seriously).&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;x0 = np.random.normal(0, 1, 1000)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def f(x):
    return 0.5*np.dot(x,x)

def optimizer(steps):
    &amp;quot;&amp;quot;&amp;quot;Optimize a quadratic with the given steps.&amp;quot;&amp;quot;&amp;quot;
    xs = gradient_descent(x0, steps, grad(f))
    return f(xs[-1])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The function &lt;code&gt;optimizer&lt;/code&gt; is a non-differentiable function of its input &lt;code&gt;steps&lt;/code&gt;. Nontheless, &lt;code&gt;autograd&lt;/code&gt; will provide a gradient that we can stick into gradient descent. That is, we&amp;rsquo;re tuning gradient descent with gradient descent.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;grad_optimizer = grad(optimizer)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;initial_steps = np.abs(np.random.normal(0, 0.1, 10))
better_steps = gradient_descent(initial_steps, [0.001]*500, grad_optimizer)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;error_plot([optimizer(steps) for steps in better_steps])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/06_convexity_99_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;As we can see, the learning rate schedules improve dramatically over time. Of course, we already know from the first example that there is a step size schedule that converges in one step. Interestingly, the last schedule we find here doesn&amp;rsquo;t look at all like what we might expect:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plt.figure()
plt.xticks(range(len(better_steps[-1])))
plt.ylabel(&#39;Step size&#39;)
plt.xlabel(&#39;Step number&#39;)
plt.plot(range(len(better_steps[-1])), better_steps[-1]);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/06_convexity_101_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Web Scraping</title>
      <link>https://matteocourthoud.github.io/course/data-science/06_web_scraping/</link>
      <pubDate>Sun, 27 Feb 2022 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/course/data-science/06_web_scraping/</guid>
      <description>&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import os
import re
import time
import requests
import pandas as pd

from bs4 import BeautifulSoup
from pprint import pprint
from selenium import webdriver
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There is no silver bullet to getting info from the internet.
The coding requirements in these notes start easy and will gradually become more demanding. We will cover the following web scraping techniques:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Pandas&lt;/li&gt;
&lt;li&gt;APIs&lt;/li&gt;
&lt;li&gt;Scraping static webpages with BeautifulSoup&lt;/li&gt;
&lt;li&gt;Scraping dynamic wepages with Selenium&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;pandas&#34;&gt;Pandas&lt;/h2&gt;
&lt;p&gt;The Pandas library has a very useful webscraping command: &lt;code&gt;read_html&lt;/code&gt;. The &lt;code&gt;read_html&lt;/code&gt; command works for webpages that contain tables that are particularly well behaved. Let&amp;rsquo;s see an example: &lt;a href=&#34;https://en.wikipedia.org/wiki/List_of_countries_by_GDP_%28nominal%29&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://en.wikipedia.org/wiki/List_of_countries_by_GDP_(nominal)&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;At first glance, it seems that there are three tables in this Wikipedia page:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;data from the IMF&lt;/li&gt;
&lt;li&gt;data from the World Bank&lt;/li&gt;
&lt;li&gt;data from the UN&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Let&amp;rsquo;s see which tables pandas recognizes.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Scrape all tables from Wikipedia page
url = &#39;https://en.wikipedia.org/wiki/List_of_countries_by_GDP_(nominal)&#39;
df_list = pd.read_html(url)

# Check number of tables on the page
print(len(df_list))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;7
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Apparently Pandas has found 10 tables in this webpage. Let&amp;rsquo;s see what is their content.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Check headers of each table
for df in df_list: print(df.shape)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;(1, 1)
(1, 3)
(216, 9)
(9, 2)
(7, 2)
(13, 2)
(2, 2)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It seems that pandas has found many more tables that we could see. The ones that are of interest to us are probably the 3rd, 4th and 5th. But that are the others? Let&amp;rsquo;s look at the them.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Check first
df_list[0].head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;0&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;Largest economies by nominal GDP in 2021[1]&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Check second
df_list[1].head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;th&gt;2&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;.mw-parser-output .legend{page-break-inside:av...&lt;/td&gt;
      &lt;td&gt;$750 billion – $1 trillion $500–50 billion $25...&lt;/td&gt;
      &lt;td&gt;$50–100 billion $25–50 billion $5–25 billion &amp;lt;...&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;Apparently, the first two are simply picture captions.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Check third
df_list[2].head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead tr th {
    text-align: left;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;Country/Territory&lt;/th&gt;
      &lt;th&gt;Subregion&lt;/th&gt;
      &lt;th&gt;Region&lt;/th&gt;
      &lt;th colspan=&#34;2&#34; halign=&#34;left&#34;&gt;IMF[1]&lt;/th&gt;
      &lt;th colspan=&#34;2&#34; halign=&#34;left&#34;&gt;United Nations[12]&lt;/th&gt;
      &lt;th colspan=&#34;2&#34; halign=&#34;left&#34;&gt;World Bank[13][14]&lt;/th&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;Country/Territory&lt;/th&gt;
      &lt;th&gt;Subregion&lt;/th&gt;
      &lt;th&gt;Region&lt;/th&gt;
      &lt;th&gt;Estimate&lt;/th&gt;
      &lt;th&gt;Year&lt;/th&gt;
      &lt;th&gt;Estimate&lt;/th&gt;
      &lt;th&gt;Year&lt;/th&gt;
      &lt;th&gt;Estimate&lt;/th&gt;
      &lt;th&gt;Year&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;United States&lt;/td&gt;
      &lt;td&gt;Northern America&lt;/td&gt;
      &lt;td&gt;Americas&lt;/td&gt;
      &lt;td&gt;22939580.0&lt;/td&gt;
      &lt;td&gt;2021&lt;/td&gt;
      &lt;td&gt;20893746.0&lt;/td&gt;
      &lt;td&gt;2020&lt;/td&gt;
      &lt;td&gt;20936600.0&lt;/td&gt;
      &lt;td&gt;2020&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;China&lt;/td&gt;
      &lt;td&gt;Eastern Asia&lt;/td&gt;
      &lt;td&gt;Asia&lt;/td&gt;
      &lt;td&gt;16862979.0&lt;/td&gt;
      &lt;td&gt;[n 2]2021&lt;/td&gt;
      &lt;td&gt;14722801.0&lt;/td&gt;
      &lt;td&gt;[n 3]2020&lt;/td&gt;
      &lt;td&gt;14722731.0&lt;/td&gt;
      &lt;td&gt;2020&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;Japan&lt;/td&gt;
      &lt;td&gt;Eastern Asia&lt;/td&gt;
      &lt;td&gt;Asia&lt;/td&gt;
      &lt;td&gt;5103110.0&lt;/td&gt;
      &lt;td&gt;2021&lt;/td&gt;
      &lt;td&gt;5057759.0&lt;/td&gt;
      &lt;td&gt;2020&lt;/td&gt;
      &lt;td&gt;4975415.0&lt;/td&gt;
      &lt;td&gt;2020&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;Germany&lt;/td&gt;
      &lt;td&gt;Western Europe&lt;/td&gt;
      &lt;td&gt;Europe&lt;/td&gt;
      &lt;td&gt;4230172.0&lt;/td&gt;
      &lt;td&gt;2021&lt;/td&gt;
      &lt;td&gt;3846414.0&lt;/td&gt;
      &lt;td&gt;2020&lt;/td&gt;
      &lt;td&gt;3806060.0&lt;/td&gt;
      &lt;td&gt;2020&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;United Kingdom&lt;/td&gt;
      &lt;td&gt;Western Europe&lt;/td&gt;
      &lt;td&gt;Europe&lt;/td&gt;
      &lt;td&gt;3108416.0&lt;/td&gt;
      &lt;td&gt;2021&lt;/td&gt;
      &lt;td&gt;2764198.0&lt;/td&gt;
      &lt;td&gt;2020&lt;/td&gt;
      &lt;td&gt;2707744.0&lt;/td&gt;
      &lt;td&gt;2020&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;This is clearly what we were looking for. A part from the footnotes, the table is already clean and organized.&lt;/p&gt;
&lt;p&gt;If we knew the name of the table, we could directly retrieve it. However, we will see more about it in the next lecture.&lt;/p&gt;
&lt;h2 id=&#34;specific-libraries&#34;&gt;Specific Libraries&lt;/h2&gt;
&lt;p&gt;Sometimes, there are libraries that are already written down to do the scraping for you. Each one is tailored for a specific website and they are usually userwritten and prone to bugs and errors. However, they are often efficient and save you the time to worry about getting around some website-specific issues.&lt;/p&gt;
&lt;p&gt;One example is the &lt;code&gt;pytrends&lt;/code&gt; library for scraping Google Trends. Let&amp;rsquo;s first install it&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;pip3 install pytrends
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s see how it works. Imagine we want to do the following search:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;words &amp;ldquo;python&amp;rdquo;, &amp;ldquo;matlab&amp;rdquo;, &amp;ldquo;stata&amp;rdquo;&lt;/li&gt;
&lt;li&gt;the the second half of in 2019&lt;/li&gt;
&lt;li&gt;daily&lt;/li&gt;
&lt;li&gt;in the US&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We can get more details on how pytrends works &lt;a href=&#34;https://github.com/GeneralMills/pytrends#historical-hourly-interest&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;. The important thing to know is that if you query a time period of more than 200 days, Google will give you weekly results, instead of daily.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Pytrends search
from pytrends.request import TrendReq

# Set parameters
words = [&#39;python&#39;, &#39;matlab&#39;, &#39;stata&#39;]
timeframe = &#39;2019-07-01 2019-12-31&#39;
country = &#39;US&#39;

# Get data
pytrend = TrendReq()
pytrend.build_payload(kw_list=words, timeframe=timeframe, geo=country)
df_trends = pytrend.interest_over_time()

# Plot
trends_plot = df_trends.plot.line()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/06_web_scraping_16_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Apparently people don&amp;rsquo;t code during the weekend&amp;hellip;.&lt;/p&gt;
&lt;h2 id=&#34;apis&#34;&gt;APIs&lt;/h2&gt;
&lt;p&gt;From Wikipedia&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;An application programming interface (API) is an interface or communication protocol between different parts of a computer program intended to simplify the implementation and maintenance of software.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;In practice, it means that the are some webpages that are structured not to be user-readable but to be computer-readable. Let&amp;rsquo;s see one example.&lt;/p&gt;
&lt;p&gt;Google provides many APIs for its services. However, they now all need identification, which means that you have to log in into your Google account and request an API key from there. This allows Google to monitor your behavior since the number of API requests is limited and beyond a certain treshold, one need to pay (a lot).&lt;/p&gt;
&lt;p&gt;There are however some free APIs. One&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s have a look at one of these: zippopotam. Zippopotam lets you retrieve location information from a zip code in the US. Other countries are supported as well.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Let&#39;s search the department locatiton
import requests

zipcode = &#39;90210&#39;
url = &#39;https://api.zippopotam.us/us/&#39;+zipcode

response = requests.get(url)
data = response.json()
data
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;{&#39;post code&#39;: &#39;90210&#39;,
 &#39;country&#39;: &#39;United States&#39;,
 &#39;country abbreviation&#39;: &#39;US&#39;,
 &#39;places&#39;: [{&#39;place name&#39;: &#39;Beverly Hills&#39;,
   &#39;longitude&#39;: &#39;-118.4065&#39;,
   &#39;state&#39;: &#39;California&#39;,
   &#39;state abbreviation&#39;: &#39;CA&#39;,
   &#39;latitude&#39;: &#39;34.0901&#39;}]}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Data is in JSON (JavaScript Object Notation) format which is basically a nested dictionary-list format. Indeed, we see that in our case, data is a dictionary where the last elements is a list with one element - another dictionary.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Check type of value
for d in data.values():
    print(type(d))
    
# Check list length
print(len(data[&#39;places&#39;]))

# Check type of content of list
print(type(data[&#39;places&#39;][0]))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;class &#39;str&#39;&amp;gt;
&amp;lt;class &#39;str&#39;&amp;gt;
&amp;lt;class &#39;str&#39;&amp;gt;
&amp;lt;class &#39;list&#39;&amp;gt;
1
&amp;lt;class &#39;dict&#39;&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The part that could be interesting to us is contained in the &lt;code&gt;places&lt;/code&gt; category. We can easily extract it and transform it into a dataframe.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Add zipcode to data
data[&#39;places&#39;][0][&#39;zipcode&#39;] = zipcode

# Export data
df = pd.DataFrame(data[&#39;places&#39;])
df
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;place name&lt;/th&gt;
      &lt;th&gt;longitude&lt;/th&gt;
      &lt;th&gt;state&lt;/th&gt;
      &lt;th&gt;state abbreviation&lt;/th&gt;
      &lt;th&gt;latitude&lt;/th&gt;
      &lt;th&gt;zipcode&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;Beverly Hills&lt;/td&gt;
      &lt;td&gt;-118.4065&lt;/td&gt;
      &lt;td&gt;California&lt;/td&gt;
      &lt;td&gt;CA&lt;/td&gt;
      &lt;td&gt;34.0901&lt;/td&gt;
      &lt;td&gt;90210&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;h2 id=&#34;static-webscraping&#34;&gt;Static Webscraping&lt;/h2&gt;
&lt;p&gt;We have so far used pre-made tools in order to do web-scraping. When the website contains the data in a nice table or an API is available, we do not need to worry much and we can directly retrieve the data. However, most of web scraping is much more complicated. Data is often the product of webscraping and is not readily available. Moreover, sometimes webscraping knowledge can supplement the need to pay for an API.&lt;/p&gt;
&lt;h3 id=&#34;http&#34;&gt;HTTP&lt;/h3&gt;
&lt;p&gt;What happens when you open a page on the internet? In short, your web browser is sending a request to the website that, in turn, sends back a reply/response. The exchange of messages is complex but its core involves a HyperText Transfer Protocol (HTTP) request message to a web server, followed by a HTTP response (or reply). All static webscraping is build on HTTP so let&amp;rsquo;s have a closer look.&lt;/p&gt;
&lt;p&gt;An HTTP message essentially has 4 components:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;A request line&lt;/li&gt;
&lt;li&gt;A number of request headers&lt;/li&gt;
&lt;li&gt;An empty line&lt;/li&gt;
&lt;li&gt;An optional message&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;em&gt;Example&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;A request message could be&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;GET /hello.htm HTTP/1.1
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The response would be&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;HTTP/1.1 200 OK
Date: Sun, 10 Oct 2010 23:26:07 GMT
Server: Apache/2.2.8 (Ubuntu) mod_ssl/2.2.8 OpenSSL/0.9.8g
Last-Modified: Sun, 26 Sep 2010 22:04:35 GMT
ETag: &amp;quot;45b6-834-49130cc1182c0&amp;quot;
Accept-Ranges: bytes
Content-Length: 12
Connection: close
Content-Type: text/html

&amp;lt;html&amp;gt;
   &amp;lt;body&amp;gt;
   
      &amp;lt;h1&amp;gt;Hello, World!&amp;lt;/h1&amp;gt;
   
   &amp;lt;/body&amp;gt;
&amp;lt;/html&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So, in this case the parts are:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;The &lt;strong&gt;request line&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code&gt;HTTP/1.1 200 OK
&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;The &lt;strong&gt;request headers&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code&gt;Date: Sun, 10 Oct 2010 23:26:07 GMT
Server: Apache/2.2.8 (Ubuntu) mod_ssl/2.2.8 OpenSSL/0.9.8g
Last-Modified: Sun, 26 Sep 2010 22:04:35 GMT
ETag: &amp;quot;45b6-834-49130cc1182c0&amp;quot;
Accept-Ranges: bytes
Content-Length: 12
Connection: close
Content-Type: text/html
&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;3&#34;&gt;
&lt;li&gt;The &lt;strong&gt;empty line&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;The &lt;strong&gt;optional message&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;html&amp;gt;
   &amp;lt;body&amp;gt;
   
      &amp;lt;h1&amp;gt;Hello, World!&amp;lt;/h1&amp;gt;
   
   &amp;lt;/body&amp;gt;
&amp;lt;/html&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We are interested in the optional message, which is essentially the content of the page we want to scrape. The content is usually written in HTML which is not a proper programming language but rather a &lt;em&gt;typesetting language&lt;/em&gt; since it is the language underlying web pages and is usually generated from other programming languages.&lt;/p&gt;
&lt;h3 id=&#34;requests&#34;&gt;Requests&lt;/h3&gt;
&lt;p&gt;There are many different packages in python to send requests to a web page and read its response. The most user-friendly is the &lt;code&gt;requests&lt;/code&gt; package. You can find plenty of useful information on the &lt;code&gt;requests&lt;/code&gt; library on its website: &lt;a href=&#34;https://requests.readthedocs.io/en/master/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://requests.readthedocs.io/en/master/&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;We are now going to have a look at a simple example: &lt;a href=&#34;http://pythonscraping.com/pages/page1.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://pythonscraping.com/pages/page1.html&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Request a simple web page
url1 = &#39;http://pythonscraping.com/pages/page1.html&#39;
response = requests.get(url1)
print(response)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;Response [200]&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We are (hopefully) getting a &lt;code&gt;&amp;lt;Response [200]&amp;gt;&lt;/code&gt; message. In short, what we got is the status code of the request we sent to the website. The status code is a 3-digit code and essentially there are two broad categories of status codes:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;2XX: success&lt;/li&gt;
&lt;li&gt;4XX, 5XX: failure&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;It can be useful to know this codes as they are a fast way to check whether your request has failed or not. When webscraping the most common reasons you get an error are&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;The link does not exist: wither the link is old/expired or you misspelled it and hence there is no page to request&lt;/li&gt;
&lt;li&gt;You have been &amp;ldquo;caught&amp;rdquo;. This is pretty common when webscraping and happens every time you are too aggressive with your scraping. How much &amp;ldquo;aggressive&amp;rdquo; is &amp;ldquo;too agrressive&amp;rdquo; depends on the website. Usually big tech websites are particularly hard to scrape and anything that is &amp;ldquo;faster than human&amp;rdquo; gets blocked. Sometimes also slow but persistent requests get blocked as well.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;We have now analyzed the response status but, what is actually the response content? Let&amp;rsquo;s inspect the response object more in detail.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Print response attributes
dir(response)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[&#39;__attrs__&#39;,
 &#39;__bool__&#39;,
 &#39;__class__&#39;,
 &#39;__delattr__&#39;,
 &#39;__dict__&#39;,
 &#39;__dir__&#39;,
 &#39;__doc__&#39;,
 &#39;__enter__&#39;,
 &#39;__eq__&#39;,
 &#39;__exit__&#39;,
 &#39;__format__&#39;,
 &#39;__ge__&#39;,
 &#39;__getattribute__&#39;,
 &#39;__getstate__&#39;,
 &#39;__gt__&#39;,
 &#39;__hash__&#39;,
 &#39;__init__&#39;,
 &#39;__init_subclass__&#39;,
 &#39;__iter__&#39;,
 &#39;__le__&#39;,
 &#39;__lt__&#39;,
 &#39;__module__&#39;,
 &#39;__ne__&#39;,
 &#39;__new__&#39;,
 &#39;__nonzero__&#39;,
 &#39;__reduce__&#39;,
 &#39;__reduce_ex__&#39;,
 &#39;__repr__&#39;,
 &#39;__setattr__&#39;,
 &#39;__setstate__&#39;,
 &#39;__sizeof__&#39;,
 &#39;__str__&#39;,
 &#39;__subclasshook__&#39;,
 &#39;__weakref__&#39;,
 &#39;_content&#39;,
 &#39;_content_consumed&#39;,
 &#39;_next&#39;,
 &#39;apparent_encoding&#39;,
 &#39;close&#39;,
 &#39;connection&#39;,
 &#39;content&#39;,
 &#39;cookies&#39;,
 &#39;elapsed&#39;,
 &#39;encoding&#39;,
 &#39;headers&#39;,
 &#39;history&#39;,
 &#39;is_permanent_redirect&#39;,
 &#39;is_redirect&#39;,
 &#39;iter_content&#39;,
 &#39;iter_lines&#39;,
 &#39;json&#39;,
 &#39;links&#39;,
 &#39;next&#39;,
 &#39;ok&#39;,
 &#39;raise_for_status&#39;,
 &#39;raw&#39;,
 &#39;reason&#39;,
 &#39;request&#39;,
 &#39;status_code&#39;,
 &#39;text&#39;,
 &#39;url&#39;]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We are actually interested in the text of the response.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Print response content
response.text
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&#39;&amp;lt;html&amp;gt;\n&amp;lt;head&amp;gt;\n&amp;lt;title&amp;gt;A Useful Page&amp;lt;/title&amp;gt;\n&amp;lt;/head&amp;gt;\n&amp;lt;body&amp;gt;\n&amp;lt;h1&amp;gt;An Interesting Title&amp;lt;/h1&amp;gt;\n&amp;lt;div&amp;gt;\nLorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\n&amp;lt;/div&amp;gt;\n&amp;lt;/body&amp;gt;\n&amp;lt;/html&amp;gt;\n&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is the whole content of the table. There is a large chunk of text and other parts which look more obscure. In order to understand the structure of the page, we need to have a closer look at the language in which the webpage is written: HTML. We will do it in the next section.&lt;/p&gt;
&lt;p&gt;However, let&amp;rsquo;s first analyze the other relevant components of the response. We have already had a look at the status. Let&amp;rsquo;s inspect the headers.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Print response headers
response.headers
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;{&#39;Server&#39;: &#39;nginx&#39;, &#39;Date&#39;: &#39;Thu, 10 Feb 2022 11:11:41 GMT&#39;, &#39;Content-Type&#39;: &#39;text/html&#39;, &#39;Content-Length&#39;: &#39;361&#39;, &#39;Connection&#39;: &#39;keep-alive&#39;, &#39;X-Accel-Version&#39;: &#39;0.01&#39;, &#39;Last-Modified&#39;: &#39;Sat, 09 Jun 2018 19:15:58 GMT&#39;, &#39;ETag&#39;: &#39;&amp;quot;234-56e3a58a63780-gzip&amp;quot;&#39;, &#39;Accept-Ranges&#39;: &#39;bytes&#39;, &#39;Vary&#39;: &#39;Accept-Encoding&#39;, &#39;Content-Encoding&#39;: &#39;gzip&#39;, &#39;X-Powered-By&#39;: &#39;PleskLin&#39;}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;From the headers we can see&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the present date&lt;/li&gt;
&lt;li&gt;the name of the server hosting the page&lt;/li&gt;
&lt;li&gt;the last time the page was modified&lt;/li&gt;
&lt;li&gt;other stuff&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Let&amp;rsquo;s now look at the headers of our request.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Request headers
def check_headers(r):
    test_headers = dict(zip(r.request.headers.keys(), r.request.headers.values()))
    pprint(test_headers)
    
check_headers(response)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;{&#39;Accept&#39;: &#39;*/*&#39;,
 &#39;Accept-Encoding&#39;: &#39;gzip, deflate, br&#39;,
 &#39;Connection&#39;: &#39;keep-alive&#39;,
 &#39;User-Agent&#39;: &#39;python-requests/2.27.1&#39;}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The headers of our request are pretty minimal. In order to see what normal headers look like, go to &lt;a href=&#34;https://www.whatismybrowser.com/developers/what-http-headers-is-my-browser-sending&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.whatismybrowser.com/developers/what-http-headers-is-my-browser-sending&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Normal headers look something like:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;{&#39;Accept&#39;: &#39;text/html,application/xhtml+xml,application/xml;q = 0.9, image / &#39;
           &#39;webp, * / *;q = 0.8&#39;,
 &#39;Accept-Encoding&#39;: &#39;gzip, deflate, br&#39;,
 &#39;Accept-Language&#39;: &#39;en-US,en;q=0.9,it-IT;q=0.8,it;q=0.7,de-DE;q=0.6,de;q=0.5&#39;,
 &#39;Connection&#39;: &#39;keep-alive&#39;,
 &#39;Host&#39;: &#39;www.whatismybrowser.com&#39;,
 &#39;Referer&#39;: &#39;http://localhost:8888/&#39;,
 &#39;User-Agent&#39;: &#39;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_6) &#39;
               &#39;AppleWebKit/537.36 (KHTML, like Gecko) Chrome/79.0.3945.88 &#39;
               &#39;Safari/537.36&#39;}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The most important difference is that the &lt;code&gt;requests&lt;/code&gt; model default &lt;em&gt;User-Agent&lt;/em&gt; is &lt;code&gt;python-requests/2.22.0&lt;/code&gt; which means that we are walking around the web with a big &lt;strong&gt;WARNING: web scrapers&lt;/strong&gt; sign. This is the simplest way to get caught and blocked by a website. Luckily, we can easily change our headers in order to be more subtle.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Change headers
headers = {&amp;quot;User-Agent&amp;quot;: &amp;quot;Mozilla/5.0&amp;quot;,
               &amp;quot;Accept&amp;quot;: &amp;quot;webp, * / *;q = 0.8&amp;quot;,
               &amp;quot;Accept-Language&amp;quot;: &amp;quot;en-US,en;q=0.9&amp;quot;,
               &amp;quot;Accept-Encoding&amp;quot;: &amp;quot;br, gzip, deflate&amp;quot;,
               &amp;quot;Referer&amp;quot;: &amp;quot;https://www.google.ch/&amp;quot;}

# Test if change worked
response = requests.get(url1, headers=headers)
check_headers(response)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;{&#39;Accept&#39;: &#39;webp, * / *;q = 0.8&#39;,
 &#39;Accept-Encoding&#39;: &#39;br, gzip, deflate&#39;,
 &#39;Accept-Language&#39;: &#39;en-US,en;q=0.9&#39;,
 &#39;Connection&#39;: &#39;keep-alive&#39;,
 &#39;Referer&#39;: &#39;https://www.google.ch/&#39;,
 &#39;User-Agent&#39;: &#39;Mozilla/5.0&#39;}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Nice! Now we are a little more stealthy.&lt;/p&gt;
&lt;p&gt;You might now be asking yourself what are the ethical limits of webscraping. Information on the internet is public but scraping a website imposes a workload on the website&amp;rsquo;s server. If the website is not protected against aggressive scrapers (most websites are), your activity could significantly slower the website or even crash it.&lt;/p&gt;
&lt;p&gt;Usually websites include their policies for scraping in a text file named &lt;code&gt;robots.txt&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s have a look at the &lt;code&gt;robots.txt&lt;/code&gt; file of &lt;a href=&#34;http://pythonscraping.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://pythonscraping.com/&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Read robots.txt
response = requests.get(&#39;http://pythonscraping.com/robots.txt&#39;)
print(response.text)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;#
# robots.txt
#
# This file is to prevent the crawling and indexing of certain parts
# of your site by web crawlers and spiders run by sites like Yahoo!
# and Google. By telling these &amp;quot;robots&amp;quot; where not to go on your site,
# you save bandwidth and server resources.
#
# This file will be ignored unless it is at the root of your host:
# Used:    http://example.com/robots.txt
# Ignored: http://example.com/site/robots.txt
#
# For more information about the robots.txt standard, see:
# http://www.robotstxt.org/robotstxt.html
#
# For syntax checking, see:
# http://www.frobee.com/robots-txt-check

User-agent: *
Crawl-delay: 10
# Directories
Disallow: /includes/
Disallow: /misc/
Disallow: /modules/
Disallow: /profiles/
Disallow: /scripts/
Disallow: /themes/
# Files
Disallow: /CHANGELOG.txt
Disallow: /cron.php
Disallow: /INSTALL.mysql.txt
Disallow: /INSTALL.pgsql.txt
Disallow: /INSTALL.sqlite.txt
Disallow: /install.php
Disallow: /INSTALL.txt
Disallow: /LICENSE.txt
Disallow: /MAINTAINERS.txt
Disallow: /update.php
Disallow: /UPGRADE.txt
Disallow: /xmlrpc.php
# Paths (clean URLs)
Disallow: /admin/
Disallow: /comment/reply/
Disallow: /filter/tips/
Disallow: /node/add/
Disallow: /search/
Disallow: /user/register/
Disallow: /user/password/
Disallow: /user/login/
Disallow: /user/logout/
# Paths (no clean URLs)
Disallow: /?q=admin/
Disallow: /?q=comment/reply/
Disallow: /?q=filter/tips/
Disallow: /?q=node/add/
Disallow: /?q=search/
Disallow: /?q=user/password/
Disallow: /?q=user/register/
Disallow: /?q=user/login/
Disallow: /?q=user/logout/
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As we can see, this &lt;code&gt;robots.txt&lt;/code&gt; file mostly deals with crawlers, i.e. scripts that are designed to recover the structure of a website by exploring it. Crawlers are mostly used by browsers that want to index websites.&lt;/p&gt;
&lt;p&gt;Now we have explored most of the issues around HTTP requests. We can now proceed to what we are interested in: the content of the web page. In order to do that, we need to know the language in which wabpages are written: HTML.&lt;/p&gt;
&lt;h3 id=&#34;html&#34;&gt;HTML&lt;/h3&gt;
&lt;p&gt;Hypertext Markup Language (HTML) is the standard markup language for documents designed to be displayed in a web browser. Web browsers receive HTML documents from a web server or from local storage and render the documents into multimedia web pages. HTML describes the structure of a web page semantically and originally included cues for the appearance of the document.&lt;/p&gt;
&lt;p&gt;HTML elements are delineated by tags, written using angle brackets.&lt;/p&gt;
&lt;h4 id=&#34;tags&#34;&gt;Tags&lt;/h4&gt;
&lt;p&gt;Tags are the cues that HTML uses to surround content and provide information about its nature. There is a very large amount of tags but some of the most common are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;&amp;lt;head&amp;gt;&lt;/code&gt; and &lt;code&gt;&amp;lt;body&amp;gt;&lt;/code&gt; for head and body of the page&lt;/li&gt;
&lt;li&gt;&lt;code&gt;&amp;lt;p&amp;gt;&lt;/code&gt; for paragraphs&lt;/li&gt;
&lt;li&gt;&lt;code&gt;&amp;lt;br&amp;gt;&lt;/code&gt; for line breaks&lt;/li&gt;
&lt;li&gt;&lt;code&gt;&amp;lt;table&amp;gt;&lt;/code&gt; for tables. These are the ones that &lt;code&gt;pandas&lt;/code&gt; reads. However, we have seen that not all elements that look like tables are actually &lt;code&gt;&amp;lt;table&amp;gt;&lt;/code&gt; and viceversa. Table elements are tagged as &lt;code&gt;&amp;lt;th&amp;gt;&lt;/code&gt; (table header), &lt;code&gt;&amp;lt;tr&amp;gt;&lt;/code&gt; (table row) and &lt;code&gt;&amp;lt;td&amp;gt;&lt;/code&gt; (table data: a cell)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;&amp;lt;img&amp;gt;&lt;/code&gt; for images&lt;/li&gt;
&lt;li&gt;&lt;code&gt;&amp;lt;h1&amp;gt;&lt;/code&gt; to &lt;code&gt;&amp;lt;h6&amp;gt;&lt;/code&gt; for headers (titles and subtitles)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;&amp;lt;div&amp;gt;&lt;/code&gt; dor divisions, i.e. for grouping elements&lt;/li&gt;
&lt;li&gt;&lt;code&gt;&amp;lt;a&amp;gt;&lt;/code&gt; for hyperlinks&lt;/li&gt;
&lt;li&gt;&lt;code&gt;&amp;lt;ul&amp;gt;&lt;/code&gt; and &lt;code&gt;&amp;lt;ol&amp;gt;&lt;/code&gt; for unordered and ordered lists where list elements are tagged as &lt;code&gt;&amp;lt;li&amp;gt;&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Let&amp;rsquo;s have a look at the previous page&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Inspect HTML
response.text
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&#39;#\n# robots.txt\n#\n# This file is to prevent the crawling and indexing of certain parts\n# of your site by web crawlers and spiders run by sites like Yahoo!\n# and Google. By telling these &amp;quot;robots&amp;quot; where not to go on your site,\n# you save bandwidth and server resources.\n#\n# This file will be ignored unless it is at the root of your host:\n# Used:    http://example.com/robots.txt\n# Ignored: http://example.com/site/robots.txt\n#\n# For more information about the robots.txt standard, see:\n# http://www.robotstxt.org/robotstxt.html\n#\n# For syntax checking, see:\n# http://www.frobee.com/robots-txt-check\n\nUser-agent: *\nCrawl-delay: 10\n# Directories\nDisallow: /includes/\nDisallow: /misc/\nDisallow: /modules/\nDisallow: /profiles/\nDisallow: /scripts/\nDisallow: /themes/\n# Files\nDisallow: /CHANGELOG.txt\nDisallow: /cron.php\nDisallow: /INSTALL.mysql.txt\nDisallow: /INSTALL.pgsql.txt\nDisallow: /INSTALL.sqlite.txt\nDisallow: /install.php\nDisallow: /INSTALL.txt\nDisallow: /LICENSE.txt\nDisallow: /MAINTAINERS.txt\nDisallow: /update.php\nDisallow: /UPGRADE.txt\nDisallow: /xmlrpc.php\n# Paths (clean URLs)\nDisallow: /admin/\nDisallow: /comment/reply/\nDisallow: /filter/tips/\nDisallow: /node/add/\nDisallow: /search/\nDisallow: /user/register/\nDisallow: /user/password/\nDisallow: /user/login/\nDisallow: /user/logout/\n# Paths (no clean URLs)\nDisallow: /?q=admin/\nDisallow: /?q=comment/reply/\nDisallow: /?q=filter/tips/\nDisallow: /?q=node/add/\nDisallow: /?q=search/\nDisallow: /?q=user/password/\nDisallow: /?q=user/register/\nDisallow: /?q=user/login/\nDisallow: /?q=user/logout/\n&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The response looks a little bit messy and not really readable.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;BeautifulSoup&lt;/code&gt; is a python library that renders http responses in a user friendly format and helps recovering elements from tags and attributes.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;pip3 install bs4
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s have a look.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Make response readable
soup = BeautifulSoup(response.text, &#39;lxml&#39;)
print(soup)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;html&amp;gt;&amp;lt;body&amp;gt;&amp;lt;p&amp;gt;#
# robots.txt
#
# This file is to prevent the crawling and indexing of certain parts
# of your site by web crawlers and spiders run by sites like Yahoo!
# and Google. By telling these &amp;quot;robots&amp;quot; where not to go on your site,
# you save bandwidth and server resources.
#
# This file will be ignored unless it is at the root of your host:
# Used:    http://example.com/robots.txt
# Ignored: http://example.com/site/robots.txt
#
# For more information about the robots.txt standard, see:
# http://www.robotstxt.org/robotstxt.html
#
# For syntax checking, see:
# http://www.frobee.com/robots-txt-check

User-agent: *
Crawl-delay: 10
# Directories
Disallow: /includes/
Disallow: /misc/
Disallow: /modules/
Disallow: /profiles/
Disallow: /scripts/
Disallow: /themes/
# Files
Disallow: /CHANGELOG.txt
Disallow: /cron.php
Disallow: /INSTALL.mysql.txt
Disallow: /INSTALL.pgsql.txt
Disallow: /INSTALL.sqlite.txt
Disallow: /install.php
Disallow: /INSTALL.txt
Disallow: /LICENSE.txt
Disallow: /MAINTAINERS.txt
Disallow: /update.php
Disallow: /UPGRADE.txt
Disallow: /xmlrpc.php
# Paths (clean URLs)
Disallow: /admin/
Disallow: /comment/reply/
Disallow: /filter/tips/
Disallow: /node/add/
Disallow: /search/
Disallow: /user/register/
Disallow: /user/password/
Disallow: /user/login/
Disallow: /user/logout/
# Paths (no clean URLs)
Disallow: /?q=admin/
Disallow: /?q=comment/reply/
Disallow: /?q=filter/tips/
Disallow: /?q=node/add/
Disallow: /?q=search/
Disallow: /?q=user/password/
Disallow: /?q=user/register/
Disallow: /?q=user/login/
Disallow: /?q=user/logout/
&amp;lt;/p&amp;gt;&amp;lt;/body&amp;gt;&amp;lt;/html&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;First of all, what is the &lt;code&gt;html5lib&lt;/code&gt; option? It&amp;rsquo;s the parser. In short, there are often small mistakes/variations in HTML and each parser interprets it differently. In principles, the latest HTML standard is HTML5, therefore the &lt;code&gt;html5lib&lt;/code&gt; parser should be the most &amp;ldquo;correct&amp;rdquo; parser. It might happen that the same code does not work for another person if you use a different parser.&lt;/p&gt;
&lt;p&gt;This is much better but it can be improved.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Prettify response
print(soup.prettify())
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;html&amp;gt;
 &amp;lt;body&amp;gt;
  &amp;lt;p&amp;gt;
   #
# robots.txt
#
# This file is to prevent the crawling and indexing of certain parts
# of your site by web crawlers and spiders run by sites like Yahoo!
# and Google. By telling these &amp;quot;robots&amp;quot; where not to go on your site,
# you save bandwidth and server resources.
#
# This file will be ignored unless it is at the root of your host:
# Used:    http://example.com/robots.txt
# Ignored: http://example.com/site/robots.txt
#
# For more information about the robots.txt standard, see:
# http://www.robotstxt.org/robotstxt.html
#
# For syntax checking, see:
# http://www.frobee.com/robots-txt-check

User-agent: *
Crawl-delay: 10
# Directories
Disallow: /includes/
Disallow: /misc/
Disallow: /modules/
Disallow: /profiles/
Disallow: /scripts/
Disallow: /themes/
# Files
Disallow: /CHANGELOG.txt
Disallow: /cron.php
Disallow: /INSTALL.mysql.txt
Disallow: /INSTALL.pgsql.txt
Disallow: /INSTALL.sqlite.txt
Disallow: /install.php
Disallow: /INSTALL.txt
Disallow: /LICENSE.txt
Disallow: /MAINTAINERS.txt
Disallow: /update.php
Disallow: /UPGRADE.txt
Disallow: /xmlrpc.php
# Paths (clean URLs)
Disallow: /admin/
Disallow: /comment/reply/
Disallow: /filter/tips/
Disallow: /node/add/
Disallow: /search/
Disallow: /user/register/
Disallow: /user/password/
Disallow: /user/login/
Disallow: /user/logout/
# Paths (no clean URLs)
Disallow: /?q=admin/
Disallow: /?q=comment/reply/
Disallow: /?q=filter/tips/
Disallow: /?q=node/add/
Disallow: /?q=search/
Disallow: /?q=user/password/
Disallow: /?q=user/register/
Disallow: /?q=user/login/
Disallow: /?q=user/logout/
  &amp;lt;/p&amp;gt;
 &amp;lt;/body&amp;gt;
&amp;lt;/html&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is much better. Now the tree structure of the HTML page is clearly visible and we can visually separate the different elements.&lt;/p&gt;
&lt;p&gt;In particular, the structure of the page is:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;page head
&lt;ul&gt;
&lt;li&gt;with ttle: &amp;ldquo;A Useful Page&amp;rdquo;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;page body
&lt;ul&gt;
&lt;li&gt;with level 1 header &amp;ldquo;An Interesting Title&amp;rdquo;&lt;/li&gt;
&lt;li&gt;a division with text &amp;ldquo;Lorem ipsum&amp;hellip;&amp;rdquo;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;How do we work with these elements? Suppose we want to recover the title and the text. The requests library has some useful functions.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Find the title
url = &#39;http://pythonscraping.com/pages/page1.html&#39;
response = requests.get(url)
soup = BeautifulSoup(response.text, &#39;lxml&#39;)
soup.find(&#39;title&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;title&amp;gt;A Useful Page&amp;lt;/title&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Extract text
soup.find(&#39;title&#39;).text
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&#39;A Useful Page&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Find all h1 elements
soup.find_all(&#39;h1&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[&amp;lt;h1&amp;gt;An Interesting Title&amp;lt;/h1&amp;gt;]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Find all title or h1 elements
soup.find_all([&#39;title&#39;,&#39;h1&#39;])
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[&amp;lt;title&amp;gt;A Useful Page&amp;lt;/title&amp;gt;, &amp;lt;h1&amp;gt;An Interesting Title&amp;lt;/h1&amp;gt;]
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;regular-expressions&#34;&gt;Regular Expressions&lt;/h3&gt;
&lt;p&gt;Note that there is always a more direct alternative: using regular expressions directly on the response!&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Find the title
re.findall(&#39;&amp;lt;title&amp;gt;(.*)&amp;lt;/title&amp;gt;&#39;, response.text)[0]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&#39;A Useful Page&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Find all h1 elements
re.findall(&#39;&amp;lt;h1&amp;gt;(.*)&amp;lt;/h1&amp;gt;&#39;, response.text)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[&#39;An Interesting Title&#39;]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Find all title or h1 elements
[x[1] for x in re.findall(&#39;&amp;lt;(title|h1)&amp;gt;(.*)&amp;lt;&#39;, response.text)]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[&#39;A Useful Page&#39;, &#39;An Interesting Title&#39;]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This was a very simple page and there was not so much to look for. Let&amp;rsquo;s now look at a more realistic example.&lt;/p&gt;
&lt;h3 id=&#34;attributes&#34;&gt;Attributes&lt;/h3&gt;
&lt;p&gt;Let&amp;rsquo;s inspect a slightly more complicated page: &lt;a href=&#34;http://pythonscraping.com/pages/page3.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://pythonscraping.com/pages/page3.html&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;In this page, there is much more content than in the previous one. There seems to be a table, there are images, hyperlinks, etc&amp;hellip; It&amp;rsquo;s the perfect playground. Let&amp;rsquo;s have a look at what does the HTML code look like.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Inspect HTML code
url2 = &#39;http://pythonscraping.com/pages/page3.html&#39;
response = requests.get(url2)
soup = BeautifulSoup(response.text,&#39;lxml&#39;)
print(soup.prettify())
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;html&amp;gt;
 &amp;lt;head&amp;gt;
  &amp;lt;style&amp;gt;
   img{
	width:75px;
}
table{
	width:50%;
}
td{
	margin:10px;
	padding:10px;
}
.wrapper{
	width:800px;
}
.excitingNote{
	font-style:italic;
	font-weight:bold;
}
  &amp;lt;/style&amp;gt;
 &amp;lt;/head&amp;gt;
 &amp;lt;body&amp;gt;
  &amp;lt;div id=&amp;quot;wrapper&amp;quot;&amp;gt;
   &amp;lt;img src=&amp;quot;../img/gifts/logo.jpg&amp;quot; style=&amp;quot;float:left;&amp;quot;/&amp;gt;
   &amp;lt;h1&amp;gt;
    Totally Normal Gifts
   &amp;lt;/h1&amp;gt;
   &amp;lt;div id=&amp;quot;content&amp;quot;&amp;gt;
    Here is a collection of totally normal, totally reasonable gifts that your friends are sure to love! Our collection is
hand-curated by well-paid, free-range Tibetan monks.
    &amp;lt;p&amp;gt;
     We haven&#39;t figured out how to make online shopping carts yet, but you can send us a check to:
     &amp;lt;br/&amp;gt;
     123 Main St.
     &amp;lt;br/&amp;gt;
     Abuja, Nigeria
We will then send your totally amazing gift, pronto! Please include an extra $5.00 for gift wrapping.
    &amp;lt;/p&amp;gt;
   &amp;lt;/div&amp;gt;
   &amp;lt;table id=&amp;quot;giftList&amp;quot;&amp;gt;
    &amp;lt;tr&amp;gt;
     &amp;lt;th&amp;gt;
      Item Title
     &amp;lt;/th&amp;gt;
     &amp;lt;th&amp;gt;
      Description
     &amp;lt;/th&amp;gt;
     &amp;lt;th&amp;gt;
      Cost
     &amp;lt;/th&amp;gt;
     &amp;lt;th&amp;gt;
      Image
     &amp;lt;/th&amp;gt;
    &amp;lt;/tr&amp;gt;
    &amp;lt;tr class=&amp;quot;gift&amp;quot; id=&amp;quot;gift1&amp;quot;&amp;gt;
     &amp;lt;td&amp;gt;
      Vegetable Basket
     &amp;lt;/td&amp;gt;
     &amp;lt;td&amp;gt;
      This vegetable basket is the perfect gift for your health conscious (or overweight) friends!
      &amp;lt;span class=&amp;quot;excitingNote&amp;quot;&amp;gt;
       Now with super-colorful bell peppers!
      &amp;lt;/span&amp;gt;
     &amp;lt;/td&amp;gt;
     &amp;lt;td&amp;gt;
      $15.00
     &amp;lt;/td&amp;gt;
     &amp;lt;td&amp;gt;
      &amp;lt;img src=&amp;quot;../img/gifts/img1.jpg&amp;quot;/&amp;gt;
     &amp;lt;/td&amp;gt;
    &amp;lt;/tr&amp;gt;
    &amp;lt;tr class=&amp;quot;gift&amp;quot; id=&amp;quot;gift2&amp;quot;&amp;gt;
     &amp;lt;td&amp;gt;
      Russian Nesting Dolls
     &amp;lt;/td&amp;gt;
     &amp;lt;td&amp;gt;
      Hand-painted by trained monkeys, these exquisite dolls are priceless! And by &amp;quot;priceless,&amp;quot; we mean &amp;quot;extremely expensive&amp;quot;!
      &amp;lt;span class=&amp;quot;excitingNote&amp;quot;&amp;gt;
       8 entire dolls per set! Octuple the presents!
      &amp;lt;/span&amp;gt;
     &amp;lt;/td&amp;gt;
     &amp;lt;td&amp;gt;
      $10,000.52
     &amp;lt;/td&amp;gt;
     &amp;lt;td&amp;gt;
      &amp;lt;img src=&amp;quot;../img/gifts/img2.jpg&amp;quot;/&amp;gt;
     &amp;lt;/td&amp;gt;
    &amp;lt;/tr&amp;gt;
    &amp;lt;tr class=&amp;quot;gift&amp;quot; id=&amp;quot;gift3&amp;quot;&amp;gt;
     &amp;lt;td&amp;gt;
      Fish Painting
     &amp;lt;/td&amp;gt;
     &amp;lt;td&amp;gt;
      If something seems fishy about this painting, it&#39;s because it&#39;s a fish!
      &amp;lt;span class=&amp;quot;excitingNote&amp;quot;&amp;gt;
       Also hand-painted by trained monkeys!
      &amp;lt;/span&amp;gt;
     &amp;lt;/td&amp;gt;
     &amp;lt;td&amp;gt;
      $10,005.00
     &amp;lt;/td&amp;gt;
     &amp;lt;td&amp;gt;
      &amp;lt;img src=&amp;quot;../img/gifts/img3.jpg&amp;quot;/&amp;gt;
     &amp;lt;/td&amp;gt;
    &amp;lt;/tr&amp;gt;
    &amp;lt;tr class=&amp;quot;gift&amp;quot; id=&amp;quot;gift4&amp;quot;&amp;gt;
     &amp;lt;td&amp;gt;
      Dead Parrot
     &amp;lt;/td&amp;gt;
     &amp;lt;td&amp;gt;
      This is an ex-parrot!
      &amp;lt;span class=&amp;quot;excitingNote&amp;quot;&amp;gt;
       Or maybe he&#39;s only resting?
      &amp;lt;/span&amp;gt;
     &amp;lt;/td&amp;gt;
     &amp;lt;td&amp;gt;
      $0.50
     &amp;lt;/td&amp;gt;
     &amp;lt;td&amp;gt;
      &amp;lt;img src=&amp;quot;../img/gifts/img4.jpg&amp;quot;/&amp;gt;
     &amp;lt;/td&amp;gt;
    &amp;lt;/tr&amp;gt;
    &amp;lt;tr class=&amp;quot;gift&amp;quot; id=&amp;quot;gift5&amp;quot;&amp;gt;
     &amp;lt;td&amp;gt;
      Mystery Box
     &amp;lt;/td&amp;gt;
     &amp;lt;td&amp;gt;
      If you love suprises, this mystery box is for you! Do not place on light-colored surfaces. May cause oil staining.
      &amp;lt;span class=&amp;quot;excitingNote&amp;quot;&amp;gt;
       Keep your friends guessing!
      &amp;lt;/span&amp;gt;
     &amp;lt;/td&amp;gt;
     &amp;lt;td&amp;gt;
      $1.50
     &amp;lt;/td&amp;gt;
     &amp;lt;td&amp;gt;
      &amp;lt;img src=&amp;quot;../img/gifts/img6.jpg&amp;quot;/&amp;gt;
     &amp;lt;/td&amp;gt;
    &amp;lt;/tr&amp;gt;
   &amp;lt;/table&amp;gt;
   &amp;lt;div id=&amp;quot;footer&amp;quot;&amp;gt;
    © Totally Normal Gifts, Inc.
    &amp;lt;br/&amp;gt;
    +234 (617) 863-0736
   &amp;lt;/div&amp;gt;
  &amp;lt;/div&amp;gt;
 &amp;lt;/body&amp;gt;
&amp;lt;/html&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As we can see, now the page is much more complicated than before. An important distintion is that now some tags have classes. For example, the first &lt;code&gt;&amp;lt;img&amp;gt;&lt;/code&gt; tag now has a class &lt;code&gt;src&lt;/code&gt; and a class &lt;code&gt;style&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;img src=&amp;quot;../img/gifts/logo.jpg&amp;quot; style=&amp;quot;float:left;&amp;quot;&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Moreover, even though &lt;code&gt;BeautifulSoup&lt;/code&gt; is formatting the page in a nicer way, it&amp;rsquo;s still pretty hard to go through it. How can one locate one specific element? And, most importantly, if you know the element only graphically, how do you recover the equivalent in the HTML code?&lt;/p&gt;
&lt;p&gt;The best way is to use the &lt;code&gt;inspect&lt;/code&gt; function from Chrome. Firefox has an equivalent function. Let&amp;rsquo;s inspect the original page.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;Suppose now you want to recover all item names. Let&amp;rsquo;s inspect the first. The corresponding line looks like this:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;    &amp;lt;tr class=&amp;quot;gift&amp;quot; id=&amp;quot;gift1&amp;quot;&amp;gt;
     &amp;lt;td&amp;gt;
      Vegetable Basket
     &amp;lt;/td&amp;gt;
     &amp;lt;td&amp;gt;
      This vegetable basket is the perfect gift for your health conscious (or overweight) friends!
      &amp;lt;span class=&amp;quot;excitingNote&amp;quot;&amp;gt;
       Now with super-colorful bell peppers!
      &amp;lt;/span&amp;gt;
     &amp;lt;/td&amp;gt;
     &amp;lt;td&amp;gt;
      $15.00
     &amp;lt;/td&amp;gt;
     &amp;lt;td&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s see some alternative ways.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Select the first td element
soup.find(&#39;td&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;td&amp;gt;
Vegetable Basket
&amp;lt;/td&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Select the first td element of the second tr element (row)
second_row = soup.find_all(&#39;tr&#39;)[1]
second_row.find(&#39;td&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;td&amp;gt;
Vegetable Basket
&amp;lt;/td&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Select the first element of the table with id=&amp;quot;giftList&amp;quot;
table = soup.find(&#39;table&#39;, {&amp;quot;id&amp;quot;:&amp;quot;giftList&amp;quot;})
table.find(&#39;td&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;td&amp;gt;
Vegetable Basket
&amp;lt;/td&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The last is the most robust way to scrape. In fact, the first two methods are likely to fail if the page gets modified. If another &lt;code&gt;td&lt;/code&gt; element gets added on top of the table, the code will recover something else entirely. In general it&amp;rsquo;s a good practice, to look if the element we want to scrape can be identified by some attribute that is likely to be invariant to changes to other parts of the web page. In this case, the table with &lt;code&gt;id=&amp;quot;giftList&amp;quot;&lt;/code&gt; is likely to be our object of interest even if another table id added, for example.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s say no we want to recover the whole table. What would you do?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import pandas as pd

# Shortcut
df = pd.read_html(url2)[0]
df
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;Item Title&lt;/th&gt;
      &lt;th&gt;Description&lt;/th&gt;
      &lt;th&gt;Cost&lt;/th&gt;
      &lt;th&gt;Image&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;Vegetable Basket&lt;/td&gt;
      &lt;td&gt;This vegetable basket is the perfect gift for ...&lt;/td&gt;
      &lt;td&gt;$15.00&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;Russian Nesting Dolls&lt;/td&gt;
      &lt;td&gt;Hand-painted by trained monkeys, these exquisi...&lt;/td&gt;
      &lt;td&gt;$10,000.52&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;Fish Painting&lt;/td&gt;
      &lt;td&gt;If something seems fishy about this painting, ...&lt;/td&gt;
      &lt;td&gt;$10,005.00&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;Dead Parrot&lt;/td&gt;
      &lt;td&gt;This is an ex-parrot! Or maybe he&#39;s only resting?&lt;/td&gt;
      &lt;td&gt;$0.50&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;Mystery Box&lt;/td&gt;
      &lt;td&gt;If you love suprises, this mystery box is for ...&lt;/td&gt;
      &lt;td&gt;$1.50&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Scraping with response
table = soup.find(&#39;table&#39;, {&amp;quot;id&amp;quot;:&amp;quot;giftList&amp;quot;})

# Create empty dataframe
col_names = [x.text.strip() for x in table.find_all(&#39;th&#39;)]
df = pd.DataFrame(columns=col_names)

# Loop over rows and append them to dataframe
for row in table.find_all(&#39;tr&#39;)[1:]:
    columns = [x.text.strip() for x in row.find_all(&#39;td&#39;)]
    df_row = dict(zip(col_names, columns))
    df = df.append(df_row, ignore_index=True)

df
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;/var/folders/xy/szm3b7211cj16ldwcywbvfzdqv7yhd/T/ipykernel_92159/3999490009.py:12: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  df = df.append(df_row, ignore_index=True)
/var/folders/xy/szm3b7211cj16ldwcywbvfzdqv7yhd/T/ipykernel_92159/3999490009.py:12: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  df = df.append(df_row, ignore_index=True)
/var/folders/xy/szm3b7211cj16ldwcywbvfzdqv7yhd/T/ipykernel_92159/3999490009.py:12: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  df = df.append(df_row, ignore_index=True)
/var/folders/xy/szm3b7211cj16ldwcywbvfzdqv7yhd/T/ipykernel_92159/3999490009.py:12: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  df = df.append(df_row, ignore_index=True)
/var/folders/xy/szm3b7211cj16ldwcywbvfzdqv7yhd/T/ipykernel_92159/3999490009.py:12: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  df = df.append(df_row, ignore_index=True)
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;Item Title&lt;/th&gt;
      &lt;th&gt;Description&lt;/th&gt;
      &lt;th&gt;Cost&lt;/th&gt;
      &lt;th&gt;Image&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;Vegetable Basket&lt;/td&gt;
      &lt;td&gt;This vegetable basket is the perfect gift for ...&lt;/td&gt;
      &lt;td&gt;$15.00&lt;/td&gt;
      &lt;td&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;Russian Nesting Dolls&lt;/td&gt;
      &lt;td&gt;Hand-painted by trained monkeys, these exquisi...&lt;/td&gt;
      &lt;td&gt;$10,000.52&lt;/td&gt;
      &lt;td&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;Fish Painting&lt;/td&gt;
      &lt;td&gt;If something seems fishy about this painting, ...&lt;/td&gt;
      &lt;td&gt;$10,005.00&lt;/td&gt;
      &lt;td&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;Dead Parrot&lt;/td&gt;
      &lt;td&gt;This is an ex-parrot! Or maybe he&#39;s only resting?&lt;/td&gt;
      &lt;td&gt;$0.50&lt;/td&gt;
      &lt;td&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;Mystery Box&lt;/td&gt;
      &lt;td&gt;If you love suprises, this mystery box is for ...&lt;/td&gt;
      &lt;td&gt;$1.50&lt;/td&gt;
      &lt;td&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Compact alternative
table = soup.find(&#39;table&#39;, {&amp;quot;id&amp;quot;:&amp;quot;giftList&amp;quot;})
content = [[x.text.strip() for x in row.find_all([&#39;th&#39;,&#39;td&#39;])] for row in table.find_all(&#39;tr&#39;)]
df = pd.DataFrame(content[1:], columns=content[0])

df
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;Item Title&lt;/th&gt;
      &lt;th&gt;Description&lt;/th&gt;
      &lt;th&gt;Cost&lt;/th&gt;
      &lt;th&gt;Image&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;Vegetable Basket&lt;/td&gt;
      &lt;td&gt;This vegetable basket is the perfect gift for ...&lt;/td&gt;
      &lt;td&gt;$15.00&lt;/td&gt;
      &lt;td&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;Russian Nesting Dolls&lt;/td&gt;
      &lt;td&gt;Hand-painted by trained monkeys, these exquisi...&lt;/td&gt;
      &lt;td&gt;$10,000.52&lt;/td&gt;
      &lt;td&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;Fish Painting&lt;/td&gt;
      &lt;td&gt;If something seems fishy about this painting, ...&lt;/td&gt;
      &lt;td&gt;$10,005.00&lt;/td&gt;
      &lt;td&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;Dead Parrot&lt;/td&gt;
      &lt;td&gt;This is an ex-parrot! Or maybe he&#39;s only resting?&lt;/td&gt;
      &lt;td&gt;$0.50&lt;/td&gt;
      &lt;td&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;Mystery Box&lt;/td&gt;
      &lt;td&gt;If you love suprises, this mystery box is for ...&lt;/td&gt;
      &lt;td&gt;$1.50&lt;/td&gt;
      &lt;td&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;We have now seen how to scrape a simple but realistic webpage. Let&amp;rsquo;s proceed with a practical example.&lt;/p&gt;
&lt;h3 id=&#34;css-selectors&#34;&gt;CSS Selectors&lt;/h3&gt;
&lt;p&gt;One alternative way of doing exactly the same thing is to use &lt;code&gt;select&lt;/code&gt;. The &lt;code&gt;select&lt;/code&gt; function is very similar to &lt;code&gt;find_all&lt;/code&gt; but has a different syntax. In particular, to search an element with a certain &lt;code&gt;tag&lt;/code&gt; and &lt;code&gt; attribute&lt;/code&gt;, we have to pass the following input:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;soup.select(tag[attribute=&amp;quot;attribute_name&amp;quot;])
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Select the first element of the table whose id contains &amp;quot;List&amp;quot;
table = soup.select(&#39;table[id*=&amp;quot;List&amp;quot;]&#39;)[0]
table.find(&#39;td&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;td&amp;gt;
Vegetable Basket
&amp;lt;/td&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;forms-and-post-requests&#34;&gt;Forms and post requests&lt;/h3&gt;
&lt;p&gt;When you are scraping, you sometimes have to fill-in forms, either to log-in into an account, or to input the arguments for a search query. Often forms are dynamic objects, but not always. Sometimes we can fill in forms also using the &lt;code&gt;requests&lt;/code&gt; library. In whis section we see a simple example.&lt;/p&gt;
&lt;h4 id=&#34;shortcut&#34;&gt;Shortcut&lt;/h4&gt;
&lt;p&gt;Often we can bypass forms, if the form redirects us to another page whose URL contains the parameters of the form. These are &amp;ldquo;well-behaved&amp;rdquo; forms and are actually quite frequent.&lt;/p&gt;
&lt;p&gt;We can find a simple example at: &lt;a href=&#34;http://www.webscrapingfordatascience.com/basicform/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://www.webscrapingfordatascience.com/basicform/&lt;/a&gt;. This form takes as input a bunch of information and when we click on &amp;ldquo;&lt;em&gt;Submit my information&lt;/em&gt;&amp;rdquo;, we get exactly the same page but with a different URL that contains the information we have inserted.&lt;/p&gt;
&lt;p&gt;Suppose I insert the following information:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Your gender: &amp;ldquo;Male&amp;rdquo;&lt;/li&gt;
&lt;li&gt;Food you like: &amp;ldquo;Pizza!&amp;rdquo; and &amp;ldquo;Fries please&amp;rdquo;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We should get the following url: &lt;a href=&#34;http://www.webscrapingfordatascience.com/basicform/?name=&amp;amp;gender=M&amp;amp;pizza=like&amp;amp;fries=like&amp;amp;haircolor=black&amp;amp;comments=&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://www.webscrapingfordatascience.com/basicform/?name=&amp;gender=M&amp;pizza=like&amp;fries=like&amp;haircolor=black&amp;comments=&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;We can decompose the url in various components, separated by one &amp;ldquo;?&amp;rdquo; and multiple &amp;ldquo;&amp;amp;&amp;quot;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://www.webscrapingfordatascience.com/basicform/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://www.webscrapingfordatascience.com/basicform/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;name=&lt;/li&gt;
&lt;li&gt;gender=M&lt;/li&gt;
&lt;li&gt;pizza=like&lt;/li&gt;
&lt;li&gt;fries=like&lt;/li&gt;
&lt;li&gt;haircolor=black&lt;/li&gt;
&lt;li&gt;comments=&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We can clearly see a pattern: the first component is the cose of the url and the other components are the form options. The ones we didn&amp;rsquo;t fill have the form &lt;code&gt;option=&lt;/code&gt; while the ones we did fill are &lt;code&gt;option=value&lt;/code&gt;. Knowing the syntax of a particular form we could fill it ourselves.&lt;/p&gt;
&lt;p&gt;For example, we could remove the fries and change the hair color to &lt;em&gt;brown&lt;/em&gt;: &lt;a href=&#34;http://www.webscrapingfordatascience.com/basicform/?name=&amp;amp;gender=M&amp;amp;pizza=like&amp;amp;fries=&amp;amp;haircolor=brown&amp;amp;comments=&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://www.webscrapingfordatascience.com/basicform/?name=&amp;gender=M&amp;pizza=like&amp;fries=&amp;haircolor=brown&amp;comments=&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Moreover, most forms work even if you remove the empty options. For example, the url above is equivalent to:http://www.webscrapingfordatascience.com/basicform/?gender=M&amp;amp;pizza=like&amp;amp;haircolor=brown&amp;amp;comments=&lt;/p&gt;
&lt;p&gt;One way to scrape websites with such forms is to create a string with the url with all the empty options and fill them using string formatting functions.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Building form url
url_core = &#39;http://www.webscrapingfordatascience.com/basicform/?&#39;
url_options = &#39;name=%s&amp;amp;gender=%s&amp;amp;pizza=%s&amp;amp;fries=%s&amp;amp;haircolor=%s&amp;amp;comments=%s&#39;
options = (&#39;&#39;,&#39;M&#39;,&#39;like&#39;,&#39;&#39;,&#39;brown&#39;,&#39;&#39;)
url = url_core + url_options % options

print(url)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;http://www.webscrapingfordatascience.com/basicform/?name=&amp;amp;gender=M&amp;amp;pizza=like&amp;amp;fries=&amp;amp;haircolor=brown&amp;amp;comments=
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;An alternative way is to name the options. This alternative is more verbose but more precise and does not require you to provide always all the options, even if empty.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Alternative 1
url_core = &#39;http://www.webscrapingfordatascience.com/basicform/?&#39;
url_options = &#39;name={name}&amp;amp;gender={gender}&amp;amp;pizza={pizza}&amp;amp;fries={fries}&amp;amp;haircolor={haircolor}&amp;amp;comments={comments}&#39;
options = {
    &#39;name&#39;: &#39;&#39;,
    &#39;gender&#39;: &#39;M&#39;,
    &#39;pizza&#39;: &#39;like&#39;,
    &#39;fries&#39;: &#39;&#39;,
    &#39;haircolor&#39;: &#39;brown&#39;,
    &#39;comments&#39;: &#39;&#39;
    }
url = url_core + url_options.format(**options)

print(url)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;http://www.webscrapingfordatascience.com/basicform/?name=&amp;amp;gender=M&amp;amp;pizza=like&amp;amp;fries=&amp;amp;haircolor=brown&amp;amp;comments=
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Lastly, one could build the url on the go.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Alternative 2
url = &#39;http://www.webscrapingfordatascience.com/basicform/?&#39;
options = {
    &#39;gender&#39;: &#39;M&#39;,
    &#39;pizza&#39;: &#39;like&#39;,
    &#39;haircolor&#39;: &#39;brown&#39;,
    }
for key, value in options.items():
    url += key + &#39;=&#39; + value + &#39;&amp;amp;&#39;

print(url)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;http://www.webscrapingfordatascience.com/basicform/?gender=M&amp;amp;pizza=like&amp;amp;haircolor=brown&amp;amp;
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;post-forms&#34;&gt;Post forms&lt;/h4&gt;
&lt;p&gt;Sometimes however, forms do not provide nice URLs as output. This is particularly true for login forms. There is however still a method, for some of them, to deal with them.&lt;/p&gt;
&lt;p&gt;For this section, we will use the same form example as before: &lt;a href=&#34;http://www.webscrapingfordatascience.com/postform2/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://www.webscrapingfordatascience.com/postform2/&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;This looks like the same form but now when the user clicks on &amp;ldquo;&lt;em&gt;Submit my information&lt;/em&gt;&amp;rdquo;, we get a page with a summary of the information. The biggest difference however, is that the output URL is exactly the same. Hence, we cannot rely on the same URL-bulding strategy as before.&lt;/p&gt;
&lt;p&gt;If we inspect the page, we observe the following line at the very beginning&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;form method=&amp;quot;POST&amp;quot;&amp;gt;
[...]
&amp;lt;/form&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And inside there are various input fields:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;&amp;lt;input type=&amp;quot;text&amp;quot;&amp;gt;&lt;/code&gt; for name&lt;/li&gt;
&lt;li&gt;&lt;code&gt;&amp;lt;input type=&amp;quot;radio&amp;quot;&amp;gt;&lt;/code&gt; for gender&lt;/li&gt;
&lt;li&gt;&lt;code&gt;&amp;lt;input type=&amp;quot;checkbox&amp;quot;&amp;gt;&lt;/code&gt; for food&lt;/li&gt;
&lt;li&gt;&lt;code&gt;&amp;lt;select&amp;gt;...&amp;lt;/select&amp;gt;&lt;/code&gt; for the hair color&lt;/li&gt;
&lt;li&gt;&lt;code&gt;&amp;lt;textarea&amp;gt;...&amp;lt;/textarea&amp;gt;&lt;/code&gt; for comments&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;These are all fields with which we can interact using the &lt;code&gt;response&lt;/code&gt; package. The main difference is that we won&amp;rsquo;t use the &lt;code&gt;get&lt;/code&gt; method to get the response from the URL but we will use the &lt;code&gt;post&lt;/code&gt; method to post our form parameters and get a response.&lt;/p&gt;
&lt;p&gt;If we input the following options:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;gender&lt;/em&gt;: male&lt;/li&gt;
&lt;li&gt;&lt;em&gt;pizza&lt;/em&gt;: yes&lt;/li&gt;
&lt;li&gt;&lt;em&gt;hair color&lt;/em&gt;: brown hair&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;and we click &amp;ldquo;&lt;em&gt;Submit my information&lt;/em&gt;&amp;rdquo; we get to a page with the following text:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Thanks for submitting your information
Here&#39;s a dump of the form data that was submitted:

array(5) {
  [&amp;quot;name&amp;quot;]=&amp;gt;
  string(0) &amp;quot;&amp;quot;
  [&amp;quot;gender&amp;quot;]=&amp;gt;
  string(1) &amp;quot;M&amp;quot;
  [&amp;quot;pizza&amp;quot;]=&amp;gt;
  string(4) &amp;quot;like&amp;quot;
  [&amp;quot;haircolor&amp;quot;]=&amp;gt;
  string(5) &amp;quot;brown&amp;quot;
  [&amp;quot;comments&amp;quot;]=&amp;gt;
  string(0) &amp;quot;&amp;quot;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We will not try to get to the same page using the &lt;code&gt;requests&lt;/code&gt; package.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# URL
url = &#39;http://www.webscrapingfordatascience.com/postform2/&#39;

# Options
options = {
    &#39;gender&#39;: &#39;M&#39;,
    &#39;pizza&#39;: &#39;like&#39;,
    &#39;haircolor&#39;: &#39;brown&#39;,
    }

# Post request
response = requests.post(url, data=options)
print(response.text)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;html&amp;gt;
	&amp;lt;body&amp;gt;


&amp;lt;h2&amp;gt;Thanks for submitting your information&amp;lt;/h2&amp;gt;

&amp;lt;p&amp;gt;Here&#39;s a dump of the form data that was submitted:&amp;lt;/p&amp;gt;

&amp;lt;pre&amp;gt;array(3) {
  [&amp;quot;gender&amp;quot;]=&amp;gt;
  string(1) &amp;quot;M&amp;quot;
  [&amp;quot;pizza&amp;quot;]=&amp;gt;
  string(4) &amp;quot;like&amp;quot;
  [&amp;quot;haircolor&amp;quot;]=&amp;gt;
  string(5) &amp;quot;brown&amp;quot;
}
&amp;lt;/pre&amp;gt;


	&amp;lt;/body&amp;gt;
&amp;lt;/html&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We have obtained exactly what we wanted! However, sometimes, websites block direct &lt;code&gt;post&lt;/code&gt; requests.&lt;/p&gt;
&lt;p&gt;One simple example is: &lt;a href=&#34;http://www.webscrapingfordatascience.com/postform3/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://www.webscrapingfordatascience.com/postform3/&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Post request
url = &#39;http://www.webscrapingfordatascience.com/postform3/&#39;
response = requests.post(url, data=options)
print(response.text)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;html&amp;gt;
	&amp;lt;body&amp;gt;


Are you trying to submit information from somewhere else?

	&amp;lt;/body&amp;gt;
&amp;lt;/html&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What happened? If we inspect the page, we can see that there is a new line at the beginning:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;input type=&amp;quot;hidden&amp;quot; name=&amp;quot;protection&amp;quot; value=&amp;quot;2c17abf5d5b4e326bea802600ff88405&amp;quot;&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now the form contains one more value - &lt;em&gt;protection&lt;/em&gt; which is conventiently hidden. In order to bypass the protection, we need to provide the correct &lt;em&gt;protection&lt;/em&gt; value to the form.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Post request
url = &#39;http://www.webscrapingfordatascience.com/postform3/&#39;
response = requests.get(url)

# Get out the value for protection
soup = BeautifulSoup(response.text, &#39;lxml&#39;)
options[&#39;protection&#39;] = soup.find(&#39;input&#39;, attrs={&#39;name&#39;: &#39;protection&#39;}).get(&#39;value&#39;)

# Post request
response = requests.post(url, data=options)
print(response.text)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;html&amp;gt;
	&amp;lt;body&amp;gt;



&amp;lt;h2&amp;gt;Thanks for submitting your information&amp;lt;/h2&amp;gt;

&amp;lt;p&amp;gt;Here&#39;s a dump of the form data that was submitted:&amp;lt;/p&amp;gt;

&amp;lt;pre&amp;gt;array(4) {
  [&amp;quot;gender&amp;quot;]=&amp;gt;
  string(1) &amp;quot;M&amp;quot;
  [&amp;quot;pizza&amp;quot;]=&amp;gt;
  string(4) &amp;quot;like&amp;quot;
  [&amp;quot;haircolor&amp;quot;]=&amp;gt;
  string(5) &amp;quot;brown&amp;quot;
  [&amp;quot;protection&amp;quot;]=&amp;gt;
  string(32) &amp;quot;16c87fc858e4d9fcb8d9c920b699388d&amp;quot;
}
&amp;lt;/pre&amp;gt;



	&amp;lt;/body&amp;gt;
&amp;lt;/html&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Indeed, now the post request was successful.&lt;/p&gt;
&lt;h3 id=&#34;proxies&#34;&gt;Proxies&lt;/h3&gt;
&lt;p&gt;We have discussed at the beginning how to be more subtle while scraping, by changing headers. In this section we will explore one step forward in anonimity: proxies.&lt;/p&gt;
&lt;p&gt;When we send an HTTP request, first the request is sent to a proxy server. The important thing is that the destination web server will which is the origin proxy server. Therefore, when one destination web server sees too many requests coming from one machine, it will block the proxy server.&lt;/p&gt;
&lt;p&gt;How can we change proxy? There are many websites that offer proxies for money but there are also some that offer proxies for free. The problem with free proxies (but often also with premium ones) is that there are many users using the same proxy, hence they are&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;slow&lt;/li&gt;
&lt;li&gt;blocked fast by many websites&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Nevertheless, it might be still useful to know how to change proxies.&lt;/p&gt;
&lt;h4 id=&#34;get-proxy-list&#34;&gt;Get proxy list&lt;/h4&gt;
&lt;p&gt;One website where we can get some free proxies to use for scraping is &lt;a href=&#34;https://free-proxy-list.net/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://free-proxy-list.net/&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;If we open the page, we see that there is a long list of proxies, from different countries and with different characteristics. Importantly, we are mostly interested in &lt;em&gt;https&lt;/em&gt; proxies. We are now going to retrieve a list of them. Note that the proxy list of this website is updated quite often. However, free proxies usually &amp;ldquo;expire&amp;rdquo; even faster.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Retrieve proxy list
def get_proxies():
    response = requests.get(&#39;https://free-proxy-list.net/&#39;)
    soup = BeautifulSoup(response.text, &#39;lxml&#39;)
    table = soup.find(&#39;table&#39;, {&#39;class&#39;:&#39;table&#39;})
    proxies = []
    rows = table.find_all(&#39;tr&#39;)
    for row in rows:
        cols = row.find_all(&#39;td&#39;)
        if len(cols)&amp;gt;0:
            line = [col.text for col in cols]
            if line[6]==&#39;yes&#39;:
                proxies += [line[0]+&#39;:&#39;+line[1]]
    return proxies
            
len(get_proxies())
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;176
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We have found many proxies. How do we use them? We have to provide them as an argment to a &lt;code&gt;requests&lt;/code&gt; session.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Test proxies
url = &#39;https://www.google.com&#39;
proxies = get_proxies()

for proxy in proxies[:10]:
    try:
        response = session.get(url, proxies={&amp;quot;https&amp;quot;: proxy}, timeout=5)
        print(response)
    except Exception as e:
        print(type(e))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;class &#39;NameError&#39;&amp;gt;
&amp;lt;class &#39;NameError&#39;&amp;gt;
&amp;lt;class &#39;NameError&#39;&amp;gt;
&amp;lt;class &#39;NameError&#39;&amp;gt;
&amp;lt;class &#39;NameError&#39;&amp;gt;
&amp;lt;class &#39;NameError&#39;&amp;gt;
&amp;lt;class &#39;NameError&#39;&amp;gt;
&amp;lt;class &#39;NameError&#39;&amp;gt;
&amp;lt;class &#39;NameError&#39;&amp;gt;
&amp;lt;class &#39;NameError&#39;&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Yes, most proxies were extremely slow (and consider we are opening &lt;em&gt;Google&lt;/em&gt;&amp;hellip;) and we got a &lt;code&gt;ConnetTimeout&lt;/code&gt; error. Other proxies worked and for one or two of the others we might have got  a &lt;code&gt;ProxyError&lt;/code&gt;.&lt;/p&gt;
&lt;h2 id=&#34;dynamic-webscraping&#34;&gt;Dynamic Webscraping&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s try to scrape the quotes from this link: &lt;a href=&#34;http://www.webscrapingfordatascience.com/simplejavascript/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://www.webscrapingfordatascience.com/simplejavascript/&lt;/a&gt;. It seems like a straightforward job.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Scrape javascript page
url = &#39;http://www.webscrapingfordatascience.com/simplejavascript/&#39;
response = requests.get(url)
print(response.text)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;html&amp;gt;

&amp;lt;head&amp;gt;
	&amp;lt;script src=&amp;quot;https://code.jquery.com/jquery-3.2.1.min.js&amp;quot;&amp;gt;&amp;lt;/script&amp;gt;
	&amp;lt;script&amp;gt;
	$(function() {
	document.cookie = &amp;quot;jsenabled=1&amp;quot;;
	$.getJSON(&amp;quot;quotes.php&amp;quot;, function(data) {
		var items = [];
		$.each(data, function(key, val) {
			items.push(&amp;quot;&amp;lt;li id=&#39;&amp;quot; + key + &amp;quot;&#39;&amp;gt;&amp;quot; + val + &amp;quot;&amp;lt;/li&amp;gt;&amp;quot;);
		});
		$(&amp;quot;&amp;lt;ul/&amp;gt;&amp;quot;, {
			html: items.join(&amp;quot;&amp;quot;)
			}).appendTo(&amp;quot;body&amp;quot;);
		});
	});
	&amp;lt;/script&amp;gt;
&amp;lt;/head&amp;gt;

&amp;lt;body&amp;gt;

&amp;lt;h1&amp;gt;Here are some quotes&amp;lt;/h1&amp;gt;

&amp;lt;/body&amp;gt;

&amp;lt;/html&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Weird. Our response does not contain the quotes on the page, even though they are clearly visible when we open it in our browser.&lt;/p&gt;
&lt;h3 id=&#34;selenium&#34;&gt;Selenium&lt;/h3&gt;
&lt;p&gt;Selenium is a python library that emulates a browser and lets us see pages exactly as with a normal browser. This is the most user-friendly way to do web scraping, however it has a huge cost: speed. This is by far the slowest way to do web scraping.&lt;/p&gt;
&lt;p&gt;After installing &lt;code&gt;selenium&lt;/code&gt;, we need to download a browser to simulate. We will use Google&amp;rsquo;s chromedriver. You can download it from here: &lt;a href=&#34;https://sites.google.com/a/chromium.org/chromedriver/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://sites.google.com/a/chromium.org/chromedriver/&lt;/a&gt;. Make sure to select &amp;ldquo;&lt;strong&gt;latest stable release&lt;/strong&gt;&amp;rdquo; and not &amp;ldquo;latest beta release&amp;rdquo;.&lt;/p&gt;
&lt;p&gt;Move the downloaded &lt;code&gt;chromedriver&lt;/code&gt; in the current directory (&amp;quot;&lt;em&gt;/11-python-webscraping&lt;/em&gt;&amp;rdquo; for me). We will now try open the url above with selenium and see if we can scrape the quotes in it.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Set your chromedriver name
chromedriver_name = &#39;/chromedriver_mac&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Open url
path = os.getcwd()
print(path)
driver = webdriver.Chrome(path+chromedriver_name)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;/Users/mcourt/Dropbox/Projects/Data-Science-Python/notebooks


/var/folders/xy/szm3b7211cj16ldwcywbvfzdqv7yhd/T/ipykernel_92159/2846782857.py:4: DeprecationWarning: executable_path has been deprecated, please pass in a Service object
  driver = webdriver.Chrome(path+chromedriver_name)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Awesome! Now, if everything went smooth, you should have a new Chrome window with a banner that says &amp;ldquo;&lt;em&gt;Chrome is being controlled by automated test software&lt;/em&gt;&amp;rdquo;. We can now open the web page and check that the list appears.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Open url
url = &#39;http://www.webscrapingfordatascience.com/simplejavascript/&#39;
driver.get(url)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Again, if averything went well, we are now abl to see our page with all the quotes in it. How do we scrape them?&lt;/p&gt;
&lt;p&gt;If we inspect the elements of the list with the right-click &lt;code&gt;inspect&lt;/code&gt; option, we should see something like:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;html&amp;gt;&amp;lt;head&amp;gt;
	&amp;lt;script src=&amp;quot;https://code.jquery.com/jquery-3.2.1.min.js&amp;quot;&amp;gt;&amp;lt;/script&amp;gt;
	&amp;lt;script&amp;gt;
	$(function() {
	document.cookie = &amp;quot;jsenabled=1&amp;quot;;
	$.getJSON(&amp;quot;quotes.php&amp;quot;, function(data) {
		var items = [];
		$.each(data, function(key, val) {
			items.push(&amp;quot;&amp;lt;li id=&#39;&amp;quot; + key + &amp;quot;&#39;&amp;gt;&amp;quot; + val + &amp;quot;&amp;lt;/li&amp;gt;&amp;quot;);
		});
		$(&amp;quot;&amp;lt;ul/&amp;gt;&amp;quot;, {
			html: items.join(&amp;quot;&amp;quot;)
			}).appendTo(&amp;quot;body&amp;quot;);
		});
	});
	&amp;lt;/script&amp;gt;
&amp;lt;/head&amp;gt;

&amp;lt;body&amp;gt;

&amp;lt;h1&amp;gt;Here are some quotes&amp;lt;/h1&amp;gt;




&amp;lt;ul&amp;gt;&amp;lt;li id=&amp;quot;0&amp;quot;&amp;gt;Every strike brings me closer to the next home run. –Babe Ruth&amp;lt;/li&amp;gt;&amp;lt;li id=&amp;quot;1&amp;quot;&amp;gt;The two most important days in your life are the day you are born and the day you find out why. –Mark Twain&amp;lt;/li&amp;gt;&amp;lt;li id=&amp;quot;2&amp;quot;&amp;gt;Whatever you can do, or dream you can, begin it.  Boldness has genius, power and magic in it. –Johann Wolfgang von Goethe&amp;lt;/li&amp;gt;&amp;lt;/ul&amp;gt;&amp;lt;/body&amp;gt;&amp;lt;/html&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we can see the content! Can we actually retrieve it? Let&amp;rsquo;s try.&lt;/p&gt;
&lt;p&gt;The most common selenium functions to get elements of a page, have a very intuitive syntax and are:
find_element_by_id&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;find_element_by_name&lt;/li&gt;
&lt;li&gt;find_element_by_xpath&lt;/li&gt;
&lt;li&gt;find_element_by_link_text&lt;/li&gt;
&lt;li&gt;find_element_by_partial_link_text&lt;/li&gt;
&lt;li&gt;find_element_by_tag_name&lt;/li&gt;
&lt;li&gt;find_element_by_class_name&lt;/li&gt;
&lt;li&gt;find_element_by_css_selector&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We will not try to recover all elements with tag &lt;code&gt;&amp;lt;li&amp;gt;&lt;/code&gt; (element of list &lt;code&gt;&amp;lt;ul&amp;gt;&lt;/code&gt;).&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Scrape content
quotes = [li.text for li in driver.find_elements_by_tag_name(&#39;li&#39;)]
quotes
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;/var/folders/xy/szm3b7211cj16ldwcywbvfzdqv7yhd/T/ipykernel_92159/157107938.py:2: DeprecationWarning: find_elements_by_* commands are deprecated. Please use find_elements() instead
  quotes = [li.text for li in driver.find_elements_by_tag_name(&#39;li&#39;)]





[]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Yes! It worked! But why?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Headless option
headless_option = webdriver.ChromeOptions()
headless_option.add_argument(&#39;--headless&#39;)

# Scraping
driver = webdriver.Chrome(path+chromedriver_name, options=headless_option)
driver.get(url)
quotes = [li.text for li in driver.find_elements_by_tag_name(&#39;li&#39;)]
quotes
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;/var/folders/xy/szm3b7211cj16ldwcywbvfzdqv7yhd/T/ipykernel_92159/2173692453.py:6: DeprecationWarning: executable_path has been deprecated, please pass in a Service object
  driver = webdriver.Chrome(path+chromedriver_name, options=headless_option)
/var/folders/xy/szm3b7211cj16ldwcywbvfzdqv7yhd/T/ipykernel_92159/2173692453.py:8: DeprecationWarning: find_elements_by_* commands are deprecated. Please use find_elements() instead
  quotes = [li.text for li in driver.find_elements_by_tag_name(&#39;li&#39;)]





[]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Mmm, it (probably) didn&amp;rsquo;t work. Why?&lt;/p&gt;
&lt;p&gt;The problem is that we are trying to retrieve the content of the page too fast. The page hasn&amp;rsquo;t loaded yet. This is a common issue with &lt;code&gt;selenium&lt;/code&gt;. Where are two ways to solve it:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;waiting&lt;/li&gt;
&lt;li&gt;waiting for the element to load&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The second way is the best way but we will first try the first and simpler one: we will just ask the browser to wait for 1 second before searching for &lt;code&gt;&amp;lt;li&amp;gt;&lt;/code&gt; tags&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Scraping
driver = webdriver.Chrome(path+chromedriver_name, options=headless_option)
driver.get(url)
time.sleep(1)
quotes = [li.text for li in driver.find_elements_by_tag_name(&#39;li&#39;)]
quotes
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;/var/folders/xy/szm3b7211cj16ldwcywbvfzdqv7yhd/T/ipykernel_92159/2964398594.py:2: DeprecationWarning: executable_path has been deprecated, please pass in a Service object
  driver = webdriver.Chrome(path+chromedriver_name, options=headless_option)
/var/folders/xy/szm3b7211cj16ldwcywbvfzdqv7yhd/T/ipykernel_92159/2964398594.py:5: DeprecationWarning: find_elements_by_* commands are deprecated. Please use find_elements() instead
  quotes = [li.text for li in driver.find_elements_by_tag_name(&#39;li&#39;)]





[&#39;The best time to plant a tree was 20 years ago. The second best time is now. –Chinese Proverb&#39;,
 &#39;The most common way people give up their power is by thinking they don’t have any. –Alice Walker&#39;,
 &#39;I am not a product of my circumstances. I am a product of my decisions. –Stephen Covey&#39;]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Nice! Now you should have obtained the list that we could not scrape with &lt;code&gt;requests&lt;/code&gt;. If it didn&amp;rsquo;t work, just increase the waiting time and it should work.&lt;/p&gt;
&lt;p&gt;We can now have a look at the &amp;ldquo;better&amp;rdquo; way to use a series of built-in functions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;WebDriverWait&lt;/code&gt;: the waiting function. We will call the &lt;code&gt;until&lt;/code&gt; method&lt;/li&gt;
&lt;li&gt;&lt;code&gt;expected_conditions&lt;/code&gt;: the condition function. We will call the &lt;code&gt;visibility_of_all_elements_located&lt;/code&gt; method&lt;/li&gt;
&lt;li&gt;&lt;code&gt;By&lt;/code&gt;: the selector function. Some of the options are:
&lt;ul&gt;
&lt;li&gt;By.ID&lt;/li&gt;
&lt;li&gt;By.XPATH&lt;/li&gt;
&lt;li&gt;By.NAME&lt;/li&gt;
&lt;li&gt;By.TAG_NAME&lt;/li&gt;
&lt;li&gt;By.CLASS_NAME&lt;/li&gt;
&lt;li&gt;By.CSS_SELECTOR&lt;/li&gt;
&lt;li&gt;By.LINK_TEXT&lt;/li&gt;
&lt;li&gt;By.PARTIAL_LINK_TEXT&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

# Scraping
driver = webdriver.Chrome(path+chromedriver_name, options=headless_option)
driver.get(url)
quotes = WebDriverWait(driver, 10).until(EC.visibility_of_all_elements_located((By.TAG_NAME, &#39;li&#39;)))
quotes = [quote.text for quote in quotes]
quotes
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;/var/folders/xy/szm3b7211cj16ldwcywbvfzdqv7yhd/T/ipykernel_92159/152412441.py:6: DeprecationWarning: executable_path has been deprecated, please pass in a Service object
  driver = webdriver.Chrome(path+chromedriver_name, options=headless_option)





[&#39;The most common way people give up their power is by thinking they don’t have any. –Alice Walker&#39;,
 &#39;The best time to plant a tree was 20 years ago. The second best time is now. –Chinese Proverb&#39;,
 &#39;An unexamined life is not worth living. –Socrates&#39;]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In this case, we have told the browser to wait until either all elements with tag &lt;code&gt;&amp;lt;li&amp;gt;&lt;/code&gt; are visible or 10 seconds have passed. After one condition is realized, the &lt;code&gt;WebDriverWait&lt;/code&gt; function also automatically retrieves all the elements which the &lt;code&gt;expected_condition&lt;/code&gt; function is conditioning on. There are many different conditions we can use. A list can be found here: &lt;a href=&#34;https://selenium-python.readthedocs.io/waits.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://selenium-python.readthedocs.io/waits.html&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;We can easily generalize the function above as follows.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Find element function
def find_elements(driver, function, identifier):
    element = WebDriverWait(driver, 10).until(EC.visibility_of_all_elements_located((function, identifier)))
    return element

quotes = [quote.text for quote in find_elements(driver, By.TAG_NAME, &#39;li&#39;)]
quotes
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[&#39;The most common way people give up their power is by thinking they don’t have any. –Alice Walker&#39;,
 &#39;The best time to plant a tree was 20 years ago. The second best time is now. –Chinese Proverb&#39;,
 &#39;An unexamined life is not worth living. –Socrates&#39;]
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Endogeneity</title>
      <link>https://matteocourthoud.github.io/course/metrics/06_endogeneity/</link>
      <pubDate>Fri, 29 Oct 2021 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/course/metrics/06_endogeneity/</guid>
      <description>&lt;h2 id=&#34;instrumental-variables&#34;&gt;Instrumental Variables&lt;/h2&gt;
&lt;h3 id=&#34;endogeneity&#34;&gt;Endogeneity&lt;/h3&gt;
&lt;p&gt;We say that there is &lt;strong&gt;endogeneity&lt;/strong&gt; in the linear regression model if
$\mathbb E[x_i \varepsilon_i] \neq 0$.&lt;/p&gt;
&lt;p&gt;The random vector $z_i$ is an &lt;strong&gt;instrumental variable&lt;/strong&gt; in the linear
regression model if the following conditions are met.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Exclusion restriction&lt;/strong&gt;: the instruments are uncorrelated with the
regression error $$
\mathbb E_n[z_i \varepsilon_i] = 0
$$ almost surely, i.e. with probability $p \to 1$.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Rank condition&lt;/strong&gt;: no linearly redundant instruments $$
\mathbb E_n[z_i z_i&#39;] \neq 0
$$ almost surely, i.e. with probability $p \to 1$.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Relevance condition&lt;/strong&gt; (need $L &amp;gt; K$): $$
rank \ (\mathbb E_n[z_i x_i&#39;]) = K
$$ almost surely, i.e. with probability $p \to 1$.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;iv-and-2sls&#34;&gt;IV and 2SLS&lt;/h3&gt;
&lt;p&gt;Let $K = dim(x_i)$ and $L = dim(z_i)$. We say that the model is
&lt;strong&gt;just-identified&lt;/strong&gt; if $L = K$ (method: IV) and &lt;strong&gt;over-identified&lt;/strong&gt; if
$L &amp;gt; K$ (method: 2SLS).&lt;/p&gt;
&lt;p&gt;Assume $z_i$ satisfies the instrumental variable assumptions above and
$dim(z_i) = dim(x_i)$, then the &lt;strong&gt;instrumental variables (IV)&lt;/strong&gt;
estimator $\hat{\beta} _ {IV}$ is given by $$
\begin{aligned}
\hat{\beta} _ {IV} &amp;amp;= \mathbb E_n[z_i x_i&#39;]^{-1} \mathbb E_n[z_i y_i] = \newline
&amp;amp;= \left( \frac{1}{n} \sum _ {i=1}^n z_i x_i\right)^{-1} \left( \frac{1}{n} \sum _ {i=1}^n z_i y_i\right) = \newline
&amp;amp;= (Z&amp;rsquo;X)^{-1} (Z&amp;rsquo;y)
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;Assume $z_i$ satisfies the instrumental variable assumptions above and
$dim(z_i) &amp;gt; dim(x_i)$, then the &lt;strong&gt;two-stage-least squares (2SLS)&lt;/strong&gt;
estimator $\hat{\beta} _ {2SLS}$ is given by $$
\hat{\beta} _ {2SLS} =  \Big( X&amp;rsquo;Z (Z&amp;rsquo;Z)^{-1} Z&amp;rsquo;X \Big)^{-1} \Big( X&amp;rsquo;Z (Z&amp;rsquo;Z)^{-1} Z&amp;rsquo;y \Big)
$$ Where $\hat{x}_i$ is the predicted $x_i$ from the &lt;strong&gt;first stage&lt;/strong&gt;
regression of $x_i$ on $z_i$. This is equivalent to the IV estimator
using $\hat{x}_i$ as an instrument for $x_i$.&lt;/p&gt;
&lt;h3 id=&#34;2sls-algebra&#34;&gt;2SLS Algebra&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;The estimator is called &lt;strong&gt;two-stage-least squares&lt;/strong&gt; since it can be
rewritten as an IV estimator that uses $\hat{X}$ as instrument: $$
\begin{aligned}
\hat{\beta} _ {\text{2SLS}} &amp;amp;= \Big( X&amp;rsquo;Z (Z&amp;rsquo;Z)^{-1} Z&amp;rsquo;X \Big)^{-1} \Big( X&amp;rsquo;Z (Z&amp;rsquo;Z)^{-1} Z&amp;rsquo;y \Big) = \newline
&amp;amp;= (\hat{X}&#39; X)^{-1} \hat{X}&#39; y = \newline
&amp;amp;= \mathbb E_n[\hat{x}_i x_i&#39;]^{-1} \mathbb E_n[\hat{x}_i y_i]
\end{aligned}
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Moreover it can be rewritten as $$
\begin{aligned}
\hat{\beta} _ {\text{2SLS}} &amp;amp;= (\hat{X}&#39; X)^{-1} \hat{X}&#39; y = \newline
&amp;amp;= (X&#39; P_Z X)^{-1} X&#39; P_Z y = \newline
&amp;amp;= (X&#39; P_Z P_Z X)^{-1} X&#39; P_Z y = \newline
&amp;amp;= (\hat{X}&#39; \hat{X})^{-1} \hat{X}&#39; y = \newline
&amp;amp;= \mathbb E_n [\hat{x}_i \hat{x}_i]^{-1} \mathbb E_n[\hat{x}_i y_i]
\end{aligned}
$$&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;rule-of-thumb&#34;&gt;Rule of Thumb&lt;/h3&gt;
&lt;p&gt;How to the test the relevance condition? Rule of thumb: $F$-test in the
first stage $&amp;gt;10$ (joint test on $z_i$).&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Problem&lt;/strong&gt;: as $n \to \infty$, with finite $L$, $F \to \infty$ (bad
rule of thumb).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;equivalence&#34;&gt;Equivalence&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;If $K=L$, $\hat{\beta} _ {\text{2SLS}} = \hat{\beta} _ {\text{IV}}$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;If $K=L$, $X&amp;rsquo;Z$ and $Z&amp;rsquo;X$ are squared matrices and, by the relevance
condition, non-singular (invertible). $$
\begin{aligned}
\hat{\beta} _ {\text{2SLS}} &amp;amp;= \Big( X&amp;rsquo;Z (Z&amp;rsquo;Z)^{-1} Z&amp;rsquo;X \Big)^{-1} \Big( X&amp;rsquo;Z (Z&amp;rsquo;Z)^{-1} Z&amp;rsquo;y \Big) = \newline
&amp;amp;= (Z&amp;rsquo;X)^{-1} (Z&amp;rsquo;Z) (X&amp;rsquo;Z)^{-1} X&amp;rsquo;Z (Z&amp;rsquo;Z)^{-1} Z&amp;rsquo;y = \newline
&amp;amp;= (Z&amp;rsquo;X)^{-1} (Z&amp;rsquo;Z) (Z&amp;rsquo;Z)^{-1} Z&amp;rsquo;y = \newline
&amp;amp;= (Z&amp;rsquo;X)^{-1} (Z&amp;rsquo;y) = \newline
&amp;amp;= \hat{\beta} _ {\text{IV}}
\end{aligned}
$$ $$\tag*{$\blacksquare$}$$&lt;/p&gt;
&lt;h3 id=&#34;demand-example&#34;&gt;Demand Example&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Example&lt;/strong&gt; from Hayiashi (2000) page 187: demand and supply
simultaneous equations. $$
\begin{aligned}
&amp;amp; q_i^D(p_i) = \alpha_0 + \alpha_1 p_i + u_i \newline
&amp;amp; q_i^S(p_i) = \beta_0 + \beta_1 p_i + v_i
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;We have an endogeneity problem. To see why, we solve the system of
equations for $(p_i, q_i)$: $$
\begin{aligned}
&amp;amp; p_i = \frac{\beta_0 - \alpha_0}{\alpha_1 - \beta_1} + \frac{v_i - u_i}{\alpha_1 - \beta_1 } \newline
&amp;amp; q_i = \frac{\alpha_1\beta_0 - \alpha_0 \beta_1}{\alpha_1 - \beta_1} + \frac{\alpha_1 v_i - \beta_1 u_i}{\alpha_1 - \beta_1 }
\end{aligned}
$$&lt;/p&gt;
&lt;h3 id=&#34;demand-example-2&#34;&gt;Demand Example (2)&lt;/h3&gt;
&lt;p&gt;Then the price variable is not independent from the error term in
neither equation: $$
\begin{aligned}
&amp;amp; Cov(p_i, u_i) = - \frac{Var(u_i)}{\alpha_1 - \beta_1 } \newline
&amp;amp; Cov(p_i, v_i) = \frac{Var(v_i)}{\alpha_1 - \beta_1 }
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;As a consequence, the OLS estimators are not consistent: $$
\begin{aligned}
&amp;amp; \hat{\alpha} _ {1, OLS} \overset{p}{\to} \alpha_1 + \frac{Cov(p_i, u_i)}{Var(p_i)} \newline
&amp;amp; \hat{\beta} _ {1, OLS} \overset{p}{\to} \beta_1 + \frac{Cov(p_i, v_i)}{Var(p_i)}
\end{aligned}
$$&lt;/p&gt;
&lt;h3 id=&#34;demand-example-3&#34;&gt;Demand Example (3)&lt;/h3&gt;
&lt;p&gt;In general, running regressing $q$ on $p$ you estimate $$
\begin{aligned}
\hat{\gamma} _ {OLS} &amp;amp;\overset{p}{\to} \frac{Cov(p_i, q_i)}{Var(p_i)} = \newline
&amp;amp;= \frac{\alpha_1 Var(v_i) + \beta_1 Var(u_i)}{(\alpha_1 - \beta_1)^2} \left( \frac{Var(v_i) + Var(u_i)}{(\alpha_1 - \beta_1)^2} \right)^{-1} = \newline
&amp;amp;= \frac{\alpha_1 Var(v_i) + \beta_1 Var(u_i)}{Var(v_i) + Var(u_i)}
\end{aligned}
$$ Which is neither $\alpha_1$ nor $\beta_1$ but a variance weighted
average of the two.&lt;/p&gt;
&lt;h3 id=&#34;demand-example-4&#34;&gt;Demand Example (4)&lt;/h3&gt;
&lt;p&gt;Suppose we have a supply shifter $z_i$ such that&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\mathbb E[z_i v_i] \neq 0$&lt;/li&gt;
&lt;li&gt;$\mathbb E[z_i u_i] = 0$.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We combine the second condition and $\mathbb E[u_i] = 0$ to get a system
of 2 equations in 2 unknowns: $\alpha_0$ and $\alpha_1$. $$
\begin{aligned}
&amp;amp; \mathbb E[z_i u_i] = \mathbb E[ z_i (q_i^D(p_i) - \alpha_0 - \alpha_1 p_i) ] = 0 \newline
&amp;amp; \mathbb E[u_i] = \mathbb E[q_i^D(p_i) - \alpha_0 - \alpha_1 p_i] = 0&lt;br&gt;
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;We could try to solve for the vector $\alpha$ that solves $$
\begin{aligned}
&amp;amp; \mathbb E_n[z_i (q_i^D - x_i\alpha)] = 0 \newline
&amp;amp; \mathbb E_n[z_i q_i^D] -  \mathbb E_n[z_ix_i\alpha] = 0
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;If $\mathbb E_n[z_ix_i]$ is invertible, we get
$\hat{\alpha} = \mathbb E_n[z_ix_i]^{-1} \mathbb E_n[z_i q^D_i]$ which
is indeed the IV estimator of $\alpha$ using $z_i$ as an instrument for
the endogenous variable $p_i$.&lt;/p&gt;
&lt;h3 id=&#34;code---dgp&#34;&gt;Code - DGP&lt;/h3&gt;
&lt;p&gt;This code draws 100 observations from the model
$y = 2 x_1 - x_2 + \varepsilon$ where $x_1, x_2 \sim U[0,1]$ and
$\varepsilon \sim N(0,1)$.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# Set seed
Random.seed!(123);

# Set the number of observations
n = 100;

# Set the dimension of Z
l = 3;

# Draw instruments
Z = rand(Uniform(0,1), n, l);

# Correlation matrix for error terms
S = [1 0.8; 0.8 1];

# Endogenous X
γ = [2 0; 0 -1; -1 3];
ε = rand(Normal(0,1), n, 2) * cholesky(S).U;
X = Z*γ .+ ε[:,1];

# Calculate y
y = X*β .+ ε[:,2];
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;code---iv&#34;&gt;Code - IV&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# Estimate beta OLS
β_OLS = (X&#39;*X)\(X&#39;*y)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 2-element Array{Float64,1}:
##   2.335699233358403
##  -0.8576266209987325
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# IV: l=k=2 instruments
Z_IV = Z[:,1:k];
β_IV = (Z_IV&#39;*X)\(Z_IV&#39;*y)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 2-element Array{Float64,1}:
##   1.6133344277861439
##  -0.6678537395714547
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# Calculate standard errors
ε_hat = y - X*β_IV;
V_NHC_IV = var(ε_hat) * inv(Z_IV&#39;*X)*Z_IV&#39;*Z_IV*inv(Z_IV&#39;*X);
V_HC0_IV = inv(Z_IV&#39;*X)*Z_IV&#39; * (I(n) .* ε_hat.^2) * Z_IV*inv(Z_IV&#39;*X);
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;code---2sls&#34;&gt;Code - 2SLS&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# 2SLS: l=3 instruments
Pz = Z*inv(Z&#39;*Z)*Z&#39;;
β_2SLS = (X&#39;*Pz*X)\(X&#39;*Pz*y)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 2-element Array{Float64,1}:
##   1.904553638377971
##  -0.8810907510370429
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# Calculate standard errors
ε_hat = y - X*β_2SLS;
V_NCH_2SLS = var(ε_hat) * inv(X&#39;*Pz*X);
V_HC0_2SLS = inv(X&#39;*Pz*X)*X&#39;*Pz * (I(n) .* ε_hat.^2) *Pz*X*inv(X&#39;*Pz*X);
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;gmm&#34;&gt;GMM&lt;/h2&gt;
&lt;h3 id=&#34;setting&#34;&gt;Setting&lt;/h3&gt;
&lt;p&gt;We have a system of $L$ moment conditions $$
\begin{aligned}
&amp;amp; \mathbb E[g_1(\omega_i, \delta_0)] = 0 \newline
&amp;amp; \vdots \newline
&amp;amp; \mathbb E[g_L(\omega_i, \delta_0)] = 0
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;If $L = \dim (\delta_0)$, no problem. If $L &amp;gt; \dim (\delta_0)$, there
may be no solution to the system of equations.&lt;/p&gt;
&lt;h3 id=&#34;options&#34;&gt;Options&lt;/h3&gt;
&lt;p&gt;There are two possibilities.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;First Solution&lt;/strong&gt;: add moment conditions until the system is
identified $$
\mathbb E[ a&#39; g(\omega_i, \delta_0)] = 0
$$ Solve $\mathbb E[Ag(\omega_i, \delta)] = 0$ for $\hat{\delta}$.
How to choose $A$? Such that it minimizes $Var(\hat{\delta})$.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Second Solution&lt;/strong&gt;: generalized method of moments (GMM) $$
\begin{aligned}
\hat{\delta} _ {GMM} &amp;amp;= \arg \min _ \delta \quad  \Big| \Big| \mathbb E_n [ g(\omega_i, \delta) ] \Big| \Big| = \newline
&amp;amp;= \arg \min _ \delta \quad n \mathbb E_n[g(\omega_i, \delta)]&#39; W \mathbb E_n [g(\omega_i, \delta)]
\end{aligned}
$$&lt;/li&gt;
&lt;/ol&gt;
&lt;blockquote&gt;
&lt;p&gt;The choice of $A$ and $W$ are closely related to each other.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;1-step-gmm&#34;&gt;1-step GMM&lt;/h3&gt;
&lt;p&gt;Since $J(\delta,W)$ is a quadratic form, a closed form solution exists:
$$
\hat{\delta}(W) = \Big(\mathbb E_n[z_i x_i&#39;] W \mathbb E_n[z_i x_i&#39;] \Big)^{-1}\mathbb E_n[z_i x_i&#39;] W \mathbb E_n[z_i y_i]
$$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Assumptions&lt;/strong&gt; for consistency of the GMM estimator given data
$\mathcal D = \lbrace y_i, x_i, z_i \rbrace _ {i=1}^n$:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Linearity&lt;/strong&gt;: $y_i = x_i\gamma_0 + \varepsilon_i$&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;IID&lt;/strong&gt;: $(y_i, x_i, z_i)$ iid&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Orthogonality&lt;/strong&gt;:
$\mathbb E [z_i(y_i - x_i\gamma_0)] = \mathbb E[z_i \varepsilon_i] = 0$&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Rank identification&lt;/strong&gt;: $\Sigma_{xz} = \mathbb E[z_i x_i&#39;]$ has
full rank&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;convergence&#34;&gt;Convergence&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Under linearity, independence, orthogonality and rank conditions, if
$\hat{W} \overset{p}{\to} W$ positive definite, then $$
\hat{\delta}(\hat{W}) \to \delta(W)
$$ If in addition to the above assumption,
$\sqrt{n} \mathbb E_n [g(\omega_i, \delta_0)] \overset{d}{\to} N(0,S)$
for a fixed positive definite $S$, then $$
\sqrt{n} (\hat{\delta} (\hat{W}) - \delta(W)) \overset{d}{\to} N(0,V)
$$ where
$V = (\Sigma&#39; _ {xz} W \Sigma _ {xz})^{-1} \Sigma _ {xz} W S W \Sigma _ {xz}(\Sigma&#39; _ {xz} W \Sigma _ {xz})^{-1}$.&lt;/p&gt;
&lt;p&gt;Finally, if a consistent estimator $\hat{S}$ of $S$ is available, then
using sample analogues $\hat{\Sigma}_{xz}$ it follows that $$
\hat{V} \overset{p}{\to} V
$$&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;If $W = S^{-1}$ then $V$ reduces to
$V = (\Sigma&#39; _ {xz} W \Sigma _ {xz})^{-1}$. Moreover,
$(\Sigma&#39; _ {xz} W \Sigma _ {xz})^{-1}$ is the smallest possible form
of $V$, in a positive definite sense.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Therefore, to have an efficient estimator, you want to construct
$\hat{W}$ such that $\hat{W} \overset{p}{\to} S^{-1}$.&lt;/p&gt;
&lt;h3 id=&#34;2-step-gmm&#34;&gt;2-step GMM&lt;/h3&gt;
&lt;p&gt;Estimation steps:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Choose an arbitrary weighting matrix $\hat{W}_{init}$ (usually the
identity matrix $I_K$)&lt;/li&gt;
&lt;li&gt;Estimate $\hat{\delta} _ {init}(\hat{W} _ {init})$&lt;/li&gt;
&lt;li&gt;Estimate $\hat{S}$ (asymptotic variance of the moment condition)&lt;/li&gt;
&lt;li&gt;Estimate $\hat{\delta}(\hat{S}^{-1})$&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;On the procedure:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;This estimator achieves the semiparametric efficiency bound.&lt;/li&gt;
&lt;li&gt;This strategy works only if $\hat{S} \overset{p}{\to} S$ exists.&lt;/li&gt;
&lt;li&gt;For iid cases: we can use
$\hat{\delta} = \mathbb E_n[(\hat{\varepsilon}_i z_i)(\hat{\varepsilon}_i z_i) &#39; ]$
where
$\hat{\varepsilon}_i = y_i - x_i \hat{\delta}(\hat{W} _ {init})$.&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;code---1-step-gmm&#34;&gt;Code - 1-step GMM&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# GMM 1-step: inefficient weighting matrix
W_1 = I(l);

# Objective function
gmm_1(b) = ( y - X*b )&#39; * Z * W_1 *  Z&#39; * ( y - X*b );

# Estimate GMM
β_gmm_1 = optimize(gmm_1, β_OLS).minimizer
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 2-element Array{Float64,1}:
##   1.91556882526808
##  -0.8769689391885799
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# Standard errors GMM
ε_hat = y - X*β_gmm_1;
S_hat = Z&#39; * (I(n) .* ε_hat.^2) * Z;
d_hat = -X&#39;*Z;
V_gmm_1 = inv(d_hat * inv(S_hat) * d_hat&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 2×2 Array{Float64,2}:
##   0.0158497   -0.00346601
##  -0.00346601   0.00616531
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;code---2-step-gmm&#34;&gt;Code - 2-step GMM&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# GMM 2-step: efficient weighting matrix
W_2 = inv(S_hat);

# Objective function
gmm_2(b) = ( y - X*b )&#39; * Z * W_2 *  Z&#39; * ( y - X*b );

# Estimate GMM
β_gmm_2 = optimize(gmm_2, β_OLS).minimizer
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 2-element Array{Float64,1}:
##   1.905326742963115
##  -0.881808949213345
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# Standard errors GMM
ε_hat = y - X*β_gmm_2;
S_hat = Z&#39; * (I(n) .* ε_hat.^2) * Z;
d_hat = -X&#39;*Z;
V_gmm_2 = inv(d_hat * inv(S_hat) * d_hat&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 2×2 Array{Float64,2}:
##   0.0162603   -0.00357632
##  -0.00357632   0.00631259
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;testing-overidentifying-restrictions&#34;&gt;Testing Overidentifying Restrictions&lt;/h3&gt;
&lt;p&gt;If the equations are &lt;strong&gt;exactly identified&lt;/strong&gt;, then it is possible to
choose $\delta$ so that all the elements of the sample moments
$\mathbb E_n[g(\omega_i; \delta)]$ are zero and thus that the distance
$$
J(\delta, \hat{W}) = n \mathbb E_n[g(\omega_i, \delta)]&#39; \hat{W} \mathbb E_n[g(\omega_i, \delta)]
$$ is zero. (The $\delta$ that does it is the IV estimator.)&lt;/p&gt;
&lt;p&gt;If the equations are &lt;strong&gt;overidentified&lt;/strong&gt;, i.e. $L$ (number of
instruments) $&amp;gt; K$ (number of equations), then the distance cannot be
zero exactly in general, but we would expect the minimized distance to
be &lt;em&gt;close&lt;/em&gt; to zero.&lt;/p&gt;
&lt;h3 id=&#34;naive-test&#34;&gt;Naive Test&lt;/h3&gt;
&lt;p&gt;Suppose your model is overidentified ($L &amp;gt; K$) and you use the following
naive testing procedure:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Estimate $\hat{\delta}$ using a subset of dimension $K$ of
instruments $\lbrace z_1 , .. , z_K\rbrace$ for
$\lbrace x_1 , &amp;hellip; , x_K\rbrace$&lt;/li&gt;
&lt;li&gt;Set $\hat{\varepsilon}_i = y_i - x_i \hat{\delta} _ {\text{GMM}}$&lt;/li&gt;
&lt;li&gt;Infer the size of the remaining $L-K$ moment conditions
$\mathbb E[z _{i, K+1} \varepsilon_i], &amp;hellip;, \mathbb E[z _{i, L} \varepsilon_i]$
looking at their empirical counterparts
$\mathbb E_n[z _{i, K+1} \hat{\varepsilon}_i], &amp;hellip;, \mathbb E_n[z _{i, L} \hat{\varepsilon}_i]$&lt;/li&gt;
&lt;li&gt;Reject exogeneity if the empirical expectations are high. How high?
Calculate p-values.&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;example&#34;&gt;Example&lt;/h3&gt;
&lt;p&gt;If you have two invalid instruments and you use one to test the validity
of the other, it might happen by chance that you don’t reject it.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Model: $y_i = x_i + \varepsilon_i$ and
$x_i = \frac{1}{2} z _{i1} - \frac{1}{2} z _{i2} + u_i$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Have $$
Cov (z _{i1}, z _{i2}, \varepsilon_i, u_i) =
\begin{bmatrix}
1 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 \newline 0 &amp;amp; 1 &amp;amp; 0 &amp;amp; 0 \newline 0 &amp;amp; 0 &amp;amp; 1 &amp;amp; 0.5 \newline 0 &amp;amp; 0 &amp;amp; 0.5 &amp;amp; 1
\end{bmatrix}
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;You want to test whether the second instrument is valid (is not
since $\mathbb E[z_2 \varepsilon] \neq 0$). You use $z_1$ and
estimate $\hat{\beta} \to$ the estimator is consistent.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;You obtain $\mathbb E_n[z _{i2} \hat{\varepsilon}_i] \simeq 0$ even
if $z_2$ is invalid&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Problem: you are using an invalid instrument in the first place.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;hansens-test&#34;&gt;Hansen’s Test&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt;: We are interested in testing
$H_0: \mathbb E[z_i \varepsilon_i] = 0$ against
$H_1: \mathbb E[z_i \varepsilon_i] \neq 0$. Suppose
$\hat{S} \overset{p}{\to} S$. Then $$
J(\hat{\delta}(\hat{S}^{-1}) , \hat{S}^{-1}) \overset{d}{\to} \chi^2 _ {L-K}
$$ For $c$ satisfying $\alpha = 1- G_{L - K} ( c )$,
$\Pr(J&amp;gt;c | H_0) \to \alpha$ so the test &lt;em&gt;reject $H_0$ if $J &amp;gt; c$&lt;/em&gt; has
asymptotic size $\alpha$.&lt;/p&gt;
&lt;h3 id=&#34;comments&#34;&gt;Comments&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;The degrees of freedom of the asymptotic distribution are the number
of overidentifying restrictions.&lt;/li&gt;
&lt;li&gt;This is a specification test, testing whether all model assumptions
are true jointly. Only when we are confident that about the other
assumptions, can we interpret a large $J$ statistic as evidence for
the endogeneity of some of the $L$ instruments included in $x$.&lt;/li&gt;
&lt;li&gt;Unlike the tests we have encountered so far, the test is not
consistent against some failures of the orthogonality conditions
(that is, it is not consistent against some fixed elements of the
alternative).&lt;/li&gt;
&lt;li&gt;Several papers in the July 1996 issue of JBES report that the
finite-sample null rejection probability of the test can far exceed
the nominal significance level $\alpha$.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;special-case-conditional-homoskedasticity&#34;&gt;Special Case: Conditional Homoskedasticity&lt;/h3&gt;
&lt;p&gt;The main implication of conditional homoskedasticity is that efficient
GMM becomes 2SLS. With efficient GMM estimation, the weighting matrix is
$\hat{S}^{-1} = \mathbb En [z_i z_i&#39; \varepsilon_i^2]^{-1}$. With
conditional homoskedasticity, the efficient weighting matrix is
$\mathbb E_n[z_iz_i&#39;]^{-1} \sigma^{-2}$, or equivalently
$\mathbb E_n[z_iz_i&#39;]^{-1}$. Then, the GMM estimator becomes $$
\hat{\delta}(\hat{S}^{-1}) = \Big(\mathbb E_n[z_i x_i&#39;]&#39; \underbrace{\mathbb E_n[z_iz_i&#39;]^{-1} \mathbb E[z_i x_i&#39;]} _ {\text{ols of } x_i \text{ on }z_i} \Big)^{-1}\mathbb E_n[z_i x_i&#39;]&#39; \underbrace{\mathbb E_n[z_iz_i&#39;]^{-1} \mathbb E[z_i y_i&#39;]} _ {\text{ols of } y_i \text{ on }z_i}= \hat{\delta} _ {2SLS}
$$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;: Consider the matrix notation. $$
\begin{aligned}
\hat{\delta} \left( \frac{Z&amp;rsquo;Z}{n}\right) &amp;amp;= \left( \frac{X&amp;rsquo;Z}{n} \left( \frac{Z&amp;rsquo;Z}{n}\right)^{-1} \frac{Z&amp;rsquo;X}{n} \right)^{-1} \frac{X&amp;rsquo;Z}{n} \left( \frac{Z&amp;rsquo;Z}{n}\right)^{-1} \frac{Z&amp;rsquo;Y}{n} = \newline
&amp;amp;= \left( X&amp;rsquo;Z(Z&amp;rsquo;Z)^{-1} Z&amp;rsquo;X \right)^{-1} X&amp;rsquo;Z(Z&amp;rsquo;Z)^{-1} Z&amp;rsquo;Y = \newline
&amp;amp;= \left(X&amp;rsquo;P_ZX\right)^{-1} X&amp;rsquo;P_ZY = \newline
&amp;amp;= \left(X&amp;rsquo;P_ZP_ZX\right)^{-1} X&amp;rsquo;P_ZY = \newline
&amp;amp;= \left(\hat{X}&#39;_Z \hat{X}_Z\right)^{-1} \hat{X}&#39;_ZY = \newline
&amp;amp;= \hat{\delta} _ {2SLS}
\end{aligned}
$$ $$\tag*{$\blacksquare$}$$&lt;/p&gt;
&lt;h3 id=&#34;small-sample-properties-of-2sls&#34;&gt;Small-Sample Properties of 2SLS&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt;: When the number of instruments is equal to the sample size
($L = n$), then $\hat{\delta} _ {2SLS} = \hat{\delta} _ {OLS}$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;: We have a perfect prediction problem. The first stage
estimated coefficient $\hat{\gamma}$ is such that it solves the normal
equations: $\hat{\gamma} = z_i^{-1} x_i$. Then $$
\begin{aligned}
\hat{\delta} _ {2SLS} &amp;amp;= \mathbb E_n[\hat{x}_i x&#39;_i]^{-1} \mathbb E_n[\hat{x}_i y_i] = \newline
&amp;amp;= \mathbb E_n[z_i z_i^{-1} x_i x&#39;_i]^{-1} \mathbb E_n[z_i z_i^{-1} x_i y_i] = \newline
&amp;amp;= \mathbb E_n[x_i x&#39;_i]^{-1} \mathbb E_n[x_i y_i] = \newline
&amp;amp;= \hat{\delta} _ {OLS}
\end{aligned}
$$ $$\tag*{$\blacksquare$}$$&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;You have this overfitting problem in general when the number of
instruments is large relative to the sample size. This problem arises
even if the instruments are valid.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;example-from-angrist-1992&#34;&gt;Example from Angrist (1992)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;They regress wages on years of schooling.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Problem&lt;/strong&gt;: endogeneity: both variables are correlated with skills
which are unobserved.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Solution&lt;/strong&gt;: instrument years of schooling with the quarter of
birth.
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Idea&lt;/strong&gt;: if born in the first three quarters, can attend school
from the year of your sixth birthday. Otherwise, you have to
wait one more year.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Problem&lt;/strong&gt;: quarters of birth are three dummies.
&lt;ul&gt;
&lt;li&gt;In order to ``improve the first stage fit” they interact them
with year of birth (180 effective instruments) and also with the
state (1527 effective instruments).&lt;/li&gt;
&lt;li&gt;This mechanically increases the $R^2$ but also increases the
bias of the 2SLS estimator.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Solutions&lt;/strong&gt;: LIML, JIVE, RJIVE (Hansen et al., 2014), Post-Lasso
(Belloni et al., 2012).&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;example-from-angrist-1992-1&#34;&gt;Example from Angrist (1992)&lt;/h3&gt;
&lt;img src=&#34;../img/Fig_441.png&#34; style=&#34;width:80.0%&#34; /&gt;
&lt;h2 id=&#34;many-instrument-robust-estimation&#34;&gt;Many Instrument Robust Estimation&lt;/h2&gt;
&lt;h3 id=&#34;issue&#34;&gt;Issue&lt;/h3&gt;
&lt;p&gt;Why having too many instruments is problematic? As the number of
instruments increases, the estimated coefficient gets closer to OLS
which is biased. As seen in the theorem above, for $L=n$, the two
estimators coincide.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;../img/Fig_451.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;liml&#34;&gt;LIML&lt;/h3&gt;
&lt;p&gt;An alternative method to estimate the parameters of the structural
equation is by maximum likelihood. Anderson and Rubin (1949) derived the
maximum likelihood estimator for the joint distribution of $(y_i, x_i)$.
The estimator is known as &lt;strong&gt;limited information maximum likelihood&lt;/strong&gt;, or
&lt;strong&gt;LIML&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;This estimator is called “limited information” because it is based on
the structural equation for $(y_i, x_i)$ combined with the reduced form
equation for $x_i$. If maximum likelihood is derived based on a
structural equation for $x_i$ as well, then this leads to what is known
as &lt;strong&gt;full information maximum likelihood (FIML)&lt;/strong&gt;. The advantage of the
LIML approach relative to FIML is that the former does not require a
structural model for $x_i$, and thus allows the researcher to focus on
the structural equation of interest - that for $y_i$.&lt;/p&gt;
&lt;h3 id=&#34;k-class-estimators&#34;&gt;K-class Estimators&lt;/h3&gt;
&lt;p&gt;The &lt;strong&gt;k-class&lt;/strong&gt; estimators have the form $$
\hat{\delta}(\alpha) = (X&#39; P_Z X - \alpha X&#39; X)^{-1} (X&#39; P_Z Y - \alpha X&#39; Y)
$$&lt;/p&gt;
&lt;p&gt;The limited information maximum likelihood estimator &lt;strong&gt;LIML&lt;/strong&gt; is the
k-class estimator $\hat{\delta}(\alpha)$ where $$
\alpha = \lambda_{min} \Big( ([X&#39; , Y]^{-1} [X&#39; , Y])^{-1} [X&#39; , Y]^{-1} P_Z [X&#39; , Y] \Big)
$$&lt;/p&gt;
&lt;p&gt;If $\alpha = 0$ then
$\hat{\delta} _ {\text{LIML}} = \hat{\delta} _ {\text{2SLS}}$ while for
$\alpha \to \infty$,
$\hat{\delta} _ {\text{LIML}} \to \hat{\delta} _ {\text{OLS}}$.&lt;/p&gt;
&lt;h3 id=&#34;comments-on-liml&#34;&gt;Comments on LIML&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;The particular choice of $\alpha$ gives a many instruments robust
estimate&lt;/li&gt;
&lt;li&gt;The LIML estimator has no finite sample moments.
$\mathbb E[\delta(\alpha_{LIML})]$ does not exist in general&lt;/li&gt;
&lt;li&gt;In simulation studies performs well&lt;/li&gt;
&lt;li&gt;Has good asymptotic properties&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Asymptotically the LIML estimator has the same distribution as 2SLS.
However, they can have quite different behaviors in finite samples.
There is considerable evidence that the LIML estimator has superior
finite sample performance to 2SLS when there are many instruments or the
reduced form is weak. However, on the other hand there is worry that
since the LIML estimator is derived under normality it may not be robust
in non-normal settings.&lt;/p&gt;
&lt;h3 id=&#34;jive&#34;&gt;JIVE&lt;/h3&gt;
&lt;p&gt;The &lt;strong&gt;Jacknife IV&lt;/strong&gt; procedure is the following&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Regress $\lbrace x_j \rbrace _ {j \neq i}$ on
$\lbrace z_j \rbrace _ {j \neq i}$ and estimate $\pi_{-i}$ (leave
the $i^{th}$ observation out).&lt;/li&gt;
&lt;li&gt;Form $\hat{x}_i = \hat{\pi} _ {-i} z_i$.&lt;/li&gt;
&lt;li&gt;Run IV using $\hat{x}_i$ as instruments. $$
\hat{\delta} _ {JIVE} = \mathbb E_n[\hat{x}_i x_i&#39;]^{-1} \mathbb E_n[\hat{x}_i y_i&#39;]
$$&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;comments-on-jive&#34;&gt;Comments on JIVE:&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Prevents overfitting.&lt;/li&gt;
&lt;li&gt;With many instruments you get bad out of sample prediction which
implies low correlation between $\hat{x}_i$ and $x_i$:
$\mathbb E_n[\hat{x}_i x_i&#39;] \simeq 0$.&lt;/li&gt;
&lt;li&gt;Use lasso/ridge regression in the first stage in case of too many
instruments.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;hausman-test&#34;&gt;Hausman Test&lt;/h3&gt;
&lt;p&gt;Here we consider testing the validity of OLS. OLS is generally preferred
to IV in terms of precision. Many researchers only doubt the (joint)
validity of the regressor $z_i$ instead of being certain that it is
invalid (in the sense of not being predetermined). So then they wish to
choose between OLS and 2SLS, assuming that they have an instrument
vector $x_i$ whose validity is not in question. Further, assume for
simplicity that $L = K$ so that the efficient GMM estimator is the IV
estimator.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;Hausman test statistic&lt;/strong&gt; $$
H \equiv n (\hat{\delta} _ {IV} - \hat{\delta} _ {OLS})&#39; [\hat{Avar} (\hat{\delta} _ {IV} - \hat{\delta} _ {OLS})]^{-1} (\hat{\delta} _ {IV} - \hat{\delta} _ {OLS})
$$ is asymptotically distributed as a $\chi^2_{L-s}$ under the null
where $s = | z_i \cup x_i |$: the number of regressors that are retained
as instruments in $x_i$.&lt;/p&gt;
&lt;h3 id=&#34;comments-1&#34;&gt;Comments&lt;/h3&gt;
&lt;p&gt;In general, the idea of the Hausman test is the following. If you have
two estimators, one which is efficient under $H_0$ but inconsistent
under $H_1$ (in this case, OLS), and another which is consistent under
$H_1$ (in this case, IV), then construct a test as a quadratic form in
the differences of the estimators. Another classic example arises in
panel data with the hypothesis $H_0$ of unconditional strict exogeneity.
In that case, under $H_0$ Random Effects estimators are efficient but
under $H_1$ they are inconsistent. Fixed Effects estimators instead are
consistent under $H_1$.&lt;/p&gt;
&lt;p&gt;The Hausman test statistic can be used as a pretest procedure: select
either OLS or IV according to the outcome of the test. Although widely
used, this pretest procedure is not advisable. When the null is false,
it is still possible that the test &lt;em&gt;accepts&lt;/em&gt; the null (committing a Type
2 error). In particular, this can happen with a high probability when
the sample size is &lt;em&gt;small&lt;/em&gt; and/or when the regressor $z_i$ is &lt;em&gt;almost
valid&lt;/em&gt;. In such an instance, estimation and also inference will be based
on incorrect methods. Therefore, the overall properties of the Hausman
pretest procedure are undesirable.&lt;/p&gt;
&lt;p&gt;The Hausman test is an example of a specification test. There are many
other specification tests. One could for example test for conditional
homoskedasticity. Unlike for the OLS case, there does not exist a
convenient test for conditional homoskedasticity for the GMM case. A
test statistic that is asymptotically chi-squared under the null is
available but is extremely cumbersome; see White (1982, note 2). If in
doubt, it is better to use the more generally valid inference methods
that allow for conditional heteroskedasticity. Similarly, there does not
exist a convenient test for serial correlation for the GMM case. If in
doubt, it is better to use the more generally valid inference methods
that allow for serial correlation; for example, when data are collected
over time (that is, time-series data).&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>OLS Inference</title>
      <link>https://matteocourthoud.github.io/course/metrics/06_ols_inference/</link>
      <pubDate>Fri, 29 Oct 2021 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/course/metrics/06_ols_inference/</guid>
      <description>&lt;h2 id=&#34;asymptotic-theory-of-the-ols-estimator&#34;&gt;Asymptotic Theory of the OLS Estimator&lt;/h2&gt;
&lt;h3 id=&#34;ols-consistency&#34;&gt;OLS Consistency&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt;: Assume that $(x_i, y_i) _ {i=1}^n$ i.i.d. ,
$\mathbb E[x_i x_i&#39;] = Q$ positive definite,
$\mathbb E[x_i x_i&#39;] &amp;lt; \infty$ and $\mathbb E [y_i^2] &amp;lt; \infty$, then
$\hat \beta _ {OLS}$ is a &lt;strong&gt;consistent&lt;/strong&gt; estimator of $\beta_0$,
i.e. $\hat \beta = \mathbb E_n [x_i x_i&#39;] \mathbb E_n [x_i y_i]\overset{p}{\to} \beta_0$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;:&lt;br&gt;
We consider 4 steps:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;$\mathbb E_n [x_i x_i&#39;] \xrightarrow{p} \mathbb E [x_i x_i&#39;]$ by
WLLN since $x_i x_i&#39;$ iid and $\mathbb E[x_i x_i&#39;] &amp;lt; \infty$.&lt;/li&gt;
&lt;li&gt;$\mathbb E_n [x_i y_i] \xrightarrow{p} \mathbb E [x_i y_i]$ by WLLN,
due to $x_i y_i$ iid, Cauchy-Schwarz and finite second moments of
$x_i$ and $y_i$ $$
\mathbb E \left[ x_i y_i \right]  \leq \sqrt{ \mathbb E[x_i^2] \mathbb E[y_i^2]} &amp;lt; \infty
$$&lt;/li&gt;
&lt;li&gt;$\mathbb E_n [x_i x_i&#39;]^{-1} \xrightarrow{p} \mathbb E [x_i x_i&#39;]^{-1}$
by CMT.&lt;/li&gt;
&lt;li&gt;$\mathbb E_n [x_i x_i&#39;]^{-1} \mathbb E_n [x_i y_i] \xrightarrow{p} \mathbb E [x_i x_i&#39;]^{-1} \mathbb E [x_i y_i] = \beta$
by CMT. $$\tag*{$\blacksquare$}$$&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;variance-and-assumptions&#34;&gt;Variance and Assumptions&lt;/h3&gt;
&lt;p&gt;Now we are going to investigate the variance of $\hat \beta _ {OLS}$
progressively relaxing the underlying assumptions.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Gaussian error term.&lt;/li&gt;
&lt;li&gt;Homoskedastic error term.&lt;/li&gt;
&lt;li&gt;Heteroskedastic error term.&lt;/li&gt;
&lt;li&gt;Heteroskedastic and autocorrelated error term.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;gaussian-error-term&#34;&gt;Gaussian Error Term&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt;: Under the GM assumption (1)-(5),
$\hat \beta - \beta |X \sim N(0, \sigma^2 (X&amp;rsquo;X)^{-1})$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;:&lt;br&gt;
We follow 2 steps:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;We can rewrite $\hat \beta$ as $$
\begin{aligned}
\hat \beta &amp;amp; = (X&amp;rsquo;X)^{-1} X&amp;rsquo;y = (X&amp;rsquo;X)^{-1} X&#39;(X\beta + \varepsilon) \newline
&amp;amp;= \beta + (X&amp;rsquo;X)^{-1} X&#39; \varepsilon = \newline
&amp;amp;= \beta + \mathbb E_n [x_i x_i&#39;]^{-1} \mathbb E_n [x_i \varepsilon_i]
\end{aligned}
$$&lt;/li&gt;
&lt;li&gt;Therefore:
$\hat \beta-\beta = \mathbb E_n [x_i x_i&#39;]^{-1} \mathbb E_n [x_i \varepsilon_i]$.
$$
\begin{aligned}
\hat \beta-\beta |X &amp;amp; \sim (X&amp;rsquo;X)^{-1} X&#39; N(0, \sigma^2 I_n) = \newline
&amp;amp;= N(0, \sigma^2 (X&amp;rsquo;X)^{-1} X&amp;rsquo;X (X&amp;rsquo;X)^{-1}) = \newline
&amp;amp;= N(0, \sigma^2 (X&amp;rsquo;X)^{-1})
\end{aligned}
$$ $$\tag*{$\blacksquare$}$$&lt;/li&gt;
&lt;/ol&gt;
&lt;blockquote&gt;
&lt;p&gt;Does it make sense to assume that $\varepsilon$ is gaussian? Not much.
But does it make sense that $\hat \beta$ is gaussian? Yes, because
it’s an average.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;homoskedastic-error-term&#34;&gt;Homoskedastic Error Term&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt;: Under the assumptions of the previous theorem, plus
$\mathbb E[x^4] &amp;lt; \infty$, the OLS estimate has an asymptotic normal
distribution:
$\hat \beta|X \overset{d}{\to} N(\beta, \sigma^2 (X&amp;rsquo;X)^{-1})$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;: $$
\sqrt{n} (\hat \beta - \beta ) = \underbrace{\mathbb E_n [x_i x_i&#39;]^{-1}} _ {\xrightarrow{p} Q^{-1} }   \underbrace{\sqrt{n} \mathbb E_n [x_i \varepsilon_i ]} _ {\xrightarrow{d} N(0, \Omega)} \rightarrow N(0, \Sigma )
$$ where in general
$\Omega = Var (x_i \varepsilon_i) = \mathbb E [(x_i \varepsilon_i)^2]$
and $\Sigma = Q^{-1} \Omega Q^{-1}$. $$\tag*{$\blacksquare$}$$&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Given that $Q = \mathbb E [x_i x_i&#39;]$ is unobserved, we estimate it
with $\hat{Q} = \mathbb E_n [x_i x_i&#39;]$. Since we have assumed
homoskedastic error term, we have $\Omega = \sigma^2 (X&amp;rsquo;X)^{-1}$.
Since we do not observe $\sigma^2$ we estimate it as
$\hat{\sigma}^2 = \mathbb E_n[\hat{\varepsilon}_i^2]$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The terms $x_i \varepsilon_i$ are called &lt;strong&gt;scores&lt;/strong&gt; and we can already
see their central importance for inference.&lt;/p&gt;
&lt;h3 id=&#34;heteroskedastic-error-term&#34;&gt;Heteroskedastic Error Term&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Assumption&lt;/strong&gt;: $\mathbb E [\varepsilon_i x_i \varepsilon_j&#39; x_j&#39;] = 0$,
for all $j \ne i$ and $\mathbb E [\varepsilon_i^4] \leq \infty$,
$\mathbb E [|| x_i||^4] \leq C &amp;lt; \infty$ a.s.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt;: Under GM assumptions (1)-(4) plus heteroskedastic error
term, the following estimators are consistent,
i.e. $\hat{\Sigma}\xrightarrow{p} \Sigma$.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Note that we are only looking at $\Omega$ of the
$\Sigma = Q^{-1} \Omega Q^{-1}$ matrix.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;HC0&lt;/strong&gt;: use the observed residual $\hat{\varepsilon}_i$ $$
\Omega _ {HC0} = \mathbb E_n [x_i x_i&#39; \hat{\varepsilon}_i^2]
$$ When $k$ is too big relative to $n$ – i.e.,
$k/n \rightarrow c &amp;gt;0$ – $\hat{\varepsilon}_i^2$ are too small
($\Omega _ {HC0}$ biased towards zero). $\Omega _ {HC1}$,
$\Omega _ {HC2}$ and $\Omega _ {HC3}$ try to correct this small
sample bias.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;HC1&lt;/strong&gt;: degree of freedom correction (default &lt;code&gt;robust&lt;/code&gt; in Stata) $$
\Omega _ {HC1} = \frac{1}{n - k }\mathbb E_n [x_i x_i&#39; \hat{\varepsilon}_i^2]
$$&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;HC2&lt;/strong&gt;: use standardized residuals $$
\Omega _ {HC2} = \mathbb E_n [x_i x_i&#39; \hat{\varepsilon}_i^2 (1-h _ {ii})^{-1}]
$$ where $h _ {ii} = [X(X&amp;rsquo;X)^{-1} X&#39;] _ {ii}$ is the &lt;strong&gt;leverage&lt;/strong&gt;
of the $i^{th}$ observation. A large $h _ {ii}$ means that
observation $i$ is unusual in the sense that the regressor $x_i$ is
far from its sample mean.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;HC3&lt;/strong&gt;: use prediction error, equivalent to Jack-knife estimator,
i.e., $\mathbb E_n [x_i x_i&#39; \hat{\varepsilon} _ {(-i)}^2]$ $$
\Omega _ {HC3} = \mathbb E_n [x_i x_i&#39; \hat{\varepsilon}_i^2 (1-h _ {ii})^{-2}]
$$ This estimator does not overfit when $k$ is relatively big with
respect to $n$. Idea: you exclude the corresponding observation when
estimating a particular $\varepsilon_i$:
$\hat{\varepsilon}_i = y_i - x_i&#39; \hat \beta _ {-i}$.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;hc0-consistency&#34;&gt;HC0 Consistency&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Under regularity conditions HC0 is consistent,
i.e. $\hat{\Sigma} _ {HC0} \overset{p}{\to} \Sigma$. $$
\hat{\Sigma} = \hat{Q}^{-1} \hat{\Omega} \hat{Q}^{-1} \xrightarrow{p} \Sigma \qquad  \text{ with } \hat{\Omega} = \mathbb E_n [x_i x_i&#39;     \hat{\varepsilon}_i^2] \quad \text{ and } \hat{Q} = \mathbb E_n [x_i x_i&#39;]^{-1}
$$&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Why is the proof relevant? You cannot directly apply the WLLN to
$\hat \Sigma$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;For the case $\mathrm{dim}(x_i) =1$.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;$\hat{Q}^{-1} \xrightarrow{p} Q^{-1}$ by WLLN since $x_i$ is iid,
$\mathbb E[x_i^4] &amp;lt; \infty$&lt;/li&gt;
&lt;li&gt;$\bar{\Omega} = \mathbb E_n [\varepsilon_i^2 x_i x_i&#39;] \xrightarrow{p} \Omega$
by WLLN since $\mathbb E_n [\varepsilon_i^4] &amp;lt; c$ and $x_i$ bounded.&lt;/li&gt;
&lt;li&gt;By the triangle inequality, $$
| \hat{\Omega} - \hat{\Omega}| \leq \underbrace{|\Omega - \bar{\Omega}|} _ {\overset{p}{\to} 0} + \underbrace{|\bar{\Omega} - \hat{\Omega}|} _ {\text{WTS:} \overset{p}{\to} 0}
$$&lt;/li&gt;
&lt;li&gt;We want to show $|\bar{\Omega} - \hat{\Omega}| \overset{p}{\to} 0$
$$
\begin{aligned}
|\bar{\Omega} - \hat{\Omega}| &amp;amp;= \mathbb E_n [\varepsilon_i^2 x_i^2] - \mathbb E_n [\hat{\varepsilon}_i^2 x_i^2]  = \newline
&amp;amp;= \mathbb E_n [\left( \varepsilon_i^2 - \hat{\varepsilon}_i^2 \right) x_i^2] \leq \newline
&amp;amp; \leq \mathbb E_n \left[ \left( \varepsilon_i^2 - \hat{\varepsilon}_i^2 \right)^2\right]^{\frac{1}{2}} \mathbb E_n [x_i^4]^{\frac{1}{2}}
\end{aligned}
$$ where
$\mathbb E_n [x_i^4]^{\frac{1}{2}} \xrightarrow{p} \mathbb E [x_i^4]^{\frac{1}{2}}$
by $x_i$ bounded, iid and CMT.&lt;/li&gt;
&lt;li&gt;We want to show that
$\mathbb E_n \left[ \left( \varepsilon_i^2 - \hat{\varepsilon}_i^2 \right)^2\right] \leq \eta$
with $\eta \rightarrow 0$. Let
$L = \max_i |\hat{\varepsilon}_i - \varepsilon_i|$ (RV depending on
$n$), with $L \xrightarrow{p} 0$ since $$
|\hat{\varepsilon}_i - \varepsilon_i| = |x_i \hat \beta - x_i \beta| \leq |x_i||\hat \beta - \beta|\xrightarrow{p} c \cdot 0
$$ We can depompose $$
\begin{aligned}
\left(\varepsilon_i^2 - \hat{\varepsilon}_i^2 \right)^2 &amp;amp; = \left(\varepsilon_i - \hat{\varepsilon}_i \right)^2 \left(\varepsilon_i + \hat{\varepsilon}_i \right)^2 \leq \newline&lt;br&gt;
&amp;amp; \leq \left(\varepsilon_i + \hat{\varepsilon}_i \right)^2 L^2 = \newline
&amp;amp;= \left(2\varepsilon_i - \varepsilon_i + \hat{\varepsilon}_i \right)^2 L^2\leq  \newline
&amp;amp; \leq  \left( 2(2\varepsilon_i)^2 + 2(\hat{\varepsilon}_i - \varepsilon_i)^2 \right)^2 L^2 \leq \newline
&amp;amp; \leq (8 \varepsilon_i^2 + 2 L^2) L^2
\end{aligned}
$$ Hence $$
\mathbb E \left[ \left(\varepsilon_i^2 - \hat{\varepsilon}_i^2 \right)^2 \right] \leq  L^2 \left( 8 \mathbb E_n [ \varepsilon_i^2] + 2 \mathbb E_n [L^2] \right)  \xrightarrow{p}0
$$ $$\tag*{$\blacksquare$}$$&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;heteroskedastic-and-autocorrelated-error-term&#34;&gt;Heteroskedastic and Autocorrelated Error Term&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Assumption&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;There esists a $\bar{d}$ such that:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\mathbb E[\varepsilon_i x_i \varepsilon&#39; _ {i-d} x&#39; _ {i-d}] \neq 0 \quad$
for $d \leq \bar{d}$&lt;/li&gt;
&lt;li&gt;$\mathbb E[\varepsilon_i x_i \varepsilon&#39; _ {i-d} x&#39; _ {i-d}] = 0 \quad$
for $d &amp;gt; \bar{d}$&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;Intuition: observations far enough from each other are not correlated.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;We can express the variance of the score as $$
\begin{aligned}
\Omega_n &amp;amp;= Var(\sqrt{n} \mathbb E_n[x_i \varepsilon_i]) = \newline
&amp;amp;= \mathbb E \left[ \left( \frac{1}{n} \sum _ {i=1}^n x_i \varepsilon_i \right) \left( \frac{1}{n} \sum _ {j=1}^n x_j \varepsilon_j \right) \right] = \newline
&amp;amp;= \frac{1}{n} \sum _ {i=1}^n \sum _ {j=1}^n \mathbb E[x_i \varepsilon_i x_j&#39; \varepsilon_j&#39;] = \newline
&amp;amp;= \frac{1}{n} \sum _ {i=1}^n \sum _ {j : |i-j|\leq \bar{d}} \mathbb E[x_i \varepsilon_i x_j&#39; \varepsilon_j&#39;] = \newline
&amp;amp;= \frac{1}{n} \sum _ {d=0}^{\bar{d}} \sum _ {i = d}^{n} \mathbb E[x_i \varepsilon_i x _ {i-d}&#39; \varepsilon _ {i-d}&#39;]
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;We estimate $\Omega_n$ by $$
\hat{\Omega}_n = \frac{1}{n} \sum _ {d=0}^{\bar{d}} \sum _ {i = d}^{n} x_i \hat{\varepsilon}_i x _ {i-d}&#39; \hat{\varepsilon} _ {i-d}&#39;
$$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;If $\bar{d}$ is a fixed integer, then $$
\hat{\Omega}_n - \Omega_n \overset{p}{\to} 0
$$&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;What if $\bar{d}$ does not exist (all $x_i, x_j$ are correlated)? $$
\hat{\Omega}_n = \frac{1}{n} \sum _ {d=0}^{n} \sum _ {i = d}^{n} x_i \hat{\varepsilon}_i x _ {i-d}&#39; \hat{\varepsilon} _ {i-d}&#39; = n \mathbb E_n[x_i \hat{\varepsilon}_i]^2 = 0
$$ By the orthogonality property of the OLS residual.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;HAC with Uniform Kernel&lt;/strong&gt; $$
\hat{\Omega}_h = \frac{1}{n} \sum _ {i,j} x_i \hat{\varepsilon}_i x_j&#39; \hat{\varepsilon}_j&#39; \mathbb{I} \lbrace |i-j| \leq h \rbrace
$$ where $h$ is the &lt;strong&gt;bandwidth&lt;/strong&gt; of the kernel. The bandwidth is chosen
such that
$\mathbb E[x_i \varepsilon_i x _ {i-d}&#39; \varepsilon _ {i-d}&#39; ]$ is small
for $d &amp;gt; h$. How small? Small enough for the estimates to be consistent.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;HAC with General Kernel&lt;/strong&gt; $$
\hat{\Omega}^{HAC} _ {k,h} = \frac{1}{n} \sum _ {i,j} x_i \hat{\varepsilon}_i x_j&#39; \hat{\varepsilon}_j&#39; k \left( \frac{|i-j|}{n} \right)
$$&lt;/p&gt;
&lt;h3 id=&#34;hac-consistency&#34;&gt;HAC Consistency&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt; If the joint distribution is stationary and $\alpha$-mixing
with $\sum _ {k=1}^\infty k^2 \alpha(k) &amp;lt; \infty$ and&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\mathbb E[ | x _ {ij} \varepsilon_i |^\nu ] &amp;lt; \infty$ $\forall \nu$&lt;/li&gt;
&lt;li&gt;$\hat{\varepsilon}_i = y_i - x_i&#39; \hat \beta$ for some
$\hat \beta \overset{p}{\to} \beta_0$&lt;/li&gt;
&lt;li&gt;$k$ smooth, symmetric, $k(0) \to \infty$ as $z \to \infty$,
$\int k^2 &amp;lt; \infty$&lt;/li&gt;
&lt;li&gt;$\frac{h}{n} \to 0$&lt;/li&gt;
&lt;li&gt;$h \to \infty$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Then the HAC estimator is &lt;strong&gt;consistent&lt;/strong&gt;. $$
\hat{\Omega}^{HAC} _ {k,h} - \Omega_n \overset{p}{\to} 0
$$&lt;/p&gt;
&lt;h3 id=&#34;comments&#34;&gt;Comments&lt;/h3&gt;
&lt;p&gt;We want to choose $h$ small relative to $n$ in order to avoid estimation
problems. But we also want to choose $h$ large so that the remainder is
small: $$
\begin{aligned}
\Omega_n &amp;amp;= Var(\sqrt{n} \mathbb E_n[x_i \varepsilon_i]) = \newline
&amp;amp;= \underbrace{\frac{1}{n} \sum _ {i,j : |i-j|\leq h} \mathbb E[x_i \varepsilon_i x_j&#39; \varepsilon_j&#39;]} _ {\Omega^h_n} + \underbrace{\frac{1}{n} \sum _ {i,j : |i-j|&amp;gt; h} \mathbb E[x_i \varepsilon_i x_j&#39; \varepsilon_j&#39;]} _ {\text{remainder: } R_n} = \newline
&amp;amp;= \Omega_n^h + R_n
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;In particular, HAC theory requires: $$
\hat{\Omega}^{HAC} \overset{p}{\to} \Omega \quad \text{ if } \quad
\begin{cases}
&amp;amp; \frac{h}{n} \to 0 \newline
&amp;amp; h \to \infty
\end{cases}
$$&lt;/p&gt;
&lt;p&gt;But in practice, long-run estimation implies $\frac{h}{n} \simeq 0$
which is not ``safe” in the sense that it does not imply
$R_n \simeq 0$. On the other hand, if $h \simeq n$, $\hat{\Omega}^{HAC}$
does not converge in probability because it’s too noisy.&lt;/p&gt;
&lt;h3 id=&#34;choice-of-h&#34;&gt;Choice of h&lt;/h3&gt;
&lt;p&gt;How to choose $h$? Look at the score autocorrelation function (ACF).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;../img/Fig_331.jpg&#34; alt=&#34;Autocorrelation Function&#34;&gt;&lt;/p&gt;
&lt;p&gt;It looks like after 10 periods the empirical autocorrelation is quite
small but still not zero.&lt;/p&gt;
&lt;h3 id=&#34;fixed-b-asymptotics&#34;&gt;Fixed b Asymptotics&lt;/h3&gt;
&lt;p&gt;[Neave, 1970]: “&lt;em&gt;When proving results on the asymptotic behavior of
estimates of the spectrum of a stationary time series, it is invariably
assumed that as the sample size $n$ tends to infinity, so does the
truncation point $h$, but at a slower rate, so that $\frac{h}{n}$ tends
to zero. This is a convenient assumption mathematically in that, in
particular, it ensures consistency of the estimates, but it is
unrealistic when such results are used as approximations to the finite
case where the value of $\frac{h}{n}$ cannot be zero.&lt;/em&gt;””&lt;/p&gt;
&lt;h3 id=&#34;fixed-b-theorem&#34;&gt;Fixed b Theorem&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Under regularity conditions, $$
\sqrt{n} \Big( V^{HAC} _ {k,h} \Big)(\hat \beta - \beta_0) \overset{d}{\to} F
$$&lt;/p&gt;
&lt;p&gt;The asymptotic critical values of the $F$ statistic depend on the choice
of the kernel. In order to do hypothesis testing, Kiefer and
Vogelsang(2005) provide critical value functions for the t-statistic for
each kernel-confidence level combination using a cubic equation: $$
cv(b) = a_0 + a_1 b + a_2 b^2 + a_3 b^3
$$&lt;/p&gt;
&lt;h3 id=&#34;example&#34;&gt;Example&lt;/h3&gt;
&lt;p&gt;Example for the Bartlett kernel:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;../img/Fig_332.png&#34; alt=&#34;Fixed-b&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;fixed-g-asymptotics&#34;&gt;Fixed G Asymptotics&lt;/h3&gt;
&lt;p&gt;[Bester, 2013]: “&lt;em&gt;Cluster covariance estimators are routinely used
with data that has a group structure with independence assumed across
groups. Typically, inference is conducted in such settings under the
assumption that there are a large number of these independent groups.&lt;/em&gt;””&lt;/p&gt;
&lt;p&gt;“&lt;em&gt;However, with enough weakly dependent data, we show that groups can be
chosen by the researcher so that group-level averages are approximately
independent. Intuitively, if groups are large enough and well shaped
(e.g. do not have gaps), the majority of points in a group will be far
from other groups, and hence approximately independent of observations
from other groups provided the data are weakly dependent. The key
prerequisite for our methods is the researcher’s ability to construct
groups whose averages are approximately independent. As we show later,
this often requires that the number of groups be kept relatively small,
which is why our main results explicitly consider a fixed (small) number
of groups.&lt;/em&gt;””&lt;/p&gt;
&lt;h3 id=&#34;assumption&#34;&gt;Assumption&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Assumption&lt;/strong&gt; Suppose you have data
$D = (y _ {it} , x _ {it}) _ {i=1, t=1}^{N, T}$ where
$y _ {it} = x _ {it}&#39; \beta + \alpha_i + \varepsilon _ {it}$ where $i$
indexes the observational unit and $t$ indexes time (could also be
space).&lt;/p&gt;
&lt;p&gt;Let $$
\begin{aligned}
&amp;amp; \tilde{y} _ {it} = y _ {it} - \frac{1}{T} \sum _ {t=1}^T y _ {it} \newline
&amp;amp; \tilde{x} _ {it} = x _ {it} - \frac{1}{T} \sum _ {t=1}^T x _ {it} \newline
&amp;amp; \tilde{\varepsilon} _ {it} = \varepsilon _ {it} - \frac{1}{T} \sum _ {t=1}^T \varepsilon _ {it}
\end{aligned}
$$ Then $$
\tilde{y} _ {it} = \tilde{x} _ {it}&#39; \beta + \tilde{\varepsilon} _ {it}
$$&lt;/p&gt;
&lt;p&gt;The $\tilde{\varepsilon} _ {it}$ are by construction correlated between
each other even if the original $\varepsilon$ was iid. The &lt;strong&gt;cluster
score variance estimator&lt;/strong&gt; is given by: $$
\hat{\Omega}^{CL} = \frac{1}{T-1} \sum _ {i=1}^n  \sum _ {t=1}^T  \sum _ {s=1}^T \tilde{x} _ {it} \hat{\tilde{\varepsilon}} _ {it} \tilde{x} _ {is}     \hat{\tilde{\varepsilon}} _ {is}
$$&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;It’s very similar too the HAC estimator since we have &lt;em&gt;dependent
cross-products&lt;/em&gt; here as well. However, here we do not consider the
$i \times j$ cross-products. We only have time-dependency (state).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;comments-1&#34;&gt;Comments (1)&lt;/h3&gt;
&lt;p&gt;On $T$ and $n$:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;If $T$ is fixed and $n \to \infty$, then the number of
cross-products considered is much smaller than the total number of
cross-products.&lt;/li&gt;
&lt;li&gt;If $T &amp;raquo; n$ issues arise since the number of cross products
considered is close to the total number of cross products. As in HAC
estimation, this is a problem because it implies that the algebraic
estimate of the cluster score variance gets close to zero because of
the orthogonality property of the residuals.&lt;/li&gt;
&lt;li&gt;The panel assumption is that observations across individuals are not
correlated.&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;Strategy: as in HAC, we want to limit the correlation across clusters
(individuals). We hope that observations are &lt;strong&gt;negligibly dependent&lt;/strong&gt;
between cluster sufficiently distant from each other.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;comments-2&#34;&gt;Comments (2)&lt;/h3&gt;
&lt;p&gt;Classical cluster robust estimator: $$
\hat{\Omega}^{CL} = \frac{1}{n} \sum _ {i=1}^n x_i \varepsilon_i x_j&#39; \varepsilon_j&#39; \mathbb{I}   \lbrace i,j \text{ in the same cluster} \rbrace
$$&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;On clusters:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;If the number of observations near a boundary is small relative to
the sample size, ignoring the dependence should not affect
inference too adversely.&lt;/li&gt;
&lt;li&gt;The higher the dimension of the data, the easier it is to have
observations near boundaries (&lt;em&gt;curse of dimensionality&lt;/em&gt;).&lt;/li&gt;
&lt;li&gt;We would like to have few clusters in order to make less
independence assumptions. However, few clusters means bigger
blocks and hence a larger number of cross-products to estimate. If
the number of cross-products is too large (relative to the sample
size), $\hat{\Omega}^{CL}$ does not converge&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt;: Under regularity conditions: $$
\hat{t} \overset{d}{\to} \sqrt{\frac{G}{G-1}} t _ {G-1}
$$&lt;/p&gt;
&lt;h3 id=&#34;code---dgp&#34;&gt;Code - DGP&lt;/h3&gt;
&lt;p&gt;This code draws 100 observations from the model
$y = 2 x_1 - x_2 + \varepsilon$ where $x_1, x_2 \sim U[0,1]$ and
$\varepsilon \sim N(0,1)$.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# Set seed
Random.seed!(123);

# Set the number of observations
n = 100;

# Set the dimension of X
k = 2;

# Draw a sample of explanatory variables
X = rand(Uniform(0,1), n, k);

# Draw the error term
σ = 1;
ε = rand(Normal(0,1), n, 1) * sqrt(σ);

# Set the parameters
β = [2; -1];

# Calculate the dependent variable
y = X*β + ε;
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;ideal-estimate&#34;&gt;Ideal Estimate&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# OLS estimator
β_hat = (X&#39;*X)\(X&#39;*y);

# Residuals
ε_hat = y - X*β_hat;

# Homoskedastic standard errors
std_h = var(ε_hat) * inv(X&#39;*X);

# Projection matrix
P = X * inv(X&#39;*X) * X&#39;;

# Leverage
h = diag(P);
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;hc-estimates&#34;&gt;HC Estimates&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# HC0 variance and standard errors
Ω_hc0 = X&#39; * (I(n) .* ε_hat.^2) * X;
std_hc0 = sqrt.(diag(inv(X&#39;*X) * Ω_hc0 * inv(X&#39;*X)))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 2-element Array{Float64,1}:
##  0.24691300271914793
##  0.28044707935951835
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# HC1 variance and standard errors
Ω_hc1 = n/(n-k) * X&#39; * (I(n) .* ε_hat.^2) * X;
std_hc1 = sqrt.(diag(inv(X&#39;*X) * Ω_hc1 * inv(X&#39;*X)))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 2-element Array{Float64,1}:
##  0.24941979797977423
##  0.2832943308272532
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# HC2 variance and standard errors
Ω_hc2 = X&#39; * (I(n) .* ε_hat.^2 ./ (1 .- h)) * X;
std_hc2 = sqrt.(diag(inv(X&#39;*X) * Ω_hc2 * inv(X&#39;*X)))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 2-element Array{Float64,1}:
##  0.2506509902982869
##  0.2850878737103963
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# HC3 variance and standard errors
Ω_hc3 = X&#39; * (I(n) .* ε_hat.^2 ./ (1 .- h).^2) * X;
std_hc3 = sqrt.(diag(inv(X&#39;*X) * Ω_hc3 * inv(X&#39;*X)))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 2-element Array{Float64,1}:
##  0.25446321015850176
##  0.2898264779289438
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# Note what happens if you allow for full autocorrelation
omega_full = X&#39;*ε_hat*ε_hat&#39;*X;
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;inference&#34;&gt;Inference&lt;/h2&gt;
&lt;h3 id=&#34;hypothesis-testing&#34;&gt;Hypothesis Testing&lt;/h3&gt;
&lt;p&gt;In order to do inference on $\hat \beta$ we need to know its
distribution. We have two options: (i) assume gaussian error term
(extended GM) or (ii) rely on asymptotic approximations (CLT).&lt;/p&gt;
&lt;p&gt;A statistical hypothesis is a subset of a statistical model,
$\mathcal K \subset \mathcal F$. A hypothesis test is a map
$\mathcal D \rightarrow \lbrace 0,1 \rbrace$, $D \mapsto T$. If
$\mathcal F$ is the statistical model and $\mathcal K$ is the
statistical hypothesis, we use the notation $H_0: \Pr \in \mathcal K$.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Generally, we are interested in understanding whether it is likely
that data $D$ are drawn from $\mathcal K$ or not.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;A hypothesis test, $T$ is our tool for deciding whether the hypothesis
is consistent with the data. $T(D)= 0$ implies fail to reject $H_0$ and
test inconclusive $T(D)=1$ $\implies$ reject $H_0$ and $D$ is
inconsistent with any $\Pr \in \mathcal K$.&lt;/p&gt;
&lt;p&gt;Let $\mathcal K \subseteq \mathcal F$ be a statistical hypothesis and
$T$ a hypothesis test.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Suppose $\Pr \in \mathcal K$. A Type I error (relative to $\Pr$) is
an event $T(D)=1$ under $\Pr$.&lt;/li&gt;
&lt;li&gt;Suppose $\Pr \in \mathcal K^c$. A Type II error (relative to $\Pr$)
is an event $T(D)=0$ under $\Pr$.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The corresponding probability of a type I error is called &lt;strong&gt;size&lt;/strong&gt;. The
corresponding probability of a type II error is called &lt;strong&gt;power&lt;/strong&gt;
(against the alternative $\Pr$).&lt;/p&gt;
&lt;p&gt;In this section, we are interested in testing three hypotheses, under
the assumptions of linearity, strict exogeneity, no multicollinearity,
normality on the error term. They are:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;$H_0: \beta _ {0k} = \bar \beta _ {0k}$ (single coefficient,
$\bar \beta _ {0k} \in \mathbb R$, $k \leq K$)&lt;/li&gt;
&lt;li&gt;$a&#39; \beta_0 = c$ (linear combination,
$a \in \mathbb R^K, c \in \mathbb R$)&lt;/li&gt;
&lt;li&gt;$R \beta_0 = r$ (linear restrictions,
$R \in \mathbb R^{p \times K}$, full rank, $r \in \mathbb R^p$)&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;testing-problem&#34;&gt;Testing Problem&lt;/h3&gt;
&lt;p&gt;Consider the testing problem $H_0: \beta _ {0k} = \bar \beta _ {0k}$
where $\bar \beta _ {0k}$ is a pre-specified value under the null. The
t-statistic for this problem is defined by $$
t_k:= \frac{b_k - \bar \beta _ {0k}}{SE(b_k)}, \ \ SE(b_k):= \sqrt{s^2 [(X&amp;rsquo;X)^{-1}] _ {kk}}
$$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt;: In the testing procedure above, the sampling distribution
under the null $H_0$ is given by $$
t_k|X \sim t _ {n-k} \ \ \text{and so} \ \ t_k \sim t _ {n-k}
$$&lt;/p&gt;
&lt;p&gt;$t _ {(n-K)}$ denotes the t-distribution with $(n-k)$ degress of
freedom. The test can be one sided or two sided. The above sampling
distribution can be used to construct a confidence interval.&lt;/p&gt;
&lt;h3 id=&#34;example-1&#34;&gt;Example&lt;/h3&gt;
&lt;p&gt;We want to asses whether or not the ``true” coefficient $\beta_0$
equals a specific value $\hat \beta$. Specifically, we are interested in
testing $H_0$ against $H_1$, where:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Null Hypothesis&lt;/em&gt;: $H_0: \beta_0 = \hat \beta$&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Alternative Hypothesis&lt;/em&gt;: $H_1: \beta_0 \ne \hat \beta$.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Hence, we are interested in a statistic informative about $H_1$, which
is the Wald test statistic $$
|T^*| = \bigg| \frac{\hat \beta - \beta_0}{\sigma(\hat \beta)}\bigg|  \sim N(0,1)
$$&lt;/p&gt;
&lt;p&gt;However, the true variance $\sigma^2(\hat \beta )$ is not known and has
to be estimated. Therefore we plug in the sample variance
$\hat \sigma^2(\hat \beta) = \frac{n}{n-1} \mathbb E_n[\hat e_i^2]$ and
we use $$
|T| = \bigg| \frac{\hat \beta - \beta_0}{\hat \sigma (\hat \beta)}\bigg|  \sim t _ {(n-k)}
$$&lt;/p&gt;
&lt;h3 id=&#34;comments-3&#34;&gt;Comments&lt;/h3&gt;
&lt;p&gt;Hypothesis testing is like proof by contradiction. Imagine the sampling
distribution was generated by $\beta$. If it is highly improbable to
observe $\hat \beta$ given $\beta_0 = \beta$ then we reject the
hypothesis that the sampling distribution was generated by $\beta$.&lt;/p&gt;
&lt;p&gt;Then, given a realized value of the statistic $|T|$, we take the
following decision:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Do not reject $H_0$&lt;/em&gt;: it is consistent with random variation under
true $H_0$—i.e., $|T|$ small as it has an exact student t
distribution with $(n-k)$ degree of freedom in the normal regression
model.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Reject $H_0$ in favor of $H_1$&lt;/em&gt;: $|T| &amp;gt; c$, with $c$ being the
critical values selected to control for false rejections:
$\Pr(|t _ {n-k}| \geq c) = \alpha$. Moreover, you can also reject
$H_0$ if the p-value $p$ is such that: $p &amp;lt; \alpha$.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;comments-2-1&#34;&gt;Comments (2)&lt;/h3&gt;
&lt;p&gt;The probability of false rejection is decreasing in $c$, i.e. the
critical value for a given significant level. $$
\begin{aligned}
\Pr (\text{Reject } H_0 | H_0)  &amp;amp; = \Pr (|T|&amp;gt; c | H_0 ) = \newline
&amp;amp; = \Pr (T &amp;gt; c | H_0 ) +     \Pr (T &amp;lt; -c | H_0 ) = \newline
&amp;amp; = 1 - F(c) + F(-c) = 2(1-F(c))
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Example&lt;/strong&gt;: Consider the testing problem $H_0: a&#39;\beta_0=c$ where $a$
is a pre-specified linear combination under study. The t-statistic for
this problem is defined by: $$
t_k:= \frac{a&amp;rsquo;b - c}{SE(a&amp;rsquo;b)}, \ \ SE(a&amp;rsquo;b):= \sqrt{s^2 a&#39;(X&amp;rsquo;X)^{-1}a}
$$&lt;/p&gt;
&lt;h3 id=&#34;t-stat&#34;&gt;t Stat&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In the testing procedure above, the sampling distribution under the null
$H_0$ is given by $$
t_a|X \sim t _ {n-K} \quad\text{and so} \quad t_a \sim t _ {n-K}
$$&lt;/p&gt;
&lt;p&gt;Like in the previous test, $t _ {(n-K)}$ denotes the t-distribution with
$(n-K)$ degress of freedom. The test can again be one sided or two
sided. The above sampling distribution can be used to construct a
confidence interval&lt;/p&gt;
&lt;h3 id=&#34;f-stat&#34;&gt;F Stat&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Example&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Consider the testing problem $$
H_0: R \beta_0 = r
$$ where $R \in \mathbb R^{p \times k}$ is a presepecified set of linear
combinations and $r \in \mathbb R^p$ is a restriction vector.&lt;/p&gt;
&lt;p&gt;The F-statistic for this problem is given by $$
F:= \frac{(Rb-r)&#39;[R(X&amp;rsquo;X)R&#39;]^{-1}(Rb-r)/p }{s^2}
$$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;For the problem, the sampling distribution of the F-statistic under the
null $H_0:$ $$
F|X \sim F _ {p,n-K} \ \ \text{and so} \ \ F \sim F _ {p,n-K}
$$&lt;/p&gt;
&lt;p&gt;The test is intrinsically two-sided. The above sampling distribution can
be used to construct a confidence interval.&lt;/p&gt;
&lt;h3 id=&#34;equivalence&#34;&gt;Equivalence&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Consider the testing problem $H_0: R \beta_0 = r$ where
$R \in \mathbb R^{p\times K}$ is a presepecified set of linear
combinations and $r \in \mathbb R^p$ is a restriction vector.&lt;/p&gt;
&lt;p&gt;Consider the restricted least squares estimator, denoted $\hat \beta_R$:
$\hat \beta_R: = \text{arg} \min _ { \beta: R \beta = r } Q( \beta)$.
Let $SSR_U = Q(b), \ \ SSR_R=Q(\hat \beta_R)$. Then the $F$ statistic is
numerically equivalent to the following expression:
$F = \frac{(SSR_R - SSR_U)/p}{SSR_U/(n-K)}$.&lt;/p&gt;
&lt;h3 id=&#34;confidence-intervals&#34;&gt;Confidence Intervals&lt;/h3&gt;
&lt;p&gt;A &lt;strong&gt;confidence interval at $(1-\alpha)$&lt;/strong&gt; is a random set $C$ such that
$$
\Pr(\beta_0 \in C) \geq 1- \alpha
$$ i.e. the probability that $C$ covers the true value $\beta$ is fixed
at $(1-\alpha)$.&lt;/p&gt;
&lt;p&gt;Since $C$ is not known, it has to be estimated ($\hat{C}$). We construct
confidence intervals such that:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;they are symmetric around $\hat \beta$;&lt;/li&gt;
&lt;li&gt;their length is proportional to
$\sigma(\hat \beta) = \sqrt{Var(\hat \beta)}$.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;A CI is equivalent to the set of parameter values such that the
t-statistic is less than $c$, i.e., $$
\hat{C} = \bigg\lbrace \beta: |T(\beta) | \leq c \bigg\rbrace = \bigg\lbrace \beta: - c\leq \frac{\beta - \hat \beta}{\sigma(\hat \beta)} \leq c \bigg\rbrace
$$&lt;/p&gt;
&lt;p&gt;In practice, to construct a 95% confidence interval for a single
coefficient estimate $\hat \beta_j$, we use the fact that $$
\Pr \left( \frac{| \hat \beta_j - \beta _ {0,j} |}{ \sqrt{\sigma^2 [(X&amp;rsquo;X)^{-1}] _ {jj} }} &amp;gt; 1.96 \right) = 0.05
$$&lt;/p&gt;
&lt;h3 id=&#34;code&#34;&gt;Code&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# t-test for beta=0
t = abs.(β_hat ./ (std_hc1));

# p-value
p_val = 1 .- cdf.(Normal(0,1), t);

# F statistic of joint significance
SSR_u = ε_hat&#39;*ε_hat;
SSR_r = y&#39;*y;
F = (SSR_r - SSR_u)/k / (SSR_u/(n-k));

# 95# confidente intervals
conf_int = [β_hat - 1.96*std_hc1, β_hat + 1.96*std_hc1];
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Tree-based Methods</title>
      <link>https://matteocourthoud.github.io/course/ml-econ/07_trees/</link>
      <pubDate>Mon, 28 Feb 2022 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/course/ml-econ/07_trees/</guid>
      <description>&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;%matplotlib inline
from utils.lecture07 import *
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;decision-trees&#34;&gt;Decision Trees&lt;/h2&gt;
&lt;p&gt;Decision trees involve &lt;strong&gt;segmenting the predictor space into a number of simple regions&lt;/strong&gt;. In order to make a prediction for a given observation, we typically use the mean or the mode of the training observations in the region to which it belongs. Since the set of splitting rules used to segment the predictor space can be summarized in a tree, these types of approaches are known as decision tree methods.&lt;/p&gt;
&lt;h3 id=&#34;regression-trees&#34;&gt;Regression Trees&lt;/h3&gt;
&lt;p&gt;For this session we will consider the &lt;code&gt;Hitters&lt;/code&gt; dataset. It consists in individual level data of baseball players. In our applications, we are interested in predicting the players &lt;code&gt;Salary&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Load the data
hitters = pd.read_csv(&#39;data/Hitters.csv&#39;).dropna()
hitters.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;Unnamed: 0&lt;/th&gt;
      &lt;th&gt;AtBat&lt;/th&gt;
      &lt;th&gt;Hits&lt;/th&gt;
      &lt;th&gt;HmRun&lt;/th&gt;
      &lt;th&gt;Runs&lt;/th&gt;
      &lt;th&gt;RBI&lt;/th&gt;
      &lt;th&gt;Walks&lt;/th&gt;
      &lt;th&gt;Years&lt;/th&gt;
      &lt;th&gt;CAtBat&lt;/th&gt;
      &lt;th&gt;CHits&lt;/th&gt;
      &lt;th&gt;...&lt;/th&gt;
      &lt;th&gt;CRuns&lt;/th&gt;
      &lt;th&gt;CRBI&lt;/th&gt;
      &lt;th&gt;CWalks&lt;/th&gt;
      &lt;th&gt;League&lt;/th&gt;
      &lt;th&gt;Division&lt;/th&gt;
      &lt;th&gt;PutOuts&lt;/th&gt;
      &lt;th&gt;Assists&lt;/th&gt;
      &lt;th&gt;Errors&lt;/th&gt;
      &lt;th&gt;Salary&lt;/th&gt;
      &lt;th&gt;NewLeague&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;-Alan Ashby&lt;/td&gt;
      &lt;td&gt;315&lt;/td&gt;
      &lt;td&gt;81&lt;/td&gt;
      &lt;td&gt;7&lt;/td&gt;
      &lt;td&gt;24&lt;/td&gt;
      &lt;td&gt;38&lt;/td&gt;
      &lt;td&gt;39&lt;/td&gt;
      &lt;td&gt;14&lt;/td&gt;
      &lt;td&gt;3449&lt;/td&gt;
      &lt;td&gt;835&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;321&lt;/td&gt;
      &lt;td&gt;414&lt;/td&gt;
      &lt;td&gt;375&lt;/td&gt;
      &lt;td&gt;N&lt;/td&gt;
      &lt;td&gt;W&lt;/td&gt;
      &lt;td&gt;632&lt;/td&gt;
      &lt;td&gt;43&lt;/td&gt;
      &lt;td&gt;10&lt;/td&gt;
      &lt;td&gt;475.0&lt;/td&gt;
      &lt;td&gt;N&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;-Alvin Davis&lt;/td&gt;
      &lt;td&gt;479&lt;/td&gt;
      &lt;td&gt;130&lt;/td&gt;
      &lt;td&gt;18&lt;/td&gt;
      &lt;td&gt;66&lt;/td&gt;
      &lt;td&gt;72&lt;/td&gt;
      &lt;td&gt;76&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;1624&lt;/td&gt;
      &lt;td&gt;457&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;224&lt;/td&gt;
      &lt;td&gt;266&lt;/td&gt;
      &lt;td&gt;263&lt;/td&gt;
      &lt;td&gt;A&lt;/td&gt;
      &lt;td&gt;W&lt;/td&gt;
      &lt;td&gt;880&lt;/td&gt;
      &lt;td&gt;82&lt;/td&gt;
      &lt;td&gt;14&lt;/td&gt;
      &lt;td&gt;480.0&lt;/td&gt;
      &lt;td&gt;A&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;-Andre Dawson&lt;/td&gt;
      &lt;td&gt;496&lt;/td&gt;
      &lt;td&gt;141&lt;/td&gt;
      &lt;td&gt;20&lt;/td&gt;
      &lt;td&gt;65&lt;/td&gt;
      &lt;td&gt;78&lt;/td&gt;
      &lt;td&gt;37&lt;/td&gt;
      &lt;td&gt;11&lt;/td&gt;
      &lt;td&gt;5628&lt;/td&gt;
      &lt;td&gt;1575&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;828&lt;/td&gt;
      &lt;td&gt;838&lt;/td&gt;
      &lt;td&gt;354&lt;/td&gt;
      &lt;td&gt;N&lt;/td&gt;
      &lt;td&gt;E&lt;/td&gt;
      &lt;td&gt;200&lt;/td&gt;
      &lt;td&gt;11&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;500.0&lt;/td&gt;
      &lt;td&gt;N&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;-Andres Galarraga&lt;/td&gt;
      &lt;td&gt;321&lt;/td&gt;
      &lt;td&gt;87&lt;/td&gt;
      &lt;td&gt;10&lt;/td&gt;
      &lt;td&gt;39&lt;/td&gt;
      &lt;td&gt;42&lt;/td&gt;
      &lt;td&gt;30&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;396&lt;/td&gt;
      &lt;td&gt;101&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;48&lt;/td&gt;
      &lt;td&gt;46&lt;/td&gt;
      &lt;td&gt;33&lt;/td&gt;
      &lt;td&gt;N&lt;/td&gt;
      &lt;td&gt;E&lt;/td&gt;
      &lt;td&gt;805&lt;/td&gt;
      &lt;td&gt;40&lt;/td&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;91.5&lt;/td&gt;
      &lt;td&gt;N&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;5&lt;/th&gt;
      &lt;td&gt;-Alfredo Griffin&lt;/td&gt;
      &lt;td&gt;594&lt;/td&gt;
      &lt;td&gt;169&lt;/td&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;74&lt;/td&gt;
      &lt;td&gt;51&lt;/td&gt;
      &lt;td&gt;35&lt;/td&gt;
      &lt;td&gt;11&lt;/td&gt;
      &lt;td&gt;4408&lt;/td&gt;
      &lt;td&gt;1133&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;501&lt;/td&gt;
      &lt;td&gt;336&lt;/td&gt;
      &lt;td&gt;194&lt;/td&gt;
      &lt;td&gt;A&lt;/td&gt;
      &lt;td&gt;W&lt;/td&gt;
      &lt;td&gt;282&lt;/td&gt;
      &lt;td&gt;421&lt;/td&gt;
      &lt;td&gt;25&lt;/td&gt;
      &lt;td&gt;750.0&lt;/td&gt;
      &lt;td&gt;A&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;5 rows × 21 columns&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;In particular, we are interested in looking how the number of &lt;code&gt;Hits&lt;/code&gt; and the &lt;code&gt;Years&lt;/code&gt; of experience predict the &lt;code&gt;Salary&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Get Features
features = [&#39;Years&#39;, &#39;Hits&#39;]
X = hitters[features].values
y = np.log(hitters.Salary.values)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We are actually going to use log(salary) since it has a more gaussian distribution.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;fig, (ax1, ax2) = plt.subplots(1,2, figsize=(11,4))

# Plot salary distribution
ax1.hist(hitters.Salary.values)
ax1.set_xlabel(&#39;Salary&#39;)
ax2.hist(y)
ax2.set_xlabel(&#39;Log(Salary)&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/07_trees_10_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;In order to understand what is a tree, let&amp;rsquo;s first have a look at one. We fit a regression three with 3 leaves or, equivalently put, 2 nodes.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Fit regression tree
tree = DecisionTreeRegressor(max_leaf_nodes=3)
tree.fit(X, y)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;DecisionTreeRegressor(max_leaf_nodes=3)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We are now going to plot the results visually. The biggest avantage of trees is interpretability.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Figure 8.1
fig, ax = plt.subplots(1,1)
ax.set_title(&#39;Figure 8.1&#39;);

# Plot tree
plot_tree(tree, filled=True, feature_names=features, fontsize=14, ax=ax);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/07_trees_14_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;The tree consists of a series of splitting rules, starting at the top of the tree. The top split assigns observations having &lt;code&gt;Years&lt;/code&gt;&amp;lt;4.5 to the left branch.1 The predicted salary for these players is given by the mean response value for the players in the data set with &lt;code&gt;Years&lt;/code&gt;&amp;lt;4.5. For such players, the mean log salary is 5.107, and so we make a prediction of 5.107 thousands of dollars, i.e. $165,174, for these players. Players with &lt;code&gt;Years&lt;/code&gt;&amp;gt;=4.5 are assigned to the right branch, and then that group is further subdivided by &lt;code&gt;Hits&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Overall, the tree stratifies or segments the players into three regions of predictor space:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;players who have played for four or fewer years&lt;/li&gt;
&lt;li&gt;players who have played for five or more years and who made fewer than 118 hits last year, and&lt;/li&gt;
&lt;li&gt;players who have played for five or more years and who made at least 118 hits last year.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;These three regions can be written as&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;R1&lt;/strong&gt; = {X | &lt;code&gt;Years&lt;/code&gt;&amp;lt;4.5}&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;R2&lt;/strong&gt; = {X | &lt;code&gt;Years&lt;/code&gt;&amp;gt;=4.5, &lt;code&gt;Hits&lt;/code&gt;&amp;lt;117.5}, and&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;R3&lt;/strong&gt; = {X | &lt;code&gt;Years&lt;/code&gt;&amp;gt;=4.5, &lt;code&gt;Hits&lt;/code&gt;&amp;gt;=117.5}.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Since the dimension of $X$ is 2, we can visualize the space and the regions in a 2-dimensional graph.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Figure 8.2
def make_figure_8_2():
    
    # Init
    hitters.plot(&#39;Years&#39;, &#39;Hits&#39;, kind=&#39;scatter&#39;, color=&#39;orange&#39;, figsize=(7,6))
    plt.title(&#39;Figure 8.2&#39;)
    plt.xlim(0,25); plt.ylim(ymin=-5);
    plt.xticks([1, 4.5, 24]); plt.yticks([1, 117.5, 238]);

    # Split lines
    plt.vlines(4.5, ymin=-5, ymax=250, color=&#39;g&#39;)
    plt.hlines(117.5, xmin=4.5, xmax=25, color=&#39;g&#39;)

    # Regions
    plt.annotate(&#39;R1&#39;, xy=(2,117.5), fontsize=&#39;xx-large&#39;)
    plt.annotate(&#39;R2&#39;, xy=(11,60), fontsize=&#39;xx-large&#39;)
    plt.annotate(&#39;R3&#39;, xy=(11,170), fontsize=&#39;xx-large&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt; make_figure_8_2()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/07_trees_19_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;We might &lt;strong&gt;interpret&lt;/strong&gt; the above regression tree as follows: Years is the most important factor in determining Salary, and players with less experience earn lower salaries than more experienced players. Given that a player is less experienced, the number of hits that he made in the previous year seems to play little role in his salary. But among players who have been in the major leagues for five or more years, the number of hits made in the previous year does affect salary, and players who made more hits last year tend to have higher salaries.&lt;/p&gt;
&lt;h3 id=&#34;building-a-tree&#34;&gt;Building a Tree&lt;/h3&gt;
&lt;p&gt;There are two main steps in the construction of a tree:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;We divide the predictor space—that is, the set of possible values for $X_1, X_2, &amp;hellip; , X_p$ into $J$ distinct and non-overlapping regions, $R_1,R_2,&amp;hellip;,R_J$.&lt;/li&gt;
&lt;li&gt;For every observation that falls into the region $R_j$ , we make the same prediction, which is simply the mean of the response values for the training observations in $R_j$.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The second step is easy. But how does one construct the regions? Our purpose is to minimize the Sum of Squared Residuals, across the different regions:&lt;/p&gt;
&lt;p&gt;$$
\sum_{j=1}^{J} \sum_{i \in R_{j}}\left(y_{i}-\hat{y}&lt;em&gt;{R&lt;/em&gt;{j}}\right)^{2}
$$&lt;/p&gt;
&lt;p&gt;Unfortunately, it is computationally infeasible to consider every possible partition of the feature space into $J$ boxes.&lt;/p&gt;
&lt;p&gt;For this reason, we take a &lt;strong&gt;top-down&lt;/strong&gt;, &lt;strong&gt;greedy&lt;/strong&gt; approach that is known as &lt;em&gt;recursive binary splitting&lt;/em&gt;. The approach is top-down because it begins at the top of the tree (at which point all observations belong to a single region) and then successively splits the predictor space; each split is indicated via two new branches further down on the tree. It is greedy because at each step of the tree-building process, the best split is made at that particular step, rather than looking ahead and picking a split that will lead to a better tree in some future step.&lt;/p&gt;
&lt;p&gt;In practice, the method is the following:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;we select the predictor $X_j$&lt;/li&gt;
&lt;li&gt;we select the cutpoint $s$ such that splitting the predictor space into the regions ${X|X_j &amp;lt; s}$ and ${X|X_j \geq s}$ leads to the greatest possible reduction in RSS&lt;/li&gt;
&lt;li&gt;we repeat (1)-(2) for all predictors $X_1, &amp;hellip; , X_p$, i.e. we solve&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;$$
\arg \min_{j,s} \ \sum_{i: x_{i} \in {X|X_j &amp;lt; s}}\left(y_{i}-\hat{y}&lt;em&gt;i\right)^{2}+\sum&lt;/em&gt;{i: x_{i} \in {X|X_j \geq s}}\left(y_{i}-\hat{y}_i\right)^{2}
$$&lt;/p&gt;
&lt;ol start=&#34;4&#34;&gt;
&lt;li&gt;we choose the predictor and cutpoint such that the resulting tree has the lowest RSS&lt;/li&gt;
&lt;li&gt;we keep repeating (1)-(4) until a certain condition is met. However, after the first iteration we also have to pick which region to split which adds a further dimension to optimize over.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Let&amp;rsquo;s build our own &lt;code&gt;Node&lt;/code&gt; class to play around with trees.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;class Node:
    &amp;quot;&amp;quot;&amp;quot;
    Class used to represent nodes in a Regression Tree
    
    Attributes
    ----------
    x : np.array
        independent variables
    y : np.array
        dependent variables
    idxs : np.array
        indexes fo x and y for current node
    depth : int
        depth of the sub-tree (default 5)

    Methods
    -------
    find_next_nodes(self)
        Keep growing the tree
        
    find_best_split(self)
        Find the best split
        
    split(self)
        Split the tree
    &amp;quot;&amp;quot;&amp;quot;
    
    def __init__(self, x, y, idxs, depth=5):
        &amp;quot;&amp;quot;&amp;quot;Initialize node&amp;quot;&amp;quot;&amp;quot;
        self.x = x
        self.y = y
        self.idxs = idxs 
        self.depth = depth
        self.get_next_nodes()

    def get_next_nodes(self):
        &amp;quot;&amp;quot;&amp;quot;If the node is not terminal, get further splits&amp;quot;&amp;quot;&amp;quot;
        if self.is_last_leaf: return 
        self.find_best_split()       
        self.split()             
        
    def find_best_split(self):
        &amp;quot;&amp;quot;&amp;quot;Loop over variables and their values to find the best split&amp;quot;&amp;quot;&amp;quot;
        best_score = float(&#39;inf&#39;)
        # Loop over variables
        for col in range(self.x.shape[1]):
            x = self.x[self.idxs, col]
            # Loop over all splits
            for s in np.unique(x):
                lhs = x &amp;lt;= s
                rhs = x &amp;gt; s
                curr_score = self.get_score(lhs, rhs)
                # If best score, save it 
                if curr_score &amp;lt; best_score: 
                    best_score = curr_score
                    self.split_col = col
                    self.split_val = s
        return self
    
    def get_score(self, lhs, rhs):
        &amp;quot;&amp;quot;&amp;quot;Get score of a given split&amp;quot;&amp;quot;&amp;quot;
        y = self.y[self.idxs]
        lhs_mse = self.get_mse(y[lhs])
        rhs_mse = self.get_mse(y[rhs])
        return lhs_mse * lhs.sum() + rhs_mse * rhs.sum()
        
    def get_mse(self, y): return np.mean((y-np.mean(y))**2)
    
    def split(self): 
        &amp;quot;&amp;quot;&amp;quot;Split a node into 2 sub-nodes (recursive)&amp;quot;&amp;quot;&amp;quot;
        x = self.x[self.idxs, self.split_col]
        lhs = x &amp;lt;= self.split_val
        rhs = x &amp;gt; self.split_val
        self.lhs = Node(self.x, self.y, self.idxs[lhs], self.depth-1)
        self.rhs = Node(self.x, self.y, self.idxs[rhs], self.depth-1)
        to_print = (self.depth, self.split_col, self.split_val, sum(lhs), sum(rhs))
        print(&#39;Split on layer %.0f: var%1.0f = %.4f (%.0f/%.0f)&#39; % to_print)
        return self
    
    @property
    def is_last_leaf(self): return self.depth&amp;lt;=1

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What does a &lt;code&gt;Node&lt;/code&gt; look like?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Init first node
tree1 = Node(X, y, np.arange(len(y)), 1)

# Documentation (always comment and document your code!)
print(tree1.__doc__)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;    Class used to represent nodes in a Regression Tree
    
    Attributes
    ----------
    x : np.array
        independent variables
    y : np.array
        dependent variables
    idxs : np.array
        indexes fo x and y for current node
    depth : int
        depth of the sub-tree (default 5)

    Methods
    -------
    find_next_nodes(self)
        Keep growing the tree
        
    find_best_split(self)
        Find the best split
        
    split(self)
        Split the tree
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Which properties does it have?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Inspect the class
dir(tree1)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[&#39;__class__&#39;,
 &#39;__delattr__&#39;,
 &#39;__dict__&#39;,
 &#39;__dir__&#39;,
 &#39;__doc__&#39;,
 &#39;__eq__&#39;,
 &#39;__format__&#39;,
 &#39;__ge__&#39;,
 &#39;__getattribute__&#39;,
 &#39;__gt__&#39;,
 &#39;__hash__&#39;,
 &#39;__init__&#39;,
 &#39;__init_subclass__&#39;,
 &#39;__le__&#39;,
 &#39;__lt__&#39;,
 &#39;__module__&#39;,
 &#39;__ne__&#39;,
 &#39;__new__&#39;,
 &#39;__reduce__&#39;,
 &#39;__reduce_ex__&#39;,
 &#39;__repr__&#39;,
 &#39;__setattr__&#39;,
 &#39;__sizeof__&#39;,
 &#39;__str__&#39;,
 &#39;__subclasshook__&#39;,
 &#39;__weakref__&#39;,
 &#39;depth&#39;,
 &#39;find_best_split&#39;,
 &#39;get_mse&#39;,
 &#39;get_next_nodes&#39;,
 &#39;get_score&#39;,
 &#39;idxs&#39;,
 &#39;is_last_leaf&#39;,
 &#39;split&#39;,
 &#39;x&#39;,
 &#39;y&#39;]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What is the depth? How many observations are there?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Get info
print(&#39;Tree of depth %.0f with %.0f observations&#39; % (tree1.depth, len(tree1.idxs)))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Tree of depth 1 with 263 observations
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Fair enough, the tree is just a single leaf.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Check if terminal
tree1.is_last_leaf
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;True
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s find the first split.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Find best split
tree1.find_best_split()
print(&#39;Split at var%1.0f = %.4f&#39; % (tree1.split_col, tree1.split_val))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Split at var0 = 4.0000
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If has selected the first variable, at the value $4$.&lt;/p&gt;
&lt;p&gt;If we call the &lt;code&gt;split&lt;/code&gt; function, it will also tell us how many observations per leaf the split generates.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Split tree
tree1.split();
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Split on layer 1: var0 = 4.0000 (90/173)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we are ready to compute even deeper trees&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Check depth-3 tree
tree3 = Node(X, y, np.arange(len(y)), 3)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Split on layer 2: var1 = 4.0000 (2/88)
Split on layer 2: var1 = 117.0000 (90/83)
Split on layer 3: var0 = 4.0000 (90/173)
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;pruning&#34;&gt;Pruning&lt;/h3&gt;
&lt;p&gt;The process described above may produce good predictions on the training set, but is likely to &lt;strong&gt;overfit&lt;/strong&gt; the data, leading to poor test set performance. This is because the resulting tree might be too complex. A smaller tree with fewer splits might lead to lower variance and better interpretation at the cost of a little bias.&lt;/p&gt;
&lt;p&gt;We can see it happening if we build the same tree as above, but with 5 leaves.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Compute tree
overfit_tree = DecisionTreeRegressor(max_leaf_nodes=5).fit(X, y)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We plot the 5-leaf tree.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Plot tree
fig, ax = plt.subplots(1,1)
plot_tree(overfit_tree, filled=True, feature_names=features, fontsize=14, ax=ax);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/07_trees_45_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;split on the far left&lt;/strong&gt; is predicting a very high &lt;code&gt;Salary&lt;/code&gt; (7.243) for players with few &lt;code&gt;Years&lt;/code&gt; of experience and few &lt;code&gt;Hits&lt;/code&gt;. Indeed this prediction is based on an extremely tiny subsample (2). They are probably outliers and our tree is most likely overfitting.&lt;/p&gt;
&lt;p&gt;One possible alternative is to insert a minimum number of observation per leaf.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Compute tree
no_overfit_tree = DecisionTreeRegressor(max_leaf_nodes=5, min_samples_leaf=10).fit(X, y)

# Plot tree
fig, ax = plt.subplots(1,1)
plot_tree(no_overfit_tree, filled=True, feature_names=features, fontsize=14, ax=ax);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/07_trees_48_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Now the tree makes much more sense: the lower the &lt;code&gt;Years&lt;/code&gt; and the &lt;code&gt;Hits&lt;/code&gt;, the lower the predicted &lt;code&gt;Salary&lt;/code&gt; as we can see from the shades getting darker and darker as we move left to right&lt;/p&gt;
&lt;p&gt;Another possible alternative to the process described above is to build the tree only so long as the decrease in the RSS due to each split exceeds some (high) threshold.&lt;/p&gt;
&lt;p&gt;This strategy will result in smaller trees, but is too short-sighted since a seemingly worthless split early on in the tree might be followed by a very good split—that is, a split that leads to a large reduction in RSS later on.&lt;/p&gt;
&lt;p&gt;We can use cross-validation to pick the optimal tree length.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Import original split
features = [&#39;Years&#39;, &#39;Hits&#39;, &#39;RBI&#39;, &#39;PutOuts&#39;, &#39;Walks&#39;, &#39;Runs&#39;, &#39;AtBat&#39;, &#39;HmRun&#39;]
X_train = pd.read_csv(&#39;data/Hitters_X_train.csv&#39;).dropna()[features]
X_test = pd.read_csv(&#39;data/Hitters_X_test.csv&#39;).dropna()[features]
y_train = pd.read_csv(&#39;data/Hitters_y_train.csv&#39;).dropna()
y_test = pd.read_csv(&#39;data/Hitters_y_test.csv&#39;).dropna()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Init
params = range(2,11)
reg_scores = np.zeros((len(params),3))
best_score = 10**6

# Loop over all parameters
for i,k in enumerate(params):
    
    # Model
    tree = DecisionTreeRegressor(max_leaf_nodes=k)

    # Loop over splits
    tree.fit(X_train, y_train)
    reg_scores[i,0] = mean_squared_error(tree.predict(X_train), y_train)
    reg_scores[i,1] = mean_squared_error(tree.predict(X_test), y_test)

    # Get CV score
    kf6 = KFold(n_splits=6)
    reg_scores[i,2] = -cross_val_score(tree, X_train, y_train, cv=kf6, scoring=&#39;neg_mean_squared_error&#39;).mean()
    
    # Save best model
    if reg_scores[i,2]&amp;lt;best_score:
        best_model = tree
        best_score = reg_scores[i,2]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s plot the optimal tree depth, using 6-fold cv.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Figure 8.5
def make_figure_8_5():
    
    # Init
    fig, (ax1,ax2) = plt.subplots(1,2,figsize=(16,6))
    fig.suptitle(&#39;Figure 8.5&#39;)

    # Plot scores
    ax1.plot(params, reg_scores);
    ax1.axvline(params[np.argmin(reg_scores[:,2])], c=&#39;k&#39;, ls=&#39;--&#39;)
    ax1.legend([&#39;Train&#39;,&#39;Test&#39;,&#39;6-fold CV&#39;]);
    ax1.set_title(&#39;Cross-Validation Scores&#39;);

    # Plot best tree
    plot_tree(best_model, filled=True, impurity=False, feature_names=features, fontsize=12, ax=ax2);
    ax2.set_title(&#39;Best Model&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;make_figure_8_5()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/07_trees_55_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;The optimal tree has 4 leaves.&lt;/p&gt;
&lt;h3 id=&#34;classification-trees&#34;&gt;Classification Trees&lt;/h3&gt;
&lt;p&gt;A classification tree is very similar to a regression tree, except that it is used to predict a qualitative response rather than a quantitative one.&lt;/p&gt;
&lt;p&gt;For a classification tree, we predict that each observation belongs to the most commonly occurring class of training observations in the region to which it belongs.&lt;/p&gt;
&lt;h3 id=&#34;building-a-classification-tree&#34;&gt;Building a Classification Tree&lt;/h3&gt;
&lt;p&gt;The task of growing a classification tree is similar to the task of growing a regression tree. However, in the classification setting, RSS cannot be used as a criterion for making the binary splits.&lt;/p&gt;
&lt;p&gt;We define $\hat p_{mk}$ as the proportion of training observations in the $m^{th}$ region that are from the $k^{th}$ class. Possible loss functions to decide the splits are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Classification error rate&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$
E = 1 - \max &lt;em&gt;{k}\left(\hat{p}&lt;/em&gt;{m k}\right)
$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Gini index&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$
G=\sum_{k=1}^{K} \hat{p}&lt;em&gt;{m k}\left(1-\hat{p}&lt;/em&gt;{m k}\right)
$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Entropy&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$
D=-\sum_{k=1}^{K} \hat{p}&lt;em&gt;{m k} \log \hat{p}&lt;/em&gt;{m k}
$$&lt;/p&gt;
&lt;p&gt;In 2-class classification problems, this is what the different scores look like, for different proportions of class 2 ($p$), when the true proportion is $p_0 =0.5$.&lt;/p&gt;
&lt;img src=&#34;figures/impurity.png&#34; alt=&#34;Drawing&#34; style=&#34;width: 600px;&#34;/&gt;
&lt;p&gt;When building a classification tree, either the Gini index or the entropy are typically used to evaluate the quality of a particular split, since these two approaches are more sensitive to node purity than is the classification error rate.&lt;/p&gt;
&lt;p&gt;For this section we will work with the &lt;code&gt;Heart&lt;/code&gt; dataset on individual heart failures. We will try to use individual characteristics in order to predict heart deseases (&lt;code&gt;HD&lt;/code&gt;). The varaible is binary: Yes, No.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Load heart dataset
heart = pd.read_csv(&#39;data/Heart.csv&#39;).drop(&#39;Unnamed: 0&#39;, axis=1).dropna()
heart.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;Age&lt;/th&gt;
      &lt;th&gt;Sex&lt;/th&gt;
      &lt;th&gt;ChestPain&lt;/th&gt;
      &lt;th&gt;RestBP&lt;/th&gt;
      &lt;th&gt;Chol&lt;/th&gt;
      &lt;th&gt;Fbs&lt;/th&gt;
      &lt;th&gt;RestECG&lt;/th&gt;
      &lt;th&gt;MaxHR&lt;/th&gt;
      &lt;th&gt;ExAng&lt;/th&gt;
      &lt;th&gt;Oldpeak&lt;/th&gt;
      &lt;th&gt;Slope&lt;/th&gt;
      &lt;th&gt;Ca&lt;/th&gt;
      &lt;th&gt;Thal&lt;/th&gt;
      &lt;th&gt;AHD&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;63&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;typical&lt;/td&gt;
      &lt;td&gt;145&lt;/td&gt;
      &lt;td&gt;233&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;150&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;2.3&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;fixed&lt;/td&gt;
      &lt;td&gt;No&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;67&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;asymptomatic&lt;/td&gt;
      &lt;td&gt;160&lt;/td&gt;
      &lt;td&gt;286&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;108&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1.5&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;3.0&lt;/td&gt;
      &lt;td&gt;normal&lt;/td&gt;
      &lt;td&gt;Yes&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;67&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;asymptomatic&lt;/td&gt;
      &lt;td&gt;120&lt;/td&gt;
      &lt;td&gt;229&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;129&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;2.6&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;2.0&lt;/td&gt;
      &lt;td&gt;reversable&lt;/td&gt;
      &lt;td&gt;Yes&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;37&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;nonanginal&lt;/td&gt;
      &lt;td&gt;130&lt;/td&gt;
      &lt;td&gt;250&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;187&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;3.5&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;normal&lt;/td&gt;
      &lt;td&gt;No&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;41&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;nontypical&lt;/td&gt;
      &lt;td&gt;130&lt;/td&gt;
      &lt;td&gt;204&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;172&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1.4&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;normal&lt;/td&gt;
      &lt;td&gt;No&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Fastorize variables
heart.ChestPain = pd.factorize(heart.ChestPain)[0]
heart.Thal = pd.factorize(heart.Thal)[0]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Set features
features = [col for col in heart.columns if col!=&#39;AHD&#39;]
X2 = heart[features]
y2 = pd.factorize(heart.AHD)[0]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We now fit our classifier.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Fit classification tree
clf = DecisionTreeClassifier(max_depth=None, max_leaf_nodes=11)
clf.fit(X2,y2)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;DecisionTreeClassifier(max_leaf_nodes=11)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What is the score?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Final score
clf.score(X2,y2)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;0.8686868686868687
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s have a look at the whole tree.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Figure 8.6 a
def make_fig_8_6a():
    
    # Init
    fig, ax = plt.subplots(1,1, figsize=(16,12))
    ax.set_title(&#39;Figure 8.6&#39;);

    # Plot tree
    plot_tree(clf, filled=True, feature_names=features, class_names=[&#39;No&#39;,&#39;Yes&#39;], fontsize=12, ax=ax);
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;make_fig_8_6a()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/07_trees_72_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;This figure has a surprising characteristic: some of the splits yield two terminal nodes that have the same predicted value.&lt;/p&gt;
&lt;p&gt;For instance, consider the split &lt;code&gt;Age&lt;/code&gt;&amp;lt;=57.5 near the bottom left of the unpruned tree. Regardless of the value of &lt;code&gt;Age&lt;/code&gt;, a response value of &lt;em&gt;No&lt;/em&gt; is predicted for those observations. Why, then, is the split performed at all?&lt;/p&gt;
&lt;p&gt;The split is performed because it leads to increased node purity. That is, 2/81 of the observations corresponding to the left-hand leaf have a response value of &lt;em&gt;Yes&lt;/em&gt;, whereas 9/36 of those corresponding to the right-hand leaf have a response value of &lt;em&gt;Yes&lt;/em&gt;. Why is node purity important? Suppose that we have a test observation that belongs to the region given by that left-hand leaf. Then we can be pretty certain that its response value is &lt;em&gt;No&lt;/em&gt;. In contrast, if a test observation belongs to the region given by the right-hand leaf, then its response value is probably &lt;em&gt;No&lt;/em&gt;, but we are much less certain. Even though the split &lt;code&gt;Age&lt;/code&gt;&amp;lt;=57.5 does not reduce the classification error, it improves the Gini index and the entropy, which are more sensitive to node purity.&lt;/p&gt;
&lt;h3 id=&#34;pruning-for-classification&#34;&gt;Pruning for Classification&lt;/h3&gt;
&lt;p&gt;We can repeat the pruning exercise also for the classification task.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Figure 8.6 b
def make_figure_8_6b():
    
    # Init
    fig, (ax1, ax2) = plt.subplots(1,2,figsize=(14,6))
    fig.suptitle(&#39;Figure 8.6&#39;)

    # Plot scores
    ax1.plot(params, clf_scores);
    ax1.legend([&#39;Train&#39;,&#39;Test&#39;,&#39;6-fold CV&#39;]);

    # Plot best tree
    plot_tree(best_model, filled=True, impurity=False, feature_names=features, fontsize=12, ax=ax2);
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Init
J = 10
params = range(2,11)
clf_scores = np.zeros((len(params),3))
best_score = 100

# Loop over all parameters
for i,k in enumerate(params):
    
    # Model
    tree = DecisionTreeClassifier(max_leaf_nodes=k)
    
    # Loop J times
    temp_scores = np.zeros((J,3))
    for j in range (J):

        # Loop over splits
        X2_train, X2_test, y2_train, y2_test = train_test_split(X2, y2, test_size=0.5, random_state=j)
        m = tree.fit(X2_train, y2_train)
        temp_scores[j,0] = mean_squared_error(m.predict(X2_train), y2_train)
        temp_scores[j,1] = mean_squared_error(m.predict(X2_test), y2_test)

        # Get CV score
        kf6 = KFold(n_splits=6)
        temp_scores[j,2] = -cross_val_score(tree, X2_train, y2_train, cv=kf6, scoring=&#39;neg_mean_squared_error&#39;).mean()
        
        # Save best model
        if temp_scores[j,2]&amp;lt;best_score:
            best_model = m
            best_score = temp_scores[j,2]
        
    # Average
    clf_scores[i,:] = np.mean(temp_scores, axis=0)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;make_figure_8_6b()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/07_trees_78_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;other-issues&#34;&gt;Other Issues&lt;/h3&gt;
&lt;h4 id=&#34;missing-predictor-values&#34;&gt;Missing Predictor Values&lt;/h4&gt;
&lt;p&gt;There are usually 2 main ways to deal with missing values:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;discard the observations&lt;/li&gt;
&lt;li&gt;fill the missing values with predictions using the other observations (e.g. mean)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;With trees we can do better:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;code them as a separate class (e.g. &amp;lsquo;missing&amp;rsquo;)&lt;/li&gt;
&lt;li&gt;generate splits using non-missing data and use non-missing variables on missing data to mimic the splits with missing data&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;categorical-predictors&#34;&gt;Categorical Predictors&lt;/h4&gt;
&lt;p&gt;When splitting a predictor having q possible unordered values, there are $2^{q−1} − 1$ possible partitions of the q values into two groups, and the computations become prohibitive for large $q$. However, with a $0 − 1$ outcome, this computation simplifies.&lt;/p&gt;
&lt;h4 id=&#34;linear-combination-splits&#34;&gt;Linear Combination Splits&lt;/h4&gt;
&lt;p&gt;Rather than restricting splits to be of the form $X_j \leq s$, one can allow splits along linear combinations of the form $a_j X_j \leq s$. The weights $a_j$ become part of the optimization procedure.&lt;/p&gt;
&lt;h4 id=&#34;other-tree-building-procedures&#34;&gt;Other Tree-Building Procedures&lt;/h4&gt;
&lt;p&gt;The procedure we have seen for building trees is called CART (Classification and Regression Tree). There are other procedures.&lt;/p&gt;
&lt;h4 id=&#34;the-loss-matrix&#34;&gt;The Loss Matrix&lt;/h4&gt;
&lt;p&gt;With respect to other methods, the choice of the loss functions plays a much more important role.&lt;/p&gt;
&lt;h4 id=&#34;binary-splits&#34;&gt;Binary Splits&lt;/h4&gt;
&lt;p&gt;You can do non-binary splits but in the end they are just weaker versions of binary splits.&lt;/p&gt;
&lt;h4 id=&#34;instability&#34;&gt;Instability&lt;/h4&gt;
&lt;p&gt;Trees have very &lt;strong&gt;high variance&lt;/strong&gt;.&lt;/p&gt;
&lt;h4 id=&#34;difficulty-in-capturing-additive-structure&#34;&gt;Difficulty in Capturing Additive Structure&lt;/h4&gt;
&lt;p&gt;Trees are quite bad at modeling additive structures.&lt;/p&gt;
&lt;h4 id=&#34;lack-of-smoothness&#34;&gt;Lack of Smoothness&lt;/h4&gt;
&lt;p&gt;Trees are not smooth.&lt;/p&gt;
&lt;h3 id=&#34;trees-vs-regression&#34;&gt;Trees vs Regression&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Advantages&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Trees are very easy to explain to people. In fact, they are even easier to explain than linear regression!&lt;/li&gt;
&lt;li&gt;Some people believe that decision trees more closely mirror human decision-making than do the regression and classification approaches seen in previous chapters.&lt;/li&gt;
&lt;li&gt;Trees can be displayed graphically, and are easily interpreted even by a non-expert (especially if they are small).&lt;/li&gt;
&lt;li&gt;Trees can easily handle qualitative predictors without the need to create dummy variables.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Disadvantages&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;trees generally do not have the same level of predictive accuracy as some of the other regression and classification approaches seen in this book.&lt;/li&gt;
&lt;li&gt;trees can be very non-robust. In other words, a small change in the data can cause a large change in the final estimated tree.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;72-bagging-random-forests-boosting&#34;&gt;7.2 Bagging, Random Forests, Boosting&lt;/h2&gt;
&lt;p&gt;Bagging, random forests, and boosting use trees as building blocks to construct more powerful prediction models.&lt;/p&gt;
&lt;h3 id=&#34;bagging&#34;&gt;Bagging&lt;/h3&gt;
&lt;p&gt;The main problem of decision trees is that they suffer from &lt;strong&gt;high variance&lt;/strong&gt;. &lt;em&gt;Bootstrap aggregation&lt;/em&gt;, or &lt;em&gt;bagging&lt;/em&gt;, is a general-purpose procedure for reducing the variance of a statistical learning method.&lt;/p&gt;
&lt;p&gt;The main idea behind &lt;em&gt;bagging&lt;/em&gt; is that, given a set of n independent observations $Z_1,&amp;hellip;,Z_n$, each with variance $\sigma^2$, the variance of the mean $\bar Z$ of the observations is given by $\sigma^2/n$. In other words, averaging a set of observations reduces variance.&lt;/p&gt;
&lt;p&gt;Indeed &lt;em&gt;bagging&lt;/em&gt; consists in taking many training sets from the population, build a separate prediction model using each training set, and &lt;strong&gt;average the resulting predictions&lt;/strong&gt;. Since we do not have access to many training sets, we resort to bootstrapping.&lt;/p&gt;
&lt;h3 id=&#34;out-of-bag-error-estimation&#34;&gt;Out-of-Bag Error Estimation&lt;/h3&gt;
&lt;p&gt;It turns out that there is a very straightforward way to estimate the test error of a bagged model, without the need to perform cross-validation or the validation set approach. Recall that the key to bagging is that trees are repeatedly fit to bootstrapped subsets of the observations. One can show that on average, each bagged tree makes use of around two-thirds of the observations. The remaining one-third of the observations not used to fit a given bagged tree are referred to as the out-of-bag (OOB) observations. We can predict the response for the ith observation using each of the trees in which that observation was OOB.&lt;/p&gt;
&lt;p&gt;We are now going to compute the Gini index for the &lt;code&gt;Heart&lt;/code&gt; dataset using different numbers of trees.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Init (takes a lot of time with J=30)
params = range(2,50)
bagging_scores = np.zeros((len(params),2))
J = 30;

# Loop over parameters
for i, k in enumerate(params):
    print(&amp;quot;Computing k=%1.0f&amp;quot; % k, end =&amp;quot;&amp;quot;)
    
    # Repeat J 
    temp_scores = np.zeros((J,2))
    for j in range(J):
        X2_train, X2_test, y2_train, y2_test = train_test_split(X2, y2, test_size=0.5, random_state=j)
        bagging = BaggingClassifier(DecisionTreeClassifier(), max_samples=k, oob_score=True)
        bagging.fit(X2_train,y2_train)
        temp_scores[j,0] = bagging.score(X2_test, y2_test)
        temp_scores[j,1] = bagging.oob_score_
        
    # Average
    bagging_scores[i,:] = np.mean(temp_scores, axis=0)
    print(&amp;quot;&amp;quot;, end=&amp;quot;\r&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Computing k=49
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s plot the Out-of-Bag error computed while generating the bagged estimator.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Make new figure 1
def make_new_figure_1():
    
    # Init
    fig, ax = plt.subplots(1,1,figsize=(10,6))
    fig.suptitle(&amp;quot;Estimated $R^2$&amp;quot;)

    # Plot scores
    ax.plot(params, bagging_scores);
    ax.legend([&#39;Test&#39;,&#39;OOB&#39;]);
    ax.set_xlabel(&#39;Number of Trees&#39;); ax.set_ylabel(&#39;R^2&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;make_new_figure_1()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/07_trees_94_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;It can be shown that with B sufficiently large, OOB error is virtually equivalent to leave-one-out cross-validation error. The OOB approach for estimating the test error is particularly convenient when performing bagging on large data sets for which cross-validation would be computationally onerous.&lt;/p&gt;
&lt;h3 id=&#34;variable-importance-measures&#34;&gt;Variable Importance Measures&lt;/h3&gt;
&lt;p&gt;As we have discussed, the main advantage of bagging is to reduce prediction variance. However, with bagging it can be &lt;strong&gt;difficult to interpret&lt;/strong&gt; the resulting model. In fact we cannot draw trees anymore given we have too many of them.&lt;/p&gt;
&lt;p&gt;However, one can obtain an overall summary of the importance of each predictor using the RSS (for bagging regression trees) or the Gini index (for bagging classification trees). In the case of bagging regression trees, we can record the total amount that the RSS is decreased due to splits over a given predictor, averaged over all trees. A large value indicates an important predictor. Similarly, in the context of bagging classification trees, we can add up the total amount that the Gini index is decreased by splits over a given predictor, averaged over all trees.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Compute feature importance
feature_importances = np.mean([tree.feature_importances_ for tree in bagging.estimators_], axis=0)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can have a look at the importance of each feature.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Figure 8.9
def make_figure_8_9():
    
    # Init
    fig, ax = plt.subplots(1,1,figsize=(8,8))
    ax.set_title(&#39;Figure 8.9: Feature Importance&#39;);

    # Plot feature importance
    h1 = pd.DataFrame({&#39;Importance&#39;:feature_importances*100}, index=features)
    h1 = h1.sort_values(by=&#39;Importance&#39;, axis=0, ascending=False)
    h1.plot(kind=&#39;barh&#39;, color=&#39;r&#39;, ax=ax)
    ax.set_xlabel(&#39;Variable Importance&#39;); 
    plt.yticks(fontsize=14);
    plt.gca().legend_ = None;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;make_figure_8_9()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/07_trees_101_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;random-forests&#34;&gt;Random Forests&lt;/h3&gt;
&lt;p&gt;Random forests provide an improvement over bagged trees by way of a &lt;strong&gt;small tweak that decorrelates the trees&lt;/strong&gt;. As in bagging, we build a number of decision trees on bootstrapped training samples. But when building these decision trees, each time a split in a tree is considered, a random sample of $m$ predictors is chosen as split candidates from the full set of $p$ predictors. The split is allowed to use only one of those m predictors. A fresh sample of $m$ predictors is taken at each split, and typically we choose $m \sim \sqrt{p}$ — that is, the number of predictors considered at each split is approximately equal to the square root of the total number of predictors&lt;/p&gt;
&lt;p&gt;In other words, in building a random forest, at each split in the tree, the algorithm is not even allowed to consider a majority of the available predictors. This may sound crazy, but it has a clever rationale. Suppose that there is one very strong predictor in the data set, along with a number of other moderately strong predictors. Then in the collection of bagged trees, most or all of the trees will use this strong predictor in the top split. Consequently, all of the bagged trees will look quite similar to each other. Hence the predictions from the bagged trees will be highly correlated. Unfortunately, averaging many highly correlated quantities does not lead to as large of a reduction in variance as averaging many uncorrelated quantities. In particular, this means that bagging will not lead to a substantial reduction in variance over a single tree in this setting.&lt;/p&gt;
&lt;p&gt;Random forests overcome this problem by &lt;strong&gt;forcing each split to consider only a subset of the predictors&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s split the data in 2 and compute test and estimated $R^2$, for both forest and trees.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import warnings
warnings.simplefilter(&#39;ignore&#39;)

# Init (takes a lot of time with J=30)
params = range(2,50)
forest_scores = np.zeros((len(params),2))
J = 30

# Loop over parameters
for i, k in enumerate(params):
    print(&amp;quot;Computing k=%1.0f&amp;quot; % k, end =&amp;quot;&amp;quot;)
    
    # Repeat J 
    temp_scores = np.zeros((J,2))
    for j in range(J):
        X2_train, X2_test, y2_train, y2_test = train_test_split(X2, y2, test_size=0.5, random_state=j)
        forest = RandomForestClassifier(n_estimators=k, oob_score=True, max_features=&amp;quot;sqrt&amp;quot;)
        forest.fit(X2_train,y2_train)
        temp_scores[j,0] = forest.score(X2_test, y2_test)
        temp_scores[j,1] = forest.oob_score_
        
    # Average
    forest_scores[i,:] = np.mean(temp_scores, axis=0)
    print(&amp;quot;&amp;quot;, end=&amp;quot;\r&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Computing k=49
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Figure 8.8
def make_figure_8_8():
    
    # Init
    fig, ax = plt.subplots(1,1,figsize=(10,6))
    ax.set_title(&#39;Figure 8.8&#39;);

    # Plot scores
    ax.plot(params, bagging_scores);
    ax.plot(params, forest_scores);
    ax.legend([&#39;Test - Bagging&#39;,&#39;OOB - Bagging&#39;, &#39;Test - Forest&#39;,&#39;OOB - Forest&#39;]);
    ax.set_xlabel(&#39;Number of Trees&#39;); ax.set_ylabel(&#39;R^2&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;make_figure_8_8()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/07_trees_108_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;As for bagging, we can plot feature importance.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Make new figure 2
def make_new_figure_2():
    
    # Init
    fig, (ax1,ax2) = plt.subplots(1,2,figsize=(15,7))

    # Plot feature importance - Bagging
    h1 = pd.DataFrame({&#39;Importance&#39;:feature_importances*100}, index=features)
    h1 = h1.sort_values(by=&#39;Importance&#39;, axis=0, ascending=False)
    h1.plot(kind=&#39;barh&#39;, color=&#39;r&#39;, ax=ax1)
    ax1.set_xlabel(&#39;Variable Importance&#39;); 
    ax1.set_title(&#39;Tree Bagging&#39;)

    # Plot feature importance
    h2 = pd.DataFrame({&#39;Importance&#39;:forest.feature_importances_*100}, index=features)
    h2 = h2.sort_values(by=&#39;Importance&#39;, axis=0, ascending=False)
    h2.plot(kind=&#39;barh&#39;, color=&#39;r&#39;, ax=ax2)
    ax2.set_title(&#39;Random Forest&#39;)

    # All plots
    for ax in fig.axes:
        ax.set_xlabel(&#39;Variable Importance&#39;); 
        ax.legend([])
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;make_new_figure_2()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/07_trees_111_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;From the figure we observe that varaible importance ranking is similar with bagging and random forests, but there are significant differences.&lt;/p&gt;
&lt;p&gt;We are now going to look at the importance of random forests using the &lt;code&gt;Khan&lt;/code&gt; gene dataset. This dataset has the peculiarity of having a large number of features and very few observations.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Load data
gene = pd.read_csv(&#39;data/Khan.csv&#39;)
print(len(gene))
gene.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;83
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;x&lt;/th&gt;
      &lt;th&gt;V1&lt;/th&gt;
      &lt;th&gt;V2&lt;/th&gt;
      &lt;th&gt;V3&lt;/th&gt;
      &lt;th&gt;V4&lt;/th&gt;
      &lt;th&gt;V5&lt;/th&gt;
      &lt;th&gt;V6&lt;/th&gt;
      &lt;th&gt;V7&lt;/th&gt;
      &lt;th&gt;V8&lt;/th&gt;
      &lt;th&gt;V9&lt;/th&gt;
      &lt;th&gt;...&lt;/th&gt;
      &lt;th&gt;V2299&lt;/th&gt;
      &lt;th&gt;V2300&lt;/th&gt;
      &lt;th&gt;V2301&lt;/th&gt;
      &lt;th&gt;V2302&lt;/th&gt;
      &lt;th&gt;V2303&lt;/th&gt;
      &lt;th&gt;V2304&lt;/th&gt;
      &lt;th&gt;V2305&lt;/th&gt;
      &lt;th&gt;V2306&lt;/th&gt;
      &lt;th&gt;V2307&lt;/th&gt;
      &lt;th&gt;V2308&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;0.773344&lt;/td&gt;
      &lt;td&gt;-2.438405&lt;/td&gt;
      &lt;td&gt;-0.482562&lt;/td&gt;
      &lt;td&gt;-2.721135&lt;/td&gt;
      &lt;td&gt;-1.217058&lt;/td&gt;
      &lt;td&gt;0.827809&lt;/td&gt;
      &lt;td&gt;1.342604&lt;/td&gt;
      &lt;td&gt;0.057042&lt;/td&gt;
      &lt;td&gt;0.133569&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;-0.238511&lt;/td&gt;
      &lt;td&gt;-0.027474&lt;/td&gt;
      &lt;td&gt;-1.660205&lt;/td&gt;
      &lt;td&gt;0.588231&lt;/td&gt;
      &lt;td&gt;-0.463624&lt;/td&gt;
      &lt;td&gt;-3.952845&lt;/td&gt;
      &lt;td&gt;-5.496768&lt;/td&gt;
      &lt;td&gt;-1.414282&lt;/td&gt;
      &lt;td&gt;-0.647600&lt;/td&gt;
      &lt;td&gt;-1.763172&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;-0.078178&lt;/td&gt;
      &lt;td&gt;-2.415754&lt;/td&gt;
      &lt;td&gt;0.412772&lt;/td&gt;
      &lt;td&gt;-2.825146&lt;/td&gt;
      &lt;td&gt;-0.626236&lt;/td&gt;
      &lt;td&gt;0.054488&lt;/td&gt;
      &lt;td&gt;1.429498&lt;/td&gt;
      &lt;td&gt;-0.120249&lt;/td&gt;
      &lt;td&gt;0.456792&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;-0.657394&lt;/td&gt;
      &lt;td&gt;-0.246284&lt;/td&gt;
      &lt;td&gt;-0.836325&lt;/td&gt;
      &lt;td&gt;-0.571284&lt;/td&gt;
      &lt;td&gt;0.034788&lt;/td&gt;
      &lt;td&gt;-2.478130&lt;/td&gt;
      &lt;td&gt;-3.661264&lt;/td&gt;
      &lt;td&gt;-1.093923&lt;/td&gt;
      &lt;td&gt;-1.209320&lt;/td&gt;
      &lt;td&gt;-0.824395&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;-0.084469&lt;/td&gt;
      &lt;td&gt;-1.649739&lt;/td&gt;
      &lt;td&gt;-0.241308&lt;/td&gt;
      &lt;td&gt;-2.875286&lt;/td&gt;
      &lt;td&gt;-0.889405&lt;/td&gt;
      &lt;td&gt;-0.027474&lt;/td&gt;
      &lt;td&gt;1.159300&lt;/td&gt;
      &lt;td&gt;0.015676&lt;/td&gt;
      &lt;td&gt;0.191942&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;-0.696352&lt;/td&gt;
      &lt;td&gt;0.024985&lt;/td&gt;
      &lt;td&gt;-1.059872&lt;/td&gt;
      &lt;td&gt;-0.403767&lt;/td&gt;
      &lt;td&gt;-0.678653&lt;/td&gt;
      &lt;td&gt;-2.939352&lt;/td&gt;
      &lt;td&gt;-2.736450&lt;/td&gt;
      &lt;td&gt;-1.965399&lt;/td&gt;
      &lt;td&gt;-0.805868&lt;/td&gt;
      &lt;td&gt;-1.139434&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;0.965614&lt;/td&gt;
      &lt;td&gt;-2.380547&lt;/td&gt;
      &lt;td&gt;0.625297&lt;/td&gt;
      &lt;td&gt;-1.741256&lt;/td&gt;
      &lt;td&gt;-0.845366&lt;/td&gt;
      &lt;td&gt;0.949687&lt;/td&gt;
      &lt;td&gt;1.093801&lt;/td&gt;
      &lt;td&gt;0.819736&lt;/td&gt;
      &lt;td&gt;-0.284620&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;0.259746&lt;/td&gt;
      &lt;td&gt;0.357115&lt;/td&gt;
      &lt;td&gt;-1.893128&lt;/td&gt;
      &lt;td&gt;0.255107&lt;/td&gt;
      &lt;td&gt;0.163309&lt;/td&gt;
      &lt;td&gt;-1.021929&lt;/td&gt;
      &lt;td&gt;-2.077843&lt;/td&gt;
      &lt;td&gt;-1.127629&lt;/td&gt;
      &lt;td&gt;0.331531&lt;/td&gt;
      &lt;td&gt;-2.179483&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;0.075664&lt;/td&gt;
      &lt;td&gt;-1.728785&lt;/td&gt;
      &lt;td&gt;0.852626&lt;/td&gt;
      &lt;td&gt;0.272695&lt;/td&gt;
      &lt;td&gt;-1.841370&lt;/td&gt;
      &lt;td&gt;0.327936&lt;/td&gt;
      &lt;td&gt;1.251219&lt;/td&gt;
      &lt;td&gt;0.771450&lt;/td&gt;
      &lt;td&gt;0.030917&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;-0.200404&lt;/td&gt;
      &lt;td&gt;0.061753&lt;/td&gt;
      &lt;td&gt;-2.273998&lt;/td&gt;
      &lt;td&gt;-0.039365&lt;/td&gt;
      &lt;td&gt;0.368801&lt;/td&gt;
      &lt;td&gt;-2.566551&lt;/td&gt;
      &lt;td&gt;-1.675044&lt;/td&gt;
      &lt;td&gt;-1.082050&lt;/td&gt;
      &lt;td&gt;-0.965218&lt;/td&gt;
      &lt;td&gt;-1.836966&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;5 rows × 2309 columns&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;The dataset has 83 rows and 2309 columns.&lt;/p&gt;
&lt;p&gt;Since it&amp;rsquo;s a very &lt;em&gt;wide&lt;/em&gt; dataset, selecting the right features is crucial.&lt;/p&gt;
&lt;p&gt;Also note that we cannot run linear regression on this dataset.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Reduce dataset size
gene_small = gene.iloc[:,0:202]
X = gene_small.iloc[:,1:]
y = gene_small.iloc[:,0]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s now cross-validate over number of trees and maximum number of features considered.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Init (takes a lot of time with J=30)
params = range(50,150,10)
m_scores = np.zeros((len(params),3))
p = np.shape(X)[1]
J = 30;

# Loop over parameters
for i, k in enumerate(params):
    
    # Array of features
    ms = [round(p/2), round(np.sqrt(p)), round(np.log(p))]
    
    # Repeat L times
    temp_scores = np.zeros((J,3))
    for j in range(J):
        print(&amp;quot;Computing k=%1.0f (iter=%1.0f)&amp;quot; % (k,j+1), end =&amp;quot;&amp;quot;)
    
        # Loop over values of m
        for index, m in enumerate(ms):
            forest = RandomForestClassifier(n_estimators=k, max_features=m, oob_score=True)
            forest.fit(X, y)
            temp_scores[j,index] = forest.oob_score_
        print(&amp;quot;&amp;quot;, end=&amp;quot;\r&amp;quot;)
            
    # Average
    m_scores[i,:] = np.mean(temp_scores, axis=0)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Computing k=140 (iter=30)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Figure 8.10
def make_figure_8_10():
    
    # Init
    fig, ax = plt.subplots(1,1,figsize=(10,6))
    ax.set_title(&#39;Figure 8.10&#39;);

    # Plot scores
    ax.plot(params, m_scores);
    ax.legend([&#39;m=p/2&#39;,&#39;m=sqrt(p)&#39;,&#39;m=log(p)&#39;]);
    ax.set_xlabel(&#39;Number of Trees&#39;); ax.set_ylabel(&#39;Test Classification Accuracy&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt; make_figure_8_10()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/07_trees_120_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;It seems that the best scores are achieved with few features and many trees.&lt;/p&gt;
&lt;h3 id=&#34;boosting&#34;&gt;Boosting&lt;/h3&gt;
&lt;p&gt;Like bagging, boosting is a general approach that can be applied to many statistical learning methods for regression or classification. Here we restrict our discussion of boosting to the context of decision trees.&lt;/p&gt;
&lt;p&gt;Boosting works similarly to bagging, except that the trees are grown sequentially: each tree is grown using information from previously grown trees. Boosting does not involve bootstrap sampling; instead each tree is fit on a modified version of the original data set.&lt;/p&gt;
&lt;p&gt;What is the idea behind this procedure? Given the current model, we fit a decision tree to the residuals from the model. That is, &lt;strong&gt;we fit a tree using the current residuals&lt;/strong&gt;, rather than the outcome $y$, as the response. We then add this new decision tree into the fitted function in order to update the residuals. Each of these trees can be rather small, with just a few terminal nodes, determined by the parameter $d$ in the algorithm. By fitting small trees to the residuals, &lt;strong&gt;we slowly improve $\hat f$ in areas where it does not perform well&lt;/strong&gt;. The shrinkage parameter λ slows the process down even further, allowing more and different shaped trees to attack the resid- uals. In general, statistical learning approaches that learn slowly tend to perform well.&lt;/p&gt;
&lt;h3 id=&#34;algorithm&#34;&gt;Algorithm&lt;/h3&gt;
&lt;p&gt;The boosting algorithm works as follows:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Set $\hat f(x)=0$ and $r_i=y_i$ for all $i$ in the training set.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;For $b=1,2,&amp;hellip;,B$ repeat:&lt;/p&gt;
&lt;p&gt;a. Fit a tree $\hat f^b $ with $d$ splits ($d+1$ terminal nodes) to the training data $(X,r)$.&lt;/p&gt;
&lt;p&gt;b. Update $\hat f$ by adding in a shrunken version of the new tree:
$$
\hat f(x) \leftarrow \hat f(x) + \lambda \hat f^b(x)
$$&lt;/p&gt;
&lt;p&gt;c. Update the residuals
$$
r_i = r_i - \lambda \hat f^b(x_i)
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Output the boosted model
$$
\hat{f}(x)=\sum_{b=1}^{B} \lambda \hat{f}^{b}(x)
$$&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Boosting has three tuning parameters:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;The &lt;strong&gt;number of trees&lt;/strong&gt; $B$&lt;/li&gt;
&lt;li&gt;The &lt;strong&gt;shrinkage parameter&lt;/strong&gt; $\lambda$. This controls the rate at which boosting learns.&lt;/li&gt;
&lt;li&gt;The &lt;strong&gt;number of splits in each tree&lt;/strong&gt; $d$ , which controls the complexity of the boosted ensemble. Often d = 1 works well, in which case each tree is a stump, consisting of a single split.&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Init , oob_score=True
params = range(50,150,10)
boost_scores = np.zeros((len(params),3))
p = np.shape(X)[1]
J = 30

# Loop over parameters
for i, k in enumerate(params):
    
    # Repeat L times
    temp_scores = np.zeros((J,3))
    for j in range(J):
        print(&amp;quot;Computing k=%1.0f (iter=%1.0f)&amp;quot; % (k,j+1), end =&amp;quot;&amp;quot;)
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=20, random_state=j)
    
        # First score: random forest
        forest = RandomForestClassifier(n_estimators=k, max_features=&amp;quot;sqrt&amp;quot;)
        forest.fit(X_train, y_train)
        temp_scores[j,0] = forest.score(X_test, y_test)

        # Second score: boosting with 1-split trees
        boost1 = GradientBoostingClassifier(learning_rate=0.01, max_depth=1, n_estimators=k, max_features=&amp;quot;sqrt&amp;quot;)
        boost1.fit(X_train, y_train)
        temp_scores[j,1] = boost1.score(X_test, y_test)

        # Third score: boosting with 1-split trees
        boost2 = GradientBoostingClassifier(learning_rate=0.01, max_depth=2, n_estimators=k, max_features=&amp;quot;sqrt&amp;quot;)
        boost2.fit(X_train, y_train)
        temp_scores[j,2] = boost2.score(X_test, y_test)
        print(&amp;quot;&amp;quot;, end=&amp;quot;\r&amp;quot;)
    
    # Average
    boost_scores[i,:] = np.mean(temp_scores, axis=0)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Computing k=140 (iter=30)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s compare boosting and forest.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Figure 8.11
def make_figure_8_11():
    
    # Init
    fig, ax = plt.subplots(1,1,figsize=(10,6))
    ax.set_title(&#39;Figure 8.11&#39;);

    # Plot scores
    ax.plot(params, m_scores);
    ax.legend([&#39;forest&#39;,&#39;boosting with d=1&#39;,&#39;boosting with d=2&#39;]);
    ax.set_xlabel(&#39;Number of Trees&#39;); ax.set_ylabel(&#39;Test Classification Accuracy&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;make_figure_8_11()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/07_trees_131_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Endogeneity</title>
      <link>https://matteocourthoud.github.io/course/metrics/07_endogeneity/</link>
      <pubDate>Fri, 29 Oct 2021 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/course/metrics/07_endogeneity/</guid>
      <description>&lt;h2 id=&#34;instrumental-variables&#34;&gt;Instrumental Variables&lt;/h2&gt;
&lt;h3 id=&#34;endogeneity&#34;&gt;Endogeneity&lt;/h3&gt;
&lt;p&gt;We say that there is &lt;strong&gt;endogeneity&lt;/strong&gt; in the linear regression model if
$\mathbb E[x_i \varepsilon_i] \neq 0$.&lt;/p&gt;
&lt;p&gt;The random vector $z_i$ is an &lt;strong&gt;instrumental variable&lt;/strong&gt; in the linear
regression model if the following conditions are met.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Exclusion restriction&lt;/strong&gt;: the instruments are uncorrelated with the
regression error $$
\mathbb E_n[z_i \varepsilon_i] = 0
$$ almost surely, i.e. with probability $p \to 1$.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Rank condition&lt;/strong&gt;: no linearly redundant instruments $$
\mathbb E_n[z_i z_i&#39;] \neq 0
$$ almost surely, i.e. with probability $p \to 1$.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Relevance condition&lt;/strong&gt; (need $L &amp;gt; K$): $$
rank \ (\mathbb E_n[z_i x_i&#39;]) = K
$$ almost surely, i.e. with probability $p \to 1$.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;iv-and-2sls&#34;&gt;IV and 2SLS&lt;/h3&gt;
&lt;p&gt;Let $K = dim(x_i)$ and $L = dim(z_i)$. We say that the model is
&lt;strong&gt;just-identified&lt;/strong&gt; if $L = K$ (method: IV) and &lt;strong&gt;over-identified&lt;/strong&gt; if
$L &amp;gt; K$ (method: 2SLS).&lt;/p&gt;
&lt;p&gt;Assume $z_i$ satisfies the instrumental variable assumptions above and
$dim(z_i) = dim(x_i)$, then the &lt;strong&gt;instrumental variables (IV)&lt;/strong&gt;
estimator $\hat{\beta} _ {IV}$ is given by $$
\begin{aligned}
\hat{\beta} _ {IV} &amp;amp;= \mathbb E_n[z_i x_i&#39;]^{-1} \mathbb E_n[z_i y_i] = \newline
&amp;amp;= \left( \frac{1}{n} \sum _ {i=1}^n z_i x_i\right)^{-1} \left( \frac{1}{n} \sum _ {i=1}^n z_i y_i\right) = \newline
&amp;amp;= (Z&amp;rsquo;X)^{-1} (Z&amp;rsquo;y)
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;Assume $z_i$ satisfies the instrumental variable assumptions above and
$dim(z_i) &amp;gt; dim(x_i)$, then the &lt;strong&gt;two-stage-least squares (2SLS)&lt;/strong&gt;
estimator $\hat{\beta} _ {2SLS}$ is given by $$
\hat{\beta} _ {2SLS} =  \Big( X&amp;rsquo;Z (Z&amp;rsquo;Z)^{-1} Z&amp;rsquo;X \Big)^{-1} \Big( X&amp;rsquo;Z (Z&amp;rsquo;Z)^{-1} Z&amp;rsquo;y \Big)
$$ Where $\hat{x}_i$ is the predicted $x_i$ from the &lt;strong&gt;first stage&lt;/strong&gt;
regression of $x_i$ on $z_i$. This is equivalent to the IV estimator
using $\hat{x}_i$ as an instrument for $x_i$.&lt;/p&gt;
&lt;h3 id=&#34;2sls-algebra&#34;&gt;2SLS Algebra&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;The estimator is called &lt;strong&gt;two-stage-least squares&lt;/strong&gt; since it can be
rewritten as an IV estimator that uses $\hat{X}$ as instrument: $$
\begin{aligned}
\hat{\beta} _ {\text{2SLS}} &amp;amp;= \Big( X&amp;rsquo;Z (Z&amp;rsquo;Z)^{-1} Z&amp;rsquo;X \Big)^{-1} \Big( X&amp;rsquo;Z (Z&amp;rsquo;Z)^{-1} Z&amp;rsquo;y \Big) = \newline
&amp;amp;= (\hat{X}&#39; X)^{-1} \hat{X}&#39; y = \newline
&amp;amp;= \mathbb E_n[\hat{x}_i x_i&#39;]^{-1} \mathbb E_n[\hat{x}_i y_i]
\end{aligned}
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Moreover it can be rewritten as $$
\begin{aligned}
\hat{\beta} _ {\text{2SLS}} &amp;amp;= (\hat{X}&#39; X)^{-1} \hat{X}&#39; y = \newline
&amp;amp;= (X&#39; P_Z X)^{-1} X&#39; P_Z y = \newline
&amp;amp;= (X&#39; P_Z P_Z X)^{-1} X&#39; P_Z y = \newline
&amp;amp;= (\hat{X}&#39; \hat{X})^{-1} \hat{X}&#39; y = \newline
&amp;amp;= \mathbb E_n [\hat{x}_i \hat{x}_i]^{-1} \mathbb E_n[\hat{x}_i y_i]
\end{aligned}
$$&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;rule-of-thumb&#34;&gt;Rule of Thumb&lt;/h3&gt;
&lt;p&gt;How to the test the relevance condition? Rule of thumb: $F$-test in the
first stage $&amp;gt;10$ (joint test on $z_i$).&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Problem&lt;/strong&gt;: as $n \to \infty$, with finite $L$, $F \to \infty$ (bad
rule of thumb).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;equivalence&#34;&gt;Equivalence&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;If $K=L$, $\hat{\beta} _ {\text{2SLS}} = \hat{\beta} _ {\text{IV}}$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;If $K=L$, $X&amp;rsquo;Z$ and $Z&amp;rsquo;X$ are squared matrices and, by the relevance
condition, non-singular (invertible). $$
\begin{aligned}
\hat{\beta} _ {\text{2SLS}} &amp;amp;= \Big( X&amp;rsquo;Z (Z&amp;rsquo;Z)^{-1} Z&amp;rsquo;X \Big)^{-1} \Big( X&amp;rsquo;Z (Z&amp;rsquo;Z)^{-1} Z&amp;rsquo;y \Big) = \newline
&amp;amp;= (Z&amp;rsquo;X)^{-1} (Z&amp;rsquo;Z) (X&amp;rsquo;Z)^{-1} X&amp;rsquo;Z (Z&amp;rsquo;Z)^{-1} Z&amp;rsquo;y = \newline
&amp;amp;= (Z&amp;rsquo;X)^{-1} (Z&amp;rsquo;Z) (Z&amp;rsquo;Z)^{-1} Z&amp;rsquo;y = \newline
&amp;amp;= (Z&amp;rsquo;X)^{-1} (Z&amp;rsquo;y) = \newline
&amp;amp;= \hat{\beta} _ {\text{IV}}
\end{aligned}
$$ $$\tag*{$\blacksquare$}$$&lt;/p&gt;
&lt;h3 id=&#34;demand-example&#34;&gt;Demand Example&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Example&lt;/strong&gt; from Hayiashi (2000) page 187: demand and supply
simultaneous equations. $$
\begin{aligned}
&amp;amp; q_i^D(p_i) = \alpha_0 + \alpha_1 p_i + u_i \newline
&amp;amp; q_i^S(p_i) = \beta_0 + \beta_1 p_i + v_i
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;We have an endogeneity problem. To see why, we solve the system of
equations for $(p_i, q_i)$: $$
\begin{aligned}
&amp;amp; p_i = \frac{\beta_0 - \alpha_0}{\alpha_1 - \beta_1} + \frac{v_i - u_i}{\alpha_1 - \beta_1 } \newline
&amp;amp; q_i = \frac{\alpha_1\beta_0 - \alpha_0 \beta_1}{\alpha_1 - \beta_1} + \frac{\alpha_1 v_i - \beta_1 u_i}{\alpha_1 - \beta_1 }
\end{aligned}
$$&lt;/p&gt;
&lt;h3 id=&#34;demand-example-2&#34;&gt;Demand Example (2)&lt;/h3&gt;
&lt;p&gt;Then the price variable is not independent from the error term in
neither equation: $$
\begin{aligned}
&amp;amp; Cov(p_i, u_i) = - \frac{Var(u_i)}{\alpha_1 - \beta_1 } \newline
&amp;amp; Cov(p_i, v_i) = \frac{Var(v_i)}{\alpha_1 - \beta_1 }
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;As a consequence, the OLS estimators are not consistent: $$
\begin{aligned}
&amp;amp; \hat{\alpha} _ {1, OLS} \overset{p}{\to} \alpha_1 + \frac{Cov(p_i, u_i)}{Var(p_i)} \newline
&amp;amp; \hat{\beta} _ {1, OLS} \overset{p}{\to} \beta_1 + \frac{Cov(p_i, v_i)}{Var(p_i)}
\end{aligned}
$$&lt;/p&gt;
&lt;h3 id=&#34;demand-example-3&#34;&gt;Demand Example (3)&lt;/h3&gt;
&lt;p&gt;In general, running regressing $q$ on $p$ you estimate $$
\begin{aligned}
\hat{\gamma} _ {OLS} &amp;amp;\overset{p}{\to} \frac{Cov(p_i, q_i)}{Var(p_i)} = \newline
&amp;amp;= \frac{\alpha_1 Var(v_i) + \beta_1 Var(u_i)}{(\alpha_1 - \beta_1)^2} \left( \frac{Var(v_i) + Var(u_i)}{(\alpha_1 - \beta_1)^2} \right)^{-1} = \newline
&amp;amp;= \frac{\alpha_1 Var(v_i) + \beta_1 Var(u_i)}{Var(v_i) + Var(u_i)}
\end{aligned}
$$ Which is neither $\alpha_1$ nor $\beta_1$ but a variance weighted
average of the two.&lt;/p&gt;
&lt;h3 id=&#34;demand-example-4&#34;&gt;Demand Example (4)&lt;/h3&gt;
&lt;p&gt;Suppose we have a supply shifter $z_i$ such that&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\mathbb E[z_i v_i] \neq 0$&lt;/li&gt;
&lt;li&gt;$\mathbb E[z_i u_i] = 0$.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We combine the second condition and $\mathbb E[u_i] = 0$ to get a system
of 2 equations in 2 unknowns: $\alpha_0$ and $\alpha_1$. $$
\begin{aligned}
&amp;amp; \mathbb E[z_i u_i] = \mathbb E[ z_i (q_i^D(p_i) - \alpha_0 - \alpha_1 p_i) ] = 0 \newline
&amp;amp; \mathbb E[u_i] = \mathbb E[q_i^D(p_i) - \alpha_0 - \alpha_1 p_i] = 0&lt;br&gt;
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;We could try to solve for the vector $\alpha$ that solves $$
\begin{aligned}
&amp;amp; \mathbb E_n[z_i (q_i^D - x_i\alpha)] = 0 \newline
&amp;amp; \mathbb E_n[z_i q_i^D] -  \mathbb E_n[z_ix_i\alpha] = 0
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;If $\mathbb E_n[z_ix_i]$ is invertible, we get
$\hat{\alpha} = \mathbb E_n[z_ix_i]^{-1} \mathbb E_n[z_i q^D_i]$ which
is indeed the IV estimator of $\alpha$ using $z_i$ as an instrument for
the endogenous variable $p_i$.&lt;/p&gt;
&lt;h3 id=&#34;code---dgp&#34;&gt;Code - DGP&lt;/h3&gt;
&lt;p&gt;This code draws 100 observations from the model
$y = 2 x_1 - x_2 + \varepsilon$ where $x_1, x_2 \sim U[0,1]$ and
$\varepsilon \sim N(0,1)$.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# Set seed
Random.seed!(123);

# Set the number of observations
n = 100;

# Set the dimension of Z
l = 3;

# Draw instruments
Z = rand(Uniform(0,1), n, l);

# Correlation matrix for error terms
S = [1 0.8; 0.8 1];

# Endogenous X
γ = [2 0; 0 -1; -1 3];
ε = rand(Normal(0,1), n, 2) * cholesky(S).U;
X = Z*γ .+ ε[:,1];

# Calculate y
y = X*β .+ ε[:,2];
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;code---iv&#34;&gt;Code - IV&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# Estimate beta OLS
β_OLS = (X&#39;*X)\(X&#39;*y)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 2-element Array{Float64,1}:
##   2.335699233358403
##  -0.8576266209987325
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# IV: l=k=2 instruments
Z_IV = Z[:,1:k];
β_IV = (Z_IV&#39;*X)\(Z_IV&#39;*y)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 2-element Array{Float64,1}:
##   1.6133344277861439
##  -0.6678537395714547
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# Calculate standard errors
ε_hat = y - X*β_IV;
V_NHC_IV = var(ε_hat) * inv(Z_IV&#39;*X)*Z_IV&#39;*Z_IV*inv(Z_IV&#39;*X);
V_HC0_IV = inv(Z_IV&#39;*X)*Z_IV&#39; * (I(n) .* ε_hat.^2) * Z_IV*inv(Z_IV&#39;*X);
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;code---2sls&#34;&gt;Code - 2SLS&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# 2SLS: l=3 instruments
Pz = Z*inv(Z&#39;*Z)*Z&#39;;
β_2SLS = (X&#39;*Pz*X)\(X&#39;*Pz*y)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 2-element Array{Float64,1}:
##   1.904553638377971
##  -0.8810907510370429
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# Calculate standard errors
ε_hat = y - X*β_2SLS;
V_NCH_2SLS = var(ε_hat) * inv(X&#39;*Pz*X);
V_HC0_2SLS = inv(X&#39;*Pz*X)*X&#39;*Pz * (I(n) .* ε_hat.^2) *Pz*X*inv(X&#39;*Pz*X);
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;gmm&#34;&gt;GMM&lt;/h2&gt;
&lt;h3 id=&#34;setting&#34;&gt;Setting&lt;/h3&gt;
&lt;p&gt;We have a system of $L$ moment conditions $$
\begin{aligned}
&amp;amp; \mathbb E[g_1(\omega_i, \delta_0)] = 0 \newline
&amp;amp; \vdots \newline
&amp;amp; \mathbb E[g_L(\omega_i, \delta_0)] = 0
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;If $L = \dim (\delta_0)$, no problem. If $L &amp;gt; \dim (\delta_0)$, there
may be no solution to the system of equations.&lt;/p&gt;
&lt;h3 id=&#34;options&#34;&gt;Options&lt;/h3&gt;
&lt;p&gt;There are two possibilities.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;First Solution&lt;/strong&gt;: add moment conditions until the system is
identified $$
\mathbb E[ a&#39; g(\omega_i, \delta_0)] = 0
$$ Solve $\mathbb E[Ag(\omega_i, \delta)] = 0$ for $\hat{\delta}$.
How to choose $A$? Such that it minimizes $Var(\hat{\delta})$.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Second Solution&lt;/strong&gt;: generalized method of moments (GMM) $$
\begin{aligned}
\hat{\delta} _ {GMM} &amp;amp;= \arg \min _ \delta \quad  \Big| \Big| \mathbb E_n [ g(\omega_i, \delta) ] \Big| \Big| = \newline
&amp;amp;= \arg \min _ \delta \quad n \mathbb E_n[g(\omega_i, \delta)]&#39; W \mathbb E_n [g(\omega_i, \delta)]
\end{aligned}
$$&lt;/li&gt;
&lt;/ol&gt;
&lt;blockquote&gt;
&lt;p&gt;The choice of $A$ and $W$ are closely related to each other.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;1-step-gmm&#34;&gt;1-step GMM&lt;/h3&gt;
&lt;p&gt;Since $J(\delta,W)$ is a quadratic form, a closed form solution exists:
$$
\hat{\delta}(W) = \Big(\mathbb E_n[z_i x_i&#39;] W \mathbb E_n[z_i x_i&#39;] \Big)^{-1}\mathbb E_n[z_i x_i&#39;] W \mathbb E_n[z_i y_i]
$$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Assumptions&lt;/strong&gt; for consistency of the GMM estimator given data
$\mathcal D = \lbrace y_i, x_i, z_i \rbrace _ {i=1}^n$:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Linearity&lt;/strong&gt;: $y_i = x_i\gamma_0 + \varepsilon_i$&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;IID&lt;/strong&gt;: $(y_i, x_i, z_i)$ iid&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Orthogonality&lt;/strong&gt;:
$\mathbb E [z_i(y_i - x_i\gamma_0)] = \mathbb E[z_i \varepsilon_i] = 0$&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Rank identification&lt;/strong&gt;: $\Sigma_{xz} = \mathbb E[z_i x_i&#39;]$ has
full rank&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;convergence&#34;&gt;Convergence&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Under linearity, independence, orthogonality and rank conditions, if
$\hat{W} \overset{p}{\to} W$ positive definite, then $$
\hat{\delta}(\hat{W}) \to \delta(W)
$$ If in addition to the above assumption,
$\sqrt{n} \mathbb E_n [g(\omega_i, \delta_0)] \overset{d}{\to} N(0,S)$
for a fixed positive definite $S$, then $$
\sqrt{n} (\hat{\delta} (\hat{W}) - \delta(W)) \overset{d}{\to} N(0,V)
$$ where
$V = (\Sigma&#39; _ {xz} W \Sigma _ {xz})^{-1} \Sigma _ {xz} W S W \Sigma _ {xz}(\Sigma&#39; _ {xz} W \Sigma _ {xz})^{-1}$.&lt;/p&gt;
&lt;p&gt;Finally, if a consistent estimator $\hat{S}$ of $S$ is available, then
using sample analogues $\hat{\Sigma}_{xz}$ it follows that $$
\hat{V} \overset{p}{\to} V
$$&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;If $W = S^{-1}$ then $V$ reduces to
$V = (\Sigma&#39; _ {xz} W \Sigma _ {xz})^{-1}$. Moreover,
$(\Sigma&#39; _ {xz} W \Sigma _ {xz})^{-1}$ is the smallest possible form
of $V$, in a positive definite sense.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Therefore, to have an efficient estimator, you want to construct
$\hat{W}$ such that $\hat{W} \overset{p}{\to} S^{-1}$.&lt;/p&gt;
&lt;h3 id=&#34;2-step-gmm&#34;&gt;2-step GMM&lt;/h3&gt;
&lt;p&gt;Estimation steps:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Choose an arbitrary weighting matrix $\hat{W}_{init}$ (usually the
identity matrix $I_K$)&lt;/li&gt;
&lt;li&gt;Estimate $\hat{\delta} _ {init}(\hat{W} _ {init})$&lt;/li&gt;
&lt;li&gt;Estimate $\hat{S}$ (asymptotic variance of the moment condition)&lt;/li&gt;
&lt;li&gt;Estimate $\hat{\delta}(\hat{S}^{-1})$&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;On the procedure:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;This estimator achieves the semiparametric efficiency bound.&lt;/li&gt;
&lt;li&gt;This strategy works only if $\hat{S} \overset{p}{\to} S$ exists.&lt;/li&gt;
&lt;li&gt;For iid cases: we can use
$\hat{\delta} = \mathbb E_n[(\hat{\varepsilon}_i z_i)(\hat{\varepsilon}_i z_i) &#39; ]$
where
$\hat{\varepsilon}_i = y_i - x_i \hat{\delta}(\hat{W} _ {init})$.&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;code---1-step-gmm&#34;&gt;Code - 1-step GMM&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# GMM 1-step: inefficient weighting matrix
W_1 = I(l);

# Objective function
gmm_1(b) = ( y - X*b )&#39; * Z * W_1 *  Z&#39; * ( y - X*b );

# Estimate GMM
β_gmm_1 = optimize(gmm_1, β_OLS).minimizer
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 2-element Array{Float64,1}:
##   1.91556882526808
##  -0.8769689391885799
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# Standard errors GMM
ε_hat = y - X*β_gmm_1;
S_hat = Z&#39; * (I(n) .* ε_hat.^2) * Z;
d_hat = -X&#39;*Z;
V_gmm_1 = inv(d_hat * inv(S_hat) * d_hat&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 2×2 Array{Float64,2}:
##   0.0158497   -0.00346601
##  -0.00346601   0.00616531
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;code---2-step-gmm&#34;&gt;Code - 2-step GMM&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# GMM 2-step: efficient weighting matrix
W_2 = inv(S_hat);

# Objective function
gmm_2(b) = ( y - X*b )&#39; * Z * W_2 *  Z&#39; * ( y - X*b );

# Estimate GMM
β_gmm_2 = optimize(gmm_2, β_OLS).minimizer
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 2-element Array{Float64,1}:
##   1.905326742963115
##  -0.881808949213345
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# Standard errors GMM
ε_hat = y - X*β_gmm_2;
S_hat = Z&#39; * (I(n) .* ε_hat.^2) * Z;
d_hat = -X&#39;*Z;
V_gmm_2 = inv(d_hat * inv(S_hat) * d_hat&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 2×2 Array{Float64,2}:
##   0.0162603   -0.00357632
##  -0.00357632   0.00631259
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;testing-overidentifying-restrictions&#34;&gt;Testing Overidentifying Restrictions&lt;/h3&gt;
&lt;p&gt;If the equations are &lt;strong&gt;exactly identified&lt;/strong&gt;, then it is possible to
choose $\delta$ so that all the elements of the sample moments
$\mathbb E_n[g(\omega_i; \delta)]$ are zero and thus that the distance
$$
J(\delta, \hat{W}) = n \mathbb E_n[g(\omega_i, \delta)]&#39; \hat{W} \mathbb E_n[g(\omega_i, \delta)]
$$ is zero. (The $\delta$ that does it is the IV estimator.)&lt;/p&gt;
&lt;p&gt;If the equations are &lt;strong&gt;overidentified&lt;/strong&gt;, i.e. $L$ (number of
instruments) $&amp;gt; K$ (number of equations), then the distance cannot be
zero exactly in general, but we would expect the minimized distance to
be &lt;em&gt;close&lt;/em&gt; to zero.&lt;/p&gt;
&lt;h3 id=&#34;naive-test&#34;&gt;Naive Test&lt;/h3&gt;
&lt;p&gt;Suppose your model is overidentified ($L &amp;gt; K$) and you use the following
naive testing procedure:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Estimate $\hat{\delta}$ using a subset of dimension $K$ of
instruments $\lbrace z_1 , .. , z_K\rbrace$ for
$\lbrace x_1 , &amp;hellip; , x_K\rbrace$&lt;/li&gt;
&lt;li&gt;Set $\hat{\varepsilon}_i = y_i - x_i \hat{\delta} _ {\text{GMM}}$&lt;/li&gt;
&lt;li&gt;Infer the size of the remaining $L-K$ moment conditions
$\mathbb E[z _{i, K+1} \varepsilon_i], &amp;hellip;, \mathbb E[z _{i, L} \varepsilon_i]$
looking at their empirical counterparts
$\mathbb E_n[z _{i, K+1} \hat{\varepsilon}_i], &amp;hellip;, \mathbb E_n[z _{i, L} \hat{\varepsilon}_i]$&lt;/li&gt;
&lt;li&gt;Reject exogeneity if the empirical expectations are high. How high?
Calculate p-values.&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;example&#34;&gt;Example&lt;/h3&gt;
&lt;p&gt;If you have two invalid instruments and you use one to test the validity
of the other, it might happen by chance that you don’t reject it.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Model: $y_i = x_i + \varepsilon_i$ and
$x_i = \frac{1}{2} z _{i1} - \frac{1}{2} z _{i2} + u_i$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Have $$
Cov (z _{i1}, z _{i2}, \varepsilon_i, u_i) =
\begin{bmatrix}
1 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 \newline 0 &amp;amp; 1 &amp;amp; 0 &amp;amp; 0 \newline 0 &amp;amp; 0 &amp;amp; 1 &amp;amp; 0.5 \newline 0 &amp;amp; 0 &amp;amp; 0.5 &amp;amp; 1
\end{bmatrix}
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;You want to test whether the second instrument is valid (is not
since $\mathbb E[z_2 \varepsilon] \neq 0$). You use $z_1$ and
estimate $\hat{\beta} \to$ the estimator is consistent.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;You obtain $\mathbb E_n[z _{i2} \hat{\varepsilon}_i] \simeq 0$ even
if $z_2$ is invalid&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Problem: you are using an invalid instrument in the first place.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;hansens-test&#34;&gt;Hansen’s Test&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt;: We are interested in testing
$H_0: \mathbb E[z_i \varepsilon_i] = 0$ against
$H_1: \mathbb E[z_i \varepsilon_i] \neq 0$. Suppose
$\hat{S} \overset{p}{\to} S$. Then $$
J(\hat{\delta}(\hat{S}^{-1}) , \hat{S}^{-1}) \overset{d}{\to} \chi^2 _ {L-K}
$$ For $c$ satisfying $\alpha = 1- G_{L - K} ( c )$,
$\Pr(J&amp;gt;c | H_0) \to \alpha$ so the test &lt;em&gt;reject $H_0$ if $J &amp;gt; c$&lt;/em&gt; has
asymptotic size $\alpha$.&lt;/p&gt;
&lt;h3 id=&#34;comments&#34;&gt;Comments&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;The degrees of freedom of the asymptotic distribution are the number
of overidentifying restrictions.&lt;/li&gt;
&lt;li&gt;This is a specification test, testing whether all model assumptions
are true jointly. Only when we are confident that about the other
assumptions, can we interpret a large $J$ statistic as evidence for
the endogeneity of some of the $L$ instruments included in $x$.&lt;/li&gt;
&lt;li&gt;Unlike the tests we have encountered so far, the test is not
consistent against some failures of the orthogonality conditions
(that is, it is not consistent against some fixed elements of the
alternative).&lt;/li&gt;
&lt;li&gt;Several papers in the July 1996 issue of JBES report that the
finite-sample null rejection probability of the test can far exceed
the nominal significance level $\alpha$.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;special-case-conditional-homoskedasticity&#34;&gt;Special Case: Conditional Homoskedasticity&lt;/h3&gt;
&lt;p&gt;The main implication of conditional homoskedasticity is that efficient
GMM becomes 2SLS. With efficient GMM estimation, the weighting matrix is
$\hat{S}^{-1} = \mathbb En [z_i z_i&#39; \varepsilon_i^2]^{-1}$. With
conditional homoskedasticity, the efficient weighting matrix is
$\mathbb E_n[z_iz_i&#39;]^{-1} \sigma^{-2}$, or equivalently
$\mathbb E_n[z_iz_i&#39;]^{-1}$. Then, the GMM estimator becomes $$
\hat{\delta}(\hat{S}^{-1}) = \Big(\mathbb E_n[z_i x_i&#39;]&#39; \underbrace{\mathbb E_n[z_iz_i&#39;]^{-1} \mathbb E[z_i x_i&#39;]} _ {\text{ols of } x_i \text{ on }z_i} \Big)^{-1}\mathbb E_n[z_i x_i&#39;]&#39; \underbrace{\mathbb E_n[z_iz_i&#39;]^{-1} \mathbb E[z_i y_i&#39;]} _ {\text{ols of } y_i \text{ on }z_i}= \hat{\delta} _ {2SLS}
$$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;: Consider the matrix notation. $$
\begin{aligned}
\hat{\delta} \left( \frac{Z&amp;rsquo;Z}{n}\right) &amp;amp;= \left( \frac{X&amp;rsquo;Z}{n} \left( \frac{Z&amp;rsquo;Z}{n}\right)^{-1} \frac{Z&amp;rsquo;X}{n} \right)^{-1} \frac{X&amp;rsquo;Z}{n} \left( \frac{Z&amp;rsquo;Z}{n}\right)^{-1} \frac{Z&amp;rsquo;Y}{n} = \newline
&amp;amp;= \left( X&amp;rsquo;Z(Z&amp;rsquo;Z)^{-1} Z&amp;rsquo;X \right)^{-1} X&amp;rsquo;Z(Z&amp;rsquo;Z)^{-1} Z&amp;rsquo;Y = \newline
&amp;amp;= \left(X&amp;rsquo;P_ZX\right)^{-1} X&amp;rsquo;P_ZY = \newline
&amp;amp;= \left(X&amp;rsquo;P_ZP_ZX\right)^{-1} X&amp;rsquo;P_ZY = \newline
&amp;amp;= \left(\hat{X}&#39;_Z \hat{X}_Z\right)^{-1} \hat{X}&#39;_ZY = \newline
&amp;amp;= \hat{\delta} _ {2SLS}
\end{aligned}
$$ $$\tag*{$\blacksquare$}$$&lt;/p&gt;
&lt;h3 id=&#34;small-sample-properties-of-2sls&#34;&gt;Small-Sample Properties of 2SLS&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt;: When the number of instruments is equal to the sample size
($L = n$), then $\hat{\delta} _ {2SLS} = \hat{\delta} _ {OLS}$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;: We have a perfect prediction problem. The first stage
estimated coefficient $\hat{\gamma}$ is such that it solves the normal
equations: $\hat{\gamma} = z_i^{-1} x_i$. Then $$
\begin{aligned}
\hat{\delta} _ {2SLS} &amp;amp;= \mathbb E_n[\hat{x}_i x&#39;_i]^{-1} \mathbb E_n[\hat{x}_i y_i] = \newline
&amp;amp;= \mathbb E_n[z_i z_i^{-1} x_i x&#39;_i]^{-1} \mathbb E_n[z_i z_i^{-1} x_i y_i] = \newline
&amp;amp;= \mathbb E_n[x_i x&#39;_i]^{-1} \mathbb E_n[x_i y_i] = \newline
&amp;amp;= \hat{\delta} _ {OLS}
\end{aligned}
$$ $$\tag*{$\blacksquare$}$$&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;You have this overfitting problem in general when the number of
instruments is large relative to the sample size. This problem arises
even if the instruments are valid.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;example-from-angrist-1992&#34;&gt;Example from Angrist (1992)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;They regress wages on years of schooling.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Problem&lt;/strong&gt;: endogeneity: both variables are correlated with skills
which are unobserved.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Solution&lt;/strong&gt;: instrument years of schooling with the quarter of
birth.
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Idea&lt;/strong&gt;: if born in the first three quarters, can attend school
from the year of your sixth birthday. Otherwise, you have to
wait one more year.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Problem&lt;/strong&gt;: quarters of birth are three dummies.
&lt;ul&gt;
&lt;li&gt;In order to ``improve the first stage fit” they interact them
with year of birth (180 effective instruments) and also with the
state (1527 effective instruments).&lt;/li&gt;
&lt;li&gt;This mechanically increases the $R^2$ but also increases the
bias of the 2SLS estimator.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Solutions&lt;/strong&gt;: LIML, JIVE, RJIVE (Hansen et al., 2014), Post-Lasso
(Belloni et al., 2012).&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;example-from-angrist-1992-1&#34;&gt;Example from Angrist (1992)&lt;/h3&gt;
&lt;img src=&#34;../img/Fig_441.png&#34; style=&#34;width:80.0%&#34; /&gt;
&lt;h2 id=&#34;many-instrument-robust-estimation&#34;&gt;Many Instrument Robust Estimation&lt;/h2&gt;
&lt;h3 id=&#34;issue&#34;&gt;Issue&lt;/h3&gt;
&lt;p&gt;Why having too many instruments is problematic? As the number of
instruments increases, the estimated coefficient gets closer to OLS
which is biased. As seen in the theorem above, for $L=n$, the two
estimators coincide.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;../img/Fig_451.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;liml&#34;&gt;LIML&lt;/h3&gt;
&lt;p&gt;An alternative method to estimate the parameters of the structural
equation is by maximum likelihood. Anderson and Rubin (1949) derived the
maximum likelihood estimator for the joint distribution of $(y_i, x_i)$.
The estimator is known as &lt;strong&gt;limited information maximum likelihood&lt;/strong&gt;, or
&lt;strong&gt;LIML&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;This estimator is called “limited information” because it is based on
the structural equation for $(y_i, x_i)$ combined with the reduced form
equation for $x_i$. If maximum likelihood is derived based on a
structural equation for $x_i$ as well, then this leads to what is known
as &lt;strong&gt;full information maximum likelihood (FIML)&lt;/strong&gt;. The advantage of the
LIML approach relative to FIML is that the former does not require a
structural model for $x_i$, and thus allows the researcher to focus on
the structural equation of interest - that for $y_i$.&lt;/p&gt;
&lt;h3 id=&#34;k-class-estimators&#34;&gt;K-class Estimators&lt;/h3&gt;
&lt;p&gt;The &lt;strong&gt;k-class&lt;/strong&gt; estimators have the form $$
\hat{\delta}(\alpha) = (X&#39; P_Z X - \alpha X&#39; X)^{-1} (X&#39; P_Z Y - \alpha X&#39; Y)
$$&lt;/p&gt;
&lt;p&gt;The limited information maximum likelihood estimator &lt;strong&gt;LIML&lt;/strong&gt; is the
k-class estimator $\hat{\delta}(\alpha)$ where $$
\alpha = \lambda_{min} \Big( ([X&#39; , Y]^{-1} [X&#39; , Y])^{-1} [X&#39; , Y]^{-1} P_Z [X&#39; , Y] \Big)
$$&lt;/p&gt;
&lt;p&gt;If $\alpha = 0$ then
$\hat{\delta} _ {\text{LIML}} = \hat{\delta} _ {\text{2SLS}}$ while for
$\alpha \to \infty$,
$\hat{\delta} _ {\text{LIML}} \to \hat{\delta} _ {\text{OLS}}$.&lt;/p&gt;
&lt;h3 id=&#34;comments-on-liml&#34;&gt;Comments on LIML&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;The particular choice of $\alpha$ gives a many instruments robust
estimate&lt;/li&gt;
&lt;li&gt;The LIML estimator has no finite sample moments.
$\mathbb E[\delta(\alpha_{LIML})]$ does not exist in general&lt;/li&gt;
&lt;li&gt;In simulation studies performs well&lt;/li&gt;
&lt;li&gt;Has good asymptotic properties&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Asymptotically the LIML estimator has the same distribution as 2SLS.
However, they can have quite different behaviors in finite samples.
There is considerable evidence that the LIML estimator has superior
finite sample performance to 2SLS when there are many instruments or the
reduced form is weak. However, on the other hand there is worry that
since the LIML estimator is derived under normality it may not be robust
in non-normal settings.&lt;/p&gt;
&lt;h3 id=&#34;jive&#34;&gt;JIVE&lt;/h3&gt;
&lt;p&gt;The &lt;strong&gt;Jacknife IV&lt;/strong&gt; procedure is the following&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Regress $\lbrace x_j \rbrace _ {j \neq i}$ on
$\lbrace z_j \rbrace _ {j \neq i}$ and estimate $\pi_{-i}$ (leave
the $i^{th}$ observation out).&lt;/li&gt;
&lt;li&gt;Form $\hat{x}_i = \hat{\pi} _ {-i} z_i$.&lt;/li&gt;
&lt;li&gt;Run IV using $\hat{x}_i$ as instruments. $$
\hat{\delta} _ {JIVE} = \mathbb E_n[\hat{x}_i x_i&#39;]^{-1} \mathbb E_n[\hat{x}_i y_i&#39;]
$$&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;comments-on-jive&#34;&gt;Comments on JIVE:&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Prevents overfitting.&lt;/li&gt;
&lt;li&gt;With many instruments you get bad out of sample prediction which
implies low correlation between $\hat{x}_i$ and $x_i$:
$\mathbb E_n[\hat{x}_i x_i&#39;] \simeq 0$.&lt;/li&gt;
&lt;li&gt;Use lasso/ridge regression in the first stage in case of too many
instruments.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;hausman-test&#34;&gt;Hausman Test&lt;/h3&gt;
&lt;p&gt;Here we consider testing the validity of OLS. OLS is generally preferred
to IV in terms of precision. Many researchers only doubt the (joint)
validity of the regressor $z_i$ instead of being certain that it is
invalid (in the sense of not being predetermined). So then they wish to
choose between OLS and 2SLS, assuming that they have an instrument
vector $x_i$ whose validity is not in question. Further, assume for
simplicity that $L = K$ so that the efficient GMM estimator is the IV
estimator.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;Hausman test statistic&lt;/strong&gt; $$
H \equiv n (\hat{\delta} _ {IV} - \hat{\delta} _ {OLS})&#39; [\hat{Avar} (\hat{\delta} _ {IV} - \hat{\delta} _ {OLS})]^{-1} (\hat{\delta} _ {IV} - \hat{\delta} _ {OLS})
$$ is asymptotically distributed as a $\chi^2_{L-s}$ under the null
where $s = | z_i \cup x_i |$: the number of regressors that are retained
as instruments in $x_i$.&lt;/p&gt;
&lt;h3 id=&#34;comments-1&#34;&gt;Comments&lt;/h3&gt;
&lt;p&gt;In general, the idea of the Hausman test is the following. If you have
two estimators, one which is efficient under $H_0$ but inconsistent
under $H_1$ (in this case, OLS), and another which is consistent under
$H_1$ (in this case, IV), then construct a test as a quadratic form in
the differences of the estimators. Another classic example arises in
panel data with the hypothesis $H_0$ of unconditional strict exogeneity.
In that case, under $H_0$ Random Effects estimators are efficient but
under $H_1$ they are inconsistent. Fixed Effects estimators instead are
consistent under $H_1$.&lt;/p&gt;
&lt;p&gt;The Hausman test statistic can be used as a pretest procedure: select
either OLS or IV according to the outcome of the test. Although widely
used, this pretest procedure is not advisable. When the null is false,
it is still possible that the test &lt;em&gt;accepts&lt;/em&gt; the null (committing a Type
2 error). In particular, this can happen with a high probability when
the sample size is &lt;em&gt;small&lt;/em&gt; and/or when the regressor $z_i$ is &lt;em&gt;almost
valid&lt;/em&gt;. In such an instance, estimation and also inference will be based
on incorrect methods. Therefore, the overall properties of the Hausman
pretest procedure are undesirable.&lt;/p&gt;
&lt;p&gt;The Hausman test is an example of a specification test. There are many
other specification tests. One could for example test for conditional
homoskedasticity. Unlike for the OLS case, there does not exist a
convenient test for conditional homoskedasticity for the GMM case. A
test statistic that is asymptotically chi-squared under the null is
available but is extremely cumbersome; see White (1982, note 2). If in
doubt, it is better to use the more generally valid inference methods
that allow for conditional heteroskedasticity. Similarly, there does not
exist a convenient test for serial correlation for the GMM case. If in
doubt, it is better to use the more generally valid inference methods
that allow for serial correlation; for example, when data are collected
over time (that is, time-series data).&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Single Agent Dynamics</title>
      <link>https://matteocourthoud.github.io/course/empirical-io/07_dynamics_singleagent/</link>
      <pubDate>Fri, 29 Oct 2021 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/course/empirical-io/07_dynamics_singleagent/</guid>
      <description>&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;h3 id=&#34;motivation&#34;&gt;Motivation&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;IO&lt;/strong&gt;: role of &lt;em&gt;market structure&lt;/em&gt; on &lt;em&gt;equilibrium outcomes&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Dynamics&lt;/strong&gt;: study the &lt;strong&gt;endogenous evolution&lt;/strong&gt; of &lt;em&gt;market structure&lt;/em&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Supply&lt;/strong&gt; side dynamics
&lt;ul&gt;
&lt;li&gt;Irreversible investment&lt;/li&gt;
&lt;li&gt;Entry sunk costs&lt;/li&gt;
&lt;li&gt;Product repositioning costs&lt;/li&gt;
&lt;li&gt;Price adjustment costs&lt;/li&gt;
&lt;li&gt;Learning by doing&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Demand&lt;/strong&gt; side dynamics
&lt;ul&gt;
&lt;li&gt;Switching costs&lt;/li&gt;
&lt;li&gt;Durable or storable products&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Bonus motivation&lt;/strong&gt;: AI literature studies essentially the same set of
problems with similar tools (&lt;a href=&#34;#ref-igami2020artificial&#34;&gt;Igami 2020&lt;/a&gt;)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Irony: niche topic in IO (super niche in econ), but at the core of
the frontier in computer science
&lt;ul&gt;
&lt;li&gt;Why? Computation is hard, estimation harder, but extremely
powerful prediction tool&lt;/li&gt;
&lt;li&gt;The world is intrinsecally dynamic&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;examples-1&#34;&gt;Examples (1)&lt;/h3&gt;
&lt;p&gt;Some examples in empirical IO&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Investment&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;Rust (&lt;a href=&#34;#ref-rust1987optimal&#34;&gt;1987&lt;/a&gt;): bus engine replacement
decision&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Durable goods&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;Gowrisankaran and Rysman
(&lt;a href=&#34;#ref-gowrisankaran2012dynamics&#34;&gt;2012&lt;/a&gt;): consumer demand in the
digital camcorder industry&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Stockpiling&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;Erdem, Imai, and Keane (&lt;a href=&#34;#ref-erdem2003brand&#34;&gt;2003&lt;/a&gt;): promotions
and stockpiling of ketchup&lt;/li&gt;
&lt;li&gt;Hendel and Nevo (&lt;a href=&#34;#ref-hendel2006measuring&#34;&gt;2006&lt;/a&gt;): stockpiling
of laundry detergents&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Learning&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;Erdem and Keane (&lt;a href=&#34;#ref-erdem1996decision&#34;&gt;1996&lt;/a&gt;): brand learning
in the laundry detergent industry&lt;/li&gt;
&lt;li&gt;Crawford and Shum (&lt;a href=&#34;#ref-crawford2005uncertainty&#34;&gt;2005&lt;/a&gt;): demand
learning of anti‐ulcer drug prescriptions&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Switching costs&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;Handel (&lt;a href=&#34;#ref-handel2013adverse&#34;&gt;2013&lt;/a&gt;): inertia in demand for
health insurance&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;examples-2&#34;&gt;Examples (2)&lt;/h3&gt;
&lt;p&gt;But also in other applied micro fields:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Labor economics&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;Should you go to college? (&lt;a href=&#34;#ref-keane1997career&#34;&gt;Keane and Wolpin
1997&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Health economics&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Which health insurance to pick given there are switching costs?
(&lt;a href=&#34;#ref-handel2013adverse&#34;&gt;Handel 2013&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Addiction (&lt;a href=&#34;#ref-becker1988theory&#34;&gt;Becker and Murphy 1988&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Public finance&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;How should you set optimal taxes in a dynamic environment?
(&lt;a href=&#34;#ref-golosov2006new&#34;&gt;Golosov et al. 2006&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;do-we-really-need-dynamics&#34;&gt;Do we really need dynamics?&lt;/h3&gt;
&lt;p&gt;In some cases, we can &lt;strong&gt;reduce&lt;/strong&gt; a dynamic problem to a:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Static problem&lt;/li&gt;
&lt;li&gt;Reduced-form problem&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;E.g., Investment decision&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Dynamic problem, as gains are realized after costs&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;“Static” solution: invest if $\mathbb E (NPV ) &amp;gt; TC$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Action today ($a_t=0$ or $1$) does not affect the amount of future
payoffs (NPV)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;But many cases where it’s hard to evaluate dynamic questions in a
static/reduced-form setting.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Typically, cases where decision today would affect payoffs tomorrow&lt;/li&gt;
&lt;li&gt;And you care about those payoffs ($\neq$ myopia)&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;“&lt;em&gt;A dynamic model can do anything a static model can.&lt;/em&gt;”&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;new-empirical-io&#34;&gt;New Empirical IO&lt;/h3&gt;
&lt;p&gt;So-called New Empirical IO (summary in Bresnahan
(&lt;a href=&#34;#ref-bresnahan1989empirical&#34;&gt;1989&lt;/a&gt;))&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Some &lt;strong&gt;decisions today&lt;/strong&gt; might affect &lt;strong&gt;payoffs tomorrow&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;But the decision today depends on the &lt;strong&gt;state today&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;And the state today might have been the result of a &lt;strong&gt;decision
yesterday&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Etc…&lt;/li&gt;
&lt;li&gt;Need &lt;strong&gt;dynamics&lt;/strong&gt; to study these questions&lt;/li&gt;
&lt;li&gt;Where does it all start?
&lt;ul&gt;
&lt;li&gt;Pakes (&lt;a href=&#34;#ref-pakes1986patents&#34;&gt;1986&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Berry (&lt;a href=&#34;#ref-berry1992estimation&#34;&gt;1992&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;pros-and-cons&#34;&gt;Pros and Cons&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Advantages&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;We can adress &lt;strong&gt;intertemporal trade-offs&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Flow vs stock stocks and benefits&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;We can examine &lt;strong&gt;transitions&lt;/strong&gt; and not only steady states&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;We are able to address &lt;strong&gt;policy questions&lt;/strong&gt; that cannot be addressed
with reduced-form methods&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Standard advantage of structural estimation&lt;/li&gt;
&lt;li&gt;But in a context with relevant intertemporal trade-offs /
decisions&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Disadvantages&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;We typically need more &lt;strong&gt;assumptions&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Robustness testing will therefore be important&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Identification&lt;/strong&gt; in dynamic models is less transparent&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Thus time should be spent articulating what variation in the
data identifies our parameters of interest)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;It is often &lt;strong&gt;computationally intensive&lt;/strong&gt; (i.e., slow / unfeasible)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;from-statics-to-dynamics&#34;&gt;From Statics to Dynamics&lt;/h3&gt;
&lt;p&gt;Typical steps&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Specify the primitives of the model
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Static&lt;/strong&gt;: single period agents’ payoff functions (utility or
profit)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Dynamic&lt;/strong&gt;: static payoffs + &lt;em&gt;evolution of state variables&lt;/em&gt;
&lt;ul&gt;
&lt;li&gt;Can be exogenous&lt;/li&gt;
&lt;li&gt;… or endogenous: decision today has an effect on the state
tomorrow&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Solve for optimal behavior
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Static&lt;/strong&gt;: tipically agents maximize current utility or profit&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Dynamic&lt;/strong&gt;: agents maximize &lt;em&gt;present discounted value&lt;/em&gt; of
future utilities or profits&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Search for parameter values that result in the “best match” between
our model predictions and observed behavior&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;1st-year-macro-recap&#34;&gt;1st year Macro Recap&lt;/h2&gt;
&lt;h3 id=&#34;markov-decision-processes&#34;&gt;Markov Decision Processes&lt;/h3&gt;
&lt;p&gt;Formally, a discrete-time MDP consists of the following &lt;strong&gt;objects&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;A discrete &lt;strong&gt;time index&lt;/strong&gt; $t \in \lbrace 0,1,2,&amp;hellip;,T \rbrace$, for
$T \leq \infty$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;A &lt;strong&gt;state space&lt;/strong&gt; $\mathcal S$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;An &lt;strong&gt;action space&lt;/strong&gt; $\mathcal A$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;and a family of &lt;strong&gt;constraint sets&lt;/strong&gt;
$\lbrace \mathcal a_t(s_t) \subseteq \mathcal A \rbrace$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;A family of &lt;strong&gt;transition probabilities&lt;/strong&gt;
$\lbrace \Pr_{t}(s_{t+1}|s_t,a_t) \rbrace$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;A &lt;strong&gt;discount factor&lt;/strong&gt;, $\beta$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;A family of single-period &lt;strong&gt;reward functions&lt;/strong&gt;
$\lbrace (u_t(s_t,a_t) \rbrace$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;so that the utility functional $U$ has an additively separable
decomposition $$
U(\boldsymbol s, \boldsymbol a) = \sum_{t=0}^{T} \beta^{t} u_{t}\left(s_t, a_{t}\right)
$$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;mdp-2&#34;&gt;MDP (2)&lt;/h3&gt;
&lt;p&gt;In words&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;The &lt;strong&gt;state space&lt;/strong&gt; $\mathcal S$ contains all the information needed
to&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;compute static utilities $u_t (s_t, a_t)$&lt;/li&gt;
&lt;li&gt;compute transition probabilities
$\lbrace \Pr_{t} (s_{t+1}|s_t,a_t) \rbrace$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The (conditional) &lt;strong&gt;action space&lt;/strong&gt; $\mathcal A (s_t)$ contains all
the actions available in state $s_t$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;How can it be different by state? E.g. entry/exit decision if
you’re in/out of the market&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The &lt;strong&gt;transition probabilities&lt;/strong&gt;
$\lbrace \Pr_{t+1}(s_{t+1}|s_t,a_t) \rbrace$ define the
probabilities of future states $s_{t+1}$ conditional on&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Present state $s_t$&lt;/li&gt;
&lt;li&gt;Present decision $a_t$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The &lt;strong&gt;discount factor&lt;/strong&gt; $\beta$ together with the static &lt;strong&gt;reward
functions&lt;/strong&gt; $\lbrace (u_t(s_t,a_t) \rbrace$ determine the
&lt;strong&gt;objective function&lt;/strong&gt; $$
\mathbb E_{\boldsymbol s&#39;} \Bigg[ \sum_{t=0}^{T} \beta^{t} u_{t}\left(s_t, a_{t}\right) \Bigg]
$$&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;notation&#34;&gt;Notation&lt;/h3&gt;
&lt;p&gt;Brief parenthesis on &lt;strong&gt;notation&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;I have seen states denoted as&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$s$ (for state)&lt;/li&gt;
&lt;li&gt;$x$&lt;/li&gt;
&lt;li&gt;$\omega$&lt;/li&gt;
&lt;li&gt;others, depending on the specific context, e.g. $e$ for
experience&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;I will try to stick to $s$ all the time&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;I have seen decisions denoted as&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$a$ (for action)&lt;/li&gt;
&lt;li&gt;$d$ (for decision)&lt;/li&gt;
&lt;li&gt;$x$&lt;/li&gt;
&lt;li&gt;others, depending on the specific context, e.g. $i$ for
investment&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;I will try to stick to $a$ all the time&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;maximization-problem&#34;&gt;Maximization Problem&lt;/h3&gt;
&lt;p&gt;The objective is to pick the decision rule (or &lt;strong&gt;policy function&lt;/strong&gt;)
$P = \boldsymbol a^* = \lbrace a^&lt;em&gt;_1, &amp;hellip;, a^&lt;/em&gt;&lt;em&gt;t \rbrace$ that solves $$
\max&lt;/em&gt;{\boldsymbol a} \ \mathbb E_{\boldsymbol s&#39;} \Bigg[ \sum_{t=0}^{T} \beta^{t} u_{t}\left(s_t, a_{t}\right) \Bigg]
$$ Where the expectation is taken over transition probabilities
generated by the decision rule $\boldsymbol a$.&lt;/p&gt;
&lt;h3 id=&#34;stationarity&#34;&gt;Stationarity&lt;/h3&gt;
&lt;p&gt;In many applications, we assume &lt;strong&gt;stationarity&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;The &lt;strong&gt;transition probabilities and utility functions do not directly
depend on&lt;/strong&gt; $t$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;i.e., are the same for all $t$
&lt;ul&gt;
&lt;li&gt;$\Pr_{{\color{red}{t}}} (s_{t+1}|s_t,a_t) \  \to \ \Pr(s_{t+1}|s_t,a_t)$&lt;/li&gt;
&lt;li&gt;$u_{{\color{red}{t}}} (s_t,a_t) \ \to \ u(s_t,a_t)$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Uncomfortable assumption?&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;You think there is some reason (variable) why today’s probabilities
should be different from tomorrow’s?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;If &lt;strong&gt;observable&lt;/strong&gt;, include that variable in the state space&lt;/li&gt;
&lt;li&gt;If &lt;strong&gt;unobservable&lt;/strong&gt;, integrate it out&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;stationarity-2&#34;&gt;Stationarity (2)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;In the &lt;strong&gt;finite horizon&lt;/strong&gt; case, ($T \leq \infty$), stationarity does
not help much&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\sum_{t=0}^{T} \beta^{t} u(s_t, a_{t})$ still depends on $t$,
conditional on $s_t$&lt;/li&gt;
&lt;li&gt;Why? Difference between $t$ and $T$ matters in the sum&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;In &lt;strong&gt;infinite-horizon&lt;/strong&gt; problems, stationarity helps a lot&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Now the difference between $t$ and $T$ is always the same,
i.e. $\infty$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$\sum_{t=0}^{\infty} \beta^{t} u(s_t, a_{t})$ does &lt;strong&gt;not&lt;/strong&gt;
depend on $t$, conditional on $s_t$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;The future looks the same whether the agent is in state $s_t$
at time $t$ or in state $s_{t+\tau} = s_t$ at time $t + \tau$&lt;/em&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;value-function&#34;&gt;Value Function&lt;/h3&gt;
&lt;p&gt;Consider a &lt;strong&gt;stationary infinite-horizon&lt;/strong&gt; problem&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;The only variable which affects the agent’s view about the future is
the current value of the state, $s_t$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;We can rewrite the &lt;strong&gt;agent’s problem&lt;/strong&gt; as $$
V_0(s_0) = \max_{\boldsymbol a} \ \mathbb E_{\boldsymbol s&#39;} \Bigg[ \sum_{t=0}^{\infty} \beta^{t} u\left(s_t, a_{t}\right) \Bigg]
$$ where&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$a_t \in \mathcal A(s_t) \ \forall t$&lt;/li&gt;
&lt;li&gt;The expectation is taken over future states $\boldsymbol s&#39;$
&lt;ul&gt;
&lt;li&gt;that evolve according to
$\lbrace \Pr(s_{t+1}|s_t,a_t) \rbrace$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;$V(\cdot)$ is called the &lt;strong&gt;value function&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;how-to-solve&#34;&gt;How to solve?&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;One could try to solve it by &lt;strong&gt;brute force&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;i.e. try to solve for the structure of all of the optimal
decisions, $\boldsymbol a^*$&lt;/li&gt;
&lt;li&gt;Indeed, for finite-horizon problems, that might be necessary&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;For &lt;strong&gt;stationary infinite-horizon&lt;/strong&gt; problems, the value and policy
function should be &lt;strong&gt;time invariant&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;$V_{\color{red}{t}} (s_t) = V(s_t)$&lt;/li&gt;
&lt;li&gt;$P_{\color{red}{t}} (s_t) = P(s_t)$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;What to we gain?&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;bellman-equation&#34;&gt;Bellman Equation&lt;/h3&gt;
&lt;p&gt;$$
\begin{align}
V(s_0) &amp;amp;= \max_{\boldsymbol a} \ \mathbb E_{\boldsymbol s&#39;} \Bigg[ \sum_{t=0}^{\infty} \beta^{t} u(s_t, a_{t}) \Bigg]
= \newline
&amp;amp;= \max_{\boldsymbol a} \ \mathbb E_{\boldsymbol s&#39;} \Bigg[ {\color{red}{u(s_{0}, a_{0})}} + \sum_{{\color{red}{t=1}}}^{\infty} \beta^{t} u(s_t, a_{t}) \Bigg]
= \newline
&amp;amp;= \max_{\boldsymbol a} \ \Bigg\lbrace u(s_{0}, a_{0}) + {\color{red}{\mathbb E_{\boldsymbol s&#39;}}} \Bigg[ \sum_{t=1}^{\infty} \beta^{t} u(s_t, a_{t}) \Bigg] \Bigg\rbrace
= \newline
&amp;amp;= \max_{\boldsymbol a} \ \Bigg\lbrace u(s_{0}, a_{0}) + {\color{red}{\beta}} \ \mathbb E_{\boldsymbol s&#39;} \Bigg[ \sum_{t=1}^{\infty} \beta^{{\color{red}{t-1}}} u(s_t, a_{t}) \Bigg] \Bigg\rbrace
= \newline
&amp;amp;= \max_{{\color{red}{a_0}}} \ \Bigg\lbrace u(s_{0}, a_{0}) + \beta \ {\color{red}{\max_{\boldsymbol a}}}\ \mathbb E_{\boldsymbol s&#39;} \Bigg[ \sum_{t=1}^{\infty} \beta^{t-1} u(s_t, a_{t}) \Bigg] \Bigg\rbrace
= \newline
&amp;amp;= \max_{a_0} \ \Bigg\lbrace u(s_{0}, a_{0}) + \beta \ {\color{red}{\int V(s_1) \Pr(s_1 | s_0, a_0)}} \Bigg\rbrace
\end{align}
$$&lt;/p&gt;
&lt;h3 id=&#34;bellman-equation-2&#34;&gt;Bellman Equation (2)&lt;/h3&gt;
&lt;p&gt;We have now a &lt;strong&gt;recursive formulation&lt;/strong&gt; of the value function: the
&lt;strong&gt;Bellman Equation&lt;/strong&gt; $$
{\color{red}{V(s_0)}} = \max_{a_0} \ \Bigg\lbrace u(s_{0}, a_{0}) + \beta \ \int {\color{red}{V(s_1)}} \Pr(s_1 | s_0, a_0) \Bigg\rbrace
$$ &lt;strong&gt;Intuition&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The Bellman Equation is a &lt;strong&gt;functional equation&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;Has to be satisfied in every state&lt;/li&gt;
&lt;li&gt;Can be written as ${\color{red}{V}} = T({\color{red}{V}})$&lt;/li&gt;
&lt;li&gt;We are actually looking for a &lt;strong&gt;fixed point&lt;/strong&gt; of $T$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The decision rule that satisfies the Bellman Equation is called the
&lt;strong&gt;policy function&lt;/strong&gt; $$
a(s_0) =  \arg \max_{a_0} \ \Bigg\lbrace u(s_{0}, a_{0}) + \beta \ \int V(s_1) \Pr(s_1 | s_0, a_0) \Bigg\rbrace
$$&lt;/p&gt;
&lt;h3 id=&#34;contractions&#34;&gt;Contractions&lt;/h3&gt;
&lt;p&gt;Under regularity conditions&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$u(s, a)$ is jointly continuous and bounded in $(s, a)$&lt;/li&gt;
&lt;li&gt;$\mathcal A (s)$ is a continuous correspondence&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;It is possible to show that $$
T(W)(s) = \max_{a \in \mathcal A(s)} \ \Bigg\lbrace u(s, a) + \beta \ \int W(s&#39;) \Pr(s&#39; | s, a) \Bigg\rbrace
$$ is a &lt;strong&gt;contraction mapping&lt;/strong&gt; of modulus $\beta$.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Contraction Mapping Theorem&lt;/strong&gt;: then $T$ has a unique fixed point!&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;solving-for-the-value-function&#34;&gt;Solving for the Value Function&lt;/h3&gt;
&lt;p&gt;How do we actually do it in practice?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;For &lt;strong&gt;finite horizon&lt;/strong&gt; MDPs: &lt;strong&gt;backward induction&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;Start from the last period: static maximization problem&lt;/li&gt;
&lt;li&gt;Move backwards taking the future value as given&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;For &lt;strong&gt;infinite horizon&lt;/strong&gt; MDPs: different options
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;value function iteration&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;most common&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;policy function iteration&lt;/li&gt;
&lt;li&gt;successive approximations&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;difference-with-1st-year-macro&#34;&gt;Difference with 1st year Macro&lt;/h3&gt;
&lt;p&gt;So what’s going to be new here?&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Estimation&lt;/strong&gt;: retrieve model primitives from observed behavior
&lt;ul&gt;
&lt;li&gt;And related: uncertainty&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Strategic interaction&lt;/strong&gt;: multiple agents taking dynamic decisions
&lt;ul&gt;
&lt;li&gt;Next lecture&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;rust-1987&#34;&gt;Rust (1987)&lt;/h2&gt;
&lt;h3 id=&#34;setting&#34;&gt;Setting&lt;/h3&gt;
&lt;p&gt;Rust (&lt;a href=&#34;#ref-rust1987optimal&#34;&gt;1987&lt;/a&gt;): &lt;em&gt;An Empirical Model of Harold
Zurcher&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Harold Zurcher (HZ) is the city bus superintendant in Madison, WI&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;As bus engines get older, the probability of malfunctions increases&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;HZ decides when to replace old bus engines with new ones&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Optimal stopping / investment problem&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Tradeoff&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Cost of a new engine (fixed, stock)&lt;/li&gt;
&lt;li&gt;Repair costs, because of engine failures (continuous, flow)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Do we care about Harold Zurcher?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Obviously not (and neither did Rust), it’s a method paper&lt;/li&gt;
&lt;li&gt;But referee asked for an application&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;data&#34;&gt;Data&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Units of observation&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Rust observes 162 buses over time&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Observables&lt;/strong&gt;: for each bus, he sees&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;monthly mileage (RHS, state variable)&lt;/li&gt;
&lt;li&gt;and whether the engine was replaced (LHS, choice variable),&lt;/li&gt;
&lt;li&gt;in a given month&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Variation&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;on average, bus engines were replaced every 5 years with over
200,000 elapsed miles&lt;/li&gt;
&lt;li&gt;considerable variation in the &lt;em&gt;time&lt;/em&gt; and &lt;em&gt;mileage&lt;/em&gt; at which
replacement occurs&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;idea&#34;&gt;Idea&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Construct a (parametric) &lt;strong&gt;model&lt;/strong&gt; which predicts the time and
mileage at which engine replacement occurs&lt;/li&gt;
&lt;li&gt;Use the model predictions (conditional on parameter values) to
&lt;strong&gt;estimate parameters&lt;/strong&gt; that “fit” the data
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;predicted&lt;/strong&gt; replacements, given mileage &lt;strong&gt;VS&lt;/strong&gt; &lt;strong&gt;observed&lt;/strong&gt;
replacements, given mileage&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Ideally use the estimates to &lt;strong&gt;learn something&lt;/strong&gt; new
&lt;ul&gt;
&lt;li&gt;e.g. the correct - dynamic - demand curve for bus engine
replacement&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;static-alternative&#34;&gt;Static Alternative&lt;/h3&gt;
&lt;p&gt;What would you do otherwise?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;You observe replacement decisions&lt;/li&gt;
&lt;li&gt;… and replacement costs&lt;/li&gt;
&lt;li&gt;$\to$ &lt;strong&gt;Regress&lt;/strong&gt; replacement decision on replacement costs&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Problem&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Replacement benefits are a flow (lower maintenance costs)&lt;/li&gt;
&lt;li&gt;… while the cost is a stock&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Outcome&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;We expect static demand would be much more elastic&lt;/li&gt;
&lt;li&gt;Overpredict substitutions at low costs&lt;/li&gt;
&lt;li&gt;and underpredict substitution at high cost&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;model&#34;&gt;Model&lt;/h3&gt;
&lt;p&gt;Assumptions of structural model&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;State&lt;/strong&gt;: $s_t \in \lbrace 0, &amp;hellip; , s_{max} \rbrace$
&lt;ul&gt;
&lt;li&gt;engine accumulated mileage at time $t$&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Note&lt;/strong&gt;: “continuous” in the data but has to be discretized
into bins&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Action&lt;/strong&gt;: $a_t \in \lbrace 0, 1 \rbrace$
&lt;ul&gt;
&lt;li&gt;replace engine at time $t$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;State transitions&lt;/strong&gt;:
$\Pr ( s_{t+1} | s_{0}, &amp;hellip; , s_t ; \theta)= \Pr (s_{t+1} | s_t ; \theta )$
&lt;ul&gt;
&lt;li&gt;mileage $s_t$ evolves exogenously according to a 1st-order
Markov process&lt;/li&gt;
&lt;li&gt;The transition function is the same for every bus.&lt;/li&gt;
&lt;li&gt;If HZ replaces in period $t$ ($a_t = 1$), then $s_t = 0$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;model-2&#34;&gt;Model (2)&lt;/h3&gt;
&lt;p&gt;HZ &lt;strong&gt;static utility function&lt;/strong&gt; (for a single bus) $$
u\left(s_t, a_{t} ; \theta\right)= \begin{cases}-c\left(s_t ; \theta\right) &amp;amp; \text { if } a_{t}=0 \text { (not replace) } \newline -R-c(0 ; \theta) &amp;amp; \text { if } a_{t}=1 \text { (replace) }\end{cases}
$$ where&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$c(s_t ; \theta)$: expected &lt;strong&gt;costs of operating&lt;/strong&gt; a bus with
mileage $s_t$
&lt;ul&gt;
&lt;li&gt;​ including maintenance costs &amp;amp; social costs of breakdown&lt;/li&gt;
&lt;li&gt;We would expect $\frac{\partial c}{\partial s}&amp;gt;0$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;$R$ is the &lt;strong&gt;cost of replacement&lt;/strong&gt; (i.e., a new engine)
&lt;ul&gt;
&lt;li&gt;Note that replacement occurs immediately&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;$u(s_t , a_t ; \theta)$: expected current utility from operating a
bus with mileage $s_t$ and making replacement decision $a_t$&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;model-3&#34;&gt;Model (3)&lt;/h3&gt;
&lt;p&gt;HZ &lt;strong&gt;objective function&lt;/strong&gt; is to maximize the expected present discounted
sum of future utilities $$
V(s_t ; \theta) = \max_{\boldsymbol a} \mathbb E_{s_{t+1}} \left[\sum_{\tau=t}^{\infty} \beta^{\tau-t} u\left(s_{\tau}, a_{\tau} ; \theta\right) \ \Bigg| \ s_t, \boldsymbol a ; \theta\right]
$$ where&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The expectation $\mathbb E$ is over future $x$, which evolve
according to Markov process&lt;/li&gt;
&lt;li&gt;$\max$ is over future choices $a_{t+1}, &amp;hellip; ,a_{\infty}$,
&lt;ul&gt;
&lt;li&gt;because HZ will observe future states $s_{\tau}$ before choosing
future actions $a_\tau$, this is a functional&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Notes&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;This is for one bus (but multiple engines).&lt;/li&gt;
&lt;li&gt;HZ has an infinite horizon for his decision making&lt;/li&gt;
&lt;li&gt;$s_t$ summarizes state at time $t$, i.e., the expected value of
future utilities only depends on $s_t$&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;bellman-equation-1&#34;&gt;Bellman Equation&lt;/h3&gt;
&lt;p&gt;This (sequential) representation of HZ’s problem is very cumbersome to
work with.&lt;/p&gt;
&lt;p&gt;We can rewrite $V (s_t; \theta)$ with the following Bellman equation $$
V\left(s_t ; \theta\right) = \max_{a_{t}} \Bigg\lbrace u\left(s_t, a_{t} ; \theta\right)+\beta \mathbb E_{s_{t+1}} \Big[V\left(s_{t+1} ; \theta\right) \Big| s_t, a_{t} ; \theta\Big] \Bigg\rbrace
$$ Basically we are dividing the infinite sum (in the sequential form)
into a present component and a future component.&lt;/p&gt;
&lt;p&gt;Notes:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Same $V$ on both sides of equation because of infinite horizon - the
future looks the same as the present for a given $s$ (i.e., it
doesn’t matter where you are in time).&lt;/li&gt;
&lt;li&gt;The expectation $\mathbb E$ is over the state-transition
probabilities, $\Pr (s_{t+1} | s_t, a_t ; \theta)$&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;order-of-markow-process&#34;&gt;Order of Markow Process&lt;/h3&gt;
&lt;p&gt;Suppose for a moment that $s_t$ follows a second-order markov process $$
s_{t+1}=f\left(s_t, {\color{red}{s_{t-1}}}, \varepsilon ; \theta\right)
$$ Now $s_t$ is not sufficient to describe current $V$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;We need both $s_t$ and $s_{t-1}$ in the state space (i.e.,
$V (s_t , {\color{red}{s_{t-1}}}; \theta)$ contains $s_{t-1}$, too),&lt;/li&gt;
&lt;li&gt;and the expectation is over the transition probability
$\Pr (s_{t+1} | s_t, {\color{red}{s_{t-1}}}, a_t ; \theta)$&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;parenthesis-state-variables&#34;&gt;Parenthesis: State Variables&lt;/h3&gt;
&lt;p&gt;Which variables should be state variables? I.e. should be included in
the state space?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;General rule&lt;/strong&gt; for 1st order markow processes: variables need to&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;define expected current payoff, &lt;strong&gt;and&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;define expectations over next period state (i.e., distribution of
$s_{t+1}$)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;What do you do otherwise? Integrate them out! &lt;strong&gt;Examples&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Weather affects static utitilies but not transition probabilities
&lt;ul&gt;
&lt;li&gt;More annoying to replace the engine if it rains&lt;/li&gt;
&lt;li&gt;Itegration means: &lt;em&gt;“compute expected utility of Harold Zurcher
before he opens the window”&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Month of the year affects transition probabilities but not utilities
&lt;ul&gt;
&lt;li&gt;Buses are used more in the winter&lt;/li&gt;
&lt;li&gt;Integration means: compute average (over months) transition
probabilities&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Note 1&lt;/strong&gt;: you can always get the non-expected value function if you
know the probability of raining or the transition probabilities by
month&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;policy-function&#34;&gt;Policy Function&lt;/h3&gt;
&lt;p&gt;Along with this value function is a corresponding &lt;strong&gt;policy (or choice)
function&lt;/strong&gt; mapping the state $s_t$ into HZ’s optimal replacement choice
$a_t$ $$
P \left(s_t ; \theta\right) =  \max_{a_{t}} \Bigg\lbrace u\left(s_t, a_{t} ; \theta\right) + \beta \mathbb E_{s_{t+1}} \Big[ V \left(s_{t+1} ; \theta\right) \Big| s_t, a_{t} ; \theta\Big] \Bigg\rbrace
$$ Given $\frac{\partial c}{\partial s}&amp;gt;0$, the policy function has the
form $$
P \left(s_t ; \theta\right) =  \begin{cases}1 &amp;amp; \text { if } s_t \geq \gamma(\theta) \newline 0 &amp;amp; \text { if } s_t&amp;lt;\gamma(\theta)\end{cases}
$$ where $\gamma$ is the replacement mileage.&lt;/p&gt;
&lt;p&gt;How would this compare with the optimal replacement mileage if HZ was
myopic?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Answer: HZ would wait until $R \leq c(s)$ for the replacement action&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;solving-the-model&#34;&gt;Solving the Model&lt;/h3&gt;
&lt;p&gt;Why do we want to solve for the value and policy functions?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;We want to know the agent’s optimal behavior and the equilibrium
outcomes&lt;/li&gt;
&lt;li&gt;and be able to conduct comparative statics/dynamics (a.k.a.
counterfactual simulations)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We have the &lt;strong&gt;Bellman Equation&lt;/strong&gt; $$
V\left(s_t ; \theta\right) = \max_{a_{t}} \Bigg\lbrace u\left(s_t, a_{t} ; \theta\right)+\beta \mathbb E_{s_{t+1}} \Big[V\left(s_{t+1} ; \theta\right) \ \Big|
\ s_t, a_{t} ; \theta\Big] \Bigg\rbrace
$$ Which we can compactly write as $$
V\left(s_t ; \theta\right) = T \Big( V\left(s_{t+1} ; \theta\right) \Big)
$$ &lt;strong&gt;Blackwell’s Theorem&lt;/strong&gt;: under regularity conditions, $T$ is a
contraction mapping with modulus $\beta$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Contraction Mapping Theorem&lt;/strong&gt;: $T$ has a fixed point and we can find
it by iterating $T$ from any starting value $V^{(0)}$.&lt;/p&gt;
&lt;h3 id=&#34;value-function-iteration&#34;&gt;Value Function Iteration&lt;/h3&gt;
&lt;p&gt;What does &lt;strong&gt;Blackwell’s Theorem&lt;/strong&gt; allow us to do?&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Start with any arbitrary function $V^{(0)}(\cdot)$&lt;/li&gt;
&lt;li&gt;Apply the mapping $T$ to get $V^{(1)}(\cdot) = T (V^{(0)}(\cdot))$&lt;/li&gt;
&lt;li&gt;Apply again $V^{(2)}(\cdot) = T (V^{(1)}(\cdot))$&lt;/li&gt;
&lt;li&gt;Continue applying $T$ , and $V^{(k)}$ will converge to the unique
fixed point of $T$
&lt;ul&gt;
&lt;li&gt;i.e., the true value function $V(s_t; \theta)$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Once we have $V(s_t; \theta)$, it’s fairly trivial to compute the
policy function $P(s_t; \theta)$
&lt;ul&gt;
&lt;li&gt;Static optimization problem (given $V$)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;This process is called &lt;strong&gt;value function iteration&lt;/strong&gt;&lt;/p&gt;
&lt;h3 id=&#34;how-to-reconcile-model-and-data&#34;&gt;How to Reconcile Model and Data?&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Ideal Estimation Routine&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Pick a parameter value $\theta$&lt;/li&gt;
&lt;li&gt;Solve value and policy function (&lt;em&gt;inner loop&lt;/em&gt;)&lt;/li&gt;
&lt;li&gt;Match &lt;em&gt;predicted choices&lt;/em&gt; with &lt;em&gt;observed choices&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;Find the parameter value $\hat \theta$ that best fits the data
(&lt;em&gt;outer loop&lt;/em&gt;)
&lt;ul&gt;
&lt;li&gt;Makes the observed choices “closest” to the predicted choices&lt;/li&gt;
&lt;li&gt;(or maximizes the likelihood of the observed choices)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;Issue&lt;/strong&gt;: model easily &lt;strong&gt;rejected&lt;/strong&gt; by the data&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;The policy function takes the the form: replace iff
$s_t \geq \gamma(\theta)$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Can’t explain the coexistence of e.g. “&lt;em&gt;a bus without replacement at
22K miles&lt;/em&gt;” and “&lt;em&gt;another bus being replaced at 17K mile&lt;/em&gt;s” in the
data&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;We need some &lt;strong&gt;unobservables&lt;/strong&gt; in the model to explain why observed
choices do not exactly match predicted choices&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;rust-1987---estimation&#34;&gt;Rust (1987) - Estimation&lt;/h2&gt;
&lt;h3 id=&#34;uncertainty&#34;&gt;Uncertainty&lt;/h3&gt;
&lt;p&gt;How can we explain different replacement actions at different mileages
in the data?&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Add other observables&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Add some stochastic element&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;But &lt;strong&gt;where&lt;/strong&gt;? Two options&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Randomness in decisions
&lt;ul&gt;
&lt;li&gt;I.e. &lt;em&gt;“Harold Zurcher sometimes would like to replace the bus
engine but he forgets”&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;Probably still falsifiable&lt;/li&gt;
&lt;li&gt;Also need “&lt;em&gt;Harold Zurcher sometimes would like not to replace
but replacement happens”&lt;/em&gt; 🤔🤔🤔&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Randomness in the state
&lt;ul&gt;
&lt;li&gt;Harold Zurcher knows something that we don’t&lt;/li&gt;
&lt;li&gt;He always makes the best decision but based on some unobservable&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;unobservables&#34;&gt;Unobservables&lt;/h3&gt;
&lt;p&gt;Rust uses the following &lt;strong&gt;utility specification&lt;/strong&gt;: $$
u\left(s_t, a_{t}, {\color{red}{\epsilon_{t}}} ; \theta\right) = u\left(s_t, a_{t} ; \theta\right) + {\color{red}{\epsilon_{a_{t} t}}} = \begin{cases} - c\left(s_t ; \theta\right) + {\color{red}{\epsilon_{0 t}}} &amp;amp; \text { if } \ a_{t}=0 \newline \newline -R-c(0 ; \theta) + {\color{red}{\epsilon_{1 t}}} &amp;amp; \text { if } \ a_{t}=1 \end{cases}
$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The $\epsilon_{it}$ are components of utility of alternative $a$
that are observed by HZ but not by us as econometrician.
&lt;ul&gt;
&lt;li&gt;E.g., the fact that an engine is running unusually smoothly
given its mileage,&lt;/li&gt;
&lt;li&gt;or the fact that HZ is sick and doesn’t feel like replacing the
engine this month&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Note&lt;/strong&gt;: we have assumed addictive separability of $\epsilon$&lt;/li&gt;
&lt;li&gt;The $\epsilon_a$s also affect HZ’s replacement decision&lt;/li&gt;
&lt;li&gt;$\epsilon_{it}$ are &lt;strong&gt;both observed and relevant&lt;/strong&gt; $\to$ part of the
state space&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;Can we still &lt;strong&gt;solve&lt;/strong&gt; the model? Can we &lt;strong&gt;estimate&lt;/strong&gt; it?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;unobservables-2&#34;&gt;Unobservables (2)&lt;/h3&gt;
&lt;p&gt;The &lt;strong&gt;Bellman Equation&lt;/strong&gt; becomes $$
V \Big( {\color{red}{ \lbrace s_\tau \rbrace_{\tau=1}^t , \lbrace \epsilon_\tau \rbrace_{\tau=1}^t }} ; \theta \Big) = \max_{a_{t}} \Bigg\lbrace u\left(s_t, a_{t} ; \theta\right) + {\color{red}{\epsilon_{it}}} + \beta \mathbb E_{s_{t+1}, {\color{red}{\epsilon_{t+1}}}} \Big[V\left(s_{t+1}, {\color{red}{\epsilon_{it+1}}} ; \theta\right) \ \Big|
\ {\color{red}{ \lbrace s_\tau \rbrace_{\tau=1}^t , \lbrace \epsilon_\tau \rbrace_{\tau=1}^t }}, a_{t} ; \theta\Big] \Bigg\rbrace
$$ &lt;strong&gt;Issues&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The problem is &lt;strong&gt;not Markow&lt;/strong&gt; anymore
&lt;ul&gt;
&lt;li&gt;Is $\epsilon_t$ correlated with $\epsilon_{t-\tau}$? How?&lt;/li&gt;
&lt;li&gt;Is $\epsilon_t$ correlated with $s_t$? And $s_{t-\tau}$? How?&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Dimension of the &lt;strong&gt;state space&lt;/strong&gt; has increased
&lt;ul&gt;
&lt;li&gt;From
$k = (k \text{ points})^{1 \text{ variable} \times 1 \text{ period}}$
points, to
$\infty = (k \text{ points})^{3 \text{ variables} \times \infty \text{ periods}}$
🤯🤯&lt;/li&gt;
&lt;li&gt;Assuming all variables assume $k$ values&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Number of variables to integrate over to compute &lt;strong&gt;expectation&lt;/strong&gt;
$\mathbb E$ has increased
&lt;ul&gt;
&lt;li&gt;From one variable, $s$, to three,
$(s, \epsilon_{0}, \epsilon_{1})$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;assumptions&#34;&gt;Assumptions&lt;/h3&gt;
&lt;p&gt;Rust makes &lt;strong&gt;4 assumptions&lt;/strong&gt; to make the problem tractable:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;First order Markow process of $\epsilon$&lt;/li&gt;
&lt;li&gt;Conditional independence of $\epsilon_t | s_t$ from $\epsilon_{t-1}$
and $s_{t-1}$&lt;/li&gt;
&lt;li&gt;Independence of $\epsilon_t$ from $s_t$&lt;/li&gt;
&lt;li&gt;Logit distribution of $\epsilon$&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;assumption-1&#34;&gt;Assumption 1&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;A1&lt;/strong&gt;: first-order markov process of $\epsilon$ $$
\Pr \Big(s_{t+1}, \epsilon_{t+1} \Big| s_{1}, &amp;hellip;, s_t, \epsilon_{1}, &amp;hellip;, \epsilon_{t}, a_{t} ; \theta\Big) = \Pr \Big(s_{t+1}, \epsilon_{t+1} \Big| s_t, \epsilon_{t}, a_{t} ; \theta \Big)
$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;What it buys&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$s$ and $\epsilon$ prior to current period are irrelevant&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;What it still allows&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;allows $s_t$ to be correlated with $\epsilon_t$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;What are we assuming away&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Any sort of longer run dependence&lt;/li&gt;
&lt;li&gt;Does it matter? If yes, just re-consider what is one time period&lt;/li&gt;
&lt;li&gt;Or make the state space larger (as usual in Markow processes)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;assumption-1---implications&#34;&gt;Assumption 1 - Implications&lt;/h3&gt;
&lt;p&gt;The Bellman Equation becomes $$
V\left(s_t, {\color{red}{\epsilon_{t}}} ; \theta\right) = \max_{a_{t}} \Bigg\lbrace u\left(s_t, a_{t} ; \theta\right) + {\color{red}{\epsilon_{a_{t} t}}} + \beta \mathbb E_{s_{t+1}, {\color{red}{\epsilon_{t+1}}}} \Big[V(s_{t+1}, {\color{red}{\epsilon_{t+1}}} ; \theta) \ \Big| \ s_t, a_{t}, {\color{red}{\epsilon_{t}}} ; \theta \Big] \Bigg\rbrace
$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Now the &lt;strong&gt;state&lt;/strong&gt; is $(s_t, \epsilon_t)$
&lt;ul&gt;
&lt;li&gt;sufficient, because defines both current utility and (the
expectation of) next-period state, under the first-order Markov
assumption&lt;/li&gt;
&lt;li&gt;$\epsilon_t$ is now analogous to $s_t$&lt;/li&gt;
&lt;li&gt;State space is not
$k^3 = (k \text{ points})^{3 \text{ variables} \times 1 \text{ period}}$
&lt;ul&gt;
&lt;li&gt;From
$\infty = (k \text{ points})^{3 \text{ variables} \times \infty \text{ periods}}$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Now we could use &lt;strong&gt;value function iteration&lt;/strong&gt; to solve the problem
&lt;ul&gt;
&lt;li&gt;If $\epsilon_t$ is continuous, has to be discretised&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;assumption-1---issues&#34;&gt;Assumption 1 - Issues&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Open issues&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Curse of dimensionality in the state space&lt;/strong&gt;:
($s_t, \epsilon_{0t}, \epsilon_{1t}$)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Before, there were $k$ points in state space (discrete values of
$x$)&lt;/li&gt;
&lt;li&gt;Now, for the same grid size, there are $k^3$ : $k$ each for $s$,
$\epsilon_0$, $\epsilon_1$
&lt;ul&gt;
&lt;li&gt;(Assuming we discretize all state variables into $k$ values)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Generally, number of points in state space (and thus
computational time) increases exponentially in the number of
variables&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Curse of dimensionality in the expected value&lt;/strong&gt;:
$\mathbb E_{s_{t+1}, \epsilon_{0,t+1}, \epsilon_{1,t+1}}$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;For each point in state space (at each iteration of the
contraction mapping), need to compute&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$
\mathbb E_{s_{t+1}, \epsilon_{t+1}} \Big[V (s_{t+1}, \epsilon_{t+1} ; \theta) \ \Big|  \ s_t, a_{t}, \epsilon_{t} ; \theta \Big]
$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Before, this was a 1-dimensional integral (or sum), now it’s
3-dimensional&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Initial conditions&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;assumption-2&#34;&gt;Assumption 2&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;A2&lt;/strong&gt;: conditional independence of $\epsilon_t | s_t$ from
$\epsilon_{t-1}$ and $s_{t-1}$ $$
\Pr \Big(s_{t+1}, \epsilon_{t+1} \Big| s_t, \epsilon_{t}, a_{t} ; \theta \Big) = \Pr \Big( \epsilon_{t+1} \Big| s_{t+1} ; \theta \Big) \Pr \Big( s_{t+1} \Big| s_t, a_{t} ; \theta \Big)
$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;What it buys&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$s_{t+1}$ is independent of $\epsilon_t$&lt;/li&gt;
&lt;li&gt;$\epsilon_{t+1}$ is independent of $\epsilon_t$ and $s_t$,
conditional on $s_{t+1}$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;What it still allows&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\epsilon$ can be correlated across time, but only through the
$s$ process&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;What are we assuming away&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Any time of persistent heterogeneity&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Does it matter? Easily yes&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;There are tons of applications where the unobservables are
either fixed or correlated over time&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;If fixed, there are methods to handle unobserved
heterogeneity (i.e. bus “types”)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;assumption-2---implications&#34;&gt;Assumption 2 - Implications&lt;/h3&gt;
&lt;p&gt;The Bellman Equation is $$
V\left(s_t, {\color{red}{\epsilon_{t}}} ; \theta\right) = \max_{a_{t}} \Bigg\lbrace u\left(s_t, a_{t} ; \theta\right) + {\color{red}{\epsilon_{a_{t} t}}} + \beta \mathbb E_{s_{t+1}, {\color{red}{\epsilon_{t+1}}}} \Big[V (s_{t+1}, {\color{red}{\epsilon_{t+1}}} ; \theta) \Big| s_t, a_{t} ; \theta \Big] \Bigg\rbrace
$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Now $\epsilon_{t}$ is noise that &lt;strong&gt;doesn’t affect the future&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;That is, conditional on $s_{t+1}$, $\epsilon_{t+1}$ is
uncorrelated with $\epsilon_{t}$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Remeber&lt;/strong&gt;: if $\epsilon$ does not affect the future, it should’t be
in the state space!&lt;/p&gt;
&lt;p&gt;How? Integrate it out.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;rust-shortcut-asv&#34;&gt;Rust Shortcut: ASV&lt;/h3&gt;
&lt;p&gt;Rust: define the &lt;strong&gt;alternative-specific value function&lt;/strong&gt; $$
\begin{align}
&amp;amp;\bar V_0 \left(s_t ; \theta\right) = u\left(s_t, 0 ; \theta\right) + \beta \mathbb E_{s_{t+1}, {\color{red}{\epsilon_{t+1}}}} \Big[V\left(s_{t+1}, {\color{red}{\epsilon_{t+1}} }; \theta\right) | s_t, a_{t}=0 ; \theta\Big] \newline
&amp;amp;\bar V_1 \left(s_t ; \theta\right) = u\left(s_t, 1 ; \theta\right) + \beta \mathbb E_{s_{t+1}, {\color{red}{\epsilon_{t+1}}}} \Big[V\left(s_{t+1}, {\color{red}{\epsilon_{t+1}}} ; \theta\right) | s_t, a_{t}=1 ; \theta\Big]
\end{align}
$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;$\bar V_0 (s_t)$ is the present discounted value of not replacing,
net of $\epsilon_{0t}$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;The state does not depend on&lt;/strong&gt; $\epsilon_{t}$!&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;What is the relationship with the value function? $$
V\left(s_t, \epsilon_{t} ; \theta\right) = \max_{a_{t}} \Bigg\lbrace \begin{array}{l}
\bar V_0 \left(s_t ; \theta\right)+\epsilon_{0 t}
\ ; \newline
\bar V_1 \left(s_t ; \theta\right)+\epsilon_{1 t}
\end{array} \Bigg\rbrace
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;We have a 1-to-1 mapping between
$V\left(s_t, \epsilon_{t} ; \theta\right)$ and
$\bar V_a \left(s_t ; \theta\right)$ !&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;If we have one, we can get the other&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;rust-shortcut&#34;&gt;Rust Shortcut&lt;/h3&gt;
&lt;p&gt;Can we solve for $\bar V$?&lt;/p&gt;
&lt;p&gt;Yes! They have a &lt;strong&gt;recursive formulation&lt;/strong&gt; $$
\begin{aligned}
&amp;amp;
\bar V_0 \left(s_t ; \theta\right) = u\left(s_t, 0 ; \theta\right) + \beta \mathbb E_{s_{t+1}, {\color{red}{\epsilon_{t+1}}}} \Bigg[ \max_{a_{t+1}} \Bigg\lbrace \begin{array}{l}
\bar V_0 \left(s_{t+1} ; \theta\right) + {\color{red}{\epsilon_{0 t+1}}}
\ ; \newline
\bar V_1 \left(s_{t+1} ; \theta\right) + {\color{red}{\epsilon_{1 t+1}}}
\end{array} \Bigg\rbrace \ \Bigg| \ s_t, a_{t}=0 ; \theta \Bigg] \newline
&amp;amp;
\bar V_1 \left(s_t ; \theta\right) = u\left(s_t, 1 ; \theta\right) + \beta \mathbb E_{s_{t+1}, {\color{red}{\epsilon_{t+1}}}} \Bigg[ \max_{a_{t+1}} \Bigg\lbrace \begin{array}{l}
\bar V_0 \left(s_{t+1} ; \theta\right) + {\color{red}{\epsilon_{0 t+1}}}
\ ; \newline
\bar V_1 \left(s_{t+1} ; \theta\right) + {\color{red}{\epsilon_{1 t+1}}}
\end{array} \Bigg\rbrace \ \Bigg| \ s_t, a_{t}=1 ; \theta \Bigg] \newline
\end{aligned}
$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Rust (&lt;a href=&#34;#ref-rust1988maximum&#34;&gt;1988&lt;/a&gt;) shows that it’s a joint
contraction mapping&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Memo&lt;/strong&gt;: the state space now is
$2k = (2 \text{ actions}) \times (k \text{ points})^{1 \text{ variables} \times 1 \text{ period}}$
&lt;ul&gt;
&lt;li&gt;instead of
$3^k = (k \text{ points})^{3 \text{ variables} \times 1 \text{ period}}$&lt;/li&gt;
&lt;li&gt;Much smaller!&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Lesson&lt;/strong&gt;: any state variable that does not affect continuation
values (the future) does not have to be in the “actual” state spa&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;assumption-2---implications-1&#34;&gt;Assumption 2 - Implications&lt;/h3&gt;
&lt;p&gt;We can also &lt;strong&gt;split the expectation&lt;/strong&gt; in the alternative-specific value
function $$
\begin{aligned}
&amp;amp;
\bar V_0 \left(s_t ; \theta\right) = u\left(s_t, 0 ; \theta\right) + \beta \mathbb E_{s_{t+1}} \Bigg[ \mathbb E_{{\color{red}{\epsilon_{t+1}}}} \Bigg[ \max_{a_{t+1}} \Bigg\lbrace \begin{array}{l}
\bar V_0 \left(s_{t+1} ; \theta\right) + {\color{red}{\epsilon_{0 t+1}}}
\ ; \newline
\bar V_1 \left(s_{t+1} ; \theta\right) + {\color{red}{\epsilon_{1 t+1}}}
\end{array} \Bigg\rbrace \ \Bigg| \ s_t \Bigg] \ \Bigg| \ s_t, a_{t}=0 ; \theta \Bigg] \newline
&amp;amp;
\bar V_1 \left(s_t ; \theta\right) = u\left(s_t, 1 ; \theta\right) + \beta \mathbb E_{s_{t+1}} \Bigg[ \mathbb E_{{\color{red}{\epsilon_{t+1}}}} \Bigg[ \max_{a_{t+1}} \Bigg\lbrace \begin{array}{l}
\bar V_0 \left(s_{t+1} ; \theta\right) + {\color{red}{\epsilon_{0 t+1}}}
\ ; \newline
\bar V_1 \left(s_{t+1} ; \theta\right) + {\color{red}{\epsilon_{1 t+1}}}
\end{array} \Bigg\rbrace \ \Bigg| \ s_t \Bigg] \ \Bigg| \ s_t, a_{t}=1 ; \theta \Bigg] \newline
\end{aligned}
$$ This allows us to concentrate on one single term $$
\mathbb E_{{\color{red}{\epsilon_{t+1}}}} \Bigg[ \max_{a_{t+1}} \Bigg\lbrace \begin{array}{l}
\bar V_0 \left(s_{t+1} ; \theta\right) + {\color{red}{\epsilon_{0 t+1}}}
\ ; \newline
\bar V_1 \left(s_{t+1} ; \theta\right) + {\color{red}{\epsilon_{1 t+1}}}
\end{array} \Bigg\rbrace \ \Bigg| \ s_t \Bigg]
$$ &lt;strong&gt;Open issues&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Distribution of $\epsilon_{t+1}$ has to be simulated&lt;/li&gt;
&lt;li&gt;Distribution of $\epsilon_{t+1}$ depends on $s_t$&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;assumption-3&#34;&gt;Assumption 3&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;A3&lt;/strong&gt;: independence of $\epsilon_t$ from $s_t$ $$
\Pr \Big( \epsilon_{t+1} \Big| s_{t+1} ; \theta \Big) \Pr \Big( s_{t+1} \Big| s_t, a_{t} ; \theta \Big) = \Pr \big( \epsilon_{t+1} \big| \theta \big) \Pr \Big( s_{t+1} \Big| s_t, a_{t} ; \theta \Big)
$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;What it buys&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\epsilon$ not correlated with anything $$
\mathbb E_{{\color{red}{\epsilon_{t+1}}}} \Bigg[ \max_{a_{t+1} \in \lbrace 0, 1 \rbrace } \Bigg\lbrace \begin{array}{l}
\bar V_0 \left(s_{t+1} ; \theta\right) + {\color{red}{\epsilon_{0 t+1}}}
\ ; \newline
\bar V_1 \left(s_{t+1} ; \theta\right) + {\color{red}{\epsilon_{1 t+1}}}
\end{array} \Bigg\rbrace \ \Bigg]
$$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;What are we assuming away&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Some state-specific noise… probably irrelevant&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Open Issues&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Distribution of $\epsilon_{t+1}$ has to be simulated&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;assumption-4&#34;&gt;Assumption 4&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;A4&lt;/strong&gt;: $\epsilon$ is type 1 extreme value distributed (logit)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;What it buys&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Closed for solution for $\mathbb E_{\epsilon_{t+1}}$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;What are we assuming away&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Different substitution patterns&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Relevant? Maybe, if there are at least three options (here
binary choice)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;As logit assumption in demand estimation&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Logit magic&lt;/strong&gt; 🧙🪄 $$
\mathbb E_{\epsilon} \Bigg[ \max_n \bigg( \Big\lbrace \delta_n + \epsilon_n \Big\rbrace_{n=1}^N \bigg) \Bigg] = 0.5772 + \ln \bigg( \sum_{n=1}^N e^{\delta_n} \bigg)
$$&lt;/p&gt;
&lt;p&gt;where $0.5772$ is Euler’s constant&lt;/p&gt;
&lt;h3 id=&#34;assumption-4---implications&#34;&gt;Assumption 4 - Implications&lt;/h3&gt;
&lt;p&gt;The Bellman equation becomes $$
\begin{aligned}
&amp;amp;
\bar V_0 \left(s_t ; \theta\right) = u\left(s_t, 0 ; \theta\right) + \beta \mathbb E_{s_{t+1}} \Bigg[ 0.5772 + \ln \Bigg( \sum_{a&#39; \in \lbrace 0, 1 \rbrace} e^{\bar V_{a&#39;} (s_{t+1} ; \theta)} \Bigg) \ \Bigg| \ s_t, a_{t}=0 ; \theta \Bigg] \newline
&amp;amp;
\bar V_1 \left(s_t ; \theta\right) = u\left(s_t, 1 ; \theta\right) + \beta \mathbb E_{s_{t+1}} \Bigg[ 0.5772 + \ln \Bigg( \sum_{a&#39; \in \lbrace 0, 1 \rbrace} e^{\bar V_{a&#39;} (s_{t+1} ; \theta)} \Bigg) \ \Bigg| \ s_t, a_{t}=1 ; \theta \Bigg] \newline
\end{aligned}
$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;We got &lt;strong&gt;fully rid of $\epsilon$&lt;/strong&gt;!
&lt;ul&gt;
&lt;li&gt;How? With a lot of assumptions&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;estimation&#34;&gt;Estimation&lt;/h3&gt;
&lt;p&gt;So far we have analysized how the &lt;strong&gt;4 assumptions&lt;/strong&gt; help &lt;strong&gt;solving the
model&lt;/strong&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;What about &lt;strong&gt;estimation&lt;/strong&gt;?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Maximum Likelihood&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;For a single bus, the &lt;strong&gt;likelihood function&lt;/strong&gt; is&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$
\mathcal L = \Pr \Big(s_{1}, &amp;hellip; , s_t, a_{0}, &amp;hellip; , a_{T} \ \Big| \ s_{0} ; \theta\Big)
$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;i.e. probability of observed decisions
$\lbrace a_{0}, &amp;hellip; , a_{T} \rbrace$&lt;/li&gt;
&lt;li&gt;and sequence of states $\lbrace s_{1}, &amp;hellip; , s_t \rbrace$&lt;/li&gt;
&lt;li&gt;conditional on the initial state $s_0$&lt;/li&gt;
&lt;li&gt;and the parameter values $\theta$&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;What is the impact of the 4 assumptions on the likelihood function?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;likelihood-function-a1&#34;&gt;Likelihood Function (A1)&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;A1&lt;/strong&gt;: First order Markow process of $\epsilon$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;We gain independence across time&lt;/li&gt;
&lt;li&gt;We can decompose the joint distribution in marginals across time&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$
\begin{align}
\mathcal L(\theta) &amp;amp;=  \Pr \Big(s_{1}, &amp;hellip; , s_t, a_{0}, &amp;hellip; , a_{T} \Big| s_{0} ; \theta\Big)\newline
&amp;amp;=  \prod_{t=1}^T \Pr \Big(a_{t+1} , s_{t+1} \Big| s_t, a_t ; \theta\Big)
\end{align}
$$&lt;/p&gt;
&lt;h3 id=&#34;likelihood-function-a2&#34;&gt;Likelihood Function (A2)&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;A2&lt;/strong&gt;: independence of $\epsilon_t$ from $\epsilon_{t-1}$ and $s_{t-1}$
on $s_t$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;We can decompose the joint distribution of $a_t$ and $s_{t+1}$ into
marginals $$
\begin{align}
\mathcal L(\theta) &amp;amp;= \prod_{t=1}^T \Pr \Big(a_{t+1} , s_{t+1} \Big| s_t, a_t ; \theta\Big)
= \newline
&amp;amp;= \prod_{t=1}^T \Pr \big(a_t \big| s_t ; \theta\big) \Pr \Big(s_{t+1} \Big| s_t, a_t ; \theta\Big)
\end{align}
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$\Pr \big(s_{t+1} \big| s_t, a_t ; \theta\big)$ can be estimated
from the data&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;we’ll come back to it&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;for $\Pr \big(a_t \big| s_t ; \theta\big)$ we need the two remaining
assumptions&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;likelihood-function-a3&#34;&gt;Likelihood Function (A3)&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;A3&lt;/strong&gt;: Independence of $\epsilon_t$ from $s_t$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;No need to condition on $s_t$&lt;/li&gt;
&lt;li&gt;E.g. probability of replacement&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$
\begin{align}
\Pr \big(a_t=1 \big| s_t ; \theta \big) &amp;amp;= \Pr \Big( \bar V_1 (s_{t+1} ; \theta) + \epsilon_{1 t+1} \geq \bar V_0 (s_{t+1} ; \theta) + \epsilon_{0 t+1} \ \Big| \ s_t ; \theta \Big)
= \newline
&amp;amp;= \Pr \Big( \bar V_1 (s_{t+1} ; \theta) + \epsilon_{1 t+1} \geq \bar V_0 (s_{t+1} ; \theta) + \epsilon_{0 t+1} \ \Big| \ \theta \Big)
\end{align}
$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;In words: same distribution of shocks in every state&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;likelihood-function-a4&#34;&gt;Likelihood Function (A4)&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;A4&lt;/strong&gt;: Logit distribution of $\epsilon$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;E.g. probability of replacement becomes&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$
\begin{align}
\Pr \big(a_t=1 \big| s_t ; \theta \big) &amp;amp;= \Pr \Big( \bar V_1 (s_{t+1} ; \theta) + \epsilon_{1 t+1} \geq \bar V_0 (s_{t+1} ; \theta) + \epsilon_{0 t+1} \ \Big| \ \theta \Big)
= \newline
&amp;amp;= \frac{e^{\bar V_1 (s_{t+1} ; \theta)}}{e^{\bar V_0 (s_{t+1} ; \theta)} + e^{\bar V_1 (s_{t+1} ; \theta)}}
\end{align}
$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;We have a closed form expression!&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;likelihood-function&#34;&gt;Likelihood Function&lt;/h3&gt;
&lt;p&gt;The &lt;strong&gt;final form&lt;/strong&gt; of the likelihood function for one bus is $$
\mathcal L(\theta) = \prod_{t=1}^T \Pr\big(a_t \big| s_t ; \theta \big) \Pr \Big(s_{t+1} \ \Big| \ s_t, a_t ; \theta\Big)
$$ where&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\Pr \Big(s_{t+1} \ \Big| \ s_t, a_t ; \theta\Big)$ can be estimated
from the data
&lt;ul&gt;
&lt;li&gt;given mileage $x$ and investment decision $a$, what are the
observed frequencies of future states $x&#39;$?&lt;/li&gt;
&lt;li&gt;does not have to depend on $\theta$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;$\Pr\big(a_t \big| s_t ; \theta \big)$ depends on
$\bar V_a (s ; \theta)$
&lt;ul&gt;
&lt;li&gt;$\bar V_a (s ; \theta)$ we know how to compute&lt;/li&gt;
&lt;li&gt;given a value of $\theta$&lt;/li&gt;
&lt;li&gt;solve by value function iteration&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;likelihood-function-2&#34;&gt;Likelihood Function (2)&lt;/h3&gt;
&lt;p&gt;Since we have may buses, $j$, the likelihood of the data is $$
\mathcal L(\theta) = \prod_{j} \mathcal L_j (\theta) = \prod_{j} \prod_{t=1}^T \Pr\big(a_{jt} \big| s_{jt} ; \theta \big) \Pr \Big(s_{j,t+1} \ \Big| \ s_{jt}, a_{jt} ; \theta\Big)
$$ And, as usual, we prefer to with log-likelihoods $$
\log \mathcal L(\theta) = \sum_{j} \sum_{t=1}^T \Bigg( \log \Pr\big(a_{jt} \big| s_{jt} ; \theta \big) + \log\Pr \Big(s_{j,t+1} \ \Big| \ s_{jt}, a_{jt} ; \theta\Big) \Bigg)
$$&lt;/p&gt;
&lt;h3 id=&#34;estimation-1&#34;&gt;Estimation&lt;/h3&gt;
&lt;p&gt;Now we have all the pieces to estimate $\theta$!&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Procedure&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Estimate the state transition probabilities
$\Pr \big(s_{t+1} \big| s_t, a_t ; \theta\big)$&lt;/li&gt;
&lt;li&gt;Select a value of $\theta$&lt;/li&gt;
&lt;li&gt;Init a choice-specific value function
$\bar V^{(0)}&lt;em&gt;a (s&lt;/em&gt;{t+1} ; \theta)$
&lt;ol&gt;
&lt;li&gt;Apply the Bellman operator to compute
$\bar V^{(1)}&lt;em&gt;a (s&lt;/em&gt;{t+1} ; \theta)$&lt;/li&gt;
&lt;li&gt;Iterate until convergence to
$\bar V_d^{(k \to \infty)} (s_{t+1} ; \theta)$ (&lt;em&gt;inner loop&lt;/em&gt;)&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;Compute the choice probabilities
$\Pr \big(a_t\big| s_t ; \theta \big)$&lt;/li&gt;
&lt;li&gt;Compute the likelihood
$\mathcal L = \prod_j \prod_{t=1}^T \Pr \big(a_t \big| s_t ; \theta\big) \Pr \Big(s_{t+1} \Big| s_t, a_t ; \theta\Big)$&lt;/li&gt;
&lt;li&gt;Iterate (2-5) until you are have found a (possibly global) minimum
(&lt;em&gt;outer loop&lt;/em&gt;)&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;results&#34;&gt;Results&lt;/h3&gt;
&lt;p&gt;What do &lt;strong&gt;dynamics&lt;/strong&gt; add?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Static&lt;/strong&gt; demand curve ($\beta =0$) is much more sensitive to the
price of engine replacement. Why?
&lt;ul&gt;
&lt;li&gt;Compares present price with present savings&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;If you compare present price with flow of future benefits, you are
less price sensitive
&lt;ul&gt;
&lt;li&gt;More realistic&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;img src=&#34;../img/7_01.png&#34; style=&#34;width:50.0%&#34; /&gt;
&lt;h3 id=&#34;extensions&#34;&gt;Extensions&lt;/h3&gt;
&lt;p&gt;Main &lt;strong&gt;limitation&lt;/strong&gt; of Rust (&lt;a href=&#34;#ref-rust1987optimal&#34;&gt;1987&lt;/a&gt;): &lt;strong&gt;value
function iteration&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Costly: has to be done for each parameter explored during
optimization&lt;/li&gt;
&lt;li&gt;Particularly costly if the state space is large&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Solutions&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Solve the model without solving a fixed point problem
&lt;ul&gt;
&lt;li&gt;Hotz and Miller (&lt;a href=&#34;#ref-hotz1993conditional&#34;&gt;1993&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Solve the model and estimate the parameters at the same time
&lt;ul&gt;
&lt;li&gt;Inner and outer loop in parallel&lt;/li&gt;
&lt;li&gt;Imai, Jain, and Ching (&lt;a href=&#34;#ref-imai2009bayesian&#34;&gt;2009&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Treat the estimation as a constrained optimization problem
&lt;ul&gt;
&lt;li&gt;MPEC, as for demand&lt;/li&gt;
&lt;li&gt;Use off-the-shelf optimization algorithms&lt;/li&gt;
&lt;li&gt;Su and Judd (&lt;a href=&#34;#ref-su2012constrained&#34;&gt;2012&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;We’ll cover Hotz and Miller (&lt;a href=&#34;#ref-hotz1993conditional&#34;&gt;1993&lt;/a&gt;) since at
the core of the estimation of dynamic games.&lt;/p&gt;
&lt;h2 id=&#34;hotz--miller-1993&#34;&gt;Hotz &amp;amp; Miller (1993)&lt;/h2&gt;
&lt;h3 id=&#34;motivation-1&#34;&gt;Motivation&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Setting&lt;/strong&gt;: Harold Zurcher problem&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;same model&lt;/li&gt;
&lt;li&gt;same assumptions&lt;/li&gt;
&lt;li&gt;same notation&lt;/li&gt;
&lt;li&gt;same objective&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Problem&lt;/strong&gt;: computationally intense to do value function iteration&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Can we solve the model without solving a fixed point problem?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;estimation-in-rust&#34;&gt;Estimation in Rust&lt;/h3&gt;
&lt;p&gt;How did we estimate the model in Rust? &lt;strong&gt;Two main equations&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Solve the &lt;strong&gt;Bellman equation&lt;/strong&gt; of the alternative-specific value
function $$
{\color{green}{\bar V(s; \theta)}} = \tilde f( {\color{green}{\bar V(s; \theta)}})
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Compute the &lt;strong&gt;expected policy function&lt;/strong&gt; $$
{\color{blue}{P( \cdot | s; \theta)}} = \tilde g( {\color{green}{\bar V(s; \theta)}} ; \theta)
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Maximize the likelihood function&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;$$
\mathcal L(\theta) = \prod_{j} \prod_{t=1}^T {\color{blue}{ \Pr\big(a_{jt} \big| s_{jt} ; \theta \big)}} \Pr \Big(s_{j,t+1} \ \Big| \ s_{jt}, a_{jt} ; \theta\Big)
$$&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Can we remove step 1?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;hotz--miller-ideas&#34;&gt;Hotz &amp;amp; Miller Idea(s)&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Idea 1&lt;/strong&gt;: it would be great if we could start from something like $$
{\color{blue}{P(\cdot|s; \theta)}} = T( {\color{blue}{P(\cdot|s; \theta)}} ; \theta)
$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;No need to solve for the value function&lt;/li&gt;
&lt;li&gt;But we would still need a to solve a &lt;strong&gt;fixed point&lt;/strong&gt; problem&lt;/li&gt;
&lt;li&gt;Back from the start? No&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Idea 2&lt;/strong&gt;: could replace the RHS element with a &lt;strong&gt;consistent estimate&lt;/strong&gt;
$$
{\color{blue}{P(\cdot|s; \theta)}} = T( {\color{red}{\hat P(\cdot|s; \theta)}} ; \theta)
$$ And this could give us an estimating equation!&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Unclear? No problem, let’s go slowly step by step&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;two-main-equations&#34;&gt;Two Main Equations&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Bellman equation&lt;/strong&gt; $$
{\color{green}{\bar V_a \left(s_t ; \theta\right)}} = u\left(s_t, a ; \theta\right) + \beta \mathbb E_{s_{t+1}, \epsilon_{t+1}} \Bigg[ \max_{a&#39;} \Big\lbrace {\color{green}{\bar V_{a&#39;}}} \left(s_{t+1}; \theta\right) + \epsilon_{a&#39;,t+1} \Big\rbrace \ \Big| \ s_t, a_t=a ; \theta \Bigg]
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Expected policy function&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;$$
{\color{blue}{\Pr \big(a_t=a \big| s_t ; \theta \big)}} = \Pr \Big( {\color{green}{\bar V_a (s_{t+1} ; \theta)}} + \epsilon_{a, t+1} \geq {\color{green}{\bar V_{a&#39;} (s_{t+1} ; \theta)}} + \epsilon_{a&#39;, t+1} , \ \forall a&#39; \ \Big| \ \theta \Big)
$$&lt;/p&gt;
&lt;p&gt;Expected decision &lt;strong&gt;before&lt;/strong&gt; the shocks $\epsilon_t$ are realized&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Not&lt;/strong&gt; the policy function
&lt;ul&gt;
&lt;li&gt;The policy function maps
$s_t \times \epsilon \to \lbrace 0 , 1 \rbrace$&lt;/li&gt;
&lt;li&gt;The expected policy function maps $s_t \to [ 0 , 1 ]$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Easier to work with: does not depend on the shocks
&lt;ul&gt;
&lt;li&gt;Not a deterministic policy, but a stochastic one&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;hotz--miller---idea-1&#34;&gt;Hotz &amp;amp; Miller - Idea 1&lt;/h3&gt;
&lt;p&gt;How do we get from the two equations $$
\begin{aligned}
{\color{green}{\bar V(s; \theta)}} &amp;amp;= \tilde f( {\color{green}{\bar V(s; \theta)}}) \newline
{\color{blue}{P(\cdot|s; \theta)}} &amp;amp;= \tilde g( {\color{green}{\bar V(s; \theta)}} ; \theta)
\end{aligned}
$$ To one? $$
{\color{blue}{P(\cdot|s; \theta)}} = T ({\color{blue}{P(\cdot|s; \theta)}}; \theta)
$$ If we could express $\bar V$ in terms of $P$, … $$
\begin{aligned}
{\color{green}{\bar V(s; \theta)}} &amp;amp; = \tilde h( {\color{blue}{P(\cdot|s; \theta)}})  \newline
{\color{blue}{P(\cdot|s; \theta)}} &amp;amp;= \tilde g( {\color{green}{\bar V(s; \theta)}} ; \theta)
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;…. we could then substitute the first equation into the second …&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;But&lt;/strong&gt;, easier to work with a different representation of the value
function.&lt;/p&gt;
&lt;h3 id=&#34;expected-value-function&#34;&gt;Expected Value Function&lt;/h3&gt;
&lt;p&gt;Recall Rust &lt;strong&gt;value function&lt;/strong&gt; (not the alternative-specific $\bar V$)
$$
V\left(s_t, \epsilon_t ; \theta\right) = \max_{a_{t}} \Bigg\lbrace u \left( s_t, a_{t} ; \theta \right)  + \epsilon_{a_{t} t} + \beta \mathbb E_{s_{t+1}, \epsilon_{t+1}} \Big[V\left(s_{t+1}, \epsilon_{t+1} ; \theta\right) \Big| s_t, a_{t} ; \theta\Big] \Bigg\rbrace
$$ We can express it in terms of &lt;strong&gt;expected value function&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;$$
V\left(s_t ; \theta\right) = \mathbb E_{\epsilon_t} \Bigg[ \max_{a_{t}} \Bigg\lbrace u\left(s_t, a_t ; \theta\right) + \epsilon_{a_{t} t}+ \beta \mathbb E_{s_{t+1}} \Big[V\left(s_{t+1}; \theta\right) \Big| s_t, a_{t} ; \theta\Big] \Bigg\rbrace \Bigg]
$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Value of being in state $s_t$ without knowing the realization of the
shock $\epsilon_t$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;“Value of Harold Zurcher before opening the window and seeing
if it’s raining or not”&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Analogous to the relationship between policy funciton and expected
policy function&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;expectation of future value now is only over $s_{t+1}$&lt;/li&gt;
&lt;li&gt;$V\left(s_t ; \theta\right)$ can be solved via value function
iteration as the operator on the RHS is a contraction&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;representation-equivalence&#34;&gt;Representation Equivalence&lt;/h3&gt;
&lt;p&gt;Recall the &lt;strong&gt;alternative-specific value function&lt;/strong&gt; of Rust&lt;/p&gt;
&lt;p&gt;$$
\begin{align}
{\color{green}{\bar V_a \left( s_t ; \theta\right)}} &amp;amp;= u\left(s_t, d ; \theta\right) + \beta \mathbb E_{s_{t+1}, \epsilon_{t+1}} \Bigg[ \max_{a&#39;} \Big\lbrace {\color{green}{\bar V_{a&#39;} \left(s_{t+1}; \theta\right)}} + \epsilon_{a&#39;,t+1} \Big\rbrace \ \Big| \ s_t, a_t=a ; \theta \Bigg] \newline
&amp;amp;=u\left(s_t, a ; \theta\right)+\beta \mathbb E_{s_{t+1}, \epsilon_{t+1}} \Big[ {\color{orange}{V \left( s_{t+1}, \epsilon_{t+1} ; \theta \right)}} \Big| s_t, a_t=a ; \theta \Big]
\newline
&amp;amp;= u \left( s_t, a ; \theta \right) + \beta \mathbb E_{s_{t+1}} \Big[ {\color{red}{V \left( s_{t+1} ; \theta \right)}} \Big| s_t, a_t=a; \theta \Big]
\end{align}
$$&lt;/p&gt;
&lt;p&gt;Relationship with the &lt;strong&gt;value function&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;$$
{\color{orange}{V \left(s_t, \epsilon_{t} ; \theta \right)}} = \max_{a_{t}} \Big\lbrace {\color{green}{ \bar  V_0 \left( s_t ; \theta \right)}} + \epsilon_{0t}, {\color{green}{\bar V_1 \left( s_t ; \theta \right)}} + \epsilon_{1t} \Big\rbrace
$$&lt;/p&gt;
&lt;p&gt;Relationship with the &lt;strong&gt;expected value function&lt;/strong&gt; $$
{\color{red}{V\left(s_t ; \theta\right)}} = \mathbb E_{\epsilon_t} \Big[ {\color{orange}{V\left(s_t, \epsilon_{t} ; \theta\right)}} \ \Big| \ s_t \Big]
$$&lt;/p&gt;
&lt;h3 id=&#34;goal&#34;&gt;Goal&lt;/h3&gt;
&lt;p&gt;We switched from &lt;strong&gt;alternative-specific value function&lt;/strong&gt;
${\color{green}{\bar V (s_t ; \theta)}}$ to &lt;strong&gt;expected value function&lt;/strong&gt;
${\color{red}{V(s_t ; \theta)}}$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;But the goal is the same&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Go from this representation $$
\begin{align}
{\color{red}{V(s ; \theta)}} &amp;amp; = f( {\color{red}{V(s ; \theta)}}) \newline
{\color{blue}{P(\cdot | s ; \theta)}} &amp;amp; = g( {\color{red}{V(s ; \theta)}}; \theta)
\end{align}
$$ To this $$
\begin{align}
{\color{red}{V(s ; \theta)}} &amp;amp; = h( {\color{blue}{P(\cdot|s ; \theta)}} ; \theta) \newline
{\color{blue}{P(\cdot|s ; \theta)}} &amp;amp; = g({\color{red}{V(s ; \theta)}}; \theta)
\end{align}
$$ I.e. we want to express the &lt;strong&gt;expected value function (EV)&lt;/strong&gt; in terms
of the &lt;strong&gt;expected policy function (EP)&lt;/strong&gt;.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;**Note **: the $f$, $g$ and $h$ functions are different functions now.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;express-ev-in-terms-of-ep-1&#34;&gt;Express EV in terms of EP (1)&lt;/h3&gt;
&lt;p&gt;First, let’s ged rid of one operator: the &lt;strong&gt;max&lt;/strong&gt; operator $$
V\left(s_t ; \theta\right)
= \sum_a \Pr \Big(a_t=a | s_t ; \theta \Big) * \left[\begin{array}{c}
u\left(s_t, a ; \theta\right) + \mathbb E_{\epsilon_t} \Big[\epsilon_{at}\Big| a_t=a, s_t\Big] \newline \qquad + \beta \mathbb E_{s_{t+1}} \Big[V\left(s_{t+1} ; \theta\right) \Big| s_t, a_t=a ; \theta\Big]
\end{array}\right]
$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;We are just substituting the $\max$ with the policy
$\Pr\left(a_t=a| s_t ; \theta\right)$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Important: we got rid of the $\max$ operator&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;But we are still taking the expectation over&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Future states $s_{t+1}$&lt;/li&gt;
&lt;li&gt;Shocks $\epsilon_t$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;express-ev-in-terms-of-ep-2&#34;&gt;Express EV in terms of EP (2)&lt;/h3&gt;
&lt;p&gt;Now we get rid of another operator: the expectation over $s_{t+1}$ $$
\mathbb E_{s_{t+1}} \Big[V\left(s_{t+1} ; \theta\right) \Big| s_t, a_t=a ; \theta\Big] \qquad \to \qquad \sum_{s_{t+1}} V\left(s_{t+1} ; \theta\right) \Pr \Big(s_{t+1} \Big| s_t, a_t=a ; \theta \Big)
$$ where&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\sum_{s_{t+1}}$ is the summation over the next states&lt;/li&gt;
&lt;li&gt;$\Pr (s_{t+1} | s_t, a_t=a ; \theta )$ is the transition probability
(conditional on a particular choice)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;so that the expected value function becomes $$
V\left(s_t ; \theta\right)
= \sum_a \Pr \Big(a_t=a | s_t ; \theta \Big) * \left[\begin{array}{c}
u\left(s_t, a ; \theta\right) + \mathbb E_{\epsilon_t} \Big[\epsilon_{at}\Big| a_t=a, s_t\Big] \newline + \beta \sum_{s_{t+1}} V\left(s_{t+1} ; \theta\right) \Pr \Big(s_{t+1} \Big| s_t, a_t=a ; \theta \Big)
\end{array}\right]
$$&lt;/p&gt;
&lt;h3 id=&#34;express-ev-in-terms-of-ep-3&#34;&gt;Express EV in terms of EP (3)&lt;/h3&gt;
&lt;p&gt;The previous equation, was defined at the state level $s_t$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;system of $k$ equations, 1 for each state (value of $x$)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If we &lt;strong&gt;stack&lt;/strong&gt; them, we can write them as $$
V\left(s ; \theta\right)
= \sum_a \Pr \Big(a \ \Big| \ s ; \theta \Big) .* \Bigg[
u\left(s, a ; \theta\right) + \mathbb E_{\epsilon} \Big[\epsilon_{a} \ \Big| \ a, s \Big] + \beta \ T(a ; \theta) \ V(s ; \theta) \Bigg]
$$ where&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$T(a)$: $k \times k$ matrix of transition probabilities from state
$s_t$ to $s_{t+1}$, given decision $a$&lt;/li&gt;
&lt;li&gt;$.*$ is the dot product operator (or element-wise matrix
multiplication)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;express-ev-in-terms-of-ep-4&#34;&gt;Express EV in terms of EP (4)&lt;/h3&gt;
&lt;p&gt;Now we have a system of $k$ equations in $k$ unknowns that we can solve.&lt;/p&gt;
&lt;p&gt;Tearing down notation to the bare minimum, we have $$
V = \sum_a P_a .* \bigg[ u_a + \mathbb E [\epsilon_a ] + \beta \ T_a \ V \bigg]
$$ which we can rewrite as $$
V - \beta \  \left( \sum_a P_a .* T_a \right) V = \sum_a P_a .* \bigg[ u_a + \mathbb E [\epsilon_a ] \bigg]
$$&lt;/p&gt;
&lt;p&gt;and finally we can solve for $V$ through the famous &lt;strong&gt;Hotz and Miller
inversion&lt;/strong&gt; $$
V = \left[I - \beta \ \sum_a P_a .* T_a \right]^{-1} \ * \ \left( \sum_a P_a \ .* \ \bigg[ u_a + \mathbb E [\epsilon_a] \bigg] \right)
$$ Solved? No. We still need to do something about
$\mathbb E [\epsilon_a]$.&lt;/p&gt;
&lt;h3 id=&#34;express-ev-in-terms-of-ep-5&#34;&gt;Express EV in terms of EP (5)&lt;/h3&gt;
&lt;p&gt;What is $\mathbb E [\epsilon_a]$?&lt;/p&gt;
&lt;p&gt;Let’s consider for example the expected value of the shock, conditional
on investment $$
\begin{aligned}
\mathbb E \Big[\epsilon_{1 t} \Big| a_{t}=1, \cdot\Big] &amp;amp;= \mathbb E \Big[\epsilon_{ t} | \bar{V_1}\left(s_t ; \theta\right)+\epsilon_{1 t}&amp;gt;\bar{V}&lt;em&gt;{0}\left(s_t ; \theta\right)+\epsilon&lt;/em&gt;{0 t}\Big] \newline
&amp;amp; = \mathbb E\Big[\epsilon_{1 t} \Big| \bar{V_1}\left(s_t ; \theta\right)  - \bar V_0 \left(s_t ; \theta\right)&amp;gt;\epsilon_{0 t}-\epsilon_{1 t} \Big]
\end{aligned}
$$ with &lt;strong&gt;logit magic&lt;/strong&gt; 🧙🪄 is $$
\mathbb E\left[\epsilon_{1 t} | a_{t}=1, s_t\right] = 0.5772 - \ln \left(P\left(s_t ; \theta\right)\right)
$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;where $0.5772$ is Euler’s constant.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We again got rid of another $\max$ operator!&lt;/p&gt;
&lt;h3 id=&#34;express-ev-in-terms-of-ep-6&#34;&gt;Express EV in terms of EP (6)&lt;/h3&gt;
&lt;p&gt;Now we can substitute it back and we have an equation which is &lt;em&gt;just&lt;/em&gt; a
function of primitives $$
\begin{aligned}
V(\cdot ; \theta) =&amp;amp; \Big[I-(1-P(\cdot ; \theta)) \beta T(0 ; \theta)-P(\cdot ; \theta) \beta T(1 ; \theta)\Big]^{-1}
\newline * &amp;amp; \left[
\begin{array}{c}
(1-P(\cdot ; \theta))\Big[u(\cdot, 0 ; \theta)+0.5772-\ln (1-P(\cdot ; \theta))\Big] \newline + P(\cdot ; \theta)\Big[u(\cdot, 1 ; \theta) + 0.5772 - \ln (P(\cdot ; \theta))\Big]
\end{array}
\right]
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;Or more compactly $$
V = \left[I - \beta \ \sum_a P_a .* T_a \right]^{-1} \ * \ \left( \sum_a P_a \ .* \ \bigg[ u_a + 0.5772 - \ln(P_d) \bigg] \right)
$$&lt;/p&gt;
&lt;h3 id=&#34;first-equation&#34;&gt;First Equation&lt;/h3&gt;
&lt;p&gt;What is the first equation? $$
V = \left[I - \beta \ \sum_a P_a .* T_a \right]^{-1} \ * \ \left( \sum_a P_a \ .* \ \bigg[ u_a + 0.5772 - \ln(P_a) \bigg] \right)
$$ &lt;strong&gt;Expected static payoff&lt;/strong&gt;:
$\sum_a P_a \ .* \ \bigg[ u_a + 0.5772 + \ln(P_a) \bigg]$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Is the expected static payoff of choice $a$ in each state,
$u_a + 0.5772 + \ln(P_a)$&lt;/li&gt;
&lt;li&gt;… integrated over the choice probabilities, $P_a$&lt;/li&gt;
&lt;li&gt;It’s a $k \times 1$ vector&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Unconditional transition probabilities&lt;/strong&gt;: $\sum_a P_a .* T_a$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Are the transition probabilities conditional on a choice $a$ for
every present and future state, $T_a$&lt;/li&gt;
&lt;li&gt;… integrated over the choice probabilities, $P_a$&lt;/li&gt;
&lt;li&gt;It’s a $k \times k$ matrix&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;recap&#34;&gt;Recap&lt;/h3&gt;
&lt;p&gt;We got our first equation $$
{\color{red}{V}} = \left[I - \beta \ \sum_a {\color{blue}{P_a}} .* T_a \right]^{-1} \ * \ \left( \sum_a {\color{blue}{P_a}} \ .* \ \bigg[ u_a + 0.5772 - \ln({\color{blue}{P_a}}) \bigg] \right)
$$&lt;/p&gt;
&lt;p&gt;I.e. $$
\begin{align}
{\color{red}{V(s ; \theta)}} &amp;amp; = h( {\color{blue}{P(s ; \theta)}} ; \theta) \newline
\end{align}
$$&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;What about the second equation
${\color{blue}{P(\cdot|s ; \theta)}} = g({\color{red}{V(s ; \theta)}}; \theta)$?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;from-v-to-p&#34;&gt;From V to P&lt;/h3&gt;
&lt;p&gt;In general, the expected probability of investment is $$
P(a=1; \theta)= \Pr \left[\begin{array}{c}
u(\cdot, 1 ; \theta)+\epsilon_{1 t}+\beta \mathbb E \Big[V(\cdot ; \theta) \Big| \cdot, a_{t}=1 ; \theta \Big]&amp;gt; \newline \qquad
u(\cdot, 0 ; \theta) + \epsilon_{0 t}+\beta \mathbb E \Big[V(\cdot ; \theta) \Big| \cdot, a_{t}=0 ; \theta \Big]
\end{array}\right]
$$&lt;/p&gt;
&lt;p&gt;With the &lt;strong&gt;logit assumption&lt;/strong&gt;, simplifies to $$
{\color{blue}{P(a=1 ; \theta)}} = \frac{\exp \Big(u(\cdot, 1 ; \theta)+\beta T(1 ; \theta) V(\cdot ; \theta) \Big)}{\sum_{a&#39;} \exp \Big(u(\cdot, a&#39; ; \theta)+\beta T(a&#39; ; \theta) V(\cdot ; \theta) \Big)} = \frac{\exp (u_1 +\beta T_1 {\color{red}{V}} )}{\sum_{a&#39;} \exp (u_{a&#39;} +\beta T_{a&#39;} {\color{red}{V}} )}
$$&lt;/p&gt;
&lt;p&gt;Now we have also the second equation! $$
\begin{align}
{\color{blue}{P(s ; \theta)}} &amp;amp; = g({\color{red}{V(s ; \theta)}}; \theta)
\end{align}
$$&lt;/p&gt;
&lt;h3 id=&#34;hotz--miller---idea-2&#34;&gt;Hotz &amp;amp; Miller - Idea 2&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Idea 2&lt;/strong&gt;: Replace ${\color{blue}{P} (\cdot)}$ on the RHS with a
&lt;em&gt;consistent&lt;/em&gt; estimator ${\color{Turquoise}{\hat P (\cdot)}}$ $$
{\color{cyan}{\bar P(\cdot ; \theta)}} = g(h({\color{Turquoise}{\hat P(\cdot)}} ; \theta); \theta)
$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;${\color{cyan}{\bar P(\cdot ; \theta_0)}}$ will converge to the true
${\color{blue}{P(\cdot ; \theta_0)}}$, because
${\color{Turquoise}{\hat P (\cdot)}}$ is converging to
${\color{blue}{P(\cdot ; \theta_0)}}$ asymptotically.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Note&lt;/strong&gt;: pay attention to $\theta_0$ vs $\theta$ here:
${\color{cyan}{\bar P(\cdot ; \theta)}}$ does &lt;strong&gt;not&lt;/strong&gt; generally
converge to ${\color{blue}{P(\cdot ; \theta)}}$for arbitrary
$\theta$, because ${\color{Turquoise}{\hat P(\cdot)}}$ is
converging to ${\color{blue}{P(\cdot ; \theta_0)}}$ but &lt;strong&gt;not&lt;/strong&gt;
${\color{blue}{P(\cdot ; \theta)}}$ with any $\theta$.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;How to compute ${\color{Turquoise}{\hat P(\cdot)}}$?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;From the data, you observe states and decisions&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;You can compute frequency of decisions given states&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;In Rust: frequency of engine replacement, given a mileage
(discretized)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Assumption&lt;/strong&gt;: you have enough data&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;What if a state is not realised?&lt;/li&gt;
&lt;li&gt;Use frequencies in observed states to extrapolate frequencies in
observed states&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;recap-1&#34;&gt;Recap&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Steps&lt;/strong&gt; so far&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Estimate the &lt;strong&gt;conditional choice probabilities&lt;/strong&gt;
${\color{Turquoise}{\hat P}}$ from the data&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Nonparametrically: frequency of each decision in each state&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Solve for the expected value function with the inverstion step $$
{\color{orange}{\hat V}} = \left[I - \beta \ \sum_a  {\color{Turquoise}{\hat P_a}} .* T_a \right]^{-1} \ * \ \left( \sum_a {\color{Turquoise}{\hat P_a}} \ .* \ \bigg[ u_a + 0.5772 - \ln({\color{Turquoise}{\hat P_a}}) \bigg] \right)
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Compute the &lt;strong&gt;predicted CCP&lt;/strong&gt;, given $V$ $$
{\color{cyan}{\bar P(a=1 ; \theta)}} = \frac{\exp (u_1 +\beta T_1 {\color{orange}{\hat V}} )}{\sum_{a&#39;} \exp (u_{a&#39;} +\beta T_{a&#39;} {\color{orange}{\hat V}} )}
$$&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;blockquote&gt;
&lt;p&gt;What now? Use the estimated CCP to build an objective function.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;objective-function&#34;&gt;Objective Function&lt;/h3&gt;
&lt;p&gt;We have (at least) 2 options&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Hotz and Miller (&lt;a href=&#34;#ref-hotz1993conditional&#34;&gt;1993&lt;/a&gt;) use GMM&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;$$
\mathbb E \Big[a_t - \bar P(s_t, \theta) \ \Big| \ s_t \Big] = 0 \quad \text{ at } \quad \theta = \theta_0
$$&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Aguirregabiria and Mira (&lt;a href=&#34;#ref-aguirregabiria2002swapping&#34;&gt;2002&lt;/a&gt;)
use MLE
&lt;ul&gt;
&lt;li&gt;by putting $\bar P(s_t, \theta)$ in the likelihood function
instead of $P(s_t, \theta)$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;We will follow the second approach&lt;/p&gt;
&lt;h3 id=&#34;pseudo-likelihood&#34;&gt;Pseudo-Likelihood&lt;/h3&gt;
&lt;p&gt;The likelihood function for one bus is $$
\mathcal{L}(\theta) = \prod_{t=1}^{T}\left(\hat{\operatorname{Pr}}\left(a=1 \mid s_{t}; \theta\right) \mathbb{1}\left(a_{t}=1\right)+\left(1-\hat{\operatorname{Pr}}\left(a=0 \mid s_{t}; \theta\right)\right) \mathbb{1}\left(a_{t}=0\right)\right)
$$ where $\hat \Pr\big(a_{t} \big| s_{t} ; \theta \big)$ is a function
of&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;CCPs $\hat P$: estimated from data&lt;/li&gt;
&lt;li&gt;transition matrix $T$: estimated from the data, given $\theta$&lt;/li&gt;
&lt;li&gt;static payoffs $u$: known, given $\theta$&lt;/li&gt;
&lt;li&gt;discount factor $\beta$ : assumed&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;Why &lt;strong&gt;pseudo-likelihood&lt;/strong&gt;? We have inputed something that is not a
primitive but a consistent estitimate of an equilibrium object,
$\hat P$&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;comments&#34;&gt;Comments&lt;/h3&gt;
&lt;p&gt;Now a few comments on Hotz and Miller (&lt;a href=&#34;#ref-hotz1993conditional&#34;&gt;1993&lt;/a&gt;)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Computational bottleneck&lt;/li&gt;
&lt;li&gt;Aguirregabiria and Mira (&lt;a href=&#34;#ref-aguirregabiria2002swapping&#34;&gt;2002&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Importance of the T1EV assumption&lt;/li&gt;
&lt;li&gt;Data requirements&lt;/li&gt;
&lt;li&gt;Unobserved heterogeneity&lt;/li&gt;
&lt;li&gt;Identification&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;computational-bottleneck&#34;&gt;Computational Bottleneck&lt;/h3&gt;
&lt;p&gt;There is still 1 computational &lt;strong&gt;bottleneck&lt;/strong&gt; in HM: the inversion step
$$
V = \left[I - \beta \ \sum_a P_a .* T_a \right]^{-1} \ * \ \left( \sum_a P_a \ .* \ \bigg[ u_a + 0.5772 - \ln(P_a) \bigg] \right)
$$ The $\left[I - \beta \ \sum_a P_a .* T_a \right]$ matrix has
dimension $k \times k$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;With large state space, hard to invert&lt;/li&gt;
&lt;li&gt;Even with modern computational power&lt;/li&gt;
&lt;li&gt;Hotz et al. (&lt;a href=&#34;#ref-hotz1994simulation&#34;&gt;1994&lt;/a&gt;): forward simulation of
the value function
&lt;ul&gt;
&lt;li&gt;You have the policy, the transitions and the utilities&lt;/li&gt;
&lt;li&gt;Just compute discounted flow of payoffs&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Core&lt;/strong&gt; idea behind the estimation of dynamic games&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;aguirregabiria-mira-2002&#34;&gt;Aguirregabiria, Mira (2002)&lt;/h3&gt;
&lt;p&gt;Hotz and Miller (&lt;a href=&#34;#ref-hotz1993conditional&#34;&gt;1993&lt;/a&gt;) inversion gets us a
recursive equation in &lt;strong&gt;probability space&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;instead of the Bellman Equation in the &lt;strong&gt;value space&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$
\bar P(\cdot ; \theta) = g(h(\hat P(\cdot) ; \theta); \theta)
$$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Idea&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Do you gain something by iterating $K$ imes?
&lt;ul&gt;
&lt;li&gt;$K=1$: Hotz and Miller (&lt;a href=&#34;#ref-hotz1993conditional&#34;&gt;1993&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;$K \to \infty$: Rust (&lt;a href=&#34;#ref-rust1987optimal&#34;&gt;1987&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Monte Carlo simulations&lt;/strong&gt;: finite sample properties of K−stage
estimators improve monotonically with K
&lt;ul&gt;
&lt;li&gt;But especially for $K=2$!&lt;/li&gt;
&lt;li&gt;Really &lt;strong&gt;worth iterating once&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;type-1-ev-errors&#34;&gt;Type 1 EV errors&lt;/h3&gt;
&lt;p&gt;Crucial assumption&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Without logit errors, we need to simulate their distribution&lt;/li&gt;
&lt;li&gt;True also for Rust&lt;/li&gt;
&lt;li&gt;But it’s generally accepted
&lt;ul&gt;
&lt;li&gt;doesn’t imply it’s innocuous&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;data-requirements&#34;&gt;Data Requirements&lt;/h3&gt;
&lt;p&gt;For both Hotz et al. (&lt;a href=&#34;#ref-hotz1994simulation&#34;&gt;1994&lt;/a&gt;) and Rust
(&lt;a href=&#34;#ref-rust1987optimal&#34;&gt;1987&lt;/a&gt;), we need to &lt;strong&gt;discretize the state
space&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Can be complicated with continuous variables&lt;/li&gt;
&lt;li&gt;Problem also in Rust&lt;/li&gt;
&lt;li&gt;But particularly problematic in Hotz et al.
(&lt;a href=&#34;#ref-hotz1994simulation&#34;&gt;1994&lt;/a&gt;)
&lt;ul&gt;
&lt;li&gt;Relies crucially on consistency of CCP estimates&lt;/li&gt;
&lt;li&gt;Need sufficient &lt;strong&gt;variation in actions for each state&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;unobserved-heterogeneity&#34;&gt;Unobserved Heterogeneity&lt;/h3&gt;
&lt;p&gt;Hotz et al. (&lt;a href=&#34;#ref-hotz1994simulation&#34;&gt;1994&lt;/a&gt;) cannot handle unobserved
heterogeneity or “unobserved state variables” that are persistent over
time.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Example&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Suppose there are 2 bus types $\tau$: high and low quality&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;We &lt;strong&gt;don’t know the share&lt;/strong&gt; of types in the data&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;With Rust&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Parametrize the effect of the difference in qualities&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;E.g. high quality engines break less often&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Parametrize the proportion of high quality buses&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Solve the value function by type $V(s_t, \tau ; \theta)$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Integrate over types when computing choice probabilities $$
P(a|s) = \int P(a|s,\tau) P(\tau) =  \Pr(a|s, \tau=0) * \Pr(\tau=0) + \Pr(a|s, \tau=1) * \Pr(\tau=1)
$$&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;unobserved-heterogeneity-2&#34;&gt;Unobserved Heterogeneity (2)&lt;/h3&gt;
&lt;p&gt;What is the problem with Hotz et al. (&lt;a href=&#34;#ref-hotz1994simulation&#34;&gt;1994&lt;/a&gt;)?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;The unobserved heterogeneity generates &lt;strong&gt;persistent over choices&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;I don’t replace today because it’s high quality, but I also
probably don’t replace tomorrow either&lt;/li&gt;
&lt;li&gt;Decisions independent across time &lt;strong&gt;only conditional on type&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Likelihood of decisions must be integrated over types $$
\mathcal L (\theta) = \sum_{\tau_a} \prod_{t=1}^{T} \Pr (a_{jt}| s_{jt}, \tau_a) \Pr(\tau_a)
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Hotz &amp;amp; Miller needs &lt;strong&gt;consistent estimates&lt;/strong&gt; of $P(a, s, \tau)$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;But hard to get given $\tau$ is not observed!&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;identification&#34;&gt;Identification&lt;/h3&gt;
&lt;p&gt;Work on identification&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Rust (&lt;a href=&#34;#ref-rust1994structural&#34;&gt;1994&lt;/a&gt;) and Magnac and Thesmar
(&lt;a href=&#34;#ref-magnac2002identifying&#34;&gt;2002&lt;/a&gt;)
&lt;ul&gt;
&lt;li&gt;Rust (&lt;a href=&#34;#ref-rust1987optimal&#34;&gt;1987&lt;/a&gt;) is non-paramentrically
underidentified $\to$ parametric assumptions are essential&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Aguirregabiria and Suzuki
(&lt;a href=&#34;#ref-aguirregabiria2014identification&#34;&gt;2014&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Kalouptsidi, Scott, and Souza-Rodrigues
(&lt;a href=&#34;#ref-kalouptsidi2017non&#34;&gt;2017&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Abbring and Daljord (&lt;a href=&#34;#ref-abbring2020identifying&#34;&gt;2020&lt;/a&gt;)
&lt;ul&gt;
&lt;li&gt;Can identify discount factor with some “instrument” that shifts
future utilities but not current payoff&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Kalouptsidi et al. (&lt;a href=&#34;#ref-kalouptsidi2020partial&#34;&gt;2020&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;appendix&#34;&gt;Appendix&lt;/h2&gt;
&lt;h3 id=&#34;references-references&#34;&gt;References [references]&lt;/h3&gt;
&lt;div id=&#34;refs&#34; class=&#34;references csl-bib-body hanging-indent&#34;
markdown=&#34;1&#34;&gt;
&lt;div id=&#34;ref-abbring2020identifying&#34; class=&#34;csl-entry&#34; markdown=&#34;1&#34;&gt;
&lt;p&gt;Abbring, Jaap H, and Øystein Daljord. 2020. “Identifying the Discount
Factor in Dynamic Discrete Choice Models.” &lt;em&gt;Quantitative Economics&lt;/em&gt; 11
(2): 471–501.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-aguirregabiria2002swapping&#34; class=&#34;csl-entry&#34; markdown=&#34;1&#34;&gt;
&lt;p&gt;Aguirregabiria, Victor, and Pedro Mira. 2002. “Swapping the Nested Fixed
Point Algorithm: A Class of Estimators for Discrete Markov Decision
Models.” &lt;em&gt;Econometrica&lt;/em&gt; 70 (4): 1519–43.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-aguirregabiria2014identification&#34; class=&#34;csl-entry&#34;
markdown=&#34;1&#34;&gt;
&lt;p&gt;Aguirregabiria, Victor, and Junichi Suzuki. 2014. “Identification and
Counterfactuals in Dynamic Models of Market Entry and Exit.”
&lt;em&gt;Quantitative Marketing and Economics&lt;/em&gt; 12 (3): 267–304.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-becker1988theory&#34; class=&#34;csl-entry&#34; markdown=&#34;1&#34;&gt;
&lt;p&gt;Becker, Gary S, and Kevin M Murphy. 1988. “A Theory of Rational
Addiction.” &lt;em&gt;Journal of Political Economy&lt;/em&gt; 96 (4): 675–700.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-berry1992estimation&#34; class=&#34;csl-entry&#34; markdown=&#34;1&#34;&gt;
&lt;p&gt;Berry, Steven T. 1992. “Estimation of a Model of Entry in the Airline
Industry.” &lt;em&gt;Econometrica: Journal of the Econometric Society&lt;/em&gt;, 889–917.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-bresnahan1989empirical&#34; class=&#34;csl-entry&#34; markdown=&#34;1&#34;&gt;
&lt;p&gt;Bresnahan, Timothy F. 1989. “Empirical Studies of Industries with Market
Power.” &lt;em&gt;Handbook of Industrial Organization&lt;/em&gt; 2: 1011–57.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-crawford2005uncertainty&#34; class=&#34;csl-entry&#34; markdown=&#34;1&#34;&gt;
&lt;p&gt;Crawford, Gregory S, and Matthew Shum. 2005. “Uncertainty and Learning
in Pharmaceutical Demand.” &lt;em&gt;Econometrica&lt;/em&gt; 73 (4): 1137–73.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-erdem2003brand&#34; class=&#34;csl-entry&#34; markdown=&#34;1&#34;&gt;
&lt;p&gt;Erdem, Tülin, Susumu Imai, and Michael P Keane. 2003. “Brand and
Quantity Choice Dynamics Under Price Uncertainty.” &lt;em&gt;Quantitative
Marketing and Economics&lt;/em&gt; 1 (1): 5–64.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-erdem1996decision&#34; class=&#34;csl-entry&#34; markdown=&#34;1&#34;&gt;
&lt;p&gt;Erdem, Tülin, and Michael P Keane. 1996. “Decision-Making Under
Uncertainty: Capturing Dynamic Brand Choice Processes in Turbulent
Consumer Goods Markets.” &lt;em&gt;Marketing Science&lt;/em&gt; 15 (1): 1–20.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-golosov2006new&#34; class=&#34;csl-entry&#34; markdown=&#34;1&#34;&gt;
&lt;p&gt;Golosov, Mikhail, Aleh Tsyvinski, Ivan Werning, Peter Diamond, and
Kenneth L Judd. 2006. “New Dynamic Public Finance: A User’s Guide [with
Comments and Discussion].” &lt;em&gt;NBER Macroeconomics Annual&lt;/em&gt; 21: 317–87.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-gowrisankaran2012dynamics&#34; class=&#34;csl-entry&#34; markdown=&#34;1&#34;&gt;
&lt;p&gt;Gowrisankaran, Gautam, and Marc Rysman. 2012. “Dynamics of Consumer
Demand for New Durable Goods.” &lt;em&gt;Journal of Political Economy&lt;/em&gt; 120 (6):
1173–1219.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-handel2013adverse&#34; class=&#34;csl-entry&#34; markdown=&#34;1&#34;&gt;
&lt;p&gt;Handel, Benjamin R. 2013. “Adverse Selection and Inertia in Health
Insurance Markets: When Nudging Hurts.” &lt;em&gt;American Economic Review&lt;/em&gt; 103
(7): 2643–82.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-hendel2006measuring&#34; class=&#34;csl-entry&#34; markdown=&#34;1&#34;&gt;
&lt;p&gt;Hendel, Igal, and Aviv Nevo. 2006. “Measuring the Implications of Sales
and Consumer Inventory Behavior.” &lt;em&gt;Econometrica&lt;/em&gt; 74 (6): 1637–73.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-hotz1993conditional&#34; class=&#34;csl-entry&#34; markdown=&#34;1&#34;&gt;
&lt;p&gt;Hotz, V Joseph, and Robert A Miller. 1993. “Conditional Choice
Probabilities and the Estimation of Dynamic Models.” &lt;em&gt;The Review of
Economic Studies&lt;/em&gt; 60 (3): 497–529.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-hotz1994simulation&#34; class=&#34;csl-entry&#34; markdown=&#34;1&#34;&gt;
&lt;p&gt;Hotz, V Joseph, Robert A Miller, Seth Sanders, and Jeffrey Smith. 1994.
“A Simulation Estimator for Dynamic Models of Discrete Choice.” &lt;em&gt;The
Review of Economic Studies&lt;/em&gt; 61 (2): 265–89.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-igami2020artificial&#34; class=&#34;csl-entry&#34; markdown=&#34;1&#34;&gt;
&lt;p&gt;Igami, Mitsuru. 2020. “Artificial Intelligence as Structural Estimation:
Deep Blue, Bonanza, and AlphaGo.” &lt;em&gt;The Econometrics Journal&lt;/em&gt; 23 (3):
S1–24.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-imai2009bayesian&#34; class=&#34;csl-entry&#34; markdown=&#34;1&#34;&gt;
&lt;p&gt;Imai, Susumu, Neelam Jain, and Andrew Ching. 2009. “Bayesian Estimation
of Dynamic Discrete Choice Models.” &lt;em&gt;Econometrica&lt;/em&gt; 77 (6): 1865–99.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-kalouptsidi2020partial&#34; class=&#34;csl-entry&#34; markdown=&#34;1&#34;&gt;
&lt;p&gt;Kalouptsidi, Myrto, Yuichi Kitamura, Lucas Lima, and Eduardo A
Souza-Rodrigues. 2020. “Partial Identification and Inference for Dynamic
Models and Counterfactuals.” National Bureau of Economic Research.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-kalouptsidi2017non&#34; class=&#34;csl-entry&#34; markdown=&#34;1&#34;&gt;
&lt;p&gt;Kalouptsidi, Myrto, Paul T Scott, and Eduardo Souza-Rodrigues. 2017. “On
the Non-Identification of Counterfactuals in Dynamic Discrete Games.”
&lt;em&gt;International Journal of Industrial Organization&lt;/em&gt; 50: 362–71.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-keane1997career&#34; class=&#34;csl-entry&#34; markdown=&#34;1&#34;&gt;
&lt;p&gt;Keane, Michael P, and Kenneth I Wolpin. 1997. “The Career Decisions of
Young Men.” &lt;em&gt;Journal of Political Economy&lt;/em&gt; 105 (3): 473–522.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-magnac2002identifying&#34; class=&#34;csl-entry&#34; markdown=&#34;1&#34;&gt;
&lt;p&gt;Magnac, Thierry, and David Thesmar. 2002. “Identifying Dynamic Discrete
Decision Processes.” &lt;em&gt;Econometrica&lt;/em&gt; 70 (2): 801–16.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-pakes1986patents&#34; class=&#34;csl-entry&#34; markdown=&#34;1&#34;&gt;
&lt;p&gt;Pakes, Ariel. 1986. “Patents as Options: Some Estimates of the Value of
Holding European Patent Stocks.” &lt;em&gt;Econometrica&lt;/em&gt; 54 (4): 755–84.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-rust1987optimal&#34; class=&#34;csl-entry&#34; markdown=&#34;1&#34;&gt;
&lt;p&gt;Rust, John. 1987. “Optimal Replacement of GMC Bus Engines: An Empirical
Model of Harold Zurcher.” &lt;em&gt;Econometrica: Journal of the Econometric
Society&lt;/em&gt;, 999–1033.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-rust1988maximum&#34; class=&#34;csl-entry&#34; markdown=&#34;1&#34;&gt;
&lt;p&gt;———. 1988. “Maximum Likelihood Estimation of Discrete Control
Processes.” &lt;em&gt;SIAM Journal on Control and Optimization&lt;/em&gt; 26 (5): 1006–24.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-rust1994structural&#34; class=&#34;csl-entry&#34; markdown=&#34;1&#34;&gt;
&lt;p&gt;———. 1994. “Structural Estimation of Markov Decision Processes.”
&lt;em&gt;Handbook of Econometrics&lt;/em&gt; 4: 3081–3143.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-su2012constrained&#34; class=&#34;csl-entry&#34; markdown=&#34;1&#34;&gt;
&lt;p&gt;Su, Che-Lin, and Kenneth L Judd. 2012. “Constrained Optimization
Approaches to Estimation of Structural Models.” &lt;em&gt;Econometrica&lt;/em&gt; 80 (5):
2213–30.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Neural Networks</title>
      <link>https://matteocourthoud.github.io/course/ml-econ/08_neuralnets/</link>
      <pubDate>Mon, 28 Feb 2022 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/course/ml-econ/08_neuralnets/</guid>
      <description>&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Remove warnings
import warnings
warnings.filterwarnings(&#39;ignore&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Import everything
import pandas as pd
import numpy as np
import seaborn as sns
import statsmodels.api as sm

import copy
import torch 
import torch.nn as nn
import torch.utils.data as Data
from torch.autograd import Variable
from sklearn.linear_model import LinearRegression
from torchviz import make_dot
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Import matplotlib for graphs
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import axes3d
from IPython.display import clear_output

# Set global parameters
%matplotlib inline
plt.style.use(&#39;seaborn-white&#39;)
plt.rcParams[&#39;lines.linewidth&#39;] = 3
plt.rcParams[&#39;figure.figsize&#39;] = (10,6)
plt.rcParams[&#39;figure.titlesize&#39;] = 20
plt.rcParams[&#39;axes.titlesize&#39;] = 18
plt.rcParams[&#39;axes.labelsize&#39;] = 14
plt.rcParams[&#39;legend.fontsize&#39;] = 14
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;While &lt;code&gt;sklearn&lt;/code&gt; has a library for neural networks, it is very basic and not the standard in the industry. The most commonly used libraries as of 2020 are &lt;strong&gt;Tensorflow&lt;/strong&gt; and &lt;strong&gt;Pytorch&lt;/strong&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;TensorFlow is developed by Google Brain and actively used at Google both for research and production needs. Its closed-source predecessor is called DistBelief.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;PyTorch is a cousin of lua-based Torch framework which was developed and used at Facebook. However, PyTorch is not a simple set of wrappers to support popular language, it was rewritten and tailored to be fast and feel native.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Here is an article that explains very well the difference between the two libraries: &lt;a href=&#34;https://towardsdatascience.com/pytorch-vs-tensorflow-spotting-the-difference-25c75777377b&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;pytorch-vs-tensorflow&lt;/a&gt;. In short, pytorch is much more intuitive for a python programmer and more user friendly. It also has a superior development and debugging experience. However, if you want more control on the fundamentals, a better community support and you need to train large models, Tensorflow is better.&lt;/p&gt;
&lt;h2 id=&#34;81-introduction&#34;&gt;8.1 Introduction&lt;/h2&gt;
&lt;p&gt;The term neural network has evolved to encompass a large class of models and learning methods. Here I describe the most widely used “vanilla” neural net, sometimes called the single hidden layer back-propagation network, or single layer perceptron.&lt;/p&gt;
&lt;h3 id=&#34;regression&#34;&gt;Regression&lt;/h3&gt;
&lt;p&gt;Imagine a setting with two &lt;strong&gt;inputs&lt;/strong&gt; available (let’s denote these inputs $i_1$ and $i_2$), and no special knowledge about the relationship between these inputs and the &lt;strong&gt;output&lt;/strong&gt; that we want to predict (denoted by $o$) except that this relationship is, a priori, pretty complex and non-linear.&lt;/p&gt;
&lt;p&gt;So we want to learn the function $f$ such that f($i_1$, $i_2$) is a good estimator of $o$. We could then suggest the following first model:&lt;/p&gt;
&lt;p&gt;$$
o = w_{11} i_1 + w_{12} i_2
$$&lt;/p&gt;
&lt;p&gt;where $w_{11}$ and $w_{12}$ are just weights/coefficients (do not take care about the indices for now). Before going any further, we should notice that, here, there is no constant term in the model. However, we could have introduced such term by setting $f(i_1, i_2) = w_{11} i_1 + w_{12} i_2 + c$. The constant is often called &lt;strong&gt;bias&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;We can represent the setting as follows.&lt;/p&gt;
&lt;img src=&#34;../figures/nn1.jpeg&#34; alt=&#34;Drawing&#34; style=&#34;width: 600px;&#34;/&gt;
&lt;p&gt;In this case, the model is easy to understand and to fit but has a big drawback : there is no non-linearity! This obviously do not respect our non-linear assumption.&lt;/p&gt;
&lt;h3 id=&#34;activation-functions&#34;&gt;Activation Functions&lt;/h3&gt;
&lt;p&gt;In order to introduce a non-linearity, let us make a little modification in the previous model and suggest the following one.&lt;/p&gt;
&lt;p&gt;$$
o = a ( w_{11} i_1 + w_{12} i_2)
$$&lt;/p&gt;
&lt;p&gt;where $a$ is a function called &lt;strong&gt;activation function&lt;/strong&gt; which is non-linear.&lt;/p&gt;
&lt;img src=&#34;../figures/nn2.jpeg&#34; alt=&#34;Drawing&#34; style=&#34;width: 600px;&#34;/&gt;
&lt;p&gt;One activation function that is well known in economics (and other disciplines) is the &lt;em&gt;sigmoid&lt;/em&gt; function or logit function&lt;/p&gt;
&lt;p&gt;$$
a (w_{11} i_1 + w_{12} i_2) = \frac{1}{1 + e^{w_{11} i_1 + w_{12} i_2}}
$$&lt;/p&gt;
&lt;h3 id=&#34;layers&#34;&gt;Layers&lt;/h3&gt;
&lt;p&gt;However, even if better than multilinear model, this model is still too simple and can’t handle the assumed underlying complexity of the relationship between inputs and output. We can make a step further and enrich the model the following way.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;First we could consider that the quantity $a ( w_{11} i_1 + w_{12} i_2)$ is no longer the final output but instead a new intermediate feature of our function, called $l_1$, which stands for &lt;strong&gt;layer&lt;/strong&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;$$
l_1 = a ( w_{11} i_1 + w_{12} i_2)
$$&lt;/p&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;Second we could consider that we build several (3 in our example) such features in the same way, but possibly with different weights and different activation functions&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;$$
l_1 = a ( w_{11} i_1 + w_{12} i_2) \
l_2 = a ( w_{21} i_1 + w_{22} i_2) \
l_3 = a ( w_{31} i_1 + w_{32} i_2)
$$&lt;/p&gt;
&lt;p&gt;where the $a$’s are just activation functions and the $w$’s are weights.&lt;/p&gt;
&lt;ol start=&#34;3&#34;&gt;
&lt;li&gt;Finally, we can consider that our final output is build based on these intermediate features with the same “template”&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;$$
a_2 ( v_1 l_1 + v_2 l_2 + v_3 * l_3 )
$$&lt;/p&gt;
&lt;p&gt;If we aggregate all the pieces, we then get our &lt;strong&gt;prediction&lt;/strong&gt; $p$&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
p = f_{3}\left(i_{1}, i_{2}\right) &amp;amp;=a_{2}\left(v_{1} l_{1}+v_{2} l_{2}+v_{3} l_{3}\right) \
&amp;amp;=a_{2}\left(v_{1} \times a_{11}\left(w_{11} i_{1}+w_{12} i_{2}\right)+v_{2} \times a_{12}\left(w_{21} i_{1}+w_{22} i_{2}\right)+v_{3} \times a_{13}\left(w_{31} i_{1}+w_{32} i_{2}\right)\right)
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;where we should mainly keep in mind that $a$’s are non-linear activation functions and $w$’s and $v$’s are weights.&lt;/p&gt;
&lt;p&gt;Graphically:&lt;/p&gt;
&lt;img src=&#34;../figures/nn3.jpeg&#34; alt=&#34;Drawing&#34; style=&#34;width: 900px;&#34;/&gt;
&lt;p&gt;This last model is a basic feedforward neural network with:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;2 entries ($i_1$ and $i_2$)&lt;/li&gt;
&lt;li&gt;1 hidden layer with 3 hidden neurones (whose outputs are $l_1$, $l_2$ and $l_3$)&lt;/li&gt;
&lt;li&gt;1 final output ($p$)&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;pytorch&#34;&gt;Pytorch&lt;/h2&gt;
&lt;h3 id=&#34;tensors&#34;&gt;Tensors&lt;/h3&gt;
&lt;p&gt;We can express the data as a &lt;code&gt;numpy&lt;/code&gt; array.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;x_np = np.arange(6).reshape((3, 2))
x_np
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;array([[0, 1],
       [2, 3],
       [4, 5]])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Or equivalently as a &lt;code&gt;pytorch&lt;/code&gt; tensor.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;x_tensor = torch.from_numpy(x_np)
x_tensor
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;tensor([[0, 1],
        [2, 3],
        [4, 5]])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can also translate tensors back to arrays.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;tensor2array = x_tensor.numpy()
tensor2array
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;array([[0, 1],
       [2, 3],
       [4, 5]])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can make operations over this data. For example we can take the mean&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;try:
    torch.mean(x_tensor)
except Exception as e:
    print(e)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;mean(): input dtype should be either floating point or complex dtypes. Got Long instead.
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We first have to convert the data in float&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;x_tensor = torch.FloatTensor(x_np)
x_tensor
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;tensor([[0., 1.],
        [2., 3.],
        [4., 5.]])
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;print(np.mean(x_np), &#39;\n\n&#39;, torch.mean(x_tensor))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;2.5 

 tensor(2.5000)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can also apply compontent-wise functions&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;print(np.sin(x_np), &#39;\n\n&#39;, torch.sin(x_tensor))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[[ 0.          0.84147098]
 [ 0.90929743  0.14112001]
 [-0.7568025  -0.95892427]] 

 tensor([[ 0.0000,  0.8415],
        [ 0.9093,  0.1411],
        [-0.7568, -0.9589]])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can multiply tensors as we multiply matrices&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;print(np.matmul(x_np.T, x_np), &#39;\n\n&#39;, torch.mm(x_tensor.T, x_tensor))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[[20 26]
 [26 35]] 

 tensor([[20., 26.],
        [26., 35.]])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;But the element-wise multiplication does not work&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;try:
    x_tensor.dot(x_tensor)
except Exception as e:
    print(e)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;1D tensors expected, but got 2D and 2D tensors
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;variables&#34;&gt;Variables&lt;/h3&gt;
&lt;p&gt;Variable in torch is to build a computational graph, but this graph is dynamic compared with a static graph in Tensorflow or Theano. So torch does not have placeholder, torch can just pass variable to the computational graph.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# build a variable, usually for compute gradients
x_variable = Variable(x_tensor, requires_grad=True)   

x_variable
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;tensor([[0., 1.],
        [2., 3.],
        [4., 5.]], requires_grad=True)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Until now the tensor and variable seem the same. However, the variable is a part of the graph, it&amp;rsquo;s a part of the auto-gradient.&lt;/p&gt;
&lt;p&gt;Suppose we are interested in:&lt;/p&gt;
&lt;p&gt;$$
y = \text{mean} (x_1^2) = \frac{1}{6} x^2
$$&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;y = torch.mean(x_variable*x_variable)
print(y)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;tensor(9.1667, grad_fn=&amp;lt;MeanBackward0&amp;gt;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can compute the gradient by backpropagation&lt;/p&gt;
&lt;p&gt;$$
\nabla y(x) = \frac{2}{3} x
$$&lt;/p&gt;
&lt;p&gt;i.e. if we call the &lt;code&gt;backward&lt;/code&gt; method on our outcome &lt;code&gt;y&lt;/code&gt;, we see that the gradient of our variable &lt;code&gt;x&lt;/code&gt; gets updated.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;print(x_variable.grad)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;None
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;y.backward()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;print(x_variable.grad)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;tensor([[0.0000, 0.3333],
        [0.6667, 1.0000],
        [1.3333, 1.6667]])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;However, its value has not changed.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;print(x_variable)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;tensor([[0., 1.],
        [2., 3.],
        [4., 5.]], requires_grad=True)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can also access the &lt;code&gt;tensor&lt;/code&gt; part of the variable alone by calling the &lt;code&gt;data&lt;/code&gt; method.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;print(x_variable.data)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;tensor([[0., 1.],
        [2., 3.],
        [4., 5.]])
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;activation-function&#34;&gt;Activation Function&lt;/h3&gt;
&lt;p&gt;The main advantage of neural networks is that they introduce non-linearities among the layers. The standard non-linear function&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ReLu&lt;/li&gt;
&lt;li&gt;Sigmoid&lt;/li&gt;
&lt;li&gt;TanH&lt;/li&gt;
&lt;li&gt;Softmax&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# X grid
x_grid = torch.linspace(-5, 5, 200)  # x data (tensor), shape=(100, 1)
x_grid = Variable(x_grid)
x_grid_np = x_grid.data.numpy()   # numpy array for plotting

# Activation functions
y_relu = torch.relu(x_grid).data.numpy()
y_sigmoid = torch.sigmoid(x_grid).data.numpy()
y_tanh = torch.tanh(x_grid).data.numpy()
y_softmax = torch.softmax(x_grid, dim=0).data.numpy() 
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# New figure 1
def make_new_figure_1():

    # Init figure
    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2,2, figsize=(8,6))
    fig.suptitle(&#39;Activation Functions&#39;)

    # Relu
    ax1.plot(x_grid_np, y_relu, c=&#39;red&#39;, label=&#39;relu&#39;)
    ax1.set_ylim((-1, 6)); ax1.legend()

    # Sigmoid
    ax2.plot(x_grid_np, y_sigmoid, c=&#39;red&#39;, label=&#39;sigmoid&#39;)
    ax2.set_ylim((-0.2, 1.2)); ax2.legend()

    # Tanh
    ax3.plot(x_grid_np, y_tanh, c=&#39;red&#39;, label=&#39;tanh&#39;)
    ax3.set_ylim((-1.2, 1.2)); ax3.legend()

    # Softmax
    ax4.plot(x_grid_np, y_softmax, c=&#39;red&#39;, label=&#39;softmax&#39;)
    ax4.set_ylim((-0.01, 0.06)); ax4.legend();
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s compare the different activation functions.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;make_new_figure_1()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/08_neuralnets_56_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;ReLu is very popular since it&amp;rsquo;s non-linear.&lt;/p&gt;
&lt;h2 id=&#34;83-optimization-and-gradient-descent&#34;&gt;8.3 Optimization and Gradient Descent&lt;/h2&gt;
&lt;h3 id=&#34;gradient-descent&#34;&gt;Gradient Descent&lt;/h3&gt;
&lt;p&gt;Gradient descent works as follows:&lt;/p&gt;
&lt;ol start=&#34;0&#34;&gt;
&lt;li&gt;Initialize the parameters&lt;/li&gt;
&lt;li&gt;Compute the Loss&lt;/li&gt;
&lt;li&gt;Compute the Gradients&lt;/li&gt;
&lt;li&gt;Update the Parameters&lt;/li&gt;
&lt;li&gt;Repeat (1)-(3) until convergence&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;gradient-descent-in-linear-regression&#34;&gt;Gradient Descent in Linear Regression&lt;/h3&gt;
&lt;p&gt;In order to understand how are NN optimized, we start with a linear regression example. Remember that linear regression can be interpreted as the simplest possible NN.&lt;/p&gt;
&lt;p&gt;We generate the following data:&lt;/p&gt;
&lt;p&gt;$$
y = 1 + 2 x - 3 x^2 + \varepsilon
$$&lt;/p&gt;
&lt;p&gt;with $x \sim N(0,1)$ and $\varepsilon \sim N(0,0.1)$&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Data Generation
np.random.seed(42)
N = 100

x = np.sort(np.random.rand(N, 1), axis=0)
e = .1*np.random.randn(N, 1)
y_true = 1 + 2*x - 3*x**2
y = y_true + e
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s plot the data.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# New figure 2
def make_new_figure_2():
    
    # Init
    fig, ax = plt.subplots(figsize=(8,6))
    fig.suptitle(&#39;Activation Functions&#39;)

    # Scatter
    ax.scatter(x,y); 
    ax.plot(x,y_true,color=&#39;orange&#39;); 
    ax.set_xlabel(&#39;X&#39;); ax.set_ylabel(&#39;Y&#39;);
    ax.legend([&#39;y true&#39;,&#39;y&#39;]);
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;make_new_figure_2()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/08_neuralnets_66_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Suppose we try to fit the data with a linear model&lt;/p&gt;
&lt;p&gt;$$
y = a + b x
$$&lt;/p&gt;
&lt;p&gt;We proceed iteratively by gradient descent. Our objective function is the Mean Squared Error.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Algorithm&lt;/strong&gt;&lt;/p&gt;
&lt;ol start=&#34;0&#34;&gt;
&lt;li&gt;
&lt;p&gt;Take an initial guess of the parameters
$$
a = a_0 \
b = b_0
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Compute the Mean Squared Error
$$
\begin{array}
\text{MSE} &amp;amp;= \frac{1}{N} \sum_{i=1}^{N}\left(y_{i}-\hat{y}&lt;em&gt;{i}\right)^{2} \
&amp;amp;= \frac{1}{N} \sum&lt;/em&gt;{i=1}^{N}\left(y_{i}-a-b x_{i}\right)^{2}
\end{array}
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Compute its derivative
$$
\begin{array}{l}
\frac{\partial M S E}{\partial a}=\frac{\partial M S E}{\partial \hat{y}&lt;em&gt;{i}} \cdot \frac{\partial \hat{y}&lt;/em&gt;{i}}{\partial a}=\frac{1}{N} \sum_{i=1}^{N} 2\left(y_{i}-a-b x_{i}\right) \cdot(-1)=-2 \frac{1}{N} \sum_{i=1}^{N}\left(y_{i}-\hat{y}&lt;em&gt;{i}\right) \
\frac{\partial M S E}{\partial b}=\frac{\partial M S E}{\partial \hat{y}&lt;/em&gt;{i}} \cdot \frac{\partial \hat{y}&lt;em&gt;{i}}{\partial b}=\frac{1}{N} \sum&lt;/em&gt;{i=1}^{N} 2\left(y_{i}-a-b x_{i}\right) \cdot\left(-x_{i}\right)=-2 \frac{1}{N} \sum_{i=1}^{N} x_{i}\left(y_{i}-\hat{y}_{i}\right)
\end{array}
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Update the parameters
$$
\begin{array}{l}
a=a-\eta \frac{\partial M S E}{\partial a} \
b=b-\eta \frac{\partial M S E}{\partial b}
\end{array}
$$&lt;/p&gt;
&lt;p&gt;Where $\eta$ is the &lt;strong&gt;learning rate&lt;/strong&gt;. A lower learning rate makes learning more stable but slower.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Repeat (1)-(3) $T$ times, where the number of total iterations $T$ is called &lt;strong&gt;epochs&lt;/strong&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;We start by taking a random guess of $\alpha$ and $\beta$.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Initializes parameters &amp;quot;a&amp;quot; and &amp;quot;b&amp;quot; randomly
np.random.seed(42)
a = np.random.randn(1)
b = np.random.randn(1)

print(a, b)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[0.49671415] [-0.1382643]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Plot gradient 
def gradient_plot(x, y, y_hat, y_true, EPOCHS, losses):
    clear_output(wait=True)
    fig, (ax1, ax2) = plt.subplots(1,2, figsize=(12,5))
    
    # First figure
    ax1.clear()
    ax1.scatter(x, y)
    ax1.plot(x, y_true, &#39;orange&#39;)
    ax1.plot(x, y_hat, &#39;r-&#39;)
    ax1.set_title(&#39;Data and Fit&#39;)
    ax1.legend([&#39;True&#39;, &#39;Predicted&#39;])
    
    # Second figure
    ax2.clear()
    ax2.plot(range(len(losses)), losses, color=&#39;g&#39;)
    ax2.set_xlim(0,EPOCHS); ax2.set_ylim(0,1.1*np.max(losses))
    ax2.set_title(&#39;True MSE = %.4f&#39; % losses[-1])
    
    # Plot
    plt.show();
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We set the learning rate $\eta = 0.1$ and the number of epochs $T=200$&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# parameters
LR = 0.1        # learning rate
EPOCHS = 200    # number of epochs
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can now plot the training and the result.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# New figure 3
def make_new_figure_3(a, b):
    
    # Init
    losses = []

    # train
    for t in range(EPOCHS):

        # compute loss
        y_hat = a + b * x
        error = (y - y_hat)
        loss = (error**2).mean()

        # compute gradient
        a_grad = -2 * error.mean()
        b_grad = -2 * (x * error).mean()

        # update parameters
        a -= LR * a_grad
        b -= LR * b_grad

        # plot
        losses += [loss]
        if (t+1) % (EPOCHS/25) == 0: # print 25 times
            gradient_plot(x, y, y_hat, y_true, EPOCHS, losses)

    print(a, b)
    return a, b
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;a_fit, b_fit = make_new_figure_3(a, b)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/08_neuralnets_76_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[1.40589939] [-0.83739496]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Sanity Check: do we get the same results as our gradient descent?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# OLS estimates
ols = LinearRegression()
ols.fit(x, y)
print(ols.intercept_, ols.coef_[0])
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1.4345303] [-0.89397853]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Close enough!&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s plot both lines in the graph.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# New figure 4
def make_new_figure_4():
    
    # Init
    fig, ax = plt.subplots(figsize=(8,6))

    # Scatter
    ax.plot(x,y_true,color=&#39;orange&#39;); 
    ax.plot(x,a_fit + b_fit*x,color=&#39;red&#39;); 
    ax.plot(x,ols.predict(x),color=&#39;green&#39;); 
    ax.legend([&#39;y true&#39;,&#39;y gd&#39;, &#39;y ols&#39;])
    ax.scatter(x,y); 
    ax.set_xlabel(&#39;X&#39;); ax.set_ylabel(&#39;Y&#39;); ax.set_title(&amp;quot;Data&amp;quot;);
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;make_new_figure_4()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/08_neuralnets_82_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Now we are going to do exactly the same but with &lt;code&gt;pytorch&lt;/code&gt;.&lt;/p&gt;
&lt;h3 id=&#34;autograd&#34;&gt;Autograd&lt;/h3&gt;
&lt;p&gt;Autograd is PyTorch’s automatic differentiation package. Thanks to it, we don’t need to worry about partial derivatives, chain rule or anything like it.&lt;/p&gt;
&lt;p&gt;So, how do we tell PyTorch to do its thing and compute all gradients? That’s what &lt;code&gt;backward()&lt;/code&gt; is good for.
§
Do you remember the starting point for computing the gradients? It was the loss, as we computed its partial derivatives w.r.t. our parameters. Hence, we need to invoke the &lt;code&gt;backward()&lt;/code&gt; method from the corresponding Python variable, like, &lt;code&gt;loss.backward()&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;What about the actual values of the gradients? We can inspect them by looking at the grad attribute of a tensor.&lt;/p&gt;
&lt;p&gt;If you check the method’s documentation, it clearly states that gradients are accumulated. So, every time we use the
gradients to update the parameters, we need to zero the gradients afterwards. And that’s what zero_() is good for.&lt;/p&gt;
&lt;p&gt;What does the underscore (_) at the end of the method name mean? Do you remember? If not, scroll back to the previous section and find out.&lt;/p&gt;
&lt;p&gt;So, let’s ditch the manual computation of gradients and use both backward() and zero_() methods instead.&lt;/p&gt;
&lt;p&gt;First, we convert our variables to tensors.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Convert data to tensors
x_tensor = torch.from_numpy(x).float().to(&#39;cpu&#39;)
y_tensor = torch.from_numpy(y).float().to(&#39;cpu&#39;)
print(type(x), type(x_tensor))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;class &#39;numpy.ndarray&#39;&amp;gt; &amp;lt;class &#39;torch.Tensor&#39;&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We take the initial parameters guess&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# initial parameter guess
torch.manual_seed(42)
a = torch.randn(1, requires_grad=True, dtype=torch.float, device=&#39;cpu&#39;)
b = torch.randn(1, requires_grad=True, dtype=torch.float, device=&#39;cpu&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we are ready to fit the model.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# New figure 5
def make_new_figure_5(a, b):
    
    # Init
    losses = []

    # parameters
    LR = 0.1
    EPOCHS = 200

    # train
    for t in range(EPOCHS):

        # compute loss
        y_hat = a + b * x_tensor
        error = y_tensor - y_hat
        loss = (error ** 2).mean()

        # compute gradient
        loss.backward()

        # update parameters
        with torch.no_grad():
            a -= LR * a.grad
            b -= LR * b.grad

        # clear gradients
        a.grad.zero_()
        b.grad.zero_()

        # Plot
        losses += [((y_true - y_hat.detach().numpy())**2).mean()]
        if (t+1) % (EPOCHS/25) == 0: # print 25 times
            gradient_plot(x, y, y_hat.data.numpy(), y_true, EPOCHS, losses)

    print(a, b)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;make_new_figure_5(a, b)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/08_neuralnets_93_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;tensor([1.3978], requires_grad=True) tensor([-0.8214], requires_grad=True)
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;optimizer&#34;&gt;Optimizer&lt;/h3&gt;
&lt;p&gt;So far, we’ve been manually updating the parameters using the computed gradients. That’s probably fine for two parameters… but what if we had a whole lot of them?! We use one of PyTorch’s optimizers.&lt;/p&gt;
&lt;p&gt;An optimizer takes the following arguments:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the parameters we want to update&lt;/li&gt;
&lt;li&gt;he learning rate we want to use&lt;/li&gt;
&lt;li&gt;(possibly many other hyper-parameters)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Moreover, we can now call the function &lt;code&gt;zero_grad()&lt;/code&gt; to automatically update the parameters. In particular, we will need to perform the following steps at each iteration:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Clear the parameters: &lt;code&gt;optimizer.zero_grad()&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Compute the gradient: &lt;code&gt;loss.backward()&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Update the parameters: &lt;code&gt;optimizer.step()&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In the code below, we create a Stochastic Gradient Descent (&lt;code&gt;SGD&lt;/code&gt;) optimizer to update our parameters $a$ and $b$.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Init parameters
torch.manual_seed(42)
a = torch.randn(1, requires_grad=True, dtype=torch.float, device=&#39;cpu&#39;)
b = torch.randn(1, requires_grad=True, dtype=torch.float, device=&#39;cpu&#39;)

# Defines a SGD optimizer to update the parameters
optimizer = torch.optim.SGD([a, b], lr=LR)
print(optimizer)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;SGD (
Parameter Group 0
    dampening: 0
    lr: 0.1
    momentum: 0
    nesterov: False
    weight_decay: 0
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can also define a default loss function so that we don&amp;rsquo;t have to compute it by hand. We are going to use the &lt;code&gt;MSE&lt;/code&gt; loss function.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Define a loss function
loss_func = torch.nn.MSELoss()
print(loss_func)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;MSELoss()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s plot the estimator and the MSE.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# New figure 6
def make_new_figure_6(a, b):
    
    # parameters
    EPOCHS = 200

    # init 
    losses = []

    # train
    for t in range(EPOCHS):

        # compute loss
        y_hat = a + b * x_tensor
        error = y_tensor - y_hat
        loss = (error ** 2).mean()  

        # update parameters
        optimizer.zero_grad()   # clear gradients for next train
        loss.backward()         # backpropagation, compute gradients
        optimizer.step()        # apply gradients, update parameters

        # Plot
        losses += [((y_true - y_hat.detach().numpy())**2).mean()]
        if (t+1) % (EPOCHS/25) == 0:
            gradient_plot(x, y, y_hat.data.numpy(), y_true, EPOCHS, losses)

    print(a, b)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;make_new_figure_6(a, b)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/08_neuralnets_103_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;tensor([1.3978], requires_grad=True) tensor([-0.8214], requires_grad=True)
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;building-a-nn&#34;&gt;Building a NN&lt;/h3&gt;
&lt;p&gt;In our model, we manually created two parameters to perform a linear regression. Let’s use PyTorch’s &lt;code&gt;Sequential&lt;/code&gt; module to create our neural network.&lt;/p&gt;
&lt;p&gt;We first want to build the linear regression framework&lt;/p&gt;
&lt;p&gt;$$
y = a + b x
$$&lt;/p&gt;
&lt;p&gt;Which essentially is a network with&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;1 input&lt;/li&gt;
&lt;li&gt;no hidden layer&lt;/li&gt;
&lt;li&gt;no activation function&lt;/li&gt;
&lt;li&gt;1 output&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Let&amp;rsquo;s build the simplest possible neural network with &lt;code&gt;PyTorch&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Simplest possible neural network
linear_net = torch.nn.Sequential(
    torch.nn.Linear(1, 1)
)

print(linear_net)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Sequential(
  (0): Linear(in_features=1, out_features=1, bias=True)
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, if we call the &lt;code&gt;parameters()&lt;/code&gt; method of this model, PyTorch will figure the parameters of its attributes in a recursive way.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;[*linear_net.parameters()]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[Parameter containing:
 tensor([[-0.2191]], requires_grad=True),
 Parameter containing:
 tensor([0.2018], requires_grad=True)]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can now define the definitive training function.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def train_NN(x, y, y_true, net, optimizer, loss_func, EPOCHS):
    
    # transform variables
    x_tensor = torch.from_numpy(x).float().to(&#39;cpu&#39;)
    y_tensor = torch.from_numpy(y).float().to(&#39;cpu&#39;)

    # init 
    losses = []
    
    # train
    for t in range(EPOCHS):        

        # compute loss
        y_hat = net(x_tensor)     
        loss = loss_func(y_hat, y_tensor)    
        
        # update parameters
        optimizer.zero_grad()   # clear gradients for next train
        loss.backward()         # backpropagation, compute gradients
        optimizer.step()        # apply gradients, update parameters

        # plot
        losses += [((y_true - y_hat.detach().numpy())**2).mean()]
        if (t+1) % (EPOCHS/25) == 0: # print 25 times
            gradient_plot(x, y, y_hat.data.numpy(), y_true, EPOCHS, losses)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we are ready to train our neural network.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;optimizer = torch.optim.SGD(linear_net.parameters(), lr=LR)

# train
train_NN(x, y, y_true, linear_net, optimizer, loss_func, EPOCHS)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/08_neuralnets_113_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;We now define a more complicated NN. In particular we, build a neural network with&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;1 input&lt;/li&gt;
&lt;li&gt;1 hidden layer with 10 neurons and Relu activation function&lt;/li&gt;
&lt;li&gt;1 output layer&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Relu Net
relu_net = torch.nn.Sequential(
    torch.nn.Linear(1, 10),
    torch.nn.ReLU(),
    torch.nn.Linear(10, 1)
)

print(relu_net)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Sequential(
  (0): Linear(in_features=1, out_features=10, bias=True)
  (1): ReLU()
  (2): Linear(in_features=10, out_features=1, bias=True)
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This network has much more parameters.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;[*relu_net.parameters()]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[Parameter containing:
 tensor([[-0.4869],
         [ 0.5873],
         [ 0.8815],
         [-0.7336],
         [ 0.8692],
         [ 0.1872],
         [ 0.7388],
         [ 0.1354],
         [ 0.4822],
         [-0.1412]], requires_grad=True),
 Parameter containing:
 tensor([ 0.7709,  0.1478, -0.4668,  0.2549, -0.4607, -0.1173, -0.4062,  0.6634,
         -0.7894, -0.4610], requires_grad=True),
 Parameter containing:
 tensor([[-0.0893, -0.1901,  0.0298, -0.3123,  0.2856, -0.2686,  0.2441,  0.0526,
          -0.1027,  0.1954]], requires_grad=True),
 Parameter containing:
 tensor([0.0493], requires_grad=True)]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We are again using Stochastic Gradient Descent (&lt;code&gt;SGD&lt;/code&gt;) as optimization algorithm and Mean Squared Error (&lt;code&gt;MSELoss&lt;/code&gt;) as objective function.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# parameters
LR = 0.1
EPOCHS = 1000

# optimizer and loss function
optimizer = torch.optim.SGD(relu_net.parameters(), lr=LR)
loss_func = torch.nn.MSELoss()

# Train
train_NN(x, y, y_true, relu_net, optimizer, loss_func, EPOCHS)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/08_neuralnets_119_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;It seems that we can use fewer nodes to get the same result.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s make a smallet network.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Relu Net
relu_net2 = torch.nn.Sequential(
    torch.nn.Linear(1, 4),
    torch.nn.ReLU(),
    torch.nn.Linear(4, 1)
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And train it.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# parameters
LR = 0.1
EPOCHS = 1000

# optimizer and loss function
optimizer = torch.optim.SGD(relu_net2.parameters(), lr=LR)
loss_func = torch.nn.MSELoss()

# Train
train_NN(x, y, y_true, relu_net2, optimizer, loss_func, EPOCHS)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/08_neuralnets_124_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;We can try different activation functions.&lt;/p&gt;
&lt;p&gt;For example the tangent.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# TanH Net
tanh_net = torch.nn.Sequential(
    torch.nn.Linear(1, 10),
    torch.nn.Tanh(),
    torch.nn.Linear(10, 1)
)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# parameters
LR = 0.2
EPOCHS = 1000

# optimizer and loss function
optimizer = torch.optim.SGD(tanh_net.parameters(), lr=LR)
loss_func = torch.nn.MSELoss()

# train
train_NN(x, y, y_true, tanh_net, optimizer, loss_func, EPOCHS)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/08_neuralnets_127_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;loss-functions&#34;&gt;Loss functions&lt;/h3&gt;
&lt;p&gt;So far we have used the Stochastic  as loss function.&lt;/p&gt;
&lt;p&gt;Notice that &lt;code&gt;nn.MSELoss&lt;/code&gt; actually creates a loss function for us — it is NOT the loss function itself. Moreover, you can specify a reduction method to be applied, that is, how do you want to aggregate the results for individual points — you can average them (reduction=&lt;code&gt;mean&lt;/code&gt;) or simply sum them up (reduction=&lt;code&gt;sum&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;We are now going to use different ones.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# parameters
LR = 0.1
EPOCHS = 25

# nets
n = torch.nn.Sequential(torch.nn.Linear(1, 10), torch.nn.ReLU(), torch.nn.Linear(10, 1))
nets = [n,n,n,n]

# optimizers
optimizers = [torch.optim.SGD(n.parameters(), lr=LR) for n in nets]

# different loss functions
loss_MSE        = torch.nn.MSELoss()
loss_L1         = torch.nn.L1Loss()
loss_NLL        = torch.nn.NLLLoss()
loss_KLD        = torch.nn.KLDivLoss()
loss_funcs = [loss_MSE, loss_L1, loss_NLL, loss_KLD]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is the description of the loss functions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;MSELoss&lt;/code&gt;: Creates a criterion that measures the mean squared error (squared L2 norm) between each element in the input $x$ and target $y$.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;L1Loss&lt;/code&gt;: Creates a criterion that measures the mean absolute error (MAE) between each element in the input $x$ and target $y$.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;NLLLoss&lt;/code&gt;: The negative log likelihood loss.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;KLDivLoss&lt;/code&gt;: The Kullback-Leibler divergence loss measure&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Train multiple nets
def train_nets(x, y, y_true, nets, optimizers, loss_funcs, labels, EPOCHS):

    # Put dateset into torch dataset
    x_tensor = torch.from_numpy(x).float().to(&#39;cpu&#39;)
    y_tensor = torch.from_numpy(y).float().to(&#39;cpu&#39;)
    torch_dataset = Data.TensorDataset(x_tensor, y_tensor)
    
    # Init
    losses = np.zeros((0,4))
    
    # Train
    for epoch in range(EPOCHS): # for each epoch
        losses = np.vstack((losses, np.zeros((1,4))))
        for k, net, opt, lf in zip(range(4), nets, optimizers, loss_funcs):
            y_hat = net(x_tensor)              # get output for every net
            loss = loss_func(y_hat, y_tensor)  # compute loss for every net
            opt.zero_grad()                    # clear gradients for next train
            loss.backward()                    # backpropagation, compute gradients
            opt.step()                         # apply gradients
            losses[-1,k] = ((y_true - y_hat.detach().numpy())**2).mean()
        plot_losses(losses, labels, EPOCHS)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Plot losses 
def plot_losses(losses, labels, EPOCHS):
    clear_output(wait=True)
    fig, ax = plt.subplots(1,1, figsize=(10,6))
    
    # Plot
    ax.clear()
    ax.plot(range(len(losses)), losses)
    ax.set_xlim(0,EPOCHS-1); ax.set_ylim(0,1.1*np.max(losses))
    ax.set_title(&#39;Compare Losses&#39;); ax.set_ylabel(&#39;True MSE&#39;)
    legend_txt = [&#39;%s=%.4f&#39; % (label, loss) for label,loss in zip(labels, losses[-1,:])]
    ax.legend(legend_txt)
    
    # Shot
    plt.show();
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s compare them.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Train
labels = [&#39;MSE&#39;, &#39;L1&#39;, &#39;LogL&#39;, &#39;KLdiv&#39;]
train_nets(x, y, y_true, nets, optimizers, loss_funcs, labels, EPOCHS)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/08_neuralnets_135_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;In this very simple case, all loss functions are very similar.&lt;/p&gt;
&lt;h3 id=&#34;optimizers&#34;&gt;Optimizers&lt;/h3&gt;
&lt;p&gt;So far we have used the Stochastic Gradient Descent to fit the neural network. We are now going to use different ones.&lt;/p&gt;
&lt;p&gt;This is the &lt;a href=&#34;https://pytorch.org/docs/stable/optim.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;description of the optimizers&lt;/a&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;SGD&lt;/code&gt;: Implements stochastic gradient descent (optionally with momentum).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;Momentum&lt;/code&gt;: Nesterov momentum is based on the formula from &lt;a href=&#34;http://www.cs.toronto.edu/~hinton/absps/momentum.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;On the importance of initialization and momentum in deep learning&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;RMSprop&lt;/code&gt;: Proposed by G. Hinton in his &lt;a href=&#34;https://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;course&lt;/a&gt;. The centered version first appears in &lt;a href=&#34;https://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Generating Sequences With Recurrent Neural Networks&lt;/a&gt;. The implementation here takes the square root of the gradient average before adding epsilon (note that TensorFlow interchanges these two operations). The effective learning rate is thus $\frac{\alpha}{\sqrt{v} + \epsilon}$ where $\alpha$ is the scheduled learning rate and $v$ is the weighted moving average of the squared gradient.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;Adam&lt;/code&gt;: Proposed in &lt;a href=&#34;https://arxiv.org/abs/1412.6980&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Adam: A Method for Stochastic Optimization&lt;/a&gt;. The implementation of the L2 penalty follows changes proposed in &lt;a href=&#34;https://arxiv.org/abs/1711.05101&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Decoupled Weight Decay Regularization&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# parameters
LR = 0.1
EPOCHS = 25

# nets
n = torch.nn.Sequential(torch.nn.Linear(1, 10), torch.nn.ReLU(), torch.nn.Linear(10, 1))
nets = [n,n,n,n]

# different optimizers
opt_SGD         = torch.optim.SGD(nets[0].parameters(), lr=LR)
opt_Momentum    = torch.optim.SGD(nets[1].parameters(), lr=LR, momentum=0.8)
opt_RMSprop     = torch.optim.RMSprop(nets[2].parameters(), lr=LR, alpha=0.9)
opt_Adam        = torch.optim.Adam(nets[3].parameters(), lr=LR, betas=(0.9, 0.99))
optimizers = [opt_SGD, opt_Momentum, opt_RMSprop, opt_Adam]

# loss functions
l = torch.nn.MSELoss()
loss_funcs = [l,l,l,l]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s prot the loss functions over training, for different optimizers.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# train
labels = [&#39;SGD&#39;, &#39;Momentum&#39;, &#39;RMSprop&#39;, &#39;Adam&#39;]
train_nets(x, y, y_true, nets, optimizers, loss_funcs, labels, EPOCHS)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/08_neuralnets_141_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;training-on-batch&#34;&gt;Training on batch&lt;/h3&gt;
&lt;p&gt;Until now, we have used the whole training data at every training step. It has been batch gradient descent all along.&lt;/p&gt;
&lt;p&gt;This is fine for our ridiculously small dataset, sure, but if we want to go serious about all this, we must use mini-batch gradient descent. Thus, we need mini-batches. Thus, we need to slice our dataset accordingly. Do you want to do it manually?! Me neither!&lt;/p&gt;
&lt;p&gt;So we use PyTorch’s &lt;code&gt;DataLoader&lt;/code&gt; class for this job. We tell it which dataset to use (the one we just built in the previous section), the desired mini-batch size and if we’d like to shuffle it or not. That’s it!&lt;/p&gt;
&lt;p&gt;Our loader will behave like an iterator, so we can loop over it and fetch a different mini-batch every time.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Init data
x_tensor = torch.from_numpy(x).float().to(&#39;cpu&#39;)
y_tensor = torch.from_numpy(y).float().to(&#39;cpu&#39;)
torch_dataset = Data.TensorDataset(x_tensor, y_tensor)

# Build DataLoader
BATCH_SIZE = 25
loader = Data.DataLoader(
    dataset=torch_dataset,      # torch TensorDataset format
    batch_size=BATCH_SIZE,      # mini batch size
    shuffle=True,               # random shuffle for training
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s try using sub-samples of dimension &lt;code&gt;BATCH_SIZE = 25&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def train_NN_batch(loader, y_true, net, optimizer, loss_func, EPOCHS):
    
    # init
    losses = []

    # train
    for t in range(EPOCHS):   
        # train entire dataset 3 times
        for step, (batch_x, batch_y) in enumerate(loader):

            # compute loss
            y_hat = net(batch_x)     
            loss = loss_func(y_hat, batch_y)    

            # update parameters
            optimizer.zero_grad()   # clear gradients for next train
            loss.backward()         # backpropagation, compute gradients
            optimizer.step()        # apply gradients

        # plt every epoch
        y_hat = net(x_tensor)  
        losses += [((y_true - y_hat.detach().numpy())**2).mean()]
        if (t+1) % (EPOCHS/25) == 0:
            gradient_plot(x, y, y_hat.data.numpy(), y_true, EPOCHS, losses)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# parameters
LR = 0.1
EPOCHS = 1000
net = torch.nn.Sequential(torch.nn.Linear(1, 10), torch.nn.ReLU(), torch.nn.Linear(10, 1))
optimizer = torch.optim.SGD(net.parameters(), lr=LR)
loss_func = torch.nn.MSELoss()

# Train
train_NN_batch(loader, y_true, net, optimizer, loss_func, EPOCHS)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/08_neuralnets_147_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Two things are different now: not only we have an inner loop to load each and every mini-batch from our DataLoader but, more importantly, we are now sending only one mini-batch to the device.&lt;/p&gt;
&lt;p&gt;For bigger datasets, loading data sample by sample (into a CPU tensor) using Dataset’s &lt;code&gt;__get_item__&lt;/code&gt; and then sending all samples that belong to the same mini-batch at once to your GPU (device) is the way to go in order to make the best use of your graphics card’s RAM.&lt;/p&gt;
&lt;p&gt;Moreover, if you have many GPUs to train your model on, it is best to keep your dataset “agnostic” and assign the batches to different GPUs during training.&lt;/p&gt;
&lt;h2 id=&#34;84-advanced-topics&#34;&gt;8.4 Advanced Topics&lt;/h2&gt;
&lt;h3 id=&#34;issues&#34;&gt;Issues&lt;/h3&gt;
&lt;h4 id=&#34;starting-values&#34;&gt;Starting Values&lt;/h4&gt;
&lt;p&gt;Usually starting values for weights are chosen to be random values near zero. Hence the model starts out nearly linear, and becomes nonlinear as the weights increase.&lt;/p&gt;
&lt;h4 id=&#34;overfitting&#34;&gt;Overfitting&lt;/h4&gt;
&lt;p&gt;In early developments of neural networks, either by design or by accident, an early stopping rule was used to avoid overfitting.&lt;/p&gt;
&lt;p&gt;A more explicit method for regularization is &lt;em&gt;weight decay&lt;/em&gt;.&lt;/p&gt;
&lt;h4 id=&#34;scaling-of-the-inputs&#34;&gt;Scaling of the Inputs&lt;/h4&gt;
&lt;p&gt;Since the scaling of the inputs determines the effective scaling of the weights in the bottom layer, it can have a large effect on the quality of the final solution. At the outset it is best to standardize all inputs to have mean zero and standard deviation one.&lt;/p&gt;
&lt;h4 id=&#34;number-of-hidden-units-and-layers&#34;&gt;Number of Hidden Units and Layers&lt;/h4&gt;
&lt;p&gt;Generally speaking it is better to have too many hidden units than too few. With too few hidden units, the model might not have enough flexibility to capture the nonlinearities in the data; with too many hidden units, the extra weights can be shrunk toward zero if appropriate regularization is used.&lt;/p&gt;
&lt;p&gt;Choice of the number of hidden layers is guided by background knowledge and experimentation. Each layer extracts features of the input for regression or classification. Use of multiple hidden layers allows construction of hierarchical features at different levels of resolution.&lt;/p&gt;
&lt;p&gt;You can get an intuition on the role of hidden layers here: &lt;a href=&#34;https://playground.tensorflow.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://playground.tensorflow.org/&lt;/a&gt;&lt;/p&gt;
&lt;h4 id=&#34;multiple-minima&#34;&gt;Multiple Minima&lt;/h4&gt;
&lt;p&gt;The error function R(θ) is nonconvex, possessing many local minima. One approach is to use the average predictions over the collection of networks as the final prediction. Another approach is via &lt;em&gt;bagging&lt;/em&gt;.&lt;/p&gt;
&lt;h3 id=&#34;deep-neural-networks-and-deep-learning&#34;&gt;Deep Neural Networks and Deep Learning&lt;/h3&gt;
&lt;p&gt;Deep Neural Networks are just Neural Networks with more than one hidden layer.&lt;/p&gt;
&lt;h3 id=&#34;convolutional-neural-nets&#34;&gt;Convolutional Neural Nets&lt;/h3&gt;
&lt;p&gt;Convolutional Neural Nets are often applied when dealing with image/video data. They are usually coded with each feature being a pixel and its value is the pixel color (3 dimensional RGB array).&lt;/p&gt;
&lt;img src=&#34;../figures/cnn1.png&#34; style=&#34;width: 400px;&#34;/&gt;
&lt;p&gt;Videos and images have 2 main characteristics:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;have lots of features&lt;/li&gt;
&lt;li&gt;&amp;ldquo;close&amp;rdquo; features are often similar&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Convolutional Neural Nets exploit the second characteristic to alleviate the computational problems arising from the first. They do it by constructing a first layer that does not build on evey feature but only on adjacent ones.&lt;/p&gt;
&lt;img src=&#34;../figures/cnn1.gif&#34; style=&#34;width: 400px;&#34;/&gt;
&lt;p&gt;In this way, most of the information is preserved, on a lower dimensional representation.&lt;/p&gt;
&lt;h3 id=&#34;recurrent-neural-nets&#34;&gt;Recurrent Neural Nets&lt;/h3&gt;
&lt;p&gt;Recurrent Neural Networks are often applied in contexts in which the data generating process is dynamic. The most important example is Natural Language Processing. The idea is that you want to make predictions &amp;ldquo;live&amp;rdquo; as data comes in. Moreover, the order of the data is relevant, so that you also what to keep track of what the model has learned so far.&lt;/p&gt;
&lt;p&gt;While RNNs learn similarly while training, in addition, they remember things learnt from prior input(s) while generating output(s). It’s part of the network. RNNs can take one or more input vectors and produce one or more output vectors and the output(s) are influenced not just by weights applied on inputs like a regular NN, but also by a “hidden” state vector representing the context based on prior input(s)/output(s). So, the same input could produce a different output depending on previous inputs in the series.&lt;/p&gt;
&lt;p&gt;Grafically:&lt;/p&gt;
&lt;img src=&#34;../figures/rnn1.png&#34; style=&#34;width: 600px;&#34;/&gt;
&lt;p&gt;In summary, in a vanilla neural network, a fixed size input vector is transformed into a fixed size output vector. Such a network becomes “recurrent” when you repeatedly apply the transformations to a series of given input and produce a series of output vectors.&lt;/p&gt;
&lt;h3 id=&#34;bidirectional-rnn&#34;&gt;Bidirectional RNN&lt;/h3&gt;
&lt;p&gt;Sometimes it’s not just about learning from the past to predict the future, but we also need to look into the future to fix the past. In speech recognition and handwriting recognition tasks, where there could be considerable ambiguity given just one part of the input, we often need to know what’s coming next to better understand the context and detect the present.&lt;/p&gt;
&lt;img src=&#34;../figures/rnn2.png&#34; style=&#34;width: 600px;&#34;/&gt;
&lt;p&gt;This does introduce the obvious challenge of how much into the future we need to look into, because if we have to wait to see all inputs then the entire operation will become costly.&lt;/p&gt;
&lt;h3 id=&#34;recursive-neural-netw&#34;&gt;Recursive Neural Netw&lt;/h3&gt;
&lt;p&gt;A recurrent neural network parses the inputs in a sequential fashion. A recursive neural network is similar to the extent that the transitions are repeatedly applied to inputs, but not necessarily in a sequential fashion. Recursive Neural Networks are a more general form of Recurrent Neural Networks. It can operate on any hierarchical tree structure. Parsing through input nodes, combining child nodes into parent nodes and combining them with other child/parent nodes to create a tree like structure. Recurrent Neural Networks do the same, but the structure there is strictly linear. i.e. weights are applied on the first input node, then the second, third and so on.&lt;/p&gt;
&lt;img src=&#34;../figures/rnn3.png&#34; style=&#34;width: 600px;&#34;/&gt;
&lt;p&gt;But this raises questions pertaining to the structure. How do we decide that? If the structure is fixed like in Recurrent Neural Networks then the process of training, backprop etc makes sense in that they are similar to a regular neural network. But if the structure isn’t fixed, is that learnt as well?&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Dynamic Games</title>
      <link>https://matteocourthoud.github.io/course/empirical-io/08_dynamics_games/</link>
      <pubDate>Fri, 29 Oct 2021 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/course/empirical-io/08_dynamics_games/</guid>
      <description>&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;h3 id=&#34;intro&#34;&gt;Intro&lt;/h3&gt;
&lt;p&gt;Setting: agents making &lt;strong&gt;strategic decisions&lt;/strong&gt; (new) in &lt;strong&gt;dynamic
environments&lt;/strong&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Entry and exit: Collard-Wexler (&lt;a href=&#34;#ref-collard2013demand&#34;&gt;2013&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Sunk costs: Ryan (&lt;a href=&#34;#ref-ryan2012costs&#34;&gt;2012&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Innovation: Goettler and Gordon (&lt;a href=&#34;#ref-goettler2011does&#34;&gt;2011&lt;/a&gt;)
&lt;ul&gt;
&lt;li&gt;(or whatever changes in response to investment)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Exploitation of natural resources: Huang and Smith
(&lt;a href=&#34;#ref-huang2014dynamic&#34;&gt;2014&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Durable goods: Esteban and Shum (&lt;a href=&#34;#ref-esteban2007durable&#34;&gt;2007&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Lit review&lt;/strong&gt;: forthcoming IO Handbook chapter Aguirregabiria,
Collard-Wexler, and Ryan (&lt;a href=&#34;#ref-aguirregabiria2021dynamic&#34;&gt;2021&lt;/a&gt;)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;single--vs-multi-agent&#34;&gt;Single- vs Multi-Agent&lt;/h3&gt;
&lt;p&gt;Typically in IO we study agents in &lt;strong&gt;strategic&lt;/strong&gt; environments.
Complicated in dynamic environments.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Curse of dimensionality&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;Single agent: need to track what the agent sees ($k$ states)&lt;/li&gt;
&lt;li&gt;Multi-agent: need to keep track what every agent sees
($k^J$states)&lt;/li&gt;
&lt;li&gt;Difference exponential in the number of agents&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Expectations&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;Need not only to keep track of how the environment evolves&lt;/li&gt;
&lt;li&gt;… but also of how other players act&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Equilibrium&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;Because of the strategic interaction, the Bellman equation is
&lt;em&gt;not a contraction&lt;/em&gt; anymore
&lt;ul&gt;
&lt;li&gt;Equilibrium existence?&lt;/li&gt;
&lt;li&gt;Equilibrium uniqueness?&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;plan&#34;&gt;Plan&lt;/h3&gt;
&lt;p&gt;We will cover &lt;strong&gt;first the estimation&lt;/strong&gt; and then the computation of
dynamic games&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Weird…&lt;/li&gt;
&lt;li&gt;Standard estimation method: Bajari, Benkard, and Levin
(&lt;a href=&#34;#ref-bajari2007estimating&#34;&gt;2007&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Does &lt;strong&gt;not&lt;/strong&gt; require to solve the model&lt;/li&gt;
&lt;li&gt;Indeed, that’s the &lt;strong&gt;advantage&lt;/strong&gt; of the method&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Disadvantages&lt;/strong&gt;: still need to solve the model for counterfactuals&lt;/li&gt;
&lt;li&gt;So we’ll cover computation afterwards&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Last&lt;/strong&gt;: bridge between Structural IO and Artificial Intelligence&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Different &lt;em&gt;objectives&lt;/em&gt; but similar &lt;em&gt;methods&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;Dynamic tools niche in IO but at the core of AI&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;bajari-benkard-levin-2008&#34;&gt;Bajari, Benkard, Levin (2008)&lt;/h2&gt;
&lt;h3 id=&#34;model&#34;&gt;Model&lt;/h3&gt;
&lt;p&gt;Stylized version of Ericson and Pakes (&lt;a href=&#34;#ref-ericson1995markov&#34;&gt;1995&lt;/a&gt;)
(no entry/exit)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;$J$ firms (products) indexed by $j \in \lbrace 1, &amp;hellip;, J \rbrace$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Time $t$ is dicrete, horizon is infinite&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;States&lt;/strong&gt; $s_{jt} \in \lbrace 1, &amp;hellip; \bar s \rbrace$: quality of
product $j$ in period $t$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Actions&lt;/strong&gt; $a_{jt} \in \mathbb R^+$: investment decision of firm
$j$ in period $t$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Static payoffs&lt;/strong&gt; $$
\pi_j (s_{jt}, \boldsymbol s_{-jt}, a_{jt}; \theta^\pi)
$$ where&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\boldsymbol s_{-it}$: state vector of all other firms in period
$t$&lt;/li&gt;
&lt;li&gt;$\theta^\pi$: parameters that govern static profits&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: if we micro-fund $\pi(\cdot)$ , e.g. with some demand and
supply model, we have 2 strategic decisions: prices (static) and
investment (dynamic).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;model-2&#34;&gt;Model (2)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;State transitions&lt;/strong&gt; $$
\boldsymbol s_{t+1} = f(\boldsymbol s_t, \boldsymbol a_t, \boldsymbol \epsilon_t; \theta^f)
$$ where&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\boldsymbol a_t$: vector of actions of all firm&lt;/li&gt;
&lt;li&gt;$\boldsymbol \epsilon_t$: vector of idiosyncratic shocks&lt;/li&gt;
&lt;li&gt;$\theta^f$: parameters that govern state transitions&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Objective function&lt;/strong&gt;: firms maximize expected discounted future
profits $$
\max_{\boldsymbol a} \ \mathbb E_t \left[ \sum_{\tau=0}^\infty \beta^{\tau} \pi_{j, t+\tau} (\theta^\pi) \right]
$$&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;value-function&#34;&gt;Value Function&lt;/h3&gt;
&lt;p&gt;The value function of firm $j$ at time $t$ in state $\boldsymbol s_{t}$,
under a set of strategy functions $\boldsymbol P$ (one for each firm) is
$$
V^{\boldsymbol P_{-j}}&lt;em&gt;{j} (\mathbf{s}&lt;/em&gt;{t}) = \max_{a_{jt} \in \mathcal{A}&lt;em&gt;j \left(\mathbf{s}&lt;/em&gt;{t}\right)} \Bigg\lbrace \pi_{j}^{\boldsymbol P_{-j}} (a_{jt}, \mathbf{s}&lt;em&gt;{t} ; \theta^\pi ) + \beta \mathbb E&lt;/em&gt;{\boldsymbol s_{t+1}} \Big[  V_{j}^{\boldsymbol P_{-j}} \left(\mathbf{s}&lt;em&gt;{t+1}\right) \ \Big| \ a&lt;/em&gt;{jt}, \boldsymbol s_{t} ; \theta^f \Big] \Bigg\rbrace
$$ where&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;$\pi_{j}^{\boldsymbol P_{-j}} (a_{jt}, \mathbf{s}&lt;em&gt;{t} ; \theta^\pi )$
are the static profits of firm $j$ given action $a&lt;/em&gt;{jt}$ and policy
functions $\boldsymbol P_{-j}$ for all firms a part from $j$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The expecation $\mathbb E$ is taken with respect to the conditional
transition probabilities
$f^{\boldsymbol P_{-j}} (\mathbf{s}&lt;em&gt;{t+1} | \mathbf{s}&lt;/em&gt;{t}, a_{jt} ; \theta^f)$&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;equilibrium&#34;&gt;Equilibrium&lt;/h3&gt;
&lt;p&gt;Equillibrium notion: &lt;strong&gt;Markow Perfect Equilibrium&lt;/strong&gt; (&lt;a href=&#34;#ref-maskin1988theory&#34;&gt;Maskin and Tirole
1988&lt;/a&gt;)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Assumption&lt;/strong&gt;: players’ strategies at period $t$ are functions only
of payoff-relevant state variables at the same period&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Definition&lt;/strong&gt;: a set of $J$ value and policy functions,
$\boldsymbol V$ and $\boldsymbol P$ such that each firm
&lt;ol&gt;
&lt;li&gt;maximizes its value function $V_j$&lt;/li&gt;
&lt;li&gt;given the policy function of every other firm
$\boldsymbol P_{-j}$&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;What is it basically?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Nash Equilibrium in the policy functions&lt;/li&gt;
&lt;li&gt;What are we ruling out?
&lt;ul&gt;
&lt;li&gt;Strategies that depend on longer histories&lt;/li&gt;
&lt;li&gt;E.g. “has anyone ever cheated in a cartel?”&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;estimation&#34;&gt;Estimation&lt;/h3&gt;
&lt;p&gt;We want to estimate 2 sets of &lt;strong&gt;parameters&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\theta^\pi$: parameterizes period profit function $\pi(\cdot)$&lt;/li&gt;
&lt;li&gt;$\theta^f$: parameterizes state transition function $f(\cdot)$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Generally 2 &lt;strong&gt;approaches&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Full solution
&lt;ul&gt;
&lt;li&gt;Impractical (we’ll see more details later)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Rely on some sort of Hotz and Miller
(&lt;a href=&#34;#ref-hotz1993conditional&#34;&gt;1993&lt;/a&gt;) CCP inversion
&lt;ul&gt;
&lt;li&gt;Aguirregabiria and Mira
(&lt;a href=&#34;#ref-aguirregabiria2007sequential&#34;&gt;2007&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Bajari, Benkard, and Levin (&lt;a href=&#34;#ref-bajari2007estimating&#34;&gt;2007&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Pakes, Ostrovsky, and Berry (&lt;a href=&#34;#ref-pakes2007simple&#34;&gt;2007&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Pesendorfer and Schmidt-Dengler
(&lt;a href=&#34;#ref-pesendorfer2008asymptotic&#34;&gt;2008&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;bbl-overview&#34;&gt;BBL Overview&lt;/h3&gt;
&lt;p&gt;Bajari, Benkard, and Levin (&lt;a href=&#34;#ref-bajari2007estimating&#34;&gt;2007&lt;/a&gt;) plan&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Estimate &lt;strong&gt;transition probabilities&lt;/strong&gt; and &lt;strong&gt;conditional choice
probabilities&lt;/strong&gt; from the data&lt;/li&gt;
&lt;li&gt;Use them to simulate the &lt;strong&gt;expected value function&lt;/strong&gt;, given a set of
parameters&lt;/li&gt;
&lt;li&gt;Use optimality of estimated choices to pin down static profit
parameters
&lt;ul&gt;
&lt;li&gt;I.e. repeat (2) for alternative strategies
&lt;ul&gt;
&lt;li&gt;By definition suboptimal&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Estimating equation&lt;/strong&gt;: values implied by observed strategies
should be higher than values implied by alternative strategies&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;bbl-first-stage&#34;&gt;BBL: First Stage&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Estimate the &lt;strong&gt;transition probabilities&lt;/strong&gt;
$f ( \cdot | a_{jt}, \boldsymbol s_t; \hat \theta^f )$
&lt;ul&gt;
&lt;li&gt;I.e. what is the observed frequency of any state-to-state
transition?&lt;/li&gt;
&lt;li&gt;For any given action of firm $j$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;… and &lt;strong&gt;conditional choice probabilities&lt;/strong&gt;
$\hat P_j(\cdot | \boldsymbol s_t)$
&lt;ul&gt;
&lt;li&gt;I.e. what is the probability of each action, for each firm $j$
in each state $\boldsymbol s$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Can be done &lt;strong&gt;non-parametrically&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;i.e. just observe frequencies&lt;/li&gt;
&lt;li&gt;Conditional on having enough data&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Note&lt;/strong&gt;: need to estimate transitions, conditional on each
state and action&lt;/li&gt;
&lt;li&gt;Problem with many states and actions, but especially with &lt;strong&gt;many
players&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;Curse of dimensionality&lt;/li&gt;
&lt;li&gt;Number of states increases exponentially in number of
players&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Important&lt;/strong&gt;: parametric assumptions would contradict the model for
the estimation of value/policy functions&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;bbl-second-stage&#34;&gt;BBL: Second Stage&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;First step&lt;/strong&gt;: from transitions $f(\hat \theta^f)$ and CCPs
$\boldsymbol{\hat P}$ to values&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;We can use transitions and CCPs to simulate &lt;strong&gt;histories&lt;/strong&gt; (of length
$\tilde T$)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;of states
$\lbrace \boldsymbol{\tilde{s}&lt;em&gt;{\tau}} \rbrace&lt;/em&gt;{\tau = 1}^{\tilde T}$&lt;/li&gt;
&lt;li&gt;and actions
$\lbrace \boldsymbol{\tilde{a}&lt;em&gt;{\tau}} \rbrace&lt;/em&gt;{\tau = 1}^{\tilde T}$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Given a parameter value $\tilde \theta^\pi$, we can compute &lt;strong&gt;static
payoffs&lt;/strong&gt;:
$\pi_{j}^{\boldsymbol {\hat{P}&lt;em&gt;{-j}}} \left( \tilde a&lt;/em&gt;{j\tau}, \boldsymbol{\tilde s}_{\tau} ; \tilde \theta^\pi \right)$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Simulated history + static payoffs = &lt;strong&gt;simulated value function&lt;/strong&gt; $$
{V}&lt;em&gt;{j}^{\boldsymbol {\hat{P}}} \left(\boldsymbol{s}&lt;/em&gt;{t} ; \tilde \theta^\pi \right) =  \sum_{\tau=0}^{\tilde T} \beta^{\tau} \pi_{j}^{\boldsymbol {\hat{P}&lt;em&gt;{-j}}} \left( \tilde a&lt;/em&gt;{j\tau}, \boldsymbol{\tilde s}_{\tau} ; \tilde \theta^\pi \right)
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;We can average over many, e.g. $R$, simulated value functions to get
an &lt;strong&gt;expected value function&lt;/strong&gt; $$
{V}&lt;em&gt;{j}^{\boldsymbol {\hat{P}}, R} \left( \boldsymbol{s}&lt;/em&gt;{t} ; \tilde \theta^\pi \right) = \frac{1}{R}  \sum_{r=0}^{R}\Bigg( \sum_{\tau=0}^{\tilde T} \beta^{\tau} \pi_{j}^{\boldsymbol {\hat{P}&lt;em&gt;{-j}}} \left(\tilde a^{(r)}&lt;/em&gt;{j\tau}, \boldsymbol{\tilde s}^{(r)}_{\tau} ; \tilde \theta^\pi \right) \Bigg)
$$&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;in-practice-for-a-parameter-value-tilde-thetapi&#34;&gt;In practice, for a parameter value $\tilde \theta^\pi$&lt;/h3&gt;
&lt;p&gt;For $r = 1, &amp;hellip;, R$ simulations do:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Initialize firms value to zero&lt;/li&gt;
&lt;li&gt;Fot $\tau=0, &amp;hellip;, \tilde T$ do
&lt;ul&gt;
&lt;li&gt;For each state in $\boldsymbol{\tilde s}^{(r)}_{\tau}$ do:
&lt;ul&gt;
&lt;li&gt;Use $\boldsymbol{\hat P}$ to &lt;em&gt;draw&lt;/em&gt; a vector of firm actions
$\boldsymbol{\tilde a}^{(r)}_{\tau}$&lt;/li&gt;
&lt;li&gt;For each firm $j = 1, &amp;hellip;, J$ do:
&lt;ul&gt;
&lt;li&gt;Compute static profits
$\pi_{j}^{\boldsymbol {\hat{P}&lt;em&gt;{-j}}} \left(\tilde a^{(r)}&lt;/em&gt;{j\tau}, \boldsymbol{\tilde s}^{(r)}_{\tau} ; \tilde \theta^\pi \right)$&lt;/li&gt;
&lt;li&gt;Add discounted profits
$\beta^{\tau} \pi_{j}^{\boldsymbol {\hat{P}&lt;em&gt;{-j}}} \left(\tilde a^{(r)}&lt;/em&gt;{j\tau}, \boldsymbol{\tilde s}^{(r)}_{\tau} ; \tilde \theta^\pi \right)$
to the value function&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Use
$f ( \cdot | \boldsymbol {a_{t}}, \boldsymbol s_t; \hat \theta^f )$
to &lt;em&gt;draw&lt;/em&gt; the next state
$\boldsymbol{\tilde s}^{(r)}_{\tau + 1}$&lt;/li&gt;
&lt;li&gt;Use the next state, $\boldsymbol{\tilde s}^{(r)}_{\tau + 1}$
as current state for the next iteration&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Then average all the value functions together to obtain an &lt;strong&gt;expected
value function&lt;/strong&gt;
$V_{j}^{\boldsymbol {\hat{P}}, R} \left(\boldsymbol{s}_{t} ; \tilde \theta^\pi \right)$&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: advantage of simulations: can be parallelized&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;objective-function&#34;&gt;Objective Function&lt;/h3&gt;
&lt;p&gt;What have we done so far?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Given some parameters $\theta^\pi$, we computed the &lt;strong&gt;expected value
function&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;How do we pick the $\theta^\pi$ that best rationalizes the data?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;I.e. what is the &lt;strong&gt;objective function&lt;/strong&gt;?&lt;/li&gt;
&lt;li&gt;Potentially many options&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;BBL idea&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the expected value function has to be optimal, given the CCPs&lt;/li&gt;
&lt;li&gt;I.e. any other policy function should give a lower expected value&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;“Best”&lt;/strong&gt; $\theta^\pi$: those for which the implied expected value
function under the estimated CCPs is greater than the one implied by
&lt;em&gt;any other&lt;/em&gt; CCP&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: it’s an inequality statement&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;objective-function-2&#34;&gt;Objective Function (2)&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Idea&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;If the observed policy ${\color{green}{\boldsymbol{\hat P}}}$ is
optimal,&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;All other policies ${\color{red}{\boldsymbol{\tilde P}}}$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;… at the true parameters $\theta^f$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;… should give a lower expected value $$
V_{j}^{{\color{red}{\boldsymbol{\tilde P}}}, R} \left( \boldsymbol{s}&lt;em&gt;{t} ; \tilde \theta^\pi \right) \leq V&lt;/em&gt;{j}^{{\color{green}{\boldsymbol{\hat P}}}, R} \left( \boldsymbol{s}_{t} ; \tilde \theta^\pi \right)
$$&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;So which are the true parameters?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Those for which any deviation from the observed policy
${\color{green}{\boldsymbol{\hat P}}}$ yields a lower value&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Objective function&lt;/strong&gt; to minimize: &lt;strong&gt;violations&lt;/strong&gt; under
alternative policies ${\color{red}{\boldsymbol{\tilde P}}}$ $$
\min_{\tilde \theta^\pi} \sum_{\boldsymbol s_{t}} \sum_{{\color{red}{\boldsymbol{\tilde P}}}} \Bigg[\min \bigg\lbrace V_{j}^{{\color{green}{\boldsymbol{\hat P}}}, R} \left( \boldsymbol{s}&lt;em&gt;{t} ; \tilde \theta^\pi \right) - V&lt;/em&gt;{j}^{{\color{red}{\boldsymbol{\tilde P}}}, R} \left( \boldsymbol{s}_{t} ; \tilde \theta^\pi \right) \ , \ 0 \bigg\rbrace \Bigg]^{2}
$$&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;estimator&#34;&gt;Estimator&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Estimator&lt;/strong&gt;: $\theta^\pi$ that minimizes the average (squared)
magnitude of violations for any alternative policy
${\color{red}{\boldsymbol{\tilde P}}}$ $$
\hat{\theta}^\pi= \arg \min_{\tilde \theta^\pi} \sum_{\boldsymbol s_{t}} \sum_{{\color{red}{\boldsymbol{\tilde P}}}} \Bigg[\min \bigg\lbrace V_{j}^{{\color{green}{\boldsymbol{\hat P}}}, R} \left( \boldsymbol{s}&lt;em&gt;{t} ; \tilde \theta^\pi \right) - V&lt;/em&gt;{j}^{{\color{red}{\boldsymbol{\tilde P}}}, R} \left( \boldsymbol{s}_{t} ; \tilde \theta^\pi \right) \ , \ 0 \bigg\rbrace \Bigg]^{2}
$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\min \Big\lbrace V_{j}^{{\color{green}{\boldsymbol{\hat P}}}, R} - V_j^{{\color{red}{\boldsymbol{\tilde P}}}, R} \ , \ 0 \Big\rbrace$
to pick only the violations
&lt;ul&gt;
&lt;li&gt;If ${\color{green}{\boldsymbol{\hat P}}}$ implies higher value,
we can ignore&lt;/li&gt;
&lt;li&gt;Doesn’t matter by how much you respect the inequality&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Which alternative policies&lt;/strong&gt;
${\color{red}{\boldsymbol{\tilde P}}}$ should we use?
&lt;ul&gt;
&lt;li&gt;In principle, any perturbation is ok&lt;/li&gt;
&lt;li&gt;But &lt;strong&gt;in practice&lt;/strong&gt;, if we perturbe it too much, we can go too
far off&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Tip 1&lt;/strong&gt;: start with very &lt;em&gt;small&lt;/em&gt; perturbations&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Tip 2&lt;/strong&gt;: use perturbation that &lt;em&gt;sensibly&lt;/em&gt; affect the dynamics
&lt;ul&gt;
&lt;li&gt;E.g. exiting in a state in which a firm is not a competitive
threat&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Tip 3&lt;/strong&gt;: use perturbations on dimensions that are &lt;em&gt;relevant&lt;/em&gt;
for the research question
&lt;ul&gt;
&lt;li&gt;E.g. they affect dimensions where you want to make
counterfactual predictions&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;advantages&#34;&gt;Advantages&lt;/h3&gt;
&lt;p&gt;We have seen that there are &lt;strong&gt;competing methods&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;What are the &lt;strong&gt;advantages&lt;/strong&gt; of Bajari, Benkard, and Levin
(&lt;a href=&#34;#ref-bajari2007estimating&#34;&gt;2007&lt;/a&gt;) over those?&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Continuous actions&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;BBL does not require actions to be discretised&lt;/li&gt;
&lt;li&gt;You can just sample actions from the data!&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Choice of alternative CCPs&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;The researcher is free to choose the alternative CCPs
${\color{red}{\boldsymbol{\tilde P}}}$&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Pros&lt;/strong&gt;: can make source of variation more transparent
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;allows the researcher to focus on those predictions of the
model that are key for the specific research questions&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Cons&lt;/strong&gt;: it’s a &lt;em&gt;very&lt;/em&gt; high dimensional space
&lt;ul&gt;
&lt;li&gt;There are &lt;em&gt;very very&lt;/em&gt; many alternative policy functions&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;problems&#34;&gt;Problems&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;Computational &lt;strong&gt;curse of dimensionality&lt;/strong&gt; is gone (in the state
space)
&lt;ul&gt;
&lt;li&gt;But we have a curse of dimensionality in data&lt;/li&gt;
&lt;li&gt;Need a lot of markets because &lt;strong&gt;now 1 market is 1 observation&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Multiple equilibria&lt;/strong&gt;??
&lt;ul&gt;
&lt;li&gt;We are basically assuming it away&lt;/li&gt;
&lt;li&gt;Estimating the CCPs in the first stage we assume that is the
equilibrium that is played in all markets at all times&lt;/li&gt;
&lt;li&gt;To run counterfactuals, we &lt;strong&gt;still need to solve the model&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Unobserved heterogeneity
&lt;ul&gt;
&lt;li&gt;Kasahara and Shimotsu (&lt;a href=&#34;#ref-kasahara2009nonparametric&#34;&gt;2009&lt;/a&gt;):
how to identify the (minimum) number of unobserved types&lt;/li&gt;
&lt;li&gt;Arcidiacono and Miller
(&lt;a href=&#34;#ref-arcidiacono2011conditional&#34;&gt;2011&lt;/a&gt;): how to use an EM
algorithm for the 1st stage estimation with unobserved types,
conditional on the number of types&lt;/li&gt;
&lt;li&gt;Berry and Compiani (&lt;a href=&#34;#ref-berry2021empirical&#34;&gt;2021&lt;/a&gt;):
instrumental variables approach, relying on observed states in
the distant past&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Non-stationarity&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;If we have a long time period, something fundamentally might
have changed&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;ericson-pakes-1995&#34;&gt;Ericson Pakes (1995)&lt;/h2&gt;
&lt;h3 id=&#34;introduction-1&#34;&gt;Introduction&lt;/h3&gt;
&lt;p&gt;Ericson and Pakes (&lt;a href=&#34;#ref-ericson1995markov&#34;&gt;1995&lt;/a&gt;) and companion paper
Pakes and McGuire (&lt;a href=&#34;#ref-pakes1994computing&#34;&gt;1994&lt;/a&gt;) for the computation&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;$J$ firms indexed by $j \in \lbrace 1, &amp;hellip;, J \rbrace$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Time $t$ is dicrete $t$, horizon is infinite&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;State $s_{jt}$: quality of firm $j$ in period $t$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Per period profits $$
\pi (s_{jt}, \boldsymbol s_{-jt}, ; \theta^\pi)
$$ where&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\boldsymbol s_{-it}$: state vector of all other firms in period
$t$&lt;/li&gt;
&lt;li&gt;$\theta^\pi$: parameters that govern static profits&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;We can micro-fund profits with some demand and supply functions&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;There can be some underlying static strategic interaction&lt;/li&gt;
&lt;li&gt;E.g. logit demand and bertrand competition in prices $p_{it}$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;state-transitions&#34;&gt;State Transitions&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Investment&lt;/strong&gt;: firms can invest an dollar amount $x$ to increase their
future quality&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Continuous decision variable ($\neq$ Rust)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Probability that investment is successful $$
\Pr \big(i_{jt} \ \big| \ a_{it} = x \big) = \frac{\alpha x}{1 + \alpha x}
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Higher investment, higher success probability&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$\alpha$ parametrizes the returns on investment&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Quality depreciation&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;With probability $\delta$, quality decreases by one level&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Law of motion&lt;/strong&gt; $$
s_{j,t+1} = s_{jt} + i_{jt} - \delta
$$&lt;/p&gt;
&lt;h3 id=&#34;decision-variables&#34;&gt;Decision Variables&lt;/h3&gt;
&lt;p&gt;Note that in Ericson and Pakes (&lt;a href=&#34;#ref-ericson1995markov&#34;&gt;1995&lt;/a&gt;) we have
two separate decision variables&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Static&lt;/strong&gt; decision variable: price $p_{jt}$&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Dynamic&lt;/strong&gt; decision variable: investment $i_{jt}$&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Does not have to be the case!&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Example&lt;/strong&gt;: Besanko et al. (&lt;a href=&#34;#ref-besanko2010learning&#34;&gt;2010&lt;/a&gt;)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Model of &lt;strong&gt;learning-by-doing&lt;/strong&gt;: firms decrease their marginal cost
through sales&lt;/li&gt;
&lt;li&gt;State variable: firm stock of know how $e$
&lt;ul&gt;
&lt;li&gt;The higher the stock of know-how, the lower the marginal cost&lt;/li&gt;
&lt;li&gt;Increases when a firm manages to make a sale
&lt;ul&gt;
&lt;li&gt;$q \in [0,1]$ now is both static quantity and transition
probability&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Single&lt;/strong&gt; decision variable: price $p$
&lt;ul&gt;
&lt;li&gt;Usual static effects on profits
$\pi_{jt} = (p_{jt} - c(e_{jt})) \cdot q_j(\boldsymbol p_t)$&lt;/li&gt;
&lt;li&gt;But also dynamic effect through transition probabilities
&lt;ul&gt;
&lt;li&gt;Probability of increasing $e_t$: $q_j(\boldsymbol p_t)$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;equilibrium-1&#34;&gt;Equilibrium&lt;/h3&gt;
&lt;p&gt;Firms maximize the expected flow of discounted profits $$
\max_{\boldsymbol a} \ \mathbb E_t \left[ \sum_{\tau=0}^\infty \beta^{\tau} \pi_{j, t+\tau} (\theta^\pi) \right]
$$ &lt;strong&gt;Markow Perfect Equilibrium&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Equillibrium notion: &lt;strong&gt;Markow Perfect Equilibrium&lt;/strong&gt; (&lt;a href=&#34;#ref-maskin1988theory&#34;&gt;Maskin and Tirole
1988&lt;/a&gt;)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A set of $J$ value and policy functions, $\boldsymbol V$ and
$\boldsymbol P$ such that each firm
&lt;ol&gt;
&lt;li&gt;maximizes its value function $V_j$&lt;/li&gt;
&lt;li&gt;given the policy function of every other firm
$\boldsymbol P_{-j}$&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;exit&#34;&gt;Exit&lt;/h3&gt;
&lt;p&gt;One important extension is &lt;strong&gt;exit&lt;/strong&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;In each time period, incuments decide whether to stay&lt;/li&gt;
&lt;li&gt;… or exit and get a scrap value $\phi^{exit}$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The Belman Equation of incumbent $j$ at time $t$ is $$
V^{\boldsymbol P_{-j}}&lt;em&gt;{j} (\mathbf{s}&lt;/em&gt;{t}) = \max_{d^{exit}&lt;em&gt;{jt} \in \lbrace 0, 1 \rbrace} \Bigg\lbrace
\begin{array}{c}
\beta \phi^{exit} \ , \newline
\max&lt;/em&gt;{a_{jt} \in \mathcal{A}&lt;em&gt;j \left(\mathbf{s}&lt;/em&gt;{t}\right)} \Big\lbrace  \pi_{j}^{\boldsymbol P_{-j}} (a_{jt}, \mathbf{s}&lt;em&gt;{t} ; \theta^\pi ) + \beta \mathbb E&lt;/em&gt;{\boldsymbol s_{t+1}} \Big[  V_{j}^{\boldsymbol P_{-j}} \left(\mathbf{s}&lt;em&gt;{t+1}\right) \ \Big| \ a&lt;/em&gt;{jt}, \boldsymbol s_{t} ; \theta^f \Big] \Big\rbrace
\end{array}
\Bigg\rbrace
$$ where&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\phi^{exit}$: exit scrap value&lt;/li&gt;
&lt;li&gt;$d^{exit}_{jt} \in \lbrace 0,1 \rbrace$: exit decision&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;entry&#34;&gt;Entry&lt;/h3&gt;
&lt;p&gt;We can also incorporate endogenous &lt;strong&gt;entry&lt;/strong&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;One or more &lt;strong&gt;potential entrants&lt;/strong&gt; exist outside the market&lt;/li&gt;
&lt;li&gt;They can pay an entry cost $\phi^{entry}$ and enter the market at a
quality state $\bar s$&lt;/li&gt;
&lt;li&gt;… or remain outside at no cost&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Value function $$
V_{j}^{\boldsymbol P_{-j}} (e, \boldsymbol x_{-jt} ; \theta) = \max_{d^{entry} \in \lbrace 0,1 \rbrace }
\Bigg\lbrace
\begin{array}{c}
0 \ ; \newline&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;\phi^{entry} + \beta \mathbb E_{\boldsymbol s_{t+1}} \Big[ V_{j}^{\boldsymbol P_{-j}} (\bar s, \boldsymbol s_{-j, t+1} ; \theta) \ \Big| \ \boldsymbol s_{t} ; \theta^f \Big]
\end{array}
\Bigg\rbrace
$$ where&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$d^{entry} \in \lbrace 0,1 \rbrace$: entry decision&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$\phi^{entry}$: entry cost&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$\bar s$: state in which entrants enters (could be random)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Do we observe potential entrants?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Igami (&lt;a href=&#34;#ref-igami2017estimating&#34;&gt;2017&lt;/a&gt;): tech industry announce
their entry&lt;/li&gt;
&lt;li&gt;Critique: not really potential entrants, they are half-way inside&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;equilibrium-existence&#34;&gt;Equilibrium Existence&lt;/h3&gt;
&lt;p&gt;Doraszelski and Satterthwaite (&lt;a href=&#34;#ref-doraszelski2010computable&#34;&gt;2010&lt;/a&gt;):
a MPE might not exist in Ericson and Pakes
(&lt;a href=&#34;#ref-ericson1995markov&#34;&gt;1995&lt;/a&gt;) model.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Solution&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Replace fixed entry costs $\phi^{entry}$ and exit scrap values
$\phi^{exit}$ with random ones&lt;/li&gt;
&lt;li&gt;It becomes a game of incomplete information
&lt;ul&gt;
&lt;li&gt;First explored in Rust (&lt;a href=&#34;#ref-rust1994structural&#34;&gt;1994&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;New equilibrium concept&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Markov Perfect Bayesian Nash Equilibrium (MPBNE)&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Basically the same, with rational beliefs on random variables&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;solving-the-model&#34;&gt;Solving the Model&lt;/h3&gt;
&lt;p&gt;Solving the model is very similar to Rust&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Given parameter values $\theta$&lt;/li&gt;
&lt;li&gt;Start with a guess for the value and policy functions&lt;/li&gt;
&lt;li&gt;Until convergence, do:
&lt;ul&gt;
&lt;li&gt;For each firm $j = 1, &amp;hellip;, J$, do:
&lt;ul&gt;
&lt;li&gt;Take the policy functions of all other firms&lt;/li&gt;
&lt;li&gt;Compute the implied transition probabilities&lt;/li&gt;
&lt;li&gt;Use them to compute the new policy function for firm $j$&lt;/li&gt;
&lt;li&gt;Compute the implied value function&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;Where do things get complicated / tricky? Policy function update&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;policy-update-example-exit-game&#34;&gt;Policy Update Example: exit game&lt;/h3&gt;
&lt;p&gt;Imagine a stylized exit game with 2 firms&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Easy to get an update rule of the form: &lt;em&gt;“exit if opponent stays,
stay if opponent exits”&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Computationally&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Initialize policy functions to $(exit, exit)$&lt;/li&gt;
&lt;li&gt;Iteration 1:
&lt;ul&gt;
&lt;li&gt;Each firm takes opponent policy as given: $exit$&lt;/li&gt;
&lt;li&gt;Update own optimal policy: $stay$&lt;/li&gt;
&lt;li&gt;New policy: $(stay, stay)$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Iteration 2: $(stay, stay) \to (exit, exit)$&lt;/li&gt;
&lt;li&gt;Iteration 2: $(exit, exit) \to (stay, stay)$&lt;/li&gt;
&lt;li&gt;Etc…&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Issues&lt;/strong&gt;: value function iteration might not converge and
equilibrium multeplicity.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;convergence-tips&#34;&gt;Convergence Tips&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Try different &lt;strong&gt;starting values&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;Often it’s what makes the biggest difference&lt;/li&gt;
&lt;li&gt;Ideally, start as close as possible to true values&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Approximation methods&lt;/strong&gt; can help (we’ll see more later)
&lt;ul&gt;
&lt;li&gt;I.e. get a fast approximation to use as starting vlaue for
solution algorithm&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Partial/stochastic &lt;strong&gt;value function update rule&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;Instead of $V&#39; = T(V)$, use $V&#39; = \alpha T(V) + (1-\alpha)V$&lt;/li&gt;
&lt;li&gt;Very good to break loops, especially if $\alpha$ is stochastic,
e.g. $\alpha \sim U(0,1)$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;How large is the &lt;strong&gt;support of the entry/exit costs&lt;/strong&gt;?
&lt;ul&gt;
&lt;li&gt;If support is too small, you end up back in the entry/exit loop&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Try alternative &lt;strong&gt;non-parallel updating schemes&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;E.g. update value one state at the time (in random order?)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Last but not least: &lt;strong&gt;change the model&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;In particular, from simultaneous to alternating moves&lt;/li&gt;
&lt;li&gt;or continuous time&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;multiple-equilibria&#34;&gt;Multiple Equilibria&lt;/h3&gt;
&lt;p&gt;How to find them?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Besanko et al. (&lt;a href=&#34;#ref-besanko2010learning&#34;&gt;2010&lt;/a&gt;) and Borkovsky,
Doraszelski, and Kryukov (&lt;a href=&#34;#ref-borkovsky2010user&#34;&gt;2010&lt;/a&gt;):
&lt;strong&gt;homotopy method&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;can find some equilibria, but not all&lt;/li&gt;
&lt;li&gt;complicated to implement: need to compute first order conditions
$H(\boldsymbol V, \theta) = 0$ and their Jacobian
$\Delta H(\boldsymbol V, \theta)$&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Idea&lt;/strong&gt;: trace the equilibrium correspondence
$H^{-1} = \lbrace (\boldsymbol V, \theta) : H(\boldsymbol V, \theta) = 0 \rbrace$
in the value-parameter space&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Eibelshäuser and Poensgen (&lt;a href=&#34;#ref-eibelshauser2019markov&#34;&gt;2019&lt;/a&gt;)
&lt;ul&gt;
&lt;li&gt;Markov Quantal Response Equilibrium&lt;/li&gt;
&lt;li&gt;approact dynamic games from a evolutionary game theory
perspective
&lt;ul&gt;
&lt;li&gt;actions played at random and those bringing highest payoffs
survive&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;$\to$ homothopy method guaranteed to find one equilibrium&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Pesendorfer and Schmidt-Dengler
(&lt;a href=&#34;#ref-pesendorfer2010sequential&#34;&gt;2010&lt;/a&gt;): some equilibria are not
Lyapunov-stable
&lt;ul&gt;
&lt;li&gt;BR iteration cannot find them unless you start exactly at the
solution&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Su and Judd (&lt;a href=&#34;#ref-su2012constrained&#34;&gt;2012&lt;/a&gt;) and Egesdal, Lai, and
Su (&lt;a href=&#34;#ref-egesdal2015estimating&#34;&gt;2015&lt;/a&gt;): same point, but numerically
&lt;ul&gt;
&lt;li&gt;using MPEC approach&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;multiple-equilibria-2&#34;&gt;Multiple Equilibria (2)&lt;/h3&gt;
&lt;p&gt;Can we assume them away?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Igami (&lt;a href=&#34;#ref-igami2017estimating&#34;&gt;2017&lt;/a&gt;)
&lt;ul&gt;
&lt;li&gt;Finite horizon&lt;/li&gt;
&lt;li&gt;Homogenous firms (in profit functions and state transitions)&lt;/li&gt;
&lt;li&gt;One dynamic move per period (overall, not per-firm)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Abbring and Campbell (&lt;a href=&#34;#ref-abbring2010last&#34;&gt;2010&lt;/a&gt;)
&lt;ul&gt;
&lt;li&gt;Entry/exit game&lt;/li&gt;
&lt;li&gt;Homogeneous firms&lt;/li&gt;
&lt;li&gt;Entry and exit decisions are follow a last-in first-out (LIFO)
structure
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;“An entrant expects to produce no longer than any
incumbent”&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Iskhakov, Rust, and Schjerning (&lt;a href=&#34;#ref-iskhakov2016recursive&#34;&gt;2016&lt;/a&gt;)
&lt;ul&gt;
&lt;li&gt;can find all equilibria, but for very specific class of dynamic
games&lt;/li&gt;
&lt;li&gt;must always proceed “forward”
&lt;ul&gt;
&lt;li&gt;e.g. either entry or exit but not both&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Idea: can solve by backward induction even if horizon is
infinite&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;curse-of-dimensionality&#34;&gt;Curse of Dimensionality&lt;/h3&gt;
&lt;p&gt;What are the computational bottlenecks? $$
V^{\boldsymbol P_{-j}}&lt;em&gt;{j} ({\color{red}{\mathbf{s}&lt;/em&gt;{t}}}) = \max_{a_{jt} \in \mathcal{A}&lt;em&gt;j \left(\mathbf{s}&lt;/em&gt;{t}\right)} \Bigg\lbrace \pi_{j}^{\boldsymbol P_{-j}} (a_{jt}, \mathbf{s}&lt;em&gt;{t} ; \theta^\pi ) + \beta \mathbb E&lt;/em&gt;{{\color{red}{\mathbf{s}&lt;em&gt;{t+1}}}} \Big[  V&lt;/em&gt;{j}^{\boldsymbol P_{-j}} \left(\mathbf{s}&lt;em&gt;{t+1}\right) \ \Big| \ a&lt;/em&gt;{jt}, \boldsymbol s_{t} ; \theta^f \Big] \Bigg\rbrace
$$&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Dimension of the state space&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;In single agent problems, we have as many states as many values
of $s_{jt}$ ($k$)&lt;/li&gt;
&lt;li&gt;In dynamics games, the state space goes from $k$ to $k^J$&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;symmetry helps&lt;/strong&gt;: state $[1,2,3]$ and $[1,3,2]$ become the
same for firm 1&lt;/li&gt;
&lt;li&gt;How much do we gain? From $k^J$ to
$k \cdot {k + J - 2 \choose k - 1}$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Dimension of the integrand&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;If in single agent problems, we have to integrate over $\kappa$
outcomes,
&lt;ul&gt;
&lt;li&gt;4 in Rust: engine replaced (yes|no) $\times$ mileage
increases (yes|no)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;… in dynamic games, we have to consider $\kappa^J$ outcomes&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: bottlenecks are not addittive but multiplicative: have to
solve the expectation for each point in the state space. Improving on
any of the two helps a lot.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;curse-of-dimensionality-2&#34;&gt;Curse of Dimensionality (2)&lt;/h3&gt;
&lt;p&gt;Two and a half classes of solutions:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Computational&lt;/strong&gt;: approximate the equilibrium
&lt;ul&gt;
&lt;li&gt;Doraszelski (&lt;a href=&#34;#ref-doraszelski2003r&#34;&gt;2003&lt;/a&gt;): use Chebyshev
polynomials for a basis function&lt;/li&gt;
&lt;li&gt;Farias, Saure, and Weintraub
(&lt;a href=&#34;#ref-farias2012approximate&#34;&gt;2012&lt;/a&gt;): combine approximations
with a MPEC-like approach&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Conceptual&lt;/strong&gt;: define another game
&lt;ul&gt;
&lt;li&gt;Weintraub, Benkard, and Van Roy
(&lt;a href=&#34;#ref-weintraub2008markov&#34;&gt;2008&lt;/a&gt;): oblivious equilibrium&lt;/li&gt;
&lt;li&gt;Ifrach and Weintraub (&lt;a href=&#34;#ref-ifrach2017framework&#34;&gt;2017&lt;/a&gt;): moment
based equilibrium&lt;/li&gt;
&lt;li&gt;Doraszelski and Judd (&lt;a href=&#34;#ref-doraszelski2012avoiding&#34;&gt;2012&lt;/a&gt;):
games in continuous time&lt;/li&gt;
&lt;li&gt;Doraszelski and Judd (&lt;a href=&#34;#ref-doraszelski2019dynamic&#34;&gt;2019&lt;/a&gt;):
games with random moves&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Kind of both: Pakes and McGuire (&lt;a href=&#34;#ref-pakes2001stochastic&#34;&gt;2001&lt;/a&gt;)
&lt;ul&gt;
&lt;li&gt;experience-based equilibrium (&lt;a href=&#34;#ref-fershtman2012dynamic&#34;&gt;Fershtman and Pakes
2012&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: useful also to get good starting values for a full solution
method!&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;oblivious-equilibrium&#34;&gt;Oblivious Equilibrium&lt;/h3&gt;
&lt;p&gt;Weintraub, Benkard, and Van Roy (&lt;a href=&#34;#ref-weintraub2008markov&#34;&gt;2008&lt;/a&gt;): what
if &lt;strong&gt;firms had no idea about the state of other firms&lt;/strong&gt;?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;or atomistic firms&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The value function becomes $$
V_{j} ({\color{red}{s_{t}}}) = \max_{a_{jt} \in \mathcal{A}&lt;em&gt;j \left({\color{red}{s&lt;/em&gt;{t}}}\right)} \Bigg\lbrace {\color{red}{\mathbb E_{\boldsymbol s_t}}} \Big[ \pi_{j} (a_{jt}, \mathbf{s}&lt;em&gt;{t} ; \theta^\pi ) \Big|  P  \Big] + \beta \mathbb E&lt;/em&gt;{{\color{red}{s_{t+1}}}} \Big[  V_{j} \left({\color{red}{s_{t+1}}}\right) \ \Big| \ a_{jt}, {\color{red}{s_{t}}} ; \theta^f \Big] \Bigg\rbrace
$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Now the state is just $s_t$ instead of $\boldsymbol s_t$
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Huge&lt;/strong&gt; computational gain: from $k^J$ points to $k$&lt;/li&gt;
&lt;li&gt;Also the expectation of future states is taken over $3$ instead
of $3^J$ points
&lt;ul&gt;
&lt;li&gt;(3 because quality can go up, down or stay the same)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;But need to compute static profits as the expected value given the
current policy function
&lt;ul&gt;
&lt;li&gt;Need to keep track of the asymptotic state distribution as you
iterate the value&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;games-with-random-moves&#34;&gt;Games with Random Moves&lt;/h3&gt;
&lt;p&gt;Doraszelski and Judd (&lt;a href=&#34;#ref-doraszelski2019dynamic&#34;&gt;2019&lt;/a&gt;): what if
instead of simultaneously, firms would &lt;strong&gt;move one at the time at
random&lt;/strong&gt;?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Important&lt;/strong&gt;: to have the same frequency of play, &lt;strong&gt;a period now is
$J$ times shorter&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The value function becomes $$
V^{\boldsymbol P_{-j}}&lt;em&gt;{j} (\mathbf{s}&lt;/em&gt;{t}, {\color{red}{n=j}}) = \max_{a_{jt} \in \mathcal{A}&lt;em&gt;j \left(\mathbf{s}&lt;/em&gt;{t}\right)} \Bigg\lbrace {\color{red}{\frac{1}{J}}}\pi_{j}^{\boldsymbol P_{-j}} (a_{jt}, \mathbf{s}&lt;em&gt;{t} ; \theta^\pi ) + {\color{red}{\sqrt[J]{\beta}}} \mathbb E&lt;/em&gt;{{\color{red}{n,  s_{j, t+1}}}} \Big[  V_{j}^{\boldsymbol P_{-j}} \left(\mathbf{s}&lt;em&gt;{t+1}, {\color{red}{n}} \right) \ \Big| \ a&lt;/em&gt;{jt}, \boldsymbol s_{t} ; \theta^f \Big] \Bigg\rbrace
$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$n$: indicates whose turn is to play&lt;/li&gt;
&lt;li&gt;since a turn is $J$ times shorter, profits are $\frac{1}{J} \pi$ and
discount factor is $\sqrt[J]{\beta}$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Computational gain&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Expectation now taken over $n, s_{j, t+1}$ instead of
$\boldsymbol s_{t+1}$&lt;/li&gt;
&lt;li&gt;I.e. $Jk$ points instead of $3^k$ (3 because quality can go up, down
or stay the same)&lt;/li&gt;
&lt;li&gt;Huge computational difference!&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;games-in-continuous-time&#34;&gt;Games in Continuous Time&lt;/h3&gt;
&lt;p&gt;Doraszelski and Judd (&lt;a href=&#34;#ref-doraszelski2012avoiding&#34;&gt;2012&lt;/a&gt;): what’s the
advantage of continuous time?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Probability that two firms take a decision simultaneously is zero&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;With continuous time, the value function becomes $$
V^{\boldsymbol P_{-j}}&lt;em&gt;{j} (\mathbf{s}&lt;/em&gt;{t}) = \max_{a_{jt} \in \mathcal{A}&lt;em&gt;j \left(\mathbf{s}&lt;/em&gt;{t}\right)} \Bigg\lbrace \frac{1}{\lambda(a_{jt}) - \log(\beta)} \Bigg( \pi_{j}^{\boldsymbol P_{-j}} (a_{jt}, \mathbf{s}&lt;em&gt;{t} ; \theta^\pi ) + \lambda(a&lt;/em&gt;{jt}) \mathbb E_{\boldsymbol s_{t+1}} \Big[  V_{j}^{\boldsymbol P_{-j}} \left(\mathbf{s}&lt;em&gt;{t+1}\right) \ \Big| \ a&lt;/em&gt;{jt}, \boldsymbol s_{t} ; \theta^f \Big] \Bigg) \Bigg\rbrace
$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\lambda(a_{jt}) = \delta + \frac{\alpha a_{jt}}{1 + \alpha a_{jt}}$
is the hazard rate for firm $j$ that &lt;strong&gt;something happens&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;i.e. either an increase in quality, with probability
$\frac{\alpha a_{jt}}{1 + \alpha a_{jt}}$&lt;/li&gt;
&lt;li&gt;… or a decrease in quality with probability $\delta$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Computational gain&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Now the expectation over future states
$\mathbb E_{\boldsymbol s_{t+1}}$ is over $2J$ points instead of
$3^J$
&lt;ul&gt;
&lt;li&gt;3 because quality can go up, down or stay the same&lt;/li&gt;
&lt;li&gt;2 because in continuous time we don’t care if the state does not
change (investment fails)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;comparison&#34;&gt;Comparison&lt;/h3&gt;
&lt;p&gt;Which method is &lt;strong&gt;best&lt;/strong&gt;?&lt;/p&gt;
&lt;p&gt;I compare them in Courthoud (&lt;a href=&#34;#ref-courthoud2020approximation&#34;&gt;2020&lt;/a&gt;)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Fastest: Weintraub, Benkard, and Van Roy
(&lt;a href=&#34;#ref-weintraub2008markov&#34;&gt;2008&lt;/a&gt;)
&lt;ul&gt;
&lt;li&gt;Effectively transforms the game into single-agent dynamics&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Best trade-off: Doraszelski and Judd
(&lt;a href=&#34;#ref-doraszelski2019dynamic&#34;&gt;2019&lt;/a&gt;)
&lt;ul&gt;
&lt;li&gt;Simple, practical and also helps in terms of equilibrium
multeplicity&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Also in Courthoud (&lt;a href=&#34;#ref-courthoud2020approximation&#34;&gt;2020&lt;/a&gt;): games
with random order
&lt;ul&gt;
&lt;li&gt;Better approximation than Doraszelski and Judd
(&lt;a href=&#34;#ref-doraszelski2019dynamic&#34;&gt;2019&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;And similar similar time&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;applications&#34;&gt;Applications&lt;/h3&gt;
&lt;p&gt;Some applications of these methods include&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Approximation methods
&lt;ul&gt;
&lt;li&gt;Sweeting (&lt;a href=&#34;#ref-sweeting2013dynamic&#34;&gt;2013&lt;/a&gt;): product
repositioning among radio stations&lt;/li&gt;
&lt;li&gt;Barwick and Pathak (&lt;a href=&#34;#ref-barwick2015costs&#34;&gt;2015&lt;/a&gt;): entry and
exit in the real estate brokerage industry&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Oblivious equilibrium
&lt;ul&gt;
&lt;li&gt;Xu and Chen (&lt;a href=&#34;#ref-xu2020structural&#34;&gt;2020&lt;/a&gt;): R&amp;amp;D investment in
the Korean electric motor industry&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Moment based equilibrium
&lt;ul&gt;
&lt;li&gt;Jeon (&lt;a href=&#34;#ref-jeon2020learning&#34;&gt;2020&lt;/a&gt;): demand learning in the
container shipping industry&lt;/li&gt;
&lt;li&gt;Caoui (&lt;a href=&#34;#ref-caoui2019estimating&#34;&gt;2019&lt;/a&gt;): technology adoption
with network effects in the movie industry&lt;/li&gt;
&lt;li&gt;Vreugdenhil (&lt;a href=&#34;#ref-vreugdenhil2020booms&#34;&gt;2020&lt;/a&gt;): search and
matching in the oil drilling industry&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Games in continuous time
&lt;ul&gt;
&lt;li&gt;Arcidiacono et al. (&lt;a href=&#34;#ref-arcidiacono2016estimation&#34;&gt;2016&lt;/a&gt;):
entry, exit and scale decisions in retail competition&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Games with random moves
&lt;ul&gt;
&lt;li&gt;Igami (&lt;a href=&#34;#ref-igami2017estimating&#34;&gt;2017&lt;/a&gt;): innovation, entry,
exit in the hard drive industry&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;from-io-to-ai&#34;&gt;From IO to AI&lt;/h2&gt;
&lt;h3 id=&#34;bridging-two-literatures&#34;&gt;Bridging two Literatures&lt;/h3&gt;
&lt;p&gt;There is one method to approximate the equilibrium in dynamic games that
is a bit different from the others: Pakes and McGuire
(&lt;a href=&#34;#ref-pakes2001stochastic&#34;&gt;2001&lt;/a&gt;)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Idea&lt;/strong&gt;: approximate the value function by Monte-Carlo simulation&lt;/li&gt;
&lt;li&gt;Firms start with a guess for the alternative-specific value function&lt;/li&gt;
&lt;li&gt;Act according to it&lt;/li&gt;
&lt;li&gt;Observe realized payoffs and state transitions&lt;/li&gt;
&lt;li&gt;And update the alternative-specific value function according to the
realized outcomes&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Experience-Based Equilibrium&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Defined in Fershtman and Pakes (&lt;a href=&#34;#ref-fershtman2012dynamic&#34;&gt;2012&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Def&lt;/strong&gt;: &lt;em&gt;policy is optimal given beliefs of state transitions and
observed transitions are consistent with the beliefs&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Note&lt;/strong&gt;: definition silent on off-equilibrium path beliefs&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;pakes-and-mcguire-2001&#34;&gt;Pakes and McGuire (2001)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Players start with &lt;strong&gt;alternative-specific value function&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;yes, the ASV from Rust (&lt;a href=&#34;#ref-rust1994structural&#34;&gt;1994&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;$\bar V_{j,a}^{(0)} (\boldsymbol s ; \theta)$: initial value of
player $j$ for action $a$ in state $\boldsymbol s$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Until convergence, do:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Compute optimal action, given
$\bar V_{j, a}^{(t)} (\boldsymbol s ; \theta)$ $$
a^* = \arg \max_a \bar V_{j, a}^{(t)} (\boldsymbol s ; \theta)
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Observe the realized payoff
$\pi_{j, a^&lt;em&gt;}(\boldsymbol s ; \theta)$ and the realized next
state $\boldsymbol {s&#39;}(\boldsymbol s, a^&lt;/em&gt;; \theta)$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Update the alternative-specific value function of the chosen
action $k^&lt;em&gt;$ $$
\bar V_{j, a^&lt;/em&gt;}^{(t+1)} (\boldsymbol s ; \theta) = (1-\alpha_{\boldsymbol s, t}) \bar V_{j, a^&lt;em&gt;}^{(t)} (\boldsymbol s ; \theta) + \alpha_{\boldsymbol s, t} \Big[\pi_{j, a^&lt;/em&gt;}(\boldsymbol s ; \theta) + \arg \max_a \bar V_{j, a}^{(t)} (\boldsymbol s&#39; ; \theta) \Big]
$$ where&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\alpha_{\boldsymbol s, t} = \frac{1}{\text{number of times state } \boldsymbol s \text{ has been visited}}$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;comments&#34;&gt;Comments&lt;/h3&gt;
&lt;p&gt;Where is the &lt;strong&gt;strategic interaction&lt;/strong&gt;?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Firm always take &lt;em&gt;“best action so far”&lt;/em&gt; in each state
&lt;ul&gt;
&lt;li&gt;Start to take a new action only when the previous best has
performed badly for many periods&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Remindful of literature of evolutionary game theory&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Importance of &lt;strong&gt;starting values&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Imagine, all payoffs are positive but value initialized to zero&lt;/li&gt;
&lt;li&gt;First action in each state $\to$ only action ever taken in that
state&lt;/li&gt;
&lt;li&gt;Loophole.
&lt;ul&gt;
&lt;li&gt;Why? Firms always take $\arg \max_a \bar V_a$ and never
&lt;em&gt;explore&lt;/em&gt; the alternatives&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Convergence&lt;/strong&gt; by desing&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;As $\lim_{t \to \infty} \alpha_{\boldsymbol s, t} = 1$&lt;/li&gt;
&lt;li&gt;Firms stop updating the value by design&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;q-learning&#34;&gt;Q-Learning&lt;/h3&gt;
&lt;p&gt;Computer Science reinforcement learning literature (AI): &lt;strong&gt;Q-learning&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Differences&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\bar V_a( \boldsymbol s)$ called $Q_a(\boldsymbol s)$, hence the
name&lt;/li&gt;
&lt;li&gt;Firms don’t always take the optimal action
&lt;ul&gt;
&lt;li&gt;At the beginning of the algorithm: &lt;strong&gt;exploration&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;Firms take actions at random&lt;/li&gt;
&lt;li&gt;Just to explore what happens taking different actions&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Gradually shift towards &lt;strong&gt;exploitation&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;I.e. take the optimal action, given
$\bar V^{(t)}( \boldsymbol s)$ at iteration $t$&lt;/li&gt;
&lt;li&gt;I.e. shift towards Pakes and McGuire
(&lt;a href=&#34;#ref-pakes2001stochastic&#34;&gt;2001&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;applications-1&#34;&gt;Applications&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Doraszelski, Lewis, and Pakes (&lt;a href=&#34;#ref-doraszelski2018just&#34;&gt;2018&lt;/a&gt;)
&lt;ul&gt;
&lt;li&gt;Firm do actually learn by trial and error&lt;/li&gt;
&lt;li&gt;Setting: demand learning in the UK frequency response market
(electricity)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Asker et al. (&lt;a href=&#34;#ref-asker2020computational&#34;&gt;2020&lt;/a&gt;)
&lt;ul&gt;
&lt;li&gt;Uses Pakes and McGuire (&lt;a href=&#34;#ref-pakes2001stochastic&#34;&gt;2001&lt;/a&gt;) for
estimation&lt;/li&gt;
&lt;li&gt;Setting: dynamic timber auctions with information sharing&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Calvano et al. (&lt;a href=&#34;#ref-calvano2020artificial&#34;&gt;2020&lt;/a&gt;)
&lt;ul&gt;
&lt;li&gt;Study Q-learning pricing algorithms&lt;/li&gt;
&lt;li&gt;In repeated price competition with differentiated products&lt;/li&gt;
&lt;li&gt;(Computational) lab experiment: what do these algorithms
converge to?&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Finding&lt;/strong&gt;: algorithms learn reward-punishment collusive
strategies&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;appendix&#34;&gt;Appendix&lt;/h2&gt;
&lt;h3 id=&#34;references-references&#34;&gt;References [references]&lt;/h3&gt;
&lt;div id=&#34;refs&#34; class=&#34;references csl-bib-body hanging-indent&#34;
markdown=&#34;1&#34;&gt;
&lt;div id=&#34;ref-abbring2010last&#34; class=&#34;csl-entry&#34; markdown=&#34;1&#34;&gt;
&lt;p&gt;Abbring, Jaap H, and Jeffrey R Campbell. 2010. “Last-in First-Out
Oligopoly Dynamics.” &lt;em&gt;Econometrica&lt;/em&gt; 78 (5): 1491–1527.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-aguirregabiria2021dynamic&#34; class=&#34;csl-entry&#34; markdown=&#34;1&#34;&gt;
&lt;p&gt;Aguirregabiria, Victor, Allan Collard-Wexler, and Stephen P Ryan. 2021.
“Dynamic Games in Empirical Industrial Organization.” National Bureau of
Economic Research.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-aguirregabiria2007sequential&#34; class=&#34;csl-entry&#34;
markdown=&#34;1&#34;&gt;
&lt;p&gt;Aguirregabiria, Victor, and Pedro Mira. 2007. “Sequential Estimation of
Dynamic Discrete Games.” &lt;em&gt;Econometrica&lt;/em&gt; 75 (1): 1–53.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-arcidiacono2016estimation&#34; class=&#34;csl-entry&#34; markdown=&#34;1&#34;&gt;
&lt;p&gt;Arcidiacono, Peter, Patrick Bayer, Jason R Blevins, and Paul B
Ellickson. 2016. “Estimation of Dynamic Discrete Choice Models in
Continuous Time with an Application to Retail Competition.” &lt;em&gt;The Review
of Economic Studies&lt;/em&gt; 83 (3): 889–931.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-arcidiacono2011conditional&#34; class=&#34;csl-entry&#34; markdown=&#34;1&#34;&gt;
&lt;p&gt;Arcidiacono, Peter, and Robert A Miller. 2011. “Conditional Choice
Probability Estimation of Dynamic Discrete Choice Models with Unobserved
Heterogeneity.” &lt;em&gt;Econometrica&lt;/em&gt; 79 (6): 1823–67.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-asker2020computational&#34; class=&#34;csl-entry&#34; markdown=&#34;1&#34;&gt;
&lt;p&gt;Asker, John, Chaim Fershtman, Jihye Jeon, and Ariel Pakes. 2020. “A
Computational Framework for Analyzing Dynamic Auctions: The Market
Impact of Information Sharing.” &lt;em&gt;The RAND Journal of Economics&lt;/em&gt; 51 (3):
805–39.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-bajari2007estimating&#34; class=&#34;csl-entry&#34; markdown=&#34;1&#34;&gt;
&lt;p&gt;Bajari, Patrick, C Lanier Benkard, and Jonathan Levin. 2007. “Estimating
Dynamic Models of Imperfect Competition.” &lt;em&gt;Econometrica&lt;/em&gt; 75 (5):
1331–70.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-barwick2015costs&#34; class=&#34;csl-entry&#34; markdown=&#34;1&#34;&gt;
&lt;p&gt;Barwick, Panle Jia, and Parag A Pathak. 2015. “The Costs of Free Entry:
An Empirical Study of Real Estate Agents in Greater Boston.” &lt;em&gt;The RAND
Journal of Economics&lt;/em&gt; 46 (1): 103–45.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-berry2021empirical&#34; class=&#34;csl-entry&#34; markdown=&#34;1&#34;&gt;
&lt;p&gt;Berry, Steven T, and Giovanni Compiani. 2021. “Empirical Models of
Industry Dynamics with Endogenous Market Structure.” &lt;em&gt;Annual Review of
Economics&lt;/em&gt; 13.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-besanko2010learning&#34; class=&#34;csl-entry&#34; markdown=&#34;1&#34;&gt;
&lt;p&gt;Besanko, David, Ulrich Doraszelski, Yaroslav Kryukov, and Mark
Satterthwaite. 2010. “Learning-by-Doing, Organizational Forgetting, and
Industry Dynamics.” &lt;em&gt;Econometrica&lt;/em&gt; 78 (2): 453–508.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-borkovsky2010user&#34; class=&#34;csl-entry&#34; markdown=&#34;1&#34;&gt;
&lt;p&gt;Borkovsky, Ron N, Ulrich Doraszelski, and Yaroslav Kryukov. 2010. “A
User’s Guide to Solving Dynamic Stochastic Games Using the Homotopy
Method.” &lt;em&gt;Operations Research&lt;/em&gt; 58 (4-part-2): 1116–32.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-calvano2020artificial&#34; class=&#34;csl-entry&#34; markdown=&#34;1&#34;&gt;
&lt;p&gt;Calvano, Emilio, Giacomo Calzolari, Vincenzo Denicolo, and Sergio
Pastorello. 2020. “Artificial Intelligence, Algorithmic Pricing, and
Collusion.” &lt;em&gt;American Economic Review&lt;/em&gt; 110 (10): 3267–97.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-caoui2019estimating&#34; class=&#34;csl-entry&#34; markdown=&#34;1&#34;&gt;
&lt;p&gt;Caoui, El Hadi. 2019. “Estimating the Costs of Standardization: Evidence
from the Movie Industry.” &lt;em&gt;R&amp;amp;R, Review of Economic Studies&lt;/em&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-collard2013demand&#34; class=&#34;csl-entry&#34; markdown=&#34;1&#34;&gt;
&lt;p&gt;Collard-Wexler, Allan. 2013. “Demand Fluctuations in the Ready-Mix
Concrete Industry.” &lt;em&gt;Econometrica&lt;/em&gt; 81 (3): 1003–37.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-courthoud2020approximation&#34; class=&#34;csl-entry&#34; markdown=&#34;1&#34;&gt;
&lt;p&gt;Courthoud, Matteo. 2020. “Approximation Methods for Large Dynamic
Stochastic Games.” &lt;em&gt;Working Paper&lt;/em&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-doraszelski2003r&#34; class=&#34;csl-entry&#34; markdown=&#34;1&#34;&gt;
&lt;p&gt;Doraszelski, Ulrich. 2003. “An r&amp;amp;d Race with Knowledge Accumulation.”
&lt;em&gt;Rand Journal of Economics&lt;/em&gt;, 20–42.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-doraszelski2012avoiding&#34; class=&#34;csl-entry&#34; markdown=&#34;1&#34;&gt;
&lt;p&gt;Doraszelski, Ulrich, and Kenneth L Judd. 2012. “Avoiding the Curse of
Dimensionality in Dynamic Stochastic Games.” &lt;em&gt;Quantitative Economics&lt;/em&gt; 3
(1): 53–93.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-doraszelski2019dynamic&#34; class=&#34;csl-entry&#34; markdown=&#34;1&#34;&gt;
&lt;p&gt;———. 2019. “Dynamic Stochastic Games with Random Moves.” &lt;em&gt;Quantitative
Marketing and Economics&lt;/em&gt; 17 (1): 59–79.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-doraszelski2018just&#34; class=&#34;csl-entry&#34; markdown=&#34;1&#34;&gt;
&lt;p&gt;Doraszelski, Ulrich, Gregory Lewis, and Ariel Pakes. 2018. “Just
Starting Out: Learning and Equilibrium in a New Market.” &lt;em&gt;American
Economic Review&lt;/em&gt; 108 (3): 565–615.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-doraszelski2010computable&#34; class=&#34;csl-entry&#34; markdown=&#34;1&#34;&gt;
&lt;p&gt;Doraszelski, Ulrich, and Mark Satterthwaite. 2010. “Computable
Markov-Perfect Industry Dynamics.” &lt;em&gt;The RAND Journal of Economics&lt;/em&gt; 41
(2): 215–43.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-egesdal2015estimating&#34; class=&#34;csl-entry&#34; markdown=&#34;1&#34;&gt;
&lt;p&gt;Egesdal, Michael, Zhenyu Lai, and Che-Lin Su. 2015. “Estimating Dynamic
Discrete-Choice Games of Incomplete Information.” &lt;em&gt;Quantitative
Economics&lt;/em&gt; 6 (3): 567–97.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-eibelshauser2019markov&#34; class=&#34;csl-entry&#34; markdown=&#34;1&#34;&gt;
&lt;p&gt;Eibelshäuser, Steffen, and David Poensgen. 2019. “Markov Quantal
Response Equilibrium and a Homotopy Method for Computing and Selecting
Markov Perfect Equilibria of Dynamic Stochastic Games.” &lt;em&gt;Working Paper&lt;/em&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-ericson1995markov&#34; class=&#34;csl-entry&#34; markdown=&#34;1&#34;&gt;
&lt;p&gt;Ericson, Richard, and Ariel Pakes. 1995. “Markov-Perfect Industry
Dynamics: A Framework for Empirical Work.” &lt;em&gt;The Review of Economic
Studies&lt;/em&gt; 62 (1): 53–82.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-esteban2007durable&#34; class=&#34;csl-entry&#34; markdown=&#34;1&#34;&gt;
&lt;p&gt;Esteban, Susanna, and Matthew Shum. 2007. “Durable-Goods Oligopoly with
Secondary Markets: The Case of Automobiles.” &lt;em&gt;The RAND Journal of
Economics&lt;/em&gt; 38 (2): 332–54.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-farias2012approximate&#34; class=&#34;csl-entry&#34; markdown=&#34;1&#34;&gt;
&lt;p&gt;Farias, Vivek, Denis Saure, and Gabriel Y Weintraub. 2012. “An
Approximate Dynamic Programming Approach to Solving Dynamic Oligopoly
Models.” &lt;em&gt;The RAND Journal of Economics&lt;/em&gt; 43 (2): 253–82.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-fershtman2012dynamic&#34; class=&#34;csl-entry&#34; markdown=&#34;1&#34;&gt;
&lt;p&gt;Fershtman, Chaim, and Ariel Pakes. 2012. “Dynamic Games with Asymmetric
Information: A Framework for Empirical Work.” &lt;em&gt;The Quarterly Journal of
Economics&lt;/em&gt; 127 (4): 1611–61.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-goettler2011does&#34; class=&#34;csl-entry&#34; markdown=&#34;1&#34;&gt;
&lt;p&gt;Goettler, Ronald L, and Brett R Gordon. 2011. “Does AMD Spur Intel to
Innovate More?” &lt;em&gt;Journal of Political Economy&lt;/em&gt; 119 (6): 1141–1200.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-hotz1993conditional&#34; class=&#34;csl-entry&#34; markdown=&#34;1&#34;&gt;
&lt;p&gt;Hotz, V Joseph, and Robert A Miller. 1993. “Conditional Choice
Probabilities and the Estimation of Dynamic Models.” &lt;em&gt;The Review of
Economic Studies&lt;/em&gt; 60 (3): 497–529.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-huang2014dynamic&#34; class=&#34;csl-entry&#34; markdown=&#34;1&#34;&gt;
&lt;p&gt;Huang, Ling, and Martin D Smith. 2014. “The Dynamic Efficiency Costs of
Common-Pool Resource Exploitation.” &lt;em&gt;American Economic Review&lt;/em&gt; 104 (12):
4071–4103.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-ifrach2017framework&#34; class=&#34;csl-entry&#34; markdown=&#34;1&#34;&gt;
&lt;p&gt;Ifrach, Bar, and Gabriel Y Weintraub. 2017. “A Framework for Dynamic
Oligopoly in Concentrated Industries.” &lt;em&gt;The Review of Economic Studies&lt;/em&gt;
84 (3): 1106–50.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-igami2017estimating&#34; class=&#34;csl-entry&#34; markdown=&#34;1&#34;&gt;
&lt;p&gt;Igami, Mitsuru. 2017. “Estimating the Innovator’s Dilemma: Structural
Analysis of Creative Destruction in the Hard Disk Drive Industry,
1981–1998.” &lt;em&gt;Journal of Political Economy&lt;/em&gt; 125 (3): 798–847.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-iskhakov2016recursive&#34; class=&#34;csl-entry&#34; markdown=&#34;1&#34;&gt;
&lt;p&gt;Iskhakov, Fedor, John Rust, and Bertel Schjerning. 2016. “Recursive
Lexicographical Search: Finding All Markov Perfect Equilibria of Finite
State Directional Dynamic Games.” &lt;em&gt;The Review of Economic Studies&lt;/em&gt; 83
(2): 658–703.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-jeon2020learning&#34; class=&#34;csl-entry&#34; markdown=&#34;1&#34;&gt;
&lt;p&gt;Jeon, Jihye. 2020. “Learning and Investment Under Demand Uncertainty in
Container Shipping.” &lt;em&gt;The RAND Journal of Economics&lt;/em&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-kasahara2009nonparametric&#34; class=&#34;csl-entry&#34; markdown=&#34;1&#34;&gt;
&lt;p&gt;Kasahara, Hiroyuki, and Katsumi Shimotsu. 2009. “Nonparametric
Identification of Finite Mixture Models of Dynamic Discrete Choices.”
&lt;em&gt;Econometrica&lt;/em&gt; 77 (1): 135–75.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-maskin1988theory&#34; class=&#34;csl-entry&#34; markdown=&#34;1&#34;&gt;
&lt;p&gt;Maskin, Eric, and Jean Tirole. 1988. “A Theory of Dynamic Oligopoly, II:
Price Competition, Kinked Demand Curves, and Edgeworth Cycles.”
&lt;em&gt;Econometrica: Journal of the Econometric Society&lt;/em&gt;, 571–99.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-pakes1994computing&#34; class=&#34;csl-entry&#34; markdown=&#34;1&#34;&gt;
&lt;p&gt;Pakes, Ariel, and Paul McGuire. 1994. “Computing Markov-Perfect Nash
Equilibria: Numerical Implications of a Dynamic Differentiated Product
Model.” &lt;em&gt;RAND Journal of Economics&lt;/em&gt; 25 (4): 555–89.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-pakes2001stochastic&#34; class=&#34;csl-entry&#34; markdown=&#34;1&#34;&gt;
&lt;p&gt;———. 2001. “Stochastic Algorithms, Symmetric Markov Perfect Equilibrium,
and the ‘Curse’of Dimensionality.” &lt;em&gt;Econometrica&lt;/em&gt; 69 (5): 1261–81.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-pakes2007simple&#34; class=&#34;csl-entry&#34; markdown=&#34;1&#34;&gt;
&lt;p&gt;Pakes, Ariel, Michael Ostrovsky, and Steven Berry. 2007. “Simple
Estimators for the Parameters of Discrete Dynamic Games (with Entry/Exit
Examples).” &lt;em&gt;The RAND Journal of Economics&lt;/em&gt; 38 (2): 373–99.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-pesendorfer2008asymptotic&#34; class=&#34;csl-entry&#34; markdown=&#34;1&#34;&gt;
&lt;p&gt;Pesendorfer, Martin, and Philipp Schmidt-Dengler. 2008. “Asymptotic
Least Squares Estimators for Dynamic Games.” &lt;em&gt;The Review of Economic
Studies&lt;/em&gt; 75 (3): 901–28.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-pesendorfer2010sequential&#34; class=&#34;csl-entry&#34; markdown=&#34;1&#34;&gt;
&lt;p&gt;———. 2010. “Sequential Estimation of Dynamic Discrete Games: A Comment.”
&lt;em&gt;Econometrica&lt;/em&gt; 78 (2): 833–42.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-rust1994structural&#34; class=&#34;csl-entry&#34; markdown=&#34;1&#34;&gt;
&lt;p&gt;Rust, John. 1994. “Structural Estimation of Markov Decision Processes.”
&lt;em&gt;Handbook of Econometrics&lt;/em&gt; 4: 3081–3143.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-ryan2012costs&#34; class=&#34;csl-entry&#34; markdown=&#34;1&#34;&gt;
&lt;p&gt;Ryan, Stephen P. 2012. “The Costs of Environmental Regulation in a
Concentrated Industry.” &lt;em&gt;Econometrica&lt;/em&gt; 80 (3): 1019–61.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-su2012constrained&#34; class=&#34;csl-entry&#34; markdown=&#34;1&#34;&gt;
&lt;p&gt;Su, Che-Lin, and Kenneth L Judd. 2012. “Constrained Optimization
Approaches to Estimation of Structural Models.” &lt;em&gt;Econometrica&lt;/em&gt; 80 (5):
2213–30.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-sweeting2013dynamic&#34; class=&#34;csl-entry&#34; markdown=&#34;1&#34;&gt;
&lt;p&gt;Sweeting, Andrew. 2013. “Dynamic Product Positioning in Differentiated
Product Markets: The Effect of Fees for Musical Performance Rights on
the Commercial Radio Industry.” &lt;em&gt;Econometrica&lt;/em&gt; 81 (5): 1763–803.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-vreugdenhil2020booms&#34; class=&#34;csl-entry&#34; markdown=&#34;1&#34;&gt;
&lt;p&gt;Vreugdenhil, Nicholas. 2020. “Booms, Busts, and Mismatch in Capital
Markets: Evidence from the Offshore Oil and Gas Industry.” &lt;em&gt;R&amp;amp;R at
Journal of Political Economy&lt;/em&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-weintraub2008markov&#34; class=&#34;csl-entry&#34; markdown=&#34;1&#34;&gt;
&lt;p&gt;Weintraub, Gabriel Y, C Lanier Benkard, and Benjamin Van Roy. 2008.
“Markov Perfect Industry Dynamics with Many Firms.” &lt;em&gt;Econometrica&lt;/em&gt; 76
(6): 1375–1411.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-xu2020structural&#34; class=&#34;csl-entry&#34; markdown=&#34;1&#34;&gt;
&lt;p&gt;Xu, Daniel Yi, and Yanyou Chen. 2020. “A Structural Empirical Model of
r&amp;amp;d, Firm Heterogeneity, and Industry Evolution.” &lt;em&gt;Journal of Industrial
Economics.&lt;/em&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Non-Parametric Estimation</title>
      <link>https://matteocourthoud.github.io/course/metrics/08_nonparametric/</link>
      <pubDate>Fri, 29 Oct 2021 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/course/metrics/08_nonparametric/</guid>
      <description>&lt;h3 id=&#34;introduction&#34;&gt;Introduction&lt;/h3&gt;
&lt;p&gt;Non-parametric regression is a flexible estimation procedure for&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;regression functions $\mathbb E [y|x ] = g (x)$ and&lt;/li&gt;
&lt;li&gt;density functions $f(x)$.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;You want to let your data to tell you how flexible you can afford to be
in terms of estimation procedures. Non-parametric regression is
naturally introduced in terms of fitting a curve.&lt;/p&gt;
&lt;p&gt;Consider the problem of estimating the Conditional Expectation Function,
defined as $\mathbb E [y_i |x_i ] = g(x_i)$ given data
$D = (x_i, y_i)_{i=1}^n$ under minimal assumption of $g(\cdot)$,
e.g. smoothness. There are two main methods:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Local methods: Kernel-based estimation&lt;/li&gt;
&lt;li&gt;Global methods: Series-based estimation&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Another way of looking at non-parametrics is to do estimation/inference
without specifying functional forms. With no assumptions, informative
inference is impossible. Non parametrics tries to work with functional
restrictions—continuity, differentiability, etc.—rather than
pre-specifying functional form.&lt;/p&gt;
&lt;h3 id=&#34;discrete-x---cell-estimator&#34;&gt;Discrete x - Cell Estimator&lt;/h3&gt;
&lt;p&gt;Suppose that $x$ can take $R$ distinct values, e.g. gender $R=2$, years
of schooling $R=20$, gender $\times$ years of schooling
$R = 2 \times 20$.&lt;/p&gt;
&lt;p&gt;A simple way for estimating $\mathbb E \left[ y |x \right] = g(x)$ is to
split the sample to include observations with $x_i = x$ and calculate
the sample mean of $\bar{y}$ for these observations. Note that this
requires no assumptions about how $\mathbb E [y_i |x_i]$ varies with $x$
since we fit a different value for each value $x$. $$
\hat{g}(x) = \frac{1}{| i: x_i = x |} \sum_{i : x_i = x} y_i
$$&lt;/p&gt;
&lt;p&gt;Issues:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Curse of dimensionality&lt;/strong&gt;: if $R$ is big compared to $n$, there
will be only a small number of observations per $x$ values. If $x_i$
is continuous, $R=n$ with probability 1. Solution: we can borrow
information about $g_0(x)$ using neighboring observations of $x$.&lt;/li&gt;
&lt;li&gt;Averaging for each separate $x_r$ value is only feasible in cases
where $x_i$ is coarsely discrete.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;local-non-parametric-estimation&#34;&gt;Local Non-Parametric Estimation&lt;/h2&gt;
&lt;h3 id=&#34;kernels&#34;&gt;Kernels&lt;/h3&gt;
&lt;p&gt;Suppose we believe that $\mathbb E [y_i |x_i]$ is a smooth function of
$x_i$ – e.g. continuous, differentiable, etc. Then it should not change
too much across values of $x$ that are close to each other: we can
estimate the conditional expectation at $x = \bar{x}$ by averaging $y$’s
over the values of $x$ that are “close”” to $\bar{x}$. This procedure
relies on two (three) arbitrary choices:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Choice of the &lt;strong&gt;kernel function&lt;/strong&gt; $K (\cdot)$; it is used to weight
“far out”” observations, such that
&lt;ul&gt;
&lt;li&gt;$K: \mathbb R \to \mathbb R$&lt;/li&gt;
&lt;li&gt;$K$ is symmetric: $K(\bar{x} + x_i) = K(\bar{x} - x_i)$&lt;/li&gt;
&lt;li&gt;$\lim_{x_i \to \infty}K(x_i - \bar{x}) = 0$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Choice of the &lt;strong&gt;bandwidth&lt;/strong&gt; $h$: it measures the size of a
``small’’ window around $\bar{x}$,
e.g. $(\bar{x} - h, \bar{x} + h)$.&lt;/li&gt;
&lt;li&gt;Choice of the local estimation procedure. Examples are locally
constant, a.k.a. Nadaraya-Watson (&lt;strong&gt;NW&lt;/strong&gt;), and locally linear
(&lt;strong&gt;LL&lt;/strong&gt;).&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;Generally, the choice of $h$ is more important than $K(\cdot)$ in low
dimensional settings.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;optimal-h&#34;&gt;Optimal h&lt;/h3&gt;
&lt;p&gt;We need to define what is an “optimal” $h$, depending on the smoothness
level of $g_0$, typically unknown. The choice of $h$ relates to the
bias-variance trade-off:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;large $h$: small variance, higher bias;&lt;/li&gt;
&lt;li&gt;small $h$: high variance, smaller bias.&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;Note that $K_h (\cdot) = K (\cdot / h)$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;locally-constant-estimator&#34;&gt;Locally Constant Estimator&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Nadaraya-Watson&lt;/strong&gt; estimator, or locally constant estimator. It
assumes the CEF locally takes the form $g(x) = \beta_0(x)$. The
local parameter is estimated as: $$
\hat{\beta}&lt;em&gt;0 (\bar{x}) = \arg\min&lt;/em&gt;{\beta_0}  \quad  \mathbb E_n \Big[ K_h (x_i - \bar{x}) \cdot  \big(y_i - \beta_0 \big)^2 \Big]
$$&lt;/li&gt;
&lt;/ul&gt;
&lt;img src=&#34;../img/Fig_521.png&#34; style=&#34;width:50.0%&#34; /&gt;
&lt;h3 id=&#34;cef&#34;&gt;CEF&lt;/h3&gt;
&lt;p&gt;The Nadaraya-Watson estimate of the CEF takes the form: $$
\mathbb E_n \left[ y | x = \bar{x}\right] = \hat{g}(\bar{x}) = \frac{\sum_{i=1}^n y_i K_h (x_i - \bar{x})}{\sum_{i=1}^n K_h (x_i - \bar{x})}
$$&lt;/p&gt;
&lt;h3 id=&#34;locally-linear-estimator&#34;&gt;Locally Linear Estimator&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Local Linear&lt;/strong&gt; estimator. It assumes the CEF locally takes the
form $g(x) = \beta_0(x) + \beta_1(x) x$. The local parameters are
estimated as:&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$
\left( \hat{\beta}_0 (\bar{x}), \hat{\beta}&lt;em&gt;1 (\bar{x}) \right) = \arg\min&lt;/em&gt;{\beta_0, \beta_1}  \quad   \mathbb E_n \Big[ K_h (x_i - \bar{x}) \cdot  \big(y_i - \beta_0 - (x_i - \bar{x}) \beta_1 \big)^2 \Big]
$$&lt;/p&gt;
&lt;img src=&#34;../img/Fig_522.png&#34; style=&#34;width:50.0%&#34; /&gt;
&lt;h3 id=&#34;cef-1&#34;&gt;CEF&lt;/h3&gt;
&lt;p&gt;In this case, we do LS estimate with $i$’s contribution of residual
weighted by the kernel $K_h (x_i - \bar{x})$. The final estimate at
$\bar{x}$ is given by: $$
\hat{g} (\bar{x}) = \hat{\beta}_0 (\bar{x}) + (\bar{x} - \bar{x}) \hat{\beta}_1 (\bar{x}) = \hat{\beta}_0 (\bar{x})
$$ since we have centered the $x_s$ at $\bar{x}$ in the kernel. - It is
possible to add linearly higher order polynomials, e.g. do locally
quadratic least squares using loss function:&lt;/p&gt;
&lt;p&gt;$$
\mathbb E_n \left[ K_h (x_i - \bar{x}) \big(y_i - \beta_0 - (x_i - \bar{x}) \beta_1 - (x_i - \bar{x})^2 \beta_2 \big)^2 \right]
$$&lt;/p&gt;
&lt;h3 id=&#34;uniform-kernel&#34;&gt;Uniform Kernel&lt;/h3&gt;
&lt;p&gt;LS restricted to sample $i$ such that $x_i$ within $h$ of $\bar{x}$. $$
\begin{aligned}
&amp;amp; K (\cdot) = \mathbb I\lbrace \cdot \in [-1, 1] \rbrace  \newline
&amp;amp; K_h (\cdot) = \mathbb I\lbrace \cdot/h \in [-1, 1] \rbrace = \mathbb I\lbrace \cdot \in [-h, h] \rbrace  \newline
&amp;amp; K_h (x_i - \bar{x}) = \mathbb I\lbrace x_i - \bar{x} \in [-h, h] \rbrace  = \mathbb I\lbrace x_i \in [\bar{x}-h, \bar{x} + h] \rbrace
\end{aligned}
$$ Employed together with the locally linear estimator, the estimation
procedure reduces to **local least squares}. The loss function is: $$
\mathbb E_n \Big[ K_n (x_i - \bar{x}) \big(y_i -\beta_0 - \beta_1 (x_i - \bar{x}) \big)^2 \Big] = \frac{1}{n} \sum_{i: x_i \in [\bar{x}-h, \bar{x} +h ]}  \big(y_i -\beta_0 - \beta_1 (x_i - \bar{x}) \big)^2
$$&lt;/p&gt;
&lt;p&gt;The more local is the estimation, the more appropriate the linear
regression: if $g_0$ is smooth,
$g_0(\bar{x}) + g_0&#39;(\bar{x}) (x_i - \bar{x})$ is a better approximation
for $g_0 (x_i)$.&lt;/p&gt;
&lt;p&gt;However, the uniform density is not a good kernel choice as it produces
discontinuous CEF estimates. The following are two popular alternative
choices that produce continuous CEF estimates.&lt;/p&gt;
&lt;h3 id=&#34;other-kernels&#34;&gt;Other Kernels&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Epanechnikov kernel&lt;/strong&gt; $$
K_h(x_i - \bar{x}) = \frac { 3 } { 4 } \left( 1 - (x_i - \bar{x}) ^ { 2 } \right)  \mathbb I\lbrace x_i \in [\bar{x}-h, \bar{x} + h] \rbrace
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Normal or Gaussian kernel&lt;/strong&gt; $$
K_\phi (x_i - \bar{x})  = \frac { 1 } { \sqrt { 2 \pi } } \exp \left( - \frac { (x_i - \bar{x}) ^ { 2 } } { 2 } \right)
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;K-Nearest Neighbors (KNN)&lt;/strong&gt;: choose bandwidth so that there is a
fixed number of observations in each kernel. This kernel is
different from the others since it takes a nonparamentric form.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;example&#34;&gt;Example&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;../img/Fig_523.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;choice-of-the-optimal-bandwidth&#34;&gt;Choice of the optimal bandwidth&lt;/h3&gt;
&lt;p&gt;Practical methods:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Eyeball Method.&lt;/strong&gt; (i) Choose a bandwidth (ii) Estimate the
regression function (iii) Look at the result: if it looks more
wiggly than you would like, increase the bandwidth: if it looks more
smooth than you would like, decrease the bandwidth. Con: It only
works for $\dim(x_i) = 1$ or $2$.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Rule of Thumb.&lt;/strong&gt; For example, Silverman’s rule of thumb:
$h = \left( \frac{4 \hat{\sigma}^5}{3n} \right)^{\frac{1}{5}}$. Con:
It requires too much knowledge about $g_0$ (i.e. normality) which
you don’t have.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Cross Validation.&lt;/strong&gt; Under some assumptions, CV will approximately
gives the MSE optimal bandwidth. The basic idea is to evaluate
quality of the bandwidth by looking at how well the resulting
estimator forecasts in the given sample.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Leave-one-out CV. For each $h &amp;gt; 0$ and each $i$, $\hat{g}&lt;em&gt;{-i} (x_i)$ is
the estimate of the conditional expectation at $x_i$ using bandwidth $h$
and all observations expect observation $i$. The CV bandwidth is defined
as $$
\hat{h} = \arg \min_h CV(h) = \arg \min_h \sum&lt;/em&gt;{i=1}^n  \Big( y_i -  \hat{g}_{-i} (x_i) \Big)^2
$$&lt;/p&gt;
&lt;h3 id=&#34;practical-tips&#34;&gt;Practical Tips&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Select a value for $h$.&lt;/li&gt;
&lt;li&gt;For each observation $i$, calculate $$
\hat{g}&lt;em&gt;{-i} (x_i) = \frac{\sum&lt;/em&gt;{j \ne i} y_j K_h (x_j - x_i) }{\sum_{i=1}^n K_h (x_j - x_i)}, \qquad e_{i,h}^2 = \left(y_i - \hat{g}_{-i} (x_i) \right)^2
$$&lt;/li&gt;
&lt;li&gt;Calculate $\text{CV}(h) = \sum_{i=1}^n e^2_{i,h}$.&lt;/li&gt;
&lt;li&gt;Repeat for each $h$ and choose the one that minimizes
$\text{CV}(h)$.&lt;/li&gt;
&lt;/ul&gt;
&lt;img src=&#34;../img/Fig_524.png&#34; style=&#34;width:50.0%&#34; /&gt;
&lt;h3 id=&#34;inference&#34;&gt;Inference&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt;: Consider data $\lbrace y_i, x_i \rbrace_{i=1}^n$, iid and
suppose that $y_i = g(x_i) + \varepsilon_i$ where
$\mathbb E[\varepsilon_i|x_i] = 0$. Assume that $x_i \in Interior(X)$
where $X \subseteq \mathbb R$, $g(x)$ and $f(x)$ are three times
continuously differentiable, and $f(x) &amp;gt; 0$ on $X$. $f(x)$ is the
probability density of $x \in X$ , and $g(x)$ is the function of
interest. Suppose that $K(\cdot)$ is a kernel function. Suppose
$n\to\infty$, $h\to0$ , $nh\to\infty$, and $nh^7\to0$. Then for any
fixed $x\in X$, $$
AMSE = \sqrt{nh} \Big( \hat{g}(x) - g(x) - h^2 B(x)\Big) \overset{d}{\to} N \left( 0, \frac{\kappa \sigma^2(x)}{f(x)}\right)
$$ for $\sigma^2(x) = Var(y_i|x_i = x)$, $\kappa = \int K^2(v)dv$, and
$B(x) = \frac{\kappa_2}{2} \frac{f&#39;(x)g&#39;(x) + f(x) g&#39;&#39;(x)}{f(x)}$ where
$\kappa_2 = \int v^2 K(v)dv$.&lt;/p&gt;
&lt;h3 id=&#34;remarks&#34;&gt;Remarks&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;If the function is smooth enough and the bandwidth small enough, you
can ignore the bias relative to sampling variation. To make this
plausible, use a smaller bandwidth than would be the “optimal”.&lt;/li&gt;
&lt;li&gt;All kernel regression estimators can be written as a weighted
average $$
\hat{g}(x) = \frac{1}{n} \sum_{i=1}^n w_i (x) y_i, \quad \text{ with } \quad w_i (x) = \frac{n K_h (x_i - x)}{\sum_{i=1}^n K_h (x_i - x)}
$$ Do inference as if you were estimating a mean $\mathbb E[z_i]$
with sample mean $\frac{1}{n} \sum_{i=1}^n z_i$ using
$z_i = w_i (x) y_i$.&lt;/li&gt;
&lt;li&gt;If you are doing inference at more than one value of $x$, do
inference as in the previous point, treating each value of $x$ as a
different sample mean and note that even with independent data,
these means will be correlated in general because there will
generally be some common observations in to each of the averages. If
you have a time series, make sure you account for correlation
between the observations going in the different averages even if
they don’t overlap.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Issue when doing inference: the estimation of the bandwidth from the
data is generally not accounted for in the distributional approximation
(when doing inference). In large-samples, this is unlikely to lead to
large changes, but uncertainty is understated in small samples.&lt;/p&gt;
&lt;h3 id=&#34;bias-variance-trade-off&#34;&gt;Bias-Variance Trade-off&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;For any estimator mean-square error MSE is decomposable into variance
and bias-squared: $$
\text{MSE} (\bar{x}, \hat{g}) = \mathbb E \left[ \left( \hat{g}(\bar{x}) - g_0 (\bar{x}) \right)^2 \right] = \mathbb E \Big[\underbrace{ \hat{g}(\bar{x}) - g_0 (\bar{x}) }_{\text{Bias}} \Big]^2 +  Var (\hat{g} (\bar{x})).
$$&lt;/p&gt;
&lt;h3 id=&#34;proof&#34;&gt;Proof&lt;/h3&gt;
&lt;p&gt;The theorem follows from the following corollary.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Corollary&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Let $A$ be a random variable and $\theta_0$ a fixed parameter. Then, $$
\mathbb E [ (A - \theta_0)^2] = Var (A) + \mathbb E [A-\theta_0]^2
$$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt; $$
\begin{aligned}
\mathbb E [ (A - \theta_0)^2] &amp;amp; = \mathbb E[A^2] - 2 \mathbb E [A \theta_0] + \mathbb E [\theta_0] \newline
&amp;amp;  = \mathbb E[A^2] \underbrace{-  \mathbb E[A]^2 + E[A]^2}_{\text{add and subtract}} - 2 \mathbb E [A \theta_0] + \mathbb E [\theta_0] \newline
&amp;amp;  = Var(A) + \mathbb E [A]^2 - 2 \theta_0 \mathbb E [A ] + \mathbb E [\theta_0] \newline
&amp;amp; = Var(A) + \mathbb E [A - \theta_0]^2
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;Note that $\mathbb E [ (A - \theta_0)^2] = \mathbb E [A - \theta_0]^2$.
$$\tag*{$\blacksquare$}$$&lt;/p&gt;
&lt;h3 id=&#34;criteria&#34;&gt;Criteria&lt;/h3&gt;
&lt;p&gt;Which criteria should we use with non-parametric estimators?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Mean squared error (MSE)&lt;/strong&gt;: $$
\text{MSE} (\bar{x}) (\hat{g}) = \mathbb E \left[ \left( \hat{g}(\bar{x}) - g_0 (\bar{x}) \right)^2 \right]
$$ &lt;strong&gt;NB!&lt;/strong&gt; This is the criterium we are going to use.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Integrated mean squared error (IMSE)&lt;/strong&gt;: $$
\text{IMSE} ( \hat{g} ) = \mathbb E \left[ \int | \hat{g} (x) - g_0 (x) |^2 \mathrm{d} F(x)  \right]
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Type I - Type II error.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;comments&#34;&gt;Comments&lt;/h3&gt;
&lt;p&gt;Hansen (2019): the theorem above implies that we can asymptotically
approximate the MSE as $$
\text{AMSE} = \Big( h^2 \sigma_k^2 B(x) \Big)^2 + \frac{\kappa \sigma^2(x)}{nh f(x)} \approx \text{const} \cdot \left( h^4 + \frac{1}{n h} \right)
$$&lt;/p&gt;
&lt;p&gt;Where&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$Var \propto \frac{1}{h n}$, where you can think of $n h$ as the
&lt;strong&gt;effective sample size&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;Bias $\propto h^2$, derived if $g_0$ is twice continuously
differentiable using Taylor expansion.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;trade-off&#34;&gt;Trade-Off&lt;/h3&gt;
&lt;p&gt;The asymptotic MSE is dominated by the larger of $h^4$ and
$\frac{1}{h n}$. Notice that the bias is increasing in $h$ and the
variance is decreasing in $h$ (more smoothing means more observations
are used for local estimation: this increases the bias but decreases
estimation variance). To select $h$ to minimize the asymptotic MSE,
these two components should balance each other: $$
\frac{1}{h n} \propto h^4 \quad \Rightarrow \quad  h \propto n^{-1/5}
$$&lt;/p&gt;
&lt;p&gt;This result means that the bandwidth should take the form
$h = c \cdot n^{-1/5}$. The optimal constant $c$ depends on the kernel
$k$ the bias function $B(x)$ and the marginal density $f_x(x)$. A common
misinterpretation is to set $h = n^{-1/5}$ which is equivalent to
setting $c = 1$ and is completely arbitrary. Instead, an empirical
bandwidth selection rule such as cross-validation should be used in
practice.&lt;/p&gt;
&lt;h2 id=&#34;global-non-parametric-estimation&#34;&gt;Global Non-Parametric Estimation&lt;/h2&gt;
&lt;h3 id=&#34;series&#34;&gt;Series&lt;/h3&gt;
&lt;p&gt;The goal is to try to globally approximate the CEF with a function
$g(x)$. Series methods are based on the &lt;strong&gt;Stone-Weierstrass theorem&lt;/strong&gt;: a
real-valued continuous function $g(x)$ defined in a compact set can be
approximated with polynomials for any degree of accuracy $$
g_0 (x) = p_1 (x) \beta _1 + \dots + p_K (x) \beta_K + r(x)
$$ where $p_1(x), \dots, p_K(x)$ are called ``a dictionary of
approximating series’’ and $r(x)$ is a remainder function. If
$p_1(x), \dots, p_K(x)$ are sufficiently rich, $r(x)$ will be small. If
$K \to \infty$, then $r \to 0$.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Example - Taylor series: if $g(x)$ is infinitely differentiable, then
$$
g(x) = \sum_{k=0}^{\infty } a_k x^k
$$ where $a_k = \frac{1}{k!} \frac{\partial^k g_0}{\partial x^k}$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;in-practice&#34;&gt;In Practice&lt;/h3&gt;
&lt;p&gt;The basic idea is to approximate the infinite sum by chopping it off
after $K$ terms and then estimate the coefficients by OLS.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Series estimation&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Choose $K$, i.e. the number of series terms, and an approximating
dictionary $p_1(x), \dots, p_K(x)$&lt;/li&gt;
&lt;li&gt;Expand data to
$D = \left( y_i, p_1(x_i), \dots, p_K(x_i) \right)_{i=1}^n$&lt;/li&gt;
&lt;li&gt;Estimate OLS to get $\hat{\beta}_1, \dots, \hat{\beta}_K$&lt;/li&gt;
&lt;li&gt;Set
$\hat{g}(x) = p_1 (x)\hat{\beta}_1 + \dots + p_K(x) \hat{\beta}_K$&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;examples&#34;&gt;Examples&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Monomials&lt;/strong&gt;: $p_1(x) = 1, p_2(x) = x, p_3(x)=x^2, \dots$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Hermite Polynomials&lt;/strong&gt;: $p_1(x) = 1$, $p_2(x) = x$,
$p_3(x)=x^2 -1$, $p_4(x)= x^3 - 3x, \dots$. Con: &lt;strong&gt;edge effects&lt;/strong&gt;.
The estimated function is particularly volatile at the edges of the
sample space (Gibbs effect)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Trig Polynomials&lt;/strong&gt;: $p_1(x) = 1$, $p_2(x) = \cos 2 \pi x$,
$p_3(x)= \sin 2 \pi x$, $p_4(x) = \cos 2 \pi x \cdot 2 x \dots$.
Pro: cyclical therefore good for series. Con: edge effects&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;B-splines&lt;/strong&gt;: recursively constructed using knot points $$
B_{i, 0} = \begin{cases}
1 &amp;amp; \text{if } t_i \leq x &amp;lt; t_{i+1} \newline 0 &amp;amp; \text{otherwise}
\end{cases} \qquad B_{i_k} (x) = \frac{x - t_i}{ t_{i+k} - t_i} B_{i, k-1} (x) +  \frac{t_{i+k+1}-x}{t_{i+k+1} - t_{i+1}} B_{i+1, k-1} (x)
$$ where $t_0, \dots, t_i, \dots$ are knot points and $k$ is the
order of the spline. Pro: faster rate of convergence and lower
asymptotic bias.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;hermite-polynomials&#34;&gt;Hermite Polynomials&lt;/h3&gt;
&lt;img src=&#34;../img/Fig_531.png&#34; style=&#34;width:50.0%&#34; /&gt;
&lt;h3 id=&#34;estimation&#34;&gt;Estimation&lt;/h3&gt;
&lt;p&gt;Given $K$, inference proceeds exactly as if one had run an OLS of $y$ on
$(p_k)_{k=1}^K$. The idea is that you ignore that you are doing
non-parametric regression as long as you believe you have put enough
terms (high $K$). Then the function is smooth enough so that the bias of
the approximation is small relative to the variance (see Newey, 1997).
Note that his approximation does not account for data-dependent
estimation of the bandwidth.&lt;/p&gt;
&lt;h3 id=&#34;consistency&#34;&gt;Consistency&lt;/h3&gt;
&lt;p&gt;Newey (1997): results about consistency of $\hat{g}$ and asymptotic
normality of $\hat{g}$.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;OLS: $\hat{\beta} \overset{p}{\to} \beta_0$&lt;/li&gt;
&lt;li&gt;Non-parametric: you have a sequence $\lbrace\beta_k\rbrace_{k=1}^K$
with $\hat{\beta}_k \overset{p}{\to} \beta_k$ as $n \to \infty$ (as
$k \to \infty$). However, this does not make sense because
$\lbrace\beta_k\rbrace$ is not constant. Moreover, $\beta_k$ is not
the quantity of interest. We want to make inference on $\hat{g}(x)$.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Under regularity conditions, including
$| | \hat{\beta} - \beta_0 | | \overset{p}{\to} 0$,&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Uniform Consistency:
$\sup_x | \hat{g}(x) - g_0(x)| \overset{p}{\to} 0$&lt;/li&gt;
&lt;li&gt;Mean-square Consistency:
$\int | \hat{g}(x) - g_0(x)|^2 \mathrm{d} F(x) \overset{p}{\to} 0$&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;imse&#34;&gt;IMSE&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Under the following assumptions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$(x_i, y_i)$ are iid and $Var(y_i|x_i)$ is bounded;&lt;/li&gt;
&lt;li&gt;For all $K$, there exists a non-singular matrix $B$ such that
$A = \left[ (B p(x)) (B p(x))&#39; \right]$ where
$p(x) = \left( p_1(x), \dots, p_K (x) \right)$ has the properties
that $\lambda_{\min} (A)^{-1} = O(1)$. In addition,
$\sup_x | | B p(x) | | = o(\sqrt{K/n})$.&lt;/li&gt;
&lt;li&gt;There exists $\alpha$ and $\beta_K$ for all $K$ such that $$
\sup_x | g_0 (x) - p(x) \beta_K | = O_p(K^{-\alpha})
$$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Then, it holds that&lt;/p&gt;
&lt;p&gt;$$
\text{IMSE = }\int \left( g_0 (x) - \hat{g} (x) \right)^2 \mathrm{d} F(x) = O_p \left( \frac{K}{n} + K^{-2\alpha}\right)
$$&lt;/p&gt;
&lt;h3 id=&#34;choice-of-the-optimal-k&#34;&gt;Choice of the optimal $K$&lt;/h3&gt;
&lt;p&gt;The bias-variance trade-off for series comes in through the choice of
$K$:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Higher $K$: smaller bias, since we are leaving out less terms form
the infinite sum.&lt;/li&gt;
&lt;li&gt;Smaller $K$: smaller variance, since we are estimating less
regression coefficients from the same amount of data.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Cross-validation for series&lt;/strong&gt;: For each $K \geq 0$ and for each
$i=1, \dots, n$, consider&lt;/p&gt;
&lt;p&gt;$$
D_{-i} = \lbrace (x_1, y_1), \dots, (x_{i-1}, y_{i-1}),(x_{i+1}, y_{i+1}), \dots (x_n, y_n) \rbrace
$$ and calculate $\hat{g}^{(K)}_{-i} (x)$ using series estimate with
$p_1(x), \dots, p_K (x)$ in order to get
$e^{(K)}&lt;em&gt;i = y_i - \hat{g}^{(K)}&lt;/em&gt;{-i} (x_i)$. Choose $\hat{K}$ such that&lt;/p&gt;
&lt;p&gt;$$
\hat{K} = \arg \min_K \mathbb E_n \left[ {e^{(K)}_i}^2 \right]
$$&lt;/p&gt;
&lt;h3 id=&#34;inference-1&#34;&gt;Inference&lt;/h3&gt;
&lt;p&gt;Consider the data $D = \lbrace (x_i, y_i) \rbrace_{i=1}^n$ such that
$y_i = g_0 (x_i) + \varepsilon_i$. You may want to form confidence
intervals for quantities that depends on $g_0$.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Example: $\theta_0$ functional forms of interests:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Point estimate: $\theta_0 = g_0 (\bar{x} )$ for fixed $\bar{x}$&lt;/li&gt;
&lt;li&gt;Interval estimate: $\theta_0 = g_0 (\bar{x}_2) - g_0 (\bar{x}_1)$&lt;/li&gt;
&lt;li&gt;Point derivative estimate: $\theta_0 = g_0 &#39; (\bar{x})$ at
$\bar{x}$&lt;/li&gt;
&lt;li&gt;Average derivative $\theta_0 = \mathbb E [g_0 &#39; (x) ]$&lt;/li&gt;
&lt;li&gt;Consumer surplus: $\theta_0 = \int_a^b g_0(x)dx \quad$ when $g_0$
is a demand function.&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;p&gt;Those estimates are functionals: maps from a function to a real number.
We are doing inference on a function now, not on a point estimate.&lt;/p&gt;
&lt;h3 id=&#34;inference-2&#34;&gt;Inference&lt;/h3&gt;
&lt;p&gt;In order to form a confidence interval for $\theta_0$, with series you
can&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Undersmooth&lt;/strong&gt;: in order to apply a
&lt;code&gt;\textit{central limit theorem}&lt;/code&gt;{=tex}, you need deviations around
the function to be approximately gaussian. Undersmoothing makes the
function oscillate much more than the curve you are estimating in
order to obtain such guassian deviations.&lt;/li&gt;
&lt;li&gt;Use the &lt;strong&gt;delta method&lt;/strong&gt;. It would usually require more series terms
than a criterion like cross-validation would suggest.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;undersmoothing&#34;&gt;Undersmoothing&lt;/h3&gt;
&lt;img src=&#34;../img/Fig_541.png&#34; style=&#34;width:50.0%&#34; /&gt;
&lt;p&gt;If on the contrary you oversmooth (e.g. $g_0$ linear), errors are going
to constantly be on either one or the other side of the curve $\to$ not
gaussian!&lt;/p&gt;
&lt;h3 id=&#34;delta-method&#34;&gt;Delta Method&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt;: Under the assumptions of the consistency theorem $$
\frac{\sqrt{n} \Big(\hat{\theta} - \theta_0 + B(r_K) \Big)}{\sqrt{v_K}} \overset{d}{\to} N (0,1)
$$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt;: Under the assumptions of the consistency theorem and
$\sqrt{n} K^{-\alpha} = o(1)$ (or equivalently $n K^{-2\alpha} = O(1)$
in Hansen), $$
\frac{\sqrt{n} \Big(\hat{\theta} - \theta_0 \Big)}{\sqrt{v_K}} \overset{d}{\to} N (0,1)
$$&lt;/p&gt;
&lt;h3 id=&#34;remark&#34;&gt;Remark&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;The rate of convergence of splines is faster than for power series
(Newey 1997).&lt;/li&gt;
&lt;li&gt;We have &lt;strong&gt;undersmoothing&lt;/strong&gt; if $\sqrt{n} K^{\alpha} = o(1)$ (see
comment below)&lt;/li&gt;
&lt;li&gt;Usually, in order to prove asymptotic normality, we first prove
unbiasedness. However here we have a &lt;strong&gt;biased&lt;/strong&gt; estimator but we
make the bias converge to zero faster than the variance.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Hansen (2019): The critical condition is the assumption that
$\sqrt{n} K^{\alpha} = o(1)$ This requires that $K \to \infty$ at a rate
faster than $n^{\frac{1}{2\alpha}}$ This is a troubling condition. The
optimal rate for estimation of $g(x)$ is
$K = O(n^{\frac{1}{1+ 2\alpha}})$. If we set
$K = n^{\frac{1}{1+ 2\alpha}}$ by this rule then
$n K^{-2\alpha} = n^{\frac{1}{1+ 2\alpha}} \to \infty$ not zero. Thus
this assumption is equivalent to assuming that $K$ is much larger than
optimal. The reason why this trick works (that is, why the bias is
negligible) is that by increasing $K$ the asymptotic bias decreases and
the asymptotic variance increases and thus the variance dominates.
Because $K$ is larger than optimal, we typically say that $\hat{g}(x)$
is &lt;strong&gt;undersmoothed&lt;/strong&gt; relative to the optimal series estimator.&lt;/p&gt;
&lt;h3 id=&#34;more-remarks&#34;&gt;More Remarks&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;Many authors like to focus their asymptotic theory on the assumptions
in the theorem, as the distribution of $\theta$ appears cleaner.
However, it is a poor use of asymptotic theory. There are three
problems with the assumption $\sqrt{n} K^{-\alpha} = o(1)$ and the
approximation of the theorem.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;First, it says that if we intentionally pick $K$ to be larger than
optimal, we can increase the estimation variance relative to the
bias so the variance will dominate the bias. But why would we want
to intentionally use an estimator which is sub-optimal?&lt;/li&gt;
&lt;li&gt;Second, the assumption $\sqrt{n} K^{-\alpha} = o(1)$ does not
eliminate the asymptotic bias, it only makes it of lower order
than the variance. So the approximation of the theorem is
technically valid, but the missing asymptotic bias term is just
slightly smaller in asymptotic order, and thus still relevant in
finite samples.&lt;/li&gt;
&lt;li&gt;Third, the condition $\sqrt{n} K^{\alpha} = o(1)$ is just an
assumption, it has nothing to do with actual empirical practice.
Thus the difference between the two theorems is in the
assumptions, not in the actual reality or in the actual empirical
practice. Eliminating a nuisance (the asymptotic bias) through an
assumption is a trick, not a substantive use of theory. My strong
view is that the result (1) is more informative than (2). It shows
that the asymptotic distribution is normal but has a non-trivial
finite sample bias.&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;kernel-vs-series&#34;&gt;Kernel vs Series&lt;/h3&gt;
&lt;p&gt;Hansen (2019): in this and the previous chapter we have presented two
distinct methods of nonparametric regression based on kernel methods and
series methods. Which should be used in practice? Both methods have
advantages and disadvantages and there is no clear overall winner.&lt;/p&gt;
&lt;p&gt;First, while the asymptotic theory of the two estimators appear quite
different, they are actually rather closely related. When the regression
function $g(x)$ is twice differentiable $(s = 2)$ then the rate of
convergence of both the MSE of the kernel regression estimator with
optimal bandwidth $h$ and the series estimator with optimal $K$ is
$n^{-\frac{2}{k+4}}$ (where $k = \dim(x)$). There is no difference. If
the regression function is smoother than twice differentiable ($s &amp;gt; 2$)
then the rate of the convergence of the series estimator improves. This
may appear to be an advantage for series methods, but kernel regression
can also take advantage of the higher smoothness by using so-called
higher-order kernels or local polynomial regression, so perhaps this
advantage is not too large.&lt;/p&gt;
&lt;p&gt;Both estimators are asymptotically normal and have straightforward
asymptotic standard error formulae. The series estimators are a bit more
convenient for this purpose, as classic parametric standard error
formula work without amendment.&lt;/p&gt;
&lt;h3 id=&#34;advantages-of-kernels&#34;&gt;Advantages of Kernels&lt;/h3&gt;
&lt;p&gt;An advantage of kernel methods is that their distributional theory is
easier to derive. The theory is all based on local averages which is
relatively straightforward. In contrast, series theory is more
challenging, dealing with increasing parameter spaces. An important
difference in the theory is that for kernel estimators we have explicit
representations for the bias while we only have rates for series
methods. This means that plug-in methods can be used for bandwidth
selection in kernel regression. However, typically we rely on
cross-validation, which is equally applicable in both kernel and series
regression.&lt;/p&gt;
&lt;p&gt;Kernel methods are also relatively easy to implement when the dimension
of $x$, $k$, is large. There is not a major change in the methodology as
$k$ increases. In contrast, series methods become quite cumbersome as
$k$ increases as the number of cross-terms increases exponentially. E.g
($K=2$) with $k=1$ you have only $\lbrace x_1, x_1^2\rbrace$; with $k=2$
you have to add $\lbrace x_2, x_2^2, x_1 x_2 \rbrace$; with $k=3$ you
have to add $\lbrace x_3, x_3^2, x_1 x_3, x_2 x_3\rbrace$, etc..&lt;/p&gt;
&lt;h3 id=&#34;advantages-of-series&#34;&gt;Advantages of Series&lt;/h3&gt;
&lt;p&gt;A major advantage of series methods is that it has inherently a high
degree of flexibility, and the user is able to implement shape
restrictions quite easily. For example, in series estimation it is
relatively simple to implement a partial linear CEF, an additively
separable CEF, monotonicity, concavity or convexity. These restrictions
are harder to implement in kernel regression.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Post-Double Selection</title>
      <link>https://matteocourthoud.github.io/course/ml-econ/09_postdoubleselection/</link>
      <pubDate>Mon, 28 Feb 2022 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/course/ml-econ/09_postdoubleselection/</guid>
      <description>&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Remove warnings
import warnings
warnings.filterwarnings(&#39;ignore&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Import everything
import pandas as pd
import numpy as np
import seaborn as sns
import statsmodels.api as sm

from numpy.linalg import inv
from statsmodels.iolib.summary2 import summary_col
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression, Lasso, Ridge
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.preprocessing import PolynomialFeatures, StandardScaler
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Import matplotlib for graphs
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import axes3d

# Set global parameters
%matplotlib inline
plt.style.use(&#39;seaborn-white&#39;)
plt.rcParams[&#39;lines.linewidth&#39;] = 3
plt.rcParams[&#39;figure.figsize&#39;] = (10,6)
plt.rcParams[&#39;figure.titlesize&#39;] = 20
plt.rcParams[&#39;axes.titlesize&#39;] = 18
plt.rcParams[&#39;axes.labelsize&#39;] = 14
plt.rcParams[&#39;legend.fontsize&#39;] = 14
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;91-frisch-waugh-theorem&#34;&gt;9.1 Frisch-Waugh theorem&lt;/h2&gt;
&lt;p&gt;Consider the data $D = { x_i, y_i, z_i }_{i=1}^n$ with DGP:&lt;/p&gt;
&lt;p&gt;$$
y_i = x_i&#39; \alpha_0+ z_i&#39; \beta_0 + \varepsilon_i
$$&lt;/p&gt;
&lt;p&gt;. The following estimators of $\alpha$ are numerically equivalent (if $[x, z]$ has full rank):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;OLS: $\hat{\alpha}$ from regressing $y$ on $x, z$&lt;/li&gt;
&lt;li&gt;Partialling out: $\tilde{\alpha}$ from regressing $y$ on $\tilde{x}$&lt;/li&gt;
&lt;li&gt;&amp;ldquo;Double&amp;rdquo; partialling out: $\bar{\alpha}$ from regressing $\tilde{y}$ on $\tilde{x}$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;where the operation of passing to $y, x$ to $\tilde{y}, \tilde{x}$ is called &lt;em&gt;projection  out $z$&lt;/em&gt;, e.g. $\tilde{x}$ are the residuals from regressing $x$ on $z$.&lt;/p&gt;
&lt;p&gt;$$
\tilde{x} = x - \hat \gamma z = (I - z (z&#39; z)^{-1} z&#39; ) x = (I-P_z) x = M_z x
$$&lt;/p&gt;
&lt;p&gt;I.e we have done the following:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;regress $x$ on $z$&lt;/li&gt;
&lt;li&gt;compute $\hat x$&lt;/li&gt;
&lt;li&gt;compute the residuals $\tilde x = x - \hat x$&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;We now explore the theorem through simulation. In particular, we generate a sample from the following model:&lt;/p&gt;
&lt;p&gt;$$
y_i = x_i - 0.3 z_i + \varepsilon_i
$$&lt;/p&gt;
&lt;p&gt;where $x_i,z_i,\varepsilon_i \sim N(0,1)$ and $n=1000$.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;np.random.seed(1)

# Init
n = 1000
a = 1
b = -.3

# Generate data
x = np.random.uniform(0,1,n).reshape(-1,1)
z = np.random.uniform(0,1,n).reshape(-1,1)
e = np.random.normal(0,1,n).reshape(-1,1)
y = a*x + b*z + e
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s compute the value of the OLS estimator.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Estimate alpha by OLS
xz = np.concatenate([x,z], axis=1)
ols_coeff = inv(xz.T @ xz) @ xz.T @ y
alpha_ols = ols_coeff[0][0]

print(&#39;alpha OLS: %.4f (true=%1.0f)&#39; % (alpha_ols, a))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;alpha OLS: 1.0928 (true=1)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The partialling out estimator.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Partialling out
x_tilde = (np.eye(n) - z @ inv(z.T @ z) @ z.T ) @ x
alpha_po = inv(x_tilde.T @ x_tilde) @ x_tilde.T @ y

print(&#39;alpha partialling out: %.4f (true=%1.0f)&#39; % (alpha_po, a))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;alpha partialling out: 1.0928 (true=1)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And lastly, the double-partialling out estimator.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# &amp;quot;Double&amp;quot; partialling out
y_tilde = (np.eye(n) - z @ inv(z.T @ z) @ z.T ) @ y
alpha_po2 = inv(x_tilde.T @ x_tilde) @ x_tilde.T @ y_tilde

print(&#39;alpha double partialling out: %.4f (true=%1.0f)&#39; % (alpha_po2, a))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;alpha double partialling out: 1.0928 (true=1)
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;92-omitted-variable-bias&#34;&gt;9.2 Omitted Variable Bias&lt;/h2&gt;
&lt;p&gt;Consider two separate statistical models. Assume the following &lt;strong&gt;long regression&lt;/strong&gt; of interest:&lt;/p&gt;
&lt;p&gt;$$
y_i = x_i&#39; \alpha_0+ z_i&#39; \beta_0 + \varepsilon_i
$$&lt;/p&gt;
&lt;p&gt;Define the corresponding &lt;strong&gt;short regression&lt;/strong&gt; as&lt;/p&gt;
&lt;p&gt;$$
y_i = x_i&#39; \alpha_0 + v_i \quad \text{ with } \quad x_i = z_i&#39; \gamma_0 + u_i
$$&lt;/p&gt;
&lt;h4 id=&#34;ovb-theorem&#34;&gt;OVB Theorem&lt;/h4&gt;
&lt;p&gt;Suppose that the DGP for the long regression corresponds to $\alpha_0$, $\beta_0$. Suppose further that $\mathbb E[x_i] = 0$, $\mathbb E[z_i] = 0$, $\mathbb E[\varepsilon_i |x_i,z_i] = 0$. Then, unless $\beta_0 = 0$ or $z_i$ is orthogonal to $x_i$, the (sole) stochastic regressor $x_i$ is correlated with the error term in the short regression which implies that the OLS estimator of the short regression is inconsistent for $\alpha_0$ due to the omitted variable bias. In particular, one can show that the plim of the OLS estimator of $\hat{\alpha}_{SHORT}$ from the short regression is&lt;/p&gt;
&lt;p&gt;$$
\hat{\alpha}_{SHORT} \overset{p}{\to} \frac{Cov(y_i, x_i)}{Var(x_i)} = \alpha_0 + \beta_0 \frac{Cov(z_i, x_i)}{Var(x_i)}
$$&lt;/p&gt;
&lt;p&gt;Consider data $D= (y_i, x_i, z_i)_{i=1}^n$, where the true model is:&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
&amp;amp; y_i = x_i&#39; \alpha_0  + z_i&#39; \beta_0 + \varepsilon_i \
&amp;amp; x_i = z_i&#39; \gamma_0 + u_i
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s investigate the Omitted Variable Bias by simulation. In particular, we generate a sample from the following model:&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
&amp;amp; y_i = x_i - 0.3 z_i + \varepsilon_i \
&amp;amp; x_i = 3 z_i + u_i \
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;where $z_i,\varepsilon_i,u_i \sim N(0,1)$ and $n=1000$.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def generate_data(a, b, c, n):

    # Generate data
    z = np.random.normal(0,1,n).reshape(-1,1)
    u = np.random.normal(0,1,n).reshape(-1,1)
    x = c*z + u
    e = np.random.normal(0,1,n).reshape(-1,1)
    y = a*x + b*z + e
    
    return x, y, z
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;First let&amp;rsquo;s compute the value of the OLS estimator.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Init
n = 1000
a = 1
b = -.3
c = 3
x, y, z = generate_data(a, b, c, n)

# Estimate alpha by OLS
ols_coeff = inv(x.T @ x) @ x.T @ y
alpha_short = ols_coeff[0][0]

print(&#39;alpha OLS: %.4f (true=%1.0f)&#39; % (alpha_short, a))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;alpha OLS: 0.9115 (true=1)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In our case the expected bias is:&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
Bias &amp;amp; = \beta_0 \frac{Cov(z_i, x_i)}{Var(x_i)} = \
&amp;amp; = \beta_0 \frac{Cov(z_i&#39; \gamma_0 + u_i, x_i)}{Var(z_i&#39; \gamma_0 + u_i)} = \
&amp;amp; = \beta_0 \frac{\gamma_0 Var(z_i)}{\gamma_0^2 Var(z_i) + Var(u_i)}
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;which in our case is $b \frac{c}{c^2 + 1}$.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Expected bias
bias = alpha_short - a
exp_bias = b * c / (c**2 + 1)

print(&#39;Empirical bias: %.4f \nExpected bias:  %.4f&#39; % (bias, exp_bias))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Empirical bias: -0.0885 
Expected bias:  -0.0900
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;93-pre-test-bias&#34;&gt;9.3 Pre-Test Bias&lt;/h2&gt;
&lt;p&gt;Consider data $D= (y_i, x_i, z_i)_{i=1}^n$, where the true model is:&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
&amp;amp; y_i = x_i&#39; \alpha_0  + z_i&#39; \beta_0 + \varepsilon_i \
&amp;amp; x_i = z_i&#39; \gamma_0 + u_i
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;Where $x_i$ is the variable of interest (we want to make inference on $\alpha_0$) and $z_i$ is a high dimensional set of control variables.&lt;/p&gt;
&lt;p&gt;From now on, we will work under the following assumptions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\dim(x_i)=1$ for all $n$&lt;/li&gt;
&lt;li&gt;$\beta_0$ uniformely bounded in $n$&lt;/li&gt;
&lt;li&gt;Strict exogeneity: $\mathbb E[\varepsilon_i | x_i, z_i] = 0$ and $\mathbb E[u_i | z_i] = 0$&lt;/li&gt;
&lt;li&gt;$\beta_0$ and $\gamma_0$ have dimension (and hence value) that depend on $n$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Pre-Testing procedure:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Regress $y_i$ on $x_i$ and $z_i$&lt;/li&gt;
&lt;li&gt;For each $j = 1, &amp;hellip;, p = \dim(z_i)$ calculate a test statistic $t_j$&lt;/li&gt;
&lt;li&gt;Let $\hat{T} = { j: |t_j| &amp;gt; C &amp;gt; 0 }$ for some constant $C$ (set of statistically significant coefficients).&lt;/li&gt;
&lt;li&gt;Re-run the new &amp;ldquo;model&amp;rdquo; using $(x_i, z_{\hat{T},i})$ (i.e. using the selected covariates with statistically significant coefficients).&lt;/li&gt;
&lt;li&gt;Perform statistical inference (i.e. confidence intervals and hypothesis tests) as if no model selection had been done.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Pre-testing leads to incorrect inference. Why? Because of test errors in the first stage.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# T-test
def t_test(y, x, k):
    beta_hat = inv(x.T @ x) @ x.T @ y
    residuals = y - x @ beta_hat
    sigma2_hat = np.var(residuals)
    beta_std = np.sqrt(np.diag(inv(x.T @ x)) * sigma2_hat )
    return beta_hat[k,0]/beta_std[k]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;First of all the t-test for $H_0: \beta_0 = 0$:&lt;/p&gt;
&lt;p&gt;$$
t = \frac{\hat \beta_k}{\hat \sigma_{\beta_k}}
$$&lt;/p&gt;
&lt;p&gt;where the standard deviation of the ols coefficient is given by&lt;/p&gt;
&lt;p&gt;$$
\hat \sigma_{\beta_k} = \sqrt{ \hat \sigma^2 \cdot (X&amp;rsquo;X)^{-1}_{[k,k]} }
$$&lt;/p&gt;
&lt;p&gt;where we estimate the variance of the error term with the variance of the residuals&lt;/p&gt;
&lt;p&gt;$$
\hat \sigma^2 = Var \big( y - \hat y \big) = Var \big( y - X (X&amp;rsquo;X)^{-1}X&amp;rsquo;y \big)
$$&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Pre-testing
def pre_testing(a, b, c, n, simulations=1000):
    np.random.seed(1)
    
    # Init
    alpha = {&#39;Long&#39;: np.zeros((simulations,1)),
            &#39;Short&#39;: np.zeros((simulations,1)),
            &#39;Pre-test&#39;: np.zeros((simulations,1))}

    # Loop over simulations
    for i in range(simulations):
        
        # Generate data
        x, y, z = generate_data(a, b, c, n)
        xz = np.concatenate([x,z], axis=1)
        
        # Compute coefficients
        alpha[&#39;Long&#39;][i] = (inv(xz.T @ xz) @ xz.T @ y)[0][0]
        alpha[&#39;Short&#39;][i] = inv(x.T @ x) @ x.T @ y
        
        # Compute significance of z on y
        t = t_test(y, xz, 1)
        
        # Select specification based on test
        if np.abs(t)&amp;gt;1.96:
            alpha[&#39;Pre-test&#39;][i] = alpha[&#39;Long&#39;][i]
        else:
            alpha[&#39;Pre-test&#39;][i] = alpha[&#39;Short&#39;][i]
    
    return alpha
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s compare the different estimates.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Get pre_test alpha
alpha = pre_testing(a, b, c, n)

for key, value in alpha.items():
    print(&#39;Mean alpha %s = %.4f&#39; % (key, np.mean(value)))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Mean alpha Long = 0.9994
Mean alpha Short = 0.9095
Mean alpha Pre-test = 0.9925
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The pre-testing coefficient is very close to the true coefficient.&lt;/p&gt;
&lt;p&gt;However, the main effect of pre-testing is on inference. With pre-testing, the distribution of the estimator is not gaussian anymore.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def plot_alpha(alpha, a):
    
    fig = plt.figure(figsize=(17,6))

    # Plot distributions
    x_max = np.max([np.max(np.abs(x-a)) for x in alpha.values()])

    # All axes
    for i, key in enumerate(alpha.keys()):
        
        # Reshape exisiting subplots
        k = len(fig.axes)
        for i in range(k):
            fig.axes[i].change_geometry(1, k+1, i+1)
            
        # Add new plot
        ax = fig.add_subplot(1, k+1, k+1)
        ax.hist(alpha[key], bins=30)
        ax.set_title(key)
        ax.set_xlim([a-x_max, a+x_max])
        ax.axvline(a, c=&#39;r&#39;, ls=&#39;--&#39;)
        legend_text = [r&#39;$\alpha_0=%.0f$&#39; % a, r&#39;$\hat \alpha=%.4f$&#39; % np.mean(alpha[key])]
        ax.legend(legend_text, prop={&#39;size&#39;: 10})
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s compare the long, short and pre-test estimators.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Plot
plot_alpha(alpha, a)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/09_postdoubleselection_36_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;As we can see, the main problem of pre-testing is inference.&lt;/p&gt;
&lt;p&gt;Because of the testing procedure, the distribution of the estimator is a combination of tho different distributions: the one resulting from the long regression and the one resulting from the short regression. &lt;strong&gt;Pre-testing is not a problem in 3 cases&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;when $\beta_0$ is very large: in this case the test always rejects the null hypothesis $H_0 : \beta_0=0$ and we always run the correct specification, i.e. the long regression&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;when $\beta_0$ is very small: in this case the test has very little power. However, as we saw from the Omitted Variable Bias formula, the bias is small.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;when $\gamma_0$ is very small: also in this case the test has very little power. However, as we saw from the Omitted Variable Bias formula, the bias is small.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Let&amp;rsquo;s compare the pre-test estimates for different values of the true parameter $\beta$.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Case 1: different betas and same sample size
b_sequence = b*np.array([0.1,0.3,1,3])
alpha = {}

# Get sequence
for k, b_ in enumerate(b_sequence):
    label = &#39;beta = %.2f&#39; % b_
    alpha[label] = pre_testing(a, b_, c, n)[&#39;Pre-test&#39;]
    print(&#39;Mean alpha with beta=%.2f: %.4f&#39; % (b_, np.mean(alpha[label])))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Mean alpha with beta=-0.03: 0.9926
Mean alpha with beta=-0.09: 0.9826
Mean alpha with beta=-0.30: 0.9925
Mean alpha with beta=-0.90: 0.9994
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The means are similar, but let&amp;rsquo;s look at the distributions.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Plot
plot_alpha(alpha, a)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/09_postdoubleselection_41_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;When $\beta_0$ is &amp;ldquo;small&amp;rdquo;, the distribution of the pre-testing estimator for $\alpha$ is not normal.&lt;/p&gt;
&lt;p&gt;However, the magnitue of $\beta_0$ is a relative concept. For an infinite sample size, $\beta_0$ is always going to be &amp;ldquo;big enough&amp;rdquo;, in the sense that with an infinite sample size the probability fo false positives in testing $H_0: \beta_0 = 0$ is going to zero. I.e. we always select the correct model specification, the long regression.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s have a look at the distibution of $\hat \alpha_{\text{PRE-TEST}}$ when the sample size increaes.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Case 2: same beta and different sample sizes
n_sequence = [100,300,1000,3000]
alpha = {}

# Get sequence
for k, n_ in enumerate(n_sequence):
    label = &#39;n = %.0f&#39; % n_
    alpha[label] = pre_testing(a, b, c, n_)[&#39;Pre-test&#39;]
    print(&#39;Mean alpha with n=%.0f: %.4f&#39; % (n_, np.mean(alpha[label])))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Mean alpha with n=100: 0.9442
Mean alpha with n=300: 0.9635
Mean alpha with n=1000: 0.9925
Mean alpha with n=3000: 0.9989
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Plot
plot_alpha(alpha, a)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/09_postdoubleselection_44_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;As we can see, for large samples, $\beta_0$ is never &amp;ldquo;small&amp;rdquo;. In the limit, when $n \to \infty$, the probability of false positives while testing $H_0: \beta_0 = 0$ goes to zero.&lt;/p&gt;
&lt;p&gt;We face a dilemma:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;pre-testing is clearlly a problem in finite samples&lt;/li&gt;
&lt;li&gt;all our econometric results are based on the assumption that $n \to \infty$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The problem is solved by assuming that the value of $\beta_0$ depends on the sample size. This might seems like a weird assumption but is just to have an asymptotically meaningful concept of &amp;ldquo;big&amp;rdquo; and &amp;ldquo;small&amp;rdquo;.&lt;/p&gt;
&lt;p&gt;We now look at what happens in the simulations when $\beta_0$ is proportional to $\frac{1}{\sqrt{n}}$.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Case 3: beta proportional to 1/sqrt(n) and different sample sizes
beta =  b * 30 / np.sqrt(n_sequence)

# Get sequence
alpha = {}
for k, n_ in enumerate(n_sequence):
    label = &#39;n = %.0f&#39; % n_
    alpha[label] = pre_testing(a, beta[k], c, n_)[&#39;Pre-test&#39;]
    print(&#39;Mean alpha with n=%.0f: %.4f&#39; % (n_, np.mean(alpha[label])))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Mean alpha with n=100: 0.9703
Mean alpha with n=300: 0.9838
Mean alpha with n=1000: 0.9914
Mean alpha with n=3000: 0.9947
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Plot
plot_alpha(alpha, a)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/09_postdoubleselection_48_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Now the distribution of $\hat \alpha$ does not converge to a normal when the sample size increases.&lt;/p&gt;
&lt;h3 id=&#34;pre-testing-and-machine-learning&#34;&gt;Pre-Testing and Machine Learning&lt;/h3&gt;
&lt;p&gt;How are machine learning and pre-testing related? The best example is Lasso. Suppose you have a dataset with many variables. This means that you have very few degrees of freedom and your OLS estimates are going to be very imprecise. At the extreme, you have more variables than observations so that your OLS coefficient is undefined since you cannot invert the design matrix $X&amp;rsquo;X$.&lt;/p&gt;
&lt;p&gt;In this case, you might want to do variable selection. One way of doing variable selection is pre-testing. Another way is Lasso. A third alternative is to use machine learning methods that do not suffer this curse of dimensionality.&lt;/p&gt;
&lt;p&gt;The purpose and outcome of pre-testing and Lasso are the same:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;you have too many variables&lt;/li&gt;
&lt;li&gt;you exclude some of them from the regression / set their coefficients to zero&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;As a consequence, also the problems are the same, i.e. pre-test bias.&lt;/p&gt;
&lt;h2 id=&#34;94-post-double-selection&#34;&gt;9.4 Post-Double Selection&lt;/h2&gt;
&lt;p&gt;Consider again data $D= (y_i, x_i, z_i)_{i=1}^n$, where the true model is:&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
&amp;amp; y_i = x_i&#39; \alpha_0  + z_i&#39; \beta_0 + \varepsilon_i \
&amp;amp; x_i = z_i&#39; \gamma_0 + u_i
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;We would like to guard against pretest bias if possible, in order to handle high dimensional models. A good pathway towards motivating procedures which guard against pretest bias is a discussion of classical partitioned regression.&lt;/p&gt;
&lt;p&gt;Consider a regression $y_i$ on $x_i$ and $z_i$. $x_i$ is the 1-dimensional variable of interest, $z_i$ is a high-dimensional set of control variables. We have the following procedure:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;First Stage&lt;/strong&gt; selection: regress $x_i$ on $z_i$. Select the statistically significant variables in the set $S_{FS} \subseteq z_i$&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Reduced Form&lt;/strong&gt; selection: lasso $y_i$ on $z_i$. Select the statistically significant variables in the set $S_{RF} \subseteq z_i$&lt;/li&gt;
&lt;li&gt;Regress $y_i$ on $x_i$ and $S_{FS} \cup S_{RF}$&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt;:
Let ${P^n}$ be a sequence of data-generating processes for $D_n = (y_i, x_i, z_i)^n_{i=1} \in (\mathbb R \times \mathbb R \times \mathbb R^p) ^n$ where $p$ depends on $n$. For each $n$, the data are iid with $yi =  x_i&#39;\alpha_0^{(n)} + z_i&#39; \beta_0^{(n)} + \varepsilon_i$ and $x_i = z_i&#39; \gamma_0^{(n)} + u_i$ where $\mathbb E[\varepsilon_i | x_i,z_i] = 0$ and $\mathbb E[u_i|z_i] = 0$. The sparsity of the vectors  $\beta_0^{(n)}$, $\gamma_0^{(n)}$ is controlled by $|| \beta_0^{(n)} ||_0 \leq s$ with $s^2 (\log p)^2/n \to 0$. Suppose that additional regularity conditions on the model selection procedures and moments of the random variables $y_i$ , $x_i$ , $z_i$ as documented in Belloni et al. (2014). Then the confidence intervals, CI, from the post double selection procedure are uniformly valid. That is, for any confidence level $\xi \in (0, 1)$
$$
\Pr(\alpha_0 \in CI) \to 1- \xi
$$&lt;/p&gt;
&lt;p&gt;In order to have valid confidence intervals you want their bias to be negligibly. Since
$$
CI = \left[ \hat{\alpha} \pm \frac{1.96 \cdot \hat{\sigma}}{\sqrt{n}} \right]
$$&lt;/p&gt;
&lt;p&gt;If the bias is $o \left( \frac{1}{\sqrt{n}} \right)$ then there is no problem since it is asymptotically negligible w.r.t. the magnitude of the confidence interval. If however the the bias is $O \left( \frac{1}{\sqrt{n}} \right)$ then it has the same magnitude of the confidence interval and it does not asymptotically vanish.&lt;/p&gt;
&lt;p&gt;The idea of the proof is to use partitioned regression. An alternative way to think about the argument is: bound the omitted variables bias. Omitted variable bias comes from the product of 2 quantities related to the omitted variable:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Its partial correlation with the outcome, and&lt;/li&gt;
&lt;li&gt;Its partial correlation with the variable of interest.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;If both those partial correlations are $O( \sqrt{\log p/n})$, then the omitted variables bias is $(s \times O( \sqrt{\log p/n})^2 = o \left( \frac{1}{\sqrt{n}} \right)$, provided $s^2 (\log p)^2/n \to 0$. Relative to the $ \frac{1}{\sqrt{n}} $ convergence rate, the omitted variables bias is negligible.&lt;/p&gt;
&lt;p&gt;In our omitted variable bias case, we want $| \beta_0 \gamma_0 | = o \left( \frac{1}{\sqrt{n}} \right)$.  Post-double selection guarantees that&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Reduced form&lt;/em&gt; selection (pre-testing): any &amp;ldquo;missing&amp;rdquo; variable has $|\beta_{0j}| \leq \frac{c}{\sqrt{n}}$&lt;/li&gt;
&lt;li&gt;&lt;em&gt;First stage&lt;/em&gt; selection (additional): any &amp;ldquo;missing&amp;rdquo; variable has $|\gamma_{0j}| \leq \frac{c}{\sqrt{n}}$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;As a consequence, as long as the number of omitted variables is finite, the omitted variable bias is
$$
OVB(\alpha) = |\beta_{0j}| \cdot|\gamma_{0j}| \leq \frac{c}{\sqrt{n}} \cdot \frac{c}{\sqrt{n}} = \frac{c^2}{n} = o \left(\frac{1}{\sqrt{n}}\right)
$$&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Pre-testing code
def post_double_selection(a, b, c, n, simulations=1000):
    np.random.seed(1)
    
    # Init
    alpha = {&#39;Long&#39;: np.zeros((simulations,1)),
            &#39;Short&#39;: np.zeros((simulations,1)),
            &#39;Pre-test&#39;: np.zeros((simulations,1)),
            &#39;Post-double&#39;: np.zeros((simulations,1))}

    # Loop over simulations
    for i in range(simulations):
        
        # Generate data
        x, y, z = generate_data(a, b, c, n)
        
        # Compute coefficients
        xz = np.concatenate([x,z], axis=1)
        alpha[&#39;Long&#39;][i] = (inv(xz.T @ xz) @ xz.T @ y)[0][0]
        alpha[&#39;Short&#39;][i] = inv(x.T @ x) @ x.T @ y
        
        # Compute significance of z on y (beta hat)
        t1 = t_test(y, xz, 1)
        
        # Compute significance of z on x (gamma hat)
        t2 = t_test(x, z, 0)
        
        # Select specification based on first test
        if np.abs(t1)&amp;gt;1.96:
            alpha[&#39;Pre-test&#39;][i] = alpha[&#39;Long&#39;][i]
        else:
            alpha[&#39;Pre-test&#39;][i] = alpha[&#39;Short&#39;][i]
            
        # Select specification based on both tests
        if np.abs(t1)&amp;gt;1.96 or np.abs(t2)&amp;gt;1.96:
            alpha[&#39;Post-double&#39;][i] = alpha[&#39;Long&#39;][i]
        else:
            alpha[&#39;Post-double&#39;][i] = alpha[&#39;Short&#39;][i]
    
    return alpha
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s now repeat the same exercise as above, but with also post-double selection&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Get pre_test alpha
alpha = post_double_selection(a, b, c, n)

for key, value in alpha.items():
    print(&#39;Mean alpha %s = %.4f&#39; % (key, np.mean(value)))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Mean alpha Long = 0.9994
Mean alpha Short = 0.9095
Mean alpha Pre-test = 0.9925
Mean alpha Post-double = 0.9994
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Plot
plot_alpha(alpha, a)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/09_postdoubleselection_62_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;As we can see, post-double selection has solved the pre-testing problem. Does it work for any magnitude of $\beta$ (relative to the sample size)?&lt;/p&gt;
&lt;p&gt;We first have a look at the case in which the sample size is fixed and $\beta_0$ changes.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Case 1: different betas and same sample size
b_sequence = b*np.array([0.1,0.3,1,3])
alpha = {}

# Get sequence
for k, b_ in enumerate(b_sequence):
    label = &#39;beta = %.2f&#39; % b_
    alpha[label] = post_double_selection(a, b_, c, n)[&#39;Post-double&#39;]
    print(&#39;Mean alpha with beta=%.2f: %.4f&#39; % (b_, np.mean(alpha[label])))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Mean alpha with beta=-0.03: 0.9994
Mean alpha with beta=-0.09: 0.9994
Mean alpha with beta=-0.30: 0.9994
Mean alpha with beta=-0.90: 0.9994
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Plot
plot_alpha(alpha, a)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/09_postdoubleselection_65_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Post-double selection always selects the correct specification, the long regression, even when $\beta$ is very small.&lt;/p&gt;
&lt;p&gt;Now we check the same but for fixed $\beta_0$ and different sample sizes.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Case 2: same beta and different sample sizes
n_sequence = [100,300,1000,3000]
alpha = {}

# Get sequence
for k, n_ in enumerate(n_sequence):
    label = &#39;N = %.0f&#39; % n_
    alpha[label] = post_double_selection(a, b, c, n_)[&#39;Post-double&#39;]
    print(&#39;Mean alpha with n=%.0f: %.4f&#39; % (n_, np.mean(alpha[label])))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Mean alpha with n=100: 0.9964
Mean alpha with n=300: 0.9985
Mean alpha with n=1000: 0.9994
Mean alpha with n=3000: 0.9990
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Plot
plot_alpha(alpha, a)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/09_postdoubleselection_69_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Post-double selection always selects the correct specification, the long regression, even when the sample size is very small.&lt;/p&gt;
&lt;p&gt;Last, we check the case of $\beta_0$ proportional to $\frac{1}{\sqrt{n}}$.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Case 3: beta proportional to 1/sqrt(n) and different sample sizes
beta =  b * 30 / np.sqrt(n_sequence)

# Get sequence
alpha = {}
for k, n_ in enumerate(n_sequence):
    label = &#39;N = %.0f&#39; % n_
    alpha[label] = post_double_selection(a, beta[k], c, n_)[&#39;Post-double&#39;]
    print(&#39;Mean alpha with n=%.0f: %.4f&#39; % (n_, np.mean(alpha[label])))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Mean alpha with n=100: 0.9964
Mean alpha with n=300: 0.9985
Mean alpha with n=1000: 0.9994
Mean alpha with n=3000: 0.9990
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Plot
plot_alpha(alpha, a)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/09_postdoubleselection_73_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Once again post-double selection always selects the correct specification, the long regression.&lt;/p&gt;
&lt;h3 id=&#34;post-double-selection-and-machine-learning&#34;&gt;Post-double Selection and Machine Learning&lt;/h3&gt;
&lt;p&gt;As we have seen at the end of the previous section, Lasso can be used to perform variable selection in high dimensional settings. Therefore, post-double selection solves the pre-test bias problem in those settings. The post-double selection procedure with Lasso is:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;First Stage&lt;/strong&gt; selection: lasso $x_i$ on $z_i$. Let the selected variables be collected in the set $S_{FS} \subseteq z_i$&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Reduced Form&lt;/strong&gt; selection: lasso $y_i$ on $z_i$. Let the selected variables be collected in the set $S_{RF} \subseteq z_i$&lt;/li&gt;
&lt;li&gt;Regress $y_i$ on $x_i$ and $S_{FS} \cup S_{RF}$&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;95-doubledebiased-machine-learning&#34;&gt;9.5 Double/debiased Machine Learning&lt;/h2&gt;
&lt;p&gt;This section is taken from &lt;a href=&#34;https://onlinelibrary.wiley.com/doi/abs/10.1111/ectj.12097&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Chernozhukov, V., Chetverikov, D., Demirer, M., Duflo, E., Hansen, C., Newey, W., &amp;amp; Robins, J. (2018). &amp;ldquo;&lt;em&gt;Double/debiased machine learning for treatment and structural parameters&lt;/em&gt;&amp;quot;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Consider the following partially linear model&lt;/p&gt;
&lt;p&gt;$$
y = \beta_0 D + g_0(X) + u \
D = m_0(X) + v
$$&lt;/p&gt;
&lt;p&gt;where $y$ is the outcome variable, $D$ is the treatment to interest and $X$ is a potentially high-dimensional set of controls.&lt;/p&gt;
&lt;h4 id=&#34;naive-approach&#34;&gt;Naive approach&lt;/h4&gt;
&lt;p&gt;A naive approach to estimation of $\beta_0$ using ML methods would be, for example, to construct a sophisticated ML estimator $\beta_0 D + g_0(X)$ for learning the regression function $\beta_0 D$ + $g_0(X)$.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Split the sample in two: main sample and auxiliary sample&lt;/li&gt;
&lt;li&gt;Use the auxiliary sample to estimate $\hat g_0(X)$&lt;/li&gt;
&lt;li&gt;Use the main sample to compute the orthogonalized component of $Y$ on $X$: $\hat u = \left(Y_{i}-\hat{g}&lt;em&gt;{0}\left(X&lt;/em&gt;{i}\right)\right)$&lt;/li&gt;
&lt;li&gt;Use the main sample to estimate the residualized OLS estimator&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;$$
\hat{\beta}&lt;em&gt;{0}=\left(\frac{1}{n} \sum&lt;/em&gt;{i \in I} D_{i}^{2}\right)^{-1} \frac{1}{n} \sum_{i \in I} D_{i} \hat u_i
$$&lt;/p&gt;
&lt;p&gt;This estimator is going to have two problems:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Slow rate of convergence, i.e. slower than $\sqrt(n)$&lt;/li&gt;
&lt;li&gt;It will be biased because we are employing highdimensional regularized estimators (e.g. we are doing variable selection)&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;orthogonalization&#34;&gt;Orthogonalization&lt;/h4&gt;
&lt;p&gt;Now consider a second construction that employs an orthogonalized formulation obtained by directly partialling out the effect of $X$ from $D$ to obtain the orthogonalized regressor $v = D − m_0(X)$.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Split the sample in two: main sample and auxiliary sample&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Use the auxiliary sample to estimate $\hat g_0(X)$ from&lt;/p&gt;
&lt;p&gt;$$
y = \beta_0 D + g_0(X) + u \
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Use the auxiliary sample to estimate $\hat m_0(X)$ from&lt;/p&gt;
&lt;p&gt;$$
D = m_0(X) + v
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Use the main sample to compute the orthogonalized component of $D$ on $X$ as&lt;/p&gt;
&lt;p&gt;$$
\hat v = D - \hat m_0(X)
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Use the main sample to estimate the double-residualized OLS estimator as&lt;/p&gt;
&lt;p&gt;$$
\hat{\beta}&lt;em&gt;{0}=\left(\frac{1}{n} \sum&lt;/em&gt;{i \in I} \hat v_i D_{i} \right)^{-1} \frac{1}{n} \sum_{i \in I} \hat v_i \left( Y - \hat g_0(X) \right)
$$&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The estimator is unbiased but still has a lower rate of convergence because of sample splitting. The problem is solved by inverting the split sample, re-estimating the coefficient and averaging the two estimates. Note that this procedure is valid since the two estimates are independent by the sample splitting procedure.&lt;/p&gt;
&lt;h3 id=&#34;application-to-ajr02&#34;&gt;Application to AJR02&lt;/h3&gt;
&lt;p&gt;In this section we are going to replicate 6.3 of the &amp;ldquo;&lt;em&gt;Double/debiased machine learning&lt;/em&gt;&amp;rdquo; paper based on &lt;a href=&#34;https://economics.mit.edu/files/4123&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Acemoglu, Johnson, Robinson (2002), &amp;ldquo;&lt;em&gt;The Colonial Origins of Comparative Development&lt;/em&gt;&amp;quot;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;We first load the dataset&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Load Acemoglu Johnson Robinson Dataset
df = pd.read_csv(&#39;data/AJR02.csv&#39;,index_col=0)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;GDP&lt;/th&gt;
      &lt;th&gt;Exprop&lt;/th&gt;
      &lt;th&gt;Mort&lt;/th&gt;
      &lt;th&gt;Latitude&lt;/th&gt;
      &lt;th&gt;Neo&lt;/th&gt;
      &lt;th&gt;Africa&lt;/th&gt;
      &lt;th&gt;Asia&lt;/th&gt;
      &lt;th&gt;Namer&lt;/th&gt;
      &lt;th&gt;Samer&lt;/th&gt;
      &lt;th&gt;logMort&lt;/th&gt;
      &lt;th&gt;Latitude2&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;8.39&lt;/td&gt;
      &lt;td&gt;6.50&lt;/td&gt;
      &lt;td&gt;78.20&lt;/td&gt;
      &lt;td&gt;0.3111&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;4.359270&lt;/td&gt;
      &lt;td&gt;0.096783&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;7.77&lt;/td&gt;
      &lt;td&gt;5.36&lt;/td&gt;
      &lt;td&gt;280.00&lt;/td&gt;
      &lt;td&gt;0.1367&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;5.634790&lt;/td&gt;
      &lt;td&gt;0.018687&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;9.13&lt;/td&gt;
      &lt;td&gt;6.39&lt;/td&gt;
      &lt;td&gt;68.90&lt;/td&gt;
      &lt;td&gt;0.3778&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;4.232656&lt;/td&gt;
      &lt;td&gt;0.142733&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;9.90&lt;/td&gt;
      &lt;td&gt;9.32&lt;/td&gt;
      &lt;td&gt;8.55&lt;/td&gt;
      &lt;td&gt;0.3000&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;2.145931&lt;/td&gt;
      &lt;td&gt;0.090000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;5&lt;/th&gt;
      &lt;td&gt;9.29&lt;/td&gt;
      &lt;td&gt;7.50&lt;/td&gt;
      &lt;td&gt;85.00&lt;/td&gt;
      &lt;td&gt;0.2683&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;4.442651&lt;/td&gt;
      &lt;td&gt;0.071985&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df.info()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;class &#39;pandas.core.frame.DataFrame&#39;&amp;gt;
Int64Index: 64 entries, 1 to 64
Data columns (total 11 columns):
 #   Column     Non-Null Count  Dtype  
---  ------     --------------  -----  
 0   GDP        64 non-null     float64
 1   Exprop     64 non-null     float64
 2   Mort       64 non-null     float64
 3   Latitude   64 non-null     float64
 4   Neo        64 non-null     int64  
 5   Africa     64 non-null     int64  
 6   Asia       64 non-null     int64  
 7   Namer      64 non-null     int64  
 8   Samer      64 non-null     int64  
 9   logMort    64 non-null     float64
 10  Latitude2  64 non-null     float64
dtypes: float64(6), int64(5)
memory usage: 6.0 KB
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In their paper, AJR note that their IV strategy will be invalidated if other factors are also highly persistent and related to the development of institutions within a country and to the country’s GDP. A leading candidate for such a factor, as they discuss, is geography. AJR address this by assuming that the confounding effect of geography is adequately captured by a linear term in distance from the equator and a set of continent dummy variables.&lt;/p&gt;
&lt;p&gt;They inclue their results in table 2.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Add constant term to dataset
df[&#39;const&#39;] = 1

# Create lists of variables to be used in each regression
X1 = df[[&#39;const&#39;, &#39;Exprop&#39;]]
X2 = df[[&#39;const&#39;, &#39;Exprop&#39;, &#39;Latitude&#39;, &#39;Latitude2&#39;]]
X3 = df[[&#39;const&#39;, &#39;Exprop&#39;, &#39;Latitude&#39;, &#39;Latitude2&#39;, &#39;Asia&#39;, &#39;Africa&#39;, &#39;Namer&#39;, &#39;Samer&#39;]]
y = df[&#39;GDP&#39;]

# Estimate an OLS regression for each set of variables
reg1 = sm.OLS(y, X1, missing=&#39;drop&#39;).fit()
reg2 = sm.OLS(y, X2, missing=&#39;drop&#39;).fit()
reg3 = sm.OLS(y, X3, missing=&#39;drop&#39;).fit()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s replicate Table 2 in AJR.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Make table 2
def make_table_2():

    info_dict={&#39;No. observations&#39; : lambda x: f&amp;quot;{int(x.nobs):d}&amp;quot;}

    results_table = summary_col(results=[reg1,reg2,reg3],
                                float_format=&#39;%0.2f&#39;,
                                stars = True,
                                model_names=[&#39;Model 1&#39;,&#39;Model 2&#39;,&#39;Model 3&#39;],
                                info_dict=info_dict,
                                regressor_order=[&#39;const&#39;,&#39;Exprop&#39;,&#39;Latitude&#39;,&#39;Latitude2&#39;])
    return results_table
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;table_2 = make_table_2()
table_2
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
          &lt;td&gt;&lt;/td&gt;         &lt;th&gt;Model 1&lt;/th&gt; &lt;th&gt;Model 2&lt;/th&gt; &lt;th&gt;Model 3&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;const&lt;/th&gt;            &lt;td&gt;4.66***&lt;/td&gt; &lt;td&gt;4.55***&lt;/td&gt; &lt;td&gt;5.95***&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;&lt;/th&gt;                 &lt;td&gt;(0.41)&lt;/td&gt;  &lt;td&gt;(0.45)&lt;/td&gt;  &lt;td&gt;(0.68)&lt;/td&gt; 
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Exprop&lt;/th&gt;           &lt;td&gt;0.52***&lt;/td&gt; &lt;td&gt;0.49***&lt;/td&gt; &lt;td&gt;0.40***&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;&lt;/th&gt;                 &lt;td&gt;(0.06)&lt;/td&gt;  &lt;td&gt;(0.07)&lt;/td&gt;  &lt;td&gt;(0.06)&lt;/td&gt; 
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Latitude&lt;/th&gt;            &lt;td&gt;&lt;/td&gt;      &lt;td&gt;2.16&lt;/td&gt;    &lt;td&gt;0.42&lt;/td&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;&lt;/th&gt;                    &lt;td&gt;&lt;/td&gt;     &lt;td&gt;(1.68)&lt;/td&gt;  &lt;td&gt;(1.47)&lt;/td&gt; 
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Latitude2&lt;/th&gt;           &lt;td&gt;&lt;/td&gt;      &lt;td&gt;-2.12&lt;/td&gt;   &lt;td&gt;0.44&lt;/td&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;&lt;/th&gt;                    &lt;td&gt;&lt;/td&gt;     &lt;td&gt;(2.86)&lt;/td&gt;  &lt;td&gt;(2.48)&lt;/td&gt; 
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Africa&lt;/th&gt;              &lt;td&gt;&lt;/td&gt;        &lt;td&gt;&lt;/td&gt;     &lt;td&gt;-1.06**&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;&lt;/th&gt;                    &lt;td&gt;&lt;/td&gt;        &lt;td&gt;&lt;/td&gt;     &lt;td&gt;(0.41)&lt;/td&gt; 
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Asia&lt;/th&gt;                &lt;td&gt;&lt;/td&gt;        &lt;td&gt;&lt;/td&gt;     &lt;td&gt;-0.74*&lt;/td&gt; 
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;&lt;/th&gt;                    &lt;td&gt;&lt;/td&gt;        &lt;td&gt;&lt;/td&gt;     &lt;td&gt;(0.42)&lt;/td&gt; 
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Namer&lt;/th&gt;               &lt;td&gt;&lt;/td&gt;        &lt;td&gt;&lt;/td&gt;      &lt;td&gt;-0.17&lt;/td&gt; 
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;&lt;/th&gt;                    &lt;td&gt;&lt;/td&gt;        &lt;td&gt;&lt;/td&gt;     &lt;td&gt;(0.40)&lt;/td&gt; 
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Samer&lt;/th&gt;               &lt;td&gt;&lt;/td&gt;        &lt;td&gt;&lt;/td&gt;      &lt;td&gt;-0.12&lt;/td&gt; 
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;&lt;/th&gt;                    &lt;td&gt;&lt;/td&gt;        &lt;td&gt;&lt;/td&gt;     &lt;td&gt;(0.42)&lt;/td&gt; 
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;No. observations&lt;/th&gt;   &lt;td&gt;64&lt;/td&gt;      &lt;td&gt;64&lt;/td&gt;      &lt;td&gt;64&lt;/td&gt;   
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;Using DML allows us to relax this assumption and to replace it by a weaker assumption that geography can be sufficiently controlled by an unknown function of distance from the equator and continent dummies, which can be learned by ML methods.&lt;/p&gt;
&lt;p&gt;In particular, our framework is&lt;/p&gt;
&lt;p&gt;$$
{GDP} = \beta_0 \times {Exprop} + g_0({geography}) + u \
{Exprop} = m_0({geography}) + u
$$&lt;/p&gt;
&lt;p&gt;So that the double/debiased machine learning procedure is&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Split the sample in two: main sample and auxiliary sample&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Use the auxiliary sample to estimate $\hat g_0({geography})$ from&lt;/p&gt;
&lt;p&gt;$$
{GDP} = \beta_0 \times {Exprop} + g_0({geography}) + u
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Use the auxiliary sample to estimate $\hat m_0({geography})$ from&lt;/p&gt;
&lt;p&gt;$$
{Exprop} = m_0({geography}) + v
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Use the main sample to compute the orthogonalized component of ${Exprop}$ on ${geography}$ as&lt;/p&gt;
&lt;p&gt;$$
\hat v = {Exprop} - \hat m_0({geography})
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Use the main sample to estimate the double-residualized OLS estimator as&lt;/p&gt;
&lt;p&gt;$$
\hat{\beta}&lt;em&gt;{0}=\left(\frac{1}{n} \sum&lt;/em&gt;{i \in I} \hat v_i \times {Exprop}&lt;em&gt;{i} \right)^{-1} \frac{1}{n} \sum&lt;/em&gt;{i \in I} \hat v_i \times \left( {GDP} - \hat g_0({geography}) \right)
$$&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Since we employ an &lt;strong&gt;intrumental variable&lt;/strong&gt; strategy, we replace $m_0({geography})$ with $m_0({geography},{logMort})$ in the first stage.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Generate variables
D = df[&#39;Exprop&#39;].values.reshape(-1,1)
X = df[[&#39;const&#39;, &#39;Latitude&#39;, &#39;Latitude2&#39;, &#39;Asia&#39;, &#39;Africa&#39;, &#39;Namer&#39;, &#39;Samer&#39;]].values
y = df[&#39;GDP&#39;].values.reshape(-1,1)
Z = df[[&#39;const&#39;, &#39;Latitude&#39;, &#39;Latitude2&#39;, &#39;Asia&#39;, &#39;Africa&#39;, &#39;Namer&#39;, &#39;Samer&#39;,&#39;logMort&#39;]].values
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def estimate_beta(algorithm, alg_name, D, X, y, Z, sample):

    # Split sample
    D_main, D_aux = (D[sample==1], D[sample==0])
    X_main, X_aux = (X[sample==1], X[sample==0])
    y_main, y_aux = (y[sample==1], y[sample==0])
    Z_main, Z_aux = (Z[sample==1], Z[sample==0])

    # Residualize y on D
    b_hat = inv(D_aux.T @ D_aux) @ D_aux.T @ y_aux
    y_resid_aux = y_aux - D_aux @ b_hat
    
    # Estimate g0
    alg_fitted = algorithm.fit(X=X_aux, y=y_resid_aux.ravel())
    g0 = alg_fitted.predict(X_main).reshape(-1,1)

    # Compute v_hat
    u_hat = y_main - g0

    # Estimate m0
    alg_fitted = algorithm.fit(X=Z_aux, y=D_aux.ravel())
    m0 = algorithm.predict(Z_main).reshape(-1,1)
    
    # Compute u_hat
    v_hat = D_main - m0

    # Estimate beta
    beta = inv(v_hat.T @ D_main) @ v_hat.T @ u_hat
        
    return beta 
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def ddml(algorithm, alg_name, D, X, y, Z, p=0.5, verbose=False):
    
    # Expand X if Lasso or Ridge
    if alg_name in [&#39;Lasso   &#39;,&#39;Ridge   &#39;]:
        X = PolynomialFeatures(degree=2).fit_transform(X)

    # Generate split (fixed proportions)
    split = np.array([i in train_test_split(range(len(D)), test_size=p)[0] for i in range(len(D))])
    
    # Compute beta
    beta = [estimate_beta(algorithm, alg_name, D, X, y, Z, split==k) for k in range(2)]
    beta = np.mean(beta)
     
    # Print and return
    if verbose:
        print(&#39;%s : %.4f&#39; % (alg_name, beta))
    return beta
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Generate sample split
p = 0.5
split = np.random.binomial(1, p, len(D))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We inspect different algorithms. In particular, we consider:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Lasso Regression&lt;/li&gt;
&lt;li&gt;Ridge Regression&lt;/li&gt;
&lt;li&gt;Regression Trees&lt;/li&gt;
&lt;li&gt;Random Forest&lt;/li&gt;
&lt;li&gt;Boosted Forests&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# List all algorithms
algorithms = {&#39;Ridge   &#39;: Ridge(alpha=.1),
              &#39;Lasso   &#39;: Lasso(alpha=.01),
              &#39;Tree    &#39;: DecisionTreeRegressor(),
              &#39;Forest  &#39;: RandomForestRegressor(n_estimators=30),
              &#39;Boosting&#39;: GradientBoostingRegressor(n_estimators=30)}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s compare the results.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Loop over algorithms
for alg_name, algorithm in algorithms.items():
    ddml(algorithm, alg_name, D, X, y, Z, verbose=True)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Ridge    : 0.1289
Lasso    : -8.7963
Tree     : 1.2879
Forest   : 2.4938
Boosting : 0.5977
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The results are extremely volatile.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Repeat K times
def estimate_beta_median(algorithms, D, X, y, Z, K):
    
    # Loop over algorithms
    for alg_name, algorithm in algorithms.items():
        betas = []
            
        # Iterate n times
        for k in range(K):
            beta = ddml(algorithm, alg_name, D, X, y, Z)
            betas = np.append(betas, beta)
    
        print(&#39;%s : %.4f&#39; % (alg_name, np.median(betas)))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s try using the median to have a more stable estimator.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;np.random.seed(123)

# Repeat 100 times and take median
estimate_beta_median(algorithms, D, X, y, Z, 100)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Ridge    : 0.6670
Lasso    : 1.2511
Tree     : 0.9605
Forest   : 0.5327
Boosting : 1.0327
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The results differ slightly from the ones in the paper, but they are at least closer.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Variable Selection</title>
      <link>https://matteocourthoud.github.io/course/metrics/09_selection/</link>
      <pubDate>Fri, 29 Oct 2021 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/course/metrics/09_selection/</guid>
      <description>&lt;h2 id=&#34;lasso&#34;&gt;Lasso&lt;/h2&gt;
&lt;h3 id=&#34;issue&#34;&gt;Issue&lt;/h3&gt;
&lt;p&gt;Lasso (Least Absolute Shrinkage and Selection Operator) is a popular
method for high dimensional regression. It does variable selection and
estimation simultaneously. It is a non-parametric (series) estimation
technique part of a general class of estimators called &lt;em&gt;penalized
estimators&lt;/em&gt;. It allows the number of regressors, $p$, to be larger than
the sample size, $n$.&lt;/p&gt;
&lt;p&gt;Consider data $D = \lbrace x_i, y_i \rbrace_{i=1}^n$ with
$\dim (x_i) = p$. Assume that $p$ is large relative to $n$. Two possible
reasons:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;we have an intrinsic problem of high dimensionality&lt;/li&gt;
&lt;li&gt;$p$ indicates the number of expansion terms of small number of
underlying important variables (e.g. series estimation)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Assumption&lt;/strong&gt;: $y_i = x_i&#39; \beta_0 + r_i + \varepsilon_i$ where
$\beta_0$ depends on $p$, $r_i$ is a remainder term.&lt;/p&gt;
&lt;p&gt;Note that in classic non-parametrics, we have $x_i&#39;\beta_0$ as
$p_1(x_i) \beta_{1,K} + \dots + p_K(x_i) \beta_{K,K}$. For simplicity,
we assume $r_i = 0$, as if we had extreme undersmoothing. Hence the
model becomes: $$
y_i = x_i&#39; \beta_0 + \varepsilon_i, \qquad p \geq n
$$ We cannot run OLS because $p \geq n$, thus the rank condition is
violated.&lt;/p&gt;
&lt;h3 id=&#34;definition&#34;&gt;Definition&lt;/h3&gt;
&lt;p&gt;We define the &lt;strong&gt;Lasso estimator&lt;/strong&gt; as $$
\hat{\beta}_L = \arg \min \quad \underbrace{\mathbb E_n \Big[ (y_i - x_i&#39; \beta)^2 \Big]} _ {\text{SSR term}} + \underbrace{\frac{\lambda}{n} \sum _ {j=1}^{P} | \beta_j |} _ {\text{Penalty term}}
$$ where $\lambda$ is called &lt;strong&gt;penalty parameter&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;penalty term&lt;/strong&gt; discourages large values of $| \beta_j |$. The
choice of $\lambda$ is analogous to the choice of $K$ in series
estimation and $h$ in kernel estimation.&lt;/p&gt;
&lt;h3 id=&#34;penalties&#34;&gt;Penalties&lt;/h3&gt;
&lt;p&gt;The shrinkage to zero of the coefficients directly follows from the
$|| \cdot ||_1$ norm. On the contrary, another famous penalized
estimator, &lt;em&gt;ridge regression&lt;/em&gt;, uses the $|| \cdot ||_2$ norm and does
not have this property.&lt;/p&gt;
&lt;img src=&#34;../img/Fig_551.png&#34; style=&#34;width:50.0%&#34; /&gt;
&lt;blockquote&gt;
&lt;p&gt;Minimizing SSR + penalty is equivalent to minimize SSR $s.t.$ pen
$\leq c$ (clear from the picture).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;sparsity&#34;&gt;Sparsity&lt;/h3&gt;
&lt;p&gt;Let $S_0 = \lbrace j: \beta_{0,j} \ne 0 \rbrace$, we define
$s_0 = |S_0|$ as the &lt;strong&gt;sparsity&lt;/strong&gt; of $\beta_0$. If $s_0/n \to 0$, we are
dealing with a &lt;strong&gt;sparse regression&lt;/strong&gt; (analogous of smooth regression).&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Remark on sparsity:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;In words, sparsity means that even if we have a lot of variables,
only a small number of them (relative to $n$) have an effect on
the dependent variable.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Approximate sparsity imposes a restriction that only $s_0$
variables among all of $x_{ij}$, where $s_0$ is much smaller than
$n$, have associated coefficients $\beta_{0j}$ that are different
from zero, while permitting a nonzero approximation error. Thus,
estimators for this kind of model attempt to learn the identities
of the variables with large nonzero coefficients, while
simultaneously estimating these coefficients.&lt;/em&gt; (Belloni et al.,
2004)&lt;/li&gt;
&lt;li&gt;Sparsity is an assumption. $\beta_0$ is said to be $s_0$-sparse
with $s_0 &amp;lt; n$ if $$
| \lbrace j: \beta_{0j} \neq 0 \rbrace | \leq s_0
$$&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;lasso-theorem&#34;&gt;Lasso Theorem&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Suppose that for data $D_n = (y_i, x_i)&lt;em&gt;{i=1}^N$ with
$y_i = x_i&#39; \beta + \varepsilon_i$. Let $\hat{\beta}&lt;em&gt;L$ be the Lasso
estimator. Let
$\mathcal{S} = 2 \max_j | \mathbb E[ x&lt;/em&gt;{ij} \varepsilon_i] |$. Suppose
$|support(\beta_0) \leq s_0$ (sparsity assumption). Let
$c_0 = (\mathcal{S} + \lambda/n )/(-\mathcal{S} + \lambda/n )$. Let $$
\kappa&lt;/em&gt;{c_0, s_0} = \min_{  d \in \mathbb R^p, A \subseteq \lbrace 1, &amp;hellip; , p \rbrace : |A| \leq s_0 ,  || d_{A^c}|| \leq c_0 || d_A ||_1  }  \sqrt{  \frac{ s_0 d&#39; \mathbb E_n [x_i x_i&#39;] d }{|| d_A ||_1^2}  }
$$ Then&lt;/p&gt;
&lt;p&gt;$$
\mathbb I_{ \left\lbrace \frac{\lambda}{n} &amp;gt; \mathcal{S}  \right\rbrace} \mathbb E_n [(x_i \beta_0 - x_i \beta_L)^2]^{\frac{1}{2}}  \leq 2 \frac{\lambda}{n} \frac{\sqrt{s_0}}{\kappa_{c_0, s_0}}
$$&lt;/p&gt;
&lt;p&gt;Intuition: for a sufficiently high lambda the root mean squared error of
Lasso is approximately zero.&lt;/p&gt;
&lt;p&gt;$$
\text{ RMSE }:  \mathbb E_n [(x_i \beta_0 - x_i \beta_L)^2]^{\frac{1}{2}}  \simeq 0  \quad \Leftrightarrow \quad \frac{\lambda}{n} &amp;gt; \mathcal{S}
$$&lt;/p&gt;
&lt;h3 id=&#34;remarks&#34;&gt;Remarks&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;The minimization region is the set of “essentially sparse” vectors
$d \in \mathbb R^p$, where “essentially sparse” is defined by
$\mathcal{C}, \mathcal{S}$. In particular the condition
$k_{\mathcal{C}, \mathcal{S}}&amp;gt;0$ means that no essentially sparse
vector $d$ has $\mathbb E[x_i x_i&#39;]d = 0$, i.e. regressors were not
added multiple times.&lt;/li&gt;
&lt;li&gt;Need to dominate the score with the penalty term $\lambda$.&lt;/li&gt;
&lt;li&gt;Need no collinearity on a small ($\leq s_0$) subset of regressors
($\to k_{c_0, s_0}&amp;gt;0$).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;When Lasso?&lt;/strong&gt; For prediction problems in high dimensional
environments. &lt;strong&gt;NB!&lt;/strong&gt; Lasso is not good for inference, only for
prediction.&lt;/p&gt;
&lt;p&gt;In particular, in econometrics it’s used for selecting either&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;instruments (predicting $\hat{x}$ in the first stage)&lt;/li&gt;
&lt;li&gt;control variables (next section: double prediction problem, in the
first stage and in the reduced form)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;choosing-the-optimal-lambda&#34;&gt;Choosing the Optimal Lambda&lt;/h3&gt;
&lt;p&gt;The choice of $\lambda$ determines the bias-variance tradeoff:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;if $\lambda$ is too big:
$\lambda \approx \infty \mathbb \Rightarrow \hat{\beta} \approx 0$;&lt;/li&gt;
&lt;li&gt;if $\lambda$ is too small: $\lambda \approx 0 \mathbb \Rightarrow$
overfitting.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Possible solutions: Bonferroni correction, bootstrapping or
$\frac{\lambda}{n} \asymp \sqrt{\frac{\log(p)}{n}}$ (asymptotically
equal to), $\mathcal{S}$ behaves like the maximum of gaussians.&lt;/p&gt;
&lt;h3 id=&#34;lasso-path&#34;&gt;Lasso Path&lt;/h3&gt;
&lt;p&gt;How the estimated $\hat{\beta}$ depends on the penalty parameter
$\lambda$?&lt;/p&gt;
&lt;img src=&#34;../img/Fig_642.png&#34; style=&#34;width:50.0%&#34; /&gt;
&lt;p&gt;&lt;strong&gt;Post Lasso&lt;/strong&gt;: fit OLS without the penalty with all the nonzero
coeficients selected by Lasso in the first step.&lt;/p&gt;
&lt;h3 id=&#34;remarks-1&#34;&gt;Remarks&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Do not do inference with post-Lasso because standard errors are not
uniformely valid.&lt;/li&gt;
&lt;li&gt;As $n \to \infty$ the CV and the &lt;strong&gt;score domination&lt;/strong&gt; bounds
converge to a unique bound.&lt;/li&gt;
&lt;li&gt;What is the problem of cross-validation? In high dimensional
settings you can overfit in so many ways that CV doesn’t work and
still overfits.&lt;/li&gt;
&lt;li&gt;Using $\lambda$ with $\frac{\lambda}{n} &amp;gt; \mathcal{S}$ small
coefficients get shrunk to zero with high probability. In this case
with small we mean $\propto \frac{1}{\sqrt{n}}$ or
$2 \max_j | \mathbb E_n[\varepsilon_i x_{ij}] |$.&lt;/li&gt;
&lt;li&gt;If $| \beta_{0j}| \leq \frac{c}{\sqrt{n}}$ for a sufficiently small
constant $c$, then $\hat{\beta}_{LASSO} \overset{p}{\to} 0$.&lt;/li&gt;
&lt;li&gt;In standard t-tests $c = 1.96$.&lt;/li&gt;
&lt;li&gt;$\sqrt{n}$ factor is important since it is the demarcation line for
reliable statistical detection.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;optimal-lambda&#34;&gt;Optimal Lambda&lt;/h3&gt;
&lt;p&gt;What is the criterium that should guide the selection of $\lambda$? $$
\frac{\lambda}{n} \geq 2 \mathbb E_n[x_{ij} \varepsilon_i] \qquad \forall j \quad \text{ if } Var(x_{ij} \varepsilon_i) = 1
$$&lt;/p&gt;
&lt;p&gt;How to choose the optimal $\lambda$:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Decide the coverage of the confidence intervals ($1-\alpha$): $$
\Pr \left( \sqrt{n} \Big| \mathbb E_n [x_{ij} \varepsilon_i] \Big| &amp;gt; t \right) = 1- \alpha
$$&lt;/li&gt;
&lt;li&gt;Solve for $t$&lt;/li&gt;
&lt;li&gt;Get $\lambda$ such that all scores are dominated by
$\frac{\lambda}{n}$ with $\alpha%$ probability.&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;It turns out that the optimal $t \propto \sqrt{\log(p)}$&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;pre-testing&#34;&gt;Pre-Testing&lt;/h2&gt;
&lt;h3 id=&#34;omitted-variable-bias&#34;&gt;Omitted Variable Bias&lt;/h3&gt;
&lt;p&gt;Consider two separate statistical models. Assume the following &lt;strong&gt;long
regression&lt;/strong&gt; of interest:&lt;/p&gt;
&lt;p&gt;$$
y_i = x_i&#39; \alpha_0+ z_i&#39; \beta_0 + \varepsilon_i
$$&lt;/p&gt;
&lt;p&gt;Define the corresponding &lt;strong&gt;short regression&lt;/strong&gt; as&lt;/p&gt;
&lt;p&gt;$$
y_i = x_i&#39; \alpha_0 + v_i \quad \text{ with } v_i = z_i&#39; \beta_0 + \varepsilon_i
$$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Suppose that the DGP for the long regression corresponds to $\alpha_0$,
$\beta_0$. Suppose further that $\mathbb E[x_i] = 0$,
$\mathbb E[z_i] = 0$, $\mathbb E[\varepsilon_i |x_i,z_i] = 0$. Then,
unless $\beta_0 = 0$ or $z_i$ is orthogonal to $x_i$, the (sole)
stochastic regressor $x_i$ is correlated with the error term in the
short regression which implies that the OLS estimator of the short
regression is inconsistent for $\alpha_0$ due to the omitted variable
bias. In particular, one can show that the plim of the OLS estimator of
$\hat{\alpha}&lt;em&gt;{SHORT}$ from the short regression is $$
\hat{\alpha}&lt;/em&gt;{SHORT} \overset{p}{\to} \frac{Cov(y_i, x_i)}{Var(x_i)} = \alpha_0 + \beta_0 \frac{Cov(z_i, x_i)}{Var(x_i)}
$$&lt;/p&gt;
&lt;h3 id=&#34;pre-test-bias&#34;&gt;Pre-test bias&lt;/h3&gt;
&lt;p&gt;Consider data $D= (y_i, x_i, z_i)_{i=1}^n$, where the true model is: $$
\begin{aligned}
&amp;amp; y_i = x_i&#39; \alpha_0  + z_i&#39; \beta_0 + \varepsilon_i \newline
&amp;amp; x_i = z_i&#39; \gamma_0 + u_i
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;Where $x_i$ is the variable of interest (we want to make inference on
$\alpha_0$) and $z_i$ is a high dimensional set of control variables.&lt;/p&gt;
&lt;p&gt;From now on, we will work under the following assumptions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\dim(x_i)=1$ for all $n$&lt;/li&gt;
&lt;li&gt;$\beta_0$ uniformely bounded in $n$&lt;/li&gt;
&lt;li&gt;Strict exogeneity: $\mathbb E[\varepsilon_i | x_i, z_i] = 0$ and
$\mathbb E[u_i | z_i] = 0$&lt;/li&gt;
&lt;li&gt;$\beta_0$ and $\gamma_0$ have dimension (and hence value) that
depend on $n$&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;pre-testing-procedure&#34;&gt;Pre-Testing procedure&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;Regress $y_i$ on $x_i$ and $z_i$&lt;/li&gt;
&lt;li&gt;For each $j = 1, &amp;hellip;, p = \dim(z_i)$ calculate a test statistic
$t_j$&lt;/li&gt;
&lt;li&gt;Let $\hat{T} = \lbrace j: |t_j| &amp;gt; C &amp;gt; 0 \rbrace$ for some constant
$C$ (set of statistically significant coefficients).&lt;/li&gt;
&lt;li&gt;Re-run the new “model” using $(x_i, z_{\hat{T},i})$ (i.e. using the
selected covariates with statistically significant coefficients).&lt;/li&gt;
&lt;li&gt;Perform statistical inference (i.e. confidence intervals and
hypothesis tests) as if no model selection had been done.&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;bias&#34;&gt;Bias&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;../img/Fig_621.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;As we can see from the figure above (code below), running the short
regression instead of the long one introduces Omitted Variable Bias
(second column). Instead, the Pre-Testing estimator is consistent but
not normally distributed (third column).&lt;/p&gt;
&lt;h3 id=&#34;issue-1&#34;&gt;Issue&lt;/h3&gt;
&lt;p&gt;Pre-testing is problematic because the post-selection estimator is not
asymptotically normal. Moreover, for particular data generating
processes, it even fails to be consistent at the rate of $\sqrt{n}$
(Belloni et al., 2014).&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Intuition: when performing pre-testing, we might have an Omitted
Variable Bias problem when $\beta_0&amp;gt;0$ but we fail to reject the null
hypothesis $H_0 : \beta_0 = 0$ because of lack of statistical power,
i.e. $|\beta_0|$ is small with respect to the sample size. In
particular, we fail to reject the null hypothesis for
$\beta_0(n) = O \left( \frac{1}{\sqrt{n}}\right)$. However, note that
the problem vanishes asymptotically, as the resulting estimator is
consistent. In fact, if
$\beta_0(n) = O \left( \frac{1}{\sqrt{n}}\right)$, then
$\alpha_0 - \hat \alpha_{PRETEST} \overset{p}{\to} \lim_{n \to \infty} \beta_0 \gamma_0 = \lim_{n \to \infty} O \left( \frac{1}{\sqrt{n}} \right) = 0$.
We now clarify what it means to have a coefficient depending on the
sample size, $\beta_0(n)$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;uniformity&#34;&gt;Uniformity&lt;/h3&gt;
&lt;p&gt;Concept of &lt;strong&gt;uniformity&lt;/strong&gt;: the DGP varies with $n$. Instead of having a
fixed “true” parameter $\beta_0$, you have a sequence $\beta_0(n)$.
Having a cofficient that depends on the sample size $n$ is useful to
preserve the concept of “small with respect to the sample size” in
asymptotic theory.&lt;/p&gt;
&lt;p&gt;In the context of Pre-Testing, all problems vanish asymptotically since
we are able to always reject the null hypothesis $H_0 : \beta_0 = 0$
when $\beta_0 \neq 0$. In the figure below, I plot simulation results
for $\hat \alpha_{PRETESTING}$ for a fixed coefficient $\beta_0$ (first
row) and variable coefficient $\beta_0(n)$ that depends on the sample
size (second row), for different sample sizes (columns). We see that if
$\beta_0$ is independent from the sample size (first row), the
distribution of $\hat \alpha_{PRETEST}$ is not normal in small samples
and it displays the bimodality that characterizes pre-testing. However,
it becomes normal in large samples. On the other hand, when $\beta_0(n)$
depends on the sample size, and in particular
$\beta_0 = O \left( \frac{1}{\sqrt{n}} \right)$ (second row), the
distribution of $\hat \alpha_{PRETEST}$ stays bimodal even when the
sample size increases.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Note that the estimator is always consistent!&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;where-is-pre-testing-a-problem&#34;&gt;Where is Pre-Testing a Problem?&lt;/h3&gt;
&lt;p&gt;If we were to draw a map of where the gaussianity assumption of
$\beta_0(n)$ holds well and where it fails, it would look like the
following figure.&lt;/p&gt;
&lt;img src=&#34;../img/Fig_623.png&#34; style=&#34;width:50.0%&#34; /&gt;
&lt;h3 id=&#34;intuition&#34;&gt;Intuition&lt;/h3&gt;
&lt;p&gt;The intuition for the three different regions (from bottom to top) is
the following.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;When $\beta_0 = o \left( \frac{1}{\sqrt{n}} \right)$, $z_i$ is
excluded with probability $p \to 1$. But, given that $\beta_0$ is
small enough, failing to control for $z_i$ does not introduce large
omitted variables bias (Belloni et al., 2014).&lt;/li&gt;
&lt;li&gt;If however the coefficient on the control is “moderately close to
zero”, $\beta_0 = O \left( \frac{1}{\sqrt{n}} \right)$, the t-test
set-up above cannot distinguish this coefficient from $0$, and the
control $z_i$ is dropped with probability $p \to 1$. However, in
this case the omitted variable bias generated by excluding $z_i$
scaled by $\sqrt{n}$ does not converge to zero. That is, the
standard post-selection estimator is not asymptotically normal and
even fails to be consistent at the rate of $\sqrt{n}$ (Belloni et
al., 2014).&lt;/li&gt;
&lt;li&gt;Lastly, when $\beta_0$ is large enough, the null pre-testing
hypothesis $H_0 : \beta_0 = 0$ will be rejected sufficiently often
so that the bias is negligible.&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;post-double-selection&#34;&gt;Post-Double Selection&lt;/h3&gt;
&lt;p&gt;The post-double-selection estimator, $\hat{\alpha}_{PDS}$ solves this
problem by doing variable selection via standard t-tests or Lasso-type
selectors with the two “true model” equations (&lt;strong&gt;first stage&lt;/strong&gt; and
&lt;strong&gt;reduced form&lt;/strong&gt;) that contain the information from the model and then
estimating $\alpha_0$ by regressing $y_i$ on $x_i$ and the union of the
selected controls. By doing so, $z_i$ is omitted only if its coefficient
in both equations is small which greatly limits the potential for
omitted variables bias (Belloni et al., 2014).&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Intuition: by performing post-double selection, we ensure that both
$\beta_0 = O \left( \frac{1}{\sqrt{n}} \right)$ and
$\gamma_0 = O \left( \frac{1}{\sqrt{n}} \right)$ so that
$\sqrt{n} ( \hat \alpha _ {PRETEST} - \alpha _ 0) \overset{p}{\to} \lim_{n \to \infty} \sqrt{n} \beta_0 \gamma_0 = \lim_{n \to \infty} \sqrt{n} O \left( \frac{1}{n} \right) = 0$
and the estimator is gaussian.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;frisch-waugh-theorem&#34;&gt;Frisch-Waugh Theorem&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Consider the data $D = \lbrace x_i, y_i, z_i \rbrace_{i=1}^\infty$ with
DGP: $Y = X \alpha + Z \beta + \varepsilon$. The following estimators of
$\alpha$ are numerically equivalent (if $[X, Z]$ has full rank):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\hat{\alpha}$ from regressing $Y$ on $X, Z$&lt;/li&gt;
&lt;li&gt;$\tilde{\alpha}$ from regressing $Y$ on $\tilde{X}$&lt;/li&gt;
&lt;li&gt;$\bar{\alpha}$ from regressing $\tilde{Y}$ on $\tilde{X}$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;where the operation of passing to $Y, X$ to $\tilde{Y}, \tilde{X}$ is
called &lt;em&gt;projection out $Z$&lt;/em&gt;, e.g.$\tilde{X}$ are the residuals from
regressing $X$ on $Z$.&lt;/p&gt;
&lt;h3 id=&#34;proof-1&#34;&gt;Proof (1)&lt;/h3&gt;
&lt;p&gt;We want to show that $\hat{\alpha} = \tilde{\alpha}$.&lt;/p&gt;
&lt;p&gt;Claim:
$\hat{\alpha } = \tilde{\alpha} \Leftrightarrow \tilde{X}&#39; \left[ (X - \tilde{X})\hat{\alpha} + Z \hat{\beta} +\hat{\varepsilon} \right] = 0$.&lt;/p&gt;
&lt;p&gt;Proof of the claim: if $\hat{\alpha} = \tilde{\alpha}$, we can write $Y$
as $$
Y =  X \hat{\alpha} + Z \hat{\beta} + \hat{\varepsilon}  = \tilde{X} \hat{\alpha} + \underbrace{(X - \tilde{X}) \hat{\alpha } + Z \hat{\beta} + \hat{\varepsilon}}_\text{residual of $Y$ on $\tilde{X} $} = \tilde{X} \tilde{\alpha} + \nu_i
$$&lt;/p&gt;
&lt;p&gt;Therefore, by the orthogonality property of the OLS residual, it must be
that $\tilde{X}&#39;\nu_i= 0$. $$\tag*{$\blacksquare$}$$&lt;/p&gt;
&lt;h3 id=&#34;proof-1-1&#34;&gt;Proof (1)&lt;/h3&gt;
&lt;p&gt;Having established the claim, we want to show that the normal equation
$\tilde{X}&#39; \left[ (X - \tilde{X})\hat{\alpha} + Z \hat{\beta} +\hat{\varepsilon} \right] = 0$
is satisfied. We follow 3 steps:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;First we have that $\tilde{X}&#39; (X - \tilde{X})\hat{\alpha} = 0$.
This follows from the fact that $\tilde{X}&#39; = X&#39; M_Z$ and hence: $$
\begin{aligned}
\tilde{X}&#39; (X - \tilde{X})  &amp;amp;  = X&#39; M_Z (X - M_Z) = X&#39; M_Z X - X&#39; \overbrace{M_Z M_Z}^{M_Z} X \newline &amp;amp; = X&amp;rsquo;M_Z X - X&#39; M_Z X = 0
\end{aligned}
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$\tilde{X}&#39; Z \hat{\beta} = 0$ since $\tilde{X}$ is the residual
from the regression of $X$ on $Z$, by normal equation it holds that
$\tilde{X}&#39; Z = 0$.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$\tilde{X}&#39; \hat{\varepsilon} = 0$. This follows from (i)
$M_Z &#39; M_{X, Z} = M_{X,Z}$ and (ii) $X&#39; M_{X, Z} = 0$: $$
\tilde{X}&#39; \hat{\varepsilon} = (M_Z X)&#39; (M_{X, Z} \varepsilon)  = X&amp;rsquo;M_Z&#39; M_{X, Z} \varepsilon = \underbrace{X&#39; M_{X, Z}}_0 \varepsilon = 0.
$$ $$\tag*{$\blacksquare$}$$&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The coefficient $\hat{\alpha}$ is a &lt;em&gt;partial regression&lt;/em&gt; coefficient
identified from the variation in $X$ that is orthogonal to $Z$. This is
often known as &lt;strong&gt;residual variation&lt;/strong&gt;.&lt;/p&gt;
&lt;h2 id=&#34;post-double-selection-1&#34;&gt;Post Double Selection&lt;/h2&gt;
&lt;h3 id=&#34;setting&#34;&gt;Setting&lt;/h3&gt;
&lt;p&gt;Consider again data $D= (y_i, x_i, z_i)_{i=1}^n$, where the true model
is: $$
\begin{aligned}
&amp;amp; y_i = x_i&#39; \alpha_0  + z_i&#39; \beta_0 + \varepsilon_i \newline
&amp;amp; x_i = z_i&#39; \gamma_0 + u_i
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;We would like to guard against pretest bias if possible, in order to
handle high dimensional models. A good pathway towards motivating
procedures which guard against pretest bias is a discussion of classical
partitioned regression.&lt;/p&gt;
&lt;p&gt;Consider a regression $y_i$ on $x_i$ and $z_i$. $x_i$ is the
1-dimensional variable of interest, $z_i$ is a high-dimensional set of
control variables. We have the following procedure:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;First Stage&lt;/strong&gt; selection: lasso $x_i$ on $z_i$. Let the selected
variables be collected in the set $S_{FS} \subseteq z_i$&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Reduced Form&lt;/strong&gt; selection: lasso $y_i$ on $z_i$. Let the selected
variables be collected in the set $S_{RF} \subseteq z_i$&lt;/li&gt;
&lt;li&gt;Regress $y_i$ on $x_i$ and $S_{FS} \cup S_{RF}$&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;pds-theorem&#34;&gt;PDS Theorem&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Let $\lbrace P^n\rbrace$ be a sequence of data-generating processes for
$D_n = (y_i, x_i, z_i)^n_{i=1} \in (\mathbb R \times \mathbb R \times \mathbb R^p) ^n$
where $p$ depends on $n$. For each $n$, the data are iid with
$yi = x_i&#39;\alpha_0^{(n)} + z_i&#39; \beta_0^{(n)} + \varepsilon_i$ and
$x_i = z_i&#39; \gamma_0^{(n)} + u_i$ where
$\mathbb E[\varepsilon_i | x_i,z_i] = 0$ and $\mathbb E[u_i|z_i] = 0$.
The sparsity of the vectors $\beta_0^{(n)}$, $\gamma_0^{(n)}$ is
controlled by $|| \beta_0^{(n)} ||_0 \leq s$ with
$s^2 (\log p)^2/n \to 0$. Suppose that additional regularity conditions
on the model selection procedures and moments of the random variables
$y_i$ , $x_i$ , $z_i$ as documented in Belloni et al. (2014). Then the
confidence intervals, CI, from the post double selection procedure are
uniformly valid. That is, for any confidence level $\xi \in (0, 1)$ $$
\Pr(\alpha_0 \in CI) \to 1- \xi
$$&lt;/p&gt;
&lt;p&gt;In order to have valid confidence intervals you want their bias to be
negligibly. Since $$
CI = \left[ \hat{\alpha} \pm \frac{1.96 \cdot \hat{\sigma}}{\sqrt{n}} \right]
$$&lt;/p&gt;
&lt;p&gt;If the bias is $o \left( \frac{1}{\sqrt{n}} \right)$ then there is no
problem since it is asymptotically negligible w.r.t. the magnitude of
the confidence interval. If however the the bias is
$O \left( \frac{1}{\sqrt{n}} \right)$ then it has the same magnitude of
the confidence interval and it does not asymptotically vanish.&lt;/p&gt;
&lt;h3 id=&#34;proof-idea&#34;&gt;Proof (Idea)&lt;/h3&gt;
&lt;p&gt;The idea of the proof is to use partitioned regression. An alternative
way to think about the argument is: bound the omitted variables bias.
Omitted variable bias comes from the product of 2 quantities related to
the omitted variable:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Its partial correlation with the outcome, and&lt;/li&gt;
&lt;li&gt;Its partial correlation with the variable of interest.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;If both those partial correlations are $O( \sqrt{\log p/n})$, then the
omitted variables bias is
$(s \times O( \sqrt{\log p/n})^2 = o \left( \frac{1}{\sqrt{n}} \right)$,
provided $s^2 (\log p)^2/n \to 0$. Relative to the $\frac{1}{\sqrt{n}}$
convergence rate, the omitted variables bias is negligible.&lt;/p&gt;
&lt;p&gt;In our omitted variable bias case, we want
$| \beta_0 \gamma_0 | = o \left( \frac{1}{\sqrt{n}} \right)$.
Post-double selection guarantees that&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Reduced form&lt;/em&gt; selection (pre-testing): any “missing” variable has
$|\beta_{0j}| \leq \frac{c}{\sqrt{n}}$&lt;/li&gt;
&lt;li&gt;&lt;em&gt;First stage&lt;/em&gt; selection (additional): any “missing” variable has
$|\gamma_{0j}| \leq \frac{c}{\sqrt{n}}$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;As a consequence, as long as the number of omitted variables is finite,
the omitted variable bias is $$
OVB(\alpha) = |\beta_{0j}| \cdot|\gamma_{0j}| \leq \frac{c}{\sqrt{n}} \cdot \frac{c}{\sqrt{n}} = \frac{c^2}{n} = o \left(\frac{1}{\sqrt{n}}\right)
$$&lt;/p&gt;
&lt;h3 id=&#34;distribution&#34;&gt;Distribution&lt;/h3&gt;
&lt;p&gt;We can plot the distribution of the post-double selection estimator
against the pre-testing one.&lt;/p&gt;
&lt;img src=&#34;../img/Fig_641.png&#34; style=&#34;width:60.0%&#34; /&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Remark&lt;/strong&gt;: under homoskedasticity, the above estimator achieves the
semiparametric efficiency bound.&lt;/p&gt;
&lt;/blockquote&gt;
</description>
    </item>
    
    <item>
      <title>Unsupervised Learning</title>
      <link>https://matteocourthoud.github.io/course/ml-econ/10_unsupervised/</link>
      <pubDate>Mon, 28 Feb 2022 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/course/ml-econ/10_unsupervised/</guid>
      <description>&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Setup
from utils.lecture10 import *
%matplotlib inline
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;supervised-vs-unsupervised-learning&#34;&gt;Supervised vs Unsupervised Learning&lt;/h3&gt;
&lt;p&gt;The difference between &lt;em&gt;supervised learning&lt;/em&gt; and &lt;em&gt;unsupervised learning&lt;/em&gt; is that in the first case we have a variable $y$ which we want to predict, given a set of variables ${ X_1, . . . , X_p }$.&lt;/p&gt;
&lt;p&gt;In &lt;em&gt;unsupervised learning&lt;/em&gt; are not interested in prediction, because we do not have an associated response variable $y$. Rather, the goal is to discover interesting properties about the measurements on ${ X_1, . . . , X_p }$.&lt;/p&gt;
&lt;p&gt;Questions that we are usually interested in are&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Clustering&lt;/li&gt;
&lt;li&gt;Dimensionality reduction&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In general, unsupervised learning can be viewed as an extention of exploratory data analysis.&lt;/p&gt;
&lt;h3 id=&#34;dimensionality-reduction&#34;&gt;Dimensionality Reduction&lt;/h3&gt;
&lt;p&gt;Working in high-dimensional spaces can be undesirable for many reasons; raw data are often sparse as a consequence of the curse of dimensionality, and analyzing the data is usually computationally intractable (hard to control or deal with).&lt;/p&gt;
&lt;p&gt;Dimensionality reduction can also be useful to plot high-dimensional data.&lt;/p&gt;
&lt;h3 id=&#34;clustering&#34;&gt;Clustering&lt;/h3&gt;
&lt;p&gt;Clustering refers to a very broad set of techniques for finding subgroups, or clusters, in a data set. When we cluster the observations of a data set, we seek to partition them into distinct groups so that the observations within each group are quite similar to each other, while observations in different groups are quite different from each other.&lt;/p&gt;
&lt;p&gt;In this section we focus on the following algorithms:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;K-means clustering&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Hierarchical clustering&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Gaussian Mixture Models&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;principal-component-analysis&#34;&gt;Principal Component Analysis&lt;/h2&gt;
&lt;p&gt;Suppose that we wish to visualize $n$ observations with measurements on a set of $p$ features, ${X_1, . . . , X_p}$, as part of an exploratory data analysis.&lt;/p&gt;
&lt;p&gt;We could do this by examining two-dimensional scatterplots of the data, each of which contains the n observations’ measurements on two of the features. However, there are $p(p−1)/2$ such scatterplots; for example,
with $p = 10$ there are $45$ plots!&lt;/p&gt;
&lt;p&gt;PCA provides a tool to do just this. It finds a low-dimensional represen- tation of a data set that contains as much as possible of the variation.&lt;/p&gt;
&lt;h3 id=&#34;first-principal-component&#34;&gt;First Principal Component&lt;/h3&gt;
&lt;p&gt;The &lt;strong&gt;first principal component&lt;/strong&gt; of a set of features ${X_1, . . . , X_p}$ is the normalized linear combination of the features $Z_1$&lt;/p&gt;
&lt;p&gt;$$
Z_1 = \phi_{11} X_1 + \phi_{21} X_2 + &amp;hellip; + \phi_{p1} X_p
$$&lt;/p&gt;
&lt;p&gt;that has the largest variance.&lt;/p&gt;
&lt;p&gt;By normalized, we mean that $\sum_{i=1}^p \phi^2_{i1} = 1$.&lt;/p&gt;
&lt;h3 id=&#34;pca-computation&#34;&gt;PCA Computation&lt;/h3&gt;
&lt;p&gt;In other words, the first principal component loading vector solves the optimization problem&lt;/p&gt;
&lt;p&gt;$$
\underset{\phi_{11}, \ldots, \phi_{p 1}}{\max} \ \Bigg \lbrace \frac{1}{n} \sum _ {i=1}^{n}\left(\sum _ {j=1}^{p} \phi _ {j1} x _ {ij} \right)^{2} \Bigg \rbrace \quad \text { subject to } \quad \sum _ {j=1}^{p} \phi _ {j1}^{2}=1
$$&lt;/p&gt;
&lt;p&gt;The objective that we are maximizing is just the sample variance of the $n$ values of $z_{i1}$.&lt;/p&gt;
&lt;p&gt;After the first principal component $Z_1$ of the features has been determined, we can find the second principal component $Z_2$. The &lt;strong&gt;second principal component&lt;/strong&gt; is the linear combination of ${X_1, . . . , X_p}$ that has maximal variance out of all linear combinations that are &lt;em&gt;uncorrelated&lt;/em&gt; with $Z_1$.&lt;/p&gt;
&lt;h3 id=&#34;example&#34;&gt;Example&lt;/h3&gt;
&lt;p&gt;We illustrate the use of PCA on the &lt;code&gt;USArrests&lt;/code&gt; data set.&lt;/p&gt;
&lt;p&gt;For each of the 50 states in the United States, the data set contains the number of arrests per $100,000$ residents for each of three crimes: &lt;code&gt;Assault&lt;/code&gt;, &lt;code&gt;Murder&lt;/code&gt;, and &lt;code&gt;Rape.&lt;/code&gt; We also record the percent of the population in each state living in urban areas, &lt;code&gt;UrbanPop&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Load crime data
df = pd.read_csv(&#39;data/USArrests.csv&#39;, index_col=0)
df.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;Murder&lt;/th&gt;
      &lt;th&gt;Assault&lt;/th&gt;
      &lt;th&gt;UrbanPop&lt;/th&gt;
      &lt;th&gt;Rape&lt;/th&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;State&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;Alabama&lt;/th&gt;
      &lt;td&gt;13.2&lt;/td&gt;
      &lt;td&gt;236&lt;/td&gt;
      &lt;td&gt;58&lt;/td&gt;
      &lt;td&gt;21.2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;Alaska&lt;/th&gt;
      &lt;td&gt;10.0&lt;/td&gt;
      &lt;td&gt;263&lt;/td&gt;
      &lt;td&gt;48&lt;/td&gt;
      &lt;td&gt;44.5&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;Arizona&lt;/th&gt;
      &lt;td&gt;8.1&lt;/td&gt;
      &lt;td&gt;294&lt;/td&gt;
      &lt;td&gt;80&lt;/td&gt;
      &lt;td&gt;31.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;Arkansas&lt;/th&gt;
      &lt;td&gt;8.8&lt;/td&gt;
      &lt;td&gt;190&lt;/td&gt;
      &lt;td&gt;50&lt;/td&gt;
      &lt;td&gt;19.5&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;California&lt;/th&gt;
      &lt;td&gt;9.0&lt;/td&gt;
      &lt;td&gt;276&lt;/td&gt;
      &lt;td&gt;91&lt;/td&gt;
      &lt;td&gt;40.6&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;h3 id=&#34;data-scaling&#34;&gt;Data Scaling&lt;/h3&gt;
&lt;p&gt;To make all the features comparable, we first need to scale them. In this case, we use the &lt;code&gt;sklearn.preprocessing.scale()&lt;/code&gt; function to normalize each variable to have zero mean and unit variance.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Scale data
X_scaled = pd.DataFrame(scale(df), index=df.index, columns=df.columns).values
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We will see later what are the practical implications of (not) scaling.&lt;/p&gt;
&lt;h3 id=&#34;fitting&#34;&gt;Fitting&lt;/h3&gt;
&lt;p&gt;Let&amp;rsquo;s fit PCA with 2 components.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Fit PCA with 2 components
pca2 = PCA(n_components=2).fit(X_scaled)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Get weights
weights = pca2.components_.T
df_weights = pd.DataFrame(weights, index=df.columns, columns=[&#39;PC1&#39;, &#39;PC2&#39;])
df_weights
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;PC1&lt;/th&gt;
      &lt;th&gt;PC2&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;Murder&lt;/th&gt;
      &lt;td&gt;0.535899&lt;/td&gt;
      &lt;td&gt;0.418181&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;Assault&lt;/th&gt;
      &lt;td&gt;0.583184&lt;/td&gt;
      &lt;td&gt;0.187986&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;UrbanPop&lt;/th&gt;
      &lt;td&gt;0.278191&lt;/td&gt;
      &lt;td&gt;-0.872806&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;Rape&lt;/th&gt;
      &lt;td&gt;0.543432&lt;/td&gt;
      &lt;td&gt;-0.167319&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;h3 id=&#34;projecting-the-data&#34;&gt;Projecting the data&lt;/h3&gt;
&lt;p&gt;What does the trasformed data looks like?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Transform X to get the principal components
X_dim2 = pca2.transform(X_scaled)
df_dim2 = pd.DataFrame(X_dim2, columns=[&#39;PC1&#39;, &#39;PC2&#39;], index=df.index)
df_dim2.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;PC1&lt;/th&gt;
      &lt;th&gt;PC2&lt;/th&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;State&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;Alabama&lt;/th&gt;
      &lt;td&gt;0.985566&lt;/td&gt;
      &lt;td&gt;1.133392&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;Alaska&lt;/th&gt;
      &lt;td&gt;1.950138&lt;/td&gt;
      &lt;td&gt;1.073213&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;Arizona&lt;/th&gt;
      &lt;td&gt;1.763164&lt;/td&gt;
      &lt;td&gt;-0.745957&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;Arkansas&lt;/th&gt;
      &lt;td&gt;-0.141420&lt;/td&gt;
      &lt;td&gt;1.119797&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;California&lt;/th&gt;
      &lt;td&gt;2.523980&lt;/td&gt;
      &lt;td&gt;-1.542934&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;h3 id=&#34;visualization&#34;&gt;Visualization&lt;/h3&gt;
&lt;p&gt;The advantage og PCA is that it allows us to see the variation in lower dimesions.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;make_figure_10_1a(df_dim2, df_weights)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/10_unsupervised_30_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;pca-and-spectral-analysis&#34;&gt;PCA and Spectral Analysis&lt;/h3&gt;
&lt;p&gt;In case you haven&amp;rsquo;t noticed, calculating principal components, is equivalent to calculating the eigenvectors of the design matrix $X&amp;rsquo;X$, i.e. the variance-covariance matrix of $X$. Indeed what we performed above is a decomposition of the variance of $X$ into orthogonal components.&lt;/p&gt;
&lt;p&gt;The constrained maximization problem above can be re-written in matrix notation as&lt;/p&gt;
&lt;p&gt;$$
\max \ \phi&#39; X&amp;rsquo;X \phi \quad \text{ s. t. } \quad \phi&#39;\phi = 1
$$&lt;/p&gt;
&lt;p&gt;Which has the following dual representation&lt;/p&gt;
&lt;p&gt;$$
\mathcal L (\phi, \lambda) = \phi&#39; X&amp;rsquo;X \phi - \lambda (\phi&#39;\phi - 1)
$$&lt;/p&gt;
&lt;p&gt;If we take the first order conditions&lt;/p&gt;
&lt;p&gt;$$
\begin{align}
&amp;amp; \frac{\partial \mathcal L}{\partial \lambda} = \phi&#39;\phi - 1 \
&amp;amp; \frac{\partial \mathcal L}{\partial \phi} = 2 X&amp;rsquo;X \phi - 2 \lambda \phi
\end{align}
$$&lt;/p&gt;
&lt;p&gt;Setting the derivatives to zero at the optimum, we get&lt;/p&gt;
&lt;p&gt;$$
\begin{align}
&amp;amp; \phi&#39;\phi = 1 \
&amp;amp; X&amp;rsquo;X \phi = \lambda \phi
\end{align}
$$&lt;/p&gt;
&lt;p&gt;Thus, $\phi$ is an &lt;strong&gt;eigenvector&lt;/strong&gt; of the covariance matrix $X&amp;rsquo;X$, and the maximizing vector will be the one associated with the largest &lt;strong&gt;eigenvalue&lt;/strong&gt; $\lambda$.&lt;/p&gt;
&lt;h3 id=&#34;eigenvalues-and-eigenvectors&#34;&gt;Eigenvalues and eigenvectors&lt;/h3&gt;
&lt;p&gt;We can now double-check it using &lt;code&gt;numpy&lt;/code&gt; linear algebra package.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;eigenval, eigenvec = np.linalg.eig(X_scaled.T @ X_scaled)
data = np.concatenate((eigenvec,eigenval.reshape(1,-1)))
idx = list(df.columns) + [&#39;Eigenvalue&#39;]
df_eigen = pd.DataFrame(data, index=idx, columns=[&#39;PC1&#39;, &#39;PC2&#39;,&#39;PC3&#39;,&#39;PC4&#39;])

df_eigen
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;PC1&lt;/th&gt;
      &lt;th&gt;PC2&lt;/th&gt;
      &lt;th&gt;PC3&lt;/th&gt;
      &lt;th&gt;PC4&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;Murder&lt;/th&gt;
      &lt;td&gt;0.535899&lt;/td&gt;
      &lt;td&gt;0.418181&lt;/td&gt;
      &lt;td&gt;0.649228&lt;/td&gt;
      &lt;td&gt;-0.341233&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;Assault&lt;/th&gt;
      &lt;td&gt;0.583184&lt;/td&gt;
      &lt;td&gt;0.187986&lt;/td&gt;
      &lt;td&gt;-0.743407&lt;/td&gt;
      &lt;td&gt;-0.268148&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;UrbanPop&lt;/th&gt;
      &lt;td&gt;0.278191&lt;/td&gt;
      &lt;td&gt;-0.872806&lt;/td&gt;
      &lt;td&gt;0.133878&lt;/td&gt;
      &lt;td&gt;-0.378016&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;Rape&lt;/th&gt;
      &lt;td&gt;0.543432&lt;/td&gt;
      &lt;td&gt;-0.167319&lt;/td&gt;
      &lt;td&gt;0.089024&lt;/td&gt;
      &lt;td&gt;0.817778&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;Eigenvalue&lt;/th&gt;
      &lt;td&gt;124.012079&lt;/td&gt;
      &lt;td&gt;49.488258&lt;/td&gt;
      &lt;td&gt;8.671504&lt;/td&gt;
      &lt;td&gt;17.828159&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;The spectral decomposition of the variance of $X$ generates a set of orthogonal vectors (eigenvectors) with different magnitudes (eigenvalues). The eigenvalues tell us the amount of variance of the data in that direction.&lt;/p&gt;
&lt;p&gt;If we combine the eigenvectors together, we form a projection matrix $P$ that we can use to transform the original variables: $\tilde X = P X$&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;X_transformed = X_scaled @ eigenvec
df_transformed = pd.DataFrame(X_transformed, index=df.index, columns=[&#39;PC1&#39;, &#39;PC2&#39;,&#39;PC3&#39;,&#39;PC4&#39;])

df_transformed.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;PC1&lt;/th&gt;
      &lt;th&gt;PC2&lt;/th&gt;
      &lt;th&gt;PC3&lt;/th&gt;
      &lt;th&gt;PC4&lt;/th&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;State&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;Alabama&lt;/th&gt;
      &lt;td&gt;0.985566&lt;/td&gt;
      &lt;td&gt;1.133392&lt;/td&gt;
      &lt;td&gt;0.156267&lt;/td&gt;
      &lt;td&gt;-0.444269&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;Alaska&lt;/th&gt;
      &lt;td&gt;1.950138&lt;/td&gt;
      &lt;td&gt;1.073213&lt;/td&gt;
      &lt;td&gt;-0.438583&lt;/td&gt;
      &lt;td&gt;2.040003&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;Arizona&lt;/th&gt;
      &lt;td&gt;1.763164&lt;/td&gt;
      &lt;td&gt;-0.745957&lt;/td&gt;
      &lt;td&gt;-0.834653&lt;/td&gt;
      &lt;td&gt;0.054781&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;Arkansas&lt;/th&gt;
      &lt;td&gt;-0.141420&lt;/td&gt;
      &lt;td&gt;1.119797&lt;/td&gt;
      &lt;td&gt;-0.182811&lt;/td&gt;
      &lt;td&gt;0.114574&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;California&lt;/th&gt;
      &lt;td&gt;2.523980&lt;/td&gt;
      &lt;td&gt;-1.542934&lt;/td&gt;
      &lt;td&gt;-0.341996&lt;/td&gt;
      &lt;td&gt;0.598557&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;This is exactly the dataset that we obtained before.&lt;/p&gt;
&lt;h3 id=&#34;scaling-the-variables&#34;&gt;Scaling the Variables&lt;/h3&gt;
&lt;p&gt;The results obtained when we perform PCA will also depend on whether the variables have been individually scaled. In fact, the variance of a variable depends on its magnitude.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Variables variance
df.var(axis=0)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Murder        18.970465
Assault     6945.165714
UrbanPop     209.518776
Rape          87.729159
dtype: float64
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Consequently, if we perform PCA on the unscaled variables, then the first principal component loading vector will have a very large loading for &lt;code&gt;Assault&lt;/code&gt;, since that variable has by far the highest variance.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Fit PCA with unscaled varaibles
X = df.values
pca2_u = PCA(n_components=2).fit(X)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Get weights
weights_u = pca2_u.components_.T
df_weights_u = pd.DataFrame(weights_u, index=df.columns, columns=[&#39;PC1&#39;, &#39;PC2&#39;])
df_weights_u
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;PC1&lt;/th&gt;
      &lt;th&gt;PC2&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;Murder&lt;/th&gt;
      &lt;td&gt;0.041704&lt;/td&gt;
      &lt;td&gt;0.044822&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;Assault&lt;/th&gt;
      &lt;td&gt;0.995221&lt;/td&gt;
      &lt;td&gt;0.058760&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;UrbanPop&lt;/th&gt;
      &lt;td&gt;0.046336&lt;/td&gt;
      &lt;td&gt;-0.976857&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;Rape&lt;/th&gt;
      &lt;td&gt;0.075156&lt;/td&gt;
      &lt;td&gt;-0.200718&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Transform X to get the principal components
X_dim2_u = pca2_u.transform(X)
df_dim2_u = pd.DataFrame(X_dim2_u, columns=[&#39;PC1&#39;, &#39;PC2&#39;], index=df.index)
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;plotting&#34;&gt;Plotting&lt;/h3&gt;
&lt;p&gt;We can compare the lower dimensional representations with and without scaling.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;make_figure_10_1b(df_dim2, df_dim2_u, df_weights, df_weights_u)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/10_unsupervised_49_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;As predicted, the first principal component loading vector places almost all of its weight on &lt;code&gt;Assault&lt;/code&gt;, while the second principal component loading vector places almost all of its weight on &lt;code&gt;UrpanPop&lt;/code&gt;. Comparing this to the left-hand plot, we see that scaling does indeed have a substantial effect on the results obtained. However, this result is simply a consequence of the scales on which the variables were measured.&lt;/p&gt;
&lt;h3 id=&#34;the-proportion-of-variance-explained&#34;&gt;The Proportion of Variance Explained&lt;/h3&gt;
&lt;p&gt;We can now ask a natural question: how much of the information in a given data set is lost by projecting the observations onto the first few principal components? That is, how much of the variance in the data is not contained in the first few principal components? More generally, we are interested in knowing the &lt;strong&gt;proportion of variance explained (PVE)&lt;/strong&gt; by each principal component.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Four components
pca4 = PCA(n_components=4).fit(X_scaled)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Variance of the four principal components
pca4.explained_variance_
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;array([2.53085875, 1.00996444, 0.36383998, 0.17696948])
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;interpretation&#34;&gt;Interpretation&lt;/h3&gt;
&lt;p&gt;We can compute it in percentage of the total variance.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# As a percentage of the total variance
pca4.explained_variance_ratio_
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;array([0.62006039, 0.24744129, 0.0891408 , 0.04335752])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In the &lt;code&gt;Arrest&lt;/code&gt; dataset, the first principal component explains $62.0%$ of the variance in the data, and the next principal component explains $24.7%$ of the variance. Together, the first two principal components explain almost $87%$ of the variance in the data, and the last two principal components explain only $13%$ of the variance.&lt;/p&gt;
&lt;h3 id=&#34;plotting-1&#34;&gt;Plotting&lt;/h3&gt;
&lt;p&gt;We can plot in a graph the percentage of the variance explained, relative to the number of components.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;make_figure_10_2(pca4)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/10_unsupervised_61_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;how-many-principal-components&#34;&gt;How Many Principal Components?&lt;/h3&gt;
&lt;p&gt;In general, a $n \times p$ data matrix $X$ has $\min{n − 1, p}$ distinct principal components. However, we usually are not interested in all of them; rather, we would like to use just the first few principal components in order to visualize or interpret the data.&lt;/p&gt;
&lt;p&gt;We typically decide on the number of principal components required to visualize the data by examining a &lt;em&gt;scree plot&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;However, there is no well-accepted objective way to decide how many principal com- ponents are enough.&lt;/p&gt;
&lt;h2 id=&#34;k-means-clustering&#34;&gt;K-Means Clustering&lt;/h2&gt;
&lt;p&gt;The idea behind K-means clustering is that a good clustering is one for which the within-cluster variation is as small as possible. Hence we want to solve the problem&lt;/p&gt;
&lt;p&gt;$$
\underset{C_{1}, \ldots, C_{K}}{\operatorname{minimize}} \Bigg\lbrace \sum_{k=1}^{K} W\left(C_{k}\right) \Bigg\rbrace
$$&lt;/p&gt;
&lt;p&gt;where $C_k$ is a cluster and $ W(C_k)$ is a measure of the amount by which the observations within a cluster differ from each other.&lt;/p&gt;
&lt;p&gt;There are many possible ways to define this concept, but by far the most common choice involves &lt;strong&gt;squared Euclidean distance&lt;/strong&gt;. That is, we define&lt;/p&gt;
&lt;p&gt;$$
W\left(C_{k}\right)=\frac{1}{\left|C_{k}\right|} \sum_{i, i^{\prime} \in C_{k}} \sum_{j=1}^{p}\left(x_{i j}-x_{i^{\prime} j}\right)^2
$$&lt;/p&gt;
&lt;p&gt;where $|C_k|$ denotes the number of observations in the $k^{th}$ cluster.&lt;/p&gt;
&lt;h3 id=&#34;algorithm&#34;&gt;Algorithm&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Randomly assign a number, from $1$ to $K$, to each of the observations. These serve as initial cluster assignments for the observations.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Iterate until the cluster assignments stop changing:&lt;/p&gt;
&lt;p&gt;a) For each of the $K$ clusters, compute the cluster centroid. The kth cluster centroid is the vector of the $p$ feature means for the observations in the $k^{th}$ cluster.&lt;/p&gt;
&lt;p&gt;b) Assign each observation to the cluster whose centroid is closest (where closest is defined using Euclidean distance).&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;generate-the-data&#34;&gt;Generate the data&lt;/h3&gt;
&lt;p&gt;We first generate a 2-dimensional dataset.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Simulate data
np.random.seed(123)
X = np.random.randn(50,2)
X[0:25, 0] = X[0:25, 0] + 3
X[0:25, 1] = X[0:25, 1] - 4
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;make_new_figure_1(X)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/10_unsupervised_71_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;step-1-random-assignement&#34;&gt;Step 1: random assignement&lt;/h3&gt;
&lt;p&gt;Now let&amp;rsquo;s randomly assign the data to two clusters, at random.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Init clusters
K = 2
clusters0 = np.random.randint(K,size=(np.size(X,0)))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;make_new_figure_2(X, clusters0)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/10_unsupervised_75_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;step-2-estimate-distributions&#34;&gt;Step 2: estimate distributions&lt;/h3&gt;
&lt;p&gt;What are the new centroids?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Compute new centroids
def compute_new_centroids(X, clusters):
    K = len(np.unique(clusters))
    centroids = np.zeros((K,np.size(X,1)))
    for k in range(K):
        if sum(clusters==k)&amp;gt;0:
            centroids[k,:] = np.mean(X[clusters==k,:], axis=0)
        else:
            centroids[k,:] = np.mean(X, axis=0)
    return centroids
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Print
centroids0 = compute_new_centroids(X, clusters0)
print(centroids0)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[[ 1.54179703 -1.65922379]
 [ 1.67917325 -2.36272948]]
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;plotting-the-centroids&#34;&gt;Plotting the centroids&lt;/h3&gt;
&lt;p&gt;Let&amp;rsquo;s add the centroids to the graph.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Plot
plot_assignment(X, centroids0, clusters0, 0, 0)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/10_unsupervised_82_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;step-3-assign-data-to-clusters&#34;&gt;Step 3: assign data to clusters&lt;/h3&gt;
&lt;p&gt;Now we can assign the data to the clusters, according to the closest centroid.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Assign X to clusters
def assign_to_cluster(X, centroids):
    K = np.size(centroids,0)
    dist = np.zeros((np.size(X,0),K))
    for k in range(K):
        dist[:,k] = np.mean((X - centroids[k,:])**2, axis=1)
    clusters = np.argmin(dist, axis=1)
    
    # Compute inertia
    inertia = 0
    for k in range(K):
        if sum(clusters==k)&amp;gt;0:
            inertia += np.sum((X[clusters==k,:] - centroids[k,:])**2)
    return clusters, inertia
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;plotting-assigned-data&#34;&gt;Plotting assigned data&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Get cluster assignment
[clusters1,d] = assign_to_cluster(X, centroids0)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Plot
plot_assignment(X, centroids0, clusters1, d, 1)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/10_unsupervised_88_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;full-algorithm&#34;&gt;Full Algorithm&lt;/h3&gt;
&lt;p&gt;We now have all the components to proceed iteratively.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def kmeans_manual(X, K):

    # Init
    i = 0
    d0 = 1e4
    d1 = 1e5
    clusters = np.random.randint(K,size=(np.size(X,0)))

    # Iterate until convergence
    while np.abs(d0-d1) &amp;gt; 1e-10:
        d1 = d0
        centroids = compute_new_centroids(X, clusters)
        [clusters, d0] = assign_to_cluster(X, centroids)
        plot_assignment(X, centroids, clusters, d0, i)
        i+=1
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;plotting-k-means-clustering&#34;&gt;Plotting k-means clustering&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Test
kmeans_manual(X, K)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/10_unsupervised_93_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Here the observations can be easily plotted because they are two-dimensional.
If there were more than two variables then we could instead perform PCA
and plot the first two principal components score vectors.&lt;/p&gt;
&lt;h3 id=&#34;more-clusters&#34;&gt;More clusters&lt;/h3&gt;
&lt;p&gt;In the previous example, we knew that there really were two clusters because
we generated the data. However, for real data, in general we do not know
the true number of clusters. We could instead have performed K-means
clustering on this example with &lt;code&gt;K  =  3&lt;/code&gt;. If we do this, K-means clustering will split up the two &amp;ldquo;real&amp;rdquo; clusters, since it has no information about them:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# K=3
kmeans_manual(X, 3)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/10_unsupervised_97_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;sklearn-package&#34;&gt;Sklearn package&lt;/h3&gt;
&lt;p&gt;The automated function in &lt;code&gt;sklearn&lt;/code&gt; to persorm $K$-means clustering is &lt;code&gt;KMeans&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# SKlearn algorithm
km1 = KMeans(n_clusters=3, n_init=1, random_state=1)
km1.fit(X)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;KMeans(n_clusters=3, n_init=1, random_state=1)
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;plotting-2&#34;&gt;Plotting&lt;/h3&gt;
&lt;p&gt;We can plot the asssignment generated by the &lt;code&gt;KMeans&lt;/code&gt; function.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Plot
plot_assignment(X, km1.cluster_centers_, km1.labels_, km1.inertia_, km1.n_iter_)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/10_unsupervised_103_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;As we can see, the results are different in the two algorithms? Why? $K$-means is susceptible to the initial values. One way to solve this problem is to run the algorithm multiple times and report only the best results&lt;/p&gt;
&lt;h3 id=&#34;initial-assignment&#34;&gt;Initial Assignment&lt;/h3&gt;
&lt;p&gt;To run the &lt;code&gt;Kmeans()&lt;/code&gt; function in python with multiple initial cluster assignments, we use the &lt;code&gt;n_init&lt;/code&gt; argument (default: 10). If a value of &lt;code&gt;n_init&lt;/code&gt; greater than one is used, then K-means clustering will be performed using multiple random assignments, and the &lt;code&gt;Kmeans()&lt;/code&gt; function will report only the best results.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# 30 runs
km_30run = KMeans(n_clusters=3, n_init=30, random_state=1).fit(X)
plot_assignment(X, km_30run.cluster_centers_, km_30run.labels_, km_30run.inertia_, km_30run.n_iter_)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/10_unsupervised_107_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;best-practices&#34;&gt;Best Practices&lt;/h3&gt;
&lt;p&gt;It is generally recommended to always run K-means clustering with a large value of &lt;code&gt;n_init&lt;/code&gt;, such as 20 or 50 to avoid getting stuck in an undesirable local optimum.&lt;/p&gt;
&lt;p&gt;When performing K-means clustering, in addition to using multiple initial cluster assignments, it is also important to set a random seed using the &lt;code&gt;random_state&lt;/code&gt; parameter. This way, the initial cluster assignments can be replicated, and the K-means output will be fully reproducible.&lt;/p&gt;
&lt;h2 id=&#34;hierarchical-clustering&#34;&gt;Hierarchical Clustering&lt;/h2&gt;
&lt;p&gt;One potential disadvantage of K-means clustering is that it requires us to pre-specify the number of clusters $K$.&lt;/p&gt;
&lt;p&gt;Hierarchical clustering is an alternative approach which does not require that we commit to a particular choice of $K$.&lt;/p&gt;
&lt;h3 id=&#34;the-dendogram&#34;&gt;The Dendogram&lt;/h3&gt;
&lt;p&gt;Hierarchical clustering has an added advantage over K-means clustering in that it results in an attractive tree-based representation of the observations, called a &lt;strong&gt;dendrogram&lt;/strong&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;d = dendrogram(
        linkage(X, &amp;quot;complete&amp;quot;),
        leaf_rotation=90.,  # rotates the x axis labels
        leaf_font_size=8.,  # font size for the x axis labels
    )
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/10_unsupervised_114_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;interpretation-1&#34;&gt;Interpretation&lt;/h3&gt;
&lt;p&gt;Each leaf of the &lt;em&gt;dendrogram&lt;/em&gt; represents one observation.&lt;/p&gt;
&lt;p&gt;As we move up the tree, some leaves begin to fuse into branches. These correspond to observations that are similar to each other. As we move higher up the tree, branches themselves fuse, either with leaves or other branches. The earlier (lower in the tree) fusions occur, the more similar the groups of observations are to each other.&lt;/p&gt;
&lt;p&gt;We can use de &lt;em&gt;dendogram&lt;/em&gt; to understand how similar two observations are: we can look for the point in the tree where branches containing those two obse rvations are first fused. The height of this fusion, as measured on the vertical axis, indicates how different the two observations are. Thus, observations that fuse at the very bottom of the tree are quite similar to each other, whereas observations that fuse close to the top of the tree will tend to be quite different.&lt;/p&gt;
&lt;p&gt;The term &lt;strong&gt;hierarchical&lt;/strong&gt; refers to the fact that clusters obtained by cutting the dendrogram at a given height are necessarily nested within the clusters obtained by cutting the dendrogram at any greater height.&lt;/p&gt;
&lt;h3 id=&#34;the-hierarchical-clustering-algorithm&#34;&gt;The Hierarchical Clustering Algorithm&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Begin with $n$ observations and a measure (such as Euclidean distance) of all the $n(n − 1)/2$ pairwise dissimilarities. Treat each 2 observation as its own cluster.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;For $i=n,n−1,&amp;hellip;,2$&lt;/p&gt;
&lt;p&gt;a) Examine all pairwise inter-cluster dissimilarities among the $i$ clusters and identify the &lt;strong&gt;pair of clusters that are least dissimilar&lt;/strong&gt; (that is, most similar). Fuse these two clusters. The dissimilarity between these two clusters indicates the height in the dendrogram at which the fusion should be placed.&lt;/p&gt;
&lt;p&gt;b) Compute the new pairwise inter-cluster dissimilarities among the $i−1$ remaining clusters.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;the-linkage-function&#34;&gt;The Linkage Function&lt;/h3&gt;
&lt;p&gt;We have a concept of the dissimilarity between pairs of observations, but how do we define the dissimilarity between two clusters if one or both of the clusters contains multiple observations?&lt;/p&gt;
&lt;p&gt;The concept of dissimilarity between a pair of observations needs to be extended to a pair of groups of observations. This extension is achieved by developing the notion of &lt;strong&gt;linkage&lt;/strong&gt;, which defines the dissimilarity between two groups of observations.&lt;/p&gt;
&lt;h3 id=&#34;linkages&#34;&gt;Linkages&lt;/h3&gt;
&lt;p&gt;The four most common types of linkage are:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Complete&lt;/strong&gt;: Maximal intercluster dissimilarity. Compute all pairwise dissimilarities between the observations in cluster A and the observations in cluster B, and record the largest of these dissimilarities.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Single&lt;/strong&gt;: Minimal intercluster dissimilarity. Compute all pairwise dissimilarities between the observations in cluster A and the observations in cluster B, and record the smallest of these dissimilarities.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Average&lt;/strong&gt;: Mean intercluster dissimilarity. Compute all pairwise dissimilarities between the observations in cluster A and the observations in cluster B, and record the average of these dissimilarities.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Centroid&lt;/strong&gt;: Dissimilarity between the centroid for cluster A (a mean vector of length p) and the centroid for cluster B. Centroid linkage can result in undesirable inversions.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Average, complete, and single linkage are most popular among statisticians. Average and complete linkage are generally preferred over single linkage, as they tend to yield more balanced dendrograms. Centroid linkage is often used in genomics, but suffers from a major drawback in that an inversion can occur, whereby two clusters are fused at a height below either of the individual clusters in the dendrogram. This can lead to difficulties in visualization as well as in interpretation of the dendrogram.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Init
linkages = [hierarchy.complete(X), hierarchy.average(X), hierarchy.single(X)]
titles = [&#39;Complete Linkage&#39;, &#39;Average Linkage&#39;, &#39;Single Linkage&#39;]
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;plotting-3&#34;&gt;Plotting&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;make_new_figure_4(linkages, titles)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/10_unsupervised_126_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;For this data, both &lt;em&gt;complete&lt;/em&gt; and &lt;em&gt;average&lt;/em&gt; linkage generally separates the observations into their correct groups.&lt;/p&gt;
&lt;h2 id=&#34;gaussian-mixture-models&#34;&gt;Gaussian Mixture Models&lt;/h2&gt;
&lt;p&gt;Clustering methods such as hierarchical clustering and K-means are based on heuristics and rely primarily on finding clusters whose members are close to one another, as measured directly with the data (no probability model involved).&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Gaussian Mixture Models&lt;/em&gt; assume that the data was generated by multiple multivariate gaussian distributions. The objective of the algorithm is to recover these latent distributions.&lt;/p&gt;
&lt;p&gt;The advantages with respect to K-means are&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;a structural interpretaion of the parameters&lt;/li&gt;
&lt;li&gt;automatically generates class probabilities&lt;/li&gt;
&lt;li&gt;can generate clusters of observations that are not necessarily close to each other&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;algorithm-1&#34;&gt;Algorithm&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Randomly assign a number, from $1$ to $K$, to each of the observations. These serve as initial cluster assignments for the observations.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Iterate until the cluster assignments stop changing:&lt;/p&gt;
&lt;p&gt;a) For each of the $K$ clusters, compute its mean and variance. The main difference with K-means is that we also compute the variance matrix.&lt;/p&gt;
&lt;p&gt;b) Assign each observation to its most likely cluster.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;dataset&#34;&gt;Dataset&lt;/h3&gt;
&lt;p&gt;Let&amp;rsquo;s use the same data we have used for k-means, for a direct comparison.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;make_new_figure_1(X)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/10_unsupervised_134_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;step-1-random-assignement-1&#34;&gt;Step 1: random assignement&lt;/h3&gt;
&lt;p&gt;Let&amp;rsquo;s also use the same random assignment of the K-means algorithm.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;make_new_figure_2(X, clusters0)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/10_unsupervised_137_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;step-2-compute-distirbutions&#34;&gt;Step 2: compute distirbutions&lt;/h3&gt;
&lt;p&gt;What are the new distributions?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Compute new centroids
def compute_distributions(X, clusters):
    K = len(np.unique(clusters))
    distr = []
    for k in range(K):
        if sum(clusters==k)&amp;gt;0:
            distr += [multivariate_normal(np.mean(X[clusters==k,:], axis=0), np.cov(X[clusters==k,:].T))]
        else:
            distr += [multivariate_normal(np.mean(X, axis=0), np.cov(X.T))]
    return distr
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Print
distr0 = compute_distributions(X, clusters0)
print(&amp;quot;Mean of the first distribution: \n&amp;quot;, distr0[0].mean)
print(&amp;quot;\nVariance of the first distribution: \n&amp;quot;, distr0[0].cov)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Mean of the first distribution: 
 [ 1.54179703 -1.65922379]

Variance of the first distribution: 
 [[ 3.7160256  -2.27290036]
 [-2.27290036  4.67223237]]
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;plotting-the-distributions&#34;&gt;Plotting the distributions&lt;/h3&gt;
&lt;p&gt;Let&amp;rsquo;s add the distributions to the graph.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Plot
plot_assignment_gmm(X, clusters0, distr0, i=0, logL=0.0)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/10_unsupervised_144_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;likelihood&#34;&gt;Likelihood&lt;/h3&gt;
&lt;p&gt;The main difference with respect with K-means is that we can now compute the probability that each observation belongs to each cluster. This is the probability that each observation was generated by one of the two bi-variate normal distributions. These probabilities are called &lt;strong&gt;likelihoods&lt;/strong&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Print first 5 likelihoods
pdfs0 = np.stack([d.pdf(X) for d in distr0], axis=1)
pdfs0[:5]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;array([[0.03700522, 0.05086876],
       [0.00932081, 0.02117353],
       [0.04092453, 0.04480732],
       [0.00717854, 0.00835799],
       [0.01169199, 0.01847373]])
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;step-3-assign-data-to-clusters-1&#34;&gt;Step 3: assign data to clusters&lt;/h3&gt;
&lt;p&gt;Now we can assign the data to the clusters, via maximum likelihood.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Assign X to clusters
def assign_to_cluster_gmm(X, distr):
    pdfs = np.stack([d.pdf(X) for d in distr], axis=1)
    clusters = np.argmax(pdfs, axis=1)
    log_likelihood = 0
    for k, pdf in enumerate(pdfs):
        log_likelihood += np.log(pdf[clusters[k]])
    return clusters, log_likelihood
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Get cluster assignment
clusters1, logL1 = assign_to_cluster_gmm(X, distr0)
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;plotting-assigned-data-1&#34;&gt;Plotting assigned data&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Compute new distributions
distr1 = compute_distributions(X, clusters1)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Plot
plot_assignment_gmm(X, clusters1, distr1, 1, logL1);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/10_unsupervised_154_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;expectation---maximization&#34;&gt;Expectation - Maximization&lt;/h3&gt;
&lt;p&gt;The two steps we have just seen, are part of a broader family of algorithms to maximize likelihoods called &lt;strong&gt;expectation&lt;/strong&gt;-&lt;strong&gt;maximization&lt;/strong&gt; algorithms.&lt;/p&gt;
&lt;p&gt;In the expectation step, we computed the expectation of the parameters, given the current cluster assignment.&lt;/p&gt;
&lt;p&gt;In the maximization step, we assigned observations to the cluster that maximized the likelihood of the single observation.&lt;/p&gt;
&lt;p&gt;The alternative, and more computationally intensive procedure, would have been to specify a global likelihood function and find the mean and variance paramenters of the two normal distributions that maximized those likelihoods.&lt;/p&gt;
&lt;h3 id=&#34;full-algorithm-1&#34;&gt;Full Algorithm&lt;/h3&gt;
&lt;p&gt;We can now deploy the full algorithm.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def gmm_manual(X, K):

    # Init
    i = 0
    logL0 = 1e4
    logL1 = 1e5
    clusters = np.random.randint(K,size=(np.size(X,0)))

    # Iterate until convergence
    while np.abs(logL0-logL1) &amp;gt; 1e-10:
        logL1 = logL0
        distr = compute_distributions(X, clusters)
        clusters, logL0 = assign_to_cluster_gmm(X, distr)
        plot_assignment_gmm(X, clusters, distr, i, logL0)
        i+=1
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;plotting-k-means-clustering-1&#34;&gt;Plotting k-means clustering&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Test
gmm_manual(X, K)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/10_unsupervised_161_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;In this case, GMM does a very poor job identifying the original clusters.&lt;/p&gt;
&lt;h3 id=&#34;overlapping-clusters&#34;&gt;Overlapping Clusters&lt;/h3&gt;
&lt;p&gt;Let&amp;rsquo;s now try with a different dataset, where the data is drawn from two overlapping bi-variate gaussian distributions, forming a cross.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Simulate data
X = np.random.randn(50,2)
X[0:25, :] = np.random.multivariate_normal([0,0], [[50,0],[0,1]], size=25)
X[25:, :] = np.random.multivariate_normal([0,0], [[1,0],[0,50]], size=25)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;make_new_figure_1(X)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/10_unsupervised_166_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;gmm-with-overlapping-distributions&#34;&gt;GMM with overlapping distributions&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# GMM
gmm_manual(X, K)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/10_unsupervised_168_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;As we can see, GMM is able to correctly recover the original clusters.&lt;/p&gt;
&lt;h3 id=&#34;k-means-with-overlapping-distributions&#34;&gt;K-means with overlapping distributions&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# K-means
kmeans_manual(X, K)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/10_unsupervised_171_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;K-means generates completely different clusters.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Coding: Logit Demand</title>
      <link>https://matteocourthoud.github.io/course/empirical-io/11_logit_demand/</link>
      <pubDate>Wed, 10 Nov 2021 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/course/empirical-io/11_logit_demand/</guid>
      <description>&lt;h3 id=&#34;intro&#34;&gt;Intro&lt;/h3&gt;
&lt;p&gt;In this session, I am going to cover demand estimation.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Compute equilibrium outcomes with Logit demand&lt;/li&gt;
&lt;li&gt;Simulate a dataset&lt;/li&gt;
&lt;li&gt;Estimate Logit demand&lt;/li&gt;
&lt;li&gt;Compare different instruments&lt;/li&gt;
&lt;li&gt;Include supply&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;model&#34;&gt;Model&lt;/h3&gt;
&lt;p&gt;In this first part, we are going to assume that consumer
$i \in \lbrace1,&amp;hellip;,I\rbrace$ utility from good
$j \in \lbrace1,&amp;hellip;,J\rbrace$ in market $t \in \lbrace1,&amp;hellip;,T\rbrace$
takes the form&lt;/p&gt;
&lt;p&gt;$$
u_{ijt} = \boldsymbol x_{jt} \boldsymbol \beta_{it} - \alpha p_{jt} + \xi_{jt} + \epsilon_{ijt}
$$&lt;/p&gt;
&lt;p&gt;where&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\xi_{jt}$ is type-1 extreme value distributed&lt;/li&gt;
&lt;li&gt;$\boldsymbol \beta$ has dimension $K$
&lt;ul&gt;
&lt;li&gt;i.e. goods have $K$ characteristics&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;setup&#34;&gt;Setup&lt;/h3&gt;
&lt;p&gt;We have $J$ firms and each product has $K$ characteristics&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;J = 3;                            # 3 firms == products
K = 2;                            # 2 product characteristics
c = rand(Uniform(0, 1), J);       # Random uniform marginal costs
ξ = rand(Normal(0, 1), J+1);      # Random normal individual shocks
X = rand(Exponential(1), J, K);   # Random exponential product characteristics
β = [.5, 2, -1];                  # Preferences (last one is for prices, i.e. alpha)
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;code-demand&#34;&gt;Code Demand&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;function demand(p::Vector, X::Matrix, β::Vector, ξ::Vector)::Tuple{Vector, Number}
    &amp;quot;&amp;quot;&amp;quot;Compute demand&amp;quot;&amp;quot;&amp;quot;
    δ = 1 .+ [X p] * β              # Mean value
    u = [δ; 0] + ξ                  # Utility
    e = exp.(u)                     # Take exponential
    q = e ./ sum(e)                 # Compute demand
    return q[1:end-1], q[end]
end;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can try with an example.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;p = 2 .* c;
demand(p, X, β, ξ)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## ([0.4120077746005573, 0.26650568009936, 0.24027826270165709], 0.08120828259842561)
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;code-supply&#34;&gt;Code Supply&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;function profits(p::Vector, c::Vector, X::Matrix, β::Vector, ξ::Vector)::Vector
    &amp;quot;&amp;quot;&amp;quot;Compute profits&amp;quot;&amp;quot;&amp;quot;
    q, _ = demand(p, X, β, ξ)       # Compute demand
    pr = (p - c) .* q               # Compute profits
    return pr
end;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can try with an example.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;profits(p, c, X, β, ξ)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 3-element Array{Float64,1}:
##  0.20289186252172428
##  0.1596422305025479
##  0.1270874470740512
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;code-best-reply&#34;&gt;Code Best Reply&lt;/h3&gt;
&lt;p&gt;We first code the best reply of firm $j$&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;function profits_j(pj::Number, j::Int, p::Vector, c::Vector, X::Matrix, β::Vector, ξ::Vector)::Number
    &amp;quot;&amp;quot;&amp;quot;Compute profits of firm j&amp;quot;&amp;quot;&amp;quot;
    p[j] = pj                       # Insert price of firm j
    pr = profits(p, c, X, β, ξ)     # Compute profits
    return pr[j]
end;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s test it.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;j = 1;
obj_fun(pj) = - profits_j(pj[1], j, copy(p), c, X, β, ξ);
pj = optimize(x -&amp;gt; obj_fun(x), [1.0], LBFGS()).minimizer[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 1.8019637881982011
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What are the implied profits now?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;print(&amp;quot;Profits old: &amp;quot;,  round.(profits(p, c, X, β, ξ), digits=4))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Profits old: [0.2029, 0.1596, 0.1271]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;p_new = copy(p);
p_new[j] = pj;
print(&amp;quot;Profits new: &amp;quot;,  round.(profits(p_new, c, X, β, ξ), digits=4))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Profits new: [0.3095, 0.2073, 0.1651]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Indeed firm 1 has increased its profits.&lt;/p&gt;
&lt;h3 id=&#34;code-equilibrium&#34;&gt;Code Equilibrium&lt;/h3&gt;
&lt;p&gt;We can now compute equilibrium prices&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;function equilibrium(c::Vector, X::Matrix, β::Vector, ξ::Vector)::Vector
    &amp;quot;&amp;quot;&amp;quot;Compute equilibrium prices and profits&amp;quot;&amp;quot;&amp;quot;
    p = 2 .* c;
    dist = 1;
    iter = 0;

    # Until convergence
    while (dist &amp;gt; 1e-8) &amp;amp;&amp;amp; (iter&amp;lt;1000)

        # Compute best reply for each firm
        p1 = copy(p);
        for j=1:length(p)
            obj_fun(pj) = - profits_j(pj[1], j, p, c, X, β, ξ);
            optimize(x -&amp;gt; obj_fun(x), [1.0], LBFGS()).minimizer[1];
        end

        # Update distance
        dist = max(abs.(p - p1)...);
        iter += 1;
    end
    return p
end;
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;code-equilibrium-1&#34;&gt;Code Equilibrium&lt;/h3&gt;
&lt;p&gt;Let’s test it&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# Compute equilibrium prices
p_eq = equilibrium(c, X, β, ξ);
print(&amp;quot;Equilibrium prices: &amp;quot;,  round.(p_eq, digits=4))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Equilibrium prices: [1.9764, 1.9602, 1.8366]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# And profits
pi_eq = profits(p_eq, c, X, β, ξ);
print(&amp;quot;Equilibrium profits: &amp;quot;,  round.(pi_eq, digits=4))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Equilibrium profits: [0.484, 0.3612, 0.3077]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As expected the prices of the first 2 firms are lower and their profits
are higher.&lt;/p&gt;
&lt;h3 id=&#34;dgp&#34;&gt;DGP&lt;/h3&gt;
&lt;p&gt;Let’s generate our Data Generating Process (DGP).&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\boldsymbol x \sim exp(V_{x})$&lt;/li&gt;
&lt;li&gt;$\xi \sim N(0, V_{\xi})$&lt;/li&gt;
&lt;li&gt;$w \sim N(0, 1)$&lt;/li&gt;
&lt;li&gt;$\omega \sim N(0, 1)$&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;function draw_data(J::Int, K::Int, rangeJ::Vector, varX::Number, varξ::Number)::Tuple
    &amp;quot;&amp;quot;&amp;quot;Draw data for one market&amp;quot;&amp;quot;&amp;quot;
    J_ = rand(rangeJ[1]:rangeJ[2])              # Number of firms (products)
    X_ = rand(Exponential(varX), J_, K)         # Product characteristics
    ξ_ = rand(Normal(0, varξ), J_+1)            # Product-level utility shocks
    w_ = rand(Uniform(0, 1), J_)                # Cost shifters
    ω_ = rand(Uniform(0, 1), J_)                # Cost shocks
    c_ = w_ + ω_                                # Cost
    j_ = sort(sample(1:J, J_, replace=false))   # Subset of firms
    return X_, ξ_, w_, c_, j_
end;
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;equilibrium&#34;&gt;Equilibrium&lt;/h3&gt;
&lt;p&gt;We first compute the equilibrium in one market.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;function compute_mkt_eq(J::Int, b::Vector, rangeJ::Vector, varX::Number, varξ::Number)::DataFrame
    &amp;quot;&amp;quot;&amp;quot;Compute equilibrium one market&amp;quot;&amp;quot;&amp;quot;

    # Initialize variables
    K = size(β, 1) - 1
    X_, ξ_, w_, c_, j_ = draw_data(J, K, rangeJ, varX, varξ)

    # Compute equilibrium
    p_ = equilibrium(c_, X_, β, ξ_)      # Equilibrium prices
    q_, q0 = demand(p_, X_, β, ξ_)       # Demand with shocks
    pr_ = (p_ - c_) .* q_               # Profits

    # Save to data
    q0_ = ones(length(j_)) .* q0
    df = DataFrame(j=j_, w=w_, p=p_, q=q_, q0=q0_, pr=pr_)
    for k=1:K
      df[!,&amp;quot;x$k&amp;quot;] = X_[:,k]
      df[!,&amp;quot;z$k&amp;quot;] = sum(X_[:,k]) .- X_[:,k]
    end
    return df
end;
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;simulate-dataset&#34;&gt;Simulate Dataset&lt;/h3&gt;
&lt;p&gt;We can now write the code to simulate the whole dataset.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;function simulate_data(J::Int, b::Vector, T::Int, rangeJ::Vector, varX::Number, varξ::Number)
    &amp;quot;&amp;quot;&amp;quot;Simulate full dataset&amp;quot;&amp;quot;&amp;quot;
    df = compute_mkt_eq(J, β, rangeJ, varX, varξ)
    df[!, &amp;quot;t&amp;quot;] = ones(nrow(df)) * 1
    for t=2:T
        df_temp = compute_mkt_eq(J, β, rangeJ, varX, varξ)
        df_temp[!, &amp;quot;t&amp;quot;] = ones(nrow(df_temp)) * t
        append!(df, df_temp)
    end
    CSV.write(&amp;quot;../data/logit.csv&amp;quot;, df)
end;
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;simulate-dataset-2&#34;&gt;Simulate Dataset (2)&lt;/h3&gt;
&lt;p&gt;We generate the dataset by simulating many markets that differ by&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;number of firms (and their identity)&lt;/li&gt;
&lt;li&gt;their marginal costs&lt;/li&gt;
&lt;li&gt;their product characteristics&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# Set parameters
J = 10;                 # Number of firms
K = 2;                  # Product caracteristics
T = 500;                # Markets
β = [.5, 2, -1];        # Preferences
rangeJ = [2, 6];        # Min and max firms per market
varX = 1;               # Variance of X
varξ = 2;               # Variance of xi

# Simulate
df = simulate_data(J, β, T, rangeJ, varX, varξ);
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;the-data&#34;&gt;The Data&lt;/h3&gt;
&lt;p&gt;What does the data look like? Let’s switch to R!&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Read data
df = fread(&amp;quot;../data/logit.csv&amp;quot;)
kable(df[1:6,], digits=4)
&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:right&#34;&gt;j&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;w&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;p&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;q&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;q0&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;pr&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;x1&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;z1&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;x2&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;z2&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;t&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right&#34;&gt;1&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;0.1491&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;1.9616&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;0.0932&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;0.0013&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;0.1028&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;2.4517&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;0.8219&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;1.6918&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;3.6779&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right&#34;&gt;2&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;0.8352&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;2.1112&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;0.0193&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;0.0013&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;0.0197&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;0.1328&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;3.1408&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;0.2075&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;5.1622&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right&#34;&gt;7&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;0.2749&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;2.2789&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;0.2710&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;0.0013&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;0.3717&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;0.1449&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;3.1287&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;1.1493&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;4.2205&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right&#34;&gt;10&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;0.4118&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;3.6386&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;0.6151&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;0.0013&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;1.5982&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;0.5442&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;2.7294&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;2.3212&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;3.0485&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right&#34;&gt;5&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;0.6071&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;2.1457&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;0.0886&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;0.0003&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;0.0972&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;0.9239&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;2.4182&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;3.6551&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;10.0474&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right&#34;&gt;6&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;0.1615&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;1.1626&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;0.0000&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;0.0003&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;0.0000&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;0.1763&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;3.1657&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;0.3629&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;13.3396&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id=&#34;estimation&#34;&gt;Estimation&lt;/h3&gt;
&lt;p&gt;First we need to compute the dependent variable&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;df$y = log(df$q) - log(df$q0)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we can estimate the logit model. The true values are $alpha=1$.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;ols &amp;lt;- lm(y ~ x1 + x2 + p, data=df)
kable(tidy(ols), digits=4)
&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left&#34;&gt;term&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;estimate&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;std.error&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;statistic&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;p.value&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;(Intercept)&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;-1.3558&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;0.1476&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;-9.1874&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;0e+00&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;x1&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;0.4176&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;0.0537&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;7.7782&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;0e+00&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;x2&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;1.1494&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;0.0719&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;15.9903&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;0e+00&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;p&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;0.2406&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;0.0656&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;3.6664&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;3e-04&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The estimate of $\alpha = 1$ is biased (positive and significant) since
$p$ is endogenous. We need instruments.&lt;/p&gt;
&lt;h3 id=&#34;iv-1-cost-shifters&#34;&gt;IV 1: Cost Shifters&lt;/h3&gt;
&lt;p&gt;First set of instruments: &lt;strong&gt;cost shifters&lt;/strong&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;fm_costiv &amp;lt;- ivreg(y ~ x1 + x2 + p | x1 + x2 + w, data=df)
kable(tidy(fm_costiv), digits=4)
&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left&#34;&gt;term&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;estimate&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;std.error&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;statistic&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;p.value&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;(Intercept)&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;0.2698&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;0.4923&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;0.5480&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;0.5837&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;x1&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;0.5249&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;0.0643&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;8.1679&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;0.0000&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;x2&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;1.8178&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;0.2064&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;8.8059&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;0.0000&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;p&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;-0.7034&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;0.2800&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;-2.5123&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;0.0121&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Now the estimate of $\alpha$ is negative and significant.&lt;/p&gt;
&lt;h3 id=&#34;iv-2-blp-instruments&#34;&gt;IV 2: BLP Instruments&lt;/h3&gt;
&lt;p&gt;Second set of instruments: &lt;strong&gt;product characteristics of other firms in
the same market&lt;/strong&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;fm_blpiv &amp;lt;- ivreg(y ~ x1 + x2 + p | x1 + x2 + z1 + z2, data=df)
kable(tidy(fm_blpiv), digits=4)
&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left&#34;&gt;term&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;estimate&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;std.error&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;statistic&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;p.value&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;(Intercept)&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;1.6616&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;0.5014&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;3.3139&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;9e-04&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;x1&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;0.6167&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;0.0698&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;8.8380&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;0e+00&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;x2&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;2.3901&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;0.2110&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;11.3279&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;0e+00&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;p&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;-1.5117&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;0.2840&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;-5.3221&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;0e+00&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Also the BLP instruments deliver an estimate of $\alpha$ is negative and
significant.&lt;/p&gt;
&lt;h2 id=&#34;appendix&#34;&gt;Appendix&lt;/h2&gt;
&lt;h3 id=&#34;references&#34;&gt;References&lt;/h3&gt;
</description>
    </item>
    
    <item>
      <title>Coding: BLP (1995)</title>
      <link>https://matteocourthoud.github.io/course/empirical-io/12_blp_1995/</link>
      <pubDate>Fri, 29 Oct 2021 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/course/empirical-io/12_blp_1995/</guid>
      <description>&lt;h3 id=&#34;intro&#34;&gt;Intro&lt;/h3&gt;
&lt;p&gt;In this session, I am going to cover demand estimation.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Compute equilibrium outcomes with RCL demand&lt;/li&gt;
&lt;li&gt;Simulate market-level data
&lt;ul&gt;
&lt;li&gt;Extremely similar to the logit demand simulation&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Build the BLP estimator from Berry, Levinsohn, and Pakes
(&lt;a href=&#34;#ref-berry1995automobile&#34;&gt;1995&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;model&#34;&gt;Model&lt;/h3&gt;
&lt;p&gt;In this first part, we are going to assume that consumer
$i \in \lbrace1,&amp;hellip;,I\rbrace$ utility from good
$j \in \lbrace1,&amp;hellip;,J\rbrace$ in market $t \in \lbrace1,&amp;hellip;,T\rbrace$
takes the form&lt;/p&gt;
&lt;p&gt;$$
u_{ijt} = \boldsymbol x_{jt} \boldsymbol \beta_{it} - \alpha p_{jt} + \xi_{jt} + \epsilon_{ijt}
$$&lt;/p&gt;
&lt;p&gt;where&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\xi_{jt}$ is type-1 extreme value distributed&lt;/li&gt;
&lt;li&gt;$\boldsymbol \beta_{it}$: has dimension $K$
$$\beta_{it}^k = \beta_0^k + \sigma_k \zeta_{it}^k$$
&lt;ul&gt;
&lt;li&gt;$\beta_0^k$: fixed taste for characteristic $k$ (the usual
$\beta$)&lt;/li&gt;
&lt;li&gt;$\zeta_{it}^k$: random taste, i.i.d. across consumers and
markets $t$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;setup&#34;&gt;Setup&lt;/h3&gt;
&lt;p&gt;We have $J$ firms and each product has $K$ characteristics.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;i = 100;                # Number of consumers
J = 10;                 # Number of firms
K = 2;                  # Product characteristics
T = 100;                # Number of markets
β = [.5, 2, -1];        # Preferences
varζ = 5;               # Variance of the random taste
rangeJ = [2, 6];        # Min and max firms per market
varX = 1;               # Variance of X
varξ = 2;               # Variance of xi
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;demand&#34;&gt;Demand&lt;/h3&gt;
&lt;p&gt;Demand is the main difference w.r.t. the logit model. Now we have
individual shocks $\zeta$ we have to integrate over.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;function demand(p::Vector, X::Matrix, β::Vector, ξ::Matrix, ζ::Matrix)::Tuple{Vector, Number}
    &amp;quot;&amp;quot;&amp;quot;Compute demand&amp;quot;&amp;quot;&amp;quot;
    δ = [X p] * (β .+ ζ)                    # Mean value
    δ0 = zeros(1, size(ζ, 2))               # Mean value of the outside option
    u = [δ; δ0] + ξ                         # Utility
    e = exp.(u)                             # Take exponential
    q = mean(e ./ sum(e, dims=1), dims=2)   # Compute demand
    return q[1:end-1], q[end]
end;
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;supply&#34;&gt;Supply&lt;/h3&gt;
&lt;p&gt;Computing profits is instead exactly the same as before. We just have to
save the shocks $\zeta$ to be sure demand is stable.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;function profits(p::Vector, c::Vector, X::Matrix, β::Vector, ξ::Matrix, ζ::Matrix)::Vector
    &amp;quot;&amp;quot;&amp;quot;Compute profits&amp;quot;&amp;quot;&amp;quot;
    q, _ = demand(p, X, β, ξ, ζ)            # Compute demand
    pr = (p - c) .* q                       # Compute profits
    return pr
end;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;function profits_j(pj::Number, j::Int, p::Vector, c::Vector, X::Matrix, β::Vector, ξ::Matrix, ζ::Matrix)::Number
    &amp;quot;&amp;quot;&amp;quot;Compute profits of firm j&amp;quot;&amp;quot;&amp;quot;
    p[j] = pj                               # Insert price of firm j
    pr = profits(p, c, X, β, ξ, ζ)          # Compute profits
    return pr[j]
end;
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;equilibrium&#34;&gt;Equilibrium&lt;/h3&gt;
&lt;p&gt;We can now compute the equilibrium for a specific market, as before.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;function equilibrium(c::Vector, X::Matrix, β::Vector, ξ::Matrix, ζ::Matrix)::Vector
    &amp;quot;&amp;quot;&amp;quot;Compute equilibrium prices and profits&amp;quot;&amp;quot;&amp;quot;
    p = 2 .* c;
    dist = 1;
    iter = 0;

    # Iterate until convergence
    while (dist &amp;gt; 1e-8) &amp;amp;&amp;amp; (iter&amp;lt;1000)

        # Compute best reply for each firm
        p_old = copy(p);
        for j=1:length(p)
            obj_fun(pj) = - profits_j(pj[1], j, p, c, X, β, ξ, ζ);
            optimize(x -&amp;gt; obj_fun(x), [1.0], LBFGS());
        end

        # Update distance
        dist = max(abs.(p - p_old)...);
        iter += 1;
    end
    return p
end;
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;simulating-data&#34;&gt;Simulating Data&lt;/h3&gt;
&lt;p&gt;We are now ready to simulate the data, i.e. equilibrium outcomes across
different markets. We first draw all the variables.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;function draw_data(I::Int, J::Int, K::Int, rangeJ::Vector, varζ::Number, varX::Number, varξ::Number)::Tuple
    &amp;quot;&amp;quot;&amp;quot;Draw data for one market&amp;quot;&amp;quot;&amp;quot;
    J_ = rand(rangeJ[1]:rangeJ[2])              # Number of firms (products)
    X_ = rand(Exponential(varX), J_, K)         # Product characteristics
    ξ_ = rand(Normal(0, varξ), J_+1, I)         # Product-level utility shocks
    # Consumer-product-level preference shocks
    ζ_ = [rand(Normal(0,1), 1, I) * varζ; zeros(K,I)]
    w_ = rand(Uniform(0, 1), J_)                # Cost shifters
    ω_ = rand(Uniform(0, 1), J_)                # Cost shocks
    c_ = w_ + ω_                                # Cost
    j_ = sort(sample(1:J, J_, replace=false))   # Subset of firms
    return X_, ξ_, ζ_, w_, c_, j_
end;
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;simulating-data-1&#34;&gt;Simulating Data&lt;/h3&gt;
&lt;p&gt;Then we simulate the data for one market.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;function compute_mkt_eq(I::Int, J::Int, β::Vector, rangeJ::Vector, varζ::Number, varX::Number, varξ::Number)::DataFrame
    &amp;quot;&amp;quot;&amp;quot;Compute equilibrium one market&amp;quot;&amp;quot;&amp;quot;

    # Initialize variables
    K = size(β, 1) - 1
    X_, ξ_, ζ_, w_, c_, j_ = draw_data(I, J, K, rangeJ, varζ, varX, varξ)

    # Compute equilibrium
    p_ = equilibrium(c_, X_, β, ξ_, ζ_)    # Equilibrium prices
    q_, q0 = demand(p_, X_, β, ξ_, ζ_)     # Demand with shocks
    pr_ = (p_ - c_) .* q_                       # Profits

    # Save to data
    q0_ = ones(length(j_)) .* q0
    df = DataFrame(j=j_, w=w_, p=p_, q=q_, q0=q0_, pr=pr_)
    for k=1:K
      df[!,&amp;quot;x$k&amp;quot;] = X_[:,k]
      df[!,&amp;quot;z$k&amp;quot;] = sum(X_[:,k]) .- X_[:,k]
    end
    return df
end;
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;simultate-the-data-2&#34;&gt;Simultate the Data (2)&lt;/h3&gt;
&lt;p&gt;We repeat for $T$ markets.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;function simulate_data(I::Int, J::Int, β::Vector, T::Int, rangeJ::Vector, varζ::Number, varX::Number, varξ::Number)
    &amp;quot;&amp;quot;&amp;quot;Simulate full dataset&amp;quot;&amp;quot;&amp;quot;
    df = compute_mkt_eq(I, J, β, rangeJ, varζ, varX, varξ)
    df[!, &amp;quot;t&amp;quot;] = ones(nrow(df)) * 1
    for t=2:T
        df_temp = compute_mkt_eq(I, J, β, rangeJ, varζ, varX, varξ)
        df_temp[!, &amp;quot;t&amp;quot;] = ones(nrow(df_temp)) * t
        append!(df, df_temp)
    end
    CSV.write(&amp;quot;../data/blp.csv&amp;quot;, df)
    return df
end;
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;simulate-the-data-3&#34;&gt;Simulate the Data (3)&lt;/h3&gt;
&lt;p&gt;Now let’s run the code&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# Simulate
df = simulate_data(i, J, β, T, rangeJ, varζ, varX, varξ);
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;the-data&#34;&gt;The Data&lt;/h3&gt;
&lt;p&gt;What does the data look like? Let’s switch to R!&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Read data
df = fread(&amp;quot;../data/blp.csv&amp;quot;)
kable(df[1:6,], digits=4)
&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:right&#34;&gt;j&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;w&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;p&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;q&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;q0&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;pr&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;x1&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;z1&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;x2&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;z2&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;t&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right&#34;&gt;4&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;0.6481&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;3.5918&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;0.2929&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;0.4558&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;0.8165&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;0.6531&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;1.0002&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;1.7063&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;1.8207&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right&#34;&gt;7&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;0.9997&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;5.1926&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;0.2513&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;0.4558&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;0.8717&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;1.0002&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;0.6531&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;1.8207&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;1.7063&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right&#34;&gt;1&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;0.5842&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;2.6999&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;0.0800&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;0.3919&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;0.1572&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;0.3591&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;7.1958&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;0.4217&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;2.1996&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right&#34;&gt;4&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;0.5291&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;4.5934&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;0.1404&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;0.3919&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;0.4467&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;2.3801&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;5.1748&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;0.1283&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;2.4929&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right&#34;&gt;5&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;0.5012&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;4.4196&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;0.1368&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;0.3919&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;0.4461&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;2.2638&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;5.2911&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;0.4408&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;2.1804&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right&#34;&gt;8&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;0.9359&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;3.2923&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;0.0863&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;0.3919&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;0.1477&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;0.5182&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;7.0367&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;1.0271&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;1.5942&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id=&#34;estimation&#34;&gt;Estimation&lt;/h3&gt;
&lt;p&gt;The BLP estimation procedure&lt;/p&gt;
&lt;h3 id=&#34;from-deltas-to-shares&#34;&gt;From deltas to shares&lt;/h3&gt;
&lt;p&gt;First, we need to compute the shares implied by aspecific vector of
$\delta$s&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;function implied_shares(Xt_::Matrix, ζt_::Matrix, δt_::Vector, δ0::Matrix)::Vector
    &amp;quot;&amp;quot;&amp;quot;Compute shares implied by deltas and shocks&amp;quot;&amp;quot;&amp;quot;
    u = [δt_ .+ (Xt_ * ζt_); δ0]                  # Utility
    e = exp.(u)                                 # Take exponential
    q = mean(e ./ sum(e, dims=1), dims=2)       # Compute demand
    return q[1:end-1]
end;
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;inner-loop&#34;&gt;Inner Loop&lt;/h3&gt;
&lt;p&gt;We can now compute the inner loop and invert the demand function: from
shares $q$ to $\delta$s&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;function inner_loop(qt_::Vector, Xt_::Matrix, ζt_::Matrix)::Vector
    &amp;quot;&amp;quot;&amp;quot;Solve the inner loop: compute delta, given the shares&amp;quot;&amp;quot;&amp;quot;
    δt_ = ones(size(qt_))
    δ0 = zeros(1, size(ζt_, 2))
    dist = 1

    # Iterate until convergence
    while (dist &amp;gt; 1e-8)
        q = implied_shares(Xt_, ζt_, δt_, δ0)
        δt2_ = δt_ + log.(qt_) - log.(q)
        dist = max(abs.(δt2_ - δt_)...)
        δt_ = δt2_
    end
    return δt_
end;
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;compute-delta&#34;&gt;Compute Delta&lt;/h3&gt;
&lt;p&gt;We can now repeat the inversion for every market and get the vector of
mean utilities $\delta$s from the observed market shares $q$.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;function compute_delta(q_::Vector, X_::Matrix, ζ_::Matrix, T::Vector)::Vector
    &amp;quot;&amp;quot;&amp;quot;Compute residuals&amp;quot;&amp;quot;&amp;quot;
    δ_ = zeros(size(T))

    # Loop over each market
    for t in unique(T)
        qt_ = q_[T.==t]                             # Quantity in market t
        Xt_ = X_[T.==t,:]                           # Characteristics in mkt t
        δ_[T.==t] = inner_loop(qt_, Xt_, ζ_)        # Solve inner loop
    end
    return δ_
end;
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;compute-xi&#34;&gt;Compute Xi&lt;/h3&gt;
&lt;p&gt;Now that we have $\delta$, it is pretty straightforward to compute
$\xi$. We just need to perform a linear regression (with instruments) of
mean utilities $\delta$ on prices $p$ and product characteristics $X$
and compute the residuals $\xi$.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;function compute_xi(X_::Matrix, IV_::Matrix, δ_::Vector)::Tuple
    &amp;quot;&amp;quot;&amp;quot;Compute residual, given delta (IV)&amp;quot;&amp;quot;&amp;quot;
    β_ = inv(IV_&#39; * X_) * (IV_&#39; * δ_)           # Compute coefficients (IV)
    ξ_ = δ_ - X_ * β_                           # Compute errors
    return ξ_, β_
end;
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;objective-function&#34;&gt;Objective Function&lt;/h3&gt;
&lt;p&gt;We now have all the ingredients to set up the GMM objective function.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;function GMM(varζ_::Number)::Tuple
    &amp;quot;&amp;quot;&amp;quot;Compute GMM objective function&amp;quot;&amp;quot;&amp;quot;
    δ_ = compute_delta(q_, X_, ζ_ * varζ_, T)   # Compute deltas
    ξ_, β_ = compute_xi(X_, IV_, δ_)            # Compute residuals
    gmm = ξ_&#39; * Z_ * Z_&#39; * ξ_ / length(ξ_)^2    # Compute ortogonality condition
    return gmm, β_
end;
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;estimation-1&#34;&gt;Estimation (1)&lt;/h3&gt;
&lt;p&gt;First, we need to set up our objects&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# Retrieve data
T = Int.(df.t)
X_ = [df.x1 df.x2 df.p]
q_ = df.q
q0_ = df.q0
IV_ = [df.x1 df.x2 df.w]
Z_ = [df.x1 df.x2 df.z1 df.z2]
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;estimation-2&#34;&gt;Estimation (2)&lt;/h3&gt;
&lt;p&gt;What would a logit regression estimate?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# Compute logit estimate
y = log.(df.q) - log.(df.q0);
β_logit = inv(IV_&#39; * X_) * (IV_&#39; * y);
print(&amp;quot;Estimated logit coefficients: $β_logit&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Estimated logit coefficients: [2.063144844221613, 1.2888511782561123, -0.9824308271558686]
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;estimation-3&#34;&gt;Estimation (3)&lt;/h3&gt;
&lt;p&gt;We can now run the BLP machinery&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# Draw shocks (less)
ζ_ = [rand(Normal(0,1), 1, i); zeros(K, i)];

# Minimize GMM objective function
varζ_ = optimize(x -&amp;gt; GMM(x[1])[1], [2.0], LBFGS()).minimizer[1];
β_blp = GMM(varζ_)[2];
print(&amp;quot;Estimated BLP coefficients: $β_blp&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Estimated BLP coefficients: [0.549234645269979, 1.1243451127088748, -0.6229637255651461]
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;appendix&#34;&gt;Appendix&lt;/h2&gt;
&lt;h3 id=&#34;references-references&#34;&gt;References [references]&lt;/h3&gt;
&lt;div id=&#34;refs&#34; class=&#34;references csl-bib-body hanging-indent&#34;
markdown=&#34;1&#34;&gt;
&lt;div id=&#34;ref-berry1995automobile&#34; class=&#34;csl-entry&#34; markdown=&#34;1&#34;&gt;
&lt;p&gt;Berry, Steven, James Levinsohn, and Ariel Pakes. 1995. “Automobile
Prices in Market Equilibrium.” &lt;em&gt;Econometrica: Journal of the Econometric
Society&lt;/em&gt;, 841–90.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Coding: Rust (1987)</title>
      <link>https://matteocourthoud.github.io/course/empirical-io/17_rust_1987/</link>
      <pubDate>Fri, 29 Oct 2021 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/course/empirical-io/17_rust_1987/</guid>
      <description>&lt;h3 id=&#34;setting&#34;&gt;Setting&lt;/h3&gt;
&lt;p&gt;From Rust (&lt;a href=&#34;#ref-rust1988maximum&#34;&gt;1988&lt;/a&gt;)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;An agent owns a fleet to buses&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Buses get old over time&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The older the bus is, the most costly it is to maintain&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The agent can decide to replace the bus engine with a new one, at a
cost&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Dynamic trade-off&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;What is the best moment to replace the engine?&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;You don’t want to replace an engine too early&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;doesn’t change much&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;You don’t want to replace an engine too late&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;avoid unnecessary maintenance costs&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;state&#34;&gt;State&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;State&lt;/strong&gt;: mileage of the bus&lt;/p&gt;
&lt;p&gt;$$s_t \in \lbrace 1, &amp;hellip;, 10 \rbrace $$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;State transitions&lt;/strong&gt;: with probability $\lambda$ the mileage of the
bus increases&lt;/p&gt;
&lt;p&gt;$$
s_{t+1} = \begin{cases}
\min \lbrace s_t + 1,10 \rbrace  &amp;amp; \text { with probability } \lambda \newline
s_t &amp;amp; \text { with probability } 1 - \lambda
\end{cases}
$$&lt;/p&gt;
&lt;p&gt;Note that $\lambda$ does not depend on the value of the state&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;actions&#34;&gt;Actions&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Action&lt;/strong&gt;: replacement decision $$
a_t \in \lbrace 0, 1 \rbrace
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Payoffs&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Per-period maintenance cost&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Cost of replacement $$
u\left(s_{t}, a_{t}, \epsilon_{1 t}, \epsilon_{2 t} ; \theta\right)=
\begin{cases}
-\theta_{1} s_{t}-\theta_{2} s_{t}^{2}+\epsilon_{0 t}, &amp;amp; \text { if } a_{t}=0 \newline
-\theta_{3} + \epsilon_{1t}, &amp;amp; \text { if } a_{t}=1
\end{cases}
$$&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;solving-the-model&#34;&gt;Solving the Model&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Start with an initial expected value function $V(s_t)=0$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Compute the alternative-specific value function $$
\bar V(s_t) = \begin{cases}
-\theta_1 s_t - \theta_2 s_t^2 + \beta \Big[(1-\lambda) V(s_t) + \lambda V(\min \lbrace s_t+1,10 \rbrace ) \Big] , &amp;amp; \text { if } a_t=0 \newline
-\theta_3 + \beta \Big[(1-\lambda) V(0) + \lambda V(1) \Big] , &amp;amp; \text { if } a_t=1
\end{cases}
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Compute the new expected value function $$
V&#39;(a_t) = \log \Big( e^{\bar V(a_t|s_t=0)} + e^{\bar V(a_t|s_t=1)} \Big)
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Repeat until convergence&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;code&#34;&gt;Code&lt;/h3&gt;
&lt;p&gt;First we set the parameter values.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# Set parameters
θ = [0.13; -0.004; 3.1];
λ = 0.82;
β = 0.95;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then we set the state space.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# State space
k = 10;
s = Vector(1:k);
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;static-utility&#34;&gt;Static Utility&lt;/h3&gt;
&lt;p&gt;First, we can compute static utility. $$
u\left(s_{t}, a_{t}, \epsilon_{1 t}, \epsilon_{2 t} ; \theta\right)=
\begin{cases}
-\theta_{1} s_{t}-\theta_{2} s_{t}^{2}+\epsilon_{0 t}, &amp;amp; \text { if } a_{t}=0 \newline
-\theta_{3} + \epsilon_{1 t}, &amp;amp; \text { if } a_{t}=1
\end{cases}
$$&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;function compute_U(θ::Vector, s::Vector)::Matrix
    &amp;quot;&amp;quot;&amp;quot;Compute static utility&amp;quot;&amp;quot;&amp;quot;
    u1 = - θ[1]*s - θ[2]*s.^2       # Utility of not investing
    u2 = - θ[3]*ones(size(s))       # Utility of investing
    U = [u1 u2]                     # Combine in a matrix
    return U
end;
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;value-function&#34;&gt;Value Function&lt;/h3&gt;
&lt;p&gt;We can now set up the value function iteration&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;function compute_Vbar(θ::Vector, λ::Number, β::Number, s::Vector)::Matrix
    &amp;quot;&amp;quot;&amp;quot;Compute value function by Bellman iteration&amp;quot;&amp;quot;&amp;quot;
    k = length(s)                                 # Dimension of the state space
    U = compute_U(θ, s)                           # Static utility
    index_λ = Int[1:k [2:k; k]];                  # Mileage index
    index_A = Int[1:k ones(k,1)];                 # Investment index
    γ = Base.MathConstants.eulergamma             # Euler&#39;s gamma

    # Iterate the Bellman equation until convergence
    Vbar = zeros(k, 2);
    Vbar1 = Vbar;
    dist = 1;
    iter = 0;
    while dist&amp;gt;1e-8
        V = γ .+ log.(sum(exp.(Vbar), dims=2))     # Compute value
        expV = V[index_λ] * [1-λ; λ]               # Compute expected value
        Vbar1 =  U + β * expV[index_A]             # Compute v-specific
        dist = max(abs.(Vbar1 - Vbar)...);         # Check distance
        iter += 1;
        Vbar = Vbar1                               # Update value function
    end
    return Vbar
end;
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;solving-the-model-1&#34;&gt;Solving the Model&lt;/h3&gt;
&lt;p&gt;We can now solve for the value function.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# Compute value function
V_bar = compute_Vbar(θ, λ, β, s);
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;dgp&#34;&gt;DGP&lt;/h3&gt;
&lt;p&gt;Now that we know how to compute the equilibrium, we can simulate the
data.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;function generate_data(θ::Vector, λ::Number, β::Number, s::Vector, N::Int)::Tuple
    &amp;quot;&amp;quot;&amp;quot;Generate data from primitives&amp;quot;&amp;quot;&amp;quot;
    Vbar = compute_Vbar(θ, λ, β, s)             # Solve model
    ε = rand(Gumbel(0,1), N, 2)                 # Draw shocks
    St = rand(s, N)                             # Draw states
    A = (((Vbar[St,:] + ε) * [-1;1]) .&amp;gt; 0)      # Compute investment decisions
    δ = (rand(Uniform(0,1), N) .&amp;lt; λ)            # Compute mileage shock
    St1 = min.(St .* (A.==0) + δ, max(s...))    # Compute neSr state
    df = DataFrame(St=St, A=A, St1=St1)         # Dataframe
    CSV.write(&amp;quot;../data/rust.csv&amp;quot;, df)
    return St, A, St1
end;
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;generate-the-data&#34;&gt;Generate the DAta&lt;/h3&gt;
&lt;p&gt;We can now generate the data&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# Generate data
N = Int(1e5);
St, A, St1 = generate_data(θ, λ, β, s, N);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;How many investment decisions do we observe?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;print(&amp;quot;we observe &amp;quot;, sum(A), &amp;quot; investment decisions in &amp;quot;, N, &amp;quot; observations&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## we observe 19207 investment decisions in 100000 observations
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;the-data&#34;&gt;The Data&lt;/h3&gt;
&lt;p&gt;What does the data look like?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Read data
df = fread(&amp;quot;../data/rust.csv&amp;quot;)
kable(df[1:6,], digits=4)
&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:right&#34;&gt;St&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;A&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;St1&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right&#34;&gt;3&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;FALSE&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;4&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right&#34;&gt;9&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;TRUE&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right&#34;&gt;8&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;TRUE&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right&#34;&gt;3&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;FALSE&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;4&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right&#34;&gt;9&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;FALSE&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;10&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right&#34;&gt;10&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;FALSE&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;10&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id=&#34;estimation---lambda&#34;&gt;Estimation - Lambda&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;First we can estimate the value of lambda as the probability of
mileage increase&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Conditional on not investing&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;And not being in the last state (mileage cannot increase any more)&lt;/p&gt;
&lt;p&gt;$$
\hat \lambda = \mathbb E_n \Big[ (s_{t+1}-s_t) \mid a_{t}=0 \wedge s_{t}&amp;lt;10 \Big]
$$&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# Estimate lambda
Δ = St1 - St;
λ_ = mean(Δ[(A.==0) .&amp;amp; (St.&amp;lt;10)]);

print(&amp;quot;Estimated lambda: $λ_ (true = $λ)&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Estimated lambda: 0.8206570869594549 (true = 0.82)
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;estimation---theta&#34;&gt;Estimation - Theta&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Take a parameter guess $\theta_0$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Compute the alternative-specific value function
$\bar V(s_t ; \hat \lambda, \theta_0)$ by iteration&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Compute the implied choice probabilities&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Compute the likelihood $$
\mathcal{L}(\theta) = \prod_{t=1}^{T}\left(\hat{\operatorname{Pr}}\left(a=1 \mid s_{t}, \theta\right) \mathbb{1}\left(a_{t}=1\right)+\left(1-\hat{\operatorname{Pr}}\left(a=0 \mid s_{t}, \theta\right)\right) \mathbb{1}\left(a_{t}=0\right)\right)
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Repeat the above to find a minimum of the likelihood function&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;likelihood-function&#34;&gt;Likelihood Function&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;function logL_Rust(θ0::Vector, λ::Number, β::Number, s::Vector, St::Vector, A::BitVector)::Number
    &amp;quot;&amp;quot;&amp;quot;Compute log-likelihood functionfor Rust problem&amp;quot;&amp;quot;&amp;quot;
    # Compute value
    Vbar = compute_Vbar(θ0, λ_, β, s)

    # Expected choice probabilities
    EP = exp.(Vbar[:,2]) ./ (exp.(Vbar[:,1]) + exp.(Vbar[:,2]))

    # Likelihood
    logL = sum(log.(EP[St[A.==1]])) + sum(log.(1 .- EP[St[A.==0]]))
    return -logL
end;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can check the likelihood at the true value:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# True likelihood value
logL_trueθ = logL_Rust(θ, λ, β, s, St, A);
print(&amp;quot;The likelihood at the true parameter is $logL_trueθ&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The likelihood at the true parameter is 45937.866092460084
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;estimating-theta&#34;&gt;Estimating Theta&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# Select starting values
θ0 = Float64[0,0,0];

# Optimize
θ_R = optimize(x -&amp;gt; logL_Rust(x, λ, β, s, St, A), θ0).minimizer;
print(&amp;quot;Estimated thetas: $θ_R (true = $θ)&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Estimated thetas: [0.12063838656559037, -0.003220197034620527, 3.0865668144650487] (true = [0.13, -0.004, 3.1])
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;starting-values&#34;&gt;Starting Values&lt;/h3&gt;
&lt;p&gt;Starting values are important!&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# Not all initial values are equally good
θ0 = Float64[1,1,1];

# Optimize
θ_R2 = optimize(x -&amp;gt; logL_Rust(x, λ, β, s, St, A), θ0).minimizer;
print(&amp;quot;Estimated thetas: $θ_R2 (true = $θ)&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Estimated thetas: [1.0, 1.0, 1.0] (true = [0.13, -0.004, 3.1])
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;hotz--miller&#34;&gt;Hotz &amp;amp; Miller&lt;/h2&gt;
&lt;h3 id=&#34;recap&#34;&gt;Recap&lt;/h3&gt;
&lt;p&gt;Hotz &amp;amp; Miller estimation procedure works as follows&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Estimate the CCPs from the data&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Hotz &amp;amp; Miller inversion $$
\hat V = \Big[I - \beta \ \sum_a P_a .* T_a \Big]^{-1} \ * \ \left( \sum_a P_a \ .* \ \bigg[ u_a + \mathbb E [\epsilon_a] \bigg] \right)
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Compute EP from EV $$
\hat \Pr(a=1 ; \theta) = \frac{\exp (u_1 +\beta T_1 \hat V )}{\sum_{a} \exp (u_a +\beta T_a \hat V )}
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Compute the objective function: the (log)likelihood $$
\mathcal{L}(\theta) = \prod_{t=1}^{T}\left(\hat{\operatorname{Pr}}\left(a=1 \mid s_{t}; \theta\right) \mathbb{1}\left(a_{t}=1\right)+\left(1-\hat{\operatorname{Pr}}\left(a=0 \mid s_{t}; \theta\right)\right) \mathbb{1}\left(a_{t}=0\right)\right)
$$&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;ccps&#34;&gt;CCPs&lt;/h3&gt;
&lt;p&gt;First, we need to estimate the &lt;strong&gt;Conditional Choice Proabilities (CCP)&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;can be done non-parametrically&lt;/li&gt;
&lt;li&gt;i.e. just look at the frequency of investment in each state&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# Estimate CCP
P = [mean(A[St.==i]) for i=s];
CCP = [(1 .- P) P]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 10×2 Array{Float64,2}:
##  0.952419  0.0475814
##  0.923046  0.0769535
##  0.894443  0.105557
##  0.853306  0.146694
##  0.819293  0.180707
##  0.788935  0.211065
##  0.747248  0.252752
##  0.717915  0.282085
##  0.6947    0.3053
##  0.678452  0.321548
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;transition-probabilities&#34;&gt;Transition Probabilities&lt;/h3&gt;
&lt;p&gt;NeSr, we need $T$, the matrices of transition probabilities, conditional
on the investment choice.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;function compute_T(k::Int, λ_::Number)::Array
    &amp;quot;&amp;quot;&amp;quot;Compute transition matrix&amp;quot;&amp;quot;&amp;quot;
    T = zeros(k, k, 2);

    # Conditional on not investing
    T[k,k,1] = 1;
    for i=1:k-1
        T[i,i,1] = 1-λ_
        T[i,i+1,1] = λ_
    end

    # Conditional on investing
    T[:,1,2] .= 1-λ_;
    T[:,2,2] .= λ_;

    return(T)
end;
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;t&#34;&gt;T&lt;/h3&gt;
&lt;p&gt;What form does the transition matrix $T$ take?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# Compute T
T = compute_T(k, λ_);

# Conditional on not investing
T[:,:,1]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 10×10 Array{Float64,2}:
##  0.179343  0.820657  0.0       0.0       …  0.0       0.0       0.0
##  0.0       0.179343  0.820657  0.0          0.0       0.0       0.0
##  0.0       0.0       0.179343  0.820657     0.0       0.0       0.0
##  0.0       0.0       0.0       0.179343     0.0       0.0       0.0
##  0.0       0.0       0.0       0.0          0.0       0.0       0.0
##  0.0       0.0       0.0       0.0       …  0.0       0.0       0.0
##  0.0       0.0       0.0       0.0          0.820657  0.0       0.0
##  0.0       0.0       0.0       0.0          0.179343  0.820657  0.0
##  0.0       0.0       0.0       0.0          0.0       0.179343  0.820657
##  0.0       0.0       0.0       0.0          0.0       0.0       1.0
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;t-2&#34;&gt;T (2)&lt;/h3&gt;
&lt;p&gt;Instead, the transitions conditional on investing are&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# T Conditional on investing
T[:,:,2]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 10×10 Array{Float64,2}:
##  0.179343  0.820657  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0
##  0.179343  0.820657  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0
##  0.179343  0.820657  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0
##  0.179343  0.820657  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0
##  0.179343  0.820657  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0
##  0.179343  0.820657  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0
##  0.179343  0.820657  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0
##  0.179343  0.820657  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0
##  0.179343  0.820657  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0
##  0.179343  0.820657  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;hotz--miller-inversion&#34;&gt;Hotz &amp;amp; Miller Inversion&lt;/h3&gt;
&lt;p&gt;We now have all the pieces to compute the &lt;strong&gt;expected value function&lt;/strong&gt;
$V$ through the Hotz &amp;amp; Miller &lt;strong&gt;inversion&lt;/strong&gt;. $$
\hat V = \left[I - \beta \ \sum_a P_a .* T_a \right]^{-1} \ * \ \left( \sum_a P_a \ .* \ \bigg[ u_a + \mathbb E [\epsilon_a] \bigg] \right)
$$&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;function HM_inversion(CCP::Matrix, T::Array, U::Matrix, β::Number)::Vector
    &amp;quot;&amp;quot;&amp;quot;Perform HM inversion&amp;quot;&amp;quot;&amp;quot;

    # Compute LHS (to be inverted)
    γ = Base.MathConstants.eulergamma
    LEFT = I - β .* (CCP[:,1] .* T[:,:,1] + CCP[:,2] .* T[:,:,2])

    # Compute LHS (not to be inverted)
    RIGHT = γ .+ sum(CCP .* (U .- log.(CCP)) , dims=2)

    # Compute V
    EV_ = inv(LEFT) * RIGHT
    return vec(EV_)
end;
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;from-ev-to-ep&#34;&gt;From EV to EP&lt;/h3&gt;
&lt;p&gt;We can now compute the expected policy function from the expected value
function $$
\hat \Pr(a=1 ; \theta) = \frac{\exp (u_1 +\beta T_1 \hat V )}{\sum_{a} \exp (u_a +\beta T_a \hat V )}
$$&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;function from_EV_to_EP(EV_::Vector, T::Array, U::Matrix, β::Number)::Vector
    &amp;quot;&amp;quot;&amp;quot;Compute expected policy from expected value&amp;quot;&amp;quot;&amp;quot;
    E = exp.( U + β .* [(T[:,:,1] * EV_) (T[:,:,2] * EV_)] )
    EP_ = E[:,2] ./ sum(E, dims=2)
    return vec(EP_)
end;
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;likelihood&#34;&gt;Likelihood&lt;/h3&gt;
&lt;p&gt;We now have all the pieces to build the likelihood function $$
\mathcal{L}(\theta) = \prod_{t=1}^{T} \left(\hat \Pr \left(a=1 \mid s_{t}; \theta\right) \mathbb{1} \left(a_{t}=1\right) + \left(1-\hat \Pr \left(a=0 \mid s_{t}; \theta\right)\right) \mathbb{1} \left(a_{t}=0\right)\right)
$$&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;function logL_HM(θ0::Vector, λ::Number, β::Number, s::Vector, St::Vector, A::BitVector, T::Array, CCP::Matrix)::Number
    &amp;quot;&amp;quot;&amp;quot;Compute log-likelihood function for HM problem&amp;quot;&amp;quot;&amp;quot;
    # Compute static utility
    U = compute_U(θ0, s)

    # Espected value by inversion
    EV_ = HM_inversion(CCP, T, U, β)

    # Implies choice probabilities
    EP_ = from_EV_to_EP(EV_, T, U, β)

    # Likelihood
    logL = sum(log.(EP_[St[A.==1]])) + sum(log.(1 .- EP_[St[A.==0]]))
    return -logL
end;
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;estimation&#34;&gt;Estimation&lt;/h3&gt;
&lt;p&gt;We can now estimate the parameters&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# Optimize
θ0 = Float64[0,0,0];
θ_HM = optimize(x -&amp;gt; logL_HM(x, λ, β, s, St, A, T, CCP), θ0).minimizer;
print(&amp;quot;Estimated thetas: $θ_HM (true = $θ)&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Estimated thetas: [0.12064911403839335, -0.003220614484856523, 3.086621855583483] (true = [0.13, -0.004, 3.1])
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;aguirregabiria-mira-2002&#34;&gt;Aguirregabiria, Mira (2002)&lt;/h3&gt;
&lt;p&gt;With Hotz and Miller, we have generated a mapping of the form&lt;/p&gt;
&lt;p&gt;$$
\bar P(\cdot ; \theta) = g(h(\hat P(\cdot) ; \theta); \theta)
$$&lt;/p&gt;
&lt;p&gt;Aguirregabiria and Mira (&lt;a href=&#34;#ref-aguirregabiria2002swapping&#34;&gt;2002&lt;/a&gt;): why
don’t we iterate it?&lt;/p&gt;
&lt;h3 id=&#34;am-likelihood-function&#34;&gt;AM Likelihood Function&lt;/h3&gt;
&lt;p&gt;The likelihood function in Aguirregabiria and Mira
(&lt;a href=&#34;#ref-aguirregabiria2002swapping&#34;&gt;2002&lt;/a&gt;) is extremely similar to Hotz
and Miller (&lt;a href=&#34;#ref-hotz1993conditional&#34;&gt;1993&lt;/a&gt;)&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;function logL_AM(θ0::Vector, λ::Number, β::Number, s::Vector, St::Vector, A::BitVector, T::Array, CCP::Matrix, K::Int)::Number
    &amp;quot;&amp;quot;&amp;quot;Compute log-likelihood function for AM problem&amp;quot;&amp;quot;&amp;quot;
    # Compute static utility
    U = compute_U(θ0, s)
    EP_ = CCP[:,2]

    # Iterate HM mapping
    for _=1:K
        EV_ = HM_inversion(CCP, T, U, β)    # Expected value by inversion
        EP_ = from_EV_to_EP(EV_, T, U, β)   # Implies choice probabilities
        CCP = [(1 .- EP_) EP_]
    end

    # Likelihood
    logL = sum(log.(EP_[St[A.==1]])) + sum(log.(1 .- EP_[St[A.==0]]))
    return -logL
end;
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;estimation-1&#34;&gt;Estimation&lt;/h3&gt;
&lt;p&gt;We can now estimate the parameters&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# Set number of iterations
K = 2;

# Optimize
θ0 = Float64[0,0,0];
θ_AM = optimize(x -&amp;gt; logL_AM(x, λ, β, s, St, A, T, CCP, K), θ0).minimizer;
print(&amp;quot;Estimated thetas: $θ_AM (true = $θ)&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Estimated thetas: [0.12063890836521114, -0.0032202282942220464, 3.086571461772538] (true = [0.13, -0.004, 3.1])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Not much changes in our case.&lt;/p&gt;
&lt;h3 id=&#34;speed&#34;&gt;Speed&lt;/h3&gt;
&lt;p&gt;We can compare the methods in terms of speed.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# Compare speed
θ0 = Float64[0,0,0];
optimize(x -&amp;gt; logL_Rust(x, λ, β, s, St, A), θ0).time_run
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 0.5477378368377686
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;optimize(x -&amp;gt; logL_HM(x, λ, β, s, St, A, T, CCP), θ0).time_run
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 0.3244161605834961
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;optimize(x -&amp;gt; logL_AM(x, λ, β, s, St, A, T, CCP, K), θ0).time_run
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 0.35499119758605957
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Even in this simple example with a very small state space, the
difference is significant.&lt;/p&gt;
&lt;h2 id=&#34;appendix&#34;&gt;Appendix&lt;/h2&gt;
&lt;h3 id=&#34;references-references&#34;&gt;References [references]&lt;/h3&gt;
&lt;div id=&#34;refs&#34; class=&#34;references csl-bib-body hanging-indent&#34;
markdown=&#34;1&#34;&gt;
&lt;div id=&#34;ref-aguirregabiria2002swapping&#34; class=&#34;csl-entry&#34; markdown=&#34;1&#34;&gt;
&lt;p&gt;Aguirregabiria, Victor, and Pedro Mira. 2002. “Swapping the Nested Fixed
Point Algorithm: A Class of Estimators for Discrete Markov Decision
Models.” &lt;em&gt;Econometrica&lt;/em&gt; 70 (4): 1519–43.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-hotz1993conditional&#34; class=&#34;csl-entry&#34; markdown=&#34;1&#34;&gt;
&lt;p&gt;Hotz, V Joseph, and Robert A Miller. 1993. “Conditional Choice
Probabilities and the Estimation of Dynamic Models.” &lt;em&gt;The Review of
Economic Studies&lt;/em&gt; 60 (3): 497–529.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-rust1988maximum&#34; class=&#34;csl-entry&#34; markdown=&#34;1&#34;&gt;
&lt;p&gt;Rust, John. 1988. “Maximum Likelihood Estimation of Discrete Control
Processes.” &lt;em&gt;SIAM Journal on Control and Optimization&lt;/em&gt; 26 (5): 1006–24.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Free PhD Courses in Economics and Data Science</title>
      <link>https://matteocourthoud.github.io/post/courses/</link>
      <pubDate>Thu, 10 Feb 2022 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/post/courses/</guid>
      <description>&lt;p&gt;In this page I will collect lectures and materials for graduate courses in Economics and Social Sciences.&lt;/p&gt;
&lt;p&gt;I will only link to lectures and materials that are freely available. I will not link to courses hosted on MOOC websites or that require university credentials to access.&lt;/p&gt;
&lt;p&gt;A special mention goes to the following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The &lt;strong&gt;NBER&lt;/strong&gt; that during each Summer Institute has a &lt;a href=&#34;https://www.nber.org/research/lectures&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;lecture series&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;The &lt;strong&gt;Chamberlain Seminar&lt;/strong&gt; that since 2021 started hosting and recording &lt;a href=&#34;https://www.chamberlainseminar.org/past-seminars&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;tutorial sessions&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;video-lectures&#34;&gt;Video Lectures&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Course Title&lt;/th&gt;
&lt;th&gt;Author&lt;/th&gt;
&lt;th&gt;University&lt;/th&gt;
&lt;th&gt;Year&lt;/th&gt;
&lt;th&gt;Material&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://chrisconlon.github.io/gradio.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Industrial Organization&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Chris Conlon&lt;/td&gt;
&lt;td&gt;NYU&lt;/td&gt;
&lt;td&gt;2021&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://chrisconlon.github.io/gradio.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Yes&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://chrisconlon.github.io/metrics.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Panel Data Econometrics&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Chris Conlon&lt;/td&gt;
&lt;td&gt;NYU&lt;/td&gt;
&lt;td&gt;2021&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://chrisconlon.github.io/metrics.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Yes&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://www.youtube.com/playlist?list=PLoROMvodv4rPLKxIpqhjhPgdQy7imNkDn&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Machine Learning with Graphs&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Yure Leskovec&lt;/td&gt;
&lt;td&gt;Stanford&lt;/td&gt;
&lt;td&gt;2021&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;http://web.stanford.edu/class/cs224w/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Yes&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://www.youtube.com/playlist?list=PLWWcL1M3lLlojLTSVf2gGYQ_9TlPyPbiJ&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Applied Methods&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Paul Goldsmith-Pinkham&lt;/td&gt;
&lt;td&gt;Yale&lt;/td&gt;
&lt;td&gt;2021&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://github.com/paulgp/applied-methods-phd&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Yes&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://taylorjwright.github.io/did-reading-group/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;DiD Reading Group&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;misc&lt;/td&gt;
&lt;td&gt;misc&lt;/td&gt;
&lt;td&gt;2021&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://taylorjwright.github.io/did-reading-group/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Yes&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://vimeo.com/user108848900&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Computational Economics&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Kenneth Judd&lt;/td&gt;
&lt;td&gt;Stanford&lt;/td&gt;
&lt;td&gt;2020&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://github.com/KennethJudd/CompEcon2020&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Yes&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://www.youtube.com/playlist?list=PLoROMvodv4rOSOPzutgyCTapiGlY2Nd8u&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Reinforcement Learning&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Emma Brunskill&lt;/td&gt;
&lt;td&gt;Stanford&lt;/td&gt;
&lt;td&gt;2020&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;http://web.stanford.edu/class/cs234/index.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Yes&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://www.youtube.com/playlist?list=PLoROMvodv4rObpMCir6rNNUlFAn56Js20&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Natural Language Understanding&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Christopher Potts&lt;/td&gt;
&lt;td&gt;Stanford&lt;/td&gt;
&lt;td&gt;2019&lt;/td&gt;
&lt;td&gt;No&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;material&#34;&gt;Material&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Course Title&lt;/th&gt;
&lt;th&gt;Author&lt;/th&gt;
&lt;th&gt;University&lt;/th&gt;
&lt;th&gt;Year&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://floswald.github.io/NumericalMethods/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Computational Economics&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://floswald.github.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Florial Oswald&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Bocconi&lt;/td&gt;
&lt;td&gt;2021&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://github.com/uo-ec607/lectures&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Data Science for Economists&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://grantmcdermott.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Grant McDermott&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Oregon&lt;/td&gt;
&lt;td&gt;2020&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;http://www.johnasker.com/IO.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Industrial Organization&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;John Asker&lt;/td&gt;
&lt;td&gt;UCLA&lt;/td&gt;
&lt;td&gt;2020&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://kohei-kawaguchi.github.io/EmpiricalIO/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Topics in Empirical Industrial Organization&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Kohei Kawaguchi&lt;/td&gt;
&lt;td&gt;Hong Kong&lt;/td&gt;
&lt;td&gt;2020&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;http://individual.utoronto.ca/vaguirre/courses/eco2901/teaching_io_toronto.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Industrial Organization&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Victor Aguirregabiria&lt;/td&gt;
&lt;td&gt;Toronto&lt;/td&gt;
&lt;td&gt;2019&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://github.com/OU-PhD-Econometrics/fall-2020&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Econometrics&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Tyler Ransom&lt;/td&gt;
&lt;td&gt;Oklahoma&lt;/td&gt;
&lt;td&gt;2020&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://github.com/MartinSpindler/Machine-Learning-in-Econometrics&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Machine Learning in Econometrics&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Martin Spindler&lt;/td&gt;
&lt;td&gt;Munich&lt;/td&gt;
&lt;td&gt;2020&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;http://comlabgames.com/structuraleconometrics/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Structural Econometrics&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Robert Miller&lt;/td&gt;
&lt;td&gt;Carnegie Mellon&lt;/td&gt;
&lt;td&gt;2019&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;iframe src=&#34;https://docs.google.com/forms/d/e/1FAIpQLSf672Vwguhe9GAmFGqtPMeGWzTamwzU2VcMtEFgaJWnP3YtBw/viewform?embedded=true&#34; width=&#34;640&#34; height=&#34;439&#34; frameborder=&#34;0&#34; marginheight=&#34;0&#34; marginwidth=&#34;0&#34;&gt;Loading…&lt;/iframe&gt;
</description>
    </item>
    
    <item>
      <title>Conferences in Economics and Finance</title>
      <link>https://matteocourthoud.github.io/post/conferences/</link>
      <pubDate>Mon, 10 Jan 2022 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/post/conferences/</guid>
      <description>&lt;p&gt;I will use this page to collect information about conferences in Economics and Finance.&lt;/p&gt;
&lt;p&gt;If you know about public conferences or meetings that are missing from this list, please either &lt;a href=&#34;mailto:matteo.courthoud@econ.uzh.ch&#34;&gt;contact me&lt;/a&gt; or &lt;a href=&#34;https://github.com/matteocourthoud/website/blob/master/content/post/conferences/index.md&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;edit the table on Github&lt;/a&gt;!&lt;/p&gt;
&lt;p&gt;Note that conferences are ordered by deadline and not by conference date.&lt;/p&gt;
&lt;h2 id=&#34;january&#34;&gt;January&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Conference&lt;/th&gt;
&lt;th&gt;Organizer&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Field&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Deadline&lt;/th&gt;
&lt;th&gt;Target&lt;/th&gt;
&lt;th&gt;Date&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://www.sgvs.ch/conferences/sses2022&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Annual Congress of the Swiss Society of Economics and Statistics&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;SSES&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;All&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://www.sgvs.ch/conferences/sses2022/call_for_papers&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;January 31&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;23/06/21&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;br&gt;
&lt;h2 id=&#34;february&#34;&gt;February&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Conference&lt;/th&gt;
&lt;th&gt;Organizer&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Field&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Deadline&lt;/th&gt;
&lt;th&gt;Target&lt;/th&gt;
&lt;th&gt;Date&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://beccle.no/event/bergen-competition-policy-conference-2020/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Bergen Competition Policy Conference&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;NHH&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Comp policy&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://beccle.no/event/bergen-competition-policy-conference-2020/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;February 03&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;23/04/20&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://cepr.org/6754/cfp-mainconference&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CEPR/JIE Conference on Applied IO&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;CEPR&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;IO&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;&#34;&gt;February 10&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Junior&lt;/td&gt;
&lt;td&gt;08/06/21&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;http://ec22.sigecom.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Economics and Computation&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;ACM SIGecom&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Theory&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://ec22.sigecom.org/call-for-contributions-acm/papers/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;February 10&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;11/07/21&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://www.econometricsociety.org/meetings/schedule/2022/06/16/2022-north-america-summer-meeting&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ES North American Summer Meeting&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Econometric Society&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;All&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://www.econometricsociety.org/meetings/schedule/2022/06/16/2022-north-america-summer-meeting&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;February 12&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Senior&lt;/td&gt;
&lt;td&gt;16/06/21&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://www.eea-esem-congresses.org/index.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;EEA Summer Meeting&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;European Economic Association&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;All&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://www.eea-esem-congresses.org/important-dates.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;February 15&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;23/08/21&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://www.eea-esem-congresses.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ES European Summer Meeting&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Econometric Society&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;All&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://www.eea-esem-congresses.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;February 15&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Senior&lt;/td&gt;
&lt;td&gt;23/08/21&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://www.economicdynamics.org/sedam_2021/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Annual Meeting of the Society for Economic Dynamics&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;SED&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Macro&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://www.economicdynamics.org/sedam_2021/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;February 15&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;01/07/21&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://games2020.hu/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GAMES 2020&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Game Theory Society&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Game Theory&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://games2020.hu/registration/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;February 20&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;19/07/21&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://www.nottingham.ac.uk/gep/news-events/conferences/2020-21/postgrad-conference-2021.aspx&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Annual GEP/CEPR Postgraduate Conference&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;University of Nottingham&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Policy&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://www.nottingham.ac.uk/gep/documents/conferences/2020-21/pg-conf-cfp-2021.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;February 26&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Junior&lt;/td&gt;
&lt;td&gt;06/05/21&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://www.digital-economics.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Doctoral Workshop on the Economics of Digitization&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;TSE&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Digitalization&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://www.digital-economics.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;February 28&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Junior&lt;/td&gt;
&lt;td&gt;12/05/22&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;br&gt;
&lt;h2 id=&#34;march&#34;&gt;March&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Conference&lt;/th&gt;
&lt;th&gt;Organizer&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Field&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Deadline&lt;/th&gt;
&lt;th&gt;Target&lt;/th&gt;
&lt;th&gt;Date&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://cssh.northeastern.edu/economics/iioc/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Annual IIOC&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Northeastern University&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;IO&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://cssh.northeastern.edu/economics/iioc/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;March 01&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;30/04/21&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://yem2020.econ.muni.cz/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Young Economists&#39; Meeting&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;University fo Munich&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;All&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://yem2020.econ.muni.cz/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;March 08&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Junior&lt;/td&gt;
&lt;td&gt;01/10/20&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://www.barcelonagse.eu/summer-forum&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GSE Summer Forum&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Barcelona GSE&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;All&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://www.barcelonagse.eu/summer-forum&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;March 14&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;07/06/21&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://www.nhh.no/en/calendar/conferences/2021/earie-2021/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;EARIE&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;NHH&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;IO&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://www.nhh.no/en/calendar/conferences/2021/earie-2021/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;March 15&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;27/08/21&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://www.sioe.org/news/economics-media-workshop-call-paper-poster-presentations&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Economics of Media Workshop&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Queen’s University&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;IO&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://www.sioe.org/news/economics-media-workshop-call-paper-poster-presentations&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;March 15&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Junior&lt;/td&gt;
&lt;td&gt;12/06/20&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://www.qmul.ac.uk/sef/events/conferences/items/3rd-qmul-economics-and-finance-workshop-for-phd--post-doctoral-students.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;QMUL Economics and Finance Workshop&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Queen Mary University&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;All&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://econ.columbia.edu/call-for-papers-3rd-qm-phd-workshop/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;March 20&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Junior&lt;/td&gt;
&lt;td&gt;26/05/20&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://static1.squarespace.com/static/56086d00e4b0fb7874bc2d42/t/5e753140c2225859fa93ba1e/1584738624656/callforpapers.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Virtual Finance and Economics Conference&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Yale University&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Finecon&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://static1.squarespace.com/static/56086d00e4b0fb7874bc2d42/t/5e753140c2225859fa93ba1e/1584738624656/callforpapers.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;March 25&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;17/04/20&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://sites.google.com/view/dc-io-day&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;DC IO Day 2020&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Georgetown University&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;IO&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://sites.google.com/view/dc-io-day&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;March 31&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;15/05/20&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;br&gt;
&lt;h2 id=&#34;april&#34;&gt;April&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Conference&lt;/th&gt;
&lt;th&gt;Organizer&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Field&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Deadline&lt;/th&gt;
&lt;th&gt;Target&lt;/th&gt;
&lt;th&gt;Date&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://economics.stanford.edu/site/site-2021&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;SITE&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Stanford University&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Theory&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://economics.stanford.edu/site/site-2021&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;April 01&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;12/06/21&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://www.nber.org/conferences/summer-institute&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;NBER Summer Meeting Workshop&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;NBER&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;All&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://www.nber.org/conferences/summer-institute&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;April 05&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;12/07/21&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://www.aeaweb.org/conference/submissions&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;AEA Annual Meeting&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;AEA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;All&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://www.aeaweb.org/conference/submissions&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;April 15&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;07/01/22&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;files/swissIOday2021_CallForPapers.pdf&#34;&gt;Swiss IO Day&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;University of Bern&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;IO&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;files/swissIOday2021_CallForPapers.pdf&#34;&gt;April 16&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;11/06/21&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://www.econometricsociety.org/meetings/schedule/2022/01/06/2022-north-american-winter-meeting&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Econometric Society - North American Winter Meetings&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;ES&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;All&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://www.econometricsociety.org/meetings/schedule/2022/01/06/2022-north-american-winter-meeting&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;April 21&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;06/01/22&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://www.cresse.info/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CRESSE&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;CRESSE&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;IO&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://www.cresse.info/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;April 30&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;26/07/21&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;br&gt;
&lt;h2 id=&#34;may&#34;&gt;May&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Conference&lt;/th&gt;
&lt;th&gt;Organizer&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Field&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Deadline&lt;/th&gt;
&lt;th&gt;Target&lt;/th&gt;
&lt;th&gt;Date&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://mailchi.mp/cepr/european-research-workshop-in-international-trade-erwit-506353&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;European Research Workshop in International Trade&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;CEPR&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Trade&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://mailchi.mp/cepr/european-research-workshop-in-international-trade-erwit-506353&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;May 02&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;22/10/21&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;http://coin.wne.uw.edu.pl/wiem/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Warsaw International Economic Meeting&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Warsaw University&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;All&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;http://coin.wne.uw.edu.pl/wiem/wiem2020-cfp.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;May 03&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;01/07/20&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://warwick.ac.uk/fac/soc/economics/events/2021/6/economics_phd_conference/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Warwick Economics PhD Conference&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;University of Warwick&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;All&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://warwick.ac.uk/fac/soc/economics/events/2021/6/economics_phd_conference/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;May 09&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;PhD&lt;/td&gt;
&lt;td&gt;24/06/21&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://www.law.northwestern.edu/research-faculty/clbe/events/antitrust/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Annual Conference on Antitrust Economics and Competition Policy&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Northwestern University&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Comp policy&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://www.law.northwestern.edu/research-faculty/clbe/events/antitrust/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;May 17&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;17/09/21&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://www.economicsofai.com/blog/2021/2/2/2021-nber-economics-of-ai-conference-call-for-papers&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;NBER Economics of AI Conference&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;NBER&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;AI&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://www.economicsofai.com/blog/2021/2/2/2021-nber-economics-of-ai-conference-call-for-papers&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;May 31&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;23/09/21&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;br&gt;
&lt;h2 id=&#34;june&#34;&gt;June&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Conference&lt;/th&gt;
&lt;th&gt;Organizer&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Field&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Deadline&lt;/th&gt;
&lt;th&gt;Target&lt;/th&gt;
&lt;th&gt;Date&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://eaamo.org/cfp/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Equity and Access in Algorithms, Mechanisms, and Optimization (EAAMO)&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;ACM&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;AI&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://eaamo.org/cfp/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;June 14&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;05/10/21&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://www.ftc.gov/news-events/events-calendar/fourteenth-annual-federal-trade-commission-microeconomics-conference&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;FTC Micro Conference&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;FTC&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;IO&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://www.ftc.gov/system/files/documents/public_events/1588356/20210326_-_micro_conf_call_for_papers.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;June 23&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Senior&lt;/td&gt;
&lt;td&gt;04/11/21&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;br&gt;
&lt;h2 id=&#34;july&#34;&gt;July&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Conference&lt;/th&gt;
&lt;th&gt;Organizer&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Field&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Deadline&lt;/th&gt;
&lt;th&gt;Target&lt;/th&gt;
&lt;th&gt;Date&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://www.emconference.net/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Empirics and Methods in Economics Conference&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Northwestern &amp;amp; Chicago&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Empirical&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://www.emconference.net/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;July 30&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Junior&lt;/td&gt;
&lt;td&gt;22/10/20&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;br&gt;
&lt;h2 id=&#34;august&#34;&gt;August&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Conference&lt;/th&gt;
&lt;th&gt;Organizer&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Field&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Deadline&lt;/th&gt;
&lt;th&gt;Target&lt;/th&gt;
&lt;th&gt;Date&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://docs.google.com/document/d/1BjHI_6HyL7pk2UYUNDlDY971B9AO83wD8DfEF6KNDfs/edit&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;AI Policy Conference&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;ETH Zurich&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;AI&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://docs.google.com/document/d/1BjHI_6HyL7pk2UYUNDlDY971B9AO83wD8DfEF6KNDfs/edit&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;August 1&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;14/09/21&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://sites.google.com/site/uscfom/finance-organizations-and-markets-fom-research-group&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Finance, Organizations and Markets (FOM) Conference&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Dartmouth College&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Finance, IO&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://sites.google.com/site/uscfom/finance-organizations-and-markets-fom-research-group&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;August 14&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;28/10/21&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;br&gt;
&lt;h2 id=&#34;september&#34;&gt;September&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Conference&lt;/th&gt;
&lt;th&gt;Organizer&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Field&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Deadline&lt;/th&gt;
&lt;th&gt;Target&lt;/th&gt;
&lt;th&gt;Date&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://why21.causalai.net/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Causal Inference &amp;amp; Machine Learning: Why now?&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;NeurIPS&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Econometrics&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://why21.causalai.net/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;September 18&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;13/12/21&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://www.ub.edu/school-economics/ewmes&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ES European Winter Meeting&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Econometricc Society&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;All&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://www.ub.edu/school-economics/ewmes&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;September 19&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Senior&lt;/td&gt;
&lt;td&gt;13/12/21&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://www.causalscience.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Causal Data Science Conference&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;causalscience&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;All&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://www.causalscience.org/blog/causal-data-science-meeting-2021-call-for-papers/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;September 30&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;15/11/21&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;br&gt;
&lt;h2 id=&#34;october&#34;&gt;October&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Conference&lt;/th&gt;
&lt;th&gt;Organizer&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Field&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Deadline&lt;/th&gt;
&lt;th&gt;Target&lt;/th&gt;
&lt;th&gt;Date&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://www.tse-fr.eu/conferences/2022-15th-digital-economics-conference&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Digital Economics Conference&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;TSE&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Digital&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://www.tse-fr.eu/sites/default/files/TSE/documents/conf/2022/Digital_Economics/call_for_papers_digital_conf_2022.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;October 3&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;13/01/22&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://apios.org.au/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Asia-Pacific IO Conference&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Asia-Pacific IO Society&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;IO&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://apios.org.au/submission/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;October 22&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;13/12/21&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://www.sgvs.ch/conferences/ysem2021&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Young Swiss Economists Meeting&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;SSES&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;All&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://www.sgvs.ch/files/Call_for_Papers_YSEM_2021.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;October 25&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Junior&lt;/td&gt;
&lt;td&gt;11/02/21&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;br&gt;
&lt;h2 id=&#34;november&#34;&gt;November&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Conference&lt;/th&gt;
&lt;th&gt;Organizer&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Field&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Deadline&lt;/th&gt;
&lt;th&gt;Target&lt;/th&gt;
&lt;th&gt;Date&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://www.law.nyu.edu/conferences/2022-NextGen-Antitrust-Conference&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Next Generation of Antitrust, Data Privacy and Data Protection Scholars Conference&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;NYU&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;IO&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://www.law.nyu.edu/conferences/2022-NextGen-Antitrust-Conference&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;November 1&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;28/01/22&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://smye2021.weebly.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Spring Meeting of Young Economists&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;University of Bologna&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;All&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://smye2021.weebly.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;November 12&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Junior&lt;/td&gt;
&lt;td&gt;17/06/21&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://www.nber.org/conferences/industrial-organization-program-meeting-spring-2021&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;NBER IO Winter Meeting&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;NBER&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;IO&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://conference.nber.org/confsubmit/backend/cfp?id=IOs21&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;November 20&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Senior&lt;/td&gt;
&lt;td&gt;12/02/21&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://www.zew.de/en/events-and-professional-training/detail/2021-macci-annual-conference/3320&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;MaCCI Annual Conference&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;University of Mannheim&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;IO&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://www.zew.de/en/events-and-professional-training/detail/2021-macci-annual-conference/3320&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;November 30&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;12/03/21&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://www.tse-fr.eu/sites/default/files/TSE/documents/conf/2022/postal/call_postal_2022_v2.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Postal Economics Conference&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;TSE&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Digital&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://www.tse-fr.eu/sites/default/files/TSE/documents/conf/2022/postal/call_postal_2022_v2.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;November 30&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;07/04/22&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;br&gt;
&lt;h2 id=&#34;december&#34;&gt;December&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Conference&lt;/th&gt;
&lt;th&gt;Organizer&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Field&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Deadline&lt;/th&gt;
&lt;th&gt;Target&lt;/th&gt;
&lt;th&gt;Date&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://sites.google.com/site/ecbeconference&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Early-Career Behavioral Economics Conference&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Princeton University&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Behavioral&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://sites.google.com/site/ecbeconference/call&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;December 15&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;junior&lt;/td&gt;
&lt;td&gt;03/06/21&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;br&gt;
&lt;h2 id=&#34;undefined&#34;&gt;Undefined&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Conference&lt;/th&gt;
&lt;th&gt;Organizer&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Field&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Deadline&lt;/th&gt;
&lt;th&gt;Target&lt;/th&gt;
&lt;th&gt;Date&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://www.law.northwestern.edu/research-faculty/clbe/events/innovation/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Annual Conference on Innovation Economics&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Northwestern University&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Innovation&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://www.law.northwestern.edu/research-faculty/clbe/callforpapers/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;forthcoming&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;27/08/20&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://conference2.aau.at/event/4/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Conference on Mechanism and Institution Design&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Universität Klagenfurt&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Market Design&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;closed&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;11/06/20&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://sites.google.com/site/dteaworkshop/home&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;D-TEA Conference&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;HEC Paris&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Theory&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;closed&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;16/06/20&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://sites.wustl.edu/egsc/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Economics Graduate Student Conference&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Washington University&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;All&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;closed&lt;/td&gt;
&lt;td&gt;Junior&lt;/td&gt;
&lt;td&gt;07/11/20&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;http://conference.nber.org/confer/2020/SI2020/SI2020.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;NBER Summer Institute&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;NBER&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;All&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;invitation&lt;/td&gt;
&lt;td&gt;Senior&lt;/td&gt;
&lt;td&gt;06/07/20&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;http://competitionpolicy.ac.uk/events/annual-conferences&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CCP Annual Conference&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Centre for Competition Policy&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Comp policy&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;closed&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;24/06/21&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://www.res.org.uk/event-listing/2021-annual-conference.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;RES Annual Conference&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Royal Economics Society&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;All&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;closed&lt;/td&gt;
&lt;td&gt;Senior&lt;/td&gt;
&lt;td&gt;12/04/21&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://www.emerginginvestigators.org/conference&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;JEI Student Conference&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Harvard University&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;All&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;canceled&lt;/td&gt;
&lt;td&gt;Junior&lt;/td&gt;
&lt;td&gt;20/06/20&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://bfi.uchicago.edu/event/sixth-annual-conference-on-network-science-and-economics/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Annual Conference on Network Science and Economics&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Becker Friedman Institute&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Networks&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;canceled&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;27/03/20&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://saet.uiowa.edu/2021-annual-saet-conference/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Annual SAET Conference&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Society for the Advancement of Economic Theory&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Theory&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;closed&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;13/06/21&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
</description>
    </item>
    
    <item>
      <title>How to Work on a Remote Machine</title>
      <link>https://matteocourthoud.github.io/post/remote/</link>
      <pubDate>Mon, 06 Dec 2021 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/post/remote/</guid>
      <description>&lt;p&gt;In this page I will share tips on how to set up a remote machine and deploy your code there. I will first analyze SSH and then look at two specific applications: coding in Python and Julia.&lt;/p&gt;
&lt;h2 id=&#34;setup&#34;&gt;Setup&lt;/h2&gt;
&lt;p&gt;In order to start working on a remote server you need&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the server&lt;/li&gt;
&lt;li&gt;local shell&lt;/li&gt;
&lt;li&gt;SSH installed&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;SSH, or Secure Shell, is a protocol designed to transfer data between a client and a server (two computers basically) over an untrusted network.&lt;/p&gt;
&lt;p&gt;The way SSH works is it encrypts the connection using a pair of keys and the server, which is the computer you would connect to, is usually waiting for an SSH connection on Port 22.&lt;/p&gt;
&lt;p&gt;SSH is normally installed by default. To check if you have SSH installed, open the terminal and write &lt;code&gt;ssh&lt;/code&gt;. You should receive a message that looks like this&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;usage: ssh [-1246AaCfGgKkMNnqsTtVvXxYy] [-b bind_address] [-c cipher_spec]
[-D [bind_address:]port] [-E log_file] [-e escape_char]
[-F configfile] [-I pkcs11] [-i identity_file]
[-J [user@]host[:port]] [-L address] [-l login_name] [-m mac_spec] [-O ctl_cmd] [-o option] [-p port] [-Q query_option] [-R address] [-S ctl_path] [-W host:port] [-w local_tun[:remote_tun]]
[user@]hostname [command]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If SSH is not installed, you can install it using the following commands.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;sudo apt-get install openssh-server
sudo systemctl enable ssh
sudo systemctl start ssh
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now that you have installed SSH, we are ready to setup a remote connection.&lt;/p&gt;
&lt;p&gt;From the computer you want to access remotey, generate the public key.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;ssh-keygen -t rsa
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You will be asked for a location. If you decide to enter one manually then that will be the pair’s location, if you leave the default one it will be inside the &lt;code&gt;.ssh&lt;/code&gt; hidden folder in your home directory.&lt;/p&gt;
&lt;p&gt;Now you will be prompted for a password. If you enter one you will be asked for it every time you use the key, this works for added security. If you don’t want a password just press enter and continue without one.&lt;/p&gt;
&lt;p&gt;Two files were created. One file ends with the ‘.pub’ extension and the other one doesn’t. The file that ends with ‘.pub’ is your public key. This key needs to be in the computer you want to connect to (the server) inside a file called &lt;code&gt;authorized_keys&lt;/code&gt; . You can accomplish this with the following command:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;ssh-copy-id username@ip
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For example in my case to send the key to my computer it would be:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;ssh-copy-id sergiop@132.132.132.132
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you have MacOS there’s a chance you don’t have ssh-copy-id installed, in that case you can install it using&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;brew install ssh-copy-id
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you haven&amp;rsquo;t installed &lt;code&gt;brew&lt;/code&gt;, you can install it by following &lt;a href=&#34;https://brew.sh/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;this guide&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;connect&#34;&gt;Connect&lt;/h2&gt;
&lt;p&gt;To permanently add the SSH key, you can use the follwing command&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;ssh-add directory\key.pem
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Lastly, to connect, just type the following command.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;ssh username@ip
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Where &lt;code&gt;username&lt;/code&gt; is the server name and &lt;code&gt;ip&lt;/code&gt; is the public IP adress, e.g. 132.132.132.132.&lt;/p&gt;
&lt;p&gt;If your server is not public, you will not be able to access it.&lt;/p&gt;
&lt;p&gt;If your server is password protected, you will be prompted to insert a password when you connect. If not, you should protect it with a password.&lt;/p&gt;
&lt;h2 id=&#34;managing-screens&#34;&gt;Managing screens&lt;/h2&gt;
&lt;p&gt;While you are connected to the remote terminal, any disturbance to your connection will interrupt the code. In order to avoid that, you want to create separate screens. This will allow your code to run remotely undisturbed, irrespectively of your connection.&lt;/p&gt;
&lt;p&gt;First, you need to install &lt;code&gt;screen&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;brew install screen
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To create a new screen, just type&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;screen
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now you can lunch your code.&lt;/p&gt;
&lt;p&gt;After that, you want to detach from that screen so that the code can run remotely undisturbed.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;screen -d
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Another option is to use &lt;code&gt;ctrl+a&lt;/code&gt; followed by &lt;code&gt;ctrl+d&lt;/code&gt;. This will detach the screen without the need to type anythin in the terminal, in case the terminal is busy (most likely).&lt;/p&gt;
&lt;p&gt;To list the current active screens type&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;screen -ls
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you want to check at any time that your code is running, without re-attaching to the screen, you can just type&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;top
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;which is the general command to check active processes. To exit, use &lt;code&gt;ctrl+z&lt;/code&gt;, which generally terminates processes in the terminal.&lt;/p&gt;
&lt;p&gt;To reattach to your screen, type&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;screen -r
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In case you have multiple screens (you can check with &lt;code&gt;screen -ls&lt;/code&gt;), you can reattach to a specific one by typing&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;screen -r 12345
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;where &lt;code&gt;12345&lt;/code&gt; is the id of the screen.&lt;/p&gt;
&lt;p&gt;To kill a screen, type&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;screen -XS 12345 quit
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;where again &lt;code&gt;12345&lt;/code&gt; is the id of the screen.&lt;/p&gt;
&lt;h2 id=&#34;python-and-pycharm&#34;&gt;Python and Pycharm&lt;/h2&gt;
&lt;p&gt;If you are coding in Python, &lt;a href=&#34;https://www.jetbrains.com/pycharm/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PyCharm&lt;/a&gt; is one of the best IDEs. Among many features, it offers the possibility to set a remote compiler for your pthon console and to sync input and output files automatically.&lt;/p&gt;
&lt;p&gt;First, you need to have setup a remote SSH connection following the steps above. Importantly, you need to have added the public key to your machine using the &lt;code&gt;ssh-add&lt;/code&gt; command, as explained above.&lt;/p&gt;
&lt;p&gt;Then open Pytharm, go to the lower-right corner, where the current interpreter is listed (e.g. Pytohn 3.8), click it and select &lt;code&gt;interpreter settings&lt;/code&gt;.&lt;/p&gt;
&lt;img src=&#34;files/interpreter_settings.png&#34; alt=&#34;interpreter_settings&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;Click on the gear icon ⚙️ on the top-right corner and select &lt;code&gt;add&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;files/add.png&#34; alt=&#34;add&#34;&gt;&lt;/p&gt;
&lt;p&gt;Insert the server &lt;code&gt;host&lt;/code&gt; (IP address, e.g. 132.132.132.132) and &lt;code&gt;username&lt;/code&gt; (e.g. sergiop).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;files/configuration.png&#34; alt=&#34;configuration&#34;&gt;&lt;/p&gt;
&lt;p&gt;Next, you have to insert your credentials. If you have a password, insert it, otherwise you have to insert the path to your SSH key file.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;files/password.png&#34; alt=&#34;configuration&#34;&gt;&lt;/p&gt;
&lt;p&gt;Lastly, select the remote interpreter. If you are using a python version that is not default, browse to the preferred python installation folder. Also, check the box for &lt;code&gt;execute code giving this interpreter with root privileges via sudo&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;You can also select which remote folder to sync with your local project. By default, you are given a &lt;code&gt;tmp/pycharm_project_XX&lt;/code&gt; folder. You can change it if you want. I recommend also to have the last option checked: &lt;code&gt;automatically sync project files to the server&lt;/code&gt;. This will automatically synch all remote changes with your local machine, in your local project folder.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;files/folder.png&#34; alt=&#34;configuration&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;julia-and-juno&#34;&gt;Julia and Juno&lt;/h2&gt;
&lt;p&gt;If you are coding in Julia, &lt;a href=&#34;https://junolab.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Juno&lt;/a&gt; is the best IDE around. It&amp;rsquo;s an integration with &lt;a href=&#34;https://atom.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Atom&lt;/a&gt; with a dedicated compiler, local variables, syntax highlight, autocompletion.&lt;/p&gt;
&lt;p&gt;On Atom, you first need to install the &lt;a href=&#34;https://github.com/h3imdall/ftp-remote-edit&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;ftp-remote-edit&lt;/code&gt;&lt;/a&gt; package.&lt;/p&gt;
&lt;p&gt;Then go to the menu item &lt;code&gt;Packages &amp;gt; Ftp-Remote-Edit &amp;gt; Toggle&lt;/code&gt;.&lt;/p&gt;
&lt;img src=&#34;files/toggle.png&#34; alt=&#34;julia&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;A new &lt;code&gt;Remote&lt;/code&gt; panel will open with the default button to &lt;code&gt;Edit a new server&lt;/code&gt;.&lt;/p&gt;
&lt;img src=&#34;files/edit.png&#34; alt=&#34;julia&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;Click it and you will be able to set up your remote connection.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Press &lt;code&gt;New&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Insert your username in &lt;code&gt;The name of the server&lt;/code&gt;, for example &lt;code&gt;sergiop&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Insert your ip adress  in &lt;code&gt;The hostname or IP adress of the server&lt;/code&gt;, for example &lt;code&gt;123.123.123.123&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Select &lt;code&gt;SFTP - SSH File Transfer Protocol&lt;/code&gt; under &lt;code&gt;Protocol&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Select your &lt;code&gt;Logon&lt;/code&gt; option. You can either insert your password every time, just once, or use a keyfile.&lt;/li&gt;
&lt;li&gt;Insert again your username in &lt;code&gt;Username for autentication&lt;/code&gt;, again for example &lt;code&gt;sergiop&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;If you don&amp;rsquo;t want to start from the root folder, you can change the &lt;code&gt;Initial Directory&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;files/julia.png&#34; alt=&#34;julia&#34;&gt;&lt;/p&gt;
&lt;p&gt;Now you will be able to see your remote directory (named for example &lt;code&gt;sergiop&lt;/code&gt;) in the &lt;code&gt;Remote&lt;/code&gt; panel.&lt;/p&gt;
&lt;p&gt;To start using Julia remotely, just start a new remote Julia process from the menu on the left.&lt;/p&gt;
&lt;img src=&#34;files/remote.png&#34; alt=&#34;julia&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;Now you are ready to deploy your Julia code on your remote server!&lt;/p&gt;
&lt;h2 id=&#34;jupyter-notebooks&#34;&gt;Jupyter Notebooks&lt;/h2&gt;
&lt;p&gt;If you want to have a &lt;a href=&#34;https://jupyter.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Jupyter Notebook&lt;/a&gt; running remotely, the steps are the following. The main advantage of a Jupyter Notebook is that it allows you to mix text and code in a single file, similarly to &lt;a href=&#34;https://rmarkdown.rstudio.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;RMarkdown&lt;/a&gt;, with the advantage of not being contrained to use a R (or Python) kernel. For example, I often use Jupyter Notebook with Julia or Matlab Kernels. Moreover, you can also make nice slides out of it!&lt;/p&gt;
&lt;p&gt;First, connect to the remote machine. Look at &lt;a href=&#34;#setup&#34;&gt;section 1&lt;/a&gt; to set up your SSH connection.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;ssh username@ip
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Start a Jupyter Notebook in the remote machine.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;jupyter notebook --no-browser
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The command will open a jupyter notebook in the remote machine. To connect to it, we need to know which port it used. The default port is &lt;code&gt;8888&lt;/code&gt;. If that port is busy, it will look for another available one. We can see the port from the output in terminal.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Jupyter Notebook is running at:  http://localhost:XXXX/&amp;hellip;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Where &lt;code&gt;XXXX&lt;/code&gt; is the repote port used.&lt;/p&gt;
&lt;p&gt;Now we need to forward the remote port &lt;code&gt;XXXX&lt;/code&gt; to our local &lt;code&gt;YYYY&lt;/code&gt; port.&lt;/p&gt;
&lt;p&gt;Open a new &lt;em&gt;local&lt;/em&gt; shell. Type&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;ssh -L localhost:YYYY:localhost:XXXX username@ip
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Where &lt;code&gt;YYYY&lt;/code&gt; can be anything. I&amp;rsquo;d use the default port &lt;code&gt;8888&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;ssh -L localhost:8889:localhost:8888 username@ip
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now go to your browser and type&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;localhost:YYYY
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Which in my case is&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;localhost:8889
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This will open the remote Jupyter Notebook.&lt;/p&gt;
&lt;p&gt;Done!&lt;/p&gt;
&lt;p&gt;In case you want to check which Jupiter notebooks are running, type&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;jupyter notebook list
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To kill a notebook use&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;jupyter notebook stop XXXX
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;sources&#34;&gt;Sources&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://medium.com/@SergioPietri/how-to-setup-and-use-ssh-for-remote-connections-e86556d804dd&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;How To Setup And Use SSH For Remote Connections&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.junolab.org/stable/man/remote/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Connecting to a Julia session on a remote machine&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://ljvmiranda921.github.io/notebook/2018/01/31/running-a-jupyter-notebook/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Running a Jupyter notebook from a remote server&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Curriculum Vitae</title>
      <link>https://matteocourthoud.github.io/cv/</link>
      <pubDate>Wed, 10 Nov 2021 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/cv/</guid>
      <description>&lt;h2 id=&#34;education&#34;&gt;Education&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Ph.D.&lt;/strong&gt; Economics, &lt;em&gt;University of Zurich&lt;/em&gt;, 2022 (expected)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Visiting Assistant in Research, &lt;em&gt;Yale University&lt;/em&gt;, fall winter 2021-2022&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;M.S.&lt;/strong&gt; Economics and Social Sciences, &lt;em&gt;Bocconi University&lt;/em&gt;, 2016&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;B.S.&lt;/strong&gt; Economics and Social Sciences, &lt;em&gt;Bocconi University&lt;/em&gt;, 2014&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Primary interests&lt;/strong&gt;: Industrial Organization, Competition Policy, Artificial Intelligence &lt;br&gt;
&lt;strong&gt;Secondary interests&lt;/strong&gt;: Econometrics, Machine Learning, Market Design, Behavioral Economics&lt;/p&gt;
&lt;h2 id=&#34;working-papers&#34;&gt;Working Papers&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://matteocourthoud.github.io/project/foreclosure/&#34;&gt;Foreclosure Complementarities: Exclusionary Bundling and Predatory Pricing&lt;/a&gt; (&lt;em&gt;with G. Crawford&lt;/em&gt;) &lt;br&gt;
We use a computational model of industry dynamics to study exclusionary bundling and predatory pricing. We show that the two foreclosure practices are complementary and we investigate policies.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://matteocourthoud.github.io/project/alg_platform/&#34;&gt;Algorithmic Collusion Detection&lt;/a&gt; &lt;br&gt;
I show that pricing algorithms can learn reward-punishment schemes without observing rival’s actions and I propose a model-free test for algorithmic collusion based on observational data.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://matteocourthoud.github.io/project/alg_detection/&#34;&gt;Algorithmic Collusion in Online Marketplaces&lt;/a&gt; &lt;br&gt;
I show that profit maximizing online marketplaces, by algorithmically controlling consumers&#39; attention, have the incentives and ability to influence algorithmic pricing collusion.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://matteocourthoud.github.io/project/approximations/&#34;&gt;Approximation Methods for Large Dynamic Stochastic Games&lt;/a&gt; &lt;br&gt;
I compare existing approximation methods to compute Markow Perfect Equilibrium in dynamic stochastic games with large state spaces and propose a new one: games with random order.&lt;/p&gt;
&lt;h2 id=&#34;teaching&#34;&gt;Teaching&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Instructor &lt;a href=&#34;https://matteocourthoud.github.io/course/empirical-io/&#34;&gt;Empirical Industrial Organization&lt;/a&gt; (PhD)
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;University of Zurich&lt;/em&gt;, fall 2021&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;T.A. for &lt;a href=&#34;https://matteocourthoud.github.io/course/ml-econ/&#34;&gt;Machine Learning for Economic Analysis&lt;/a&gt; (MSc)
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;University of Zurich&lt;/em&gt;, fall 2020-21&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Instructor &lt;a href=&#34;https://pp4rs.github.io/2020-uzh/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Programming Practices for Research Economists&lt;/a&gt; (PhD)
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;University of Zurich&lt;/em&gt;, winter 2019&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;T.A. &lt;a href=&#34;https://matteocourthoud.github.io/course/metrics/&#34;&gt;Econometrics for Research Students&lt;/a&gt; (PhD)
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;University of Zurich&lt;/em&gt;, fall 2018-19&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;T.A. Markets, Organizations and Incentives (BSc)
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Bocconi University&lt;/em&gt;, fall 2016&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;T.A. Industrial Organization (BSc)
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Bocconi University&lt;/em&gt;, spring 2017&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;presentations&#34;&gt;Presentations&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;2021&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Asia and Pacific IO Conference (14/12)&lt;/li&gt;
&lt;li&gt;European Winter Meeting of the Econometric Society (13/12)&lt;/li&gt;
&lt;li&gt;Industrial Organization Prospectus Workshop &lt;em&gt;at Yale University&lt;/em&gt; (14/10)&lt;/li&gt;
&lt;li&gt;2nd AI and Policy Conference (14/09)&lt;/li&gt;
&lt;li&gt;Swiss IO Day (10/06)&lt;/li&gt;
&lt;li&gt;Annual Congress of the Swiss Society of Economics and Statistics Microeconomics (14/05)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;2020/19&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Applied Microeconomics Seminar &lt;em&gt;at University of Zürich&lt;/em&gt; (x4)&lt;/li&gt;
&lt;li&gt;Theory Seminar &lt;em&gt;at University of Zürich&lt;/em&gt; (x3)&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;academic-activities&#34;&gt;Academic Activities&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Referee&lt;/strong&gt; for the Journal of Competition Law and Economics&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Research Assistant&lt;/strong&gt; for Paolo Pinotti, Eliana La Ferrara, Anna Gibert
&lt;ul&gt;
&lt;li&gt;Bocconi University, 2016-17&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;non-academic-activities&#34;&gt;Non-Academic Activities&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Antitrust Consultant&lt;/strong&gt; for large online retailer platform
&lt;ul&gt;
&lt;li&gt;ongoing&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Intern&lt;/strong&gt; at EU Commission, Brussels (BE), DG-COMP, Chief Economist Office
&lt;ul&gt;
&lt;li&gt;European Commission, spring 2016&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;honors-awards-and-fellowships&#34;&gt;Honors, Awards, and Fellowships&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Doc.Mobility Scholarship (09/2021)
&lt;ul&gt;
&lt;li&gt;Swiss National Science Foundation&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;1st place, &lt;a href=&#34;https://analytics-club.org/wordpress/datathon/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Machine Learning Datathlon&lt;/a&gt; (05/2021)
&lt;ul&gt;
&lt;li&gt;ETH Zurich, Analytics Club&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Doctoral Scholarship (08/2017)
&lt;ul&gt;
&lt;li&gt;University of Zurich&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;thesis-supervisions&#34;&gt;Thesis Supervisions&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Jannick Sicher (MSc), &amp;ldquo;&lt;em&gt;Adversarial Approach to Algorithmic Collusion Detection&lt;/em&gt;&amp;rdquo;&amp;quot;&lt;/li&gt;
&lt;li&gt;Tim Herter (MSc), &amp;ldquo;&lt;em&gt;The Effect of U.S. Food Aid on Underweight&lt;/em&gt;&amp;rdquo;&lt;/li&gt;
&lt;li&gt;Andrin Pluss (BSc), &amp;ldquo;&lt;em&gt;Merger Simulation and Competition Models&lt;/em&gt;&amp;rdquo;&lt;/li&gt;
&lt;li&gt;Yuming Pan (MSc), &amp;ldquo;&lt;em&gt;The Impact of Free Legal Aid on Criminal Activities&lt;/em&gt;&amp;rdquo;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;skills&#34;&gt;Skills&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Computational&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Computing:&lt;/strong&gt; Python, Julia, R, MATLAB, Stata&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Design:&lt;/strong&gt; LaTeX, Markdown, Tableau, CSS&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Cloud&lt;/strong&gt;: AWS, Redshift&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Other&lt;/strong&gt;: Git, SQL, Unix, Docker&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Languages&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Italian (native)&lt;/li&gt;
&lt;li&gt;English (fluent)&lt;/li&gt;
&lt;li&gt;French (conversational)&lt;/li&gt;
&lt;li&gt;German (beginner)&lt;/li&gt;
&lt;li&gt;Spanish (beginner)&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.econ.uzh.ch/en/people/faculty/crawford.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Gregory Crawford&lt;/a&gt;, University of Zurich&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.econ.uzh.ch/en/people/faculty/schmutzler.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Armin Schmutzler&lt;/a&gt;, University of Zurich&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.econ.uzh.ch/en/people/faculty/kozbur.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Damian Kozbur&lt;/a&gt;, University of Zurich&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://faculty.unibocconi.eu/chiarafumagalli/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Chiara Fumagalli&lt;/a&gt;, Bocconi University&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;contact&#34;&gt;Contact&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;email&lt;/strong&gt; matteocourthoud &lt;code&gt;at&lt;/code&gt; uzh &lt;code&gt;dot&lt;/code&gt; ch&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;website&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;personal &lt;a href=&#34;https://matteocourthoud.github.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;matteocourthoud.github.io&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;official &lt;a href=&#34;https://www.econ.uzh.ch/en/people/graduatestudents/courthoud.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;econ.uzh.ch/people/graduatestudents/courthoud&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;twitter&lt;/strong&gt; &lt;a href=&#34;https://twitter.com/MatteoCourthoud/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;@matteocourthoud&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;github&lt;/strong&gt; &lt;a href=&#34;https://github.com/matteocourthoud&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;matteocourthoud&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Algorithmic Collusion on Online Marketplaces</title>
      <link>https://matteocourthoud.github.io/project/alg_platform/</link>
      <pubDate>Mon, 25 Oct 2021 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/project/alg_platform/</guid>
      <description>&lt;p&gt;The use of algorithms to set prices is particularly popular in online marketplaces, where sellers need to take quick decisions in complex dynamic environments. In this article, I investigate the role of online marketplaces in facilitating or preventing collusion among sellers that use pricing algorithms. In particular, I investigate a platform that has the ability to give prominence to certain products and automates this decision through a reinforcement learning algorithm, that maximizes the platform&amp;rsquo;s profits. Depending on whether the business model of the platform is more aligned with consumer welfare or with sellers&#39; profits (e.g., if it collects quantity or profit fees), the platform either prevents or facilitates collusion among algorithmic sellers. If the platform is also active as a seller, the so-called dual role, it is able to both induce sellers to set high prices and appropriate most of the profits. Importantly, self-preferencing only happens during the learning phase and not in equilibrium. I investigate a potential solution: separating the sales and marketplace divisions. The policy is effective but does not fully restore the competitive outcome when the fee is distortive, as in the case of a revenue fee.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>My Color Palette</title>
      <link>https://matteocourthoud.github.io/post/colors/</link>
      <pubDate>Sun, 24 Oct 2021 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/post/colors/</guid>
      <description>&lt;p&gt;Ok, this is a fun post. I am choosing&amp;hellip; &lt;strong&gt;my color palette&lt;/strong&gt;!&lt;/p&gt;
&lt;p&gt;I have decided to unify all the color palettes I have on my website, slides, graphs, etc&amp;hellip; into a unique universal color palette.&lt;/p&gt;
&lt;h2 id=&#34;main-color&#34;&gt;Main Color&lt;/h2&gt;
&lt;p&gt;First of all, I have to choose my main color.&lt;/p&gt;
&lt;!-- Coolors Palette Widget --&gt;   
&lt;script src=&#34;https://coolors.co/palette-widget/widget.js&#34;&gt;&lt;/script&gt;     
&lt;script data-id=&#34;038499353489456634&#34;&gt;new CoolorsPaletteWidget(&#34;038499353489456634&#34;, [&#34;003f5c&#34;]); &lt;/script&gt;
&lt;br&gt;
&lt;p&gt;Here are some shades of it.&lt;/p&gt;
&lt;!-- Coolors Palette Widget --&gt;
&lt;script src=&#34;https://coolors.co/palette-widget/widget.js&#34;&gt;&lt;/script&gt;
&lt;script data-id=&#34;030474315233368743&#34;&gt;new CoolorsPaletteWidget(&#34;030474315233368743&#34;, [&#34;002637&#34;,&#34;00324a&#34;,&#34;003f5c&#34;,&#34;17506b&#34;,&#34;2c6078&#34;]); &lt;/script&gt;
&lt;br&gt;
&lt;h2 id=&#34;related-palettes&#34;&gt;Related Palettes&lt;/h2&gt;
&lt;p&gt;Now I will build a couple of colors palettes based on it.&lt;/p&gt;
&lt;p&gt;The first one, is red oriented.&lt;/p&gt;
&lt;!-- Coolors Palette Widget --&gt;
&lt;script src=&#34;https://coolors.co/palette-widget/widget.js&#34;&gt;&lt;/script&gt;
&lt;script data-id=&#34;04738379522618503&#34;&gt;new CoolorsPaletteWidget(&#34;04738379522618503&#34;, [&#34;003f5c&#34;,&#34;444e86&#34;,&#34;955196&#34;,&#34;dd5182&#34;,&#34;ff6e54&#34;,&#34;ffa600&#34;]); &lt;/script&gt;
&lt;br&gt;
&lt;p&gt;Second one, is green oriented.&lt;/p&gt;
&lt;!-- Coolors Palette Widget --&gt;
&lt;script src=&#34;https://coolors.co/palette-widget/widget.js&#34;&gt;&lt;/script&gt;
&lt;script data-id=&#34;041398899421411417&#34;&gt;new CoolorsPaletteWidget(&#34;041398899421411417&#34;, [&#34;003f5c&#34;,&#34;00677f&#34;,&#34;00908f&#34;,&#34;2db88b&#34;,&#34;94dc7b&#34;,&#34;f9f871&#34;]); &lt;/script&gt;
&lt;br&gt;
&lt;h2 id=&#34;color-sequence&#34;&gt;Color Sequence&lt;/h2&gt;
&lt;p&gt;Now I need a high contrast scheme for graphs. I add one color at the time to check that contrast is always maximized.&lt;/p&gt;
&lt;!-- Coolors Palette Widget --&gt;
&lt;script src=&#34;https://coolors.co/palette-widget/widget.js&#34;&gt;&lt;/script&gt;
&lt;script data-id=&#34;09725467508229444&#34;&gt;new CoolorsPaletteWidget(&#34;09725467508229444&#34;, [&#34;003f5c&#34;,&#34;ff6e54&#34;,&#34;f9f871&#34;,&#34;2db88b&#34;,&#34;955196&#34;]); &lt;/script&gt;
&lt;br&gt;
&lt;p&gt;A milder version of the same palette is:&lt;/p&gt;
&lt;!-- Coolors Palette Widget --&gt;
&lt;script src=&#34;https://coolors.co/palette-widget/widget.js&#34;&gt;&lt;/script&gt;
&lt;script data-id=&#34;09460303923704312&#34;&gt;new CoolorsPaletteWidget(&#34;09460303923704312&#34;, [&#34;00798c&#34;,&#34;d1495b&#34;,&#34;edae49&#34;,&#34;52a369&#34;,&#34;756ab2&#34;]); &lt;/script&gt;
&lt;h2 id=&#34;resources&#34;&gt;Resources&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;To generate the palette: &lt;a href=&#34;https://learnui.design/tools/data-color-picker.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://learnui.design/tools/data-color-picker.html&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;To generate the palette (2): &lt;a href=&#34;https://mycolor.space/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://mycolor.space/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;To get different shades of the same color: &lt;a href=&#34;https://www.tutorialrepublic.com/html-reference/html-color-picker.php&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.tutorialrepublic.com&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;To build the palette: &lt;a href=&#34;https://htmlcolorcodes.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://htmlcolorcodes.com/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;To test the palette: &lt;a href=&#34;http://colormind.io/bootstrap/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://colormind.io/bootstrap/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;To embed the palette in a HTML page: &lt;a href=&#34;https://coolors.co/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://coolors.co/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Online Seminars in Economics and Finance</title>
      <link>https://matteocourthoud.github.io/post/seminars/</link>
      <pubDate>Sat, 09 Oct 2021 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/post/seminars/</guid>
      <description>&lt;p&gt;I will use this page to collect information about online seminars in Economics and Finance.&lt;/p&gt;
&lt;p&gt;If you know about public seminars that are missing from this list, please either &lt;a href=&#34;mailto:matteo.courthoud@econ.uzh.ch&#34;&gt;contact me&lt;/a&gt; or &lt;a href=&#34;https://github.com/matteocourthoud/website/blob/master/content/post/seminars/index.md&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;edit the table on Github&lt;/a&gt;!&lt;/p&gt;
&lt;h2 id=&#34;monday&#34;&gt;Monday&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Seminar&lt;/th&gt;
&lt;th&gt;Field&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Time (CET)&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Registration&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Recorded&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://gametheorynetwork.com/one-world-game-theory-seminar/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;One World Mathematical Game Theory Seminar&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Theory&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;15:00&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;No&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;No&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;http://virtual-md-seminar.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Virtual Market Design Seminar Series&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Theory&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;16:00&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;http://virtual-md-seminar.com/registration.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Yes&lt;/a&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;No&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://sites.google.com/site/berlinappliedmicroseminar/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Berlin Applied Micro Seminar (BAMS)&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Applied&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;16:00&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://sites.google.com/site/berlinappliedmicroseminar/co_hosted_events&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Yes&lt;/a&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;No&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://sites.google.com/view/vibesecon/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Virtual Behavioral Economics Seminar (VIBES)&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Behavioral&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;17:00&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://forms.gle/nAE6VLEZQqeRkSuK6&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Yes&lt;/a&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;No&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://vquantmarketing.substack.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Virtual Quantitative Marketing Seminar&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Marketing&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;17:30&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://vquantmarketing.substack.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Yes&lt;/a&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;No&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://sammf.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Search and Matching in Macro and Finance Virtual Seminar Series&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Macro&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;18:00&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://sammf.com/sign-up/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Yes&lt;/a&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;No&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;tuesday&#34;&gt;Tuesday&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Seminar&lt;/th&gt;
&lt;th&gt;Field&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Time (CET)&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Registration&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Recorded&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://www.tse-fr.eu/new-online-seminar-economics-platforms&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Online Economics of Platforms Seminar&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;IO&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;14:00&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;mailto:marie-helene.dufour@tse-fr.eu&#34;&gt;Yes&lt;/a&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://www.youtube.com/channel/UCQLTomj3LkQ_8rKGfxxmYvw&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Yes&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://persuasion.wp.st-andrews.ac.uk/seminars/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Seminar Series on Communication and Persuasion&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Theory&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;16:00&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://docs.google.com/forms/d/e/1FAIpQLSc3Sq3GGNDEIa5lcbda9a45sjVldHZrJlRKH-jyPobZ1oE2Aw/viewform?usp=sf_link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Yes&lt;/a&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;No&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://sites.google.com/view/ocis/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Online Causal Inference Seminar&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Econometrics&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;16:30&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://www.google.com/url?q=https%3A%2F%2Fmailman.stanford.edu%2Fmailman%2Flistinfo%2Fonline-causal-inference-seminar&amp;amp;sa=D&amp;amp;sntz=1&amp;amp;usg=AFQjCNGgPMLB-5Iv0SRBiJHXlIhxo2ta2A&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Yes&lt;/a&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://sites.google.com/view/ocis/past-talks-and-recordings?authuser=0&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Yes&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://vdevecon.wixsite.com/website&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Virtual Development Economics Seminar Series (VDEV)&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Development&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;17:00&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://us02web.zoom.us/webinar/register/WN_m4Ws1VxXRry_kwZoT8T5WA&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Yes&lt;/a&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://www.youtube.com/channel/UC9NMehzZBlChKSiie1DFCaA/featured?view_as=public&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Yes&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://sites.google.com/view/virtualmacro/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Virtual Macro Seminar Series&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Macro&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;18:00&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://www.google.com/url?q=https%3A%2F%2Fstockholmuniversity.zoom.us%2Fwebinar%2Fregister%2F7815862675026%2FWN_yNkq5FpES9yJjpjdqYWFbA&amp;amp;sa=D&amp;amp;sntz=1&amp;amp;usg=AFQjCNHWMu_cBPhLdq_uqy1eugAzEy2bzg&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Yes&lt;/a&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://sites.google.com/view/virtualmacro/past-seminars&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Yes&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://sites.google.com/view/lfos&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Labor &amp;amp; Finance Online Seminar&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Labor&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;19:15&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://sites.google.com/view/lfos/register&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Yes&lt;/a&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;No&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://web.stanford.edu/~leinav/teaching/IOIOspring2020.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Interactive Online IO Seminar (IO^2)&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;IO&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;21:00&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://stanford.zoom.us/webinar/register/WN_A85qA0DSQmmJeHBimyw2MQ&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Yes&lt;/a&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;No&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;wednesday&#34;&gt;Wednesday&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Seminar&lt;/th&gt;
&lt;th&gt;Field&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Time (CET)&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Registration&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Recorded&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://sites.google.com/view/virtual-io-seminar/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CEPR Virtual IO Seminar (VIOS)&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;IO&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;16:00&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://drive.google.com/open?id=1-dtGhiM2EqtXnkTztoLzDO7cNB_ekOFfbuckSSIotxU&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Yes&lt;/a&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;No&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://mailchi.mp/cepr/cepr-webinar-polecon-series-reminder&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CEPR Political Economy Seminar Series&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Political&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;16:00&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://zoom.us/webinar/register/WN_3MqzgQR2RQ-1nu8DD2LxeA&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Yes&lt;/a&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;No&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://www.jku.at/en/department-of-economics/research/research-events/online-economics-research-seminar/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ONLINE Economics Research Seminar (JKU)&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Applied&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;16:00&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;mailto:alexander.ahammer@jku.at&#34;&gt;Yes&lt;/a&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://www.jku.at/en/department-of-economics/research/research-events/online-economics-research-seminar/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Yes&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://economics.uchicago.edu/content/afe-seminar-series&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Virtual AFE Seminar Series&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Experimental&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;18:00&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://economics.uchicago.edu/content/afe-2020&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Yes&lt;/a&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;No&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://drive.google.com/file/d/1FEnt42opuzpQiJtPUF1lHqN3Zdp9mdhZ/view&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Virtual Finance Theory Seminar&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Finance&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;18:00&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;mailto:[virtualfinancetheoryseminar.com]%28mailto:mail@virtualfinancetheoryseminar.com%29&#34;&gt;Yes&lt;/a&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;No&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://sites.google.com/site/plustcs/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;TCS+&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Computer Science&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;19:00&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://sites.google.com/site/plustcs/livetalk/live-seat-reservation&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Yes&lt;/a&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://sites.google.com/site/plustcs/past-talks&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Yes&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;http://tamuz.caltech.edu/cettc/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Caltech Economic Theory at the Time of Cholera&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Theory&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;21:00&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;No&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;http://tamuz.caltech.edu/cettc/#x1-50003&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Yes&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;thursday&#34;&gt;Thursday&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Seminar&lt;/th&gt;
&lt;th&gt;Field&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Time (CET)&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Registration&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Recorded&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://sites.google.com/view/euro-quant-marketing-seminar/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;European Quant Marketing Seminar&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Marketing&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;14:00&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;No&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://sites.google.com/view/euro-quant-marketing-seminar/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Yes&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://sites.google.com/view/macci-epos-virtual-io-seminar&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;MaCCI EPoS Virtual IO Seminar&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;IO&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;15:00&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://sites.google.com/view/macci-epos-virtual-io-seminar/program/registration&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Yes&lt;/a&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;No&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://www.digitalecon.org/seminar&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Virtual Digital Economy Seminar (VIDE)&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Media&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;16:00&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://www.digitalecon.org/seminar&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Yes&lt;/a&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://www.digitalecon.org/seminar/past-seminars&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Yes&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;http://middexlab.weebly.com/virtual-seminar-series.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;MiddExLab Virtual Seminar Series&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Experimental&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;18:00&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;http://middexlab.weebly.com/virtual-seminar-series.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Yes&lt;/a&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;No&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://www.tradedynamics.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Virtual International Trade and Macro Seminar&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Macro&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;20:00&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://forms.gle/uiNjSptWjvucDbHSA&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Yes&lt;/a&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://www.tradedynamics.org/video-archive&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Yes&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;friday&#34;&gt;Friday&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Seminar&lt;/th&gt;
&lt;th&gt;Field&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Time (CET)&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Registration&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Recorded&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://sites.google.com/view/comsoc-seminar/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;International Seminar Series on Social Choice (COMSOC)&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Theory&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;15:00&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://www.google.com/url?q=https%3A%2F%2Flist.uva.nl%2Fmailman%2Flistinfo%2Fcomsoc-video-seminar&amp;amp;sa=D&amp;amp;sntz=1&amp;amp;usg=AFQjCNEAx47kVC25VUer3fp05Mw7vcZwuA&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Yes&lt;/a&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://sites.google.com/view/comsoc-seminar/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Yes&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://web.stanford.edu/~leinav/teaching/IOIOspring2020.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Interactive Online IO Seminar (IO^2)&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;IO&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;16:00&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://stanford.zoom.us/webinar/register/WN_Tb_FyMJ1RCeNF1HmCVvjMQ&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Yes&lt;/a&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;No&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://sites.google.com/view/dcpec/events/webinar&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Political Economy Webinar Series&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Political&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;17:00&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://forms.gle/yohLo3pk898Yq9Sf9&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Yes&lt;/a&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;No&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://www.chamberlainseminar.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The Gary Chamberlain Online Seminar&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Econometrics&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;18:00&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://www.google.com/url?q=https%3A%2F%2Fmailman.stanford.edu%2Fmailman%2Flistinfo%2Fchamberlainseminar&amp;amp;sa=D&amp;amp;sntz=1&amp;amp;usg=AFQjCNHJFMMiJMowt_vAtuBWbrK-4PA2IA&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Yes&lt;/a&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://www.chamberlainseminar.org/past-seminars&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Yes&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://www.virtualfinance.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Virtual Finance Workshop&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Finance&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;18:30&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;http://www.google.com/url?q=http%3A%2F%2Feepurl.com%2FgYBYvP&amp;amp;sa=D&amp;amp;sntz=1&amp;amp;usg=AFQjCNGNgL-zAqniptGfniTAeZ_cTkOyLg&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Yes&lt;/a&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;No&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;http://www.rohitlamba.com/penntheon&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Penn State Theory Online&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Theory&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;20:00&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;mailto:l-micropenntheon-subscribe-request@lists.psu.edu&#34;&gt;Yes&lt;/a&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;http://www.rohitlamba.com/penntheon&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Yes&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://sites.google.com/view/dstheory/home&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Foundations of Data Science Virtual Talk Series&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Computer Science&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;20:00&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;No&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://sites.google.com/view/dstheory/past-talks&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Yes&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
</description>
    </item>
    
    <item>
      <title>Frequently Asked Questions in a PhD</title>
      <link>https://matteocourthoud.github.io/post/faq/</link>
      <pubDate>Tue, 05 Oct 2021 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/post/faq/</guid>
      <description>&lt;p&gt;In this page I will collect anquestions that I frequently asked myself during my PhD, possibly with answers.&lt;/p&gt;
&lt;p&gt;Personally, the article for PhD students that helped me the most is &lt;a href=&#34;https://medium.com/@paul.niehaus/doing-research-18cb310529e0&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;“Doing research”&lt;/a&gt; by Paul Niehaus. But beware, it might not work for everyone.&lt;/p&gt;
&lt;h2 id=&#34;starting-the-phd&#34;&gt;Starting the PhD&lt;/h2&gt;
&lt;h3 id=&#34;information&#34;&gt;Information&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.nature.com/articles/d41586-019-03459-7?sf223557541=1&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;ldquo;PhDs: the tortuous truth&amp;rdquo;&lt;/a&gt;, Chris Woolston, 2019.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://medium.economist.com/why-doing-a-phd-is-often-a-waste-of-time-349206f9addb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;ldquo;Why doing a PhD is often a waste of time&amp;rdquo;&lt;/a&gt;, The Economist, 2016&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.theguardian.com/careers/phd-right-career-option&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;ldquo;Should you do a PhD?&amp;quot;&lt;/a&gt;, Daniel K. Sokol, 2012.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://tertilt.vwl.uni-mannheim.de/bachelor/GradSchoolGuide.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;ldquo;So, you want to go to a grad school in economics?&amp;quot;&lt;/a&gt;, Ceyhun Elgin and Mario Solis-Garcia, 2007.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;applying&#34;&gt;Applying&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://james-tierney.medium.com/how-to-ask-your-professor-for-a-letter-of-recommendation-f06e8b2f2c64&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;ldquo;How to Ask Your Professor for a Letter of Recommendation&amp;rdquo;&lt;/a&gt;, James Tierney, 2020.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://drive.google.com/file/d/16eUvtahziPyBTpX_ZeyXjPck2OyinfHH/view&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;ldquo;Pre-Doc Guide&amp;rdquo;&lt;/a&gt;, Alvin Christian, 2019.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://athey.people.stanford.edu/professional-advice&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;ldquo;Advice for Applying to Grad School in Economics&amp;rdquo;&lt;/a&gt;, Susan Athey, 2016.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://qz.com/116081/the-complete-guide-to-getting-into-an-economics-phd-program/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;ldquo;The complete guide to getting into an economics PhD program&amp;rdquo;&lt;/a&gt;, Miles Kimball, 2013.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.ericzwick.com/public_goods/twelve_steps.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;ldquo;The 12 Step Program for Grad School&amp;rdquo;&lt;/a&gt;, Erik Zwick.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;starting&#34;&gt;Starting&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/hagertynw/grad-school-reflections/blob/master/grad_school_reflections.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;ldquo;Reflections on Grad School in Economics&amp;rdquo;&lt;/a&gt;, Nick Hagerty, 2020.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://law.vanderbilt.edu/phd/How_to_Survive_1st_Year.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;ldquo;How to survive your first year of graduate school in economics&amp;rdquo;&lt;/a&gt;, Matthew Pearson, 2005.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;during-the-phd&#34;&gt;During the PhD&lt;/h2&gt;
&lt;h3 id=&#34;mental-health&#34;&gt;Mental Health&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://scholar.harvard.edu/files/bolotnyy/files/bbb_mentalhealth_paper.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;ldquo;Graduate Student Mental Health: Lessons from American Economics Departments&amp;rdquo;&lt;/a&gt;, Bolotnyy, Valentin, Matthew Basilico, and Paul Barreira, 2021.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.insidehighered.com/news/2019/11/14/phd-student-poll-finds-mental-health-bullying-and-career-uncertainty-are-top&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;ldquo;Mental Health, Bullying, Career Uncertainty&amp;rdquo;&lt;/a&gt;, Colleen Flahert, 2019.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.sciencemag.org/careers/2019/03/how-mindfulness-can-help-phd-students-deal-mental-health-challenges&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;ldquo;How mindfulness can help Ph.D. students deal with mental health challenges&amp;rdquo;&lt;/a&gt;, Katie Langin, 2019.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.phdstudies.com/article/managing-your-mental-health-as-a-phd-student/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;ldquo;Managing Your Mental Health as a PhD Student&amp;rdquo;&lt;/a&gt;, Joanna Hughes, 2019.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.psychologytoday.com/us/blog/emotional-mastery/201904/what-makes-it-so-hard-ask-help&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;ldquo;What Makes It So Hard to Ask for Help?&amp;quot;&lt;/a&gt;, Joan Rosenberg, 2019.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.sciencemag.org/careers/2018/11/grad-school-depression-almost-took-me-end-road-i-found-new-start&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;ldquo;Grad school depression almost took me to the end of the road—but I found a new start&amp;rdquo;&lt;/a&gt;, Francis Aguisanda, 2018.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.nature.com/articles/nj7587-555a.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;ldquo;Faking it&amp;rdquo;&lt;/a&gt;, Chris Woolston, 2016.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://blogs.nature.com/naturejobs/2016/09/14/panic-and-a-phd/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;ldquo;Panic and a PhD&amp;rdquo;&lt;/a&gt;, Jack Leeming, 2016.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://qz.com/547641/theres-an-awful-cost-to-getting-a-phd-that-no-one-talks-about/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;ldquo;There’s an awful cost to getting a PhD that no one talks about&amp;rdquo;&lt;/a&gt;, Jennifer Walker, 2015.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;research-and-ideas&#34;&gt;Research and Ideas&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://www.ricardodahis.com/files/papers/Dahis_Advice_Research.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;ldquo;Advice for Academic Research&amp;rdquo;&lt;/a&gt;, Ricardo Dahis, 2021.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.aeaweb.org/articles?id=10.1257/jel.20191573&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;ldquo;Sins of Omission and the Practice of Economics&amp;rdquo;&lt;/a&gt;, George A. Akerlof, 2020.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://medium.com/@paul.niehaus/doing-research-18cb310529e0&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;ldquo;Doing research&amp;rdquo;&lt;/a&gt;, Paul Niehaus, 2019.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://static1.squarespace.com/static/55c143d9e4b0cb07521c6d17/t/5b4f409f575d1ff83c2f12d8/1531920545061/PhDGuidebook.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;ldquo;An unofficial guidebook for PhD students in economics and education&amp;rdquo;&lt;/a&gt;, Alex Eble, 2018.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://iu.box.com/s/0ha9gcq0t22kyyy1rqv15mkmauw1py18&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;ldquo;The Research Productivity of New PhDs in Economics: The Surprisingly High Non-Success of the Successful&amp;rdquo;&lt;/a&gt;, John P. Conley and Ali Sina Önder, 2014.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://econ.lse.ac.uk/staff/spischke/phds/How%20to%20start.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;ldquo;How to get started on research in economics?&amp;quot;&lt;/a&gt;, Steve Pischke, 2009.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://iu.box.com/s/km7cxhcxgfcdpk4cp38b47x7is7lum11&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;ldquo;The Importance of Stupidity in Scientific Research&amp;rdquo;&lt;/a&gt;, Martin A. Schwartz, 2008.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://stevepavlina.com/blog/2007/01/7-rules-for-maximizing-your-creative-output/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;ldquo;7 Rules for Maximizing Your Creative Output&amp;rdquo;&lt;/a&gt;, Steve Pavlina, 2007.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://people.ischool.berkeley.edu/~hal/Papers/how.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;ldquo;How To Build An Economic Model in Your Spare Time&amp;rdquo;&lt;/a&gt;, Hal. R. Varian, 1998.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.columbia.edu/~drd28/Thesis%20Research.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;ldquo;Ph.D. Thesis Research: Where do I Start?&amp;quot;&lt;/a&gt;, Don Davis.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;presenting&#34;&gt;Presenting&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://david-schindler.de/unfair-questions/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;ldquo;Unfair Questions&amp;rdquo;&lt;/a&gt;, David Schindler, 2021.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/paulgp/beamer-tips/blob/master/slides.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;ldquo;Beamer Tips for Presentations&amp;rdquo;&lt;/a&gt;, Paul Goldsmith-Pinkham, 2020.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.princeton.edu/~reddings/tradephd/public_speaking_for_academic_economists.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;ldquo;Public Speaking for Academic Economists&amp;rdquo;&lt;/a&gt;, Rachel Meager, 2017.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.europeanjobmarketofeconomists.org/uploads/HowToPresent_LaFerrara.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;ldquo;How to present your job market paper&amp;rdquo;&lt;/a&gt;, Eliana La Ferrara, 2018.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://people.bu.edu/guren/Guren_HowToGiveALunchTalk.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;ldquo;How To Give a Lunch Talk&amp;rdquo;&lt;/a&gt;, Adam Guren, 2018.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://chrisblattman.com/2010/02/22/the-discussants-art/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;ldquo;The Discussant&amp;rsquo;s Art&amp;rdquo;&lt;/a&gt;, Chris Blattman, 2010.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://papers.ssrn.com/sol3/papers.cfm?abstract_id=1332144&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;ldquo;How to be a Great Conference Participants&amp;rdquo;&lt;/a&gt;, Art Carden, 2009.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://econ.lse.ac.uk/staff/spischke/phds/The%20Big%205.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;ldquo;The &amp;ldquo;Big 5&amp;rdquo; and Other Ideas For Presentations&amp;rdquo;&lt;/a&gt;, Cox, Donald,  2000.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://iu.box.com/s/aw92d7kl7xh5s4zsub8jq3qnknq9zcsi&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;ldquo;How to Give an Applied Micro Talk&amp;rdquo;&lt;/a&gt;, Jesse M. Shapiro.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://iu.box.com/s/37j3eip7x9fdg30n4eeepu92228eb999&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;ldquo;Tips on How to Avoid Disaster in Presentations&amp;rdquo;&lt;/a&gt;, Monika Piazzesi.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.ssc.wisc.edu/~bhansen/placement/Seminar%20Slides.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;ldquo;Seminar Slides &amp;ldquo;&lt;/a&gt;, Bruce Hansen.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;writing&#34;&gt;Writing&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.google.com/url?q=https%3A%2F%2Fwww.dropbox.com%2Fs%2Fq7wjaidl5w91srt%2FGuest%20lecture%20FS.pdf%3Fdl%3D0&amp;amp;sa=D&amp;amp;sntz=1&amp;amp;usg=AFQjCNG_nRs6QlkZzWBHAy0PjF4jfEYBAw&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;ldquo;5 Steps Toward a Paper&amp;rdquo;&lt;/a&gt;, Frank Schilbach, 2019.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.nature.com/articles/d41586-019-02918-5&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;ldquo;Novelist Cormac McCarthy’s tips on how to write a great science paper&amp;rdquo;&lt;/a&gt;, Van Savage and Pamela Yeh, 2019.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://marcfbellemare.com/wordpress/12797&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;ldquo;The “Middle Bits” Formula for Applied Papers&amp;rdquo;&lt;/a&gt;, Marc Bellamare, 2018.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://marcfbellemare.com/wordpress/12060&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;ldquo;The Conclusion Formula&amp;rdquo;&lt;/a&gt;, Marc Bellamare, 2018.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.albany.edu/spatial/training/5-The%20Introduction%20Formula.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;ldquo;The Introduction Formula&amp;rdquo;&lt;/a&gt;, Keith Head, 2015.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.people.fas.harvard.edu/~pnikolov/resources/writingtips.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;ldquo;Writing Tips For Economics Research Papers&amp;rdquo;&lt;/a&gt;, Plamen Nikolov, 2013.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://economics.harvard.edu/files/economics/files/tenruleswriting.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;ldquo;The Ten Most Important Rules of Writing Your Job Market Paper&amp;rdquo;&lt;/a&gt;, Goldin, Claudia and Lawrence Katz, 2008.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://schwert.ssb.rochester.edu/aec510/phd_paper_writing.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;ldquo;Writing Tips for Ph.D. Students&amp;rdquo;&lt;/a&gt;, John Cochrane, 2005.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://qed.econ.queensu.ca/pub/faculty/sumon/mkremer_checklist_paper.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;ldquo;Writing Papers: A Checklist&amp;rdquo;&lt;/a&gt;, Michael Kremer.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;referiing&#34;&gt;Referiing&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://blog.academicsequitur.com/2019/06/30/how-to-write-a-good-referee-report/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;ldquo;How To Write A Good Referee Report&amp;rdquo;&lt;/a&gt;, Tatyana Deryugina, 2019.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://iu.box.com/s/lgmhqw5uxvrb7qdrhxxskzki9pcwx7o6&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;ldquo;How to Review Manuscripts&amp;rdquo;&lt;/a&gt;, Elsevier, 2015.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://marcfbellemare.com/wordpress/5542&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;ldquo;Contributing to Public Goods: My 20 Rules for Refereeing&amp;rdquo;&lt;/a&gt;, Marc F. Bellemare, 2012&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;finishing-the-phd&#34;&gt;Finishing the PhD&lt;/h2&gt;
&lt;h3 id=&#34;the-job-market&#34;&gt;The Job Market&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.aeaweb.org/content/file?id=869&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;ldquo;A Guide and Advice for Economists on the U.S. Junior Academic Job Market 2018-2019 Edition&amp;rdquo;&lt;/a&gt;, John Cawley, 2018.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://chrisblattman.com/job-market/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;ldquo;Academic job market advice for economics, political science, public policy, and other professional schools&amp;rdquo;&lt;/a&gt;, Blattman, Christopher,  2015.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.ericzwick.com/public_goods/love_the_market.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;ldquo;How I Learned to Stop Worrying and Love the Job Market&amp;rdquo;&lt;/a&gt;, Erik Zwick, 2014.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;the-private-sector&#34;&gt;The Private Sector&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.linkedin.com/pulse/my-journey-from-economics-phd-data-scientist-tech-rose-tan&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;ldquo;My Journey from Economics PhD to Data Scientist in Tech&amp;rdquo;&lt;/a&gt;, Rose Tan, 2021.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://scarlet-chen.medium.com/tech-industry-jobs-for-econ-phds-54a276dda80b&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;ldquo;Tech Industry Jobs for Econ PhDs&amp;rdquo;&lt;/a&gt;, Scarlet Chen, 2020.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://scarlet-chen.medium.com/my-journey-from-econ-phd-to-tech-part-1-interview-prep-networking-d256918410a2&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;ldquo;My Journey from Econ PhD to Tech&amp;rdquo;&lt;/a&gt;, Scarlet Chen, 2020.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.nature.com/articles/d41586-018-05838-y&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;ldquo;Why it is not a ‘failure’ to leave academia&amp;rdquo;&lt;/a&gt;, Philipp Kruger, 2018.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;the-tenure-track&#34;&gt;The Tenure Track&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://blogs.scientificamerican.com/guest-blog/the-awesomest-7-year-postdoc-or-how-i-learned-to-stop-worrying-and-love-the-tenure-track-faculty-life/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;ldquo;The Awesomest 7-Year Postdoc or: How I Learned to Stop Worrying and Love the Tenure-Track Faculty Life&amp;rdquo;&lt;/a&gt;, Radhika Nagpal, 2013.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;more&#34;&gt;More&lt;/h2&gt;
&lt;p&gt;You can find more resources here:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;AEA &lt;a href=&#34;https://www.aeaweb.org/about-aea/committees/cswep/mentoring/reading&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Mentoring Reading Materials&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Johannes Pfeifer &lt;a href=&#34;https://sites.google.com/site/pfeiferecon/job-market-resources&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Job Market Resources&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Kristoph Kronenberg &lt;a href=&#34;https://sites.google.com/view/christoph-kronenberg/home/resources&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Resources&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Patrick Button &lt;a href=&#34;https://www.patrickbutton.com/resources&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Resources&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Ryan Edwards &lt;a href=&#34;http://www.ryanbedwards.com/resources&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Resources&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Jennifer Doleac &lt;a href=&#34;http://jenniferdoleac.com/resources/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Resources&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Amanda Agan &lt;a href=&#34;https://sites.google.com/site/amandayagan/writingadvice&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Writing and Presentation Advice&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Random forum &lt;a href=&#34;http://www.inhe365.com/thread-17506-1-1.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Resource Collection&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Foreclosure Complementarities</title>
      <link>https://matteocourthoud.github.io/project/foreclosure/</link>
      <pubDate>Fri, 01 Oct 2021 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/project/foreclosure/</guid>
      <description>&lt;p&gt;In recent merger cases across complementary markets, antitrust authorities have expressed foreclosure concerns. In particular, the presence of scale economies in one market might propagate to the complementary market, ultimately leading to the monopolization of both. In this paper, we investigate the interplay between two foreclosure practices: exclusionary bundling and predatory pricing in the setting of complementary markets with economies of scale. We show that the two practices are complementary when markets display economies of scale, exclusionary bundling is more likely and, when bundling is allowed, predatory pricing is more likely. We show that this outcome is due to exit-inducing behavior of dominant firms: shutting down predatory incentives restores competition in both markets. We investigate different policies: banning mergers between market leaders, allowing product bundling only when more than one firm is integrated and able to offer the bundle, and lastly knowledge sharing across firms in order to limit the economies of scale. All policies are effective, each for a different reason.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>How To Make A Personal Website with Hugo</title>
      <link>https://matteocourthoud.github.io/post/website/</link>
      <pubDate>Thu, 23 Sep 2021 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/post/website/</guid>
      <description>&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;In this tutorial, I am going to explain how to build a personal website on &lt;a href=&#34;https://www.github.com&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt; using the &lt;strong&gt;&lt;a href=&#34;https://wowchemy.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Wowchemy&lt;/a&gt;&lt;/strong&gt; theme from &lt;a href=&#34;https://gohugo.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Hugo&lt;/a&gt;, also known as &lt;a href=&#34;https://themes.gohugo.io/academic/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Hugo Academic&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Note that Hugo offers &lt;a href=&#34;https://themes.gohugo.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;many other website templates&lt;/a&gt;. I suggest checking them out. Some interesting alternatives are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://themes.gohugo.io/theme/hugo-story/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://themes.gohugo.io/theme/hugo-story/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://themes.gohugo.io/theme/somrat/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://themes.gohugo.io/theme/somrat/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://themes.gohugo.io/theme/hugo-uilite/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://themes.gohugo.io/theme/hugo-uilite/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://themes.gohugo.io/theme/story/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://themes.gohugo.io/theme/story/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;However, I will concentrate on the Hugo Academic theme, since it&amp;rsquo;s the one I used for this website and it&amp;rsquo;s the the best one for building academic profile pages. I also suggest checking out &lt;a href=&#34;https://sites.google.com/new&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Google Sites&lt;/a&gt; as they are a valid alternative.&lt;/p&gt;
&lt;p&gt;The first part of this guide is general and valid for any Hugo theme. The original guide can be found &lt;a href=&#34;https://wowchemy.com/docs/getting-started/install/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;create-website&#34;&gt;Create Website&lt;/h2&gt;
&lt;h3 id=&#34;0-prerequisites&#34;&gt;0. Prerequisites&lt;/h3&gt;
&lt;p&gt;Before we start, I will take for granted the following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;that you have an account on &lt;a href=&#34;https://www.github.com&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;that you have &lt;a href=&#34;https://www.r-project.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;R&lt;/a&gt; installed&lt;/li&gt;
&lt;li&gt;that you have  &lt;a href=&#34;https://www.rstudio.com&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;RStudio&lt;/a&gt; installed&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;1-create-github-repository&#34;&gt;1. Create Github Repository&lt;/h3&gt;
&lt;p&gt;First, go to your &lt;a href=&#34;https://www.github.com&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt; page and create a new repository (&lt;code&gt;+&lt;/code&gt; button in the top-right corner).&lt;/p&gt;
&lt;img src=&#34;img/new_repo.png&#34; alt=&#34;new_repo&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;Name the repository &lt;code&gt;username.github.io&lt;/code&gt; where &lt;code&gt;username&lt;/code&gt; is your Github username.&lt;/p&gt;
&lt;img src=&#34;img/name_repo.png&#34; alt=&#34;name_repo&#34; style=&#34;zoom:40%;&#34; /&gt;
&lt;p&gt;In my case, my github username is &lt;code&gt;matteocourthoud&lt;/code&gt;, therefore the repository is &lt;code&gt;matteocourthoud.github.io&lt;/code&gt; and my personal website is &lt;a href=&#34;https://matteocourthoud.github.io&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://matteocourthoud.github.io&lt;/a&gt;. Use the default settings when creating the repository.&lt;/p&gt;
&lt;h3 id=&#34;2-install-blogdown-and-hugo&#34;&gt;2. Install Blogdown and Hugo&lt;/h3&gt;
&lt;p&gt;Now you need to install Blogdown, which is the program what will allow you to build and deploy your website, and Hugo, which is the template generator.&lt;/p&gt;
&lt;p&gt;Switch to RStudio and type the following commands&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Install blogdown
install.packages(&amp;quot;blogdown&amp;quot;)

# Install Hugo
blogdown::install_hugo()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now everything should be ready.&lt;/p&gt;
&lt;h3 id=&#34;3-setup-folder&#34;&gt;3. Setup folder&lt;/h3&gt;
&lt;p&gt;Open RStudio and select &lt;code&gt;New Project&lt;/code&gt;&lt;/p&gt;
&lt;img src=&#34;img/new_project.png&#34; alt=&#34;new_project&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;Select &lt;code&gt;New Directory&lt;/code&gt; when asked where to create the project&lt;/p&gt;
&lt;img src=&#34;img/new_project2.png&#34; alt=&#34;new_project2&#34; style=&#34;zoom:40%;&#34; /&gt;
&lt;p&gt;Then select &lt;code&gt;Website using blogdown&lt;/code&gt; as project type&lt;/p&gt;
&lt;img src=&#34;img/new_project3.png&#34; alt=&#34;new_project3&#34; style=&#34;zoom:40%;&#34; /&gt;
&lt;p&gt;Now you have to select a couple of options&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;Directory name&lt;/code&gt;: here input the name of the folder which will contain all the website files. The name is irrelevant. I called mine &lt;code&gt;website&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Create project as a subdirectory of&lt;/code&gt;: select the directory in which you want to put the website folder.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Theme&lt;/code&gt;: input &lt;code&gt;wowchemy/starter-academic&lt;/code&gt; instead of the default theme.&lt;/li&gt;
&lt;/ul&gt;
&lt;img src=&#34;img/new_project4.png&#34; alt=&#34;new_project4&#34; style=&#34;zoom:40%;&#34; /&gt;
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: if you want to install a different theme, just go on the corresponding Github page (for example &lt;a href=&#34;https://github.com/caressofsteel/hugo-story&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/caressofsteel/hugo-story&lt;/a&gt;) and instead of &lt;code&gt;gcushen/hugo-academic&lt;/code&gt;, insert the corresponding Github repository (for example &lt;code&gt;caressofsteel/hugo-story&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;If you go into the website folder, it should look something like this.&lt;/p&gt;
&lt;img src=&#34;img/folder.png&#34; alt=&#34;folder&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;h3 id=&#34;4-build-website&#34;&gt;4. Build website&lt;/h3&gt;
&lt;p&gt;To build the website, open the RProject file &lt;code&gt;website.Rproj&lt;/code&gt; in RStudio and type&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;blogdown::hugo_build(local=TRUE)
&lt;/code&gt;&lt;/pre&gt;
&lt;img src=&#34;img/hugo_build.png&#34; alt=&#34;hugo_build&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;This command will generate a &lt;code&gt;public/&lt;/code&gt; subfolder in which the actual code of the website is stored.&lt;/p&gt;
&lt;p&gt;Don&amp;rsquo;t ask me why, but the option &lt;code&gt;local=TRUE&lt;/code&gt; seems to make a difference. Updating without it sometimes does not change the content in the &lt;code&gt;\public&lt;/code&gt; subfolder.&lt;/p&gt;
&lt;img src=&#34;img/folder2.png&#34; alt=&#34;folder2&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;To preview the website, type in RStudio&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;blogdown::serve_site()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The following preview should automatically open in your browser.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;img/prevew.png&#34; alt=&#34;prevew&#34;&gt;&lt;/p&gt;
&lt;p&gt;Previewing the website is very useful as it allows you to see live changes locally inside RStudio, before publishing them. This is the &lt;strong&gt;main advantage of working in RStudio&lt;/strong&gt;.&lt;/p&gt;
&lt;h3 id=&#34;5-publish-website&#34;&gt;5. Publish website&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Importantly&lt;/strong&gt;, before pushing the code online, you need to open the file &lt;code&gt;config.yaml&lt;/code&gt; and change the &lt;code&gt;baseurl&lt;/code&gt; to your future website url, which will be &lt;code&gt;https://username.github.io/&lt;/code&gt;, where &lt;code&gt;username&lt;/code&gt; is our Github username.&lt;/p&gt;
&lt;img src=&#34;img/username.png&#34; alt=&#34;username&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;Now that you have set the correct url, you have to push the changes from the &lt;code&gt;public/&lt;/code&gt; folder to your &lt;code&gt;username.github.io&lt;/code&gt; repository on Github.&lt;/p&gt;
&lt;p&gt;To do that, you need to get to the website folder. Let&amp;rsquo;s assume that the path to your folder is &lt;code&gt;Documents/website&lt;/code&gt;. Open the Terminal and type&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;cd Documents/website/public
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The following code will link the &lt;code&gt;public/&lt;/code&gt; folder, containing the actual code of the website, to your  &lt;code&gt;username.github.io&lt;/code&gt; repository.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Init git in the /website/public/ folder
git init

# Add and commit the changes
git add .
git commit -m &amp;quot;first commit&amp;quot;

# Set origin
git remote add origin https://github.com/username/username.github.io.git

# Rename local branch
git branch -M main

# And push your updates online
git push -u origin main
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Wait a few seconds (or minutes for heavy changes) and your website should be online!&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;If the website is not working&lt;/strong&gt;, you can check the following:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Is there anything in your &lt;code&gt;\public&lt;/code&gt; folder? (does it even exist?) If not, something went wrong when compiling the website with &lt;code&gt;blogdown::hugo_build()&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Inside your &lt;code&gt;\public&lt;/code&gt; folder, there should be an &lt;code&gt;index.html&lt;/code&gt; file. If you double-click on it, you should see a local preview of your website in your browser. If not, something in the website code is wrong.&lt;/li&gt;
&lt;li&gt;Is the content of your &lt;code&gt;\public&lt;/code&gt; folder exactly the same of the content of your Github repository? If not, something went wrong when pushing to Github.&lt;/li&gt;
&lt;li&gt;Did you name your Github repository &lt;code&gt;username.github.io&lt;/code&gt;, where &lt;code&gt;username&lt;/code&gt; is your Github username?&lt;/li&gt;
&lt;li&gt;Did you change the &lt;code&gt;baseurl&lt;/code&gt; option in the file &lt;code&gt;config.yaml&lt;/code&gt; to &lt;code&gt;https://username.github.io/&lt;/code&gt;, where &lt;code&gt;username&lt;/code&gt; is your Github username?&lt;/li&gt;
&lt;li&gt;You can check the list of websites deployments at &lt;code&gt;https://github.com/username/username.github.io/deployments&lt;/code&gt;. Control that they correspond with your commits.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;If all the conditions are satisfied, but the website is still not online, maybe it&amp;rsquo;s just a matter of time. Have some patience.&lt;/p&gt;
&lt;h2 id=&#34;basic-customization&#34;&gt;Basic Customization&lt;/h2&gt;
&lt;p&gt;The basic files that you want to modify to customize your website are the following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;config.yaml&lt;/code&gt;: general website information&lt;/li&gt;
&lt;li&gt;&lt;code&gt;config/_default/params.yaml&lt;/code&gt;: website customization&lt;/li&gt;
&lt;li&gt;&lt;code&gt;config/_default/menus.yaml&lt;/code&gt;: top bar / menu customization&lt;/li&gt;
&lt;li&gt;&lt;code&gt;content/authors/admin/_index.md&lt;/code&gt;: personal information&lt;/li&gt;
&lt;/ul&gt;
&lt;img src=&#34;img/files.png&#34; alt=&#34;files&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;For what concerns &lt;strong&gt;images&lt;/strong&gt;, there are two main things you might want to modify:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Profile picture: change the &lt;code&gt;content/authors/admin/avatar.jpg&lt;/code&gt; picture&lt;/li&gt;
&lt;li&gt;Website icon: change the &lt;code&gt;assets/media/icon.png&lt;/code&gt; picture&lt;/li&gt;
&lt;/ul&gt;
&lt;img src=&#34;img/images.png&#34; alt=&#34;images&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;In order to modify the &lt;strong&gt;widgets&lt;/strong&gt; on your homepage, go to &lt;code&gt;content/home/&lt;/code&gt; and modify the files inside. If you want to remove a section, just open the corresponding file and select &lt;code&gt;active: false&lt;/code&gt;. If there is no &lt;code&gt;active&lt;/code&gt; option, just copy the line &lt;code&gt;active: false&lt;/code&gt; in the corresponding file.&lt;/p&gt;
&lt;img src=&#34;img/widgets.png&#34; alt=&#34;widgets&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;In my website I have only the following sections set to true:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;about&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;projects&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;posts&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;contact&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;To change the &lt;strong&gt;color palette&lt;/strong&gt; of the website, go to &lt;code&gt;data\theme&lt;/code&gt; and generate a &lt;code&gt;custom.toml&lt;/code&gt; file with the following content:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-toml&#34;&gt;# Theme metadata
name = &amp;quot;Custom&amp;quot;

# Is theme light or dark?
light = true

# Primary
primary = &amp;quot;#284f7a&amp;quot;

# Menu
menu_primary = &amp;quot;#fff&amp;quot;
menu_text = &amp;quot;#34495e&amp;quot;
menu_text_active = &amp;quot;#284f7a&amp;quot;
menu_title = &amp;quot;#2b2b2b&amp;quot;

# Home sections
home_section_odd = &amp;quot;rgb(255, 255, 255)&amp;quot;
home_section_even = &amp;quot;rgb(247, 247, 247)&amp;quot;

[dark]
  link = &amp;quot;#bbdefb&amp;quot;
  link_hover = &amp;quot;#bbdefb&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then go to the &lt;code&gt;config/_default/params.yaml&lt;/code&gt; file and set the &lt;code&gt;theme&lt;/code&gt; to &lt;code&gt;custom&lt;/code&gt;.&lt;/p&gt;
&lt;img src=&#34;img/custom_theme.png&#34; alt=&#34;custom_theme&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;You can get more information on how to personalize it &lt;a href=&#34;https://wowchemy.com/docs/getting-started/customization/#custom-theme&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;To change the &lt;strong&gt;font&lt;/strong&gt;, go to &lt;code&gt;data\fonts&lt;/code&gt; and generate a &lt;code&gt;custom.toml&lt;/code&gt; file with the following content:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-toml&#34;&gt;# Font style metadata
name = &amp;quot;Custom&amp;quot;

# Optional Google font URL
google_fonts = &amp;quot;family=Roboto+Mono&amp;amp;family=Source+Sans+Pro:wght@200;300;400;700&amp;quot;

# Font families
heading_font = &amp;quot;Source Sans Pro&amp;quot;
body_font = &amp;quot;Source Sans Pro&amp;quot;
nav_font = &amp;quot;Source Sans Pro&amp;quot;
mono_font = &amp;quot;Roboto Mono&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then go to the &lt;code&gt;config/_default/params.yaml&lt;/code&gt; file and set the &lt;code&gt;font&lt;/code&gt; to &lt;code&gt;Custom&lt;/code&gt;.&lt;/p&gt;
&lt;img src=&#34;img/custom_font.png&#34; alt=&#34;custom_font&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;You can get more information on how to personalize it &lt;a href=&#34;https://wowchemy.com/docs/getting-started/customization/#custom-font&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;. Importantly, by default the website supports only fonts of weight 400 and 700. If you want a lighter font, as the &lt;a href=&#34;https://fonts.google.com/specimen/Source&amp;#43;Sans&amp;#43;Pro&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Source Sans Pro&lt;/a&gt; I use for my website, you have to dig into the advanced customization (which requires HTML and CSS skills).&lt;/p&gt;
&lt;h2 id=&#34;advanced-customization&#34;&gt;Advanced Customization&lt;/h2&gt;
&lt;p&gt;Advanced customization is possible but &lt;strong&gt;it&amp;rsquo;s a pain&lt;/strong&gt;. You basically want to go inside &lt;code&gt;themes\github.com\wowchemy\wowchemy-hugo-modules\wowchemy&lt;/code&gt; and start digging. Tip: you want to start digging in the folowing places:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;In &lt;code&gt;layouts\partials&lt;/code&gt; to edit the HTML files&lt;/li&gt;
&lt;li&gt;In &lt;code&gt;assets\scss&lt;/code&gt; to edit the SCSS code&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If you want to copy my exact theme, I have published my custom theme here: &lt;a href=&#34;https://github.com/matteocourthoud/custom-wowchemy-settings&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/matteocourthoud/custom-wowchemy-settings&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;You have to do the following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;go inside the &lt;code&gt;theme&lt;/code&gt; folder&lt;/li&gt;
&lt;li&gt;copy the content of the &lt;code&gt;custom-wowchemy-theme&lt;/code&gt; repository in a folder there&lt;/li&gt;
&lt;li&gt;go to the &lt;code&gt;custom_toml&lt;/code&gt; file into the MODULES section&lt;/li&gt;
&lt;/ul&gt;
&lt;img src=&#34;img/config_before.png&#34; alt=&#34;config_before&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;ul&gt;
&lt;li&gt;change the second link to the folder with the custom settings&lt;/li&gt;
&lt;/ul&gt;
&lt;img src=&#34;img/config_after.png&#34; alt=&#34;config_after&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;Now your website should look quite similar to mine! :)&lt;/p&gt;
&lt;h2 id=&#34;google-analytics&#34;&gt;Google Analytics&lt;/h2&gt;
&lt;p&gt;In order for the website to be displayed in Google searches, you need to ask Google to track it.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Go to the &lt;a href=&#34;https://search.google.com/search-console&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Google Search Console website&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Use the &lt;a href=&#34;https://search.google.com/search-console?action=inspect&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;URL Inspection tool&lt;/a&gt; to inspect the URL of your personal website: &lt;code&gt;https://username.github.io&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Use &lt;strong&gt;Request indexing&lt;/strong&gt; to request Google to index your website so that it will apprear in Google searches.&lt;/li&gt;
&lt;li&gt;Under &lt;strong&gt;Sitemap&lt;/strong&gt; provide the link to your website sitemap to Google. It should be &lt;code&gt;https://username.github.io/sitemap.xml&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In order to receive statistics on your website, you first need to get your associated tracking code.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Go to the &lt;a href=&#34;https://www.google.com/analytics/web/#home/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Google Analytics website&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Click &lt;a href=&#34;https://support.google.com/analytics/answer/6132368&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Admin&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Select an account from the menu in the &lt;strong&gt;ACCOUNT&lt;/strong&gt; column.&lt;/li&gt;
&lt;li&gt;Select a property from the menu in the &lt;strong&gt;PROPERTY&lt;/strong&gt; column.&lt;/li&gt;
&lt;li&gt;Under &lt;strong&gt;PROPERTY&lt;/strong&gt;, click Tracking Info &amp;gt; Tracking Code.&lt;/li&gt;
&lt;li&gt;Your tracking ID and property number are displayed at the top of the page. It should have the form &lt;code&gt;UA-xxxxxxxxx-1&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Now that we have the website tracking code, we need to insert it into the &lt;code&gt;googleAnalytics&lt;/code&gt; section of the &lt;code&gt;config/_default/params.yaml&lt;/code&gt; file.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;marketing:
  google_analytics: &#39;UA-xxxxxxxxx-1&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The mobile application of &lt;a href=&#34;https://analytics.google.com/analytics/web/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Google Analytics&lt;/a&gt; is particular intuitive and allows you to monitor your website traffic in detail. You just need to link the website from the &lt;a href=&#34;https://search.google.com/search-console&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Google Sesarch Console&lt;/a&gt; and then you can motitor you website from this platform. There is also a very nice mobile app for both &lt;a href=&#34;https://play.google.com/store/apps/details?id=com.google.android.apps.giant&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Android&lt;/a&gt; and &lt;a href=&#34;https://apps.apple.com/us/app/google-analytics/id881599038&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;iOS&lt;/a&gt; to monitor your website from your smartphone.&lt;/p&gt;
&lt;p&gt;Another good free tool to analyze the &amp;ldquo;quality&amp;rdquo; of your website is &lt;a href=&#34;https://www.seomechanic.com/seo-analyzer/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;SEO Mechanic&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;p&gt;Here are the main resources I used to write this guide:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Wowchemy website: &lt;a href=&#34;https://wowchemy.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://wowchemy.com/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Old Academic website: &lt;a href=&#34;https://sourcethemes.com/academic/docs/install/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://sourcethemes.com/academic/docs/install/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Guide for the Terminal: &lt;a href=&#34;https://github.com/fliptanedo/FlipWebsite2017&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/fliptanedo/FlipWebsite2017&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;iframe src=&#34;https://docs.google.com/forms/d/e/1FAIpQLSdKqWZQAbHkcNLYvjxZ_fVrAZCJkNmsBtUBCCcfZxYzJCaqIQ/viewform?embedded=true&#34; width=&#34;640&#34; height=&#34;530&#34; frameborder=&#34;0&#34; marginheight=&#34;0&#34; marginwidth=&#34;0&#34;&gt;Loading…&lt;/iframe&gt;
</description>
    </item>
    
    <item>
      <title>Algorithmic Collusion Detection</title>
      <link>https://matteocourthoud.github.io/project/alg_detection/</link>
      <pubDate>Sun, 12 Sep 2021 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/project/alg_detection/</guid>
      <description>&lt;p&gt;Reinforcement learning algorithms are gradually replacing humans in many decision-making processes, such as pricing in high-frequency markets. Recent studies on algorithmic pricing have shown that algorithms can learn sophisticated grim-trigger strategies with the intent of keeping supra-competitive prices. This paper focuses on algorithmic collusion detection. One frequent suggestion is to look at the inputs of the strategies, for example at whether the algorithms condition their prices on previous competitors&#39; prices. The first part of the paper shows that this approach might not be sufficient to detect collusion since the algorithms can learn reward-punishment schemes that are fully independent of the rival’s actions. The mechanism that ensures the stability of supra-competitive prices is self-punishment.&lt;/p&gt;
&lt;p&gt;The second part of the paper explores a novel test for algorithmic collusion detection. The test builds on the intuition that as algorithms are able to learn to collude, they might be able to learn to exploit collusive strategies. In fact, since they are not designed to learn sub-game perfect equilibrium strategies, there is the possibility that their strategies could be exploited. When one algorithm is unilaterally retrained, keeping the collusive strategies of its competitor fixed, it learns more profitable strategies. Usually, these strategies are more competitive, but not always. Since this change in strategies happens only when algorithms are colluding, retraining can be used as a test to detect algorithmic collusion.&lt;/p&gt;
&lt;p&gt;To make the test implementable, the last part of the paper studies whether one could get the same insights on collusive behavior using only observational data, from a single algorithm. The result is a unilateral empirical test for algorithmic collusion that does not require any assumptions neither on the algorithms themselves nor on the underlying environment. The key insight is that algorithms, during their learning phase, produce natural experiments that allow an observer to estimate their behavior in counterfactual scenarios. The simulations show that, at least in a controlled experimental setting, the test is extremely successful in detecting algorithmic collusion.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Coding Resources for Social Sciences</title>
      <link>https://matteocourthoud.github.io/post/coding/</link>
      <pubDate>Tue, 24 Aug 2021 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/post/coding/</guid>
      <description>&lt;p&gt;In this page I will collect useful resources for coding for researchers in social sciences. A mention goes to &lt;a href=&#34;https://maxkasy.github.io/home/computationlinks/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Maximilian Kasy&lt;/a&gt; that inspired me to build this page.&lt;/p&gt;
&lt;p&gt;A quick legend:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;📗 book&lt;/li&gt;
&lt;li&gt;🌐 webpage&lt;/li&gt;
&lt;li&gt;📈 charts&lt;/li&gt;
&lt;li&gt;🎥 videos&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;econometrics-and-statistics&#34;&gt;Econometrics and Statistics&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;📗&lt;a href=&#34;https://www.ssc.wisc.edu/~bhansen/econometrics/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Bruce Hansen&amp;rsquo;s Econometrics&lt;/strong&gt;&lt;/a&gt;:
By far the best freely available and regularly updated resource for Econometrics&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;machine-learning&#34;&gt;Machine Learning&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;📗&lt;a href=&#34;https://web.stanford.edu/~hastie/Papers/ESLII.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;The Elements of Statistical Learning&lt;/strong&gt;&lt;/a&gt;:
General introduction to machine learning&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;📗&lt;a href=&#34;http://www.gaussianprocess.org/gpml/chapters/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Gaussian Processes for Machine Learning&lt;/strong&gt;&lt;/a&gt;:
Extremely useful tools for nonparametric Bayesian modeling&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;📗&lt;a href=&#34;https://www.deeplearningbook.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Deep Learning&lt;/strong&gt;&lt;/a&gt;:
The theory and implementation of neural nets&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;📗&lt;a href=&#34;https://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning/understanding-machine-learning-theory-algorithms.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Understanding Machine Learning: From Theory to Algorithms&lt;/strong&gt;&lt;/a&gt;:
An introduction to statistical learning theory in the tradition of Vapnik&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;📗&lt;a href=&#34;http://www.incompleteideas.net/book/RLbook2018.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Reinforcement Learning - An Introduction&lt;/strong&gt;&lt;/a&gt;:
Adaptive learning for Markov decision problems&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;📗&lt;a href=&#34;http://jeffe.cs.illinois.edu/teaching/algorithms/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Algorithms&lt;/strong&gt;&lt;/a&gt;:
Introduction to the theory of algorithms&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;🌐&lt;a href=&#34;https://playground.tensorflow.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Tensorflow Playground&lt;/strong&gt;&lt;/a&gt;:
Visualisation tool for neural networks&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;🌐&lt;a href=&#34;https://www.fast.ai/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Artificial Intelligence&lt;/strong&gt;&lt;/a&gt;:
Online lectures on AI&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;🌐&lt;a href=&#34;https://global.oup.com/academic/product/the-ethical-algorithm-9780190948207&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;The Ethical Algorithm&lt;/strong&gt;&lt;/a&gt;:
How to impose normative constraints on ML and other algorithms&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;python&#34;&gt;Python&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;🌐&lt;a href=&#34;https://realpython.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;RealPython&lt;/strong&gt;&lt;/a&gt;:
Collection of Python tutorials, from introductory to advanced. Also contains &lt;a href=&#34;https://realpython.com/learning-paths/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;learning paths&lt;/a&gt; for specific topics&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;🌐&lt;a href=&#34;https://python.quantecon.org/intro.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;QuantEcon Python&lt;/strong&gt;&lt;/a&gt;
Tutorials and economic applications in Python, especially for macroeconomics&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;🌐&lt;a href=&#34;https://blog.finxter.com/python-cheat-sheets/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Cheat Sheets&lt;/strong&gt;&lt;/a&gt;:
Collection of cheat sheets for python&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;🌐&lt;a href=&#34;https://docs.python-guide.org/writing/structure/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Structuring a Python project&lt;/strong&gt;&lt;/a&gt;:
Advanced tutorial on how to structure a Python program&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;🌐&lt;a href=&#34;https://www.softwaretestinghelp.com/python-ide-code-editors/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;IDE Guide&lt;/strong&gt;&lt;/a&gt;:
Comparison of IDEs for Python. Suggested: &lt;a href=&#34;https://www.jetbrains.com/pycharm/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PyCharm&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;🌐&lt;a href=&#34;https://www.jetbrains.com/help/pycharm/configuring-remote-interpreters-via-ssh.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Configuring remote interpreters via SSH&lt;/strong&gt;&lt;/a&gt;:
How to use Python remotely via SSH via PyCharm&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;📈&lt;a href=&#34;https://www.kaggle.com/maheshdadhich/strength-of-visualization-python-visuals-tutorial/notebook&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Visualization in Python&lt;/strong&gt;&lt;/a&gt;:
How to make nice graphs in Python, with a dedicated jupyter notebook&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;📈&lt;a href=&#34;https://python-graph-gallery.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Python Graph Gallery&lt;/strong&gt;&lt;/a&gt;:
Graph examples in Python&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;matlab&#34;&gt;Matlab&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;🌐&lt;a href=&#34;https://mathworks.com/help/matlab/matlab_oop/user-defined-classes.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;User defined classes in Matlab&lt;/strong&gt;&lt;/a&gt;:
How to work with classes in Matlab&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;🌐&lt;a href=&#34;https://am111.readthedocs.io/en/latest/jmatlab_use.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Julyter Notebooks&lt;/strong&gt;&lt;/a&gt;:
How to run a jupyter notebook with Matlab kernel&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;📈&lt;a href=&#34;https://www.bradleymonk.com/wp/how-to-make-professional-looking-plots-for-journal-publication-using-matlab-r2014a-and-r2014b/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Graph Tips in Matlab&lt;/strong&gt;&lt;/a&gt; and &lt;a href=&#34;https://mathworks.com/matlabcentral/answers/133372-how-to-make-nice-plots&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;link2&lt;/a&gt;:
Suggestions on how to make pretty graphs in Matlab&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;julia&#34;&gt;Julia&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;🌐&lt;a href=&#34;https://docs.julialang.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Julia Manual&lt;/strong&gt;&lt;/a&gt;:
Julia unfortunately lacks a big community and tutorials, but it has a very good manual&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;🌐&lt;a href=&#34;https://julia.quantecon.org/index_toc.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;QuantEcon Julia&lt;/strong&gt;&lt;/a&gt;
Tutorials and economic applications in Julia, especially for macroeconomics&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;🌐&lt;a href=&#34;https://medium.com/dev-genius/what-is-the-best-ide-for-developing-in-the-programming-language-julia-484c913f07bc&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;IDE Guide&lt;/strong&gt;&lt;/a&gt;:
Guide for IDEs for Julia. Suggested: Juno for Atom.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;r&#34;&gt;R&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;📗&lt;a href=&#34;https://cran.r-project.org/doc/manuals/r-release/R-intro.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;An Introduction to R&lt;/strong&gt;&lt;/a&gt;:
Complete introduction to base R&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;📗&lt;a href=&#34;http://r4ds.had.co.nz/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;R for Data Science&lt;/strong&gt;&lt;/a&gt;
Introduction to data analysis using R, focused on the tidyverse packages&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;📗&lt;a href=&#34;https://adv-r.hadley.nz/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Advanced R&lt;/strong&gt;&lt;/a&gt;:
In depth discussion of programming in R&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;📗&lt;a href=&#34;https://bradleyboehmke.github.io/HOML/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Hands-On Machine Learning with R&lt;/strong&gt;&lt;/a&gt;:
Fitting ML models in R&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;🌐&lt;a href=&#34;https://github.com/stan-dev/rstan/wiki/RStan-Getting-Started&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Bayesian statistics using Stan&lt;/strong&gt;&lt;/a&gt; and &lt;a href=&#34;https://mc-stan.org/docs/2_20/stan-users-guide/index.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;link&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;🌐&lt;a href=&#34;https://www.rstudio.com/resources/cheatsheets/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;RStudio Cheat Sheets&lt;/strong&gt;&lt;/a&gt;
for various extensions, including data processing, visualization, writing web apps, …&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;📈&lt;a href=&#34;https://www.r-graph-gallery.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;R Graph Gallery&lt;/strong&gt;&lt;/a&gt;:
Graph examples in R&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;📈&lt;a href=&#34;https://www.christophenicault.com/pages/visualizations/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Nice Graphs with code&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;A collection of elaborate graphs with code in R&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;others&#34;&gt;Others&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;📗&lt;a href=&#34;https://git-scm.com/book/en/v2&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Github Advanced&lt;/strong&gt;&lt;/a&gt;:
Advanced guide for version control with Github&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;🎥&lt;a href=&#34;https://missing.csail.mit.edu/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;The Missing Semester of Your CS Education&lt;/strong&gt;&lt;/a&gt;
Video lectures and notes on tools for computer scientists (version control, debugging, &amp;hellip;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;📈&lt;a href=&#34;http://pgfplots.sourceforge.net/gallery.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;PGF plots in Latex&lt;/strong&gt;&lt;/a&gt;:
Gallery and examples to make plots directly in Latex&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;🌐&lt;a href=&#34;https://medium.com/@SergioPietri/how-to-setup-and-use-ssh-for-remote-connections-e86556d804dd&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Work remotely from server&lt;/strong&gt;&lt;/a&gt;:
How to setup SSH for remote computing&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>How to access WRDS in Python</title>
      <link>https://matteocourthoud.github.io/post/wrds/</link>
      <pubDate>Tue, 09 Mar 2021 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/post/wrds/</guid>
      <description>&lt;p&gt;In this page, I explain how to work with the WRDS database using Python.&lt;/p&gt;
&lt;h2 id=&#34;setup&#34;&gt;Setup&lt;/h2&gt;
&lt;p&gt;The first thing we need to do, is to set up a connection to the &lt;a href=&#34;https://wrds-www.wharton.upenn.edu/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;WRDS database&lt;/a&gt;. I am assuming you have credentials to log in. Check the &lt;a href=&#34;https://wrds-www.wharton.upenn.edu/login/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;log in page&lt;/a&gt; to make sure.&lt;/p&gt;
&lt;p&gt;The second requirement is the &lt;a href=&#34;https://pypi.org/project/wrds/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;wrds&lt;/a&gt; Python package.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pip3 install wrds
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, in order to connect to the WRDS database, you just need to run the following commang in Python.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import wrds
db = wrds.Connection() 
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then, you will be propted to input your WRDS username and password.&lt;/p&gt;
&lt;p&gt;However, if you are using a Python IDE such as &lt;a href=&#34;https://www.jetbrains.com/pycharm/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PyCharm&lt;/a&gt;, you cannot run the command from the Python Console. Moreover, you might want to save your credentials once and for all, so that you don&amp;rsquo;t have to log in every time.&lt;/p&gt;
&lt;p&gt;First, walk to your home directory from the Terminal (&lt;code&gt;/Users/username&lt;/code&gt;).&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;cd
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now create an empty &lt;code&gt;.pgpass&lt;/code&gt; file.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;touch .pgpass
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now you write &lt;code&gt;your_username&lt;/code&gt; and &lt;code&gt;your_password&lt;/code&gt; into the &lt;code&gt;.pgpass&lt;/code&gt; file.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;echo &amp;quot;wrds-pgdata.wharton.upenn.edu:9737:wrds:your_username:your_password&amp;quot; &amp;gt;&amp;gt; .pgpass
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You also need to restrict permissions to the file.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;chmod 600 ~/.pgpass
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now you can go back to your Python IDE and access the database by just inputing your username.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import wrds
db = wrds.Connection(wrds_username=&#39;your_username&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If everything works, you should see the following output.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Loading library list...
Done
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;query&#34;&gt;Query&lt;/h2&gt;
&lt;p&gt;The available functions are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;db.connection() &lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;db.list_libraries()&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;db.list_tables() &lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;db.get_table() &lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;db.describe_table() &lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;db.raw_sql() &lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;db.close()&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I make a simple example of how they work. Suppose first you want to list all the libraries in the WRDS database.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;db.list_libraries()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then you can list all the datasets within a given library.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;db.list_tables(library=&#39;comp&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Before downloading a table, you can describe it.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;df = db.describe_table(library=&#39;comp&#39;, table=&#39;funda&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To download the dataset you can use the &lt;code&gt;get_table()&lt;/code&gt; function.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;df = db.get_table(library=&#39;comp&#39;, table=&#39;funda&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can restrict both the rows and the columns you want to query.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;df_short = db.get_table(library=&#39;comp&#39;, table=&#39;funda&#39;, columns = [&#39;conm&#39;, &#39;gvkey&#39;, &#39;cik&#39;], obs=5)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can also query the database directly using SQL.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;df_sql = db.raw_sql(&#39;&#39;&#39;select conm, gvkey, cik FROM comp.funda WHERE fyear&amp;gt;2010 AND (indfmt=&#39;INDL&#39;)&#39;&#39;&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;sources&#34;&gt;Sources&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://wrds-www.wharton.upenn.edu/pages/support/programming-wrds/programming-python/querying-wrds-data-python/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Querying WRDS Data using Python&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://wrds-www.wharton.upenn.edu/documents/1443/wrds_connection.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Using Python on WRDS Platform&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://sites.duke.edu/kevinstandridge/2020/03/07/introduction-to-the-wrds-python-package/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Introduction to the WRDS Python Package&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://wizardkingz.github.io/wrdsdataaccesspython-tutorial/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;WRDS Data Access Via Python API&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Summer Schools in Economics and Finance</title>
      <link>https://matteocourthoud.github.io/post/summerschools/</link>
      <pubDate>Mon, 15 Feb 2021 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/post/summerschools/</guid>
      <description>&lt;p&gt;I will use this page to collect information about summer schools in Economics.&lt;/p&gt;
&lt;p&gt;If you know about summer schools that are missing from this list, please either &lt;a href=&#34;mailto:matteo.courthoud@econ.uzh.ch&#34;&gt;contact me&lt;/a&gt; or &lt;a href=&#34;https://github.com/matteocourthoud/website/blob/master/content/post/summerschools/index.md&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;edit the table on Github&lt;/a&gt;!&lt;/p&gt;
&lt;h2 id=&#34;2020&#34;&gt;2020&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Name and Link&lt;/th&gt;
&lt;th&gt;Field&lt;/th&gt;
&lt;th&gt;Organizer&lt;/th&gt;
&lt;th&gt;Location&lt;/th&gt;
&lt;th&gt;Instructor(s)&lt;/th&gt;
&lt;th&gt;Dates&lt;/th&gt;
&lt;th&gt;Deadline&lt;/th&gt;
&lt;th&gt;Fee&lt;/th&gt;
&lt;th&gt;Aid&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://dseconf.org/dse2020&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Dynamic Structural Econometrics&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Econometrics, IO&lt;/td&gt;
&lt;td&gt;Econometrics Society&lt;/td&gt;
&lt;td&gt;Zurich&lt;/td&gt;
&lt;td&gt;John Rust et al.&lt;/td&gt;
&lt;td&gt;June 15-21&lt;/td&gt;
&lt;td&gt;March 15&lt;/td&gt;
&lt;td&gt;500$&lt;/td&gt;
&lt;td&gt;no&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://www.cresse.info/default.aspx?articleID=3398&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CRESSE&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Comp. Policy, IO&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;Crete&lt;/td&gt;
&lt;td&gt;various&lt;/td&gt;
&lt;td&gt;June 20 - July 02&lt;/td&gt;
&lt;td&gt;FCFS&lt;/td&gt;
&lt;td&gt;3200€&lt;/td&gt;
&lt;td&gt;-30%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://www.barcelonagse.eu/study/summer-school/digital-economy&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Digital Economy&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Comp. Policy, IO&lt;/td&gt;
&lt;td&gt;Barcelona GSE&lt;/td&gt;
&lt;td&gt;Barcelona&lt;/td&gt;
&lt;td&gt;Martin Peitz&lt;/td&gt;
&lt;td&gt;July 13-17&lt;/td&gt;
&lt;td&gt;March 10&lt;/td&gt;
&lt;td&gt;550€&lt;/td&gt;
&lt;td&gt;maybe&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://www.parisschoolofeconomics.eu/en/teaching/pse-summer-school/social-networks-platforms/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Social Networks, Platforms&amp;hellip;&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Comp. Policy, IO&lt;/td&gt;
&lt;td&gt;PSE&lt;/td&gt;
&lt;td&gt;Paris&lt;/td&gt;
&lt;td&gt;from Paris&lt;/td&gt;
&lt;td&gt;June 15-19&lt;/td&gt;
&lt;td&gt;March 31&lt;/td&gt;
&lt;td&gt;1200€&lt;/td&gt;
&lt;td&gt;no&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;2019&#34;&gt;2019&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Name and Link&lt;/th&gt;
&lt;th&gt;Field&lt;/th&gt;
&lt;th&gt;Organizer&lt;/th&gt;
&lt;th&gt;Location&lt;/th&gt;
&lt;th&gt;Instructor(s)&lt;/th&gt;
&lt;th&gt;Dates&lt;/th&gt;
&lt;th&gt;Deadline&lt;/th&gt;
&lt;th&gt;Fee&lt;/th&gt;
&lt;th&gt;Aid&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://dseconf.org/dse2019&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Dynamic Structural Econometrics&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Econometrics, IO&lt;/td&gt;
&lt;td&gt;Econometrics Society&lt;/td&gt;
&lt;td&gt;Chicago&lt;/td&gt;
&lt;td&gt;John Rust et al.&lt;/td&gt;
&lt;td&gt;July 08-14&lt;/td&gt;
&lt;td&gt;March 15&lt;/td&gt;
&lt;td&gt;500$&lt;/td&gt;
&lt;td&gt;no&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;CRESSE&lt;/td&gt;
&lt;td&gt;Competition Policy, IO&lt;/td&gt;
&lt;td&gt;various&lt;/td&gt;
&lt;td&gt;Crete&lt;/td&gt;
&lt;td&gt;various&lt;/td&gt;
&lt;td&gt;June 20 - July 02&lt;/td&gt;
&lt;td&gt;FCFS&lt;/td&gt;
&lt;td&gt;3200€&lt;/td&gt;
&lt;td&gt;-30%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://www.cemfi.es/studies/css/course.asp?cu=10&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Empirical Analysis of Firm Performance&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;IO, Trade&lt;/td&gt;
&lt;td&gt;CEMFI&lt;/td&gt;
&lt;td&gt;Madrid&lt;/td&gt;
&lt;td&gt;Jan de Loecker&lt;/td&gt;
&lt;td&gt;August 19-23&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://www.cemfi.es/studies/css/course.asp?cu=16&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Panel Data Econometrics&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Econometrics&lt;/td&gt;
&lt;td&gt;CEMFI&lt;/td&gt;
&lt;td&gt;Madrid&lt;/td&gt;
&lt;td&gt;Steve Bond&lt;/td&gt;
&lt;td&gt;September 02-06&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;2018&#34;&gt;2018&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Name and Link&lt;/th&gt;
&lt;th&gt;Field&lt;/th&gt;
&lt;th&gt;Organizer&lt;/th&gt;
&lt;th&gt;Location&lt;/th&gt;
&lt;th&gt;Instructor(s)&lt;/th&gt;
&lt;th&gt;Dates&lt;/th&gt;
&lt;th&gt;Deadline&lt;/th&gt;
&lt;th&gt;Fee&lt;/th&gt;
&lt;th&gt;Aid&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://www.econ.ku.dk/cce/events/summerschool/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Dynamic Structural Models&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Econometrics, IO&lt;/td&gt;
&lt;td&gt;University of Copenhagen&lt;/td&gt;
&lt;td&gt;Copenhagen&lt;/td&gt;
&lt;td&gt;John Rust et al.&lt;/td&gt;
&lt;td&gt;May 28 - Jun 03&lt;/td&gt;
&lt;td&gt;March 15&lt;/td&gt;
&lt;td&gt;600€&lt;/td&gt;
&lt;td&gt;no&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://www.cemfi.es/studies/css/course_previous_years.asp?c=12&amp;amp;y=2018&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Empirical Analysis of Innovation in Oligopoly Industries&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Econometrics, IO&lt;/td&gt;
&lt;td&gt;CEMFI&lt;/td&gt;
&lt;td&gt;Madrid&lt;/td&gt;
&lt;td&gt;Victor Aguirregabiria&lt;/td&gt;
&lt;td&gt;September 03-07&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
</description>
    </item>
    
    <item>
      <title>Approximation Methods for Large Dynamic Stochastic Games</title>
      <link>https://matteocourthoud.github.io/project/approximations/</link>
      <pubDate>Fri, 01 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/project/approximations/</guid>
      <description>&lt;p&gt;Dynamic stochastic games notoriously suffer from a curse of dimensionality that makes computing the Markov Perfect Equilibrium of large games infeasible. This article compares the existing approximation methods and alternative equilibrium concepts that have been proposed in the literature to overcome this problem. No method clearly dominates the others but some are dominated in all dimensions. In general, alternative equilibrium concepts outperform sampling-based approximation methods. I propose a new game structure, games with random order, in which players move sequentially and the order of play is unknown. The Markov Perfect equilibrium of this game consistently outperforms all existing approximation methods in terms of approximation accuracy while still being extremely efficient in terms of computational time.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Ownership and Product Similarity</title>
      <link>https://matteocourthoud.github.io/project/ownership/</link>
      <pubDate>Sat, 01 Feb 2020 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/project/ownership/</guid>
      <description>&lt;p&gt;I generate a time-varying measure of S&amp;amp;P500 firm similarity using a zero-shot clustering model. The model takes as input BERT embeddings of product descriptions and is trained on market definitions from the EU commission. The objective is to estimate the causal effect of common ownership on product similarity.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Slides</title>
      <link>https://matteocourthoud.github.io/slides/example/</link>
      <pubDate>Tue, 05 Feb 2019 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/slides/example/</guid>
      <description>&lt;h1 id=&#34;create-slides-in-markdown-with-wowchemy&#34;&gt;Create slides in Markdown with Wowchemy&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;https://wowchemy.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Wowchemy&lt;/a&gt; | &lt;a href=&#34;https://owchemy.com/docs/managing-content/#create-slides&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Documentation&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;features&#34;&gt;Features&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Efficiently write slides in Markdown&lt;/li&gt;
&lt;li&gt;3-in-1: Create, Present, and Publish your slides&lt;/li&gt;
&lt;li&gt;Supports speaker notes&lt;/li&gt;
&lt;li&gt;Mobile friendly slides&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;controls&#34;&gt;Controls&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Next: &lt;code&gt;Right Arrow&lt;/code&gt; or &lt;code&gt;Space&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Previous: &lt;code&gt;Left Arrow&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Start: &lt;code&gt;Home&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Finish: &lt;code&gt;End&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Overview: &lt;code&gt;Esc&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Speaker notes: &lt;code&gt;S&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Fullscreen: &lt;code&gt;F&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Zoom: &lt;code&gt;Alt + Click&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/hakimel/reveal.js#pdf-export&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PDF Export&lt;/a&gt;: &lt;code&gt;E&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;code-highlighting&#34;&gt;Code Highlighting&lt;/h2&gt;
&lt;p&gt;Inline code: &lt;code&gt;variable&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Code block:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;porridge = &amp;quot;blueberry&amp;quot;
if porridge == &amp;quot;blueberry&amp;quot;:
    print(&amp;quot;Eating...&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h2 id=&#34;math&#34;&gt;Math&lt;/h2&gt;
&lt;p&gt;In-line math: $x + y = z$&lt;/p&gt;
&lt;p&gt;Block math:&lt;/p&gt;
&lt;p&gt;$$
f\left( x \right) = ;\frac{{2\left( {x + 4} \right)\left( {x - 4} \right)}}{{\left( {x + 4} \right)\left( {x + 1} \right)}}
$$&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;fragments&#34;&gt;Fragments&lt;/h2&gt;
&lt;p&gt;Make content appear incrementally&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;{{% fragment %}} One {{% /fragment %}}
{{% fragment %}} **Two** {{% /fragment %}}
{{% fragment %}} Three {{% /fragment %}}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Press &lt;code&gt;Space&lt;/code&gt; to play!&lt;/p&gt;
&lt;span class=&#34;fragment &#34; &gt;
   One 
&lt;/span&gt;
&lt;span class=&#34;fragment &#34; &gt;
   **Two** 
&lt;/span&gt;
&lt;span class=&#34;fragment &#34; &gt;
   Three 
&lt;/span&gt;
&lt;hr&gt;
&lt;p&gt;A fragment can accept two optional parameters:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;class&lt;/code&gt;: use a custom style (requires definition in custom CSS)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;weight&lt;/code&gt;: sets the order in which a fragment appears&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;speaker-notes&#34;&gt;Speaker Notes&lt;/h2&gt;
&lt;p&gt;Add speaker notes to your presentation&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-markdown&#34;&gt;{{% speaker_note %}}
- Only the speaker can read these notes
- Press `S` key to view
{{% /speaker_note %}}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Press the &lt;code&gt;S&lt;/code&gt; key to view the speaker notes!&lt;/p&gt;
&lt;aside class=&#34;notes&#34;&gt;
  &lt;ul&gt;
&lt;li&gt;Only the speaker can read these notes&lt;/li&gt;
&lt;li&gt;Press &lt;code&gt;S&lt;/code&gt; key to view&lt;/li&gt;
&lt;/ul&gt;

&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;themes&#34;&gt;Themes&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;black: Black background, white text, blue links (default)&lt;/li&gt;
&lt;li&gt;white: White background, black text, blue links&lt;/li&gt;
&lt;li&gt;league: Gray background, white text, blue links&lt;/li&gt;
&lt;li&gt;beige: Beige background, dark text, brown links&lt;/li&gt;
&lt;li&gt;sky: Blue background, thin dark text, blue links&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;ul&gt;
&lt;li&gt;night: Black background, thick white text, orange links&lt;/li&gt;
&lt;li&gt;serif: Cappuccino background, gray text, brown links&lt;/li&gt;
&lt;li&gt;simple: White background, black text, blue links&lt;/li&gt;
&lt;li&gt;solarized: Cream-colored background, dark green text, blue links&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;/media/boards.jpg&#34;
  &gt;

&lt;h2 id=&#34;custom-slide&#34;&gt;Custom Slide&lt;/h2&gt;
&lt;p&gt;Customize the slide style and background&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-markdown&#34;&gt;{{&amp;lt; slide background-image=&amp;quot;/media/boards.jpg&amp;quot; &amp;gt;}}
{{&amp;lt; slide background-color=&amp;quot;#0000FF&amp;quot; &amp;gt;}}
{{&amp;lt; slide class=&amp;quot;my-style&amp;quot; &amp;gt;}}
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h2 id=&#34;custom-css-example&#34;&gt;Custom CSS Example&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s make headers navy colored.&lt;/p&gt;
&lt;p&gt;Create &lt;code&gt;assets/css/reveal_custom.css&lt;/code&gt; with:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-css&#34;&gt;.reveal section h1,
.reveal section h2,
.reveal section h3 {
  color: navy;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h1 id=&#34;questions&#34;&gt;Questions?&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/wowchemy/wowchemy-hugo-modules/discussions&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Ask&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://wowchemy.com/docs/managing-content/#create-slides&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Documentation&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Privacy Policy</title>
      <link>https://matteocourthoud.github.io/privacy/</link>
      <pubDate>Thu, 28 Jun 2018 00:00:00 +0100</pubDate>
      <guid>https://matteocourthoud.github.io/privacy/</guid>
      <description>&lt;p&gt;&lt;strong&gt;My website does not host third-party cookies&lt;/strong&gt; and hosts three first-party cookies just to generally understand the audience of the website. The cookies are from Google Analytics.&lt;/p&gt;
&lt;p&gt;You do not know what cookies are? You would like to learn more details? More information below.&lt;/p&gt;
&lt;h2 id=&#34;cookies-intro&#34;&gt;Cookies Intro&lt;/h2&gt;
&lt;h3 id=&#34;what-is-a-cookie&#34;&gt;What is a cookie?&lt;/h3&gt;
&lt;p&gt;A cookie is a small text file that a server (website) passes to a client (your computer) through your web browser (most likely Chrome). This small text file is then stored on your computer. The browser may &lt;strong&gt;store&lt;/strong&gt; the cookie and &lt;strong&gt;send it&lt;/strong&gt; with later requests.&lt;/p&gt;
&lt;p&gt;A cookie &lt;strong&gt;is not a program&lt;/strong&gt;. It doesn’t perform a function. It’s just text. You can open and read cookies with a basic word processor. The two most common types of cookies are &lt;strong&gt;first-party cookies&lt;/strong&gt; and &lt;strong&gt;third-party&lt;/strong&gt; &lt;strong&gt;cookies&lt;/strong&gt;. Before arriving to that, we first need to understand how are cookies generated, and how they are shared and with whom.&lt;/p&gt;
&lt;h3 id=&#34;how-are-cookies-generated&#34;&gt;How are cookies generated?&lt;/h3&gt;
&lt;p&gt;When you open a website, you send an HTTP request to a server, which replies with a response. After receiving an HTTP request, a server can send one or more &lt;code&gt;Set-Cookie&lt;/code&gt; headers with the response. The browser usually stores the cookie and sends it with requests made to the same server inside a &lt;code&gt;Cookie&lt;/code&gt; HTTP header.&lt;/p&gt;
&lt;p&gt;For example, this instructs the server sending headers to tell the client to store a pair of cookies&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;HTTP/2.0 200 OK
Content-Type: text/html
Set-Cookie: yummy_cookie=choco
Set-Cookie: tasty_cookie=strawberry

[page content]
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;what-do-cookies-do&#34;&gt;What do cookies do?&lt;/h3&gt;
&lt;p&gt;Once a cookies is stored in your browser, with every subsequent request to the server, the browser sends all previously stored cookies back to the server using the &lt;code&gt;Cookie&lt;/code&gt; header.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;GET /sample_page.html HTTP/2.0
Host: www.example.org
Cookie: yummy_cookie=choco; tasty_cookie=strawberry
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There are restrictions to which cookies get sent to which servers. More details below.&lt;/p&gt;
&lt;h2 id=&#34;cookies-content&#34;&gt;Cookies Content&lt;/h2&gt;
&lt;p&gt;A cookie has the following attributes:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;Expires&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Max-Age&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;SameSite&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Domain&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Path&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;HTTPOnly&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Secure&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;how-long-are-cookies-stored&#34;&gt;How long are cookies stored?&lt;/h3&gt;
&lt;p&gt;The &lt;code&gt;Expires&lt;/code&gt; and &lt;code&gt;Max-Age&lt;/code&gt; attributes define the lifetime of a cookie: how long a cookie is stored in your browser.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;Expires&lt;/code&gt; indicates the maximum lifetime of the cookie. If unspecified, the cookie becomes a session cookie. A session finishes when the client shuts down, after which the session cookie is removed.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Max-Age&lt;/code&gt; indicates the number of seconds until the cookie expires. A zero or negative number will expire the cookie immediately. If both &lt;code&gt;Expires&lt;/code&gt; and &lt;code&gt;Max-Age&lt;/code&gt; are set, &lt;code&gt;Max-Age&lt;/code&gt; has precedence.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;who-are-my-cookes-sent-to&#34;&gt;Who are my cookes sent to?&lt;/h3&gt;
&lt;p&gt;The &lt;code&gt;SameSite&lt;/code&gt;, &lt;code&gt;Domain&lt;/code&gt; and &lt;code&gt;Path&lt;/code&gt; attributes define the &lt;em&gt;scope&lt;/em&gt; of a cookie: what URLs the cookies should be sent to.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;The &lt;code&gt;SameSite&lt;/code&gt; attribute controls whether or not a cookie is sent with cross-origin requests, providing some protection against cross-site request forgery attacks.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The &lt;code&gt;Domain&lt;/code&gt; attribute specifies which hosts can receive a cookie. If unspecified, the attribute defaults to the same &lt;a href=&#34;https://developer.mozilla.org/en-US/docs/Glossary/Host&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;host&lt;/a&gt; that set the cookie, &lt;em&gt;excluding subdomains&lt;/em&gt;. If &lt;code&gt;Domain&lt;/code&gt; &lt;em&gt;is&lt;/em&gt; specified, then subdomains are always included. Therefore, specifying &lt;code&gt;Domain&lt;/code&gt; is less restrictive than omitting it. However, it can be helpful when subdomains need to share information about a user. For example, if you set &lt;code&gt;Domain=mozilla.org&lt;/code&gt;, cookies are available on subdomains like &lt;code&gt;developer.mozilla.org&lt;/code&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The &lt;code&gt;Path&lt;/code&gt; attribute indicates a URL path that must exist in the requested URL in order to send the &lt;code&gt;Cookie&lt;/code&gt; header.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;are-my-cookies-safe&#34;&gt;Are my cookies safe?&lt;/h3&gt;
&lt;p&gt;The &lt;code&gt;HTTPOnly&lt;/code&gt; and &lt;code&gt;Secure&lt;/code&gt; attributes help protecting your cookies from malicious server and client attacks, respectively.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Setting the &lt;code&gt;HTTPOnly&lt;/code&gt; attribute makes sure that no client-side scripts are not allowed to access the cookie, i.e. no app or program on your machine can read your cookies. This protects cookies from Cross Site Scripting attacks that can be used to steal cookies with the help of client-side scripts.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The &lt;code&gt;Secure&lt;/code&gt; attribute makes sure that the cookie will only be sent with requests made over an encrypted connection and an attacker won&amp;rsquo;t be able to steal cookies by &lt;em&gt;sniffing&lt;/em&gt;. Sniffing can be defined as passively reading data that is being transmitted. In order to overcome this problem, we encrypt data before transmission. Encryption of data ensures that any potential attacker who sniffs traffic will not be able to steal clear text data, thus ensuring their safety.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;cookies-purpose&#34;&gt;Cookies Purpose&lt;/h2&gt;
&lt;p&gt;Cookies are mainly used for three purposes:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Session management&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;Logins, shopping carts, game scores, or anything else the server should remember&lt;/li&gt;
&lt;li&gt;E.g. you add something to che cart (without logging in!) and when you come back the cart is not empty? That&amp;rsquo;s a session management cookie!&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Personalization&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;User preferences, themes, and other settings&lt;/li&gt;
&lt;li&gt;E.g. you put a website in dark mode and when you come back is still dark? That&amp;rsquo;s a personalization cookie!&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Tracking&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;Recording and analyzing user behavior&lt;/li&gt;
&lt;li&gt;You search for a product and on a different website you see an ad for it? That&amp;rsquo;s a tracking cookie!&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;first-party-cookies&#34;&gt;First-Party Cookies&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Session management and personalization cookies&lt;/strong&gt; cookies are always first-party cookies. Few people have problems with first-party cookies because they’re typically used to store information that could be useful to &lt;strong&gt;enhance the user experience&lt;/strong&gt; later on. Some example of information and its purpose are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Logins&lt;/strong&gt;: so that you don&amp;rsquo;t have to log in every website every time you visit them&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Searches&lt;/strong&gt;: so that you get suggesions for past searches&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Settings&lt;/strong&gt;: so that you don&amp;rsquo;t have to change them every time&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Game scores&lt;/strong&gt;: so that you can compare your new scores to the old ones&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;third-party-cookies&#34;&gt;Third Party Cookies&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Tracking cookies&lt;/strong&gt; are almost always third-party cookies. They collect and share user data through networks of websites, often without the user’s consent. These networks aggregate and sync countless data points. In the end, they know more about you than you expect.&lt;/p&gt;
&lt;p&gt;Look at it like this: You visit three websites - A, B, and C. On website A, you take some action that signals you want to buy running shoes. On website B, you do something that indicates you are a man (maybe you browse the men’s section). On website C, you see an ad for men’s running shoes, even though you haven’t given that site any information yet. You wouldn’t expect Website C to know anything about you, but the cookies saved on your computer from other websites provide it with plenty of information.&lt;/p&gt;
&lt;p&gt;As cookie use became sophisticated, users became less comfortable. The first time you browse for a product on Amazon and then see an ad for it on Facebook is unsettling. It’s a clear sign that your Internet habits aren’t as anonymous as you thought.&lt;/p&gt;
&lt;h2 id=&#34;regulation&#34;&gt;Regulation&lt;/h2&gt;
&lt;p&gt;Legislation or regulations that cover the use of cookies include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The General Data Privacy Regulation (GDPR) in the European Union&lt;/li&gt;
&lt;li&gt;The ePrivacy Directive in the EU&lt;/li&gt;
&lt;li&gt;The California Consumer Privacy Act&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;These regulations have global reach. They apply to any site on the &lt;em&gt;World Wide&lt;/em&gt; Web that users from these jurisdictions access (the EU and California, with the caveat that California&amp;rsquo;s law applies only to entities with gross revenue over 25 million USD, among things).&lt;/p&gt;
&lt;h3 id=&#34;gdpr&#34;&gt;GDPR&lt;/h3&gt;
&lt;p&gt;The &lt;a href=&#34;https://www.osano.com/articles/gdpr-guide&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;General Data Protection Regulation (GDPR)&lt;/a&gt; and &lt;a href=&#34;https://www.osano.com/articles/eprivacy-guide&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ePrivacy &lt;/a&gt;Directive are the strongest examples of this. These EU laws treat cookies as “personal data,” which makes them subject to regulation. Any website that serves EU residents must collect consent from users before serving any non-essential cookies to the user’s device.&lt;/p&gt;
&lt;h3 id=&#34;others&#34;&gt;Others&lt;/h3&gt;
&lt;p&gt;The EU’s efforts to protect personal data ignited a global trend, and that’s changing the data privacy landscape. Other jurisdictions have passed or are working on passing their own data privacy initiatives.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The &lt;a href=&#34;https://www.osano.com/articles/ccpa-guide&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;California Consumer Privacy Act (CCPA)&lt;/a&gt; gives California residents the right to know the types of personal information organizations collect about them and the right to prohibit the sale of their personal information to other parties. (It’s a big law with other data security measures as well.)&lt;/li&gt;
&lt;li&gt;The &lt;a href=&#34;https://www.osano.com/articles/lgpd-enforcement-begins&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Brazilian General Data Protection Law (LGPD)&lt;/a&gt; is an entirely new legal framework in Brazil to protect personal information. Users must consent to the use of third-party cookies when data is transferred.&lt;/li&gt;
&lt;li&gt;The Vermont Act 171 of 2018 Data Broker Regulation requires data brokers (businesses that collect and sell data on individuals they don’t have a relationship with) to register with the state, provide users with an opt-out mechanism, and comply with a list of security requirements.&lt;/li&gt;
&lt;li&gt;The &lt;a href=&#34;https://www.osano.com/articles/new-york-shield-law&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Stop Hacks and Improve Electronic Data Security (SHIELD) Act&lt;/a&gt; creates a definition for privacy information, encompassing many of the data points typically stored within cookies.&lt;/li&gt;
&lt;li&gt;India, Chile, and New Zealand are working on similar data privacy laws.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;sources&#34;&gt;Sources&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://developer.mozilla.org/en-US/docs/Web/HTTP/Cookies&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Using HTTP cookies&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.osano.com/articles/how-cookies-work&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;How cookies work, and how to conduct a cookie audit&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Terms</title>
      <link>https://matteocourthoud.github.io/terms/</link>
      <pubDate>Thu, 28 Jun 2018 00:00:00 +0100</pubDate>
      <guid>https://matteocourthoud.github.io/terms/</guid>
      <description>&lt;p&gt;&amp;hellip;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title></title>
      <link>https://matteocourthoud.github.io/admin/config.yml</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/admin/config.yml</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
