<!DOCTYPE html><html lang="en-us" >

<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  
  
  
    <meta name="generator" content="Wowchemy 5.1.0 for Hugo" />
  

  

  
  
  
  
  
    
    
    
  
  

  

  
  
  
    
  
  <meta name="description" content="Introduction Intro Setting: agents making strategic decisions (new) in dynamic environments.
Entry and exit: Collard-Wexler (2013) Sunk costs: Ryan (2012) Innovation: Goettler and Gordon (2011) (or whatever changes in response to investment) Exploitation of natural resources: Huang and Smith (2014) Durable goods: Esteban and Shum (2007) Lit review: forthcoming IO Handbook chapter Aguirregabiria, Collard-Wexler, and Ryan (2021)" />

  
  <link rel="alternate" hreflang="en-us" href="https://matteocourthoud.github.io/course/empirical-io/08_dynamics_games/" />

  







  




  
  
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
  

  
  
  
    <meta name="theme-color" content="#003f5c" />
  

  
  
    
    <script src="/js/mathjax-config.js"></script>
  

  
  
  
  
    
    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha256-FMvZuGapsJLjouA6k7Eo2lusoAX9i0ShlWFG6qt7SLc=" crossorigin="anonymous">

    
    

    
    
    
      
    
    
      
      
        
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/styles/github.min.css" crossorigin="anonymous" title="hl-light" media="print" onload="this.media='all'">
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/styles/dracula.min.css" crossorigin="anonymous" title="hl-dark" media="print" onload="this.media='all'" disabled>
        
      
    

    
    
    
      <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.7.1/leaflet.min.css" integrity="sha512-1xoFisiGdy9nvho8EgXuXvnpR5GAMSjFwp40gSRE3NwdUdIMIKuPa7bqoUhLD0O/5tPNhteAsE5XyyMi5reQVA==" crossorigin="anonymous" media="print" onload="this.media='all'">
    

    

    
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js" integrity="" crossorigin="anonymous" async></script>
      
    
      
      

      
      

      
    
      
      

      
      

      
    
  

  
  
  
    
      
      <link rel="preload" as="style" href="https://fonts.googleapis.com/css2?family=Roboto+Mono&family=Source+Sans+Pro:wght@200;300;400;700&display=swap">
      <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Roboto+Mono&family=Source+Sans+Pro:wght@200;300;400;700&display=swap" media="print" onload="this.media='all'">
    
  

  
  
  
  
  
  <link rel="stylesheet" href="/css/wowchemy.4f7182ca394d705ee32d9d7750e9aa1d.css" />

  




<script async src="https://www.googletagmanager.com/gtag/js?id=UA-144780600-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];

  function gtag() {
      dataLayer.push(arguments);
  }

  function trackOutboundLink(url, target) {
    gtag('event', 'click', {
         'event_category': 'outbound',
         'event_label': url,
         'transport_type': 'beacon',
         'event_callback': function () {
           if (target !== '_blank') {
             document.location = url;
           }
         }
    });
    console.debug("Outbound link clicked: " + url);
  }

  function onClickCallback(event) {
    if ((event.target.tagName !== 'A') || (event.target.host === window.location.host)) {
      return;
    }
    trackOutboundLink(event.target, event.target.getAttribute('target'));  
  }

  gtag('js', new Date());
  gtag('config', 'UA-144780600-1', { 'anonymize_ip': true });
  gtag('set', {'cookie_flags': 'SameSite=None;Secure'});

  
  document.addEventListener('click', onClickCallback, false);
</script>


  

  

  




  
  
  

  

  
    <link rel="manifest" href="/index.webmanifest" />
  

  <link rel="icon" type="image/png" href="/media/icon_hu03e9b3967b83fd39296ec9da5ff1ea05_201175_32x32_fill_lanczos_center_3.png" />
  <link rel="apple-touch-icon" type="image/png" href="/media/icon_hu03e9b3967b83fd39296ec9da5ff1ea05_201175_180x180_fill_lanczos_center_3.png" />

  <link rel="canonical" href="https://matteocourthoud.github.io/course/empirical-io/08_dynamics_games/" />

  
  
  
  
  
  
  
  
    
    
  
  
  <meta property="twitter:card" content="summary" />
  
  <meta property="og:site_name" content="Matteo Courthoud" />
  <meta property="og:url" content="https://matteocourthoud.github.io/course/empirical-io/08_dynamics_games/" />
  <meta property="og:title" content="Dynamic Games | Matteo Courthoud" />
  <meta property="og:description" content="Introduction Intro Setting: agents making strategic decisions (new) in dynamic environments.
Entry and exit: Collard-Wexler (2013) Sunk costs: Ryan (2012) Innovation: Goettler and Gordon (2011) (or whatever changes in response to investment) Exploitation of natural resources: Huang and Smith (2014) Durable goods: Esteban and Shum (2007) Lit review: forthcoming IO Handbook chapter Aguirregabiria, Collard-Wexler, and Ryan (2021)" /><meta property="og:image" content="https://matteocourthoud.github.io/media/icon_hu03e9b3967b83fd39296ec9da5ff1ea05_201175_512x512_fill_lanczos_center_3.png" />
    <meta property="twitter:image" content="https://matteocourthoud.github.io/media/icon_hu03e9b3967b83fd39296ec9da5ff1ea05_201175_512x512_fill_lanczos_center_3.png" /><meta property="og:locale" content="en-us" />
  
    
      <meta
        property="article:published_time"
        content="2021-10-29T00:00:00&#43;00:00"
      />
    
    <meta property="article:modified_time" content="2021-10-29T00:00:00&#43;00:00">
  

  



  

  
  
  
  
  
    <script src="https://cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.1.1/cookieconsent.min.js" integrity="sha256-5VhCqFam2Cn+yjw61zbBNrbHVJ6SRydPeKopYlngbiQ=" crossorigin="anonymous"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.1.1/cookieconsent.min.css" integrity="sha256-zQ0LblD/Af8vOppw18+2anxsuaz3pWYyVWi+bTvTH8Q=" crossorigin="anonymous">
  
  <script>
  window.addEventListener("load", function(){
    window.cookieconsent.initialise({
      "palette": {
        "popup": {
          "background": "#003f5c",
          "text": "rgb(255, 255, 255)"
        },
        "button": {
          "background": "rgb(255, 255, 255)",
          "text": "#003f5c"
        }
      },
      "theme": "classic",
      "content": {
        "message": "This website uses cookies to ensure you get the best experience on our website.",
        "dismiss": "Got it!",
        "link": "Learn more",
        "href": "/privacy/"
      }
    })});
  </script>


  





  <title>Dynamic Games | Matteo Courthoud</title>
</head>


<body id="top" data-spy="scroll" data-offset="70" data-target="#TableOfContents" class="page-wrapper   " data-wc-page-id="503282b0c5ba446d2962ab39615c5eae" >

  
  
  
  
  
  
  
  
  <script src="/js/wowchemy-init.min.66d3e0fff6d32c4ece05adee927fbd96.js"></script>

  


<aside class="search-modal" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Search</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#" aria-label="Close"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        
        <input name="q" id="search-query" placeholder="Search..." autocapitalize="off"
        autocomplete="off" autocorrect="off" spellcheck="false" type="search" class="form-control"
        aria-label="Search...">
        
      </div>

      
      

      

    </section>
    <section class="section-search-results">

      <div id="search-hits">
        
      </div>

    </section>
  </div>
</aside>



  <div class="page-header">
    












<nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id="navbar-main">
  <div class="container-xl">

    
    <div class="d-none d-lg-inline-flex">
      <a class="navbar-brand" href="/">Matteo Courthoud</a>
    </div>
    

    
    <button type="button" class="navbar-toggler" data-toggle="collapse"
            data-target="#navbar-content" aria-controls="navbar-content" aria-expanded="false" aria-label="Toggle navigation">
    <span><i class="fas fa-bars"></i></span>
    </button>
    

    
    <div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none">
      <a class="navbar-brand" href="/">Matteo Courthoud</a>
    </div>
    

    
    
    <div class="navbar-collapse main-menu-item collapse justify-content-end" id="navbar-content">

      
      <ul class="navbar-nav d-md-inline-flex">
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
            
            
            
              
            
            
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#about"><span>Home</span></a>
        </li>

        
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
            
            
            
              
            
            
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#posts"><span>Posts</span></a>
        </li>

        
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
            
            
            
              
            
            
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#projects"><span>Research</span></a>
        </li>

        
        

        
        <li class="nav-item dropdown">
          <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown" aria-haspopup="true"><span>Courses</span><span class="caret"></span>
          </a>
          <div class="dropdown-menu">
            
              <a class="dropdown-item" href="/course/ml-econ/"><span>Machine Learning for Economics</span></a>
            
              <a class="dropdown-item" href="/course/data-science/"><span>Data Science with Python</span></a>
            
              <a class="dropdown-item" href="/course/empirical-io/"><span>PhD Industrial Organization</span></a>
            
              <a class="dropdown-item" href="/course/metrics/"><span>PhD Econometrics</span></a>
            
              <a class="dropdown-item" href="https://pp4rs.github.io/"><span>Programming Practices for Research</span></a>
            
          </div>
        </li>

        
        

        

        
        
        

        

        
        
        
        

        
          
            
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/cv"><span>CV</span></a>
        </li>

        
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
            
            
            
              
            
            
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#contact"><span>Contact</span></a>
        </li>

        
        

      

        
      </ul>
    </div>

    <ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2">

      
      
        
      

      
      

      
      

      
      

    </ul>

  </div>
</nav>


  </div>

  <div class="page-body">
    

<div class="container-fluid docs">
  <div class="row flex-xl-nowrap">
    <div class="col-12 col-md-3 col-xl-2 docs-sidebar">
      
<nav class="collapse docs-links" id="docs-nav">
  
  
  
  
  
  

  
  
    

    
      

      <ul class="nav docs-sidenav">
        <li><a href="/course/"><i class="fas fa-arrow-left pr-1"></i>Courses</a></li>
      </ul>

      
      
        
          
        
      


  
    
    
    
    
      
    
    

    
      <div class="docs-toc-item">
        <a class="docs-toc-link " href="/course/empirical-io/">Empirical IO</a>
    
      
        <ul class="nav docs-sidenav">
      


  <li class=""><a href="/course/empirical-io/02_demand_estimation/">Demand Estimation</a></li>



  <li class=""><a href="/course/empirical-io/07_dynamics_singleagent/">Single Agent Dynamics</a></li>



  <li class="active"><a href="/course/empirical-io/08_dynamics_games/">Dynamic Games</a></li>



  <li class=""><a href="/course/empirical-io/11_logit_demand/">Coding: Logit Demand</a></li>



  <li class=""><a href="/course/empirical-io/12_blp_1995/">Coding: BLP (1995)</a></li>



  <li class=""><a href="/course/empirical-io/17_rust_1987/">Coding: Rust (1987)</a></li>

      
        </ul>
      
    

    
      </div>
    

    
  
</nav>

    </div>

    
    
    <div class="d-none d-xl-block col-xl-2 docs-toc">
      

      <ul class="nav toc-top">
        <li><a href="#" id="back_to_top" class="docs-toc-title">Contents</a></li>
      </ul>

      <nav id="TableOfContents">
  <ul>
    <li><a href="#introduction">Introduction</a>
      <ul>
        <li><a href="#intro">Intro</a></li>
        <li><a href="#single--vs-multi-agent">Single- vs Multi-Agent</a></li>
        <li><a href="#plan">Plan</a></li>
      </ul>
    </li>
    <li><a href="#bajari-benkard-levin-2008">Bajari, Benkard, Levin (2008)</a>
      <ul>
        <li><a href="#model">Model</a></li>
        <li><a href="#model-2">Model (2)</a></li>
        <li><a href="#value-function">Value Function</a></li>
        <li><a href="#equilibrium">Equilibrium</a></li>
        <li><a href="#estimation">Estimation</a></li>
        <li><a href="#bbl-overview">BBL Overview</a></li>
        <li><a href="#bbl-first-stage">BBL: First Stage</a></li>
        <li><a href="#bbl-second-stage">BBL: Second Stage</a></li>
        <li><a href="#in-practice-for-a-parameter-value-tilde-thetapi">In practice, for a parameter value $\tilde \theta^\pi$</a></li>
        <li><a href="#objective-function">Objective Function</a></li>
        <li><a href="#objective-function-2">Objective Function (2)</a></li>
        <li><a href="#estimator">Estimator</a></li>
        <li><a href="#advantages">Advantages</a></li>
        <li><a href="#problems">Problems</a></li>
      </ul>
    </li>
    <li><a href="#ericson-pakes-1995">Ericson Pakes (1995)</a>
      <ul>
        <li><a href="#introduction-1">Introduction</a></li>
        <li><a href="#state-transitions">State Transitions</a></li>
        <li><a href="#decision-variables">Decision Variables</a></li>
        <li><a href="#equilibrium-1">Equilibrium</a></li>
        <li><a href="#exit">Exit</a></li>
        <li><a href="#entry">Entry</a></li>
        <li><a href="#equilibrium-existence">Equilibrium Existence</a></li>
        <li><a href="#solving-the-model">Solving the Model</a></li>
        <li><a href="#policy-update-example-exit-game">Policy Update Example: exit game</a></li>
        <li><a href="#convergence-tips">Convergence Tips</a></li>
        <li><a href="#multiple-equilibria">Multiple Equilibria</a></li>
        <li><a href="#multiple-equilibria-2">Multiple Equilibria (2)</a></li>
        <li><a href="#curse-of-dimensionality">Curse of Dimensionality</a></li>
        <li><a href="#curse-of-dimensionality-2">Curse of Dimensionality (2)</a></li>
        <li><a href="#oblivious-equilibrium">Oblivious Equilibrium</a></li>
        <li><a href="#games-with-random-moves">Games with Random Moves</a></li>
        <li><a href="#games-in-continuous-time">Games in Continuous Time</a></li>
        <li><a href="#comparison">Comparison</a></li>
        <li><a href="#applications">Applications</a></li>
      </ul>
    </li>
    <li><a href="#from-io-to-ai">From IO to AI</a>
      <ul>
        <li><a href="#bridging-two-literatures">Bridging two Literatures</a></li>
        <li><a href="#pakes-and-mcguire-2001">Pakes and McGuire (2001)</a></li>
        <li><a href="#comments">Comments</a></li>
        <li><a href="#q-learning">Q-Learning</a></li>
        <li><a href="#applications-1">Applications</a></li>
      </ul>
    </li>
    <li><a href="#appendix">Appendix</a>
      <ul>
        <li><a href="#references-references">References [references]</a></li>
      </ul>
    </li>
  </ul>
</nav>

      
    </div>
    

    <main class="col-12 col-md-9 col-xl-8 py-md-3 pl-md-5 docs-content" role="main">

      <article class="article">

        <div class="docs-article-container">

          <h1>Dynamic Games</h1>

          <p>Last updated on Oct 29, 2021</p>

          <div class="article-style">
            <h2 id="introduction">Introduction</h2>
<h3 id="intro">Intro</h3>
<p>Setting: agents making <strong>strategic decisions</strong> (new) in <strong>dynamic
environments</strong>.</p>
<ul>
<li>Entry and exit: Collard-Wexler (<a href="#ref-collard2013demand">2013</a>)</li>
<li>Sunk costs: Ryan (<a href="#ref-ryan2012costs">2012</a>)</li>
<li>Innovation: Goettler and Gordon (<a href="#ref-goettler2011does">2011</a>)
<ul>
<li>(or whatever changes in response to investment)</li>
</ul>
</li>
<li>Exploitation of natural resources: Huang and Smith
(<a href="#ref-huang2014dynamic">2014</a>)</li>
<li>Durable goods: Esteban and Shum (<a href="#ref-esteban2007durable">2007</a>)</li>
</ul>
<blockquote>
<p><strong>Lit review</strong>: forthcoming IO Handbook chapter Aguirregabiria,
Collard-Wexler, and Ryan (<a href="#ref-aguirregabiria2021dynamic">2021</a>)</p>
</blockquote>
<h3 id="single--vs-multi-agent">Single- vs Multi-Agent</h3>
<p>Typically in IO we study agents in <strong>strategic</strong> environments.
Complicated in dynamic environments.</p>
<ul>
<li><strong>Curse of dimensionality</strong>
<ul>
<li>Single agent: need to track what the agent sees ($k$ states)</li>
<li>Multi-agent: need to keep track what every agent sees
($k^J$states)</li>
<li>Difference exponential in the number of agents</li>
</ul>
</li>
<li><strong>Expectations</strong>
<ul>
<li>Need not only to keep track of how the environment evolves</li>
<li>… but also of how other players act</li>
</ul>
</li>
<li><strong>Equilibrium</strong>
<ul>
<li>Because of the strategic interaction, the Bellman equation is
<em>not a contraction</em> anymore
<ul>
<li>Equilibrium existence?</li>
<li>Equilibrium uniqueness?</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="plan">Plan</h3>
<p>We will cover <strong>first the estimation</strong> and then the computation of
dynamic games</p>
<ul>
<li>Weird…</li>
<li>Standard estimation method: Bajari, Benkard, and Levin
(<a href="#ref-bajari2007estimating">2007</a>)</li>
<li>Does <strong>not</strong> require to solve the model</li>
<li>Indeed, that’s the <strong>advantage</strong> of the method</li>
<li><strong>Disadvantages</strong>: still need to solve the model for counterfactuals</li>
<li>So we’ll cover computation afterwards</li>
</ul>
<p><strong>Last</strong>: bridge between Structural IO and Artificial Intelligence</p>
<ul>
<li>Different <em>objectives</em> but similar <em>methods</em></li>
<li>Dynamic tools niche in IO but at the core of AI</li>
</ul>
<h2 id="bajari-benkard-levin-2008">Bajari, Benkard, Levin (2008)</h2>
<h3 id="model">Model</h3>
<p>Stylized version of Ericson and Pakes (<a href="#ref-ericson1995markov">1995</a>)
(no entry/exit)</p>
<ul>
<li>
<p>$J$ firms (products) indexed by $j \in \lbrace 1, &hellip;, J \rbrace$</p>
</li>
<li>
<p>Time $t$ is dicrete, horizon is infinite</p>
</li>
<li>
<p><strong>States</strong> $s_{jt} \in \lbrace 1, &hellip; \bar s \rbrace$: quality of
product $j$ in period $t$</p>
</li>
<li>
<p><strong>Actions</strong> $a_{jt} \in \mathbb R^+$: investment decision of firm
$j$ in period $t$</p>
</li>
<li>
<p><strong>Static payoffs</strong> $$
\pi_j (s_{jt}, \boldsymbol s_{-jt}, a_{jt}; \theta^\pi)
$$ where</p>
<ul>
<li>$\boldsymbol s_{-it}$: state vector of all other firms in period
$t$</li>
<li>$\theta^\pi$: parameters that govern static profits</li>
</ul>
</li>
</ul>
<blockquote>
<p><strong>Note</strong>: if we micro-fund $\pi(\cdot)$ , e.g. with some demand and
supply model, we have 2 strategic decisions: prices (static) and
investment (dynamic).</p>
</blockquote>
<h3 id="model-2">Model (2)</h3>
<ul>
<li>
<p><strong>State transitions</strong> $$
\boldsymbol s_{t+1} = f(\boldsymbol s_t, \boldsymbol a_t, \boldsymbol \epsilon_t; \theta^f)
$$ where</p>
<ul>
<li>$\boldsymbol a_t$: vector of actions of all firm</li>
<li>$\boldsymbol \epsilon_t$: vector of idiosyncratic shocks</li>
<li>$\theta^f$: parameters that govern state transitions</li>
</ul>
</li>
<li>
<p><strong>Objective function</strong>: firms maximize expected discounted future
profits $$
\max_{\boldsymbol a} \ \mathbb E_t \left[ \sum_{\tau=0}^\infty \beta^{\tau} \pi_{j, t+\tau} (\theta^\pi) \right]
$$</p>
</li>
</ul>
<h3 id="value-function">Value Function</h3>
<p>The value function of firm $j$ at time $t$ in state $\boldsymbol s_{t}$,
under a set of strategy functions $\boldsymbol P$ (one for each firm) is
$$
V^{\boldsymbol P_{-j}}<em>{j} (\mathbf{s}</em>{t}) = \max_{a_{jt} \in \mathcal{A}<em>j \left(\mathbf{s}</em>{t}\right)} \Bigg\lbrace \pi_{j}^{\boldsymbol P_{-j}} (a_{jt}, \mathbf{s}<em>{t} ; \theta^\pi ) + \beta \mathbb E</em>{\boldsymbol s_{t+1}} \Big[  V_{j}^{\boldsymbol P_{-j}} \left(\mathbf{s}<em>{t+1}\right) \ \Big| \ a</em>{jt}, \boldsymbol s_{t} ; \theta^f \Big] \Bigg\rbrace
$$ where</p>
<ul>
<li>
<p>$\pi_{j}^{\boldsymbol P_{-j}} (a_{jt}, \mathbf{s}<em>{t} ; \theta^\pi )$
are the static profits of firm $j$ given action $a</em>{jt}$ and policy
functions $\boldsymbol P_{-j}$ for all firms a part from $j$</p>
</li>
<li>
<p>The expecation $\mathbb E$ is taken with respect to the conditional
transition probabilities
$f^{\boldsymbol P_{-j}} (\mathbf{s}<em>{t+1} | \mathbf{s}</em>{t}, a_{jt} ; \theta^f)$</p>
</li>
</ul>
<h3 id="equilibrium">Equilibrium</h3>
<p>Equillibrium notion: <strong>Markow Perfect Equilibrium</strong> (<a href="#ref-maskin1988theory">Maskin and Tirole
1988</a>)</p>
<ul>
<li><strong>Assumption</strong>: players’ strategies at period $t$ are functions only
of payoff-relevant state variables at the same period</li>
<li><strong>Definition</strong>: a set of $J$ value and policy functions,
$\boldsymbol V$ and $\boldsymbol P$ such that each firm
<ol>
<li>maximizes its value function $V_j$</li>
<li>given the policy function of every other firm
$\boldsymbol P_{-j}$</li>
</ol>
</li>
</ul>
<p>What is it basically?</p>
<ul>
<li>Nash Equilibrium in the policy functions</li>
<li>What are we ruling out?
<ul>
<li>Strategies that depend on longer histories</li>
<li>E.g. “has anyone ever cheated in a cartel?”</li>
</ul>
</li>
</ul>
<h3 id="estimation">Estimation</h3>
<p>We want to estimate 2 sets of <strong>parameters</strong>:</p>
<ul>
<li>$\theta^\pi$: parameterizes period profit function $\pi(\cdot)$</li>
<li>$\theta^f$: parameterizes state transition function $f(\cdot)$</li>
</ul>
<p>Generally 2 <strong>approaches</strong></p>
<ol>
<li>Full solution
<ul>
<li>Impractical (we’ll see more details later)</li>
</ul>
</li>
<li>Rely on some sort of Hotz and Miller
(<a href="#ref-hotz1993conditional">1993</a>) CCP inversion
<ul>
<li>Aguirregabiria and Mira
(<a href="#ref-aguirregabiria2007sequential">2007</a>)</li>
<li>Bajari, Benkard, and Levin (<a href="#ref-bajari2007estimating">2007</a>)</li>
<li>Pakes, Ostrovsky, and Berry (<a href="#ref-pakes2007simple">2007</a>)</li>
<li>Pesendorfer and Schmidt-Dengler
(<a href="#ref-pesendorfer2008asymptotic">2008</a>)</li>
</ul>
</li>
</ol>
<h3 id="bbl-overview">BBL Overview</h3>
<p>Bajari, Benkard, and Levin (<a href="#ref-bajari2007estimating">2007</a>) plan</p>
<ol>
<li>Estimate <strong>transition probabilities</strong> and <strong>conditional choice
probabilities</strong> from the data</li>
<li>Use them to simulate the <strong>expected value function</strong>, given a set of
parameters</li>
<li>Use optimality of estimated choices to pin down static profit
parameters
<ul>
<li>I.e. repeat (2) for alternative strategies
<ul>
<li>By definition suboptimal</li>
</ul>
</li>
<li><strong>Estimating equation</strong>: values implied by observed strategies
should be higher than values implied by alternative strategies</li>
</ul>
</li>
</ol>
<h3 id="bbl-first-stage">BBL: First Stage</h3>
<ul>
<li>Estimate the <strong>transition probabilities</strong>
$f ( \cdot | a_{jt}, \boldsymbol s_t; \hat \theta^f )$
<ul>
<li>I.e. what is the observed frequency of any state-to-state
transition?</li>
<li>For any given action of firm $j$</li>
</ul>
</li>
<li>… and <strong>conditional choice probabilities</strong>
$\hat P_j(\cdot | \boldsymbol s_t)$
<ul>
<li>I.e. what is the probability of each action, for each firm $j$
in each state $\boldsymbol s$</li>
</ul>
</li>
<li>Can be done <strong>non-parametrically</strong>
<ul>
<li>i.e. just observe frequencies</li>
<li>Conditional on having enough data</li>
<li><strong>Note</strong>: need to estimate transitions, conditional on each
state and action</li>
<li>Problem with many states and actions, but especially with <strong>many
players</strong>
<ul>
<li>Curse of dimensionality</li>
<li>Number of states increases exponentially in number of
players</li>
</ul>
</li>
</ul>
</li>
</ul>
<blockquote>
<p><strong>Important</strong>: parametric assumptions would contradict the model for
the estimation of value/policy functions</p>
</blockquote>
<h3 id="bbl-second-stage">BBL: Second Stage</h3>
<p><strong>First step</strong>: from transitions $f(\hat \theta^f)$ and CCPs
$\boldsymbol{\hat P}$ to values</p>
<ul>
<li>
<p>We can use transitions and CCPs to simulate <strong>histories</strong> (of length
$\tilde T$)</p>
<ul>
<li>of states
$\lbrace \boldsymbol{\tilde{s}<em>{\tau}} \rbrace</em>{\tau = 1}^{\tilde T}$</li>
<li>and actions
$\lbrace \boldsymbol{\tilde{a}<em>{\tau}} \rbrace</em>{\tau = 1}^{\tilde T}$</li>
</ul>
</li>
<li>
<p>Given a parameter value $\tilde \theta^\pi$, we can compute <strong>static
payoffs</strong>:
$\pi_{j}^{\boldsymbol {\hat{P}<em>{-j}}} \left( \tilde a</em>{j\tau}, \boldsymbol{\tilde s}_{\tau} ; \tilde \theta^\pi \right)$</p>
</li>
<li>
<p>Simulated history + static payoffs = <strong>simulated value function</strong> $$
{V}<em>{j}^{\boldsymbol {\hat{P}}} \left(\boldsymbol{s}</em>{t} ; \tilde \theta^\pi \right) =  \sum_{\tau=0}^{\tilde T} \beta^{\tau} \pi_{j}^{\boldsymbol {\hat{P}<em>{-j}}} \left( \tilde a</em>{j\tau}, \boldsymbol{\tilde s}_{\tau} ; \tilde \theta^\pi \right)
$$</p>
</li>
<li>
<p>We can average over many, e.g. $R$, simulated value functions to get
an <strong>expected value function</strong> $$
{V}<em>{j}^{\boldsymbol {\hat{P}}, R} \left( \boldsymbol{s}</em>{t} ; \tilde \theta^\pi \right) = \frac{1}{R}  \sum_{r=0}^{R}\Bigg( \sum_{\tau=0}^{\tilde T} \beta^{\tau} \pi_{j}^{\boldsymbol {\hat{P}<em>{-j}}} \left(\tilde a^{(r)}</em>{j\tau}, \boldsymbol{\tilde s}^{(r)}_{\tau} ; \tilde \theta^\pi \right) \Bigg)
$$</p>
</li>
</ul>
<h3 id="in-practice-for-a-parameter-value-tilde-thetapi">In practice, for a parameter value $\tilde \theta^\pi$</h3>
<p>For $r = 1, &hellip;, R$ simulations do:</p>
<ul>
<li>Initialize firms value to zero</li>
<li>Fot $\tau=0, &hellip;, \tilde T$ do
<ul>
<li>For each state in $\boldsymbol{\tilde s}^{(r)}_{\tau}$ do:
<ul>
<li>Use $\boldsymbol{\hat P}$ to <em>draw</em> a vector of firm actions
$\boldsymbol{\tilde a}^{(r)}_{\tau}$</li>
<li>For each firm $j = 1, &hellip;, J$ do:
<ul>
<li>Compute static profits
$\pi_{j}^{\boldsymbol {\hat{P}<em>{-j}}} \left(\tilde a^{(r)}</em>{j\tau}, \boldsymbol{\tilde s}^{(r)}_{\tau} ; \tilde \theta^\pi \right)$</li>
<li>Add discounted profits
$\beta^{\tau} \pi_{j}^{\boldsymbol {\hat{P}<em>{-j}}} \left(\tilde a^{(r)}</em>{j\tau}, \boldsymbol{\tilde s}^{(r)}_{\tau} ; \tilde \theta^\pi \right)$
to the value function</li>
</ul>
</li>
<li>Use
$f ( \cdot | \boldsymbol {a_{t}}, \boldsymbol s_t; \hat \theta^f )$
to <em>draw</em> the next state
$\boldsymbol{\tilde s}^{(r)}_{\tau + 1}$</li>
<li>Use the next state, $\boldsymbol{\tilde s}^{(r)}_{\tau + 1}$
as current state for the next iteration</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>Then average all the value functions together to obtain an <strong>expected
value function</strong>
$V_{j}^{\boldsymbol {\hat{P}}, R} \left(\boldsymbol{s}_{t} ; \tilde \theta^\pi \right)$</p>
<blockquote>
<p><strong>Note</strong>: advantage of simulations: can be parallelized</p>
</blockquote>
<h3 id="objective-function">Objective Function</h3>
<p>What have we done so far?</p>
<ul>
<li>Given some parameters $\theta^\pi$, we computed the <strong>expected value
function</strong></li>
</ul>
<p>How do we pick the $\theta^\pi$ that best rationalizes the data?</p>
<ul>
<li>I.e. what is the <strong>objective function</strong>?</li>
<li>Potentially many options</li>
</ul>
<p><strong>BBL idea</strong></p>
<ul>
<li>the expected value function has to be optimal, given the CCPs</li>
<li>I.e. any other policy function should give a lower expected value</li>
<li><strong>“Best”</strong> $\theta^\pi$: those for which the implied expected value
function under the estimated CCPs is greater than the one implied by
<em>any other</em> CCP</li>
</ul>
<blockquote>
<p><strong>Note</strong>: it’s an inequality statement</p>
</blockquote>
<h3 id="objective-function-2">Objective Function (2)</h3>
<p><strong>Idea</strong></p>
<ul>
<li>
<p>If the observed policy ${\color{green}{\boldsymbol{\hat P}}}$ is
optimal,</p>
<ul>
<li>
<p>All other policies ${\color{red}{\boldsymbol{\tilde P}}}$</p>
</li>
<li>
<p>… at the true parameters $\theta^f$</p>
</li>
<li>
<p>… should give a lower expected value $$
V_{j}^{{\color{red}{\boldsymbol{\tilde P}}}, R} \left( \boldsymbol{s}<em>{t} ; \tilde \theta^\pi \right) \leq V</em>{j}^{{\color{green}{\boldsymbol{\hat P}}}, R} \left( \boldsymbol{s}_{t} ; \tilde \theta^\pi \right)
$$</p>
</li>
</ul>
</li>
<li>
<p>So which are the true parameters?</p>
<ul>
<li>
<p>Those for which any deviation from the observed policy
${\color{green}{\boldsymbol{\hat P}}}$ yields a lower value</p>
</li>
<li>
<p><strong>Objective function</strong> to minimize: <strong>violations</strong> under
alternative policies ${\color{red}{\boldsymbol{\tilde P}}}$ $$
\min_{\tilde \theta^\pi} \sum_{\boldsymbol s_{t}} \sum_{{\color{red}{\boldsymbol{\tilde P}}}} \Bigg[\min \bigg\lbrace V_{j}^{{\color{green}{\boldsymbol{\hat P}}}, R} \left( \boldsymbol{s}<em>{t} ; \tilde \theta^\pi \right) - V</em>{j}^{{\color{red}{\boldsymbol{\tilde P}}}, R} \left( \boldsymbol{s}_{t} ; \tilde \theta^\pi \right) \ , \ 0 \bigg\rbrace \Bigg]^{2}
$$</p>
</li>
</ul>
</li>
</ul>
<h3 id="estimator">Estimator</h3>
<p><strong>Estimator</strong>: $\theta^\pi$ that minimizes the average (squared)
magnitude of violations for any alternative policy
${\color{red}{\boldsymbol{\tilde P}}}$ $$
\hat{\theta}^\pi= \arg \min_{\tilde \theta^\pi} \sum_{\boldsymbol s_{t}} \sum_{{\color{red}{\boldsymbol{\tilde P}}}} \Bigg[\min \bigg\lbrace V_{j}^{{\color{green}{\boldsymbol{\hat P}}}, R} \left( \boldsymbol{s}<em>{t} ; \tilde \theta^\pi \right) - V</em>{j}^{{\color{red}{\boldsymbol{\tilde P}}}, R} \left( \boldsymbol{s}_{t} ; \tilde \theta^\pi \right) \ , \ 0 \bigg\rbrace \Bigg]^{2}
$$</p>
<ul>
<li>$\min \Big\lbrace V_{j}^{{\color{green}{\boldsymbol{\hat P}}}, R} - V_j^{{\color{red}{\boldsymbol{\tilde P}}}, R} \ , \ 0 \Big\rbrace$
to pick only the violations
<ul>
<li>If ${\color{green}{\boldsymbol{\hat P}}}$ implies higher value,
we can ignore</li>
<li>Doesn’t matter by how much you respect the inequality</li>
</ul>
</li>
<li><strong>Which alternative policies</strong>
${\color{red}{\boldsymbol{\tilde P}}}$ should we use?
<ul>
<li>In principle, any perturbation is ok</li>
<li>But <strong>in practice</strong>, if we perturbe it too much, we can go too
far off</li>
<li><strong>Tip 1</strong>: start with very <em>small</em> perturbations</li>
<li><strong>Tip 2</strong>: use perturbation that <em>sensibly</em> affect the dynamics
<ul>
<li>E.g. exiting in a state in which a firm is not a competitive
threat</li>
</ul>
</li>
<li><strong>Tip 3</strong>: use perturbations on dimensions that are <em>relevant</em>
for the research question
<ul>
<li>E.g. they affect dimensions where you want to make
counterfactual predictions</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="advantages">Advantages</h3>
<p>We have seen that there are <strong>competing methods</strong>.</p>
<p>What are the <strong>advantages</strong> of Bajari, Benkard, and Levin
(<a href="#ref-bajari2007estimating">2007</a>) over those?</p>
<ol>
<li><strong>Continuous actions</strong>
<ul>
<li>BBL does not require actions to be discretised</li>
<li>You can just sample actions from the data!</li>
</ul>
</li>
<li><strong>Choice of alternative CCPs</strong>
<ul>
<li>The researcher is free to choose the alternative CCPs
${\color{red}{\boldsymbol{\tilde P}}}$</li>
<li><strong>Pros</strong>: can make source of variation more transparent
<ul>
<li><em>allows the researcher to focus on those predictions of the
model that are key for the specific research questions</em></li>
</ul>
</li>
<li><strong>Cons</strong>: it’s a <em>very</em> high dimensional space
<ul>
<li>There are <em>very very</em> many alternative policy functions</li>
</ul>
</li>
</ul>
</li>
</ol>
<h3 id="problems">Problems</h3>
<ol>
<li>Computational <strong>curse of dimensionality</strong> is gone (in the state
space)
<ul>
<li>But we have a curse of dimensionality in data</li>
<li>Need a lot of markets because <strong>now 1 market is 1 observation</strong></li>
</ul>
</li>
<li><strong>Multiple equilibria</strong>??
<ul>
<li>We are basically assuming it away</li>
<li>Estimating the CCPs in the first stage we assume that is the
equilibrium that is played in all markets at all times</li>
<li>To run counterfactuals, we <strong>still need to solve the model</strong></li>
</ul>
</li>
<li>Unobserved heterogeneity
<ul>
<li>Kasahara and Shimotsu (<a href="#ref-kasahara2009nonparametric">2009</a>):
how to identify the (minimum) number of unobserved types</li>
<li>Arcidiacono and Miller
(<a href="#ref-arcidiacono2011conditional">2011</a>): how to use an EM
algorithm for the 1st stage estimation with unobserved types,
conditional on the number of types</li>
<li>Berry and Compiani (<a href="#ref-berry2021empirical">2021</a>):
instrumental variables approach, relying on observed states in
the distant past</li>
</ul>
</li>
<li><strong>Non-stationarity</strong>
<ul>
<li>If we have a long time period, something fundamentally might
have changed</li>
</ul>
</li>
</ol>
<h2 id="ericson-pakes-1995">Ericson Pakes (1995)</h2>
<h3 id="introduction-1">Introduction</h3>
<p>Ericson and Pakes (<a href="#ref-ericson1995markov">1995</a>) and companion paper
Pakes and McGuire (<a href="#ref-pakes1994computing">1994</a>) for the computation</p>
<ul>
<li>
<p>$J$ firms indexed by $j \in \lbrace 1, &hellip;, J \rbrace$</p>
</li>
<li>
<p>Time $t$ is dicrete $t$, horizon is infinite</p>
</li>
<li>
<p>State $s_{jt}$: quality of firm $j$ in period $t$</p>
</li>
<li>
<p>Per period profits $$
\pi (s_{jt}, \boldsymbol s_{-jt}, ; \theta^\pi)
$$ where</p>
<ul>
<li>$\boldsymbol s_{-it}$: state vector of all other firms in period
$t$</li>
<li>$\theta^\pi$: parameters that govern static profits</li>
</ul>
</li>
<li>
<p>We can micro-fund profits with some demand and supply functions</p>
<ul>
<li>There can be some underlying static strategic interaction</li>
<li>E.g. logit demand and bertrand competition in prices $p_{it}$</li>
</ul>
</li>
</ul>
<h3 id="state-transitions">State Transitions</h3>
<p><strong>Investment</strong>: firms can invest an dollar amount $x$ to increase their
future quality</p>
<ul>
<li>
<p>Continuous decision variable ($\neq$ Rust)</p>
</li>
<li>
<p>Probability that investment is successful $$
\Pr \big(i_{jt} \ \big| \ a_{it} = x \big) = \frac{\alpha x}{1 + \alpha x}
$$</p>
</li>
<li>
<p>Higher investment, higher success probability</p>
</li>
<li>
<p>$\alpha$ parametrizes the returns on investment</p>
</li>
</ul>
<p><strong>Quality depreciation</strong></p>
<ul>
<li>With probability $\delta$, quality decreases by one level</li>
</ul>
<p><strong>Law of motion</strong> $$
s_{j,t+1} = s_{jt} + i_{jt} - \delta
$$</p>
<h3 id="decision-variables">Decision Variables</h3>
<p>Note that in Ericson and Pakes (<a href="#ref-ericson1995markov">1995</a>) we have
two separate decision variables</p>
<ol>
<li><strong>Static</strong> decision variable: price $p_{jt}$</li>
<li><strong>Dynamic</strong> decision variable: investment $i_{jt}$</li>
</ol>
<p>Does not have to be the case!</p>
<p><strong>Example</strong>: Besanko et al. (<a href="#ref-besanko2010learning">2010</a>)</p>
<ul>
<li>Model of <strong>learning-by-doing</strong>: firms decrease their marginal cost
through sales</li>
<li>State variable: firm stock of know how $e$
<ul>
<li>The higher the stock of know-how, the lower the marginal cost</li>
<li>Increases when a firm manages to make a sale
<ul>
<li>$q \in [0,1]$ now is both static quantity and transition
probability</li>
</ul>
</li>
</ul>
</li>
<li><strong>Single</strong> decision variable: price $p$
<ul>
<li>Usual static effects on profits
$\pi_{jt} = (p_{jt} - c(e_{jt})) \cdot q_j(\boldsymbol p_t)$</li>
<li>But also dynamic effect through transition probabilities
<ul>
<li>Probability of increasing $e_t$: $q_j(\boldsymbol p_t)$</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="equilibrium-1">Equilibrium</h3>
<p>Firms maximize the expected flow of discounted profits $$
\max_{\boldsymbol a} \ \mathbb E_t \left[ \sum_{\tau=0}^\infty \beta^{\tau} \pi_{j, t+\tau} (\theta^\pi) \right]
$$ <strong>Markow Perfect Equilibrium</strong></p>
<p>Equillibrium notion: <strong>Markow Perfect Equilibrium</strong> (<a href="#ref-maskin1988theory">Maskin and Tirole
1988</a>)</p>
<ul>
<li>A set of $J$ value and policy functions, $\boldsymbol V$ and
$\boldsymbol P$ such that each firm
<ol>
<li>maximizes its value function $V_j$</li>
<li>given the policy function of every other firm
$\boldsymbol P_{-j}$</li>
</ol>
</li>
</ul>
<h3 id="exit">Exit</h3>
<p>One important extension is <strong>exit</strong>.</p>
<ul>
<li>In each time period, incuments decide whether to stay</li>
<li>… or exit and get a scrap value $\phi^{exit}$</li>
</ul>
<p>The Belman Equation of incumbent $j$ at time $t$ is $$
V^{\boldsymbol P_{-j}}<em>{j} (\mathbf{s}</em>{t}) = \max_{d^{exit}<em>{jt} \in \lbrace 0, 1 \rbrace} \Bigg\lbrace
\begin{array}{c}
\beta \phi^{exit} \ , \newline
\max</em>{a_{jt} \in \mathcal{A}<em>j \left(\mathbf{s}</em>{t}\right)} \Big\lbrace  \pi_{j}^{\boldsymbol P_{-j}} (a_{jt}, \mathbf{s}<em>{t} ; \theta^\pi ) + \beta \mathbb E</em>{\boldsymbol s_{t+1}} \Big[  V_{j}^{\boldsymbol P_{-j}} \left(\mathbf{s}<em>{t+1}\right) \ \Big| \ a</em>{jt}, \boldsymbol s_{t} ; \theta^f \Big] \Big\rbrace
\end{array}
\Bigg\rbrace
$$ where</p>
<ul>
<li>$\phi^{exit}$: exit scrap value</li>
<li>$d^{exit}_{jt} \in \lbrace 0,1 \rbrace$: exit decision</li>
</ul>
<h3 id="entry">Entry</h3>
<p>We can also incorporate endogenous <strong>entry</strong>.</p>
<ul>
<li>One or more <strong>potential entrants</strong> exist outside the market</li>
<li>They can pay an entry cost $\phi^{entry}$ and enter the market at a
quality state $\bar s$</li>
<li>… or remain outside at no cost</li>
</ul>
<p>Value function $$
V_{j}^{\boldsymbol P_{-j}} (e, \boldsymbol x_{-jt} ; \theta) = \max_{d^{entry} \in \lbrace 0,1 \rbrace }
\Bigg\lbrace
\begin{array}{c}
0 \ ; \newline</p>
<ul>
<li>
<p>\phi^{entry} + \beta \mathbb E_{\boldsymbol s_{t+1}} \Big[ V_{j}^{\boldsymbol P_{-j}} (\bar s, \boldsymbol s_{-j, t+1} ; \theta) \ \Big| \ \boldsymbol s_{t} ; \theta^f \Big]
\end{array}
\Bigg\rbrace
$$ where</p>
</li>
<li>
<p>$d^{entry} \in \lbrace 0,1 \rbrace$: entry decision</p>
</li>
<li>
<p>$\phi^{entry}$: entry cost</p>
</li>
<li>
<p>$\bar s$: state in which entrants enters (could be random)</p>
</li>
</ul>
<p>Do we observe potential entrants?</p>
<ul>
<li>Igami (<a href="#ref-igami2017estimating">2017</a>): tech industry announce
their entry</li>
<li>Critique: not really potential entrants, they are half-way inside</li>
</ul>
<h3 id="equilibrium-existence">Equilibrium Existence</h3>
<p>Doraszelski and Satterthwaite (<a href="#ref-doraszelski2010computable">2010</a>):
a MPE might not exist in Ericson and Pakes
(<a href="#ref-ericson1995markov">1995</a>) model.</p>
<p><strong>Solution</strong></p>
<ul>
<li>Replace fixed entry costs $\phi^{entry}$ and exit scrap values
$\phi^{exit}$ with random ones</li>
<li>It becomes a game of incomplete information
<ul>
<li>First explored in Rust (<a href="#ref-rust1994structural">1994</a>)</li>
</ul>
</li>
<li>New equilibrium concept</li>
</ul>
<p><strong>Markov Perfect Bayesian Nash Equilibrium (MPBNE)</strong></p>
<ul>
<li>Basically the same, with rational beliefs on random variables</li>
</ul>
<h3 id="solving-the-model">Solving the Model</h3>
<p>Solving the model is very similar to Rust</p>
<ul>
<li>Given parameter values $\theta$</li>
<li>Start with a guess for the value and policy functions</li>
<li>Until convergence, do:
<ul>
<li>For each firm $j = 1, &hellip;, J$, do:
<ul>
<li>Take the policy functions of all other firms</li>
<li>Compute the implied transition probabilities</li>
<li>Use them to compute the new policy function for firm $j$</li>
<li>Compute the implied value function</li>
</ul>
</li>
</ul>
</li>
</ul>
<blockquote>
<p>Where do things get complicated / tricky? Policy function update</p>
</blockquote>
<h3 id="policy-update-example-exit-game">Policy Update Example: exit game</h3>
<p>Imagine a stylized exit game with 2 firms</p>
<ul>
<li>Easy to get an update rule of the form: <em>“exit if opponent stays,
stay if opponent exits”</em></li>
</ul>
<p><strong>Computationally</strong></p>
<ul>
<li>Initialize policy functions to $(exit, exit)$</li>
<li>Iteration 1:
<ul>
<li>Each firm takes opponent policy as given: $exit$</li>
<li>Update own optimal policy: $stay$</li>
<li>New policy: $(stay, stay)$</li>
</ul>
</li>
<li>Iteration 2: $(stay, stay) \to (exit, exit)$</li>
<li>Iteration 2: $(exit, exit) \to (stay, stay)$</li>
<li>Etc…</li>
</ul>
<blockquote>
<p><strong>Issues</strong>: value function iteration might not converge and
equilibrium multeplicity.</p>
</blockquote>
<h3 id="convergence-tips">Convergence Tips</h3>
<ul>
<li>Try different <strong>starting values</strong>
<ul>
<li>Often it’s what makes the biggest difference</li>
<li>Ideally, start as close as possible to true values</li>
<li><strong>Approximation methods</strong> can help (we’ll see more later)
<ul>
<li>I.e. get a fast approximation to use as starting vlaue for
solution algorithm</li>
</ul>
</li>
</ul>
</li>
<li>Partial/stochastic <strong>value function update rule</strong>
<ul>
<li>Instead of $V&rsquo; = T(V)$, use $V&rsquo; = \alpha T(V) + (1-\alpha)V$</li>
<li>Very good to break loops, especially if $\alpha$ is stochastic,
e.g. $\alpha \sim U(0,1)$</li>
</ul>
</li>
<li>How large is the <strong>support of the entry/exit costs</strong>?
<ul>
<li>If support is too small, you end up back in the entry/exit loop</li>
</ul>
</li>
<li>Try alternative <strong>non-parallel updating schemes</strong>
<ul>
<li>E.g. update value one state at the time (in random order?)</li>
</ul>
</li>
<li>Last but not least: <strong>change the model</strong>
<ul>
<li>In particular, from simultaneous to alternating moves</li>
<li>or continuous time</li>
</ul>
</li>
</ul>
<h3 id="multiple-equilibria">Multiple Equilibria</h3>
<p>How to find them?</p>
<ul>
<li>Besanko et al. (<a href="#ref-besanko2010learning">2010</a>) and Borkovsky,
Doraszelski, and Kryukov (<a href="#ref-borkovsky2010user">2010</a>):
<strong>homotopy method</strong>
<ul>
<li>can find some equilibria, but not all</li>
<li>complicated to implement: need to compute first order conditions
$H(\boldsymbol V, \theta) = 0$ and their Jacobian
$\Delta H(\boldsymbol V, \theta)$</li>
<li><strong>Idea</strong>: trace the equilibrium correspondence
$H^{-1} = \lbrace (\boldsymbol V, \theta) : H(\boldsymbol V, \theta) = 0 \rbrace$
in the value-parameter space</li>
</ul>
</li>
<li>Eibelshäuser and Poensgen (<a href="#ref-eibelshauser2019markov">2019</a>)
<ul>
<li>Markov Quantal Response Equilibrium</li>
<li>approact dynamic games from a evolutionary game theory
perspective
<ul>
<li>actions played at random and those bringing highest payoffs
survive</li>
</ul>
</li>
<li>$\to$ homothopy method guaranteed to find one equilibrium</li>
</ul>
</li>
<li>Pesendorfer and Schmidt-Dengler
(<a href="#ref-pesendorfer2010sequential">2010</a>): some equilibria are not
Lyapunov-stable
<ul>
<li>BR iteration cannot find them unless you start exactly at the
solution</li>
</ul>
</li>
<li>Su and Judd (<a href="#ref-su2012constrained">2012</a>) and Egesdal, Lai, and
Su (<a href="#ref-egesdal2015estimating">2015</a>): same point, but numerically
<ul>
<li>using MPEC approach</li>
</ul>
</li>
</ul>
<h3 id="multiple-equilibria-2">Multiple Equilibria (2)</h3>
<p>Can we assume them away?</p>
<ul>
<li>Igami (<a href="#ref-igami2017estimating">2017</a>)
<ul>
<li>Finite horizon</li>
<li>Homogenous firms (in profit functions and state transitions)</li>
<li>One dynamic move per period (overall, not per-firm)</li>
</ul>
</li>
<li>Abbring and Campbell (<a href="#ref-abbring2010last">2010</a>)
<ul>
<li>Entry/exit game</li>
<li>Homogeneous firms</li>
<li>Entry and exit decisions are follow a last-in first-out (LIFO)
structure
<ul>
<li><em>“An entrant expects to produce no longer than any
incumbent”</em></li>
</ul>
</li>
</ul>
</li>
<li>Iskhakov, Rust, and Schjerning (<a href="#ref-iskhakov2016recursive">2016</a>)
<ul>
<li>can find all equilibria, but for very specific class of dynamic
games</li>
<li>must always proceed “forward”
<ul>
<li>e.g. either entry or exit but not both</li>
</ul>
</li>
<li>Idea: can solve by backward induction even if horizon is
infinite</li>
</ul>
</li>
</ul>
<h3 id="curse-of-dimensionality">Curse of Dimensionality</h3>
<p>What are the computational bottlenecks? $$
V^{\boldsymbol P_{-j}}<em>{j} ({\color{red}{\mathbf{s}</em>{t}}}) = \max_{a_{jt} \in \mathcal{A}<em>j \left(\mathbf{s}</em>{t}\right)} \Bigg\lbrace \pi_{j}^{\boldsymbol P_{-j}} (a_{jt}, \mathbf{s}<em>{t} ; \theta^\pi ) + \beta \mathbb E</em>{{\color{red}{\mathbf{s}<em>{t+1}}}} \Big[  V</em>{j}^{\boldsymbol P_{-j}} \left(\mathbf{s}<em>{t+1}\right) \ \Big| \ a</em>{jt}, \boldsymbol s_{t} ; \theta^f \Big] \Bigg\rbrace
$$</p>
<ol>
<li><strong>Dimension of the state space</strong>
<ul>
<li>In single agent problems, we have as many states as many values
of $s_{jt}$ ($k$)</li>
<li>In dynamics games, the state space goes from $k$ to $k^J$</li>
<li><strong>symmetry helps</strong>: state $[1,2,3]$ and $[1,3,2]$ become the
same for firm 1</li>
<li>How much do we gain? From $k^J$ to
$k \cdot {k + J - 2 \choose k - 1}$</li>
</ul>
</li>
<li><strong>Dimension of the integrand</strong>
<ul>
<li>If in single agent problems, we have to integrate over $\kappa$
outcomes,
<ul>
<li>4 in Rust: engine replaced (yes|no) $\times$ mileage
increases (yes|no)</li>
</ul>
</li>
<li>… in dynamic games, we have to consider $\kappa^J$ outcomes</li>
</ul>
</li>
</ol>
<blockquote>
<p><strong>Note</strong>: bottlenecks are not addittive but multiplicative: have to
solve the expectation for each point in the state space. Improving on
any of the two helps a lot.</p>
</blockquote>
<h3 id="curse-of-dimensionality-2">Curse of Dimensionality (2)</h3>
<p>Two and a half classes of solutions:</p>
<ol>
<li><strong>Computational</strong>: approximate the equilibrium
<ul>
<li>Doraszelski (<a href="#ref-doraszelski2003r">2003</a>): use Chebyshev
polynomials for a basis function</li>
<li>Farias, Saure, and Weintraub
(<a href="#ref-farias2012approximate">2012</a>): combine approximations
with a MPEC-like approach</li>
</ul>
</li>
<li><strong>Conceptual</strong>: define another game
<ul>
<li>Weintraub, Benkard, and Van Roy
(<a href="#ref-weintraub2008markov">2008</a>): oblivious equilibrium</li>
<li>Ifrach and Weintraub (<a href="#ref-ifrach2017framework">2017</a>): moment
based equilibrium</li>
<li>Doraszelski and Judd (<a href="#ref-doraszelski2012avoiding">2012</a>):
games in continuous time</li>
<li>Doraszelski and Judd (<a href="#ref-doraszelski2019dynamic">2019</a>):
games with random moves</li>
</ul>
</li>
<li>Kind of both: Pakes and McGuire (<a href="#ref-pakes2001stochastic">2001</a>)
<ul>
<li>experience-based equilibrium (<a href="#ref-fershtman2012dynamic">Fershtman and Pakes
2012</a>)</li>
</ul>
</li>
</ol>
<blockquote>
<p><strong>Note</strong>: useful also to get good starting values for a full solution
method!</p>
</blockquote>
<h3 id="oblivious-equilibrium">Oblivious Equilibrium</h3>
<p>Weintraub, Benkard, and Van Roy (<a href="#ref-weintraub2008markov">2008</a>): what
if <strong>firms had no idea about the state of other firms</strong>?</p>
<ul>
<li>or atomistic firms</li>
</ul>
<p>The value function becomes $$
V_{j} ({\color{red}{s_{t}}}) = \max_{a_{jt} \in \mathcal{A}<em>j \left({\color{red}{s</em>{t}}}\right)} \Bigg\lbrace {\color{red}{\mathbb E_{\boldsymbol s_t}}} \Big[ \pi_{j} (a_{jt}, \mathbf{s}<em>{t} ; \theta^\pi ) \Big|  P  \Big] + \beta \mathbb E</em>{{\color{red}{s_{t+1}}}} \Big[  V_{j} \left({\color{red}{s_{t+1}}}\right) \ \Big| \ a_{jt}, {\color{red}{s_{t}}} ; \theta^f \Big] \Bigg\rbrace
$$</p>
<ul>
<li>Now the state is just $s_t$ instead of $\boldsymbol s_t$
<ul>
<li><strong>Huge</strong> computational gain: from $k^J$ points to $k$</li>
<li>Also the expectation of future states is taken over $3$ instead
of $3^J$ points
<ul>
<li>(3 because quality can go up, down or stay the same)</li>
</ul>
</li>
</ul>
</li>
<li>But need to compute static profits as the expected value given the
current policy function
<ul>
<li>Need to keep track of the asymptotic state distribution as you
iterate the value</li>
</ul>
</li>
</ul>
<h3 id="games-with-random-moves">Games with Random Moves</h3>
<p>Doraszelski and Judd (<a href="#ref-doraszelski2019dynamic">2019</a>): what if
instead of simultaneously, firms would <strong>move one at the time at
random</strong>?</p>
<ul>
<li><strong>Important</strong>: to have the same frequency of play, <strong>a period now is
$J$ times shorter</strong></li>
</ul>
<p>The value function becomes $$
V^{\boldsymbol P_{-j}}<em>{j} (\mathbf{s}</em>{t}, {\color{red}{n=j}}) = \max_{a_{jt} \in \mathcal{A}<em>j \left(\mathbf{s}</em>{t}\right)} \Bigg\lbrace {\color{red}{\frac{1}{J}}}\pi_{j}^{\boldsymbol P_{-j}} (a_{jt}, \mathbf{s}<em>{t} ; \theta^\pi ) + {\color{red}{\sqrt[J]{\beta}}} \mathbb E</em>{{\color{red}{n,  s_{j, t+1}}}} \Big[  V_{j}^{\boldsymbol P_{-j}} \left(\mathbf{s}<em>{t+1}, {\color{red}{n}} \right) \ \Big| \ a</em>{jt}, \boldsymbol s_{t} ; \theta^f \Big] \Bigg\rbrace
$$</p>
<ul>
<li>$n$: indicates whose turn is to play</li>
<li>since a turn is $J$ times shorter, profits are $\frac{1}{J} \pi$ and
discount factor is $\sqrt[J]{\beta}$</li>
</ul>
<p><strong>Computational gain</strong></p>
<ul>
<li>Expectation now taken over $n, s_{j, t+1}$ instead of
$\boldsymbol s_{t+1}$</li>
<li>I.e. $Jk$ points instead of $3^k$ (3 because quality can go up, down
or stay the same)</li>
<li>Huge computational difference!</li>
</ul>
<h3 id="games-in-continuous-time">Games in Continuous Time</h3>
<p>Doraszelski and Judd (<a href="#ref-doraszelski2012avoiding">2012</a>): what’s the
advantage of continuous time?</p>
<ul>
<li>Probability that two firms take a decision simultaneously is zero</li>
</ul>
<p>With continuous time, the value function becomes $$
V^{\boldsymbol P_{-j}}<em>{j} (\mathbf{s}</em>{t}) = \max_{a_{jt} \in \mathcal{A}<em>j \left(\mathbf{s}</em>{t}\right)} \Bigg\lbrace \frac{1}{\lambda(a_{jt}) - \log(\beta)} \Bigg( \pi_{j}^{\boldsymbol P_{-j}} (a_{jt}, \mathbf{s}<em>{t} ; \theta^\pi ) + \lambda(a</em>{jt}) \mathbb E_{\boldsymbol s_{t+1}} \Big[  V_{j}^{\boldsymbol P_{-j}} \left(\mathbf{s}<em>{t+1}\right) \ \Big| \ a</em>{jt}, \boldsymbol s_{t} ; \theta^f \Big] \Bigg) \Bigg\rbrace
$$</p>
<ul>
<li>$\lambda(a_{jt}) = \delta + \frac{\alpha a_{jt}}{1 + \alpha a_{jt}}$
is the hazard rate for firm $j$ that <strong>something happens</strong>
<ul>
<li>i.e. either an increase in quality, with probability
$\frac{\alpha a_{jt}}{1 + \alpha a_{jt}}$</li>
<li>… or a decrease in quality with probability $\delta$</li>
</ul>
</li>
</ul>
<p><strong>Computational gain</strong></p>
<ul>
<li>Now the expectation over future states
$\mathbb E_{\boldsymbol s_{t+1}}$ is over $2J$ points instead of
$3^J$
<ul>
<li>3 because quality can go up, down or stay the same</li>
<li>2 because in continuous time we don’t care if the state does not
change (investment fails)</li>
</ul>
</li>
</ul>
<h3 id="comparison">Comparison</h3>
<p>Which method is <strong>best</strong>?</p>
<p>I compare them in Courthoud (<a href="#ref-courthoud2020approximation">2020</a>)</p>
<ul>
<li>Fastest: Weintraub, Benkard, and Van Roy
(<a href="#ref-weintraub2008markov">2008</a>)
<ul>
<li>Effectively transforms the game into single-agent dynamics</li>
</ul>
</li>
<li>Best trade-off: Doraszelski and Judd
(<a href="#ref-doraszelski2019dynamic">2019</a>)
<ul>
<li>Simple, practical and also helps in terms of equilibrium
multeplicity</li>
</ul>
</li>
<li>Also in Courthoud (<a href="#ref-courthoud2020approximation">2020</a>): games
with random order
<ul>
<li>Better approximation than Doraszelski and Judd
(<a href="#ref-doraszelski2019dynamic">2019</a>)</li>
<li>And similar similar time</li>
</ul>
</li>
</ul>
<h3 id="applications">Applications</h3>
<p>Some applications of these methods include</p>
<ul>
<li>Approximation methods
<ul>
<li>Sweeting (<a href="#ref-sweeting2013dynamic">2013</a>): product
repositioning among radio stations</li>
<li>Barwick and Pathak (<a href="#ref-barwick2015costs">2015</a>): entry and
exit in the real estate brokerage industry</li>
</ul>
</li>
<li>Oblivious equilibrium
<ul>
<li>Xu and Chen (<a href="#ref-xu2020structural">2020</a>): R&amp;D investment in
the Korean electric motor industry</li>
</ul>
</li>
<li>Moment based equilibrium
<ul>
<li>Jeon (<a href="#ref-jeon2020learning">2020</a>): demand learning in the
container shipping industry</li>
<li>Caoui (<a href="#ref-caoui2019estimating">2019</a>): technology adoption
with network effects in the movie industry</li>
<li>Vreugdenhil (<a href="#ref-vreugdenhil2020booms">2020</a>): search and
matching in the oil drilling industry</li>
</ul>
</li>
<li>Games in continuous time
<ul>
<li>Arcidiacono et al. (<a href="#ref-arcidiacono2016estimation">2016</a>):
entry, exit and scale decisions in retail competition</li>
</ul>
</li>
<li>Games with random moves
<ul>
<li>Igami (<a href="#ref-igami2017estimating">2017</a>): innovation, entry,
exit in the hard drive industry</li>
</ul>
</li>
</ul>
<h2 id="from-io-to-ai">From IO to AI</h2>
<h3 id="bridging-two-literatures">Bridging two Literatures</h3>
<p>There is one method to approximate the equilibrium in dynamic games that
is a bit different from the others: Pakes and McGuire
(<a href="#ref-pakes2001stochastic">2001</a>)</p>
<ul>
<li><strong>Idea</strong>: approximate the value function by Monte-Carlo simulation</li>
<li>Firms start with a guess for the alternative-specific value function</li>
<li>Act according to it</li>
<li>Observe realized payoffs and state transitions</li>
<li>And update the alternative-specific value function according to the
realized outcomes</li>
</ul>
<p><strong>Experience-Based Equilibrium</strong></p>
<ul>
<li>Defined in Fershtman and Pakes (<a href="#ref-fershtman2012dynamic">2012</a>)</li>
<li><strong>Def</strong>: <em>policy is optimal given beliefs of state transitions and
observed transitions are consistent with the beliefs</em></li>
<li><strong>Note</strong>: definition silent on off-equilibrium path beliefs</li>
</ul>
<h3 id="pakes-and-mcguire-2001">Pakes and McGuire (2001)</h3>
<ul>
<li>
<p>Players start with <strong>alternative-specific value function</strong></p>
<ul>
<li>yes, the ASV from Rust (<a href="#ref-rust1994structural">1994</a>)</li>
<li>$\bar V_{j,a}^{(0)} (\boldsymbol s ; \theta)$: initial value of
player $j$ for action $a$ in state $\boldsymbol s$</li>
</ul>
</li>
<li>
<p>Until convergence, do:</p>
<ul>
<li>
<p>Compute optimal action, given
$\bar V_{j, a}^{(t)} (\boldsymbol s ; \theta)$ $$
a^* = \arg \max_a \bar V_{j, a}^{(t)} (\boldsymbol s ; \theta)
$$</p>
</li>
<li>
<p>Observe the realized payoff
$\pi_{j, a^<em>}(\boldsymbol s ; \theta)$ and the realized next
state $\boldsymbol {s&rsquo;}(\boldsymbol s, a^</em>; \theta)$</p>
</li>
<li>
<p>Update the alternative-specific value function of the chosen
action $k^<em>$ $$
\bar V_{j, a^</em>}^{(t+1)} (\boldsymbol s ; \theta) = (1-\alpha_{\boldsymbol s, t}) \bar V_{j, a^<em>}^{(t)} (\boldsymbol s ; \theta) + \alpha_{\boldsymbol s, t} \Big[\pi_{j, a^</em>}(\boldsymbol s ; \theta) + \arg \max_a \bar V_{j, a}^{(t)} (\boldsymbol s&rsquo; ; \theta) \Big]
$$ where</p>
<ul>
<li>$\alpha_{\boldsymbol s, t} = \frac{1}{\text{number of times state } \boldsymbol s \text{ has been visited}}$</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="comments">Comments</h3>
<p>Where is the <strong>strategic interaction</strong>?</p>
<ul>
<li>Firm always take <em>“best action so far”</em> in each state
<ul>
<li>Start to take a new action only when the previous best has
performed badly for many periods</li>
</ul>
</li>
<li>Remindful of literature of evolutionary game theory</li>
</ul>
<p>Importance of <strong>starting values</strong></p>
<ul>
<li>Imagine, all payoffs are positive but value initialized to zero</li>
<li>First action in each state $\to$ only action ever taken in that
state</li>
<li>Loophole.
<ul>
<li>Why? Firms always take $\arg \max_a \bar V_a$ and never
<em>explore</em> the alternatives</li>
</ul>
</li>
</ul>
<p><strong>Convergence</strong> by desing</p>
<ul>
<li>As $\lim_{t \to \infty} \alpha_{\boldsymbol s, t} = 1$</li>
<li>Firms stop updating the value by design</li>
</ul>
<h3 id="q-learning">Q-Learning</h3>
<p>Computer Science reinforcement learning literature (AI): <strong>Q-learning</strong></p>
<p><strong>Differences</strong></p>
<ul>
<li>$\bar V_a( \boldsymbol s)$ called $Q_a(\boldsymbol s)$, hence the
name</li>
<li>Firms don’t always take the optimal action
<ul>
<li>At the beginning of the algorithm: <strong>exploration</strong>
<ul>
<li>Firms take actions at random</li>
<li>Just to explore what happens taking different actions</li>
</ul>
</li>
<li>Gradually shift towards <strong>exploitation</strong>
<ul>
<li>I.e. take the optimal action, given
$\bar V^{(t)}( \boldsymbol s)$ at iteration $t$</li>
<li>I.e. shift towards Pakes and McGuire
(<a href="#ref-pakes2001stochastic">2001</a>)</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="applications-1">Applications</h3>
<ul>
<li>Doraszelski, Lewis, and Pakes (<a href="#ref-doraszelski2018just">2018</a>)
<ul>
<li>Firm do actually learn by trial and error</li>
<li>Setting: demand learning in the UK frequency response market
(electricity)</li>
</ul>
</li>
<li>Asker et al. (<a href="#ref-asker2020computational">2020</a>)
<ul>
<li>Uses Pakes and McGuire (<a href="#ref-pakes2001stochastic">2001</a>) for
estimation</li>
<li>Setting: dynamic timber auctions with information sharing</li>
</ul>
</li>
<li>Calvano et al. (<a href="#ref-calvano2020artificial">2020</a>)
<ul>
<li>Study Q-learning pricing algorithms</li>
<li>In repeated price competition with differentiated products</li>
<li>(Computational) lab experiment: what do these algorithms
converge to?</li>
<li><strong>Finding</strong>: algorithms learn reward-punishment collusive
strategies</li>
</ul>
</li>
</ul>
<h2 id="appendix">Appendix</h2>
<h3 id="references-references">References [references]</h3>
<div id="refs" class="references csl-bib-body hanging-indent"
markdown="1">
<div id="ref-abbring2010last" class="csl-entry" markdown="1">
<p>Abbring, Jaap H, and Jeffrey R Campbell. 2010. “Last-in First-Out
Oligopoly Dynamics.” <em>Econometrica</em> 78 (5): 1491–1527.</p>
</div>
<div id="ref-aguirregabiria2021dynamic" class="csl-entry" markdown="1">
<p>Aguirregabiria, Victor, Allan Collard-Wexler, and Stephen P Ryan. 2021.
“Dynamic Games in Empirical Industrial Organization.” National Bureau of
Economic Research.</p>
</div>
<div id="ref-aguirregabiria2007sequential" class="csl-entry"
markdown="1">
<p>Aguirregabiria, Victor, and Pedro Mira. 2007. “Sequential Estimation of
Dynamic Discrete Games.” <em>Econometrica</em> 75 (1): 1–53.</p>
</div>
<div id="ref-arcidiacono2016estimation" class="csl-entry" markdown="1">
<p>Arcidiacono, Peter, Patrick Bayer, Jason R Blevins, and Paul B
Ellickson. 2016. “Estimation of Dynamic Discrete Choice Models in
Continuous Time with an Application to Retail Competition.” <em>The Review
of Economic Studies</em> 83 (3): 889–931.</p>
</div>
<div id="ref-arcidiacono2011conditional" class="csl-entry" markdown="1">
<p>Arcidiacono, Peter, and Robert A Miller. 2011. “Conditional Choice
Probability Estimation of Dynamic Discrete Choice Models with Unobserved
Heterogeneity.” <em>Econometrica</em> 79 (6): 1823–67.</p>
</div>
<div id="ref-asker2020computational" class="csl-entry" markdown="1">
<p>Asker, John, Chaim Fershtman, Jihye Jeon, and Ariel Pakes. 2020. “A
Computational Framework for Analyzing Dynamic Auctions: The Market
Impact of Information Sharing.” <em>The RAND Journal of Economics</em> 51 (3):
805–39.</p>
</div>
<div id="ref-bajari2007estimating" class="csl-entry" markdown="1">
<p>Bajari, Patrick, C Lanier Benkard, and Jonathan Levin. 2007. “Estimating
Dynamic Models of Imperfect Competition.” <em>Econometrica</em> 75 (5):
1331–70.</p>
</div>
<div id="ref-barwick2015costs" class="csl-entry" markdown="1">
<p>Barwick, Panle Jia, and Parag A Pathak. 2015. “The Costs of Free Entry:
An Empirical Study of Real Estate Agents in Greater Boston.” <em>The RAND
Journal of Economics</em> 46 (1): 103–45.</p>
</div>
<div id="ref-berry2021empirical" class="csl-entry" markdown="1">
<p>Berry, Steven T, and Giovanni Compiani. 2021. “Empirical Models of
Industry Dynamics with Endogenous Market Structure.” <em>Annual Review of
Economics</em> 13.</p>
</div>
<div id="ref-besanko2010learning" class="csl-entry" markdown="1">
<p>Besanko, David, Ulrich Doraszelski, Yaroslav Kryukov, and Mark
Satterthwaite. 2010. “Learning-by-Doing, Organizational Forgetting, and
Industry Dynamics.” <em>Econometrica</em> 78 (2): 453–508.</p>
</div>
<div id="ref-borkovsky2010user" class="csl-entry" markdown="1">
<p>Borkovsky, Ron N, Ulrich Doraszelski, and Yaroslav Kryukov. 2010. “A
User’s Guide to Solving Dynamic Stochastic Games Using the Homotopy
Method.” <em>Operations Research</em> 58 (4-part-2): 1116–32.</p>
</div>
<div id="ref-calvano2020artificial" class="csl-entry" markdown="1">
<p>Calvano, Emilio, Giacomo Calzolari, Vincenzo Denicolo, and Sergio
Pastorello. 2020. “Artificial Intelligence, Algorithmic Pricing, and
Collusion.” <em>American Economic Review</em> 110 (10): 3267–97.</p>
</div>
<div id="ref-caoui2019estimating" class="csl-entry" markdown="1">
<p>Caoui, El Hadi. 2019. “Estimating the Costs of Standardization: Evidence
from the Movie Industry.” <em>R&amp;R, Review of Economic Studies</em>.</p>
</div>
<div id="ref-collard2013demand" class="csl-entry" markdown="1">
<p>Collard-Wexler, Allan. 2013. “Demand Fluctuations in the Ready-Mix
Concrete Industry.” <em>Econometrica</em> 81 (3): 1003–37.</p>
</div>
<div id="ref-courthoud2020approximation" class="csl-entry" markdown="1">
<p>Courthoud, Matteo. 2020. “Approximation Methods for Large Dynamic
Stochastic Games.” <em>Working Paper</em>.</p>
</div>
<div id="ref-doraszelski2003r" class="csl-entry" markdown="1">
<p>Doraszelski, Ulrich. 2003. “An r&amp;d Race with Knowledge Accumulation.”
<em>Rand Journal of Economics</em>, 20–42.</p>
</div>
<div id="ref-doraszelski2012avoiding" class="csl-entry" markdown="1">
<p>Doraszelski, Ulrich, and Kenneth L Judd. 2012. “Avoiding the Curse of
Dimensionality in Dynamic Stochastic Games.” <em>Quantitative Economics</em> 3
(1): 53–93.</p>
</div>
<div id="ref-doraszelski2019dynamic" class="csl-entry" markdown="1">
<p>———. 2019. “Dynamic Stochastic Games with Random Moves.” <em>Quantitative
Marketing and Economics</em> 17 (1): 59–79.</p>
</div>
<div id="ref-doraszelski2018just" class="csl-entry" markdown="1">
<p>Doraszelski, Ulrich, Gregory Lewis, and Ariel Pakes. 2018. “Just
Starting Out: Learning and Equilibrium in a New Market.” <em>American
Economic Review</em> 108 (3): 565–615.</p>
</div>
<div id="ref-doraszelski2010computable" class="csl-entry" markdown="1">
<p>Doraszelski, Ulrich, and Mark Satterthwaite. 2010. “Computable
Markov-Perfect Industry Dynamics.” <em>The RAND Journal of Economics</em> 41
(2): 215–43.</p>
</div>
<div id="ref-egesdal2015estimating" class="csl-entry" markdown="1">
<p>Egesdal, Michael, Zhenyu Lai, and Che-Lin Su. 2015. “Estimating Dynamic
Discrete-Choice Games of Incomplete Information.” <em>Quantitative
Economics</em> 6 (3): 567–97.</p>
</div>
<div id="ref-eibelshauser2019markov" class="csl-entry" markdown="1">
<p>Eibelshäuser, Steffen, and David Poensgen. 2019. “Markov Quantal
Response Equilibrium and a Homotopy Method for Computing and Selecting
Markov Perfect Equilibria of Dynamic Stochastic Games.” <em>Working Paper</em>.</p>
</div>
<div id="ref-ericson1995markov" class="csl-entry" markdown="1">
<p>Ericson, Richard, and Ariel Pakes. 1995. “Markov-Perfect Industry
Dynamics: A Framework for Empirical Work.” <em>The Review of Economic
Studies</em> 62 (1): 53–82.</p>
</div>
<div id="ref-esteban2007durable" class="csl-entry" markdown="1">
<p>Esteban, Susanna, and Matthew Shum. 2007. “Durable-Goods Oligopoly with
Secondary Markets: The Case of Automobiles.” <em>The RAND Journal of
Economics</em> 38 (2): 332–54.</p>
</div>
<div id="ref-farias2012approximate" class="csl-entry" markdown="1">
<p>Farias, Vivek, Denis Saure, and Gabriel Y Weintraub. 2012. “An
Approximate Dynamic Programming Approach to Solving Dynamic Oligopoly
Models.” <em>The RAND Journal of Economics</em> 43 (2): 253–82.</p>
</div>
<div id="ref-fershtman2012dynamic" class="csl-entry" markdown="1">
<p>Fershtman, Chaim, and Ariel Pakes. 2012. “Dynamic Games with Asymmetric
Information: A Framework for Empirical Work.” <em>The Quarterly Journal of
Economics</em> 127 (4): 1611–61.</p>
</div>
<div id="ref-goettler2011does" class="csl-entry" markdown="1">
<p>Goettler, Ronald L, and Brett R Gordon. 2011. “Does AMD Spur Intel to
Innovate More?” <em>Journal of Political Economy</em> 119 (6): 1141–1200.</p>
</div>
<div id="ref-hotz1993conditional" class="csl-entry" markdown="1">
<p>Hotz, V Joseph, and Robert A Miller. 1993. “Conditional Choice
Probabilities and the Estimation of Dynamic Models.” <em>The Review of
Economic Studies</em> 60 (3): 497–529.</p>
</div>
<div id="ref-huang2014dynamic" class="csl-entry" markdown="1">
<p>Huang, Ling, and Martin D Smith. 2014. “The Dynamic Efficiency Costs of
Common-Pool Resource Exploitation.” <em>American Economic Review</em> 104 (12):
4071–4103.</p>
</div>
<div id="ref-ifrach2017framework" class="csl-entry" markdown="1">
<p>Ifrach, Bar, and Gabriel Y Weintraub. 2017. “A Framework for Dynamic
Oligopoly in Concentrated Industries.” <em>The Review of Economic Studies</em>
84 (3): 1106–50.</p>
</div>
<div id="ref-igami2017estimating" class="csl-entry" markdown="1">
<p>Igami, Mitsuru. 2017. “Estimating the Innovator’s Dilemma: Structural
Analysis of Creative Destruction in the Hard Disk Drive Industry,
1981–1998.” <em>Journal of Political Economy</em> 125 (3): 798–847.</p>
</div>
<div id="ref-iskhakov2016recursive" class="csl-entry" markdown="1">
<p>Iskhakov, Fedor, John Rust, and Bertel Schjerning. 2016. “Recursive
Lexicographical Search: Finding All Markov Perfect Equilibria of Finite
State Directional Dynamic Games.” <em>The Review of Economic Studies</em> 83
(2): 658–703.</p>
</div>
<div id="ref-jeon2020learning" class="csl-entry" markdown="1">
<p>Jeon, Jihye. 2020. “Learning and Investment Under Demand Uncertainty in
Container Shipping.” <em>The RAND Journal of Economics</em>.</p>
</div>
<div id="ref-kasahara2009nonparametric" class="csl-entry" markdown="1">
<p>Kasahara, Hiroyuki, and Katsumi Shimotsu. 2009. “Nonparametric
Identification of Finite Mixture Models of Dynamic Discrete Choices.”
<em>Econometrica</em> 77 (1): 135–75.</p>
</div>
<div id="ref-maskin1988theory" class="csl-entry" markdown="1">
<p>Maskin, Eric, and Jean Tirole. 1988. “A Theory of Dynamic Oligopoly, II:
Price Competition, Kinked Demand Curves, and Edgeworth Cycles.”
<em>Econometrica: Journal of the Econometric Society</em>, 571–99.</p>
</div>
<div id="ref-pakes1994computing" class="csl-entry" markdown="1">
<p>Pakes, Ariel, and Paul McGuire. 1994. “Computing Markov-Perfect Nash
Equilibria: Numerical Implications of a Dynamic Differentiated Product
Model.” <em>RAND Journal of Economics</em> 25 (4): 555–89.</p>
</div>
<div id="ref-pakes2001stochastic" class="csl-entry" markdown="1">
<p>———. 2001. “Stochastic Algorithms, Symmetric Markov Perfect Equilibrium,
and the ‘Curse’of Dimensionality.” <em>Econometrica</em> 69 (5): 1261–81.</p>
</div>
<div id="ref-pakes2007simple" class="csl-entry" markdown="1">
<p>Pakes, Ariel, Michael Ostrovsky, and Steven Berry. 2007. “Simple
Estimators for the Parameters of Discrete Dynamic Games (with Entry/Exit
Examples).” <em>The RAND Journal of Economics</em> 38 (2): 373–99.</p>
</div>
<div id="ref-pesendorfer2008asymptotic" class="csl-entry" markdown="1">
<p>Pesendorfer, Martin, and Philipp Schmidt-Dengler. 2008. “Asymptotic
Least Squares Estimators for Dynamic Games.” <em>The Review of Economic
Studies</em> 75 (3): 901–28.</p>
</div>
<div id="ref-pesendorfer2010sequential" class="csl-entry" markdown="1">
<p>———. 2010. “Sequential Estimation of Dynamic Discrete Games: A Comment.”
<em>Econometrica</em> 78 (2): 833–42.</p>
</div>
<div id="ref-rust1994structural" class="csl-entry" markdown="1">
<p>Rust, John. 1994. “Structural Estimation of Markov Decision Processes.”
<em>Handbook of Econometrics</em> 4: 3081–3143.</p>
</div>
<div id="ref-ryan2012costs" class="csl-entry" markdown="1">
<p>Ryan, Stephen P. 2012. “The Costs of Environmental Regulation in a
Concentrated Industry.” <em>Econometrica</em> 80 (3): 1019–61.</p>
</div>
<div id="ref-su2012constrained" class="csl-entry" markdown="1">
<p>Su, Che-Lin, and Kenneth L Judd. 2012. “Constrained Optimization
Approaches to Estimation of Structural Models.” <em>Econometrica</em> 80 (5):
2213–30.</p>
</div>
<div id="ref-sweeting2013dynamic" class="csl-entry" markdown="1">
<p>Sweeting, Andrew. 2013. “Dynamic Product Positioning in Differentiated
Product Markets: The Effect of Fees for Musical Performance Rights on
the Commercial Radio Industry.” <em>Econometrica</em> 81 (5): 1763–803.</p>
</div>
<div id="ref-vreugdenhil2020booms" class="csl-entry" markdown="1">
<p>Vreugdenhil, Nicholas. 2020. “Booms, Busts, and Mismatch in Capital
Markets: Evidence from the Offshore Oil and Gas Industry.” <em>R&amp;R at
Journal of Political Economy</em>.</p>
</div>
<div id="ref-weintraub2008markov" class="csl-entry" markdown="1">
<p>Weintraub, Gabriel Y, C Lanier Benkard, and Benjamin Van Roy. 2008.
“Markov Perfect Industry Dynamics with Many Firms.” <em>Econometrica</em> 76
(6): 1375–1411.</p>
</div>
<div id="ref-xu2020structural" class="csl-entry" markdown="1">
<p>Xu, Daniel Yi, and Yanyou Chen. 2020. “A Structural Empirical Model of
r&amp;d, Firm Heterogeneity, and Industry Evolution.” <em>Journal of Industrial
Economics.</em></p>
</div>
</div>

          </div>

          



          
          
          <div class="article-widget">
            
<div class="post-nav">
  
  
  
  <div class="post-nav-item">
    <div class="meta-nav">Previous</div>
    <a href="/course/empirical-io/07_dynamics_singleagent/" rel="next">Single Agent Dynamics</a>
  </div>
  
  
  
  <div class="post-nav-item">
    <div class="meta-nav">Next</div>
    <a href="/course/empirical-io/11_logit_demand/" rel="prev">Coding: Logit Demand</a>
  </div>
  
</div>

          </div>
          
        </div>

        <div class="body-footer">

          





          




          


  
  



        </div>

      </article>

      <footer class="site-footer">

  

  
  <p class="powered-by">
    
      <a href="/privacy/">Privacy Policy</a>
    
    
       &middot; 
      <a href="/terms/">Terms</a>
    
  </p>
  

  

  
  







</footer>


    </main>
  </div>
</div>

  </div>

  <div class="page-footer">
    
    
  </div>

  
<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        <pre><code class="tex hljs"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

      

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/instant.page/5.1.0/instantpage.min.js" integrity="sha512-1+qUtKoh9XZW7j+6LhRMAyOrgSQKenQ4mluTR+cvxXjP1Z54RxZuzstR/H9kgPXQsVB8IW7DMDFUJpzLjvhGSQ==" crossorigin="anonymous"></script>

      
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js" integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin="anonymous"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js" integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin="anonymous"></script>
      

      
      

      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mermaid/8.8.4/mermaid.min.js" integrity="sha512-as1BF4+iHZ3BVO6LLDQ7zrbvTXM+c/1iZ1qII/c3c4L8Rn5tHLpFUtpaEtBNS92f+xGsCzsD7b62XP3XYap6oA==" crossorigin="anonymous" title="mermaid"></script>
      

      
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/highlight.min.js" integrity="sha512-TDKKr+IvoqZnPzc3l35hdjpHD0m+b2EC2SrLEgKDRWpxf2rFCxemkgvJ5kfU48ip+Y+m2XVKyOCD85ybtlZDmw==" crossorigin="anonymous"></script>
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/languages/r.min.js"></script>
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/languages/python.min.js"></script>
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/languages/julia.min.js"></script>
        
      

    

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.7.1/leaflet.min.js" integrity="sha512-SeiQaaDh73yrb56sTW/RgVdi/mMqNeM2oBwubFHagc5BkixSpP1fvqF47mKzPGWYSSy4RwbBunrJBQ4Co8fRWA==" crossorigin="anonymous"></script>
    

    
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/anchor-js/4.2.2/anchor.min.js" integrity="sha512-I7w3ZdSFzw5j3jU3ZkNikBNeIrl3i+hEuEdwNmqUJvwNcaBUNcijnP2gd9DtGlgVYDplfjGoD8vTNsID+lCjqg==" crossorigin="anonymous"></script>
    <script>
      anchors.add();
    </script>
    

    
    
    
      
      <script id="search-hit-fuse-template" type="text/x-template">
        <div class="search-hit" id="summary-{{key}}">
          <div class="search-hit-content">
            <div class="search-hit-name">
              <a href="{{relpermalink}}">{{title}}</a>
              <div class="article-metadata search-hit-type">{{type}}</div>
              <p class="search-hit-description">{{snippet}}</p>
            </div>
          </div>
        </div>
      </script>
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js" integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin="anonymous"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js" integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin="anonymous"></script>
      
    

    
    

    
    
    
    

    
    <script src="/js/bootstrap.bundle.min.6aed84840afc03ab4d5750157f69c120.js"></script>

    
    
      
      
      
      
      
      
      
    

    
    
    
    
    
    
    
    
      
      
    
    
    <script src="/en/js/wowchemy.min.4ea9cc8d09c5c158656ac1a804743b34.js"></script>

    
  <script async defer src="https://buttons.github.io/buttons.js"></script>




</body>
</html>
