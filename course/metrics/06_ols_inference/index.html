<!DOCTYPE html><html lang="en-us" >

<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  
  
  
    <meta name="generator" content="Wowchemy 5.1.0 for Hugo" />
  

  

  
  
  
  
  
    
    
    
  
  

  

  
  
  
    
  
  <meta name="description" content="Asymptotic Theory of the OLS Estimator OLS Consistency Theorem: Assume that $(x_i, y_i) _ {i=1}^n$ i.i.d. , $\mathbb E[x_i x_i&#39;] = Q$ positive definite, $\mathbb E[x_i x_i&#39;] &lt; \infty$ and $\mathbb E [y_i^2] &lt; \infty$, then $\hat \beta _ {OLS}$ is a consistent estimator of $\beta_0$, i." />

  
  <link rel="alternate" hreflang="en-us" href="https://matteocourthoud.github.io/course/metrics/06_ols_inference/" />

  







  




  
  
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
  

  
  
  
    <meta name="theme-color" content="#003f5c" />
  

  
  
    
    <script src="/js/mathjax-config.js"></script>
  

  
  
  
  
    
    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha256-FMvZuGapsJLjouA6k7Eo2lusoAX9i0ShlWFG6qt7SLc=" crossorigin="anonymous">

    
    

    
    
    
      
    
    
      
      
        
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/styles/github.min.css" crossorigin="anonymous" title="hl-light" media="print" onload="this.media='all'">
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/styles/dracula.min.css" crossorigin="anonymous" title="hl-dark" media="print" onload="this.media='all'" disabled>
        
      
    

    
    
    
      <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.7.1/leaflet.min.css" integrity="sha512-1xoFisiGdy9nvho8EgXuXvnpR5GAMSjFwp40gSRE3NwdUdIMIKuPa7bqoUhLD0O/5tPNhteAsE5XyyMi5reQVA==" crossorigin="anonymous" media="print" onload="this.media='all'">
    

    

    
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js" integrity="" crossorigin="anonymous" async></script>
      
    
      
      

      
      

      
    
      
      

      
      

      
    
  

  
  
  
    
      
      <link rel="preload" as="style" href="https://fonts.googleapis.com/css2?family=Roboto+Mono&family=Source+Sans+Pro:wght@200;300;400;700&display=swap">
      <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Roboto+Mono&family=Source+Sans+Pro:wght@200;300;400;700&display=swap" media="print" onload="this.media='all'">
    
  

  
  
  
  
  
  <link rel="stylesheet" href="/css/wowchemy.5c4def4f00a521426f4eb098155f3342.css" />

  




<script async src="https://www.googletagmanager.com/gtag/js?id=UA-144780600-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];

  function gtag() {
      dataLayer.push(arguments);
  }

  function trackOutboundLink(url, target) {
    gtag('event', 'click', {
         'event_category': 'outbound',
         'event_label': url,
         'transport_type': 'beacon',
         'event_callback': function () {
           if (target !== '_blank') {
             document.location = url;
           }
         }
    });
    console.debug("Outbound link clicked: " + url);
  }

  function onClickCallback(event) {
    if ((event.target.tagName !== 'A') || (event.target.host === window.location.host)) {
      return;
    }
    trackOutboundLink(event.target, event.target.getAttribute('target'));  
  }

  gtag('js', new Date());
  gtag('config', 'UA-144780600-1', { 'anonymize_ip': true });
  gtag('set', {'cookie_flags': 'SameSite=None;Secure'});

  
  document.addEventListener('click', onClickCallback, false);
</script>


  

  

  




  
  
  

  

  
    <link rel="manifest" href="/index.webmanifest" />
  

  <link rel="icon" type="image/png" href="/media/icon_hu03e9b3967b83fd39296ec9da5ff1ea05_201175_32x32_fill_lanczos_center_3.png" />
  <link rel="apple-touch-icon" type="image/png" href="/media/icon_hu03e9b3967b83fd39296ec9da5ff1ea05_201175_180x180_fill_lanczos_center_3.png" />

  <link rel="canonical" href="https://matteocourthoud.github.io/course/metrics/06_ols_inference/" />

  
  
  
  
  
  
  
  
    
    
  
  
  <meta property="twitter:card" content="summary" />
  
  <meta property="og:site_name" content="Matteo Courthoud" />
  <meta property="og:url" content="https://matteocourthoud.github.io/course/metrics/06_ols_inference/" />
  <meta property="og:title" content="OLS Inference | Matteo Courthoud" />
  <meta property="og:description" content="Asymptotic Theory of the OLS Estimator OLS Consistency Theorem: Assume that $(x_i, y_i) _ {i=1}^n$ i.i.d. , $\mathbb E[x_i x_i&#39;] = Q$ positive definite, $\mathbb E[x_i x_i&#39;] &lt; \infty$ and $\mathbb E [y_i^2] &lt; \infty$, then $\hat \beta _ {OLS}$ is a consistent estimator of $\beta_0$, i." /><meta property="og:image" content="https://matteocourthoud.github.io/media/icon_hu03e9b3967b83fd39296ec9da5ff1ea05_201175_512x512_fill_lanczos_center_3.png" />
    <meta property="twitter:image" content="https://matteocourthoud.github.io/media/icon_hu03e9b3967b83fd39296ec9da5ff1ea05_201175_512x512_fill_lanczos_center_3.png" /><meta property="og:locale" content="en-us" />
  
    
      <meta
        property="article:published_time"
        content="2021-10-29T00:00:00&#43;00:00"
      />
    
    <meta property="article:modified_time" content="2021-10-29T00:00:00&#43;00:00">
  

  



  

  
  
  
  
  
    <script src="https://cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.1.1/cookieconsent.min.js" integrity="sha256-5VhCqFam2Cn+yjw61zbBNrbHVJ6SRydPeKopYlngbiQ=" crossorigin="anonymous"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.1.1/cookieconsent.min.css" integrity="sha256-zQ0LblD/Af8vOppw18+2anxsuaz3pWYyVWi+bTvTH8Q=" crossorigin="anonymous">
  
  <script>
  window.addEventListener("load", function(){
    window.cookieconsent.initialise({
      "palette": {
        "popup": {
          "background": "#003f5c",
          "text": "rgb(255, 255, 255)"
        },
        "button": {
          "background": "rgb(255, 255, 255)",
          "text": "#003f5c"
        }
      },
      "theme": "classic",
      "content": {
        "message": "This website uses cookies to ensure you get the best experience on our website.",
        "dismiss": "Got it!",
        "link": "Learn more",
        "href": "/privacy/"
      }
    })});
  </script>


  





  <title>OLS Inference | Matteo Courthoud</title>
</head>


<body id="top" data-spy="scroll" data-offset="70" data-target="#TableOfContents" class="page-wrapper   " data-wc-page-id="3a3dba8681825325823978a98a7af4ac" >

  
  
  
  
  
  
  
  
  <script src="/js/wowchemy-init.min.6edaf3b475ce43de30d98828aea698be.js"></script>

  


<aside class="search-modal" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Search</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#" aria-label="Close"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        
        <input name="q" id="search-query" placeholder="Search..." autocapitalize="off"
        autocomplete="off" autocorrect="off" spellcheck="false" type="search" class="form-control"
        aria-label="Search...">
        
      </div>

      
      

      

    </section>
    <section class="section-search-results">

      <div id="search-hits">
        
      </div>

    </section>
  </div>
</aside>



  <div class="page-header">
    












<nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id="navbar-main">
  <div class="container-xl">

    
    <div class="d-none d-lg-inline-flex">
      <a class="navbar-brand" href="/">Matteo Courthoud</a>
    </div>
    

    
    <button type="button" class="navbar-toggler" data-toggle="collapse"
            data-target="#navbar-content" aria-controls="navbar-content" aria-expanded="false" aria-label="Toggle navigation">
    <span><i class="fas fa-bars"></i></span>
    </button>
    

    
    <div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none">
      <a class="navbar-brand" href="/">Matteo Courthoud</a>
    </div>
    

    
    
    <div class="navbar-collapse main-menu-item collapse justify-content-end" id="navbar-content">

      
      <ul class="navbar-nav d-md-inline-flex">
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
            
            
            
              
            
            
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#about"><span>Home</span></a>
        </li>

        
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
            
            
            
              
            
            
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#projects"><span>Research</span></a>
        </li>

        
        

        
        <li class="nav-item dropdown">
          <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown" aria-haspopup="true"><span>Courses</span><span class="caret"></span>
          </a>
          <div class="dropdown-menu">
            
              <a class="dropdown-item" href="/course/ml-econ/"><span>Machine Learning for Economics</span></a>
            
              <a class="dropdown-item" href="/course/data-science/"><span>Data Science with Python</span></a>
            
              <a class="dropdown-item" href="/course/empirical-io/"><span>PhD Industrial Organization</span></a>
            
              <a class="dropdown-item" href="/course/metrics/"><span>PhD Econometrics</span></a>
            
              <a class="dropdown-item" href="https://pp4rs.github.io/"><span>Programming Practices for Research</span></a>
            
          </div>
        </li>

        
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/post/"><span>Posts</span></a>
        </li>

        
        

        

        
        
        

        

        
        
        
        

        
          
            
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/cv"><span>CV</span></a>
        </li>

        
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
            
            
            
              
            
            
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#contact"><span>Contact</span></a>
        </li>

        
        

      

        
      </ul>
    </div>

    <ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2">

      
      
        
      

      
      

      
      

      
      

    </ul>

  </div>
</nav>


  </div>

  <div class="page-body">
    

<div class="container-fluid docs">
  <div class="row flex-xl-nowrap">
    <div class="col-12 col-md-3 col-xl-2 docs-sidebar">
      
<nav class="collapse docs-links" id="docs-nav">
  
  
  
  
  
  

  
  
    

    
      

      <ul class="nav docs-sidenav">
        <li><a href="/course/"><i class="fas fa-arrow-left pr-1"></i>Courses</a></li>
      </ul>

      
      
        
          
        
      


  
    
    
    
    
      
    
    

    
      <div class="docs-toc-item">
        <a class="docs-toc-link " href="/course/metrics/">Econometrics</a>
    
      
        <ul class="nav docs-sidenav">
      


  <li class=""><a href="/course/metrics/01_matrices/">Matrix Algebra</a></li>



  <li class=""><a href="/course/metrics/02_probability/">Probability Theory</a></li>



  <li class=""><a href="/course/metrics/03_asymptotics/">Asymptotic Theory</a></li>



  <li class=""><a href="/course/metrics/04_inference/">Inference</a></li>



  <li class=""><a href="/course/metrics/05_ols_algebra/">OLS Algebra</a></li>



  <li class=""><a href="/course/metrics/06_endogeneity/">Endogeneity</a></li>



  <li class="active"><a href="/course/metrics/06_ols_inference/">OLS Inference</a></li>



  <li class=""><a href="/course/metrics/07_endogeneity/">Endogeneity</a></li>



  <li class=""><a href="/course/metrics/08_nonparametric/">Non-Parametric Estimation</a></li>



  <li class=""><a href="/course/metrics/09_selection/">Variable Selection</a></li>

      
        </ul>
      
    

    
      </div>
    

    
  
</nav>

    </div>

    
    
    <div class="d-none d-xl-block col-xl-2 docs-toc">
      

      <ul class="nav toc-top">
        <li><a href="#" id="back_to_top" class="docs-toc-title">Contents</a></li>
      </ul>

      <nav id="TableOfContents">
  <ul>
    <li><a href="#asymptotic-theory-of-the-ols-estimator">Asymptotic Theory of the OLS Estimator</a>
      <ul>
        <li><a href="#ols-consistency">OLS Consistency</a></li>
        <li><a href="#variance-and-assumptions">Variance and Assumptions</a></li>
        <li><a href="#gaussian-error-term">Gaussian Error Term</a></li>
        <li><a href="#homoskedastic-error-term">Homoskedastic Error Term</a></li>
        <li><a href="#heteroskedastic-error-term">Heteroskedastic Error Term</a></li>
        <li><a href="#hc0-consistency">HC0 Consistency</a></li>
        <li><a href="#heteroskedastic-and-autocorrelated-error-term">Heteroskedastic and Autocorrelated Error Term</a></li>
        <li><a href="#hac-consistency">HAC Consistency</a></li>
        <li><a href="#comments">Comments</a></li>
        <li><a href="#choice-of-h">Choice of h</a></li>
        <li><a href="#fixed-b-asymptotics">Fixed b Asymptotics</a></li>
        <li><a href="#fixed-b-theorem">Fixed b Theorem</a></li>
        <li><a href="#example">Example</a></li>
        <li><a href="#fixed-g-asymptotics">Fixed G Asymptotics</a></li>
        <li><a href="#assumption">Assumption</a></li>
        <li><a href="#comments-1">Comments (1)</a></li>
        <li><a href="#comments-2">Comments (2)</a></li>
        <li><a href="#code---dgp">Code - DGP</a></li>
        <li><a href="#ideal-estimate">Ideal Estimate</a></li>
        <li><a href="#hc-estimates">HC Estimates</a></li>
      </ul>
    </li>
    <li><a href="#inference">Inference</a>
      <ul>
        <li><a href="#hypothesis-testing">Hypothesis Testing</a></li>
        <li><a href="#testing-problem">Testing Problem</a></li>
        <li><a href="#example-1">Example</a></li>
        <li><a href="#comments-3">Comments</a></li>
        <li><a href="#comments-2-1">Comments (2)</a></li>
        <li><a href="#t-stat">t Stat</a></li>
        <li><a href="#f-stat">F Stat</a></li>
        <li><a href="#equivalence">Equivalence</a></li>
        <li><a href="#confidence-intervals">Confidence Intervals</a></li>
        <li><a href="#code">Code</a></li>
      </ul>
    </li>
  </ul>
</nav>

      
    </div>
    

    <main class="col-12 col-md-9 col-xl-8 py-md-3 pl-md-5 docs-content" role="main">

      <article class="article">

        <div class="docs-article-container">

          <h1>OLS Inference</h1>

          <p>Last updated on Oct 29, 2021</p>

          <div class="article-style">
            <h2 id="asymptotic-theory-of-the-ols-estimator">Asymptotic Theory of the OLS Estimator</h2>
<h3 id="ols-consistency">OLS Consistency</h3>
<p><strong>Theorem</strong>: Assume that $(x_i, y_i) _ {i=1}^n$ i.i.d. ,
$\mathbb E[x_i x_i'] = Q$ positive definite,
$\mathbb E[x_i x_i'] &lt; \infty$ and $\mathbb E [y_i^2] &lt; \infty$, then
$\hat \beta _ {OLS}$ is a <strong>consistent</strong> estimator of $\beta_0$,
i.e. $\hat \beta = \mathbb E_n [x_i x_i'] \mathbb E_n [x_i y_i]\overset{p}{\to} \beta_0$.</p>
<p><strong>Proof</strong>:<br>
We consider 4 steps:</p>
<ol>
<li>$\mathbb E_n [x_i x_i'] \xrightarrow{p} \mathbb E [x_i x_i']$ by
WLLN since $x_i x_i'$ iid and $\mathbb E[x_i x_i'] &lt; \infty$.</li>
<li>$\mathbb E_n [x_i y_i] \xrightarrow{p} \mathbb E [x_i y_i]$ by WLLN,
due to $x_i y_i$ iid, Cauchy-Schwarz and finite second moments of
$x_i$ and $y_i$ $$
\mathbb E \left[ x_i y_i \right]  \leq \sqrt{ \mathbb E[x_i^2] \mathbb E[y_i^2]} &lt; \infty
$$</li>
<li>$\mathbb E_n [x_i x_i']^{-1} \xrightarrow{p} \mathbb E [x_i x_i']^{-1}$
by CMT.</li>
<li>$\mathbb E_n [x_i x_i']^{-1} \mathbb E_n [x_i y_i] \xrightarrow{p} \mathbb E [x_i x_i']^{-1} \mathbb E [x_i y_i] = \beta$
by CMT. $$\tag*{$\blacksquare$}$$</li>
</ol>
<h3 id="variance-and-assumptions">Variance and Assumptions</h3>
<p>Now we are going to investigate the variance of $\hat \beta _ {OLS}$
progressively relaxing the underlying assumptions.</p>
<ul>
<li>Gaussian error term.</li>
<li>Homoskedastic error term.</li>
<li>Heteroskedastic error term.</li>
<li>Heteroskedastic and autocorrelated error term.</li>
</ul>
<h3 id="gaussian-error-term">Gaussian Error Term</h3>
<p><strong>Theorem</strong>: Under the GM assumption (1)-(5),
$\hat \beta - \beta |X \sim N(0, \sigma^2 (X&rsquo;X)^{-1})$</p>
<p><strong>Proof</strong>:<br>
We follow 2 steps:</p>
<ol>
<li>We can rewrite $\hat \beta$ as $$
\begin{aligned}
\hat \beta &amp; = (X&rsquo;X)^{-1} X&rsquo;y = (X&rsquo;X)^{-1} X'(X\beta + \varepsilon) \newline
&amp;= \beta + (X&rsquo;X)^{-1} X' \varepsilon = \newline
&amp;= \beta + \mathbb E_n [x_i x_i']^{-1} \mathbb E_n [x_i \varepsilon_i]
\end{aligned}
$$</li>
<li>Therefore:
$\hat \beta-\beta = \mathbb E_n [x_i x_i']^{-1} \mathbb E_n [x_i \varepsilon_i]$.
$$
\begin{aligned}
\hat \beta-\beta |X &amp; \sim (X&rsquo;X)^{-1} X' N(0, \sigma^2 I_n) = \newline
&amp;= N(0, \sigma^2 (X&rsquo;X)^{-1} X&rsquo;X (X&rsquo;X)^{-1}) = \newline
&amp;= N(0, \sigma^2 (X&rsquo;X)^{-1})
\end{aligned}
$$ $$\tag*{$\blacksquare$}$$</li>
</ol>
<blockquote>
<p>Does it make sense to assume that $\varepsilon$ is gaussian? Not much.
But does it make sense that $\hat \beta$ is gaussian? Yes, because
it’s an average.</p>
</blockquote>
<h3 id="homoskedastic-error-term">Homoskedastic Error Term</h3>
<p><strong>Theorem</strong>: Under the assumptions of the previous theorem, plus
$\mathbb E[x^4] &lt; \infty$, the OLS estimate has an asymptotic normal
distribution:
$\hat \beta|X \overset{d}{\to} N(\beta, \sigma^2 (X&rsquo;X)^{-1})$.</p>
<p><strong>Proof</strong>: $$
\sqrt{n} (\hat \beta - \beta ) = \underbrace{\mathbb E_n [x_i x_i']^{-1}} _ {\xrightarrow{p} Q^{-1} }   \underbrace{\sqrt{n} \mathbb E_n [x_i \varepsilon_i ]} _ {\xrightarrow{d} N(0, \Omega)} \rightarrow N(0, \Sigma )
$$ where in general
$\Omega = Var (x_i \varepsilon_i) = \mathbb E [(x_i \varepsilon_i)^2]$
and $\Sigma = Q^{-1} \Omega Q^{-1}$. $$\tag*{$\blacksquare$}$$</p>
<blockquote>
<p>Given that $Q = \mathbb E [x_i x_i']$ is unobserved, we estimate it
with $\hat{Q} = \mathbb E_n [x_i x_i']$. Since we have assumed
homoskedastic error term, we have $\Omega = \sigma^2 (X&rsquo;X)^{-1}$.
Since we do not observe $\sigma^2$ we estimate it as
$\hat{\sigma}^2 = \mathbb E_n[\hat{\varepsilon}_i^2]$.</p>
</blockquote>
<p>The terms $x_i \varepsilon_i$ are called <strong>scores</strong> and we can already
see their central importance for inference.</p>
<h3 id="heteroskedastic-error-term">Heteroskedastic Error Term</h3>
<p><strong>Assumption</strong>: $\mathbb E [\varepsilon_i x_i \varepsilon_j' x_j'] = 0$,
for all $j \ne i$ and $\mathbb E [\varepsilon_i^4] \leq \infty$,
$\mathbb E [|| x_i||^4] \leq C &lt; \infty$ a.s.</p>
<p><strong>Theorem</strong>: Under GM assumptions (1)-(4) plus heteroskedastic error
term, the following estimators are consistent,
i.e. $\hat{\Sigma}\xrightarrow{p} \Sigma$.</p>
<blockquote>
<p>Note that we are only looking at $\Omega$ of the
$\Sigma = Q^{-1} \Omega Q^{-1}$ matrix.</p>
</blockquote>
<ul>
<li><strong>HC0</strong>: use the observed residual $\hat{\varepsilon}_i$ $$
\Omega _ {HC0} = \mathbb E_n [x_i x_i' \hat{\varepsilon}_i^2]
$$ When $k$ is too big relative to $n$ – i.e.,
$k/n \rightarrow c &gt;0$ – $\hat{\varepsilon}_i^2$ are too small
($\Omega _ {HC0}$ biased towards zero). $\Omega _ {HC1}$,
$\Omega _ {HC2}$ and $\Omega _ {HC3}$ try to correct this small
sample bias.</li>
<li><strong>HC1</strong>: degree of freedom correction (default <code>robust</code> in Stata) $$
\Omega _ {HC1} = \frac{1}{n - k }\mathbb E_n [x_i x_i' \hat{\varepsilon}_i^2]
$$</li>
<li><strong>HC2</strong>: use standardized residuals $$
\Omega _ {HC2} = \mathbb E_n [x_i x_i' \hat{\varepsilon}_i^2 (1-h _ {ii})^{-1}]
$$ where $h _ {ii} = [X(X&rsquo;X)^{-1} X'] _ {ii}$ is the <strong>leverage</strong>
of the $i^{th}$ observation. A large $h _ {ii}$ means that
observation $i$ is unusual in the sense that the regressor $x_i$ is
far from its sample mean.</li>
<li><strong>HC3</strong>: use prediction error, equivalent to Jack-knife estimator,
i.e., $\mathbb E_n [x_i x_i' \hat{\varepsilon} _ {(-i)}^2]$ $$
\Omega _ {HC3} = \mathbb E_n [x_i x_i' \hat{\varepsilon}_i^2 (1-h _ {ii})^{-2}]
$$ This estimator does not overfit when $k$ is relatively big with
respect to $n$. Idea: you exclude the corresponding observation when
estimating a particular $\varepsilon_i$:
$\hat{\varepsilon}_i = y_i - x_i' \hat \beta _ {-i}$.</li>
</ul>
<h3 id="hc0-consistency">HC0 Consistency</h3>
<p><strong>Theorem</strong></p>
<p>Under regularity conditions HC0 is consistent,
i.e. $\hat{\Sigma} _ {HC0} \overset{p}{\to} \Sigma$. $$
\hat{\Sigma} = \hat{Q}^{-1} \hat{\Omega} \hat{Q}^{-1} \xrightarrow{p} \Sigma \qquad  \text{ with } \hat{\Omega} = \mathbb E_n [x_i x_i'     \hat{\varepsilon}_i^2] \quad \text{ and } \hat{Q} = \mathbb E_n [x_i x_i']^{-1}
$$</p>
<blockquote>
<p>Why is the proof relevant? You cannot directly apply the WLLN to
$\hat \Sigma$.</p>
</blockquote>
<p><strong>Proof</strong></p>
<p>For the case $\mathrm{dim}(x_i) =1$.</p>
<ol>
<li>$\hat{Q}^{-1} \xrightarrow{p} Q^{-1}$ by WLLN since $x_i$ is iid,
$\mathbb E[x_i^4] &lt; \infty$</li>
<li>$\bar{\Omega} = \mathbb E_n [\varepsilon_i^2 x_i x_i'] \xrightarrow{p} \Omega$
by WLLN since $\mathbb E_n [\varepsilon_i^4] &lt; c$ and $x_i$ bounded.</li>
<li>By the triangle inequality, $$
| \hat{\Omega} - \hat{\Omega}| \leq \underbrace{|\Omega - \bar{\Omega}|} _ {\overset{p}{\to} 0} + \underbrace{|\bar{\Omega} - \hat{\Omega}|} _ {\text{WTS:} \overset{p}{\to} 0}
$$</li>
<li>We want to show $|\bar{\Omega} - \hat{\Omega}| \overset{p}{\to} 0$
$$
\begin{aligned}
|\bar{\Omega} - \hat{\Omega}| &amp;= \mathbb E_n [\varepsilon_i^2 x_i^2] - \mathbb E_n [\hat{\varepsilon}_i^2 x_i^2]  = \newline
&amp;= \mathbb E_n [\left( \varepsilon_i^2 - \hat{\varepsilon}_i^2 \right) x_i^2] \leq \newline
&amp; \leq \mathbb E_n \left[ \left( \varepsilon_i^2 - \hat{\varepsilon}_i^2 \right)^2\right]^{\frac{1}{2}} \mathbb E_n [x_i^4]^{\frac{1}{2}}
\end{aligned}
$$ where
$\mathbb E_n [x_i^4]^{\frac{1}{2}} \xrightarrow{p} \mathbb E [x_i^4]^{\frac{1}{2}}$
by $x_i$ bounded, iid and CMT.</li>
<li>We want to show that
$\mathbb E_n \left[ \left( \varepsilon_i^2 - \hat{\varepsilon}_i^2 \right)^2\right] \leq \eta$
with $\eta \rightarrow 0$. Let
$L = \max_i |\hat{\varepsilon}_i - \varepsilon_i|$ (RV depending on
$n$), with $L \xrightarrow{p} 0$ since $$
|\hat{\varepsilon}_i - \varepsilon_i| = |x_i \hat \beta - x_i \beta| \leq |x_i||\hat \beta - \beta|\xrightarrow{p} c \cdot 0
$$ We can depompose $$
\begin{aligned}
\left(\varepsilon_i^2 - \hat{\varepsilon}_i^2 \right)^2 &amp; = \left(\varepsilon_i - \hat{\varepsilon}_i \right)^2 \left(\varepsilon_i + \hat{\varepsilon}_i \right)^2 \leq \newline<br>
&amp; \leq \left(\varepsilon_i + \hat{\varepsilon}_i \right)^2 L^2 = \newline
&amp;= \left(2\varepsilon_i - \varepsilon_i + \hat{\varepsilon}_i \right)^2 L^2\leq  \newline
&amp; \leq  \left( 2(2\varepsilon_i)^2 + 2(\hat{\varepsilon}_i - \varepsilon_i)^2 \right)^2 L^2 \leq \newline
&amp; \leq (8 \varepsilon_i^2 + 2 L^2) L^2
\end{aligned}
$$ Hence $$
\mathbb E \left[ \left(\varepsilon_i^2 - \hat{\varepsilon}_i^2 \right)^2 \right] \leq  L^2 \left( 8 \mathbb E_n [ \varepsilon_i^2] + 2 \mathbb E_n [L^2] \right)  \xrightarrow{p}0
$$ $$\tag*{$\blacksquare$}$$</li>
</ol>
<h3 id="heteroskedastic-and-autocorrelated-error-term">Heteroskedastic and Autocorrelated Error Term</h3>
<p><strong>Assumption</strong></p>
<p>There esists a $\bar{d}$ such that:</p>
<ul>
<li>$\mathbb E[\varepsilon_i x_i \varepsilon' _ {i-d} x' _ {i-d}] \neq 0 \quad$
for $d \leq \bar{d}$</li>
<li>$\mathbb E[\varepsilon_i x_i \varepsilon' _ {i-d} x' _ {i-d}] = 0 \quad$
for $d &gt; \bar{d}$</li>
</ul>
<blockquote>
<p>Intuition: observations far enough from each other are not correlated.</p>
</blockquote>
<p>We can express the variance of the score as $$
\begin{aligned}
\Omega_n &amp;= Var(\sqrt{n} \mathbb E_n[x_i \varepsilon_i]) = \newline
&amp;= \mathbb E \left[ \left( \frac{1}{n} \sum _ {i=1}^n x_i \varepsilon_i \right) \left( \frac{1}{n} \sum _ {j=1}^n x_j \varepsilon_j \right) \right] = \newline
&amp;= \frac{1}{n} \sum _ {i=1}^n \sum _ {j=1}^n \mathbb E[x_i \varepsilon_i x_j' \varepsilon_j'] = \newline
&amp;= \frac{1}{n} \sum _ {i=1}^n \sum _ {j : |i-j|\leq \bar{d}} \mathbb E[x_i \varepsilon_i x_j' \varepsilon_j'] = \newline
&amp;= \frac{1}{n} \sum _ {d=0}^{\bar{d}} \sum _ {i = d}^{n} \mathbb E[x_i \varepsilon_i x _ {i-d}' \varepsilon _ {i-d}']
\end{aligned}
$$</p>
<p>We estimate $\Omega_n$ by $$
\hat{\Omega}_n = \frac{1}{n} \sum _ {d=0}^{\bar{d}} \sum _ {i = d}^{n} x_i \hat{\varepsilon}_i x _ {i-d}' \hat{\varepsilon} _ {i-d}'
$$</p>
<p><strong>Theorem</strong></p>
<p>If $\bar{d}$ is a fixed integer, then $$
\hat{\Omega}_n - \Omega_n \overset{p}{\to} 0
$$</p>
<blockquote>
<p>What if $\bar{d}$ does not exist (all $x_i, x_j$ are correlated)? $$
\hat{\Omega}_n = \frac{1}{n} \sum _ {d=0}^{n} \sum _ {i = d}^{n} x_i \hat{\varepsilon}_i x _ {i-d}' \hat{\varepsilon} _ {i-d}' = n \mathbb E_n[x_i \hat{\varepsilon}_i]^2 = 0
$$ By the orthogonality property of the OLS residual.</p>
</blockquote>
<p><strong>HAC with Uniform Kernel</strong> $$
\hat{\Omega}_h = \frac{1}{n} \sum _ {i,j} x_i \hat{\varepsilon}_i x_j' \hat{\varepsilon}_j' \mathbb{I} \lbrace |i-j| \leq h \rbrace
$$ where $h$ is the <strong>bandwidth</strong> of the kernel. The bandwidth is chosen
such that
$\mathbb E[x_i \varepsilon_i x _ {i-d}' \varepsilon _ {i-d}' ]$ is small
for $d &gt; h$. How small? Small enough for the estimates to be consistent.</p>
<p><strong>HAC with General Kernel</strong> $$
\hat{\Omega}^{HAC} _ {k,h} = \frac{1}{n} \sum _ {i,j} x_i \hat{\varepsilon}_i x_j' \hat{\varepsilon}_j' k \left( \frac{|i-j|}{n} \right)
$$</p>
<h3 id="hac-consistency">HAC Consistency</h3>
<p><strong>Theorem</strong> If the joint distribution is stationary and $\alpha$-mixing
with $\sum _ {k=1}^\infty k^2 \alpha(k) &lt; \infty$ and</p>
<ul>
<li>$\mathbb E[ | x _ {ij} \varepsilon_i |^\nu ] &lt; \infty$ $\forall \nu$</li>
<li>$\hat{\varepsilon}_i = y_i - x_i' \hat \beta$ for some
$\hat \beta \overset{p}{\to} \beta_0$</li>
<li>$k$ smooth, symmetric, $k(0) \to \infty$ as $z \to \infty$,
$\int k^2 &lt; \infty$</li>
<li>$\frac{h}{n} \to 0$</li>
<li>$h \to \infty$</li>
</ul>
<p>Then the HAC estimator is <strong>consistent</strong>. $$
\hat{\Omega}^{HAC} _ {k,h} - \Omega_n \overset{p}{\to} 0
$$</p>
<h3 id="comments">Comments</h3>
<p>We want to choose $h$ small relative to $n$ in order to avoid estimation
problems. But we also want to choose $h$ large so that the remainder is
small: $$
\begin{aligned}
\Omega_n &amp;= Var(\sqrt{n} \mathbb E_n[x_i \varepsilon_i]) = \newline
&amp;= \underbrace{\frac{1}{n} \sum _ {i,j : |i-j|\leq h} \mathbb E[x_i \varepsilon_i x_j' \varepsilon_j']} _ {\Omega^h_n} + \underbrace{\frac{1}{n} \sum _ {i,j : |i-j|&gt; h} \mathbb E[x_i \varepsilon_i x_j' \varepsilon_j']} _ {\text{remainder: } R_n} = \newline
&amp;= \Omega_n^h + R_n
\end{aligned}
$$</p>
<p>In particular, HAC theory requires: $$
\hat{\Omega}^{HAC} \overset{p}{\to} \Omega \quad \text{ if } \quad
\begin{cases}
&amp; \frac{h}{n} \to 0 \newline
&amp; h \to \infty
\end{cases}
$$</p>
<p>But in practice, long-run estimation implies $\frac{h}{n} \simeq 0$
which is not ``safe” in the sense that it does not imply
$R_n \simeq 0$. On the other hand, if $h \simeq n$, $\hat{\Omega}^{HAC}$
does not converge in probability because it’s too noisy.</p>
<h3 id="choice-of-h">Choice of h</h3>
<p>How to choose $h$? Look at the score autocorrelation function (ACF).</p>
<p><img src="../img/Fig_331.jpg" alt="Autocorrelation Function"></p>
<p>It looks like after 10 periods the empirical autocorrelation is quite
small but still not zero.</p>
<h3 id="fixed-b-asymptotics">Fixed b Asymptotics</h3>
<p>[Neave, 1970]: “<em>When proving results on the asymptotic behavior of
estimates of the spectrum of a stationary time series, it is invariably
assumed that as the sample size $n$ tends to infinity, so does the
truncation point $h$, but at a slower rate, so that $\frac{h}{n}$ tends
to zero. This is a convenient assumption mathematically in that, in
particular, it ensures consistency of the estimates, but it is
unrealistic when such results are used as approximations to the finite
case where the value of $\frac{h}{n}$ cannot be zero.</em>””</p>
<h3 id="fixed-b-theorem">Fixed b Theorem</h3>
<p><strong>Theorem</strong></p>
<p>Under regularity conditions, $$
\sqrt{n} \Big( V^{HAC} _ {k,h} \Big)(\hat \beta - \beta_0) \overset{d}{\to} F
$$</p>
<p>The asymptotic critical values of the $F$ statistic depend on the choice
of the kernel. In order to do hypothesis testing, Kiefer and
Vogelsang(2005) provide critical value functions for the t-statistic for
each kernel-confidence level combination using a cubic equation: $$
cv(b) = a_0 + a_1 b + a_2 b^2 + a_3 b^3
$$</p>
<h3 id="example">Example</h3>
<p>Example for the Bartlett kernel:</p>
<p><img src="../img/Fig_332.png" alt="Fixed-b"></p>
<h3 id="fixed-g-asymptotics">Fixed G Asymptotics</h3>
<p>[Bester, 2013]: “<em>Cluster covariance estimators are routinely used
with data that has a group structure with independence assumed across
groups. Typically, inference is conducted in such settings under the
assumption that there are a large number of these independent groups.</em>””</p>
<p>“<em>However, with enough weakly dependent data, we show that groups can be
chosen by the researcher so that group-level averages are approximately
independent. Intuitively, if groups are large enough and well shaped
(e.g. do not have gaps), the majority of points in a group will be far
from other groups, and hence approximately independent of observations
from other groups provided the data are weakly dependent. The key
prerequisite for our methods is the researcher’s ability to construct
groups whose averages are approximately independent. As we show later,
this often requires that the number of groups be kept relatively small,
which is why our main results explicitly consider a fixed (small) number
of groups.</em>””</p>
<h3 id="assumption">Assumption</h3>
<p><strong>Assumption</strong> Suppose you have data
$D = (y _ {it} , x _ {it}) _ {i=1, t=1}^{N, T}$ where
$y _ {it} = x _ {it}' \beta + \alpha_i + \varepsilon _ {it}$ where $i$
indexes the observational unit and $t$ indexes time (could also be
space).</p>
<p>Let $$
\begin{aligned}
&amp; \tilde{y} _ {it} = y _ {it} - \frac{1}{T} \sum _ {t=1}^T y _ {it} \newline
&amp; \tilde{x} _ {it} = x _ {it} - \frac{1}{T} \sum _ {t=1}^T x _ {it} \newline
&amp; \tilde{\varepsilon} _ {it} = \varepsilon _ {it} - \frac{1}{T} \sum _ {t=1}^T \varepsilon _ {it}
\end{aligned}
$$ Then $$
\tilde{y} _ {it} = \tilde{x} _ {it}' \beta + \tilde{\varepsilon} _ {it}
$$</p>
<p>The $\tilde{\varepsilon} _ {it}$ are by construction correlated between
each other even if the original $\varepsilon$ was iid. The <strong>cluster
score variance estimator</strong> is given by: $$
\hat{\Omega}^{CL} = \frac{1}{T-1} \sum _ {i=1}^n  \sum _ {t=1}^T  \sum _ {s=1}^T \tilde{x} _ {it} \hat{\tilde{\varepsilon}} _ {it} \tilde{x} _ {is}     \hat{\tilde{\varepsilon}} _ {is}
$$</p>
<blockquote>
<p>It’s very similar too the HAC estimator since we have <em>dependent
cross-products</em> here as well. However, here we do not consider the
$i \times j$ cross-products. We only have time-dependency (state).</p>
</blockquote>
<h3 id="comments-1">Comments (1)</h3>
<p>On $T$ and $n$:</p>
<ul>
<li>If $T$ is fixed and $n \to \infty$, then the number of
cross-products considered is much smaller than the total number of
cross-products.</li>
<li>If $T &raquo; n$ issues arise since the number of cross products
considered is close to the total number of cross products. As in HAC
estimation, this is a problem because it implies that the algebraic
estimate of the cluster score variance gets close to zero because of
the orthogonality property of the residuals.</li>
<li>The panel assumption is that observations across individuals are not
correlated.</li>
</ul>
<blockquote>
<p>Strategy: as in HAC, we want to limit the correlation across clusters
(individuals). We hope that observations are <strong>negligibly dependent</strong>
between cluster sufficiently distant from each other.</p>
</blockquote>
<h3 id="comments-2">Comments (2)</h3>
<p>Classical cluster robust estimator: $$
\hat{\Omega}^{CL} = \frac{1}{n} \sum _ {i=1}^n x_i \varepsilon_i x_j' \varepsilon_j' \mathbb{I}   \lbrace i,j \text{ in the same cluster} \rbrace
$$</p>
<blockquote>
<p>On clusters:</p>
<ul>
<li>If the number of observations near a boundary is small relative to
the sample size, ignoring the dependence should not affect
inference too adversely.</li>
<li>The higher the dimension of the data, the easier it is to have
observations near boundaries (<em>curse of dimensionality</em>).</li>
<li>We would like to have few clusters in order to make less
independence assumptions. However, few clusters means bigger
blocks and hence a larger number of cross-products to estimate. If
the number of cross-products is too large (relative to the sample
size), $\hat{\Omega}^{CL}$ does not converge</li>
</ul>
</blockquote>
<p><strong>Theorem</strong>: Under regularity conditions: $$
\hat{t} \overset{d}{\to} \sqrt{\frac{G}{G-1}} t _ {G-1}
$$</p>
<h3 id="code---dgp">Code - DGP</h3>
<p>This code draws 100 observations from the model
$y = 2 x_1 - x_2 + \varepsilon$ where $x_1, x_2 \sim U[0,1]$ and
$\varepsilon \sim N(0,1)$.</p>
<pre><code class="language-julia"># Set seed
Random.seed!(123);

# Set the number of observations
n = 100;

# Set the dimension of X
k = 2;

# Draw a sample of explanatory variables
X = rand(Uniform(0,1), n, k);

# Draw the error term
σ = 1;
ε = rand(Normal(0,1), n, 1) * sqrt(σ);

# Set the parameters
β = [2; -1];

# Calculate the dependent variable
y = X*β + ε;
</code></pre>
<h3 id="ideal-estimate">Ideal Estimate</h3>
<pre><code class="language-julia"># OLS estimator
β_hat = (X'*X)\(X'*y);

# Residuals
ε_hat = y - X*β_hat;

# Homoskedastic standard errors
std_h = var(ε_hat) * inv(X'*X);

# Projection matrix
P = X * inv(X'*X) * X';

# Leverage
h = diag(P);
</code></pre>
<h3 id="hc-estimates">HC Estimates</h3>
<pre><code class="language-julia"># HC0 variance and standard errors
Ω_hc0 = X' * (I(n) .* ε_hat.^2) * X;
std_hc0 = sqrt.(diag(inv(X'*X) * Ω_hc0 * inv(X'*X)))
</code></pre>
<pre><code>## 2-element Array{Float64,1}:
##  0.24691300271914793
##  0.28044707935951835
</code></pre>
<pre><code class="language-julia"># HC1 variance and standard errors
Ω_hc1 = n/(n-k) * X' * (I(n) .* ε_hat.^2) * X;
std_hc1 = sqrt.(diag(inv(X'*X) * Ω_hc1 * inv(X'*X)))
</code></pre>
<pre><code>## 2-element Array{Float64,1}:
##  0.24941979797977423
##  0.2832943308272532
</code></pre>
<pre><code class="language-julia"># HC2 variance and standard errors
Ω_hc2 = X' * (I(n) .* ε_hat.^2 ./ (1 .- h)) * X;
std_hc2 = sqrt.(diag(inv(X'*X) * Ω_hc2 * inv(X'*X)))
</code></pre>
<pre><code>## 2-element Array{Float64,1}:
##  0.2506509902982869
##  0.2850878737103963
</code></pre>
<pre><code class="language-julia"># HC3 variance and standard errors
Ω_hc3 = X' * (I(n) .* ε_hat.^2 ./ (1 .- h).^2) * X;
std_hc3 = sqrt.(diag(inv(X'*X) * Ω_hc3 * inv(X'*X)))
</code></pre>
<pre><code>## 2-element Array{Float64,1}:
##  0.25446321015850176
##  0.2898264779289438
</code></pre>
<pre><code class="language-julia"># Note what happens if you allow for full autocorrelation
omega_full = X'*ε_hat*ε_hat'*X;
</code></pre>
<h2 id="inference">Inference</h2>
<h3 id="hypothesis-testing">Hypothesis Testing</h3>
<p>In order to do inference on $\hat \beta$ we need to know its
distribution. We have two options: (i) assume gaussian error term
(extended GM) or (ii) rely on asymptotic approximations (CLT).</p>
<p>A statistical hypothesis is a subset of a statistical model,
$\mathcal K \subset \mathcal F$. A hypothesis test is a map
$\mathcal D \rightarrow \lbrace 0,1 \rbrace$, $D \mapsto T$. If
$\mathcal F$ is the statistical model and $\mathcal K$ is the
statistical hypothesis, we use the notation $H_0: \Pr \in \mathcal K$.</p>
<blockquote>
<p>Generally, we are interested in understanding whether it is likely
that data $D$ are drawn from $\mathcal K$ or not.</p>
</blockquote>
<p>A hypothesis test, $T$ is our tool for deciding whether the hypothesis
is consistent with the data. $T(D)= 0$ implies fail to reject $H_0$ and
test inconclusive $T(D)=1$ $\implies$ reject $H_0$ and $D$ is
inconsistent with any $\Pr \in \mathcal K$.</p>
<p>Let $\mathcal K \subseteq \mathcal F$ be a statistical hypothesis and
$T$ a hypothesis test.</p>
<ol>
<li>Suppose $\Pr \in \mathcal K$. A Type I error (relative to $\Pr$) is
an event $T(D)=1$ under $\Pr$.</li>
<li>Suppose $\Pr \in \mathcal K^c$. A Type II error (relative to $\Pr$)
is an event $T(D)=0$ under $\Pr$.</li>
</ol>
<p>The corresponding probability of a type I error is called <strong>size</strong>. The
corresponding probability of a type II error is called <strong>power</strong>
(against the alternative $\Pr$).</p>
<p>In this section, we are interested in testing three hypotheses, under
the assumptions of linearity, strict exogeneity, no multicollinearity,
normality on the error term. They are:</p>
<ol>
<li>$H_0: \beta _ {0k} = \bar \beta _ {0k}$ (single coefficient,
$\bar \beta _ {0k} \in \mathbb R$, $k \leq K$)</li>
<li>$a' \beta_0 = c$ (linear combination,
$a \in \mathbb R^K, c \in \mathbb R$)</li>
<li>$R \beta_0 = r$ (linear restrictions,
$R \in \mathbb R^{p \times K}$, full rank, $r \in \mathbb R^p$)</li>
</ol>
<h3 id="testing-problem">Testing Problem</h3>
<p>Consider the testing problem $H_0: \beta _ {0k} = \bar \beta _ {0k}$
where $\bar \beta _ {0k}$ is a pre-specified value under the null. The
t-statistic for this problem is defined by $$
t_k:= \frac{b_k - \bar \beta _ {0k}}{SE(b_k)}, \ \ SE(b_k):= \sqrt{s^2 [(X&rsquo;X)^{-1}] _ {kk}}
$$</p>
<p><strong>Theorem</strong>: In the testing procedure above, the sampling distribution
under the null $H_0$ is given by $$
t_k|X \sim t _ {n-k} \ \ \text{and so} \ \ t_k \sim t _ {n-k}
$$</p>
<p>$t _ {(n-K)}$ denotes the t-distribution with $(n-k)$ degress of
freedom. The test can be one sided or two sided. The above sampling
distribution can be used to construct a confidence interval.</p>
<h3 id="example-1">Example</h3>
<p>We want to asses whether or not the ``true” coefficient $\beta_0$
equals a specific value $\hat \beta$. Specifically, we are interested in
testing $H_0$ against $H_1$, where:</p>
<ul>
<li><em>Null Hypothesis</em>: $H_0: \beta_0 = \hat \beta$</li>
<li><em>Alternative Hypothesis</em>: $H_1: \beta_0 \ne \hat \beta$.</li>
</ul>
<p>Hence, we are interested in a statistic informative about $H_1$, which
is the Wald test statistic $$
|T^*| = \bigg| \frac{\hat \beta - \beta_0}{\sigma(\hat \beta)}\bigg|  \sim N(0,1)
$$</p>
<p>However, the true variance $\sigma^2(\hat \beta )$ is not known and has
to be estimated. Therefore we plug in the sample variance
$\hat \sigma^2(\hat \beta) = \frac{n}{n-1} \mathbb E_n[\hat e_i^2]$ and
we use $$
|T| = \bigg| \frac{\hat \beta - \beta_0}{\hat \sigma (\hat \beta)}\bigg|  \sim t _ {(n-k)}
$$</p>
<h3 id="comments-3">Comments</h3>
<p>Hypothesis testing is like proof by contradiction. Imagine the sampling
distribution was generated by $\beta$. If it is highly improbable to
observe $\hat \beta$ given $\beta_0 = \beta$ then we reject the
hypothesis that the sampling distribution was generated by $\beta$.</p>
<p>Then, given a realized value of the statistic $|T|$, we take the
following decision:</p>
<ul>
<li><em>Do not reject $H_0$</em>: it is consistent with random variation under
true $H_0$—i.e., $|T|$ small as it has an exact student t
distribution with $(n-k)$ degree of freedom in the normal regression
model.</li>
<li><em>Reject $H_0$ in favor of $H_1$</em>: $|T| &gt; c$, with $c$ being the
critical values selected to control for false rejections:
$\Pr(|t _ {n-k}| \geq c) = \alpha$. Moreover, you can also reject
$H_0$ if the p-value $p$ is such that: $p &lt; \alpha$.</li>
</ul>
<h3 id="comments-2-1">Comments (2)</h3>
<p>The probability of false rejection is decreasing in $c$, i.e. the
critical value for a given significant level. $$
\begin{aligned}
\Pr (\text{Reject } H_0 | H_0)  &amp; = \Pr (|T|&gt; c | H_0 ) = \newline
&amp; = \Pr (T &gt; c | H_0 ) +     \Pr (T &lt; -c | H_0 ) = \newline
&amp; = 1 - F(c) + F(-c) = 2(1-F(c))
\end{aligned}
$$</p>
<p><strong>Example</strong>: Consider the testing problem $H_0: a'\beta_0=c$ where $a$
is a pre-specified linear combination under study. The t-statistic for
this problem is defined by: $$
t_k:= \frac{a&rsquo;b - c}{SE(a&rsquo;b)}, \ \ SE(a&rsquo;b):= \sqrt{s^2 a'(X&rsquo;X)^{-1}a}
$$</p>
<h3 id="t-stat">t Stat</h3>
<p><strong>Theorem</strong></p>
<p>In the testing procedure above, the sampling distribution under the null
$H_0$ is given by $$
t_a|X \sim t _ {n-K} \quad\text{and so} \quad t_a \sim t _ {n-K}
$$</p>
<p>Like in the previous test, $t _ {(n-K)}$ denotes the t-distribution with
$(n-K)$ degress of freedom. The test can again be one sided or two
sided. The above sampling distribution can be used to construct a
confidence interval</p>
<h3 id="f-stat">F Stat</h3>
<p><strong>Example</strong></p>
<p>Consider the testing problem $$
H_0: R \beta_0 = r
$$ where $R \in \mathbb R^{p \times k}$ is a presepecified set of linear
combinations and $r \in \mathbb R^p$ is a restriction vector.</p>
<p>The F-statistic for this problem is given by $$
F:= \frac{(Rb-r)'[R(X&rsquo;X)R']^{-1}(Rb-r)/p }{s^2}
$$</p>
<p><strong>Theorem</strong></p>
<p>For the problem, the sampling distribution of the F-statistic under the
null $H_0:$ $$
F|X \sim F _ {p,n-K} \ \ \text{and so} \ \ F \sim F _ {p,n-K}
$$</p>
<p>The test is intrinsically two-sided. The above sampling distribution can
be used to construct a confidence interval.</p>
<h3 id="equivalence">Equivalence</h3>
<p><strong>Theorem</strong></p>
<p>Consider the testing problem $H_0: R \beta_0 = r$ where
$R \in \mathbb R^{p\times K}$ is a presepecified set of linear
combinations and $r \in \mathbb R^p$ is a restriction vector.</p>
<p>Consider the restricted least squares estimator, denoted $\hat \beta_R$:
$\hat \beta_R: = \text{arg} \min _ { \beta: R \beta = r } Q( \beta)$.
Let $SSR_U = Q(b), \ \ SSR_R=Q(\hat \beta_R)$. Then the $F$ statistic is
numerically equivalent to the following expression:
$F = \frac{(SSR_R - SSR_U)/p}{SSR_U/(n-K)}$.</p>
<h3 id="confidence-intervals">Confidence Intervals</h3>
<p>A <strong>confidence interval at $(1-\alpha)$</strong> is a random set $C$ such that
$$
\Pr(\beta_0 \in C) \geq 1- \alpha
$$ i.e. the probability that $C$ covers the true value $\beta$ is fixed
at $(1-\alpha)$.</p>
<p>Since $C$ is not known, it has to be estimated ($\hat{C}$). We construct
confidence intervals such that:</p>
<ul>
<li>they are symmetric around $\hat \beta$;</li>
<li>their length is proportional to
$\sigma(\hat \beta) = \sqrt{Var(\hat \beta)}$.</li>
</ul>
<p>A CI is equivalent to the set of parameter values such that the
t-statistic is less than $c$, i.e., $$
\hat{C} = \bigg\lbrace \beta: |T(\beta) | \leq c \bigg\rbrace = \bigg\lbrace \beta: - c\leq \frac{\beta - \hat \beta}{\sigma(\hat \beta)} \leq c \bigg\rbrace
$$</p>
<p>In practice, to construct a 95% confidence interval for a single
coefficient estimate $\hat \beta_j$, we use the fact that $$
\Pr \left( \frac{| \hat \beta_j - \beta _ {0,j} |}{ \sqrt{\sigma^2 [(X&rsquo;X)^{-1}] _ {jj} }} &gt; 1.96 \right) = 0.05
$$</p>
<h3 id="code">Code</h3>
<pre><code class="language-julia"># t-test for beta=0
t = abs.(β_hat ./ (std_hc1));

# p-value
p_val = 1 .- cdf.(Normal(0,1), t);

# F statistic of joint significance
SSR_u = ε_hat'*ε_hat;
SSR_r = y'*y;
F = (SSR_r - SSR_u)/k / (SSR_u/(n-k));

# 95# confidente intervals
conf_int = [β_hat - 1.96*std_hc1, β_hat + 1.96*std_hc1];
</code></pre>

          </div>

          



          
          
          <div class="article-widget">
            
<div class="post-nav">
  
  
  
  <div class="post-nav-item">
    <div class="meta-nav">Previous</div>
    <a href="/course/metrics/06_endogeneity/" rel="next">Endogeneity</a>
  </div>
  
  
  
  <div class="post-nav-item">
    <div class="meta-nav">Next</div>
    <a href="/course/metrics/07_endogeneity/" rel="prev">Endogeneity</a>
  </div>
  
</div>

          </div>
          
        </div>

        <div class="body-footer">

          





          




          


  
  



        </div>

      </article>

      <footer class="site-footer">

  

  
  <p class="powered-by">
    
      <a href="/privacy/">Privacy Policy</a>
    
    
       &middot; 
      <a href="/terms/">Terms</a>
    
  </p>
  

  

  
  







</footer>


    </main>
  </div>
</div>

  </div>

  <div class="page-footer">
    
    
  </div>

  
<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        <pre><code class="tex hljs"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

      

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/instant.page/5.1.0/instantpage.min.js" integrity="sha512-1+qUtKoh9XZW7j+6LhRMAyOrgSQKenQ4mluTR+cvxXjP1Z54RxZuzstR/H9kgPXQsVB8IW7DMDFUJpzLjvhGSQ==" crossorigin="anonymous"></script>

      
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js" integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin="anonymous"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js" integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin="anonymous"></script>
      

      
      

      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mermaid/8.8.4/mermaid.min.js" integrity="sha512-as1BF4+iHZ3BVO6LLDQ7zrbvTXM+c/1iZ1qII/c3c4L8Rn5tHLpFUtpaEtBNS92f+xGsCzsD7b62XP3XYap6oA==" crossorigin="anonymous" title="mermaid"></script>
      

      
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/highlight.min.js" integrity="sha512-TDKKr+IvoqZnPzc3l35hdjpHD0m+b2EC2SrLEgKDRWpxf2rFCxemkgvJ5kfU48ip+Y+m2XVKyOCD85ybtlZDmw==" crossorigin="anonymous"></script>
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/languages/r.min.js"></script>
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/languages/python.min.js"></script>
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/languages/julia.min.js"></script>
        
      

    

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.7.1/leaflet.min.js" integrity="sha512-SeiQaaDh73yrb56sTW/RgVdi/mMqNeM2oBwubFHagc5BkixSpP1fvqF47mKzPGWYSSy4RwbBunrJBQ4Co8fRWA==" crossorigin="anonymous"></script>
    

    
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/anchor-js/4.2.2/anchor.min.js" integrity="sha512-I7w3ZdSFzw5j3jU3ZkNikBNeIrl3i+hEuEdwNmqUJvwNcaBUNcijnP2gd9DtGlgVYDplfjGoD8vTNsID+lCjqg==" crossorigin="anonymous"></script>
    <script>
      anchors.add();
    </script>
    

    
    
    
      
      <script id="search-hit-fuse-template" type="text/x-template">
        <div class="search-hit" id="summary-{{key}}">
          <div class="search-hit-content">
            <div class="search-hit-name">
              <a href="{{relpermalink}}">{{title}}</a>
              <div class="article-metadata search-hit-type">{{type}}</div>
              <p class="search-hit-description">{{snippet}}</p>
            </div>
          </div>
        </div>
      </script>
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js" integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin="anonymous"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js" integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin="anonymous"></script>
      
    

    
    

    
    
    
    

    
    <script src="/js/bootstrap.bundle.min.6aed84840afc03ab4d5750157f69c120.js"></script>

    
    
      
      
      
      
      
      
      
    

    
    
    
    
    
    
    
    
      
      
    
    
    <script src="/en/js/wowchemy.min.cf8ca859a9b74f8b1cd804621b13e5f1.js"></script>

    
  <script async defer src="https://buttons.github.io/buttons.js"></script>




</body>
</html>
