<!DOCTYPE html><html lang="en-us" >

<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  
  
  
    <meta name="generator" content="Wowchemy 5.1.0 for Hugo" />
  

  

  
  
  
  
  
    
    
    
  
  

  

  
  
  
    
  
  <meta name="description" content="Convergence Sequences A sequence of nonrandom numbers $\lbrace a_n \rbrace$ converges to $a$ (has limit $a$) if for all $\varepsilon&gt;0$, there exists $n _ \varepsilon$ such that if $n &gt; n_ \varepsilon$, then $|a_n - a| &lt; \varepsilon$." />

  
  <link rel="alternate" hreflang="en-us" href="https://matteocourthoud.github.io/course/metrics/03_asymptotics/" />

  







  




  
  
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
  

  
  
  
    <meta name="theme-color" content="#003f5c" />
  

  
  
    
    <script src="/js/mathjax-config.js"></script>
  

  
  
  
  
    
    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha256-FMvZuGapsJLjouA6k7Eo2lusoAX9i0ShlWFG6qt7SLc=" crossorigin="anonymous">

    
    

    
    
    
      
    
    
      
      
        
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/styles/github.min.css" crossorigin="anonymous" title="hl-light" media="print" onload="this.media='all'">
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/styles/dracula.min.css" crossorigin="anonymous" title="hl-dark" media="print" onload="this.media='all'" disabled>
        
      
    

    
    
    
      <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.7.1/leaflet.min.css" integrity="sha512-1xoFisiGdy9nvho8EgXuXvnpR5GAMSjFwp40gSRE3NwdUdIMIKuPa7bqoUhLD0O/5tPNhteAsE5XyyMi5reQVA==" crossorigin="anonymous" media="print" onload="this.media='all'">
    

    

    
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js" integrity="" crossorigin="anonymous" async></script>
      
    
      
      

      
      

      
    
      
      

      
      

      
    
  

  
  
  
    
      
      <link rel="preload" as="style" href="https://fonts.googleapis.com/css2?family=Roboto+Mono&family=Source+Sans+Pro:wght@200;300;400;700&display=swap">
      <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Roboto+Mono&family=Source+Sans+Pro:wght@200;300;400;700&display=swap" media="print" onload="this.media='all'">
    
  

  
  
  
  
  
  <link rel="stylesheet" href="/css/wowchemy.4f7182ca394d705ee32d9d7750e9aa1d.css" />

  




<script async src="https://www.googletagmanager.com/gtag/js?id=UA-144780600-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];

  function gtag() {
      dataLayer.push(arguments);
  }

  function trackOutboundLink(url, target) {
    gtag('event', 'click', {
         'event_category': 'outbound',
         'event_label': url,
         'transport_type': 'beacon',
         'event_callback': function () {
           if (target !== '_blank') {
             document.location = url;
           }
         }
    });
    console.debug("Outbound link clicked: " + url);
  }

  function onClickCallback(event) {
    if ((event.target.tagName !== 'A') || (event.target.host === window.location.host)) {
      return;
    }
    trackOutboundLink(event.target, event.target.getAttribute('target'));  
  }

  gtag('js', new Date());
  gtag('config', 'UA-144780600-1', { 'anonymize_ip': true });
  gtag('set', {'cookie_flags': 'SameSite=None;Secure'});

  
  document.addEventListener('click', onClickCallback, false);
</script>


  

  

  




  
  
  

  

  
    <link rel="manifest" href="/index.webmanifest" />
  

  <link rel="icon" type="image/png" href="/media/icon_hu03e9b3967b83fd39296ec9da5ff1ea05_201175_32x32_fill_lanczos_center_3.png" />
  <link rel="apple-touch-icon" type="image/png" href="/media/icon_hu03e9b3967b83fd39296ec9da5ff1ea05_201175_180x180_fill_lanczos_center_3.png" />

  <link rel="canonical" href="https://matteocourthoud.github.io/course/metrics/03_asymptotics/" />

  
  
  
  
  
  
  
  
    
    
  
  
  <meta property="twitter:card" content="summary" />
  
  <meta property="og:site_name" content="Matteo Courthoud" />
  <meta property="og:url" content="https://matteocourthoud.github.io/course/metrics/03_asymptotics/" />
  <meta property="og:title" content="Asymptotic Theory | Matteo Courthoud" />
  <meta property="og:description" content="Convergence Sequences A sequence of nonrandom numbers $\lbrace a_n \rbrace$ converges to $a$ (has limit $a$) if for all $\varepsilon&gt;0$, there exists $n _ \varepsilon$ such that if $n &gt; n_ \varepsilon$, then $|a_n - a| &lt; \varepsilon$." /><meta property="og:image" content="https://matteocourthoud.github.io/media/icon_hu03e9b3967b83fd39296ec9da5ff1ea05_201175_512x512_fill_lanczos_center_3.png" />
    <meta property="twitter:image" content="https://matteocourthoud.github.io/media/icon_hu03e9b3967b83fd39296ec9da5ff1ea05_201175_512x512_fill_lanczos_center_3.png" /><meta property="og:locale" content="en-us" />
  
    
      <meta
        property="article:published_time"
        content="2021-10-29T00:00:00&#43;00:00"
      />
    
    <meta property="article:modified_time" content="2021-10-29T00:00:00&#43;00:00">
  

  



  

  
  
  
  
  
    <script src="https://cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.1.1/cookieconsent.min.js" integrity="sha256-5VhCqFam2Cn+yjw61zbBNrbHVJ6SRydPeKopYlngbiQ=" crossorigin="anonymous"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.1.1/cookieconsent.min.css" integrity="sha256-zQ0LblD/Af8vOppw18+2anxsuaz3pWYyVWi+bTvTH8Q=" crossorigin="anonymous">
  
  <script>
  window.addEventListener("load", function(){
    window.cookieconsent.initialise({
      "palette": {
        "popup": {
          "background": "#003f5c",
          "text": "rgb(255, 255, 255)"
        },
        "button": {
          "background": "rgb(255, 255, 255)",
          "text": "#003f5c"
        }
      },
      "theme": "classic",
      "content": {
        "message": "This website uses cookies to ensure you get the best experience on our website.",
        "dismiss": "Got it!",
        "link": "Learn more",
        "href": "/privacy/"
      }
    })});
  </script>


  





  <title>Asymptotic Theory | Matteo Courthoud</title>
</head>


<body id="top" data-spy="scroll" data-offset="70" data-target="#TableOfContents" class="page-wrapper   " data-wc-page-id="70668fa894234c30126070b28f67c601" >

  
  
  
  
  
  
  
  
  <script src="/js/wowchemy-init.min.66d3e0fff6d32c4ece05adee927fbd96.js"></script>

  


<aside class="search-modal" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Search</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#" aria-label="Close"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        
        <input name="q" id="search-query" placeholder="Search..." autocapitalize="off"
        autocomplete="off" autocorrect="off" spellcheck="false" type="search" class="form-control"
        aria-label="Search...">
        
      </div>

      
      

      

    </section>
    <section class="section-search-results">

      <div id="search-hits">
        
      </div>

    </section>
  </div>
</aside>



  <div class="page-header">
    












<nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id="navbar-main">
  <div class="container-xl">

    
    <div class="d-none d-lg-inline-flex">
      <a class="navbar-brand" href="/">Matteo Courthoud</a>
    </div>
    

    
    <button type="button" class="navbar-toggler" data-toggle="collapse"
            data-target="#navbar-content" aria-controls="navbar-content" aria-expanded="false" aria-label="Toggle navigation">
    <span><i class="fas fa-bars"></i></span>
    </button>
    

    
    <div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none">
      <a class="navbar-brand" href="/">Matteo Courthoud</a>
    </div>
    

    
    
    <div class="navbar-collapse main-menu-item collapse justify-content-end" id="navbar-content">

      
      <ul class="navbar-nav d-md-inline-flex">
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
            
            
            
              
            
            
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#about"><span>Home</span></a>
        </li>

        
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
            
            
            
              
            
            
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#posts"><span>Posts</span></a>
        </li>

        
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
            
            
            
              
            
            
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#projects"><span>Research</span></a>
        </li>

        
        

        
        <li class="nav-item dropdown">
          <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown" aria-haspopup="true"><span>Courses</span><span class="caret"></span>
          </a>
          <div class="dropdown-menu">
            
              <a class="dropdown-item" href="/course/ml-econ/"><span>Machine Learning for Economics</span></a>
            
              <a class="dropdown-item" href="/course/data-science/"><span>Data Science with Python</span></a>
            
              <a class="dropdown-item" href="/course/empirical-io/"><span>PhD Industrial Organization</span></a>
            
              <a class="dropdown-item" href="/course/metrics/"><span>PhD Econometrics</span></a>
            
              <a class="dropdown-item" href="https://pp4rs.github.io/"><span>Programming Practices for Research</span></a>
            
          </div>
        </li>

        
        

        

        
        
        

        

        
        
        
        

        
          
            
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/cv"><span>CV</span></a>
        </li>

        
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
            
            
            
              
            
            
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#contact"><span>Contact</span></a>
        </li>

        
        

      

        
      </ul>
    </div>

    <ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2">

      
      
        
      

      
      

      
      

      
      

    </ul>

  </div>
</nav>


  </div>

  <div class="page-body">
    

<div class="container-fluid docs">
  <div class="row flex-xl-nowrap">
    <div class="col-12 col-md-3 col-xl-2 docs-sidebar">
      
<nav class="collapse docs-links" id="docs-nav">
  
  
  
  
  
  

  
  
    

    
      

      <ul class="nav docs-sidenav">
        <li><a href="/course/"><i class="fas fa-arrow-left pr-1"></i>Courses</a></li>
      </ul>

      
      
        
          
        
      


  
    
    
    
    
      
    
    

    
      <div class="docs-toc-item">
        <a class="docs-toc-link " href="/course/metrics/">Econometrics</a>
    
      
        <ul class="nav docs-sidenav">
      


  <li class=""><a href="/course/metrics/01_matrices/">Matrix Algebra</a></li>



  <li class=""><a href="/course/metrics/02_probability/">Probability Theory</a></li>



  <li class="active"><a href="/course/metrics/03_asymptotics/">Asymptotic Theory</a></li>



  <li class=""><a href="/course/metrics/04_inference/">Inference</a></li>



  <li class=""><a href="/course/metrics/05_ols_algebra/">OLS Algebra</a></li>



  <li class=""><a href="/course/metrics/06_endogeneity/">Endogeneity</a></li>



  <li class=""><a href="/course/metrics/06_ols_inference/">OLS Inference</a></li>



  <li class=""><a href="/course/metrics/07_endogeneity/">Endogeneity</a></li>



  <li class=""><a href="/course/metrics/08_nonparametric/">Non-Parametric Estimation</a></li>



  <li class=""><a href="/course/metrics/09_selection/">Variable Selection</a></li>

      
        </ul>
      
    

    
      </div>
    

    
  
</nav>

    </div>

    
    
    <div class="d-none d-xl-block col-xl-2 docs-toc">
      

      <ul class="nav toc-top">
        <li><a href="#" id="back_to_top" class="docs-toc-title">Contents</a></li>
      </ul>

      <nav id="TableOfContents">
  <ul>
    <li><a href="#convergence">Convergence</a>
      <ul>
        <li><a href="#sequences">Sequences</a></li>
        <li><a href="#big-o-and-small-o-notation">Big-O and Small-o Notation</a></li>
        <li><a href="#convergence-in-probability">Convergence in Probability</a></li>
        <li><a href="#other-convergences">Other Convergences</a></li>
        <li><a href="#compare-convergences">Compare Convergences</a></li>
      </ul>
    </li>
    <li><a href="#theorems">Theorems</a>
      <ul>
        <li><a href="#slutsky-theorem">Slutsky Theorem</a></li>
        <li><a href="#continuous-mapping-theorem">Continuous Mapping Theorem</a></li>
        <li><a href="#weak-law-of-large-numbers">Weak Law of Large Numbers</a></li>
        <li><a href="#wlln-proof">WLLN Proof</a></li>
        <li><a href="#central-limit-theorem">Central Limit Theorem</a></li>
        <li><a href="#clt-proof-1">CLT Proof (1)</a></li>
        <li><a href="#clt-proof-2">CLT Proof (2)</a></li>
        <li><a href="#delta-method">Delta Method</a></li>
      </ul>
    </li>
    <li><a href="#ergodic-theory">Ergodic Theory</a>
      <ul>
        <li><a href="#ppt">PPT</a></li>
        <li><a href="#poincarè-recurrence">Poincarè Recurrence</a></li>
        <li><a href="#comment">Comment</a></li>
        <li><a href="#ergodic-theorem">Ergodic Theorem</a></li>
        <li><a href="#comment-1">Comment</a></li>
        <li><a href="#comment-2">Comment 2</a></li>
        <li><a href="#mixing">Mixing</a></li>
        <li><a href="#stationarity">Stationarity</a></li>
        <li><a href="#gordins-central-limit-theorem">Gordin’s Central Limit Theorem</a></li>
        <li><a href="#ergodic-central-limit-theorem">Ergodic Central Limit Theorem</a></li>
      </ul>
    </li>
  </ul>
</nav>

      
    </div>
    

    <main class="col-12 col-md-9 col-xl-8 py-md-3 pl-md-5 docs-content" role="main">

      <article class="article">

        <div class="docs-article-container">

          <h1>Asymptotic Theory</h1>

          <p>Last updated on Oct 29, 2021</p>

          <div class="article-style">
            <h2 id="convergence">Convergence</h2>
<h3 id="sequences">Sequences</h3>
<p>A sequence of nonrandom numbers $\lbrace a_n \rbrace$ <strong>converges</strong> to
$a$ (has limit $a$) if for all $\varepsilon&gt;0$, there exists
$n _ \varepsilon$ such that if $n &gt; n_ \varepsilon$, then
$|a_n - a| &lt; \varepsilon$. We write $a_n \to a$ as $n \to \infty$.</p>
<p>A sequence of nonrandom numbers $\lbrace a_n \rbrace$ is <strong>bounded</strong> if
and only if there is some $B &lt; \infty$ such that $|a_n| \leq B$ for all
$n=1,2,&hellip;$ Otherwise, we say that $\lbrace a_n \rbrace$ is unbounded.</p>
<h3 id="big-o-and-small-o-notation">Big-O and Small-o Notation</h3>
<p>A sequence of nonrandom numbers $\lbrace a_n \rbrace$ is $O(N^\delta)$
(at most of order $N^\delta$) if $N^{-\delta} a_n$ is bounded. When
$\delta=0$, $a_n$ is bounded, and we also write $a_n = O(1)$ (big oh
one).</p>
<p>A sequence of nonrandom numbers $\lbrace a_n \rbrace$ is $o(N^\delta)$
if $N^{-\delta} a_n \to 0$. When $\delta=0$, $a_n$ converges to zero,
and we also write $a_n = o(1)$ (little oh one).</p>
<blockquote>
<p><strong>Properties</strong></p>
<ul>
<li>if $a_n = o(N^{\delta})$, then $a_n = O(N^\delta)$</li>
<li>if $a_n = o(1)$, then $a_n = O(1)$</li>
<li>if each element of a sequence of vectors or matrices is
$O(N^\delta)$, we say the sequence of vectors or matrices is
$O(N^\delta)$</li>
<li>similarly for $o(N^\delta)$.</li>
</ul>
</blockquote>
<h3 id="convergence-in-probability">Convergence in Probability</h3>
<p>A sequence of random variables $\lbrace X_n \rbrace$ <strong>converges in
probability</strong> to a constant $c \in \mathbb R$ if for all $\varepsilon&gt;0$
$$
\Pr \big( |X_n - c| &gt; \varepsilon \big) \to 0 \qquad \text{ as } n \to \infty
$$ We write $X_n \overset{p}{\to} c$ and say that $a$ is the probability
limit (<em>plim</em>) of $X_n$: $\mathrm{plim} X_n = c$. In the special case
where $c=0$, we also say that $\lbrace X_n \rbrace$ is $o_p(1)$ (little
oh p one). We also write $X_n = o_p(1)$ or $X_n \overset{p}{\to} 0$.</p>
<p>A sequence of random variables $\lbrace X_n \rbrace$ is bounded in
probability if for every $\varepsilon&gt;0$, there exists a
$B _ \varepsilon &lt; \infty$ and an integer $n_ \varepsilon$ such that $$
\Pr \big( |x_ n| &gt; B_ \varepsilon \big) &lt; \varepsilon \qquad \text{ for all } n &gt; n_ \varepsilon
$$ We write $X_n = O_p(1)$ ($\lbrace X_n \rbrace$ is big oh p one).</p>
<p>A sequence of random variables $\lbrace X_n \rbrace$ is $o_p(a_n)$ where
$\lbrace a_n \rbrace$ is a nonrandom positive sequence, if
$X_n/a_n = o_p(1)$. We write $X_n = o_p(a_n)$.</p>
<p>A sequence of random variables $\lbrace X_n \rbrace$ is $O_p(a_n)$ where
$\lbrace a_n \rbrace$ is a nonrandom positive sequence, if
$X_n/a_n = O_p(1)$. We write $X_n = O_p(a_n)$.</p>
<h3 id="other-convergences">Other Convergences</h3>
<p>A sequence of random variables $\lbrace X_n \rbrace$ <strong>converges almost
surely</strong> to a constant $c \in \mathbb R$ if $$
\Pr \big( X_n \overset{p}{\to} c \big) = 1
$$ We write $X_n \overset{as}{\to} c$.</p>
<p>A sequence of random variables $\lbrace X_n \rbrace$ <strong>converges in mean
square</strong> to a constant $c \in \mathbb R$ if $$
\mathbb E [(X_n - c)^2] \to 0  \qquad \text{ as } n \to \infty
$$ We write $X_n \overset{ms}{\to} c$.</p>
<p>Let $\lbrace X_n \rbrace$ be a sequence of random variables and $F_n$ be
the cumulative distribution function (cdf) of $X_n$. We say that $X_n$
<strong>converges in distribution</strong> to a random variable $x$ with cdf $F$ if
the cdf $F_n$ of $X_n$ converges to the cdf $F$ of $x$ <em>at every
continuity point</em> of $F$. We write $X_n \overset{d}{\to} x$ and we call
$F$ the <strong>asymptotic distribution</strong> of $X_n$.</p>
<h3 id="compare-convergences">Compare Convergences</h3>
<p><strong>Lemma</strong>: Let $\lbrace X_n \rbrace$ be a sequence of random variables
and $c \in \mathbb R$</p>
<ul>
<li>$X_n \overset{ms}{\to} c \ \Rightarrow \ X_n \overset{p}{\to} c$</li>
<li>$X_n \overset{as}{\to} c \ \Rightarrow \ X_n \overset{p}{\to} c$</li>
<li>$X_n \overset{p}{\to} c \ \Rightarrow \ X_n \overset{d}{\to} c$</li>
</ul>
<blockquote>
<p>Note that all the above definitions naturally extend to a sequence of
random vectors by requiring element-by-element convergence. For
example, a sequence of $K \times 1$ random vectors
$\lbrace X_n \rbrace$ <strong>converges in probability</strong> to a constant
$c \in \mathbb R^K$ if for all $\varepsilon&gt;0$ $$
\Pr \big( |X _ {nk} - c_k| &gt; \varepsilon \big) \to 0 \qquad \text{ as } n \to \infty \quad \forall k = 1&hellip;K
$$</p>
</blockquote>
<h2 id="theorems">Theorems</h2>
<h3 id="slutsky-theorem">Slutsky Theorem</h3>
<p><strong>Theorem</strong></p>
<p>Let $\lbrace X_n \rbrace$ and $\lbrace Y_n \rbrace$ be two sequences of
random variables, $x$ a random variable and $c \in \mathbb R$ a constant
such that $\lbrace X_n \rbrace \overset{d}{\to} X$ and
$\lbrace Y_n \rbrace \overset{p}{\to} c$. Then</p>
<ul>
<li>$X_n + Y_n \overset{d}{\to} X + c$</li>
<li>$X_n \cdot Y_n \overset{d}{\to} X \cdot c$</li>
</ul>
<h3 id="continuous-mapping-theorem">Continuous Mapping Theorem</h3>
<p><strong>Theorem</strong></p>
<p>Let $\lbrace X_n \rbrace$ be sequence of $K \times 1$ random vectors and
$g: \mathbb{R}^K \to \mathbb{R}^J$ a continuous function that does not
depend on $n$.Then</p>
<ul>
<li>$x _n \overset{as}{\to} x \ \Rightarrow \ g(X_n) \overset{as}{\to} g(x)$</li>
<li>$x _n \overset{p}{\to} x \ \Rightarrow \ g(X_n) \overset{p}{\to} g(x)$</li>
<li>$x _n \overset{d}{\to} x \ \Rightarrow \ g(X_n) \overset{d}{\to} g(x)$</li>
</ul>
<h3 id="weak-law-of-large-numbers">Weak Law of Large Numbers</h3>
<p><strong>Theorem</strong></p>
<p>Let $\lbrace x_i \rbrace _ {i=1}^n$ be a sequence of independent,
identically distributed random variables such that
$\mathbb{E}[|x_i|] &lt; \infty$. Then the sequence satisfies the <strong>weak law
of large numbers (WLLN)</strong>: $$
\mathbb{E}_n[x_i] = \frac{1}{n} \sum _ {i=1}^n x_i \overset{p}{\to} \mu \qquad \text{ where } \mu \equiv \mathbb{E}[x_i]
$$</p>
<blockquote>
<p><strong>Intuitions</strong> for the law of large numbers:</p>
<ul>
<li>Cancellation with high probability.</li>
<li>Re-visiting regions of the sample space over and over again.</li>
</ul>
</blockquote>
<h3 id="wlln-proof">WLLN Proof</h3>
<p>The independence of the random variables implies no correlation between
them, and we have that $$
Var \left( \mathbb{E}_n[x_i] \right) = Var \left( \frac{1}{n} \sum _ {i=1}^n x_i \right) = \frac{1}{n^2} Var\left( \sum _ {i=1}^n x_i \right) = \frac{n \sigma^2}{n^2} = \frac{\sigma^2}{n}
$$ Using Chebyshev’s inequality on $\mathbb{E}_n[x_i]$ results in $$
\Pr \big( \left|\mathbb{E}_n[x_i]-\mu \right| &gt; \varepsilon \big) \leq {\frac {\sigma ^{2}}{n\varepsilon ^{2}}}
$$ As $n$ approaches infinity, the right hand side approaches $0$. And
by definition of convergence in probability, we have obtained
$\mathbb{E}_n[x_i] \overset{p}{\to} \mu$ as $n \to \infty$.
$$\tag*{$\blacksquare$}$$</p>
<h3 id="central-limit-theorem">Central Limit Theorem</h3>
<p><strong>Lindberg-Levy Central Limit Theorem</strong></p>
<p>Let $\lbrace x_i \rbrace _ {i=1}^n$ be a sequence of independent,
identically distributed random variables such that
$\mathbb{E}[x_i^2] &lt; \infty$, and $\mathbb{E}[x_i] = \mu$. Then
$\lbrace x_i \rbrace$ satisfies the <strong>central limit theorem (CLT)</strong>;
that is, $$
\frac{1}{\sqrt{n}} \sum _ {i=1}^{n} (x_i - \mu) \overset{d}{\to} N(0,\sigma^2)
$$ where $\sigma^2 = Var(x_i) = \mathbb{E}[x_i x_i&rsquo;]$ is necessarily
positive semidefinite.</p>
<h3 id="clt-proof-1">CLT Proof (1)</h3>
<p>Suppose $\lbrace x_i \rbrace$ are independent and identically
distributed random variables, each with mean $\mu$ and finite variance
$\sigma^2$. The sum $x_1 + &hellip; + X_n$ has mean $n \mu$ and variance
$n \sigma^2$.</p>
<p>Consider the random variable $$
Z_n = \frac{x_1 + &hellip; + X_n - n\mu}{\sqrt{n \sigma^2}} = \sum _ {i=1}^n \frac{x_i - \mu}{\sqrt{n \sigma^2}} = \sum _ {i=1}^n \frac{1}{\sqrt{n}} \tilde x_i
$$</p>
<p>where in the last step we defined the new random variables
$\tilde x_i = \frac{x_i - \mu}{\sigma}$ each with zero mean and unit
variance. The characteristic function of $Z_n$ is given by $$
\varphi _ {Z_n} (t) = \varphi _ { \sum _ {i=1}^n \frac{1}{\sqrt{n} } \tilde{x}_i}(t) = \varphi _ {\tilde x_1} \left( \frac{t}{\sqrt{n}} \right) \times &hellip; \times \varphi _ {Y_n} \left( \frac{t}{\sqrt{n}} \right) = \left[ \varphi _ {\tilde x_1} \left( \frac{t}{\sqrt{n}} \right) \right]^n
$$</p>
<p>where in the last step we used the fact that all of the $\tilde{x}_i$
are identically distributed.</p>
<h3 id="clt-proof-2">CLT Proof (2)</h3>
<p>The characteristic function of $\tilde{x}_1$ is, by Taylor’s theorem, $$
\varphi _ {\tilde{x}_1} \left( \frac{t}{\sqrt{n}} \right) = 1 - \frac{t^2}{2n} + o \left( \frac{t^2}{n} \right) \qquad \text{ for } n \to \infty
$$</p>
<p>where $o(t^2)$ is “little o notation” for some function of $t$ that goes
to zero more rapidly than $t^2$. By the limit of the exponential
function, the characteristic function of $Z_n$ equals $$
\varphi _ {Z_ n}(t) = \left[  1 - \frac{t^2}{2n} + o \left( \frac{t^2}{n} \right) \right]^n \to e^{ -\frac{1}{2}t^2 } \qquad \text{ for } n \to \infty
$$</p>
<p>Note that all of the higher order terms vanish in the limit
$n \to \infty$. The right hand side equals the characteristic function
of a standard normal distribution $N(0,1)$, which implies through Lévy’s
continuity theorem that the distribution of $Z_ n$ will approach
$N(0,1)$ as $n \to \infty$. Therefore, the sum $x_1 + &hellip; + x_n$ will
approach that of the normal distribution $N(n_{\mu}, n\sigma^2)$, and
the sample average $$
\mathbb{E}_n [x_i] = \frac{1}{n} \sum _ {i=1}^n x_i
$$</p>
<p>converges to the normal distribution $N(\mu, \sigma^2)$, from which the
central limit theorem follows. $$\tag*{$\blacksquare$}$$</p>
<h3 id="delta-method">Delta Method</h3>
<p>Let $\lbrace X_n \rbrace$ be a sequence of independent, identically
distributed $K \times 1$ random vectors such that</p>
<ul>
<li>$\sqrt{n} (X_n - c) \overset{d}{\to} Z$ for some fixed
$c \in \mathbb{R}^K$</li>
<li>and $\Sigma$ a $K \times K$ positive definite matrix.</li>
</ul>
<p>Suppose $g : \mathbb{R}^K \to \mathbb{R}^J$ with $J \leq K$ is
continuously differentiable and full rank at $c$, then $$
\sqrt{n} \Big[ g(X_n) - g( c ) \Big] \overset{d}{\to} G Z
$$</p>
<p>where $G = \frac{\partial g( c )}{\partial x}$ is the $J \times K$
matrix of partial derivatives evaluated at $c$.</p>
<blockquote>
<p>Note that the most common utilization is with the random variable
$\mathbb E_n [x_i]$. In fact, under the assumptions of the CLT, we
have that $$
\sqrt{n} \Big[ g \big( \mathbb E_n [x_i] \big) - g(\mu) \Big] \overset{d}{\to} N(0, G \Sigma G&rsquo;)
$$</p>
</blockquote>
<h2 id="ergodic-theory">Ergodic Theory</h2>
<h3 id="ppt">PPT</h3>
<p>Let $(\Omega, \mathcal{B}, P)$ be a probability space and
$T: \Omega \rightarrow \Omega$ a measurable map. $T$ is a <strong>probability
preserving transformation</strong> if the probability of the pre-image of every
set is the same as the probability of the set itself,
i.e. $\forall G, \Pr(T^{-1}(G)) = \Pr(G)$.</p>
<p>Let $(\Omega, \mathcal{B}, P)$ be a probability space and
$T: \Omega \rightarrow \Omega$ a PPT. A set $G \in \mathcal{B}$ is
<strong>invariant</strong> if $T^{-1}(G)=G$.</p>
<blockquote>
<p>Note that it does not have to work the other way around:
$G \neq T(G)$.</p>
</blockquote>
<p>Let $(\Omega, \mathcal{B}, P)$ be a probability space and
$T: \Omega \rightarrow \Omega$ a PPT. $T$ is <strong>ergodic</strong> if every
invariant set $G \in \mathcal{B}$ has probability zero or one,
i.e. $\Pr(G) = 0 \lor \Pr(G) = 1$.</p>
<h3 id="poincarè-recurrence">Poincarè Recurrence</h3>
<p><strong>Theorem</strong></p>
<p>Let $(\Omega, \mathcal{B}, P)$ be a probability space and
$T: \Omega \rightarrow \Omega$ a PPT. Suppose $A \in \mathcal{B}$ is
measurable. Then, for almost every $\omega \in A$, $T^n(\omega)\in A$
for infinitely many $n$.</p>
<p><strong>Proof</strong></p>
<p>We follow 5 steps:</p>
<ol>
<li>Let
$G = \lbrace \omega \in A : T^K(\omega) \notin A \quad \forall k &gt;0 \rbrace$:
the set of all points of A that never ``return” in A.</li>
<li>Note that $\forall j \geq 1$, $T^{-j}(G) \cap G = \emptyset$. In
fact, suppose $\omega \in T^{-j}(G)$. Then $\omega \notin G$ since
otherwise we would have $\omega \in G \subseteq A$ and
$\omega \in T^J(G) \subseteq A$ which contradicts the definition of
$G$.</li>
<li>It follows that $\forall l,n \geq 1$,
$T^{-l}(G) \cap T^{-n}(G) = \emptyset$</li>
<li>Since $T$ is a PPT, $\Pr(T^{-j}(G)) = \Pr(G)$ $\forall j$</li>
<li>Then $$
\Pr (T^{-1}(G) \cup T^{-2}(G) \cup &hellip; \cup T^{-l}(G)) = l \cdot \Pr(G) \leq 1 \Rightarrow \Pr(G) \leq \frac{1}{l} \quad \Rightarrow \quad \lim_ {l \to \infty} \Pr(G) = 0
$$ $$\tag*{$\blacksquare$}$$</li>
</ol>
<h3 id="comment">Comment</h3>
<p>Halmos: “<em>The recurrence theorem says that under the appropriate
conditions on a transformation T almost every point of each measurable
set $A$ returns to $A$ infinitely often. It is natural to ask: exactly
how long a time do the images of such recurrent points spend in $A$? The
precise formulation of the problem runs as follows: given a point $x$
(for present purposes it does not matter whether $x$ is in $A$ or not),
and given a positive integer $n$, form the ratio of the number of these
points that belong to $A$ to the total number (i.e., to $n$), and
evaluate the limit of these ratios as $n$ tends to infinity. It is, of
course, not at all obvious in what sense, if any, that limit exists. If
$f$ is the characteristic function of $A$ then the ratio just discussed
is</em>” $$
\frac{1}{n} \sum _ {i=1}^n f(T^{i}x) = \frac{1}{n} \sum _ {i=1}^n x_i
$$</p>
<h3 id="ergodic-theorem">Ergodic Theorem</h3>
<p><strong>Theorem</strong></p>
<p>Let $T$ be an ergodic PPT on $\Omega$. Let $x$ be a random variable on
$\Omega$ with $\mathbb{E}[x] &lt; \infty$. Let $x_i = x \circ T^i$. Then,
$$
\frac{1}{n} \sum _ {i=1}^n x_i \overset{as}{\to} \mathbb{E}[x]
$$</p>
<blockquote>
<p>To figure out whether a PPT is ergodic, it’s useful to draw a graph
with $T^{-1}(G)$ on the y-axis and $G$ on the x-axis.</p>
</blockquote>
<h3 id="comment-1">Comment</h3>
<p>From the ergodic theorem, we have that $$
\lim _ {n \to \infty} \frac{1}{n} \sum _ {i=1}^n f(T^{i}x) g(x) = f^* (x)g(x) \quad \Rightarrow \quad  \lim _ {n \to \infty} \Pr(T^{-n}G \cap H) = \Pr(G)\Pr(H)
$$ where $f^* (x) = \int f(x) dx = \mathbb{E}[f]$.</p>
<p>[Halmos]: <em>We have seen that if a transformation $T$ is ergodic, then
$\Pr(T^{-n}G \cap H)$ converges in the sense of Cesaro to
$\Pr(G)\Pr(H)$. The validity of this condition for all $G$ and $H$ is,
in fact, equivalent to ergodicity. To prove this, suppose that $A$ is a
measurable invariant set, and take both $G$ and $H$ equal to $A$. It
follows that $\Pr(A) = (\Pr(A))^2$, and hence that $\Pr(A)$ is either 0
or 1.</em></p>
<h3 id="comment-2">Comment 2</h3>
<p><em>The Cesaro convergence condition has a natural intuitive
interpretation. We may visualize the transformation $T$ as a particular
way of stirring the contents of a vessel (of total volume 1) full of an
incompressible fluid, which may be thought of as 90 per cent gin ($G$)
and 10 per cent vermouth ($H$). If $H$ is the region originally occupied
by the vermouth, then, for any part $G$ of the vessel, the relative
amount of vermouth in $G$, after $n$ repetitions of the act of stirring,
is given by $\Pr(T^{-n}G \cap H)/\Pr(H)$. The ergodicity of $T$ implies
therefore that on the average this relative amount is exactly equal to
10 per cent. In general, in physical situations like this one, one
expects to be justified in making a much stronger statement, namely
that, after the liquid has been stirred sufficiently often
($n \to \infty$), every part $G$ of the container will contain
approximately 10 per cent vermouth. In mathematical language this
expectation amounts to replacing Cesaro convergence by ordinary
convergence, i.e., to the condition
$\lim_ {n\to \infty} \Pr(T^{-n}G \cap H) = \Pr(G)\Pr(H)$. If a
transformation $T$ satisfies this condition for every pair $G$ and $H$
of measurable sets, it is called mixing, or, in distinction from a
related but slightly weaker concept, strongly mixing.</em>”</p>
<h3 id="mixing">Mixing</h3>
<p>Let $\lbrace\Omega, \mathcal{B}, P \rbrace$ be a probability space. Let
$T$ be a probability preserving transform. Then $T$ is <strong>strongly
mixing</strong> if for every invariant sets $G$,$H \in \mathcal{B}$ $$
P(G \cap T^{-k}H) \to P(G)P(H) \quad \text{ as } k \to \infty
$$ where $T^{-k}H$ is defined as
$T^{-k}H = T^{-1}(&hellip;T^{-1}(T^{-1} H)&hellip;)$ repeated $k$ times.</p>
<p>Let $\lbrace X_i\rbrace _ {i=-\infty}^{\infty}$ be a two sided sequence
of random variables. Let $\mathcal{B}_ {-\infty}^n$ be the sigma algebra
generated by $\lbrace X_i\rbrace _ {i=-\infty}^{n}$ and
$\mathcal{B}_ {n+k}^\infty$ the sigma algebra generated by
$\lbrace X_i \rbrace _ {i=n+k}^{\infty}$. Define the mixing coefficient
$$
\alpha(k) = \sup_ {n \in \mathbb{Z}} \sup_ {G \in \mathcal{B}_ {-\infty}^n} \sup_ {H \in \mathcal{B}_ {n+k}^\infty} | \Pr(G \cap H) - \Pr(G) \Pr(H)|
$$ $\lbrace X_i \rbrace$ is $\mathbb{\alpha}$<strong>-mixing</strong> if
$\alpha(k) \to 0$ if $k \to \infty$.</p>
<blockquote>
<p>Note that mixing implies ergodicity.</p>
</blockquote>
<h3 id="stationarity">Stationarity</h3>
<p>Let $X_i : \Omega \to \mathbb{R}$ be a (two sided) sequence of random
variables with $i \in \mathbb{Z}$. $X_i$ is <strong>strongly stationary</strong> or
simply stationary if $$
\Pr (X _ {i_ 1} \leq a_ 1 , &hellip; , X _ {i_ k} \leq a_ k ) = \Pr (X _ { i _ {1-s}} \leq a_ 1 , &hellip; , X _ {i _ {k-s}} \leq a_ k)  \quad \text{ for every } i_ 1, &hellip;, i_ k, a_ 1, &hellip;, a_ k, s \in \mathbb{R}.
$$</p>
<p>Let $X_i : \Omega \to \mathbb{R}$ be a (two sided) sequence of random
variables with $i \in \mathbb{Z}$. $X_i$ is <strong>covariance stationary</strong> if
$\mathbb{E}[X_i] = \mathbb{E}[X_j]$ for every $i,j$ and
$\mathbb{E}[X_i X_j] = \mathbb{E}[X _ {i+k} X _ {j+k}]$ for all $i,j,k$.
All of the second moments above are assumed to exist.</p>
<p>Let $X_t : \Omega \to \mathbb{R}$ be a sequence of random variables
indexed by $t \in \mathbb{Z}$ such that $\mathbb{E}[|X_t|] &lt; 1$ for each
$t$. $X_t$ is a <strong>martingale</strong> if
$\mathbb{E} [X _ t |X _ {t-1} , X _ {t-2} , &hellip;] = X _ t$. $X_t$ is a
<strong>martingale difference</strong> if
$\mathbb{E} [X _ t | X _ {t-1} , X _ {t-2} ,&hellip;] = 0$.</p>
<h3 id="gordins-central-limit-theorem">Gordin’s Central Limit Theorem</h3>
<p><strong>Theorem</strong></p>
<p>Let $\lbrace z_i \rbrace$ be a stationary, $\alpha$-mixing sequence of
random variables. If moreover</p>
<ul>
<li>$\sum_ {m=1}^\infty \alpha(m)^{\frac{\delta}{2 + \delta}} &lt; \infty$</li>
<li>$\mathbb{E}[z_i] = 0$</li>
<li>$\mathbb{E}\Big[ ||z_i || ^ {2+\delta} \Big] &lt; \infty$</li>
</ul>
<p>Then $$
\sqrt{n} \mathbb{E}_n [z_i] \overset{d}{\to} N(0,\Omega) \quad  \text{ where } \quad  \Omega = \lim _ {n \to \infty} Var(\sqrt{n} \mathbb{E}_n [z_i])
$$</p>
<p>Let $\Omega_k = \mathbb{E}[ z_i z _ {i+k}&rsquo;]$. Then a necessary condition
for Gordin’s CLT is covariance summability:
$\sum _ {k=1}^\infty \Omega_k &lt; \infty$.</p>
<h3 id="ergodic-central-limit-theorem">Ergodic Central Limit Theorem</h3>
<p><strong>Theorem</strong></p>
<p>Let $\lbrace z_i \rbrace$ be a stationary, ergodic, martingale
difference sequence. Then $$
\sqrt{n} \mathbb{E}_n [z_i] \overset{d}{\to} N(0,\Omega) \quad  \text{ where } \quad  \Omega = \lim _ {n \to \infty} Var(\sqrt{n}\mathbb{E}_n[z_i])
$$</p>

          </div>

          



          
          
          <div class="article-widget">
            
<div class="post-nav">
  
  
  
  <div class="post-nav-item">
    <div class="meta-nav">Previous</div>
    <a href="/course/metrics/02_probability/" rel="next">Probability Theory</a>
  </div>
  
  
  
  <div class="post-nav-item">
    <div class="meta-nav">Next</div>
    <a href="/course/metrics/04_inference/" rel="prev">Inference</a>
  </div>
  
</div>

          </div>
          
        </div>

        <div class="body-footer">

          





          




          


  
  



        </div>

      </article>

      <footer class="site-footer">

  

  
  <p class="powered-by">
    
      <a href="/privacy/">Privacy Policy</a>
    
    
       &middot; 
      <a href="/terms/">Terms</a>
    
  </p>
  

  

  
  







</footer>


    </main>
  </div>
</div>

  </div>

  <div class="page-footer">
    
    
  </div>

  
<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        <pre><code class="tex hljs"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

      

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/instant.page/5.1.0/instantpage.min.js" integrity="sha512-1+qUtKoh9XZW7j+6LhRMAyOrgSQKenQ4mluTR+cvxXjP1Z54RxZuzstR/H9kgPXQsVB8IW7DMDFUJpzLjvhGSQ==" crossorigin="anonymous"></script>

      
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js" integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin="anonymous"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js" integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin="anonymous"></script>
      

      
      

      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mermaid/8.8.4/mermaid.min.js" integrity="sha512-as1BF4+iHZ3BVO6LLDQ7zrbvTXM+c/1iZ1qII/c3c4L8Rn5tHLpFUtpaEtBNS92f+xGsCzsD7b62XP3XYap6oA==" crossorigin="anonymous" title="mermaid"></script>
      

      
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/highlight.min.js" integrity="sha512-TDKKr+IvoqZnPzc3l35hdjpHD0m+b2EC2SrLEgKDRWpxf2rFCxemkgvJ5kfU48ip+Y+m2XVKyOCD85ybtlZDmw==" crossorigin="anonymous"></script>
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/languages/r.min.js"></script>
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/languages/python.min.js"></script>
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/languages/julia.min.js"></script>
        
      

    

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.7.1/leaflet.min.js" integrity="sha512-SeiQaaDh73yrb56sTW/RgVdi/mMqNeM2oBwubFHagc5BkixSpP1fvqF47mKzPGWYSSy4RwbBunrJBQ4Co8fRWA==" crossorigin="anonymous"></script>
    

    
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/anchor-js/4.2.2/anchor.min.js" integrity="sha512-I7w3ZdSFzw5j3jU3ZkNikBNeIrl3i+hEuEdwNmqUJvwNcaBUNcijnP2gd9DtGlgVYDplfjGoD8vTNsID+lCjqg==" crossorigin="anonymous"></script>
    <script>
      anchors.add();
    </script>
    

    
    
    
      
      <script id="search-hit-fuse-template" type="text/x-template">
        <div class="search-hit" id="summary-{{key}}">
          <div class="search-hit-content">
            <div class="search-hit-name">
              <a href="{{relpermalink}}">{{title}}</a>
              <div class="article-metadata search-hit-type">{{type}}</div>
              <p class="search-hit-description">{{snippet}}</p>
            </div>
          </div>
        </div>
      </script>
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js" integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin="anonymous"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js" integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin="anonymous"></script>
      
    

    
    

    
    
    
    

    
    <script src="/js/bootstrap.bundle.min.6aed84840afc03ab4d5750157f69c120.js"></script>

    
    
      
      
      
      
      
      
      
    

    
    
    
    
    
    
    
    
      
      
    
    
    <script src="/en/js/wowchemy.min.4ea9cc8d09c5c158656ac1a804743b34.js"></script>

    
  <script async defer src="https://buttons.github.io/buttons.js"></script>




</body>
</html>
