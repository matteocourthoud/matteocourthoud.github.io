<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>PhD Econometrics | Matteo Courthoud</title>
    <link>https://matteocourthoud.github.io/course/metrics/</link>
      <atom:link href="https://matteocourthoud.github.io/course/metrics/index.xml" rel="self" type="application/rss+xml" />
    <description>PhD Econometrics</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><copyright>Theme edited by Matteo CourthoudÂ© - Want to have a similar website? [Guide here](https://matteocourthoud.github.io/post/website/).</copyright><lastBuildDate>Fri, 29 Oct 2021 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://matteocourthoud.github.io/media/icon_hu03e9b3967b83fd39296ec9da5ff1ea05_201175_512x512_fill_lanczos_center_3.png</url>
      <title>PhD Econometrics</title>
      <link>https://matteocourthoud.github.io/course/metrics/</link>
    </image>
    
    <item>
      <title>Matrix Algebra</title>
      <link>https://matteocourthoud.github.io/course/metrics/01_matrices/</link>
      <pubDate>Fri, 29 Oct 2021 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/course/metrics/01_matrices/</guid>
      <description>&lt;h2 id=&#34;basics&#34;&gt;Basics&lt;/h2&gt;
&lt;h3 id=&#34;matrix-definition&#34;&gt;Matrix Definition&lt;/h3&gt;
&lt;p&gt;A real $n \times m$ matrix $A$ is an array&lt;/p&gt;
&lt;p&gt;$$
A=
\begin{bmatrix}
a_{11} &amp;amp; a_{12} &amp;amp; \dots  &amp;amp; a_{1m} \newline
a_{21} &amp;amp; a_{22} &amp;amp; \dots  &amp;amp; a_{2m} \newline
\vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots \newline
a_{n1} &amp;amp; a_{n2} &amp;amp; \dots  &amp;amp; a_{nm}
\end{bmatrix}
$$&lt;/p&gt;
&lt;p&gt;We write $[A]_ {ij} = a_ {ij}$ to indicate the $(i,j)$-element of $A$.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;We will usually take the convention that a real vector
$x \in \mathbb R^n$ is identified with an $n \times 1$ matrix.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The $n \times n$ &lt;strong&gt;identity matrix&lt;/strong&gt; $I_n$ is given by&lt;br&gt;
$$
[I_n] _ {ij} = \begin{cases} 1 \ \ \ \text{if} \ i=j \newline
0 \ \ \ \text{if} \ i \neq j \end{cases}
$$&lt;/p&gt;
&lt;h3 id=&#34;fundamental-operations&#34;&gt;Fundamental Operations&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;Two $n \times m$ matrices, $A,B$, are added element-wise so that
$[A+B]_{ij} = [A] _{ij} + [B] _{ij}$.&lt;/li&gt;
&lt;li&gt;A matrix $A$ can be multiplied by a scalar $c\in \mathbb{R}$ in
which case we set $[cA]_{ij} = c[A] _{ij}$.&lt;/li&gt;
&lt;li&gt;An $n \times m$ matrix $A$ can be multiplied with an $m \times p$
matrix $B$.&lt;/li&gt;
&lt;li&gt;The product $AB$ is defined according to the rule
$[AB] _ {ij} = \sum_{k=1}^m [A] _{ik} [B] _{kj}$.&lt;/li&gt;
&lt;li&gt;An $n \times n$ matrix is invertible if there exists a matrix $B$
such that $AB=I$. In this case, we use the notational convention of
writing $B = A^{-1}$.&lt;/li&gt;
&lt;li&gt;Matrix transposition is defined by $[A&#39;] _{ij} = [A] _{ji}$.&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;trace-and-determinant&#34;&gt;Trace and Determinant&lt;/h3&gt;
&lt;p&gt;The &lt;strong&gt;trace&lt;/strong&gt; of a square matrix $A$ with dimension $n \times n$ is
$\text{tr}(A) = \sum_{i=1}^n a_{ii}$.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;determinant&lt;/strong&gt; of a square $n \times n$ matrix A is defined
according to one of the following three (equivalent) definitions.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Recursively as
$det(A) = \sum_{i=1}^n a_{ij} (-1)^{i+j} det([A]&lt;em&gt;{-i,-j})$ where
$[A]&lt;/em&gt;{-i,-j}$ is the matrix obtained by deleting the $i$th row and
the $j$th column.&lt;/li&gt;
&lt;li&gt;$A \mapsto det(A)$ under the unique alternating multilinear map on
$n \times n$ matrices such that $I \mapsto 1$.&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;linear-independence&#34;&gt;Linear Independence&lt;/h3&gt;
&lt;p&gt;Vectors $x_1,&amp;hellip;,x_k$ are &lt;strong&gt;linearly independent&lt;/strong&gt; if the only solution
to the equation $b_1x_1 + &amp;hellip; + b_k x_k=0, \ b_j \in \mathbb R$, is
$b_1=b_2=&amp;hellip;=b_k=0$.&lt;/p&gt;
&lt;h3 id=&#34;useful-identities&#34;&gt;Useful Identities&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;$(A+B)&#39; =A&#39;+B&#39;$&lt;/li&gt;
&lt;li&gt;$(AB)C = A(BC)$&lt;/li&gt;
&lt;li&gt;$A(B+C) = AB+AC$&lt;/li&gt;
&lt;li&gt;$(AB&#39;) = B&amp;rsquo;A&#39;$&lt;/li&gt;
&lt;li&gt;$(A^{-1})&#39; = (A&#39;)^{-1}$&lt;/li&gt;
&lt;li&gt;$(AB)^{-1} = B^{-1}A^{-1}$&lt;/li&gt;
&lt;li&gt;$\text{tr}(cA) = c\text{tr}(A)$&lt;/li&gt;
&lt;li&gt;$\text{tr}(A+B) = \text{tr}(A) + \text{tr}(B)$&lt;/li&gt;
&lt;li&gt;$\text{tr}(AB) =\text{tr}(BA)$&lt;/li&gt;
&lt;li&gt;$det(I)=1$&lt;/li&gt;
&lt;li&gt;$det(cA) = c^ndet(A)$ if $A$ is $n \times n$ and $c \in \mathbb R$&lt;/li&gt;
&lt;li&gt;$det(A) = det(A&#39;)$&lt;/li&gt;
&lt;li&gt;$det(AB) = det(A)det(B)$&lt;/li&gt;
&lt;li&gt;$det(A^{-1}) = (det(A))^{-1}$&lt;/li&gt;
&lt;li&gt;$A^{-1}$ exists iff $det(A) \neq 0$&lt;/li&gt;
&lt;li&gt;$rank(A) = rank(A&#39;) = rank(A&amp;rsquo;A) = rank(AA&#39;)$&lt;/li&gt;
&lt;li&gt;$A^{-1}$ exists iff $rank(A)=n$ for $A$ $n \times n$&lt;/li&gt;
&lt;li&gt;$rank(AB) \leq \min \lbrace rank(A), rank(B) \rbrace$&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;matrix-rank&#34;&gt;Matrix Rank&lt;/h3&gt;
&lt;p&gt;The &lt;strong&gt;rank&lt;/strong&gt; of a matrix, $rank(A)$ is equal to the maximal number of
linearly independent rows for $A$.&lt;/p&gt;
&lt;p&gt;Let $A$ be an $n \times n$ matrix. The $n \times 1$ vector $x \neq 0$ is
an &lt;strong&gt;eigenvector&lt;/strong&gt; of $A$ with corresponding &lt;strong&gt;eigenvalue&lt;/strong&gt; $\lambda$ is
$Ax = \lambda x$.&lt;/p&gt;
&lt;h3 id=&#34;definitions&#34;&gt;Definitions&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;A matrix $A$ is diagonal if $[A]_ {ij} \neq 0$ only if $i=j$.&lt;/li&gt;
&lt;li&gt;An $n \times n$ matrix $A$ is orthogonal if $A&amp;rsquo;A = I$&lt;/li&gt;
&lt;li&gt;A matrix $A$ is symmetric if $[A]_ {ij} = [A]_ {ji}$.&lt;/li&gt;
&lt;li&gt;An $n \times n$ matrix $A$ is idempotent if $A^2=A$.&lt;/li&gt;
&lt;li&gt;The matrix of zeros ($[A]_ {ij} =0$ for each $i,j$) is simply
denoted 0.&lt;/li&gt;
&lt;li&gt;An $n \times n$ matrix $A$ is nilpotent if $A^k=0$ for some integer
$k&amp;gt;0$.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;spectral-decomposition&#34;&gt;Spectral Decomposition&lt;/h2&gt;
&lt;h3 id=&#34;spectral-theorem&#34;&gt;Spectral Theorem&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt;: Let $A$ be an $n \times n$ symmetric matrix. Then $A$ can
be factored as $A = C \Lambda C&#39;$ where $C$ is orthogonal and $\Lambda$
is diagonal.&lt;/p&gt;
&lt;p&gt;If we postmultiply $A$ by $C$, we get&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$AC = C \Lambda C&amp;rsquo;C$ and&lt;/li&gt;
&lt;li&gt;$AC = C \Lambda$.&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;This is a matrix equation which can be split into columns. The $i$th
column of the equation reads $A c_i = \lambda_i c_i$ which corresponds
to the definition of eigenvalues and eigenvectors. So if the
decomposition exists, then $C$ is the eigenvector matrix and $\Lambda$
contains the eigenvalues.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;rank-and-trace&#34;&gt;Rank and Trace&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt;: The rank of a symmetric matrix equals the number of non
zero eigenvalues.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;:
$rank(A) = rank(C\Lambda C&#39;) = rank(\Lambda) = | \lbrace i: \lambda_i \neq 0 \rbrace |$.
$$\tag*{$\blacksquare$}$$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt;: The nonzero eigenvalues of $AA&#39;$ and $A&amp;rsquo;A$ are identical.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt;: The trace of a symmetric matrix equals the sum of its
eignevalues.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;:
$tr(A) = tr(C \Lambda C&#39;) = tr((C \Lambda)C&#39;) = tr(C&amp;rsquo;C \Lambda) = tr(\Lambda) = \sum_ {i=1}^n \lambda_i.$
$$\tag*{$\blacksquare$}$$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt;: The determinant of a symmetric matrix equals the product of
its eignevalues.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;:
$det(A) = det(C \Lambda C&#39;) = det(C)det(\Lambda)det(C&#39;) = det(C)det(C&#39;)det(\Lambda) = det(CC&#39;) det(\Lambda) = det(I)det(\Lambda) = det(\Lambda) = \prod_ {i=1}^n \lambda_i.$
$$\tag*{$\blacksquare$}$$&lt;/p&gt;
&lt;h3 id=&#34;eigenvalues&#34;&gt;Eigenvalues&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt;: For any symmetric matrix $A$, the eigenvalues of $A^2$ are
the square of the eignevalues of $A$, and the eigenvectors are the same.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;:
$A = C \Lambda C&#39; \implies A^2 = C \Lambda C&#39; C \Lambda C&#39; = C \Lambda I \Lambda C&#39; = C \Lambda^2 C&#39;$
$$\tag*{$\blacksquare$}$$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt;: For any symmetric matrix $A$, and any integer $k&amp;gt;0$, the
eigenvalues of $A^k$ are the $k$th power of the eigenvalues of $A$, and
the eigenvectors are the same.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt;: Any square symmetric matrix $A$ with positive eigenvalues
can be written as the product of a lower triangular matrix $L$ and its
(upper triangular) transpose $L&#39; = U$. That is $A = LU = LL&#39;$&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Note that $$
A = LL&#39; = LU = U&amp;rsquo;U  = (L&#39;)^{-1}L^{-1} = U^{-1}(U&#39;)^{-1}
$$ where $L^{-1}$ is lower triangular and $U^{ -1}$ is upper
trianguar. You can check this for the $2 \times 2$ case. Also note
that the validity of the theorem can be extended to symmetric matrices
with non- negative eigenvalues by a limiting argument. However, then
the proof is not constructive anymore.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;quadratic-forms-and-definite-matrices&#34;&gt;Quadratic Forms and Definite Matrices&lt;/h2&gt;
&lt;h3 id=&#34;definition&#34;&gt;Definition&lt;/h3&gt;
&lt;p&gt;A &lt;strong&gt;quadratic form&lt;/strong&gt; in the $n \times n$ matrix $A$ and $n \times 1$
vector $x$ is defined by the scalar $x&amp;rsquo;Ax$.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;$A$ is negative definite (ND) if for each $x \neq 0$, $x&amp;rsquo;Ax &amp;lt; 0$&lt;/li&gt;
&lt;li&gt;$A$ is negative semidefinite (NSD) if for each $x \neq 0$,
$x&amp;rsquo;Ax \leq 0$&lt;/li&gt;
&lt;li&gt;$A$ is positive definite (PD) if for each $x \neq 0$, $x&amp;rsquo;Ax &amp;gt; 0$&lt;/li&gt;
&lt;li&gt;$A$ is positive semidefinite (PSD) if for each $x \neq 0$,
$x&amp;rsquo;Ax \geq 0$&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;equivalence&#34;&gt;Equivalence&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt;: Let $A$ be a symmetric matrix. Then $A$ is PD(ND) $\iff$
all of its eigenvalues are positive (negative).&lt;/p&gt;
&lt;p&gt;Some more results:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;If a symmetric matrix $A$ is PD (PSD, ND, NSD), then
$\text{det}(A) &amp;gt;(\geq,&amp;lt;,\leq) 0$.&lt;/li&gt;
&lt;li&gt;If symmetric matrix $A$ is PD (ND) then $A^{-1}$ is symmetric PD
(ND).&lt;/li&gt;
&lt;li&gt;The identity matrix is PD (since all eigenvalues are equal to 1).&lt;/li&gt;
&lt;li&gt;Every symmetric idempotent matrix is PSD (since the eigenvalues are
only 0 or 1).&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt;: If $A$ is $n\times k$ with $n&amp;gt;k$ and $rank(A)=k$, then
$A&amp;rsquo;A$ is PD and $AA&#39;$ is PSD.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;semidefinite partial order&lt;/strong&gt; is defined by $A \geq B$ iff $A-B$ is
PSD.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt;: Let $A$, $B$ be symmetric,square , PD, conformable. Then
$A-B$ is PD iff $A^{-1}-B^{-1}$ is PD.&lt;/p&gt;
&lt;h2 id=&#34;matrix-calculus&#34;&gt;Matrix Calculus&lt;/h2&gt;
&lt;h3 id=&#34;comformable-matrices&#34;&gt;Comformable Matrices&lt;/h3&gt;
&lt;p&gt;We first define matrices blockwise when they are conformable. In
particular, we assume that if $A_1, A_2, A_3, A_4$ are matrices with
appropriate dimensions then the matrix $$
A = \begin{bmatrix} A_1 &amp;amp; A_1 \newline
A_3 &amp;amp; A_4 \end{bmatrix}
$$ is defined in the obvious way.&lt;/p&gt;
&lt;h3 id=&#34;matrix-functions&#34;&gt;Matrix Functions&lt;/h3&gt;
&lt;p&gt;Let
$F: \mathbb R^m \times \mathbb R^n \rightarrow \mathbb R^p \times \mathbb R^q$
be a matrix valued function. More precisely, given a real $m \times n$
matrix $X$, $F(X)$ returns the $p \times q$ matrix&lt;br&gt;
$$
\begin{bmatrix}
f_ {11}(X) &amp;amp; &amp;hellip; &amp;amp; f_ {1q}(X) \newline \vdots &amp;amp; \ddots &amp;amp; \vdots \newline
f_ {p1}(X)&amp;amp; &amp;hellip; &amp;amp; f_ {pq}(X)
\end{bmatrix}
$$&lt;/p&gt;
&lt;h3 id=&#34;matrix-derivatives&#34;&gt;Matrix Derivatives&lt;/h3&gt;
&lt;p&gt;The derivative of $F$ with respect to the matrix $X$ is the
$mp \times nq$ matrix $$
\frac{\partial F(X)}{\partial X} = \begin{bmatrix}
\frac{\partial F(X)}{\partial x_ {11}} &amp;amp; &amp;hellip; &amp;amp; \frac{\partial F(X)}{\partial x_ {1n}} \newline \vdots &amp;amp; \ddots &amp;amp; \vdots \newline
\frac{\partial F(X)}{\partial x_ {m1}} &amp;amp; &amp;hellip; &amp;amp; \frac{\partial F(X)}{\partial x_ {mn}}
\end{bmatrix}
$$ where each $\frac{\partial F(X)}{\partial x_ {ij}}$ is a $p\times q$
matrix given by&lt;br&gt;
$$
\frac{\partial F(X)}{\partial x_ {ij}} = \begin{bmatrix}
\frac{\partial f_ {11}(X)}{\partial x_ {ij}} &amp;amp; &amp;hellip; &amp;amp; \frac{\partial f_ {1q}(X)}{\partial x_ {ij}} \newline
\vdots &amp;amp; \ddots &amp;amp; \vdots \newline
\frac{\partial f_ {p1}(X)}{\partial x_ {ij}} &amp;amp; &amp;hellip; &amp;amp; \frac{\partial f_ {pq}(X)}{\partial x_ {ij}}
\end{bmatrix}
$$ The most important case is when
$F: \mathbb R^n \rightarrow \mathbb R$ since this simplifies the
derivation of the least squares estimator. Also, the trickiest thing is
to make sure that dimensions are correct.&lt;/p&gt;
&lt;h3 id=&#34;useful-results-in-matrix-calculus&#34;&gt;Useful Results in Matrix Calculus&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;$\frac{\partial b&amp;rsquo;x}{\partial x}= b$ for $dim(b) = dim(x)$&lt;/li&gt;
&lt;li&gt;$\frac{\partial B&amp;rsquo;x}{\partial x}= B$ for arbitrary, conformable $B$&lt;/li&gt;
&lt;li&gt;$\frac{\partial B&amp;rsquo;x}{\partial x&#39;}= B&#39;$ for arbitrary, conformable
$B$&lt;/li&gt;
&lt;li&gt;$\frac{\partial x&amp;rsquo;Ax}{\partial x} = (A + A&#39;)x$&lt;/li&gt;
&lt;li&gt;$\frac{\partial x&amp;rsquo;Ax}{\partial A} = xx&#39;$&lt;/li&gt;
&lt;li&gt;$\frac{\partial x&amp;rsquo;Ax}{\partial x} = det(A) (A^{-1})&#39;$&lt;/li&gt;
&lt;li&gt;$\frac{\partial \ln det(A)}{\partial A} = (A^{-1})&#39;$&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>Probability Theory</title>
      <link>https://matteocourthoud.github.io/course/metrics/02_probability/</link>
      <pubDate>Fri, 29 Oct 2021 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/course/metrics/02_probability/</guid>
      <description>&lt;h2 id=&#34;probability&#34;&gt;Probability&lt;/h2&gt;
&lt;h3 id=&#34;probability-space&#34;&gt;Probability Space&lt;/h3&gt;
&lt;p&gt;A &lt;strong&gt;probability space&lt;/strong&gt; is a triple $(\Omega, \mathcal A, P)$ where&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\Omega$ is the sample space.&lt;/li&gt;
&lt;li&gt;$\mathcal A$ is the $\sigma$-algebra on $\Omega$.&lt;/li&gt;
&lt;li&gt;$P$ is a probability measure.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The &lt;strong&gt;sample space&lt;/strong&gt; $\Omega$ is the space of all possible events.&lt;/p&gt;
&lt;p&gt;What is a $\sigma$-algebra and a probability measure?&lt;/p&gt;
&lt;h3 id=&#34;sigma-algebra&#34;&gt;Sigma Algebra&lt;/h3&gt;
&lt;p&gt;A nonempty set (of subsets of $\Omega$) $\mathcal A \in 2^\Omega$ is a
&lt;strong&gt;sigma algebra&lt;/strong&gt; ($\sigma$-algebra) of $\Omega$ if the following
conditions hold:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;$\Omega \in \mathcal A$&lt;/li&gt;
&lt;li&gt;If $A \in \mathcal A$, then $(\Omega - A) \in \mathcal A$&lt;/li&gt;
&lt;li&gt;If $A_1, A_2, &amp;hellip; \in \mathcal A$, then
$\bigcup _ {i=1}^{\infty} A_i \in \mathcal A$&lt;/li&gt;
&lt;/ol&gt;
&lt;blockquote&gt;
&lt;p&gt;The smallest $\sigma$-algebra is $\lbrace \emptyset, \Omega \rbrace$
and the largest one is $2^\Omega$ (in cardinality terms).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Suppose $\Omega = \mathbb R$. Let
$\mathcal{C} = \lbrace (a, b],-\infty \leq a&amp;lt;b&amp;lt;\infty \rbrace$. Then the
&lt;strong&gt;Borel&lt;/strong&gt; $\sigma$&lt;strong&gt;- algebra&lt;/strong&gt; on $\mathbb R$ is defined by $$
\mathcal B (\mathbb R) = \sigma (\mathcal C)
$$&lt;/p&gt;
&lt;h3 id=&#34;probability-measure&#34;&gt;Probability Measure&lt;/h3&gt;
&lt;p&gt;A &lt;strong&gt;probability measure&lt;/strong&gt; $P: \mathcal A \to [0,1]$ is a set function
with domain $\mathcal A$ and codomain $[0,1]$ such that&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;$P(A) \geq 0 \ \forall A \in \mathcal A$&lt;/li&gt;
&lt;li&gt;$P$ is $\sigma$-additive: is $A_n \in \mathcal A$ are pairwise
disjoint events ($A_j \cap A_k = \emptyset$ for $j \neq k$), then $$
P\left(\bigcup _ {n=1}^{\infty} A_{n} \right)=\sum _ {n=1}^{\infty} P\left(A_{n}\right)
$$&lt;/li&gt;
&lt;li&gt;$P(\Omega) = 1$&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;properties&#34;&gt;Properties&lt;/h3&gt;
&lt;p&gt;Some properties of probability measures&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$P\left(A^{c}\right)=1-P(A)$&lt;/li&gt;
&lt;li&gt;$P(\emptyset)=0$&lt;/li&gt;
&lt;li&gt;For $A, B \in \mathcal{A}$, $P(A \cup B)=P(A)+P(B)-P(A \cap B)$&lt;/li&gt;
&lt;li&gt;For $A, B \in \mathcal{A}$, if $A \subset B$ then $P(A) \leq P(B)$&lt;/li&gt;
&lt;li&gt;For $A_n \in \mathcal{A}$,
$P \left(\cup _ {n=1}^\infty A_{n} \right) \leq \sum _ {n=1}^\infty P(A_n)$&lt;/li&gt;
&lt;li&gt;For $A_n \in \mathcal{A}$, if $A_n \uparrow A$ then
$\lim _ {n \to \infty} P(A_n) = P(A)$&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;conditional-probability&#34;&gt;Conditional Probability&lt;/h3&gt;
&lt;p&gt;Let $A, B \in \mathcal A$ and $P(B) &amp;gt; 0$, the &lt;strong&gt;conditional
probability&lt;/strong&gt; of $A$ given $B$ is $$
P(A | B)=\frac{P(A \cap B)}{P(B)}
$$&lt;/p&gt;
&lt;p&gt;Two events $A$ and $B$ are &lt;strong&gt;independent&lt;/strong&gt; if $P(A \cap B)=P(A) P(B)$.&lt;/p&gt;
&lt;h3 id=&#34;law-of-total-probability&#34;&gt;Law of Total Probability&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt; (Law of Total Probability)&lt;/p&gt;
&lt;p&gt;Let $(E_n) _ {n \geq 1}$ be a finite or countable partition of $\Omega$.
Then, if $A \in \mathcal A$, $$
P(A) = \sum_n P(A | E_n ) P(E_n)
$$&lt;/p&gt;
&lt;h3 id=&#34;bayes-theorem&#34;&gt;Bayes Theorem&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt; (Bayes Theorem)&lt;/p&gt;
&lt;p&gt;Let $(E_n) _ {n \geq 1}$ be a finite or countable partition of $\Omega$,
and suppose $P(A) &amp;gt; 0$. Then, $$
P(E_n | A) = \frac{P(A | E_n) P(E_n)}{\sum_m P(A | E_m) P(E_m)}
$$&lt;/p&gt;
&lt;p&gt;For a single event $E \in \Omega$, $$
P(E|A) = \frac{P(A|E) P(E)}{P(A)}
$$&lt;/p&gt;
&lt;h2 id=&#34;random-variables&#34;&gt;Random Variables&lt;/h2&gt;
&lt;h3 id=&#34;definition&#34;&gt;Definition&lt;/h3&gt;
&lt;p&gt;A &lt;strong&gt;random variable&lt;/strong&gt; $X$ on a probability space
$(\Omega,\mathcal A, P)$ is a (measurable) mapping
$X : \Omega \to \mathbb{R}$ such that $$
\forall B \in \mathcal{B}(\mathbb{R}), \quad X^{-1}(B) \in \mathcal{A}
$$&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The measurability condition states that the inverse image is a
measurable set of $\Omega$ i.e.Â $X^{-1}(B) \in \mathcal A$. This is
essential since probabilities are defined only on $\mathcal A$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;In words, a random variable itâs a mapping from events to real numbers
such that each interval on the real line can be mapped back into an
element of the sigma algebra (it can be the empty set).&lt;/p&gt;
&lt;h3 id=&#34;distribution-function&#34;&gt;Distribution Function&lt;/h3&gt;
&lt;p&gt;Let $X$ be a real valued random variable. The &lt;strong&gt;distribution function&lt;/strong&gt;
(also called cumulative distribution function) of $X$, commonly denoted
$F_X(x)$ is defined by $$
F_X(x) = \Pr(X \leq x)
$$&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Properties&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$F$ is monotone non-decreasing&lt;/li&gt;
&lt;li&gt;$F$ is right continuous&lt;/li&gt;
&lt;li&gt;$\lim _ {x \to - \infty} F(x)=0$ and
$\lim _ {x \to + \infty} F(x)=1$&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;p&gt;The random variables $(X_1, .. , X_n)$ are independent if and only if $$
F _ {(X_1, &amp;hellip; , X_n)} (x) = \prod _ {i=1}^n F_{X_i} (x_i) \quad \forall x \in \mathbb R^n
$$&lt;/p&gt;
&lt;h3 id=&#34;density-function&#34;&gt;Density Function&lt;/h3&gt;
&lt;p&gt;Let $X$ be a real valued random variable. $X$ has a &lt;strong&gt;probability
density function&lt;/strong&gt; if there exists $f_X(x)$ such that for all measurable
$A \subset \mathbb{R}$, $$
P(X \in A) = \int_A f_X(x) \mathrm{d} x
$$&lt;/p&gt;
&lt;h2 id=&#34;moments&#34;&gt;Moments&lt;/h2&gt;
&lt;h3 id=&#34;expected-value&#34;&gt;Expected Value&lt;/h3&gt;
&lt;p&gt;The &lt;strong&gt;expected value&lt;/strong&gt; of a random variable, when it exists, is given by
$$
\mathbb{E}[ X ] = \int_ \Omega X(\omega) \mathrm{d} P
$$ When $X$ has a density, then $$
\mathbb{E} [ X ] = \int_ \mathbb{R} x f_X (x) \mathrm{d} x = \int _ \mathbb{R} x \mathrm{d} F_X (x)
$$&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;empirical expectation&lt;/strong&gt; (or &lt;strong&gt;sample average&lt;/strong&gt;) is given by $$
\mathbb{E}_n [x_i] = \frac{1}{n} \sum _ {i=1}^N x_i
$$&lt;/p&gt;
&lt;h3 id=&#34;variance-and-covariance&#34;&gt;Variance and Covariance&lt;/h3&gt;
&lt;p&gt;The &lt;strong&gt;covariance&lt;/strong&gt; of two random variables $X$, $Y$ defined on $\Omega$
is $$
Cov(X, Y ) = \mathbb{E}[ (X - \mathbb{E}[ X ]) (Y - \mathbb{E}[ Y ]) ]  = \mathbb{E}[XY ] - \mathbb{E}[ X ]E[ Y ]
$$ In vector notation,
$Cov(X, Y) = \mathbb{E}[XY&#39;] - \mathbb{E}[ X ]\mathbb{E}[Y&#39;]$.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;variance&lt;/strong&gt; of a random variable $X$, when it exists, is given by
$$
Var(X) = \mathbb{E}[ (X - \mathbb{E}[ X ])^2 ] = \mathbb{E}[X^2] - \mathbb{E}[ X ]^2
$$ In vector notation,
$Var(X) = \mathbb{E}[XX&#39;] - \mathbb{E}[ X ]\mathbb{E}[X&#39;]$.&lt;/p&gt;
&lt;h3 id=&#34;properties-1&#34;&gt;Properties&lt;/h3&gt;
&lt;p&gt;Let $X, Y, Z, T \in \mathcal{L}^{2}$ and $a, b, c, d \in \mathbb{R}$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$Cov(X, X) = Var(X)$&lt;/li&gt;
&lt;li&gt;$Cov(X, Y) = Cov(Y, X)$&lt;/li&gt;
&lt;li&gt;$Cov(aX + b, Y) = a \ Cov(X,Y)$&lt;/li&gt;
&lt;li&gt;$Cov(X+Z, Y) = Cov(X,Y) + Cov(Z,Y)$&lt;/li&gt;
&lt;li&gt;$Cov(aX + bZ, cY + dT) = ac * Cov(X,Y) + ad * Cov(X,T) + bc * Cov(Z,Y) + bd * Cov(Z,T)$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Let $X, Y \in \mathcal L^1$ be independent. Then,
$\mathbb E[XY] = \mathbb E[ X ] \mathbb E[ Y ]$.&lt;/p&gt;
&lt;p&gt;If $X$ and $Y$ are independent, then $Cov(X,Y) = 0$.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Note that the converse does not hold:
$Cov(X,Y) = 0 \not \to X \perp Y$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;sample-variance&#34;&gt;Sample Variance&lt;/h3&gt;
&lt;p&gt;The &lt;strong&gt;sample variance&lt;/strong&gt; is given by $$
Var_n (x_i) = \frac{1}{n} \sum _ {i=1}^N (x_i - \bar{x})^2
$$ where
$\bar{x_i} = \mathbb{E}_n [x_i] = \frac{1}{n} \sum _ {i=1}^N x_i$.&lt;/p&gt;
&lt;h3 id=&#34;finite-sample-bias-theorem&#34;&gt;Finite Sample Bias Theorem&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt;: The expected sample variance
$\mathbb{E} [\sigma^2_n] = \mathbb{E} \left[ \frac{1}{n} \sum _ {i=1}^N \left(y_i - \mathbb{E}_n[ Y ] \right)^2 \right]$
gives an estimate of the population variance that is biased by a factor
of $\frac{1}{n}$ and is therefore referred to as &lt;strong&gt;biased sample
variance&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;: $$
\begin{aligned}
&amp;amp;\mathbb{E}[\sigma^2_n] =  \mathbb{E} \left[ \frac{1}{n} \sum _ {i=1}^n \left( y_i - \mathbb{E}_n [ Y ] \right)^2 \right] =
\newline
&amp;amp;= \mathbb{E} \left[ \frac{1}{n} \sum _ {i=1}^n \left( y_i - \frac{1}{n} \sum _ {i=1}^n y_i \right )^2 \right] =
\newline
&amp;amp;= \frac{1}{n} \sum _ {i=1}^n \mathbb{E} \left[ y_i^2 - \frac{2}{n} y_i \sum _ {j=1}^n y_j + \frac{1}{n^2} \sum _ {j=1}^n y_j \sum _ {k=1}^{n}y_k  \right] =
\newline
&amp;amp;= \frac{1}{n} \sum _ {i=1}^n  \left[ \frac{n-2}{n} \mathbb{E}[y_i^2]  - \frac{2}{n} \sum _ {j\neq i} \mathbb{E}[y_i y_j] + \frac{1}{n^2} \sum _ {j=1}^n \sum _ {k\neq j} \mathbb{E}[y_j y_k] + \frac{1}{n^2} \sum _ {j=1}^n \mathbb{E}[y_j^2] \right] =
\newline
&amp;amp;= \frac{1}{n} \sum _ {i=1}^n  \left[ \frac{n-2}{n}(\mu^2 + \sigma^2) - \frac{2}{n} (n-1) \mu^2 + \frac{1}{n^2} n(n-1)\mu^2 + \frac{1}{n^2} n (\mu^2 + \sigma^2)]\right] =
\newline
&amp;amp;= \frac{n-1}{n} \sigma^2
\end{aligned}
$$ $$\tag*{$\blacksquare$}$$&lt;/p&gt;
&lt;h3 id=&#34;inequalities&#34;&gt;Inequalities&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Triangle Inequality&lt;/strong&gt;: if $\mathbb{E} [ X ] &amp;lt; \infty$, then $$
|\mathbb{E} [ X ] | \leq \mathbb{E} [|X|]
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Markovâs Inequality&lt;/strong&gt;: if $\mathbb{E}[ X ] &amp;lt; \infty$, then $$
\Pr(|X| &amp;gt; t) \leq \frac{1}{t} \mathbb{E}[|X|]
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Chebyshevâs Inequality&lt;/strong&gt;: if $\mathbb{E}[X^2] &amp;lt; \infty$, then $$
\Pr(|X- \mu|&amp;gt; t \sigma) \leq \frac{1}{t^2}\Leftrightarrow \Pr(|X- \mu|&amp;gt; t ) \leq \frac{\sigma^2}{t^2}
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Cauchy-Schwarzâs Inequality&lt;/strong&gt;: $$
\mathbb{E} [|XY|] \leq \sqrt{\mathbb{E}[X^2] \mathbb{E}[Y^2]}
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Minkowski Inequality&lt;/strong&gt;: $$
\left( \sum _ {k=1}^n | x_k + y_k |^p \right) ^ {\frac{1}{p}} \leq \left( \sum _ {k=1}^n | x_k |^p \right) ^ {\frac{1}{p}} + \left( \sum _ {k=1}^n | y_k | ^p \right) ^ { \frac{1}{p} }
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Jensenâs Inequality&lt;/strong&gt;: if $g( \cdot)$ is concave (e.g.Â logarithmic
function), then $$
\mathbb{E}[g(x)] \leq g(\mathbb{E}[ X ])
$$ Similarly, if $g(\cdot)$ is convex (e.g.Â exponential function),
then $$
\mathbb{E}[g(x)] \geq g(\mathbb{E}[ X ])
$$&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;law-of-iterated-expectations&#34;&gt;Law of Iterated Expectations&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt; (Law of Iterated Expectations) $$
\mathbb{E}(Y) = \mathbb{E}_X [\mathbb{E}(Y|X)]
$$ &amp;gt; This states that the expectation of the conditional expectation is
the unconditional expectation. &amp;gt; &amp;gt; In other words the average of the
conditional averages is the unconditional average.&lt;/p&gt;
&lt;h3 id=&#34;law-of-total-variance&#34;&gt;Law of Total Variance&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt; (Law of Total Variance) $$
Var(Y) = Var_X (\mathbb{E}[Y |X]) + \mathbb{E}_X [Var(Y|X)]
$$&lt;/p&gt;
&lt;p&gt;Since variances are always non-negative, the law of total variance
implies $$
Var(Y) \geq Var_X (\mathbb{E}[Y |X])
$$&lt;/p&gt;
&lt;h2 id=&#34;distributions&#34;&gt;Distributions&lt;/h2&gt;
&lt;h3 id=&#34;normal-distribution&#34;&gt;Normal Distribution&lt;/h3&gt;
&lt;p&gt;We say that a random variable $Z$ has the &lt;strong&gt;standard normal
distribution&lt;/strong&gt;, or &lt;strong&gt;Gaussian&lt;/strong&gt;, written $Z \sim N(0,1)$, if it has the
density $$
\phi(x)=\frac{1}{\sqrt{2 \pi}} \exp \left(-\frac{x^{2}}{2}\right), \quad-\infty&amp;lt;x&amp;lt;\infty
$$ If $Z \sim N(0, 1)$ and $X = \mu + \sigma Z$ for $\mu \in \mathbb R$
and $\sigma \geq 0$, then $X$ has a &lt;strong&gt;univariate normal distribution&lt;/strong&gt;,
written $X \sim N(\mu, \sigma^2)$. By change-of-variables &lt;em&gt;X&lt;/em&gt; has the
density $$
f(x)=\frac{1}{\sqrt{2 \pi \sigma^{2}}} \exp \left(-\frac{(x-\mu)^{2}}{2 \sigma^{2}}\right), \quad-\infty&amp;lt;x&amp;lt;\infty
$$&lt;/p&gt;
&lt;h3 id=&#34;multinomial-normal-distribution&#34;&gt;Multinomial Normal Distribution&lt;/h3&gt;
&lt;p&gt;We say that the &lt;em&gt;k&lt;/em&gt; -vector &lt;em&gt;Z&lt;/em&gt; has a &lt;strong&gt;multivariate standard normal
distribution&lt;/strong&gt;, written $Z \sim N(0, I_k)$ if it has the joint density
$$
f(x)=\frac{1}{(2 \pi)^{k / 2}} \exp \left(-\frac{x^{\prime} x}{2}\right), \quad x \in \mathbb{R}^{k}
$$ If $Z \sim N(0, I_k)$ and $X = \mu + B Z$, then the &lt;em&gt;k&lt;/em&gt;-vector $X$
has a &lt;strong&gt;multivariate normal distribution&lt;/strong&gt;, written
$X \sim N(\mu, \Sigma)$ where $\Sigma = BB&#39; \geq 0$. If $\sigma &amp;gt; 0$,
then by change-of-variables $X$ has the joint density function $$
f(x)=\frac{1}{(2 \pi)^{k / 2} \operatorname{det}(\Sigma)^{1 / 2}} \exp \left(-\frac{(x-\mu)^{\prime} \Sigma^{-1}(x-\mu)}{2}\right), \quad x \in \mathbb{R}^{k}
$$&lt;/p&gt;
&lt;h3 id=&#34;properties-2&#34;&gt;Properties&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;The expectation and covariance matrix of $X \sim N(\mu, \Sigma)$ are
$\mathbb E &lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; = \mu$ and $Var&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; =\Sigma$.&lt;/li&gt;
&lt;li&gt;If $(X,Y)$ are multivariate normal, $X$ and $Y$ are uncorrelated if
and only if they are independent.&lt;/li&gt;
&lt;li&gt;If $X \sim N(\mu, \Sigma)$ and $Y = a + bB$, then
$X \sim N(a + B\mu, B \Sigma B&#39;)$.&lt;/li&gt;
&lt;li&gt;If $X \sim N(0, I_k)$, then $X&amp;rsquo;X \sim \chi^2_k$, chi-square with $k$
degrees of freedom.&lt;/li&gt;
&lt;li&gt;If $X \sim N(0, \Sigma)$ with $\Sigma&amp;gt;0$, then
$X&#39; \Sigma X \sim \chi_k$ where $k = \dim (X)$.&lt;/li&gt;
&lt;li&gt;If $Z \sim N(0,1)$ and $Q \sim \chi^2_k$ are independent then
$\frac{Z}{\sqrt{Q/k}} \sim t_k$, student t with &lt;em&gt;k&lt;/em&gt; degrees of
freedom.&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;normal-distribution-relatives&#34;&gt;Normal Distribution Relatives&lt;/h3&gt;
&lt;p&gt;These distributions are relatives of the normal distribution&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;$\chi^2_q \sim \sum _ {i=1}^q Z_i^2$ where $Z_i \sim N(0,1)$&lt;/li&gt;
&lt;li&gt;$t_n \sim \frac{Z}{\sqrt{\chi^2 _ n}/n }$&lt;/li&gt;
&lt;li&gt;$F(n_1 , n_2) \sim \frac{\chi^2 _ {n_1} / n_1}{\chi^2 _ {n_2}/n_2}$&lt;/li&gt;
&lt;/ol&gt;
&lt;blockquote&gt;
&lt;p&gt;The $t$ distribution is approximately standard normal but has heavier
tails. The approximation is good for $n \geq 30$:
$t_{n\geq 30} \sim N(0,1)$&lt;/p&gt;
&lt;/blockquote&gt;
</description>
    </item>
    
    <item>
      <title>Asymptotic Theory</title>
      <link>https://matteocourthoud.github.io/course/metrics/03_asymptotics/</link>
      <pubDate>Fri, 29 Oct 2021 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/course/metrics/03_asymptotics/</guid>
      <description>&lt;h2 id=&#34;convergence&#34;&gt;Convergence&lt;/h2&gt;
&lt;h3 id=&#34;sequences&#34;&gt;Sequences&lt;/h3&gt;
&lt;p&gt;A sequence of nonrandom numbers $\lbrace a_n \rbrace$ &lt;strong&gt;converges&lt;/strong&gt; to
$a$ (has limit $a$) if for all $\varepsilon&amp;gt;0$, there exists
$n _ \varepsilon$ such that if $n &amp;gt; n_ \varepsilon$, then
$|a_n - a| &amp;lt; \varepsilon$. We write $a_n \to a$ as $n \to \infty$.&lt;/p&gt;
&lt;p&gt;A sequence of nonrandom numbers $\lbrace a_n \rbrace$ is &lt;strong&gt;bounded&lt;/strong&gt; if
and only if there is some $B &amp;lt; \infty$ such that $|a_n| \leq B$ for all
$n=1,2,&amp;hellip;$ Otherwise, we say that $\lbrace a_n \rbrace$ is unbounded.&lt;/p&gt;
&lt;h3 id=&#34;big-o-and-small-o-notation&#34;&gt;Big-O and Small-o Notation&lt;/h3&gt;
&lt;p&gt;A sequence of nonrandom numbers $\lbrace a_n \rbrace$ is $O(N^\delta)$
(at most of order $N^\delta$) if $N^{-\delta} a_n$ is bounded. When
$\delta=0$, $a_n$ is bounded, and we also write $a_n = O(1)$ (big oh
one).&lt;/p&gt;
&lt;p&gt;A sequence of nonrandom numbers $\lbrace a_n \rbrace$ is $o(N^\delta)$
if $N^{-\delta} a_n \to 0$. When $\delta=0$, $a_n$ converges to zero,
and we also write $a_n = o(1)$ (little oh one).&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Properties&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;if $a_n = o(N^{\delta})$, then $a_n = O(N^\delta)$&lt;/li&gt;
&lt;li&gt;if $a_n = o(1)$, then $a_n = O(1)$&lt;/li&gt;
&lt;li&gt;if each element of a sequence of vectors or matrices is
$O(N^\delta)$, we say the sequence of vectors or matrices is
$O(N^\delta)$&lt;/li&gt;
&lt;li&gt;similarly for $o(N^\delta)$.&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;convergence-in-probability&#34;&gt;Convergence in Probability&lt;/h3&gt;
&lt;p&gt;A sequence of random variables $\lbrace X_n \rbrace$ &lt;strong&gt;converges in
probability&lt;/strong&gt; to a constant $c \in \mathbb R$ if for all $\varepsilon&amp;gt;0$
$$
\Pr \big( |X_n - c| &amp;gt; \varepsilon \big) \to 0 \qquad \text{ as } n \to \infty
$$ We write $X_n \overset{p}{\to} c$ and say that $a$ is the probability
limit (&lt;em&gt;plim&lt;/em&gt;) of $X_n$: $\mathrm{plim} X_n = c$. In the special case
where $c=0$, we also say that $\lbrace X_n \rbrace$ is $o_p(1)$ (little
oh p one). We also write $X_n = o_p(1)$ or $X_n \overset{p}{\to} 0$.&lt;/p&gt;
&lt;p&gt;A sequence of random variables $\lbrace X_n \rbrace$ is bounded in
probability if for every $\varepsilon&amp;gt;0$, there exists a
$B _ \varepsilon &amp;lt; \infty$ and an integer $n_ \varepsilon$ such that $$
\Pr \big( |x_ n| &amp;gt; B_ \varepsilon \big) &amp;lt; \varepsilon \qquad \text{ for all } n &amp;gt; n_ \varepsilon
$$ We write $X_n = O_p(1)$ ($\lbrace X_n \rbrace$ is big oh p one).&lt;/p&gt;
&lt;p&gt;A sequence of random variables $\lbrace X_n \rbrace$ is $o_p(a_n)$ where
$\lbrace a_n \rbrace$ is a nonrandom positive sequence, if
$X_n/a_n = o_p(1)$. We write $X_n = o_p(a_n)$.&lt;/p&gt;
&lt;p&gt;A sequence of random variables $\lbrace X_n \rbrace$ is $O_p(a_n)$ where
$\lbrace a_n \rbrace$ is a nonrandom positive sequence, if
$X_n/a_n = O_p(1)$. We write $X_n = O_p(a_n)$.&lt;/p&gt;
&lt;h3 id=&#34;other-convergences&#34;&gt;Other Convergences&lt;/h3&gt;
&lt;p&gt;A sequence of random variables $\lbrace X_n \rbrace$ &lt;strong&gt;converges almost
surely&lt;/strong&gt; to a constant $c \in \mathbb R$ if $$
\Pr \big( X_n \overset{p}{\to} c \big) = 1
$$ We write $X_n \overset{as}{\to} c$.&lt;/p&gt;
&lt;p&gt;A sequence of random variables $\lbrace X_n \rbrace$ &lt;strong&gt;converges in mean
square&lt;/strong&gt; to a constant $c \in \mathbb R$ if $$
\mathbb E [(X_n - c)^2] \to 0  \qquad \text{ as } n \to \infty
$$ We write $X_n \overset{ms}{\to} c$.&lt;/p&gt;
&lt;p&gt;Let $\lbrace X_n \rbrace$ be a sequence of random variables and $F_n$ be
the cumulative distribution function (cdf) of $X_n$. We say that $X_n$
&lt;strong&gt;converges in distribution&lt;/strong&gt; to a random variable $x$ with cdf $F$ if
the cdf $F_n$ of $X_n$ converges to the cdf $F$ of $x$ &lt;em&gt;at every
continuity point&lt;/em&gt; of $F$. We write $X_n \overset{d}{\to} x$ and we call
$F$ the &lt;strong&gt;asymptotic distribution&lt;/strong&gt; of $X_n$.&lt;/p&gt;
&lt;h3 id=&#34;compare-convergences&#34;&gt;Compare Convergences&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Lemma&lt;/strong&gt;: Let $\lbrace X_n \rbrace$ be a sequence of random variables
and $c \in \mathbb R$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$X_n \overset{ms}{\to} c \ \Rightarrow \ X_n \overset{p}{\to} c$&lt;/li&gt;
&lt;li&gt;$X_n \overset{as}{\to} c \ \Rightarrow \ X_n \overset{p}{\to} c$&lt;/li&gt;
&lt;li&gt;$X_n \overset{p}{\to} c \ \Rightarrow \ X_n \overset{d}{\to} c$&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;Note that all the above definitions naturally extend to a sequence of
random vectors by requiring element-by-element convergence. For
example, a sequence of $K \times 1$ random vectors
$\lbrace X_n \rbrace$ &lt;strong&gt;converges in probability&lt;/strong&gt; to a constant
$c \in \mathbb R^K$ if for all $\varepsilon&amp;gt;0$ $$
\Pr \big( |X _ {nk} - c_k| &amp;gt; \varepsilon \big) \to 0 \qquad \text{ as } n \to \infty \quad \forall k = 1&amp;hellip;K
$$&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;theorems&#34;&gt;Theorems&lt;/h2&gt;
&lt;h3 id=&#34;slutsky-theorem&#34;&gt;Slutsky Theorem&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Let $\lbrace X_n \rbrace$ and $\lbrace Y_n \rbrace$ be two sequences of
random variables, $x$ a random variable and $c \in \mathbb R$ a constant
such that $\lbrace X_n \rbrace \overset{d}{\to} X$ and
$\lbrace Y_n \rbrace \overset{p}{\to} c$. Then&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$X_n + Y_n \overset{d}{\to} X + c$&lt;/li&gt;
&lt;li&gt;$X_n \cdot Y_n \overset{d}{\to} X \cdot c$&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;continuous-mapping-theorem&#34;&gt;Continuous Mapping Theorem&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Let $\lbrace X_n \rbrace$ be sequence of $K \times 1$ random vectors and
$g: \mathbb{R}^K \to \mathbb{R}^J$ a continuous function that does not
depend on $n$.Then&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$x _n \overset{as}{\to} x \ \Rightarrow \ g(X_n) \overset{as}{\to} g(x)$&lt;/li&gt;
&lt;li&gt;$x _n \overset{p}{\to} x \ \Rightarrow \ g(X_n) \overset{p}{\to} g(x)$&lt;/li&gt;
&lt;li&gt;$x _n \overset{d}{\to} x \ \Rightarrow \ g(X_n) \overset{d}{\to} g(x)$&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;weak-law-of-large-numbers&#34;&gt;Weak Law of Large Numbers&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Let $\lbrace x_i \rbrace _ {i=1}^n$ be a sequence of independent,
identically distributed random variables such that
$\mathbb{E}[|x_i|] &amp;lt; \infty$. Then the sequence satisfies the &lt;strong&gt;weak law
of large numbers (WLLN)&lt;/strong&gt;: $$
\mathbb{E}_n[x_i] = \frac{1}{n} \sum _ {i=1}^n x_i \overset{p}{\to} \mu \qquad \text{ where } \mu \equiv \mathbb{E}[x_i]
$$&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Intuitions&lt;/strong&gt; for the law of large numbers:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Cancellation with high probability.&lt;/li&gt;
&lt;li&gt;Re-visiting regions of the sample space over and over again.&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;wlln-proof&#34;&gt;WLLN Proof&lt;/h3&gt;
&lt;p&gt;The independence of the random variables implies no correlation between
them, and we have that $$
Var \left( \mathbb{E}_n[x_i] \right) = Var \left( \frac{1}{n} \sum _ {i=1}^n x_i \right) = \frac{1}{n^2} Var\left( \sum _ {i=1}^n x_i \right) = \frac{n \sigma^2}{n^2} = \frac{\sigma^2}{n}
$$ Using Chebyshevâs inequality on $\mathbb{E}_n[x_i]$ results in $$
\Pr \big( \left|\mathbb{E}_n[x_i]-\mu \right| &amp;gt; \varepsilon \big) \leq {\frac {\sigma ^{2}}{n\varepsilon ^{2}}}
$$ As $n$ approaches infinity, the right hand side approaches $0$. And
by definition of convergence in probability, we have obtained
$\mathbb{E}_n[x_i] \overset{p}{\to} \mu$ as $n \to \infty$.
$$\tag*{$\blacksquare$}$$&lt;/p&gt;
&lt;h3 id=&#34;central-limit-theorem&#34;&gt;Central Limit Theorem&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Lindberg-Levy Central Limit Theorem&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Let $\lbrace x_i \rbrace _ {i=1}^n$ be a sequence of independent,
identically distributed random variables such that
$\mathbb{E}[x_i^2] &amp;lt; \infty$, and $\mathbb{E}[x_i] = \mu$. Then
$\lbrace x_i \rbrace$ satisfies the &lt;strong&gt;central limit theorem (CLT)&lt;/strong&gt;;
that is, $$
\frac{1}{\sqrt{n}} \sum _ {i=1}^{n} (x_i - \mu) \overset{d}{\to} N(0,\sigma^2)
$$ where $\sigma^2 = Var(x_i) = \mathbb{E}[x_i x_i&#39;]$ is necessarily
positive semidefinite.&lt;/p&gt;
&lt;h3 id=&#34;clt-proof-1&#34;&gt;CLT Proof (1)&lt;/h3&gt;
&lt;p&gt;Suppose $\lbrace x_i \rbrace$ are independent and identically
distributed random variables, each with mean $\mu$ and finite variance
$\sigma^2$. The sum $x_1 + &amp;hellip; + X_n$ has mean $n \mu$ and variance
$n \sigma^2$.&lt;/p&gt;
&lt;p&gt;Consider the random variable $$
Z_n = \frac{x_1 + &amp;hellip; + X_n - n\mu}{\sqrt{n \sigma^2}} = \sum _ {i=1}^n \frac{x_i - \mu}{\sqrt{n \sigma^2}} = \sum _ {i=1}^n \frac{1}{\sqrt{n}} \tilde x_i
$$&lt;/p&gt;
&lt;p&gt;where in the last step we defined the new random variables
$\tilde x_i = \frac{x_i - \mu}{\sigma}$ each with zero mean and unit
variance. The characteristic function of $Z_n$ is given by $$
\varphi _ {Z_n} (t) = \varphi _ { \sum _ {i=1}^n \frac{1}{\sqrt{n} } \tilde{x}_i}(t) = \varphi _ {\tilde x_1} \left( \frac{t}{\sqrt{n}} \right) \times &amp;hellip; \times \varphi _ {Y_n} \left( \frac{t}{\sqrt{n}} \right) = \left[ \varphi _ {\tilde x_1} \left( \frac{t}{\sqrt{n}} \right) \right]^n
$$&lt;/p&gt;
&lt;p&gt;where in the last step we used the fact that all of the $\tilde{x}_i$
are identically distributed.&lt;/p&gt;
&lt;h3 id=&#34;clt-proof-2&#34;&gt;CLT Proof (2)&lt;/h3&gt;
&lt;p&gt;The characteristic function of $\tilde{x}_1$ is, by Taylorâs theorem, $$
\varphi _ {\tilde{x}_1} \left( \frac{t}{\sqrt{n}} \right) = 1 - \frac{t^2}{2n} + o \left( \frac{t^2}{n} \right) \qquad \text{ for } n \to \infty
$$&lt;/p&gt;
&lt;p&gt;where $o(t^2)$ is âlittle o notationâ for some function of $t$ that goes
to zero more rapidly than $t^2$. By the limit of the exponential
function, the characteristic function of $Z_n$ equals $$
\varphi _ {Z_ n}(t) = \left[  1 - \frac{t^2}{2n} + o \left( \frac{t^2}{n} \right) \right]^n \to e^{ -\frac{1}{2}t^2 } \qquad \text{ for } n \to \infty
$$&lt;/p&gt;
&lt;p&gt;Note that all of the higher order terms vanish in the limit
$n \to \infty$. The right hand side equals the characteristic function
of a standard normal distribution $N(0,1)$, which implies through LÃ©vyâs
continuity theorem that the distribution of $Z_ n$ will approach
$N(0,1)$ as $n \to \infty$. Therefore, the sum $x_1 + &amp;hellip; + x_n$ will
approach that of the normal distribution $N(n_{\mu}, n\sigma^2)$, and
the sample average $$
\mathbb{E}_n [x_i] = \frac{1}{n} \sum _ {i=1}^n x_i
$$&lt;/p&gt;
&lt;p&gt;converges to the normal distribution $N(\mu, \sigma^2)$, from which the
central limit theorem follows. $$\tag*{$\blacksquare$}$$&lt;/p&gt;
&lt;h3 id=&#34;delta-method&#34;&gt;Delta Method&lt;/h3&gt;
&lt;p&gt;Let $\lbrace X_n \rbrace$ be a sequence of independent, identically
distributed $K \times 1$ random vectors such that&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\sqrt{n} (X_n - c) \overset{d}{\to} Z$ for some fixed
$c \in \mathbb{R}^K$&lt;/li&gt;
&lt;li&gt;and $\Sigma$ a $K \times K$ positive definite matrix.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Suppose $g : \mathbb{R}^K \to \mathbb{R}^J$ with $J \leq K$ is
continuously differentiable and full rank at $c$, then $$
\sqrt{n} \Big[ g(X_n) - g( c ) \Big] \overset{d}{\to} G Z
$$&lt;/p&gt;
&lt;p&gt;where $G = \frac{\partial g( c )}{\partial x}$ is the $J \times K$
matrix of partial derivatives evaluated at $c$.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Note that the most common utilization is with the random variable
$\mathbb E_n [x_i]$. In fact, under the assumptions of the CLT, we
have that $$
\sqrt{n} \Big[ g \big( \mathbb E_n [x_i] \big) - g(\mu) \Big] \overset{d}{\to} N(0, G \Sigma G&#39;)
$$&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;ergodic-theory&#34;&gt;Ergodic Theory&lt;/h2&gt;
&lt;h3 id=&#34;ppt&#34;&gt;PPT&lt;/h3&gt;
&lt;p&gt;Let $(\Omega, \mathcal{B}, P)$ be a probability space and
$T: \Omega \rightarrow \Omega$ a measurable map. $T$ is a &lt;strong&gt;probability
preserving transformation&lt;/strong&gt; if the probability of the pre-image of every
set is the same as the probability of the set itself,
i.e.Â $\forall G, \Pr(T^{-1}(G)) = \Pr(G)$.&lt;/p&gt;
&lt;p&gt;Let $(\Omega, \mathcal{B}, P)$ be a probability space and
$T: \Omega \rightarrow \Omega$ a PPT. A set $G \in \mathcal{B}$ is
&lt;strong&gt;invariant&lt;/strong&gt; if $T^{-1}(G)=G$.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Note that it does not have to work the other way around:
$G \neq T(G)$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Let $(\Omega, \mathcal{B}, P)$ be a probability space and
$T: \Omega \rightarrow \Omega$ a PPT. $T$ is &lt;strong&gt;ergodic&lt;/strong&gt; if every
invariant set $G \in \mathcal{B}$ has probability zero or one,
i.e.Â $\Pr(G) = 0 \lor \Pr(G) = 1$.&lt;/p&gt;
&lt;h3 id=&#34;poincarÃ¨-recurrence&#34;&gt;PoincarÃ¨ Recurrence&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Let $(\Omega, \mathcal{B}, P)$ be a probability space and
$T: \Omega \rightarrow \Omega$ a PPT. Suppose $A \in \mathcal{B}$ is
measurable. Then, for almost every $\omega \in A$, $T^n(\omega)\in A$
for infinitely many $n$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;We follow 5 steps:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Let
$G = \lbrace \omega \in A : T^K(\omega) \notin A \quad \forall k &amp;gt;0 \rbrace$:
the set of all points of A that never ``returnâ in A.&lt;/li&gt;
&lt;li&gt;Note that $\forall j \geq 1$, $T^{-j}(G) \cap G = \emptyset$. In
fact, suppose $\omega \in T^{-j}(G)$. Then $\omega \notin G$ since
otherwise we would have $\omega \in G \subseteq A$ and
$\omega \in T^J(G) \subseteq A$ which contradicts the definition of
$G$.&lt;/li&gt;
&lt;li&gt;It follows that $\forall l,n \geq 1$,
$T^{-l}(G) \cap T^{-n}(G) = \emptyset$&lt;/li&gt;
&lt;li&gt;Since $T$ is a PPT, $\Pr(T^{-j}(G)) = \Pr(G)$ $\forall j$&lt;/li&gt;
&lt;li&gt;Then $$
\Pr (T^{-1}(G) \cup T^{-2}(G) \cup &amp;hellip; \cup T^{-l}(G)) = l \cdot \Pr(G) \leq 1 \Rightarrow \Pr(G) \leq \frac{1}{l} \quad \Rightarrow \quad \lim_ {l \to \infty} \Pr(G) = 0
$$ $$\tag*{$\blacksquare$}$$&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;comment&#34;&gt;Comment&lt;/h3&gt;
&lt;p&gt;Halmos: â&lt;em&gt;The recurrence theorem says that under the appropriate
conditions on a transformation T almost every point of each measurable
set $A$ returns to $A$ infinitely often. It is natural to ask: exactly
how long a time do the images of such recurrent points spend in $A$? The
precise formulation of the problem runs as follows: given a point $x$
(for present purposes it does not matter whether $x$ is in $A$ or not),
and given a positive integer $n$, form the ratio of the number of these
points that belong to $A$ to the total number (i.e., to $n$), and
evaluate the limit of these ratios as $n$ tends to infinity. It is, of
course, not at all obvious in what sense, if any, that limit exists. If
$f$ is the characteristic function of $A$ then the ratio just discussed
is&lt;/em&gt;â $$
\frac{1}{n} \sum _ {i=1}^n f(T^{i}x) = \frac{1}{n} \sum _ {i=1}^n x_i
$$&lt;/p&gt;
&lt;h3 id=&#34;ergodic-theorem&#34;&gt;Ergodic Theorem&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Let $T$ be an ergodic PPT on $\Omega$. Let $x$ be a random variable on
$\Omega$ with $\mathbb{E}[x] &amp;lt; \infty$. Let $x_i = x \circ T^i$. Then,
$$
\frac{1}{n} \sum _ {i=1}^n x_i \overset{as}{\to} \mathbb{E}[x]
$$&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;To figure out whether a PPT is ergodic, itâs useful to draw a graph
with $T^{-1}(G)$ on the y-axis and $G$ on the x-axis.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;comment-1&#34;&gt;Comment&lt;/h3&gt;
&lt;p&gt;From the ergodic theorem, we have that $$
\lim _ {n \to \infty} \frac{1}{n} \sum _ {i=1}^n f(T^{i}x) g(x) = f^* (x)g(x) \quad \Rightarrow \quad  \lim _ {n \to \infty} \Pr(T^{-n}G \cap H) = \Pr(G)\Pr(H)
$$ where $f^* (x) = \int f(x) dx = \mathbb{E}[f]$.&lt;/p&gt;
&lt;p&gt;[Halmos]: &lt;em&gt;We have seen that if a transformation $T$ is ergodic, then
$\Pr(T^{-n}G \cap H)$ converges in the sense of Cesaro to
$\Pr(G)\Pr(H)$. The validity of this condition for all $G$ and $H$ is,
in fact, equivalent to ergodicity. To prove this, suppose that $A$ is a
measurable invariant set, and take both $G$ and $H$ equal to $A$. It
follows that $\Pr(A) = (\Pr(A))^2$, and hence that $\Pr(A)$ is either 0
or 1.&lt;/em&gt;&lt;/p&gt;
&lt;h3 id=&#34;comment-2&#34;&gt;Comment 2&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;The Cesaro convergence condition has a natural intuitive
interpretation. We may visualize the transformation $T$ as a particular
way of stirring the contents of a vessel (of total volume 1) full of an
incompressible fluid, which may be thought of as 90 per cent gin ($G$)
and 10 per cent vermouth ($H$). If $H$ is the region originally occupied
by the vermouth, then, for any part $G$ of the vessel, the relative
amount of vermouth in $G$, after $n$ repetitions of the act of stirring,
is given by $\Pr(T^{-n}G \cap H)/\Pr(H)$. The ergodicity of $T$ implies
therefore that on the average this relative amount is exactly equal to
10 per cent. In general, in physical situations like this one, one
expects to be justified in making a much stronger statement, namely
that, after the liquid has been stirred sufficiently often
($n \to \infty$), every part $G$ of the container will contain
approximately 10 per cent vermouth. In mathematical language this
expectation amounts to replacing Cesaro convergence by ordinary
convergence, i.e., to the condition
$\lim_ {n\to \infty} \Pr(T^{-n}G \cap H) = \Pr(G)\Pr(H)$. If a
transformation $T$ satisfies this condition for every pair $G$ and $H$
of measurable sets, it is called mixing, or, in distinction from a
related but slightly weaker concept, strongly mixing.&lt;/em&gt;â&lt;/p&gt;
&lt;h3 id=&#34;mixing&#34;&gt;Mixing&lt;/h3&gt;
&lt;p&gt;Let $\lbrace\Omega, \mathcal{B}, P \rbrace$ be a probability space. Let
$T$ be a probability preserving transform. Then $T$ is &lt;strong&gt;strongly
mixing&lt;/strong&gt; if for every invariant sets $G$,$H \in \mathcal{B}$ $$
P(G \cap T^{-k}H) \to P(G)P(H) \quad \text{ as } k \to \infty
$$ where $T^{-k}H$ is defined as
$T^{-k}H = T^{-1}(&amp;hellip;T^{-1}(T^{-1} H)&amp;hellip;)$ repeated $k$ times.&lt;/p&gt;
&lt;p&gt;Let $\lbrace X_i\rbrace _ {i=-\infty}^{\infty}$ be a two sided sequence
of random variables. Let $\mathcal{B}_ {-\infty}^n$ be the sigma algebra
generated by $\lbrace X_i\rbrace _ {i=-\infty}^{n}$ and
$\mathcal{B}_ {n+k}^\infty$ the sigma algebra generated by
$\lbrace X_i \rbrace _ {i=n+k}^{\infty}$. Define the mixing coefficient
$$
\alpha(k) = \sup_ {n \in \mathbb{Z}} \sup_ {G \in \mathcal{B}_ {-\infty}^n} \sup_ {H \in \mathcal{B}_ {n+k}^\infty} | \Pr(G \cap H) - \Pr(G) \Pr(H)|
$$ $\lbrace X_i \rbrace$ is $\mathbb{\alpha}$&lt;strong&gt;-mixing&lt;/strong&gt; if
$\alpha(k) \to 0$ if $k \to \infty$.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Note that mixing implies ergodicity.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;stationarity&#34;&gt;Stationarity&lt;/h3&gt;
&lt;p&gt;Let $X_i : \Omega \to \mathbb{R}$ be a (two sided) sequence of random
variables with $i \in \mathbb{Z}$. $X_i$ is &lt;strong&gt;strongly stationary&lt;/strong&gt; or
simply stationary if $$
\Pr (X _ {i_ 1} \leq a_ 1 , &amp;hellip; , X _ {i_ k} \leq a_ k ) = \Pr (X _ { i _ {1-s}} \leq a_ 1 , &amp;hellip; , X _ {i _ {k-s}} \leq a_ k)  \quad \text{ for every } i_ 1, &amp;hellip;, i_ k, a_ 1, &amp;hellip;, a_ k, s \in \mathbb{R}.
$$&lt;/p&gt;
&lt;p&gt;Let $X_i : \Omega \to \mathbb{R}$ be a (two sided) sequence of random
variables with $i \in \mathbb{Z}$. $X_i$ is &lt;strong&gt;covariance stationary&lt;/strong&gt; if
$\mathbb{E}[X_i] = \mathbb{E}[X_j]$ for every $i,j$ and
$\mathbb{E}[X_i X_j] = \mathbb{E}[X _ {i+k} X _ {j+k}]$ for all $i,j,k$.
All of the second moments above are assumed to exist.&lt;/p&gt;
&lt;p&gt;Let $X_t : \Omega \to \mathbb{R}$ be a sequence of random variables
indexed by $t \in \mathbb{Z}$ such that $\mathbb{E}[|X_t|] &amp;lt; 1$ for each
$t$. $X_t$ is a &lt;strong&gt;martingale&lt;/strong&gt; if
$\mathbb{E} [X _ t |X _ {t-1} , X _ {t-2} , &amp;hellip;] = X _ t$. $X_t$ is a
&lt;strong&gt;martingale difference&lt;/strong&gt; if
$\mathbb{E} [X _ t | X _ {t-1} , X _ {t-2} ,&amp;hellip;] = 0$.&lt;/p&gt;
&lt;h3 id=&#34;gordins-central-limit-theorem&#34;&gt;Gordinâs Central Limit Theorem&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Let $\lbrace z_i \rbrace$ be a stationary, $\alpha$-mixing sequence of
random variables. If moreover&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\sum_ {m=1}^\infty \alpha(m)^{\frac{\delta}{2 + \delta}} &amp;lt; \infty$&lt;/li&gt;
&lt;li&gt;$\mathbb{E}[z_i] = 0$&lt;/li&gt;
&lt;li&gt;$\mathbb{E}\Big[ ||z_i || ^ {2+\delta} \Big] &amp;lt; \infty$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Then $$
\sqrt{n} \mathbb{E}_n [z_i] \overset{d}{\to} N(0,\Omega) \quad  \text{ where } \quad  \Omega = \lim _ {n \to \infty} Var(\sqrt{n} \mathbb{E}_n [z_i])
$$&lt;/p&gt;
&lt;p&gt;Let $\Omega_k = \mathbb{E}[ z_i z _ {i+k}&#39;]$. Then a necessary condition
for Gordinâs CLT is covariance summability:
$\sum _ {k=1}^\infty \Omega_k &amp;lt; \infty$.&lt;/p&gt;
&lt;h3 id=&#34;ergodic-central-limit-theorem&#34;&gt;Ergodic Central Limit Theorem&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Let $\lbrace z_i \rbrace$ be a stationary, ergodic, martingale
difference sequence. Then $$
\sqrt{n} \mathbb{E}_n [z_i] \overset{d}{\to} N(0,\Omega) \quad  \text{ where } \quad  \Omega = \lim _ {n \to \infty} Var(\sqrt{n}\mathbb{E}_n[z_i])
$$&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Inference</title>
      <link>https://matteocourthoud.github.io/course/metrics/04_inference/</link>
      <pubDate>Fri, 29 Oct 2021 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/course/metrics/04_inference/</guid>
      <description>&lt;h2 id=&#34;statistical-models&#34;&gt;Statistical Models&lt;/h2&gt;
&lt;h3 id=&#34;definition&#34;&gt;Definition&lt;/h3&gt;
&lt;p&gt;A &lt;strong&gt;statistical model&lt;/strong&gt; is a set of probability distributions
$\lbrace P \rbrace$.&lt;/p&gt;
&lt;p&gt;More precisely, a &lt;strong&gt;statistical model over data&lt;/strong&gt; $D \in \mathcal{D}$ is
a set of probability distribution over datasets $D$ which takes values
in $\mathcal{D}$.&lt;/p&gt;
&lt;p&gt;Suppose you have regression data $\lbrace x_i , y_i \rbrace _ {i=1}^N$
with $x_i \in \mathbb{R}^p$ and $y_i \in \mathbb{R}$. The statistical
model is&lt;/p&gt;
&lt;p&gt;$$
\Big\lbrace   P : y_i = f(x_i) + \varepsilon_i, \ x_i \sim F_x , \ \varepsilon_i \sim F _\varepsilon , \ \varepsilon_i \perp x_i , \ f \in C^2 (\mathbb{R}^p) \Big\rbrace
$$&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;In words&lt;/strong&gt;: the statistical model is the set of distributions $P$
such that an additive decomposition of $y_i$ as
$f(x_i) + \varepsilon_i$ exists for some $x_i$; where $f$ is twice
continuously differentiable.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;A &lt;strong&gt;data generating process&lt;/strong&gt; (DGP) is a single statistical distribution
over&lt;/p&gt;
&lt;h3 id=&#34;parametrization&#34;&gt;Parametrization&lt;/h3&gt;
&lt;p&gt;A statistical model parameterized by $\theta \in \Theta$ is &lt;strong&gt;well
specified&lt;/strong&gt; if the data generating process corresponds to some
$\theta_0$ and $\theta_0 \in \Theta$. Otherwise, the statistical model
is &lt;strong&gt;misspecified&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;A statistical model can be parametrized as
$\mathcal{F} = \lbrace P_\theta \rbrace _ {\lbrace \theta \in \Theta \rbrace }$.&lt;/p&gt;
&lt;p&gt;We can divide statistical models into 3 classes&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;h2 id=&#34;parametric-the-stochastic-features-of-the-model-are-completly-specified-up-to-a-finite-dimensional-parameter-lbrace-p_theta-rbrace-_--lbrace-theta-in-theta-rbrace--with-theta-subseteq-mathbbrk-kinfty&#34;&gt;&lt;strong&gt;Parametric&lt;/strong&gt;: the stochastic features of the model are completly specified up to a finite dimensional parameter: $\lbrace P_\theta \rbrace _ { \lbrace \theta \in \Theta \rbrace }$ with $\Theta \subseteq \mathbb{R}^k, k&amp;lt;\infty$;&lt;/h2&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Semiparametric&lt;/strong&gt;: it is a partially specified model, e.g.,
$\lbrace P_\theta \rbrace _ { \lbrace \theta \in \Theta, \gamma \in \Gamma \rbrace }$
with $\Theta$ of finite dimension and $\Gamma$ of infinite
dimension;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Non parametric&lt;/strong&gt;: there is no finite dimensional component of the
model.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;estimation&#34;&gt;Estimation&lt;/h3&gt;
&lt;p&gt;Let $\mathcal{D}$ be the set of possible data realizations. Let
$D \in \mathcal{D}$ be your data. Let $\mathcal{F}$ be a statistical
model indexed by some parameter $\theta \in \Theta$. An &lt;strong&gt;estimator&lt;/strong&gt; is
a map $$
\mathcal{D} \to \mathcal{F} \quad , \quad  D \mapsto \hat{\theta}
$$&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;In words&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;An estimator is a map from the set of data realizations to the set
of statistical models.&lt;/li&gt;
&lt;li&gt;It takes as inputs a dataset $D$ and outputs a parameter estimate
$\hat \theta$.&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;inference&#34;&gt;Inference&lt;/h3&gt;
&lt;p&gt;Let $\alpha &amp;gt; 0$ be a small tolerance. Statistical &lt;strong&gt;inference&lt;/strong&gt; is a
map into subsets of $\mathcal{F}$ given by $$
\mathcal{D} \to \mathcal{G} \subseteq \mathcal{F}: \min _ \theta P_\theta (\mathcal{G} | \theta \in \mathcal{G}) \geq 1-\alpha
$$&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;In words&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Inference maps datasets into sets of models&lt;/li&gt;
&lt;li&gt;The set contains only models that generate the observed data with
high probability&lt;/li&gt;
&lt;li&gt;I.e. at least $1-\alpha$&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;hypotesis-testing&#34;&gt;Hypotesis Testing&lt;/h2&gt;
&lt;h3 id=&#34;hypothesis&#34;&gt;Hypothesis&lt;/h3&gt;
&lt;p&gt;A &lt;strong&gt;statistical hypothesis&lt;/strong&gt; $H_0$, is a subset of a statistical model,
$\mathcal K \subset \mathcal F$.&lt;/p&gt;
&lt;p&gt;If $\mathcal F$ is the statistical model and $\mathcal K$ is the
statistical hypothesis, we use the notation $H_0 : P \in \mathcal K$.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Example&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Common hypothesis are&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A single coefficient being equal to zero,
$\beta_k = c \in \mathbb R$&lt;/li&gt;
&lt;li&gt;Multiple linear combination of coefficients being equal to some
values: $\boldsymbol R&#39; \beta = r \in \mathbb R^p$&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;test&#34;&gt;Test&lt;/h3&gt;
&lt;p&gt;A &lt;strong&gt;hypothesis test&lt;/strong&gt; $T$ is a map from the space of datasets to a
decision, rejection (0) or acceptance (1) $$
\mathcal D \to \lbrace 0, 1 \rbrace \quad, \quad D \mapsto T
$$&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Generally, we are interested in understanding whether it is likely
that data $D$ are drawn from a model $\mathcal K$ or not.&lt;/p&gt;
&lt;p&gt;A hypothesis test, $T$ is our tool for deciding whether the hypothesis
is consistent with the data.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$T(D) = 0 \to$ fail to reject $H_0$ and test inconclusive&lt;/li&gt;
&lt;li&gt;$T (D) = 1 \to$ reject $H_0$ and D is inconsistent with any
$P \in \mathcal K$&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;errors&#34;&gt;Errors&lt;/h3&gt;
&lt;p&gt;Let $\mathcal K \subset \mathcal F$ be a statistical hypothesis and $T$
a hypothesis test.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;A &lt;strong&gt;Type I error&lt;/strong&gt; is an event $T(D)=1$ under $P \in \mathcal K$.
&lt;ul&gt;
&lt;li&gt;In words: rejecting the null hypothesis, when it is is true&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;A &lt;strong&gt;Type II error&lt;/strong&gt; is an event $T(D)=0$ under $P \in \mathcal K^C$.
&lt;ul&gt;
&lt;li&gt;In words: not rejecting the null hypothesis, when it is false&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The corresponding probability of a type I error is called &lt;strong&gt;size&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;The corresponding probability of a type II error is called &lt;strong&gt;power&lt;/strong&gt;
(against the alternative P).&lt;/p&gt;
&lt;h3 id=&#34;type-i-error-and-test-size&#34;&gt;Type I Error and Test Size&lt;/h3&gt;
&lt;p&gt;Test &lt;strong&gt;size&lt;/strong&gt; is the probability of a Type I error, i.e. $$
\Pr \Big[ \text{ Reject } H_0 \Big| H_0 \text{ is true } \Big] = \Pr \Big[ T(D)=1 \Big| P \in \mathcal K \Big]
$$ A primary goal of test construction is to limit the incidence of Type
I error by bounding the size of the test.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;In the dominant approach to hypothesis testing the researcher
pre-selects a &lt;strong&gt;significance level&lt;/strong&gt; $\alpha \in (0,1)$ and then
selects the test so that its size is no larger than $\alpha$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;type-ii-error-and-power&#34;&gt;Type II Error and Power&lt;/h3&gt;
&lt;p&gt;Test &lt;strong&gt;power&lt;/strong&gt; is the probability of a Type II error, i.e. $$
\Pr \Big[ \text{ Not Reject } H_0 \Big| H_0 \text{ is false } \Big] = \Pr \Big[ T(D)=0 \Big| P \in \mathcal K^C \Big]
$$&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;In the dominant approach to hypothesis testing the goal of test
construction is to have high power subject to the constraint that the
size of the test is lower than the pre-specified significance level.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;statistical-significance&#34;&gt;Statistical Significance&lt;/h3&gt;
&lt;p&gt;TBD&lt;/p&gt;
&lt;h3 id=&#34;p-values&#34;&gt;P-Values&lt;/h3&gt;
&lt;h3 id=&#34;recap&#34;&gt;Recap&lt;/h3&gt;
&lt;p&gt;We now summarize the main features of hypothesis testing.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Select a significance level $\alpha$.&lt;/li&gt;
&lt;li&gt;Select a test statistic $T$ with asymptotic distribution $T\to \xi$
under $H_0$.&lt;/li&gt;
&lt;li&gt;Set the asymptotic critical value $c$ so that 1â&lt;em&gt;G&lt;/em&gt;(&lt;em&gt;c&lt;/em&gt;)=Î±, where
&lt;em&gt;G&lt;/em&gt; is the distribution function of $\xi$.&lt;/li&gt;
&lt;li&gt;Calculate the asymptotic p-value &lt;em&gt;p&lt;/em&gt;=1â&lt;em&gt;G&lt;/em&gt;(&lt;em&gt;T&lt;/em&gt;).&lt;/li&gt;
&lt;li&gt;Reject $H_0$ if &lt;em&gt;T&lt;/em&gt; &amp;gt; &lt;em&gt;c&lt;/em&gt;, or equivalently &lt;em&gt;p&lt;/em&gt; &amp;lt; Î±.&lt;/li&gt;
&lt;li&gt;Accept $H_0$ if &lt;em&gt;T&lt;/em&gt; â¤ &lt;em&gt;c&lt;/em&gt;, or equivalently &lt;em&gt;p&lt;/em&gt; â¥ Î±.&lt;/li&gt;
&lt;li&gt;Report $p$ to summarize the evidence concerning $H_0$ versus $H_1$.&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;examples&#34;&gt;Examples&lt;/h3&gt;
&lt;p&gt;Letâs focus two hypotheses:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;$\beta_k = c \in \mathbb R$&lt;/li&gt;
&lt;li&gt;$\boldsymbol R&#39; \beta = r \in \mathbb R^p$&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;t-test-with-known-variance&#34;&gt;t-test with Known Variance&lt;/h3&gt;
&lt;p&gt;Consider the testing problem $H_0 : \beta_k = c$, where $c$ is a
pre-specified value under the null. Suppose the variance of the esimator
$\hat \beta_k$ is &lt;strong&gt;known&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;The t-statistic for this problem is defined by $$
n_{k}:=\frac{\hat \beta_{k} - c}{\sigma_{\hat \beta_{k}}}
$$ In the testing procedure above, the sampling distribution under the
null $H_0$ is given by $$
n_k \sim N(0,1)
$$ Where $N(0,1)$ the standard normal distribution.&lt;/p&gt;
&lt;h3 id=&#34;t-test-with-unknown-variance&#34;&gt;t-test with Unknown Variance&lt;/h3&gt;
&lt;p&gt;Consider the testing problem $H_0 : \beta_k = c$, where $c$ is a
presepecified value under the null. In case the variance of the
estimator $\hat \beta_k$ is &lt;strong&gt;not known&lt;/strong&gt;, we have to replace it with a
consistent estimate $\hat \sigma^2_{\hat \beta}$&lt;/p&gt;
&lt;p&gt;The t-statistic for this problem is defined by $$
t_{k}:=\frac{\hat \beta_{k} - c}{\hat \sigma_{\hat \beta_{k}}}
$$ In the testing procedure above, the sampling distribution under the
null $H_0$ is given by $$
t_k \sim t_{n-K}
$$ Where $t_{n-K}$ denotes the t-distribution with $n-K$ degress of
freedom.&lt;/p&gt;
&lt;h3 id=&#34;wald-test&#34;&gt;Wald-test&lt;/h3&gt;
&lt;p&gt;Consider the testing problem $\boldsymbol R&#39; \beta = r$, where
$\boldsymbol R \in \mathbb R^{p+K}$ is a pre-specified set of linear
combinations and $r \in \mathbb R^p$ is a restriction vector. Suppose
the variance of the esimator $\hat \beta$ is &lt;strong&gt;known&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;The Wald statistic for this problem is given by $$
W := \frac{(R \hat \beta-r)^{\prime}(R \hat \beta-r) }{R&#39; \sigma^{2}&lt;em&gt;{\hat \beta} R}
$$ In the testing procedure above, the sampling distribution under the
null $H_0$ is given by $$
W \sim \chi^2&lt;/em&gt;{n-K}
$$ Where $\chi^2_{n-K}$ denotes the chi-squared distribution with $n-K$
degress of freedom.&lt;/p&gt;
&lt;h3 id=&#34;comments-on-the-wald-test&#34;&gt;Comments on the Wald test&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;The Wald statistic $W$ is a weighted Euclidean measure of the length
of the vector $R \hat \beta-r$&lt;/li&gt;
&lt;li&gt;The Wald test is intrinsecally 2-sided&lt;/li&gt;
&lt;li&gt;When $p=1$ then $W = |T|$ , the square of the t-statistic, so
hypothesis tests based on $W$ and $|T|$ are equivalent.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;f-test&#34;&gt;F-test&lt;/h3&gt;
&lt;p&gt;Consider the testing problem $\boldsymbol R&#39; \beta = r$, where
$\boldsymbol R \in \mathbb R^{p+K}$ is a pre-specified set of linear
combinations and $r \in \mathbb R^p$ is a restriction vector. In case
the variance of the estimator $\hat \beta$ is &lt;strong&gt;not known&lt;/strong&gt;, we have to
replace it with a consistent estimate $\hat \sigma^2_{\hat \beta}$.&lt;/p&gt;
&lt;p&gt;The F-statistic for this problem is given by $$
F := \frac{(R \hat \beta-r)^{\prime}(R \hat \beta-r) / p }{R&#39; \hat \sigma^{2} _ {\hat \beta} R}
$$ In the testing procedure above, the sampling distribution under the
null $H_0$ is given by $$
F \sim F_{p, n-K}
$$ Where $F_{p, n-K}$ denotes the F-distribution with $n-K$ degress of
freedom, with $p$ restrictions.&lt;/p&gt;
&lt;h3 id=&#34;f-test-equivalence&#34;&gt;F-test Equivalence&lt;/h3&gt;
&lt;p&gt;Consider the testing problem $\boldsymbol R&#39; \beta = r$, where
$\boldsymbol R \in \mathbb R^{p+K}$ is a pre-specified set of linear
combinations and $r \in \mathbb R^p$ is a restriction vector. Consider
two estimators&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\hat \beta_U = \arg \min_b \frac{1}{n} (y - X \beta)&#39; (y - X\beta)$&lt;/li&gt;
&lt;li&gt;$\hat \beta_R = \arg \min_{b : \boldsymbol R&#39; \beta = r} \frac{1}{n} (y - X \beta)&#39; (y - X\beta)$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Then the F statistic is numerically equivalent to the following
expression $$
F = \frac{\left(S S R_{R}-S S R_{U}\right) / p}{S S R_{U} /(n-K)}
$$ where SSR is the sum of squared residuals.&lt;/p&gt;
&lt;h3 id=&#34;confidence-intervals&#34;&gt;Confidence Intervals&lt;/h3&gt;
&lt;p&gt;TBD&lt;/p&gt;
&lt;h3 id=&#34;minimum-distance-tests&#34;&gt;Minimum Distance Tests&lt;/h3&gt;
&lt;p&gt;TBD&lt;/p&gt;
&lt;h2 id=&#34;asymptotics&#34;&gt;Asymptotics&lt;/h2&gt;
&lt;h3 id=&#34;estimator-properties&#34;&gt;Estimator Properties&lt;/h3&gt;
&lt;p&gt;Given a sequence of well specified data generating processes
$\mathcal F_n$, each indexed by the same parameter space $\Theta$, with
$\theta_0$ a component of the true parameter for each $n$.&lt;/p&gt;
&lt;p&gt;Then estimator $\hat \theta$ is&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;unbiased&lt;/strong&gt; if $\mathbb E [\hat \theta] = \theta_0$&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;consistent&lt;/strong&gt; if $\hat \theta \overset{p}{\to} \theta_0$&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;asymptotically normal&lt;/strong&gt;
$\sqrt{n} (\hat \theta - \theta_0) \overset{d}{\to} N(0, V)$ for
some positive definite $V$&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;test-consistency&#34;&gt;Test Consistency&lt;/h3&gt;
&lt;p&gt;The &lt;strong&gt;asymptotic size&lt;/strong&gt; of a testing procedure is defined as the
limiting probability of rejecting $H_0$ when $H_0$ is true.
Mathematically, we can write this as
$\lim _ {n \to \infty} \Pr_n ( \text{reject } H_0 | H_0)$, where the $n$
subscript indexes the sample size.&lt;/p&gt;
&lt;p&gt;A test is said to be &lt;strong&gt;consistent&lt;/strong&gt; against the alternative $H_1$ if the
null hypothesis is rejected with probability approaching $1$ when $H_1$
is true:
$\lim _ {N \to \infty} \Pr_N (\text{reject } H_0 | H_1) \overset{p}{\to} 1$.&lt;/p&gt;
&lt;h3 id=&#34;convergence&#34;&gt;Convergence&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt;: Suppose that
$\sqrt{n}(\hat{\theta} - \theta_0) \overset{d}{\to} N(0, V)$, where $V$
is positive definite. Then for any non-stochastic $Q\times P$ matrix
$R$, $Q \leq P$, with rank$( R ) = Q$ $$
\sqrt{n} R (\hat{\theta} - \theta_0) \sim N(0, R VR&#39;)
$$ and $$
[\sqrt{n}R(\hat{\theta} - \theta_0)]&#39;[RVR&#39;]^{-1}[\sqrt{n}R(\hat{\theta} - \theta_0)] \overset{d}{\to} \chi^2_Q
$$ In addition, if $\text{plim} \hat{V} _n = V$, then $$
(\hat{\theta} - \theta_0)&#39; R&#39;[R (\hat{V} _n/n) R&#39;]^{-1}R (\hat{\theta} - \theta_0) \overset{d}{\to} \chi^2_Q
$$&lt;/p&gt;
&lt;h3 id=&#34;wald-statistic&#34;&gt;Wald Statistic&lt;/h3&gt;
&lt;p&gt;For testing the null hypothesis $H_0: R\theta_0 = r$, where $r$ is a
$Q\times1$ random vector, define the &lt;strong&gt;Wald statistic&lt;/strong&gt; for testing
$H_0$ against $H_1 : R\theta_0 \neq r$ as $$
W_n = (R\hat{\theta} - r)&#39;[R (\hat{V} _n/n) R&#39;]^{-1} (R\hat{\theta} - r)
$$ Under $H_0$, $W_n \overset{d}{\to} \chi^2_Q$. If we abuse the
asymptotics and we treat $\hat{\theta}$ as being distributed as Normal
we get the equation exactly.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>OLS Algebra</title>
      <link>https://matteocourthoud.github.io/course/metrics/05_ols_algebra/</link>
      <pubDate>Fri, 29 Oct 2021 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/course/metrics/05_ols_algebra/</guid>
      <description>&lt;h2 id=&#34;the-gauss-markov-model&#34;&gt;The Gauss Markov Model&lt;/h2&gt;
&lt;h3 id=&#34;definition&#34;&gt;Definition&lt;/h3&gt;
&lt;p&gt;A statistical model for regression data is the &lt;strong&gt;Gauss Markov Model&lt;/strong&gt; if
each of its distributions satisfies the conditions&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Linearity&lt;/strong&gt;: a statistical model $\mathcal{F}$ over data
$\mathcal{D}$ satisfies linearity if for each element of
$\mathcal{F}$, the data can be decomposed in $$
\begin{aligned}
y_ i &amp;amp;= \beta_ 1 x _ {i1} + \dots + \beta_ k x _ {ik} + \varepsilon_ i = x_ i&#39;\beta + \varepsilon_ i \newline
\underset{n \times 1}{\vphantom{\beta_ \beta} y} &amp;amp;= \underset{n \times k}{\vphantom{\beta}X} \cdot \underset{k \times 1}{\beta} + \underset{n \times 1}{\vphantom{\beta}\varepsilon}
\end{aligned}
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Strict Exogeneity&lt;/strong&gt;:
$\mathbb E [\varepsilon_i|x_1, \dots, x_n] = 0, \forall i$.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;No Multicollinerity&lt;/strong&gt;: $\mathbb E_n [x_i x_i&#39;]$ is strictly
positive definite almost surely. Equivalent to require $rank(X)=k$
with probability $p \to 1$. Intuition: no regressor is a linear
combination of other regressors.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Spherical Error Variance&lt;/strong&gt;:
-$\mathbb E[\varepsilon_i^2 | x] = \sigma^2 &amp;gt; 0, \ \forall i$
-$\mathbb E [\varepsilon_i \varepsilon_j |x ] = 0, \ \forall$
$1 \leq i &amp;lt; j \leq n$&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The &lt;strong&gt;Extended Gauss Markov Model&lt;/strong&gt; also satisfies assumption&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Normal error term&lt;/strong&gt;: $\varepsilon|X \sim N(0, \sigma^2 I_n)$ and
$\varepsilon \perp X$.&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;implications&#34;&gt;Implications&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Note that by (2) and (4) you get &lt;strong&gt;homoskedasticity&lt;/strong&gt;:&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$
Var(\varepsilon_i|x) = \mathbb E[\varepsilon_i^2|x]- \mathbb E[\varepsilon_i|x]^2 = \sigma^2 I \qquad \forall i
$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Strict exogeneity is not restrictive since it is sufficient to
include a constant in the regression to enforce it $$
y_i = \alpha + x_i&#39;\beta + (\varepsilon_i - \alpha) \quad \Rightarrow \quad \mathbb E[\varepsilon_i] = \mathbb E_x [ \mathbb E[ \varepsilon_i | x]] = 0
$$&lt;/li&gt;
&lt;li&gt;This implies $\mathbb E[x _ {jk} \varepsilon_i ] = 0$ by the LIE.&lt;/li&gt;
&lt;li&gt;These two conditions together imply
$Cov (x _ {jk} \varepsilon_i ) = 0$.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;projection&#34;&gt;Projection&lt;/h3&gt;
&lt;p&gt;A map $\Pi: V \to V$ is a &lt;strong&gt;projection&lt;/strong&gt; if $\Pi \circ \Pi = \Pi$.&lt;/p&gt;
&lt;p&gt;The Gauss Markov Model assumes that the &lt;strong&gt;conditional expectation
function (CEF)&lt;/strong&gt; $f(X) = \mathbb E[Y|X]$ and the &lt;strong&gt;linear projection&lt;/strong&gt;
$g(X) = X \beta$ coincide.&lt;/p&gt;
&lt;h3 id=&#34;code---dgp&#34;&gt;Code - DGP&lt;/h3&gt;
&lt;p&gt;This code draws 100 observations from the model
$y = 2 x_1 - x_2 + \varepsilon$ where $x_1, x_2 \sim U[0,1]$ and
$\varepsilon \sim N(0,1)$.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# Set seed
Random.seed!(123);

# Set the number of observations
n = 100;

# Set the dimension of X
k = 2;

# Draw a sample of explanatory variables
X = rand(Uniform(0,1), n, k);

# Draw the error term
Ï = 1;
Îµ = rand(Normal(0,1), n, 1) * sqrt(Ï);

# Set the parameters
Î² = [2; -1];

# Calculate the dependent variable
y = X*Î² + Îµ;
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;the-ols-estimator&#34;&gt;The OLS estimator&lt;/h2&gt;
&lt;h3 id=&#34;definition-1&#34;&gt;Definition&lt;/h3&gt;
&lt;p&gt;The &lt;strong&gt;sum of squared residuals (SSR)&lt;/strong&gt; is given by $$
Q_n (\beta) \equiv   \frac{1}{n} \sum _ {i=1}^n \left( y_i - x_i&#39;\beta \right)^2 = \frac{1}{n} (y - X\beta)&#39; (y - X \beta)
$$&lt;/p&gt;
&lt;p&gt;Consider a dataset $\mathcal{D}$ and define
$Q_n(\beta) = \mathbb E_n[(y_i - x_i&#39;\beta )^2 ]$. Then the &lt;strong&gt;ordinary
least squares (OLS)&lt;/strong&gt; estimator $\hat \beta _ {OLS}$ is the value of
$\beta$ that minimizes $Q_n(\beta)$.&lt;/p&gt;
&lt;p&gt;When we can write $D = (y, X)$ in matrix form, then $$
\hat \beta _ {OLS} = \arg \min_\beta \frac{1}{n} (y - X \beta)&#39; (y - X\beta)
$$&lt;/p&gt;
&lt;h3 id=&#34;derivation&#34;&gt;Derivation&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Under the assumption that $X$ has full rank, the OLS estimator is unique
and it is determined by the normal equations. More explicitly,
$\hat \beta$ is the OLS estimate precisely when $X&amp;rsquo;X \hat \beta = X&amp;rsquo;y$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Taking the FOC: $$
\frac{\partial Q_n (\beta)}{\partial \beta} = -\frac{2}{n} X&#39; y  + \frac{2}{n} X&amp;rsquo;X\beta = 0 \quad \Leftrightarrow \quad X&amp;rsquo;X \beta = X&amp;rsquo;y
$$ Since $(X&amp;rsquo;X)^{-1}$ exists by assumption,&lt;/p&gt;
&lt;p&gt;Finally,
$\frac{\partial^2 Q_n (\beta)}{\partial \beta \partial \beta&#39;} = X&amp;rsquo;X/n$
is positive definite since $X&amp;rsquo;X$ is positive semi-definite and
$(X&amp;rsquo;X)^{-1}$ exists because $X$ is full rank. Therefore, $Q_n(\beta)$
minimized at $\hat \beta_n$. $$\tag*{$\blacksquare$}$$&lt;/p&gt;
&lt;p&gt;The $k$ equations $X&amp;rsquo;X \hat \beta = X&amp;rsquo;y$ are called &lt;strong&gt;normal
equations&lt;/strong&gt;.&lt;/p&gt;
&lt;h3 id=&#34;futher-objects&#34;&gt;Futher Objects&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Fitted coefficient:
$\hat \beta _ {OLS} = (X&amp;rsquo;X)^{-1} X&amp;rsquo;y = \mathbb E_n [x_i x_i&#39;] \mathbb E_n [x_i y_i]$&lt;/li&gt;
&lt;li&gt;Fitted residual: $\hat \varepsilon_i = y_i - x_i&#39;\hat \beta$&lt;/li&gt;
&lt;li&gt;Fitted value: $\hat y_i = x_i&#39; \hat \beta$&lt;/li&gt;
&lt;li&gt;Predicted coefficient:
$\hat \beta _ {-i} = \mathbb E_n [x _ {-i} x&#39; _ {-i}] \mathbb E_n [x _ {-i} y _ {-i}]$&lt;/li&gt;
&lt;li&gt;Prediction error:
$\hat \varepsilon _ {-i} = y_i - x_i&#39;\hat \beta _ {-i}$&lt;/li&gt;
&lt;li&gt;Predicted value: $\hat y_i = x_i&#39; \hat \beta _ {-i}$&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;notes-on-orthogonality-conditions&#34;&gt;Notes on Orthogonality Conditions&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;The normal equations are equivalent to the moment condition
$\mathbb E_n [x_i \varepsilon_i]= 0$.&lt;/li&gt;
&lt;li&gt;The algebraic result $\mathbb E_n [x_i \hat \varepsilon_i]= 0$ is
called &lt;strong&gt;ortogonality property&lt;/strong&gt; of the OLS residual
$\hat \varepsilon_i$.&lt;/li&gt;
&lt;li&gt;If we have included a constant in the regression,
$\mathbb E_n [\hat \varepsilon_i] = 0$.&lt;/li&gt;
&lt;li&gt;$\mathbb E \Big[\mathbb E_n [x_i \varepsilon_i ] \Big] = 0$ by
strict exogeneity (assumed in GM), but
$\mathbb E_n [x_i \varepsilon_i] \ne \mathbb E [x_i \varepsilon_i] = 0$.
This is why $\hat \beta _ {OLS}$ is just an estimate of $\beta_0$.&lt;/li&gt;
&lt;li&gt;Calculating OLS is like replacing the $j$ equations
$\mathbb E [x _ {ij} \varepsilon_i] = 0$ $\forall j$ with
$\mathbb E_n [x _ {ij} \varepsilon_i] = 0$ $\forall j$ and forcing
them to hold (remindful of GMM).&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;the-projection-matrix&#34;&gt;The Projection Matrix&lt;/h3&gt;
&lt;p&gt;The &lt;strong&gt;projection matrix&lt;/strong&gt; is given by $P = X(X&amp;rsquo;X)^{-1} X&#39;$. It has the
following properties: - $PX = X$ - $P \hat \varepsilon = 0 \quad$ ($P$,
$\varepsilon$ orthogonal) -
$P y = X(X&amp;rsquo;X)^{-1} X&amp;rsquo;y = X\hat \beta = \hat y$ - Symmetric: $P=P&#39;$,
Idempotent: $PP = P$ -
$tr(P) = tr( X(X&amp;rsquo;X)^{-1} X&#39;) = tr( X&amp;rsquo;X(X&amp;rsquo;X)^{-1}) = tr(I_k) = k$ - Its
diagonal elements are $h_{ii} = x_i (X&amp;rsquo;X)^{-1} x_i&#39;$ and are called
&lt;strong&gt;leverage&lt;/strong&gt;.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;$h _ {ii} \in [0,1]$ is a normalized length of the observed regressor
vector $x_i$. In the OLS regression framework it captures the relative
influence of observation $i$ on the estimated coefficient. Note that
$\sum _ n h_{ii} = k$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;the-annihilator-matrix&#34;&gt;The Annihilator Matrix&lt;/h3&gt;
&lt;p&gt;The &lt;strong&gt;annihilator matrix&lt;/strong&gt; is given by $M = I_n - P$. It has the
following properties: - $MX = 0 \quad$ ($M$, $X$ orthogonal) -
$M \hat \varepsilon = \hat \varepsilon$ - $M y = \hat \varepsilon$ -
Symmetric: $M=M&#39;$, idempotent: $MM = M$ - $tr(M) = n - k$ - Its diagonal
elements are $1 - h_{ii} \in [0,1]$&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Then we can equivalently write $\hat y$ (defined by stacking
$\hat y_i$ into a vector) as $\hat y = Py$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;estimating-beta&#34;&gt;Estimating Beta&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# Estimate beta
Î²_hat = inv(X&#39;*X)*(X&#39;*y)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 2Ã1 Array{Float64,2}:
##   1.8821600407711814
##  -0.9429354944506099
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# Equivalent but faster formulation
Î²_hat = (X&#39;*X)\(X&#39;*y)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 2Ã1 Array{Float64,2}:
##   1.8821600407711816
##  -0.9429354944506098
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# Even faster (but less intuitive) formulation
Î²_hat = X\y
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 2Ã1 Array{Float64,2}:
##   1.8821600407711807
##  -0.9429354944506088
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;equivalent-formulation&#34;&gt;Equivalent Formulation?&lt;/h3&gt;
&lt;p&gt;Generally itâs not true that $$
\hat \beta_{OLS} = \frac{Var(X)}{Cov(X,y)}
$$&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# Wrong formulation
Î²_wrong = inv(cov(X)) * cov(X, y)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 2Ã1 Array{Float64,2}:
##   1.8490257777704475
##  -0.9709213554007003
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;equivalent-formulation-correct&#34;&gt;Equivalent Formulation (correct)&lt;/h3&gt;
&lt;p&gt;But itâs true if you include a constant, $\alpha$ $$
y = \alpha + X \beta  + \varepsilon
$$&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# Correct, with constant
Î± = 3;
y1 = Î± .+ X*Î² + Îµ;
Î²_hat1 = [ones(n,1) X] \ y1
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 3Ã1 Array{Float64,2}:
##   3.0362313477745615
##   1.8490257777704477
##  -0.9709213554007007
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;Î²_correct1 = inv(cov(X)) * cov(X, y1)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 2Ã1 Array{Float64,2}:
##   1.8490257777704477
##  -0.9709213554007006
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;some-more-objects&#34;&gt;Some More Objects&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# Predicted y
y_hat = X*Î²_hat;

# Residuals
Îµ_hat = y - X*Î²_hat;

# Projection matrix
P = X * inv(X&#39;*X) * X&#39;;

# Annihilator matrix
M = I - P;

# Leverage
h = diag(P);
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;ols-residuals&#34;&gt;OLS Residuals&lt;/h2&gt;
&lt;h3 id=&#34;homoskedasticity&#34;&gt;Homoskedasticity&lt;/h3&gt;
&lt;p&gt;The error is &lt;strong&gt;homoskedastic&lt;/strong&gt; if
$\mathbb E [\varepsilon^2 | x] = \sigma^2$ does not depend on $x$. $$
Var(\varepsilon) = I \sigma^2 = \begin{bmatrix}
\sigma^2 &amp;amp; \dots &amp;amp; 0 \newline\newline&lt;br&gt;
\vdots &amp;amp; \ddots &amp;amp; \vdots \newline
0 &amp;amp; \dots &amp;amp; \sigma^2
\end{bmatrix}
$$&lt;/p&gt;
&lt;p&gt;The error is &lt;strong&gt;heteroskedastic&lt;/strong&gt; if
$\mathbb E [\varepsilon^2 | x] = \sigma^2(x)$ does depend on $x$. $$
Var(\varepsilon) = I \sigma_i^2 =
\begin{bmatrix}
\sigma_1^2 &amp;amp; \dots &amp;amp; 0 \newline
\vdots &amp;amp; \ddots &amp;amp; \vdots \newline
0 &amp;amp; \dots &amp;amp; \sigma_n^2
\end{bmatrix}
$$&lt;/p&gt;
&lt;h3 id=&#34;residual-variance&#34;&gt;Residual Variance&lt;/h3&gt;
&lt;p&gt;The OLS &lt;strong&gt;residual variance&lt;/strong&gt; can be an object of interest even in a
heteroskedastic regression. Its method of moments estimator is given by
$$
\hat \sigma^2 = \frac{1}{n} \sum _ {i=1}^n \hat \varepsilon_i^2
$$&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Note that $\hat \sigma^2$ can be rewritten as $$
\hat \sigma^2 = \frac{1}{n} \varepsilon&#39; M&#39; M \varepsilon = \frac{1}{n} tr(\varepsilon&#39; M \varepsilon) = \frac{1}{n} tr(M \varepsilon&#39; \varepsilon)
$$&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;However, the method of moments estimator is a biesed estimator. In fact
$$
\mathbb E[\hat \sigma^2 | X] = \frac{1}{n} \mathbb E [ tr(M \varepsilon&#39; \varepsilon) | X] =  \frac{1}{n} tr( M\mathbb E[\varepsilon&#39; \varepsilon |X]) = \frac{1}{n} \sum _ {i=1}^n (1-h_{ii}) \sigma^2_i
$$&lt;/p&gt;
&lt;p&gt;Under conditional homoskedasticity, the above expression simplifies to
$$
\mathbb E[\hat \sigma^2 | X] = \frac{1}{n} tr(M) \sigma^2 = \frac{n-k}{n} \sigma^2
$$&lt;/p&gt;
&lt;h3 id=&#34;sample-variance&#34;&gt;Sample Variance&lt;/h3&gt;
&lt;p&gt;The OLS &lt;strong&gt;residual sample variance&lt;/strong&gt; is denoted by $s^2$ and is given by
$$
s^2 = \frac{SSR}{n-k} = \frac{\hat \varepsilon&#39;\hat \varepsilon}{n-k} = \frac{1}{n-k}\sum _ {i=1}^n \hat \varepsilon_i^2
$$ Furthermore, the square root of $s^2$, denoted $s$, is called the
standard error of the regression (SER) or the standard error of the
equation (SEE). Not to be confused with other notions of standard error
to be defined later in the course.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The sum of squared residuals can be rewritten as:
$SSR = \hat \varepsilon&#39; \hat \varepsilon = \varepsilon&#39; M \varepsilon$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The OLS residual sample variance is an unbiased estimator of the error
variance $\sigma^2$.&lt;/p&gt;
&lt;p&gt;Another unbiased estimator of $\sigma^2$ is given by $$
\bar \sigma^2 = \frac{1}{n} \sum _ {i=1}^n (1-h_{ii})^{-1} \hat \varepsilon_i^2
$$&lt;/p&gt;
&lt;h3 id=&#34;uncentered-r2&#34;&gt;Uncentered R^2&lt;/h3&gt;
&lt;p&gt;One measure of the variability of the dependent variable $y_i$ is the
sum of squares $\sum _ {i=1}^n y_i^2 = y&amp;rsquo;y$. There is a decomposition:
$$
\begin{aligned}
y&amp;rsquo;y &amp;amp;= (\hat y + e)&#39; (\hat y + \hat \varepsilon) \newline
&amp;amp;= \hat y&#39; \hat y + 2 \hat y&#39; \hat \varepsilon + \hat \varepsilon&#39; \hat \varepsilon e \newline
&amp;amp;= \hat y&#39; \hat y + 2 b&amp;rsquo;X&#39;\hat \varepsilon + \hat \varepsilon&#39; \hat \varepsilon \ \ (\text{since} \ \hat y = Xb) \newline
&amp;amp;= \hat y&#39; \hat y + \hat \varepsilon&#39;\hat \varepsilon \ \ (\text{since} \ X&#39;\hat \varepsilon =0)
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;uncentered&lt;/strong&gt; $\mathbf{R^2}$ is defined as: $$
R^2 _ {uc} \equiv 1 - \frac{\hat \varepsilon&#39;\hat \varepsilon}{y&amp;rsquo;y} = 1 - \frac{\mathbb E_n[\hat \varepsilon_i^2]}{\mathbb E_n[y_i^2]} = \frac{ \mathbb E [\hat y_i^2]}{ \mathbb E [y_i^2]}
$$&lt;/p&gt;
&lt;h3 id=&#34;centered-r2&#34;&gt;Centered R^2&lt;/h3&gt;
&lt;p&gt;A more natural measure of variability is the sum of centered squares
$\sum _ {i=1}^n (y_i - \bar y)^2,$ where
$\bar y := \frac{1}{n}\sum _ {i=1}^n y_i$. If the regressors include a
constant, it can be decomposed as $$
\sum _ {i=1}^n (y_i - \bar y)^2 = \sum _ {i=1}^n (\hat y_i - \bar y)^2 + \sum _ {i=1}^n \hat \varepsilon_i^2
$$&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;coefficient of determination&lt;/strong&gt;, $\mathbf{R^2}$, is defined as $$
R^2 \equiv 1 - \frac{\sum _ {i=1}^n \hat \varepsilon_i^2}{\sum _ {i=1}^n (y_i - \bar y)^2 }= \frac{  \sum _ {i=1}^n (\hat y_i - \bar y)^2 } { \sum _ {i=1}^n (y_i - \bar y)^2} = \frac{\mathbb E_n[(\hat y_i - \bar y)^2]}{\mathbb E_n[(y_i - \bar y)^2]}
$$&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Always use the centered $R^2$ unless you really know what you are
doing.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;code---variance&#34;&gt;Code - Variance&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# Biased variance estimator
Ï_hat = Îµ_hat&#39;*Îµ_hat / n;

# Unbiased estimator 1
Ï_hat_2 = Îµ_hat&#39;*Îµ_hat / (n-k);

# Unbiased estimator 2
Ï_hat_3 = mean( Îµ_hat.^2 ./ (1 .- h) );
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;code---r2&#34;&gt;Code - R^2&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# R squared - uncentered
R2_uc = (y_hat&#39;*y_hat)/ (y&#39;*y);

# R squared
y_bar = mean(y);
R2 = ((y_hat .- y_bar)&#39;*(y_hat .- y_bar))/ ((y .- y_bar)&#39;*(y .- y_bar));
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;finite-sample-properties-of-ols&#34;&gt;Finite Sample Properties of OLS&lt;/h2&gt;
&lt;h3 id=&#34;conditional-unbiasedness&#34;&gt;Conditional Unbiasedness&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Under the GM assumptions (1)-(3), the OLS estimator is &lt;strong&gt;conditionally
unbiased&lt;/strong&gt;, i.e.Â the distribution of $\hat \beta _ {OLS}$ is centered at
$\beta_0$: $\mathbb E [\hat \beta | X] = \beta_0$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt; $$
\begin{aligned}
\mathbb E [\hat \beta  | X] &amp;amp;= \mathbb E [ (X&amp;rsquo;X)^{-1} X&amp;rsquo;y | X] = \newline
&amp;amp;= (X&amp;rsquo;X)^{-1} X &#39; \mathbb E  [y | X] = \newline
&amp;amp;= (X&amp;rsquo;X)^{-1} X&#39; \mathbb E  [X \beta + \varepsilon | X] = \newline
&amp;amp;= (X&amp;rsquo;X)^{-1} X&amp;rsquo;X \beta + (X&amp;rsquo;X)^{-1} X&#39; \mathbb E  [\varepsilon | X] = \newline
&amp;amp;= \beta
\end{aligned}
$$ $$\tag*{$\blacksquare$}$$&lt;/p&gt;
&lt;h3 id=&#34;ols-variance&#34;&gt;OLS Variance&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Under the GM assumptions (1)-(3),
$Var(\hat \beta |X) = \sigma^2 (X&amp;rsquo;X)^{-1}$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;: $$
\begin{aligned}
Var(\hat \beta |X) &amp;amp;= Var( (X&amp;rsquo;X)^{-1} X&amp;rsquo;y|X) = \newline
&amp;amp;= ((X&amp;rsquo;X)^{-1} X&#39; ) Var(y|X) ((X&amp;rsquo;X)^{-1} X&#39; )&#39; = \newline
&amp;amp;= ((X&amp;rsquo;X)^{-1} X&#39; ) Var(X\beta + \varepsilon|X) ((X&amp;rsquo;X)^{-1} X&#39; )&#39; = \newline
&amp;amp;= ((X&amp;rsquo;X)^{-1} X&#39; ) Var(\varepsilon|X) ((X&amp;rsquo;X)^{-1} X&#39; )&#39; = \newline
&amp;amp;= ((X&amp;rsquo;X)^{-1} X&#39; ) \sigma^2 I ((X&amp;rsquo;X)^{-1} X&#39; )&#39; =  \newline
&amp;amp;= \sigma^2 (X&amp;rsquo;X)^{-1}
\end{aligned}
$$ $$\tag*{$\blacksquare$}$$&lt;/p&gt;
&lt;p&gt;Higher correlation of the $X$ implies higher variance of the OLS
estimator.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Intuition: individual observations carry less information. You are
exploring a smaller region of the $X$ space.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;blue&#34;&gt;BLUE&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Under the GM assumptions (1)-(3),
$Cov (\hat \beta, \hat \varepsilon ) = 0$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Under the GM assumptions (1)-(3), $\hat \beta _ {OLS}$ is the best (most
efficient) linear, unbiased estimator (&lt;strong&gt;BLUE&lt;/strong&gt;), i.e., for any unbiased
linear estimator $b$: $Var (b|X) \geq Var (\hat \beta |X)$.&lt;/p&gt;
&lt;h3 id=&#34;blue-proof&#34;&gt;BLUE Proof&lt;/h3&gt;
&lt;p&gt;Consider four steps:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Define three objects: (i) $b= Cy$, (ii) $A = (X&amp;rsquo;X)^{-1} X&#39;$ such
that $\hat \beta = A y$, and (iii) $D = C-A$.&lt;/li&gt;
&lt;li&gt;Decompose $b$ as $$
\begin{aligned}
b &amp;amp;= (D + A) y = \newline
&amp;amp;=  Dy + Ay = \newline&lt;br&gt;
&amp;amp;= D (X\beta + \varepsilon) + \hat \beta = \newline
&amp;amp;= DX\beta + D \varepsilon + \hat \beta
\end{aligned}
$$&lt;/li&gt;
&lt;li&gt;By assumption, $b$ must be unbiased: $$
\begin{aligned}
\mathbb E [b|X] &amp;amp;= \mathbb E [D(X\beta + \varepsilon) + Ay |X] = \newline
&amp;amp;= \mathbb E [DX\beta|X] + \mathbb E [D\varepsilon |X] + \mathbb E [\hat \beta |X] = \newline
&amp;amp;= DX\beta + D \mathbb E [\varepsilon |X] +\beta \newline&lt;br&gt;
&amp;amp;= DX\beta + \beta
\end{aligned}
$$ Hence, it must be that $DX = 0$&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;blue-proof-2&#34;&gt;BLUE Proof (2)&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;We know by (2)-(3) that $b = D \varepsilon + \hat \beta$. We can now
calculate its variance. $$
\begin{aligned}
Var (b|X) &amp;amp;= Var (\hat \beta + D\varepsilon|X) = \newline
&amp;amp;= Var (Ay + D\varepsilon|X) = \newline
&amp;amp;= Var (AX\beta + (D + A)\varepsilon|X) = \newline
&amp;amp;= Var((D+A)\varepsilon |X) = \newline
&amp;amp;= (D+A)\sigma^2 I (D+A)&#39; = \newline
&amp;amp;= \sigma^2 I (DD&#39; + AA&#39; + DA&#39; + AD&#39;) = \newline
&amp;amp;= \sigma^2 I (DD&#39; + AA&#39;) \geq \newline
&amp;amp;\geq \sigma^2 AA&#39;= \newline
&amp;amp;= \sigma^2 (X&amp;rsquo;X)^{-1} = \newline
&amp;amp;= Var (\hat \beta|X)
\end{aligned}
$$ since $DA&#39;= AD&#39; = 0$, $DX = 0$ and $AA&#39; = (X&amp;rsquo;X)^{-1}$.
$$\tag*{$\blacksquare$}$$&lt;/li&gt;
&lt;/ol&gt;
&lt;blockquote&gt;
&lt;p&gt;$Var(b | X) \geq Var (\hat{\beta} | X)$ is meant in a positive
definite sense.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;code---variance-1&#34;&gt;Code - Variance&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# Ideal variance of the OLS estimator
var_Î² = Ï * inv(X&#39;*X)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 2Ã2 Array{Float64,2}:
##   0.0609402  -0.0467732
##  -0.0467732   0.0656808
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# Standard errors
std_Î² = sqrt.(diag(var_Î²))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 2-element Array{Float64,1}:
##  0.24686077212177054
##  0.25628257446345265
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Endogeneity</title>
      <link>https://matteocourthoud.github.io/course/metrics/06_endogeneity/</link>
      <pubDate>Fri, 29 Oct 2021 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/course/metrics/06_endogeneity/</guid>
      <description>&lt;h2 id=&#34;instrumental-variables&#34;&gt;Instrumental Variables&lt;/h2&gt;
&lt;h3 id=&#34;endogeneity&#34;&gt;Endogeneity&lt;/h3&gt;
&lt;p&gt;We say that there is &lt;strong&gt;endogeneity&lt;/strong&gt; in the linear regression model if
$\mathbb E[x_i \varepsilon_i] \neq 0$.&lt;/p&gt;
&lt;p&gt;The random vector $z_i$ is an &lt;strong&gt;instrumental variable&lt;/strong&gt; in the linear
regression model if the following conditions are met.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Exclusion restriction&lt;/strong&gt;: the instruments are uncorrelated with the
regression error $$
\mathbb E_n[z_i \varepsilon_i] = 0
$$ almost surely, i.e.Â with probability $p \to 1$.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Rank condition&lt;/strong&gt;: no linearly redundant instruments $$
\mathbb E_n[z_i z_i&#39;] \neq 0
$$ almost surely, i.e.Â with probability $p \to 1$.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Relevance condition&lt;/strong&gt; (need $L &amp;gt; K$): $$
rank \ (\mathbb E_n[z_i x_i&#39;]) = K
$$ almost surely, i.e.Â with probability $p \to 1$.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;iv-and-2sls&#34;&gt;IV and 2SLS&lt;/h3&gt;
&lt;p&gt;Let $K = dim(x_i)$ and $L = dim(z_i)$. We say that the model is
&lt;strong&gt;just-identified&lt;/strong&gt; if $L = K$ (method: IV) and &lt;strong&gt;over-identified&lt;/strong&gt; if
$L &amp;gt; K$ (method: 2SLS).&lt;/p&gt;
&lt;p&gt;Assume $z_i$ satisfies the instrumental variable assumptions above and
$dim(z_i) = dim(x_i)$, then the &lt;strong&gt;instrumental variables (IV)&lt;/strong&gt;
estimator $\hat{\beta} _ {IV}$ is given by $$
\begin{aligned}
\hat{\beta} _ {IV} &amp;amp;= \mathbb E_n[z_i x_i&#39;]^{-1} \mathbb E_n[z_i y_i] = \newline
&amp;amp;= \left( \frac{1}{n} \sum _ {i=1}^n z_i x_i\right)^{-1} \left( \frac{1}{n} \sum _ {i=1}^n z_i y_i\right) = \newline
&amp;amp;= (Z&amp;rsquo;X)^{-1} (Z&amp;rsquo;y)
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;Assume $z_i$ satisfies the instrumental variable assumptions above and
$dim(z_i) &amp;gt; dim(x_i)$, then the &lt;strong&gt;two-stage-least squares (2SLS)&lt;/strong&gt;
estimator $\hat{\beta} _ {2SLS}$ is given by $$
\hat{\beta} _ {2SLS} =  \Big( X&amp;rsquo;Z (Z&amp;rsquo;Z)^{-1} Z&amp;rsquo;X \Big)^{-1} \Big( X&amp;rsquo;Z (Z&amp;rsquo;Z)^{-1} Z&amp;rsquo;y \Big)
$$ Where $\hat{x}_i$ is the predicted $x_i$ from the &lt;strong&gt;first stage&lt;/strong&gt;
regression of $x_i$ on $z_i$. This is equivalent to the IV estimator
using $\hat{x}_i$ as an instrument for $x_i$.&lt;/p&gt;
&lt;h3 id=&#34;2sls-algebra&#34;&gt;2SLS Algebra&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;The estimator is called &lt;strong&gt;two-stage-least squares&lt;/strong&gt; since it can be
rewritten as an IV estimator that uses $\hat{X}$ as instrument: $$
\begin{aligned}
\hat{\beta} _ {\text{2SLS}} &amp;amp;= \Big( X&amp;rsquo;Z (Z&amp;rsquo;Z)^{-1} Z&amp;rsquo;X \Big)^{-1} \Big( X&amp;rsquo;Z (Z&amp;rsquo;Z)^{-1} Z&amp;rsquo;y \Big) = \newline
&amp;amp;= (\hat{X}&#39; X)^{-1} \hat{X}&#39; y = \newline
&amp;amp;= \mathbb E_n[\hat{x}_i x_i&#39;]^{-1} \mathbb E_n[\hat{x}_i y_i]
\end{aligned}
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Moreover it can be rewritten as $$
\begin{aligned}
\hat{\beta} _ {\text{2SLS}} &amp;amp;= (\hat{X}&#39; X)^{-1} \hat{X}&#39; y = \newline
&amp;amp;= (X&#39; P_Z X)^{-1} X&#39; P_Z y = \newline
&amp;amp;= (X&#39; P_Z P_Z X)^{-1} X&#39; P_Z y = \newline
&amp;amp;= (\hat{X}&#39; \hat{X})^{-1} \hat{X}&#39; y = \newline
&amp;amp;= \mathbb E_n [\hat{x}_i \hat{x}_i]^{-1} \mathbb E_n[\hat{x}_i y_i]
\end{aligned}
$$&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;rule-of-thumb&#34;&gt;Rule of Thumb&lt;/h3&gt;
&lt;p&gt;How to the test the relevance condition? Rule of thumb: $F$-test in the
first stage $&amp;gt;10$ (joint test on $z_i$).&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Problem&lt;/strong&gt;: as $n \to \infty$, with finite $L$, $F \to \infty$ (bad
rule of thumb).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;equivalence&#34;&gt;Equivalence&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;If $K=L$, $\hat{\beta} _ {\text{2SLS}} = \hat{\beta} _ {\text{IV}}$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;If $K=L$, $X&amp;rsquo;Z$ and $Z&amp;rsquo;X$ are squared matrices and, by the relevance
condition, non-singular (invertible). $$
\begin{aligned}
\hat{\beta} _ {\text{2SLS}} &amp;amp;= \Big( X&amp;rsquo;Z (Z&amp;rsquo;Z)^{-1} Z&amp;rsquo;X \Big)^{-1} \Big( X&amp;rsquo;Z (Z&amp;rsquo;Z)^{-1} Z&amp;rsquo;y \Big) = \newline
&amp;amp;= (Z&amp;rsquo;X)^{-1} (Z&amp;rsquo;Z) (X&amp;rsquo;Z)^{-1} X&amp;rsquo;Z (Z&amp;rsquo;Z)^{-1} Z&amp;rsquo;y = \newline
&amp;amp;= (Z&amp;rsquo;X)^{-1} (Z&amp;rsquo;Z) (Z&amp;rsquo;Z)^{-1} Z&amp;rsquo;y = \newline
&amp;amp;= (Z&amp;rsquo;X)^{-1} (Z&amp;rsquo;y) = \newline
&amp;amp;= \hat{\beta} _ {\text{IV}}
\end{aligned}
$$ $$\tag*{$\blacksquare$}$$&lt;/p&gt;
&lt;h3 id=&#34;demand-example&#34;&gt;Demand Example&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Example&lt;/strong&gt; from Hayiashi (2000) page 187: demand and supply
simultaneous equations. $$
\begin{aligned}
&amp;amp; q_i^D(p_i) = \alpha_0 + \alpha_1 p_i + u_i \newline
&amp;amp; q_i^S(p_i) = \beta_0 + \beta_1 p_i + v_i
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;We have an endogeneity problem. To see why, we solve the system of
equations for $(p_i, q_i)$: $$
\begin{aligned}
&amp;amp; p_i = \frac{\beta_0 - \alpha_0}{\alpha_1 - \beta_1} + \frac{v_i - u_i}{\alpha_1 - \beta_1 } \newline
&amp;amp; q_i = \frac{\alpha_1\beta_0 - \alpha_0 \beta_1}{\alpha_1 - \beta_1} + \frac{\alpha_1 v_i - \beta_1 u_i}{\alpha_1 - \beta_1 }
\end{aligned}
$$&lt;/p&gt;
&lt;h3 id=&#34;demand-example-2&#34;&gt;Demand Example (2)&lt;/h3&gt;
&lt;p&gt;Then the price variable is not independent from the error term in
neither equation: $$
\begin{aligned}
&amp;amp; Cov(p_i, u_i) = - \frac{Var(u_i)}{\alpha_1 - \beta_1 } \newline
&amp;amp; Cov(p_i, v_i) = \frac{Var(v_i)}{\alpha_1 - \beta_1 }
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;As a consequence, the OLS estimators are not consistent: $$
\begin{aligned}
&amp;amp; \hat{\alpha} _ {1, OLS} \overset{p}{\to} \alpha_1 + \frac{Cov(p_i, u_i)}{Var(p_i)} \newline
&amp;amp; \hat{\beta} _ {1, OLS} \overset{p}{\to} \beta_1 + \frac{Cov(p_i, v_i)}{Var(p_i)}
\end{aligned}
$$&lt;/p&gt;
&lt;h3 id=&#34;demand-example-3&#34;&gt;Demand Example (3)&lt;/h3&gt;
&lt;p&gt;In general, running regressing $q$ on $p$ you estimate $$
\begin{aligned}
\hat{\gamma} _ {OLS} &amp;amp;\overset{p}{\to} \frac{Cov(p_i, q_i)}{Var(p_i)} = \newline
&amp;amp;= \frac{\alpha_1 Var(v_i) + \beta_1 Var(u_i)}{(\alpha_1 - \beta_1)^2} \left( \frac{Var(v_i) + Var(u_i)}{(\alpha_1 - \beta_1)^2} \right)^{-1} = \newline
&amp;amp;= \frac{\alpha_1 Var(v_i) + \beta_1 Var(u_i)}{Var(v_i) + Var(u_i)}
\end{aligned}
$$ Which is neither $\alpha_1$ nor $\beta_1$ but a variance weighted
average of the two.&lt;/p&gt;
&lt;h3 id=&#34;demand-example-4&#34;&gt;Demand Example (4)&lt;/h3&gt;
&lt;p&gt;Suppose we have a supply shifter $z_i$ such that&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\mathbb E[z_i v_i] \neq 0$&lt;/li&gt;
&lt;li&gt;$\mathbb E[z_i u_i] = 0$.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We combine the second condition and $\mathbb E[u_i] = 0$ to get a system
of 2 equations in 2 unknowns: $\alpha_0$ and $\alpha_1$. $$
\begin{aligned}
&amp;amp; \mathbb E[z_i u_i] = \mathbb E[ z_i (q_i^D(p_i) - \alpha_0 - \alpha_1 p_i) ] = 0 \newline
&amp;amp; \mathbb E[u_i] = \mathbb E[q_i^D(p_i) - \alpha_0 - \alpha_1 p_i] = 0&lt;br&gt;
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;We could try to solve for the vector $\alpha$ that solves $$
\begin{aligned}
&amp;amp; \mathbb E_n[z_i (q_i^D - x_i\alpha)] = 0 \newline
&amp;amp; \mathbb E_n[z_i q_i^D] -  \mathbb E_n[z_ix_i\alpha] = 0
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;If $\mathbb E_n[z_ix_i]$ is invertible, we get
$\hat{\alpha} = \mathbb E_n[z_ix_i]^{-1} \mathbb E_n[z_i q^D_i]$ which
is indeed the IV estimator of $\alpha$ using $z_i$ as an instrument for
the endogenous variable $p_i$.&lt;/p&gt;
&lt;h3 id=&#34;code---dgp&#34;&gt;Code - DGP&lt;/h3&gt;
&lt;p&gt;This code draws 100 observations from the model
$y = 2 x_1 - x_2 + \varepsilon$ where $x_1, x_2 \sim U[0,1]$ and
$\varepsilon \sim N(0,1)$.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# Set seed
Random.seed!(123);

# Set the number of observations
n = 100;

# Set the dimension of Z
l = 3;

# Draw instruments
Z = rand(Uniform(0,1), n, l);

# Correlation matrix for error terms
S = [1 0.8; 0.8 1];

# Endogenous X
Î³ = [2 0; 0 -1; -1 3];
Îµ = rand(Normal(0,1), n, 2) * cholesky(S).U;
X = Z*Î³ .+ Îµ[:,1];

# Calculate y
y = X*Î² .+ Îµ[:,2];
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;code---iv&#34;&gt;Code - IV&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# Estimate beta OLS
Î²_OLS = (X&#39;*X)\(X&#39;*y)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 2-element Array{Float64,1}:
##   2.335699233358403
##  -0.8576266209987325
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# IV: l=k=2 instruments
Z_IV = Z[:,1:k];
Î²_IV = (Z_IV&#39;*X)\(Z_IV&#39;*y)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 2-element Array{Float64,1}:
##   1.6133344277861439
##  -0.6678537395714547
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# Calculate standard errors
Îµ_hat = y - X*Î²_IV;
V_NHC_IV = var(Îµ_hat) * inv(Z_IV&#39;*X)*Z_IV&#39;*Z_IV*inv(Z_IV&#39;*X);
V_HC0_IV = inv(Z_IV&#39;*X)*Z_IV&#39; * (I(n) .* Îµ_hat.^2) * Z_IV*inv(Z_IV&#39;*X);
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;code---2sls&#34;&gt;Code - 2SLS&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# 2SLS: l=3 instruments
Pz = Z*inv(Z&#39;*Z)*Z&#39;;
Î²_2SLS = (X&#39;*Pz*X)\(X&#39;*Pz*y)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 2-element Array{Float64,1}:
##   1.904553638377971
##  -0.8810907510370429
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# Calculate standard errors
Îµ_hat = y - X*Î²_2SLS;
V_NCH_2SLS = var(Îµ_hat) * inv(X&#39;*Pz*X);
V_HC0_2SLS = inv(X&#39;*Pz*X)*X&#39;*Pz * (I(n) .* Îµ_hat.^2) *Pz*X*inv(X&#39;*Pz*X);
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;gmm&#34;&gt;GMM&lt;/h2&gt;
&lt;h3 id=&#34;setting&#34;&gt;Setting&lt;/h3&gt;
&lt;p&gt;We have a system of $L$ moment conditions $$
\begin{aligned}
&amp;amp; \mathbb E[g_1(\omega_i, \delta_0)] = 0 \newline
&amp;amp; \vdots \newline
&amp;amp; \mathbb E[g_L(\omega_i, \delta_0)] = 0
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;If $L = \dim (\delta_0)$, no problem. If $L &amp;gt; \dim (\delta_0)$, there
may be no solution to the system of equations.&lt;/p&gt;
&lt;h3 id=&#34;options&#34;&gt;Options&lt;/h3&gt;
&lt;p&gt;There are two possibilities.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;First Solution&lt;/strong&gt;: add moment conditions until the system is
identified $$
\mathbb E[ a&#39; g(\omega_i, \delta_0)] = 0
$$ Solve $\mathbb E[Ag(\omega_i, \delta)] = 0$ for $\hat{\delta}$.
How to choose $A$? Such that it minimizes $Var(\hat{\delta})$.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Second Solution&lt;/strong&gt;: generalized method of moments (GMM) $$
\begin{aligned}
\hat{\delta} _ {GMM} &amp;amp;= \arg \min _ \delta \quad  \Big| \Big| \mathbb E_n [ g(\omega_i, \delta) ] \Big| \Big| = \newline
&amp;amp;= \arg \min _ \delta \quad n \mathbb E_n[g(\omega_i, \delta)]&#39; W \mathbb E_n [g(\omega_i, \delta)]
\end{aligned}
$$&lt;/li&gt;
&lt;/ol&gt;
&lt;blockquote&gt;
&lt;p&gt;The choice of $A$ and $W$ are closely related to each other.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;1-step-gmm&#34;&gt;1-step GMM&lt;/h3&gt;
&lt;p&gt;Since $J(\delta,W)$ is a quadratic form, a closed form solution exists:
$$
\hat{\delta}(W) = \Big(\mathbb E_n[z_i x_i&#39;] W \mathbb E_n[z_i x_i&#39;] \Big)^{-1}\mathbb E_n[z_i x_i&#39;] W \mathbb E_n[z_i y_i]
$$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Assumptions&lt;/strong&gt; for consistency of the GMM estimator given data
$\mathcal D = \lbrace y_i, x_i, z_i \rbrace _ {i=1}^n$:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Linearity&lt;/strong&gt;: $y_i = x_i\gamma_0 + \varepsilon_i$&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;IID&lt;/strong&gt;: $(y_i, x_i, z_i)$ iid&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Orthogonality&lt;/strong&gt;:
$\mathbb E [z_i(y_i - x_i\gamma_0)] = \mathbb E[z_i \varepsilon_i] = 0$&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Rank identification&lt;/strong&gt;: $\Sigma_{xz} = \mathbb E[z_i x_i&#39;]$ has
full rank&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;convergence&#34;&gt;Convergence&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Under linearity, independence, orthogonality and rank conditions, if
$\hat{W} \overset{p}{\to} W$ positive definite, then $$
\hat{\delta}(\hat{W}) \to \delta(W)
$$ If in addition to the above assumption,
$\sqrt{n} \mathbb E_n [g(\omega_i, \delta_0)] \overset{d}{\to} N(0,S)$
for a fixed positive definite $S$, then $$
\sqrt{n} (\hat{\delta} (\hat{W}) - \delta(W)) \overset{d}{\to} N(0,V)
$$ where
$V = (\Sigma&#39; _ {xz} W \Sigma _ {xz})^{-1} \Sigma _ {xz} W S W \Sigma _ {xz}(\Sigma&#39; _ {xz} W \Sigma _ {xz})^{-1}$.&lt;/p&gt;
&lt;p&gt;Finally, if a consistent estimator $\hat{S}$ of $S$ is available, then
using sample analogues $\hat{\Sigma}_{xz}$ it follows that $$
\hat{V} \overset{p}{\to} V
$$&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;If $W = S^{-1}$ then $V$ reduces to
$V = (\Sigma&#39; _ {xz} W \Sigma _ {xz})^{-1}$. Moreover,
$(\Sigma&#39; _ {xz} W \Sigma _ {xz})^{-1}$ is the smallest possible form
of $V$, in a positive definite sense.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Therefore, to have an efficient estimator, you want to construct
$\hat{W}$ such that $\hat{W} \overset{p}{\to} S^{-1}$.&lt;/p&gt;
&lt;h3 id=&#34;2-step-gmm&#34;&gt;2-step GMM&lt;/h3&gt;
&lt;p&gt;Estimation steps:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Choose an arbitrary weighting matrix $\hat{W}_{init}$ (usually the
identity matrix $I_K$)&lt;/li&gt;
&lt;li&gt;Estimate $\hat{\delta} _ {init}(\hat{W} _ {init})$&lt;/li&gt;
&lt;li&gt;Estimate $\hat{S}$ (asymptotic variance of the moment condition)&lt;/li&gt;
&lt;li&gt;Estimate $\hat{\delta}(\hat{S}^{-1})$&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;On the procedure:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;This estimator achieves the semiparametric efficiency bound.&lt;/li&gt;
&lt;li&gt;This strategy works only if $\hat{S} \overset{p}{\to} S$ exists.&lt;/li&gt;
&lt;li&gt;For iid cases: we can use
$\hat{\delta} = \mathbb E_n[(\hat{\varepsilon}_i z_i)(\hat{\varepsilon}_i z_i) &#39; ]$
where
$\hat{\varepsilon}_i = y_i - x_i \hat{\delta}(\hat{W} _ {init})$.&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;code---1-step-gmm&#34;&gt;Code - 1-step GMM&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# GMM 1-step: inefficient weighting matrix
W_1 = I(l);

# Objective function
gmm_1(b) = ( y - X*b )&#39; * Z * W_1 *  Z&#39; * ( y - X*b );

# Estimate GMM
Î²_gmm_1 = optimize(gmm_1, Î²_OLS).minimizer
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 2-element Array{Float64,1}:
##   1.91556882526808
##  -0.8769689391885799
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# Standard errors GMM
Îµ_hat = y - X*Î²_gmm_1;
S_hat = Z&#39; * (I(n) .* Îµ_hat.^2) * Z;
d_hat = -X&#39;*Z;
V_gmm_1 = inv(d_hat * inv(S_hat) * d_hat&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 2Ã2 Array{Float64,2}:
##   0.0158497   -0.00346601
##  -0.00346601   0.00616531
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;code---2-step-gmm&#34;&gt;Code - 2-step GMM&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# GMM 2-step: efficient weighting matrix
W_2 = inv(S_hat);

# Objective function
gmm_2(b) = ( y - X*b )&#39; * Z * W_2 *  Z&#39; * ( y - X*b );

# Estimate GMM
Î²_gmm_2 = optimize(gmm_2, Î²_OLS).minimizer
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 2-element Array{Float64,1}:
##   1.905326742963115
##  -0.881808949213345
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# Standard errors GMM
Îµ_hat = y - X*Î²_gmm_2;
S_hat = Z&#39; * (I(n) .* Îµ_hat.^2) * Z;
d_hat = -X&#39;*Z;
V_gmm_2 = inv(d_hat * inv(S_hat) * d_hat&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 2Ã2 Array{Float64,2}:
##   0.0162603   -0.00357632
##  -0.00357632   0.00631259
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;testing-overidentifying-restrictions&#34;&gt;Testing Overidentifying Restrictions&lt;/h3&gt;
&lt;p&gt;If the equations are &lt;strong&gt;exactly identified&lt;/strong&gt;, then it is possible to
choose $\delta$ so that all the elements of the sample moments
$\mathbb E_n[g(\omega_i; \delta)]$ are zero and thus that the distance
$$
J(\delta, \hat{W}) = n \mathbb E_n[g(\omega_i, \delta)]&#39; \hat{W} \mathbb E_n[g(\omega_i, \delta)]
$$ is zero. (The $\delta$ that does it is the IV estimator.)&lt;/p&gt;
&lt;p&gt;If the equations are &lt;strong&gt;overidentified&lt;/strong&gt;, i.e.Â $L$ (number of
instruments) $&amp;gt; K$ (number of equations), then the distance cannot be
zero exactly in general, but we would expect the minimized distance to
be &lt;em&gt;close&lt;/em&gt; to zero.&lt;/p&gt;
&lt;h3 id=&#34;naive-test&#34;&gt;Naive Test&lt;/h3&gt;
&lt;p&gt;Suppose your model is overidentified ($L &amp;gt; K$) and you use the following
naive testing procedure:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Estimate $\hat{\delta}$ using a subset of dimension $K$ of
instruments $\lbrace z_1 , .. , z_K\rbrace$ for
$\lbrace x_1 , &amp;hellip; , x_K\rbrace$&lt;/li&gt;
&lt;li&gt;Set $\hat{\varepsilon}_i = y_i - x_i \hat{\delta} _ {\text{GMM}}$&lt;/li&gt;
&lt;li&gt;Infer the size of the remaining $L-K$ moment conditions
$\mathbb E[z _{i, K+1} \varepsilon_i], &amp;hellip;, \mathbb E[z _{i, L} \varepsilon_i]$
looking at their empirical counterparts
$\mathbb E_n[z _{i, K+1} \hat{\varepsilon}_i], &amp;hellip;, \mathbb E_n[z _{i, L} \hat{\varepsilon}_i]$&lt;/li&gt;
&lt;li&gt;Reject exogeneity if the empirical expectations are high. How high?
Calculate p-values.&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;example&#34;&gt;Example&lt;/h3&gt;
&lt;p&gt;If you have two invalid instruments and you use one to test the validity
of the other, it might happen by chance that you donât reject it.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Model: $y_i = x_i + \varepsilon_i$ and
$x_i = \frac{1}{2} z _{i1} - \frac{1}{2} z _{i2} + u_i$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Have $$
Cov (z _{i1}, z _{i2}, \varepsilon_i, u_i) =
\begin{bmatrix}
1 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 \newline 0 &amp;amp; 1 &amp;amp; 0 &amp;amp; 0 \newline 0 &amp;amp; 0 &amp;amp; 1 &amp;amp; 0.5 \newline 0 &amp;amp; 0 &amp;amp; 0.5 &amp;amp; 1
\end{bmatrix}
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;You want to test whether the second instrument is valid (is not
since $\mathbb E[z_2 \varepsilon] \neq 0$). You use $z_1$ and
estimate $\hat{\beta} \to$ the estimator is consistent.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;You obtain $\mathbb E_n[z _{i2} \hat{\varepsilon}_i] \simeq 0$ even
if $z_2$ is invalid&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Problem: you are using an invalid instrument in the first place.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;hansens-test&#34;&gt;Hansenâs Test&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt;: We are interested in testing
$H_0: \mathbb E[z_i \varepsilon_i] = 0$ against
$H_1: \mathbb E[z_i \varepsilon_i] \neq 0$. Suppose
$\hat{S} \overset{p}{\to} S$. Then $$
J(\hat{\delta}(\hat{S}^{-1}) , \hat{S}^{-1}) \overset{d}{\to} \chi^2 _ {L-K}
$$ For $c$ satisfying $\alpha = 1- G_{L - K} ( c )$,
$\Pr(J&amp;gt;c | H_0) \to \alpha$ so the test &lt;em&gt;reject $H_0$ if $J &amp;gt; c$&lt;/em&gt; has
asymptotic size $\alpha$.&lt;/p&gt;
&lt;h3 id=&#34;comments&#34;&gt;Comments&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;The degrees of freedom of the asymptotic distribution are the number
of overidentifying restrictions.&lt;/li&gt;
&lt;li&gt;This is a specification test, testing whether all model assumptions
are true jointly. Only when we are confident that about the other
assumptions, can we interpret a large $J$ statistic as evidence for
the endogeneity of some of the $L$ instruments included in $x$.&lt;/li&gt;
&lt;li&gt;Unlike the tests we have encountered so far, the test is not
consistent against some failures of the orthogonality conditions
(that is, it is not consistent against some fixed elements of the
alternative).&lt;/li&gt;
&lt;li&gt;Several papers in the July 1996 issue of JBES report that the
finite-sample null rejection probability of the test can far exceed
the nominal significance level $\alpha$.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;special-case-conditional-homoskedasticity&#34;&gt;Special Case: Conditional Homoskedasticity&lt;/h3&gt;
&lt;p&gt;The main implication of conditional homoskedasticity is that efficient
GMM becomes 2SLS. With efficient GMM estimation, the weighting matrix is
$\hat{S}^{-1} = \mathbb En [z_i z_i&#39; \varepsilon_i^2]^{-1}$. With
conditional homoskedasticity, the efficient weighting matrix is
$\mathbb E_n[z_iz_i&#39;]^{-1} \sigma^{-2}$, or equivalently
$\mathbb E_n[z_iz_i&#39;]^{-1}$. Then, the GMM estimator becomes $$
\hat{\delta}(\hat{S}^{-1}) = \Big(\mathbb E_n[z_i x_i&#39;]&#39; \underbrace{\mathbb E_n[z_iz_i&#39;]^{-1} \mathbb E[z_i x_i&#39;]} _ {\text{ols of } x_i \text{ on }z_i} \Big)^{-1}\mathbb E_n[z_i x_i&#39;]&#39; \underbrace{\mathbb E_n[z_iz_i&#39;]^{-1} \mathbb E[z_i y_i&#39;]} _ {\text{ols of } y_i \text{ on }z_i}= \hat{\delta} _ {2SLS}
$$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;: Consider the matrix notation. $$
\begin{aligned}
\hat{\delta} \left( \frac{Z&amp;rsquo;Z}{n}\right) &amp;amp;= \left( \frac{X&amp;rsquo;Z}{n} \left( \frac{Z&amp;rsquo;Z}{n}\right)^{-1} \frac{Z&amp;rsquo;X}{n} \right)^{-1} \frac{X&amp;rsquo;Z}{n} \left( \frac{Z&amp;rsquo;Z}{n}\right)^{-1} \frac{Z&amp;rsquo;Y}{n} = \newline
&amp;amp;= \left( X&amp;rsquo;Z(Z&amp;rsquo;Z)^{-1} Z&amp;rsquo;X \right)^{-1} X&amp;rsquo;Z(Z&amp;rsquo;Z)^{-1} Z&amp;rsquo;Y = \newline
&amp;amp;= \left(X&amp;rsquo;P_ZX\right)^{-1} X&amp;rsquo;P_ZY = \newline
&amp;amp;= \left(X&amp;rsquo;P_ZP_ZX\right)^{-1} X&amp;rsquo;P_ZY = \newline
&amp;amp;= \left(\hat{X}&#39;_Z \hat{X}_Z\right)^{-1} \hat{X}&#39;_ZY = \newline
&amp;amp;= \hat{\delta} _ {2SLS}
\end{aligned}
$$ $$\tag*{$\blacksquare$}$$&lt;/p&gt;
&lt;h3 id=&#34;small-sample-properties-of-2sls&#34;&gt;Small-Sample Properties of 2SLS&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt;: When the number of instruments is equal to the sample size
($L = n$), then $\hat{\delta} _ {2SLS} = \hat{\delta} _ {OLS}$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;: We have a perfect prediction problem. The first stage
estimated coefficient $\hat{\gamma}$ is such that it solves the normal
equations: $\hat{\gamma} = z_i^{-1} x_i$. Then $$
\begin{aligned}
\hat{\delta} _ {2SLS} &amp;amp;= \mathbb E_n[\hat{x}_i x&#39;_i]^{-1} \mathbb E_n[\hat{x}_i y_i] = \newline
&amp;amp;= \mathbb E_n[z_i z_i^{-1} x_i x&#39;_i]^{-1} \mathbb E_n[z_i z_i^{-1} x_i y_i] = \newline
&amp;amp;= \mathbb E_n[x_i x&#39;_i]^{-1} \mathbb E_n[x_i y_i] = \newline
&amp;amp;= \hat{\delta} _ {OLS}
\end{aligned}
$$ $$\tag*{$\blacksquare$}$$&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;You have this overfitting problem in general when the number of
instruments is large relative to the sample size. This problem arises
even if the instruments are valid.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;example-from-angrist-1992&#34;&gt;Example from Angrist (1992)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;They regress wages on years of schooling.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Problem&lt;/strong&gt;: endogeneity: both variables are correlated with skills
which are unobserved.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Solution&lt;/strong&gt;: instrument years of schooling with the quarter of
birth.
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Idea&lt;/strong&gt;: if born in the first three quarters, can attend school
from the year of your sixth birthday. Otherwise, you have to
wait one more year.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Problem&lt;/strong&gt;: quarters of birth are three dummies.
&lt;ul&gt;
&lt;li&gt;In order to ``improve the first stage fitâ they interact them
with year of birth (180 effective instruments) and also with the
state (1527 effective instruments).&lt;/li&gt;
&lt;li&gt;This mechanically increases the $R^2$ but also increases the
bias of the 2SLS estimator.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Solutions&lt;/strong&gt;: LIML, JIVE, RJIVE (Hansen et al., 2014), Post-Lasso
(Belloni et al., 2012).&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;example-from-angrist-1992-1&#34;&gt;Example from Angrist (1992)&lt;/h3&gt;
&lt;img src=&#34;../img/Fig_441.png&#34; style=&#34;width:80.0%&#34; /&gt;
&lt;h2 id=&#34;many-instrument-robust-estimation&#34;&gt;Many Instrument Robust Estimation&lt;/h2&gt;
&lt;h3 id=&#34;issue&#34;&gt;Issue&lt;/h3&gt;
&lt;p&gt;Why having too many instruments is problematic? As the number of
instruments increases, the estimated coefficient gets closer to OLS
which is biased. As seen in the theorem above, for $L=n$, the two
estimators coincide.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;../img/Fig_451.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;liml&#34;&gt;LIML&lt;/h3&gt;
&lt;p&gt;An alternative method to estimate the parameters of the structural
equation is by maximum likelihood. Anderson and Rubin (1949) derived the
maximum likelihood estimator for the joint distribution of $(y_i, x_i)$.
The estimator is known as &lt;strong&gt;limited information maximum likelihood&lt;/strong&gt;, or
&lt;strong&gt;LIML&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;This estimator is called âlimited informationâ because it is based on
the structural equation for $(y_i, x_i)$ combined with the reduced form
equation for $x_i$. If maximum likelihood is derived based on a
structural equation for $x_i$ as well, then this leads to what is known
as &lt;strong&gt;full information maximum likelihood (FIML)&lt;/strong&gt;. The advantage of the
LIML approach relative to FIML is that the former does not require a
structural model for $x_i$, and thus allows the researcher to focus on
the structural equation of interest - that for $y_i$.&lt;/p&gt;
&lt;h3 id=&#34;k-class-estimators&#34;&gt;K-class Estimators&lt;/h3&gt;
&lt;p&gt;The &lt;strong&gt;k-class&lt;/strong&gt; estimators have the form $$
\hat{\delta}(\alpha) = (X&#39; P_Z X - \alpha X&#39; X)^{-1} (X&#39; P_Z Y - \alpha X&#39; Y)
$$&lt;/p&gt;
&lt;p&gt;The limited information maximum likelihood estimator &lt;strong&gt;LIML&lt;/strong&gt; is the
k-class estimator $\hat{\delta}(\alpha)$ where $$
\alpha = \lambda_{min} \Big( ([X&#39; , Y]^{-1} [X&#39; , Y])^{-1} [X&#39; , Y]^{-1} P_Z [X&#39; , Y] \Big)
$$&lt;/p&gt;
&lt;p&gt;If $\alpha = 0$ then
$\hat{\delta} _ {\text{LIML}} = \hat{\delta} _ {\text{2SLS}}$ while for
$\alpha \to \infty$,
$\hat{\delta} _ {\text{LIML}} \to \hat{\delta} _ {\text{OLS}}$.&lt;/p&gt;
&lt;h3 id=&#34;comments-on-liml&#34;&gt;Comments on LIML&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;The particular choice of $\alpha$ gives a many instruments robust
estimate&lt;/li&gt;
&lt;li&gt;The LIML estimator has no finite sample moments.
$\mathbb E[\delta(\alpha_{LIML})]$ does not exist in general&lt;/li&gt;
&lt;li&gt;In simulation studies performs well&lt;/li&gt;
&lt;li&gt;Has good asymptotic properties&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Asymptotically the LIML estimator has the same distribution as 2SLS.
However, they can have quite different behaviors in finite samples.
There is considerable evidence that the LIML estimator has superior
finite sample performance to 2SLS when there are many instruments or the
reduced form is weak. However, on the other hand there is worry that
since the LIML estimator is derived under normality it may not be robust
in non-normal settings.&lt;/p&gt;
&lt;h3 id=&#34;jive&#34;&gt;JIVE&lt;/h3&gt;
&lt;p&gt;The &lt;strong&gt;Jacknife IV&lt;/strong&gt; procedure is the following&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Regress $\lbrace x_j \rbrace _ {j \neq i}$ on
$\lbrace z_j \rbrace _ {j \neq i}$ and estimate $\pi_{-i}$ (leave
the $i^{th}$ observation out).&lt;/li&gt;
&lt;li&gt;Form $\hat{x}_i = \hat{\pi} _ {-i} z_i$.&lt;/li&gt;
&lt;li&gt;Run IV using $\hat{x}_i$ as instruments. $$
\hat{\delta} _ {JIVE} = \mathbb E_n[\hat{x}_i x_i&#39;]^{-1} \mathbb E_n[\hat{x}_i y_i&#39;]
$$&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;comments-on-jive&#34;&gt;Comments on JIVE:&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Prevents overfitting.&lt;/li&gt;
&lt;li&gt;With many instruments you get bad out of sample prediction which
implies low correlation between $\hat{x}_i$ and $x_i$:
$\mathbb E_n[\hat{x}_i x_i&#39;] \simeq 0$.&lt;/li&gt;
&lt;li&gt;Use lasso/ridge regression in the first stage in case of too many
instruments.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;hausman-test&#34;&gt;Hausman Test&lt;/h3&gt;
&lt;p&gt;Here we consider testing the validity of OLS. OLS is generally preferred
to IV in terms of precision. Many researchers only doubt the (joint)
validity of the regressor $z_i$ instead of being certain that it is
invalid (in the sense of not being predetermined). So then they wish to
choose between OLS and 2SLS, assuming that they have an instrument
vector $x_i$ whose validity is not in question. Further, assume for
simplicity that $L = K$ so that the efficient GMM estimator is the IV
estimator.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;Hausman test statistic&lt;/strong&gt; $$
H \equiv n (\hat{\delta} _ {IV} - \hat{\delta} _ {OLS})&#39; [\hat{Avar} (\hat{\delta} _ {IV} - \hat{\delta} _ {OLS})]^{-1} (\hat{\delta} _ {IV} - \hat{\delta} _ {OLS})
$$ is asymptotically distributed as a $\chi^2_{L-s}$ under the null
where $s = | z_i \cup x_i |$: the number of regressors that are retained
as instruments in $x_i$.&lt;/p&gt;
&lt;h3 id=&#34;comments-1&#34;&gt;Comments&lt;/h3&gt;
&lt;p&gt;In general, the idea of the Hausman test is the following. If you have
two estimators, one which is efficient under $H_0$ but inconsistent
under $H_1$ (in this case, OLS), and another which is consistent under
$H_1$ (in this case, IV), then construct a test as a quadratic form in
the differences of the estimators. Another classic example arises in
panel data with the hypothesis $H_0$ of unconditional strict exogeneity.
In that case, under $H_0$ Random Effects estimators are efficient but
under $H_1$ they are inconsistent. Fixed Effects estimators instead are
consistent under $H_1$.&lt;/p&gt;
&lt;p&gt;The Hausman test statistic can be used as a pretest procedure: select
either OLS or IV according to the outcome of the test. Although widely
used, this pretest procedure is not advisable. When the null is false,
it is still possible that the test &lt;em&gt;accepts&lt;/em&gt; the null (committing a Type
2 error). In particular, this can happen with a high probability when
the sample size is &lt;em&gt;small&lt;/em&gt; and/or when the regressor $z_i$ is &lt;em&gt;almost
valid&lt;/em&gt;. In such an instance, estimation and also inference will be based
on incorrect methods. Therefore, the overall properties of the Hausman
pretest procedure are undesirable.&lt;/p&gt;
&lt;p&gt;The Hausman test is an example of a specification test. There are many
other specification tests. One could for example test for conditional
homoskedasticity. Unlike for the OLS case, there does not exist a
convenient test for conditional homoskedasticity for the GMM case. A
test statistic that is asymptotically chi-squared under the null is
available but is extremely cumbersome; see White (1982, note 2). If in
doubt, it is better to use the more generally valid inference methods
that allow for conditional heteroskedasticity. Similarly, there does not
exist a convenient test for serial correlation for the GMM case. If in
doubt, it is better to use the more generally valid inference methods
that allow for serial correlation; for example, when data are collected
over time (that is, time-series data).&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>OLS Inference</title>
      <link>https://matteocourthoud.github.io/course/metrics/06_ols_inference/</link>
      <pubDate>Fri, 29 Oct 2021 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/course/metrics/06_ols_inference/</guid>
      <description>&lt;h2 id=&#34;asymptotic-theory-of-the-ols-estimator&#34;&gt;Asymptotic Theory of the OLS Estimator&lt;/h2&gt;
&lt;h3 id=&#34;ols-consistency&#34;&gt;OLS Consistency&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt;: Assume that $(x_i, y_i) _ {i=1}^n$ i.i.d. ,
$\mathbb E[x_i x_i&#39;] = Q$ positive definite,
$\mathbb E[x_i x_i&#39;] &amp;lt; \infty$ and $\mathbb E [y_i^2] &amp;lt; \infty$, then
$\hat \beta _ {OLS}$ is a &lt;strong&gt;consistent&lt;/strong&gt; estimator of $\beta_0$,
i.e.Â $\hat \beta = \mathbb E_n [x_i x_i&#39;] \mathbb E_n [x_i y_i]\overset{p}{\to} \beta_0$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;:&lt;br&gt;
We consider 4 steps:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;$\mathbb E_n [x_i x_i&#39;] \xrightarrow{p} \mathbb E [x_i x_i&#39;]$ by
WLLN since $x_i x_i&#39;$ iid and $\mathbb E[x_i x_i&#39;] &amp;lt; \infty$.&lt;/li&gt;
&lt;li&gt;$\mathbb E_n [x_i y_i] \xrightarrow{p} \mathbb E [x_i y_i]$ by WLLN,
due to $x_i y_i$ iid, Cauchy-Schwarz and finite second moments of
$x_i$ and $y_i$ $$
\mathbb E \left[ x_i y_i \right]  \leq \sqrt{ \mathbb E[x_i^2] \mathbb E[y_i^2]} &amp;lt; \infty
$$&lt;/li&gt;
&lt;li&gt;$\mathbb E_n [x_i x_i&#39;]^{-1} \xrightarrow{p} \mathbb E [x_i x_i&#39;]^{-1}$
by CMT.&lt;/li&gt;
&lt;li&gt;$\mathbb E_n [x_i x_i&#39;]^{-1} \mathbb E_n [x_i y_i] \xrightarrow{p} \mathbb E [x_i x_i&#39;]^{-1} \mathbb E [x_i y_i] = \beta$
by CMT. $$\tag*{$\blacksquare$}$$&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;variance-and-assumptions&#34;&gt;Variance and Assumptions&lt;/h3&gt;
&lt;p&gt;Now we are going to investigate the variance of $\hat \beta _ {OLS}$
progressively relaxing the underlying assumptions.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Gaussian error term.&lt;/li&gt;
&lt;li&gt;Homoskedastic error term.&lt;/li&gt;
&lt;li&gt;Heteroskedastic error term.&lt;/li&gt;
&lt;li&gt;Heteroskedastic and autocorrelated error term.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;gaussian-error-term&#34;&gt;Gaussian Error Term&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt;: Under the GM assumption (1)-(5),
$\hat \beta - \beta |X \sim N(0, \sigma^2 (X&amp;rsquo;X)^{-1})$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;:&lt;br&gt;
We follow 2 steps:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;We can rewrite $\hat \beta$ as $$
\begin{aligned}
\hat \beta &amp;amp; = (X&amp;rsquo;X)^{-1} X&amp;rsquo;y = (X&amp;rsquo;X)^{-1} X&#39;(X\beta + \varepsilon) \newline
&amp;amp;= \beta + (X&amp;rsquo;X)^{-1} X&#39; \varepsilon = \newline
&amp;amp;= \beta + \mathbb E_n [x_i x_i&#39;]^{-1} \mathbb E_n [x_i \varepsilon_i]
\end{aligned}
$$&lt;/li&gt;
&lt;li&gt;Therefore:
$\hat \beta-\beta = \mathbb E_n [x_i x_i&#39;]^{-1} \mathbb E_n [x_i \varepsilon_i]$.
$$
\begin{aligned}
\hat \beta-\beta |X &amp;amp; \sim (X&amp;rsquo;X)^{-1} X&#39; N(0, \sigma^2 I_n) = \newline
&amp;amp;= N(0, \sigma^2 (X&amp;rsquo;X)^{-1} X&amp;rsquo;X (X&amp;rsquo;X)^{-1}) = \newline
&amp;amp;= N(0, \sigma^2 (X&amp;rsquo;X)^{-1})
\end{aligned}
$$ $$\tag*{$\blacksquare$}$$&lt;/li&gt;
&lt;/ol&gt;
&lt;blockquote&gt;
&lt;p&gt;Does it make sense to assume that $\varepsilon$ is gaussian? Not much.
But does it make sense that $\hat \beta$ is gaussian? Yes, because
itâs an average.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;homoskedastic-error-term&#34;&gt;Homoskedastic Error Term&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt;: Under the assumptions of the previous theorem, plus
$\mathbb E[x^4] &amp;lt; \infty$, the OLS estimate has an asymptotic normal
distribution:
$\hat \beta|X \overset{d}{\to} N(\beta, \sigma^2 (X&amp;rsquo;X)^{-1})$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;: $$
\sqrt{n} (\hat \beta - \beta ) = \underbrace{\mathbb E_n [x_i x_i&#39;]^{-1}} _ {\xrightarrow{p} Q^{-1} }   \underbrace{\sqrt{n} \mathbb E_n [x_i \varepsilon_i ]} _ {\xrightarrow{d} N(0, \Omega)} \rightarrow N(0, \Sigma )
$$ where in general
$\Omega = Var (x_i \varepsilon_i) = \mathbb E [(x_i \varepsilon_i)^2]$
and $\Sigma = Q^{-1} \Omega Q^{-1}$. $$\tag*{$\blacksquare$}$$&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Given that $Q = \mathbb E [x_i x_i&#39;]$ is unobserved, we estimate it
with $\hat{Q} = \mathbb E_n [x_i x_i&#39;]$. Since we have assumed
homoskedastic error term, we have $\Omega = \sigma^2 (X&amp;rsquo;X)^{-1}$.
Since we do not observe $\sigma^2$ we estimate it as
$\hat{\sigma}^2 = \mathbb E_n[\hat{\varepsilon}_i^2]$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The terms $x_i \varepsilon_i$ are called &lt;strong&gt;scores&lt;/strong&gt; and we can already
see their central importance for inference.&lt;/p&gt;
&lt;h3 id=&#34;heteroskedastic-error-term&#34;&gt;Heteroskedastic Error Term&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Assumption&lt;/strong&gt;: $\mathbb E [\varepsilon_i x_i \varepsilon_j&#39; x_j&#39;] = 0$,
for all $j \ne i$ and $\mathbb E [\varepsilon_i^4] \leq \infty$,
$\mathbb E [|| x_i||^4] \leq C &amp;lt; \infty$ a.s.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt;: Under GM assumptions (1)-(4) plus heteroskedastic error
term, the following estimators are consistent,
i.e.Â $\hat{\Sigma}\xrightarrow{p} \Sigma$.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Note that we are only looking at $\Omega$ of the
$\Sigma = Q^{-1} \Omega Q^{-1}$ matrix.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;HC0&lt;/strong&gt;: use the observed residual $\hat{\varepsilon}_i$ $$
\Omega _ {HC0} = \mathbb E_n [x_i x_i&#39; \hat{\varepsilon}_i^2]
$$ When $k$ is too big relative to $n$ â i.e.,
$k/n \rightarrow c &amp;gt;0$ â $\hat{\varepsilon}_i^2$ are too small
($\Omega _ {HC0}$ biased towards zero). $\Omega _ {HC1}$,
$\Omega _ {HC2}$ and $\Omega _ {HC3}$ try to correct this small
sample bias.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;HC1&lt;/strong&gt;: degree of freedom correction (default &lt;code&gt;robust&lt;/code&gt; in Stata) $$
\Omega _ {HC1} = \frac{1}{n - k }\mathbb E_n [x_i x_i&#39; \hat{\varepsilon}_i^2]
$$&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;HC2&lt;/strong&gt;: use standardized residuals $$
\Omega _ {HC2} = \mathbb E_n [x_i x_i&#39; \hat{\varepsilon}_i^2 (1-h _ {ii})^{-1}]
$$ where $h _ {ii} = [X(X&amp;rsquo;X)^{-1} X&#39;] _ {ii}$ is the &lt;strong&gt;leverage&lt;/strong&gt;
of the $i^{th}$ observation. A large $h _ {ii}$ means that
observation $i$ is unusual in the sense that the regressor $x_i$ is
far from its sample mean.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;HC3&lt;/strong&gt;: use prediction error, equivalent to Jack-knife estimator,
i.e., $\mathbb E_n [x_i x_i&#39; \hat{\varepsilon} _ {(-i)}^2]$ $$
\Omega _ {HC3} = \mathbb E_n [x_i x_i&#39; \hat{\varepsilon}_i^2 (1-h _ {ii})^{-2}]
$$ This estimator does not overfit when $k$ is relatively big with
respect to $n$. Idea: you exclude the corresponding observation when
estimating a particular $\varepsilon_i$:
$\hat{\varepsilon}_i = y_i - x_i&#39; \hat \beta _ {-i}$.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;hc0-consistency&#34;&gt;HC0 Consistency&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Under regularity conditions HC0 is consistent,
i.e.Â $\hat{\Sigma} _ {HC0} \overset{p}{\to} \Sigma$. $$
\hat{\Sigma} = \hat{Q}^{-1} \hat{\Omega} \hat{Q}^{-1} \xrightarrow{p} \Sigma \qquad  \text{ with } \hat{\Omega} = \mathbb E_n [x_i x_i&#39;     \hat{\varepsilon}_i^2] \quad \text{ and } \hat{Q} = \mathbb E_n [x_i x_i&#39;]^{-1}
$$&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Why is the proof relevant? You cannot directly apply the WLLN to
$\hat \Sigma$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;For the case $\mathrm{dim}(x_i) =1$.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;$\hat{Q}^{-1} \xrightarrow{p} Q^{-1}$ by WLLN since $x_i$ is iid,
$\mathbb E[x_i^4] &amp;lt; \infty$&lt;/li&gt;
&lt;li&gt;$\bar{\Omega} = \mathbb E_n [\varepsilon_i^2 x_i x_i&#39;] \xrightarrow{p} \Omega$
by WLLN since $\mathbb E_n [\varepsilon_i^4] &amp;lt; c$ and $x_i$ bounded.&lt;/li&gt;
&lt;li&gt;By the triangle inequality, $$
| \hat{\Omega} - \hat{\Omega}| \leq \underbrace{|\Omega - \bar{\Omega}|} _ {\overset{p}{\to} 0} + \underbrace{|\bar{\Omega} - \hat{\Omega}|} _ {\text{WTS:} \overset{p}{\to} 0}
$$&lt;/li&gt;
&lt;li&gt;We want to show $|\bar{\Omega} - \hat{\Omega}| \overset{p}{\to} 0$
$$
\begin{aligned}
|\bar{\Omega} - \hat{\Omega}| &amp;amp;= \mathbb E_n [\varepsilon_i^2 x_i^2] - \mathbb E_n [\hat{\varepsilon}_i^2 x_i^2]  = \newline
&amp;amp;= \mathbb E_n [\left( \varepsilon_i^2 - \hat{\varepsilon}_i^2 \right) x_i^2] \leq \newline
&amp;amp; \leq \mathbb E_n \left[ \left( \varepsilon_i^2 - \hat{\varepsilon}_i^2 \right)^2\right]^{\frac{1}{2}} \mathbb E_n [x_i^4]^{\frac{1}{2}}
\end{aligned}
$$ where
$\mathbb E_n [x_i^4]^{\frac{1}{2}} \xrightarrow{p} \mathbb E [x_i^4]^{\frac{1}{2}}$
by $x_i$ bounded, iid and CMT.&lt;/li&gt;
&lt;li&gt;We want to show that
$\mathbb E_n \left[ \left( \varepsilon_i^2 - \hat{\varepsilon}_i^2 \right)^2\right] \leq \eta$
with $\eta \rightarrow 0$. Let
$L = \max_i |\hat{\varepsilon}_i - \varepsilon_i|$ (RV depending on
$n$), with $L \xrightarrow{p} 0$ since $$
|\hat{\varepsilon}_i - \varepsilon_i| = |x_i \hat \beta - x_i \beta| \leq |x_i||\hat \beta - \beta|\xrightarrow{p} c \cdot 0
$$ We can depompose $$
\begin{aligned}
\left(\varepsilon_i^2 - \hat{\varepsilon}_i^2 \right)^2 &amp;amp; = \left(\varepsilon_i - \hat{\varepsilon}_i \right)^2 \left(\varepsilon_i + \hat{\varepsilon}_i \right)^2 \leq \newline&lt;br&gt;
&amp;amp; \leq \left(\varepsilon_i + \hat{\varepsilon}_i \right)^2 L^2 = \newline
&amp;amp;= \left(2\varepsilon_i - \varepsilon_i + \hat{\varepsilon}_i \right)^2 L^2\leq  \newline
&amp;amp; \leq  \left( 2(2\varepsilon_i)^2 + 2(\hat{\varepsilon}_i - \varepsilon_i)^2 \right)^2 L^2 \leq \newline
&amp;amp; \leq (8 \varepsilon_i^2 + 2 L^2) L^2
\end{aligned}
$$ Hence $$
\mathbb E \left[ \left(\varepsilon_i^2 - \hat{\varepsilon}_i^2 \right)^2 \right] \leq  L^2 \left( 8 \mathbb E_n [ \varepsilon_i^2] + 2 \mathbb E_n [L^2] \right)  \xrightarrow{p}0
$$ $$\tag*{$\blacksquare$}$$&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;heteroskedastic-and-autocorrelated-error-term&#34;&gt;Heteroskedastic and Autocorrelated Error Term&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Assumption&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;There esists a $\bar{d}$ such that:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\mathbb E[\varepsilon_i x_i \varepsilon&#39; _ {i-d} x&#39; _ {i-d}] \neq 0 \quad$
for $d \leq \bar{d}$&lt;/li&gt;
&lt;li&gt;$\mathbb E[\varepsilon_i x_i \varepsilon&#39; _ {i-d} x&#39; _ {i-d}] = 0 \quad$
for $d &amp;gt; \bar{d}$&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;Intuition: observations far enough from each other are not correlated.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;We can express the variance of the score as $$
\begin{aligned}
\Omega_n &amp;amp;= Var(\sqrt{n} \mathbb E_n[x_i \varepsilon_i]) = \newline
&amp;amp;= \mathbb E \left[ \left( \frac{1}{n} \sum _ {i=1}^n x_i \varepsilon_i \right) \left( \frac{1}{n} \sum _ {j=1}^n x_j \varepsilon_j \right) \right] = \newline
&amp;amp;= \frac{1}{n} \sum _ {i=1}^n \sum _ {j=1}^n \mathbb E[x_i \varepsilon_i x_j&#39; \varepsilon_j&#39;] = \newline
&amp;amp;= \frac{1}{n} \sum _ {i=1}^n \sum _ {j : |i-j|\leq \bar{d}} \mathbb E[x_i \varepsilon_i x_j&#39; \varepsilon_j&#39;] = \newline
&amp;amp;= \frac{1}{n} \sum _ {d=0}^{\bar{d}} \sum _ {i = d}^{n} \mathbb E[x_i \varepsilon_i x _ {i-d}&#39; \varepsilon _ {i-d}&#39;]
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;We estimate $\Omega_n$ by $$
\hat{\Omega}_n = \frac{1}{n} \sum _ {d=0}^{\bar{d}} \sum _ {i = d}^{n} x_i \hat{\varepsilon}_i x _ {i-d}&#39; \hat{\varepsilon} _ {i-d}&#39;
$$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;If $\bar{d}$ is a fixed integer, then $$
\hat{\Omega}_n - \Omega_n \overset{p}{\to} 0
$$&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;What if $\bar{d}$ does not exist (all $x_i, x_j$ are correlated)? $$
\hat{\Omega}_n = \frac{1}{n} \sum _ {d=0}^{n} \sum _ {i = d}^{n} x_i \hat{\varepsilon}_i x _ {i-d}&#39; \hat{\varepsilon} _ {i-d}&#39; = n \mathbb E_n[x_i \hat{\varepsilon}_i]^2 = 0
$$ By the orthogonality property of the OLS residual.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;HAC with Uniform Kernel&lt;/strong&gt; $$
\hat{\Omega}_h = \frac{1}{n} \sum _ {i,j} x_i \hat{\varepsilon}_i x_j&#39; \hat{\varepsilon}_j&#39; \mathbb{I} \lbrace |i-j| \leq h \rbrace
$$ where $h$ is the &lt;strong&gt;bandwidth&lt;/strong&gt; of the kernel. The bandwidth is chosen
such that
$\mathbb E[x_i \varepsilon_i x _ {i-d}&#39; \varepsilon _ {i-d}&#39; ]$ is small
for $d &amp;gt; h$. How small? Small enough for the estimates to be consistent.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;HAC with General Kernel&lt;/strong&gt; $$
\hat{\Omega}^{HAC} _ {k,h} = \frac{1}{n} \sum _ {i,j} x_i \hat{\varepsilon}_i x_j&#39; \hat{\varepsilon}_j&#39; k \left( \frac{|i-j|}{n} \right)
$$&lt;/p&gt;
&lt;h3 id=&#34;hac-consistency&#34;&gt;HAC Consistency&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt; If the joint distribution is stationary and $\alpha$-mixing
with $\sum _ {k=1}^\infty k^2 \alpha(k) &amp;lt; \infty$ and&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\mathbb E[ | x _ {ij} \varepsilon_i |^\nu ] &amp;lt; \infty$ $\forall \nu$&lt;/li&gt;
&lt;li&gt;$\hat{\varepsilon}_i = y_i - x_i&#39; \hat \beta$ for some
$\hat \beta \overset{p}{\to} \beta_0$&lt;/li&gt;
&lt;li&gt;$k$ smooth, symmetric, $k(0) \to \infty$ as $z \to \infty$,
$\int k^2 &amp;lt; \infty$&lt;/li&gt;
&lt;li&gt;$\frac{h}{n} \to 0$&lt;/li&gt;
&lt;li&gt;$h \to \infty$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Then the HAC estimator is &lt;strong&gt;consistent&lt;/strong&gt;. $$
\hat{\Omega}^{HAC} _ {k,h} - \Omega_n \overset{p}{\to} 0
$$&lt;/p&gt;
&lt;h3 id=&#34;comments&#34;&gt;Comments&lt;/h3&gt;
&lt;p&gt;We want to choose $h$ small relative to $n$ in order to avoid estimation
problems. But we also want to choose $h$ large so that the remainder is
small: $$
\begin{aligned}
\Omega_n &amp;amp;= Var(\sqrt{n} \mathbb E_n[x_i \varepsilon_i]) = \newline
&amp;amp;= \underbrace{\frac{1}{n} \sum _ {i,j : |i-j|\leq h} \mathbb E[x_i \varepsilon_i x_j&#39; \varepsilon_j&#39;]} _ {\Omega^h_n} + \underbrace{\frac{1}{n} \sum _ {i,j : |i-j|&amp;gt; h} \mathbb E[x_i \varepsilon_i x_j&#39; \varepsilon_j&#39;]} _ {\text{remainder: } R_n} = \newline
&amp;amp;= \Omega_n^h + R_n
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;In particular, HAC theory requires: $$
\hat{\Omega}^{HAC} \overset{p}{\to} \Omega \quad \text{ if } \quad
\begin{cases}
&amp;amp; \frac{h}{n} \to 0 \newline
&amp;amp; h \to \infty
\end{cases}
$$&lt;/p&gt;
&lt;p&gt;But in practice, long-run estimation implies $\frac{h}{n} \simeq 0$
which is not ``safeâ in the sense that it does not imply
$R_n \simeq 0$. On the other hand, if $h \simeq n$, $\hat{\Omega}^{HAC}$
does not converge in probability because itâs too noisy.&lt;/p&gt;
&lt;h3 id=&#34;choice-of-h&#34;&gt;Choice of h&lt;/h3&gt;
&lt;p&gt;How to choose $h$? Look at the score autocorrelation function (ACF).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;../img/Fig_331.jpg&#34; alt=&#34;Autocorrelation Function&#34;&gt;&lt;/p&gt;
&lt;p&gt;It looks like after 10 periods the empirical autocorrelation is quite
small but still not zero.&lt;/p&gt;
&lt;h3 id=&#34;fixed-b-asymptotics&#34;&gt;Fixed b Asymptotics&lt;/h3&gt;
&lt;p&gt;[Neave, 1970]: â&lt;em&gt;When proving results on the asymptotic behavior of
estimates of the spectrum of a stationary time series, it is invariably
assumed that as the sample size $n$ tends to infinity, so does the
truncation point $h$, but at a slower rate, so that $\frac{h}{n}$ tends
to zero. This is a convenient assumption mathematically in that, in
particular, it ensures consistency of the estimates, but it is
unrealistic when such results are used as approximations to the finite
case where the value of $\frac{h}{n}$ cannot be zero.&lt;/em&gt;ââ&lt;/p&gt;
&lt;h3 id=&#34;fixed-b-theorem&#34;&gt;Fixed b Theorem&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Under regularity conditions, $$
\sqrt{n} \Big( V^{HAC} _ {k,h} \Big)(\hat \beta - \beta_0) \overset{d}{\to} F
$$&lt;/p&gt;
&lt;p&gt;The asymptotic critical values of the $F$ statistic depend on the choice
of the kernel. In order to do hypothesis testing, Kiefer and
Vogelsang(2005) provide critical value functions for the t-statistic for
each kernel-confidence level combination using a cubic equation: $$
cv(b) = a_0 + a_1 b + a_2 b^2 + a_3 b^3
$$&lt;/p&gt;
&lt;h3 id=&#34;example&#34;&gt;Example&lt;/h3&gt;
&lt;p&gt;Example for the Bartlett kernel:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;../img/Fig_332.png&#34; alt=&#34;Fixed-b&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;fixed-g-asymptotics&#34;&gt;Fixed G Asymptotics&lt;/h3&gt;
&lt;p&gt;[Bester, 2013]: â&lt;em&gt;Cluster covariance estimators are routinely used
with data that has a group structure with independence assumed across
groups. Typically, inference is conducted in such settings under the
assumption that there are a large number of these independent groups.&lt;/em&gt;ââ&lt;/p&gt;
&lt;p&gt;â&lt;em&gt;However, with enough weakly dependent data, we show that groups can be
chosen by the researcher so that group-level averages are approximately
independent. Intuitively, if groups are large enough and well shaped
(e.g.Â do not have gaps), the majority of points in a group will be far
from other groups, and hence approximately independent of observations
from other groups provided the data are weakly dependent. The key
prerequisite for our methods is the researcherâs ability to construct
groups whose averages are approximately independent. As we show later,
this often requires that the number of groups be kept relatively small,
which is why our main results explicitly consider a fixed (small) number
of groups.&lt;/em&gt;ââ&lt;/p&gt;
&lt;h3 id=&#34;assumption&#34;&gt;Assumption&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Assumption&lt;/strong&gt; Suppose you have data
$D = (y _ {it} , x _ {it}) _ {i=1, t=1}^{N, T}$ where
$y _ {it} = x _ {it}&#39; \beta + \alpha_i + \varepsilon _ {it}$ where $i$
indexes the observational unit and $t$ indexes time (could also be
space).&lt;/p&gt;
&lt;p&gt;Let $$
\begin{aligned}
&amp;amp; \tilde{y} _ {it} = y _ {it} - \frac{1}{T} \sum _ {t=1}^T y _ {it} \newline
&amp;amp; \tilde{x} _ {it} = x _ {it} - \frac{1}{T} \sum _ {t=1}^T x _ {it} \newline
&amp;amp; \tilde{\varepsilon} _ {it} = \varepsilon _ {it} - \frac{1}{T} \sum _ {t=1}^T \varepsilon _ {it}
\end{aligned}
$$ Then $$
\tilde{y} _ {it} = \tilde{x} _ {it}&#39; \beta + \tilde{\varepsilon} _ {it}
$$&lt;/p&gt;
&lt;p&gt;The $\tilde{\varepsilon} _ {it}$ are by construction correlated between
each other even if the original $\varepsilon$ was iid. The &lt;strong&gt;cluster
score variance estimator&lt;/strong&gt; is given by: $$
\hat{\Omega}^{CL} = \frac{1}{T-1} \sum _ {i=1}^n  \sum _ {t=1}^T  \sum _ {s=1}^T \tilde{x} _ {it} \hat{\tilde{\varepsilon}} _ {it} \tilde{x} _ {is}     \hat{\tilde{\varepsilon}} _ {is}
$$&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Itâs very similar too the HAC estimator since we have &lt;em&gt;dependent
cross-products&lt;/em&gt; here as well. However, here we do not consider the
$i \times j$ cross-products. We only have time-dependency (state).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;comments-1&#34;&gt;Comments (1)&lt;/h3&gt;
&lt;p&gt;On $T$ and $n$:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;If $T$ is fixed and $n \to \infty$, then the number of
cross-products considered is much smaller than the total number of
cross-products.&lt;/li&gt;
&lt;li&gt;If $T &amp;raquo; n$ issues arise since the number of cross products
considered is close to the total number of cross products. As in HAC
estimation, this is a problem because it implies that the algebraic
estimate of the cluster score variance gets close to zero because of
the orthogonality property of the residuals.&lt;/li&gt;
&lt;li&gt;The panel assumption is that observations across individuals are not
correlated.&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;Strategy: as in HAC, we want to limit the correlation across clusters
(individuals). We hope that observations are &lt;strong&gt;negligibly dependent&lt;/strong&gt;
between cluster sufficiently distant from each other.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;comments-2&#34;&gt;Comments (2)&lt;/h3&gt;
&lt;p&gt;Classical cluster robust estimator: $$
\hat{\Omega}^{CL} = \frac{1}{n} \sum _ {i=1}^n x_i \varepsilon_i x_j&#39; \varepsilon_j&#39; \mathbb{I}   \lbrace i,j \text{ in the same cluster} \rbrace
$$&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;On clusters:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;If the number of observations near a boundary is small relative to
the sample size, ignoring the dependence should not affect
inference too adversely.&lt;/li&gt;
&lt;li&gt;The higher the dimension of the data, the easier it is to have
observations near boundaries (&lt;em&gt;curse of dimensionality&lt;/em&gt;).&lt;/li&gt;
&lt;li&gt;We would like to have few clusters in order to make less
independence assumptions. However, few clusters means bigger
blocks and hence a larger number of cross-products to estimate. If
the number of cross-products is too large (relative to the sample
size), $\hat{\Omega}^{CL}$ does not converge&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt;: Under regularity conditions: $$
\hat{t} \overset{d}{\to} \sqrt{\frac{G}{G-1}} t _ {G-1}
$$&lt;/p&gt;
&lt;h3 id=&#34;code---dgp&#34;&gt;Code - DGP&lt;/h3&gt;
&lt;p&gt;This code draws 100 observations from the model
$y = 2 x_1 - x_2 + \varepsilon$ where $x_1, x_2 \sim U[0,1]$ and
$\varepsilon \sim N(0,1)$.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# Set seed
Random.seed!(123);

# Set the number of observations
n = 100;

# Set the dimension of X
k = 2;

# Draw a sample of explanatory variables
X = rand(Uniform(0,1), n, k);

# Draw the error term
Ï = 1;
Îµ = rand(Normal(0,1), n, 1) * sqrt(Ï);

# Set the parameters
Î² = [2; -1];

# Calculate the dependent variable
y = X*Î² + Îµ;
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;ideal-estimate&#34;&gt;Ideal Estimate&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# OLS estimator
Î²_hat = (X&#39;*X)\(X&#39;*y);

# Residuals
Îµ_hat = y - X*Î²_hat;

# Homoskedastic standard errors
std_h = var(Îµ_hat) * inv(X&#39;*X);

# Projection matrix
P = X * inv(X&#39;*X) * X&#39;;

# Leverage
h = diag(P);
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;hc-estimates&#34;&gt;HC Estimates&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# HC0 variance and standard errors
Î©_hc0 = X&#39; * (I(n) .* Îµ_hat.^2) * X;
std_hc0 = sqrt.(diag(inv(X&#39;*X) * Î©_hc0 * inv(X&#39;*X)))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 2-element Array{Float64,1}:
##  0.24691300271914793
##  0.28044707935951835
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# HC1 variance and standard errors
Î©_hc1 = n/(n-k) * X&#39; * (I(n) .* Îµ_hat.^2) * X;
std_hc1 = sqrt.(diag(inv(X&#39;*X) * Î©_hc1 * inv(X&#39;*X)))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 2-element Array{Float64,1}:
##  0.24941979797977423
##  0.2832943308272532
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# HC2 variance and standard errors
Î©_hc2 = X&#39; * (I(n) .* Îµ_hat.^2 ./ (1 .- h)) * X;
std_hc2 = sqrt.(diag(inv(X&#39;*X) * Î©_hc2 * inv(X&#39;*X)))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 2-element Array{Float64,1}:
##  0.2506509902982869
##  0.2850878737103963
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# HC3 variance and standard errors
Î©_hc3 = X&#39; * (I(n) .* Îµ_hat.^2 ./ (1 .- h).^2) * X;
std_hc3 = sqrt.(diag(inv(X&#39;*X) * Î©_hc3 * inv(X&#39;*X)))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 2-element Array{Float64,1}:
##  0.25446321015850176
##  0.2898264779289438
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# Note what happens if you allow for full autocorrelation
omega_full = X&#39;*Îµ_hat*Îµ_hat&#39;*X;
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;inference&#34;&gt;Inference&lt;/h2&gt;
&lt;h3 id=&#34;hypothesis-testing&#34;&gt;Hypothesis Testing&lt;/h3&gt;
&lt;p&gt;In order to do inference on $\hat \beta$ we need to know its
distribution. We have two options: (i) assume gaussian error term
(extended GM) or (ii) rely on asymptotic approximations (CLT).&lt;/p&gt;
&lt;p&gt;A statistical hypothesis is a subset of a statistical model,
$\mathcal K \subset \mathcal F$. A hypothesis test is a map
$\mathcal D \rightarrow \lbrace 0,1 \rbrace$, $D \mapsto T$. If
$\mathcal F$ is the statistical model and $\mathcal K$ is the
statistical hypothesis, we use the notation $H_0: \Pr \in \mathcal K$.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Generally, we are interested in understanding whether it is likely
that data $D$ are drawn from $\mathcal K$ or not.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;A hypothesis test, $T$ is our tool for deciding whether the hypothesis
is consistent with the data. $T(D)= 0$ implies fail to reject $H_0$ and
test inconclusive $T(D)=1$ $\implies$ reject $H_0$ and $D$ is
inconsistent with any $\Pr \in \mathcal K$.&lt;/p&gt;
&lt;p&gt;Let $\mathcal K \subseteq \mathcal F$ be a statistical hypothesis and
$T$ a hypothesis test.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Suppose $\Pr \in \mathcal K$. A Type I error (relative to $\Pr$) is
an event $T(D)=1$ under $\Pr$.&lt;/li&gt;
&lt;li&gt;Suppose $\Pr \in \mathcal K^c$. A Type II error (relative to $\Pr$)
is an event $T(D)=0$ under $\Pr$.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The corresponding probability of a type I error is called &lt;strong&gt;size&lt;/strong&gt;. The
corresponding probability of a type II error is called &lt;strong&gt;power&lt;/strong&gt;
(against the alternative $\Pr$).&lt;/p&gt;
&lt;p&gt;In this section, we are interested in testing three hypotheses, under
the assumptions of linearity, strict exogeneity, no multicollinearity,
normality on the error term. They are:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;$H_0: \beta _ {0k} = \bar \beta _ {0k}$ (single coefficient,
$\bar \beta _ {0k} \in \mathbb R$, $k \leq K$)&lt;/li&gt;
&lt;li&gt;$a&#39; \beta_0 = c$ (linear combination,
$a \in \mathbb R^K, c \in \mathbb R$)&lt;/li&gt;
&lt;li&gt;$R \beta_0 = r$ (linear restrictions,
$R \in \mathbb R^{p \times K}$, full rank, $r \in \mathbb R^p$)&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;testing-problem&#34;&gt;Testing Problem&lt;/h3&gt;
&lt;p&gt;Consider the testing problem $H_0: \beta _ {0k} = \bar \beta _ {0k}$
where $\bar \beta _ {0k}$ is a pre-specified value under the null. The
t-statistic for this problem is defined by $$
t_k:= \frac{b_k - \bar \beta _ {0k}}{SE(b_k)}, \ \ SE(b_k):= \sqrt{s^2 [(X&amp;rsquo;X)^{-1}] _ {kk}}
$$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt;: In the testing procedure above, the sampling distribution
under the null $H_0$ is given by $$
t_k|X \sim t _ {n-k} \ \ \text{and so} \ \ t_k \sim t _ {n-k}
$$&lt;/p&gt;
&lt;p&gt;$t _ {(n-K)}$ denotes the t-distribution with $(n-k)$ degress of
freedom. The test can be one sided or two sided. The above sampling
distribution can be used to construct a confidence interval.&lt;/p&gt;
&lt;h3 id=&#34;example-1&#34;&gt;Example&lt;/h3&gt;
&lt;p&gt;We want to asses whether or not the ``trueâ coefficient $\beta_0$
equals a specific value $\hat \beta$. Specifically, we are interested in
testing $H_0$ against $H_1$, where:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Null Hypothesis&lt;/em&gt;: $H_0: \beta_0 = \hat \beta$&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Alternative Hypothesis&lt;/em&gt;: $H_1: \beta_0 \ne \hat \beta$.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Hence, we are interested in a statistic informative about $H_1$, which
is the Wald test statistic $$
|T^*| = \bigg| \frac{\hat \beta - \beta_0}{\sigma(\hat \beta)}\bigg|  \sim N(0,1)
$$&lt;/p&gt;
&lt;p&gt;However, the true variance $\sigma^2(\hat \beta )$ is not known and has
to be estimated. Therefore we plug in the sample variance
$\hat \sigma^2(\hat \beta) = \frac{n}{n-1} \mathbb E_n[\hat e_i^2]$ and
we use $$
|T| = \bigg| \frac{\hat \beta - \beta_0}{\hat \sigma (\hat \beta)}\bigg|  \sim t _ {(n-k)}
$$&lt;/p&gt;
&lt;h3 id=&#34;comments-3&#34;&gt;Comments&lt;/h3&gt;
&lt;p&gt;Hypothesis testing is like proof by contradiction. Imagine the sampling
distribution was generated by $\beta$. If it is highly improbable to
observe $\hat \beta$ given $\beta_0 = \beta$ then we reject the
hypothesis that the sampling distribution was generated by $\beta$.&lt;/p&gt;
&lt;p&gt;Then, given a realized value of the statistic $|T|$, we take the
following decision:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Do not reject $H_0$&lt;/em&gt;: it is consistent with random variation under
true $H_0$âi.e., $|T|$ small as it has an exact student t
distribution with $(n-k)$ degree of freedom in the normal regression
model.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Reject $H_0$ in favor of $H_1$&lt;/em&gt;: $|T| &amp;gt; c$, with $c$ being the
critical values selected to control for false rejections:
$\Pr(|t _ {n-k}| \geq c) = \alpha$. Moreover, you can also reject
$H_0$ if the p-value $p$ is such that: $p &amp;lt; \alpha$.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;comments-2-1&#34;&gt;Comments (2)&lt;/h3&gt;
&lt;p&gt;The probability of false rejection is decreasing in $c$, i.e.Â the
critical value for a given significant level. $$
\begin{aligned}
\Pr (\text{Reject } H_0 | H_0)  &amp;amp; = \Pr (|T|&amp;gt; c | H_0 ) = \newline
&amp;amp; = \Pr (T &amp;gt; c | H_0 ) +     \Pr (T &amp;lt; -c | H_0 ) = \newline
&amp;amp; = 1 - F(c) + F(-c) = 2(1-F(c))
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Example&lt;/strong&gt;: Consider the testing problem $H_0: a&#39;\beta_0=c$ where $a$
is a pre-specified linear combination under study. The t-statistic for
this problem is defined by: $$
t_k:= \frac{a&amp;rsquo;b - c}{SE(a&amp;rsquo;b)}, \ \ SE(a&amp;rsquo;b):= \sqrt{s^2 a&#39;(X&amp;rsquo;X)^{-1}a}
$$&lt;/p&gt;
&lt;h3 id=&#34;t-stat&#34;&gt;t Stat&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In the testing procedure above, the sampling distribution under the null
$H_0$ is given by $$
t_a|X \sim t _ {n-K} \quad\text{and so} \quad t_a \sim t _ {n-K}
$$&lt;/p&gt;
&lt;p&gt;Like in the previous test, $t _ {(n-K)}$ denotes the t-distribution with
$(n-K)$ degress of freedom. The test can again be one sided or two
sided. The above sampling distribution can be used to construct a
confidence interval&lt;/p&gt;
&lt;h3 id=&#34;f-stat&#34;&gt;F Stat&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Example&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Consider the testing problem $$
H_0: R \beta_0 = r
$$ where $R \in \mathbb R^{p \times k}$ is a presepecified set of linear
combinations and $r \in \mathbb R^p$ is a restriction vector.&lt;/p&gt;
&lt;p&gt;The F-statistic for this problem is given by $$
F:= \frac{(Rb-r)&#39;[R(X&amp;rsquo;X)R&#39;]^{-1}(Rb-r)/p }{s^2}
$$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;For the problem, the sampling distribution of the F-statistic under the
null $H_0:$ $$
F|X \sim F _ {p,n-K} \ \ \text{and so} \ \ F \sim F _ {p,n-K}
$$&lt;/p&gt;
&lt;p&gt;The test is intrinsically two-sided. The above sampling distribution can
be used to construct a confidence interval.&lt;/p&gt;
&lt;h3 id=&#34;equivalence&#34;&gt;Equivalence&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Consider the testing problem $H_0: R \beta_0 = r$ where
$R \in \mathbb R^{p\times K}$ is a presepecified set of linear
combinations and $r \in \mathbb R^p$ is a restriction vector.&lt;/p&gt;
&lt;p&gt;Consider the restricted least squares estimator, denoted $\hat \beta_R$:
$\hat \beta_R: = \text{arg} \min _ { \beta: R \beta = r } Q( \beta)$.
Let $SSR_U = Q(b), \ \ SSR_R=Q(\hat \beta_R)$. Then the $F$ statistic is
numerically equivalent to the following expression:
$F = \frac{(SSR_R - SSR_U)/p}{SSR_U/(n-K)}$.&lt;/p&gt;
&lt;h3 id=&#34;confidence-intervals&#34;&gt;Confidence Intervals&lt;/h3&gt;
&lt;p&gt;A &lt;strong&gt;confidence interval at $(1-\alpha)$&lt;/strong&gt; is a random set $C$ such that
$$
\Pr(\beta_0 \in C) \geq 1- \alpha
$$ i.e.Â the probability that $C$ covers the true value $\beta$ is fixed
at $(1-\alpha)$.&lt;/p&gt;
&lt;p&gt;Since $C$ is not known, it has to be estimated ($\hat{C}$). We construct
confidence intervals such that:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;they are symmetric around $\hat \beta$;&lt;/li&gt;
&lt;li&gt;their length is proportional to
$\sigma(\hat \beta) = \sqrt{Var(\hat \beta)}$.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;A CI is equivalent to the set of parameter values such that the
t-statistic is less than $c$, i.e., $$
\hat{C} = \bigg\lbrace \beta: |T(\beta) | \leq c \bigg\rbrace = \bigg\lbrace \beta: - c\leq \frac{\beta - \hat \beta}{\sigma(\hat \beta)} \leq c \bigg\rbrace
$$&lt;/p&gt;
&lt;p&gt;In practice, to construct a 95% confidence interval for a single
coefficient estimate $\hat \beta_j$, we use the fact that $$
\Pr \left( \frac{| \hat \beta_j - \beta _ {0,j} |}{ \sqrt{\sigma^2 [(X&amp;rsquo;X)^{-1}] _ {jj} }} &amp;gt; 1.96 \right) = 0.05
$$&lt;/p&gt;
&lt;h3 id=&#34;code&#34;&gt;Code&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# t-test for beta=0
t = abs.(Î²_hat ./ (std_hc1));

# p-value
p_val = 1 .- cdf.(Normal(0,1), t);

# F statistic of joint significance
SSR_u = Îµ_hat&#39;*Îµ_hat;
SSR_r = y&#39;*y;
F = (SSR_r - SSR_u)/k / (SSR_u/(n-k));

# 95# confidente intervals
conf_int = [Î²_hat - 1.96*std_hc1, Î²_hat + 1.96*std_hc1];
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Endogeneity</title>
      <link>https://matteocourthoud.github.io/course/metrics/07_endogeneity/</link>
      <pubDate>Fri, 29 Oct 2021 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/course/metrics/07_endogeneity/</guid>
      <description>&lt;h2 id=&#34;instrumental-variables&#34;&gt;Instrumental Variables&lt;/h2&gt;
&lt;h3 id=&#34;endogeneity&#34;&gt;Endogeneity&lt;/h3&gt;
&lt;p&gt;We say that there is &lt;strong&gt;endogeneity&lt;/strong&gt; in the linear regression model if
$\mathbb E[x_i \varepsilon_i] \neq 0$.&lt;/p&gt;
&lt;p&gt;The random vector $z_i$ is an &lt;strong&gt;instrumental variable&lt;/strong&gt; in the linear
regression model if the following conditions are met.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Exclusion restriction&lt;/strong&gt;: the instruments are uncorrelated with the
regression error $$
\mathbb E_n[z_i \varepsilon_i] = 0
$$ almost surely, i.e.Â with probability $p \to 1$.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Rank condition&lt;/strong&gt;: no linearly redundant instruments $$
\mathbb E_n[z_i z_i&#39;] \neq 0
$$ almost surely, i.e.Â with probability $p \to 1$.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Relevance condition&lt;/strong&gt; (need $L &amp;gt; K$): $$
rank \ (\mathbb E_n[z_i x_i&#39;]) = K
$$ almost surely, i.e.Â with probability $p \to 1$.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;iv-and-2sls&#34;&gt;IV and 2SLS&lt;/h3&gt;
&lt;p&gt;Let $K = dim(x_i)$ and $L = dim(z_i)$. We say that the model is
&lt;strong&gt;just-identified&lt;/strong&gt; if $L = K$ (method: IV) and &lt;strong&gt;over-identified&lt;/strong&gt; if
$L &amp;gt; K$ (method: 2SLS).&lt;/p&gt;
&lt;p&gt;Assume $z_i$ satisfies the instrumental variable assumptions above and
$dim(z_i) = dim(x_i)$, then the &lt;strong&gt;instrumental variables (IV)&lt;/strong&gt;
estimator $\hat{\beta} _ {IV}$ is given by $$
\begin{aligned}
\hat{\beta} _ {IV} &amp;amp;= \mathbb E_n[z_i x_i&#39;]^{-1} \mathbb E_n[z_i y_i] = \newline
&amp;amp;= \left( \frac{1}{n} \sum _ {i=1}^n z_i x_i\right)^{-1} \left( \frac{1}{n} \sum _ {i=1}^n z_i y_i\right) = \newline
&amp;amp;= (Z&amp;rsquo;X)^{-1} (Z&amp;rsquo;y)
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;Assume $z_i$ satisfies the instrumental variable assumptions above and
$dim(z_i) &amp;gt; dim(x_i)$, then the &lt;strong&gt;two-stage-least squares (2SLS)&lt;/strong&gt;
estimator $\hat{\beta} _ {2SLS}$ is given by $$
\hat{\beta} _ {2SLS} =  \Big( X&amp;rsquo;Z (Z&amp;rsquo;Z)^{-1} Z&amp;rsquo;X \Big)^{-1} \Big( X&amp;rsquo;Z (Z&amp;rsquo;Z)^{-1} Z&amp;rsquo;y \Big)
$$ Where $\hat{x}_i$ is the predicted $x_i$ from the &lt;strong&gt;first stage&lt;/strong&gt;
regression of $x_i$ on $z_i$. This is equivalent to the IV estimator
using $\hat{x}_i$ as an instrument for $x_i$.&lt;/p&gt;
&lt;h3 id=&#34;2sls-algebra&#34;&gt;2SLS Algebra&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;The estimator is called &lt;strong&gt;two-stage-least squares&lt;/strong&gt; since it can be
rewritten as an IV estimator that uses $\hat{X}$ as instrument: $$
\begin{aligned}
\hat{\beta} _ {\text{2SLS}} &amp;amp;= \Big( X&amp;rsquo;Z (Z&amp;rsquo;Z)^{-1} Z&amp;rsquo;X \Big)^{-1} \Big( X&amp;rsquo;Z (Z&amp;rsquo;Z)^{-1} Z&amp;rsquo;y \Big) = \newline
&amp;amp;= (\hat{X}&#39; X)^{-1} \hat{X}&#39; y = \newline
&amp;amp;= \mathbb E_n[\hat{x}_i x_i&#39;]^{-1} \mathbb E_n[\hat{x}_i y_i]
\end{aligned}
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Moreover it can be rewritten as $$
\begin{aligned}
\hat{\beta} _ {\text{2SLS}} &amp;amp;= (\hat{X}&#39; X)^{-1} \hat{X}&#39; y = \newline
&amp;amp;= (X&#39; P_Z X)^{-1} X&#39; P_Z y = \newline
&amp;amp;= (X&#39; P_Z P_Z X)^{-1} X&#39; P_Z y = \newline
&amp;amp;= (\hat{X}&#39; \hat{X})^{-1} \hat{X}&#39; y = \newline
&amp;amp;= \mathbb E_n [\hat{x}_i \hat{x}_i]^{-1} \mathbb E_n[\hat{x}_i y_i]
\end{aligned}
$$&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;rule-of-thumb&#34;&gt;Rule of Thumb&lt;/h3&gt;
&lt;p&gt;How to the test the relevance condition? Rule of thumb: $F$-test in the
first stage $&amp;gt;10$ (joint test on $z_i$).&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Problem&lt;/strong&gt;: as $n \to \infty$, with finite $L$, $F \to \infty$ (bad
rule of thumb).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;equivalence&#34;&gt;Equivalence&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;If $K=L$, $\hat{\beta} _ {\text{2SLS}} = \hat{\beta} _ {\text{IV}}$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;If $K=L$, $X&amp;rsquo;Z$ and $Z&amp;rsquo;X$ are squared matrices and, by the relevance
condition, non-singular (invertible). $$
\begin{aligned}
\hat{\beta} _ {\text{2SLS}} &amp;amp;= \Big( X&amp;rsquo;Z (Z&amp;rsquo;Z)^{-1} Z&amp;rsquo;X \Big)^{-1} \Big( X&amp;rsquo;Z (Z&amp;rsquo;Z)^{-1} Z&amp;rsquo;y \Big) = \newline
&amp;amp;= (Z&amp;rsquo;X)^{-1} (Z&amp;rsquo;Z) (X&amp;rsquo;Z)^{-1} X&amp;rsquo;Z (Z&amp;rsquo;Z)^{-1} Z&amp;rsquo;y = \newline
&amp;amp;= (Z&amp;rsquo;X)^{-1} (Z&amp;rsquo;Z) (Z&amp;rsquo;Z)^{-1} Z&amp;rsquo;y = \newline
&amp;amp;= (Z&amp;rsquo;X)^{-1} (Z&amp;rsquo;y) = \newline
&amp;amp;= \hat{\beta} _ {\text{IV}}
\end{aligned}
$$ $$\tag*{$\blacksquare$}$$&lt;/p&gt;
&lt;h3 id=&#34;demand-example&#34;&gt;Demand Example&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Example&lt;/strong&gt; from Hayiashi (2000) page 187: demand and supply
simultaneous equations. $$
\begin{aligned}
&amp;amp; q_i^D(p_i) = \alpha_0 + \alpha_1 p_i + u_i \newline
&amp;amp; q_i^S(p_i) = \beta_0 + \beta_1 p_i + v_i
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;We have an endogeneity problem. To see why, we solve the system of
equations for $(p_i, q_i)$: $$
\begin{aligned}
&amp;amp; p_i = \frac{\beta_0 - \alpha_0}{\alpha_1 - \beta_1} + \frac{v_i - u_i}{\alpha_1 - \beta_1 } \newline
&amp;amp; q_i = \frac{\alpha_1\beta_0 - \alpha_0 \beta_1}{\alpha_1 - \beta_1} + \frac{\alpha_1 v_i - \beta_1 u_i}{\alpha_1 - \beta_1 }
\end{aligned}
$$&lt;/p&gt;
&lt;h3 id=&#34;demand-example-2&#34;&gt;Demand Example (2)&lt;/h3&gt;
&lt;p&gt;Then the price variable is not independent from the error term in
neither equation: $$
\begin{aligned}
&amp;amp; Cov(p_i, u_i) = - \frac{Var(u_i)}{\alpha_1 - \beta_1 } \newline
&amp;amp; Cov(p_i, v_i) = \frac{Var(v_i)}{\alpha_1 - \beta_1 }
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;As a consequence, the OLS estimators are not consistent: $$
\begin{aligned}
&amp;amp; \hat{\alpha} _ {1, OLS} \overset{p}{\to} \alpha_1 + \frac{Cov(p_i, u_i)}{Var(p_i)} \newline
&amp;amp; \hat{\beta} _ {1, OLS} \overset{p}{\to} \beta_1 + \frac{Cov(p_i, v_i)}{Var(p_i)}
\end{aligned}
$$&lt;/p&gt;
&lt;h3 id=&#34;demand-example-3&#34;&gt;Demand Example (3)&lt;/h3&gt;
&lt;p&gt;In general, running regressing $q$ on $p$ you estimate $$
\begin{aligned}
\hat{\gamma} _ {OLS} &amp;amp;\overset{p}{\to} \frac{Cov(p_i, q_i)}{Var(p_i)} = \newline
&amp;amp;= \frac{\alpha_1 Var(v_i) + \beta_1 Var(u_i)}{(\alpha_1 - \beta_1)^2} \left( \frac{Var(v_i) + Var(u_i)}{(\alpha_1 - \beta_1)^2} \right)^{-1} = \newline
&amp;amp;= \frac{\alpha_1 Var(v_i) + \beta_1 Var(u_i)}{Var(v_i) + Var(u_i)}
\end{aligned}
$$ Which is neither $\alpha_1$ nor $\beta_1$ but a variance weighted
average of the two.&lt;/p&gt;
&lt;h3 id=&#34;demand-example-4&#34;&gt;Demand Example (4)&lt;/h3&gt;
&lt;p&gt;Suppose we have a supply shifter $z_i$ such that&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\mathbb E[z_i v_i] \neq 0$&lt;/li&gt;
&lt;li&gt;$\mathbb E[z_i u_i] = 0$.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We combine the second condition and $\mathbb E[u_i] = 0$ to get a system
of 2 equations in 2 unknowns: $\alpha_0$ and $\alpha_1$. $$
\begin{aligned}
&amp;amp; \mathbb E[z_i u_i] = \mathbb E[ z_i (q_i^D(p_i) - \alpha_0 - \alpha_1 p_i) ] = 0 \newline
&amp;amp; \mathbb E[u_i] = \mathbb E[q_i^D(p_i) - \alpha_0 - \alpha_1 p_i] = 0&lt;br&gt;
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;We could try to solve for the vector $\alpha$ that solves $$
\begin{aligned}
&amp;amp; \mathbb E_n[z_i (q_i^D - x_i\alpha)] = 0 \newline
&amp;amp; \mathbb E_n[z_i q_i^D] -  \mathbb E_n[z_ix_i\alpha] = 0
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;If $\mathbb E_n[z_ix_i]$ is invertible, we get
$\hat{\alpha} = \mathbb E_n[z_ix_i]^{-1} \mathbb E_n[z_i q^D_i]$ which
is indeed the IV estimator of $\alpha$ using $z_i$ as an instrument for
the endogenous variable $p_i$.&lt;/p&gt;
&lt;h3 id=&#34;code---dgp&#34;&gt;Code - DGP&lt;/h3&gt;
&lt;p&gt;This code draws 100 observations from the model
$y = 2 x_1 - x_2 + \varepsilon$ where $x_1, x_2 \sim U[0,1]$ and
$\varepsilon \sim N(0,1)$.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# Set seed
Random.seed!(123);

# Set the number of observations
n = 100;

# Set the dimension of Z
l = 3;

# Draw instruments
Z = rand(Uniform(0,1), n, l);

# Correlation matrix for error terms
S = [1 0.8; 0.8 1];

# Endogenous X
Î³ = [2 0; 0 -1; -1 3];
Îµ = rand(Normal(0,1), n, 2) * cholesky(S).U;
X = Z*Î³ .+ Îµ[:,1];

# Calculate y
y = X*Î² .+ Îµ[:,2];
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;code---iv&#34;&gt;Code - IV&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# Estimate beta OLS
Î²_OLS = (X&#39;*X)\(X&#39;*y)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 2-element Array{Float64,1}:
##   2.335699233358403
##  -0.8576266209987325
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# IV: l=k=2 instruments
Z_IV = Z[:,1:k];
Î²_IV = (Z_IV&#39;*X)\(Z_IV&#39;*y)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 2-element Array{Float64,1}:
##   1.6133344277861439
##  -0.6678537395714547
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# Calculate standard errors
Îµ_hat = y - X*Î²_IV;
V_NHC_IV = var(Îµ_hat) * inv(Z_IV&#39;*X)*Z_IV&#39;*Z_IV*inv(Z_IV&#39;*X);
V_HC0_IV = inv(Z_IV&#39;*X)*Z_IV&#39; * (I(n) .* Îµ_hat.^2) * Z_IV*inv(Z_IV&#39;*X);
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;code---2sls&#34;&gt;Code - 2SLS&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# 2SLS: l=3 instruments
Pz = Z*inv(Z&#39;*Z)*Z&#39;;
Î²_2SLS = (X&#39;*Pz*X)\(X&#39;*Pz*y)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 2-element Array{Float64,1}:
##   1.904553638377971
##  -0.8810907510370429
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# Calculate standard errors
Îµ_hat = y - X*Î²_2SLS;
V_NCH_2SLS = var(Îµ_hat) * inv(X&#39;*Pz*X);
V_HC0_2SLS = inv(X&#39;*Pz*X)*X&#39;*Pz * (I(n) .* Îµ_hat.^2) *Pz*X*inv(X&#39;*Pz*X);
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;gmm&#34;&gt;GMM&lt;/h2&gt;
&lt;h3 id=&#34;setting&#34;&gt;Setting&lt;/h3&gt;
&lt;p&gt;We have a system of $L$ moment conditions $$
\begin{aligned}
&amp;amp; \mathbb E[g_1(\omega_i, \delta_0)] = 0 \newline
&amp;amp; \vdots \newline
&amp;amp; \mathbb E[g_L(\omega_i, \delta_0)] = 0
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;If $L = \dim (\delta_0)$, no problem. If $L &amp;gt; \dim (\delta_0)$, there
may be no solution to the system of equations.&lt;/p&gt;
&lt;h3 id=&#34;options&#34;&gt;Options&lt;/h3&gt;
&lt;p&gt;There are two possibilities.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;First Solution&lt;/strong&gt;: add moment conditions until the system is
identified $$
\mathbb E[ a&#39; g(\omega_i, \delta_0)] = 0
$$ Solve $\mathbb E[Ag(\omega_i, \delta)] = 0$ for $\hat{\delta}$.
How to choose $A$? Such that it minimizes $Var(\hat{\delta})$.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Second Solution&lt;/strong&gt;: generalized method of moments (GMM) $$
\begin{aligned}
\hat{\delta} _ {GMM} &amp;amp;= \arg \min _ \delta \quad  \Big| \Big| \mathbb E_n [ g(\omega_i, \delta) ] \Big| \Big| = \newline
&amp;amp;= \arg \min _ \delta \quad n \mathbb E_n[g(\omega_i, \delta)]&#39; W \mathbb E_n [g(\omega_i, \delta)]
\end{aligned}
$$&lt;/li&gt;
&lt;/ol&gt;
&lt;blockquote&gt;
&lt;p&gt;The choice of $A$ and $W$ are closely related to each other.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;1-step-gmm&#34;&gt;1-step GMM&lt;/h3&gt;
&lt;p&gt;Since $J(\delta,W)$ is a quadratic form, a closed form solution exists:
$$
\hat{\delta}(W) = \Big(\mathbb E_n[z_i x_i&#39;] W \mathbb E_n[z_i x_i&#39;] \Big)^{-1}\mathbb E_n[z_i x_i&#39;] W \mathbb E_n[z_i y_i]
$$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Assumptions&lt;/strong&gt; for consistency of the GMM estimator given data
$\mathcal D = \lbrace y_i, x_i, z_i \rbrace _ {i=1}^n$:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Linearity&lt;/strong&gt;: $y_i = x_i\gamma_0 + \varepsilon_i$&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;IID&lt;/strong&gt;: $(y_i, x_i, z_i)$ iid&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Orthogonality&lt;/strong&gt;:
$\mathbb E [z_i(y_i - x_i\gamma_0)] = \mathbb E[z_i \varepsilon_i] = 0$&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Rank identification&lt;/strong&gt;: $\Sigma_{xz} = \mathbb E[z_i x_i&#39;]$ has
full rank&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;convergence&#34;&gt;Convergence&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Under linearity, independence, orthogonality and rank conditions, if
$\hat{W} \overset{p}{\to} W$ positive definite, then $$
\hat{\delta}(\hat{W}) \to \delta(W)
$$ If in addition to the above assumption,
$\sqrt{n} \mathbb E_n [g(\omega_i, \delta_0)] \overset{d}{\to} N(0,S)$
for a fixed positive definite $S$, then $$
\sqrt{n} (\hat{\delta} (\hat{W}) - \delta(W)) \overset{d}{\to} N(0,V)
$$ where
$V = (\Sigma&#39; _ {xz} W \Sigma _ {xz})^{-1} \Sigma _ {xz} W S W \Sigma _ {xz}(\Sigma&#39; _ {xz} W \Sigma _ {xz})^{-1}$.&lt;/p&gt;
&lt;p&gt;Finally, if a consistent estimator $\hat{S}$ of $S$ is available, then
using sample analogues $\hat{\Sigma}_{xz}$ it follows that $$
\hat{V} \overset{p}{\to} V
$$&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;If $W = S^{-1}$ then $V$ reduces to
$V = (\Sigma&#39; _ {xz} W \Sigma _ {xz})^{-1}$. Moreover,
$(\Sigma&#39; _ {xz} W \Sigma _ {xz})^{-1}$ is the smallest possible form
of $V$, in a positive definite sense.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Therefore, to have an efficient estimator, you want to construct
$\hat{W}$ such that $\hat{W} \overset{p}{\to} S^{-1}$.&lt;/p&gt;
&lt;h3 id=&#34;2-step-gmm&#34;&gt;2-step GMM&lt;/h3&gt;
&lt;p&gt;Estimation steps:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Choose an arbitrary weighting matrix $\hat{W}_{init}$ (usually the
identity matrix $I_K$)&lt;/li&gt;
&lt;li&gt;Estimate $\hat{\delta} _ {init}(\hat{W} _ {init})$&lt;/li&gt;
&lt;li&gt;Estimate $\hat{S}$ (asymptotic variance of the moment condition)&lt;/li&gt;
&lt;li&gt;Estimate $\hat{\delta}(\hat{S}^{-1})$&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;On the procedure:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;This estimator achieves the semiparametric efficiency bound.&lt;/li&gt;
&lt;li&gt;This strategy works only if $\hat{S} \overset{p}{\to} S$ exists.&lt;/li&gt;
&lt;li&gt;For iid cases: we can use
$\hat{\delta} = \mathbb E_n[(\hat{\varepsilon}_i z_i)(\hat{\varepsilon}_i z_i) &#39; ]$
where
$\hat{\varepsilon}_i = y_i - x_i \hat{\delta}(\hat{W} _ {init})$.&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;code---1-step-gmm&#34;&gt;Code - 1-step GMM&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# GMM 1-step: inefficient weighting matrix
W_1 = I(l);

# Objective function
gmm_1(b) = ( y - X*b )&#39; * Z * W_1 *  Z&#39; * ( y - X*b );

# Estimate GMM
Î²_gmm_1 = optimize(gmm_1, Î²_OLS).minimizer
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 2-element Array{Float64,1}:
##   1.91556882526808
##  -0.8769689391885799
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# Standard errors GMM
Îµ_hat = y - X*Î²_gmm_1;
S_hat = Z&#39; * (I(n) .* Îµ_hat.^2) * Z;
d_hat = -X&#39;*Z;
V_gmm_1 = inv(d_hat * inv(S_hat) * d_hat&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 2Ã2 Array{Float64,2}:
##   0.0158497   -0.00346601
##  -0.00346601   0.00616531
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;code---2-step-gmm&#34;&gt;Code - 2-step GMM&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# GMM 2-step: efficient weighting matrix
W_2 = inv(S_hat);

# Objective function
gmm_2(b) = ( y - X*b )&#39; * Z * W_2 *  Z&#39; * ( y - X*b );

# Estimate GMM
Î²_gmm_2 = optimize(gmm_2, Î²_OLS).minimizer
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 2-element Array{Float64,1}:
##   1.905326742963115
##  -0.881808949213345
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# Standard errors GMM
Îµ_hat = y - X*Î²_gmm_2;
S_hat = Z&#39; * (I(n) .* Îµ_hat.^2) * Z;
d_hat = -X&#39;*Z;
V_gmm_2 = inv(d_hat * inv(S_hat) * d_hat&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 2Ã2 Array{Float64,2}:
##   0.0162603   -0.00357632
##  -0.00357632   0.00631259
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;testing-overidentifying-restrictions&#34;&gt;Testing Overidentifying Restrictions&lt;/h3&gt;
&lt;p&gt;If the equations are &lt;strong&gt;exactly identified&lt;/strong&gt;, then it is possible to
choose $\delta$ so that all the elements of the sample moments
$\mathbb E_n[g(\omega_i; \delta)]$ are zero and thus that the distance
$$
J(\delta, \hat{W}) = n \mathbb E_n[g(\omega_i, \delta)]&#39; \hat{W} \mathbb E_n[g(\omega_i, \delta)]
$$ is zero. (The $\delta$ that does it is the IV estimator.)&lt;/p&gt;
&lt;p&gt;If the equations are &lt;strong&gt;overidentified&lt;/strong&gt;, i.e.Â $L$ (number of
instruments) $&amp;gt; K$ (number of equations), then the distance cannot be
zero exactly in general, but we would expect the minimized distance to
be &lt;em&gt;close&lt;/em&gt; to zero.&lt;/p&gt;
&lt;h3 id=&#34;naive-test&#34;&gt;Naive Test&lt;/h3&gt;
&lt;p&gt;Suppose your model is overidentified ($L &amp;gt; K$) and you use the following
naive testing procedure:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Estimate $\hat{\delta}$ using a subset of dimension $K$ of
instruments $\lbrace z_1 , .. , z_K\rbrace$ for
$\lbrace x_1 , &amp;hellip; , x_K\rbrace$&lt;/li&gt;
&lt;li&gt;Set $\hat{\varepsilon}_i = y_i - x_i \hat{\delta} _ {\text{GMM}}$&lt;/li&gt;
&lt;li&gt;Infer the size of the remaining $L-K$ moment conditions
$\mathbb E[z _{i, K+1} \varepsilon_i], &amp;hellip;, \mathbb E[z _{i, L} \varepsilon_i]$
looking at their empirical counterparts
$\mathbb E_n[z _{i, K+1} \hat{\varepsilon}_i], &amp;hellip;, \mathbb E_n[z _{i, L} \hat{\varepsilon}_i]$&lt;/li&gt;
&lt;li&gt;Reject exogeneity if the empirical expectations are high. How high?
Calculate p-values.&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;example&#34;&gt;Example&lt;/h3&gt;
&lt;p&gt;If you have two invalid instruments and you use one to test the validity
of the other, it might happen by chance that you donât reject it.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Model: $y_i = x_i + \varepsilon_i$ and
$x_i = \frac{1}{2} z _{i1} - \frac{1}{2} z _{i2} + u_i$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Have $$
Cov (z _{i1}, z _{i2}, \varepsilon_i, u_i) =
\begin{bmatrix}
1 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 \newline 0 &amp;amp; 1 &amp;amp; 0 &amp;amp; 0 \newline 0 &amp;amp; 0 &amp;amp; 1 &amp;amp; 0.5 \newline 0 &amp;amp; 0 &amp;amp; 0.5 &amp;amp; 1
\end{bmatrix}
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;You want to test whether the second instrument is valid (is not
since $\mathbb E[z_2 \varepsilon] \neq 0$). You use $z_1$ and
estimate $\hat{\beta} \to$ the estimator is consistent.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;You obtain $\mathbb E_n[z _{i2} \hat{\varepsilon}_i] \simeq 0$ even
if $z_2$ is invalid&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Problem: you are using an invalid instrument in the first place.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;hansens-test&#34;&gt;Hansenâs Test&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt;: We are interested in testing
$H_0: \mathbb E[z_i \varepsilon_i] = 0$ against
$H_1: \mathbb E[z_i \varepsilon_i] \neq 0$. Suppose
$\hat{S} \overset{p}{\to} S$. Then $$
J(\hat{\delta}(\hat{S}^{-1}) , \hat{S}^{-1}) \overset{d}{\to} \chi^2 _ {L-K}
$$ For $c$ satisfying $\alpha = 1- G_{L - K} ( c )$,
$\Pr(J&amp;gt;c | H_0) \to \alpha$ so the test &lt;em&gt;reject $H_0$ if $J &amp;gt; c$&lt;/em&gt; has
asymptotic size $\alpha$.&lt;/p&gt;
&lt;h3 id=&#34;comments&#34;&gt;Comments&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;The degrees of freedom of the asymptotic distribution are the number
of overidentifying restrictions.&lt;/li&gt;
&lt;li&gt;This is a specification test, testing whether all model assumptions
are true jointly. Only when we are confident that about the other
assumptions, can we interpret a large $J$ statistic as evidence for
the endogeneity of some of the $L$ instruments included in $x$.&lt;/li&gt;
&lt;li&gt;Unlike the tests we have encountered so far, the test is not
consistent against some failures of the orthogonality conditions
(that is, it is not consistent against some fixed elements of the
alternative).&lt;/li&gt;
&lt;li&gt;Several papers in the July 1996 issue of JBES report that the
finite-sample null rejection probability of the test can far exceed
the nominal significance level $\alpha$.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;special-case-conditional-homoskedasticity&#34;&gt;Special Case: Conditional Homoskedasticity&lt;/h3&gt;
&lt;p&gt;The main implication of conditional homoskedasticity is that efficient
GMM becomes 2SLS. With efficient GMM estimation, the weighting matrix is
$\hat{S}^{-1} = \mathbb En [z_i z_i&#39; \varepsilon_i^2]^{-1}$. With
conditional homoskedasticity, the efficient weighting matrix is
$\mathbb E_n[z_iz_i&#39;]^{-1} \sigma^{-2}$, or equivalently
$\mathbb E_n[z_iz_i&#39;]^{-1}$. Then, the GMM estimator becomes $$
\hat{\delta}(\hat{S}^{-1}) = \Big(\mathbb E_n[z_i x_i&#39;]&#39; \underbrace{\mathbb E_n[z_iz_i&#39;]^{-1} \mathbb E[z_i x_i&#39;]} _ {\text{ols of } x_i \text{ on }z_i} \Big)^{-1}\mathbb E_n[z_i x_i&#39;]&#39; \underbrace{\mathbb E_n[z_iz_i&#39;]^{-1} \mathbb E[z_i y_i&#39;]} _ {\text{ols of } y_i \text{ on }z_i}= \hat{\delta} _ {2SLS}
$$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;: Consider the matrix notation. $$
\begin{aligned}
\hat{\delta} \left( \frac{Z&amp;rsquo;Z}{n}\right) &amp;amp;= \left( \frac{X&amp;rsquo;Z}{n} \left( \frac{Z&amp;rsquo;Z}{n}\right)^{-1} \frac{Z&amp;rsquo;X}{n} \right)^{-1} \frac{X&amp;rsquo;Z}{n} \left( \frac{Z&amp;rsquo;Z}{n}\right)^{-1} \frac{Z&amp;rsquo;Y}{n} = \newline
&amp;amp;= \left( X&amp;rsquo;Z(Z&amp;rsquo;Z)^{-1} Z&amp;rsquo;X \right)^{-1} X&amp;rsquo;Z(Z&amp;rsquo;Z)^{-1} Z&amp;rsquo;Y = \newline
&amp;amp;= \left(X&amp;rsquo;P_ZX\right)^{-1} X&amp;rsquo;P_ZY = \newline
&amp;amp;= \left(X&amp;rsquo;P_ZP_ZX\right)^{-1} X&amp;rsquo;P_ZY = \newline
&amp;amp;= \left(\hat{X}&#39;_Z \hat{X}_Z\right)^{-1} \hat{X}&#39;_ZY = \newline
&amp;amp;= \hat{\delta} _ {2SLS}
\end{aligned}
$$ $$\tag*{$\blacksquare$}$$&lt;/p&gt;
&lt;h3 id=&#34;small-sample-properties-of-2sls&#34;&gt;Small-Sample Properties of 2SLS&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt;: When the number of instruments is equal to the sample size
($L = n$), then $\hat{\delta} _ {2SLS} = \hat{\delta} _ {OLS}$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;: We have a perfect prediction problem. The first stage
estimated coefficient $\hat{\gamma}$ is such that it solves the normal
equations: $\hat{\gamma} = z_i^{-1} x_i$. Then $$
\begin{aligned}
\hat{\delta} _ {2SLS} &amp;amp;= \mathbb E_n[\hat{x}_i x&#39;_i]^{-1} \mathbb E_n[\hat{x}_i y_i] = \newline
&amp;amp;= \mathbb E_n[z_i z_i^{-1} x_i x&#39;_i]^{-1} \mathbb E_n[z_i z_i^{-1} x_i y_i] = \newline
&amp;amp;= \mathbb E_n[x_i x&#39;_i]^{-1} \mathbb E_n[x_i y_i] = \newline
&amp;amp;= \hat{\delta} _ {OLS}
\end{aligned}
$$ $$\tag*{$\blacksquare$}$$&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;You have this overfitting problem in general when the number of
instruments is large relative to the sample size. This problem arises
even if the instruments are valid.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;example-from-angrist-1992&#34;&gt;Example from Angrist (1992)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;They regress wages on years of schooling.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Problem&lt;/strong&gt;: endogeneity: both variables are correlated with skills
which are unobserved.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Solution&lt;/strong&gt;: instrument years of schooling with the quarter of
birth.
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Idea&lt;/strong&gt;: if born in the first three quarters, can attend school
from the year of your sixth birthday. Otherwise, you have to
wait one more year.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Problem&lt;/strong&gt;: quarters of birth are three dummies.
&lt;ul&gt;
&lt;li&gt;In order to ``improve the first stage fitâ they interact them
with year of birth (180 effective instruments) and also with the
state (1527 effective instruments).&lt;/li&gt;
&lt;li&gt;This mechanically increases the $R^2$ but also increases the
bias of the 2SLS estimator.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Solutions&lt;/strong&gt;: LIML, JIVE, RJIVE (Hansen et al., 2014), Post-Lasso
(Belloni et al., 2012).&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;example-from-angrist-1992-1&#34;&gt;Example from Angrist (1992)&lt;/h3&gt;
&lt;img src=&#34;../img/Fig_441.png&#34; style=&#34;width:80.0%&#34; /&gt;
&lt;h2 id=&#34;many-instrument-robust-estimation&#34;&gt;Many Instrument Robust Estimation&lt;/h2&gt;
&lt;h3 id=&#34;issue&#34;&gt;Issue&lt;/h3&gt;
&lt;p&gt;Why having too many instruments is problematic? As the number of
instruments increases, the estimated coefficient gets closer to OLS
which is biased. As seen in the theorem above, for $L=n$, the two
estimators coincide.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;../img/Fig_451.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;liml&#34;&gt;LIML&lt;/h3&gt;
&lt;p&gt;An alternative method to estimate the parameters of the structural
equation is by maximum likelihood. Anderson and Rubin (1949) derived the
maximum likelihood estimator for the joint distribution of $(y_i, x_i)$.
The estimator is known as &lt;strong&gt;limited information maximum likelihood&lt;/strong&gt;, or
&lt;strong&gt;LIML&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;This estimator is called âlimited informationâ because it is based on
the structural equation for $(y_i, x_i)$ combined with the reduced form
equation for $x_i$. If maximum likelihood is derived based on a
structural equation for $x_i$ as well, then this leads to what is known
as &lt;strong&gt;full information maximum likelihood (FIML)&lt;/strong&gt;. The advantage of the
LIML approach relative to FIML is that the former does not require a
structural model for $x_i$, and thus allows the researcher to focus on
the structural equation of interest - that for $y_i$.&lt;/p&gt;
&lt;h3 id=&#34;k-class-estimators&#34;&gt;K-class Estimators&lt;/h3&gt;
&lt;p&gt;The &lt;strong&gt;k-class&lt;/strong&gt; estimators have the form $$
\hat{\delta}(\alpha) = (X&#39; P_Z X - \alpha X&#39; X)^{-1} (X&#39; P_Z Y - \alpha X&#39; Y)
$$&lt;/p&gt;
&lt;p&gt;The limited information maximum likelihood estimator &lt;strong&gt;LIML&lt;/strong&gt; is the
k-class estimator $\hat{\delta}(\alpha)$ where $$
\alpha = \lambda_{min} \Big( ([X&#39; , Y]^{-1} [X&#39; , Y])^{-1} [X&#39; , Y]^{-1} P_Z [X&#39; , Y] \Big)
$$&lt;/p&gt;
&lt;p&gt;If $\alpha = 0$ then
$\hat{\delta} _ {\text{LIML}} = \hat{\delta} _ {\text{2SLS}}$ while for
$\alpha \to \infty$,
$\hat{\delta} _ {\text{LIML}} \to \hat{\delta} _ {\text{OLS}}$.&lt;/p&gt;
&lt;h3 id=&#34;comments-on-liml&#34;&gt;Comments on LIML&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;The particular choice of $\alpha$ gives a many instruments robust
estimate&lt;/li&gt;
&lt;li&gt;The LIML estimator has no finite sample moments.
$\mathbb E[\delta(\alpha_{LIML})]$ does not exist in general&lt;/li&gt;
&lt;li&gt;In simulation studies performs well&lt;/li&gt;
&lt;li&gt;Has good asymptotic properties&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Asymptotically the LIML estimator has the same distribution as 2SLS.
However, they can have quite different behaviors in finite samples.
There is considerable evidence that the LIML estimator has superior
finite sample performance to 2SLS when there are many instruments or the
reduced form is weak. However, on the other hand there is worry that
since the LIML estimator is derived under normality it may not be robust
in non-normal settings.&lt;/p&gt;
&lt;h3 id=&#34;jive&#34;&gt;JIVE&lt;/h3&gt;
&lt;p&gt;The &lt;strong&gt;Jacknife IV&lt;/strong&gt; procedure is the following&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Regress $\lbrace x_j \rbrace _ {j \neq i}$ on
$\lbrace z_j \rbrace _ {j \neq i}$ and estimate $\pi_{-i}$ (leave
the $i^{th}$ observation out).&lt;/li&gt;
&lt;li&gt;Form $\hat{x}_i = \hat{\pi} _ {-i} z_i$.&lt;/li&gt;
&lt;li&gt;Run IV using $\hat{x}_i$ as instruments. $$
\hat{\delta} _ {JIVE} = \mathbb E_n[\hat{x}_i x_i&#39;]^{-1} \mathbb E_n[\hat{x}_i y_i&#39;]
$$&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;comments-on-jive&#34;&gt;Comments on JIVE:&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Prevents overfitting.&lt;/li&gt;
&lt;li&gt;With many instruments you get bad out of sample prediction which
implies low correlation between $\hat{x}_i$ and $x_i$:
$\mathbb E_n[\hat{x}_i x_i&#39;] \simeq 0$.&lt;/li&gt;
&lt;li&gt;Use lasso/ridge regression in the first stage in case of too many
instruments.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;hausman-test&#34;&gt;Hausman Test&lt;/h3&gt;
&lt;p&gt;Here we consider testing the validity of OLS. OLS is generally preferred
to IV in terms of precision. Many researchers only doubt the (joint)
validity of the regressor $z_i$ instead of being certain that it is
invalid (in the sense of not being predetermined). So then they wish to
choose between OLS and 2SLS, assuming that they have an instrument
vector $x_i$ whose validity is not in question. Further, assume for
simplicity that $L = K$ so that the efficient GMM estimator is the IV
estimator.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;Hausman test statistic&lt;/strong&gt; $$
H \equiv n (\hat{\delta} _ {IV} - \hat{\delta} _ {OLS})&#39; [\hat{Avar} (\hat{\delta} _ {IV} - \hat{\delta} _ {OLS})]^{-1} (\hat{\delta} _ {IV} - \hat{\delta} _ {OLS})
$$ is asymptotically distributed as a $\chi^2_{L-s}$ under the null
where $s = | z_i \cup x_i |$: the number of regressors that are retained
as instruments in $x_i$.&lt;/p&gt;
&lt;h3 id=&#34;comments-1&#34;&gt;Comments&lt;/h3&gt;
&lt;p&gt;In general, the idea of the Hausman test is the following. If you have
two estimators, one which is efficient under $H_0$ but inconsistent
under $H_1$ (in this case, OLS), and another which is consistent under
$H_1$ (in this case, IV), then construct a test as a quadratic form in
the differences of the estimators. Another classic example arises in
panel data with the hypothesis $H_0$ of unconditional strict exogeneity.
In that case, under $H_0$ Random Effects estimators are efficient but
under $H_1$ they are inconsistent. Fixed Effects estimators instead are
consistent under $H_1$.&lt;/p&gt;
&lt;p&gt;The Hausman test statistic can be used as a pretest procedure: select
either OLS or IV according to the outcome of the test. Although widely
used, this pretest procedure is not advisable. When the null is false,
it is still possible that the test &lt;em&gt;accepts&lt;/em&gt; the null (committing a Type
2 error). In particular, this can happen with a high probability when
the sample size is &lt;em&gt;small&lt;/em&gt; and/or when the regressor $z_i$ is &lt;em&gt;almost
valid&lt;/em&gt;. In such an instance, estimation and also inference will be based
on incorrect methods. Therefore, the overall properties of the Hausman
pretest procedure are undesirable.&lt;/p&gt;
&lt;p&gt;The Hausman test is an example of a specification test. There are many
other specification tests. One could for example test for conditional
homoskedasticity. Unlike for the OLS case, there does not exist a
convenient test for conditional homoskedasticity for the GMM case. A
test statistic that is asymptotically chi-squared under the null is
available but is extremely cumbersome; see White (1982, note 2). If in
doubt, it is better to use the more generally valid inference methods
that allow for conditional heteroskedasticity. Similarly, there does not
exist a convenient test for serial correlation for the GMM case. If in
doubt, it is better to use the more generally valid inference methods
that allow for serial correlation; for example, when data are collected
over time (that is, time-series data).&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Non-Parametric Estimation</title>
      <link>https://matteocourthoud.github.io/course/metrics/08_nonparametric/</link>
      <pubDate>Fri, 29 Oct 2021 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/course/metrics/08_nonparametric/</guid>
      <description>&lt;h3 id=&#34;introduction&#34;&gt;Introduction&lt;/h3&gt;
&lt;p&gt;Non-parametric regression is a flexible estimation procedure for&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;regression functions $\mathbb E [y|x ] = g (x)$ and&lt;/li&gt;
&lt;li&gt;density functions $f(x)$.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;You want to let your data to tell you how flexible you can afford to be
in terms of estimation procedures. Non-parametric regression is
naturally introduced in terms of fitting a curve.&lt;/p&gt;
&lt;p&gt;Consider the problem of estimating the Conditional Expectation Function,
defined as $\mathbb E [y_i |x_i ] = g(x_i)$ given data
$D = (x_i, y_i)_{i=1}^n$ under minimal assumption of $g(\cdot)$,
e.g.Â smoothness. There are two main methods:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Local methods: Kernel-based estimation&lt;/li&gt;
&lt;li&gt;Global methods: Series-based estimation&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Another way of looking at non-parametrics is to do estimation/inference
without specifying functional forms. With no assumptions, informative
inference is impossible. Non parametrics tries to work with functional
restrictionsâcontinuity, differentiability, etc.ârather than
pre-specifying functional form.&lt;/p&gt;
&lt;h3 id=&#34;discrete-x---cell-estimator&#34;&gt;Discrete x - Cell Estimator&lt;/h3&gt;
&lt;p&gt;Suppose that $x$ can take $R$ distinct values, e.g.Â gender $R=2$, years
of schooling $R=20$, gender $\times$ years of schooling
$R = 2 \times 20$.&lt;/p&gt;
&lt;p&gt;A simple way for estimating $\mathbb E \left[ y |x \right] = g(x)$ is to
split the sample to include observations with $x_i = x$ and calculate
the sample mean of $\bar{y}$ for these observations. Note that this
requires no assumptions about how $\mathbb E [y_i |x_i]$ varies with $x$
since we fit a different value for each value $x$. $$
\hat{g}(x) = \frac{1}{| i: x_i = x |} \sum_{i : x_i = x} y_i
$$&lt;/p&gt;
&lt;p&gt;Issues:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Curse of dimensionality&lt;/strong&gt;: if $R$ is big compared to $n$, there
will be only a small number of observations per $x$ values. If $x_i$
is continuous, $R=n$ with probability 1. Solution: we can borrow
information about $g_0(x)$ using neighboring observations of $x$.&lt;/li&gt;
&lt;li&gt;Averaging for each separate $x_r$ value is only feasible in cases
where $x_i$ is coarsely discrete.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;local-non-parametric-estimation&#34;&gt;Local Non-Parametric Estimation&lt;/h2&gt;
&lt;h3 id=&#34;kernels&#34;&gt;Kernels&lt;/h3&gt;
&lt;p&gt;Suppose we believe that $\mathbb E [y_i |x_i]$ is a smooth function of
$x_i$ â e.g.Â continuous, differentiable, etc. Then it should not change
too much across values of $x$ that are close to each other: we can
estimate the conditional expectation at $x = \bar{x}$ by averaging $y$âs
over the values of $x$ that are âcloseââ to $\bar{x}$. This procedure
relies on two (three) arbitrary choices:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Choice of the &lt;strong&gt;kernel function&lt;/strong&gt; $K (\cdot)$; it is used to weight
âfar outââ observations, such that
&lt;ul&gt;
&lt;li&gt;$K: \mathbb R \to \mathbb R$&lt;/li&gt;
&lt;li&gt;$K$ is symmetric: $K(\bar{x} + x_i) = K(\bar{x} - x_i)$&lt;/li&gt;
&lt;li&gt;$\lim_{x_i \to \infty}K(x_i - \bar{x}) = 0$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Choice of the &lt;strong&gt;bandwidth&lt;/strong&gt; $h$: it measures the size of a
``smallââ window around $\bar{x}$,
e.g.Â $(\bar{x} - h, \bar{x} + h)$.&lt;/li&gt;
&lt;li&gt;Choice of the local estimation procedure. Examples are locally
constant, a.k.a. Nadaraya-Watson (&lt;strong&gt;NW&lt;/strong&gt;), and locally linear
(&lt;strong&gt;LL&lt;/strong&gt;).&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;Generally, the choice of $h$ is more important than $K(\cdot)$ in low
dimensional settings.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;optimal-h&#34;&gt;Optimal h&lt;/h3&gt;
&lt;p&gt;We need to define what is an âoptimalâ $h$, depending on the smoothness
level of $g_0$, typically unknown. The choice of $h$ relates to the
bias-variance trade-off:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;large $h$: small variance, higher bias;&lt;/li&gt;
&lt;li&gt;small $h$: high variance, smaller bias.&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;Note that $K_h (\cdot) = K (\cdot / h)$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;locally-constant-estimator&#34;&gt;Locally Constant Estimator&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Nadaraya-Watson&lt;/strong&gt; estimator, or locally constant estimator. It
assumes the CEF locally takes the form $g(x) = \beta_0(x)$. The
local parameter is estimated as: $$
\hat{\beta}&lt;em&gt;0 (\bar{x}) = \arg\min&lt;/em&gt;{\beta_0}  \quad  \mathbb E_n \Big[ K_h (x_i - \bar{x}) \cdot  \big(y_i - \beta_0 \big)^2 \Big]
$$&lt;/li&gt;
&lt;/ul&gt;
&lt;img src=&#34;../img/Fig_521.png&#34; style=&#34;width:50.0%&#34; /&gt;
&lt;h3 id=&#34;cef&#34;&gt;CEF&lt;/h3&gt;
&lt;p&gt;The Nadaraya-Watson estimate of the CEF takes the form: $$
\mathbb E_n \left[ y | x = \bar{x}\right] = \hat{g}(\bar{x}) = \frac{\sum_{i=1}^n y_i K_h (x_i - \bar{x})}{\sum_{i=1}^n K_h (x_i - \bar{x})}
$$&lt;/p&gt;
&lt;h3 id=&#34;locally-linear-estimator&#34;&gt;Locally Linear Estimator&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Local Linear&lt;/strong&gt; estimator. It assumes the CEF locally takes the
form $g(x) = \beta_0(x) + \beta_1(x) x$. The local parameters are
estimated as:&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$
\left( \hat{\beta}_0 (\bar{x}), \hat{\beta}&lt;em&gt;1 (\bar{x}) \right) = \arg\min&lt;/em&gt;{\beta_0, \beta_1}  \quad   \mathbb E_n \Big[ K_h (x_i - \bar{x}) \cdot  \big(y_i - \beta_0 - (x_i - \bar{x}) \beta_1 \big)^2 \Big]
$$&lt;/p&gt;
&lt;img src=&#34;../img/Fig_522.png&#34; style=&#34;width:50.0%&#34; /&gt;
&lt;h3 id=&#34;cef-1&#34;&gt;CEF&lt;/h3&gt;
&lt;p&gt;In this case, we do LS estimate with $i$âs contribution of residual
weighted by the kernel $K_h (x_i - \bar{x})$. The final estimate at
$\bar{x}$ is given by: $$
\hat{g} (\bar{x}) = \hat{\beta}_0 (\bar{x}) + (\bar{x} - \bar{x}) \hat{\beta}_1 (\bar{x}) = \hat{\beta}_0 (\bar{x})
$$ since we have centered the $x_s$ at $\bar{x}$ in the kernel. - It is
possible to add linearly higher order polynomials, e.g.Â do locally
quadratic least squares using loss function:&lt;/p&gt;
&lt;p&gt;$$
\mathbb E_n \left[ K_h (x_i - \bar{x}) \big(y_i - \beta_0 - (x_i - \bar{x}) \beta_1 - (x_i - \bar{x})^2 \beta_2 \big)^2 \right]
$$&lt;/p&gt;
&lt;h3 id=&#34;uniform-kernel&#34;&gt;Uniform Kernel&lt;/h3&gt;
&lt;p&gt;LS restricted to sample $i$ such that $x_i$ within $h$ of $\bar{x}$. $$
\begin{aligned}
&amp;amp; K (\cdot) = \mathbb I\lbrace \cdot \in [-1, 1] \rbrace  \newline
&amp;amp; K_h (\cdot) = \mathbb I\lbrace \cdot/h \in [-1, 1] \rbrace = \mathbb I\lbrace \cdot \in [-h, h] \rbrace  \newline
&amp;amp; K_h (x_i - \bar{x}) = \mathbb I\lbrace x_i - \bar{x} \in [-h, h] \rbrace  = \mathbb I\lbrace x_i \in [\bar{x}-h, \bar{x} + h] \rbrace
\end{aligned}
$$ Employed together with the locally linear estimator, the estimation
procedure reduces to **local least squares}. The loss function is: $$
\mathbb E_n \Big[ K_n (x_i - \bar{x}) \big(y_i -\beta_0 - \beta_1 (x_i - \bar{x}) \big)^2 \Big] = \frac{1}{n} \sum_{i: x_i \in [\bar{x}-h, \bar{x} +h ]}  \big(y_i -\beta_0 - \beta_1 (x_i - \bar{x}) \big)^2
$$&lt;/p&gt;
&lt;p&gt;The more local is the estimation, the more appropriate the linear
regression: if $g_0$ is smooth,
$g_0(\bar{x}) + g_0&#39;(\bar{x}) (x_i - \bar{x})$ is a better approximation
for $g_0 (x_i)$.&lt;/p&gt;
&lt;p&gt;However, the uniform density is not a good kernel choice as it produces
discontinuous CEF estimates. The following are two popular alternative
choices that produce continuous CEF estimates.&lt;/p&gt;
&lt;h3 id=&#34;other-kernels&#34;&gt;Other Kernels&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Epanechnikov kernel&lt;/strong&gt; $$
K_h(x_i - \bar{x}) = \frac { 3 } { 4 } \left( 1 - (x_i - \bar{x}) ^ { 2 } \right)  \mathbb I\lbrace x_i \in [\bar{x}-h, \bar{x} + h] \rbrace
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Normal or Gaussian kernel&lt;/strong&gt; $$
K_\phi (x_i - \bar{x})  = \frac { 1 } { \sqrt { 2 \pi } } \exp \left( - \frac { (x_i - \bar{x}) ^ { 2 } } { 2 } \right)
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;K-Nearest Neighbors (KNN)&lt;/strong&gt;: choose bandwidth so that there is a
fixed number of observations in each kernel. This kernel is
different from the others since it takes a nonparamentric form.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;example&#34;&gt;Example&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;../img/Fig_523.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;choice-of-the-optimal-bandwidth&#34;&gt;Choice of the optimal bandwidth&lt;/h3&gt;
&lt;p&gt;Practical methods:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Eyeball Method.&lt;/strong&gt; (i) Choose a bandwidth (ii) Estimate the
regression function (iii) Look at the result: if it looks more
wiggly than you would like, increase the bandwidth: if it looks more
smooth than you would like, decrease the bandwidth. Con: It only
works for $\dim(x_i) = 1$ or $2$.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Rule of Thumb.&lt;/strong&gt; For example, Silvermanâs rule of thumb:
$h = \left( \frac{4 \hat{\sigma}^5}{3n} \right)^{\frac{1}{5}}$. Con:
It requires too much knowledge about $g_0$ (i.e.Â normality) which
you donât have.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Cross Validation.&lt;/strong&gt; Under some assumptions, CV will approximately
gives the MSE optimal bandwidth. The basic idea is to evaluate
quality of the bandwidth by looking at how well the resulting
estimator forecasts in the given sample.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Leave-one-out CV. For each $h &amp;gt; 0$ and each $i$, $\hat{g}&lt;em&gt;{-i} (x_i)$ is
the estimate of the conditional expectation at $x_i$ using bandwidth $h$
and all observations expect observation $i$. The CV bandwidth is defined
as $$
\hat{h} = \arg \min_h CV(h) = \arg \min_h \sum&lt;/em&gt;{i=1}^n  \Big( y_i -  \hat{g}_{-i} (x_i) \Big)^2
$$&lt;/p&gt;
&lt;h3 id=&#34;practical-tips&#34;&gt;Practical Tips&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Select a value for $h$.&lt;/li&gt;
&lt;li&gt;For each observation $i$, calculate $$
\hat{g}&lt;em&gt;{-i} (x_i) = \frac{\sum&lt;/em&gt;{j \ne i} y_j K_h (x_j - x_i) }{\sum_{i=1}^n K_h (x_j - x_i)}, \qquad e_{i,h}^2 = \left(y_i - \hat{g}_{-i} (x_i) \right)^2
$$&lt;/li&gt;
&lt;li&gt;Calculate $\text{CV}(h) = \sum_{i=1}^n e^2_{i,h}$.&lt;/li&gt;
&lt;li&gt;Repeat for each $h$ and choose the one that minimizes
$\text{CV}(h)$.&lt;/li&gt;
&lt;/ul&gt;
&lt;img src=&#34;../img/Fig_524.png&#34; style=&#34;width:50.0%&#34; /&gt;
&lt;h3 id=&#34;inference&#34;&gt;Inference&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt;: Consider data $\lbrace y_i, x_i \rbrace_{i=1}^n$, iid and
suppose that $y_i = g(x_i) + \varepsilon_i$ where
$\mathbb E[\varepsilon_i|x_i] = 0$. Assume that $x_i \in Interior(X)$
where $X \subseteq \mathbb R$, $g(x)$ and $f(x)$ are three times
continuously differentiable, and $f(x) &amp;gt; 0$ on $X$. $f(x)$ is the
probability density of $x \in X$ , and $g(x)$ is the function of
interest. Suppose that $K(\cdot)$ is a kernel function. Suppose
$n\to\infty$, $h\to0$ , $nh\to\infty$, and $nh^7\to0$. Then for any
fixed $x\in X$, $$
AMSE = \sqrt{nh} \Big( \hat{g}(x) - g(x) - h^2 B(x)\Big) \overset{d}{\to} N \left( 0, \frac{\kappa \sigma^2(x)}{f(x)}\right)
$$ for $\sigma^2(x) = Var(y_i|x_i = x)$, $\kappa = \int K^2(v)dv$, and
$B(x) = \frac{\kappa_2}{2} \frac{f&#39;(x)g&#39;(x) + f(x) g&#39;&#39;(x)}{f(x)}$ where
$\kappa_2 = \int v^2 K(v)dv$.&lt;/p&gt;
&lt;h3 id=&#34;remarks&#34;&gt;Remarks&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;If the function is smooth enough and the bandwidth small enough, you
can ignore the bias relative to sampling variation. To make this
plausible, use a smaller bandwidth than would be the âoptimalâ.&lt;/li&gt;
&lt;li&gt;All kernel regression estimators can be written as a weighted
average $$
\hat{g}(x) = \frac{1}{n} \sum_{i=1}^n w_i (x) y_i, \quad \text{ with } \quad w_i (x) = \frac{n K_h (x_i - x)}{\sum_{i=1}^n K_h (x_i - x)}
$$ Do inference as if you were estimating a mean $\mathbb E[z_i]$
with sample mean $\frac{1}{n} \sum_{i=1}^n z_i$ using
$z_i = w_i (x) y_i$.&lt;/li&gt;
&lt;li&gt;If you are doing inference at more than one value of $x$, do
inference as in the previous point, treating each value of $x$ as a
different sample mean and note that even with independent data,
these means will be correlated in general because there will
generally be some common observations in to each of the averages. If
you have a time series, make sure you account for correlation
between the observations going in the different averages even if
they donât overlap.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Issue when doing inference: the estimation of the bandwidth from the
data is generally not accounted for in the distributional approximation
(when doing inference). In large-samples, this is unlikely to lead to
large changes, but uncertainty is understated in small samples.&lt;/p&gt;
&lt;h3 id=&#34;bias-variance-trade-off&#34;&gt;Bias-Variance Trade-off&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;For any estimator mean-square error MSE is decomposable into variance
and bias-squared: $$
\text{MSE} (\bar{x}, \hat{g}) = \mathbb E \left[ \left( \hat{g}(\bar{x}) - g_0 (\bar{x}) \right)^2 \right] = \mathbb E \Big[\underbrace{ \hat{g}(\bar{x}) - g_0 (\bar{x}) }_{\text{Bias}} \Big]^2 +  Var (\hat{g} (\bar{x})).
$$&lt;/p&gt;
&lt;h3 id=&#34;proof&#34;&gt;Proof&lt;/h3&gt;
&lt;p&gt;The theorem follows from the following corollary.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Corollary&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Let $A$ be a random variable and $\theta_0$ a fixed parameter. Then, $$
\mathbb E [ (A - \theta_0)^2] = Var (A) + \mathbb E [A-\theta_0]^2
$$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt; $$
\begin{aligned}
\mathbb E [ (A - \theta_0)^2] &amp;amp; = \mathbb E[A^2] - 2 \mathbb E [A \theta_0] + \mathbb E [\theta_0] \newline
&amp;amp;  = \mathbb E[A^2] \underbrace{-  \mathbb E[A]^2 + E[A]^2}_{\text{add and subtract}} - 2 \mathbb E [A \theta_0] + \mathbb E [\theta_0] \newline
&amp;amp;  = Var(A) + \mathbb E [A]^2 - 2 \theta_0 \mathbb E [A ] + \mathbb E [\theta_0] \newline
&amp;amp; = Var(A) + \mathbb E [A - \theta_0]^2
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;Note that $\mathbb E [ (A - \theta_0)^2] = \mathbb E [A - \theta_0]^2$.
$$\tag*{$\blacksquare$}$$&lt;/p&gt;
&lt;h3 id=&#34;criteria&#34;&gt;Criteria&lt;/h3&gt;
&lt;p&gt;Which criteria should we use with non-parametric estimators?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Mean squared error (MSE)&lt;/strong&gt;: $$
\text{MSE} (\bar{x}) (\hat{g}) = \mathbb E \left[ \left( \hat{g}(\bar{x}) - g_0 (\bar{x}) \right)^2 \right]
$$ &lt;strong&gt;NB!&lt;/strong&gt; This is the criterium we are going to use.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Integrated mean squared error (IMSE)&lt;/strong&gt;: $$
\text{IMSE} ( \hat{g} ) = \mathbb E \left[ \int | \hat{g} (x) - g_0 (x) |^2 \mathrm{d} F(x)  \right]
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Type I - Type II error.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;comments&#34;&gt;Comments&lt;/h3&gt;
&lt;p&gt;Hansen (2019): the theorem above implies that we can asymptotically
approximate the MSE as $$
\text{AMSE} = \Big( h^2 \sigma_k^2 B(x) \Big)^2 + \frac{\kappa \sigma^2(x)}{nh f(x)} \approx \text{const} \cdot \left( h^4 + \frac{1}{n h} \right)
$$&lt;/p&gt;
&lt;p&gt;Where&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$Var \propto \frac{1}{h n}$, where you can think of $n h$ as the
&lt;strong&gt;effective sample size&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;Bias $\propto h^2$, derived if $g_0$ is twice continuously
differentiable using Taylor expansion.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;trade-off&#34;&gt;Trade-Off&lt;/h3&gt;
&lt;p&gt;The asymptotic MSE is dominated by the larger of $h^4$ and
$\frac{1}{h n}$. Notice that the bias is increasing in $h$ and the
variance is decreasing in $h$ (more smoothing means more observations
are used for local estimation: this increases the bias but decreases
estimation variance). To select $h$ to minimize the asymptotic MSE,
these two components should balance each other: $$
\frac{1}{h n} \propto h^4 \quad \Rightarrow \quad  h \propto n^{-1/5}
$$&lt;/p&gt;
&lt;p&gt;This result means that the bandwidth should take the form
$h = c \cdot n^{-1/5}$. The optimal constant $c$ depends on the kernel
$k$ the bias function $B(x)$ and the marginal density $f_x(x)$. A common
misinterpretation is to set $h = n^{-1/5}$ which is equivalent to
setting $c = 1$ and is completely arbitrary. Instead, an empirical
bandwidth selection rule such as cross-validation should be used in
practice.&lt;/p&gt;
&lt;h2 id=&#34;global-non-parametric-estimation&#34;&gt;Global Non-Parametric Estimation&lt;/h2&gt;
&lt;h3 id=&#34;series&#34;&gt;Series&lt;/h3&gt;
&lt;p&gt;The goal is to try to globally approximate the CEF with a function
$g(x)$. Series methods are based on the &lt;strong&gt;Stone-Weierstrass theorem&lt;/strong&gt;: a
real-valued continuous function $g(x)$ defined in a compact set can be
approximated with polynomials for any degree of accuracy $$
g_0 (x) = p_1 (x) \beta _1 + \dots + p_K (x) \beta_K + r(x)
$$ where $p_1(x), \dots, p_K(x)$ are called ``a dictionary of
approximating seriesââ and $r(x)$ is a remainder function. If
$p_1(x), \dots, p_K(x)$ are sufficiently rich, $r(x)$ will be small. If
$K \to \infty$, then $r \to 0$.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Example - Taylor series: if $g(x)$ is infinitely differentiable, then
$$
g(x) = \sum_{k=0}^{\infty } a_k x^k
$$ where $a_k = \frac{1}{k!} \frac{\partial^k g_0}{\partial x^k}$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;in-practice&#34;&gt;In Practice&lt;/h3&gt;
&lt;p&gt;The basic idea is to approximate the infinite sum by chopping it off
after $K$ terms and then estimate the coefficients by OLS.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Series estimation&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Choose $K$, i.e.Â the number of series terms, and an approximating
dictionary $p_1(x), \dots, p_K(x)$&lt;/li&gt;
&lt;li&gt;Expand data to
$D = \left( y_i, p_1(x_i), \dots, p_K(x_i) \right)_{i=1}^n$&lt;/li&gt;
&lt;li&gt;Estimate OLS to get $\hat{\beta}_1, \dots, \hat{\beta}_K$&lt;/li&gt;
&lt;li&gt;Set
$\hat{g}(x) = p_1 (x)\hat{\beta}_1 + \dots + p_K(x) \hat{\beta}_K$&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;examples&#34;&gt;Examples&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Monomials&lt;/strong&gt;: $p_1(x) = 1, p_2(x) = x, p_3(x)=x^2, \dots$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Hermite Polynomials&lt;/strong&gt;: $p_1(x) = 1$, $p_2(x) = x$,
$p_3(x)=x^2 -1$, $p_4(x)= x^3 - 3x, \dots$. Con: &lt;strong&gt;edge effects&lt;/strong&gt;.
The estimated function is particularly volatile at the edges of the
sample space (Gibbs effect)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Trig Polynomials&lt;/strong&gt;: $p_1(x) = 1$, $p_2(x) = \cos 2 \pi x$,
$p_3(x)= \sin 2 \pi x$, $p_4(x) = \cos 2 \pi x \cdot 2 x \dots$.
Pro: cyclical therefore good for series. Con: edge effects&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;B-splines&lt;/strong&gt;: recursively constructed using knot points $$
B_{i, 0} = \begin{cases}
1 &amp;amp; \text{if } t_i \leq x &amp;lt; t_{i+1} \newline 0 &amp;amp; \text{otherwise}
\end{cases} \qquad B_{i_k} (x) = \frac{x - t_i}{ t_{i+k} - t_i} B_{i, k-1} (x) +  \frac{t_{i+k+1}-x}{t_{i+k+1} - t_{i+1}} B_{i+1, k-1} (x)
$$ where $t_0, \dots, t_i, \dots$ are knot points and $k$ is the
order of the spline. Pro: faster rate of convergence and lower
asymptotic bias.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;hermite-polynomials&#34;&gt;Hermite Polynomials&lt;/h3&gt;
&lt;img src=&#34;../img/Fig_531.png&#34; style=&#34;width:50.0%&#34; /&gt;
&lt;h3 id=&#34;estimation&#34;&gt;Estimation&lt;/h3&gt;
&lt;p&gt;Given $K$, inference proceeds exactly as if one had run an OLS of $y$ on
$(p_k)_{k=1}^K$. The idea is that you ignore that you are doing
non-parametric regression as long as you believe you have put enough
terms (high $K$). Then the function is smooth enough so that the bias of
the approximation is small relative to the variance (see Newey, 1997).
Note that his approximation does not account for data-dependent
estimation of the bandwidth.&lt;/p&gt;
&lt;h3 id=&#34;consistency&#34;&gt;Consistency&lt;/h3&gt;
&lt;p&gt;Newey (1997): results about consistency of $\hat{g}$ and asymptotic
normality of $\hat{g}$.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;OLS: $\hat{\beta} \overset{p}{\to} \beta_0$&lt;/li&gt;
&lt;li&gt;Non-parametric: you have a sequence $\lbrace\beta_k\rbrace_{k=1}^K$
with $\hat{\beta}_k \overset{p}{\to} \beta_k$ as $n \to \infty$ (as
$k \to \infty$). However, this does not make sense because
$\lbrace\beta_k\rbrace$ is not constant. Moreover, $\beta_k$ is not
the quantity of interest. We want to make inference on $\hat{g}(x)$.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Under regularity conditions, including
$| | \hat{\beta} - \beta_0 | | \overset{p}{\to} 0$,&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Uniform Consistency:
$\sup_x | \hat{g}(x) - g_0(x)| \overset{p}{\to} 0$&lt;/li&gt;
&lt;li&gt;Mean-square Consistency:
$\int | \hat{g}(x) - g_0(x)|^2 \mathrm{d} F(x) \overset{p}{\to} 0$&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;imse&#34;&gt;IMSE&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Under the following assumptions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$(x_i, y_i)$ are iid and $Var(y_i|x_i)$ is bounded;&lt;/li&gt;
&lt;li&gt;For all $K$, there exists a non-singular matrix $B$ such that
$A = \left[ (B p(x)) (B p(x))&#39; \right]$ where
$p(x) = \left( p_1(x), \dots, p_K (x) \right)$ has the properties
that $\lambda_{\min} (A)^{-1} = O(1)$. In addition,
$\sup_x | | B p(x) | | = o(\sqrt{K/n})$.&lt;/li&gt;
&lt;li&gt;There exists $\alpha$ and $\beta_K$ for all $K$ such that $$
\sup_x | g_0 (x) - p(x) \beta_K | = O_p(K^{-\alpha})
$$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Then, it holds that&lt;/p&gt;
&lt;p&gt;$$
\text{IMSE = }\int \left( g_0 (x) - \hat{g} (x) \right)^2 \mathrm{d} F(x) = O_p \left( \frac{K}{n} + K^{-2\alpha}\right)
$$&lt;/p&gt;
&lt;h3 id=&#34;choice-of-the-optimal-k&#34;&gt;Choice of the optimal $K$&lt;/h3&gt;
&lt;p&gt;The bias-variance trade-off for series comes in through the choice of
$K$:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Higher $K$: smaller bias, since we are leaving out less terms form
the infinite sum.&lt;/li&gt;
&lt;li&gt;Smaller $K$: smaller variance, since we are estimating less
regression coefficients from the same amount of data.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Cross-validation for series&lt;/strong&gt;: For each $K \geq 0$ and for each
$i=1, \dots, n$, consider&lt;/p&gt;
&lt;p&gt;$$
D_{-i} = \lbrace (x_1, y_1), \dots, (x_{i-1}, y_{i-1}),(x_{i+1}, y_{i+1}), \dots (x_n, y_n) \rbrace
$$ and calculate $\hat{g}^{(K)}_{-i} (x)$ using series estimate with
$p_1(x), \dots, p_K (x)$ in order to get
$e^{(K)}&lt;em&gt;i = y_i - \hat{g}^{(K)}&lt;/em&gt;{-i} (x_i)$. Choose $\hat{K}$ such that&lt;/p&gt;
&lt;p&gt;$$
\hat{K} = \arg \min_K \mathbb E_n \left[ {e^{(K)}_i}^2 \right]
$$&lt;/p&gt;
&lt;h3 id=&#34;inference-1&#34;&gt;Inference&lt;/h3&gt;
&lt;p&gt;Consider the data $D = \lbrace (x_i, y_i) \rbrace_{i=1}^n$ such that
$y_i = g_0 (x_i) + \varepsilon_i$. You may want to form confidence
intervals for quantities that depends on $g_0$.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Example: $\theta_0$ functional forms of interests:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Point estimate: $\theta_0 = g_0 (\bar{x} )$ for fixed $\bar{x}$&lt;/li&gt;
&lt;li&gt;Interval estimate: $\theta_0 = g_0 (\bar{x}_2) - g_0 (\bar{x}_1)$&lt;/li&gt;
&lt;li&gt;Point derivative estimate: $\theta_0 = g_0 &#39; (\bar{x})$ at
$\bar{x}$&lt;/li&gt;
&lt;li&gt;Average derivative $\theta_0 = \mathbb E [g_0 &#39; (x) ]$&lt;/li&gt;
&lt;li&gt;Consumer surplus: $\theta_0 = \int_a^b g_0(x)dx \quad$ when $g_0$
is a demand function.&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;p&gt;Those estimates are functionals: maps from a function to a real number.
We are doing inference on a function now, not on a point estimate.&lt;/p&gt;
&lt;h3 id=&#34;inference-2&#34;&gt;Inference&lt;/h3&gt;
&lt;p&gt;In order to form a confidence interval for $\theta_0$, with series you
can&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Undersmooth&lt;/strong&gt;: in order to apply a
&lt;code&gt;\textit{central limit theorem}&lt;/code&gt;{=tex}, you need deviations around
the function to be approximately gaussian. Undersmoothing makes the
function oscillate much more than the curve you are estimating in
order to obtain such guassian deviations.&lt;/li&gt;
&lt;li&gt;Use the &lt;strong&gt;delta method&lt;/strong&gt;. It would usually require more series terms
than a criterion like cross-validation would suggest.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;undersmoothing&#34;&gt;Undersmoothing&lt;/h3&gt;
&lt;img src=&#34;../img/Fig_541.png&#34; style=&#34;width:50.0%&#34; /&gt;
&lt;p&gt;If on the contrary you oversmooth (e.g.Â $g_0$ linear), errors are going
to constantly be on either one or the other side of the curve $\to$ not
gaussian!&lt;/p&gt;
&lt;h3 id=&#34;delta-method&#34;&gt;Delta Method&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt;: Under the assumptions of the consistency theorem $$
\frac{\sqrt{n} \Big(\hat{\theta} - \theta_0 + B(r_K) \Big)}{\sqrt{v_K}} \overset{d}{\to} N (0,1)
$$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt;: Under the assumptions of the consistency theorem and
$\sqrt{n} K^{-\alpha} = o(1)$ (or equivalently $n K^{-2\alpha} = O(1)$
in Hansen), $$
\frac{\sqrt{n} \Big(\hat{\theta} - \theta_0 \Big)}{\sqrt{v_K}} \overset{d}{\to} N (0,1)
$$&lt;/p&gt;
&lt;h3 id=&#34;remark&#34;&gt;Remark&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;The rate of convergence of splines is faster than for power series
(Newey 1997).&lt;/li&gt;
&lt;li&gt;We have &lt;strong&gt;undersmoothing&lt;/strong&gt; if $\sqrt{n} K^{\alpha} = o(1)$ (see
comment below)&lt;/li&gt;
&lt;li&gt;Usually, in order to prove asymptotic normality, we first prove
unbiasedness. However here we have a &lt;strong&gt;biased&lt;/strong&gt; estimator but we
make the bias converge to zero faster than the variance.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Hansen (2019): The critical condition is the assumption that
$\sqrt{n} K^{\alpha} = o(1)$ This requires that $K \to \infty$ at a rate
faster than $n^{\frac{1}{2\alpha}}$ This is a troubling condition. The
optimal rate for estimation of $g(x)$ is
$K = O(n^{\frac{1}{1+ 2\alpha}})$. If we set
$K = n^{\frac{1}{1+ 2\alpha}}$ by this rule then
$n K^{-2\alpha} = n^{\frac{1}{1+ 2\alpha}} \to \infty$ not zero. Thus
this assumption is equivalent to assuming that $K$ is much larger than
optimal. The reason why this trick works (that is, why the bias is
negligible) is that by increasing $K$ the asymptotic bias decreases and
the asymptotic variance increases and thus the variance dominates.
Because $K$ is larger than optimal, we typically say that $\hat{g}(x)$
is &lt;strong&gt;undersmoothed&lt;/strong&gt; relative to the optimal series estimator.&lt;/p&gt;
&lt;h3 id=&#34;more-remarks&#34;&gt;More Remarks&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;Many authors like to focus their asymptotic theory on the assumptions
in the theorem, as the distribution of $\theta$ appears cleaner.
However, it is a poor use of asymptotic theory. There are three
problems with the assumption $\sqrt{n} K^{-\alpha} = o(1)$ and the
approximation of the theorem.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;First, it says that if we intentionally pick $K$ to be larger than
optimal, we can increase the estimation variance relative to the
bias so the variance will dominate the bias. But why would we want
to intentionally use an estimator which is sub-optimal?&lt;/li&gt;
&lt;li&gt;Second, the assumption $\sqrt{n} K^{-\alpha} = o(1)$ does not
eliminate the asymptotic bias, it only makes it of lower order
than the variance. So the approximation of the theorem is
technically valid, but the missing asymptotic bias term is just
slightly smaller in asymptotic order, and thus still relevant in
finite samples.&lt;/li&gt;
&lt;li&gt;Third, the condition $\sqrt{n} K^{\alpha} = o(1)$ is just an
assumption, it has nothing to do with actual empirical practice.
Thus the difference between the two theorems is in the
assumptions, not in the actual reality or in the actual empirical
practice. Eliminating a nuisance (the asymptotic bias) through an
assumption is a trick, not a substantive use of theory. My strong
view is that the result (1) is more informative than (2). It shows
that the asymptotic distribution is normal but has a non-trivial
finite sample bias.&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;kernel-vs-series&#34;&gt;Kernel vs Series&lt;/h3&gt;
&lt;p&gt;Hansen (2019): in this and the previous chapter we have presented two
distinct methods of nonparametric regression based on kernel methods and
series methods. Which should be used in practice? Both methods have
advantages and disadvantages and there is no clear overall winner.&lt;/p&gt;
&lt;p&gt;First, while the asymptotic theory of the two estimators appear quite
different, they are actually rather closely related. When the regression
function $g(x)$ is twice differentiable $(s = 2)$ then the rate of
convergence of both the MSE of the kernel regression estimator with
optimal bandwidth $h$ and the series estimator with optimal $K$ is
$n^{-\frac{2}{k+4}}$ (where $k = \dim(x)$). There is no difference. If
the regression function is smoother than twice differentiable ($s &amp;gt; 2$)
then the rate of the convergence of the series estimator improves. This
may appear to be an advantage for series methods, but kernel regression
can also take advantage of the higher smoothness by using so-called
higher-order kernels or local polynomial regression, so perhaps this
advantage is not too large.&lt;/p&gt;
&lt;p&gt;Both estimators are asymptotically normal and have straightforward
asymptotic standard error formulae. The series estimators are a bit more
convenient for this purpose, as classic parametric standard error
formula work without amendment.&lt;/p&gt;
&lt;h3 id=&#34;advantages-of-kernels&#34;&gt;Advantages of Kernels&lt;/h3&gt;
&lt;p&gt;An advantage of kernel methods is that their distributional theory is
easier to derive. The theory is all based on local averages which is
relatively straightforward. In contrast, series theory is more
challenging, dealing with increasing parameter spaces. An important
difference in the theory is that for kernel estimators we have explicit
representations for the bias while we only have rates for series
methods. This means that plug-in methods can be used for bandwidth
selection in kernel regression. However, typically we rely on
cross-validation, which is equally applicable in both kernel and series
regression.&lt;/p&gt;
&lt;p&gt;Kernel methods are also relatively easy to implement when the dimension
of $x$, $k$, is large. There is not a major change in the methodology as
$k$ increases. In contrast, series methods become quite cumbersome as
$k$ increases as the number of cross-terms increases exponentially. E.g
($K=2$) with $k=1$ you have only $\lbrace x_1, x_1^2\rbrace$; with $k=2$
you have to add $\lbrace x_2, x_2^2, x_1 x_2 \rbrace$; with $k=3$ you
have to add $\lbrace x_3, x_3^2, x_1 x_3, x_2 x_3\rbrace$, etc..&lt;/p&gt;
&lt;h3 id=&#34;advantages-of-series&#34;&gt;Advantages of Series&lt;/h3&gt;
&lt;p&gt;A major advantage of series methods is that it has inherently a high
degree of flexibility, and the user is able to implement shape
restrictions quite easily. For example, in series estimation it is
relatively simple to implement a partial linear CEF, an additively
separable CEF, monotonicity, concavity or convexity. These restrictions
are harder to implement in kernel regression.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Variable Selection</title>
      <link>https://matteocourthoud.github.io/course/metrics/09_selection/</link>
      <pubDate>Fri, 29 Oct 2021 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/course/metrics/09_selection/</guid>
      <description>&lt;h2 id=&#34;lasso&#34;&gt;Lasso&lt;/h2&gt;
&lt;h3 id=&#34;issue&#34;&gt;Issue&lt;/h3&gt;
&lt;p&gt;Lasso (Least Absolute Shrinkage and Selection Operator) is a popular
method for high dimensional regression. It does variable selection and
estimation simultaneously. It is a non-parametric (series) estimation
technique part of a general class of estimators called &lt;em&gt;penalized
estimators&lt;/em&gt;. It allows the number of regressors, $p$, to be larger than
the sample size, $n$.&lt;/p&gt;
&lt;p&gt;Consider data $D = \lbrace x_i, y_i \rbrace_{i=1}^n$ with
$\dim (x_i) = p$. Assume that $p$ is large relative to $n$. Two possible
reasons:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;we have an intrinsic problem of high dimensionality&lt;/li&gt;
&lt;li&gt;$p$ indicates the number of expansion terms of small number of
underlying important variables (e.g.Â series estimation)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Assumption&lt;/strong&gt;: $y_i = x_i&#39; \beta_0 + r_i + \varepsilon_i$ where
$\beta_0$ depends on $p$, $r_i$ is a remainder term.&lt;/p&gt;
&lt;p&gt;Note that in classic non-parametrics, we have $x_i&#39;\beta_0$ as
$p_1(x_i) \beta_{1,K} + \dots + p_K(x_i) \beta_{K,K}$. For simplicity,
we assume $r_i = 0$, as if we had extreme undersmoothing. Hence the
model becomes: $$
y_i = x_i&#39; \beta_0 + \varepsilon_i, \qquad p \geq n
$$ We cannot run OLS because $p \geq n$, thus the rank condition is
violated.&lt;/p&gt;
&lt;h3 id=&#34;definition&#34;&gt;Definition&lt;/h3&gt;
&lt;p&gt;We define the &lt;strong&gt;Lasso estimator&lt;/strong&gt; as $$
\hat{\beta}_L = \arg \min \quad \underbrace{\mathbb E_n \Big[ (y_i - x_i&#39; \beta)^2 \Big]} _ {\text{SSR term}} + \underbrace{\frac{\lambda}{n} \sum _ {j=1}^{P} | \beta_j |} _ {\text{Penalty term}}
$$ where $\lambda$ is called &lt;strong&gt;penalty parameter&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;penalty term&lt;/strong&gt; discourages large values of $| \beta_j |$. The
choice of $\lambda$ is analogous to the choice of $K$ in series
estimation and $h$ in kernel estimation.&lt;/p&gt;
&lt;h3 id=&#34;penalties&#34;&gt;Penalties&lt;/h3&gt;
&lt;p&gt;The shrinkage to zero of the coefficients directly follows from the
$|| \cdot ||_1$ norm. On the contrary, another famous penalized
estimator, &lt;em&gt;ridge regression&lt;/em&gt;, uses the $|| \cdot ||_2$ norm and does
not have this property.&lt;/p&gt;
&lt;img src=&#34;../img/Fig_551.png&#34; style=&#34;width:50.0%&#34; /&gt;
&lt;blockquote&gt;
&lt;p&gt;Minimizing SSR + penalty is equivalent to minimize SSR $s.t.$ pen
$\leq c$ (clear from the picture).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;sparsity&#34;&gt;Sparsity&lt;/h3&gt;
&lt;p&gt;Let $S_0 = \lbrace j: \beta_{0,j} \ne 0 \rbrace$, we define
$s_0 = |S_0|$ as the &lt;strong&gt;sparsity&lt;/strong&gt; of $\beta_0$. If $s_0/n \to 0$, we are
dealing with a &lt;strong&gt;sparse regression&lt;/strong&gt; (analogous of smooth regression).&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Remark on sparsity:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;In words, sparsity means that even if we have a lot of variables,
only a small number of them (relative to $n$) have an effect on
the dependent variable.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Approximate sparsity imposes a restriction that only $s_0$
variables among all of $x_{ij}$, where $s_0$ is much smaller than
$n$, have associated coefficients $\beta_{0j}$ that are different
from zero, while permitting a nonzero approximation error. Thus,
estimators for this kind of model attempt to learn the identities
of the variables with large nonzero coefficients, while
simultaneously estimating these coefficients.&lt;/em&gt; (Belloni et al.,
2004)&lt;/li&gt;
&lt;li&gt;Sparsity is an assumption. $\beta_0$ is said to be $s_0$-sparse
with $s_0 &amp;lt; n$ if $$
| \lbrace j: \beta_{0j} \neq 0 \rbrace | \leq s_0
$$&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;lasso-theorem&#34;&gt;Lasso Theorem&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Suppose that for data $D_n = (y_i, x_i)&lt;em&gt;{i=1}^N$ with
$y_i = x_i&#39; \beta + \varepsilon_i$. Let $\hat{\beta}&lt;em&gt;L$ be the Lasso
estimator. Let
$\mathcal{S} = 2 \max_j | \mathbb E[ x&lt;/em&gt;{ij} \varepsilon_i] |$. Suppose
$|support(\beta_0) \leq s_0$ (sparsity assumption). Let
$c_0 = (\mathcal{S} + \lambda/n )/(-\mathcal{S} + \lambda/n )$. Let $$
\kappa&lt;/em&gt;{c_0, s_0} = \min_{  d \in \mathbb R^p, A \subseteq \lbrace 1, &amp;hellip; , p \rbrace : |A| \leq s_0 ,  || d_{A^c}|| \leq c_0 || d_A ||_1  }  \sqrt{  \frac{ s_0 d&#39; \mathbb E_n [x_i x_i&#39;] d }{|| d_A ||_1^2}  }
$$ Then&lt;/p&gt;
&lt;p&gt;$$
\mathbb I_{ \left\lbrace \frac{\lambda}{n} &amp;gt; \mathcal{S}  \right\rbrace} \mathbb E_n [(x_i \beta_0 - x_i \beta_L)^2]^{\frac{1}{2}}  \leq 2 \frac{\lambda}{n} \frac{\sqrt{s_0}}{\kappa_{c_0, s_0}}
$$&lt;/p&gt;
&lt;p&gt;Intuition: for a sufficiently high lambda the root mean squared error of
Lasso is approximately zero.&lt;/p&gt;
&lt;p&gt;$$
\text{ RMSE }:  \mathbb E_n [(x_i \beta_0 - x_i \beta_L)^2]^{\frac{1}{2}}  \simeq 0  \quad \Leftrightarrow \quad \frac{\lambda}{n} &amp;gt; \mathcal{S}
$$&lt;/p&gt;
&lt;h3 id=&#34;remarks&#34;&gt;Remarks&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;The minimization region is the set of âessentially sparseâ vectors
$d \in \mathbb R^p$, where âessentially sparseâ is defined by
$\mathcal{C}, \mathcal{S}$. In particular the condition
$k_{\mathcal{C}, \mathcal{S}}&amp;gt;0$ means that no essentially sparse
vector $d$ has $\mathbb E[x_i x_i&#39;]d = 0$, i.e.Â regressors were not
added multiple times.&lt;/li&gt;
&lt;li&gt;Need to dominate the score with the penalty term $\lambda$.&lt;/li&gt;
&lt;li&gt;Need no collinearity on a small ($\leq s_0$) subset of regressors
($\to k_{c_0, s_0}&amp;gt;0$).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;When Lasso?&lt;/strong&gt; For prediction problems in high dimensional
environments. &lt;strong&gt;NB!&lt;/strong&gt; Lasso is not good for inference, only for
prediction.&lt;/p&gt;
&lt;p&gt;In particular, in econometrics itâs used for selecting either&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;instruments (predicting $\hat{x}$ in the first stage)&lt;/li&gt;
&lt;li&gt;control variables (next section: double prediction problem, in the
first stage and in the reduced form)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;choosing-the-optimal-lambda&#34;&gt;Choosing the Optimal Lambda&lt;/h3&gt;
&lt;p&gt;The choice of $\lambda$ determines the bias-variance tradeoff:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;if $\lambda$ is too big:
$\lambda \approx \infty \mathbb \Rightarrow \hat{\beta} \approx 0$;&lt;/li&gt;
&lt;li&gt;if $\lambda$ is too small: $\lambda \approx 0 \mathbb \Rightarrow$
overfitting.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Possible solutions: Bonferroni correction, bootstrapping or
$\frac{\lambda}{n} \asymp \sqrt{\frac{\log(p)}{n}}$ (asymptotically
equal to), $\mathcal{S}$ behaves like the maximum of gaussians.&lt;/p&gt;
&lt;h3 id=&#34;lasso-path&#34;&gt;Lasso Path&lt;/h3&gt;
&lt;p&gt;How the estimated $\hat{\beta}$ depends on the penalty parameter
$\lambda$?&lt;/p&gt;
&lt;img src=&#34;../img/Fig_642.png&#34; style=&#34;width:50.0%&#34; /&gt;
&lt;p&gt;&lt;strong&gt;Post Lasso&lt;/strong&gt;: fit OLS without the penalty with all the nonzero
coeficients selected by Lasso in the first step.&lt;/p&gt;
&lt;h3 id=&#34;remarks-1&#34;&gt;Remarks&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Do not do inference with post-Lasso because standard errors are not
uniformely valid.&lt;/li&gt;
&lt;li&gt;As $n \to \infty$ the CV and the &lt;strong&gt;score domination&lt;/strong&gt; bounds
converge to a unique bound.&lt;/li&gt;
&lt;li&gt;What is the problem of cross-validation? In high dimensional
settings you can overfit in so many ways that CV doesnât work and
still overfits.&lt;/li&gt;
&lt;li&gt;Using $\lambda$ with $\frac{\lambda}{n} &amp;gt; \mathcal{S}$ small
coefficients get shrunk to zero with high probability. In this case
with small we mean $\propto \frac{1}{\sqrt{n}}$ or
$2 \max_j | \mathbb E_n[\varepsilon_i x_{ij}] |$.&lt;/li&gt;
&lt;li&gt;If $| \beta_{0j}| \leq \frac{c}{\sqrt{n}}$ for a sufficiently small
constant $c$, then $\hat{\beta}_{LASSO} \overset{p}{\to} 0$.&lt;/li&gt;
&lt;li&gt;In standard t-tests $c = 1.96$.&lt;/li&gt;
&lt;li&gt;$\sqrt{n}$ factor is important since it is the demarcation line for
reliable statistical detection.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;optimal-lambda&#34;&gt;Optimal Lambda&lt;/h3&gt;
&lt;p&gt;What is the criterium that should guide the selection of $\lambda$? $$
\frac{\lambda}{n} \geq 2 \mathbb E_n[x_{ij} \varepsilon_i] \qquad \forall j \quad \text{ if } Var(x_{ij} \varepsilon_i) = 1
$$&lt;/p&gt;
&lt;p&gt;How to choose the optimal $\lambda$:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Decide the coverage of the confidence intervals ($1-\alpha$): $$
\Pr \left( \sqrt{n} \Big| \mathbb E_n [x_{ij} \varepsilon_i] \Big| &amp;gt; t \right) = 1- \alpha
$$&lt;/li&gt;
&lt;li&gt;Solve for $t$&lt;/li&gt;
&lt;li&gt;Get $\lambda$ such that all scores are dominated by
$\frac{\lambda}{n}$ with $\alpha%$ probability.&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;It turns out that the optimal $t \propto \sqrt{\log(p)}$&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;pre-testing&#34;&gt;Pre-Testing&lt;/h2&gt;
&lt;h3 id=&#34;omitted-variable-bias&#34;&gt;Omitted Variable Bias&lt;/h3&gt;
&lt;p&gt;Consider two separate statistical models. Assume the following &lt;strong&gt;long
regression&lt;/strong&gt; of interest:&lt;/p&gt;
&lt;p&gt;$$
y_i = x_i&#39; \alpha_0+ z_i&#39; \beta_0 + \varepsilon_i
$$&lt;/p&gt;
&lt;p&gt;Define the corresponding &lt;strong&gt;short regression&lt;/strong&gt; as&lt;/p&gt;
&lt;p&gt;$$
y_i = x_i&#39; \alpha_0 + v_i \quad \text{ with } v_i = z_i&#39; \beta_0 + \varepsilon_i
$$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Suppose that the DGP for the long regression corresponds to $\alpha_0$,
$\beta_0$. Suppose further that $\mathbb E[x_i] = 0$,
$\mathbb E[z_i] = 0$, $\mathbb E[\varepsilon_i |x_i,z_i] = 0$. Then,
unless $\beta_0 = 0$ or $z_i$ is orthogonal to $x_i$, the (sole)
stochastic regressor $x_i$ is correlated with the error term in the
short regression which implies that the OLS estimator of the short
regression is inconsistent for $\alpha_0$ due to the omitted variable
bias. In particular, one can show that the plim of the OLS estimator of
$\hat{\alpha}&lt;em&gt;{SHORT}$ from the short regression is $$
\hat{\alpha}&lt;/em&gt;{SHORT} \overset{p}{\to} \frac{Cov(y_i, x_i)}{Var(x_i)} = \alpha_0 + \beta_0 \frac{Cov(z_i, x_i)}{Var(x_i)}
$$&lt;/p&gt;
&lt;h3 id=&#34;pre-test-bias&#34;&gt;Pre-test bias&lt;/h3&gt;
&lt;p&gt;Consider data $D= (y_i, x_i, z_i)_{i=1}^n$, where the true model is: $$
\begin{aligned}
&amp;amp; y_i = x_i&#39; \alpha_0  + z_i&#39; \beta_0 + \varepsilon_i \newline
&amp;amp; x_i = z_i&#39; \gamma_0 + u_i
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;Where $x_i$ is the variable of interest (we want to make inference on
$\alpha_0$) and $z_i$ is a high dimensional set of control variables.&lt;/p&gt;
&lt;p&gt;From now on, we will work under the following assumptions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\dim(x_i)=1$ for all $n$&lt;/li&gt;
&lt;li&gt;$\beta_0$ uniformely bounded in $n$&lt;/li&gt;
&lt;li&gt;Strict exogeneity: $\mathbb E[\varepsilon_i | x_i, z_i] = 0$ and
$\mathbb E[u_i | z_i] = 0$&lt;/li&gt;
&lt;li&gt;$\beta_0$ and $\gamma_0$ have dimension (and hence value) that
depend on $n$&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;pre-testing-procedure&#34;&gt;Pre-Testing procedure&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;Regress $y_i$ on $x_i$ and $z_i$&lt;/li&gt;
&lt;li&gt;For each $j = 1, &amp;hellip;, p = \dim(z_i)$ calculate a test statistic
$t_j$&lt;/li&gt;
&lt;li&gt;Let $\hat{T} = \lbrace j: |t_j| &amp;gt; C &amp;gt; 0 \rbrace$ for some constant
$C$ (set of statistically significant coefficients).&lt;/li&gt;
&lt;li&gt;Re-run the new âmodelâ using $(x_i, z_{\hat{T},i})$ (i.e.Â using the
selected covariates with statistically significant coefficients).&lt;/li&gt;
&lt;li&gt;Perform statistical inference (i.e.Â confidence intervals and
hypothesis tests) as if no model selection had been done.&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;bias&#34;&gt;Bias&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;../img/Fig_621.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;As we can see from the figure above (code below), running the short
regression instead of the long one introduces Omitted Variable Bias
(second column). Instead, the Pre-Testing estimator is consistent but
not normally distributed (third column).&lt;/p&gt;
&lt;h3 id=&#34;issue-1&#34;&gt;Issue&lt;/h3&gt;
&lt;p&gt;Pre-testing is problematic because the post-selection estimator is not
asymptotically normal. Moreover, for particular data generating
processes, it even fails to be consistent at the rate of $\sqrt{n}$
(Belloni et al., 2014).&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Intuition: when performing pre-testing, we might have an Omitted
Variable Bias problem when $\beta_0&amp;gt;0$ but we fail to reject the null
hypothesis $H_0 : \beta_0 = 0$ because of lack of statistical power,
i.e.Â $|\beta_0|$ is small with respect to the sample size. In
particular, we fail to reject the null hypothesis for
$\beta_0(n) = O \left( \frac{1}{\sqrt{n}}\right)$. However, note that
the problem vanishes asymptotically, as the resulting estimator is
consistent. In fact, if
$\beta_0(n) = O \left( \frac{1}{\sqrt{n}}\right)$, then
$\alpha_0 - \hat \alpha_{PRETEST} \overset{p}{\to} \lim_{n \to \infty} \beta_0 \gamma_0 = \lim_{n \to \infty} O \left( \frac{1}{\sqrt{n}} \right) = 0$.
We now clarify what it means to have a coefficient depending on the
sample size, $\beta_0(n)$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;uniformity&#34;&gt;Uniformity&lt;/h3&gt;
&lt;p&gt;Concept of &lt;strong&gt;uniformity&lt;/strong&gt;: the DGP varies with $n$. Instead of having a
fixed âtrueâ parameter $\beta_0$, you have a sequence $\beta_0(n)$.
Having a cofficient that depends on the sample size $n$ is useful to
preserve the concept of âsmall with respect to the sample sizeâ in
asymptotic theory.&lt;/p&gt;
&lt;p&gt;In the context of Pre-Testing, all problems vanish asymptotically since
we are able to always reject the null hypothesis $H_0 : \beta_0 = 0$
when $\beta_0 \neq 0$. In the figure below, I plot simulation results
for $\hat \alpha_{PRETESTING}$ for a fixed coefficient $\beta_0$ (first
row) and variable coefficient $\beta_0(n)$ that depends on the sample
size (second row), for different sample sizes (columns). We see that if
$\beta_0$ is independent from the sample size (first row), the
distribution of $\hat \alpha_{PRETEST}$ is not normal in small samples
and it displays the bimodality that characterizes pre-testing. However,
it becomes normal in large samples. On the other hand, when $\beta_0(n)$
depends on the sample size, and in particular
$\beta_0 = O \left( \frac{1}{\sqrt{n}} \right)$ (second row), the
distribution of $\hat \alpha_{PRETEST}$ stays bimodal even when the
sample size increases.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Note that the estimator is always consistent!&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;where-is-pre-testing-a-problem&#34;&gt;Where is Pre-Testing a Problem?&lt;/h3&gt;
&lt;p&gt;If we were to draw a map of where the gaussianity assumption of
$\beta_0(n)$ holds well and where it fails, it would look like the
following figure.&lt;/p&gt;
&lt;img src=&#34;../img/Fig_623.png&#34; style=&#34;width:50.0%&#34; /&gt;
&lt;h3 id=&#34;intuition&#34;&gt;Intuition&lt;/h3&gt;
&lt;p&gt;The intuition for the three different regions (from bottom to top) is
the following.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;When $\beta_0 = o \left( \frac{1}{\sqrt{n}} \right)$, $z_i$ is
excluded with probability $p \to 1$. But, given that $\beta_0$ is
small enough, failing to control for $z_i$ does not introduce large
omitted variables bias (Belloni et al., 2014).&lt;/li&gt;
&lt;li&gt;If however the coefficient on the control is âmoderately close to
zeroâ, $\beta_0 = O \left( \frac{1}{\sqrt{n}} \right)$, the t-test
set-up above cannot distinguish this coefficient from $0$, and the
control $z_i$ is dropped with probability $p \to 1$. However, in
this case the omitted variable bias generated by excluding $z_i$
scaled by $\sqrt{n}$ does not converge to zero. That is, the
standard post-selection estimator is not asymptotically normal and
even fails to be consistent at the rate of $\sqrt{n}$ (Belloni et
al., 2014).&lt;/li&gt;
&lt;li&gt;Lastly, when $\beta_0$ is large enough, the null pre-testing
hypothesis $H_0 : \beta_0 = 0$ will be rejected sufficiently often
so that the bias is negligible.&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;post-double-selection&#34;&gt;Post-Double Selection&lt;/h3&gt;
&lt;p&gt;The post-double-selection estimator, $\hat{\alpha}_{PDS}$ solves this
problem by doing variable selection via standard t-tests or Lasso-type
selectors with the two âtrue modelâ equations (&lt;strong&gt;first stage&lt;/strong&gt; and
&lt;strong&gt;reduced form&lt;/strong&gt;) that contain the information from the model and then
estimating $\alpha_0$ by regressing $y_i$ on $x_i$ and the union of the
selected controls. By doing so, $z_i$ is omitted only if its coefficient
in both equations is small which greatly limits the potential for
omitted variables bias (Belloni et al., 2014).&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Intuition: by performing post-double selection, we ensure that both
$\beta_0 = O \left( \frac{1}{\sqrt{n}} \right)$ and
$\gamma_0 = O \left( \frac{1}{\sqrt{n}} \right)$ so that
$\sqrt{n} ( \hat \alpha _ {PRETEST} - \alpha _ 0) \overset{p}{\to} \lim_{n \to \infty} \sqrt{n} \beta_0 \gamma_0 = \lim_{n \to \infty} \sqrt{n} O \left( \frac{1}{n} \right) = 0$
and the estimator is gaussian.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;frisch-waugh-theorem&#34;&gt;Frisch-Waugh Theorem&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Consider the data $D = \lbrace x_i, y_i, z_i \rbrace_{i=1}^\infty$ with
DGP: $Y = X \alpha + Z \beta + \varepsilon$. The following estimators of
$\alpha$ are numerically equivalent (if $[X, Z]$ has full rank):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\hat{\alpha}$ from regressing $Y$ on $X, Z$&lt;/li&gt;
&lt;li&gt;$\tilde{\alpha}$ from regressing $Y$ on $\tilde{X}$&lt;/li&gt;
&lt;li&gt;$\bar{\alpha}$ from regressing $\tilde{Y}$ on $\tilde{X}$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;where the operation of passing to $Y, X$ to $\tilde{Y}, \tilde{X}$ is
called &lt;em&gt;projection out $Z$&lt;/em&gt;, e.g.$\tilde{X}$ are the residuals from
regressing $X$ on $Z$.&lt;/p&gt;
&lt;h3 id=&#34;proof-1&#34;&gt;Proof (1)&lt;/h3&gt;
&lt;p&gt;We want to show that $\hat{\alpha} = \tilde{\alpha}$.&lt;/p&gt;
&lt;p&gt;Claim:
$\hat{\alpha } = \tilde{\alpha} \Leftrightarrow \tilde{X}&#39; \left[ (X - \tilde{X})\hat{\alpha} + Z \hat{\beta} +\hat{\varepsilon} \right] = 0$.&lt;/p&gt;
&lt;p&gt;Proof of the claim: if $\hat{\alpha} = \tilde{\alpha}$, we can write $Y$
as $$
Y =  X \hat{\alpha} + Z \hat{\beta} + \hat{\varepsilon}  = \tilde{X} \hat{\alpha} + \underbrace{(X - \tilde{X}) \hat{\alpha } + Z \hat{\beta} + \hat{\varepsilon}}_\text{residual of $Y$ on $\tilde{X} $} = \tilde{X} \tilde{\alpha} + \nu_i
$$&lt;/p&gt;
&lt;p&gt;Therefore, by the orthogonality property of the OLS residual, it must be
that $\tilde{X}&#39;\nu_i= 0$. $$\tag*{$\blacksquare$}$$&lt;/p&gt;
&lt;h3 id=&#34;proof-1-1&#34;&gt;Proof (1)&lt;/h3&gt;
&lt;p&gt;Having established the claim, we want to show that the normal equation
$\tilde{X}&#39; \left[ (X - \tilde{X})\hat{\alpha} + Z \hat{\beta} +\hat{\varepsilon} \right] = 0$
is satisfied. We follow 3 steps:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;First we have that $\tilde{X}&#39; (X - \tilde{X})\hat{\alpha} = 0$.
This follows from the fact that $\tilde{X}&#39; = X&#39; M_Z$ and hence: $$
\begin{aligned}
\tilde{X}&#39; (X - \tilde{X})  &amp;amp;  = X&#39; M_Z (X - M_Z) = X&#39; M_Z X - X&#39; \overbrace{M_Z M_Z}^{M_Z} X \newline &amp;amp; = X&amp;rsquo;M_Z X - X&#39; M_Z X = 0
\end{aligned}
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$\tilde{X}&#39; Z \hat{\beta} = 0$ since $\tilde{X}$ is the residual
from the regression of $X$ on $Z$, by normal equation it holds that
$\tilde{X}&#39; Z = 0$.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$\tilde{X}&#39; \hat{\varepsilon} = 0$. This follows from (i)
$M_Z &#39; M_{X, Z} = M_{X,Z}$ and (ii) $X&#39; M_{X, Z} = 0$: $$
\tilde{X}&#39; \hat{\varepsilon} = (M_Z X)&#39; (M_{X, Z} \varepsilon)  = X&amp;rsquo;M_Z&#39; M_{X, Z} \varepsilon = \underbrace{X&#39; M_{X, Z}}_0 \varepsilon = 0.
$$ $$\tag*{$\blacksquare$}$$&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The coefficient $\hat{\alpha}$ is a &lt;em&gt;partial regression&lt;/em&gt; coefficient
identified from the variation in $X$ that is orthogonal to $Z$. This is
often known as &lt;strong&gt;residual variation&lt;/strong&gt;.&lt;/p&gt;
&lt;h2 id=&#34;post-double-selection-1&#34;&gt;Post Double Selection&lt;/h2&gt;
&lt;h3 id=&#34;setting&#34;&gt;Setting&lt;/h3&gt;
&lt;p&gt;Consider again data $D= (y_i, x_i, z_i)_{i=1}^n$, where the true model
is: $$
\begin{aligned}
&amp;amp; y_i = x_i&#39; \alpha_0  + z_i&#39; \beta_0 + \varepsilon_i \newline
&amp;amp; x_i = z_i&#39; \gamma_0 + u_i
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;We would like to guard against pretest bias if possible, in order to
handle high dimensional models. A good pathway towards motivating
procedures which guard against pretest bias is a discussion of classical
partitioned regression.&lt;/p&gt;
&lt;p&gt;Consider a regression $y_i$ on $x_i$ and $z_i$. $x_i$ is the
1-dimensional variable of interest, $z_i$ is a high-dimensional set of
control variables. We have the following procedure:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;First Stage&lt;/strong&gt; selection: lasso $x_i$ on $z_i$. Let the selected
variables be collected in the set $S_{FS} \subseteq z_i$&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Reduced Form&lt;/strong&gt; selection: lasso $y_i$ on $z_i$. Let the selected
variables be collected in the set $S_{RF} \subseteq z_i$&lt;/li&gt;
&lt;li&gt;Regress $y_i$ on $x_i$ and $S_{FS} \cup S_{RF}$&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;pds-theorem&#34;&gt;PDS Theorem&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Let $\lbrace P^n\rbrace$ be a sequence of data-generating processes for
$D_n = (y_i, x_i, z_i)^n_{i=1} \in (\mathbb R \times \mathbb R \times \mathbb R^p) ^n$
where $p$ depends on $n$. For each $n$, the data are iid with
$yi = x_i&#39;\alpha_0^{(n)} + z_i&#39; \beta_0^{(n)} + \varepsilon_i$ and
$x_i = z_i&#39; \gamma_0^{(n)} + u_i$ where
$\mathbb E[\varepsilon_i | x_i,z_i] = 0$ and $\mathbb E[u_i|z_i] = 0$.
The sparsity of the vectors $\beta_0^{(n)}$, $\gamma_0^{(n)}$ is
controlled by $|| \beta_0^{(n)} ||_0 \leq s$ with
$s^2 (\log p)^2/n \to 0$. Suppose that additional regularity conditions
on the model selection procedures and moments of the random variables
$y_i$ , $x_i$ , $z_i$ as documented in Belloni et al.Â (2014). Then the
confidence intervals, CI, from the post double selection procedure are
uniformly valid. That is, for any confidence level $\xi \in (0, 1)$ $$
\Pr(\alpha_0 \in CI) \to 1- \xi
$$&lt;/p&gt;
&lt;p&gt;In order to have valid confidence intervals you want their bias to be
negligibly. Since $$
CI = \left[ \hat{\alpha} \pm \frac{1.96 \cdot \hat{\sigma}}{\sqrt{n}} \right]
$$&lt;/p&gt;
&lt;p&gt;If the bias is $o \left( \frac{1}{\sqrt{n}} \right)$ then there is no
problem since it is asymptotically negligible w.r.t. the magnitude of
the confidence interval. If however the the bias is
$O \left( \frac{1}{\sqrt{n}} \right)$ then it has the same magnitude of
the confidence interval and it does not asymptotically vanish.&lt;/p&gt;
&lt;h3 id=&#34;proof-idea&#34;&gt;Proof (Idea)&lt;/h3&gt;
&lt;p&gt;The idea of the proof is to use partitioned regression. An alternative
way to think about the argument is: bound the omitted variables bias.
Omitted variable bias comes from the product of 2 quantities related to
the omitted variable:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Its partial correlation with the outcome, and&lt;/li&gt;
&lt;li&gt;Its partial correlation with the variable of interest.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;If both those partial correlations are $O( \sqrt{\log p/n})$, then the
omitted variables bias is
$(s \times O( \sqrt{\log p/n})^2 = o \left( \frac{1}{\sqrt{n}} \right)$,
provided $s^2 (\log p)^2/n \to 0$. Relative to the $\frac{1}{\sqrt{n}}$
convergence rate, the omitted variables bias is negligible.&lt;/p&gt;
&lt;p&gt;In our omitted variable bias case, we want
$| \beta_0 \gamma_0 | = o \left( \frac{1}{\sqrt{n}} \right)$.
Post-double selection guarantees that&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Reduced form&lt;/em&gt; selection (pre-testing): any âmissingâ variable has
$|\beta_{0j}| \leq \frac{c}{\sqrt{n}}$&lt;/li&gt;
&lt;li&gt;&lt;em&gt;First stage&lt;/em&gt; selection (additional): any âmissingâ variable has
$|\gamma_{0j}| \leq \frac{c}{\sqrt{n}}$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;As a consequence, as long as the number of omitted variables is finite,
the omitted variable bias is $$
OVB(\alpha) = |\beta_{0j}| \cdot|\gamma_{0j}| \leq \frac{c}{\sqrt{n}} \cdot \frac{c}{\sqrt{n}} = \frac{c^2}{n} = o \left(\frac{1}{\sqrt{n}}\right)
$$&lt;/p&gt;
&lt;h3 id=&#34;distribution&#34;&gt;Distribution&lt;/h3&gt;
&lt;p&gt;We can plot the distribution of the post-double selection estimator
against the pre-testing one.&lt;/p&gt;
&lt;img src=&#34;../img/Fig_641.png&#34; style=&#34;width:60.0%&#34; /&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Remark&lt;/strong&gt;: under homoskedasticity, the above estimator achieves the
semiparametric efficiency bound.&lt;/p&gt;
&lt;/blockquote&gt;
</description>
    </item>
    
  </channel>
</rss>
