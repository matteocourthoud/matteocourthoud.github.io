<!DOCTYPE html><html lang="en-us" >

<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  
  
  
    <meta name="generator" content="Wowchemy 5.1.0 for Hugo" />
  

  

  
  
  
  
  
    
    
    
  
  

  

  
  
  
    
  
  <meta name="description" content="# Remove warnings import warnings warnings.filterwarnings(&#39;ignore&#39;) # Import import autograd.numpy as np from autograd import grad import seaborn as sns # Import matplotlib for graphs import matplotlib.pyplot as plt from mpl_toolkits." />

  
  <link rel="alternate" hreflang="en-us" href="https://matteocourthoud.github.io/course/ml-econ/06_convexity/" />

  







  




  
  
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
  

  
  
  
    <meta name="theme-color" content="#003f5c" />
  

  
  
    
    <script src="/js/mathjax-config.js"></script>
  

  
  
  
  
    
    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha256-FMvZuGapsJLjouA6k7Eo2lusoAX9i0ShlWFG6qt7SLc=" crossorigin="anonymous">

    
    

    
    
    
      
    
    
      
      
        
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/styles/github.min.css" crossorigin="anonymous" title="hl-light" media="print" onload="this.media='all'">
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/styles/dracula.min.css" crossorigin="anonymous" title="hl-dark" media="print" onload="this.media='all'" disabled>
        
      
    

    
    
    
      <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.7.1/leaflet.min.css" integrity="sha512-1xoFisiGdy9nvho8EgXuXvnpR5GAMSjFwp40gSRE3NwdUdIMIKuPa7bqoUhLD0O/5tPNhteAsE5XyyMi5reQVA==" crossorigin="anonymous" media="print" onload="this.media='all'">
    

    

    
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js" integrity="" crossorigin="anonymous" async></script>
      
    
      
      

      
      

      
    
      
      

      
      

      
    
  

  
  
  
    
      
      <link rel="preload" as="style" href="https://fonts.googleapis.com/css2?family=Roboto+Mono&family=Source+Sans+Pro:wght@200;300;400;700&display=swap">
      <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Roboto+Mono&family=Source+Sans+Pro:wght@200;300;400;700&display=swap" media="print" onload="this.media='all'">
    
  

  
  
  
  
  
  <link rel="stylesheet" href="/css/wowchemy.4f7182ca394d705ee32d9d7750e9aa1d.css" />

  




<script async src="https://www.googletagmanager.com/gtag/js?id=UA-144780600-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];

  function gtag() {
      dataLayer.push(arguments);
  }

  function trackOutboundLink(url, target) {
    gtag('event', 'click', {
         'event_category': 'outbound',
         'event_label': url,
         'transport_type': 'beacon',
         'event_callback': function () {
           if (target !== '_blank') {
             document.location = url;
           }
         }
    });
    console.debug("Outbound link clicked: " + url);
  }

  function onClickCallback(event) {
    if ((event.target.tagName !== 'A') || (event.target.host === window.location.host)) {
      return;
    }
    trackOutboundLink(event.target, event.target.getAttribute('target'));  
  }

  gtag('js', new Date());
  gtag('config', 'UA-144780600-1', { 'anonymize_ip': true });
  gtag('set', {'cookie_flags': 'SameSite=None;Secure'});

  
  document.addEventListener('click', onClickCallback, false);
</script>


  

  

  




  
  
  

  

  
    <link rel="manifest" href="/index.webmanifest" />
  

  <link rel="icon" type="image/png" href="/media/icon_hu03e9b3967b83fd39296ec9da5ff1ea05_201175_32x32_fill_lanczos_center_3.png" />
  <link rel="apple-touch-icon" type="image/png" href="/media/icon_hu03e9b3967b83fd39296ec9da5ff1ea05_201175_180x180_fill_lanczos_center_3.png" />

  <link rel="canonical" href="https://matteocourthoud.github.io/course/ml-econ/06_convexity/" />

  
  
  
  
  
  
  
  
    
    
  
  
  <meta property="twitter:card" content="summary" />
  
  <meta property="og:site_name" content="Matteo Courthoud" />
  <meta property="og:url" content="https://matteocourthoud.github.io/course/ml-econ/06_convexity/" />
  <meta property="og:title" content="Convexity and Optimization | Matteo Courthoud" />
  <meta property="og:description" content="# Remove warnings import warnings warnings.filterwarnings(&#39;ignore&#39;) # Import import autograd.numpy as np from autograd import grad import seaborn as sns # Import matplotlib for graphs import matplotlib.pyplot as plt from mpl_toolkits." /><meta property="og:image" content="https://matteocourthoud.github.io/media/icon_hu03e9b3967b83fd39296ec9da5ff1ea05_201175_512x512_fill_lanczos_center_3.png" />
    <meta property="twitter:image" content="https://matteocourthoud.github.io/media/icon_hu03e9b3967b83fd39296ec9da5ff1ea05_201175_512x512_fill_lanczos_center_3.png" /><meta property="og:locale" content="en-us" />
  
    
      <meta
        property="article:published_time"
        content="2022-03-09T00:00:00&#43;00:00"
      />
    
    <meta property="article:modified_time" content="2022-03-09T00:00:00&#43;00:00">
  

  



  

  
  
  
  
  
    <script src="https://cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.1.1/cookieconsent.min.js" integrity="sha256-5VhCqFam2Cn+yjw61zbBNrbHVJ6SRydPeKopYlngbiQ=" crossorigin="anonymous"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.1.1/cookieconsent.min.css" integrity="sha256-zQ0LblD/Af8vOppw18+2anxsuaz3pWYyVWi+bTvTH8Q=" crossorigin="anonymous">
  
  <script>
  window.addEventListener("load", function(){
    window.cookieconsent.initialise({
      "palette": {
        "popup": {
          "background": "#003f5c",
          "text": "rgb(255, 255, 255)"
        },
        "button": {
          "background": "rgb(255, 255, 255)",
          "text": "#003f5c"
        }
      },
      "theme": "classic",
      "content": {
        "message": "This website uses cookies to ensure you get the best experience on our website.",
        "dismiss": "Got it!",
        "link": "Learn more",
        "href": "/privacy/"
      }
    })});
  </script>


  





  <title>Convexity and Optimization | Matteo Courthoud</title>
</head>


<body id="top" data-spy="scroll" data-offset="70" data-target="#TableOfContents" class="page-wrapper   " data-wc-page-id="95b7f39b86cd1dae5bb9b292f50b65a4" >

  
  
  
  
  
  
  
  
  <script src="/js/wowchemy-init.min.66d3e0fff6d32c4ece05adee927fbd96.js"></script>

  


<aside class="search-modal" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Search</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#" aria-label="Close"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        
        <input name="q" id="search-query" placeholder="Search..." autocapitalize="off"
        autocomplete="off" autocorrect="off" spellcheck="false" type="search" class="form-control"
        aria-label="Search...">
        
      </div>

      
      

      

    </section>
    <section class="section-search-results">

      <div id="search-hits">
        
      </div>

    </section>
  </div>
</aside>



  <div class="page-header">
    












<nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id="navbar-main">
  <div class="container-xl">

    
    <div class="d-none d-lg-inline-flex">
      <a class="navbar-brand" href="/">Matteo Courthoud</a>
    </div>
    

    
    <button type="button" class="navbar-toggler" data-toggle="collapse"
            data-target="#navbar-content" aria-controls="navbar-content" aria-expanded="false" aria-label="Toggle navigation">
    <span><i class="fas fa-bars"></i></span>
    </button>
    

    
    <div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none">
      <a class="navbar-brand" href="/">Matteo Courthoud</a>
    </div>
    

    
    
    <div class="navbar-collapse main-menu-item collapse justify-content-end" id="navbar-content">

      
      <ul class="navbar-nav d-md-inline-flex">
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
            
            
            
              
            
            
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#about"><span>Home</span></a>
        </li>

        
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
            
            
            
              
            
            
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#posts"><span>Posts</span></a>
        </li>

        
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
            
            
            
              
            
            
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#projects"><span>Research</span></a>
        </li>

        
        

        
        <li class="nav-item dropdown">
          <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown" aria-haspopup="true"><span>Courses</span><span class="caret"></span>
          </a>
          <div class="dropdown-menu">
            
              <a class="dropdown-item" href="/course/ml-econ/"><span>Machine Learning for Economics</span></a>
            
              <a class="dropdown-item" href="/course/data-science/"><span>Data Science with Python</span></a>
            
              <a class="dropdown-item" href="/course/empirical-io/"><span>PhD Industrial Organization</span></a>
            
              <a class="dropdown-item" href="/course/metrics/"><span>PhD Econometrics</span></a>
            
              <a class="dropdown-item" href="https://pp4rs.github.io/"><span>Programming Practices for Research</span></a>
            
          </div>
        </li>

        
        

        

        
        
        

        

        
        
        
        

        
          
            
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/cv"><span>CV</span></a>
        </li>

        
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
            
            
            
              
            
            
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#contact"><span>Contact</span></a>
        </li>

        
        

      

        
      </ul>
    </div>

    <ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2">

      
      
        
      

      
      

      
      

      
      

    </ul>

  </div>
</nav>


  </div>

  <div class="page-body">
    

<div class="container-fluid docs">
  <div class="row flex-xl-nowrap">
    <div class="col-12 col-md-3 col-xl-2 docs-sidebar">
      
<nav class="collapse docs-links" id="docs-nav">
  
  
  
  
  
  

  
  
    

    
      

      <ul class="nav docs-sidenav">
        <li><a href="/course/"><i class="fas fa-arrow-left pr-1"></i>Courses</a></li>
      </ul>

      
      
        
          
        
      


  
    
    
    
    
      
    
    

    
      <div class="docs-toc-item">
        <a class="docs-toc-link " href="/course/ml-econ/">ML for Economics</a>
    
      
        <ul class="nav docs-sidenav">
      


  <li class=""><a href="/course/ml-econ/01_regression/">Linear Regression</a></li>



  <li class=""><a href="/course/ml-econ/02_iv/">Instrumental Variables</a></li>



  <li class=""><a href="/course/ml-econ/03_nonparametric/">Non-Parametric Regression</a></li>



  <li class=""><a href="/course/ml-econ/04_crossvalidation/">Resampling Methods</a></li>



  <li class=""><a href="/course/ml-econ/05_regularization/">Model Selection and Regularization</a></li>



  <li class="active"><a href="/course/ml-econ/06_convexity/">Convexity and Optimization</a></li>



  <li class=""><a href="/course/ml-econ/07_trees/">Tree-based Methods</a></li>



  <li class=""><a href="/course/ml-econ/08_neuralnets/">Neural Networks</a></li>



  <li class=""><a href="/course/ml-econ/09_postdoubleselection/">Post-Double Selection</a></li>



  <li class=""><a href="/course/ml-econ/10_unsupervised/">Unsupervised Learning</a></li>

      
        </ul>
      
    

    
      </div>
    

    
  
</nav>

    </div>

    
    
    <div class="d-none d-xl-block col-xl-2 docs-toc">
      

      <ul class="nav toc-top">
        <li><a href="#" id="back_to_top" class="docs-toc-title">Contents</a></li>
      </ul>

      <nav id="TableOfContents">
  <ul>
    <li><a href="#61-gradient-descent">6.1 Gradient Descent</a>
      <ul>
        <li><a href="#warm-up-optimizing-a-quadratic">Warm-up: Optimizing a quadratic</a></li>
        <li><a href="#constrained-optimization">Constrained Optimization</a></li>
        <li><a href="#least-squares">Least Squares</a></li>
        <li><a href="#overdetermined-case-mge-n">Overdetermined case $m\ge n$</a></li>
        <li><a href="#convergence-in-objective">Convergence in Objective</a></li>
        <li><a href="#convergence-in-domain">Convergence in Domain</a></li>
        <li><a href="#underdetermined-case-m--n">Underdetermined Case $m &lt; n$</a></li>
      </ul>
    </li>
    <li><a href="#ell_2-regularized-least-squares">$\ell_2$-regularized least squares</a></li>
    <li><a href="#the-magic-of-implicit-regularization">The Magic of Implicit Regularization</a></li>
    <li><a href="#lasso">LASSO</a></li>
    <li><a href="#support-vector-machines">Support Vector Machines</a></li>
    <li><a href="#sparse-inverse-covariance-estimation">Sparse Inverse Covariance Estimation</a></li>
    <li><a href="#going-crazy-with-autograd">Going crazy with autograd</a></li>
  </ul>
</nav>

      
    </div>
    

    <main class="col-12 col-md-9 col-xl-8 py-md-3 pl-md-5 docs-content" role="main">

      <article class="article">

        <div class="docs-article-container">

          <h1>Convexity and Optimization</h1>

          <p>Last updated on Mar 9, 2022</p>

          <div class="article-style">
            <pre><code class="language-python"># Remove warnings
import warnings
warnings.filterwarnings('ignore')
</code></pre>
<pre><code class="language-python"># Import
import autograd.numpy as np
from autograd import grad
import seaborn as sns
</code></pre>
<pre><code class="language-python"># Import matplotlib for graphs
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import axes3d

# Set global parameters
%matplotlib inline
plt.style.use('seaborn-white')
plt.rcParams['lines.linewidth'] = 3
plt.rcParams['figure.figsize'] = (10,6)
plt.rcParams['figure.titlesize'] = 20
plt.rcParams['axes.titlesize'] = 18
plt.rcParams['axes.labelsize'] = 14
plt.rcParams['legend.fontsize'] = 14
</code></pre>
<pre><code class="language-python"># Function to plot errors
def error_plot(ys, yscale='log'):
    plt.figure()
    plt.xlabel('Step')
    plt.ylabel('Error')
    plt.yscale(yscale)
    plt.plot(range(len(ys)), ys)
</code></pre>
<h2 id="61-gradient-descent">6.1 Gradient Descent</h2>
<p>We start with a basic implementation of projected gradient descent.</p>
<pre><code class="language-python">def gradient_descent(init, steps, grad, proj=lambda x: x):
    &quot;&quot;&quot;Projected gradient descent.
    
    Inputs:
        initial: starting point
        steps: list of scalar step sizes
        grad: function mapping points to gradients
        proj (optional): function mapping points to points
        
    Returns:
        List of all points computed by projected gradient descent.
    &quot;&quot;&quot;
    xs = [init]
    for step in steps:
        xs.append(proj(xs[-1] - step * grad(xs[-1])))
    return xs
</code></pre>
<p>Note that this implementation keeps around all points computed along the way. This is clearly not what you would do on large instances. We do this for illustrative purposes to be able to easily inspect the computed sequence of points.</p>
<h3 id="warm-up-optimizing-a-quadratic">Warm-up: Optimizing a quadratic</h3>
<p>As a toy example, let&rsquo;s optimize $$f(x)=\frac12|x|^2,$$ which has the gradient map $\nabla f(x)=x.$</p>
<pre><code class="language-python">def quadratic(x):
    return 0.5*x.dot(x)

def quadratic_gradient(x):
    return x
</code></pre>
<p>Note the function is $1$-smooth and $1$-strongly convex. Our theorems would then suggest that we use a constant step size of $1.$ If you think about it, for this step size the algorithm will actually find the optimal solution in just one step.</p>
<pre><code class="language-python">x0 = np.random.normal(0, 1, (1000))
_, x1 = gradient_descent(x0, [1.0], quadratic_gradient)
</code></pre>
<p>Indeed, it does.</p>
<pre><code class="language-python">x1.all() == 0
</code></pre>
<pre><code>True
</code></pre>
<p>Let&rsquo;s say we don&rsquo;t have the right learning rate.</p>
<pre><code class="language-python">xs = gradient_descent(x0, [0.1]*50, quadratic_gradient)
</code></pre>
<pre><code class="language-python"># Plot errors along steps
error_plot([quadratic(x) for x in xs])
</code></pre>
<p><img src="../img/06_convexity_18_0.png" alt="png"></p>
<h3 id="constrained-optimization">Constrained Optimization</h3>
<p>Let&rsquo;s say we want to optimize the function inside some affine subspace. Recall that affine subspaces are convex sets. Below we pick a random low dimensional affine subspace $b+U$ and define the corresponding linear projection operator.</p>
<pre><code class="language-python"># U is an orthonormal basis of a random 100-dimensional subspace.
U = np.linalg.qr(np.random.normal(0, 1, (1000, 100)))[0]
b = np.random.normal(0, 1, 1000)

def proj(x):
    &quot;&quot;&quot;Projection of x onto an affine subspace&quot;&quot;&quot;
    return b + U.dot(U.T).dot(x-b)
</code></pre>
<pre><code class="language-python">x0 = np.random.normal(0, 1, (1000))
xs = gradient_descent(x0, [0.1]*50, quadratic_gradient, proj)
# the optimal solution is the projection of the origin
x_opt = proj(0)
</code></pre>
<p>Let&rsquo;s plot the results.</p>
<pre><code class="language-python">error_plot([quadratic(x) for x in xs])
plt.plot(range(len(xs)), [quadratic(x_opt)]*len(xs),
        label='$\\frac{1}{2}|\!|x_{\mathrm{opt}}|\!|^2$')
plt.legend();
</code></pre>
<p><img src="../img/06_convexity_24_0.png" alt="png"></p>
<p>The orangle line shows the optimal error, which the algorithm reaches quickly.</p>
<p>The iterates also converge to the optimal solution in domain as the following plot shows.</p>
<pre><code class="language-python">error_plot([np.linalg.norm(x_opt-x)**2 for x in xs])
</code></pre>
<p><img src="../img/06_convexity_27_0.png" alt="png"></p>
<h3 id="least-squares">Least Squares</h3>
<p>One of the most fundamental data analysis tools is <em>linear least squares</em>. Given an $m\times n$ matrix $A$ and a vector $b$ our goal is to find a vector $x\in\mathbb{R}^n$ that minimizes the following objective:</p>
<p>
$$f(x) = \frac 1{2m}\sum_{i=1}^m (a_i^\top x - b_j)^2 
=\frac1{2m}\|Ax-b\|^2$$
</p>
<p>We can verify that $\nabla f(x) = A^\top(Ax-b)$ and
$\nabla^2 f(x) = A^\top A.$</p>
<p>Hence, the objective is $\beta$-smooth with
$\beta=\lambda_{\mathrm{max}}(A^\top A)$, and $\alpha$-strongly convex with $\alpha=\lambda_{\mathrm{min}}(A^\top A)$.</p>
<pre><code class="language-python">def least_squares(A, b, x):
    &quot;&quot;&quot;Least squares objective.&quot;&quot;&quot;
    return (0.5/m) * np.linalg.norm(A.dot(x)-b)**2

def least_squares_gradient(A, b, x):
    &quot;&quot;&quot;Gradient of least squares objective at x.&quot;&quot;&quot;
    return A.T.dot(A.dot(x)-b)/m
</code></pre>
<h3 id="overdetermined-case-mge-n">Overdetermined case $m\ge n$</h3>
<pre><code class="language-python">m, n = 1000, 100
A = np.random.normal(0, 1, (m, n))
x_opt = np.random.normal(0, 1, n)
noise = np.random.normal(0, 0.1, m)
b = A.dot(x_opt) + noise
objective = lambda x: least_squares(A, b, x)
gradient = lambda x: least_squares_gradient(A, b, x)
</code></pre>
<h3 id="convergence-in-objective">Convergence in Objective</h3>
<pre><code class="language-python">x0 = np.random.normal(0, 1, n)
xs = gradient_descent(x0, [0.1]*100, gradient)
</code></pre>
<pre><code class="language-python">error_plot([objective(x) for x in xs])
plt.plot(range(len(xs)), [np.linalg.norm(noise)**2]*len(xs),
        label='noise level')
plt.plot(range(len(xs)), [least_squares(A,b,x_opt)]*len(xs),
        label='optimal')
plt.legend();
</code></pre>
<p><img src="../img/06_convexity_35_0.png" alt="png"></p>
<h3 id="convergence-in-domain">Convergence in Domain</h3>
<pre><code class="language-python">error_plot([np.linalg.norm(x-x_opt)**2 for x in xs])
</code></pre>
<p><img src="../img/06_convexity_37_0.png" alt="png"></p>
<h3 id="underdetermined-case-m--n">Underdetermined Case $m &lt; n$</h3>
<p>In the underdetermined case, the least squares objective is inevitably not strongly convex, since $A^\top A$ is a rank deficient matrix and hence $\lambda_{\mathrm{min}}(A^\top A)=0.$</p>
<pre><code class="language-python">m, n = 100, 1000
A = np.random.normal(0, 1, (m, n))
b = np.random.normal(0, 1, m)
# The least norm solution is given by the pseudo-inverse
x_opt = np.linalg.pinv(A).dot(b)
objective = lambda x: least_squares(A, b, x)
gradient = lambda x: least_squares_gradient(A, b, x)
x0 = np.random.normal(0, 1, n)
xs = gradient_descent(x0, [0.1]*100, gradient)
</code></pre>
<p>Results.</p>
<pre><code class="language-python">error_plot([objective(x) for x in xs])
plt.plot(range(len(xs)), [least_squares(A,b,x_opt)]*len(xs),
        label='optimal')
plt.legend();
</code></pre>
<p><img src="../img/06_convexity_41_0.png" alt="png"></p>
<p>While we quickly reduce the error, we don&rsquo;t actually converge in domain to the least norm solution. This is just because the function is no longer strongly convex in the underdetermined case.</p>
<pre><code class="language-python">error_plot([np.linalg.norm(x-x_opt)**2 for x in xs], yscale='linear')
plt.plot(range(len(xs)), [np.linalg.norm(x_opt)**2]*len(xs),
         label='$|\!|x_{\mathrm{opt}}|\!|^2$')
plt.legend();
</code></pre>
<p><img src="../img/06_convexity_43_0.png" alt="png"></p>
<h2 id="ell_2-regularized-least-squares">$\ell_2$-regularized least squares</h2>
<p>In the underdetermined case, it is often desirable to restore strong convexity of the objective function by adding an $\ell_2^2$-penality, also known as <em>Tikhonov regularization</em>, $\ell_2$-regularization, or <em>weight decay</em>.</p>
<p>
$$\frac1{2m}\|Ax-b\|^2 + \frac{\alpha}2\|x\|^2$$
</p>
<p>Note: With this modification the objective is $\alpha$-strongly convex again.</p>
<pre><code class="language-python">def least_squares_l2(A, b, x, alpha=0.1):
    return least_squares(A, b, x) + (alpha/2) * x.dot(x)

def least_squares_l2_gradient(A, b, x, alpha=0.1):
    return least_squares_gradient(A, b, x) + alpha * x
</code></pre>
<p>Let&rsquo;s create a least squares instance.</p>
<pre><code class="language-python">m, n = 100, 1000
A = np.random.normal(0, 1, (m, n))
b = A.dot(np.random.normal(0, 1, n))
objective = lambda x: least_squares_l2(A, b, x)
gradient = lambda x: least_squares_l2_gradient(A, b, x)
</code></pre>
<p>Note that we can find the optimal solution to the optimization problem in closed form without even running gradient descent by computing $x_{\mathrm{opt}}=(A^\top+\alpha I)^{-1}A^\top b.$ Please verify that this point is indeed optimal.</p>
<pre><code class="language-python">x_opt = np.linalg.inv(A.T.dot(A) + 0.1*np.eye(1000)).dot(A.T).dot(b)
</code></pre>
<p>Here&rsquo;s how gradient descent fares.</p>
<pre><code class="language-python">x0 = np.random.normal(0, 1, n)
xs = gradient_descent(x0, [0.1]*500, gradient)
</code></pre>
<p>We plot the descent.</p>
<pre><code class="language-python">error_plot([objective(x) for x in xs])
plt.plot(range(len(xs)), [least_squares_l2(A,b,x_opt)]*len(xs),
        label='optimal')
plt.legend();
</code></pre>
<p><img src="../img/06_convexity_54_0.png" alt="png"></p>
<p>You see that the error doesn&rsquo;t decrease below a certain level due to the regularization term. This is not a bad thing. In fact, the regularization term gives as <em>strong convexity</em> which leads to convergence in domain again:</p>
<pre><code class="language-python">xs = gradient_descent(x0, [0.1]*500, gradient)
</code></pre>
<pre><code class="language-python">error_plot([np.linalg.norm(x-x_opt)**2 for x in xs])
plt.plot(range(len(xs)), [np.linalg.norm(x_opt)**2]*len(xs),
        label='squared norm of $x_{\mathrm{opt}}$')
plt.legend();
</code></pre>
<p><img src="../img/06_convexity_57_0.png" alt="png"></p>
<h2 id="the-magic-of-implicit-regularization">The Magic of Implicit Regularization</h2>
<p>Sometimes simply running gradient descent from a suitable initial point has a regularizing effect on its own <strong>without introducing an explicit regularization term</strong>.</p>
<p>We will see this below where we revisit the unregularized least squares objective, but initialize gradient descent from the origin rather than a random gaussian point.</p>
<pre><code class="language-python"># We initialize from 0
x0 = np.zeros(n)
# Note this is the gradient w.r.t. the unregularized objective!
gradient = lambda x: least_squares_gradient(A, b, x)
xs = gradient_descent(x0, [0.1]*50, gradient)
error_plot([np.linalg.norm(x_opt-x)**2 for x in xs], yscale='linear')
plt.plot(range(len(xs)), [np.linalg.norm(x_opt)**2]*len(xs),
         label='$|\!|x_{\mathrm{opt}}|\!|^2$')
plt.legend();
</code></pre>
<p><img src="../img/06_convexity_60_0.png" alt="png"></p>
<p><em>Incredible!</em> We converge to the minimum norm solution!</p>
<p>Implicit regularization is a deep phenomenon that&rsquo;s an active research topic in learning and optimization. It&rsquo;s exciting that we see it play out in this simple least squares problem already!</p>
<h2 id="lasso">LASSO</h2>
<p>LASSO is the name for $\ell_1$-regularized least squares regression:</p>
<p>
$$\frac1{2m}\|Ax-b\|^2 + \alpha\|x\|_1$$
</p>
<p>We will see that LASSO is able to fine <em>sparse</em> solutions if they exist. This is a common motivation for using an $\ell_1$-regularizer.</p>
<pre><code class="language-python">def lasso(A, b, x, alpha=0.1):
    return least_squares(A, b, x) + alpha * np.linalg.norm(x, 1)

def ell1_subgradient(x):
    &quot;&quot;&quot;Subgradient of the ell1-norm at x.&quot;&quot;&quot;
    g = np.ones(x.shape)
    g[x &lt; 0.] = -1.0
    return g

def lasso_subgradient(A, b, x, alpha=0.1):
    &quot;&quot;&quot;Subgradient of the lasso objective at x&quot;&quot;&quot;
    return least_squares_gradient(A, b, x) + alpha*ell1_subgradient(x)
</code></pre>
<pre><code class="language-python">m, n = 100, 1000
A = np.random.normal(0, 1, (m, n))
x_opt = np.zeros(n)
x_opt[:10] = 1.0
b = A.dot(x_opt)
x0 = np.random.normal(0, 1, n)
xs = gradient_descent(x0, [0.1]*500, lambda x: lasso_subgradient(A, b, x))
</code></pre>
<pre><code class="language-python">error_plot([lasso(A, b, x) for x in xs])
</code></pre>
<p><img src="../img/06_convexity_66_0.png" alt="png"></p>
<pre><code class="language-python">plt.figure()
plt.title('Comparison of initial, optimal, and computed point')
idxs = range(50)
plt.plot(idxs, x0[idxs], '--', color='#aaaaaa', label='initial')
plt.plot(idxs, x_opt[idxs], 'r-', label='optimal')
plt.plot(idxs, xs[-1][idxs], 'g-', label='final')
plt.xlabel('Coordinate')
plt.ylabel('Value')
plt.legend();
</code></pre>
<p><img src="../img/06_convexity_67_0.png" alt="png"></p>
<p>As promised, LASSO correctly identifies the significant coordinates of the optimal solution. This is why, in practice, LASSO is a popular tool for feature selection.</p>
<p>Play around with this plot to inspect other points along the way, e.g., the point that achieves lowest objective value. Why does the objective value go up even though we continue to get better solutions?</p>
<h2 id="support-vector-machines">Support Vector Machines</h2>
<p>In a linear classification problem, we&rsquo;re given $m$ labeled points $(a_i, y_i)$ and we wish to find a hyperplane given by a point $x$ that separates them so that</p>
<ul>
<li>$\langle a_i, x\rangle \ge 1$ when $y_i=1$, and</li>
<li>$\langle a_i, x\rangle \le -1$ when $y_i = -1$</li>
</ul>
<p>The smaller the norm $|x|$ the larger the <em>margin</em> between positive and negative instances. Therefore, it makes sense to throw in a regularizer that penalizes large norms. This leads to the objective.</p>
<p>
$$\frac 1m \sum_{i=1}^m \max\{1-y_i(a_i^\top x), 0\} + \frac{\alpha}2\|x\|^2$$
</p>
<pre><code class="language-python">def hinge_loss(z):
    return np.maximum(1.-z, np.zeros(z.shape))

def svm_objective(A, y, x, alpha=0.1):
    &quot;&quot;&quot;SVM objective.&quot;&quot;&quot;
    m, _ = A.shape
    return np.mean(hinge_loss(np.diag(y).dot(A.dot(x))))+(alpha/2)*x.dot(x)
</code></pre>
<pre><code class="language-python">z = np.linspace(-2, 2, 100)
</code></pre>
<pre><code class="language-python">plt.figure()
plt.plot(z, hinge_loss(z));
</code></pre>
<p><img src="../img/06_convexity_73_0.png" alt="png"></p>
<pre><code class="language-python">def hinge_subgradient(z):
    g = np.zeros(z.shape)
    g[z &lt; 1] = -1.
    return g

def svm_subgradient(A, y, x, alpha=0.1):
    g1 = hinge_subgradient(np.diag(y).dot(A.dot(x)))
    g2 = np.diag(y).dot(A)
    return g1.dot(g2) + alpha*x
</code></pre>
<pre><code class="language-python">plt.figure()
plt.plot(z, hinge_subgradient(z));
</code></pre>
<p><img src="../img/06_convexity_75_0.png" alt="png"></p>
<pre><code class="language-python">m, n = 1000, 100
A = np.vstack([np.random.normal(0.1, 1, (m//2, n)),
               np.random.normal(-0.1, 1, (m//2, n))])
y = np.hstack([np.ones(m//2), -1.*np.ones(m//2)])
x0 = np.random.normal(0, 1, n)
xs = gradient_descent(x0, [0.01]*100, 
                      lambda x: svm_subgradient(A, y, x, 0.05))
</code></pre>
<pre><code class="language-python">error_plot([svm_objective(A, y, x) for x in xs])
</code></pre>
<p><img src="../img/06_convexity_77_0.png" alt="png"></p>
<p>Let&rsquo;s see if averaging out the solutions gives us an improved function value.</p>
<pre><code class="language-python">xavg = 0.0
for x in xs:
    xavg += x
svm_objective(A, y, xs[-1]), svm_objective(A, y, xavg/len(xs))
</code></pre>
<pre><code>(1.0710162653835846, 0.9069593413738611)
</code></pre>
<p>We can also look at the accuracy of our linear model for predicting the labels. From how we defined the data, we can see that the all ones vector is the highest accuracy classifier in the limit of infinite data (very large $m$). For a finite data set, the accuracy could be even higher due to random fluctuations.</p>
<pre><code class="language-python">def accuracy(A, y, x):
    return np.mean(np.diag(y).dot(A.dot(x))&gt;0)
</code></pre>
<pre><code class="language-python">plt.figure()
plt.ylabel('Accuracy')
plt.xlabel('Step')
plt.plot(range(len(xs)), [accuracy(A, y, x) for x in xs])
plt.plot(range(len(xs)), [accuracy(A, y, np.ones(n))]*len(xs),
        label='Population optimum')
plt.legend();
</code></pre>
<p><img src="../img/06_convexity_82_0.png" alt="png"></p>
<p>We see that the accuracy spikes pretty early and drops a bit as we train for too long.</p>
<h2 id="sparse-inverse-covariance-estimation">Sparse Inverse Covariance Estimation</h2>
<p>Given a positive semidefinite matrix $S\in\mathbb{R}^{n\times n}$ the objective function in sparse inverse covariance estimation is as follows:</p>
<p>
$$ \min_{X\in\mathbb{R}^{n\times n}, X\succeq 0} 
\langle S, X\rangle - \log\det(X) + \alpha\|X\|_1$$
</p>
<p>Here, we define
$$\langle S, X\rangle = \mathrm{trace}(S^\top X)$$
and
$$|X|<em>1 = \sum</em>{ij}|X_{ij}|.$$</p>
<p>Typically, we think of the matrix $S$ as a sample covariance matrix of a set of vectors $a_1,\dots, a_m,$ defined as:
$$
S = \frac1{m-1}\sum_{i=1}^n a_ia_i^\top
$$
The example also highlights the utility of automatic differentiation as provided by the <code>autograd</code> package that we&rsquo;ll regularly use. In a later lecture we will understand exactly how automatic differentiation works. For now we just treat it as a blackbox that gives us gradients.</p>
<pre><code class="language-python">np.random.seed(1337)
</code></pre>
<pre><code class="language-python">def sparse_inv_cov(S, X, alpha=0.1):
    return (np.trace(S.T.dot(X))
            - np.log(np.linalg.det(X))
            + alpha * np.sum(np.abs(X)))
</code></pre>
<pre><code class="language-python">n = 5
A = np.random.normal(0, 1, (n, n))
S = A.dot(A.T)
objective = lambda X: sparse_inv_cov(S, X)
# autograd provides a &quot;gradient&quot;, yay!
gradient = grad(objective)
</code></pre>
<p>We also need to worry about the projection onto the positive semidefinite cone, which corresponds to truncating eigenvalues.</p>
<pre><code class="language-python">def projection(X):
    &quot;&quot;&quot;Projection onto positive semidefinite cone.&quot;&quot;&quot;
    es, U = np.linalg.eig(X)
    es[es&lt;0] = 0.
    return U.dot(np.diag(es).dot(U.T))
</code></pre>
<pre><code class="language-python">A0 = np.random.normal(0, 1, (n,n))
X0 = A0.dot(A0.T)
Xs = gradient_descent(X0, [0.01]*500, gradient, projection)
error_plot([objective(X) for X in Xs])
</code></pre>
<p><img src="../img/06_convexity_91_0.png" alt="png"></p>
<h2 id="going-crazy-with-autograd">Going crazy with autograd</h2>
<p>Just for fun, we&rsquo;ll go through a crazy example below. We can use <code>autograd</code> not just for getting gradients for natural objectives, we can in principle also use it to tune hyperparameters of our optimizer, like the step size schedulde.</p>
<p>Below we see how we can find a better 10-step learning rate schedules for optimizing a quadratic. This is mostly just for illustrative purposes (although some researchers are exploring these kinds of ideas more seriously).</p>
<pre><code class="language-python">x0 = np.random.normal(0, 1, 1000)
</code></pre>
<pre><code class="language-python">def f(x):
    return 0.5*np.dot(x,x)

def optimizer(steps):
    &quot;&quot;&quot;Optimize a quadratic with the given steps.&quot;&quot;&quot;
    xs = gradient_descent(x0, steps, grad(f))
    return f(xs[-1])
</code></pre>
<p>The function <code>optimizer</code> is a non-differentiable function of its input <code>steps</code>. Nontheless, <code>autograd</code> will provide a gradient that we can stick into gradient descent. That is, we&rsquo;re tuning gradient descent with gradient descent.</p>
<pre><code class="language-python">grad_optimizer = grad(optimizer)
</code></pre>
<pre><code class="language-python">initial_steps = np.abs(np.random.normal(0, 0.1, 10))
better_steps = gradient_descent(initial_steps, [0.001]*500, grad_optimizer)
</code></pre>
<pre><code class="language-python">error_plot([optimizer(steps) for steps in better_steps])
</code></pre>
<p><img src="../img/06_convexity_99_0.png" alt="png"></p>
<p>As we can see, the learning rate schedules improve dramatically over time. Of course, we already know from the first example that there is a step size schedule that converges in one step. Interestingly, the last schedule we find here doesn&rsquo;t look at all like what we might expect:</p>
<pre><code class="language-python">plt.figure()
plt.xticks(range(len(better_steps[-1])))
plt.ylabel('Step size')
plt.xlabel('Step number')
plt.plot(range(len(better_steps[-1])), better_steps[-1]);
</code></pre>
<p><img src="../img/06_convexity_101_0.png" alt="png"></p>

          </div>

          



          
          
          <div class="article-widget">
            
<div class="post-nav">
  
  
  
  <div class="post-nav-item">
    <div class="meta-nav">Previous</div>
    <a href="/course/ml-econ/05_regularization/" rel="next">Model Selection and Regularization</a>
  </div>
  
  
  
  <div class="post-nav-item">
    <div class="meta-nav">Next</div>
    <a href="/course/ml-econ/07_trees/" rel="prev">Tree-based Methods</a>
  </div>
  
</div>

          </div>
          
        </div>

        <div class="body-footer">

          





          




          


  
  



        </div>

      </article>

      <footer class="site-footer">

  

  
  <p class="powered-by">
    
      <a href="/privacy/">Privacy Policy</a>
    
    
       &middot; 
      <a href="/terms/">Terms</a>
    
  </p>
  

  

  
  







</footer>


    </main>
  </div>
</div>

  </div>

  <div class="page-footer">
    
    
  </div>

  
<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        <pre><code class="tex hljs"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

      

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/instant.page/5.1.0/instantpage.min.js" integrity="sha512-1+qUtKoh9XZW7j+6LhRMAyOrgSQKenQ4mluTR+cvxXjP1Z54RxZuzstR/H9kgPXQsVB8IW7DMDFUJpzLjvhGSQ==" crossorigin="anonymous"></script>

      
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js" integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin="anonymous"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js" integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin="anonymous"></script>
      

      
      

      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mermaid/8.8.4/mermaid.min.js" integrity="sha512-as1BF4+iHZ3BVO6LLDQ7zrbvTXM+c/1iZ1qII/c3c4L8Rn5tHLpFUtpaEtBNS92f+xGsCzsD7b62XP3XYap6oA==" crossorigin="anonymous" title="mermaid"></script>
      

      
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/highlight.min.js" integrity="sha512-TDKKr+IvoqZnPzc3l35hdjpHD0m+b2EC2SrLEgKDRWpxf2rFCxemkgvJ5kfU48ip+Y+m2XVKyOCD85ybtlZDmw==" crossorigin="anonymous"></script>
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/languages/r.min.js"></script>
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/languages/python.min.js"></script>
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/languages/julia.min.js"></script>
        
      

    

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.7.1/leaflet.min.js" integrity="sha512-SeiQaaDh73yrb56sTW/RgVdi/mMqNeM2oBwubFHagc5BkixSpP1fvqF47mKzPGWYSSy4RwbBunrJBQ4Co8fRWA==" crossorigin="anonymous"></script>
    

    
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/anchor-js/4.2.2/anchor.min.js" integrity="sha512-I7w3ZdSFzw5j3jU3ZkNikBNeIrl3i+hEuEdwNmqUJvwNcaBUNcijnP2gd9DtGlgVYDplfjGoD8vTNsID+lCjqg==" crossorigin="anonymous"></script>
    <script>
      anchors.add();
    </script>
    

    
    
    
      
      <script id="search-hit-fuse-template" type="text/x-template">
        <div class="search-hit" id="summary-{{key}}">
          <div class="search-hit-content">
            <div class="search-hit-name">
              <a href="{{relpermalink}}">{{title}}</a>
              <div class="article-metadata search-hit-type">{{type}}</div>
              <p class="search-hit-description">{{snippet}}</p>
            </div>
          </div>
        </div>
      </script>
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js" integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin="anonymous"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js" integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin="anonymous"></script>
      
    

    
    

    
    
    
    

    
    <script src="/js/bootstrap.bundle.min.6aed84840afc03ab4d5750157f69c120.js"></script>

    
    
      
      
      
      
      
      
      
    

    
    
    
    
    
    
    
    
      
      
    
    
    <script src="/en/js/wowchemy.min.4ea9cc8d09c5c158656ac1a804743b34.js"></script>

    
  <script async defer src="https://buttons.github.io/buttons.js"></script>




</body>
</html>
