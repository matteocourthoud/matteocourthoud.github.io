<!DOCTYPE html><html lang="en-us" >

<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  
  
  
    <meta name="generator" content="Wowchemy 5.1.0 for Hugo" />
  

  

  
  
  
  
  
    
    
    
  
  

  

  
  
  
    
  
  <meta name="description" content="%matplotlib inline from utils.lecture07 import *  Decision Trees Decision trees involve segmenting the predictor space into a number of simple regions. In order to make a prediction for a given observation, we typically use the mean or the mode of the training observations in the region to which it belongs." />

  
  <link rel="alternate" hreflang="en-us" href="https://matteocourthoud.github.io/course/ml-econ/07_trees/" />

  







  




  
  
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
  

  
  
  
    <meta name="theme-color" content="#003f5c" />
  

  
  
    
    <script src="/js/mathjax-config.js"></script>
  

  
  
  
  
    
    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha256-FMvZuGapsJLjouA6k7Eo2lusoAX9i0ShlWFG6qt7SLc=" crossorigin="anonymous">

    
    

    
    
    
      
    
    
      
      
        
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/styles/github.min.css" crossorigin="anonymous" title="hl-light" media="print" onload="this.media='all'">
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/styles/dracula.min.css" crossorigin="anonymous" title="hl-dark" media="print" onload="this.media='all'" disabled>
        
      
    

    
    
    
      <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.7.1/leaflet.min.css" integrity="sha512-1xoFisiGdy9nvho8EgXuXvnpR5GAMSjFwp40gSRE3NwdUdIMIKuPa7bqoUhLD0O/5tPNhteAsE5XyyMi5reQVA==" crossorigin="anonymous" media="print" onload="this.media='all'">
    

    

    
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js" integrity="" crossorigin="anonymous" async></script>
      
    
      
      

      
      

      
    
      
      

      
      

      
    
  

  
  
  
    
      
      <link rel="preload" as="style" href="https://fonts.googleapis.com/css2?family=Roboto+Mono&family=Source+Sans+Pro:wght@200;300;400;700&display=swap">
      <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Roboto+Mono&family=Source+Sans+Pro:wght@200;300;400;700&display=swap" media="print" onload="this.media='all'">
    
  

  
  
  
  
  
  <link rel="stylesheet" href="/css/wowchemy.5c4def4f00a521426f4eb098155f3342.css" />

  




<script async src="https://www.googletagmanager.com/gtag/js?id=UA-144780600-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];

  function gtag() {
      dataLayer.push(arguments);
  }

  function trackOutboundLink(url, target) {
    gtag('event', 'click', {
         'event_category': 'outbound',
         'event_label': url,
         'transport_type': 'beacon',
         'event_callback': function () {
           if (target !== '_blank') {
             document.location = url;
           }
         }
    });
    console.debug("Outbound link clicked: " + url);
  }

  function onClickCallback(event) {
    if ((event.target.tagName !== 'A') || (event.target.host === window.location.host)) {
      return;
    }
    trackOutboundLink(event.target, event.target.getAttribute('target'));  
  }

  gtag('js', new Date());
  gtag('config', 'UA-144780600-1', { 'anonymize_ip': true });
  gtag('set', {'cookie_flags': 'SameSite=None;Secure'});

  
  document.addEventListener('click', onClickCallback, false);
</script>


  

  

  




  
  
  

  

  
    <link rel="manifest" href="/index.webmanifest" />
  

  <link rel="icon" type="image/png" href="/media/icon_hu03e9b3967b83fd39296ec9da5ff1ea05_201175_32x32_fill_lanczos_center_3.png" />
  <link rel="apple-touch-icon" type="image/png" href="/media/icon_hu03e9b3967b83fd39296ec9da5ff1ea05_201175_180x180_fill_lanczos_center_3.png" />

  <link rel="canonical" href="https://matteocourthoud.github.io/course/ml-econ/07_trees/" />

  
  
  
  
  
  
  
  
    
    
  
  
  <meta property="twitter:card" content="summary" />
  
  <meta property="og:site_name" content="Matteo Courthoud" />
  <meta property="og:url" content="https://matteocourthoud.github.io/course/ml-econ/07_trees/" />
  <meta property="og:title" content="Tree-based Methods | Matteo Courthoud" />
  <meta property="og:description" content="%matplotlib inline from utils.lecture07 import *  Decision Trees Decision trees involve segmenting the predictor space into a number of simple regions. In order to make a prediction for a given observation, we typically use the mean or the mode of the training observations in the region to which it belongs." /><meta property="og:image" content="https://matteocourthoud.github.io/media/icon_hu03e9b3967b83fd39296ec9da5ff1ea05_201175_512x512_fill_lanczos_center_3.png" />
    <meta property="twitter:image" content="https://matteocourthoud.github.io/media/icon_hu03e9b3967b83fd39296ec9da5ff1ea05_201175_512x512_fill_lanczos_center_3.png" /><meta property="og:locale" content="en-us" />
  
    
      <meta
        property="article:published_time"
        content="2022-03-09T00:00:00&#43;00:00"
      />
    
    <meta property="article:modified_time" content="2022-03-09T00:00:00&#43;00:00">
  

  



  

  
  
  
  
  
    <script src="https://cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.1.1/cookieconsent.min.js" integrity="sha256-5VhCqFam2Cn+yjw61zbBNrbHVJ6SRydPeKopYlngbiQ=" crossorigin="anonymous"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.1.1/cookieconsent.min.css" integrity="sha256-zQ0LblD/Af8vOppw18+2anxsuaz3pWYyVWi+bTvTH8Q=" crossorigin="anonymous">
  
  <script>
  window.addEventListener("load", function(){
    window.cookieconsent.initialise({
      "palette": {
        "popup": {
          "background": "#003f5c",
          "text": "rgb(255, 255, 255)"
        },
        "button": {
          "background": "rgb(255, 255, 255)",
          "text": "#003f5c"
        }
      },
      "theme": "classic",
      "content": {
        "message": "This website uses cookies to ensure you get the best experience on our website.",
        "dismiss": "Got it!",
        "link": "Learn more",
        "href": "/privacy/"
      }
    })});
  </script>


  





  <title>Tree-based Methods | Matteo Courthoud</title>
</head>


<body id="top" data-spy="scroll" data-offset="70" data-target="#TableOfContents" class="page-wrapper   " data-wc-page-id="54aa48269b53b49cab7798a7b9e67a1b" >

  
  
  
  
  
  
  
  
  <script src="/js/wowchemy-init.min.6edaf3b475ce43de30d98828aea698be.js"></script>

  


<aside class="search-modal" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Search</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#" aria-label="Close"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        
        <input name="q" id="search-query" placeholder="Search..." autocapitalize="off"
        autocomplete="off" autocorrect="off" spellcheck="false" type="search" class="form-control"
        aria-label="Search...">
        
      </div>

      
      

      

    </section>
    <section class="section-search-results">

      <div id="search-hits">
        
      </div>

    </section>
  </div>
</aside>



  <div class="page-header">
    












<nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id="navbar-main">
  <div class="container-xl">

    
    <div class="d-none d-lg-inline-flex">
      <a class="navbar-brand" href="/">Matteo Courthoud</a>
    </div>
    

    
    <button type="button" class="navbar-toggler" data-toggle="collapse"
            data-target="#navbar-content" aria-controls="navbar-content" aria-expanded="false" aria-label="Toggle navigation">
    <span><i class="fas fa-bars"></i></span>
    </button>
    

    
    <div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none">
      <a class="navbar-brand" href="/">Matteo Courthoud</a>
    </div>
    

    
    
    <div class="navbar-collapse main-menu-item collapse justify-content-end" id="navbar-content">

      
      <ul class="navbar-nav d-md-inline-flex">
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
            
            
            
              
            
            
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#about"><span>Home</span></a>
        </li>

        
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
            
            
            
              
            
            
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#projects"><span>Research</span></a>
        </li>

        
        

        
        <li class="nav-item dropdown">
          <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown" aria-haspopup="true"><span>Courses</span><span class="caret"></span>
          </a>
          <div class="dropdown-menu">
            
              <a class="dropdown-item" href="/course/ml-econ/"><span>Machine Learning for Economics</span></a>
            
              <a class="dropdown-item" href="/course/data-science/"><span>Data Science with Python</span></a>
            
              <a class="dropdown-item" href="/course/empirical-io/"><span>PhD Industrial Organization</span></a>
            
              <a class="dropdown-item" href="/course/metrics/"><span>PhD Econometrics</span></a>
            
              <a class="dropdown-item" href="https://pp4rs.github.io/"><span>Programming Practices for Research</span></a>
            
          </div>
        </li>

        
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/post/"><span>Posts</span></a>
        </li>

        
        

        

        
        
        

        

        
        
        
        

        
          
            
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/cv"><span>CV</span></a>
        </li>

        
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
            
            
            
              
            
            
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#contact"><span>Contact</span></a>
        </li>

        
        

      

        
      </ul>
    </div>

    <ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2">

      
      
        
      

      
      

      
      

      
      

    </ul>

  </div>
</nav>


  </div>

  <div class="page-body">
    

<div class="container-fluid docs">
  <div class="row flex-xl-nowrap">
    <div class="col-12 col-md-3 col-xl-2 docs-sidebar">
      
<nav class="collapse docs-links" id="docs-nav">
  
  
  
  
  
  

  
  
    

    
      

      <ul class="nav docs-sidenav">
        <li><a href="/course/"><i class="fas fa-arrow-left pr-1"></i>Courses</a></li>
      </ul>

      
      
        
          
        
      


  
    
    
    
    
      
    
    

    
      <div class="docs-toc-item">
        <a class="docs-toc-link " href="/course/ml-econ/">ML for Economics</a>
    
      
        <ul class="nav docs-sidenav">
      


  <li class=""><a href="/course/ml-econ/01_regression/">Linear Regression</a></li>



  <li class=""><a href="/course/ml-econ/02_iv/">Instrumental Variables</a></li>



  <li class=""><a href="/course/ml-econ/03_nonparametric/">Non-Parametric Regression</a></li>



  <li class=""><a href="/course/ml-econ/04_crossvalidation/">Resampling Methods</a></li>



  <li class=""><a href="/course/ml-econ/05_regularization/">Model Selection and Regularization</a></li>



  <li class=""><a href="/course/ml-econ/06_convexity/">Convexity and Optimization</a></li>



  <li class="active"><a href="/course/ml-econ/07_trees/">Tree-based Methods</a></li>



  <li class=""><a href="/course/ml-econ/08_neuralnets/">Neural Networks</a></li>



  <li class=""><a href="/course/ml-econ/09_postdoubleselection/">Post-Double Selection</a></li>



  <li class=""><a href="/course/ml-econ/10_unsupervised/">Unsupervised Learning</a></li>

      
        </ul>
      
    

    
      </div>
    

    
  
</nav>

    </div>

    
    
    <div class="d-none d-xl-block col-xl-2 docs-toc">
      

      <ul class="nav toc-top">
        <li><a href="#" id="back_to_top" class="docs-toc-title">Contents</a></li>
      </ul>

      <nav id="TableOfContents">
  <ul>
    <li><a href="#decision-trees">Decision Trees</a>
      <ul>
        <li><a href="#regression-trees">Regression Trees</a></li>
        <li><a href="#building-a-tree">Building a Tree</a></li>
        <li><a href="#pruning">Pruning</a></li>
        <li><a href="#classification-trees">Classification Trees</a></li>
        <li><a href="#building-a-classification-tree">Building a Classification Tree</a></li>
        <li><a href="#pruning-for-classification">Pruning for Classification</a></li>
        <li><a href="#other-issues">Other Issues</a></li>
        <li><a href="#trees-vs-regression">Trees vs Regression</a></li>
      </ul>
    </li>
    <li><a href="#72-bagging-random-forests-boosting">7.2 Bagging, Random Forests, Boosting</a>
      <ul>
        <li><a href="#bagging">Bagging</a></li>
        <li><a href="#out-of-bag-error-estimation">Out-of-Bag Error Estimation</a></li>
        <li><a href="#variable-importance-measures">Variable Importance Measures</a></li>
        <li><a href="#random-forests">Random Forests</a></li>
        <li><a href="#boosting">Boosting</a></li>
        <li><a href="#algorithm">Algorithm</a></li>
      </ul>
    </li>
  </ul>
</nav>

      
    </div>
    

    <main class="col-12 col-md-9 col-xl-8 py-md-3 pl-md-5 docs-content" role="main">

      <article class="article">

        <div class="docs-article-container">

          <h1>Tree-based Methods</h1>

          <p>Last updated on Mar 9, 2022</p>

          <div class="article-style">
            <pre><code class="language-python">%matplotlib inline
from utils.lecture07 import *
</code></pre>
<h2 id="decision-trees">Decision Trees</h2>
<p>Decision trees involve <strong>segmenting the predictor space into a number of simple regions</strong>. In order to make a prediction for a given observation, we typically use the mean or the mode of the training observations in the region to which it belongs. Since the set of splitting rules used to segment the predictor space can be summarized in a tree, these types of approaches are known as decision tree methods.</p>
<h3 id="regression-trees">Regression Trees</h3>
<p>For this session we will consider the <code>Hitters</code> dataset. It consists in individual level data of baseball players. In our applications, we are interested in predicting the players <code>Salary</code>.</p>
<pre><code class="language-python"># Load the data
hitters = pd.read_csv('data/Hitters.csv').dropna()
hitters.head()
</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
</code></pre>
<p></style></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Unnamed: 0</th>
      <th>AtBat</th>
      <th>Hits</th>
      <th>HmRun</th>
      <th>Runs</th>
      <th>RBI</th>
      <th>Walks</th>
      <th>Years</th>
      <th>CAtBat</th>
      <th>CHits</th>
      <th>...</th>
      <th>CRuns</th>
      <th>CRBI</th>
      <th>CWalks</th>
      <th>League</th>
      <th>Division</th>
      <th>PutOuts</th>
      <th>Assists</th>
      <th>Errors</th>
      <th>Salary</th>
      <th>NewLeague</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>-Alan Ashby</td>
      <td>315</td>
      <td>81</td>
      <td>7</td>
      <td>24</td>
      <td>38</td>
      <td>39</td>
      <td>14</td>
      <td>3449</td>
      <td>835</td>
      <td>...</td>
      <td>321</td>
      <td>414</td>
      <td>375</td>
      <td>N</td>
      <td>W</td>
      <td>632</td>
      <td>43</td>
      <td>10</td>
      <td>475.0</td>
      <td>N</td>
    </tr>
    <tr>
      <th>2</th>
      <td>-Alvin Davis</td>
      <td>479</td>
      <td>130</td>
      <td>18</td>
      <td>66</td>
      <td>72</td>
      <td>76</td>
      <td>3</td>
      <td>1624</td>
      <td>457</td>
      <td>...</td>
      <td>224</td>
      <td>266</td>
      <td>263</td>
      <td>A</td>
      <td>W</td>
      <td>880</td>
      <td>82</td>
      <td>14</td>
      <td>480.0</td>
      <td>A</td>
    </tr>
    <tr>
      <th>3</th>
      <td>-Andre Dawson</td>
      <td>496</td>
      <td>141</td>
      <td>20</td>
      <td>65</td>
      <td>78</td>
      <td>37</td>
      <td>11</td>
      <td>5628</td>
      <td>1575</td>
      <td>...</td>
      <td>828</td>
      <td>838</td>
      <td>354</td>
      <td>N</td>
      <td>E</td>
      <td>200</td>
      <td>11</td>
      <td>3</td>
      <td>500.0</td>
      <td>N</td>
    </tr>
    <tr>
      <th>4</th>
      <td>-Andres Galarraga</td>
      <td>321</td>
      <td>87</td>
      <td>10</td>
      <td>39</td>
      <td>42</td>
      <td>30</td>
      <td>2</td>
      <td>396</td>
      <td>101</td>
      <td>...</td>
      <td>48</td>
      <td>46</td>
      <td>33</td>
      <td>N</td>
      <td>E</td>
      <td>805</td>
      <td>40</td>
      <td>4</td>
      <td>91.5</td>
      <td>N</td>
    </tr>
    <tr>
      <th>5</th>
      <td>-Alfredo Griffin</td>
      <td>594</td>
      <td>169</td>
      <td>4</td>
      <td>74</td>
      <td>51</td>
      <td>35</td>
      <td>11</td>
      <td>4408</td>
      <td>1133</td>
      <td>...</td>
      <td>501</td>
      <td>336</td>
      <td>194</td>
      <td>A</td>
      <td>W</td>
      <td>282</td>
      <td>421</td>
      <td>25</td>
      <td>750.0</td>
      <td>A</td>
    </tr>
  </tbody>
</table>
<p>5 rows × 21 columns</p>
</div>
<p>In particular, we are interested in looking how the number of <code>Hits</code> and the <code>Years</code> of experience predict the <code>Salary</code>.</p>
<pre><code class="language-python"># Get Features
features = ['Years', 'Hits']
X = hitters[features].values
y = np.log(hitters.Salary.values)
</code></pre>
<p>We are actually going to use log(salary) since it has a more gaussian distribution.</p>
<pre><code class="language-python">fig, (ax1, ax2) = plt.subplots(1,2, figsize=(11,4))

# Plot salary distribution
ax1.hist(hitters.Salary.values)
ax1.set_xlabel('Salary')
ax2.hist(y)
ax2.set_xlabel('Log(Salary)');
</code></pre>
<p><img src="../img/07_trees_10_0.png" alt="png"></p>
<p>In order to understand what is a tree, let&rsquo;s first have a look at one. We fit a regression three with 3 leaves or, equivalently put, 2 nodes.</p>
<pre><code class="language-python"># Fit regression tree
tree = DecisionTreeRegressor(max_leaf_nodes=3)
tree.fit(X, y)
</code></pre>
<pre><code>DecisionTreeRegressor(max_leaf_nodes=3)
</code></pre>
<p>We are now going to plot the results visually. The biggest avantage of trees is interpretability.</p>
<pre><code class="language-python"># Figure 8.1
fig, ax = plt.subplots(1,1)
ax.set_title('Figure 8.1');

# Plot tree
plot_tree(tree, filled=True, feature_names=features, fontsize=14, ax=ax);
</code></pre>
<p><img src="../img/07_trees_14_0.png" alt="png"></p>
<p>The tree consists of a series of splitting rules, starting at the top of the tree. The top split assigns observations having <code>Years</code>&lt;4.5 to the left branch.1 The predicted salary for these players is given by the mean response value for the players in the data set with <code>Years</code>&lt;4.5. For such players, the mean log salary is 5.107, and so we make a prediction of 5.107 thousands of dollars, i.e. $165,174, for these players. Players with <code>Years</code>&gt;=4.5 are assigned to the right branch, and then that group is further subdivided by <code>Hits</code>.</p>
<p>Overall, the tree stratifies or segments the players into three regions of predictor space:</p>
<ol>
<li>players who have played for four or fewer years</li>
<li>players who have played for five or more years and who made fewer than 118 hits last year, and</li>
<li>players who have played for five or more years and who made at least 118 hits last year.</li>
</ol>
<p>These three regions can be written as</p>
<ol>
<li><strong>R1</strong> = {X | <code>Years</code>&lt;4.5}</li>
<li><strong>R2</strong> = {X | <code>Years</code>&gt;=4.5, <code>Hits</code>&lt;117.5}, and</li>
<li><strong>R3</strong> = {X | <code>Years</code>&gt;=4.5, <code>Hits</code>&gt;=117.5}.</li>
</ol>
<p>Since the dimension of $X$ is 2, we can visualize the space and the regions in a 2-dimensional graph.</p>
<pre><code class="language-python"># Figure 8.2
def make_figure_8_2():
    
    # Init
    hitters.plot('Years', 'Hits', kind='scatter', color='orange', figsize=(7,6))
    plt.title('Figure 8.2')
    plt.xlim(0,25); plt.ylim(ymin=-5);
    plt.xticks([1, 4.5, 24]); plt.yticks([1, 117.5, 238]);

    # Split lines
    plt.vlines(4.5, ymin=-5, ymax=250, color='g')
    plt.hlines(117.5, xmin=4.5, xmax=25, color='g')

    # Regions
    plt.annotate('R1', xy=(2,117.5), fontsize='xx-large')
    plt.annotate('R2', xy=(11,60), fontsize='xx-large')
    plt.annotate('R3', xy=(11,170), fontsize='xx-large');
</code></pre>
<pre><code class="language-python"> make_figure_8_2()
</code></pre>
<p><img src="../img/07_trees_19_0.png" alt="png"></p>
<p>We might <strong>interpret</strong> the above regression tree as follows: Years is the most important factor in determining Salary, and players with less experience earn lower salaries than more experienced players. Given that a player is less experienced, the number of hits that he made in the previous year seems to play little role in his salary. But among players who have been in the major leagues for five or more years, the number of hits made in the previous year does affect salary, and players who made more hits last year tend to have higher salaries.</p>
<h3 id="building-a-tree">Building a Tree</h3>
<p>There are two main steps in the construction of a tree:</p>
<ol>
<li>We divide the predictor space—that is, the set of possible values for $X_1, X_2, &hellip; , X_p$ into $J$ distinct and non-overlapping regions, $R_1,R_2,&hellip;,R_J$.</li>
<li>For every observation that falls into the region $R_j$ , we make the same prediction, which is simply the mean of the response values for the training observations in $R_j$.</li>
</ol>
<p>The second step is easy. But how does one construct the regions? Our purpose is to minimize the Sum of Squared Residuals, across the different regions:</p>
<p>$$
\sum_{j=1}^{J} \sum_{i \in R_{j}}\left(y_{i}-\hat{y}<em>{R</em>{j}}\right)^{2}
$$</p>
<p>Unfortunately, it is computationally infeasible to consider every possible partition of the feature space into $J$ boxes.</p>
<p>For this reason, we take a <strong>top-down</strong>, <strong>greedy</strong> approach that is known as <em>recursive binary splitting</em>. The approach is top-down because it begins at the top of the tree (at which point all observations belong to a single region) and then successively splits the predictor space; each split is indicated via two new branches further down on the tree. It is greedy because at each step of the tree-building process, the best split is made at that particular step, rather than looking ahead and picking a split that will lead to a better tree in some future step.</p>
<p>In practice, the method is the following:</p>
<ol>
<li>we select the predictor $X_j$</li>
<li>we select the cutpoint $s$ such that splitting the predictor space into the regions ${X|X_j &lt; s}$ and ${X|X_j \geq s}$ leads to the greatest possible reduction in RSS</li>
<li>we repeat (1)-(2) for all predictors $X_1, &hellip; , X_p$, i.e. we solve</li>
</ol>
<p>$$
\arg \min_{j,s} \ \sum_{i: x_{i} \in {X|X_j &lt; s}}\left(y_{i}-\hat{y}<em>i\right)^{2}+\sum</em>{i: x_{i} \in {X|X_j \geq s}}\left(y_{i}-\hat{y}_i\right)^{2}
$$</p>
<ol start="4">
<li>we choose the predictor and cutpoint such that the resulting tree has the lowest RSS</li>
<li>we keep repeating (1)-(4) until a certain condition is met. However, after the first iteration we also have to pick which region to split which adds a further dimension to optimize over.</li>
</ol>
<p>Let&rsquo;s build our own <code>Node</code> class to play around with trees.</p>
<pre><code class="language-python">class Node:
    &quot;&quot;&quot;
    Class used to represent nodes in a Regression Tree
    
    Attributes
    ----------
    x : np.array
        independent variables
    y : np.array
        dependent variables
    idxs : np.array
        indexes fo x and y for current node
    depth : int
        depth of the sub-tree (default 5)

    Methods
    -------
    find_next_nodes(self)
        Keep growing the tree
        
    find_best_split(self)
        Find the best split
        
    split(self)
        Split the tree
    &quot;&quot;&quot;
    
    def __init__(self, x, y, idxs, depth=5):
        &quot;&quot;&quot;Initialize node&quot;&quot;&quot;
        self.x = x
        self.y = y
        self.idxs = idxs 
        self.depth = depth
        self.get_next_nodes()

    def get_next_nodes(self):
        &quot;&quot;&quot;If the node is not terminal, get further splits&quot;&quot;&quot;
        if self.is_last_leaf: return 
        self.find_best_split()       
        self.split()             
        
    def find_best_split(self):
        &quot;&quot;&quot;Loop over variables and their values to find the best split&quot;&quot;&quot;
        best_score = float('inf')
        # Loop over variables
        for col in range(self.x.shape[1]):
            x = self.x[self.idxs, col]
            # Loop over all splits
            for s in np.unique(x):
                lhs = x &lt;= s
                rhs = x &gt; s
                curr_score = self.get_score(lhs, rhs)
                # If best score, save it 
                if curr_score &lt; best_score: 
                    best_score = curr_score
                    self.split_col = col
                    self.split_val = s
        return self
    
    def get_score(self, lhs, rhs):
        &quot;&quot;&quot;Get score of a given split&quot;&quot;&quot;
        y = self.y[self.idxs]
        lhs_mse = self.get_mse(y[lhs])
        rhs_mse = self.get_mse(y[rhs])
        return lhs_mse * lhs.sum() + rhs_mse * rhs.sum()
        
    def get_mse(self, y): return np.mean((y-np.mean(y))**2)
    
    def split(self): 
        &quot;&quot;&quot;Split a node into 2 sub-nodes (recursive)&quot;&quot;&quot;
        x = self.x[self.idxs, self.split_col]
        lhs = x &lt;= self.split_val
        rhs = x &gt; self.split_val
        self.lhs = Node(self.x, self.y, self.idxs[lhs], self.depth-1)
        self.rhs = Node(self.x, self.y, self.idxs[rhs], self.depth-1)
        to_print = (self.depth, self.split_col, self.split_val, sum(lhs), sum(rhs))
        print('Split on layer %.0f: var%1.0f = %.4f (%.0f/%.0f)' % to_print)
        return self
    
    @property
    def is_last_leaf(self): return self.depth&lt;=1

</code></pre>
<p>What does a <code>Node</code> look like?</p>
<pre><code class="language-python"># Init first node
tree1 = Node(X, y, np.arange(len(y)), 1)

# Documentation (always comment and document your code!)
print(tree1.__doc__)
</code></pre>
<pre><code>    Class used to represent nodes in a Regression Tree
    
    Attributes
    ----------
    x : np.array
        independent variables
    y : np.array
        dependent variables
    idxs : np.array
        indexes fo x and y for current node
    depth : int
        depth of the sub-tree (default 5)

    Methods
    -------
    find_next_nodes(self)
        Keep growing the tree
        
    find_best_split(self)
        Find the best split
        
    split(self)
        Split the tree
</code></pre>
<p>Which properties does it have?</p>
<pre><code class="language-python"># Inspect the class
dir(tree1)
</code></pre>
<pre><code>['__class__',
 '__delattr__',
 '__dict__',
 '__dir__',
 '__doc__',
 '__eq__',
 '__format__',
 '__ge__',
 '__getattribute__',
 '__gt__',
 '__hash__',
 '__init__',
 '__init_subclass__',
 '__le__',
 '__lt__',
 '__module__',
 '__ne__',
 '__new__',
 '__reduce__',
 '__reduce_ex__',
 '__repr__',
 '__setattr__',
 '__sizeof__',
 '__str__',
 '__subclasshook__',
 '__weakref__',
 'depth',
 'find_best_split',
 'get_mse',
 'get_next_nodes',
 'get_score',
 'idxs',
 'is_last_leaf',
 'split',
 'x',
 'y']
</code></pre>
<p>What is the depth? How many observations are there?</p>
<pre><code class="language-python"># Get info
print('Tree of depth %.0f with %.0f observations' % (tree1.depth, len(tree1.idxs)))
</code></pre>
<pre><code>Tree of depth 1 with 263 observations
</code></pre>
<p>Fair enough, the tree is just a single leaf.</p>
<pre><code class="language-python"># Check if terminal
tree1.is_last_leaf
</code></pre>
<pre><code>True
</code></pre>
<p>Let&rsquo;s find the first split.</p>
<pre><code class="language-python"># Find best split
tree1.find_best_split()
print('Split at var%1.0f = %.4f' % (tree1.split_col, tree1.split_val))
</code></pre>
<pre><code>Split at var0 = 4.0000
</code></pre>
<p>If has selected the first variable, at the value $4$.</p>
<p>If we call the <code>split</code> function, it will also tell us how many observations per leaf the split generates.</p>
<pre><code class="language-python"># Split tree
tree1.split();
</code></pre>
<pre><code>Split on layer 1: var0 = 4.0000 (90/173)
</code></pre>
<p>Now we are ready to compute even deeper trees</p>
<pre><code class="language-python"># Check depth-3 tree
tree3 = Node(X, y, np.arange(len(y)), 3)
</code></pre>
<pre><code>Split on layer 2: var1 = 4.0000 (2/88)
Split on layer 2: var1 = 117.0000 (90/83)
Split on layer 3: var0 = 4.0000 (90/173)
</code></pre>
<h3 id="pruning">Pruning</h3>
<p>The process described above may produce good predictions on the training set, but is likely to <strong>overfit</strong> the data, leading to poor test set performance. This is because the resulting tree might be too complex. A smaller tree with fewer splits might lead to lower variance and better interpretation at the cost of a little bias.</p>
<p>We can see it happening if we build the same tree as above, but with 5 leaves.</p>
<pre><code class="language-python"># Compute tree
overfit_tree = DecisionTreeRegressor(max_leaf_nodes=5).fit(X, y)
</code></pre>
<p>We plot the 5-leaf tree.</p>
<pre><code class="language-python"># Plot tree
fig, ax = plt.subplots(1,1)
plot_tree(overfit_tree, filled=True, feature_names=features, fontsize=14, ax=ax);
</code></pre>
<p><img src="../img/07_trees_45_0.png" alt="png"></p>
<p>The <strong>split on the far left</strong> is predicting a very high <code>Salary</code> (7.243) for players with few <code>Years</code> of experience and few <code>Hits</code>. Indeed this prediction is based on an extremely tiny subsample (2). They are probably outliers and our tree is most likely overfitting.</p>
<p>One possible alternative is to insert a minimum number of observation per leaf.</p>
<pre><code class="language-python"># Compute tree
no_overfit_tree = DecisionTreeRegressor(max_leaf_nodes=5, min_samples_leaf=10).fit(X, y)

# Plot tree
fig, ax = plt.subplots(1,1)
plot_tree(no_overfit_tree, filled=True, feature_names=features, fontsize=14, ax=ax);
</code></pre>
<p><img src="../img/07_trees_48_0.png" alt="png"></p>
<p>Now the tree makes much more sense: the lower the <code>Years</code> and the <code>Hits</code>, the lower the predicted <code>Salary</code> as we can see from the shades getting darker and darker as we move left to right</p>
<p>Another possible alternative to the process described above is to build the tree only so long as the decrease in the RSS due to each split exceeds some (high) threshold.</p>
<p>This strategy will result in smaller trees, but is too short-sighted since a seemingly worthless split early on in the tree might be followed by a very good split—that is, a split that leads to a large reduction in RSS later on.</p>
<p>We can use cross-validation to pick the optimal tree length.</p>
<pre><code class="language-python"># Import original split
features = ['Years', 'Hits', 'RBI', 'PutOuts', 'Walks', 'Runs', 'AtBat', 'HmRun']
X_train = pd.read_csv('data/Hitters_X_train.csv').dropna()[features]
X_test = pd.read_csv('data/Hitters_X_test.csv').dropna()[features]
y_train = pd.read_csv('data/Hitters_y_train.csv').dropna()
y_test = pd.read_csv('data/Hitters_y_test.csv').dropna()
</code></pre>
<pre><code class="language-python"># Init
params = range(2,11)
reg_scores = np.zeros((len(params),3))
best_score = 10**6

# Loop over all parameters
for i,k in enumerate(params):
    
    # Model
    tree = DecisionTreeRegressor(max_leaf_nodes=k)

    # Loop over splits
    tree.fit(X_train, y_train)
    reg_scores[i,0] = mean_squared_error(tree.predict(X_train), y_train)
    reg_scores[i,1] = mean_squared_error(tree.predict(X_test), y_test)

    # Get CV score
    kf6 = KFold(n_splits=6)
    reg_scores[i,2] = -cross_val_score(tree, X_train, y_train, cv=kf6, scoring='neg_mean_squared_error').mean()
    
    # Save best model
    if reg_scores[i,2]&lt;best_score:
        best_model = tree
        best_score = reg_scores[i,2]
</code></pre>
<p>Let&rsquo;s plot the optimal tree depth, using 6-fold cv.</p>
<pre><code class="language-python"># Figure 8.5
def make_figure_8_5():
    
    # Init
    fig, (ax1,ax2) = plt.subplots(1,2,figsize=(16,6))
    fig.suptitle('Figure 8.5')

    # Plot scores
    ax1.plot(params, reg_scores);
    ax1.axvline(params[np.argmin(reg_scores[:,2])], c='k', ls='--')
    ax1.legend(['Train','Test','6-fold CV']);
    ax1.set_title('Cross-Validation Scores');

    # Plot best tree
    plot_tree(best_model, filled=True, impurity=False, feature_names=features, fontsize=12, ax=ax2);
    ax2.set_title('Best Model');
</code></pre>
<pre><code class="language-python">make_figure_8_5()
</code></pre>
<p><img src="../img/07_trees_55_0.png" alt="png"></p>
<p>The optimal tree has 4 leaves.</p>
<h3 id="classification-trees">Classification Trees</h3>
<p>A classification tree is very similar to a regression tree, except that it is used to predict a qualitative response rather than a quantitative one.</p>
<p>For a classification tree, we predict that each observation belongs to the most commonly occurring class of training observations in the region to which it belongs.</p>
<h3 id="building-a-classification-tree">Building a Classification Tree</h3>
<p>The task of growing a classification tree is similar to the task of growing a regression tree. However, in the classification setting, RSS cannot be used as a criterion for making the binary splits.</p>
<p>We define $\hat p_{mk}$ as the proportion of training observations in the $m^{th}$ region that are from the $k^{th}$ class. Possible loss functions to decide the splits are:</p>
<ul>
<li>Classification error rate</li>
</ul>
<p>$$
E = 1 - \max <em>{k}\left(\hat{p}</em>{m k}\right)
$$</p>
<ul>
<li>Gini index</li>
</ul>
<p>$$
G=\sum_{k=1}^{K} \hat{p}<em>{m k}\left(1-\hat{p}</em>{m k}\right)
$$</p>
<ul>
<li>Entropy</li>
</ul>
<p>$$
D=-\sum_{k=1}^{K} \hat{p}<em>{m k} \log \hat{p}</em>{m k}
$$</p>
<p>In 2-class classification problems, this is what the different scores look like, for different proportions of class 2 ($p$), when the true proportion is $p_0 =0.5$.</p>
<img src="figures/impurity.png" alt="Drawing" style="width: 600px;"/>
<p>When building a classification tree, either the Gini index or the entropy are typically used to evaluate the quality of a particular split, since these two approaches are more sensitive to node purity than is the classification error rate.</p>
<p>For this section we will work with the <code>Heart</code> dataset on individual heart failures. We will try to use individual characteristics in order to predict heart deseases (<code>HD</code>). The varaible is binary: Yes, No.</p>
<pre><code class="language-python"># Load heart dataset
heart = pd.read_csv('data/Heart.csv').drop('Unnamed: 0', axis=1).dropna()
heart.head()
</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
</code></pre>
<p></style></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Age</th>
      <th>Sex</th>
      <th>ChestPain</th>
      <th>RestBP</th>
      <th>Chol</th>
      <th>Fbs</th>
      <th>RestECG</th>
      <th>MaxHR</th>
      <th>ExAng</th>
      <th>Oldpeak</th>
      <th>Slope</th>
      <th>Ca</th>
      <th>Thal</th>
      <th>AHD</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>63</td>
      <td>1</td>
      <td>typical</td>
      <td>145</td>
      <td>233</td>
      <td>1</td>
      <td>2</td>
      <td>150</td>
      <td>0</td>
      <td>2.3</td>
      <td>3</td>
      <td>0.0</td>
      <td>fixed</td>
      <td>No</td>
    </tr>
    <tr>
      <th>1</th>
      <td>67</td>
      <td>1</td>
      <td>asymptomatic</td>
      <td>160</td>
      <td>286</td>
      <td>0</td>
      <td>2</td>
      <td>108</td>
      <td>1</td>
      <td>1.5</td>
      <td>2</td>
      <td>3.0</td>
      <td>normal</td>
      <td>Yes</td>
    </tr>
    <tr>
      <th>2</th>
      <td>67</td>
      <td>1</td>
      <td>asymptomatic</td>
      <td>120</td>
      <td>229</td>
      <td>0</td>
      <td>2</td>
      <td>129</td>
      <td>1</td>
      <td>2.6</td>
      <td>2</td>
      <td>2.0</td>
      <td>reversable</td>
      <td>Yes</td>
    </tr>
    <tr>
      <th>3</th>
      <td>37</td>
      <td>1</td>
      <td>nonanginal</td>
      <td>130</td>
      <td>250</td>
      <td>0</td>
      <td>0</td>
      <td>187</td>
      <td>0</td>
      <td>3.5</td>
      <td>3</td>
      <td>0.0</td>
      <td>normal</td>
      <td>No</td>
    </tr>
    <tr>
      <th>4</th>
      <td>41</td>
      <td>0</td>
      <td>nontypical</td>
      <td>130</td>
      <td>204</td>
      <td>0</td>
      <td>2</td>
      <td>172</td>
      <td>0</td>
      <td>1.4</td>
      <td>1</td>
      <td>0.0</td>
      <td>normal</td>
      <td>No</td>
    </tr>
  </tbody>
</table>
</div>
<pre><code class="language-python"># Fastorize variables
heart.ChestPain = pd.factorize(heart.ChestPain)[0]
heart.Thal = pd.factorize(heart.Thal)[0]
</code></pre>
<pre><code class="language-python"># Set features
features = [col for col in heart.columns if col!='AHD']
X2 = heart[features]
y2 = pd.factorize(heart.AHD)[0]
</code></pre>
<p>We now fit our classifier.</p>
<pre><code class="language-python"># Fit classification tree
clf = DecisionTreeClassifier(max_depth=None, max_leaf_nodes=11)
clf.fit(X2,y2)
</code></pre>
<pre><code>DecisionTreeClassifier(max_leaf_nodes=11)
</code></pre>
<p>What is the score?</p>
<pre><code class="language-python"># Final score
clf.score(X2,y2)
</code></pre>
<pre><code>0.8686868686868687
</code></pre>
<p>Let&rsquo;s have a look at the whole tree.</p>
<pre><code class="language-python"># Figure 8.6 a
def make_fig_8_6a():
    
    # Init
    fig, ax = plt.subplots(1,1, figsize=(16,12))
    ax.set_title('Figure 8.6');

    # Plot tree
    plot_tree(clf, filled=True, feature_names=features, class_names=['No','Yes'], fontsize=12, ax=ax);
</code></pre>
<pre><code class="language-python">make_fig_8_6a()
</code></pre>
<p><img src="../img/07_trees_72_0.png" alt="png"></p>
<p>This figure has a surprising characteristic: some of the splits yield two terminal nodes that have the same predicted value.</p>
<p>For instance, consider the split <code>Age</code>&lt;=57.5 near the bottom left of the unpruned tree. Regardless of the value of <code>Age</code>, a response value of <em>No</em> is predicted for those observations. Why, then, is the split performed at all?</p>
<p>The split is performed because it leads to increased node purity. That is, 2/81 of the observations corresponding to the left-hand leaf have a response value of <em>Yes</em>, whereas 9/36 of those corresponding to the right-hand leaf have a response value of <em>Yes</em>. Why is node purity important? Suppose that we have a test observation that belongs to the region given by that left-hand leaf. Then we can be pretty certain that its response value is <em>No</em>. In contrast, if a test observation belongs to the region given by the right-hand leaf, then its response value is probably <em>No</em>, but we are much less certain. Even though the split <code>Age</code>&lt;=57.5 does not reduce the classification error, it improves the Gini index and the entropy, which are more sensitive to node purity.</p>
<h3 id="pruning-for-classification">Pruning for Classification</h3>
<p>We can repeat the pruning exercise also for the classification task.</p>
<pre><code class="language-python"># Figure 8.6 b
def make_figure_8_6b():
    
    # Init
    fig, (ax1, ax2) = plt.subplots(1,2,figsize=(14,6))
    fig.suptitle('Figure 8.6')

    # Plot scores
    ax1.plot(params, clf_scores);
    ax1.legend(['Train','Test','6-fold CV']);

    # Plot best tree
    plot_tree(best_model, filled=True, impurity=False, feature_names=features, fontsize=12, ax=ax2);
</code></pre>
<pre><code class="language-python"># Init
J = 10
params = range(2,11)
clf_scores = np.zeros((len(params),3))
best_score = 100

# Loop over all parameters
for i,k in enumerate(params):
    
    # Model
    tree = DecisionTreeClassifier(max_leaf_nodes=k)
    
    # Loop J times
    temp_scores = np.zeros((J,3))
    for j in range (J):

        # Loop over splits
        X2_train, X2_test, y2_train, y2_test = train_test_split(X2, y2, test_size=0.5, random_state=j)
        m = tree.fit(X2_train, y2_train)
        temp_scores[j,0] = mean_squared_error(m.predict(X2_train), y2_train)
        temp_scores[j,1] = mean_squared_error(m.predict(X2_test), y2_test)

        # Get CV score
        kf6 = KFold(n_splits=6)
        temp_scores[j,2] = -cross_val_score(tree, X2_train, y2_train, cv=kf6, scoring='neg_mean_squared_error').mean()
        
        # Save best model
        if temp_scores[j,2]&lt;best_score:
            best_model = m
            best_score = temp_scores[j,2]
        
    # Average
    clf_scores[i,:] = np.mean(temp_scores, axis=0)
</code></pre>
<pre><code class="language-python">make_figure_8_6b()
</code></pre>
<p><img src="../img/07_trees_78_0.png" alt="png"></p>
<h3 id="other-issues">Other Issues</h3>
<h4 id="missing-predictor-values">Missing Predictor Values</h4>
<p>There are usually 2 main ways to deal with missing values:</p>
<ul>
<li>discard the observations</li>
<li>fill the missing values with predictions using the other observations (e.g. mean)</li>
</ul>
<p>With trees we can do better:</p>
<ul>
<li>code them as a separate class (e.g. &lsquo;missing&rsquo;)</li>
<li>generate splits using non-missing data and use non-missing variables on missing data to mimic the splits with missing data</li>
</ul>
<h4 id="categorical-predictors">Categorical Predictors</h4>
<p>When splitting a predictor having q possible unordered values, there are $2^{q−1} − 1$ possible partitions of the q values into two groups, and the computations become prohibitive for large $q$. However, with a $0 − 1$ outcome, this computation simplifies.</p>
<h4 id="linear-combination-splits">Linear Combination Splits</h4>
<p>Rather than restricting splits to be of the form $X_j \leq s$, one can allow splits along linear combinations of the form $a_j X_j \leq s$. The weights $a_j$ become part of the optimization procedure.</p>
<h4 id="other-tree-building-procedures">Other Tree-Building Procedures</h4>
<p>The procedure we have seen for building trees is called CART (Classification and Regression Tree). There are other procedures.</p>
<h4 id="the-loss-matrix">The Loss Matrix</h4>
<p>With respect to other methods, the choice of the loss functions plays a much more important role.</p>
<h4 id="binary-splits">Binary Splits</h4>
<p>You can do non-binary splits but in the end they are just weaker versions of binary splits.</p>
<h4 id="instability">Instability</h4>
<p>Trees have very <strong>high variance</strong>.</p>
<h4 id="difficulty-in-capturing-additive-structure">Difficulty in Capturing Additive Structure</h4>
<p>Trees are quite bad at modeling additive structures.</p>
<h4 id="lack-of-smoothness">Lack of Smoothness</h4>
<p>Trees are not smooth.</p>
<h3 id="trees-vs-regression">Trees vs Regression</h3>
<p><strong>Advantages</strong></p>
<ul>
<li>Trees are very easy to explain to people. In fact, they are even easier to explain than linear regression!</li>
<li>Some people believe that decision trees more closely mirror human decision-making than do the regression and classification approaches seen in previous chapters.</li>
<li>Trees can be displayed graphically, and are easily interpreted even by a non-expert (especially if they are small).</li>
<li>Trees can easily handle qualitative predictors without the need to create dummy variables.</li>
</ul>
<p><strong>Disadvantages</strong></p>
<ul>
<li>trees generally do not have the same level of predictive accuracy as some of the other regression and classification approaches seen in this book.</li>
<li>trees can be very non-robust. In other words, a small change in the data can cause a large change in the final estimated tree.</li>
</ul>
<h2 id="72-bagging-random-forests-boosting">7.2 Bagging, Random Forests, Boosting</h2>
<p>Bagging, random forests, and boosting use trees as building blocks to construct more powerful prediction models.</p>
<h3 id="bagging">Bagging</h3>
<p>The main problem of decision trees is that they suffer from <strong>high variance</strong>. <em>Bootstrap aggregation</em>, or <em>bagging</em>, is a general-purpose procedure for reducing the variance of a statistical learning method.</p>
<p>The main idea behind <em>bagging</em> is that, given a set of n independent observations $Z_1,&hellip;,Z_n$, each with variance $\sigma^2$, the variance of the mean $\bar Z$ of the observations is given by $\sigma^2/n$. In other words, averaging a set of observations reduces variance.</p>
<p>Indeed <em>bagging</em> consists in taking many training sets from the population, build a separate prediction model using each training set, and <strong>average the resulting predictions</strong>. Since we do not have access to many training sets, we resort to bootstrapping.</p>
<h3 id="out-of-bag-error-estimation">Out-of-Bag Error Estimation</h3>
<p>It turns out that there is a very straightforward way to estimate the test error of a bagged model, without the need to perform cross-validation or the validation set approach. Recall that the key to bagging is that trees are repeatedly fit to bootstrapped subsets of the observations. One can show that on average, each bagged tree makes use of around two-thirds of the observations. The remaining one-third of the observations not used to fit a given bagged tree are referred to as the out-of-bag (OOB) observations. We can predict the response for the ith observation using each of the trees in which that observation was OOB.</p>
<p>We are now going to compute the Gini index for the <code>Heart</code> dataset using different numbers of trees.</p>
<pre><code class="language-python"># Init (takes a lot of time with J=30)
params = range(2,50)
bagging_scores = np.zeros((len(params),2))
J = 30;

# Loop over parameters
for i, k in enumerate(params):
    print(&quot;Computing k=%1.0f&quot; % k, end =&quot;&quot;)
    
    # Repeat J 
    temp_scores = np.zeros((J,2))
    for j in range(J):
        X2_train, X2_test, y2_train, y2_test = train_test_split(X2, y2, test_size=0.5, random_state=j)
        bagging = BaggingClassifier(DecisionTreeClassifier(), max_samples=k, oob_score=True)
        bagging.fit(X2_train,y2_train)
        temp_scores[j,0] = bagging.score(X2_test, y2_test)
        temp_scores[j,1] = bagging.oob_score_
        
    # Average
    bagging_scores[i,:] = np.mean(temp_scores, axis=0)
    print(&quot;&quot;, end=&quot;\r&quot;)
</code></pre>
<pre><code>Computing k=49
</code></pre>
<p>Let&rsquo;s plot the Out-of-Bag error computed while generating the bagged estimator.</p>
<pre><code class="language-python"># Make new figure 1
def make_new_figure_1():
    
    # Init
    fig, ax = plt.subplots(1,1,figsize=(10,6))
    fig.suptitle(&quot;Estimated $R^2$&quot;)

    # Plot scores
    ax.plot(params, bagging_scores);
    ax.legend(['Test','OOB']);
    ax.set_xlabel('Number of Trees'); ax.set_ylabel('R^2');
</code></pre>
<pre><code class="language-python">make_new_figure_1()
</code></pre>
<p><img src="../img/07_trees_94_0.png" alt="png"></p>
<p>It can be shown that with B sufficiently large, OOB error is virtually equivalent to leave-one-out cross-validation error. The OOB approach for estimating the test error is particularly convenient when performing bagging on large data sets for which cross-validation would be computationally onerous.</p>
<h3 id="variable-importance-measures">Variable Importance Measures</h3>
<p>As we have discussed, the main advantage of bagging is to reduce prediction variance. However, with bagging it can be <strong>difficult to interpret</strong> the resulting model. In fact we cannot draw trees anymore given we have too many of them.</p>
<p>However, one can obtain an overall summary of the importance of each predictor using the RSS (for bagging regression trees) or the Gini index (for bagging classification trees). In the case of bagging regression trees, we can record the total amount that the RSS is decreased due to splits over a given predictor, averaged over all trees. A large value indicates an important predictor. Similarly, in the context of bagging classification trees, we can add up the total amount that the Gini index is decreased by splits over a given predictor, averaged over all trees.</p>
<pre><code class="language-python"># Compute feature importance
feature_importances = np.mean([tree.feature_importances_ for tree in bagging.estimators_], axis=0)
</code></pre>
<p>We can have a look at the importance of each feature.</p>
<pre><code class="language-python"># Figure 8.9
def make_figure_8_9():
    
    # Init
    fig, ax = plt.subplots(1,1,figsize=(8,8))
    ax.set_title('Figure 8.9: Feature Importance');

    # Plot feature importance
    h1 = pd.DataFrame({'Importance':feature_importances*100}, index=features)
    h1 = h1.sort_values(by='Importance', axis=0, ascending=False)
    h1.plot(kind='barh', color='r', ax=ax)
    ax.set_xlabel('Variable Importance'); 
    plt.yticks(fontsize=14);
    plt.gca().legend_ = None;
</code></pre>
<pre><code class="language-python">make_figure_8_9()
</code></pre>
<p><img src="../img/07_trees_101_0.png" alt="png"></p>
<h3 id="random-forests">Random Forests</h3>
<p>Random forests provide an improvement over bagged trees by way of a <strong>small tweak that decorrelates the trees</strong>. As in bagging, we build a number of decision trees on bootstrapped training samples. But when building these decision trees, each time a split in a tree is considered, a random sample of $m$ predictors is chosen as split candidates from the full set of $p$ predictors. The split is allowed to use only one of those m predictors. A fresh sample of $m$ predictors is taken at each split, and typically we choose $m \sim \sqrt{p}$ — that is, the number of predictors considered at each split is approximately equal to the square root of the total number of predictors</p>
<p>In other words, in building a random forest, at each split in the tree, the algorithm is not even allowed to consider a majority of the available predictors. This may sound crazy, but it has a clever rationale. Suppose that there is one very strong predictor in the data set, along with a number of other moderately strong predictors. Then in the collection of bagged trees, most or all of the trees will use this strong predictor in the top split. Consequently, all of the bagged trees will look quite similar to each other. Hence the predictions from the bagged trees will be highly correlated. Unfortunately, averaging many highly correlated quantities does not lead to as large of a reduction in variance as averaging many uncorrelated quantities. In particular, this means that bagging will not lead to a substantial reduction in variance over a single tree in this setting.</p>
<p>Random forests overcome this problem by <strong>forcing each split to consider only a subset of the predictors</strong>.</p>
<p>Let&rsquo;s split the data in 2 and compute test and estimated $R^2$, for both forest and trees.</p>
<pre><code class="language-python">import warnings
warnings.simplefilter('ignore')

# Init (takes a lot of time with J=30)
params = range(2,50)
forest_scores = np.zeros((len(params),2))
J = 30

# Loop over parameters
for i, k in enumerate(params):
    print(&quot;Computing k=%1.0f&quot; % k, end =&quot;&quot;)
    
    # Repeat J 
    temp_scores = np.zeros((J,2))
    for j in range(J):
        X2_train, X2_test, y2_train, y2_test = train_test_split(X2, y2, test_size=0.5, random_state=j)
        forest = RandomForestClassifier(n_estimators=k, oob_score=True, max_features=&quot;sqrt&quot;)
        forest.fit(X2_train,y2_train)
        temp_scores[j,0] = forest.score(X2_test, y2_test)
        temp_scores[j,1] = forest.oob_score_
        
    # Average
    forest_scores[i,:] = np.mean(temp_scores, axis=0)
    print(&quot;&quot;, end=&quot;\r&quot;)
</code></pre>
<pre><code>Computing k=49
</code></pre>
<pre><code class="language-python"># Figure 8.8
def make_figure_8_8():
    
    # Init
    fig, ax = plt.subplots(1,1,figsize=(10,6))
    ax.set_title('Figure 8.8');

    # Plot scores
    ax.plot(params, bagging_scores);
    ax.plot(params, forest_scores);
    ax.legend(['Test - Bagging','OOB - Bagging', 'Test - Forest','OOB - Forest']);
    ax.set_xlabel('Number of Trees'); ax.set_ylabel('R^2');
</code></pre>
<pre><code class="language-python">make_figure_8_8()
</code></pre>
<p><img src="../img/07_trees_108_0.png" alt="png"></p>
<p>As for bagging, we can plot feature importance.</p>
<pre><code class="language-python"># Make new figure 2
def make_new_figure_2():
    
    # Init
    fig, (ax1,ax2) = plt.subplots(1,2,figsize=(15,7))

    # Plot feature importance - Bagging
    h1 = pd.DataFrame({'Importance':feature_importances*100}, index=features)
    h1 = h1.sort_values(by='Importance', axis=0, ascending=False)
    h1.plot(kind='barh', color='r', ax=ax1)
    ax1.set_xlabel('Variable Importance'); 
    ax1.set_title('Tree Bagging')

    # Plot feature importance
    h2 = pd.DataFrame({'Importance':forest.feature_importances_*100}, index=features)
    h2 = h2.sort_values(by='Importance', axis=0, ascending=False)
    h2.plot(kind='barh', color='r', ax=ax2)
    ax2.set_title('Random Forest')

    # All plots
    for ax in fig.axes:
        ax.set_xlabel('Variable Importance'); 
        ax.legend([])
</code></pre>
<pre><code class="language-python">make_new_figure_2()
</code></pre>
<p><img src="../img/07_trees_111_0.png" alt="png"></p>
<p>From the figure we observe that varaible importance ranking is similar with bagging and random forests, but there are significant differences.</p>
<p>We are now going to look at the importance of random forests using the <code>Khan</code> gene dataset. This dataset has the peculiarity of having a large number of features and very few observations.</p>
<pre><code class="language-python"># Load data
gene = pd.read_csv('data/Khan.csv')
print(len(gene))
gene.head()
</code></pre>
<pre><code>83
</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
</code></pre>
<p></style></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>x</th>
      <th>V1</th>
      <th>V2</th>
      <th>V3</th>
      <th>V4</th>
      <th>V5</th>
      <th>V6</th>
      <th>V7</th>
      <th>V8</th>
      <th>V9</th>
      <th>...</th>
      <th>V2299</th>
      <th>V2300</th>
      <th>V2301</th>
      <th>V2302</th>
      <th>V2303</th>
      <th>V2304</th>
      <th>V2305</th>
      <th>V2306</th>
      <th>V2307</th>
      <th>V2308</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>2</td>
      <td>0.773344</td>
      <td>-2.438405</td>
      <td>-0.482562</td>
      <td>-2.721135</td>
      <td>-1.217058</td>
      <td>0.827809</td>
      <td>1.342604</td>
      <td>0.057042</td>
      <td>0.133569</td>
      <td>...</td>
      <td>-0.238511</td>
      <td>-0.027474</td>
      <td>-1.660205</td>
      <td>0.588231</td>
      <td>-0.463624</td>
      <td>-3.952845</td>
      <td>-5.496768</td>
      <td>-1.414282</td>
      <td>-0.647600</td>
      <td>-1.763172</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2</td>
      <td>-0.078178</td>
      <td>-2.415754</td>
      <td>0.412772</td>
      <td>-2.825146</td>
      <td>-0.626236</td>
      <td>0.054488</td>
      <td>1.429498</td>
      <td>-0.120249</td>
      <td>0.456792</td>
      <td>...</td>
      <td>-0.657394</td>
      <td>-0.246284</td>
      <td>-0.836325</td>
      <td>-0.571284</td>
      <td>0.034788</td>
      <td>-2.478130</td>
      <td>-3.661264</td>
      <td>-1.093923</td>
      <td>-1.209320</td>
      <td>-0.824395</td>
    </tr>
    <tr>
      <th>2</th>
      <td>2</td>
      <td>-0.084469</td>
      <td>-1.649739</td>
      <td>-0.241308</td>
      <td>-2.875286</td>
      <td>-0.889405</td>
      <td>-0.027474</td>
      <td>1.159300</td>
      <td>0.015676</td>
      <td>0.191942</td>
      <td>...</td>
      <td>-0.696352</td>
      <td>0.024985</td>
      <td>-1.059872</td>
      <td>-0.403767</td>
      <td>-0.678653</td>
      <td>-2.939352</td>
      <td>-2.736450</td>
      <td>-1.965399</td>
      <td>-0.805868</td>
      <td>-1.139434</td>
    </tr>
    <tr>
      <th>3</th>
      <td>2</td>
      <td>0.965614</td>
      <td>-2.380547</td>
      <td>0.625297</td>
      <td>-1.741256</td>
      <td>-0.845366</td>
      <td>0.949687</td>
      <td>1.093801</td>
      <td>0.819736</td>
      <td>-0.284620</td>
      <td>...</td>
      <td>0.259746</td>
      <td>0.357115</td>
      <td>-1.893128</td>
      <td>0.255107</td>
      <td>0.163309</td>
      <td>-1.021929</td>
      <td>-2.077843</td>
      <td>-1.127629</td>
      <td>0.331531</td>
      <td>-2.179483</td>
    </tr>
    <tr>
      <th>4</th>
      <td>2</td>
      <td>0.075664</td>
      <td>-1.728785</td>
      <td>0.852626</td>
      <td>0.272695</td>
      <td>-1.841370</td>
      <td>0.327936</td>
      <td>1.251219</td>
      <td>0.771450</td>
      <td>0.030917</td>
      <td>...</td>
      <td>-0.200404</td>
      <td>0.061753</td>
      <td>-2.273998</td>
      <td>-0.039365</td>
      <td>0.368801</td>
      <td>-2.566551</td>
      <td>-1.675044</td>
      <td>-1.082050</td>
      <td>-0.965218</td>
      <td>-1.836966</td>
    </tr>
  </tbody>
</table>
<p>5 rows × 2309 columns</p>
</div>
<p>The dataset has 83 rows and 2309 columns.</p>
<p>Since it&rsquo;s a very <em>wide</em> dataset, selecting the right features is crucial.</p>
<p>Also note that we cannot run linear regression on this dataset.</p>
<pre><code class="language-python"># Reduce dataset size
gene_small = gene.iloc[:,0:202]
X = gene_small.iloc[:,1:]
y = gene_small.iloc[:,0]
</code></pre>
<p>Let&rsquo;s now cross-validate over number of trees and maximum number of features considered.</p>
<pre><code class="language-python"># Init (takes a lot of time with J=30)
params = range(50,150,10)
m_scores = np.zeros((len(params),3))
p = np.shape(X)[1]
J = 30;

# Loop over parameters
for i, k in enumerate(params):
    
    # Array of features
    ms = [round(p/2), round(np.sqrt(p)), round(np.log(p))]
    
    # Repeat L times
    temp_scores = np.zeros((J,3))
    for j in range(J):
        print(&quot;Computing k=%1.0f (iter=%1.0f)&quot; % (k,j+1), end =&quot;&quot;)
    
        # Loop over values of m
        for index, m in enumerate(ms):
            forest = RandomForestClassifier(n_estimators=k, max_features=m, oob_score=True)
            forest.fit(X, y)
            temp_scores[j,index] = forest.oob_score_
        print(&quot;&quot;, end=&quot;\r&quot;)
            
    # Average
    m_scores[i,:] = np.mean(temp_scores, axis=0)
</code></pre>
<pre><code>Computing k=140 (iter=30)
</code></pre>
<pre><code class="language-python"># Figure 8.10
def make_figure_8_10():
    
    # Init
    fig, ax = plt.subplots(1,1,figsize=(10,6))
    ax.set_title('Figure 8.10');

    # Plot scores
    ax.plot(params, m_scores);
    ax.legend(['m=p/2','m=sqrt(p)','m=log(p)']);
    ax.set_xlabel('Number of Trees'); ax.set_ylabel('Test Classification Accuracy');
</code></pre>
<pre><code class="language-python"> make_figure_8_10()
</code></pre>
<p><img src="../img/07_trees_120_0.png" alt="png"></p>
<p>It seems that the best scores are achieved with few features and many trees.</p>
<h3 id="boosting">Boosting</h3>
<p>Like bagging, boosting is a general approach that can be applied to many statistical learning methods for regression or classification. Here we restrict our discussion of boosting to the context of decision trees.</p>
<p>Boosting works similarly to bagging, except that the trees are grown sequentially: each tree is grown using information from previously grown trees. Boosting does not involve bootstrap sampling; instead each tree is fit on a modified version of the original data set.</p>
<p>What is the idea behind this procedure? Given the current model, we fit a decision tree to the residuals from the model. That is, <strong>we fit a tree using the current residuals</strong>, rather than the outcome $y$, as the response. We then add this new decision tree into the fitted function in order to update the residuals. Each of these trees can be rather small, with just a few terminal nodes, determined by the parameter $d$ in the algorithm. By fitting small trees to the residuals, <strong>we slowly improve $\hat f$ in areas where it does not perform well</strong>. The shrinkage parameter λ slows the process down even further, allowing more and different shaped trees to attack the resid- uals. In general, statistical learning approaches that learn slowly tend to perform well.</p>
<h3 id="algorithm">Algorithm</h3>
<p>The boosting algorithm works as follows:</p>
<ol>
<li>
<p>Set $\hat f(x)=0$ and $r_i=y_i$ for all $i$ in the training set.</p>
</li>
<li>
<p>For $b=1,2,&hellip;,B$ repeat:</p>
<p>a. Fit a tree $\hat f^b $ with $d$ splits ($d+1$ terminal nodes) to the training data $(X,r)$.</p>
<p>b. Update $\hat f$ by adding in a shrunken version of the new tree:
$$
\hat f(x) \leftarrow \hat f(x) + \lambda \hat f^b(x)
$$</p>
<p>c. Update the residuals
$$
r_i = r_i - \lambda \hat f^b(x_i)
$$</p>
</li>
<li>
<p>Output the boosted model
$$
\hat{f}(x)=\sum_{b=1}^{B} \lambda \hat{f}^{b}(x)
$$</p>
</li>
</ol>
<p>Boosting has three tuning parameters:</p>
<ol>
<li>The <strong>number of trees</strong> $B$</li>
<li>The <strong>shrinkage parameter</strong> $\lambda$. This controls the rate at which boosting learns.</li>
<li>The <strong>number of splits in each tree</strong> $d$ , which controls the complexity of the boosted ensemble. Often d = 1 works well, in which case each tree is a stump, consisting of a single split.</li>
</ol>
<pre><code class="language-python"># Init , oob_score=True
params = range(50,150,10)
boost_scores = np.zeros((len(params),3))
p = np.shape(X)[1]
J = 30

# Loop over parameters
for i, k in enumerate(params):
    
    # Repeat L times
    temp_scores = np.zeros((J,3))
    for j in range(J):
        print(&quot;Computing k=%1.0f (iter=%1.0f)&quot; % (k,j+1), end =&quot;&quot;)
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=20, random_state=j)
    
        # First score: random forest
        forest = RandomForestClassifier(n_estimators=k, max_features=&quot;sqrt&quot;)
        forest.fit(X_train, y_train)
        temp_scores[j,0] = forest.score(X_test, y_test)

        # Second score: boosting with 1-split trees
        boost1 = GradientBoostingClassifier(learning_rate=0.01, max_depth=1, n_estimators=k, max_features=&quot;sqrt&quot;)
        boost1.fit(X_train, y_train)
        temp_scores[j,1] = boost1.score(X_test, y_test)

        # Third score: boosting with 1-split trees
        boost2 = GradientBoostingClassifier(learning_rate=0.01, max_depth=2, n_estimators=k, max_features=&quot;sqrt&quot;)
        boost2.fit(X_train, y_train)
        temp_scores[j,2] = boost2.score(X_test, y_test)
        print(&quot;&quot;, end=&quot;\r&quot;)
    
    # Average
    boost_scores[i,:] = np.mean(temp_scores, axis=0)
</code></pre>
<pre><code>Computing k=140 (iter=30)
</code></pre>
<p>Let&rsquo;s compare boosting and forest.</p>
<pre><code class="language-python"># Figure 8.11
def make_figure_8_11():
    
    # Init
    fig, ax = plt.subplots(1,1,figsize=(10,6))
    ax.set_title('Figure 8.11');

    # Plot scores
    ax.plot(params, m_scores);
    ax.legend(['forest','boosting with d=1','boosting with d=2']);
    ax.set_xlabel('Number of Trees'); ax.set_ylabel('Test Classification Accuracy');
</code></pre>
<pre><code class="language-python">make_figure_8_11()
</code></pre>
<p><img src="../img/07_trees_131_0.png" alt="png"></p>

          </div>

          



          
          
          <div class="article-widget">
            
<div class="post-nav">
  
  
  
  <div class="post-nav-item">
    <div class="meta-nav">Previous</div>
    <a href="/course/ml-econ/06_convexity/" rel="next">Convexity and Optimization</a>
  </div>
  
  
  
  <div class="post-nav-item">
    <div class="meta-nav">Next</div>
    <a href="/course/ml-econ/08_neuralnets/" rel="prev">Neural Networks</a>
  </div>
  
</div>

          </div>
          
        </div>

        <div class="body-footer">

          





          




          


  
  



        </div>

      </article>

      <footer class="site-footer">

  

  
  <p class="powered-by">
    
      <a href="/privacy/">Privacy Policy</a>
    
    
       &middot; 
      <a href="/terms/">Terms</a>
    
  </p>
  

  

  
  







</footer>


    </main>
  </div>
</div>

  </div>

  <div class="page-footer">
    
    
  </div>

  
<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        <pre><code class="tex hljs"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

      

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/instant.page/5.1.0/instantpage.min.js" integrity="sha512-1+qUtKoh9XZW7j+6LhRMAyOrgSQKenQ4mluTR+cvxXjP1Z54RxZuzstR/H9kgPXQsVB8IW7DMDFUJpzLjvhGSQ==" crossorigin="anonymous"></script>

      
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js" integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin="anonymous"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js" integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin="anonymous"></script>
      

      
      

      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mermaid/8.8.4/mermaid.min.js" integrity="sha512-as1BF4+iHZ3BVO6LLDQ7zrbvTXM+c/1iZ1qII/c3c4L8Rn5tHLpFUtpaEtBNS92f+xGsCzsD7b62XP3XYap6oA==" crossorigin="anonymous" title="mermaid"></script>
      

      
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/highlight.min.js" integrity="sha512-TDKKr+IvoqZnPzc3l35hdjpHD0m+b2EC2SrLEgKDRWpxf2rFCxemkgvJ5kfU48ip+Y+m2XVKyOCD85ybtlZDmw==" crossorigin="anonymous"></script>
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/languages/r.min.js"></script>
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/languages/python.min.js"></script>
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/languages/julia.min.js"></script>
        
      

    

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.7.1/leaflet.min.js" integrity="sha512-SeiQaaDh73yrb56sTW/RgVdi/mMqNeM2oBwubFHagc5BkixSpP1fvqF47mKzPGWYSSy4RwbBunrJBQ4Co8fRWA==" crossorigin="anonymous"></script>
    

    
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/anchor-js/4.2.2/anchor.min.js" integrity="sha512-I7w3ZdSFzw5j3jU3ZkNikBNeIrl3i+hEuEdwNmqUJvwNcaBUNcijnP2gd9DtGlgVYDplfjGoD8vTNsID+lCjqg==" crossorigin="anonymous"></script>
    <script>
      anchors.add();
    </script>
    

    
    
    
      
      <script id="search-hit-fuse-template" type="text/x-template">
        <div class="search-hit" id="summary-{{key}}">
          <div class="search-hit-content">
            <div class="search-hit-name">
              <a href="{{relpermalink}}">{{title}}</a>
              <div class="article-metadata search-hit-type">{{type}}</div>
              <p class="search-hit-description">{{snippet}}</p>
            </div>
          </div>
        </div>
      </script>
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js" integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin="anonymous"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js" integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin="anonymous"></script>
      
    

    
    

    
    
    
    

    
    <script src="/js/bootstrap.bundle.min.6aed84840afc03ab4d5750157f69c120.js"></script>

    
    
      
      
      
      
      
      
      
    

    
    
    
    
    
    
    
    
      
      
    
    
    <script src="/en/js/wowchemy.min.cf8ca859a9b74f8b1cd804621b13e5f1.js"></script>

    
  <script async defer src="https://buttons.github.io/buttons.js"></script>




</body>
</html>
