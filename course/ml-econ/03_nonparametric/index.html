<!DOCTYPE html><html lang="en-us" >

<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  
  
  
    <meta name="generator" content="Wowchemy 5.1.0 for Hugo" />
  

  

  
  
  
  
  
    
    
    
  
  

  

  
  
  
    
  
  <meta name="description" content="# Setup %matplotlib inline from utils.lecture03 import *  Dataset For this session, we are mostly going to work with the wage dataset.
df = pd.read_csv(&#39;data/Wage.csv&#39;, index_col=0) df.head(3)   ." />

  
  <link rel="alternate" hreflang="en-us" href="https://matteocourthoud.github.io/course/ml-econ/03_nonparametric/" />

  







  




  
  
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
  

  
  
  
    <meta name="theme-color" content="#003f5c" />
  

  
  
    
    <script src="/js/mathjax-config.js"></script>
  

  
  
  
  
    
    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha256-FMvZuGapsJLjouA6k7Eo2lusoAX9i0ShlWFG6qt7SLc=" crossorigin="anonymous">

    
    

    
    
    
      
    
    
      
      
        
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/styles/github.min.css" crossorigin="anonymous" title="hl-light" media="print" onload="this.media='all'">
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/styles/dracula.min.css" crossorigin="anonymous" title="hl-dark" media="print" onload="this.media='all'" disabled>
        
      
    

    
    
    
      <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.7.1/leaflet.min.css" integrity="sha512-1xoFisiGdy9nvho8EgXuXvnpR5GAMSjFwp40gSRE3NwdUdIMIKuPa7bqoUhLD0O/5tPNhteAsE5XyyMi5reQVA==" crossorigin="anonymous" media="print" onload="this.media='all'">
    

    

    
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js" integrity="" crossorigin="anonymous" async></script>
      
    
      
      

      
      

      
    
      
      

      
      

      
    
  

  
  
  
    
      
      <link rel="preload" as="style" href="https://fonts.googleapis.com/css2?family=Roboto+Mono&family=Source+Sans+Pro:wght@200;300;400;700&display=swap">
      <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Roboto+Mono&family=Source+Sans+Pro:wght@200;300;400;700&display=swap" media="print" onload="this.media='all'">
    
  

  
  
  
  
  
  <link rel="stylesheet" href="/css/wowchemy.5c4def4f00a521426f4eb098155f3342.css" />

  




<script async src="https://www.googletagmanager.com/gtag/js?id=UA-144780600-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];

  function gtag() {
      dataLayer.push(arguments);
  }

  function trackOutboundLink(url, target) {
    gtag('event', 'click', {
         'event_category': 'outbound',
         'event_label': url,
         'transport_type': 'beacon',
         'event_callback': function () {
           if (target !== '_blank') {
             document.location = url;
           }
         }
    });
    console.debug("Outbound link clicked: " + url);
  }

  function onClickCallback(event) {
    if ((event.target.tagName !== 'A') || (event.target.host === window.location.host)) {
      return;
    }
    trackOutboundLink(event.target, event.target.getAttribute('target'));  
  }

  gtag('js', new Date());
  gtag('config', 'UA-144780600-1', { 'anonymize_ip': true });
  gtag('set', {'cookie_flags': 'SameSite=None;Secure'});

  
  document.addEventListener('click', onClickCallback, false);
</script>


  

  

  




  
  
  

  

  
    <link rel="manifest" href="/index.webmanifest" />
  

  <link rel="icon" type="image/png" href="/media/icon_hu03e9b3967b83fd39296ec9da5ff1ea05_201175_32x32_fill_lanczos_center_3.png" />
  <link rel="apple-touch-icon" type="image/png" href="/media/icon_hu03e9b3967b83fd39296ec9da5ff1ea05_201175_180x180_fill_lanczos_center_3.png" />

  <link rel="canonical" href="https://matteocourthoud.github.io/course/ml-econ/03_nonparametric/" />

  
  
  
  
  
  
  
  
    
    
  
  
  <meta property="twitter:card" content="summary" />
  
  <meta property="og:site_name" content="Matteo Courthoud" />
  <meta property="og:url" content="https://matteocourthoud.github.io/course/ml-econ/03_nonparametric/" />
  <meta property="og:title" content="Non-Parametric Regression | Matteo Courthoud" />
  <meta property="og:description" content="# Setup %matplotlib inline from utils.lecture03 import *  Dataset For this session, we are mostly going to work with the wage dataset.
df = pd.read_csv(&#39;data/Wage.csv&#39;, index_col=0) df.head(3)   ." /><meta property="og:image" content="https://matteocourthoud.github.io/media/icon_hu03e9b3967b83fd39296ec9da5ff1ea05_201175_512x512_fill_lanczos_center_3.png" />
    <meta property="twitter:image" content="https://matteocourthoud.github.io/media/icon_hu03e9b3967b83fd39296ec9da5ff1ea05_201175_512x512_fill_lanczos_center_3.png" /><meta property="og:locale" content="en-us" />
  
    
      <meta
        property="article:published_time"
        content="2022-03-09T00:00:00&#43;00:00"
      />
    
    <meta property="article:modified_time" content="2022-03-09T00:00:00&#43;00:00">
  

  



  

  
  
  
  
  
    <script src="https://cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.1.1/cookieconsent.min.js" integrity="sha256-5VhCqFam2Cn+yjw61zbBNrbHVJ6SRydPeKopYlngbiQ=" crossorigin="anonymous"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.1.1/cookieconsent.min.css" integrity="sha256-zQ0LblD/Af8vOppw18+2anxsuaz3pWYyVWi+bTvTH8Q=" crossorigin="anonymous">
  
  <script>
  window.addEventListener("load", function(){
    window.cookieconsent.initialise({
      "palette": {
        "popup": {
          "background": "#003f5c",
          "text": "rgb(255, 255, 255)"
        },
        "button": {
          "background": "rgb(255, 255, 255)",
          "text": "#003f5c"
        }
      },
      "theme": "classic",
      "content": {
        "message": "This website uses cookies to ensure you get the best experience on our website.",
        "dismiss": "Got it!",
        "link": "Learn more",
        "href": "/privacy/"
      }
    })});
  </script>


  





  <title>Non-Parametric Regression | Matteo Courthoud</title>
</head>


<body id="top" data-spy="scroll" data-offset="70" data-target="#TableOfContents" class="page-wrapper   " data-wc-page-id="0f8ac3b5f6c7bc841b2b14305295ef5f" >

  
  
  
  
  
  
  
  
  <script src="/js/wowchemy-init.min.6edaf3b475ce43de30d98828aea698be.js"></script>

  


<aside class="search-modal" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Search</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#" aria-label="Close"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        
        <input name="q" id="search-query" placeholder="Search..." autocapitalize="off"
        autocomplete="off" autocorrect="off" spellcheck="false" type="search" class="form-control"
        aria-label="Search...">
        
      </div>

      
      

      

    </section>
    <section class="section-search-results">

      <div id="search-hits">
        
      </div>

    </section>
  </div>
</aside>



  <div class="page-header">
    












<nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id="navbar-main">
  <div class="container-xl">

    
    <div class="d-none d-lg-inline-flex">
      <a class="navbar-brand" href="/">Matteo Courthoud</a>
    </div>
    

    
    <button type="button" class="navbar-toggler" data-toggle="collapse"
            data-target="#navbar-content" aria-controls="navbar-content" aria-expanded="false" aria-label="Toggle navigation">
    <span><i class="fas fa-bars"></i></span>
    </button>
    

    
    <div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none">
      <a class="navbar-brand" href="/">Matteo Courthoud</a>
    </div>
    

    
    
    <div class="navbar-collapse main-menu-item collapse justify-content-end" id="navbar-content">

      
      <ul class="navbar-nav d-md-inline-flex">
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
            
            
            
              
            
            
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#about"><span>Home</span></a>
        </li>

        
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
            
            
            
              
            
            
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#projects"><span>Research</span></a>
        </li>

        
        

        
        <li class="nav-item dropdown">
          <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown" aria-haspopup="true"><span>Courses</span><span class="caret"></span>
          </a>
          <div class="dropdown-menu">
            
              <a class="dropdown-item" href="/course/ml-econ/"><span>Machine Learning for Economics</span></a>
            
              <a class="dropdown-item" href="/course/data-science/"><span>Data Science with Python</span></a>
            
              <a class="dropdown-item" href="/course/empirical-io/"><span>PhD Industrial Organization</span></a>
            
              <a class="dropdown-item" href="/course/metrics/"><span>PhD Econometrics</span></a>
            
              <a class="dropdown-item" href="https://pp4rs.github.io/"><span>Programming Practices for Research</span></a>
            
          </div>
        </li>

        
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/post/"><span>Posts</span></a>
        </li>

        
        

        

        
        
        

        

        
        
        
        

        
          
            
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/cv"><span>CV</span></a>
        </li>

        
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
            
            
            
              
            
            
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#contact"><span>Contact</span></a>
        </li>

        
        

      

        
      </ul>
    </div>

    <ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2">

      
      
        
      

      
      

      
      

      
      

    </ul>

  </div>
</nav>


  </div>

  <div class="page-body">
    

<div class="container-fluid docs">
  <div class="row flex-xl-nowrap">
    <div class="col-12 col-md-3 col-xl-2 docs-sidebar">
      
<nav class="collapse docs-links" id="docs-nav">
  
  
  
  
  
  

  
  
    

    
      

      <ul class="nav docs-sidenav">
        <li><a href="/course/"><i class="fas fa-arrow-left pr-1"></i>Courses</a></li>
      </ul>

      
      
        
          
        
      


  
    
    
    
    
      
    
    

    
      <div class="docs-toc-item">
        <a class="docs-toc-link " href="/course/ml-econ/">ML for Economics</a>
    
      
        <ul class="nav docs-sidenav">
      


  <li class=""><a href="/course/ml-econ/01_regression/">Linear Regression</a></li>



  <li class=""><a href="/course/ml-econ/02_iv/">Instrumental Variables</a></li>



  <li class="active"><a href="/course/ml-econ/03_nonparametric/">Non-Parametric Regression</a></li>



  <li class=""><a href="/course/ml-econ/04_crossvalidation/">Resampling Methods</a></li>



  <li class=""><a href="/course/ml-econ/05_regularization/">Model Selection and Regularization</a></li>



  <li class=""><a href="/course/ml-econ/06_convexity/">Convexity and Optimization</a></li>



  <li class=""><a href="/course/ml-econ/07_trees/">Tree-based Methods</a></li>



  <li class=""><a href="/course/ml-econ/08_neuralnets/">Neural Networks</a></li>



  <li class=""><a href="/course/ml-econ/09_postdoubleselection/">Post-Double Selection</a></li>



  <li class=""><a href="/course/ml-econ/10_unsupervised/">Unsupervised Learning</a></li>

      
        </ul>
      
    

    
      </div>
    

    
  
</nav>

    </div>

    
    
    <div class="d-none d-xl-block col-xl-2 docs-toc">
      

      <ul class="nav toc-top">
        <li><a href="#" id="back_to_top" class="docs-toc-title">Contents</a></li>
      </ul>

      <nav id="TableOfContents">
  <ul>
    <li>
      <ul>
        <li><a href="#dataset">Dataset</a></li>
      </ul>
    </li>
    <li><a href="#polynomial-regression-and-step-functions">Polynomial Regression and Step Functions</a>
      <ul>
        <li><a href="#explore-the-data">Explore the Data</a></li>
        <li><a href="#polynomials-of-different-degrees">Polynomials of different degrees</a></li>
        <li><a href="#variables">Variables</a></li>
        <li><a href="#polynomia-regression">Polynomia Regression</a></li>
        <li><a href="#measures-of-fit">Measures of Fit</a></li>
        <li><a href="#binary-dependent-variable">Binary Dependent Variable</a></li>
        <li><a href="#binomial-link-functions">Binomial Link Functions</a></li>
        <li><a href="#logit-link-function">Logit Link Function</a></li>
        <li><a href="#logistic-regression">Logistic Regression</a></li>
        <li><a href="#linear-model-comparison">Linear Model Comparison</a></li>
        <li><a href="#plot-data-and-predictions">Plot data and predictions</a></li>
      </ul>
    </li>
    <li><a href="#step-functions">Step Functions</a>
      <ul>
        <li><a href="#binning">Binning</a></li>
        <li><a href="#dummy-variables">Dummy Variables</a></li>
        <li><a href="#stepwise-regression">Stepwise Regression</a></li>
        <li><a href="#logistic-step-regression">Logistic Step Regression</a></li>
        <li><a href="#plotting">Plotting</a></li>
      </ul>
    </li>
    <li><a href="#regression-splines">Regression Splines</a>
      <ul>
        <li><a href="#example">Example</a></li>
        <li><a href="#generate-predictions">Generate Predictions</a></li>
        <li><a href="#plotting-1">Plotting</a></li>
        <li><a href="#comment">Comment</a></li>
        <li><a href="#the-spline-basis-representation">The Spline Basis Representation</a></li>
        <li><a href="#cubic-splines">Cubic Splines</a></li>
        <li><a href="#no-knots">No Knots</a></li>
        <li><a href="#natural-splines">Natural Splines</a></li>
        <li><a href="#comparison">Comparison</a></li>
        <li><a href="#comparison-to-polynomial-regression">Comparison to Polynomial Regression</a></li>
        <li><a href="#plotting-2">Plotting</a></li>
      </ul>
    </li>
    <li><a href="#local-regression">Local Regression</a>
      <ul>
        <li><a href="#details">Details</a></li>
        <li><a href="#generate-data">Generate Data</a></li>
        <li><a href="#plotting-3">Plotting</a></li>
        <li><a href="#fit-ll-regression">Fit LL Regression</a></li>
        <li><a href="#prediction">Prediction</a></li>
        <li><a href="#details-1">Details</a></li>
        <li><a href="#visualization">Visualization</a></li>
        <li><a href="#zooming-in">Zooming in</a></li>
        <li><a href="#plotting-4">Plotting</a></li>
      </ul>
    </li>
    <li><a href="#generalized-additive-models">Generalized Additive Models</a>
      <ul>
        <li><a href="#gam-for-regression-problems">GAM for Regression Problems</a></li>
        <li><a href="#example-1">Example</a></li>
        <li><a href="#plotting-5">Plotting</a></li>
        <li><a href="#pros-and-cons">Pros and Cons</a></li>
        <li><a href="#gams-for-classification-problems">GAMs for Classification Problems</a></li>
        <li><a href="#plotting-6">Plotting</a></li>
      </ul>
    </li>
  </ul>
</nav>

      
    </div>
    

    <main class="col-12 col-md-9 col-xl-8 py-md-3 pl-md-5 docs-content" role="main">

      <article class="article">

        <div class="docs-article-container">

          <h1>Non-Parametric Regression</h1>

          <p>Last updated on Mar 9, 2022</p>

          <div class="article-style">
            <pre><code class="language-python"># Setup
%matplotlib inline
from utils.lecture03 import *
</code></pre>
<h3 id="dataset">Dataset</h3>
<p>For this session, we are mostly going to work with the wage dataset.</p>
<pre><code class="language-python">df = pd.read_csv('data/Wage.csv', index_col=0)
df.head(3)
</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
</code></pre>
<p></style></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>year</th>
      <th>age</th>
      <th>maritl</th>
      <th>race</th>
      <th>education</th>
      <th>region</th>
      <th>jobclass</th>
      <th>health</th>
      <th>health_ins</th>
      <th>logwage</th>
      <th>wage</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>231655</th>
      <td>2006</td>
      <td>18</td>
      <td>1. Never Married</td>
      <td>1. White</td>
      <td>1. &lt; HS Grad</td>
      <td>2. Middle Atlantic</td>
      <td>1. Industrial</td>
      <td>1. &lt;=Good</td>
      <td>2. No</td>
      <td>4.318063</td>
      <td>75.043154</td>
    </tr>
    <tr>
      <th>86582</th>
      <td>2004</td>
      <td>24</td>
      <td>1. Never Married</td>
      <td>1. White</td>
      <td>4. College Grad</td>
      <td>2. Middle Atlantic</td>
      <td>2. Information</td>
      <td>2. &gt;=Very Good</td>
      <td>2. No</td>
      <td>4.255273</td>
      <td>70.476020</td>
    </tr>
    <tr>
      <th>161300</th>
      <td>2003</td>
      <td>45</td>
      <td>2. Married</td>
      <td>1. White</td>
      <td>3. Some College</td>
      <td>2. Middle Atlantic</td>
      <td>1. Industrial</td>
      <td>1. &lt;=Good</td>
      <td>1. Yes</td>
      <td>4.875061</td>
      <td>130.982177</td>
    </tr>
  </tbody>
</table>
</div>
<p>This dataset contains information on wages and individual characteristics.</p>
<p>Our main objective is going to be to explain wages using the observables contained in the dataset.</p>
<h2 id="polynomial-regression-and-step-functions">Polynomial Regression and Step Functions</h2>
<p>As we have seen in the first lecture, the most common way to introduce linearities is to replace the standard linear model</p>
<p>$$
y_i = \beta_0 + \beta_1 x_i + \varepsilon_i
$$</p>
<p>with a polynomial function</p>
<p>$$
y_i = \beta_0 + \beta_1 x_i + \beta_2 x_i^2 + \beta_2 x_i^3 + &hellip; + \varepsilon_i
$$</p>
<h3 id="explore-the-data">Explore the Data</h3>
<p>Suppose we want to investigate the relationship between <code>wage</code> and <code>age</code>. Let&rsquo;s first plot the two variables.</p>
<pre><code class="language-python"># Scatterplot of the data
df.plot.scatter('age','wage',color='w', edgecolors='k', alpha=0.3);
</code></pre>
<p><img src="../img/03_nonparametric_10_0.png" alt="png"></p>
<h3 id="polynomials-of-different-degrees">Polynomials of different degrees</h3>
<p>The relationship is highly complex and non-linear. Let&rsquo;s expand our linear regression polynomials of different degrees: 1 to 5.</p>
<pre><code class="language-python">X_poly1 = PolynomialFeatures(1).fit_transform(df.age.values.reshape(-1,1))
X_poly2 = PolynomialFeatures(2).fit_transform(df.age.values.reshape(-1,1))
X_poly3 = PolynomialFeatures(3).fit_transform(df.age.values.reshape(-1,1))
X_poly4 = PolynomialFeatures(4).fit_transform(df.age.values.reshape(-1,1))
X_poly5 = PolynomialFeatures(5).fit_transform(df.age.values.reshape(-1,1))
</code></pre>
<h3 id="variables">Variables</h3>
<p>Our dependent varaible is going to be a dummy for income above 250.000 USD.</p>
<pre><code class="language-python"># Get X and y
X = df.age
y = df.wage
y01 = (df.wage &gt; 250).map({False:0, True:1}).values
</code></pre>
<h3 id="polynomia-regression">Polynomia Regression</h3>
<p>If we run a linear regression on a 4-degree polinomial expansion of <code>age</code>, this is what it looks like`:</p>
<pre><code class="language-python"># Fit ols on 4th degree polynomial
fit = sm.OLS(y, X_poly4).fit()
fit.summary().tables[1]
</code></pre>
<table class="simpletable">
<tr>
    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  
</tr>
<tr>
  <th>const</th> <td> -184.1542</td> <td>   60.040</td> <td>   -3.067</td> <td> 0.002</td> <td> -301.879</td> <td>  -66.430</td>
</tr>
<tr>
  <th>x1</th>    <td>   21.2455</td> <td>    5.887</td> <td>    3.609</td> <td> 0.000</td> <td>    9.703</td> <td>   32.788</td>
</tr>
<tr>
  <th>x2</th>    <td>   -0.5639</td> <td>    0.206</td> <td>   -2.736</td> <td> 0.006</td> <td>   -0.968</td> <td>   -0.160</td>
</tr>
<tr>
  <th>x3</th>    <td>    0.0068</td> <td>    0.003</td> <td>    2.221</td> <td> 0.026</td> <td>    0.001</td> <td>    0.013</td>
</tr>
<tr>
  <th>x4</th>    <td>-3.204e-05</td> <td> 1.64e-05</td> <td>   -1.952</td> <td> 0.051</td> <td>-6.42e-05</td> <td> 1.45e-07</td>
</tr>
</table>
<h3 id="measures-of-fit">Measures of Fit</h3>
<p>In this case, the single coefficients are not of particular interest. We are mostly interested in the best capturing the relationship between <code>age</code> and <code>wage</code>. How can we pick among thedifferent polynomials?</p>
<p>We compare different polynomial degrees. For each regression, we are going to look at a series of metrics:</p>
<ul>
<li>absolute residuals</li>
<li>sum of squared residuals</li>
<li>the difference in SSR w.r (SSR).t the 0-degree case</li>
<li>F statistic</li>
</ul>
<pre><code class="language-python"># Run regressions
fit_1 = sm.OLS(y, X_poly1).fit()
fit_2 = sm.OLS(y, X_poly2).fit()
fit_3 = sm.OLS(y, X_poly3).fit()
fit_4 = sm.OLS(y, X_poly4).fit()
fit_5 = sm.OLS(y, X_poly5).fit()

# Compare fit
sm.stats.anova_lm(fit_1, fit_2, fit_3, fit_4, fit_5, typ=1)
</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
</code></pre>
<p></style></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>df_resid</th>
      <th>ssr</th>
      <th>df_diff</th>
      <th>ss_diff</th>
      <th>F</th>
      <th>Pr(&gt;F)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>2998.0</td>
      <td>5.022216e+06</td>
      <td>0.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2997.0</td>
      <td>4.793430e+06</td>
      <td>1.0</td>
      <td>228786.010128</td>
      <td>143.593107</td>
      <td>2.363850e-32</td>
    </tr>
    <tr>
      <th>2</th>
      <td>2996.0</td>
      <td>4.777674e+06</td>
      <td>1.0</td>
      <td>15755.693664</td>
      <td>9.888756</td>
      <td>1.679202e-03</td>
    </tr>
    <tr>
      <th>3</th>
      <td>2995.0</td>
      <td>4.771604e+06</td>
      <td>1.0</td>
      <td>6070.152124</td>
      <td>3.809813</td>
      <td>5.104620e-02</td>
    </tr>
    <tr>
      <th>4</th>
      <td>2994.0</td>
      <td>4.770322e+06</td>
      <td>1.0</td>
      <td>1282.563017</td>
      <td>0.804976</td>
      <td>3.696820e-01</td>
    </tr>
  </tbody>
</table>
</div>
<p>The polynomial degree 4 seems best.</p>
<pre><code class="language-python"># Set polynomial X to 4th degree
X_poly = X_poly4
</code></pre>
<h3 id="binary-dependent-variable">Binary Dependent Variable</h3>
<p>Since we have a binary dependent variable, it would be best to account for it in our regression framework. One way to do so, is to run a logistic regression.</p>
<p>How to interpret a Logistic Regression?</p>
<p>$$
y = \mathbb I \ \Big( \beta_0 + \beta_1 x_i + \beta_2 x_i^2 + \beta_2 x_i^3 + &hellip; + \varepsilon_i \Big)
$$</p>
<p>where $\mathbb I(\cdot)$ is an indicator function and now $\varepsilon_i$ is the error term.</p>
<h3 id="binomial-link-functions">Binomial Link Functions</h3>
<p>Depending on the assumed distribution of the error term, we get different results. I list below the error types supported by the <code>Binomial</code> family.</p>
<pre><code class="language-python"># List link functions for the Binomial family
sm.families.family.Binomial.links
</code></pre>
<pre><code>[statsmodels.genmod.families.links.logit,
 statsmodels.genmod.families.links.probit,
 statsmodels.genmod.families.links.cauchy,
 statsmodels.genmod.families.links.log,
 statsmodels.genmod.families.links.cloglog,
 statsmodels.genmod.families.links.identity]
</code></pre>
<h3 id="logit-link-function">Logit Link Function</h3>
<p>We are going to pick the <code>logit</code> link, i.e. we are going to assume that the error term is Type 1 Extreme Value (or Gumbel) distributed. It instead we take the usual standard normal distribution assumption for $\varepsilon_i$, we get <code>probit</code> regression.</p>
<pre><code class="language-python"># Pick the logit link for the Binomial family
logit_link = sm.families.Binomial(sm.genmod.families.links.logit())
</code></pre>
<p>Given the error distribution, we can write the probability that $y=1$ as</p>
<p>$$
\Pr(y=1) = \frac{e^{ \beta_0 + \beta_1 x_i + \beta_2 x_i^2 + \beta_2 x_i^3 + &hellip; + \varepsilon_i }}{1 + e^{ \beta_0 + \beta_1 x_i + \beta_2 x_i^2 + \beta_2 x_i^3 + &hellip; + \varepsilon_i } }
$$</p>
<h3 id="logistic-regression">Logistic Regression</h3>
<p>We now estimate the regression and plot the estimated relationship between <code>age</code> and <code>wage</code>.</p>
<pre><code class="language-python"># Run logistic regression
logit_poly = sm.GLM(y01, X_poly, family=logit_link).fit()
logit_poly.summary().tables[1]
</code></pre>
<table class="simpletable">
<tr>
    <td></td>       <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  
</tr>
<tr>
  <th>const</th> <td> -109.5530</td> <td>   47.655</td> <td>   -2.299</td> <td> 0.022</td> <td> -202.956</td> <td>  -16.150</td>
</tr>
<tr>
  <th>x1</th>    <td>    8.9950</td> <td>    4.187</td> <td>    2.148</td> <td> 0.032</td> <td>    0.789</td> <td>   17.201</td>
</tr>
<tr>
  <th>x2</th>    <td>   -0.2816</td> <td>    0.135</td> <td>   -2.081</td> <td> 0.037</td> <td>   -0.547</td> <td>   -0.016</td>
</tr>
<tr>
  <th>x3</th>    <td>    0.0039</td> <td>    0.002</td> <td>    2.022</td> <td> 0.043</td> <td>    0.000</td> <td>    0.008</td>
</tr>
<tr>
  <th>x4</th>    <td>-1.949e-05</td> <td> 9.91e-06</td> <td>   -1.966</td> <td> 0.049</td> <td>-3.89e-05</td> <td>-6.41e-08</td>
</tr>
</table>
<h3 id="linear-model-comparison">Linear Model Comparison</h3>
<p>What is the difference with the linear model?</p>
<pre><code class="language-python"># Run OLS regression with binary outcome
ols_poly = sm.OLS(y01, X_poly).fit()
ols_poly.summary().tables[1]
</code></pre>
<table class="simpletable">
<tr>
    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  
</tr>
<tr>
  <th>const</th> <td>   -0.1126</td> <td>    0.240</td> <td>   -0.468</td> <td> 0.640</td> <td>   -0.584</td> <td>    0.359</td>
</tr>
<tr>
  <th>x1</th>    <td>    0.0086</td> <td>    0.024</td> <td>    0.363</td> <td> 0.717</td> <td>   -0.038</td> <td>    0.055</td>
</tr>
<tr>
  <th>x2</th>    <td>   -0.0002</td> <td>    0.001</td> <td>   -0.270</td> <td> 0.787</td> <td>   -0.002</td> <td>    0.001</td>
</tr>
<tr>
  <th>x3</th>    <td> 3.194e-06</td> <td> 1.23e-05</td> <td>    0.260</td> <td> 0.795</td> <td>-2.09e-05</td> <td> 2.73e-05</td>
</tr>
<tr>
  <th>x4</th>    <td>-1.939e-08</td> <td> 6.57e-08</td> <td>   -0.295</td> <td> 0.768</td> <td>-1.48e-07</td> <td> 1.09e-07</td>
</tr>
</table>
<p>The magnitude of the coefficients is different, but the signs are the same.</p>
<h3 id="plot-data-and-predictions">Plot data and predictions</h3>
<p>Let&rsquo;s plot the estimated curves against the data distribution.</p>
<pre><code class="language-python"># Generate predictions
x_grid = np.arange(df.age.min(), df.age.max()).reshape(-1,1)
X_poly_test = PolynomialFeatures(4).fit_transform(x_grid)
y_hat1 = sm.OLS(y, X_poly).fit().predict(X_poly_test)
y01_hat1 = logit_poly.predict(X_poly_test)
</code></pre>
<pre><code class="language-python">plot_predictions(X, y, x_grid, y01, y_hat1, y01_hat1, 'Figure 7.1: Degree-4 Polynomial')
</code></pre>
<p><img src="../img/03_nonparametric_44_0.png" alt="png"></p>
<p>Which is remindful of
<img src="../figures/nonlinearities.jpg" alt="Le Petit Prince - Elephant figure" title="Nonlinearities"></p>
<p>Using polynomial functions of the features as predictors in a linear model imposes a global structure on the non-linear function of <code>age</code>. We can instead use step functions in order to avoid imposing such a global structure.</p>
<p>For example, we could break the range of <code>age</code> into bins, and fit a different constant in each bin.</p>
<h2 id="step-functions">Step Functions</h2>
<p>Building a step function means first picking $K$ cutpoints $c_1 , c_2 , . . . , c_K$ in the range of <code>age</code>,
and then construct $K + 1$ new variables</p>
<p>$$
C_0(<code>age</code>) = \mathbb I ( <code>age</code> &lt; c_1) \
C_1(<code>age</code>) = \mathbb I ( c_1 &lt; <code>age</code> &lt; c_2) \
C_2(<code>age</code>) = \mathbb I ( c_2 &lt; <code>age</code> &lt; c_3) \
&hellip; \
C_{K-1}(<code>age</code>) = \mathbb I ( c_{K-1} &lt; <code>age</code> &lt; c_K) \
C_K(<code>age</code>) = \mathbb I ( c_K &lt; <code>age</code>) \
$$</p>
<p>where $\mathbb I(\cdot)$ is the indicator function.</p>
<h3 id="binning">Binning</h3>
<p>First, we generate the cuts.</p>
<pre><code class="language-python"># Generate cuts for the variable age
df_cut, bins = pd.cut(df.age, 4, retbins=True, right=True)
df_cut.value_counts(sort=False)
type(df_cut)
</code></pre>
<pre><code>pandas.core.series.Series
</code></pre>
<p>Let&rsquo;s generate a DataFrame out of this series.</p>
<pre><code class="language-python"># Generate bins for &quot;age&quot; from the cuts
df_steps = pd.concat([df.age, df_cut, df.wage], keys=['age','age_cuts','wage'], axis=1)
df_steps.head(5)
</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
</code></pre>
<p></style></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>age</th>
      <th>age_cuts</th>
      <th>wage</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>231655</th>
      <td>18</td>
      <td>(17.938, 33.5]</td>
      <td>75.043154</td>
    </tr>
    <tr>
      <th>86582</th>
      <td>24</td>
      <td>(17.938, 33.5]</td>
      <td>70.476020</td>
    </tr>
    <tr>
      <th>161300</th>
      <td>45</td>
      <td>(33.5, 49.0]</td>
      <td>130.982177</td>
    </tr>
    <tr>
      <th>155159</th>
      <td>43</td>
      <td>(33.5, 49.0]</td>
      <td>154.685293</td>
    </tr>
    <tr>
      <th>11443</th>
      <td>50</td>
      <td>(49.0, 64.5]</td>
      <td>75.043154</td>
    </tr>
  </tbody>
</table>
</div>
<h3 id="dummy-variables">Dummy Variables</h3>
<p>Now we can generate different dummy variables out of each bin.</p>
<pre><code class="language-python"># Create dummy variables for the age groups
df_steps_dummies = pd.get_dummies(df_steps['age_cuts'])

# Statsmodels requires explicit adding of a constant (intercept)
df_steps_dummies = sm.add_constant(df_steps_dummies)
df_steps_dummies.head(5)
</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
</code></pre>
<p></style></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>const</th>
      <th>(17.938, 33.5]</th>
      <th>(33.5, 49.0]</th>
      <th>(49.0, 64.5]</th>
      <th>(64.5, 80.0]</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>231655</th>
      <td>1.0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>86582</th>
      <td>1.0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>161300</th>
      <td>1.0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>155159</th>
      <td>1.0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>11443</th>
      <td>1.0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div>
<h3 id="stepwise-regression">Stepwise Regression</h3>
<p>We are now ready to run our regression</p>
<pre><code class="language-python"># Generate our new X variable
X_step = df_steps_dummies.drop(df_steps_dummies.columns[1], axis=1)

# OLS Regression on step functions
ols_step = sm.OLS(y, X_step).fit()
ols_step.summary().tables[1]
</code></pre>
<table class="simpletable">
<tr>
        <td></td>          <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  
</tr>
<tr>
  <th>const</th>        <td>   94.1584</td> <td>    1.476</td> <td>   63.790</td> <td> 0.000</td> <td>   91.264</td> <td>   97.053</td>
</tr>
<tr>
  <th>(33.5, 49.0]</th> <td>   24.0535</td> <td>    1.829</td> <td>   13.148</td> <td> 0.000</td> <td>   20.466</td> <td>   27.641</td>
</tr>
<tr>
  <th>(49.0, 64.5]</th> <td>   23.6646</td> <td>    2.068</td> <td>   11.443</td> <td> 0.000</td> <td>   19.610</td> <td>   27.719</td>
</tr>
<tr>
  <th>(64.5, 80.0]</th> <td>    7.6406</td> <td>    4.987</td> <td>    1.532</td> <td> 0.126</td> <td>   -2.139</td> <td>   17.420</td>
</tr>
</table>
<p>From the regression outcome we can see that most bin coefficients are significant, except for the last one.</p>
<pre><code class="language-python"># Put the test data in the same bins as the training data.
bin_mapping = np.digitize(x_grid.ravel(), bins)
bin_mapping
</code></pre>
<pre><code>array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2,
       2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
       3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4])
</code></pre>
<pre><code class="language-python"># Get dummies, drop first dummy category, add constant
X_step_test = sm.add_constant(pd.get_dummies(bin_mapping).drop(1, axis=1))
X_step_test.head()
</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
</code></pre>
<p></style></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>const</th>
      <th>2</th>
      <th>3</th>
      <th>4</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1.0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1.0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>1.0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>1.0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>1.0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div>
<pre><code class="language-python"># Step prediction
y_hat2 = ols_step.predict(X_step_test)
</code></pre>
<h3 id="logistic-step-regression">Logistic Step Regression</h3>
<p>We are going again to run a logistic regression, given that our outcome is binary.</p>
<pre><code class="language-python"># Logistic regression on step functions
logit_step = sm.GLM(y01, X_step, family=logit_link).fit()
y01_hat2 = logit_step.predict(X_step_test)
logit_step.summary().tables[1]
</code></pre>
<table class="simpletable">
<tr>
        <td></td>          <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  
</tr>
<tr>
  <th>const</th>        <td>   -5.0039</td> <td>    0.449</td> <td>  -11.152</td> <td> 0.000</td> <td>   -5.883</td> <td>   -4.124</td>
</tr>
<tr>
  <th>(33.5, 49.0]</th> <td>    1.5998</td> <td>    0.474</td> <td>    3.378</td> <td> 0.001</td> <td>    0.672</td> <td>    2.528</td>
</tr>
<tr>
  <th>(49.0, 64.5]</th> <td>    1.7147</td> <td>    0.488</td> <td>    3.512</td> <td> 0.000</td> <td>    0.758</td> <td>    2.672</td>
</tr>
<tr>
  <th>(64.5, 80.0]</th> <td>    0.7413</td> <td>    1.102</td> <td>    0.672</td> <td> 0.501</td> <td>   -1.420</td> <td>    2.902</td>
</tr>
</table>
<h3 id="plotting">Plotting</h3>
<p>How does the predicted function looks like?</p>
<pre><code class="language-python">plot_predictions(X, y, x_grid, y01, y_hat2, y01_hat2, 'Figure 7.2: Piecewise Constant')
</code></pre>
<p><img src="../img/03_nonparametric_69_0.png" alt="png"></p>
<h2 id="regression-splines">Regression Splines</h2>
<p>Spline regression, or piece-wise polynomial regression, involves fitting separate low-degree polynomials
over different regions of $X$. The idea is to have one regression specification but with different coefficients in different parts of the $X$ range. The points where the coefficients change are called knots.</p>
<p>For example, we could have a third degree polynomial <em>and</em> splitting the sample in two.</p>
<p>$$
y_{i}=\left{\begin{array}{ll}
\beta_{01}+\beta_{11} x_{i}+\beta_{21} x_{i}^{2}+\beta_{31} x_{i}^{3}+\epsilon_{i} &amp; \text { if } x_{i}&lt;c \
\beta_{02}+\beta_{12} x_{i}+\beta_{22} x_{i}^{2}+\beta_{32} x_{i}^{3}+\epsilon_{i} &amp; \text { if } x_{i} \geq c
\end{array}\right.
$$</p>
<p>We have now two sets of coefficients, one for each subsample.</p>
<p>Generally, using more knots leads to a more flexible piecewise polynomial. Also increasing the degree of the polynomial increases the degree of flexibility.</p>
<h3 id="example">Example</h3>
<p>We are now going to plot 4 different examples for the <code>age</code> <code>wage</code> relationship:</p>
<ol>
<li>Discontinuous piecewise cubic</li>
<li>Continuous piecewise cubic</li>
<li>Quadratic (continuous)</li>
<li>Continuous piecewise linear</li>
</ol>
<pre><code class="language-python"># Cut dataset
df_short = df.iloc[:80,:]
X_short = df_short.age
y_short = df_short.wage
x_grid_short = np.arange(df_short.age.min(), df_short.age.max()+1).reshape(-1,1)

# 1. Discontinuous piecewise cubic
spline1 = &quot;bs(x, knots=(50,50,50,50), degree=3, include_intercept=False)&quot;

# 2. Continuous piecewise cubic
spline2 = &quot;bs(x, knots=(50,50,50), degree=3, include_intercept=False)&quot;

# 3. Quadratic (continuous)
spline3 = &quot;bs(x, knots=(%s,%s), degree=2, include_intercept=False)&quot; % (min(df.age), min(df.age))

# 4. Continuous piecewise linear
spline4 = &quot;bs(x, knots=(%s,50), degree=1, include_intercept=False)&quot; % min(df.age)
</code></pre>
<h3 id="generate-predictions">Generate Predictions</h3>
<pre><code class="language-python"># Generate spline predictions
def fit_predict_spline(spline, X, y, x_grid):
    transformed_x = dmatrix(spline, {&quot;x&quot;: X}, return_type='dataframe')
    fit = sm.GLM(y, transformed_x).fit()
    y_hat = fit.predict(dmatrix(spline, {&quot;x&quot;: x_grid}, return_type='dataframe'))
    return y_hat

y_hats = [fit_predict_spline(s, X_short, y_short, x_grid_short) for s in [spline1, spline2, spline3, spline4]]
</code></pre>
<h3 id="plotting-1">Plotting</h3>
<pre><code class="language-python">plot_splines(df_short, x_grid_short, y_hats)
</code></pre>
<p><img src="../img/03_nonparametric_78_0.png" alt="png"></p>
<h3 id="comment">Comment</h3>
<p>The first example makes us think on why would we want out function to be discontinuous. Unless we expect a sudden wage jump at a certain age, we would like the function to be continuous. However, if for example we split <code>age</code> around the retirement age, we might expect a discontinuity.</p>
<p>The second example (top right) makes us think on why would we want out function not to be differentiable. Unless we have some specific mechanism in mind, ususally there is a trade-off between making the function non-differentiable or increasing the degree of the polynomial, as the last two examples show us. We get a similar fit with a quadratic fit or a discontinuous linear fit. The main difference is that in the second case we are picking the discontinuity point by hand instead of letting the data choose how to change the slope of the curve.</p>
<h3 id="the-spline-basis-representation">The Spline Basis Representation</h3>
<p>How can we fit a piecewise degree-d polynomial under the constraint that it (and possibly its first d − 1 derivatives) be continuous?</p>
<p>The most direct way to represent a cubic spline is to start off with a basis for a cubic polynomial—namely, x,x2,x3—and then add one truncated power basis function per knot. A truncated power basis function is defined as</p>
<p>$$
h(x, c)=(x-c)_{+}^{3} = \Bigg{\begin{array}{cl}
(x-c)^{3} &amp; \text { if } x&gt;c \
0 &amp; \text { otherwise }
\end{array}
$$</p>
<p>One can show that adding a term of the form $\beta_4 h(x, c)$ to the model for a cubic polynomial will lead to a discontinuity in only the third derivative at $c$; the function will remain continuous, with continuous first and second derivatives, at each of the knots.</p>
<h3 id="cubic-splines">Cubic Splines</h3>
<p>One way to specify the spline is using nodes and degrees of freedom.</p>
<pre><code class="language-python"># Specifying 3 knots and 3 degrees of freedom
spline5 = &quot;bs(x, knots=(25,40,60), degree=3, include_intercept=False)&quot;
pred5 = fit_predict_spline(spline5, X, y, x_grid)
</code></pre>
<h3 id="no-knots">No Knots</h3>
<p>When we fit a spline, where should we place the knots?</p>
<p>The regression spline is most flexible in regions that contain a lot of knots, because in those regions the polynomial coefficients can change rapidly. Hence, one option is to place more knots in places where we feel the function might vary most rapidly, and to place fewer knots where it seems more stable. While this option can work well, in practice it is common to place knots in a uniform fashion. One way to do this is to specify the desired degrees of freedom, and then have the software automatically place the corresponding number of knots at uniform quantiles of the data.</p>
<pre><code class="language-python"># Specifying degree 3 and 6 degrees of freedom 
spline6 = &quot;bs(x, df=6, degree=3, include_intercept=False)&quot;
pred6 = fit_predict_spline(spline6, X, y, x_grid)
</code></pre>
<h3 id="natural-splines">Natural Splines</h3>
<p>A natural spline is a regression spline with additional boundary constraints: the function is required to be linear at the boundary (in the region where X is smaller than the smallest knot, or larger than the largest knot). This addi- tional constraint means that natural splines generally produce more stable estimates at the boundaries.</p>
<pre><code class="language-python"># Natural spline with 4 degrees of freedom
spline7 = &quot;cr(x, df=4)&quot;
pred7 = fit_predict_spline(spline7, X, y, x_grid)
</code></pre>
<h3 id="comparison">Comparison</h3>
<pre><code class="language-python"># Compare predictons
preds = [pred5, pred6, pred7]
labels = ['degree 3, knots 3', 'degree 3, degrees of freedom 3', 'natural, degrees of freedom 4']
compare_predictions(X, y, x_grid, preds, labels)
</code></pre>
<p><img src="../img/03_nonparametric_93_0.png" alt="png"></p>
<h3 id="comparison-to-polynomial-regression">Comparison to Polynomial Regression</h3>
<p>Regression splines often give superior results to polynomial regression. This is because unlike polynomials, which must use a high degree to produce flexible fits, splines introduce flexibility by increasing the number of knots but keeping the degree fixed.</p>
<p>We are now fitting a polynomial of degree 15 and a spline with 15 degrees of freedom.</p>
<pre><code class="language-python"># Polynomial of degree 15
X_poly15 = PolynomialFeatures(15).fit_transform(df.age.values.reshape(-1,1))
ols_poly_15 = sm.OLS(y, X_poly15).fit()
pred8 = ols_poly_15.predict(PolynomialFeatures(15).fit_transform(x_grid))

# Spline with 15 degrees of freedon
spline9 = &quot;bs(x, df=15, degree=3, include_intercept=False)&quot;
pred9 = fit_predict_spline(spline9, X, y, x_grid)
</code></pre>
<h3 id="plotting-2">Plotting</h3>
<pre><code class="language-python"># Compare predictons
preds = [pred8, pred9]
labels = ['Polynomial', 'Spline']
compare_predictions(X, y, x_grid, preds, labels)
</code></pre>
<p><img src="../img/03_nonparametric_98_0.png" alt="png"></p>
<p>As we can see, despite the two regressions having the same degrees of freedom, the polynomial fit is much more volatile. We can compare them along some dimensions.</p>
<h2 id="local-regression">Local Regression</h2>
<p>So far we have looked at so-called &ldquo;<em>global methods</em>&quot;: methods that try to fit a unique function specification over the whole data. The function specification can be complex, as in the case of splines, but can be expressed globally.</p>
<p>Local regression is a different approach for fitting flexible non-linear functions, which involves computing the fit at a target point $x_0$ using only the nearby training observations.</p>
<h3 id="details">Details</h3>
<p>How does local regression work?</p>
<p>Ingredients: $X$, $y$.</p>
<p>How to you output a prediction $\hat y_i$ at a new point $x_i$?</p>
<ol>
<li>Take a number of points in $X$ close to $x_i$: $X_{\text{close-to-i}}$</li>
<li>Assign a weight to each of there points</li>
<li>Fit a weigthed least squares regression of $X_{\text{close-to-i}}$ on $y_{\text{close-to-i}}$</li>
<li>Use the estimated coefficients $\hat \beta$ to predict $\hat y_i = \hat \beta_0 + \hat \beta_1 x_i$</li>
</ol>
<h3 id="generate-data">Generate Data</h3>
<pre><code class="language-python"># Set seed
np.random.seed(1)

# Generate data
X_sim = np.sort(np.random.uniform(0,1,100))
e = np.random.uniform(-.5,.5,100)
y_sim = -4*X_sim**2 + 3*X_sim + e

# True Generating process without noise
X_grid = np.linspace(0,1,100)
y_grid = -4*X_grid**2 + 3*X_grid
</code></pre>
<h3 id="plotting-3">Plotting</h3>
<p>Let&rsquo;s visualize the simulated data and the curve without noise.</p>
<pre><code class="language-python">plot_simulated_data(X_sim, y_sim, X_grid, y_grid);
</code></pre>
<p><img src="../img/03_nonparametric_108_0.png" alt="png"></p>
<h3 id="fit-ll-regression">Fit LL Regression</h3>
<p>Now we fit a local linear regression.</p>
<pre><code class="language-python"># Settings
spec = 'll'
bandwidth = 0.1
kernel = 'gaussian'

# Locally linear regression
local_reg = KernelReg(y_sim, X_sim.reshape(-1,1), 
                      var_type='c', 
                      reg_type=spec, 
                      bw=[bandwidth])
y_hat = KernelReg.fit(local_reg)
</code></pre>
<p>What do the parameters mean?</p>
<ul>
<li><code>var_type</code>: dependent variable type (<code>c</code> i.e. <em>continuous</em>)</li>
<li><code>reg_type</code>: local regression specification (<code>ll</code> i.e. <em>locally linear</em>)</li>
<li><code>bw</code>      : bandwidth length (<em>0.1</em>)</li>
<li><code>ckertype</code>: kernel type (<em>gaussian</em>)</li>
</ul>
<h3 id="prediction">Prediction</h3>
<p>What does the prediction looks like?</p>
<pre><code class="language-python">fig, ax = plot_simulated_data(X_sim, y_sim, X_grid, y_grid);
make_figure_7_9a(fig, ax, X_sim, y_hat);
</code></pre>
<p><img src="../img/03_nonparametric_115_0.png" alt="png"></p>
<h3 id="details-1">Details</h3>
<p>How exactly was the prediction generated? It was generated pointwise. We are now going to look at the prediction at one particular point: $x_i=0.5$.</p>
<p>We proceed as follows:</p>
<ol>
<li>We select the focal point: $x_i=0.5$</li>
<li>We select observations close to $\ x_i$, i.e. $x_{\text{close to i}} = { x \in X : |x_i - x| &lt; 0.1 } \ $ and $ \ y_{\text{close to i}} = { y \in Y : |x_i - x| &lt; 0.1 }$</li>
<li>We apply gaussian weights</li>
<li>We run a weighted linear regression of $y_{\text{close to i}}$ on $x_{\text{close to i}}$</li>
</ol>
<pre><code class="language-python"># Get local X and y
x_i = 0.5
close_to_i = (x_i-bandwidth &lt; X_sim) &amp; (X_sim &lt; x_i+bandwidth)
X_tilde = X_sim[close_to_i]
y_tilde = y_sim[close_to_i]

# Get local estimates
local_estimate = KernelReg.fit(local_reg, data_predict=[x_i])
y_i_hat = local_estimate[0]
beta_i_hat = local_estimate[1]
alpha_i_hat = y_i_hat - beta_i_hat*x_i
print('Estimates: alpha=%1.4f, beta=%1.4f' % (alpha_i_hat, beta_i_hat))
</code></pre>
<pre><code>Estimates: alpha=0.7006, beta=-0.6141
</code></pre>
<h3 id="visualization">Visualization</h3>
<p>Now we can use the locally estimated coefficients to predict the value of $\hat y_i(x_i)$ for $x_i = 0.5$.</p>
<pre><code class="language-python"># Build local predictions
close_to_i_grid = (x_i-bandwidth &lt; X_grid) &amp; (X_grid &lt; x_i+bandwidth)
X_grid_tilde = X_grid[close_to_i_grid].reshape(-1,1)
y_grid_tilde = alpha_i_hat + X_grid_tilde*beta_i_hat
</code></pre>
<pre><code class="language-python">fig, ax = plot_simulated_data(X_sim, y_sim, X_grid, y_grid);
make_figure_7_9a(fig, ax, X_sim, y_hat);
make_figure_7_9b(fig, ax, X_tilde, y_tilde, X_grid_tilde, y_grid_tilde, x_i, y_i_hat)
</code></pre>
<p><img src="../img/03_nonparametric_122_0.png" alt="png"></p>
<h3 id="zooming-in">Zooming in</h3>
<p>We can zoom in and look only at the &ldquo;<em>close to i</em>&rdquo; sample.</p>
<pre><code class="language-python">sns.regplot(X_tilde, y_tilde);
</code></pre>
<p><img src="../img/03_nonparametric_125_0.png" alt="png"></p>
<p>Why is the line upward sloped? We forgot the gaussian weights.</p>
<pre><code class="language-python"># Weights
w = norm.pdf((X_sim-x_i)/bandwidth)

# Estimate LWS
mod_wls = sm.WLS(y_sim, sm.add_constant(X_sim), weights=w)
results = mod_wls.fit()

print('Estimates: alpha=%1.4f, beta=%1.4f' % tuple(results.params))
</code></pre>
<pre><code>Estimates: alpha=0.7006, beta=-0.6141
</code></pre>
<p>We indeed got the same estimates as before. Note two things:</p>
<ol>
<li>the badwidth defines the scale parameter of the gaussian weights</li>
<li>our locally linear regression is acqually global</li>
</ol>
<h3 id="plotting-4">Plotting</h3>
<pre><code class="language-python">make_figure_7_9d(X_sim, y_sim, w, results, X_grid, x_i, y_i_hat)
</code></pre>
<p><img src="../img/03_nonparametric_130_0.png" alt="png"></p>
<p>Now the slope is indeed negative, as in the locally linear regression.</p>
<h2 id="generalized-additive-models">Generalized Additive Models</h2>
<p>Generalized additive models (GAMs) provide a general framework for extending a standard linear model by allowing non-linear functions of each of the variables, while maintaining additivity.</p>
<h3 id="gam-for-regression-problems">GAM for Regression Problems</h3>
<p>Imagine to extend the general regression framework to some separabily additive model of the form</p>
<p>$$
y_i = \beta_0 + \sum_{k=1}^K \beta_k f_k(x_{ik}) + \varepsilon_i
$$</p>
<p>It is called an additive model because we calculate a separate $f_k$ for each $X_k$, and then add together all of their contributions.</p>
<p>Consider for example the following model</p>
<p>$$
\text{wage} = \beta_0 + f_1(\text{year}) + f_2(\text{age}) + f_3(\text{education}) + \varepsilon
$$</p>
<h3 id="example-1">Example</h3>
<p>We are going to use the following functions:</p>
<ul>
<li>$f_1$: natural spline with 8 degrees of freedom</li>
<li>$f_2$: natural spline with 10 degrees of freedom</li>
<li>$f_3$: step function</li>
</ul>
<pre><code class="language-python"># Set X and y
df['education_'] = LabelEncoder().fit_transform(df[&quot;education&quot;])
X = df[['year','age','education_']].to_numpy()
y = df[['wage']].to_numpy()

## model
linear_gam = LinearGAM(s(0, n_splines=8) + s(1, n_splines=10) + f(2))
linear_gam.gridsearch(X, y);
</code></pre>
<pre><code>100% (11 of 11) |########################| Elapsed Time: 0:00:00 Time:  0:00:00
</code></pre>
<h3 id="plotting-5">Plotting</h3>
<pre><code class="language-python">plot_gam(linear_gam)
</code></pre>
<p><img src="../img/03_nonparametric_140_0.png" alt="png"></p>
<h3 id="pros-and-cons">Pros and Cons</h3>
<p>Before we move on, let us summarize the <strong>advantages</strong> of a GAM.</p>
<ul>
<li>GAMs allow us to fit a non-linear $f_k$ to each $X_k$, so that we can automatically model non-linear relationships that standard linear regression will miss</li>
<li>The non-linear fits can potentially make more accurate predictions</li>
<li>Because the model is additive, we can still examine the effect of each $X_k$ on $Y$ separately</li>
<li>The smoothness of the function $f_k$ for the variable $X_k$ can be summarized via degrees of freedom.</li>
</ul>
<p>The main <strong>limitation</strong> of GAMs is that the model is restricted to be additive. With many variables, important interactions can be missed. However, as with linear regression, we can manually add interaction terms to the GAM model by including additional predictors of the form $X_j \times X_k$.</p>
<h3 id="gams-for-classification-problems">GAMs for Classification Problems</h3>
<p>We can use GAMs also with a binary dependent variable.</p>
<pre><code class="language-python"># Binary dependent variable
y_binary = (y&gt;250)

## Logit link function
logit_gam = LogisticGAM(s(0, n_splines=8) + s(1, n_splines=10) + f(2), fit_intercept=True)
logit_gam.gridsearch(X, y_binary);
</code></pre>
<pre><code>100% (11 of 11) |########################| Elapsed Time: 0:00:00 Time:  0:00:00
</code></pre>
<h3 id="plotting-6">Plotting</h3>
<pre><code class="language-python">plot_gam(logit_gam)
</code></pre>
<p><img src="../img/03_nonparametric_147_0.png" alt="png"></p>
<p>The results are qualitatively similar to the non-binary case.</p>

          </div>

          



          
          
          <div class="article-widget">
            
<div class="post-nav">
  
  
  
  <div class="post-nav-item">
    <div class="meta-nav">Previous</div>
    <a href="/course/ml-econ/02_iv/" rel="next">Instrumental Variables</a>
  </div>
  
  
  
  <div class="post-nav-item">
    <div class="meta-nav">Next</div>
    <a href="/course/ml-econ/04_crossvalidation/" rel="prev">Resampling Methods</a>
  </div>
  
</div>

          </div>
          
        </div>

        <div class="body-footer">

          





          




          


  
  



        </div>

      </article>

      <footer class="site-footer">

  

  
  <p class="powered-by">
    
      <a href="/privacy/">Privacy Policy</a>
    
    
       &middot; 
      <a href="/terms/">Terms</a>
    
  </p>
  

  

  
  







</footer>


    </main>
  </div>
</div>

  </div>

  <div class="page-footer">
    
    
  </div>

  
<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        <pre><code class="tex hljs"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

      

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/instant.page/5.1.0/instantpage.min.js" integrity="sha512-1+qUtKoh9XZW7j+6LhRMAyOrgSQKenQ4mluTR+cvxXjP1Z54RxZuzstR/H9kgPXQsVB8IW7DMDFUJpzLjvhGSQ==" crossorigin="anonymous"></script>

      
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js" integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin="anonymous"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js" integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin="anonymous"></script>
      

      
      

      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mermaid/8.8.4/mermaid.min.js" integrity="sha512-as1BF4+iHZ3BVO6LLDQ7zrbvTXM+c/1iZ1qII/c3c4L8Rn5tHLpFUtpaEtBNS92f+xGsCzsD7b62XP3XYap6oA==" crossorigin="anonymous" title="mermaid"></script>
      

      
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/highlight.min.js" integrity="sha512-TDKKr+IvoqZnPzc3l35hdjpHD0m+b2EC2SrLEgKDRWpxf2rFCxemkgvJ5kfU48ip+Y+m2XVKyOCD85ybtlZDmw==" crossorigin="anonymous"></script>
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/languages/r.min.js"></script>
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/languages/python.min.js"></script>
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/languages/julia.min.js"></script>
        
      

    

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.7.1/leaflet.min.js" integrity="sha512-SeiQaaDh73yrb56sTW/RgVdi/mMqNeM2oBwubFHagc5BkixSpP1fvqF47mKzPGWYSSy4RwbBunrJBQ4Co8fRWA==" crossorigin="anonymous"></script>
    

    
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/anchor-js/4.2.2/anchor.min.js" integrity="sha512-I7w3ZdSFzw5j3jU3ZkNikBNeIrl3i+hEuEdwNmqUJvwNcaBUNcijnP2gd9DtGlgVYDplfjGoD8vTNsID+lCjqg==" crossorigin="anonymous"></script>
    <script>
      anchors.add();
    </script>
    

    
    
    
      
      <script id="search-hit-fuse-template" type="text/x-template">
        <div class="search-hit" id="summary-{{key}}">
          <div class="search-hit-content">
            <div class="search-hit-name">
              <a href="{{relpermalink}}">{{title}}</a>
              <div class="article-metadata search-hit-type">{{type}}</div>
              <p class="search-hit-description">{{snippet}}</p>
            </div>
          </div>
        </div>
      </script>
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js" integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin="anonymous"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js" integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin="anonymous"></script>
      
    

    
    

    
    
    
    

    
    <script src="/js/bootstrap.bundle.min.6aed84840afc03ab4d5750157f69c120.js"></script>

    
    
      
      
      
      
      
      
      
    

    
    
    
    
    
    
    
    
      
      
    
    
    <script src="/en/js/wowchemy.min.cf8ca859a9b74f8b1cd804621b13e5f1.js"></script>

    
  <script async defer src="https://buttons.github.io/buttons.js"></script>




</body>
</html>
