<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Machine Learning for Economics | Matteo Courthoud</title>
    <link>https://matteocourthoud.github.io/course/ml-econ/</link>
      <atom:link href="https://matteocourthoud.github.io/course/ml-econ/index.xml" rel="self" type="application/rss+xml" />
    <description>Machine Learning for Economics</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><copyright>Theme edited by Matteo Courthoud© - Want to have a similar website? [Guide here](https://matteocourthoud.github.io/post/website/).</copyright><lastBuildDate>Fri, 01 Oct 2021 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://matteocourthoud.github.io/media/icon_hu03e9b3967b83fd39296ec9da5ff1ea05_201175_512x512_fill_lanczos_center_3.png</url>
      <title>Machine Learning for Economics</title>
      <link>https://matteocourthoud.github.io/course/ml-econ/</link>
    </image>
    
    <item>
      <title>Linear Regression</title>
      <link>https://matteocourthoud.github.io/course/ml-econ/01_regression/</link>
      <pubDate>Wed, 09 Mar 2022 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/course/ml-econ/01_regression/</guid>
      <description>&lt;p&gt;This chapter follows closely Chapter 3 of &lt;a href=&#34;https://hastie.su.domains/ISLR2/ISLRv2_website.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;An Introduction to Statistical Learning&lt;/a&gt; by James, Witten, Tibshirani, Friedman.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Remove warnings
import warnings
warnings.filterwarnings(&#39;ignore&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Import everything
import pandas as pd
import numpy as np
import seaborn as sns
import statsmodels.api as sm

from sklearn.linear_model import LinearRegression
from numpy.linalg import inv
from numpy.random import normal as rnorm
from statsmodels.stats.outliers_influence import OLSInfluence
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Setup matplotlib for graphs
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import axes3d

# Set global parameters
%matplotlib inline
plt.style.use(&#39;seaborn-white&#39;)
plt.rcParams[&#39;lines.linewidth&#39;] = 3
plt.rcParams[&#39;figure.figsize&#39;] = (10,6)
plt.rcParams[&#39;figure.titlesize&#39;] = 20
plt.rcParams[&#39;axes.titlesize&#39;] = 18
plt.rcParams[&#39;axes.labelsize&#39;] = 14
plt.rcParams[&#39;legend.fontsize&#39;] = 14
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can inspect all the available global parameter options &lt;a href=&#34;https://matplotlib.org/3.3.2/tutorials/introductory/customizing.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;11-simple-linear-regression&#34;&gt;1.1 Simple Linear Regression&lt;/h2&gt;
&lt;p&gt;First, let&amp;rsquo;s load the Advertising dataset. It contains information on displays sales (in thousands of units) for a particular product and a list of advertising budgets (in thousands of dollars) for TV, radio, and newspaper media.&lt;/p&gt;
&lt;p&gt;We open the dataset using the &lt;code&gt;pandas&lt;/code&gt; library which is &lt;strong&gt;the&lt;/strong&gt; library for handling datasets and data analysis in Python.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Advertisement spending data
advertising = pd.read_csv(&#39;data/Advertising.csv&#39;, usecols=[1,2,3,4])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s have a look at the content. We can have a glance at the first rows by using the function &lt;code&gt;head&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Preview of the data
advertising.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;TV&lt;/th&gt;
      &lt;th&gt;Radio&lt;/th&gt;
      &lt;th&gt;Newspaper&lt;/th&gt;
      &lt;th&gt;Sales&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;230.1&lt;/td&gt;
      &lt;td&gt;37.8&lt;/td&gt;
      &lt;td&gt;69.2&lt;/td&gt;
      &lt;td&gt;22.1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;44.5&lt;/td&gt;
      &lt;td&gt;39.3&lt;/td&gt;
      &lt;td&gt;45.1&lt;/td&gt;
      &lt;td&gt;10.4&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;17.2&lt;/td&gt;
      &lt;td&gt;45.9&lt;/td&gt;
      &lt;td&gt;69.3&lt;/td&gt;
      &lt;td&gt;9.3&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;151.5&lt;/td&gt;
      &lt;td&gt;41.3&lt;/td&gt;
      &lt;td&gt;58.5&lt;/td&gt;
      &lt;td&gt;18.5&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;180.8&lt;/td&gt;
      &lt;td&gt;10.8&lt;/td&gt;
      &lt;td&gt;58.4&lt;/td&gt;
      &lt;td&gt;12.9&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;We can have a general overview of the dataset using the function &lt;code&gt;info&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Overview of all variables
advertising.info()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;class &#39;pandas.core.frame.DataFrame&#39;&amp;gt;
RangeIndex: 200 entries, 0 to 199
Data columns (total 4 columns):
 #   Column     Non-Null Count  Dtype  
---  ------     --------------  -----  
 0   TV         200 non-null    float64
 1   Radio      200 non-null    float64
 2   Newspaper  200 non-null    float64
 3   Sales      200 non-null    float64
dtypes: float64(4)
memory usage: 6.4 KB
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can have more information on the single variables using the function &lt;code&gt;describe&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Summary of all variables
advertising.describe()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;TV&lt;/th&gt;
      &lt;th&gt;Radio&lt;/th&gt;
      &lt;th&gt;Newspaper&lt;/th&gt;
      &lt;th&gt;Sales&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;count&lt;/th&gt;
      &lt;td&gt;200.000000&lt;/td&gt;
      &lt;td&gt;200.000000&lt;/td&gt;
      &lt;td&gt;200.000000&lt;/td&gt;
      &lt;td&gt;200.000000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;mean&lt;/th&gt;
      &lt;td&gt;147.042500&lt;/td&gt;
      &lt;td&gt;23.264000&lt;/td&gt;
      &lt;td&gt;30.554000&lt;/td&gt;
      &lt;td&gt;14.022500&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;std&lt;/th&gt;
      &lt;td&gt;85.854236&lt;/td&gt;
      &lt;td&gt;14.846809&lt;/td&gt;
      &lt;td&gt;21.778621&lt;/td&gt;
      &lt;td&gt;5.217457&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;min&lt;/th&gt;
      &lt;td&gt;0.700000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.300000&lt;/td&gt;
      &lt;td&gt;1.600000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;25%&lt;/th&gt;
      &lt;td&gt;74.375000&lt;/td&gt;
      &lt;td&gt;9.975000&lt;/td&gt;
      &lt;td&gt;12.750000&lt;/td&gt;
      &lt;td&gt;10.375000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;50%&lt;/th&gt;
      &lt;td&gt;149.750000&lt;/td&gt;
      &lt;td&gt;22.900000&lt;/td&gt;
      &lt;td&gt;25.750000&lt;/td&gt;
      &lt;td&gt;12.900000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;75%&lt;/th&gt;
      &lt;td&gt;218.825000&lt;/td&gt;
      &lt;td&gt;36.525000&lt;/td&gt;
      &lt;td&gt;45.100000&lt;/td&gt;
      &lt;td&gt;17.400000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;max&lt;/th&gt;
      &lt;td&gt;296.400000&lt;/td&gt;
      &lt;td&gt;49.600000&lt;/td&gt;
      &lt;td&gt;114.000000&lt;/td&gt;
      &lt;td&gt;27.000000&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;If you just want to call a variable in &lt;code&gt;pandas&lt;/code&gt;, you have 3 options:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;use squared brackets as if the varaible was a component of a dictionary&lt;/li&gt;
&lt;li&gt;use or dot subscripts as if the variable was a function of the data&lt;/li&gt;
&lt;li&gt;use the &lt;code&gt;loc&lt;/code&gt; function (best practice)&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# 1. Brackets
advertising[&#39;TV&#39;]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;0      230.1
1       44.5
2       17.2
3      151.5
4      180.8
       ...  
195     38.2
196     94.2
197    177.0
198    283.6
199    232.1
Name: TV, Length: 200, dtype: float64
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# 2. Brackets
advertising.TV
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;0      230.1
1       44.5
2       17.2
3      151.5
4      180.8
       ...  
195     38.2
196     94.2
197    177.0
198    283.6
199    232.1
Name: TV, Length: 200, dtype: float64
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# The loc function
advertising.loc[:,&#39;TV&#39;]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;0      230.1
1       44.5
2       17.2
3      151.5
4      180.8
       ...  
195     38.2
196     94.2
197    177.0
198    283.6
199    232.1
Name: TV, Length: 200, dtype: float64
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that the &lt;code&gt;loc&lt;/code&gt; function is more powerful and is generally used to subset lines and columns.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Select multiple columns and subset of rows
advertising.loc[0:5,[&#39;Sales&#39;,&#39;TV&#39;]]
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;Sales&lt;/th&gt;
      &lt;th&gt;TV&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;22.1&lt;/td&gt;
      &lt;td&gt;230.1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;10.4&lt;/td&gt;
      &lt;td&gt;44.5&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;9.3&lt;/td&gt;
      &lt;td&gt;17.2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;18.5&lt;/td&gt;
      &lt;td&gt;151.5&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;12.9&lt;/td&gt;
      &lt;td&gt;180.8&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;5&lt;/th&gt;
      &lt;td&gt;7.2&lt;/td&gt;
      &lt;td&gt;8.7&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;Suppose we are interested in the (linear) relationship between sales and tv advertisement.&lt;/p&gt;
&lt;p&gt;$$
sales ≈ \beta_0 + \beta_1 TV.
$$&lt;/p&gt;
&lt;p&gt;How are the two two variables related? Visual inspection: scatterplot.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Figure 3.1
def make_fig_3_1a():
    
    # Init figure
    fig, ax = plt.subplots(1,1)
    ax.set_title(&#39;Figure 3.1&#39;);

    # Plot scatter and best fit line
    sns.regplot(x=advertising.TV, y=advertising.Sales, ax=ax, order=1, ci=None, scatter_kws={&#39;color&#39;:&#39;r&#39;, &#39;s&#39;:20})
    ax.set_xlim(-10,310); ax.set_ylim(ymin=0)
    ax.legend([&#39;Least Squares Fit&#39;,&#39;Data&#39;]);
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;make_fig_3_1a()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/01_regression_23_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;estimating-the-coefficients&#34;&gt;Estimating the Coefficients&lt;/h3&gt;
&lt;p&gt;How do we estimate the best fit line? Minimize the Residual Sum of Squares (RSS).&lt;/p&gt;
&lt;p&gt;First, suppose we have a dataset $\mathcal D = {x_i, y_i}_{i=1}^N$. We define the prediction of $y$ based on $X$ as&lt;/p&gt;
&lt;p&gt;$$
\hat y_i = \hat \beta X_i
$$&lt;/p&gt;
&lt;p&gt;The residuals are the unexplained component of $y$&lt;/p&gt;
&lt;p&gt;$$
e_i = y_i - \hat y_i
$$&lt;/p&gt;
&lt;p&gt;Our objective function (to be minimized) is the Resdual Sum of Squares (RSS):&lt;/p&gt;
&lt;p&gt;$$
RSS := \sum_{n=1}^N e_i^2
$$&lt;/p&gt;
&lt;p&gt;And the OLS coefficient is defined as its minimizer:&lt;/p&gt;
&lt;p&gt;$$
\hat \beta_{OLS} := \arg\min_{\beta} \sum_{n=1}^N e_i^2 = \arg\min_{\beta} \sum_{n=1}^N (y_i - X_i \beta)^2
$$&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s use the &lt;code&gt;sklearn&lt;/code&gt; library to fit a linear regression model of &lt;em&gt;Sales&lt;/em&gt; on &lt;em&gt;TV&lt;/em&gt; advertisement.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Define X and y
X = advertising.TV.values.reshape(-1,1)
y = advertising.Sales.values

# Fit linear regressions
reg = LinearRegression().fit(X,y)
print(reg.intercept_)
print(reg.coef_)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;7.0325935491276885
[0.04753664]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can visualize the residuals as the vertical distances between the data and the prediction line. The objective function RSS is the sum of the squares of the lengths of vertical lines.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Compute predicted values
y_hat = reg.predict(X)

# Figure 3.1
def make_figure_3_1b():

    # Init figure
    fig, ax = plt.subplots(1,1)
    ax.set_title(&#39;Figure 3.1&#39;);

    # Add residuals
    sns.regplot(x=advertising.TV, y=advertising.Sales, ax=ax, order=1, ci=None, scatter_kws={&#39;color&#39;:&#39;r&#39;, &#39;s&#39;:20})
    ax.vlines(X, np.minimum(y,y_hat), np.maximum(y,y_hat), linestyle=&#39;--&#39;, color=&#39;k&#39;, alpha=0.5, linewidth=1)
    plt.legend([&#39;Least Squares Fit&#39;,&#39;Data&#39;,&#39;Residuals&#39;]);
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;make_figure_3_1b()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/01_regression_29_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;The closed form solution in matrix algebra is
$$
\hat \beta_{OLS} = (X&amp;rsquo;X)^{-1}(X&amp;rsquo;y)
$$&lt;/p&gt;
&lt;p&gt;Python has a series of shortcuts to make the syntax less verbose. However, we still need to import the &lt;code&gt;inv&lt;/code&gt; function from &lt;code&gt;numpy&lt;/code&gt;. In Matlab it would be &lt;code&gt;(X&#39;*X)^{-1}*(X&#39;*y)&lt;/code&gt;, almost literal.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Compute OLS coefficient with matrix algebra
beta = inv(X.T @ X) @ X.T @ y

print(beta)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[0.08324961]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Why is the result different?&lt;/p&gt;
&lt;p&gt;We are missing one coefficient: the intercept. Our regression now looks like this&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# New figure 1
def make_new_figure_1():

    # Init figure
    fig, ax = plt.subplots(1,1)
    fig.suptitle(&#39;Role of the Intercept&#39;)

    # Add new line on the previous plot
    sns.regplot(x=advertising.TV, y=advertising.Sales, ax=ax, order=1, ci=None, scatter_kws={&#39;color&#39;:&#39;r&#39;, &#39;s&#39;:10})
    ax.plot(X, beta*X, color=&#39;g&#39;)
    plt.xlim(-10,310); plt.ylim(ymin=0);
    ax.legend([&#39;With Intercept&#39;, &#39;Without intercept&#39;]);
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;make_new_figure_1()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/01_regression_34_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;How do we insert an intercept using matrix algebra? We add a column of ones.&lt;/p&gt;
&lt;p&gt;$$
X_1 = [\boldsymbol{1}, X]
$$&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# How to insert intercept? Add constant: column of ones
one = np.ones(np.shape(X))
X1 = np.concatenate([one,X],axis=1)

print(np.shape(X1))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;(200, 2)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we compute again the coefficients as before.&lt;/p&gt;
&lt;p&gt;$$
\hat \beta_{OLS} = (X_1&amp;rsquo;X_1)^{-1}(X_1&amp;rsquo;y)
$$&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Compute beta OLS with intercept
beta_OLS = inv(X1.T @ X1) @ X1.T @ y

print(beta_OLS)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[7.03259355 0.04753664]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we have indeed obtained the same exact coefficients.&lt;/p&gt;
&lt;p&gt;What does minimizing the Residual Sum of Squares means in practice? How does the objective function looks like?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from sklearn.preprocessing import scale

# First, scale the data
X = scale(advertising.TV, with_mean=True, with_std=False).reshape(-1,1)
y = advertising.Sales
regr = LinearRegression().fit(X,y)

# Create grid coordinates for plotting
B0 = np.linspace(regr.intercept_-2, regr.intercept_+2, 50)
B1 = np.linspace(regr.coef_-0.02, regr.coef_+0.02, 50)
xx, yy = np.meshgrid(B0, B1, indexing=&#39;xy&#39;)
Z = np.zeros((B0.size,B1.size))

# Calculate Z-values (RSS) based on grid of coefficients
for (i,j),v in np.ndenumerate(Z):
    Z[i,j] =((y - (xx[i,j]+X.ravel()*yy[i,j]))**2).sum()/1000

# Minimized RSS
min_RSS = r&#39;$\beta_0$, $\beta_1$ for minimized RSS&#39;
min_rss = np.sum((regr.intercept_+regr.coef_*X - y.values.reshape(-1,1))**2)/1000
min_rss
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;2.1025305831313514
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Figure 3.2 - Regression coefficients - RSS
def make_fig_3_2():
    fig = plt.figure(figsize=(15,6))
    fig.suptitle(&#39;RSS - Regression coefficients&#39;)

    ax1 = fig.add_subplot(121)
    ax2 = fig.add_subplot(122, projection=&#39;3d&#39;)

    # Left plot
    CS = ax1.contour(xx, yy, Z, cmap=plt.cm.Set1, levels=[2.15, 2.2, 2.3, 2.5, 3])
    ax1.scatter(regr.intercept_, regr.coef_[0], c=&#39;r&#39;, label=min_RSS)
    ax1.clabel(CS, inline=True, fontsize=10, fmt=&#39;%1.1f&#39;)

    # Right plot
    ax2.plot_surface(xx, yy, Z, rstride=3, cstride=3, alpha=0.3)
    ax2.contour(xx, yy, Z, zdir=&#39;z&#39;, offset=Z.min(), cmap=plt.cm.Set1,
                alpha=0.4, levels=[2.15, 2.2, 2.3, 2.5, 3])
    ax2.scatter3D(regr.intercept_, regr.coef_[0], min_rss, c=&#39;r&#39;, label=min_RSS)
    ax2.set_zlabel(&#39;RSS&#39;)
    ax2.set_zlim(Z.min(),Z.max())
    ax2.set_ylim(0.02,0.07)

    # settings common to both plots
    for ax in fig.axes:
        ax.set_xlabel(r&#39;$\beta_0$&#39;)
        ax.set_ylabel(r&#39;$\beta_1$&#39;)
        ax.set_yticks([0.03,0.04,0.05,0.06])
        ax.legend()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;make_fig_3_2()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/01_regression_43_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;assessing-the-accuracy-of-the-coefficient-estimates&#34;&gt;Assessing the Accuracy of the Coefficient Estimates&lt;/h3&gt;
&lt;p&gt;How accurate is our regression fit? Suppose we were drawing different (small) samples from the same data generating process, for example&lt;/p&gt;
&lt;p&gt;$$
y_i = 2 + 3x_i + \varepsilon_i
$$&lt;/p&gt;
&lt;p&gt;where $x_i \sim N(0,1)$ and $\varepsilon \sim N(0,3)$.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Init
N = 30;    # Sample size
K = 100;   # Number of simulations
beta_hat = np.zeros((2,K))
x = np.linspace(-4,4,N)

# Set seed
np.random.seed(1)

# K simulations
for i in range(K):
    # Simulate data
    x1 = np.random.normal(0,1,N).reshape([-1,1])
    X = np.concatenate([np.ones(np.shape(x1)), x1], axis=1)
    epsilon = np.random.normal(0,5,N)
    beta0 = [2,3]
    y = X @ beta0 + epsilon

    # Estimate coefficients
    beta_hat[:,i] = inv(X.T @ X) @ X.T @ y
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# new figure 2
def make_new_fig_2():

    # Init figure
    fig, ax = plt.subplots(1,1)
    
    for i in range(K):
        # Plot line
        ax.plot(x, beta_hat[0,i] + x*beta_hat[1,i], color=&#39;blue&#39;, alpha=0.2, linewidth=1)
        if i==K-1:
            ax.plot(x, beta_hat[0,i] + x*beta_hat[1,i], color=&#39;blue&#39;, alpha=0.2, linewidth=1, label=&#39;Estimated Lines&#39;)

    # Plot true line
    ax.plot(x, 2 + 3*x, color=&#39;red&#39;, linewidth=3, label=&#39;True Line&#39;);
    ax.set_xlabel(&#39;X&#39;); ax.set_ylabel(&#39;Y&#39;); ax.legend();
    ax.set_xlim(-4,4);
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;make_new_fig_2()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/01_regression_48_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;regplot&lt;/code&gt; command lets us automatically draw confidence intervals. Let&amp;rsquo;s draw the last simulated dataset with conficence intervals.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;fig, ax = plt.subplots(1,1)

# Plot last simulation scatterplot with confidence interval
sns.regplot(x=x1, y=y, ax=ax, order=1, scatter_kws={&#39;color&#39;:&#39;r&#39;, &#39;s&#39;:20});
ax.set_xlabel(&#39;X&#39;); ax.set_ylabel(&#39;Y&#39;); 
ax.legend([&#39;Best fit&#39;,&#39;Data&#39;, &#39;Confidence Intervals&#39;]);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/01_regression_50_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;As we can see, depending on the sample, we get a different estimate of the linear relationship between $x$ and $y$. However, there estimates are on average correct. Indeed, we can visualize their distribution.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Plot distribution of coefficients
plot = sns.jointplot(x=beta_hat[0,:], y=beta_hat[1,:], color=&#39;red&#39;, edgecolor=&amp;quot;white&amp;quot;);
plot.ax_joint.axvline(x=2);
plot.ax_joint.axhline(y=3);
plot.set_axis_labels(&#39;beta_0&#39;, &#39;beta_1&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/01_regression_52_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;How do we compute confidence intervals by hand?&lt;/p&gt;
&lt;p&gt;$$
Var(\hat \beta_{OLS}) = \sigma^2 (X&amp;rsquo;X)^{-1}
$$&lt;/p&gt;
&lt;p&gt;where $\sigma^2 = Var(\varepsilon)$. Since we do not know $Var(\varepsilon)$, we estimate it as $Var(e)$.&lt;/p&gt;
&lt;p&gt;$$
\hat Var(\hat \beta_{OLS}) = \hat \sigma^2 (X&amp;rsquo;X)^{-1}
$$&lt;/p&gt;
&lt;p&gt;If we assume the standard errors are normally distributed (or we apply the Central Limit Theorem, assuming $n \to \infty$), a 95% confidence interval for the OLS coefficient takes the form&lt;/p&gt;
&lt;p&gt;$$
CI(\hat \beta_{OLS}) = \Big[ \hat \beta_{OLS} - 1.96 \times \hat SE(\hat \beta_{OLS}) \ , \ \hat \beta_{OLS} + 1.96 \times \hat SE(\hat \beta_{OLS}) \Big]
$$&lt;/p&gt;
&lt;p&gt;where $\hat SE(\hat \beta_{OLS}) = \sqrt{\hat Var(\hat \beta_{OLS})}$.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Import again X and y from example above
X = advertising.TV.values.reshape(-1,1)
X1 = np.concatenate([np.ones(np.shape(X)), X], axis=1)
y = advertising.Sales.values

# Compute residual variance
X_hat = X1 @ beta_OLS
e = y - X_hat
sigma_hat = np.var(e)
var_beta_OLS = sigma_hat * inv(X1.T @ X1)

# Take elements on the diagonal and square them
std_beta_OLS = [var_beta_OLS[0,0]**.5, var_beta_OLS[1,1]**.5]

print(std_beta_OLS)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[0.4555479737400674, 0.0026771203500466564]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;statsmodels&lt;/code&gt; library allows us to produce nice tables with parameter estimates and standard errors.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Table 3.1 &amp;amp; 3.2
est = sm.OLS.from_formula(&#39;Sales ~ TV&#39;, advertising).fit()
est.summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
      &lt;td&gt;&lt;/td&gt;         &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt; &lt;td&gt;    7.0326&lt;/td&gt; &lt;td&gt;    0.458&lt;/td&gt; &lt;td&gt;   15.360&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    6.130&lt;/td&gt; &lt;td&gt;    7.935&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;TV&lt;/th&gt;        &lt;td&gt;    0.0475&lt;/td&gt; &lt;td&gt;    0.003&lt;/td&gt; &lt;td&gt;   17.668&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    0.042&lt;/td&gt; &lt;td&gt;    0.053&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;h3 id=&#34;assessing-the-accuracy-of-the-model&#34;&gt;Assessing the Accuracy of the Model&lt;/h3&gt;
&lt;p&gt;What metrics can we use to assess whether the model is a good model, in terms of capturing the relationship between the variables?&lt;/p&gt;
&lt;p&gt;First, we can compute our objective function: the Residual Sum of Squares (&lt;em&gt;RSS&lt;/em&gt;). Lower values of our objective function imply that we got a better fit.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# RSS with regression coefficients
RSS = sum(e**2)

print(RSS)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;2102.530583131351
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The problem with &lt;em&gt;RSS&lt;/em&gt; as a metric is that it&amp;rsquo;s hard to compare different regressions since its scale depends on the magnitude of the variables.&lt;/p&gt;
&lt;p&gt;One measure of fit that does not depend on the magnitude of the variables is $R^2$: the percentage of our explanatory variable explained by the model&lt;/p&gt;
&lt;p&gt;$$
R^2 = 1 - \frac{\text{RSS}}{\text{TSS}}
$$&lt;/p&gt;
&lt;p&gt;where&lt;/p&gt;
&lt;p&gt;$$
TSS = \sum_{i=1}^N (y_i - \bar y)^2
$$&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# TSS
TSS = sum( (y-np.mean(y))**2 )

# R2
R2 = 1 - RSS/TSS

print(R2)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;0.6118750508500709
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Can the $R^2$ metric be negative? When?&lt;/p&gt;
&lt;h2 id=&#34;22-multiple-linear-regression&#34;&gt;2.2 Multiple Linear Regression&lt;/h2&gt;
&lt;p&gt;What if we have more than one explanatory variable? Spoiler: we already did, but one was a constant.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s have a look at the regression of &lt;em&gt;Sales&lt;/em&gt; on &lt;em&gt;Radio&lt;/em&gt; and &lt;em&gt;TV&lt;/em&gt; advertisement expenditure separately.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Table 3.3 (1)
est = sm.OLS.from_formula(&#39;Sales ~ Radio&#39;, advertising).fit()
est.summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
      &lt;td&gt;&lt;/td&gt;         &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt; &lt;td&gt;    9.3116&lt;/td&gt; &lt;td&gt;    0.563&lt;/td&gt; &lt;td&gt;   16.542&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    8.202&lt;/td&gt; &lt;td&gt;   10.422&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Radio&lt;/th&gt;     &lt;td&gt;    0.2025&lt;/td&gt; &lt;td&gt;    0.020&lt;/td&gt; &lt;td&gt;    9.921&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    0.162&lt;/td&gt; &lt;td&gt;    0.243&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Table 3.3 (2)
est = sm.OLS.from_formula(&#39;Sales ~ Newspaper&#39;, advertising).fit()
est.summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
      &lt;td&gt;&lt;/td&gt;         &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt; &lt;td&gt;   12.3514&lt;/td&gt; &lt;td&gt;    0.621&lt;/td&gt; &lt;td&gt;   19.876&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;   11.126&lt;/td&gt; &lt;td&gt;   13.577&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Newspaper&lt;/th&gt; &lt;td&gt;    0.0547&lt;/td&gt; &lt;td&gt;    0.017&lt;/td&gt; &lt;td&gt;    3.300&lt;/td&gt; &lt;td&gt; 0.001&lt;/td&gt; &lt;td&gt;    0.022&lt;/td&gt; &lt;td&gt;    0.087&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;It seems that both Radio and Newspapers are positively correlated with &lt;em&gt;Sales&lt;/em&gt;. Why don&amp;rsquo;t we estimate a unique regression with both dependent variables?&lt;/p&gt;
&lt;h3 id=&#34;estimating-the-regression-coefficients&#34;&gt;Estimating the Regression Coefficients&lt;/h3&gt;
&lt;p&gt;Suppose now we enrich our previous model adding all different forms of advertisement:&lt;/p&gt;
&lt;p&gt;$$
\text{Sales} = \beta_0 + \beta_1 \text{TV} + \beta_2 \text{Radio} + \beta_3 \text{Newspaper} + \varepsilon
$$&lt;/p&gt;
&lt;p&gt;We estimate it using the &lt;code&gt;statsmodels&lt;/code&gt; &lt;code&gt;ols&lt;/code&gt; library.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Table 3.4
est = sm.OLS.from_formula(&#39;Sales ~ TV + Radio + Newspaper&#39;, advertising).fit()
est.summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
      &lt;td&gt;&lt;/td&gt;         &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt; &lt;td&gt;    2.9389&lt;/td&gt; &lt;td&gt;    0.312&lt;/td&gt; &lt;td&gt;    9.422&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    2.324&lt;/td&gt; &lt;td&gt;    3.554&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;TV&lt;/th&gt;        &lt;td&gt;    0.0458&lt;/td&gt; &lt;td&gt;    0.001&lt;/td&gt; &lt;td&gt;   32.809&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    0.043&lt;/td&gt; &lt;td&gt;    0.049&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Radio&lt;/th&gt;     &lt;td&gt;    0.1885&lt;/td&gt; &lt;td&gt;    0.009&lt;/td&gt; &lt;td&gt;   21.893&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    0.172&lt;/td&gt; &lt;td&gt;    0.206&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Newspaper&lt;/th&gt; &lt;td&gt;   -0.0010&lt;/td&gt; &lt;td&gt;    0.006&lt;/td&gt; &lt;td&gt;   -0.177&lt;/td&gt; &lt;td&gt; 0.860&lt;/td&gt; &lt;td&gt;   -0.013&lt;/td&gt; &lt;td&gt;    0.011&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;Why now it seems that there is no relationship between Sales and Newspaper while the univariate regression told us the opposite?&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s explore the correlation between those variables.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Table 3.5 - Correlation Matrix
advertising.corr()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;TV&lt;/th&gt;
      &lt;th&gt;Radio&lt;/th&gt;
      &lt;th&gt;Newspaper&lt;/th&gt;
      &lt;th&gt;Sales&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;TV&lt;/th&gt;
      &lt;td&gt;1.000000&lt;/td&gt;
      &lt;td&gt;0.054809&lt;/td&gt;
      &lt;td&gt;0.056648&lt;/td&gt;
      &lt;td&gt;0.782224&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;Radio&lt;/th&gt;
      &lt;td&gt;0.054809&lt;/td&gt;
      &lt;td&gt;1.000000&lt;/td&gt;
      &lt;td&gt;0.354104&lt;/td&gt;
      &lt;td&gt;0.576223&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;Newspaper&lt;/th&gt;
      &lt;td&gt;0.056648&lt;/td&gt;
      &lt;td&gt;0.354104&lt;/td&gt;
      &lt;td&gt;1.000000&lt;/td&gt;
      &lt;td&gt;0.228299&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;Sales&lt;/th&gt;
      &lt;td&gt;0.782224&lt;/td&gt;
      &lt;td&gt;0.576223&lt;/td&gt;
      &lt;td&gt;0.228299&lt;/td&gt;
      &lt;td&gt;1.000000&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;Let&amp;rsquo;s try to inspect the relationship visually. Note that now the linear best fit is going to be 3-dimensional. In order to make it visually accessible, we consider only on &lt;em&gt;TV&lt;/em&gt; and &lt;em&gt;Radio&lt;/em&gt; advertisement expediture as dependent variables. The best fit will be a plane instead of a line.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Fit regression
est = sm.OLS.from_formula(&#39;Sales ~ Radio + TV&#39;, advertising).fit()
print(est.params)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Intercept    2.921100
Radio        0.187994
TV           0.045755
dtype: float64
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Create a coordinate grid
Radio = np.arange(0,50)
TV = np.arange(0,300)
B1, B2 = np.meshgrid(Radio, TV, indexing=&#39;xy&#39;)

# Compute predicted plane
Z = np.zeros((TV.size, Radio.size))
for (i,j),v in np.ndenumerate(Z):
        Z[i,j] =(est.params[0] + B1[i,j]*est.params[1] + B2[i,j]*est.params[2])
        
# Compute residuals
e = est.predict() - advertising.Sales
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Figure 3.5 - Multiple Linear Regression
def make_fig_3_5():

    # Init figure
    fig = plt.figure()
    ax = axes3d.Axes3D(fig, auto_add_to_figure=False)
    fig.add_axes(ax)
    fig.suptitle(&#39;Figure 3.5&#39;);


    # Plot best fit plane
    ax.plot_surface(B1, B2, Z, color=&#39;k&#39;, alpha=0.3)
    points = ax.scatter3D(advertising.Radio, advertising.TV, advertising.Sales, c=e, cmap=&amp;quot;seismic&amp;quot;, vmin=-5, vmax=5)
    plt.colorbar(points, cax=fig.add_axes([0.9, 0.1, 0.03, 0.8]))
    ax.set_xlabel(&#39;Radio&#39;); ax.set_xlim(0,50)
    ax.set_ylabel(&#39;TV&#39;); ax.set_ylim(bottom=0)
    ax.set_zlabel(&#39;Sales&#39;);
    ax.view_init(20, 20)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;make_fig_3_5()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/01_regression_79_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;some-important-questions&#34;&gt;Some Important Questions&lt;/h3&gt;
&lt;p&gt;How do you check whether the model fit well the data with multiple regressors? &lt;code&gt;statmodels&lt;/code&gt; and most regression packages automatically outputs more information about the least squares model.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Measires of fit
est.summary().tables[0]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;caption&gt;OLS Regression Results&lt;/caption&gt;
&lt;tr&gt;
  &lt;th&gt;Dep. Variable:&lt;/th&gt;          &lt;td&gt;Sales&lt;/td&gt;      &lt;th&gt;  R-squared:         &lt;/th&gt; &lt;td&gt;   0.897&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Model:&lt;/th&gt;                   &lt;td&gt;OLS&lt;/td&gt;       &lt;th&gt;  Adj. R-squared:    &lt;/th&gt; &lt;td&gt;   0.896&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Method:&lt;/th&gt;             &lt;td&gt;Least Squares&lt;/td&gt;  &lt;th&gt;  F-statistic:       &lt;/th&gt; &lt;td&gt;   859.6&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Date:&lt;/th&gt;             &lt;td&gt;Mon, 03 Jan 2022&lt;/td&gt; &lt;th&gt;  Prob (F-statistic):&lt;/th&gt; &lt;td&gt;4.83e-98&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Time:&lt;/th&gt;                 &lt;td&gt;18:28:21&lt;/td&gt;     &lt;th&gt;  Log-Likelihood:    &lt;/th&gt; &lt;td&gt; -386.20&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;No. Observations:&lt;/th&gt;      &lt;td&gt;   200&lt;/td&gt;      &lt;th&gt;  AIC:               &lt;/th&gt; &lt;td&gt;   778.4&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Df Residuals:&lt;/th&gt;          &lt;td&gt;   197&lt;/td&gt;      &lt;th&gt;  BIC:               &lt;/th&gt; &lt;td&gt;   788.3&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Df Model:&lt;/th&gt;              &lt;td&gt;     2&lt;/td&gt;      &lt;th&gt;                     &lt;/th&gt;     &lt;td&gt; &lt;/td&gt;   
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Covariance Type:&lt;/th&gt;      &lt;td&gt;nonrobust&lt;/td&gt;    &lt;th&gt;                     &lt;/th&gt;     &lt;td&gt; &lt;/td&gt;   
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;First measure: the &lt;strong&gt;F-test&lt;/strong&gt;. The F-test tries to answe the question &amp;ldquo;&lt;em&gt;Is There a Relationship Between the Response and Predictors?&lt;/em&gt;&amp;rdquo;&lt;/p&gt;
&lt;p&gt;In particular, it tests the following hypothesis&lt;/p&gt;
&lt;p&gt;$$
H_1: \text{is at least one coefficient different from zero?}
$$&lt;/p&gt;
&lt;p&gt;against the null hypothesis&lt;/p&gt;
&lt;p&gt;$$
H_0: \beta_0 = \beta_1 = &amp;hellip; = 0
$$&lt;/p&gt;
&lt;p&gt;This hypothesis test is performed by computing the F-statistic,&lt;/p&gt;
&lt;p&gt;$$
F=\frac{(\mathrm{TSS}-\mathrm{RSS}) / p}{\operatorname{RSS} /(n-p-1)}
$$&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s try to compute it by hand.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Init
X = advertising[[&#39;Radio&#39;, &#39;TV&#39;]]
y = advertising.Sales
e = y - est.predict(X)
RSS = np.sum(e**2)
TSS = np.sum((y - np.mean(y))**2)
(n,p) = np.shape(X)

# Compute F
F = ((TSS - RSS)/p) / (RSS/(n-p-1))
print(&#39;F = %.4f&#39; % F)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;F = 859.6177
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;A rule of thumb is to reject $H_0$ if $F &amp;gt; 10$.&lt;/p&gt;
&lt;p&gt;We can also test that a particular subset of coefficients are equal to zero. In that case, we just substitute the Total Sum of Squares (TSS) with the Residual Sum of Squares under the null.&lt;/p&gt;
&lt;p&gt;$$
F=\frac{(\mathrm{RSS_0}-\mathrm{RSS}) / p}{\operatorname{RSS} /(n-p-1)}
$$&lt;/p&gt;
&lt;p&gt;i.e. we perfome the regression under the null hypothesis and we compute&lt;/p&gt;
&lt;p&gt;$$
RSS_0 = \sum_{n=1}^N (y_i - X_i \beta)^2 \quad s.t. \quad  H_0
$$&lt;/p&gt;
&lt;h2 id=&#34;23-other-considerations-in-the-regression-model&#34;&gt;2.3 Other Considerations in the Regression Model&lt;/h2&gt;
&lt;h3 id=&#34;qualitative-predictors&#34;&gt;Qualitative Predictors&lt;/h3&gt;
&lt;p&gt;What if some variables are qualitative instead of quantitative? Let&amp;rsquo;s change dataset and use the &lt;code&gt;credit&lt;/code&gt; dataset.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Credit ratings dataset
credit = pd.read_csv(&#39;data/Credit.csv&#39;, usecols=list(range(1,12)))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This dataset contains information on credit ratings, i.e. each person is assigned a &lt;code&gt;Rating&lt;/code&gt; score based on his/her own individual characteristics.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s have a look at data types.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Summary
credit.info()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;class &#39;pandas.core.frame.DataFrame&#39;&amp;gt;
RangeIndex: 400 entries, 0 to 399
Data columns (total 11 columns):
 #   Column     Non-Null Count  Dtype  
---  ------     --------------  -----  
 0   Income     400 non-null    float64
 1   Limit      400 non-null    int64  
 2   Rating     400 non-null    int64  
 3   Cards      400 non-null    int64  
 4   Age        400 non-null    int64  
 5   Education  400 non-null    int64  
 6   Gender     400 non-null    object 
 7   Student    400 non-null    object 
 8   Married    400 non-null    object 
 9   Ethnicity  400 non-null    object 
 10  Balance    400 non-null    int64  
dtypes: float64(1), int64(6), object(4)
memory usage: 34.5+ KB
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As we can see, some variables like &lt;code&gt;Gender&lt;/code&gt;, &lt;code&gt;Student&lt;/code&gt; or &lt;code&gt;Married&lt;/code&gt; are not numeric.&lt;/p&gt;
&lt;p&gt;We can have a closer look at what these variables look like.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Look at data
credit.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;Income&lt;/th&gt;
      &lt;th&gt;Limit&lt;/th&gt;
      &lt;th&gt;Rating&lt;/th&gt;
      &lt;th&gt;Cards&lt;/th&gt;
      &lt;th&gt;Age&lt;/th&gt;
      &lt;th&gt;Education&lt;/th&gt;
      &lt;th&gt;Gender&lt;/th&gt;
      &lt;th&gt;Student&lt;/th&gt;
      &lt;th&gt;Married&lt;/th&gt;
      &lt;th&gt;Ethnicity&lt;/th&gt;
      &lt;th&gt;Balance&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;14.891&lt;/td&gt;
      &lt;td&gt;3606&lt;/td&gt;
      &lt;td&gt;283&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;34&lt;/td&gt;
      &lt;td&gt;11&lt;/td&gt;
      &lt;td&gt;Male&lt;/td&gt;
      &lt;td&gt;No&lt;/td&gt;
      &lt;td&gt;Yes&lt;/td&gt;
      &lt;td&gt;Caucasian&lt;/td&gt;
      &lt;td&gt;333&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;106.025&lt;/td&gt;
      &lt;td&gt;6645&lt;/td&gt;
      &lt;td&gt;483&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;82&lt;/td&gt;
      &lt;td&gt;15&lt;/td&gt;
      &lt;td&gt;Female&lt;/td&gt;
      &lt;td&gt;Yes&lt;/td&gt;
      &lt;td&gt;Yes&lt;/td&gt;
      &lt;td&gt;Asian&lt;/td&gt;
      &lt;td&gt;903&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;104.593&lt;/td&gt;
      &lt;td&gt;7075&lt;/td&gt;
      &lt;td&gt;514&lt;/td&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;71&lt;/td&gt;
      &lt;td&gt;11&lt;/td&gt;
      &lt;td&gt;Male&lt;/td&gt;
      &lt;td&gt;No&lt;/td&gt;
      &lt;td&gt;No&lt;/td&gt;
      &lt;td&gt;Asian&lt;/td&gt;
      &lt;td&gt;580&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;148.924&lt;/td&gt;
      &lt;td&gt;9504&lt;/td&gt;
      &lt;td&gt;681&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;36&lt;/td&gt;
      &lt;td&gt;11&lt;/td&gt;
      &lt;td&gt;Female&lt;/td&gt;
      &lt;td&gt;No&lt;/td&gt;
      &lt;td&gt;No&lt;/td&gt;
      &lt;td&gt;Asian&lt;/td&gt;
      &lt;td&gt;964&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;55.882&lt;/td&gt;
      &lt;td&gt;4897&lt;/td&gt;
      &lt;td&gt;357&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;68&lt;/td&gt;
      &lt;td&gt;16&lt;/td&gt;
      &lt;td&gt;Male&lt;/td&gt;
      &lt;td&gt;No&lt;/td&gt;
      &lt;td&gt;Yes&lt;/td&gt;
      &lt;td&gt;Caucasian&lt;/td&gt;
      &lt;td&gt;331&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;Let&amp;rsquo;s consider the variable &lt;code&gt;Student&lt;/code&gt;. From a quick inspection it looks like it&amp;rsquo;s a binary &lt;em&gt;Yes/No&lt;/em&gt; variable. Let&amp;rsquo;s check by listing all its values.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# What values does the Student variable take?
credit[&#39;Student&#39;].unique()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;array([&#39;No&#39;, &#39;Yes&#39;], dtype=object)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What happens if you pass a binary varaible to &lt;code&gt;statsmodel&lt;/code&gt;? It automatically generates a dummy out of it.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Table 3.7
est = sm.OLS.from_formula(&#39;Balance ~ Student&#39;, credit).fit()
est.summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
         &lt;td&gt;&lt;/td&gt;           &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt;      &lt;td&gt;  480.3694&lt;/td&gt; &lt;td&gt;   23.434&lt;/td&gt; &lt;td&gt;   20.499&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;  434.300&lt;/td&gt; &lt;td&gt;  526.439&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Student[T.Yes]&lt;/th&gt; &lt;td&gt;  396.4556&lt;/td&gt; &lt;td&gt;   74.104&lt;/td&gt; &lt;td&gt;    5.350&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;  250.771&lt;/td&gt; &lt;td&gt;  542.140&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;If a variable takes more than one value, &lt;code&gt;statsmodel&lt;/code&gt; automatically generates a uniqe dummy for each level (-1).&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Table 3.8
est = sm.OLS.from_formula(&#39;Balance ~ Ethnicity&#39;, credit).fit()
est.summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
             &lt;td&gt;&lt;/td&gt;               &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt;              &lt;td&gt;  531.0000&lt;/td&gt; &lt;td&gt;   46.319&lt;/td&gt; &lt;td&gt;   11.464&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;  439.939&lt;/td&gt; &lt;td&gt;  622.061&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Ethnicity[T.Asian]&lt;/th&gt;     &lt;td&gt;  -18.6863&lt;/td&gt; &lt;td&gt;   65.021&lt;/td&gt; &lt;td&gt;   -0.287&lt;/td&gt; &lt;td&gt; 0.774&lt;/td&gt; &lt;td&gt; -146.515&lt;/td&gt; &lt;td&gt;  109.142&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Ethnicity[T.Caucasian]&lt;/th&gt; &lt;td&gt;  -12.5025&lt;/td&gt; &lt;td&gt;   56.681&lt;/td&gt; &lt;td&gt;   -0.221&lt;/td&gt; &lt;td&gt; 0.826&lt;/td&gt; &lt;td&gt; -123.935&lt;/td&gt; &lt;td&gt;   98.930&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;h3 id=&#34;relaxing-the-additive-assumption&#34;&gt;Relaxing the Additive Assumption&lt;/h3&gt;
&lt;p&gt;We have seen that both TV and Radio advertisement are positively associated with Sales. What if there is a synergy? For example it might be that if someone sees an ad &lt;em&gt;both&lt;/em&gt; on TV and on the radio, s/he is much more likely to buy the product.&lt;/p&gt;
&lt;p&gt;Consider the following model&lt;/p&gt;
&lt;p&gt;$$
\text{Sales} ≈ \beta_0 + \beta_1 \text{TV} + \beta_2 \text{Radio} + \beta_3 \text{TV} \times \text{Radio}
$$&lt;/p&gt;
&lt;p&gt;which can be rewritten as&lt;/p&gt;
&lt;p&gt;$$
\text{Sales} ≈ \beta_0 + (\beta_1 + \beta_3 \text{Radio}) \times \text{TV} + \beta_2 \text{Radio}
$$&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s estimate the linear regression model, with the intercept.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Table 3.9 - Interaction Variables
est = sm.OLS.from_formula(&#39;Sales ~ TV + Radio + TV*Radio&#39;, advertising).fit()
est.summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
      &lt;td&gt;&lt;/td&gt;         &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt; &lt;td&gt;    6.7502&lt;/td&gt; &lt;td&gt;    0.248&lt;/td&gt; &lt;td&gt;   27.233&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    6.261&lt;/td&gt; &lt;td&gt;    7.239&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;TV&lt;/th&gt;        &lt;td&gt;    0.0191&lt;/td&gt; &lt;td&gt;    0.002&lt;/td&gt; &lt;td&gt;   12.699&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    0.016&lt;/td&gt; &lt;td&gt;    0.022&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Radio&lt;/th&gt;     &lt;td&gt;    0.0289&lt;/td&gt; &lt;td&gt;    0.009&lt;/td&gt; &lt;td&gt;    3.241&lt;/td&gt; &lt;td&gt; 0.001&lt;/td&gt; &lt;td&gt;    0.011&lt;/td&gt; &lt;td&gt;    0.046&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;TV:Radio&lt;/th&gt;  &lt;td&gt;    0.0011&lt;/td&gt; &lt;td&gt; 5.24e-05&lt;/td&gt; &lt;td&gt;   20.727&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    0.001&lt;/td&gt; &lt;td&gt;    0.001&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;A positive and significant interaction term indicates a hint of a sinergy effect.&lt;/p&gt;
&lt;h3 id=&#34;heterogeneous-effects&#34;&gt;Heterogeneous Effects&lt;/h3&gt;
&lt;p&gt;We can do interactions with qualitative variables as well. Conside the credit rating dataset.&lt;/p&gt;
&lt;p&gt;What if &lt;code&gt;Balance&lt;/code&gt; depends by &lt;code&gt;Income&lt;/code&gt; differently, depending on whether one is a &lt;code&gt;Student&lt;/code&gt; or not?&lt;/p&gt;
&lt;p&gt;Consider the following model:&lt;/p&gt;
&lt;p&gt;$$
\text{Balance} ≈ \beta_0 + \beta_1 \text{Income} + \beta_2 \text{Student} + \beta_3 \text{Income} \times \text{Student}
$$&lt;/p&gt;
&lt;p&gt;The last coefficient $\beta_3$ should tell us how much &lt;code&gt;Balance&lt;/code&gt; increases in &lt;code&gt;Income&lt;/code&gt; for &lt;code&gt;Students&lt;/code&gt; with respect to non-Students.&lt;/p&gt;
&lt;p&gt;Indeed, we can decompose the regression in the following equivalent way:&lt;/p&gt;
&lt;p&gt;$$
\text{Balance} ≈ \beta_0 + \beta_1 \text{Income} + \beta_2 \text{Student} + \beta_3 \text{Income} \times \text{Student}
$$&lt;/p&gt;
&lt;p&gt;which can be interpreted in the following way since &lt;code&gt;Student&lt;/code&gt; is a binary variable&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;If the person is &lt;em&gt;not&lt;/em&gt; a student
$$
\text{Balance} ≈ \beta_0 + \beta_1 \text{Income}
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;If the person is a student
$$
\text{Balance} ≈ (\beta_0 + \beta_2) + (\beta_1 + \beta_3 ) \text{Income}
$$&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;We are allowing not only for a different intercept for &lt;code&gt;Students&lt;/code&gt;, $\beta_0 \to \beta_0 + \beta_2$,  but also for a different impact of &lt;code&gt;Income&lt;/code&gt;, $\beta_1 \to \beta_1 + \beta_3$.&lt;/p&gt;
&lt;p&gt;We can visually inspect the distribution of &lt;code&gt;Income&lt;/code&gt; across the two groups.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Divide data into students and non-students
x_student = credit.loc[credit.Student==&#39;Yes&#39;,&#39;Income&#39;]
y_student = credit.loc[credit.Student==&#39;Yes&#39;,&#39;Balance&#39;]
x_nonstudent = credit.loc[credit.Student==&#39;No&#39;,&#39;Income&#39;]
y_nonstudent = credit.loc[credit.Student==&#39;No&#39;,&#39;Balance&#39;]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Make figure 3.8
def make_fig_3_8():
    
    # Init figure
    fig, ax = plt.subplots(1,1)
    fig.suptitle(&#39;Figure 3.8&#39;)

    # Relationship betweeen income and balance for students and non-students
    ax.scatter(x=x_nonstudent, y=y_nonstudent, facecolors=&#39;None&#39;, edgecolors=&#39;k&#39;, alpha=0.5);
    ax.scatter(x=x_student, y=y_student, facecolors=&#39;r&#39;, edgecolors=&#39;r&#39;, alpha=0.7);
    ax.legend([&#39;non-student&#39;, &#39;student&#39;]);
    ax.set_xlabel(&#39;Income&#39;); ax.set_ylabel(&#39;Balance&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;make_fig_3_8()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/01_regression_114_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;It is hard from the scatterplot to see whether there is a different relationship between income and balance for students and non-students.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s fit two separate regressions.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Interaction between qualitative and quantative variables
est1 = sm.OLS.from_formula(&#39;Balance ~ Income + Student&#39;, credit).fit()
reg1 = est1.params
est2 = sm.OLS.from_formula(&#39;Balance ~ Income + Student + Income*Student&#39;, credit).fit()
reg2 = est2.params

print(&#39;Regression 1 - without interaction term&#39;)
print(reg1)
print(&#39;\nRegression 2 - with interaction term&#39;)
print(reg2)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Regression 1 - without interaction term
Intercept         211.142964
Student[T.Yes]    382.670539
Income              5.984336
dtype: float64

Regression 2 - with interaction term
Intercept                200.623153
Student[T.Yes]           476.675843
Income                     6.218169
Income:Student[T.Yes]     -1.999151
dtype: float64
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Without the interaction term, the two lines have different levels but the same slope. Introducing an interaction term allows the two groups to have different responses to Income.&lt;/p&gt;
&lt;p&gt;We can visualize the relationship in a graph.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Income (x-axis)
income = np.linspace(0,150)

# Balance without interaction term (y-axis)
student1 = np.linspace(reg1[&#39;Intercept&#39;]+reg1[&#39;Student[T.Yes]&#39;],
                       reg1[&#39;Intercept&#39;]+reg1[&#39;Student[T.Yes]&#39;]+150*reg1[&#39;Income&#39;])
non_student1 =  np.linspace(reg1[&#39;Intercept&#39;], reg1[&#39;Intercept&#39;]+150*reg1[&#39;Income&#39;])

# Balance with iteraction term (y-axis)
student2 = np.linspace(reg2[&#39;Intercept&#39;]+reg2[&#39;Student[T.Yes]&#39;],
                       reg2[&#39;Intercept&#39;]+reg2[&#39;Student[T.Yes]&#39;]+
                       150*(reg2[&#39;Income&#39;]+reg2[&#39;Income:Student[T.Yes]&#39;]))
non_student2 =  np.linspace(reg2[&#39;Intercept&#39;], reg2[&#39;Intercept&#39;]+150*reg2[&#39;Income&#39;])
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Figure 3.7
def make_fig_3_7():
    
    fig, (ax1,ax2) = plt.subplots(1,2, figsize=(12,5))
    fig.suptitle(&#39;Figure 3.7&#39;)

    # Plot best fit with and without interaction
    ax1.plot(income, student1, &#39;r&#39;, income, non_student1, &#39;k&#39;)
    ax2.plot(income, student2, &#39;r&#39;, income, non_student2, &#39;k&#39;)
    
    titles = [&#39;Dummy&#39;, &#39;Dummy + Interaction&#39;]
    for ax, t in zip(fig.axes, titles):
        ax.legend([&#39;student&#39;, &#39;non-student&#39;], loc=2)
        ax.set_xlabel(&#39;Income&#39;)
        ax.set_ylabel(&#39;Balance&#39;)
        ax.set_ylim(ymax=1550)
        ax.set_title(t)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;make_fig_3_7()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/01_regression_120_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;non-linear-relationships&#34;&gt;Non-Linear Relationships&lt;/h3&gt;
&lt;p&gt;What if we allow for further non-linearities? Let&amp;rsquo;s change dataset again and use the &lt;code&gt;car&lt;/code&gt; dataset.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Automobile dataset (dropping missing values)
auto = pd.read_csv(&#39;data/Auto.csv&#39;, na_values=&#39;?&#39;).dropna()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This dataset contains information of a wide variety of car models.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;auto.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;mpg&lt;/th&gt;
      &lt;th&gt;cylinders&lt;/th&gt;
      &lt;th&gt;displacement&lt;/th&gt;
      &lt;th&gt;horsepower&lt;/th&gt;
      &lt;th&gt;weight&lt;/th&gt;
      &lt;th&gt;acceleration&lt;/th&gt;
      &lt;th&gt;year&lt;/th&gt;
      &lt;th&gt;origin&lt;/th&gt;
      &lt;th&gt;name&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;18.0&lt;/td&gt;
      &lt;td&gt;8&lt;/td&gt;
      &lt;td&gt;307.0&lt;/td&gt;
      &lt;td&gt;130&lt;/td&gt;
      &lt;td&gt;3504&lt;/td&gt;
      &lt;td&gt;12.0&lt;/td&gt;
      &lt;td&gt;70&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;chevrolet chevelle malibu&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;15.0&lt;/td&gt;
      &lt;td&gt;8&lt;/td&gt;
      &lt;td&gt;350.0&lt;/td&gt;
      &lt;td&gt;165&lt;/td&gt;
      &lt;td&gt;3693&lt;/td&gt;
      &lt;td&gt;11.5&lt;/td&gt;
      &lt;td&gt;70&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;buick skylark 320&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;18.0&lt;/td&gt;
      &lt;td&gt;8&lt;/td&gt;
      &lt;td&gt;318.0&lt;/td&gt;
      &lt;td&gt;150&lt;/td&gt;
      &lt;td&gt;3436&lt;/td&gt;
      &lt;td&gt;11.0&lt;/td&gt;
      &lt;td&gt;70&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;plymouth satellite&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;16.0&lt;/td&gt;
      &lt;td&gt;8&lt;/td&gt;
      &lt;td&gt;304.0&lt;/td&gt;
      &lt;td&gt;150&lt;/td&gt;
      &lt;td&gt;3433&lt;/td&gt;
      &lt;td&gt;12.0&lt;/td&gt;
      &lt;td&gt;70&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;amc rebel sst&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;17.0&lt;/td&gt;
      &lt;td&gt;8&lt;/td&gt;
      &lt;td&gt;302.0&lt;/td&gt;
      &lt;td&gt;140&lt;/td&gt;
      &lt;td&gt;3449&lt;/td&gt;
      &lt;td&gt;10.5&lt;/td&gt;
      &lt;td&gt;70&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;ford torino&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;Suppose we wanted to understand which car caracteristics are correlated with higher efficiency, i.e. &lt;code&gt;mpg&lt;/code&gt; (miles per gallon).&lt;/p&gt;
&lt;p&gt;Consider in particular the relationship between &lt;code&gt;mpg&lt;/code&gt; and &lt;code&gt;horsepower&lt;/code&gt;. It might be a highly non-linear relationship.&lt;/p&gt;
&lt;p&gt;$$
\text{mpg} ≈ \beta_0 + \beta_1 \text{horsepower} + \beta_2 \text{horsepower}^2 + &amp;hellip; ???
$$&lt;/p&gt;
&lt;p&gt;How many terms should we include?&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s look at the data to understand if it naturally suggests non-linearities.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;fig, ax = plt.subplots(1,1)

# Plot polinomials of different degree
plt.scatter(x=auto.horsepower, y=auto.mpg, facecolors=&#39;None&#39;, edgecolors=&#39;k&#39;, alpha=.3) 
plt.ylim(5,55); plt.xlim(40,240); 
plt.xlabel(&#39;horsepower&#39;); plt.ylabel(&#39;mpg&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/01_regression_128_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;The relationship looks non-linear but in which way exactly? Let&amp;rsquo;s try to fit polinomials of different degrees.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def make_fig_38():
    
    # Figure 3.8 
    fig, ax = plt.subplots(1,1)
    ax.set_title(&#39;Figure 3.8&#39;)

    # Plot polinomials of different degree
    plt.scatter(x=auto.horsepower, y=auto.mpg, facecolors=&#39;None&#39;, edgecolors=&#39;k&#39;, alpha=.3) 
    sns.regplot(x=auto.horsepower, y=auto.mpg, ci=None, label=&#39;Linear&#39;, scatter=False, color=&#39;orange&#39;)
    sns.regplot(x=auto.horsepower, y=auto.mpg, ci=None, label=&#39;Degree 2&#39;, order=2, scatter=False, color=&#39;lightblue&#39;)
    sns.regplot(x=auto.horsepower, y=auto.mpg, ci=None, label=&#39;Degree 5&#39;, order=5, scatter=False, color=&#39;g&#39;)
    plt.legend()
    plt.ylim(5,55)
    plt.xlim(40,240);
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;make_fig_38()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/01_regression_131_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;As we can see, the tails are highly unstable depending on the specification.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s add a quadratic term&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Table 3.10
auto[&#39;horsepower2&#39;] = auto.horsepower**2
auto.head(3)
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;mpg&lt;/th&gt;
      &lt;th&gt;cylinders&lt;/th&gt;
      &lt;th&gt;displacement&lt;/th&gt;
      &lt;th&gt;horsepower&lt;/th&gt;
      &lt;th&gt;weight&lt;/th&gt;
      &lt;th&gt;acceleration&lt;/th&gt;
      &lt;th&gt;year&lt;/th&gt;
      &lt;th&gt;origin&lt;/th&gt;
      &lt;th&gt;name&lt;/th&gt;
      &lt;th&gt;horsepower2&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;18.0&lt;/td&gt;
      &lt;td&gt;8&lt;/td&gt;
      &lt;td&gt;307.0&lt;/td&gt;
      &lt;td&gt;130&lt;/td&gt;
      &lt;td&gt;3504&lt;/td&gt;
      &lt;td&gt;12.0&lt;/td&gt;
      &lt;td&gt;70&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;chevrolet chevelle malibu&lt;/td&gt;
      &lt;td&gt;16900&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;15.0&lt;/td&gt;
      &lt;td&gt;8&lt;/td&gt;
      &lt;td&gt;350.0&lt;/td&gt;
      &lt;td&gt;165&lt;/td&gt;
      &lt;td&gt;3693&lt;/td&gt;
      &lt;td&gt;11.5&lt;/td&gt;
      &lt;td&gt;70&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;buick skylark 320&lt;/td&gt;
      &lt;td&gt;27225&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;18.0&lt;/td&gt;
      &lt;td&gt;8&lt;/td&gt;
      &lt;td&gt;318.0&lt;/td&gt;
      &lt;td&gt;150&lt;/td&gt;
      &lt;td&gt;3436&lt;/td&gt;
      &lt;td&gt;11.0&lt;/td&gt;
      &lt;td&gt;70&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;plymouth satellite&lt;/td&gt;
      &lt;td&gt;22500&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;How does the regression change?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;est = sm.OLS.from_formula(&#39;mpg ~ horsepower + horsepower2&#39;, auto).fit()
est.summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
       &lt;td&gt;&lt;/td&gt;          &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt;   &lt;td&gt;   56.9001&lt;/td&gt; &lt;td&gt;    1.800&lt;/td&gt; &lt;td&gt;   31.604&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;   53.360&lt;/td&gt; &lt;td&gt;   60.440&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;horsepower&lt;/th&gt;  &lt;td&gt;   -0.4662&lt;/td&gt; &lt;td&gt;    0.031&lt;/td&gt; &lt;td&gt;  -14.978&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;   -0.527&lt;/td&gt; &lt;td&gt;   -0.405&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;horsepower2&lt;/th&gt; &lt;td&gt;    0.0012&lt;/td&gt; &lt;td&gt;    0.000&lt;/td&gt; &lt;td&gt;   10.080&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    0.001&lt;/td&gt; &lt;td&gt;    0.001&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;h3 id=&#34;non-linearities&#34;&gt;Non-Linearities&lt;/h3&gt;
&lt;p&gt;How can we assess if there are non-linearities and of which kind? We can look at the residuals.&lt;/p&gt;
&lt;p&gt;If the residuals show some kind of pattern, probably we could have fit the line better. Moreover, we can use the pattern itself to understand how.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Linear fit
X = auto.horsepower.values.reshape(-1,1)
y = auto.mpg
regr = LinearRegression().fit(X, y)

auto[&#39;pred1&#39;] = regr.predict(X)
auto[&#39;resid1&#39;] = auto.mpg - auto.pred1

# Quadratic fit
X2 = auto[[&#39;horsepower&#39;, &#39;horsepower2&#39;]]
regr.fit(X2, y)

auto[&#39;pred2&#39;] = regr.predict(X2)
auto[&#39;resid2&#39;] = auto.mpg - auto.pred2
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Figure 3.9
def make_fig_39():
    
    fig, (ax1,ax2) = plt.subplots(1,2, figsize=(12,5))
    fig.suptitle(&#39;Figure 3.9&#39;)

    # Left plot
    sns.regplot(x=auto.pred1, y=auto.resid1, lowess=True, 
                ax=ax1, line_kws={&#39;color&#39;:&#39;r&#39;, &#39;lw&#39;:1},
                scatter_kws={&#39;facecolors&#39;:&#39;None&#39;, &#39;edgecolors&#39;:&#39;k&#39;, &#39;alpha&#39;:0.5})
    ax1.hlines(0,xmin=ax1.xaxis.get_data_interval()[0],
               xmax=ax1.xaxis.get_data_interval()[1], linestyles=&#39;dotted&#39;)
    ax1.set_title(&#39;Residual Plot for Linear Fit&#39;)

    # Right plot
    sns.regplot(x=auto.pred2, y=auto.resid2, lowess=True,
                line_kws={&#39;color&#39;:&#39;r&#39;, &#39;lw&#39;:1}, ax=ax2,
                scatter_kws={&#39;facecolors&#39;:&#39;None&#39;, &#39;edgecolors&#39;:&#39;k&#39;, &#39;alpha&#39;:0.5})
    ax2.hlines(0,xmin=ax2.xaxis.get_data_interval()[0],
               xmax=ax2.xaxis.get_data_interval()[1], linestyles=&#39;dotted&#39;)
    ax2.set_title(&#39;Residual Plot for Quadratic Fit&#39;)

    for ax in fig.axes:
        ax.set_xlabel(&#39;Fitted values&#39;)
        ax.set_ylabel(&#39;Residuals&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt; make_fig_39()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/01_regression_141_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;It looks like the residuals from the linear fit (on the left) exibit a pattern:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;positive values at the tails&lt;/li&gt;
&lt;li&gt;negative values in the center&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This suggests a quadratic fit. Indeed, the residuals when we include &lt;code&gt;horsepower^2&lt;/code&gt; (on the right) seem more uniformly centered around zero.&lt;/p&gt;
&lt;h3 id=&#34;outliers&#34;&gt;Outliers&lt;/h3&gt;
&lt;p&gt;Observations with high residuals have a good chance of being highly influentials. However, they do not have to be.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s use the following data generating process:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$X \sim N(0,1)$&lt;/li&gt;
&lt;li&gt;$\varepsilon \sim N(0,0.5)$&lt;/li&gt;
&lt;li&gt;$\beta_0 = 3$&lt;/li&gt;
&lt;li&gt;$y = \beta_0 X + \varepsilon$&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;np.random.seed(1)

# Generate random y
n = 50
X = rnorm(1,1,(n,1))
e = rnorm(0,0.5,(n,1))
b0 = 3
y = X*b0 + e
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now let&amp;rsquo;s change observation &lt;code&gt;20&lt;/code&gt; so that it becomes an outlier, i.e. it has a high residual.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Generate outlier
X[20] = 1
y[20] = 7

# Short regression without observation number 41
X_small = np.delete(X, 20)
y_small = np.delete(y, 20)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s now plot the data and the residuals&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Figure 3.12
def make_fig_3_12():
    
    # Init figure
    fig, (ax1,ax2) = plt.subplots(1,2, figsize=(12,5)) 
    fig.suptitle(&#39;Figure 3.12&#39;)

    # Plot 1
    ax1.scatter(x=X, y=y, facecolors=&#39;None&#39;, edgecolors=&#39;k&#39;, alpha=.5) 
    sns.regplot(x=X, y=y, ax=ax1, order=1, ci=None, scatter=False, line_kws={&#39;color&#39;:&#39;r&#39;, &#39;lw&#39;:1})
    sns.regplot(x=X_small, y=y_small, ax=ax1, order=1, ci=None, scatter=False, line_kws={&#39;color&#39;:&#39;b&#39;, &#39;lw&#39;:1})
    ax1.set_xlabel(&#39;X&#39;); ax1.set_ylabel(&#39;Y&#39;); 
    ax1.legend([&#39;With obs. 20&#39;, &#39;Without obs. 20&#39;], fontsize=12);

    # Hihglight outliers
    ax1.scatter(x=X[20], y=y[20], facecolors=&#39;None&#39;, edgecolors=&#39;r&#39;, alpha=1) 
    ax1.annotate(&amp;quot;20&amp;quot;, (1.1, 7), color=&#39;r&#39;)

    # Compute fitted values and residuals
    r = regr.fit(X, y)
    y_hat = r.predict(X)
    e = np.abs(y - y_hat)

    # Plot 2
    ax2.scatter(x=y_hat, y=e, facecolors=&#39;None&#39;, edgecolors=&#39;k&#39;, alpha=.5)
    ax2.set_xlabel(&#39;Fitted Values&#39;); ax2.set_ylabel(&#39;Residuals&#39;);
    ax2.hlines(0,xmin=ax2.xaxis.get_data_interval()[0],
               xmax=ax2.xaxis.get_data_interval()[1], linestyles=&#39;dotted&#39;,color=&#39;k&#39;)

    # Highlight outlier
    ax2.scatter(x=y_hat[20], y=e[20], facecolors=&#39;None&#39;, edgecolors=&#39;r&#39;, alpha=1) 
    ax2.annotate(&amp;quot;20&amp;quot;, (2.2, 3.6), color=&#39;r&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;make_fig_3_12()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/01_regression_150_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;high-leverage-points&#34;&gt;High Leverage Points&lt;/h3&gt;
&lt;p&gt;A better concept of &amp;ldquo;influential observation&amp;rdquo; is the Leverage, which represents how much an observation is distant from the others in terms of observables.&lt;/p&gt;
&lt;p&gt;The leverage formula of observation $i$ is&lt;/p&gt;
&lt;p&gt;$$
h_i = x_i (X&amp;rsquo; X)^{-1} x_i&#39;
$$&lt;/p&gt;
&lt;p&gt;However, leverage alone is not necessarily enough for an observation to being highly influential.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s modify observation &lt;code&gt;41&lt;/code&gt; so that it has a high leverage.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Generate observation with high leverage
X[41] = 4
y[41] = 12

# Short regression without observation number 41
X_small = np.delete(X_small, 41)
y_small = np.delete(y_small, 41)

# Compute leverage
H = X @ inv(X.T @ X) @ X.T
h = np.diagonal(H)

# Compute fitted values and residuals
y_hat = X @ inv(X.T @ X) @ X.T @ y
e = np.abs(y - y_hat) 
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What happens now that we have added an observation with high leverage? How does the levarage look like?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Figure 3.13
def make_fig_3_13():

    # Init figure
    fig, (ax1,ax2) = plt.subplots(1,2, figsize=(12,5)) 
    fig.suptitle(&#39;Figure 3.12&#39;)

    # Plot 1
    ax1.scatter(x=X, y=y, facecolors=&#39;None&#39;, edgecolors=&#39;k&#39;, alpha=.5) 
    ax1.scatter(x=X[[20,41]], y=y[[20,41]], facecolors=&#39;None&#39;, edgecolors=&#39;r&#39;, alpha=1) 
    sns.regplot(x=X, y=y, ax=ax1, order=1, ci=None, scatter=False, line_kws={&#39;color&#39;:&#39;r&#39;, &#39;lw&#39;:1})
    sns.regplot(x=X_small, y=y_small, ax=ax1, order=1, ci=None, scatter=False, line_kws={&#39;color&#39;:&#39;b&#39;, &#39;lw&#39;:1})
    ax1.set_xlabel(&#39;X&#39;); ax1.set_ylabel(&#39;Y&#39;); ax1.axis(xmax=4.5);
    ax1.legend([&#39;With obs. 20,41&#39;, &#39;Without obs. 20,41&#39;]);

    # Highlight points
    ax1.annotate(&amp;quot;20&amp;quot;, (1.1, 7), color=&#39;r&#39;)
    ax1.annotate(&amp;quot;41&amp;quot;, (3.6, 12), color=&#39;r&#39;);



    # Plot 2
    ax2.scatter(x=h, y=e, facecolors=&#39;None&#39;, edgecolors=&#39;k&#39;, alpha=.5)
    ax2.set_xlabel(&#39;Leverage&#39;); ax2.set_ylabel(&#39;Residuals&#39;); 
    ax2.hlines(0,xmin=ax2.xaxis.get_data_interval()[0],
               xmax=ax2.xaxis.get_data_interval()[1], linestyles=&#39;dotted&#39;,color=&#39;k&#39;)
    # Highlight outlier
    ax2.scatter(x=h[[20,41]], y=e[[20,41]], facecolors=&#39;None&#39;, edgecolors=&#39;r&#39;, alpha=1);

    # Highlight points
    ax2.annotate(&amp;quot;20&amp;quot;, (0, 3.7), color=&#39;r&#39;)
    ax2.annotate(&amp;quot;41&amp;quot;, (0.14, 0.4), color=&#39;r&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;make_fig_3_13()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/01_regression_156_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;influential-observations&#34;&gt;Influential Observations&lt;/h3&gt;
&lt;p&gt;As we have seen, being an outliers or having high leverage alone might be not enough to conclude that an observation is influential.&lt;/p&gt;
&lt;p&gt;What really matters is a combination of both: observations with high leverage and high residuals, i.e. observations that are not only different in terms of observables (high leverage) but are also different in terms of their relationship between observables and dependent variable (high residual).&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s now modify observation &lt;code&gt;7&lt;/code&gt; so that it is an outlier and has high leverage.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Generate outlier with high leverage
X[7] = 4
y[7] = 7
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Short regression without observation number 41
X_small = np.delete(X, 7)
y_small = np.delete(y, 7)

# Compute leverage
H = X @ inv(X.T @ X) @ X.T
h = np.diagonal(H)

# Compute fitted values and residuals
r = regr.fit(X, y)
y_hat = r.predict(X)
e = np.abs(y - y_hat)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now the best linear fit line has noticeably moved.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def make_fig_extra_3():

    fig, (ax1,ax2) = plt.subplots(1,2, figsize=(12,5)) 

    # Plot 1
    ax1.scatter(x=X, y=y, facecolors=&#39;None&#39;, edgecolors=&#39;k&#39;, alpha=.5) 
    ax1.scatter(x=X[[7,20,41]], y=y[[7,20,41]], facecolors=&#39;None&#39;, edgecolors=&#39;r&#39;, alpha=1) 
    sns.regplot(x=X, y=y, ax=ax1, order=1, ci=None, scatter=False, line_kws={&#39;color&#39;:&#39;r&#39;, &#39;lw&#39;:1})
    sns.regplot(x=X_small, y=y_small, ax=ax1, order=1, ci=None, scatter=False, line_kws={&#39;color&#39;:&#39;b&#39;, &#39;lw&#39;:1})
    ax1.set_xlabel(&#39;X&#39;); ax1.set_ylabel(&#39;Y&#39;); ax1.axis(xmax=4.5);
    ax1.legend([&#39;With obs. 7,20,41&#39;, &#39;Without obs. 7,20,41&#39;]);

    # Highlight points
    ax1.annotate(&amp;quot;7&amp;quot;, (3.7, 7), color=&#39;r&#39;)
    ax1.annotate(&amp;quot;20&amp;quot;, (1.15, 7.05), color=&#39;r&#39;)
    ax1.annotate(&amp;quot;41&amp;quot;, (3.6, 12), color=&#39;r&#39;);



    # Plot 2
    ax2.scatter(x=h, y=e, facecolors=&#39;None&#39;, edgecolors=&#39;k&#39;, alpha=.5)
    ax2.set_xlabel(&#39;Leverage&#39;); ax2.set_ylabel(&#39;Residuals&#39;); 
    ax2.hlines(0,xmin=ax2.xaxis.get_data_interval()[0],
               xmax=ax2.xaxis.get_data_interval()[1], linestyles=&#39;dotted&#39;,color=&#39;k&#39;)
    # Highlight outlier
    ax2.scatter(x=h[[7,20,41]], y=e[[7,20,41]], facecolors=&#39;None&#39;, edgecolors=&#39;r&#39;, alpha=1);

    # Highlight points
    ax2.annotate(&amp;quot;7&amp;quot;, (0.12, 4.0), color=&#39;r&#39;);
    ax2.annotate(&amp;quot;20&amp;quot;, (0, 3.8), color=&#39;r&#39;)
    ax2.annotate(&amp;quot;41&amp;quot;, (0.12, 0.9), color=&#39;r&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;make_fig_extra_3()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/01_regression_164_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;collinearity&#34;&gt;Collinearity&lt;/h3&gt;
&lt;p&gt;Collinearity is the situation in which two dependent varaibles are higly correlated with each other. Algebraically, this is a problem because the $X&amp;rsquo;X$ matrix becomes almost-non-invertible.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s have a look at the &lt;code&gt;ratings&lt;/code&gt; dataset.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Inspect dataset
sns.pairplot(credit[[&#39;Age&#39;, &#39;Balance&#39;, &#39;Limit&#39;, &#39;Rating&#39;]], height=1.8);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/01_regression_167_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;If we zoom into the variable &lt;code&gt;Limit&lt;/code&gt;, we see that for example it is not very correlated with &lt;code&gt;Age&lt;/code&gt; but is very correlated with &lt;code&gt;Rating&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Figure 3.14
def make_fig_3_14():
    
    # Init figure
    fig, (ax1,ax2) = plt.subplots(1,2, figsize=(12,5)) 
    fig.suptitle(&#39;Figure 3.14&#39;)

    # Left plot
    ax1.scatter(credit.Limit, credit.Age, facecolor=&#39;None&#39;, edgecolor=&#39;brown&#39;)
    ax1.set_ylabel(&#39;Age&#39;)

    # Right plot
    ax2.scatter(credit.Limit, credit.Rating, facecolor=&#39;None&#39;, edgecolor=&#39;brown&#39;)
    ax2.set_ylabel(&#39;Rating&#39;)

    for ax in fig.axes:
        ax.set_xlabel(&#39;Limit&#39;)
        ax.set_xticks([2000,4000,6000,8000,12000])
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;make_fig_3_14()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/01_regression_170_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;If we regress &lt;code&gt;Balance&lt;/code&gt; on &lt;code&gt;Limit&lt;/code&gt; and &lt;code&gt;Age&lt;/code&gt;, the coefficient of &lt;code&gt;Limit&lt;/code&gt; is positive and highly significant.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Regress balance on limit and age
reg1 = sm.OLS.from_formula(&#39;Balance ~ Limit + Age&#39;, credit).fit()
reg1.summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
      &lt;td&gt;&lt;/td&gt;         &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt; &lt;td&gt; -173.4109&lt;/td&gt; &lt;td&gt;   43.828&lt;/td&gt; &lt;td&gt;   -3.957&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt; -259.576&lt;/td&gt; &lt;td&gt;  -87.246&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Limit&lt;/th&gt;     &lt;td&gt;    0.1734&lt;/td&gt; &lt;td&gt;    0.005&lt;/td&gt; &lt;td&gt;   34.496&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    0.163&lt;/td&gt; &lt;td&gt;    0.183&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Age&lt;/th&gt;       &lt;td&gt;   -2.2915&lt;/td&gt; &lt;td&gt;    0.672&lt;/td&gt; &lt;td&gt;   -3.407&lt;/td&gt; &lt;td&gt; 0.001&lt;/td&gt; &lt;td&gt;   -3.614&lt;/td&gt; &lt;td&gt;   -0.969&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;However, if we regress &lt;code&gt;Balance&lt;/code&gt; on &lt;code&gt;Limit&lt;/code&gt; and &lt;code&gt;Rating&lt;/code&gt;, the coefficient of &lt;code&gt;Limit&lt;/code&gt; is now not significant anymore.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Regress balance on limit and rating
reg2 = sm.OLS.from_formula(&#39;Balance ~ Limit + Rating&#39;, credit).fit()
reg2.summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
      &lt;td&gt;&lt;/td&gt;         &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt; &lt;td&gt; -377.5368&lt;/td&gt; &lt;td&gt;   45.254&lt;/td&gt; &lt;td&gt;   -8.343&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt; -466.505&lt;/td&gt; &lt;td&gt; -288.569&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Limit&lt;/th&gt;     &lt;td&gt;    0.0245&lt;/td&gt; &lt;td&gt;    0.064&lt;/td&gt; &lt;td&gt;    0.384&lt;/td&gt; &lt;td&gt; 0.701&lt;/td&gt; &lt;td&gt;   -0.101&lt;/td&gt; &lt;td&gt;    0.150&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Rating&lt;/th&gt;    &lt;td&gt;    2.2017&lt;/td&gt; &lt;td&gt;    0.952&lt;/td&gt; &lt;td&gt;    2.312&lt;/td&gt; &lt;td&gt; 0.021&lt;/td&gt; &lt;td&gt;    0.330&lt;/td&gt; &lt;td&gt;    4.074&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;Looking at the objective function, the Residual Sum of Squares, helps understanding what is the problem.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# First scale variables
y = credit.Balance
regr1 = LinearRegression().fit(scale(credit[[&#39;Age&#39;, &#39;Limit&#39;]].astype(&#39;float&#39;), with_std=False), y)
regr2 = LinearRegression().fit(scale(credit[[&#39;Rating&#39;, &#39;Limit&#39;]], with_std=False), y)

# Create grid coordinates for plotting
B_Age = np.linspace(regr1.coef_[0]-3, regr1.coef_[0]+3, 100)
B_Limit = np.linspace(regr1.coef_[1]-0.02, regr1.coef_[1]+0.02, 100)

B_Rating = np.linspace(regr2.coef_[0]-3, regr2.coef_[0]+3, 100)
B_Limit2 = np.linspace(regr2.coef_[1]-0.2, regr2.coef_[1]+0.2, 100)

X1, Y1 = np.meshgrid(B_Limit, B_Age, indexing=&#39;xy&#39;)
X2, Y2 = np.meshgrid(B_Limit2, B_Rating, indexing=&#39;xy&#39;)
Z1 = np.zeros((B_Age.size,B_Limit.size))
Z2 = np.zeros((B_Rating.size,B_Limit2.size))

Limit_scaled = scale(credit.Limit.astype(&#39;float&#39;), with_std=False)
Age_scaled = scale(credit.Age.astype(&#39;float&#39;), with_std=False)
Rating_scaled = scale(credit.Rating.astype(&#39;float&#39;), with_std=False)

# Calculate Z-values (RSS) based on grid of coefficients
for (i,j),v in np.ndenumerate(Z1):
    Z1[i,j] =((y - (regr1.intercept_ + X1[i,j]*Limit_scaled +
                    Y1[i,j]*Age_scaled))**2).sum()/1000000
    
for (i,j),v in np.ndenumerate(Z2):
    Z2[i,j] =((y - (regr2.intercept_ + X2[i,j]*Limit_scaled +
                    Y2[i,j]*Rating_scaled))**2).sum()/1000000
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Figure 3.15
def make_fig_3_15():

    # Init figure
    fig, (ax1,ax2) = plt.subplots(1,2, figsize=(12,5)) 
    fig.suptitle(&#39;Figure 3.15&#39;)

    # Minimum
    min_RSS = r&#39;$\beta_0$, $\beta_1$ for minimized RSS&#39;

    # Left plot
    CS = ax1.contour(X1, Y1, Z1, cmap=plt.cm.Set1, levels=[21.25, 21.5, 21.8])
    ax1.scatter(reg1.params[1], reg1.params[2], c=&#39;r&#39;, label=min_RSS)
    ax1.clabel(CS, inline=True, fontsize=10, fmt=&#39;%1.1f&#39;)
    ax1.set_ylabel(r&#39;$\beta_{Age}$&#39;)

    # Right plot
    CS = ax2.contour(X2, Y2, Z2, cmap=plt.cm.Set1, levels=[21.5, 21.8])
    ax2.scatter(reg2.params[1], reg2.params[2], c=&#39;r&#39;, label=min_RSS)
    ax2.clabel(CS, inline=True, fontsize=10, fmt=&#39;%1.1f&#39;)
    ax2.set_ylabel(r&#39;$\beta_{Rating}$&#39;)
    #ax2.set_xticks([-0.1, 0, 0.1, 0.2])

    for ax in fig.axes:
        ax.set_xlabel(r&#39;$\beta_{Limit}$&#39;)
        ax.legend()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;make_fig_3_15()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/01_regression_178_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;As we can see, in the left plot the minimum is much better defined than in the right plot.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Instrumental Variables</title>
      <link>https://matteocourthoud.github.io/course/ml-econ/02_iv/</link>
      <pubDate>Wed, 09 Mar 2022 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/course/ml-econ/02_iv/</guid>
      <description>&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Remove warnings
import warnings
warnings.filterwarnings(&#39;ignore&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Import everything
import pandas as pd
import numpy as np
import seaborn as sns
import statsmodels.api as sm

from numpy.linalg import inv
from statsmodels.iolib.summary2 import summary_col
from linearmodels.iv import IV2SLS
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Import matplotlib for graphs
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import axes3d

# Set global parameters
%matplotlib inline
plt.style.use(&#39;seaborn-white&#39;)
plt.rcParams[&#39;lines.linewidth&#39;] = 3
plt.rcParams[&#39;figure.figsize&#39;] = (10,6)
plt.rcParams[&#39;figure.titlesize&#39;] = 20
plt.rcParams[&#39;axes.titlesize&#39;] = 18
plt.rcParams[&#39;axes.labelsize&#39;] = 14
plt.rcParams[&#39;legend.fontsize&#39;] = 14
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;21-simple-linear-regression&#34;&gt;2.1 Simple Linear Regression&lt;/h2&gt;
&lt;p&gt;In &lt;a href=&#34;https://economics.mit.edu/files/4123&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Acemoglu, Johnson, Robinson (2002), &amp;ldquo;&lt;em&gt;The Colonial Origins of Comparative Development&lt;/em&gt;&amp;rdquo;&lt;/a&gt; the authors wish to determine whether or not differences in institutions can help to explain observed economic outcomes.&lt;/p&gt;
&lt;p&gt;How do we measure &lt;em&gt;institutional differences&lt;/em&gt; and &lt;em&gt;economic outcomes&lt;/em&gt;?&lt;/p&gt;
&lt;p&gt;In this paper,&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;economic outcomes are proxied by log GDP per capita in 1995, adjusted for exchange rates.&lt;/li&gt;
&lt;li&gt;institutional differences are proxied by an index of protection against expropriation on average over 1985-95, constructed by the &lt;a href=&#34;https://www.prsgroup.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Political Risk Services Group&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;These variables and other data used in the paper are available for download on Daron Acemoglu’s &lt;a href=&#34;https://economics.mit.edu/faculty/acemoglu/data/ajr2001&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;webpage&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The original dataset in in Stata &lt;code&gt;.dta&lt;/code&gt; format but has been converted to &lt;code&gt;.csv&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;First, let&amp;rsquo;s load the data and have a look at it.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Load Acemoglu Johnson Robinson Dataset
df = pd.read_csv(&#39;data/AJR02.csv&#39;,index_col=0)
df.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;GDP&lt;/th&gt;
      &lt;th&gt;Exprop&lt;/th&gt;
      &lt;th&gt;Mort&lt;/th&gt;
      &lt;th&gt;Latitude&lt;/th&gt;
      &lt;th&gt;Neo&lt;/th&gt;
      &lt;th&gt;Africa&lt;/th&gt;
      &lt;th&gt;Asia&lt;/th&gt;
      &lt;th&gt;Namer&lt;/th&gt;
      &lt;th&gt;Samer&lt;/th&gt;
      &lt;th&gt;logMort&lt;/th&gt;
      &lt;th&gt;Latitude2&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;8.39&lt;/td&gt;
      &lt;td&gt;6.50&lt;/td&gt;
      &lt;td&gt;78.20&lt;/td&gt;
      &lt;td&gt;0.3111&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;4.359270&lt;/td&gt;
      &lt;td&gt;0.096783&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;7.77&lt;/td&gt;
      &lt;td&gt;5.36&lt;/td&gt;
      &lt;td&gt;280.00&lt;/td&gt;
      &lt;td&gt;0.1367&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;5.634790&lt;/td&gt;
      &lt;td&gt;0.018687&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;9.13&lt;/td&gt;
      &lt;td&gt;6.39&lt;/td&gt;
      &lt;td&gt;68.90&lt;/td&gt;
      &lt;td&gt;0.3778&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;4.232656&lt;/td&gt;
      &lt;td&gt;0.142733&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;9.90&lt;/td&gt;
      &lt;td&gt;9.32&lt;/td&gt;
      &lt;td&gt;8.55&lt;/td&gt;
      &lt;td&gt;0.3000&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;2.145931&lt;/td&gt;
      &lt;td&gt;0.090000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;5&lt;/th&gt;
      &lt;td&gt;9.29&lt;/td&gt;
      &lt;td&gt;7.50&lt;/td&gt;
      &lt;td&gt;85.00&lt;/td&gt;
      &lt;td&gt;0.2683&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;4.442651&lt;/td&gt;
      &lt;td&gt;0.071985&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;Let’s use a scatterplot to see whether any obvious relationship exists between GDP per capita and the protection against expropriation.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Plot relationship between GDP and expropriation rate
fig, ax = plt.subplots(1,1)
ax.set_title(&#39;Figure 1: joint distribution of GDP and expropriation&#39;)
df.plot(x=&#39;Exprop&#39;, y=&#39;GDP&#39;, kind=&#39;scatter&#39;, s=50, ax=ax);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/02_iv_9_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;The plot shows a fairly strong positive relationship between
protection against expropriation and log GDP per capita.&lt;/p&gt;
&lt;p&gt;Specifically, if higher protection against expropriation is a measure of
institutional quality, then better institutions appear to be positively
correlated with better economic outcomes (higher GDP per capita).&lt;/p&gt;
&lt;p&gt;Given the plot, choosing a linear model to describe this relationship
seems like a reasonable assumption.&lt;/p&gt;
&lt;p&gt;We can write our model as&lt;/p&gt;
&lt;p&gt;$$
{GDP}_i = \beta_0 + \beta_1 {Exprop}_i + \varepsilon_i
$$&lt;/p&gt;
&lt;p&gt;where:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$ \beta_0 $ is the intercept of the linear trend line on the
y-axis&lt;/li&gt;
&lt;li&gt;$ \beta_1 $ is the slope of the linear trend line, representing
the &lt;em&gt;marginal effect&lt;/em&gt; of protection against risk on log GDP per
capita&lt;/li&gt;
&lt;li&gt;$ \varepsilon_i $ is a random error term (deviations of observations from
the linear trend due to factors not included in the model)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The most common technique to estimate the parameters ($ \beta $’s)
of the linear model is Ordinary Least Squares (OLS).&lt;/p&gt;
&lt;p&gt;As the name implies, an OLS model is solved by finding the parameters
that minimize &lt;em&gt;the sum of squared residuals&lt;/em&gt;, i.e.&lt;/p&gt;
&lt;p&gt;$$
\underset{\hat{\beta}}{\min} \sum^N_{i=1}{\hat{u}^2_i}
$$&lt;/p&gt;
&lt;p&gt;where $ \hat{u}_i $ is the difference between the observation and
the predicted value of the dependent variable.&lt;/p&gt;
&lt;p&gt;To estimate the constant term $ \beta_0 $, we need to add a column
of 1’s to our dataset (consider the equation if $ \beta_0 $ was
replaced with $ \beta_0 x_i $ and $ x_i = 1 $)&lt;/p&gt;
&lt;p&gt;Now we can construct our model in &lt;code&gt;statsmodels&lt;/code&gt; using the OLS function.&lt;/p&gt;
&lt;p&gt;We will use &lt;code&gt;pandas&lt;/code&gt; dataframes with &lt;code&gt;statsmodels&lt;/code&gt;, however standard arrays can also be used as arguments&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Regress GDP on Expropriation Rate
reg1 = sm.OLS.from_formula(&#39;GDP ~ Exprop&#39;, df)
type(reg1)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;statsmodels.regression.linear_model.OLS
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So far we have simply constructed our model.&lt;/p&gt;
&lt;p&gt;We need to use &lt;code&gt;.fit()&lt;/code&gt; to obtain parameter estimates
$ \hat{\beta}_0 $ and $ \hat{\beta}_1 $&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Fit regression
results = reg1.fit()
type(results)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;statsmodels.regression.linear_model.RegressionResultsWrapper
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We now have the fitted regression model stored in &lt;code&gt;results&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;To view the OLS regression results, we can call the &lt;code&gt;.summary()&lt;/code&gt;
method.&lt;/p&gt;
&lt;p&gt;Note that an observation was mistakenly dropped from the results in the
original paper (see the note located in maketable2.do from Acemoglu’s webpage), and thus the
coefficients differ slightly.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;results.summary()
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;caption&gt;OLS Regression Results&lt;/caption&gt;
&lt;tr&gt;
  &lt;th&gt;Dep. Variable:&lt;/th&gt;           &lt;td&gt;GDP&lt;/td&gt;       &lt;th&gt;  R-squared:         &lt;/th&gt; &lt;td&gt;   0.540&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Model:&lt;/th&gt;                   &lt;td&gt;OLS&lt;/td&gt;       &lt;th&gt;  Adj. R-squared:    &lt;/th&gt; &lt;td&gt;   0.532&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Method:&lt;/th&gt;             &lt;td&gt;Least Squares&lt;/td&gt;  &lt;th&gt;  F-statistic:       &lt;/th&gt; &lt;td&gt;   72.71&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Date:&lt;/th&gt;             &lt;td&gt;Mon, 03 Jan 2022&lt;/td&gt; &lt;th&gt;  Prob (F-statistic):&lt;/th&gt; &lt;td&gt;4.84e-12&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Time:&lt;/th&gt;                 &lt;td&gt;18:31:09&lt;/td&gt;     &lt;th&gt;  Log-Likelihood:    &lt;/th&gt; &lt;td&gt; -68.214&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;No. Observations:&lt;/th&gt;      &lt;td&gt;    64&lt;/td&gt;      &lt;th&gt;  AIC:               &lt;/th&gt; &lt;td&gt;   140.4&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Df Residuals:&lt;/th&gt;          &lt;td&gt;    62&lt;/td&gt;      &lt;th&gt;  BIC:               &lt;/th&gt; &lt;td&gt;   144.7&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Df Model:&lt;/th&gt;              &lt;td&gt;     1&lt;/td&gt;      &lt;th&gt;                     &lt;/th&gt;     &lt;td&gt; &lt;/td&gt;   
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Covariance Type:&lt;/th&gt;      &lt;td&gt;nonrobust&lt;/td&gt;    &lt;th&gt;                     &lt;/th&gt;     &lt;td&gt; &lt;/td&gt;   
&lt;/tr&gt;
&lt;/table&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
      &lt;td&gt;&lt;/td&gt;         &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt; &lt;td&gt;    4.6609&lt;/td&gt; &lt;td&gt;    0.409&lt;/td&gt; &lt;td&gt;   11.402&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    3.844&lt;/td&gt; &lt;td&gt;    5.478&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Exprop&lt;/th&gt;    &lt;td&gt;    0.5220&lt;/td&gt; &lt;td&gt;    0.061&lt;/td&gt; &lt;td&gt;    8.527&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    0.400&lt;/td&gt; &lt;td&gt;    0.644&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
  &lt;th&gt;Omnibus:&lt;/th&gt;       &lt;td&gt; 7.134&lt;/td&gt; &lt;th&gt;  Durbin-Watson:     &lt;/th&gt; &lt;td&gt;   2.081&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Prob(Omnibus):&lt;/th&gt; &lt;td&gt; 0.028&lt;/td&gt; &lt;th&gt;  Jarque-Bera (JB):  &lt;/th&gt; &lt;td&gt;   6.698&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Skew:&lt;/th&gt;          &lt;td&gt;-0.784&lt;/td&gt; &lt;th&gt;  Prob(JB):          &lt;/th&gt; &lt;td&gt;  0.0351&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Kurtosis:&lt;/th&gt;      &lt;td&gt; 3.234&lt;/td&gt; &lt;th&gt;  Cond. No.          &lt;/th&gt; &lt;td&gt;    31.2&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;&lt;br/&gt;&lt;br/&gt;Warnings:&lt;br/&gt;[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
&lt;p&gt;From our results, we see that&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The intercept $ \hat{\beta}_0 = 4.63 $.&lt;/li&gt;
&lt;li&gt;The slope $ \hat{\beta}_1 = 0.53 $.&lt;/li&gt;
&lt;li&gt;The positive $ \hat{\beta}_1 $ parameter estimate implies that.
institutional quality has a positive effect on economic outcomes, as
we saw in the figure.&lt;/li&gt;
&lt;li&gt;The p-value of 0.000 for $ \hat{\beta}_1 $ implies that the
effect of institutions on GDP is statistically significant (using p &amp;lt;
0.05 as a rejection rule).&lt;/li&gt;
&lt;li&gt;The R-squared value of 0.611 indicates that around 61% of variation
in log GDP per capita is explained by protection against
expropriation.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Using our parameter estimates, we can now write our estimated
relationship as&lt;/p&gt;
&lt;p&gt;$$
\widehat{GDP}_i = 4.63 + 0.53 \ {Exprop}_i
$$&lt;/p&gt;
&lt;p&gt;This equation describes the line that best fits our data, as shown in
Figure 2.&lt;/p&gt;
&lt;p&gt;We can use this equation to predict the level of log GDP per capita for
a value of the index of expropriation protection.&lt;/p&gt;
&lt;p&gt;For example, for a country with an index value of 6.51 (the average for
the dataset), we find that their predicted level of log GDP per capita
in 1995 is 8.09.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;mean_expr = np.mean(df[&#39;Exprop&#39;])
mean_expr
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;6.5160937500000005
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;predicted_logpdp95 = results.params[0] + results.params[1] * mean_expr
predicted_logpdp95
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;8.062499999999995
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;An easier (and more accurate) way to obtain this result is to use
&lt;code&gt;.predict()&lt;/code&gt; and set $ constant = 1 $ and
$ {Exprop}_i = mean_expr $&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;results.predict(exog=[1, mean_expr])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can obtain an array of predicted $ {GDP}_i $ for every value
of $ {Exprop}_i $ in our dataset by calling &lt;code&gt;.predict()&lt;/code&gt; on our
results.&lt;/p&gt;
&lt;p&gt;Plotting the predicted values against $ {Exprop}_i $ shows that the
predicted values lie along the linear line that we fitted above.&lt;/p&gt;
&lt;p&gt;The observed values of $ {GDP}_i $ are also plotted for
comparison purposes&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Make first new figure
def make_new_fig_2():

    # Init figure
    fig, ax = plt.subplots(1,1)
    ax.set_title(&#39;Figure 2: OLS predicted values&#39;)

    # Drop missing observations from whole sample
    df_plot = df.dropna(subset=[&#39;GDP&#39;, &#39;Exprop&#39;])
    sns.regplot(x=df_plot[&#39;Exprop&#39;], y=df_plot[&#39;GDP&#39;], ax=ax, order=1, ci=None, line_kws={&#39;color&#39;:&#39;r&#39;})

    ax.legend([&#39;predicted&#39;, &#39;observed&#39;])
    ax.set_xlabel(&#39;Exprop&#39;)
    ax.set_ylabel(&#39;GDP&#39;)
    plt.show()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;make_new_fig_2()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;ERROR! Session/line number was not unique in database. History logging moved to new session 305
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/02_iv_28_1.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;22-extending-the-linear-regression-model&#34;&gt;2.2 Extending the Linear Regression Model&lt;/h2&gt;
&lt;p&gt;So far we have only accounted for institutions affecting economic performance - almost certainly there are numerous other factors affecting GDP that are not included in our model.&lt;/p&gt;
&lt;p&gt;Leaving out variables that affect $ GDP_i $ will result in &lt;strong&gt;omitted variable bias&lt;/strong&gt;, yielding biased and inconsistent parameter estimates.&lt;/p&gt;
&lt;p&gt;We can extend our bivariate regression model to a &lt;strong&gt;multivariate regression model&lt;/strong&gt; by adding in other factors that may affect $ GDP_i $.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://python-programming.quantecon.org/zreferences.html#acemoglu2001&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[AJR01]&lt;/a&gt; consider other factors such as:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the effect of climate on economic outcomes; latitude is used to proxy
this&lt;/li&gt;
&lt;li&gt;differences that affect both economic performance and institutions,
eg. cultural, historical, etc.; controlled for with the use of
continent dummies&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Let’s estimate some of the extended models considered in the paper
(Table 2) using data from &lt;code&gt;maketable2.dta&lt;/code&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Add constant term to dataset
df[&#39;const&#39;] = 1

# Create lists of variables to be used in each regression
X1 = df[[&#39;const&#39;, &#39;Exprop&#39;]]
X2 = df[[&#39;const&#39;, &#39;Exprop&#39;, &#39;Latitude&#39;, &#39;Latitude2&#39;]]
X3 = df[[&#39;const&#39;, &#39;Exprop&#39;, &#39;Latitude&#39;, &#39;Latitude2&#39;, &#39;Asia&#39;, &#39;Africa&#39;, &#39;Namer&#39;, &#39;Samer&#39;]]

# Estimate an OLS regression for each set of variables
reg1 = sm.OLS(df[&#39;GDP&#39;], X1, missing=&#39;drop&#39;).fit()
reg2 = sm.OLS(df[&#39;GDP&#39;], X2, missing=&#39;drop&#39;).fit()
reg3 = sm.OLS(df[&#39;GDP&#39;], X3, missing=&#39;drop&#39;).fit()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now that we have fitted our model, we will use &lt;code&gt;summary_col&lt;/code&gt; to
display the results in a single table (model numbers correspond to those
in the paper)&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;info_dict={&#39;No. observations&#39; : lambda x: f&amp;quot;{int(x.nobs):d}&amp;quot;}

results_table = summary_col(results=[reg1,reg2,reg3],
                            float_format=&#39;%0.2f&#39;,
                            stars = True,
                            model_names=[&#39;Model 1&#39;,&#39;Model 2&#39;,&#39;Model 3&#39;],
                            info_dict=info_dict,
                            regressor_order=[&#39;const&#39;,&#39;Exprop&#39;,&#39;Latitude&#39;,&#39;Latitude2&#39;])

results_table
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
          &lt;td&gt;&lt;/td&gt;         &lt;th&gt;Model 1&lt;/th&gt; &lt;th&gt;Model 2&lt;/th&gt; &lt;th&gt;Model 3&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;const&lt;/th&gt;            &lt;td&gt;4.66***&lt;/td&gt; &lt;td&gt;4.55***&lt;/td&gt; &lt;td&gt;5.95***&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;&lt;/th&gt;                 &lt;td&gt;(0.41)&lt;/td&gt;  &lt;td&gt;(0.45)&lt;/td&gt;  &lt;td&gt;(0.68)&lt;/td&gt; 
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Exprop&lt;/th&gt;           &lt;td&gt;0.52***&lt;/td&gt; &lt;td&gt;0.49***&lt;/td&gt; &lt;td&gt;0.40***&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;&lt;/th&gt;                 &lt;td&gt;(0.06)&lt;/td&gt;  &lt;td&gt;(0.07)&lt;/td&gt;  &lt;td&gt;(0.06)&lt;/td&gt; 
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Latitude&lt;/th&gt;            &lt;td&gt;&lt;/td&gt;      &lt;td&gt;2.16&lt;/td&gt;    &lt;td&gt;0.42&lt;/td&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;&lt;/th&gt;                    &lt;td&gt;&lt;/td&gt;     &lt;td&gt;(1.68)&lt;/td&gt;  &lt;td&gt;(1.47)&lt;/td&gt; 
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Latitude2&lt;/th&gt;           &lt;td&gt;&lt;/td&gt;      &lt;td&gt;-2.12&lt;/td&gt;   &lt;td&gt;0.44&lt;/td&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;&lt;/th&gt;                    &lt;td&gt;&lt;/td&gt;     &lt;td&gt;(2.86)&lt;/td&gt;  &lt;td&gt;(2.48)&lt;/td&gt; 
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Africa&lt;/th&gt;              &lt;td&gt;&lt;/td&gt;        &lt;td&gt;&lt;/td&gt;     &lt;td&gt;-1.06**&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;&lt;/th&gt;                    &lt;td&gt;&lt;/td&gt;        &lt;td&gt;&lt;/td&gt;     &lt;td&gt;(0.41)&lt;/td&gt; 
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Asia&lt;/th&gt;                &lt;td&gt;&lt;/td&gt;        &lt;td&gt;&lt;/td&gt;     &lt;td&gt;-0.74*&lt;/td&gt; 
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;&lt;/th&gt;                    &lt;td&gt;&lt;/td&gt;        &lt;td&gt;&lt;/td&gt;     &lt;td&gt;(0.42)&lt;/td&gt; 
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Namer&lt;/th&gt;               &lt;td&gt;&lt;/td&gt;        &lt;td&gt;&lt;/td&gt;      &lt;td&gt;-0.17&lt;/td&gt; 
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;&lt;/th&gt;                    &lt;td&gt;&lt;/td&gt;        &lt;td&gt;&lt;/td&gt;     &lt;td&gt;(0.40)&lt;/td&gt; 
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Samer&lt;/th&gt;               &lt;td&gt;&lt;/td&gt;        &lt;td&gt;&lt;/td&gt;      &lt;td&gt;-0.12&lt;/td&gt; 
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;&lt;/th&gt;                    &lt;td&gt;&lt;/td&gt;        &lt;td&gt;&lt;/td&gt;     &lt;td&gt;(0.42)&lt;/td&gt; 
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;No. observations&lt;/th&gt;   &lt;td&gt;64&lt;/td&gt;      &lt;td&gt;64&lt;/td&gt;      &lt;td&gt;64&lt;/td&gt;   
&lt;/tr&gt;
&lt;/table&gt;
&lt;h2 id=&#34;23-endogeneity&#34;&gt;2.3 Endogeneity&lt;/h2&gt;
&lt;p&gt;As &lt;a href=&#34;https://python-programming.quantecon.org/zreferences.html#acemoglu2001&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[AJR01]&lt;/a&gt; discuss, the OLS models likely suffer from &lt;strong&gt;endogeneity&lt;/strong&gt; issues, resulting in biased and inconsistent model estimates.&lt;/p&gt;
&lt;p&gt;Namely, there is likely a two-way relationship between institutions an economic outcomes:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;richer countries may be able to afford or prefer better institutions&lt;/li&gt;
&lt;li&gt;variables that affect income may also be correlated with institutional differences&lt;/li&gt;
&lt;li&gt;the construction of the index may be biased; analysts may be biased towards seeing countries with higher income having better institutions&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;To deal with endogeneity, we can use &lt;strong&gt;two-stage least squares (2SLS) regression&lt;/strong&gt;, which is an extension of OLS regression.&lt;/p&gt;
&lt;p&gt;This method requires replacing the endogenous variable $ {Exprop}_i $ with a variable that is:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;correlated with $ {Exprop}_i $&lt;/li&gt;
&lt;li&gt;not correlated with the error term (ie. it should not directly affect the dependent variable, otherwise it would be correlated with $ u_i $ due to omitted variable bias)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;We can write our model as&lt;/p&gt;
&lt;p&gt;$$
{GDP}_i = \beta_0 + \beta_1 {Exprop}_i + \varepsilon_i \
{Exprop}_i = \delta_0 + \delta_1 {logMort}_i + v_i
$$&lt;/p&gt;
&lt;p&gt;The new set of regressors &lt;code&gt;logMort&lt;/code&gt; is called an &lt;strong&gt;instrument&lt;/strong&gt;, which aims to remove endogeneity in our proxy of institutional differences.&lt;/p&gt;
&lt;p&gt;The main contribution of &lt;a href=&#34;https://python-programming.quantecon.org/zreferences.html#acemoglu2001&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[AJR01]&lt;/a&gt; is the use of settler mortality rates to instrument for institutional differences.&lt;/p&gt;
&lt;p&gt;They hypothesize that higher mortality rates of colonizers led to the establishment of institutions that were more extractive in nature (less protection against expropriation), and these institutions still persist today.&lt;/p&gt;
&lt;p&gt;Using a scatterplot (Figure 3 in &lt;a href=&#34;https://python-programming.quantecon.org/zreferences.html#acemoglu2001&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[AJR01]&lt;/a&gt;), we can see protection against expropriation is negatively correlated with settler mortality rates, coinciding with the authors’ hypothesis and satisfying the first condition of a valid instrument.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Dropping NA&#39;s is required to use numpy&#39;s polyfit
df2 = df.dropna(subset=[&#39;logMort&#39;, &#39;Exprop&#39;])
X = df2[&#39;logMort&#39;]
y = df2[&#39;Exprop&#39;]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Make new figure 2
def make_new_figure_2():

    # Init figure
    fig, ax = plt.subplots(1,1)
    ax.set_title(&#39;Figure 3: First-stage&#39;)

    # Fit a linear trend line
    sns.regplot(x=X, y=y, ax=ax, order=1, scatter=True, ci=None, line_kws={&amp;quot;color&amp;quot;: &amp;quot;r&amp;quot;})

    ax.set_xlim([1.8,8.4])
    ax.set_ylim([3.3,10.4])
    ax.set_xlabel(&#39;Log of Settler Mortality&#39;)
    ax.set_ylabel(&#39;Average Expropriation Risk 1985-95&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;make_new_figure_2()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/02_iv_43_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;The second condition may not be satisfied if settler mortality rates in the 17th to 19th centuries have a direct effect on current GDP (in addition to their indirect effect through institutions).&lt;/p&gt;
&lt;p&gt;For example, settler mortality rates may be related to the current disease environment in a country, which could affect current economic performance.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://python-programming.quantecon.org/zreferences.html#acemoglu2001&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[AJR01]&lt;/a&gt; argue this is unlikely because:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The majority of settler deaths were due to malaria and yellow fever
and had a limited effect on local people.&lt;/li&gt;
&lt;li&gt;The disease burden on local people in Africa or India, for example,
did not appear to be higher than average, supported by relatively
high population densities in these areas before colonization.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;As we appear to have a valid instrument, we can use 2SLS regression to
obtain consistent and unbiased parameter estimates.&lt;/p&gt;
&lt;h3 id=&#34;first-stage&#34;&gt;First stage&lt;/h3&gt;
&lt;p&gt;The first stage involves regressing the endogenous variable
($ {Exprop}_i $) on the instrument.&lt;/p&gt;
&lt;p&gt;The instrument is the set of all exogenous variables in our model (and
not just the variable we have replaced).&lt;/p&gt;
&lt;p&gt;Using model 1 as an example, our instrument is simply a constant and
settler mortality rates $ {logMort}_i $.&lt;/p&gt;
&lt;p&gt;Therefore, we will estimate the first-stage regression as&lt;/p&gt;
&lt;p&gt;$$
{Exprop}_i = \delta_0 + \delta_1 {logMort}_i + v_i
$$&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Add a constant variable
df[&#39;const&#39;] = 1

# Fit the first stage regression and print summary
results_fs = sm.OLS(df[&#39;Exprop&#39;],
                    df.loc[:,[&#39;const&#39;, &#39;logMort&#39;]],
                    missing=&#39;drop&#39;).fit()
results_fs.summary()
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;caption&gt;OLS Regression Results&lt;/caption&gt;
&lt;tr&gt;
  &lt;th&gt;Dep. Variable:&lt;/th&gt;         &lt;td&gt;Exprop&lt;/td&gt;      &lt;th&gt;  R-squared:         &lt;/th&gt; &lt;td&gt;   0.274&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Model:&lt;/th&gt;                   &lt;td&gt;OLS&lt;/td&gt;       &lt;th&gt;  Adj. R-squared:    &lt;/th&gt; &lt;td&gt;   0.262&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Method:&lt;/th&gt;             &lt;td&gt;Least Squares&lt;/td&gt;  &lt;th&gt;  F-statistic:       &lt;/th&gt; &lt;td&gt;   23.34&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Date:&lt;/th&gt;             &lt;td&gt;Mon, 03 Jan 2022&lt;/td&gt; &lt;th&gt;  Prob (F-statistic):&lt;/th&gt; &lt;td&gt;9.27e-06&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Time:&lt;/th&gt;                 &lt;td&gt;18:31:10&lt;/td&gt;     &lt;th&gt;  Log-Likelihood:    &lt;/th&gt; &lt;td&gt; -104.69&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;No. Observations:&lt;/th&gt;      &lt;td&gt;    64&lt;/td&gt;      &lt;th&gt;  AIC:               &lt;/th&gt; &lt;td&gt;   213.4&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Df Residuals:&lt;/th&gt;          &lt;td&gt;    62&lt;/td&gt;      &lt;th&gt;  BIC:               &lt;/th&gt; &lt;td&gt;   217.7&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Df Model:&lt;/th&gt;              &lt;td&gt;     1&lt;/td&gt;      &lt;th&gt;                     &lt;/th&gt;     &lt;td&gt; &lt;/td&gt;   
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Covariance Type:&lt;/th&gt;      &lt;td&gt;nonrobust&lt;/td&gt;    &lt;th&gt;                     &lt;/th&gt;     &lt;td&gt; &lt;/td&gt;   
&lt;/tr&gt;
&lt;/table&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
     &lt;td&gt;&lt;/td&gt;        &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;const&lt;/th&gt;   &lt;td&gt;    9.3659&lt;/td&gt; &lt;td&gt;    0.611&lt;/td&gt; &lt;td&gt;   15.339&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    8.145&lt;/td&gt; &lt;td&gt;   10.586&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;logMort&lt;/th&gt; &lt;td&gt;   -0.6133&lt;/td&gt; &lt;td&gt;    0.127&lt;/td&gt; &lt;td&gt;   -4.831&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;   -0.867&lt;/td&gt; &lt;td&gt;   -0.360&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
  &lt;th&gt;Omnibus:&lt;/th&gt;       &lt;td&gt; 0.047&lt;/td&gt; &lt;th&gt;  Durbin-Watson:     &lt;/th&gt; &lt;td&gt;   1.592&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Prob(Omnibus):&lt;/th&gt; &lt;td&gt; 0.977&lt;/td&gt; &lt;th&gt;  Jarque-Bera (JB):  &lt;/th&gt; &lt;td&gt;   0.154&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Skew:&lt;/th&gt;          &lt;td&gt; 0.060&lt;/td&gt; &lt;th&gt;  Prob(JB):          &lt;/th&gt; &lt;td&gt;   0.926&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Kurtosis:&lt;/th&gt;      &lt;td&gt; 2.792&lt;/td&gt; &lt;th&gt;  Cond. No.          &lt;/th&gt; &lt;td&gt;    19.4&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;&lt;br/&gt;&lt;br/&gt;Warnings:&lt;br/&gt;[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
&lt;p&gt;We need to retrieve the predicted values of $ {Exprop}_i $ using
&lt;code&gt;.predict()&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;We then replace the endogenous variable $ {Exprop}_i $ with the
predicted values $ \widehat{Exprop}_i $ in the original linear model.&lt;/p&gt;
&lt;p&gt;Our second stage regression is thus&lt;/p&gt;
&lt;p&gt;$$
{GDP}_i = \beta_0 + \beta_1 \widehat{Exprop}_i + u_i
$$&lt;/p&gt;
&lt;h3 id=&#34;second-stage&#34;&gt;Second stage&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Second stage
df[&#39;predicted_Exprop&#39;] = results_fs.predict()
results_ss = sm.OLS.from_formula(&#39;GDP ~ predicted_Exprop&#39;, df).fit()

# Print
results_ss.summary()
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;caption&gt;OLS Regression Results&lt;/caption&gt;
&lt;tr&gt;
  &lt;th&gt;Dep. Variable:&lt;/th&gt;           &lt;td&gt;GDP&lt;/td&gt;       &lt;th&gt;  R-squared:         &lt;/th&gt; &lt;td&gt;   0.462&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Model:&lt;/th&gt;                   &lt;td&gt;OLS&lt;/td&gt;       &lt;th&gt;  Adj. R-squared:    &lt;/th&gt; &lt;td&gt;   0.453&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Method:&lt;/th&gt;             &lt;td&gt;Least Squares&lt;/td&gt;  &lt;th&gt;  F-statistic:       &lt;/th&gt; &lt;td&gt;   53.24&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Date:&lt;/th&gt;             &lt;td&gt;Mon, 03 Jan 2022&lt;/td&gt; &lt;th&gt;  Prob (F-statistic):&lt;/th&gt; &lt;td&gt;6.58e-10&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Time:&lt;/th&gt;                 &lt;td&gt;18:31:10&lt;/td&gt;     &lt;th&gt;  Log-Likelihood:    &lt;/th&gt; &lt;td&gt; -73.208&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;No. Observations:&lt;/th&gt;      &lt;td&gt;    64&lt;/td&gt;      &lt;th&gt;  AIC:               &lt;/th&gt; &lt;td&gt;   150.4&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Df Residuals:&lt;/th&gt;          &lt;td&gt;    62&lt;/td&gt;      &lt;th&gt;  BIC:               &lt;/th&gt; &lt;td&gt;   154.7&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Df Model:&lt;/th&gt;              &lt;td&gt;     1&lt;/td&gt;      &lt;th&gt;                     &lt;/th&gt;     &lt;td&gt; &lt;/td&gt;   
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Covariance Type:&lt;/th&gt;      &lt;td&gt;nonrobust&lt;/td&gt;    &lt;th&gt;                     &lt;/th&gt;     &lt;td&gt; &lt;/td&gt;   
&lt;/tr&gt;
&lt;/table&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
          &lt;td&gt;&lt;/td&gt;            &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt;        &lt;td&gt;    2.0448&lt;/td&gt; &lt;td&gt;    0.830&lt;/td&gt; &lt;td&gt;    2.463&lt;/td&gt; &lt;td&gt; 0.017&lt;/td&gt; &lt;td&gt;    0.385&lt;/td&gt; &lt;td&gt;    3.705&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;predicted_Exprop&lt;/th&gt; &lt;td&gt;    0.9235&lt;/td&gt; &lt;td&gt;    0.127&lt;/td&gt; &lt;td&gt;    7.297&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    0.671&lt;/td&gt; &lt;td&gt;    1.177&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
  &lt;th&gt;Omnibus:&lt;/th&gt;       &lt;td&gt;10.463&lt;/td&gt; &lt;th&gt;  Durbin-Watson:     &lt;/th&gt; &lt;td&gt;   2.052&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Prob(Omnibus):&lt;/th&gt; &lt;td&gt; 0.005&lt;/td&gt; &lt;th&gt;  Jarque-Bera (JB):  &lt;/th&gt; &lt;td&gt;  10.693&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Skew:&lt;/th&gt;          &lt;td&gt;-0.806&lt;/td&gt; &lt;th&gt;  Prob(JB):          &lt;/th&gt; &lt;td&gt; 0.00476&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Kurtosis:&lt;/th&gt;      &lt;td&gt; 4.188&lt;/td&gt; &lt;th&gt;  Cond. No.          &lt;/th&gt; &lt;td&gt;    57.8&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;&lt;br/&gt;&lt;br/&gt;Warnings:&lt;br/&gt;[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
&lt;p&gt;The second-stage regression results give us an unbiased and consistent
estimate of the effect of institutions on economic outcomes.&lt;/p&gt;
&lt;p&gt;The result suggests a stronger positive relationship than what the OLS
results indicated.&lt;/p&gt;
&lt;p&gt;Note that while our parameter estimates are correct, our standard errors
are not and for this reason, computing 2SLS ‘manually’ (in stages with
OLS) is not recommended.&lt;/p&gt;
&lt;p&gt;We can correctly estimate a 2SLS regression in one step using the
&lt;a href=&#34;https://github.com/bashtage/linearmodels&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;linearmodels&lt;/a&gt; package, an extension of &lt;code&gt;statsmodels&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Note that when using &lt;code&gt;IV2SLS&lt;/code&gt;, the exogenous and instrument variables
are split up in the function arguments (whereas before the instrument
included exogenous variables)&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# IV regression
iv = IV2SLS(dependent=df[&#39;GDP&#39;],
            exog=df[&#39;const&#39;],
            endog=df[&#39;Exprop&#39;],
            instruments=df[&#39;logMort&#39;]).fit()

# Print
iv.summary
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;caption&gt;IV-2SLS Estimation Summary&lt;/caption&gt;
&lt;tr&gt;
  &lt;th&gt;Dep. Variable:&lt;/th&gt;           &lt;td&gt;GDP&lt;/td&gt;       &lt;th&gt;  R-squared:         &lt;/th&gt; &lt;td&gt;0.2205&lt;/td&gt; 
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Estimator:&lt;/th&gt;             &lt;td&gt;IV-2SLS&lt;/td&gt;     &lt;th&gt;  Adj. R-squared:    &lt;/th&gt; &lt;td&gt;0.2079&lt;/td&gt; 
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;No. Observations:&lt;/th&gt;        &lt;td&gt;64&lt;/td&gt;        &lt;th&gt;  F-statistic:       &lt;/th&gt; &lt;td&gt;29.811&lt;/td&gt; 
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Date:&lt;/th&gt;             &lt;td&gt;Mon, Jan 03 2022&lt;/td&gt; &lt;th&gt;  P-value (F-stat)   &lt;/th&gt; &lt;td&gt;0.0000&lt;/td&gt; 
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Time:&lt;/th&gt;                 &lt;td&gt;18:31:10&lt;/td&gt;     &lt;th&gt;  Distribution:      &lt;/th&gt; &lt;td&gt;chi2(1)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Cov. Estimator:&lt;/th&gt;        &lt;td&gt;robust&lt;/td&gt;      &lt;th&gt;                     &lt;/th&gt;    &lt;td&gt;&lt;/td&gt;    
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;&lt;/th&gt;                          &lt;td&gt;&lt;/td&gt;         &lt;th&gt;                     &lt;/th&gt;    &lt;td&gt;&lt;/td&gt;    
&lt;/tr&gt;
&lt;/table&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;caption&gt;Parameter Estimates&lt;/caption&gt;
&lt;tr&gt;
     &lt;td&gt;&lt;/td&gt;    &lt;th&gt;Parameter&lt;/th&gt; &lt;th&gt;Std. Err.&lt;/th&gt; &lt;th&gt;T-stat&lt;/th&gt; &lt;th&gt;P-value&lt;/th&gt; &lt;th&gt;Lower CI&lt;/th&gt; &lt;th&gt;Upper CI&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;const&lt;/th&gt;   &lt;td&gt;2.0448&lt;/td&gt;    &lt;td&gt;1.1273&lt;/td&gt;   &lt;td&gt;1.8139&lt;/td&gt; &lt;td&gt;0.0697&lt;/td&gt;   &lt;td&gt;-0.1647&lt;/td&gt;  &lt;td&gt;4.2542&lt;/td&gt; 
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Exprop&lt;/th&gt;  &lt;td&gt;0.9235&lt;/td&gt;    &lt;td&gt;0.1691&lt;/td&gt;   &lt;td&gt;5.4599&lt;/td&gt; &lt;td&gt;0.0000&lt;/td&gt;   &lt;td&gt;0.5920&lt;/td&gt;   &lt;td&gt;1.2550&lt;/td&gt; 
&lt;/tr&gt;
&lt;/table&gt;&lt;br/&gt;&lt;br/&gt;Endogenous: Exprop&lt;br/&gt;Instruments: logMort&lt;br/&gt;Robust Covariance (Heteroskedastic)&lt;br/&gt;Debiased: False
&lt;p&gt;Given that we now have consistent and unbiased estimates, we can infer
from the model we have estimated that institutional differences
(stemming from institutions set up during colonization) can help
to explain differences in income levels across countries today.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://python-programming.quantecon.org/zreferences.html#acemoglu2001&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[AJR01]&lt;/a&gt; use a marginal effect of 0.94 to calculate that the
difference in the index between Chile and Nigeria (ie. institutional
quality) implies up to a 7-fold difference in income, emphasizing the
significance of institutions in economic development.&lt;/p&gt;
&lt;h2 id=&#34;24-matrix-algebra&#34;&gt;2.4 Matrix Algebra&lt;/h2&gt;
&lt;p&gt;The OLS parameter $ \beta $ can also be estimated using matrix
algebra and &lt;code&gt;numpy&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;The linear equation we want to estimate is (written in matrix form)&lt;/p&gt;
&lt;p&gt;$$
y = X\beta + \varepsilon
$$&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Init 
X = df[[&#39;const&#39;, &#39;Exprop&#39;]].values
Z = df[[&#39;const&#39;, &#39;logMort&#39;]].values
y = df[&#39;GDP&#39;].values
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To solve for the unknown parameter $ \beta $, we want to minimize
the sum of squared residuals&lt;/p&gt;
&lt;p&gt;$$
\underset{\hat{\beta}}{\min} \ \hat{\varepsilon}&amp;rsquo;\hat{\varepsilon}
$$&lt;/p&gt;
&lt;p&gt;Rearranging the first equation and substituting into the second
equation, we can write&lt;/p&gt;
&lt;p&gt;$$
\underset{\hat{\beta}}{\min} \ (Y - X\hat{\beta})&amp;rsquo; (Y - X\hat{\beta})
$$&lt;/p&gt;
&lt;p&gt;Solving this optimization problem gives the solution for the
$ \hat{\beta} $ coefficients&lt;/p&gt;
&lt;p&gt;$$
\hat{\beta} = (X&amp;rsquo;X)^{-1}X&amp;rsquo;y
$$&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Compute beta OLS
beta_OLS = inv(X.T @ X) @ X.T @ y

print(beta_OLS)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[4.66087966 0.52203367]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As we as see above, the OLS coefficient might suffer from endogeneity bias. We can solve the issue by instrumenting the predicted average expropriation rate with the average settler mortality.&lt;/p&gt;
&lt;p&gt;If we define settler mortality as $Z$, our full model is&lt;/p&gt;
&lt;p&gt;$$
y = X\beta + \varepsilon \
X = Z\gamma + \mu
$$&lt;/p&gt;
&lt;p&gt;Where we refer to the second equation as second stage and to the first equation as the reduced form equation. In our case, since the number of endogenous varaibles is equal to the number of insturments, there are two equivalent estimators that do not suffer from endogeneity bias: 2SLS and IV.&lt;/p&gt;
&lt;p&gt;IV, the one stage estimator&lt;/p&gt;
&lt;p&gt;$$
\hat \beta_{IV} = (Z&amp;rsquo;X)^{-1} Z&amp;rsquo; y
$$&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Compute beta IV
beta_IV = inv(Z.T @ X) @ Z.T @ y

print(beta_IV)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[2.0447613  0.92351936]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;One of the hypothesis behind the IV estimator is the &lt;em&gt;relevance&lt;/em&gt; of the instrument, i.e. we have a strong predictor in the first stage. This is the only hypothesis that we can empirically assess by checking the significance of the first stage coefficient.&lt;/p&gt;
&lt;p&gt;$$
\hat \gamma = (Z&amp;rsquo; Z)^{-1} Z&amp;rsquo;X \
\hat Var (\hat \gamma) = \sigma_u^2 (Z&amp;rsquo; Z)^{-1}
$$&lt;/p&gt;
&lt;p&gt;where&lt;/p&gt;
&lt;p&gt;$$
u = X - Z \hat \gamma
$$&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Estimate first stage coefficient
gamma_hat = (inv(Z.T @ Z) @ Z.T @ X)

print(gamma_hat[1,1])
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;-0.613289272386864
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Compute variance of the estimator
u = X - Z @ gamma_hat
var_gamma_hat = np.var(u) * inv(Z.T @ Z)

# Compute standard errors
std_gamma_hat = var_gamma_hat[1,1]**.5
print(std_gamma_hat)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;0.08834733362858548
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Compute 95% confidence interval
CI = [gamma_hat[1,1] - 1.96*std_gamma_hat, gamma_hat[1,1] + 1.96*std_gamma_hat]

print(CI)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[-0.7864500462988916, -0.4401284984748365]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The first stage coefficient is negative and significant, i.e. settler mortality is negatively correlated with the expropriation rate.&lt;/p&gt;
&lt;p&gt;How does it work when we have more instruments than endogenous variables? Two-State Least Squares.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Regress $X$ on $Z$ and obtain $\hat X$:
$$
\hat X = Z (Z&amp;rsquo; Z)^{-1} Z&amp;rsquo;X
$$&lt;/li&gt;
&lt;li&gt;Regress $Y$ on $\hat X$ and obtain $\hat \beta_{2SLS}$
$$
\hat \beta_{2SLS} = (\hat X&amp;rsquo; \hat X)^{-1} \hat X&amp;rsquo; y
$$&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In our case, just for the sake of exposition, let&amp;rsquo;s generate a second instrument: the settler mortality squared, &lt;code&gt;logMort_2&lt;/code&gt; = &lt;code&gt;logMort&lt;/code&gt;^2.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df[&#39;logMort_2&#39;] = df[&#39;logMort&#39;]**2
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Define Z
Z1 = df[[&#39;const&#39;, &#39;logMort&#39;, &#39;logMort_2&#39;]].values

# Compute beta 2SLS in two steps
X_hat = Z1 @ inv(Z1.T @ Z1) @ Z1.T @ X
beta_2SLS = inv(X_hat.T @ X_hat) @ X_hat.T @ y

print(beta_2SLS)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[3.08817432 0.76339075]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The 2SLS estimator does not have to be actually estimated in two stages. Combining the two formulas above, we get&lt;/p&gt;
&lt;p&gt;$$
\hat{\beta} _ {2SLS} =  \Big( X&amp;rsquo;Z (Z&amp;rsquo;Z)^{-1} Z&amp;rsquo;X \Big)^{-1} \Big( X&amp;rsquo;Z (Z&amp;rsquo;Z)^{-1} Z&amp;rsquo;y \Big)
$$&lt;/p&gt;
&lt;p&gt;which can be computed in one step.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Compute beta 2SLS in one step
beta_2SLS = inv(X_hat.T @ Z1 @ inv(Z1.T @ Z1) @ Z1.T @ X_hat) @ X_hat.T @ Z1 @ inv(Z1.T @ Z1) @ Z1.T @ y
    
print(beta_2SLS)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[3.08817432 0.76339075]
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Non-Parametric Regression</title>
      <link>https://matteocourthoud.github.io/course/ml-econ/03_nonparametric/</link>
      <pubDate>Wed, 09 Mar 2022 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/course/ml-econ/03_nonparametric/</guid>
      <description>&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Setup
%matplotlib inline
from utils.lecture03 import *
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;dataset&#34;&gt;Dataset&lt;/h3&gt;
&lt;p&gt;For this session, we are mostly going to work with the wage dataset.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df = pd.read_csv(&#39;data/Wage.csv&#39;, index_col=0)
df.head(3)
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;year&lt;/th&gt;
      &lt;th&gt;age&lt;/th&gt;
      &lt;th&gt;maritl&lt;/th&gt;
      &lt;th&gt;race&lt;/th&gt;
      &lt;th&gt;education&lt;/th&gt;
      &lt;th&gt;region&lt;/th&gt;
      &lt;th&gt;jobclass&lt;/th&gt;
      &lt;th&gt;health&lt;/th&gt;
      &lt;th&gt;health_ins&lt;/th&gt;
      &lt;th&gt;logwage&lt;/th&gt;
      &lt;th&gt;wage&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;231655&lt;/th&gt;
      &lt;td&gt;2006&lt;/td&gt;
      &lt;td&gt;18&lt;/td&gt;
      &lt;td&gt;1. Never Married&lt;/td&gt;
      &lt;td&gt;1. White&lt;/td&gt;
      &lt;td&gt;1. &amp;lt; HS Grad&lt;/td&gt;
      &lt;td&gt;2. Middle Atlantic&lt;/td&gt;
      &lt;td&gt;1. Industrial&lt;/td&gt;
      &lt;td&gt;1. &amp;lt;=Good&lt;/td&gt;
      &lt;td&gt;2. No&lt;/td&gt;
      &lt;td&gt;4.318063&lt;/td&gt;
      &lt;td&gt;75.043154&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;86582&lt;/th&gt;
      &lt;td&gt;2004&lt;/td&gt;
      &lt;td&gt;24&lt;/td&gt;
      &lt;td&gt;1. Never Married&lt;/td&gt;
      &lt;td&gt;1. White&lt;/td&gt;
      &lt;td&gt;4. College Grad&lt;/td&gt;
      &lt;td&gt;2. Middle Atlantic&lt;/td&gt;
      &lt;td&gt;2. Information&lt;/td&gt;
      &lt;td&gt;2. &amp;gt;=Very Good&lt;/td&gt;
      &lt;td&gt;2. No&lt;/td&gt;
      &lt;td&gt;4.255273&lt;/td&gt;
      &lt;td&gt;70.476020&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;161300&lt;/th&gt;
      &lt;td&gt;2003&lt;/td&gt;
      &lt;td&gt;45&lt;/td&gt;
      &lt;td&gt;2. Married&lt;/td&gt;
      &lt;td&gt;1. White&lt;/td&gt;
      &lt;td&gt;3. Some College&lt;/td&gt;
      &lt;td&gt;2. Middle Atlantic&lt;/td&gt;
      &lt;td&gt;1. Industrial&lt;/td&gt;
      &lt;td&gt;1. &amp;lt;=Good&lt;/td&gt;
      &lt;td&gt;1. Yes&lt;/td&gt;
      &lt;td&gt;4.875061&lt;/td&gt;
      &lt;td&gt;130.982177&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;This dataset contains information on wages and individual characteristics.&lt;/p&gt;
&lt;p&gt;Our main objective is going to be to explain wages using the observables contained in the dataset.&lt;/p&gt;
&lt;h2 id=&#34;polynomial-regression-and-step-functions&#34;&gt;Polynomial Regression and Step Functions&lt;/h2&gt;
&lt;p&gt;As we have seen in the first lecture, the most common way to introduce linearities is to replace the standard linear model&lt;/p&gt;
&lt;p&gt;$$
y_i = \beta_0 + \beta_1 x_i + \varepsilon_i
$$&lt;/p&gt;
&lt;p&gt;with a polynomial function&lt;/p&gt;
&lt;p&gt;$$
y_i = \beta_0 + \beta_1 x_i + \beta_2 x_i^2 + \beta_2 x_i^3 + &amp;hellip; + \varepsilon_i
$$&lt;/p&gt;
&lt;h3 id=&#34;explore-the-data&#34;&gt;Explore the Data&lt;/h3&gt;
&lt;p&gt;Suppose we want to investigate the relationship between &lt;code&gt;wage&lt;/code&gt; and &lt;code&gt;age&lt;/code&gt;. Let&amp;rsquo;s first plot the two variables.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Scatterplot of the data
df.plot.scatter(&#39;age&#39;,&#39;wage&#39;,color=&#39;w&#39;, edgecolors=&#39;k&#39;, alpha=0.3);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/03_nonparametric_10_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;polynomials-of-different-degrees&#34;&gt;Polynomials of different degrees&lt;/h3&gt;
&lt;p&gt;The relationship is highly complex and non-linear. Let&amp;rsquo;s expand our linear regression polynomials of different degrees: 1 to 5.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;X_poly1 = PolynomialFeatures(1).fit_transform(df.age.values.reshape(-1,1))
X_poly2 = PolynomialFeatures(2).fit_transform(df.age.values.reshape(-1,1))
X_poly3 = PolynomialFeatures(3).fit_transform(df.age.values.reshape(-1,1))
X_poly4 = PolynomialFeatures(4).fit_transform(df.age.values.reshape(-1,1))
X_poly5 = PolynomialFeatures(5).fit_transform(df.age.values.reshape(-1,1))
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;variables&#34;&gt;Variables&lt;/h3&gt;
&lt;p&gt;Our dependent varaible is going to be a dummy for income above 250.000 USD.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Get X and y
X = df.age
y = df.wage
y01 = (df.wage &amp;gt; 250).map({False:0, True:1}).values
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;polynomia-regression&#34;&gt;Polynomia Regression&lt;/h3&gt;
&lt;p&gt;If we run a linear regression on a 4-degree polinomial expansion of &lt;code&gt;age&lt;/code&gt;, this is what it looks like`:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Fit ols on 4th degree polynomial
fit = sm.OLS(y, X_poly4).fit()
fit.summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
    &lt;td&gt;&lt;/td&gt;       &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;const&lt;/th&gt; &lt;td&gt; -184.1542&lt;/td&gt; &lt;td&gt;   60.040&lt;/td&gt; &lt;td&gt;   -3.067&lt;/td&gt; &lt;td&gt; 0.002&lt;/td&gt; &lt;td&gt; -301.879&lt;/td&gt; &lt;td&gt;  -66.430&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;x1&lt;/th&gt;    &lt;td&gt;   21.2455&lt;/td&gt; &lt;td&gt;    5.887&lt;/td&gt; &lt;td&gt;    3.609&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    9.703&lt;/td&gt; &lt;td&gt;   32.788&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;x2&lt;/th&gt;    &lt;td&gt;   -0.5639&lt;/td&gt; &lt;td&gt;    0.206&lt;/td&gt; &lt;td&gt;   -2.736&lt;/td&gt; &lt;td&gt; 0.006&lt;/td&gt; &lt;td&gt;   -0.968&lt;/td&gt; &lt;td&gt;   -0.160&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;x3&lt;/th&gt;    &lt;td&gt;    0.0068&lt;/td&gt; &lt;td&gt;    0.003&lt;/td&gt; &lt;td&gt;    2.221&lt;/td&gt; &lt;td&gt; 0.026&lt;/td&gt; &lt;td&gt;    0.001&lt;/td&gt; &lt;td&gt;    0.013&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;x4&lt;/th&gt;    &lt;td&gt;-3.204e-05&lt;/td&gt; &lt;td&gt; 1.64e-05&lt;/td&gt; &lt;td&gt;   -1.952&lt;/td&gt; &lt;td&gt; 0.051&lt;/td&gt; &lt;td&gt;-6.42e-05&lt;/td&gt; &lt;td&gt; 1.45e-07&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;h3 id=&#34;measures-of-fit&#34;&gt;Measures of Fit&lt;/h3&gt;
&lt;p&gt;In this case, the single coefficients are not of particular interest. We are mostly interested in the best capturing the relationship between &lt;code&gt;age&lt;/code&gt; and &lt;code&gt;wage&lt;/code&gt;. How can we pick among thedifferent polynomials?&lt;/p&gt;
&lt;p&gt;We compare different polynomial degrees. For each regression, we are going to look at a series of metrics:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;absolute residuals&lt;/li&gt;
&lt;li&gt;sum of squared residuals&lt;/li&gt;
&lt;li&gt;the difference in SSR w.r (SSR).t the 0-degree case&lt;/li&gt;
&lt;li&gt;F statistic&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Run regressions
fit_1 = sm.OLS(y, X_poly1).fit()
fit_2 = sm.OLS(y, X_poly2).fit()
fit_3 = sm.OLS(y, X_poly3).fit()
fit_4 = sm.OLS(y, X_poly4).fit()
fit_5 = sm.OLS(y, X_poly5).fit()

# Compare fit
sm.stats.anova_lm(fit_1, fit_2, fit_3, fit_4, fit_5, typ=1)
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;df_resid&lt;/th&gt;
      &lt;th&gt;ssr&lt;/th&gt;
      &lt;th&gt;df_diff&lt;/th&gt;
      &lt;th&gt;ss_diff&lt;/th&gt;
      &lt;th&gt;F&lt;/th&gt;
      &lt;th&gt;Pr(&amp;gt;F)&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;2998.0&lt;/td&gt;
      &lt;td&gt;5.022216e+06&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;2997.0&lt;/td&gt;
      &lt;td&gt;4.793430e+06&lt;/td&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;228786.010128&lt;/td&gt;
      &lt;td&gt;143.593107&lt;/td&gt;
      &lt;td&gt;2.363850e-32&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;2996.0&lt;/td&gt;
      &lt;td&gt;4.777674e+06&lt;/td&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;15755.693664&lt;/td&gt;
      &lt;td&gt;9.888756&lt;/td&gt;
      &lt;td&gt;1.679202e-03&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;2995.0&lt;/td&gt;
      &lt;td&gt;4.771604e+06&lt;/td&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;6070.152124&lt;/td&gt;
      &lt;td&gt;3.809813&lt;/td&gt;
      &lt;td&gt;5.104620e-02&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;2994.0&lt;/td&gt;
      &lt;td&gt;4.770322e+06&lt;/td&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;1282.563017&lt;/td&gt;
      &lt;td&gt;0.804976&lt;/td&gt;
      &lt;td&gt;3.696820e-01&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;The polynomial degree 4 seems best.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Set polynomial X to 4th degree
X_poly = X_poly4
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;binary-dependent-variable&#34;&gt;Binary Dependent Variable&lt;/h3&gt;
&lt;p&gt;Since we have a binary dependent variable, it would be best to account for it in our regression framework. One way to do so, is to run a logistic regression.&lt;/p&gt;
&lt;p&gt;How to interpret a Logistic Regression?&lt;/p&gt;
&lt;p&gt;$$
y = \mathbb I \ \Big( \beta_0 + \beta_1 x_i + \beta_2 x_i^2 + \beta_2 x_i^3 + &amp;hellip; + \varepsilon_i \Big)
$$&lt;/p&gt;
&lt;p&gt;where $\mathbb I(\cdot)$ is an indicator function and now $\varepsilon_i$ is the error term.&lt;/p&gt;
&lt;h3 id=&#34;binomial-link-functions&#34;&gt;Binomial Link Functions&lt;/h3&gt;
&lt;p&gt;Depending on the assumed distribution of the error term, we get different results. I list below the error types supported by the &lt;code&gt;Binomial&lt;/code&gt; family.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# List link functions for the Binomial family
sm.families.family.Binomial.links
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[statsmodels.genmod.families.links.logit,
 statsmodels.genmod.families.links.probit,
 statsmodels.genmod.families.links.cauchy,
 statsmodels.genmod.families.links.log,
 statsmodels.genmod.families.links.cloglog,
 statsmodels.genmod.families.links.identity]
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;logit-link-function&#34;&gt;Logit Link Function&lt;/h3&gt;
&lt;p&gt;We are going to pick the &lt;code&gt;logit&lt;/code&gt; link, i.e. we are going to assume that the error term is Type 1 Extreme Value (or Gumbel) distributed. It instead we take the usual standard normal distribution assumption for $\varepsilon_i$, we get &lt;code&gt;probit&lt;/code&gt; regression.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Pick the logit link for the Binomial family
logit_link = sm.families.Binomial(sm.genmod.families.links.logit())
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Given the error distribution, we can write the probability that $y=1$ as&lt;/p&gt;
&lt;p&gt;$$
\Pr(y=1) = \frac{e^{ \beta_0 + \beta_1 x_i + \beta_2 x_i^2 + \beta_2 x_i^3 + &amp;hellip; + \varepsilon_i }}{1 + e^{ \beta_0 + \beta_1 x_i + \beta_2 x_i^2 + \beta_2 x_i^3 + &amp;hellip; + \varepsilon_i } }
$$&lt;/p&gt;
&lt;h3 id=&#34;logistic-regression&#34;&gt;Logistic Regression&lt;/h3&gt;
&lt;p&gt;We now estimate the regression and plot the estimated relationship between &lt;code&gt;age&lt;/code&gt; and &lt;code&gt;wage&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Run logistic regression
logit_poly = sm.GLM(y01, X_poly, family=logit_link).fit()
logit_poly.summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
    &lt;td&gt;&lt;/td&gt;       &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;z&lt;/th&gt;      &lt;th&gt;P&gt;|z|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;const&lt;/th&gt; &lt;td&gt; -109.5530&lt;/td&gt; &lt;td&gt;   47.655&lt;/td&gt; &lt;td&gt;   -2.299&lt;/td&gt; &lt;td&gt; 0.022&lt;/td&gt; &lt;td&gt; -202.956&lt;/td&gt; &lt;td&gt;  -16.150&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;x1&lt;/th&gt;    &lt;td&gt;    8.9950&lt;/td&gt; &lt;td&gt;    4.187&lt;/td&gt; &lt;td&gt;    2.148&lt;/td&gt; &lt;td&gt; 0.032&lt;/td&gt; &lt;td&gt;    0.789&lt;/td&gt; &lt;td&gt;   17.201&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;x2&lt;/th&gt;    &lt;td&gt;   -0.2816&lt;/td&gt; &lt;td&gt;    0.135&lt;/td&gt; &lt;td&gt;   -2.081&lt;/td&gt; &lt;td&gt; 0.037&lt;/td&gt; &lt;td&gt;   -0.547&lt;/td&gt; &lt;td&gt;   -0.016&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;x3&lt;/th&gt;    &lt;td&gt;    0.0039&lt;/td&gt; &lt;td&gt;    0.002&lt;/td&gt; &lt;td&gt;    2.022&lt;/td&gt; &lt;td&gt; 0.043&lt;/td&gt; &lt;td&gt;    0.000&lt;/td&gt; &lt;td&gt;    0.008&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;x4&lt;/th&gt;    &lt;td&gt;-1.949e-05&lt;/td&gt; &lt;td&gt; 9.91e-06&lt;/td&gt; &lt;td&gt;   -1.966&lt;/td&gt; &lt;td&gt; 0.049&lt;/td&gt; &lt;td&gt;-3.89e-05&lt;/td&gt; &lt;td&gt;-6.41e-08&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;h3 id=&#34;linear-model-comparison&#34;&gt;Linear Model Comparison&lt;/h3&gt;
&lt;p&gt;What is the difference with the linear model?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Run OLS regression with binary outcome
ols_poly = sm.OLS(y01, X_poly).fit()
ols_poly.summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
    &lt;td&gt;&lt;/td&gt;       &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;const&lt;/th&gt; &lt;td&gt;   -0.1126&lt;/td&gt; &lt;td&gt;    0.240&lt;/td&gt; &lt;td&gt;   -0.468&lt;/td&gt; &lt;td&gt; 0.640&lt;/td&gt; &lt;td&gt;   -0.584&lt;/td&gt; &lt;td&gt;    0.359&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;x1&lt;/th&gt;    &lt;td&gt;    0.0086&lt;/td&gt; &lt;td&gt;    0.024&lt;/td&gt; &lt;td&gt;    0.363&lt;/td&gt; &lt;td&gt; 0.717&lt;/td&gt; &lt;td&gt;   -0.038&lt;/td&gt; &lt;td&gt;    0.055&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;x2&lt;/th&gt;    &lt;td&gt;   -0.0002&lt;/td&gt; &lt;td&gt;    0.001&lt;/td&gt; &lt;td&gt;   -0.270&lt;/td&gt; &lt;td&gt; 0.787&lt;/td&gt; &lt;td&gt;   -0.002&lt;/td&gt; &lt;td&gt;    0.001&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;x3&lt;/th&gt;    &lt;td&gt; 3.194e-06&lt;/td&gt; &lt;td&gt; 1.23e-05&lt;/td&gt; &lt;td&gt;    0.260&lt;/td&gt; &lt;td&gt; 0.795&lt;/td&gt; &lt;td&gt;-2.09e-05&lt;/td&gt; &lt;td&gt; 2.73e-05&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;x4&lt;/th&gt;    &lt;td&gt;-1.939e-08&lt;/td&gt; &lt;td&gt; 6.57e-08&lt;/td&gt; &lt;td&gt;   -0.295&lt;/td&gt; &lt;td&gt; 0.768&lt;/td&gt; &lt;td&gt;-1.48e-07&lt;/td&gt; &lt;td&gt; 1.09e-07&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;The magnitude of the coefficients is different, but the signs are the same.&lt;/p&gt;
&lt;h3 id=&#34;plot-data-and-predictions&#34;&gt;Plot data and predictions&lt;/h3&gt;
&lt;p&gt;Let&amp;rsquo;s plot the estimated curves against the data distribution.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Generate predictions
x_grid = np.arange(df.age.min(), df.age.max()).reshape(-1,1)
X_poly_test = PolynomialFeatures(4).fit_transform(x_grid)
y_hat1 = sm.OLS(y, X_poly).fit().predict(X_poly_test)
y01_hat1 = logit_poly.predict(X_poly_test)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plot_predictions(X, y, x_grid, y01, y_hat1, y01_hat1, &#39;Figure 7.1: Degree-4 Polynomial&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/03_nonparametric_44_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Which is remindful of
&lt;img src=&#34;../figures/nonlinearities.jpg&#34; alt=&#34;Le Petit Prince - Elephant figure&#34; title=&#34;Nonlinearities&#34;&gt;&lt;/p&gt;
&lt;p&gt;Using polynomial functions of the features as predictors in a linear model imposes a global structure on the non-linear function of &lt;code&gt;age&lt;/code&gt;. We can instead use step functions in order to avoid imposing such a global structure.&lt;/p&gt;
&lt;p&gt;For example, we could break the range of &lt;code&gt;age&lt;/code&gt; into bins, and fit a different constant in each bin.&lt;/p&gt;
&lt;h2 id=&#34;step-functions&#34;&gt;Step Functions&lt;/h2&gt;
&lt;p&gt;Building a step function means first picking $K$ cutpoints $c_1 , c_2 , . . . , c_K$ in the range of &lt;code&gt;age&lt;/code&gt;,
and then construct $K + 1$ new variables&lt;/p&gt;
&lt;p&gt;$$
C_0(&lt;code&gt;age&lt;/code&gt;) = \mathbb I ( &lt;code&gt;age&lt;/code&gt; &amp;lt; c_1) \
C_1(&lt;code&gt;age&lt;/code&gt;) = \mathbb I ( c_1 &amp;lt; &lt;code&gt;age&lt;/code&gt; &amp;lt; c_2) \
C_2(&lt;code&gt;age&lt;/code&gt;) = \mathbb I ( c_2 &amp;lt; &lt;code&gt;age&lt;/code&gt; &amp;lt; c_3) \
&amp;hellip; \
C_{K-1}(&lt;code&gt;age&lt;/code&gt;) = \mathbb I ( c_{K-1} &amp;lt; &lt;code&gt;age&lt;/code&gt; &amp;lt; c_K) \
C_K(&lt;code&gt;age&lt;/code&gt;) = \mathbb I ( c_K &amp;lt; &lt;code&gt;age&lt;/code&gt;) \
$$&lt;/p&gt;
&lt;p&gt;where $\mathbb I(\cdot)$ is the indicator function.&lt;/p&gt;
&lt;h3 id=&#34;binning&#34;&gt;Binning&lt;/h3&gt;
&lt;p&gt;First, we generate the cuts.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Generate cuts for the variable age
df_cut, bins = pd.cut(df.age, 4, retbins=True, right=True)
df_cut.value_counts(sort=False)
type(df_cut)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;pandas.core.series.Series
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s generate a DataFrame out of this series.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Generate bins for &amp;quot;age&amp;quot; from the cuts
df_steps = pd.concat([df.age, df_cut, df.wage], keys=[&#39;age&#39;,&#39;age_cuts&#39;,&#39;wage&#39;], axis=1)
df_steps.head(5)
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;age&lt;/th&gt;
      &lt;th&gt;age_cuts&lt;/th&gt;
      &lt;th&gt;wage&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;231655&lt;/th&gt;
      &lt;td&gt;18&lt;/td&gt;
      &lt;td&gt;(17.938, 33.5]&lt;/td&gt;
      &lt;td&gt;75.043154&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;86582&lt;/th&gt;
      &lt;td&gt;24&lt;/td&gt;
      &lt;td&gt;(17.938, 33.5]&lt;/td&gt;
      &lt;td&gt;70.476020&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;161300&lt;/th&gt;
      &lt;td&gt;45&lt;/td&gt;
      &lt;td&gt;(33.5, 49.0]&lt;/td&gt;
      &lt;td&gt;130.982177&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;155159&lt;/th&gt;
      &lt;td&gt;43&lt;/td&gt;
      &lt;td&gt;(33.5, 49.0]&lt;/td&gt;
      &lt;td&gt;154.685293&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;11443&lt;/th&gt;
      &lt;td&gt;50&lt;/td&gt;
      &lt;td&gt;(49.0, 64.5]&lt;/td&gt;
      &lt;td&gt;75.043154&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;h3 id=&#34;dummy-variables&#34;&gt;Dummy Variables&lt;/h3&gt;
&lt;p&gt;Now we can generate different dummy variables out of each bin.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Create dummy variables for the age groups
df_steps_dummies = pd.get_dummies(df_steps[&#39;age_cuts&#39;])

# Statsmodels requires explicit adding of a constant (intercept)
df_steps_dummies = sm.add_constant(df_steps_dummies)
df_steps_dummies.head(5)
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;const&lt;/th&gt;
      &lt;th&gt;(17.938, 33.5]&lt;/th&gt;
      &lt;th&gt;(33.5, 49.0]&lt;/th&gt;
      &lt;th&gt;(49.0, 64.5]&lt;/th&gt;
      &lt;th&gt;(64.5, 80.0]&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;231655&lt;/th&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;86582&lt;/th&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;161300&lt;/th&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;155159&lt;/th&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;11443&lt;/th&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;h3 id=&#34;stepwise-regression&#34;&gt;Stepwise Regression&lt;/h3&gt;
&lt;p&gt;We are now ready to run our regression&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Generate our new X variable
X_step = df_steps_dummies.drop(df_steps_dummies.columns[1], axis=1)

# OLS Regression on step functions
ols_step = sm.OLS(y, X_step).fit()
ols_step.summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
        &lt;td&gt;&lt;/td&gt;          &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;const&lt;/th&gt;        &lt;td&gt;   94.1584&lt;/td&gt; &lt;td&gt;    1.476&lt;/td&gt; &lt;td&gt;   63.790&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;   91.264&lt;/td&gt; &lt;td&gt;   97.053&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;(33.5, 49.0]&lt;/th&gt; &lt;td&gt;   24.0535&lt;/td&gt; &lt;td&gt;    1.829&lt;/td&gt; &lt;td&gt;   13.148&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;   20.466&lt;/td&gt; &lt;td&gt;   27.641&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;(49.0, 64.5]&lt;/th&gt; &lt;td&gt;   23.6646&lt;/td&gt; &lt;td&gt;    2.068&lt;/td&gt; &lt;td&gt;   11.443&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;   19.610&lt;/td&gt; &lt;td&gt;   27.719&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;(64.5, 80.0]&lt;/th&gt; &lt;td&gt;    7.6406&lt;/td&gt; &lt;td&gt;    4.987&lt;/td&gt; &lt;td&gt;    1.532&lt;/td&gt; &lt;td&gt; 0.126&lt;/td&gt; &lt;td&gt;   -2.139&lt;/td&gt; &lt;td&gt;   17.420&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;From the regression outcome we can see that most bin coefficients are significant, except for the last one.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Put the test data in the same bins as the training data.
bin_mapping = np.digitize(x_grid.ravel(), bins)
bin_mapping
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2,
       2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
       3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4])
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Get dummies, drop first dummy category, add constant
X_step_test = sm.add_constant(pd.get_dummies(bin_mapping).drop(1, axis=1))
X_step_test.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;const&lt;/th&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;th&gt;4&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Step prediction
y_hat2 = ols_step.predict(X_step_test)
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;logistic-step-regression&#34;&gt;Logistic Step Regression&lt;/h3&gt;
&lt;p&gt;We are going again to run a logistic regression, given that our outcome is binary.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Logistic regression on step functions
logit_step = sm.GLM(y01, X_step, family=logit_link).fit()
y01_hat2 = logit_step.predict(X_step_test)
logit_step.summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
        &lt;td&gt;&lt;/td&gt;          &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;z&lt;/th&gt;      &lt;th&gt;P&gt;|z|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;const&lt;/th&gt;        &lt;td&gt;   -5.0039&lt;/td&gt; &lt;td&gt;    0.449&lt;/td&gt; &lt;td&gt;  -11.152&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;   -5.883&lt;/td&gt; &lt;td&gt;   -4.124&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;(33.5, 49.0]&lt;/th&gt; &lt;td&gt;    1.5998&lt;/td&gt; &lt;td&gt;    0.474&lt;/td&gt; &lt;td&gt;    3.378&lt;/td&gt; &lt;td&gt; 0.001&lt;/td&gt; &lt;td&gt;    0.672&lt;/td&gt; &lt;td&gt;    2.528&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;(49.0, 64.5]&lt;/th&gt; &lt;td&gt;    1.7147&lt;/td&gt; &lt;td&gt;    0.488&lt;/td&gt; &lt;td&gt;    3.512&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    0.758&lt;/td&gt; &lt;td&gt;    2.672&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;(64.5, 80.0]&lt;/th&gt; &lt;td&gt;    0.7413&lt;/td&gt; &lt;td&gt;    1.102&lt;/td&gt; &lt;td&gt;    0.672&lt;/td&gt; &lt;td&gt; 0.501&lt;/td&gt; &lt;td&gt;   -1.420&lt;/td&gt; &lt;td&gt;    2.902&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;h3 id=&#34;plotting&#34;&gt;Plotting&lt;/h3&gt;
&lt;p&gt;How does the predicted function looks like?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plot_predictions(X, y, x_grid, y01, y_hat2, y01_hat2, &#39;Figure 7.2: Piecewise Constant&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/03_nonparametric_69_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;regression-splines&#34;&gt;Regression Splines&lt;/h2&gt;
&lt;p&gt;Spline regression, or piece-wise polynomial regression, involves fitting separate low-degree polynomials
over different regions of $X$. The idea is to have one regression specification but with different coefficients in different parts of the $X$ range. The points where the coefficients change are called knots.&lt;/p&gt;
&lt;p&gt;For example, we could have a third degree polynomial &lt;em&gt;and&lt;/em&gt; splitting the sample in two.&lt;/p&gt;
&lt;p&gt;$$
y_{i}=\left{\begin{array}{ll}
\beta_{01}+\beta_{11} x_{i}+\beta_{21} x_{i}^{2}+\beta_{31} x_{i}^{3}+\epsilon_{i} &amp;amp; \text { if } x_{i}&amp;lt;c \
\beta_{02}+\beta_{12} x_{i}+\beta_{22} x_{i}^{2}+\beta_{32} x_{i}^{3}+\epsilon_{i} &amp;amp; \text { if } x_{i} \geq c
\end{array}\right.
$$&lt;/p&gt;
&lt;p&gt;We have now two sets of coefficients, one for each subsample.&lt;/p&gt;
&lt;p&gt;Generally, using more knots leads to a more flexible piecewise polynomial. Also increasing the degree of the polynomial increases the degree of flexibility.&lt;/p&gt;
&lt;h3 id=&#34;example&#34;&gt;Example&lt;/h3&gt;
&lt;p&gt;We are now going to plot 4 different examples for the &lt;code&gt;age&lt;/code&gt; &lt;code&gt;wage&lt;/code&gt; relationship:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Discontinuous piecewise cubic&lt;/li&gt;
&lt;li&gt;Continuous piecewise cubic&lt;/li&gt;
&lt;li&gt;Quadratic (continuous)&lt;/li&gt;
&lt;li&gt;Continuous piecewise linear&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Cut dataset
df_short = df.iloc[:80,:]
X_short = df_short.age
y_short = df_short.wage
x_grid_short = np.arange(df_short.age.min(), df_short.age.max()+1).reshape(-1,1)

# 1. Discontinuous piecewise cubic
spline1 = &amp;quot;bs(x, knots=(50,50,50,50), degree=3, include_intercept=False)&amp;quot;

# 2. Continuous piecewise cubic
spline2 = &amp;quot;bs(x, knots=(50,50,50), degree=3, include_intercept=False)&amp;quot;

# 3. Quadratic (continuous)
spline3 = &amp;quot;bs(x, knots=(%s,%s), degree=2, include_intercept=False)&amp;quot; % (min(df.age), min(df.age))

# 4. Continuous piecewise linear
spline4 = &amp;quot;bs(x, knots=(%s,50), degree=1, include_intercept=False)&amp;quot; % min(df.age)
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;generate-predictions&#34;&gt;Generate Predictions&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Generate spline predictions
def fit_predict_spline(spline, X, y, x_grid):
    transformed_x = dmatrix(spline, {&amp;quot;x&amp;quot;: X}, return_type=&#39;dataframe&#39;)
    fit = sm.GLM(y, transformed_x).fit()
    y_hat = fit.predict(dmatrix(spline, {&amp;quot;x&amp;quot;: x_grid}, return_type=&#39;dataframe&#39;))
    return y_hat

y_hats = [fit_predict_spline(s, X_short, y_short, x_grid_short) for s in [spline1, spline2, spline3, spline4]]
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;plotting-1&#34;&gt;Plotting&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plot_splines(df_short, x_grid_short, y_hats)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/03_nonparametric_78_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;comment&#34;&gt;Comment&lt;/h3&gt;
&lt;p&gt;The first example makes us think on why would we want out function to be discontinuous. Unless we expect a sudden wage jump at a certain age, we would like the function to be continuous. However, if for example we split &lt;code&gt;age&lt;/code&gt; around the retirement age, we might expect a discontinuity.&lt;/p&gt;
&lt;p&gt;The second example (top right) makes us think on why would we want out function not to be differentiable. Unless we have some specific mechanism in mind, ususally there is a trade-off between making the function non-differentiable or increasing the degree of the polynomial, as the last two examples show us. We get a similar fit with a quadratic fit or a discontinuous linear fit. The main difference is that in the second case we are picking the discontinuity point by hand instead of letting the data choose how to change the slope of the curve.&lt;/p&gt;
&lt;h3 id=&#34;the-spline-basis-representation&#34;&gt;The Spline Basis Representation&lt;/h3&gt;
&lt;p&gt;How can we fit a piecewise degree-d polynomial under the constraint that it (and possibly its first d − 1 derivatives) be continuous?&lt;/p&gt;
&lt;p&gt;The most direct way to represent a cubic spline is to start off with a basis for a cubic polynomial—namely, x,x2,x3—and then add one truncated power basis function per knot. A truncated power basis function is defined as&lt;/p&gt;
&lt;p&gt;$$
h(x, c)=(x-c)_{+}^{3} = \Bigg{\begin{array}{cl}
(x-c)^{3} &amp;amp; \text { if } x&amp;gt;c \
0 &amp;amp; \text { otherwise }
\end{array}
$$&lt;/p&gt;
&lt;p&gt;One can show that adding a term of the form $\beta_4 h(x, c)$ to the model for a cubic polynomial will lead to a discontinuity in only the third derivative at $c$; the function will remain continuous, with continuous first and second derivatives, at each of the knots.&lt;/p&gt;
&lt;h3 id=&#34;cubic-splines&#34;&gt;Cubic Splines&lt;/h3&gt;
&lt;p&gt;One way to specify the spline is using nodes and degrees of freedom.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Specifying 3 knots and 3 degrees of freedom
spline5 = &amp;quot;bs(x, knots=(25,40,60), degree=3, include_intercept=False)&amp;quot;
pred5 = fit_predict_spline(spline5, X, y, x_grid)
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;no-knots&#34;&gt;No Knots&lt;/h3&gt;
&lt;p&gt;When we fit a spline, where should we place the knots?&lt;/p&gt;
&lt;p&gt;The regression spline is most flexible in regions that contain a lot of knots, because in those regions the polynomial coefficients can change rapidly. Hence, one option is to place more knots in places where we feel the function might vary most rapidly, and to place fewer knots where it seems more stable. While this option can work well, in practice it is common to place knots in a uniform fashion. One way to do this is to specify the desired degrees of freedom, and then have the software automatically place the corresponding number of knots at uniform quantiles of the data.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Specifying degree 3 and 6 degrees of freedom 
spline6 = &amp;quot;bs(x, df=6, degree=3, include_intercept=False)&amp;quot;
pred6 = fit_predict_spline(spline6, X, y, x_grid)
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;natural-splines&#34;&gt;Natural Splines&lt;/h3&gt;
&lt;p&gt;A natural spline is a regression spline with additional boundary constraints: the function is required to be linear at the boundary (in the region where X is smaller than the smallest knot, or larger than the largest knot). This addi- tional constraint means that natural splines generally produce more stable estimates at the boundaries.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Natural spline with 4 degrees of freedom
spline7 = &amp;quot;cr(x, df=4)&amp;quot;
pred7 = fit_predict_spline(spline7, X, y, x_grid)
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;comparison&#34;&gt;Comparison&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Compare predictons
preds = [pred5, pred6, pred7]
labels = [&#39;degree 3, knots 3&#39;, &#39;degree 3, degrees of freedom 3&#39;, &#39;natural, degrees of freedom 4&#39;]
compare_predictions(X, y, x_grid, preds, labels)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/03_nonparametric_93_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;comparison-to-polynomial-regression&#34;&gt;Comparison to Polynomial Regression&lt;/h3&gt;
&lt;p&gt;Regression splines often give superior results to polynomial regression. This is because unlike polynomials, which must use a high degree to produce flexible fits, splines introduce flexibility by increasing the number of knots but keeping the degree fixed.&lt;/p&gt;
&lt;p&gt;We are now fitting a polynomial of degree 15 and a spline with 15 degrees of freedom.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Polynomial of degree 15
X_poly15 = PolynomialFeatures(15).fit_transform(df.age.values.reshape(-1,1))
ols_poly_15 = sm.OLS(y, X_poly15).fit()
pred8 = ols_poly_15.predict(PolynomialFeatures(15).fit_transform(x_grid))

# Spline with 15 degrees of freedon
spline9 = &amp;quot;bs(x, df=15, degree=3, include_intercept=False)&amp;quot;
pred9 = fit_predict_spline(spline9, X, y, x_grid)
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;plotting-2&#34;&gt;Plotting&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Compare predictons
preds = [pred8, pred9]
labels = [&#39;Polynomial&#39;, &#39;Spline&#39;]
compare_predictions(X, y, x_grid, preds, labels)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/03_nonparametric_98_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;As we can see, despite the two regressions having the same degrees of freedom, the polynomial fit is much more volatile. We can compare them along some dimensions.&lt;/p&gt;
&lt;h2 id=&#34;local-regression&#34;&gt;Local Regression&lt;/h2&gt;
&lt;p&gt;So far we have looked at so-called &amp;ldquo;&lt;em&gt;global methods&lt;/em&gt;&amp;rdquo;: methods that try to fit a unique function specification over the whole data. The function specification can be complex, as in the case of splines, but can be expressed globally.&lt;/p&gt;
&lt;p&gt;Local regression is a different approach for fitting flexible non-linear functions, which involves computing the fit at a target point $x_0$ using only the nearby training observations.&lt;/p&gt;
&lt;h3 id=&#34;details&#34;&gt;Details&lt;/h3&gt;
&lt;p&gt;How does local regression work?&lt;/p&gt;
&lt;p&gt;Ingredients: $X$, $y$.&lt;/p&gt;
&lt;p&gt;How to you output a prediction $\hat y_i$ at a new point $x_i$?&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Take a number of points in $X$ close to $x_i$: $X_{\text{close-to-i}}$&lt;/li&gt;
&lt;li&gt;Assign a weight to each of there points&lt;/li&gt;
&lt;li&gt;Fit a weigthed least squares regression of $X_{\text{close-to-i}}$ on $y_{\text{close-to-i}}$&lt;/li&gt;
&lt;li&gt;Use the estimated coefficients $\hat \beta$ to predict $\hat y_i = \hat \beta_0 + \hat \beta_1 x_i$&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;generate-data&#34;&gt;Generate Data&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Set seed
np.random.seed(1)

# Generate data
X_sim = np.sort(np.random.uniform(0,1,100))
e = np.random.uniform(-.5,.5,100)
y_sim = -4*X_sim**2 + 3*X_sim + e

# True Generating process without noise
X_grid = np.linspace(0,1,100)
y_grid = -4*X_grid**2 + 3*X_grid
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;plotting-3&#34;&gt;Plotting&lt;/h3&gt;
&lt;p&gt;Let&amp;rsquo;s visualize the simulated data and the curve without noise.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plot_simulated_data(X_sim, y_sim, X_grid, y_grid);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/03_nonparametric_108_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;fit-ll-regression&#34;&gt;Fit LL Regression&lt;/h3&gt;
&lt;p&gt;Now we fit a local linear regression.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Settings
spec = &#39;ll&#39;
bandwidth = 0.1
kernel = &#39;gaussian&#39;

# Locally linear regression
local_reg = KernelReg(y_sim, X_sim.reshape(-1,1), 
                      var_type=&#39;c&#39;, 
                      reg_type=spec, 
                      bw=[bandwidth])
y_hat = KernelReg.fit(local_reg)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What do the parameters mean?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;var_type&lt;/code&gt;: dependent variable type (&lt;code&gt;c&lt;/code&gt; i.e. &lt;em&gt;continuous&lt;/em&gt;)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;reg_type&lt;/code&gt;: local regression specification (&lt;code&gt;ll&lt;/code&gt; i.e. &lt;em&gt;locally linear&lt;/em&gt;)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;bw&lt;/code&gt;      : bandwidth length (&lt;em&gt;0.1&lt;/em&gt;)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;ckertype&lt;/code&gt;: kernel type (&lt;em&gt;gaussian&lt;/em&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;prediction&#34;&gt;Prediction&lt;/h3&gt;
&lt;p&gt;What does the prediction looks like?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;fig, ax = plot_simulated_data(X_sim, y_sim, X_grid, y_grid);
make_figure_7_9a(fig, ax, X_sim, y_hat);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/03_nonparametric_115_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;details-1&#34;&gt;Details&lt;/h3&gt;
&lt;p&gt;How exactly was the prediction generated? It was generated pointwise. We are now going to look at the prediction at one particular point: $x_i=0.5$.&lt;/p&gt;
&lt;p&gt;We proceed as follows:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;We select the focal point: $x_i=0.5$&lt;/li&gt;
&lt;li&gt;We select observations close to $\ x_i$, i.e. $x_{\text{close to i}} = { x \in X : |x_i - x| &amp;lt; 0.1 } \ $ and $ \ y_{\text{close to i}} = { y \in Y : |x_i - x| &amp;lt; 0.1 }$&lt;/li&gt;
&lt;li&gt;We apply gaussian weights&lt;/li&gt;
&lt;li&gt;We run a weighted linear regression of $y_{\text{close to i}}$ on $x_{\text{close to i}}$&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Get local X and y
x_i = 0.5
close_to_i = (x_i-bandwidth &amp;lt; X_sim) &amp;amp; (X_sim &amp;lt; x_i+bandwidth)
X_tilde = X_sim[close_to_i]
y_tilde = y_sim[close_to_i]

# Get local estimates
local_estimate = KernelReg.fit(local_reg, data_predict=[x_i])
y_i_hat = local_estimate[0]
beta_i_hat = local_estimate[1]
alpha_i_hat = y_i_hat - beta_i_hat*x_i
print(&#39;Estimates: alpha=%1.4f, beta=%1.4f&#39; % (alpha_i_hat, beta_i_hat))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Estimates: alpha=0.7006, beta=-0.6141
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;visualization&#34;&gt;Visualization&lt;/h3&gt;
&lt;p&gt;Now we can use the locally estimated coefficients to predict the value of $\hat y_i(x_i)$ for $x_i = 0.5$.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Build local predictions
close_to_i_grid = (x_i-bandwidth &amp;lt; X_grid) &amp;amp; (X_grid &amp;lt; x_i+bandwidth)
X_grid_tilde = X_grid[close_to_i_grid].reshape(-1,1)
y_grid_tilde = alpha_i_hat + X_grid_tilde*beta_i_hat
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;fig, ax = plot_simulated_data(X_sim, y_sim, X_grid, y_grid);
make_figure_7_9a(fig, ax, X_sim, y_hat);
make_figure_7_9b(fig, ax, X_tilde, y_tilde, X_grid_tilde, y_grid_tilde, x_i, y_i_hat)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/03_nonparametric_122_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;zooming-in&#34;&gt;Zooming in&lt;/h3&gt;
&lt;p&gt;We can zoom in and look only at the &amp;ldquo;&lt;em&gt;close to i&lt;/em&gt;&amp;rdquo; sample.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sns.regplot(X_tilde, y_tilde);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/03_nonparametric_125_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Why is the line upward sloped? We forgot the gaussian weights.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Weights
w = norm.pdf((X_sim-x_i)/bandwidth)

# Estimate LWS
mod_wls = sm.WLS(y_sim, sm.add_constant(X_sim), weights=w)
results = mod_wls.fit()

print(&#39;Estimates: alpha=%1.4f, beta=%1.4f&#39; % tuple(results.params))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Estimates: alpha=0.7006, beta=-0.6141
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We indeed got the same estimates as before. Note two things:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;the badwidth defines the scale parameter of the gaussian weights&lt;/li&gt;
&lt;li&gt;our locally linear regression is acqually global&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;plotting-4&#34;&gt;Plotting&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;make_figure_7_9d(X_sim, y_sim, w, results, X_grid, x_i, y_i_hat)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/03_nonparametric_130_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Now the slope is indeed negative, as in the locally linear regression.&lt;/p&gt;
&lt;h2 id=&#34;generalized-additive-models&#34;&gt;Generalized Additive Models&lt;/h2&gt;
&lt;p&gt;Generalized additive models (GAMs) provide a general framework for extending a standard linear model by allowing non-linear functions of each of the variables, while maintaining additivity.&lt;/p&gt;
&lt;h3 id=&#34;gam-for-regression-problems&#34;&gt;GAM for Regression Problems&lt;/h3&gt;
&lt;p&gt;Imagine to extend the general regression framework to some separabily additive model of the form&lt;/p&gt;
&lt;p&gt;$$
y_i = \beta_0 + \sum_{k=1}^K \beta_k f_k(x_{ik}) + \varepsilon_i
$$&lt;/p&gt;
&lt;p&gt;It is called an additive model because we calculate a separate $f_k$ for each $X_k$, and then add together all of their contributions.&lt;/p&gt;
&lt;p&gt;Consider for example the following model&lt;/p&gt;
&lt;p&gt;$$
\text{wage} = \beta_0 + f_1(\text{year}) + f_2(\text{age}) + f_3(\text{education}) + \varepsilon
$$&lt;/p&gt;
&lt;h3 id=&#34;example-1&#34;&gt;Example&lt;/h3&gt;
&lt;p&gt;We are going to use the following functions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$f_1$: natural spline with 8 degrees of freedom&lt;/li&gt;
&lt;li&gt;$f_2$: natural spline with 10 degrees of freedom&lt;/li&gt;
&lt;li&gt;$f_3$: step function&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Set X and y
df[&#39;education_&#39;] = LabelEncoder().fit_transform(df[&amp;quot;education&amp;quot;])
X = df[[&#39;year&#39;,&#39;age&#39;,&#39;education_&#39;]].to_numpy()
y = df[[&#39;wage&#39;]].to_numpy()

## model
linear_gam = LinearGAM(s(0, n_splines=8) + s(1, n_splines=10) + f(2))
linear_gam.gridsearch(X, y);
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;100% (11 of 11) |########################| Elapsed Time: 0:00:00 Time:  0:00:00
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;plotting-5&#34;&gt;Plotting&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plot_gam(linear_gam)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/03_nonparametric_140_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;pros-and-cons&#34;&gt;Pros and Cons&lt;/h3&gt;
&lt;p&gt;Before we move on, let us summarize the &lt;strong&gt;advantages&lt;/strong&gt; of a GAM.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;GAMs allow us to fit a non-linear $f_k$ to each $X_k$, so that we can automatically model non-linear relationships that standard linear regression will miss&lt;/li&gt;
&lt;li&gt;The non-linear fits can potentially make more accurate predictions&lt;/li&gt;
&lt;li&gt;Because the model is additive, we can still examine the effect of each $X_k$ on $Y$ separately&lt;/li&gt;
&lt;li&gt;The smoothness of the function $f_k$ for the variable $X_k$ can be summarized via degrees of freedom.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The main &lt;strong&gt;limitation&lt;/strong&gt; of GAMs is that the model is restricted to be additive. With many variables, important interactions can be missed. However, as with linear regression, we can manually add interaction terms to the GAM model by including additional predictors of the form $X_j \times X_k$.&lt;/p&gt;
&lt;h3 id=&#34;gams-for-classification-problems&#34;&gt;GAMs for Classification Problems&lt;/h3&gt;
&lt;p&gt;We can use GAMs also with a binary dependent variable.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Binary dependent variable
y_binary = (y&amp;gt;250)

## Logit link function
logit_gam = LogisticGAM(s(0, n_splines=8) + s(1, n_splines=10) + f(2), fit_intercept=True)
logit_gam.gridsearch(X, y_binary);
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;100% (11 of 11) |########################| Elapsed Time: 0:00:00 Time:  0:00:00
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;plotting-6&#34;&gt;Plotting&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plot_gam(logit_gam)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/03_nonparametric_147_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;The results are qualitatively similar to the non-binary case.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Resampling Methods</title>
      <link>https://matteocourthoud.github.io/course/ml-econ/04_crossvalidation/</link>
      <pubDate>Wed, 09 Mar 2022 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/course/ml-econ/04_crossvalidation/</guid>
      <description>&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Remove warnings
import warnings
warnings.filterwarnings(&#39;ignore&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Import
import pandas as pd
import numpy as np
import seaborn as sns
import time

from numpy.linalg import inv
from numpy.random import normal
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import train_test_split, LeaveOneOut, KFold, cross_val_score
from sklearn.preprocessing import PolynomialFeatures
from sklearn.utils import resample
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Import matplotlib for graphs
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import axes3d

# Set global parameters
%matplotlib inline
plt.style.use(&#39;seaborn-white&#39;)
plt.rcParams[&#39;lines.linewidth&#39;] = 3
plt.rcParams[&#39;figure.figsize&#39;] = (10,6)
plt.rcParams[&#39;figure.titlesize&#39;] = 20
plt.rcParams[&#39;axes.titlesize&#39;] = 18
plt.rcParams[&#39;axes.labelsize&#39;] = 14
plt.rcParams[&#39;legend.fontsize&#39;] = 14
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Resampling methods involve repeatedly drawing samples from a training set and refitting a model of interest on each sample in order to obtain additional information about the fitted model.&lt;/p&gt;
&lt;h2 id=&#34;41-cross-validation&#34;&gt;4.1 Cross-Validation&lt;/h2&gt;
&lt;p&gt;Cross-validation can be used to estimate the test error associated with a given statistical learning method in order to evaluate its performance, or to select the appropriate level of flexibility. The process of evaluating a model’s performance is known as &lt;strong&gt;model assessment&lt;/strong&gt;, whereas the process of selecting the proper level of flexibility for a model is known as &lt;strong&gt;model selection&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s use the &lt;code&gt;auto&lt;/code&gt; dataset we have used for nonparametric models.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Load car dataset
df1 = pd.read_csv(&#39;data/Auto.csv&#39;, na_values=&#39;?&#39;).dropna()
df1.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;mpg&lt;/th&gt;
      &lt;th&gt;cylinders&lt;/th&gt;
      &lt;th&gt;displacement&lt;/th&gt;
      &lt;th&gt;horsepower&lt;/th&gt;
      &lt;th&gt;weight&lt;/th&gt;
      &lt;th&gt;acceleration&lt;/th&gt;
      &lt;th&gt;year&lt;/th&gt;
      &lt;th&gt;origin&lt;/th&gt;
      &lt;th&gt;name&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;18.0&lt;/td&gt;
      &lt;td&gt;8&lt;/td&gt;
      &lt;td&gt;307.0&lt;/td&gt;
      &lt;td&gt;130&lt;/td&gt;
      &lt;td&gt;3504&lt;/td&gt;
      &lt;td&gt;12.0&lt;/td&gt;
      &lt;td&gt;70&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;chevrolet chevelle malibu&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;15.0&lt;/td&gt;
      &lt;td&gt;8&lt;/td&gt;
      &lt;td&gt;350.0&lt;/td&gt;
      &lt;td&gt;165&lt;/td&gt;
      &lt;td&gt;3693&lt;/td&gt;
      &lt;td&gt;11.5&lt;/td&gt;
      &lt;td&gt;70&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;buick skylark 320&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;18.0&lt;/td&gt;
      &lt;td&gt;8&lt;/td&gt;
      &lt;td&gt;318.0&lt;/td&gt;
      &lt;td&gt;150&lt;/td&gt;
      &lt;td&gt;3436&lt;/td&gt;
      &lt;td&gt;11.0&lt;/td&gt;
      &lt;td&gt;70&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;plymouth satellite&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;16.0&lt;/td&gt;
      &lt;td&gt;8&lt;/td&gt;
      &lt;td&gt;304.0&lt;/td&gt;
      &lt;td&gt;150&lt;/td&gt;
      &lt;td&gt;3433&lt;/td&gt;
      &lt;td&gt;12.0&lt;/td&gt;
      &lt;td&gt;70&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;amc rebel sst&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;17.0&lt;/td&gt;
      &lt;td&gt;8&lt;/td&gt;
      &lt;td&gt;302.0&lt;/td&gt;
      &lt;td&gt;140&lt;/td&gt;
      &lt;td&gt;3449&lt;/td&gt;
      &lt;td&gt;10.5&lt;/td&gt;
      &lt;td&gt;70&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;ford torino&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;h3 id=&#34;the-validation-set-approach&#34;&gt;The Validation Set Approach&lt;/h3&gt;
&lt;p&gt;Suppose that we would like to estimate the test error associated with fitting a particular statistical learning method on a set of observations. The validation set approach is a very simple strategy for this task. It involves randomly dividing the available set of observations into two parts&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;a &lt;strong&gt;training set&lt;/strong&gt; and&lt;/li&gt;
&lt;li&gt;a &lt;strong&gt;validation set&lt;/strong&gt; or hold-out set&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The model is fit on the training set, and the fitted model is used to predict the responses for the observations in the validation set. The resulting validation set error rate-typically assessed using MSE in the case of a quantitative response—provides an estimate of the test error rate.&lt;/p&gt;
&lt;p&gt;In the following example we are are going to compute the MSE fit polynomial of different order (one to ten). We are going to split the data 50-50 across training and test set.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Cross-validation function for polynomials
def cv_poly(X, y, p_order, r_states, t_prop):
    start = time.time()
    
    # Init scores
    scores = np.zeros((p_order.size,r_states.size))
    
    # Generate 10 random splits of the dataset
    for j in r_states:
        
        # Split sample in train and test
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=t_prop, random_state=j)
        
            
        # For every polynomial degree
        for i in p_order:

            # Generate polynomial
            X_train_poly = PolynomialFeatures(i+1).fit_transform(X_train)
            X_test_poly = PolynomialFeatures(i+1).fit_transform(X_test)

            # Fit regression                                                                    
            ols = LinearRegression().fit(X_train_poly, y_train)
            pred = ols.predict(X_test_poly)
            scores[i,j]= mean_squared_error(y_test, pred)
    print(&#39;Time elapsed: %.4f seconds&#39; % (time.time()-start))
    return scores
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Init
t_prop = 0.5
p_order = np.arange(10)
r_states = np.arange(10)

# Get X,y 
X = df1.horsepower.values.reshape(-1,1)
y = df1.mpg.ravel()

# Compute scores
cv_scores = cv_poly(X, y, p_order, r_states, t_prop)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Time elapsed: 0.0277 seconds
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s test the score for polynomials of different orders.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Figure 5.2
def make_figure_5_2():
    
    # Init
    fig, (ax1, ax2) = plt.subplots(1,2,figsize=(14,6))
    fig.suptitle(&#39;Figure 5.2&#39;)

    # Left plot (first split)
    ax1.plot(p_order+1,cv_scores[:,0], &#39;-o&#39;)
    ax1.set_title(&#39;Random split of the data set&#39;)

    # Right plot (all splits)
    ax2.plot(p_order+1,cv_scores)
    ax2.set_title(&#39;10 random splits of the data set&#39;)

    for ax in fig.axes:
        ax.set_ylabel(&#39;Mean Squared Error&#39;)
        ax.set_ylim(15,30)
        ax.set_xlabel(&#39;Degree of Polynomial&#39;)
        ax.set_xlim(0.5,10.5)
        ax.set_xticks(range(2,11,2));
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;make_figure_5_2()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/04_crossvalidation_15_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;This figure illustrates a &lt;strong&gt;first drawback&lt;/strong&gt; of the validation approach: the estimate of the test error rate can be highly variable, depending on precisely which observations are included in the training set and which observations are included in the validation set.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;second drawback&lt;/strong&gt; of the validation approach is that only a subset of the observations—those that are included in the training set rather than in the validation set—are used to fit the model. Since statistical methods tend to per- form worse when trained on fewer observations, this suggests that the validation set error rate may tend to overestimate the test error rate for the model fit on the entire data set.&lt;/p&gt;
&lt;h3 id=&#34;leave-one-out-cross-validation&#34;&gt;Leave-One-Out Cross-Validation&lt;/h3&gt;
&lt;p&gt;Leave-one-out cross-validation (LOOCV) attempts to address that method’s drawbacks.&lt;/p&gt;
&lt;p&gt;Like the validation set approach, LOOCV involves splitting the set of observations into two parts. However, instead of creating two subsets of comparable size, a single observation $i$ is used for the validation set, and the remaining $n-1$ observations make up the training set. The statistical learning method is fit on the $n−1$ training observations and the MSE is computed using the excluded observation $i$. The procedure is repeated $n$ times, for $i=1,&amp;hellip;,n$.&lt;/p&gt;
&lt;p&gt;The LOOCV estimate for the test MSE is the average of these $n$ test error estimates:&lt;/p&gt;
&lt;p&gt;$$
\mathrm{CV}&lt;em&gt;{(n)}=\frac{1}{n} \sum&lt;/em&gt;{i=1}^{n} \mathrm{MSE}_{i}
$$&lt;/p&gt;
&lt;p&gt;LOOCV has a couple of major &lt;strong&gt;advantages&lt;/strong&gt; over the validation set approach.&lt;/p&gt;
&lt;p&gt;First, it has far less bias. In LOOCV, we repeatedly fit the statistical learning method using training sets that contain $n − 1$ observations, almost as many as are in the entire data set. However, this also means that LOOCV is more computationally intense.&lt;/p&gt;
&lt;p&gt;Second, in contrast to the validation approach which will yield different results when applied repeatedly due to randomness in the training/validation set splits, performing LOOCV multiple times will always yield the same results: there is no randomness in the training/validation set splits.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# LeaveOneOut CV function for polynomials
def loo_cv_poly(X, y, p_order):
    start = time.time()
    
    # Init
    loo = LeaveOneOut().get_n_splits(y)
    loo_scores = np.zeros((p_order.size,1))
    
    # For every polynomial degree
    for i in p_order:
        # Generate polynomial
        X_poly = PolynomialFeatures(i+1).fit_transform(X)

        # Get score
        loo_scores[i] = cross_val_score(LinearRegression(), X_poly, y, cv=loo, scoring=&#39;neg_mean_squared_error&#39;).mean()
        
    print(&#39;Time elapsed: %.4f seconds&#39; % (time.time()-start))
    return loo_scores
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s compare the validation set approach against LOO in terms of computational time.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Validation set approach
cv_scores = cv_poly(X, y, p_order, r_states, t_prop)
    
# Leave One Out CV
loo_scores = loo_cv_poly(X, y, p_order)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Time elapsed: 0.0270 seconds
Time elapsed: 1.1495 seconds
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As expected, LOOCV is much more computationally intense. Even accounting for the fact that we repeat every the validation set approach 10 times.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s now compare them in terms of accuracy in minimizing the MSE.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Make new figure 1
def make_new_figure_1():

    # Init
    fig, ax = plt.subplots(1,1, figsize=(7,6))

    # Left plot
    ax.plot(p_order+1, np.array(loo_scores)*-1, &#39;-o&#39;, label=&#39;LOOCV&#39;)
    ax.plot(p_order+1, np.mean(cv_scores, axis=1), &#39;-o&#39;, c=&#39;orange&#39;, label=&#39;Standard CV&#39;)
    ax.set_ylabel(&#39;Mean Squared Error&#39;); ax.set_xlabel(&#39;Degree of Polynomial&#39;);
    ax.set_ylim(15,30); ax.set_xlim(0.5,10.5);
    ax.set_xticks(range(2,11,2));
    ax.legend();
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;make_new_figure_1()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/04_crossvalidation_26_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;With least squares linear or polynomial regression, an amazing shortcut makes the cost of LOOCV the same as that of a single model fit! The following formula holds:&lt;/p&gt;
&lt;p&gt;$$
\mathrm{CV}&lt;em&gt;{(n)}=\frac{1}{n} \sum&lt;/em&gt;{i=1}^{n}\left(\frac{y_{i}-\hat{y}&lt;em&gt;{i}}{1-h&lt;/em&gt;{i}}\right)^{2}
$$&lt;/p&gt;
&lt;p&gt;where $\hat y_i$ is the $i^{th}$ fitted value from the original least squares fit, and $h_i$ is the leverage of observation $i$.&lt;/p&gt;
&lt;h3 id=&#34;k-fold-cross-validation&#34;&gt;k-Fold Cross-Validation&lt;/h3&gt;
&lt;p&gt;An alternative to LOOCV is k-fold CV. This approach involves the following steps:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Randomly dividing the set of observations into $k$ groups, or folds, of approximately equal size.&lt;/li&gt;
&lt;li&gt;The first fold is treated as a validation set, and the method is fit on the remaining $k − 1$ folds.&lt;/li&gt;
&lt;li&gt;The mean squared error, MSE1, is then computed on the observations in the held-out fold.&lt;/li&gt;
&lt;li&gt;Steps (1)-(3) are repeated $k$ times; each time, a different group of observations is treated as a validation set.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The k-fold CV estimate is computed by averaging these values&lt;/p&gt;
&lt;p&gt;$$
\mathrm{CV}&lt;em&gt;{(k)}=\frac{1}{k} \sum&lt;/em&gt;{i=1}^{k} \mathrm{MSE}_{i}
$$&lt;/p&gt;
&lt;p&gt;LOOCV is a special case of k-fold CV in which $k$ is set to equal $n$. In practice, one typically performs k-fold CV using $k = 5$ or $k = 10$.&lt;/p&gt;
&lt;p&gt;The most obvious &lt;strong&gt;advantage&lt;/strong&gt; is computational. LOOCV requires fitting the statistical learning method $n$ times, while k-fold CV only requires $k$ splits.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# 10fold CV function for polynomials
def k10_cv_poly(X, y, p_order, r_states, folds):
    start = time.time()
    
    # Init
    k10_scores = np.zeros((p_order.size,r_states.size))

    # Generate 10 random splits of the dataset
    for j in r_states:

        # For every polynomial degree
        for i in p_order:

            # Generate polynomial
            X_poly = PolynomialFeatures(i+1).fit_transform(X)

            # Split sample in train and test
            kf10 = KFold(n_splits=folds, shuffle=True, random_state=j)
            k10_scores[i,j] = cross_val_score(LinearRegression(), X_poly, y, cv=kf10, 
                                               scoring=&#39;neg_mean_squared_error&#39;).mean()  
    
    print(&#39;Time elapsed: %.4f seconds&#39; % (time.time()-start))
    return k10_scores
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s now compare 10 fold cross-validation with LOO in terms of computational time.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Leave One Out CV
loo_scores = loo_cv_poly(X, y, p_order)
    
# 10-fold CV
folds = 10
k10_scores = k10_cv_poly(X, y, p_order, r_states, folds)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Time elapsed: 1.1153 seconds
Time elapsed: 0.3078 seconds
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Indeed we see that the LOOCV approach is more computationally intense. Even accounting for the fact that we repeat every 10-fold cross-validation 10 times.&lt;/p&gt;
&lt;p&gt;We can now compare all the methods in terms of accuracy.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Figure 5.4
def make_figure_5_4():

    fig, (ax1, ax2, ax3) = plt.subplots(1,3,figsize=(17,5))
    fig.suptitle(&#39;Figure 5.4&#39;)

    # Left plot
    ax1.plot(p_order+1, np.array(loo_scores)*-1, &#39;-o&#39;)
    ax1.set_title(&#39;LOOCV&#39;, fontsize=12)

    # Center plot
    ax2.plot(p_order+1,k10_scores*-1)
    ax2.set_title(&#39;10-fold CV&#39;, fontsize=12)

    # Right plot
    ax3.plot(p_order+1, np.array(loo_scores)*-1, &#39;-o&#39;, label=&#39;LOOCV&#39;)
    ax3.plot(p_order+1, np.mean(cv_scores, axis=1), label=&#39;Standard CV&#39;)
    ax3.plot(p_order+1,np.mean(k10_scores,axis=1)*-1, label=&#39;10-fold CV&#39;)
    ax3.set_title(&#39;Comparison&#39;, fontsize=12);
    ax3.legend();

    for ax in fig.axes:
        ax.set_ylabel(&#39;Mean Squared Error&#39;)
        ax.set_ylim(15,30)
        ax.set_xlabel(&#39;Degree of Polynomial&#39;)
        ax.set_xlim(0.5,10.5)
        ax.set_xticks(range(2,11,2));
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;make_figure_5_4()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/04_crossvalidation_37_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;10-fold cross-validation outputs a very similar MSE with respect to LOOCV, but with considerably less computational time.&lt;/p&gt;
&lt;h2 id=&#34;42-the-bootstrap&#34;&gt;4.2 The Bootstrap&lt;/h2&gt;
&lt;p&gt;The bootstrap is a widely applicable and extremely powerful statistical tool that can be used to &lt;strong&gt;quantify the uncertainty&lt;/strong&gt; associated with a given estimator or statistical learning method. In the specific case of linear regression, this is not particularly useful since there exist a formula for the standard errors. However, there are many models (almost all actually) for which there exists no closed for solution to the estimator variance.&lt;/p&gt;
&lt;p&gt;In pricinple, we would like to draw independent samples from the true data generating process and assessing the uncertainty of an estimator by comparing its values across the different samples. However, this is clearly unfeasible since we do not know the true data generating process.&lt;/p&gt;
&lt;p&gt;With the bootstrap, rather than repeatedly obtaining independent data sets from the population, we instead obtain distinct data sets by repeatedly sampling observations from the original data set. The power of the bootstrap lies in the fact that it can be easily applied to a wide range of statistical learning methods, including some for which a measure of variability is otherwise difficult to obtain and is not automatically output by statistical software.&lt;/p&gt;
&lt;p&gt;We are now going to assess its usefulness through simulation. Take the following model:&lt;/p&gt;
&lt;p&gt;$$
y_i = \beta_0 \cdot x_i + \varepsilon_i
$$&lt;/p&gt;
&lt;p&gt;where $\beta_0 = 0.6$ and $\varepsilon \sim N(0,1)$. We are now going to assess the variance of the OLS estimator $\hat \beta$ with the standard formula, simulating different samples and with bootstrap.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Set seed
np.random.seed(1)

# Init
simulations = 1000
N = 1000
beta_0 = 0.6
beta_sim = np.zeros((simulations,1))

# Generate X
X = normal(0,3,N).reshape(-1,1)

# Loop over simulations
for i in range(simulations):
    
    # Generate y
    e = normal(0,1,N).reshape(-1,1)
    y = beta_0*X + e
    
    # Estimate beta OLS
    beta_sim[i] = inv(X.T @ X) @ X.T @ y
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Init Bootstrap
beta_boot = np.zeros((simulations,1))

# Loop over simulations
for i in range(simulations):
    
    # Sample y
    X_sample, y_sample = resample(X, y, random_state=i)
    
    # Estimate beta OLS
    beta_boot[i] = inv(X_sample.T @ X_sample) @ X_sample.T @ y_sample
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can first compare the means.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Print means
print(&#39;True value      : %.4f&#39; % beta_0)
print(&#39;Mean Simulations: %.4f&#39; % np.mean(beta_sim))
print(&#39;Mean One Sample : %.4f&#39; % beta_sim[-1])
print(&#39;Mean Boostrap   : %.4f&#39; % np.mean(beta_boot))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;True value      : 0.6000
Mean Simulations: 0.6003
Mean One Sample : 0.5815
Mean Boostrap   : 0.5816
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The mean of the bootstrap estimtor is quite off. But this is not its actual purpose: it is designed to assess the uncertainty of an estimator, not its value.&lt;/p&gt;
&lt;p&gt;Now we compare the variances.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Print variances
print(&#39;True std       : %.6f&#39; % np.sqrt(inv(X.T @ X)))
print(&#39;Std Simulations: %.6f&#39; % np.std(beta_sim))
print(&#39;Std One Sample : %.6f&#39; % np.sqrt(inv(X.T @ X) * np.var(y - beta_sim[-1]*X)))
print(&#39;Std Boostrap   : %.6f&#39; % np.std(beta_boot))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;True std       : 0.010737
Std Simulations: 0.010830
Std One Sample : 0.010536
Std Boostrap   : 0.010812
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Bootstrap gets as close to the true standard deviation of the estimator as the simulation with the true data generating process. Impressive!&lt;/p&gt;
&lt;p&gt;We can now have a visual inspection.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Figure 5.10
def make_figure_5_10():

    fig, (ax1, ax2, ax3) = plt.subplots(1,3,figsize=(14,6))
    fig.suptitle(&#39;Figure 5.10&#39;)

    # Left plot
    ax1.hist(beta_sim, bins=10, edgecolor=&#39;black&#39;);
    ax1.axvline(x=beta_0, color=&#39;r&#39;, label=&#39;beta_0&#39;)
    ax1.set_xlabel(&#39;beta simulated&#39;);

    # Center plot
    ax2.hist(beta_boot, bins=10, color=&#39;orange&#39;, edgecolor=&#39;black&#39;);
    ax2.axvline(x=beta_0, color=&#39;r&#39;, label=&#39;beta_0&#39;)
    ax2.set_xlabel(&#39;beta bootstrap&#39;);

    # Right plot
    df_bootstrap = pd.DataFrame({&#39;simulated&#39;: beta_sim.ravel(), &#39;bootstrap&#39;:beta_boot.ravel()}, 
                                index=range(simulations))
    ax3 = sns.boxplot(data=df_bootstrap, width=0.5, linewidth=2);
    ax3.axhline(y=beta_0, color=&#39;r&#39;, label=&#39;beta_0&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;make_figure_5_10()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/04_crossvalidation_52_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;As we can see, the bootstrap is a powerful tool to assess the uncertainty of an estimator.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Model Selection and Regularization</title>
      <link>https://matteocourthoud.github.io/course/ml-econ/05_regularization/</link>
      <pubDate>Wed, 09 Mar 2022 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/course/ml-econ/05_regularization/</guid>
      <description>&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Remove warnings
import warnings
warnings.filterwarnings(&#39;ignore&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Import
import pandas as pd
import numpy as np
import time
import itertools
import statsmodels.api as sm
import seaborn as sns

from numpy.random import normal, uniform
from itertools import combinations
from statsmodels.api import add_constant
from statsmodels.regression.linear_model import OLS
from sklearn.linear_model import LinearRegression, Ridge, Lasso, RidgeCV, LassoCV
from sklearn.cross_decomposition import PLSRegression, PLSSVD
from sklearn.model_selection import KFold, cross_val_score, train_test_split, LeaveOneOut, ShuffleSplit
from sklearn.preprocessing import scale
from sklearn.decomposition import PCA
from sklearn.metrics import mean_squared_error
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Import matplotlib for graphs
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import axes3d

# Set global parameters
%matplotlib inline
plt.style.use(&#39;seaborn-white&#39;)
plt.rcParams[&#39;lines.linewidth&#39;] = 3
plt.rcParams[&#39;figure.figsize&#39;] = (12,5)
plt.rcParams[&#39;figure.titlesize&#39;] = 20
plt.rcParams[&#39;axes.titlesize&#39;] = 18
plt.rcParams[&#39;axes.labelsize&#39;] = 14
plt.rcParams[&#39;legend.fontsize&#39;] = 14
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;When we talk about big data, we do not only talk about bigger sample size, $n$, but also about a larger number of explanatory variables, $p$. However, with ordinary least squares, we are limited by the identification constraint that $p &amp;lt; n$. Moreover, for inference and prediction accuracy, we would actually like to have $k &amp;laquo; n$.&lt;/p&gt;
&lt;p&gt;This session adresses methods to use a least squares fit in a setting in which the number of regressors, $p$, is large with respect to the sample size, $n$&lt;/p&gt;
&lt;h2 id=&#34;51-subset-selection&#34;&gt;5.1 Subset Selection&lt;/h2&gt;
&lt;p&gt;The Subset Selection approach involves identifying a subset of the $p$ predictors that we believe to be related to the response. We then fit a model using least squares on the reduced set of variables.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s load the &lt;code&gt;credit rating&lt;/code&gt; dataset.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Credit ratings dataset
credit = pd.read_csv(&#39;data/Credit.csv&#39;, usecols=list(range(1,12)))
credit.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;Income&lt;/th&gt;
      &lt;th&gt;Limit&lt;/th&gt;
      &lt;th&gt;Rating&lt;/th&gt;
      &lt;th&gt;Cards&lt;/th&gt;
      &lt;th&gt;Age&lt;/th&gt;
      &lt;th&gt;Education&lt;/th&gt;
      &lt;th&gt;Gender&lt;/th&gt;
      &lt;th&gt;Student&lt;/th&gt;
      &lt;th&gt;Married&lt;/th&gt;
      &lt;th&gt;Ethnicity&lt;/th&gt;
      &lt;th&gt;Balance&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;14.891&lt;/td&gt;
      &lt;td&gt;3606&lt;/td&gt;
      &lt;td&gt;283&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;34&lt;/td&gt;
      &lt;td&gt;11&lt;/td&gt;
      &lt;td&gt;Male&lt;/td&gt;
      &lt;td&gt;No&lt;/td&gt;
      &lt;td&gt;Yes&lt;/td&gt;
      &lt;td&gt;Caucasian&lt;/td&gt;
      &lt;td&gt;333&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;106.025&lt;/td&gt;
      &lt;td&gt;6645&lt;/td&gt;
      &lt;td&gt;483&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;82&lt;/td&gt;
      &lt;td&gt;15&lt;/td&gt;
      &lt;td&gt;Female&lt;/td&gt;
      &lt;td&gt;Yes&lt;/td&gt;
      &lt;td&gt;Yes&lt;/td&gt;
      &lt;td&gt;Asian&lt;/td&gt;
      &lt;td&gt;903&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;104.593&lt;/td&gt;
      &lt;td&gt;7075&lt;/td&gt;
      &lt;td&gt;514&lt;/td&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;71&lt;/td&gt;
      &lt;td&gt;11&lt;/td&gt;
      &lt;td&gt;Male&lt;/td&gt;
      &lt;td&gt;No&lt;/td&gt;
      &lt;td&gt;No&lt;/td&gt;
      &lt;td&gt;Asian&lt;/td&gt;
      &lt;td&gt;580&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;148.924&lt;/td&gt;
      &lt;td&gt;9504&lt;/td&gt;
      &lt;td&gt;681&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;36&lt;/td&gt;
      &lt;td&gt;11&lt;/td&gt;
      &lt;td&gt;Female&lt;/td&gt;
      &lt;td&gt;No&lt;/td&gt;
      &lt;td&gt;No&lt;/td&gt;
      &lt;td&gt;Asian&lt;/td&gt;
      &lt;td&gt;964&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;55.882&lt;/td&gt;
      &lt;td&gt;4897&lt;/td&gt;
      &lt;td&gt;357&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;68&lt;/td&gt;
      &lt;td&gt;16&lt;/td&gt;
      &lt;td&gt;Male&lt;/td&gt;
      &lt;td&gt;No&lt;/td&gt;
      &lt;td&gt;Yes&lt;/td&gt;
      &lt;td&gt;Caucasian&lt;/td&gt;
      &lt;td&gt;331&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;We are going to look at the relationship between individual characteristics and account &lt;code&gt;Balance&lt;/code&gt; in the &lt;code&gt;Credit&lt;/code&gt; dataset.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# X and y
X = credit.loc[:, credit.columns != &#39;Balance&#39;]
y = credit.loc[:,&#39;Balance&#39;]
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;best-subset-selection&#34;&gt;Best Subset Selection&lt;/h3&gt;
&lt;p&gt;To perform best subset selection, we fit a separate least squares regression for each possible combination of the $p$ predictors. That is, we fit all $p$ models that contain exactly one predictor, all $p = p(p−1)/2$ models that contain 2 exactly two predictors, and so forth. We then look at all of the resulting models, with the goal of identifying the one that is best.&lt;/p&gt;
&lt;p&gt;Clearly the &lt;strong&gt;main disadvantage&lt;/strong&gt; of &lt;em&gt;best subset selection&lt;/em&gt; is computational power.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def model_selection(X, y, *args):
    
    # Init 
    scores = list(itertools.repeat(np.zeros((0,2)), len(args)))

    # Categorical variables 
    categ_cols = {&amp;quot;Gender&amp;quot;, &amp;quot;Student&amp;quot;, &amp;quot;Married&amp;quot;, &amp;quot;Ethnicity&amp;quot;}

    # Loop over all admissible number of regressors
    K = np.shape(X)[1]
    for k in range(K+1):
        print(&amp;quot;Computing k=%1.0f&amp;quot; % k, end =&amp;quot;&amp;quot;)
        
        # Loop over all combinations
        for i in combinations(range(K), k):

            # Subset X
            X_subset = X.iloc[:,list(i)]

            # Get dummies for categorical variables
            if k&amp;gt;0:
                categ_subset = list(categ_cols &amp;amp; set(X_subset.columns))
                X_subset = pd.get_dummies(X_subset, columns=categ_subset, drop_first=True)

            # Regress
            reg = OLS(y,add_constant(X_subset)).fit()

            # Metrics
            for i,metric in enumerate(args):
                score = np.reshape([k,metric(reg)], (1,-1))
                scores[i] = np.append(scores[i], score, axis=0)
        print(&amp;quot;&amp;quot;, end=&amp;quot;\r&amp;quot;)
    return scores
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We are going to consider 10 variables and two difference metrics: the Sum of Squares Residuals and $R^2$.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Set metrics
rss = lambda reg : reg.ssr
r2 = lambda reg : reg.rsquared

# Compute scores
scores = model_selection(X, y, rss, r2)
ms_RSS = scores[0]
ms_R2 = scores[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Computing k=10
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Save best scores
K = np.shape(X)[1]
ms_RSS_best = [np.min(ms_RSS[ms_RSS[:,0]==k,1]) for k in range(K+1)]
ms_R2_best = [np.max(ms_R2[ms_R2[:,0]==k,1]) for k in range(K+1)]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s plot the best scores.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Figure 6.1
def make_figure_6_1():

    fig, (ax1,ax2) = plt.subplots(1,2)
    fig.suptitle(&#39;Figure 6.1: Best Model Selection&#39;)

    # RSS
    ax1.scatter(x=ms_RSS[:,0], y=ms_RSS[:,1], facecolors=&#39;None&#39;, edgecolors=&#39;k&#39;, alpha=0.5);
    ax1.plot(range(K+1), ms_RSS_best, c=&#39;r&#39;);
    ax1.scatter(np.argmin(ms_RSS_best), np.min(ms_RSS_best), marker=&#39;x&#39;, s=300)
    ax1.set_ylabel(&#39;RSS&#39;);

    # R2
    ax2.scatter(x=ms_R2[:,0], y=ms_R2[:,1], facecolors=&#39;None&#39;, edgecolors=&#39;k&#39;, alpha=0.5);
    ax2.plot(range(K+1), ms_R2_best, c=&#39;r&#39;);
    ax2.scatter(np.argmax(ms_R2_best), np.max(ms_R2_best), marker=&#39;x&#39;, s=300)
    ax2.set_ylabel(&#39;R2&#39;);

    # All axes;
    for ax in fig.axes:
        ax.set_xlabel(&#39;Number of Predictors&#39;); 
        ax.set_yticks([]);
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;make_figure_6_1()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/05_regularization_19_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;The figure shows that, as expected, both metrics improve as the number of variables increases; however, from the three-variable model on, there is little improvement in RSS and $R^2$ as a result of including additional predictors.&lt;/p&gt;
&lt;h3 id=&#34;forward-stepwise-selection&#34;&gt;Forward Stepwise Selection&lt;/h3&gt;
&lt;p&gt;For computational reasons, best subset selection cannot be applied with very large $p$.&lt;/p&gt;
&lt;p&gt;While the best subset selection procedure considers all $2^p$ possible models containing subsets of the p predictors, forward step-wise considers a much smaller set of models. Forward stepwise selection begins with a model containing no predictors, and then adds predictors to the model, one-at-a-time, until all of the predictors are in the model. In particular, at each step the variable that gives the greatest additional improvement to the fit is added to the model.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def forward_selection(X, y, f):

    # Init RSS and R2
    K = np.shape(X)[1]
    fms_scores = np.zeros((K,1))
    
    # Categorical variables 
    categ_cols = {&amp;quot;Gender&amp;quot;, &amp;quot;Student&amp;quot;, &amp;quot;Married&amp;quot;, &amp;quot;Ethnicity&amp;quot;}

    # Loop over p
    selected_cols = []
    for k in range(1,K+1):

        # Loop over selected columns
        remaining_cols = [col for col in X.columns if col not in selected_cols]
        temp_scores = np.zeros((0,1))

        # Loop on remaining columns    
        for col in remaining_cols:
            # Subset X
            X_subset = X.loc[:,selected_cols + [col]]
            if k&amp;gt;0:
                categ_subset = list(categ_cols &amp;amp; set(X_subset.columns))
                X_subset = pd.get_dummies(X_subset, columns=categ_subset, drop_first=True)

            # Regress
            reg = OLS(y,add_constant(X_subset).values).fit()

            # Metrics
            temp_scores = np.append(temp_scores, f(reg))

        # Pick best variable
        best_col = remaining_cols[np.argmin(temp_scores)]
        print(best_col)
        selected_cols += [best_col]
        fms_scores[k-1] = np.min(temp_scores)
        
    return fms_scores
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s select the best model according, using the sum of squared residuals as a metric.&lt;/p&gt;
&lt;p&gt;What are the most important variables?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Forward selection by RSS
rss = lambda reg : reg.ssr
fms_RSS = forward_selection(X, y, rss)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Rating
Income
Student
Limit
Cards
Age
Ethnicity
Gender
Married
Education
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What happens if we use $R^2$ instead?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Forward selection by R2
r2 = lambda reg : -reg.rsquared
fms_R2 = -forward_selection(X, y, r2)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Rating
Income
Student
Limit
Cards
Age
Ethnicity
Gender
Married
Education
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Unsurprisingly, both methods select the same models. Why? In the end $R^2$ is just a normalized version of RSS.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s plot the scores of the two methods, for different number of predictors.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# New figure 1
def make_new_figure_1():

    # Init
    fig, (ax1,ax2) = plt.subplots(1,2)
    fig.suptitle(&#39;Forward Model Selection&#39;)

    # RSS
    ax1.plot(range(1,K+1), fms_RSS, c=&#39;r&#39;);
    ax1.scatter(np.argmin(fms_RSS)+1, np.min(fms_RSS), marker=&#39;x&#39;, s=300)
    ax1.set_ylabel(&#39;RSS&#39;);

    # R2
    ax2.plot(range(1,K+1), fms_R2, c=&#39;r&#39;);
    ax2.scatter(np.argmax(fms_R2)+1, np.max(fms_R2), marker=&#39;x&#39;, s=300)
    ax2.set_ylabel(&#39;R2&#39;);

    # All axes;
    for ax in fig.axes:
        ax.set_xlabel(&#39;Number of Predictors&#39;); 
        ax.set_yticks([]);
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;make_new_figure_1()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/05_regularization_31_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;backward-stepwise-selection&#34;&gt;Backward Stepwise Selection&lt;/h3&gt;
&lt;p&gt;Like forward stepwise selection, backward stepwise selection provides an efficient alternative to best subset selection. However, unlike forward stepwise selection, it begins with the full least squares model containing all p predictors, and then iteratively removes the least useful predictor, one-at-a-time.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def backward_selection(X, y, f):

    # Init RSS and R2
    K = np.shape(X)[1]
    fms_scores = np.zeros((K,1))
    
    # Categorical variables 
    categ_cols = {&amp;quot;Gender&amp;quot;, &amp;quot;Student&amp;quot;, &amp;quot;Married&amp;quot;, &amp;quot;Ethnicity&amp;quot;}

    # Loop over p
    selected_cols = list(X.columns)
    for k in range(K,0,-1):

        # Loop over selected columns
        temp_scores = np.zeros((0,1))

        # Loop on remaining columns    
        for col in selected_cols:
            # Subset X
            X_subset = X.loc[:,[x for x in selected_cols if x != col]]
            if k&amp;gt;1:
                categ_subset = list(categ_cols &amp;amp; set(X_subset.columns))
                X_subset = pd.get_dummies(X_subset, columns=categ_subset, drop_first=True)

            # Regress
            reg = OLS(y,add_constant(X_subset).values).fit()

            # Metrics
            temp_scores = np.append(temp_scores, f(reg))

        # Pick best variable
        worst_col = selected_cols[np.argmin(temp_scores)]
        print(worst_col)
        selected_cols.remove(worst_col)
        fms_scores[k-1] = np.min(temp_scores)
        
    return fms_scores
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s select the best model according, using the sum of squared residuals as a metric.&lt;/p&gt;
&lt;p&gt;What are the most important variables?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Backward selection by RSS
rss = lambda reg : reg.ssr
bms_RSS = backward_selection(X, y, rss)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Education
Married
Gender
Ethnicity
Age
Rating
Cards
Student
Income
Limit
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What if we use $R^2$ instead?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Backward selection by R2
r2 = lambda reg : -reg.rsquared
bms_R2 = -backward_selection(X, y, r2)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Education
Married
Gender
Ethnicity
Age
Rating
Cards
Student
Income
Limit
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The interesting part here is that the the variable &lt;code&gt;Rating&lt;/code&gt; that was selected first by forward model selection, is now dropped $5^{th}$ to last. Why? It&amp;rsquo;s probably because it contains a lot of information by itself (hence first in FMS) but it&amp;rsquo;s highly correlated with &lt;code&gt;Student&lt;/code&gt;, &lt;code&gt;Income&lt;/code&gt; and &lt;code&gt;Limit&lt;/code&gt; while these variables are more ortogonal to each other, and hence it gets dropped before them in BMS.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Plot correlations
sns.pairplot(credit[[&#39;Rating&#39;,&#39;Student&#39;,&#39;Income&#39;,&#39;Limit&#39;]], height=1.8);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/05_regularization_40_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;If is indeed what we see: &lt;code&gt;Rating&lt;/code&gt; and &lt;code&gt;Limit&lt;/code&gt; are highly correlated.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s plot the scores for different number of predictors.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# New figure 2
def make_new_figure_2():

    # Init
    fig, (ax1,ax2) = plt.subplots(1,2)
    fig.suptitle(&#39;Backward Model Selection&#39;)

    # RSS
    ax1.plot(range(1,K+1), bms_RSS, c=&#39;r&#39;);
    ax1.scatter(np.argmin(bms_RSS)+1, np.min(bms_RSS), marker=&#39;x&#39;, s=300)
    ax1.set_ylabel(&#39;RSS&#39;);

    # R2
    ax2.plot(range(1,K+1), bms_R2, c=&#39;r&#39;);
    ax2.scatter(np.argmax(bms_R2)+1, np.max(bms_R2), marker=&#39;x&#39;, s=300)
    ax2.set_ylabel(&#39;R2&#39;);

    # All axes;
    for ax in fig.axes:
        ax.set_xlabel(&#39;Number of Predictors&#39;); 
        ax.set_yticks([]);
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;make_new_figure_2()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/05_regularization_44_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;choosing-the-optimal-model&#34;&gt;Choosing the Optimal Model&lt;/h3&gt;
&lt;p&gt;So far we have use the trainint error in order to select the model. However, the training error can be a poor estimate of the test error. Therefore, RSS and R2 are not suitable for selecting the best model among a collection of models with different numbers of predictors.&lt;/p&gt;
&lt;p&gt;In order to select the best model with respect to test error, we need to estimate this test error. There are two common approaches:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;We can indirectly estimate test error by making an adjustment to the training error to account for the bias due to overfitting.&lt;/li&gt;
&lt;li&gt;We can directly estimate the test error, using either a validation set approach or a cross-validation approach.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Some metrics that account for the trainint error are&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Akaike Information Criterium (AIC)&lt;/li&gt;
&lt;li&gt;Bayesian Information Criterium (BIC)&lt;/li&gt;
&lt;li&gt;Adjusted $R^2$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The idea behind all these varaibles is to insert a penalty for the number of parameters used in the model. All these measure have theoretical fundations which are beyond the scope of this session.&lt;/p&gt;
&lt;p&gt;We are now going to test the three metrics&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Set metrics
aic = lambda reg : reg.aic
bic = lambda reg : reg.bic
r2a = lambda reg : reg.rsquared_adj

# Compute best model selection scores
scores = model_selection(X, y, aic, bic, r2a)
ms_AIC = scores[0]
ms_BIC = scores[1]
ms_R2a = scores[2]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Computing k=10
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Save best scores
ms_AIC_best = [np.min(ms_AIC[ms_AIC[:,0]==k,1]) for k in range(K+1)]
ms_BIC_best = [np.min(ms_BIC[ms_BIC[:,0]==k,1]) for k in range(K+1)]
ms_R2a_best = [np.max(ms_R2a[ms_R2a[:,0]==k,1]) for k in range(K+1)]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We plot the scores for different model selection methods.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Figure 6.2
def make_figure_6_2():

    # Init
    fig, (ax1,ax2,ax3) = plt.subplots(1,3, figsize=(16,5))
    fig.suptitle(&#39;Figure 6.2&#39;)

    # AIC
    ax1.scatter(x=ms_AIC[:,0], y=ms_AIC[:,1], facecolors=&#39;None&#39;, edgecolors=&#39;k&#39;, alpha=0.5);
    ax1.plot(range(K+1),ms_AIC_best, c=&#39;r&#39;);
    ax1.scatter(np.argmin(ms_AIC_best), np.min(ms_AIC_best), marker=&#39;x&#39;, s=300)
    ax1.set_ylabel(&#39;AIC&#39;);

    # BIC
    ax2.scatter(x=ms_BIC[:,0], y=ms_BIC[:,1], facecolors=&#39;None&#39;, edgecolors=&#39;k&#39;, alpha=0.5);
    ax2.plot(range(K+1), ms_BIC_best, c=&#39;r&#39;);
    ax2.scatter(np.argmin(ms_BIC_best), np.min(ms_BIC_best), marker=&#39;x&#39;, s=300)
    ax2.set_ylabel(&#39;BIC&#39;);

    # R2 adj
    ax3.scatter(x=ms_R2a[:,0], y=ms_R2a[:,1], facecolors=&#39;None&#39;, edgecolors=&#39;k&#39;, alpha=0.5);
    ax3.plot(range(K+1), ms_R2a_best, c=&#39;r&#39;);
    ax3.scatter(np.argmax(ms_R2a_best), np.max(ms_R2a_best), marker=&#39;x&#39;, s=300)
    ax3.set_ylabel(&#39;R2_adj&#39;);

    # All axes;
    for ax in fig.axes:
        ax.set_xlabel(&#39;Number of Predictors&#39;); 
        ax.set_yticks([]);
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;make_figure_6_2()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/05_regularization_52_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;As we can see, all three metrics select more parsimonious models, with BIC being particularly conservative with only 4 variables and $R^2_{adj}$ selecting the larger model with 7 variables.&lt;/p&gt;
&lt;h3 id=&#34;validation-and-cross-validation&#34;&gt;Validation and Cross-Validation&lt;/h3&gt;
&lt;p&gt;As an alternative to the approaches just discussed, we can directly estimate the test error using the validation set and cross-validation methods discussed in the previous session.&lt;/p&gt;
&lt;p&gt;The main problem with cross-validation is the computational burden. We are now going to perform &lt;em&gt;best model selection&lt;/em&gt; using the following cross-validation algorithms:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Validation set approach, 50-50 split, repeated 10 times&lt;/li&gt;
&lt;li&gt;5-fold cross-validation&lt;/li&gt;
&lt;li&gt;10-fold cross-validation&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We are not going to perform Leave-One-Out cross-validation for computational reasons.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def cv_scores(X, y, *args):

    # Init 
    scores = list(itertools.repeat(np.zeros((0,2)), len(args)))

    # Categorical variables 
    categ_cols = {&amp;quot;Gender&amp;quot;, &amp;quot;Student&amp;quot;, &amp;quot;Married&amp;quot;, &amp;quot;Ethnicity&amp;quot;}

    # Loop over all possible combinations of regressions
    K = np.shape(X)[1]
    for k in range(K+1):
        print(&amp;quot;Computing k=%1.0f&amp;quot; % k, end =&amp;quot;&amp;quot;)
        for i in combinations(range(K), k):

            # Subset X
            X_subset = X.iloc[:,list(i)]

            # Get dummies for categorical variables
            if k&amp;gt;0:
                categ_subset = list(categ_cols &amp;amp; set(X_subset.columns))
                X_subset = pd.get_dummies(X_subset, columns=categ_subset, drop_first=True)

            # Metrics
            for i,cv_method in enumerate(args):
                score = cross_val_score(LinearRegression(), add_constant(X_subset), y, 
                                        cv=cv_method, scoring=&#39;neg_mean_squared_error&#39;).mean()
                score_pair = np.reshape([k,score], (1,-1))
                scores[i] = np.append(scores[i], score_pair, axis=0)
        print(&amp;quot;&amp;quot;, end=&amp;quot;\r&amp;quot;)
                
    return scores
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s compute the scores for different model selection methods.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Define cv methods
vset = ShuffleSplit(n_splits=10, test_size=0.5)
kf5 = KFold(n_splits=5, shuffle=True)
kf10 = KFold(n_splits=10, shuffle=True)

# Get best model selection scores
scores = cv_scores(X, y, vset, kf5, kf10)
ms_vset = scores[0]
ms_kf5 = scores[1]
ms_kf10 = scores[2]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Computing k=10
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Save best scores
ms_vset_best = [np.max(ms_vset[ms_vset[:,0]==k,1]) for k in range(K+1)]
ms_kf5_best = [np.max(ms_kf5[ms_kf5[:,0]==k,1]) for k in range(K+1)]
ms_kf10_best = [np.max(ms_kf10[ms_kf10[:,0]==k,1]) for k in range(K+1)]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We not plot the scores.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Figure 6.3
def make_figure_6_3():

    # Init
    fig, (ax1,ax2,ax3) = plt.subplots(1,3, figsize=(16,5))
    fig.suptitle(&#39;Figure 6.3&#39;)

    # Validation Set
    ax1.scatter(x=ms_vset[:,0], y=ms_vset[:,1], facecolors=&#39;None&#39;, edgecolors=&#39;k&#39;, alpha=0.5);
    ax1.plot(range(K+1),ms_vset_best, c=&#39;r&#39;);
    ax1.scatter(np.argmax(ms_vset_best), np.max(ms_vset_best), marker=&#39;x&#39;, s=300)
    ax1.set_ylabel(&#39;Validation Set&#39;);


    # 5-Fold Cross Validation
    ax2.scatter(x=ms_kf5[:,0], y=ms_kf5[:,1], facecolors=&#39;None&#39;, edgecolors=&#39;k&#39;, alpha=0.5);
    ax2.plot(range(K+1), ms_kf5_best, c=&#39;r&#39;);
    ax2.scatter(np.argmax(ms_kf5_best), np.max(ms_kf5_best), marker=&#39;x&#39;, s=300)
    ax2.set_ylabel(&#39;5-Fold Cross Validation&#39;);


    # 10-Fold Cross-Validation
    ax3.scatter(x=ms_kf10[:,0], y=ms_kf10[:,1], facecolors=&#39;None&#39;, edgecolors=&#39;k&#39;, alpha=0.5);
    ax3.plot(range(K+1), ms_kf10_best, c=&#39;r&#39;);
    ax3.scatter(np.argmax(ms_kf10_best), np.max(ms_kf10_best), marker=&#39;x&#39;, s=300)
    ax3.set_ylabel(&#39;10-Fold Cross-Validation&#39;);

    # All axes;
    for ax in fig.axes:
        ax.set_xlabel(&#39;Number of Predictors&#39;); 
        ax.set_yticks([]);
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;make_figure_6_3()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/05_regularization_62_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;From the figure we see that each cross-validation method selects a different model and the most accurate one, K-fold CV, select 5 predictors.&lt;/p&gt;
&lt;h2 id=&#34;52-shrinkage-methods&#34;&gt;5.2 Shrinkage Methods&lt;/h2&gt;
&lt;p&gt;Model selection methods constrained the number of varaibles &lt;em&gt;before&lt;/em&gt; running a linear regression. Shrinkage methods attempt to do the two things simultaneously. In particular they &lt;em&gt;constrain&lt;/em&gt; or &lt;em&gt;shrink&lt;/em&gt; coefficients by imposing penalties in the objective functions for high values of the parameters.&lt;/p&gt;
&lt;h3 id=&#34;ridge-regression&#34;&gt;Ridge Regression&lt;/h3&gt;
&lt;p&gt;The Least Squares Regression minimizes the Residual Sum of Squares&lt;/p&gt;
&lt;p&gt;$$
\mathrm{RSS}=\sum_{i=1}^{n}\left(y_{i}-\beta_{0}-\sum_{j=1}^{p} \beta_{j} x_{i j}\right)^{2}
$$&lt;/p&gt;
&lt;p&gt;The Ridge Regression objective function instead is&lt;/p&gt;
&lt;p&gt;$$
\sum_{i=1}^{n}\left(y_{i}-\beta_{0}-\sum_{j=1}^{p} \beta_{j} x_{i j}\right)^{2}+\lambda \sum_{j=1}^{p} \beta_{j}^{2}=\mathrm{RSS}+\lambda \sum_{j=1}^{p} \beta_{j}^{2}
$$&lt;/p&gt;
&lt;p&gt;where $\lambda&amp;gt;0$ is a tuning parameter that regulates the extent to which large parameters are penalized.&lt;/p&gt;
&lt;p&gt;In matrix notation, the objective function is&lt;/p&gt;
&lt;p&gt;$$
||X\beta - y||^2_2 + \alpha ||\beta||^2_2
$$&lt;/p&gt;
&lt;p&gt;which is equivalent to optimizing&lt;/p&gt;
&lt;p&gt;$$
\frac{1}{N}||X\beta - y||^2_2 + \frac{\alpha}{N} ||\beta||^2_2
$$&lt;/p&gt;
&lt;p&gt;We are now going to run Ridge Regression on the &lt;code&gt;Credit&lt;/code&gt; dataset trying to explain account &lt;code&gt;Balance&lt;/code&gt; with a set of observable individual characteristics.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# X and y
categ_cols = [&amp;quot;Gender&amp;quot;, &amp;quot;Student&amp;quot;, &amp;quot;Married&amp;quot;, &amp;quot;Ethnicity&amp;quot;]
X = credit.loc[:, credit.columns != &#39;Balance&#39;]
X = pd.get_dummies(X, columns=categ_cols, drop_first=True)
y = credit.loc[:,&#39;Balance&#39;]
n = len(credit)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We run ridge regression over a range of values for the penalty paramenter $\lambda$.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Init alpha grid
n_grid = 100
alphas = 10**np.linspace(-2,5,n_grid).reshape(-1,1)
ridge = Ridge()
ridge_coefs = []

# Loop over values of alpha
for a in alphas:
    ridge.set_params(alpha=a)
    ridge.fit(scale(X), y)
    ridge_coefs.append(ridge.coef_)
ridge_coefs = np.reshape(ridge_coefs,(n_grid,-1))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We use linear regression as a comparison.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# OLS regression
ols = LinearRegression().fit(scale(X),y)
ols_coefs = ols.coef_;
mod_ols = np.linalg.norm(ols_coefs)

# Relative magnitude
rel_beta = [np.linalg.norm(ridge_coefs[k,:])/mod_ols for k in range(n_grid)]
rel_beta = np.reshape(rel_beta, (-1,1))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We plot the results&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Figure 6.4
def make_figure_6_4():

    # Init
    fig, (ax1,ax2) = plt.subplots(1,2)
    fig.suptitle(&#39;Figure 6.4: Ridge Regression Coefficients&#39;)

    highlight = [0,1,2,7];

    # Plot coefficients - absolute
    ax1.plot(alphas, ridge_coefs[:,highlight], alpha=1)
    ax1.plot(alphas, ridge_coefs, c=&#39;grey&#39;, alpha=0.3)
    ax1.set_xscale(&#39;log&#39;)
    ax1.set_xlabel(&#39;lambda&#39;); ax1.set_ylabel(&#39;Standardized coefficients&#39;);
    ax1.legend([&#39;Income&#39;, &#39;Limit&#39;, &#39;Rating&#39;, &#39;Student&#39;])

    # Plot coefficients - relative
    ax2.plot(rel_beta, ridge_coefs[:,highlight], alpha=1)
    ax2.plot(rel_beta, ridge_coefs, c=&#39;grey&#39;, alpha=0.3)
    ax2.set_xlabel(&#39;Relative Beta&#39;); ax2.set_ylabel(&#39;Standardized coefficients&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;make_figure_6_4()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/05_regularization_76_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;As we decrease $\lambda$, the Ridge coefficients get larger. Moreover, the variables with the consistently largest coefficients are &lt;code&gt;Income&lt;/code&gt;, &lt;code&gt;Limit&lt;/code&gt;, &lt;code&gt;Rating&lt;/code&gt; and &lt;code&gt;Student&lt;/code&gt;.&lt;/p&gt;
&lt;h3 id=&#34;bias-variance-trade-off&#34;&gt;Bias-Variance Trade-off&lt;/h3&gt;
&lt;p&gt;Ridge regression’s advantage over least squares is rooted in the bias-variance trade-off. As $\lambda$ increases, the flexibility of the ridge regression fit decreases, leading to decreased variance but increased bias.&lt;/p&gt;
&lt;p&gt;$$
y_0 = f(x_0) + \varepsilon
$$&lt;/p&gt;
&lt;p&gt;Recap: we can decompose the Mean Squared Error of an estimator into two components: the &lt;em&gt;variance&lt;/em&gt; and the squared &lt;em&gt;bias&lt;/em&gt;:&lt;/p&gt;
&lt;p&gt;$$
\mathbb E\left(y_{0}-\hat{f}\left(x_{0}\right)\right)^{2} = \mathbb E\left(f(x_0) + \varepsilon - \hat f(x_{0})\right)^{2} = \
= \mathbb E\left(f(x_0) - \mathbb E[\hat f(x_{0})] + \varepsilon - \hat f(x_{0}) + \mathbb E[\hat f(x_{0})] \right)^{2} = \
= \mathbb E \left[ \mathbb E [\hat{f} (x_{0}) ] - f(x_0) \right]^2 + \mathbb E \left[ \left( \hat{f} (x_{0}) - \mathbb E [\hat{f} (x_{0})] \right)^2 \right] + \mathbb E[\varepsilon^2] \
= \operatorname{Bias} \left( \hat{f} (x_{0}) \right)^2 + \operatorname{Var}\left(\hat{f}\left(x_{0}\right)\right) + \operatorname{Var}(\varepsilon)
$$&lt;/p&gt;
&lt;p&gt;The last term is the variance of the error term, sometimes also called the &lt;em&gt;irreducible error&lt;/em&gt; since it&amp;rsquo;s pure noise, and we cannot account for it.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Compute var-bias
def compute_var_bias(X_train, b0, x0, a, k, n, sim, f):
    
    # Init 
    y_hat = np.zeros(sim)
    coefs = np.zeros((sim, k))
    
    # Loop over simulations
    for s in range(sim):
        e_train = normal(0,1,(n,1))
        y_train = X_train @ b0 + e_train
        fit = f(a).fit(X_train, y_train)
        y_hat[s] = fit.predict(x0)
        coefs[s,:] = fit.coef_
        
    # Compute MSE, Var and Bias2   
    e_test = normal(0,1,(sim,1))
    y_test = x0 @ b0 + e_test
    mse = np.mean((y_test - y_hat)**2)
    var = np.var(y_hat)
    bias2 = np.mean(x0 @ b0 - y_hat)**2
    
    return [mse, var, bias2], np.mean(coefs, axis=0)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;np.random.seed(1)

# Generate random data
n = 50
k = 45
N = 50000
X_train = normal(0.2,1,(n,k))
x0 = normal(0.2,1,(1,k))
e_train = normal(0,1,(n,1))
b0 = uniform(0,1,(k,1))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Init alpha grid
sim = 1000
n_grid = 30
df = pd.DataFrame({&#39;alpha&#39;:10**np.linspace(-5,5,n_grid)})
ridge_coefs2 = []

# Init simulations
sim = 1000
ridge = lambda a: Ridge(alpha=a, fit_intercept=False)

# Loop over values of alpha
for i in range(len(df)):
    print(&amp;quot;Alpha %1.0f/%1.0f&amp;quot; % (i+1,len(df)), end =&amp;quot;&amp;quot;)
    a = df.loc[i,&#39;alpha&#39;]
    df.loc[i,[&#39;mse&#39;,&#39;var&#39;,&#39;bias2&#39;]], c = compute_var_bias(X_train, b0, x0, a, k, n, sim, ridge)
    ridge_coefs2.append(c)
    print(&amp;quot;&amp;quot;, end=&amp;quot;\r&amp;quot;)
ridge_coefs2 = np.reshape(ridge_coefs2,(n_grid,-1))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Alpha 30/30
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# OLS regression
y_train = X_train @ b0 + e_train
ols = LinearRegression().fit(X_train,y_train)
ols_coefs = ols.coef_;
mod_ols = np.linalg.norm(ols_coefs)

# Relative magnitude
rel_beta = [np.linalg.norm(ridge_coefs2[i,:])/mod_ols for i in range(n_grid)]
rel_beta = np.reshape(rel_beta, (-1,1))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Figure 6.5
def make_figure_6_5():
    
    # Init
    fig, (ax1,ax2) = plt.subplots(1,2)
    fig.suptitle(&#39;Figure 6.5: Ridge Bias-Var decomposition&#39;)

    # MSE
    ax1.plot(df[&#39;alpha&#39;], df[[&#39;bias2&#39;,&#39;var&#39;,&#39;mse&#39;]]);
    ax1.set_xscale(&#39;log&#39;);
    ax1.set_xlabel(&#39;lambda&#39;); ax1.set_ylabel(&#39;Mean Squared Error&#39;);
    ax1.legend([&#39;Bias2&#39;,&#39;Variance&#39;,&#39;MSE&#39;], fontsize=12);

    # MSE
    ax2.plot(rel_beta, df[[&#39;bias2&#39;,&#39;var&#39;,&#39;mse&#39;]]);
    ax2.set_xlabel(&#39;Relative Beta&#39;); ax2.set_ylabel(&#39;Mean Squared Error&#39;);
    ax2.legend([&#39;Bias2&#39;,&#39;Variance&#39;,&#39;MSE&#39;], fontsize=12);
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;make_figure_6_5()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/05_regularization_86_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Ridge regression has the advantage of shrinking coefficients. However, unlike best subset, forward stepwise, and backward stepwise selection, which will generally select models that involve just a subset of the variables, ridge regression will include all $p$ predictors in the final model.&lt;/p&gt;
&lt;p&gt;Lasso solves that problem by using a different penalty function.&lt;/p&gt;
&lt;h3 id=&#34;lasso&#34;&gt;Lasso&lt;/h3&gt;
&lt;p&gt;The lasso coefficients minimize the following objective function:&lt;/p&gt;
&lt;p&gt;$$
\sum_{i=1}^{n}\left(y_{i}-\beta_{0}-\sum_{j=1}^{p} \beta_{j} x_{i j}\right)^{2}+\lambda \sum_{j=1}^{p}\left|\beta_{j}\right| = \mathrm{RSS} + \lambda \sum_{j=1}^p|\beta_j|
$$&lt;/p&gt;
&lt;p&gt;so that the main difference with respect to ridge regression is the penalty function $\lambda \sum_{j=1}^{p}\left|\beta_{j}\right|$ instead of $\lambda \sum_{j=1}^p (\beta_j)^2$.&lt;/p&gt;
&lt;p&gt;A consequence of this objective function is that Lasso is much more likely to shrink coefficients to exactly zero, while Ridge only decreases their magnitude. The reason why lies in the shape of the objective function. You can rewrite the Ridge and Lasso minimization problems as constrained optimization:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Ridge
$$
\underset{\beta}{\operatorname{min}} \ \left{\sum_{i=1}^{n}\left(y_{i}-\beta_{0}-\sum_{j=1}^{p} \beta_{j} x_{i j}\right)^{2}\right} \quad \text { subject to } \quad \sum_{j=1}^{p}\left|\beta_{j}\right| \leq s
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Lasso
$$
\underset{\beta}{\operatorname{min}} \ \left{\sum_{i=1}^{n}\left(y_{i}-\beta_{0}-\sum_{j=1}^{p} \beta_{j} x_{i j}\right)^{2}\right} \quad \text { subject to } \quad \sum_{j=1}^{p} \beta_{j}^{2} \leq s
$$&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In pictures, constrained optimization problem lookes like this.&lt;/p&gt;
&lt;img src=&#34;figures/ridgelasso.png&#34; alt=&#34;Drawing&#34; style=&#34;width: 600px;&#34;/&gt;
&lt;p&gt;The red curves represents the contour sets of the RSS. They are elliptical since the objective function is quadratic. The blue area represents the admissible set, i.e. the constraints. As we can see, it is much easier with Lasso to have the constrained optimum on one of the edges of the rhombus.&lt;/p&gt;
&lt;p&gt;We are now going to repeat the same exercise on the &lt;code&gt;Credit&lt;/code&gt; dataset, trying to predict account &lt;code&gt;Balance&lt;/code&gt; with a set of obsevable induvidual characteristics, for different values of the penalty paramenter $\lambda$.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# X and y
categ_cols = [&amp;quot;Gender&amp;quot;, &amp;quot;Student&amp;quot;, &amp;quot;Married&amp;quot;, &amp;quot;Ethnicity&amp;quot;]
X = credit.loc[:, credit.columns != &#39;Balance&#39;]
X = pd.get_dummies(X, columns=categ_cols, drop_first=True)
y = credit.loc[:,&#39;Balance&#39;]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The $\lambda$ grid is going to be slightly different now.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Init alpha grid
n_grid = 100
alphas = 10**np.linspace(0,3,n_grid).reshape(-1,1)
lasso = Lasso()
lasso_coefs = []

# Loop over values of alpha
for a in alphas:
    lasso.set_params(alpha=a)
    lasso.fit(scale(X), y)
    lasso_coefs.append(lasso.coef_)
lasso_coefs = np.reshape(lasso_coefs,(n_grid,-1))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We run OLS to plot the relative magnitude of the Lasso coefficients.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Relative magnitude
mod_ols = np.linalg.norm(ols_coefs)
rel_beta = [np.linalg.norm(lasso_coefs[i,:])/mod_ols for i in range(n_grid)]
rel_beta = np.reshape(rel_beta, (-1,1))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We plot the magnitude of the coefficients $\beta$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;for different values of $\lambda$&lt;/li&gt;
&lt;li&gt;for different values of of $||\beta||$&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Figure 6.6
def make_figure_6_6():

    # Init
    fig, (ax1,ax2) = plt.subplots(1,2)
    fig.suptitle(&#39;Figure 6.6&#39;)

    highlight = [0,1,2,7];

    # Plot coefficients - absolute
    ax1.plot(alphas, lasso_coefs[:,highlight], alpha=1)
    ax1.plot(alphas, lasso_coefs, c=&#39;grey&#39;, alpha=0.3)
    ax1.set_xscale(&#39;log&#39;)
    ax1.set_xlabel(&#39;lambda&#39;); ax1.set_ylabel(&#39;Standardized coefficients&#39;);
    ax1.legend([&#39;Income&#39;, &#39;Limit&#39;, &#39;Rating&#39;, &#39;Student&#39;], fontsize=12)

    # Plot coefficients - relative
    ax2.plot(rel_beta, lasso_coefs[:,highlight], alpha=1)
    ax2.plot(rel_beta, lasso_coefs, c=&#39;grey&#39;, alpha=0.3)
    ax2.set_xlabel(&#39;relative mod beta&#39;); ax2.set_ylabel(&#39;Standardized coefficients&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;make_figure_6_6()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/05_regularization_100_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Rating&lt;/code&gt; seems to be the most important variable, followed by &lt;code&gt;Limit&lt;/code&gt; and &lt;code&gt;Student&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;As with ridge regression, the lasso shrinks the coefficient estimates towards zero. However, in the case of the lasso, the $l_1$ penalty has the effect of forcing some of the coefficient estimates to be exactly equal to zero when the tuning parameter $\lambda$ is sufficiently large. Hence, much like best subset selection, the lasso performs variable selection.&lt;/p&gt;
&lt;p&gt;We say that the lasso yields &lt;strong&gt;sparse&lt;/strong&gt; models — that is, models that involve only a subset of the variable&lt;/p&gt;
&lt;p&gt;We now plot how the choice of $\lambda$ affects the bias-variance trade-off.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Init alpha grid
sim = 1000
n_grid = 30
df = pd.DataFrame({&#39;alpha&#39;:10**np.linspace(-1,1,n_grid)})
lasso_coefs2 = []

# Init simulations
sim = 1000
lasso = lambda a: Lasso(alpha=a, fit_intercept=False)

# Loop over values of alpha
for i in range(len(df)):
    print(&amp;quot;Alpha %1.0f/%1.0f&amp;quot; % (i+1,len(df)), end =&amp;quot;&amp;quot;)
    a = df.loc[i,&#39;alpha&#39;]
    df.loc[i,[&#39;mse&#39;,&#39;var&#39;,&#39;bias2&#39;]], c = compute_var_bias(X_train, b0, x0, a, k, n, sim, lasso)
    lasso_coefs2.append(c)
    print(&amp;quot;&amp;quot;, end=&amp;quot;\r&amp;quot;)
lasso_coefs2 = np.reshape(lasso_coefs2,(n_grid,-1))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Alpha 30/30
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Relative magnitude
mod_ols = np.linalg.norm(ols_coefs)
rel_beta = [np.linalg.norm(lasso_coefs2[k,:])/mod_ols for k in range(n_grid)]
rel_beta = np.reshape(rel_beta, (-1,1))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# OLS regression
y_train = X_train @ b0 + e_train
ols = LinearRegression().fit(X_train,y_train)
ols_coefs = ols.coef_;
mod_ols = np.linalg.norm(ols_coefs)

# Relative magnitude
mod_ols = np.linalg.norm(ols_coefs)
rel_beta = [np.linalg.norm(lasso_coefs2[k,:])/mod_ols for k in range(n_grid)]
rel_beta = np.reshape(rel_beta, (-1,1))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Figure 6.8
def make_figure_6_8():

    fig, (ax1,ax2) = plt.subplots(1,2, figsize=(12,5))
    fig.suptitle(&#39;Figure 6.8: Lasso Bias-Var decomposition&#39;)

    # MSE
    ax1.plot(df[&#39;alpha&#39;], df[[&#39;bias2&#39;,&#39;var&#39;,&#39;mse&#39;]]);
    ax1.set_xscale(&#39;log&#39;);
    ax1.set_xlabel(&#39;lambda&#39;); ax1.set_ylabel(&#39;Mean Squared Error&#39;);
    ax1.legend([&#39;Bias2&#39;,&#39;Variance&#39;,&#39;MSE&#39;], fontsize=12);

    # MSE
    ax2.plot(rel_beta, df[[&#39;bias2&#39;,&#39;var&#39;,&#39;mse&#39;]]);
    ax2.set_xlabel(&#39;Relative Beta&#39;); ax1.set_ylabel(&#39;Mean Squared Error&#39;);
    ax2.legend([&#39;Bias2&#39;,&#39;Variance&#39;,&#39;MSE&#39;], fontsize=12);
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;make_figure_6_8()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/05_regularization_108_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;As $\lambda$ increases the squared bias increases and the variance decreases.&lt;/p&gt;
&lt;h3 id=&#34;comparing-the-lasso-and-ridge-regression&#34;&gt;Comparing the Lasso and Ridge Regression&lt;/h3&gt;
&lt;p&gt;In order to obtain a better intuition about the behavior of ridge regression and the lasso, consider a simple special case with $n = p$, and $X$ a diagonal matrix with $1$’s on the diagonal and $0$’s in all off-diagonal elements. To simplify the problem further, assume also that we are performing regression without an intercept.&lt;/p&gt;
&lt;p&gt;With these assumptions, the usual least squares problem simplifies to the coefficients that minimize&lt;/p&gt;
&lt;p&gt;$$
\sum_{j=1}^{p}\left(y_{j}-\beta_{j}\right)^{2}
$$&lt;/p&gt;
&lt;p&gt;In this case, the least squares solution is given by&lt;/p&gt;
&lt;p&gt;$$
\hat \beta_j = y_j
$$&lt;/p&gt;
&lt;p&gt;One can show that in this setting, the ridge regression estimates take the form&lt;/p&gt;
&lt;p&gt;$$
\hat \beta_j^{RIDGE} = \frac{y_j}{1+\lambda}
$$&lt;/p&gt;
&lt;p&gt;and the lasso estimates take the form&lt;/p&gt;
&lt;p&gt;$$
\hat{\beta}&lt;em&gt;{j}^{LASSO}=\left{\begin{array}{ll}
y&lt;/em&gt;{j}-\lambda / 2 &amp;amp; \text { if } y_{j}&amp;gt;\lambda / 2 \
y_{j}+\lambda / 2 &amp;amp; \text { if } y_{j}&amp;lt;-\lambda / 2 \
0 &amp;amp; \text { if }\left|y_{j}\right| \leq \lambda / 2
\end{array}\right.
$$&lt;/p&gt;
&lt;p&gt;We plot the relationship visually.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;np.random.seed(3)

# Generate random data
n = 100
k = n
X = np.eye(k)
e = normal(0,1,(n,1))
b0 = uniform(-1,1,(k,1))
y = X @ b0 + e
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# OLS regression
reg = LinearRegression().fit(X,y)
ols_coefs = reg.coef_;

# Ridge regression
ridge = Ridge(alpha=1).fit(X,y)
ridge_coefs = ridge.coef_;

# Ridge regression
lasso = Lasso(alpha=0.01).fit(X,y)
lasso_coefs = lasso.coef_.reshape(1,-1);

# sort
order = np.argsort(y.reshape(1,-1), axis=1)
y_sorted = np.take_along_axis(ols_coefs, order, axis=1) 
ols_coefs = np.take_along_axis(ols_coefs, order, axis=1) 
ridge_coefs = np.take_along_axis(ridge_coefs, order, axis=1) 
lasso_coefs = np.take_along_axis(lasso_coefs, order, axis=1) 
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Figure 6.10
def make_figure_6_10():
    
    # Init
    fig, (ax1,ax2) = plt.subplots(1,2)
    fig.suptitle(&#39;Figure 6.10&#39;)

    # Ridge
    ax1.plot(y_sorted.T, ols_coefs.T)
    ax1.plot(y_sorted.T, ridge_coefs.T)
    ax1.set_xlabel(&#39;True Coefficient&#39;); ax1.set_ylabel(&#39;Estimated Coefficient&#39;);
    ax1.legend([&#39;OLS&#39;,&#39;Ridge&#39;], fontsize=12);

    # Lasso
    ax2.plot(y_sorted.T, ols_coefs.T)
    ax2.plot(y_sorted.T, lasso_coefs.T)
    ax2.set_xlabel(&#39;True Coefficient&#39;); ax2.set_ylabel(&#39;Estimated Coefficient&#39;);
    ax2.legend([&#39;OLS&#39;,&#39;Lasso&#39;], fontsize=12);
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt; make_figure_6_10()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/05_regularization_117_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;We see that ridge regression shrinks every dimension of the data by the same proportion, whereas the lasso hrinks all coefficients toward zero by a similar amount, and sufficiently small coefficients are shrunken all the way to zero.&lt;/p&gt;
&lt;h3 id=&#34;selecting-the-tuning-parameter&#34;&gt;Selecting the Tuning Parameter&lt;/h3&gt;
&lt;p&gt;Implementing ridge regression and the lasso requires a method for selecting a value for the tuning parameter $\lambda$.&lt;/p&gt;
&lt;p&gt;Cross-validation provides a simple way to tackle this problem. We choose a grid of $\lambda$ values, and compute the cross-validation error for each value of $\lambda$. We then select the tuning parameter value for which the cross-validation error is smallest. Finally, the model is re-fit using all of the available observations and the selected value of the tuning parameter.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# X and y
categ_cols = [&amp;quot;Gender&amp;quot;, &amp;quot;Student&amp;quot;, &amp;quot;Married&amp;quot;, &amp;quot;Ethnicity&amp;quot;]
X = credit.loc[:, credit.columns != &#39;Balance&#39;]
X = pd.get_dummies(X, columns=categ_cols, drop_first=True).values
y = credit.loc[:,&#39;Balance&#39;]
n = len(credit)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We are going to use 10-fold CV as cross-validation algorithm.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Get MSE
def cv_lasso(X,y,a):
    # Init mse
    mse = []
    
    # Generate splits
    kf10 = KFold(n_splits=10, random_state=None, shuffle=False)
    kf10.get_n_splits(X)
    
    # Loop over splits
    for train_index, test_index in kf10.split(X):
        X_train, X_test = X[train_index], X[test_index]
        y_train, y_test = y[train_index], y[test_index]
        lasso = Lasso(alpha=a).fit(X_train, y_train)
        y_hat = lasso.predict(X_test)
        mse.append(mean_squared_error(y_test, y_hat))
    return np.mean(mse)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Compute MSE over grid of alphas
n_grid = 30
alphas = 10**np.linspace(0,3,n_grid).reshape(-1,1)
MSE = [cv_lasso(X,y,a) for a in alphas]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What is the optimal $\lambda$?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Find minimum alpha
alpha_min = alphas[np.argmin(MSE)]
print(&#39;Best alpha by 10fold CV:&#39;,alpha_min[0])
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Best alpha by 10fold CV: 2.592943797404667
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We now plot the objective function and the implied coefficients at the optimal $\lambda$.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Get coefficients
coefs = []

# Loop over values of alpha
for a in alphas:
    lasso = Lasso(alpha=a).fit(scale(X), y)
    coefs.append(lasso.coef_)
coefs = np.reshape(coefs,(n_grid,-1))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;np.shape(coefs)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;(30, 11)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Figure 6.12
def make_figure_6_12():
    
    # Init
    fig, (ax1,ax2) = plt.subplots(1,2)
    fig.suptitle(&#39;Figure 6.12: Lasso 10-fold CV&#39;)

    # MSE by LOO CV
    ax1.plot(alphas, MSE, alpha=1);
    ax1.axvline(alpha_min, c=&#39;k&#39;, ls=&#39;--&#39;)
    ax1.set_xscale(&#39;log&#39;)
    ax1.set_xlabel(&#39;lambda&#39;); ax1.set_ylabel(&#39;MSE&#39;);

    highlight = [0,1,2,7];

    # Plot coefficients - absolute
    ax2.plot(alphas, coefs[:,highlight], alpha=1)
    ax2.plot(alphas, coefs, c=&#39;grey&#39;, alpha=0.3)
    ax2.axvline(alpha_min, c=&#39;k&#39;, ls=&#39;--&#39;)
    ax2.set_xscale(&#39;log&#39;)
    ax2.set_xlabel(&#39;lambda&#39;); ax2.set_ylabel(&#39;Standardized coefficients&#39;);
    ax2.legend([&#39;Income&#39;, &#39;Limit&#39;, &#39;Rating&#39;, &#39;Student&#39;], fontsize=10);
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;make_figure_6_12()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/05_regularization_131_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Convexity and Optimization</title>
      <link>https://matteocourthoud.github.io/course/ml-econ/06_convexity/</link>
      <pubDate>Wed, 09 Mar 2022 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/course/ml-econ/06_convexity/</guid>
      <description>&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Remove warnings
import warnings
warnings.filterwarnings(&#39;ignore&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Import
import autograd.numpy as np
from autograd import grad
import seaborn as sns
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Import matplotlib for graphs
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import axes3d

# Set global parameters
%matplotlib inline
plt.style.use(&#39;seaborn-white&#39;)
plt.rcParams[&#39;lines.linewidth&#39;] = 3
plt.rcParams[&#39;figure.figsize&#39;] = (10,6)
plt.rcParams[&#39;figure.titlesize&#39;] = 20
plt.rcParams[&#39;axes.titlesize&#39;] = 18
plt.rcParams[&#39;axes.labelsize&#39;] = 14
plt.rcParams[&#39;legend.fontsize&#39;] = 14
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Function to plot errors
def error_plot(ys, yscale=&#39;log&#39;):
    plt.figure()
    plt.xlabel(&#39;Step&#39;)
    plt.ylabel(&#39;Error&#39;)
    plt.yscale(yscale)
    plt.plot(range(len(ys)), ys)
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;61-gradient-descent&#34;&gt;6.1 Gradient Descent&lt;/h2&gt;
&lt;p&gt;We start with a basic implementation of projected gradient descent.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def gradient_descent(init, steps, grad, proj=lambda x: x):
    &amp;quot;&amp;quot;&amp;quot;Projected gradient descent.
    
    Inputs:
        initial: starting point
        steps: list of scalar step sizes
        grad: function mapping points to gradients
        proj (optional): function mapping points to points
        
    Returns:
        List of all points computed by projected gradient descent.
    &amp;quot;&amp;quot;&amp;quot;
    xs = [init]
    for step in steps:
        xs.append(proj(xs[-1] - step * grad(xs[-1])))
    return xs
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that this implementation keeps around all points computed along the way. This is clearly not what you would do on large instances. We do this for illustrative purposes to be able to easily inspect the computed sequence of points.&lt;/p&gt;
&lt;h3 id=&#34;warm-up-optimizing-a-quadratic&#34;&gt;Warm-up: Optimizing a quadratic&lt;/h3&gt;
&lt;p&gt;As a toy example, let&amp;rsquo;s optimize $$f(x)=\frac12|x|^2,$$ which has the gradient map $\nabla f(x)=x.$&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def quadratic(x):
    return 0.5*x.dot(x)

def quadratic_gradient(x):
    return x
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note the function is $1$-smooth and $1$-strongly convex. Our theorems would then suggest that we use a constant step size of $1.$ If you think about it, for this step size the algorithm will actually find the optimal solution in just one step.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;x0 = np.random.normal(0, 1, (1000))
_, x1 = gradient_descent(x0, [1.0], quadratic_gradient)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Indeed, it does.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;x1.all() == 0
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;True
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s say we don&amp;rsquo;t have the right learning rate.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;xs = gradient_descent(x0, [0.1]*50, quadratic_gradient)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Plot errors along steps
error_plot([quadratic(x) for x in xs])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/06_convexity_18_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;constrained-optimization&#34;&gt;Constrained Optimization&lt;/h3&gt;
&lt;p&gt;Let&amp;rsquo;s say we want to optimize the function inside some affine subspace. Recall that affine subspaces are convex sets. Below we pick a random low dimensional affine subspace $b+U$ and define the corresponding linear projection operator.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# U is an orthonormal basis of a random 100-dimensional subspace.
U = np.linalg.qr(np.random.normal(0, 1, (1000, 100)))[0]
b = np.random.normal(0, 1, 1000)

def proj(x):
    &amp;quot;&amp;quot;&amp;quot;Projection of x onto an affine subspace&amp;quot;&amp;quot;&amp;quot;
    return b + U.dot(U.T).dot(x-b)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;x0 = np.random.normal(0, 1, (1000))
xs = gradient_descent(x0, [0.1]*50, quadratic_gradient, proj)
# the optimal solution is the projection of the origin
x_opt = proj(0)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s plot the results.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;error_plot([quadratic(x) for x in xs])
plt.plot(range(len(xs)), [quadratic(x_opt)]*len(xs),
        label=&#39;$\\frac{1}{2}|\!|x_{\mathrm{opt}}|\!|^2$&#39;)
plt.legend();
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/06_convexity_24_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;The orangle line shows the optimal error, which the algorithm reaches quickly.&lt;/p&gt;
&lt;p&gt;The iterates also converge to the optimal solution in domain as the following plot shows.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;error_plot([np.linalg.norm(x_opt-x)**2 for x in xs])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/06_convexity_27_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;least-squares&#34;&gt;Least Squares&lt;/h3&gt;
&lt;p&gt;One of the most fundamental data analysis tools is &lt;em&gt;linear least squares&lt;/em&gt;. Given an $m\times n$ matrix $A$ and a vector $b$ our goal is to find a vector $x\in\mathbb{R}^n$ that minimizes the following objective:&lt;/p&gt;
&lt;p&gt;
$$f(x) = \frac 1{2m}\sum_{i=1}^m (a_i^\top x - b_j)^2 
=\frac1{2m}\|Ax-b\|^2$$
&lt;/p&gt;
&lt;p&gt;We can verify that $\nabla f(x) = A^\top(Ax-b)$ and
$\nabla^2 f(x) = A^\top A.$&lt;/p&gt;
&lt;p&gt;Hence, the objective is $\beta$-smooth with
$\beta=\lambda_{\mathrm{max}}(A^\top A)$, and $\alpha$-strongly convex with $\alpha=\lambda_{\mathrm{min}}(A^\top A)$.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def least_squares(A, b, x):
    &amp;quot;&amp;quot;&amp;quot;Least squares objective.&amp;quot;&amp;quot;&amp;quot;
    return (0.5/m) * np.linalg.norm(A.dot(x)-b)**2

def least_squares_gradient(A, b, x):
    &amp;quot;&amp;quot;&amp;quot;Gradient of least squares objective at x.&amp;quot;&amp;quot;&amp;quot;
    return A.T.dot(A.dot(x)-b)/m
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;overdetermined-case-mge-n&#34;&gt;Overdetermined case $m\ge n$&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;m, n = 1000, 100
A = np.random.normal(0, 1, (m, n))
x_opt = np.random.normal(0, 1, n)
noise = np.random.normal(0, 0.1, m)
b = A.dot(x_opt) + noise
objective = lambda x: least_squares(A, b, x)
gradient = lambda x: least_squares_gradient(A, b, x)
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;convergence-in-objective&#34;&gt;Convergence in Objective&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;x0 = np.random.normal(0, 1, n)
xs = gradient_descent(x0, [0.1]*100, gradient)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;error_plot([objective(x) for x in xs])
plt.plot(range(len(xs)), [np.linalg.norm(noise)**2]*len(xs),
        label=&#39;noise level&#39;)
plt.plot(range(len(xs)), [least_squares(A,b,x_opt)]*len(xs),
        label=&#39;optimal&#39;)
plt.legend();
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/06_convexity_35_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;convergence-in-domain&#34;&gt;Convergence in Domain&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;error_plot([np.linalg.norm(x-x_opt)**2 for x in xs])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/06_convexity_37_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;underdetermined-case-m--n&#34;&gt;Underdetermined Case $m &amp;lt; n$&lt;/h3&gt;
&lt;p&gt;In the underdetermined case, the least squares objective is inevitably not strongly convex, since $A^\top A$ is a rank deficient matrix and hence $\lambda_{\mathrm{min}}(A^\top A)=0.$&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;m, n = 100, 1000
A = np.random.normal(0, 1, (m, n))
b = np.random.normal(0, 1, m)
# The least norm solution is given by the pseudo-inverse
x_opt = np.linalg.pinv(A).dot(b)
objective = lambda x: least_squares(A, b, x)
gradient = lambda x: least_squares_gradient(A, b, x)
x0 = np.random.normal(0, 1, n)
xs = gradient_descent(x0, [0.1]*100, gradient)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Results.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;error_plot([objective(x) for x in xs])
plt.plot(range(len(xs)), [least_squares(A,b,x_opt)]*len(xs),
        label=&#39;optimal&#39;)
plt.legend();
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/06_convexity_41_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;While we quickly reduce the error, we don&amp;rsquo;t actually converge in domain to the least norm solution. This is just because the function is no longer strongly convex in the underdetermined case.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;error_plot([np.linalg.norm(x-x_opt)**2 for x in xs], yscale=&#39;linear&#39;)
plt.plot(range(len(xs)), [np.linalg.norm(x_opt)**2]*len(xs),
         label=&#39;$|\!|x_{\mathrm{opt}}|\!|^2$&#39;)
plt.legend();
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/06_convexity_43_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;ell_2-regularized-least-squares&#34;&gt;$\ell_2$-regularized least squares&lt;/h2&gt;
&lt;p&gt;In the underdetermined case, it is often desirable to restore strong convexity of the objective function by adding an $\ell_2^2$-penality, also known as &lt;em&gt;Tikhonov regularization&lt;/em&gt;, $\ell_2$-regularization, or &lt;em&gt;weight decay&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;
$$\frac1{2m}\|Ax-b\|^2 + \frac{\alpha}2\|x\|^2$$
&lt;/p&gt;
&lt;p&gt;Note: With this modification the objective is $\alpha$-strongly convex again.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def least_squares_l2(A, b, x, alpha=0.1):
    return least_squares(A, b, x) + (alpha/2) * x.dot(x)

def least_squares_l2_gradient(A, b, x, alpha=0.1):
    return least_squares_gradient(A, b, x) + alpha * x
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s create a least squares instance.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;m, n = 100, 1000
A = np.random.normal(0, 1, (m, n))
b = A.dot(np.random.normal(0, 1, n))
objective = lambda x: least_squares_l2(A, b, x)
gradient = lambda x: least_squares_l2_gradient(A, b, x)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that we can find the optimal solution to the optimization problem in closed form without even running gradient descent by computing $x_{\mathrm{opt}}=(A^\top+\alpha I)^{-1}A^\top b.$ Please verify that this point is indeed optimal.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;x_opt = np.linalg.inv(A.T.dot(A) + 0.1*np.eye(1000)).dot(A.T).dot(b)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here&amp;rsquo;s how gradient descent fares.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;x0 = np.random.normal(0, 1, n)
xs = gradient_descent(x0, [0.1]*500, gradient)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We plot the descent.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;error_plot([objective(x) for x in xs])
plt.plot(range(len(xs)), [least_squares_l2(A,b,x_opt)]*len(xs),
        label=&#39;optimal&#39;)
plt.legend();
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/06_convexity_54_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;You see that the error doesn&amp;rsquo;t decrease below a certain level due to the regularization term. This is not a bad thing. In fact, the regularization term gives as &lt;em&gt;strong convexity&lt;/em&gt; which leads to convergence in domain again:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;xs = gradient_descent(x0, [0.1]*500, gradient)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;error_plot([np.linalg.norm(x-x_opt)**2 for x in xs])
plt.plot(range(len(xs)), [np.linalg.norm(x_opt)**2]*len(xs),
        label=&#39;squared norm of $x_{\mathrm{opt}}$&#39;)
plt.legend();
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/06_convexity_57_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;the-magic-of-implicit-regularization&#34;&gt;The Magic of Implicit Regularization&lt;/h2&gt;
&lt;p&gt;Sometimes simply running gradient descent from a suitable initial point has a regularizing effect on its own &lt;strong&gt;without introducing an explicit regularization term&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;We will see this below where we revisit the unregularized least squares objective, but initialize gradient descent from the origin rather than a random gaussian point.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# We initialize from 0
x0 = np.zeros(n)
# Note this is the gradient w.r.t. the unregularized objective!
gradient = lambda x: least_squares_gradient(A, b, x)
xs = gradient_descent(x0, [0.1]*50, gradient)
error_plot([np.linalg.norm(x_opt-x)**2 for x in xs], yscale=&#39;linear&#39;)
plt.plot(range(len(xs)), [np.linalg.norm(x_opt)**2]*len(xs),
         label=&#39;$|\!|x_{\mathrm{opt}}|\!|^2$&#39;)
plt.legend();
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/06_convexity_60_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Incredible!&lt;/em&gt; We converge to the minimum norm solution!&lt;/p&gt;
&lt;p&gt;Implicit regularization is a deep phenomenon that&amp;rsquo;s an active research topic in learning and optimization. It&amp;rsquo;s exciting that we see it play out in this simple least squares problem already!&lt;/p&gt;
&lt;h2 id=&#34;lasso&#34;&gt;LASSO&lt;/h2&gt;
&lt;p&gt;LASSO is the name for $\ell_1$-regularized least squares regression:&lt;/p&gt;
&lt;p&gt;
$$\frac1{2m}\|Ax-b\|^2 + \alpha\|x\|_1$$
&lt;/p&gt;
&lt;p&gt;We will see that LASSO is able to fine &lt;em&gt;sparse&lt;/em&gt; solutions if they exist. This is a common motivation for using an $\ell_1$-regularizer.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def lasso(A, b, x, alpha=0.1):
    return least_squares(A, b, x) + alpha * np.linalg.norm(x, 1)

def ell1_subgradient(x):
    &amp;quot;&amp;quot;&amp;quot;Subgradient of the ell1-norm at x.&amp;quot;&amp;quot;&amp;quot;
    g = np.ones(x.shape)
    g[x &amp;lt; 0.] = -1.0
    return g

def lasso_subgradient(A, b, x, alpha=0.1):
    &amp;quot;&amp;quot;&amp;quot;Subgradient of the lasso objective at x&amp;quot;&amp;quot;&amp;quot;
    return least_squares_gradient(A, b, x) + alpha*ell1_subgradient(x)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;m, n = 100, 1000
A = np.random.normal(0, 1, (m, n))
x_opt = np.zeros(n)
x_opt[:10] = 1.0
b = A.dot(x_opt)
x0 = np.random.normal(0, 1, n)
xs = gradient_descent(x0, [0.1]*500, lambda x: lasso_subgradient(A, b, x))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;error_plot([lasso(A, b, x) for x in xs])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/06_convexity_66_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plt.figure()
plt.title(&#39;Comparison of initial, optimal, and computed point&#39;)
idxs = range(50)
plt.plot(idxs, x0[idxs], &#39;--&#39;, color=&#39;#aaaaaa&#39;, label=&#39;initial&#39;)
plt.plot(idxs, x_opt[idxs], &#39;r-&#39;, label=&#39;optimal&#39;)
plt.plot(idxs, xs[-1][idxs], &#39;g-&#39;, label=&#39;final&#39;)
plt.xlabel(&#39;Coordinate&#39;)
plt.ylabel(&#39;Value&#39;)
plt.legend();
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/06_convexity_67_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;As promised, LASSO correctly identifies the significant coordinates of the optimal solution. This is why, in practice, LASSO is a popular tool for feature selection.&lt;/p&gt;
&lt;p&gt;Play around with this plot to inspect other points along the way, e.g., the point that achieves lowest objective value. Why does the objective value go up even though we continue to get better solutions?&lt;/p&gt;
&lt;h2 id=&#34;support-vector-machines&#34;&gt;Support Vector Machines&lt;/h2&gt;
&lt;p&gt;In a linear classification problem, we&amp;rsquo;re given $m$ labeled points $(a_i, y_i)$ and we wish to find a hyperplane given by a point $x$ that separates them so that&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\langle a_i, x\rangle \ge 1$ when $y_i=1$, and&lt;/li&gt;
&lt;li&gt;$\langle a_i, x\rangle \le -1$ when $y_i = -1$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The smaller the norm $|x|$ the larger the &lt;em&gt;margin&lt;/em&gt; between positive and negative instances. Therefore, it makes sense to throw in a regularizer that penalizes large norms. This leads to the objective.&lt;/p&gt;
&lt;p&gt;
$$\frac 1m \sum_{i=1}^m \max\{1-y_i(a_i^\top x), 0\} + \frac{\alpha}2\|x\|^2$$
&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def hinge_loss(z):
    return np.maximum(1.-z, np.zeros(z.shape))

def svm_objective(A, y, x, alpha=0.1):
    &amp;quot;&amp;quot;&amp;quot;SVM objective.&amp;quot;&amp;quot;&amp;quot;
    m, _ = A.shape
    return np.mean(hinge_loss(np.diag(y).dot(A.dot(x))))+(alpha/2)*x.dot(x)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;z = np.linspace(-2, 2, 100)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plt.figure()
plt.plot(z, hinge_loss(z));
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/06_convexity_73_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def hinge_subgradient(z):
    g = np.zeros(z.shape)
    g[z &amp;lt; 1] = -1.
    return g

def svm_subgradient(A, y, x, alpha=0.1):
    g1 = hinge_subgradient(np.diag(y).dot(A.dot(x)))
    g2 = np.diag(y).dot(A)
    return g1.dot(g2) + alpha*x
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plt.figure()
plt.plot(z, hinge_subgradient(z));
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/06_convexity_75_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;m, n = 1000, 100
A = np.vstack([np.random.normal(0.1, 1, (m//2, n)),
               np.random.normal(-0.1, 1, (m//2, n))])
y = np.hstack([np.ones(m//2), -1.*np.ones(m//2)])
x0 = np.random.normal(0, 1, n)
xs = gradient_descent(x0, [0.01]*100, 
                      lambda x: svm_subgradient(A, y, x, 0.05))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;error_plot([svm_objective(A, y, x) for x in xs])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/06_convexity_77_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s see if averaging out the solutions gives us an improved function value.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;xavg = 0.0
for x in xs:
    xavg += x
svm_objective(A, y, xs[-1]), svm_objective(A, y, xavg/len(xs))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;(1.0710162653835846, 0.9069593413738611)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can also look at the accuracy of our linear model for predicting the labels. From how we defined the data, we can see that the all ones vector is the highest accuracy classifier in the limit of infinite data (very large $m$). For a finite data set, the accuracy could be even higher due to random fluctuations.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def accuracy(A, y, x):
    return np.mean(np.diag(y).dot(A.dot(x))&amp;gt;0)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plt.figure()
plt.ylabel(&#39;Accuracy&#39;)
plt.xlabel(&#39;Step&#39;)
plt.plot(range(len(xs)), [accuracy(A, y, x) for x in xs])
plt.plot(range(len(xs)), [accuracy(A, y, np.ones(n))]*len(xs),
        label=&#39;Population optimum&#39;)
plt.legend();
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/06_convexity_82_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;We see that the accuracy spikes pretty early and drops a bit as we train for too long.&lt;/p&gt;
&lt;h2 id=&#34;sparse-inverse-covariance-estimation&#34;&gt;Sparse Inverse Covariance Estimation&lt;/h2&gt;
&lt;p&gt;Given a positive semidefinite matrix $S\in\mathbb{R}^{n\times n}$ the objective function in sparse inverse covariance estimation is as follows:&lt;/p&gt;
&lt;p&gt;
$$ \min_{X\in\mathbb{R}^{n\times n}, X\succeq 0} 
\langle S, X\rangle - \log\det(X) + \alpha\|X\|_1$$
&lt;/p&gt;
&lt;p&gt;Here, we define
$$\langle S, X\rangle = \mathrm{trace}(S^\top X)$$
and
$$|X|&lt;em&gt;1 = \sum&lt;/em&gt;{ij}|X_{ij}|.$$&lt;/p&gt;
&lt;p&gt;Typically, we think of the matrix $S$ as a sample covariance matrix of a set of vectors $a_1,\dots, a_m,$ defined as:
$$
S = \frac1{m-1}\sum_{i=1}^n a_ia_i^\top
$$
The example also highlights the utility of automatic differentiation as provided by the &lt;code&gt;autograd&lt;/code&gt; package that we&amp;rsquo;ll regularly use. In a later lecture we will understand exactly how automatic differentiation works. For now we just treat it as a blackbox that gives us gradients.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;np.random.seed(1337)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def sparse_inv_cov(S, X, alpha=0.1):
    return (np.trace(S.T.dot(X))
            - np.log(np.linalg.det(X))
            + alpha * np.sum(np.abs(X)))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;n = 5
A = np.random.normal(0, 1, (n, n))
S = A.dot(A.T)
objective = lambda X: sparse_inv_cov(S, X)
# autograd provides a &amp;quot;gradient&amp;quot;, yay!
gradient = grad(objective)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We also need to worry about the projection onto the positive semidefinite cone, which corresponds to truncating eigenvalues.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def projection(X):
    &amp;quot;&amp;quot;&amp;quot;Projection onto positive semidefinite cone.&amp;quot;&amp;quot;&amp;quot;
    es, U = np.linalg.eig(X)
    es[es&amp;lt;0] = 0.
    return U.dot(np.diag(es).dot(U.T))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;A0 = np.random.normal(0, 1, (n,n))
X0 = A0.dot(A0.T)
Xs = gradient_descent(X0, [0.01]*500, gradient, projection)
error_plot([objective(X) for X in Xs])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/06_convexity_91_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;going-crazy-with-autograd&#34;&gt;Going crazy with autograd&lt;/h2&gt;
&lt;p&gt;Just for fun, we&amp;rsquo;ll go through a crazy example below. We can use &lt;code&gt;autograd&lt;/code&gt; not just for getting gradients for natural objectives, we can in principle also use it to tune hyperparameters of our optimizer, like the step size schedulde.&lt;/p&gt;
&lt;p&gt;Below we see how we can find a better 10-step learning rate schedules for optimizing a quadratic. This is mostly just for illustrative purposes (although some researchers are exploring these kinds of ideas more seriously).&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;x0 = np.random.normal(0, 1, 1000)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def f(x):
    return 0.5*np.dot(x,x)

def optimizer(steps):
    &amp;quot;&amp;quot;&amp;quot;Optimize a quadratic with the given steps.&amp;quot;&amp;quot;&amp;quot;
    xs = gradient_descent(x0, steps, grad(f))
    return f(xs[-1])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The function &lt;code&gt;optimizer&lt;/code&gt; is a non-differentiable function of its input &lt;code&gt;steps&lt;/code&gt;. Nontheless, &lt;code&gt;autograd&lt;/code&gt; will provide a gradient that we can stick into gradient descent. That is, we&amp;rsquo;re tuning gradient descent with gradient descent.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;grad_optimizer = grad(optimizer)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;initial_steps = np.abs(np.random.normal(0, 0.1, 10))
better_steps = gradient_descent(initial_steps, [0.001]*500, grad_optimizer)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;error_plot([optimizer(steps) for steps in better_steps])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/06_convexity_99_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;As we can see, the learning rate schedules improve dramatically over time. Of course, we already know from the first example that there is a step size schedule that converges in one step. Interestingly, the last schedule we find here doesn&amp;rsquo;t look at all like what we might expect:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plt.figure()
plt.xticks(range(len(better_steps[-1])))
plt.ylabel(&#39;Step size&#39;)
plt.xlabel(&#39;Step number&#39;)
plt.plot(range(len(better_steps[-1])), better_steps[-1]);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/06_convexity_101_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Tree-based Methods</title>
      <link>https://matteocourthoud.github.io/course/ml-econ/07_trees/</link>
      <pubDate>Wed, 09 Mar 2022 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/course/ml-econ/07_trees/</guid>
      <description>&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;%matplotlib inline
from utils.lecture07 import *
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;decision-trees&#34;&gt;Decision Trees&lt;/h2&gt;
&lt;p&gt;Decision trees involve &lt;strong&gt;segmenting the predictor space into a number of simple regions&lt;/strong&gt;. In order to make a prediction for a given observation, we typically use the mean or the mode of the training observations in the region to which it belongs. Since the set of splitting rules used to segment the predictor space can be summarized in a tree, these types of approaches are known as decision tree methods.&lt;/p&gt;
&lt;h3 id=&#34;regression-trees&#34;&gt;Regression Trees&lt;/h3&gt;
&lt;p&gt;For this session we will consider the &lt;code&gt;Hitters&lt;/code&gt; dataset. It consists in individual level data of baseball players. In our applications, we are interested in predicting the players &lt;code&gt;Salary&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Load the data
hitters = pd.read_csv(&#39;data/Hitters.csv&#39;).dropna()
hitters.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;Unnamed: 0&lt;/th&gt;
      &lt;th&gt;AtBat&lt;/th&gt;
      &lt;th&gt;Hits&lt;/th&gt;
      &lt;th&gt;HmRun&lt;/th&gt;
      &lt;th&gt;Runs&lt;/th&gt;
      &lt;th&gt;RBI&lt;/th&gt;
      &lt;th&gt;Walks&lt;/th&gt;
      &lt;th&gt;Years&lt;/th&gt;
      &lt;th&gt;CAtBat&lt;/th&gt;
      &lt;th&gt;CHits&lt;/th&gt;
      &lt;th&gt;...&lt;/th&gt;
      &lt;th&gt;CRuns&lt;/th&gt;
      &lt;th&gt;CRBI&lt;/th&gt;
      &lt;th&gt;CWalks&lt;/th&gt;
      &lt;th&gt;League&lt;/th&gt;
      &lt;th&gt;Division&lt;/th&gt;
      &lt;th&gt;PutOuts&lt;/th&gt;
      &lt;th&gt;Assists&lt;/th&gt;
      &lt;th&gt;Errors&lt;/th&gt;
      &lt;th&gt;Salary&lt;/th&gt;
      &lt;th&gt;NewLeague&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;-Alan Ashby&lt;/td&gt;
      &lt;td&gt;315&lt;/td&gt;
      &lt;td&gt;81&lt;/td&gt;
      &lt;td&gt;7&lt;/td&gt;
      &lt;td&gt;24&lt;/td&gt;
      &lt;td&gt;38&lt;/td&gt;
      &lt;td&gt;39&lt;/td&gt;
      &lt;td&gt;14&lt;/td&gt;
      &lt;td&gt;3449&lt;/td&gt;
      &lt;td&gt;835&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;321&lt;/td&gt;
      &lt;td&gt;414&lt;/td&gt;
      &lt;td&gt;375&lt;/td&gt;
      &lt;td&gt;N&lt;/td&gt;
      &lt;td&gt;W&lt;/td&gt;
      &lt;td&gt;632&lt;/td&gt;
      &lt;td&gt;43&lt;/td&gt;
      &lt;td&gt;10&lt;/td&gt;
      &lt;td&gt;475.0&lt;/td&gt;
      &lt;td&gt;N&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;-Alvin Davis&lt;/td&gt;
      &lt;td&gt;479&lt;/td&gt;
      &lt;td&gt;130&lt;/td&gt;
      &lt;td&gt;18&lt;/td&gt;
      &lt;td&gt;66&lt;/td&gt;
      &lt;td&gt;72&lt;/td&gt;
      &lt;td&gt;76&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;1624&lt;/td&gt;
      &lt;td&gt;457&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;224&lt;/td&gt;
      &lt;td&gt;266&lt;/td&gt;
      &lt;td&gt;263&lt;/td&gt;
      &lt;td&gt;A&lt;/td&gt;
      &lt;td&gt;W&lt;/td&gt;
      &lt;td&gt;880&lt;/td&gt;
      &lt;td&gt;82&lt;/td&gt;
      &lt;td&gt;14&lt;/td&gt;
      &lt;td&gt;480.0&lt;/td&gt;
      &lt;td&gt;A&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;-Andre Dawson&lt;/td&gt;
      &lt;td&gt;496&lt;/td&gt;
      &lt;td&gt;141&lt;/td&gt;
      &lt;td&gt;20&lt;/td&gt;
      &lt;td&gt;65&lt;/td&gt;
      &lt;td&gt;78&lt;/td&gt;
      &lt;td&gt;37&lt;/td&gt;
      &lt;td&gt;11&lt;/td&gt;
      &lt;td&gt;5628&lt;/td&gt;
      &lt;td&gt;1575&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;828&lt;/td&gt;
      &lt;td&gt;838&lt;/td&gt;
      &lt;td&gt;354&lt;/td&gt;
      &lt;td&gt;N&lt;/td&gt;
      &lt;td&gt;E&lt;/td&gt;
      &lt;td&gt;200&lt;/td&gt;
      &lt;td&gt;11&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;500.0&lt;/td&gt;
      &lt;td&gt;N&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;-Andres Galarraga&lt;/td&gt;
      &lt;td&gt;321&lt;/td&gt;
      &lt;td&gt;87&lt;/td&gt;
      &lt;td&gt;10&lt;/td&gt;
      &lt;td&gt;39&lt;/td&gt;
      &lt;td&gt;42&lt;/td&gt;
      &lt;td&gt;30&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;396&lt;/td&gt;
      &lt;td&gt;101&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;48&lt;/td&gt;
      &lt;td&gt;46&lt;/td&gt;
      &lt;td&gt;33&lt;/td&gt;
      &lt;td&gt;N&lt;/td&gt;
      &lt;td&gt;E&lt;/td&gt;
      &lt;td&gt;805&lt;/td&gt;
      &lt;td&gt;40&lt;/td&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;91.5&lt;/td&gt;
      &lt;td&gt;N&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;5&lt;/th&gt;
      &lt;td&gt;-Alfredo Griffin&lt;/td&gt;
      &lt;td&gt;594&lt;/td&gt;
      &lt;td&gt;169&lt;/td&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;74&lt;/td&gt;
      &lt;td&gt;51&lt;/td&gt;
      &lt;td&gt;35&lt;/td&gt;
      &lt;td&gt;11&lt;/td&gt;
      &lt;td&gt;4408&lt;/td&gt;
      &lt;td&gt;1133&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;501&lt;/td&gt;
      &lt;td&gt;336&lt;/td&gt;
      &lt;td&gt;194&lt;/td&gt;
      &lt;td&gt;A&lt;/td&gt;
      &lt;td&gt;W&lt;/td&gt;
      &lt;td&gt;282&lt;/td&gt;
      &lt;td&gt;421&lt;/td&gt;
      &lt;td&gt;25&lt;/td&gt;
      &lt;td&gt;750.0&lt;/td&gt;
      &lt;td&gt;A&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;5 rows × 21 columns&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;In particular, we are interested in looking how the number of &lt;code&gt;Hits&lt;/code&gt; and the &lt;code&gt;Years&lt;/code&gt; of experience predict the &lt;code&gt;Salary&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Get Features
features = [&#39;Years&#39;, &#39;Hits&#39;]
X = hitters[features].values
y = np.log(hitters.Salary.values)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We are actually going to use log(salary) since it has a more gaussian distribution.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;fig, (ax1, ax2) = plt.subplots(1,2, figsize=(11,4))

# Plot salary distribution
ax1.hist(hitters.Salary.values)
ax1.set_xlabel(&#39;Salary&#39;)
ax2.hist(y)
ax2.set_xlabel(&#39;Log(Salary)&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/07_trees_10_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;In order to understand what is a tree, let&amp;rsquo;s first have a look at one. We fit a regression three with 3 leaves or, equivalently put, 2 nodes.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Fit regression tree
tree = DecisionTreeRegressor(max_leaf_nodes=3)
tree.fit(X, y)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;DecisionTreeRegressor(max_leaf_nodes=3)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We are now going to plot the results visually. The biggest avantage of trees is interpretability.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Figure 8.1
fig, ax = plt.subplots(1,1)
ax.set_title(&#39;Figure 8.1&#39;);

# Plot tree
plot_tree(tree, filled=True, feature_names=features, fontsize=14, ax=ax);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/07_trees_14_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;The tree consists of a series of splitting rules, starting at the top of the tree. The top split assigns observations having &lt;code&gt;Years&lt;/code&gt;&amp;lt;4.5 to the left branch.1 The predicted salary for these players is given by the mean response value for the players in the data set with &lt;code&gt;Years&lt;/code&gt;&amp;lt;4.5. For such players, the mean log salary is 5.107, and so we make a prediction of 5.107 thousands of dollars, i.e. $165,174, for these players. Players with &lt;code&gt;Years&lt;/code&gt;&amp;gt;=4.5 are assigned to the right branch, and then that group is further subdivided by &lt;code&gt;Hits&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Overall, the tree stratifies or segments the players into three regions of predictor space:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;players who have played for four or fewer years&lt;/li&gt;
&lt;li&gt;players who have played for five or more years and who made fewer than 118 hits last year, and&lt;/li&gt;
&lt;li&gt;players who have played for five or more years and who made at least 118 hits last year.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;These three regions can be written as&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;R1&lt;/strong&gt; = {X | &lt;code&gt;Years&lt;/code&gt;&amp;lt;4.5}&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;R2&lt;/strong&gt; = {X | &lt;code&gt;Years&lt;/code&gt;&amp;gt;=4.5, &lt;code&gt;Hits&lt;/code&gt;&amp;lt;117.5}, and&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;R3&lt;/strong&gt; = {X | &lt;code&gt;Years&lt;/code&gt;&amp;gt;=4.5, &lt;code&gt;Hits&lt;/code&gt;&amp;gt;=117.5}.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Since the dimension of $X$ is 2, we can visualize the space and the regions in a 2-dimensional graph.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Figure 8.2
def make_figure_8_2():
    
    # Init
    hitters.plot(&#39;Years&#39;, &#39;Hits&#39;, kind=&#39;scatter&#39;, color=&#39;orange&#39;, figsize=(7,6))
    plt.title(&#39;Figure 8.2&#39;)
    plt.xlim(0,25); plt.ylim(ymin=-5);
    plt.xticks([1, 4.5, 24]); plt.yticks([1, 117.5, 238]);

    # Split lines
    plt.vlines(4.5, ymin=-5, ymax=250, color=&#39;g&#39;)
    plt.hlines(117.5, xmin=4.5, xmax=25, color=&#39;g&#39;)

    # Regions
    plt.annotate(&#39;R1&#39;, xy=(2,117.5), fontsize=&#39;xx-large&#39;)
    plt.annotate(&#39;R2&#39;, xy=(11,60), fontsize=&#39;xx-large&#39;)
    plt.annotate(&#39;R3&#39;, xy=(11,170), fontsize=&#39;xx-large&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt; make_figure_8_2()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/07_trees_19_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;We might &lt;strong&gt;interpret&lt;/strong&gt; the above regression tree as follows: Years is the most important factor in determining Salary, and players with less experience earn lower salaries than more experienced players. Given that a player is less experienced, the number of hits that he made in the previous year seems to play little role in his salary. But among players who have been in the major leagues for five or more years, the number of hits made in the previous year does affect salary, and players who made more hits last year tend to have higher salaries.&lt;/p&gt;
&lt;h3 id=&#34;building-a-tree&#34;&gt;Building a Tree&lt;/h3&gt;
&lt;p&gt;There are two main steps in the construction of a tree:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;We divide the predictor space—that is, the set of possible values for $X_1, X_2, &amp;hellip; , X_p$ into $J$ distinct and non-overlapping regions, $R_1,R_2,&amp;hellip;,R_J$.&lt;/li&gt;
&lt;li&gt;For every observation that falls into the region $R_j$ , we make the same prediction, which is simply the mean of the response values for the training observations in $R_j$.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The second step is easy. But how does one construct the regions? Our purpose is to minimize the Sum of Squared Residuals, across the different regions:&lt;/p&gt;
&lt;p&gt;$$
\sum_{j=1}^{J} \sum_{i \in R_{j}}\left(y_{i}-\hat{y}&lt;em&gt;{R&lt;/em&gt;{j}}\right)^{2}
$$&lt;/p&gt;
&lt;p&gt;Unfortunately, it is computationally infeasible to consider every possible partition of the feature space into $J$ boxes.&lt;/p&gt;
&lt;p&gt;For this reason, we take a &lt;strong&gt;top-down&lt;/strong&gt;, &lt;strong&gt;greedy&lt;/strong&gt; approach that is known as &lt;em&gt;recursive binary splitting&lt;/em&gt;. The approach is top-down because it begins at the top of the tree (at which point all observations belong to a single region) and then successively splits the predictor space; each split is indicated via two new branches further down on the tree. It is greedy because at each step of the tree-building process, the best split is made at that particular step, rather than looking ahead and picking a split that will lead to a better tree in some future step.&lt;/p&gt;
&lt;p&gt;In practice, the method is the following:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;we select the predictor $X_j$&lt;/li&gt;
&lt;li&gt;we select the cutpoint $s$ such that splitting the predictor space into the regions ${X|X_j &amp;lt; s}$ and ${X|X_j \geq s}$ leads to the greatest possible reduction in RSS&lt;/li&gt;
&lt;li&gt;we repeat (1)-(2) for all predictors $X_1, &amp;hellip; , X_p$, i.e. we solve&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;$$
\arg \min_{j,s} \ \sum_{i: x_{i} \in {X|X_j &amp;lt; s}}\left(y_{i}-\hat{y}&lt;em&gt;i\right)^{2}+\sum&lt;/em&gt;{i: x_{i} \in {X|X_j \geq s}}\left(y_{i}-\hat{y}_i\right)^{2}
$$&lt;/p&gt;
&lt;ol start=&#34;4&#34;&gt;
&lt;li&gt;we choose the predictor and cutpoint such that the resulting tree has the lowest RSS&lt;/li&gt;
&lt;li&gt;we keep repeating (1)-(4) until a certain condition is met. However, after the first iteration we also have to pick which region to split which adds a further dimension to optimize over.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Let&amp;rsquo;s build our own &lt;code&gt;Node&lt;/code&gt; class to play around with trees.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;class Node:
    &amp;quot;&amp;quot;&amp;quot;
    Class used to represent nodes in a Regression Tree
    
    Attributes
    ----------
    x : np.array
        independent variables
    y : np.array
        dependent variables
    idxs : np.array
        indexes fo x and y for current node
    depth : int
        depth of the sub-tree (default 5)

    Methods
    -------
    find_next_nodes(self)
        Keep growing the tree
        
    find_best_split(self)
        Find the best split
        
    split(self)
        Split the tree
    &amp;quot;&amp;quot;&amp;quot;
    
    def __init__(self, x, y, idxs, depth=5):
        &amp;quot;&amp;quot;&amp;quot;Initialize node&amp;quot;&amp;quot;&amp;quot;
        self.x = x
        self.y = y
        self.idxs = idxs 
        self.depth = depth
        self.get_next_nodes()

    def get_next_nodes(self):
        &amp;quot;&amp;quot;&amp;quot;If the node is not terminal, get further splits&amp;quot;&amp;quot;&amp;quot;
        if self.is_last_leaf: return 
        self.find_best_split()       
        self.split()             
        
    def find_best_split(self):
        &amp;quot;&amp;quot;&amp;quot;Loop over variables and their values to find the best split&amp;quot;&amp;quot;&amp;quot;
        best_score = float(&#39;inf&#39;)
        # Loop over variables
        for col in range(self.x.shape[1]):
            x = self.x[self.idxs, col]
            # Loop over all splits
            for s in np.unique(x):
                lhs = x &amp;lt;= s
                rhs = x &amp;gt; s
                curr_score = self.get_score(lhs, rhs)
                # If best score, save it 
                if curr_score &amp;lt; best_score: 
                    best_score = curr_score
                    self.split_col = col
                    self.split_val = s
        return self
    
    def get_score(self, lhs, rhs):
        &amp;quot;&amp;quot;&amp;quot;Get score of a given split&amp;quot;&amp;quot;&amp;quot;
        y = self.y[self.idxs]
        lhs_mse = self.get_mse(y[lhs])
        rhs_mse = self.get_mse(y[rhs])
        return lhs_mse * lhs.sum() + rhs_mse * rhs.sum()
        
    def get_mse(self, y): return np.mean((y-np.mean(y))**2)
    
    def split(self): 
        &amp;quot;&amp;quot;&amp;quot;Split a node into 2 sub-nodes (recursive)&amp;quot;&amp;quot;&amp;quot;
        x = self.x[self.idxs, self.split_col]
        lhs = x &amp;lt;= self.split_val
        rhs = x &amp;gt; self.split_val
        self.lhs = Node(self.x, self.y, self.idxs[lhs], self.depth-1)
        self.rhs = Node(self.x, self.y, self.idxs[rhs], self.depth-1)
        to_print = (self.depth, self.split_col, self.split_val, sum(lhs), sum(rhs))
        print(&#39;Split on layer %.0f: var%1.0f = %.4f (%.0f/%.0f)&#39; % to_print)
        return self
    
    @property
    def is_last_leaf(self): return self.depth&amp;lt;=1

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What does a &lt;code&gt;Node&lt;/code&gt; look like?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Init first node
tree1 = Node(X, y, np.arange(len(y)), 1)

# Documentation (always comment and document your code!)
print(tree1.__doc__)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;    Class used to represent nodes in a Regression Tree
    
    Attributes
    ----------
    x : np.array
        independent variables
    y : np.array
        dependent variables
    idxs : np.array
        indexes fo x and y for current node
    depth : int
        depth of the sub-tree (default 5)

    Methods
    -------
    find_next_nodes(self)
        Keep growing the tree
        
    find_best_split(self)
        Find the best split
        
    split(self)
        Split the tree
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Which properties does it have?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Inspect the class
dir(tree1)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[&#39;__class__&#39;,
 &#39;__delattr__&#39;,
 &#39;__dict__&#39;,
 &#39;__dir__&#39;,
 &#39;__doc__&#39;,
 &#39;__eq__&#39;,
 &#39;__format__&#39;,
 &#39;__ge__&#39;,
 &#39;__getattribute__&#39;,
 &#39;__gt__&#39;,
 &#39;__hash__&#39;,
 &#39;__init__&#39;,
 &#39;__init_subclass__&#39;,
 &#39;__le__&#39;,
 &#39;__lt__&#39;,
 &#39;__module__&#39;,
 &#39;__ne__&#39;,
 &#39;__new__&#39;,
 &#39;__reduce__&#39;,
 &#39;__reduce_ex__&#39;,
 &#39;__repr__&#39;,
 &#39;__setattr__&#39;,
 &#39;__sizeof__&#39;,
 &#39;__str__&#39;,
 &#39;__subclasshook__&#39;,
 &#39;__weakref__&#39;,
 &#39;depth&#39;,
 &#39;find_best_split&#39;,
 &#39;get_mse&#39;,
 &#39;get_next_nodes&#39;,
 &#39;get_score&#39;,
 &#39;idxs&#39;,
 &#39;is_last_leaf&#39;,
 &#39;split&#39;,
 &#39;x&#39;,
 &#39;y&#39;]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What is the depth? How many observations are there?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Get info
print(&#39;Tree of depth %.0f with %.0f observations&#39; % (tree1.depth, len(tree1.idxs)))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Tree of depth 1 with 263 observations
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Fair enough, the tree is just a single leaf.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Check if terminal
tree1.is_last_leaf
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;True
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s find the first split.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Find best split
tree1.find_best_split()
print(&#39;Split at var%1.0f = %.4f&#39; % (tree1.split_col, tree1.split_val))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Split at var0 = 4.0000
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If has selected the first variable, at the value $4$.&lt;/p&gt;
&lt;p&gt;If we call the &lt;code&gt;split&lt;/code&gt; function, it will also tell us how many observations per leaf the split generates.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Split tree
tree1.split();
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Split on layer 1: var0 = 4.0000 (90/173)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we are ready to compute even deeper trees&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Check depth-3 tree
tree3 = Node(X, y, np.arange(len(y)), 3)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Split on layer 2: var1 = 4.0000 (2/88)
Split on layer 2: var1 = 117.0000 (90/83)
Split on layer 3: var0 = 4.0000 (90/173)
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;pruning&#34;&gt;Pruning&lt;/h3&gt;
&lt;p&gt;The process described above may produce good predictions on the training set, but is likely to &lt;strong&gt;overfit&lt;/strong&gt; the data, leading to poor test set performance. This is because the resulting tree might be too complex. A smaller tree with fewer splits might lead to lower variance and better interpretation at the cost of a little bias.&lt;/p&gt;
&lt;p&gt;We can see it happening if we build the same tree as above, but with 5 leaves.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Compute tree
overfit_tree = DecisionTreeRegressor(max_leaf_nodes=5).fit(X, y)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We plot the 5-leaf tree.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Plot tree
fig, ax = plt.subplots(1,1)
plot_tree(overfit_tree, filled=True, feature_names=features, fontsize=14, ax=ax);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/07_trees_45_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;split on the far left&lt;/strong&gt; is predicting a very high &lt;code&gt;Salary&lt;/code&gt; (7.243) for players with few &lt;code&gt;Years&lt;/code&gt; of experience and few &lt;code&gt;Hits&lt;/code&gt;. Indeed this prediction is based on an extremely tiny subsample (2). They are probably outliers and our tree is most likely overfitting.&lt;/p&gt;
&lt;p&gt;One possible alternative is to insert a minimum number of observation per leaf.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Compute tree
no_overfit_tree = DecisionTreeRegressor(max_leaf_nodes=5, min_samples_leaf=10).fit(X, y)

# Plot tree
fig, ax = plt.subplots(1,1)
plot_tree(no_overfit_tree, filled=True, feature_names=features, fontsize=14, ax=ax);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/07_trees_48_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Now the tree makes much more sense: the lower the &lt;code&gt;Years&lt;/code&gt; and the &lt;code&gt;Hits&lt;/code&gt;, the lower the predicted &lt;code&gt;Salary&lt;/code&gt; as we can see from the shades getting darker and darker as we move left to right&lt;/p&gt;
&lt;p&gt;Another possible alternative to the process described above is to build the tree only so long as the decrease in the RSS due to each split exceeds some (high) threshold.&lt;/p&gt;
&lt;p&gt;This strategy will result in smaller trees, but is too short-sighted since a seemingly worthless split early on in the tree might be followed by a very good split—that is, a split that leads to a large reduction in RSS later on.&lt;/p&gt;
&lt;p&gt;We can use cross-validation to pick the optimal tree length.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Import original split
features = [&#39;Years&#39;, &#39;Hits&#39;, &#39;RBI&#39;, &#39;PutOuts&#39;, &#39;Walks&#39;, &#39;Runs&#39;, &#39;AtBat&#39;, &#39;HmRun&#39;]
X_train = pd.read_csv(&#39;data/Hitters_X_train.csv&#39;).dropna()[features]
X_test = pd.read_csv(&#39;data/Hitters_X_test.csv&#39;).dropna()[features]
y_train = pd.read_csv(&#39;data/Hitters_y_train.csv&#39;).dropna()
y_test = pd.read_csv(&#39;data/Hitters_y_test.csv&#39;).dropna()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Init
params = range(2,11)
reg_scores = np.zeros((len(params),3))
best_score = 10**6

# Loop over all parameters
for i,k in enumerate(params):
    
    # Model
    tree = DecisionTreeRegressor(max_leaf_nodes=k)

    # Loop over splits
    tree.fit(X_train, y_train)
    reg_scores[i,0] = mean_squared_error(tree.predict(X_train), y_train)
    reg_scores[i,1] = mean_squared_error(tree.predict(X_test), y_test)

    # Get CV score
    kf6 = KFold(n_splits=6)
    reg_scores[i,2] = -cross_val_score(tree, X_train, y_train, cv=kf6, scoring=&#39;neg_mean_squared_error&#39;).mean()
    
    # Save best model
    if reg_scores[i,2]&amp;lt;best_score:
        best_model = tree
        best_score = reg_scores[i,2]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s plot the optimal tree depth, using 6-fold cv.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Figure 8.5
def make_figure_8_5():
    
    # Init
    fig, (ax1,ax2) = plt.subplots(1,2,figsize=(16,6))
    fig.suptitle(&#39;Figure 8.5&#39;)

    # Plot scores
    ax1.plot(params, reg_scores);
    ax1.axvline(params[np.argmin(reg_scores[:,2])], c=&#39;k&#39;, ls=&#39;--&#39;)
    ax1.legend([&#39;Train&#39;,&#39;Test&#39;,&#39;6-fold CV&#39;]);
    ax1.set_title(&#39;Cross-Validation Scores&#39;);

    # Plot best tree
    plot_tree(best_model, filled=True, impurity=False, feature_names=features, fontsize=12, ax=ax2);
    ax2.set_title(&#39;Best Model&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;make_figure_8_5()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/07_trees_55_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;The optimal tree has 4 leaves.&lt;/p&gt;
&lt;h3 id=&#34;classification-trees&#34;&gt;Classification Trees&lt;/h3&gt;
&lt;p&gt;A classification tree is very similar to a regression tree, except that it is used to predict a qualitative response rather than a quantitative one.&lt;/p&gt;
&lt;p&gt;For a classification tree, we predict that each observation belongs to the most commonly occurring class of training observations in the region to which it belongs.&lt;/p&gt;
&lt;h3 id=&#34;building-a-classification-tree&#34;&gt;Building a Classification Tree&lt;/h3&gt;
&lt;p&gt;The task of growing a classification tree is similar to the task of growing a regression tree. However, in the classification setting, RSS cannot be used as a criterion for making the binary splits.&lt;/p&gt;
&lt;p&gt;We define $\hat p_{mk}$ as the proportion of training observations in the $m^{th}$ region that are from the $k^{th}$ class. Possible loss functions to decide the splits are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Classification error rate&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$
E = 1 - \max &lt;em&gt;{k}\left(\hat{p}&lt;/em&gt;{m k}\right)
$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Gini index&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$
G=\sum_{k=1}^{K} \hat{p}&lt;em&gt;{m k}\left(1-\hat{p}&lt;/em&gt;{m k}\right)
$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Entropy&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$
D=-\sum_{k=1}^{K} \hat{p}&lt;em&gt;{m k} \log \hat{p}&lt;/em&gt;{m k}
$$&lt;/p&gt;
&lt;p&gt;In 2-class classification problems, this is what the different scores look like, for different proportions of class 2 ($p$), when the true proportion is $p_0 =0.5$.&lt;/p&gt;
&lt;img src=&#34;figures/impurity.png&#34; alt=&#34;Drawing&#34; style=&#34;width: 600px;&#34;/&gt;
&lt;p&gt;When building a classification tree, either the Gini index or the entropy are typically used to evaluate the quality of a particular split, since these two approaches are more sensitive to node purity than is the classification error rate.&lt;/p&gt;
&lt;p&gt;For this section we will work with the &lt;code&gt;Heart&lt;/code&gt; dataset on individual heart failures. We will try to use individual characteristics in order to predict heart deseases (&lt;code&gt;HD&lt;/code&gt;). The varaible is binary: Yes, No.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Load heart dataset
heart = pd.read_csv(&#39;data/Heart.csv&#39;).drop(&#39;Unnamed: 0&#39;, axis=1).dropna()
heart.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;Age&lt;/th&gt;
      &lt;th&gt;Sex&lt;/th&gt;
      &lt;th&gt;ChestPain&lt;/th&gt;
      &lt;th&gt;RestBP&lt;/th&gt;
      &lt;th&gt;Chol&lt;/th&gt;
      &lt;th&gt;Fbs&lt;/th&gt;
      &lt;th&gt;RestECG&lt;/th&gt;
      &lt;th&gt;MaxHR&lt;/th&gt;
      &lt;th&gt;ExAng&lt;/th&gt;
      &lt;th&gt;Oldpeak&lt;/th&gt;
      &lt;th&gt;Slope&lt;/th&gt;
      &lt;th&gt;Ca&lt;/th&gt;
      &lt;th&gt;Thal&lt;/th&gt;
      &lt;th&gt;AHD&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;63&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;typical&lt;/td&gt;
      &lt;td&gt;145&lt;/td&gt;
      &lt;td&gt;233&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;150&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;2.3&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;fixed&lt;/td&gt;
      &lt;td&gt;No&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;67&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;asymptomatic&lt;/td&gt;
      &lt;td&gt;160&lt;/td&gt;
      &lt;td&gt;286&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;108&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1.5&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;3.0&lt;/td&gt;
      &lt;td&gt;normal&lt;/td&gt;
      &lt;td&gt;Yes&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;67&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;asymptomatic&lt;/td&gt;
      &lt;td&gt;120&lt;/td&gt;
      &lt;td&gt;229&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;129&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;2.6&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;2.0&lt;/td&gt;
      &lt;td&gt;reversable&lt;/td&gt;
      &lt;td&gt;Yes&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;37&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;nonanginal&lt;/td&gt;
      &lt;td&gt;130&lt;/td&gt;
      &lt;td&gt;250&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;187&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;3.5&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;normal&lt;/td&gt;
      &lt;td&gt;No&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;41&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;nontypical&lt;/td&gt;
      &lt;td&gt;130&lt;/td&gt;
      &lt;td&gt;204&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;172&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1.4&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;normal&lt;/td&gt;
      &lt;td&gt;No&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Fastorize variables
heart.ChestPain = pd.factorize(heart.ChestPain)[0]
heart.Thal = pd.factorize(heart.Thal)[0]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Set features
features = [col for col in heart.columns if col!=&#39;AHD&#39;]
X2 = heart[features]
y2 = pd.factorize(heart.AHD)[0]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We now fit our classifier.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Fit classification tree
clf = DecisionTreeClassifier(max_depth=None, max_leaf_nodes=11)
clf.fit(X2,y2)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;DecisionTreeClassifier(max_leaf_nodes=11)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What is the score?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Final score
clf.score(X2,y2)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;0.8686868686868687
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s have a look at the whole tree.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Figure 8.6 a
def make_fig_8_6a():
    
    # Init
    fig, ax = plt.subplots(1,1, figsize=(16,12))
    ax.set_title(&#39;Figure 8.6&#39;);

    # Plot tree
    plot_tree(clf, filled=True, feature_names=features, class_names=[&#39;No&#39;,&#39;Yes&#39;], fontsize=12, ax=ax);
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;make_fig_8_6a()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/07_trees_72_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;This figure has a surprising characteristic: some of the splits yield two terminal nodes that have the same predicted value.&lt;/p&gt;
&lt;p&gt;For instance, consider the split &lt;code&gt;Age&lt;/code&gt;&amp;lt;=57.5 near the bottom left of the unpruned tree. Regardless of the value of &lt;code&gt;Age&lt;/code&gt;, a response value of &lt;em&gt;No&lt;/em&gt; is predicted for those observations. Why, then, is the split performed at all?&lt;/p&gt;
&lt;p&gt;The split is performed because it leads to increased node purity. That is, 2/81 of the observations corresponding to the left-hand leaf have a response value of &lt;em&gt;Yes&lt;/em&gt;, whereas 9/36 of those corresponding to the right-hand leaf have a response value of &lt;em&gt;Yes&lt;/em&gt;. Why is node purity important? Suppose that we have a test observation that belongs to the region given by that left-hand leaf. Then we can be pretty certain that its response value is &lt;em&gt;No&lt;/em&gt;. In contrast, if a test observation belongs to the region given by the right-hand leaf, then its response value is probably &lt;em&gt;No&lt;/em&gt;, but we are much less certain. Even though the split &lt;code&gt;Age&lt;/code&gt;&amp;lt;=57.5 does not reduce the classification error, it improves the Gini index and the entropy, which are more sensitive to node purity.&lt;/p&gt;
&lt;h3 id=&#34;pruning-for-classification&#34;&gt;Pruning for Classification&lt;/h3&gt;
&lt;p&gt;We can repeat the pruning exercise also for the classification task.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Figure 8.6 b
def make_figure_8_6b():
    
    # Init
    fig, (ax1, ax2) = plt.subplots(1,2,figsize=(14,6))
    fig.suptitle(&#39;Figure 8.6&#39;)

    # Plot scores
    ax1.plot(params, clf_scores);
    ax1.legend([&#39;Train&#39;,&#39;Test&#39;,&#39;6-fold CV&#39;]);

    # Plot best tree
    plot_tree(best_model, filled=True, impurity=False, feature_names=features, fontsize=12, ax=ax2);
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Init
J = 10
params = range(2,11)
clf_scores = np.zeros((len(params),3))
best_score = 100

# Loop over all parameters
for i,k in enumerate(params):
    
    # Model
    tree = DecisionTreeClassifier(max_leaf_nodes=k)
    
    # Loop J times
    temp_scores = np.zeros((J,3))
    for j in range (J):

        # Loop over splits
        X2_train, X2_test, y2_train, y2_test = train_test_split(X2, y2, test_size=0.5, random_state=j)
        m = tree.fit(X2_train, y2_train)
        temp_scores[j,0] = mean_squared_error(m.predict(X2_train), y2_train)
        temp_scores[j,1] = mean_squared_error(m.predict(X2_test), y2_test)

        # Get CV score
        kf6 = KFold(n_splits=6)
        temp_scores[j,2] = -cross_val_score(tree, X2_train, y2_train, cv=kf6, scoring=&#39;neg_mean_squared_error&#39;).mean()
        
        # Save best model
        if temp_scores[j,2]&amp;lt;best_score:
            best_model = m
            best_score = temp_scores[j,2]
        
    # Average
    clf_scores[i,:] = np.mean(temp_scores, axis=0)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;make_figure_8_6b()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/07_trees_78_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;other-issues&#34;&gt;Other Issues&lt;/h3&gt;
&lt;h4 id=&#34;missing-predictor-values&#34;&gt;Missing Predictor Values&lt;/h4&gt;
&lt;p&gt;There are usually 2 main ways to deal with missing values:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;discard the observations&lt;/li&gt;
&lt;li&gt;fill the missing values with predictions using the other observations (e.g. mean)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;With trees we can do better:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;code them as a separate class (e.g. &amp;lsquo;missing&amp;rsquo;)&lt;/li&gt;
&lt;li&gt;generate splits using non-missing data and use non-missing variables on missing data to mimic the splits with missing data&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;categorical-predictors&#34;&gt;Categorical Predictors&lt;/h4&gt;
&lt;p&gt;When splitting a predictor having q possible unordered values, there are $2^{q−1} − 1$ possible partitions of the q values into two groups, and the computations become prohibitive for large $q$. However, with a $0 − 1$ outcome, this computation simplifies.&lt;/p&gt;
&lt;h4 id=&#34;linear-combination-splits&#34;&gt;Linear Combination Splits&lt;/h4&gt;
&lt;p&gt;Rather than restricting splits to be of the form $X_j \leq s$, one can allow splits along linear combinations of the form $a_j X_j \leq s$. The weights $a_j$ become part of the optimization procedure.&lt;/p&gt;
&lt;h4 id=&#34;other-tree-building-procedures&#34;&gt;Other Tree-Building Procedures&lt;/h4&gt;
&lt;p&gt;The procedure we have seen for building trees is called CART (Classification and Regression Tree). There are other procedures.&lt;/p&gt;
&lt;h4 id=&#34;the-loss-matrix&#34;&gt;The Loss Matrix&lt;/h4&gt;
&lt;p&gt;With respect to other methods, the choice of the loss functions plays a much more important role.&lt;/p&gt;
&lt;h4 id=&#34;binary-splits&#34;&gt;Binary Splits&lt;/h4&gt;
&lt;p&gt;You can do non-binary splits but in the end they are just weaker versions of binary splits.&lt;/p&gt;
&lt;h4 id=&#34;instability&#34;&gt;Instability&lt;/h4&gt;
&lt;p&gt;Trees have very &lt;strong&gt;high variance&lt;/strong&gt;.&lt;/p&gt;
&lt;h4 id=&#34;difficulty-in-capturing-additive-structure&#34;&gt;Difficulty in Capturing Additive Structure&lt;/h4&gt;
&lt;p&gt;Trees are quite bad at modeling additive structures.&lt;/p&gt;
&lt;h4 id=&#34;lack-of-smoothness&#34;&gt;Lack of Smoothness&lt;/h4&gt;
&lt;p&gt;Trees are not smooth.&lt;/p&gt;
&lt;h3 id=&#34;trees-vs-regression&#34;&gt;Trees vs Regression&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Advantages&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Trees are very easy to explain to people. In fact, they are even easier to explain than linear regression!&lt;/li&gt;
&lt;li&gt;Some people believe that decision trees more closely mirror human decision-making than do the regression and classification approaches seen in previous chapters.&lt;/li&gt;
&lt;li&gt;Trees can be displayed graphically, and are easily interpreted even by a non-expert (especially if they are small).&lt;/li&gt;
&lt;li&gt;Trees can easily handle qualitative predictors without the need to create dummy variables.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Disadvantages&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;trees generally do not have the same level of predictive accuracy as some of the other regression and classification approaches seen in this book.&lt;/li&gt;
&lt;li&gt;trees can be very non-robust. In other words, a small change in the data can cause a large change in the final estimated tree.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;72-bagging-random-forests-boosting&#34;&gt;7.2 Bagging, Random Forests, Boosting&lt;/h2&gt;
&lt;p&gt;Bagging, random forests, and boosting use trees as building blocks to construct more powerful prediction models.&lt;/p&gt;
&lt;h3 id=&#34;bagging&#34;&gt;Bagging&lt;/h3&gt;
&lt;p&gt;The main problem of decision trees is that they suffer from &lt;strong&gt;high variance&lt;/strong&gt;. &lt;em&gt;Bootstrap aggregation&lt;/em&gt;, or &lt;em&gt;bagging&lt;/em&gt;, is a general-purpose procedure for reducing the variance of a statistical learning method.&lt;/p&gt;
&lt;p&gt;The main idea behind &lt;em&gt;bagging&lt;/em&gt; is that, given a set of n independent observations $Z_1,&amp;hellip;,Z_n$, each with variance $\sigma^2$, the variance of the mean $\bar Z$ of the observations is given by $\sigma^2/n$. In other words, averaging a set of observations reduces variance.&lt;/p&gt;
&lt;p&gt;Indeed &lt;em&gt;bagging&lt;/em&gt; consists in taking many training sets from the population, build a separate prediction model using each training set, and &lt;strong&gt;average the resulting predictions&lt;/strong&gt;. Since we do not have access to many training sets, we resort to bootstrapping.&lt;/p&gt;
&lt;h3 id=&#34;out-of-bag-error-estimation&#34;&gt;Out-of-Bag Error Estimation&lt;/h3&gt;
&lt;p&gt;It turns out that there is a very straightforward way to estimate the test error of a bagged model, without the need to perform cross-validation or the validation set approach. Recall that the key to bagging is that trees are repeatedly fit to bootstrapped subsets of the observations. One can show that on average, each bagged tree makes use of around two-thirds of the observations. The remaining one-third of the observations not used to fit a given bagged tree are referred to as the out-of-bag (OOB) observations. We can predict the response for the ith observation using each of the trees in which that observation was OOB.&lt;/p&gt;
&lt;p&gt;We are now going to compute the Gini index for the &lt;code&gt;Heart&lt;/code&gt; dataset using different numbers of trees.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Init (takes a lot of time with J=30)
params = range(2,50)
bagging_scores = np.zeros((len(params),2))
J = 30;

# Loop over parameters
for i, k in enumerate(params):
    print(&amp;quot;Computing k=%1.0f&amp;quot; % k, end =&amp;quot;&amp;quot;)
    
    # Repeat J 
    temp_scores = np.zeros((J,2))
    for j in range(J):
        X2_train, X2_test, y2_train, y2_test = train_test_split(X2, y2, test_size=0.5, random_state=j)
        bagging = BaggingClassifier(DecisionTreeClassifier(), max_samples=k, oob_score=True)
        bagging.fit(X2_train,y2_train)
        temp_scores[j,0] = bagging.score(X2_test, y2_test)
        temp_scores[j,1] = bagging.oob_score_
        
    # Average
    bagging_scores[i,:] = np.mean(temp_scores, axis=0)
    print(&amp;quot;&amp;quot;, end=&amp;quot;\r&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Computing k=49
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s plot the Out-of-Bag error computed while generating the bagged estimator.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Make new figure 1
def make_new_figure_1():
    
    # Init
    fig, ax = plt.subplots(1,1,figsize=(10,6))
    fig.suptitle(&amp;quot;Estimated $R^2$&amp;quot;)

    # Plot scores
    ax.plot(params, bagging_scores);
    ax.legend([&#39;Test&#39;,&#39;OOB&#39;]);
    ax.set_xlabel(&#39;Number of Trees&#39;); ax.set_ylabel(&#39;R^2&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;make_new_figure_1()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/07_trees_94_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;It can be shown that with B sufficiently large, OOB error is virtually equivalent to leave-one-out cross-validation error. The OOB approach for estimating the test error is particularly convenient when performing bagging on large data sets for which cross-validation would be computationally onerous.&lt;/p&gt;
&lt;h3 id=&#34;variable-importance-measures&#34;&gt;Variable Importance Measures&lt;/h3&gt;
&lt;p&gt;As we have discussed, the main advantage of bagging is to reduce prediction variance. However, with bagging it can be &lt;strong&gt;difficult to interpret&lt;/strong&gt; the resulting model. In fact we cannot draw trees anymore given we have too many of them.&lt;/p&gt;
&lt;p&gt;However, one can obtain an overall summary of the importance of each predictor using the RSS (for bagging regression trees) or the Gini index (for bagging classification trees). In the case of bagging regression trees, we can record the total amount that the RSS is decreased due to splits over a given predictor, averaged over all trees. A large value indicates an important predictor. Similarly, in the context of bagging classification trees, we can add up the total amount that the Gini index is decreased by splits over a given predictor, averaged over all trees.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Compute feature importance
feature_importances = np.mean([tree.feature_importances_ for tree in bagging.estimators_], axis=0)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can have a look at the importance of each feature.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Figure 8.9
def make_figure_8_9():
    
    # Init
    fig, ax = plt.subplots(1,1,figsize=(8,8))
    ax.set_title(&#39;Figure 8.9: Feature Importance&#39;);

    # Plot feature importance
    h1 = pd.DataFrame({&#39;Importance&#39;:feature_importances*100}, index=features)
    h1 = h1.sort_values(by=&#39;Importance&#39;, axis=0, ascending=False)
    h1.plot(kind=&#39;barh&#39;, color=&#39;r&#39;, ax=ax)
    ax.set_xlabel(&#39;Variable Importance&#39;); 
    plt.yticks(fontsize=14);
    plt.gca().legend_ = None;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;make_figure_8_9()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/07_trees_101_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;random-forests&#34;&gt;Random Forests&lt;/h3&gt;
&lt;p&gt;Random forests provide an improvement over bagged trees by way of a &lt;strong&gt;small tweak that decorrelates the trees&lt;/strong&gt;. As in bagging, we build a number of decision trees on bootstrapped training samples. But when building these decision trees, each time a split in a tree is considered, a random sample of $m$ predictors is chosen as split candidates from the full set of $p$ predictors. The split is allowed to use only one of those m predictors. A fresh sample of $m$ predictors is taken at each split, and typically we choose $m \sim \sqrt{p}$ — that is, the number of predictors considered at each split is approximately equal to the square root of the total number of predictors&lt;/p&gt;
&lt;p&gt;In other words, in building a random forest, at each split in the tree, the algorithm is not even allowed to consider a majority of the available predictors. This may sound crazy, but it has a clever rationale. Suppose that there is one very strong predictor in the data set, along with a number of other moderately strong predictors. Then in the collection of bagged trees, most or all of the trees will use this strong predictor in the top split. Consequently, all of the bagged trees will look quite similar to each other. Hence the predictions from the bagged trees will be highly correlated. Unfortunately, averaging many highly correlated quantities does not lead to as large of a reduction in variance as averaging many uncorrelated quantities. In particular, this means that bagging will not lead to a substantial reduction in variance over a single tree in this setting.&lt;/p&gt;
&lt;p&gt;Random forests overcome this problem by &lt;strong&gt;forcing each split to consider only a subset of the predictors&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s split the data in 2 and compute test and estimated $R^2$, for both forest and trees.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import warnings
warnings.simplefilter(&#39;ignore&#39;)

# Init (takes a lot of time with J=30)
params = range(2,50)
forest_scores = np.zeros((len(params),2))
J = 30

# Loop over parameters
for i, k in enumerate(params):
    print(&amp;quot;Computing k=%1.0f&amp;quot; % k, end =&amp;quot;&amp;quot;)
    
    # Repeat J 
    temp_scores = np.zeros((J,2))
    for j in range(J):
        X2_train, X2_test, y2_train, y2_test = train_test_split(X2, y2, test_size=0.5, random_state=j)
        forest = RandomForestClassifier(n_estimators=k, oob_score=True, max_features=&amp;quot;sqrt&amp;quot;)
        forest.fit(X2_train,y2_train)
        temp_scores[j,0] = forest.score(X2_test, y2_test)
        temp_scores[j,1] = forest.oob_score_
        
    # Average
    forest_scores[i,:] = np.mean(temp_scores, axis=0)
    print(&amp;quot;&amp;quot;, end=&amp;quot;\r&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Computing k=49
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Figure 8.8
def make_figure_8_8():
    
    # Init
    fig, ax = plt.subplots(1,1,figsize=(10,6))
    ax.set_title(&#39;Figure 8.8&#39;);

    # Plot scores
    ax.plot(params, bagging_scores);
    ax.plot(params, forest_scores);
    ax.legend([&#39;Test - Bagging&#39;,&#39;OOB - Bagging&#39;, &#39;Test - Forest&#39;,&#39;OOB - Forest&#39;]);
    ax.set_xlabel(&#39;Number of Trees&#39;); ax.set_ylabel(&#39;R^2&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;make_figure_8_8()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/07_trees_108_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;As for bagging, we can plot feature importance.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Make new figure 2
def make_new_figure_2():
    
    # Init
    fig, (ax1,ax2) = plt.subplots(1,2,figsize=(15,7))

    # Plot feature importance - Bagging
    h1 = pd.DataFrame({&#39;Importance&#39;:feature_importances*100}, index=features)
    h1 = h1.sort_values(by=&#39;Importance&#39;, axis=0, ascending=False)
    h1.plot(kind=&#39;barh&#39;, color=&#39;r&#39;, ax=ax1)
    ax1.set_xlabel(&#39;Variable Importance&#39;); 
    ax1.set_title(&#39;Tree Bagging&#39;)

    # Plot feature importance
    h2 = pd.DataFrame({&#39;Importance&#39;:forest.feature_importances_*100}, index=features)
    h2 = h2.sort_values(by=&#39;Importance&#39;, axis=0, ascending=False)
    h2.plot(kind=&#39;barh&#39;, color=&#39;r&#39;, ax=ax2)
    ax2.set_title(&#39;Random Forest&#39;)

    # All plots
    for ax in fig.axes:
        ax.set_xlabel(&#39;Variable Importance&#39;); 
        ax.legend([])
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;make_new_figure_2()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/07_trees_111_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;From the figure we observe that varaible importance ranking is similar with bagging and random forests, but there are significant differences.&lt;/p&gt;
&lt;p&gt;We are now going to look at the importance of random forests using the &lt;code&gt;Khan&lt;/code&gt; gene dataset. This dataset has the peculiarity of having a large number of features and very few observations.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Load data
gene = pd.read_csv(&#39;data/Khan.csv&#39;)
print(len(gene))
gene.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;83
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;x&lt;/th&gt;
      &lt;th&gt;V1&lt;/th&gt;
      &lt;th&gt;V2&lt;/th&gt;
      &lt;th&gt;V3&lt;/th&gt;
      &lt;th&gt;V4&lt;/th&gt;
      &lt;th&gt;V5&lt;/th&gt;
      &lt;th&gt;V6&lt;/th&gt;
      &lt;th&gt;V7&lt;/th&gt;
      &lt;th&gt;V8&lt;/th&gt;
      &lt;th&gt;V9&lt;/th&gt;
      &lt;th&gt;...&lt;/th&gt;
      &lt;th&gt;V2299&lt;/th&gt;
      &lt;th&gt;V2300&lt;/th&gt;
      &lt;th&gt;V2301&lt;/th&gt;
      &lt;th&gt;V2302&lt;/th&gt;
      &lt;th&gt;V2303&lt;/th&gt;
      &lt;th&gt;V2304&lt;/th&gt;
      &lt;th&gt;V2305&lt;/th&gt;
      &lt;th&gt;V2306&lt;/th&gt;
      &lt;th&gt;V2307&lt;/th&gt;
      &lt;th&gt;V2308&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;0.773344&lt;/td&gt;
      &lt;td&gt;-2.438405&lt;/td&gt;
      &lt;td&gt;-0.482562&lt;/td&gt;
      &lt;td&gt;-2.721135&lt;/td&gt;
      &lt;td&gt;-1.217058&lt;/td&gt;
      &lt;td&gt;0.827809&lt;/td&gt;
      &lt;td&gt;1.342604&lt;/td&gt;
      &lt;td&gt;0.057042&lt;/td&gt;
      &lt;td&gt;0.133569&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;-0.238511&lt;/td&gt;
      &lt;td&gt;-0.027474&lt;/td&gt;
      &lt;td&gt;-1.660205&lt;/td&gt;
      &lt;td&gt;0.588231&lt;/td&gt;
      &lt;td&gt;-0.463624&lt;/td&gt;
      &lt;td&gt;-3.952845&lt;/td&gt;
      &lt;td&gt;-5.496768&lt;/td&gt;
      &lt;td&gt;-1.414282&lt;/td&gt;
      &lt;td&gt;-0.647600&lt;/td&gt;
      &lt;td&gt;-1.763172&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;-0.078178&lt;/td&gt;
      &lt;td&gt;-2.415754&lt;/td&gt;
      &lt;td&gt;0.412772&lt;/td&gt;
      &lt;td&gt;-2.825146&lt;/td&gt;
      &lt;td&gt;-0.626236&lt;/td&gt;
      &lt;td&gt;0.054488&lt;/td&gt;
      &lt;td&gt;1.429498&lt;/td&gt;
      &lt;td&gt;-0.120249&lt;/td&gt;
      &lt;td&gt;0.456792&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;-0.657394&lt;/td&gt;
      &lt;td&gt;-0.246284&lt;/td&gt;
      &lt;td&gt;-0.836325&lt;/td&gt;
      &lt;td&gt;-0.571284&lt;/td&gt;
      &lt;td&gt;0.034788&lt;/td&gt;
      &lt;td&gt;-2.478130&lt;/td&gt;
      &lt;td&gt;-3.661264&lt;/td&gt;
      &lt;td&gt;-1.093923&lt;/td&gt;
      &lt;td&gt;-1.209320&lt;/td&gt;
      &lt;td&gt;-0.824395&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;-0.084469&lt;/td&gt;
      &lt;td&gt;-1.649739&lt;/td&gt;
      &lt;td&gt;-0.241308&lt;/td&gt;
      &lt;td&gt;-2.875286&lt;/td&gt;
      &lt;td&gt;-0.889405&lt;/td&gt;
      &lt;td&gt;-0.027474&lt;/td&gt;
      &lt;td&gt;1.159300&lt;/td&gt;
      &lt;td&gt;0.015676&lt;/td&gt;
      &lt;td&gt;0.191942&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;-0.696352&lt;/td&gt;
      &lt;td&gt;0.024985&lt;/td&gt;
      &lt;td&gt;-1.059872&lt;/td&gt;
      &lt;td&gt;-0.403767&lt;/td&gt;
      &lt;td&gt;-0.678653&lt;/td&gt;
      &lt;td&gt;-2.939352&lt;/td&gt;
      &lt;td&gt;-2.736450&lt;/td&gt;
      &lt;td&gt;-1.965399&lt;/td&gt;
      &lt;td&gt;-0.805868&lt;/td&gt;
      &lt;td&gt;-1.139434&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;0.965614&lt;/td&gt;
      &lt;td&gt;-2.380547&lt;/td&gt;
      &lt;td&gt;0.625297&lt;/td&gt;
      &lt;td&gt;-1.741256&lt;/td&gt;
      &lt;td&gt;-0.845366&lt;/td&gt;
      &lt;td&gt;0.949687&lt;/td&gt;
      &lt;td&gt;1.093801&lt;/td&gt;
      &lt;td&gt;0.819736&lt;/td&gt;
      &lt;td&gt;-0.284620&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;0.259746&lt;/td&gt;
      &lt;td&gt;0.357115&lt;/td&gt;
      &lt;td&gt;-1.893128&lt;/td&gt;
      &lt;td&gt;0.255107&lt;/td&gt;
      &lt;td&gt;0.163309&lt;/td&gt;
      &lt;td&gt;-1.021929&lt;/td&gt;
      &lt;td&gt;-2.077843&lt;/td&gt;
      &lt;td&gt;-1.127629&lt;/td&gt;
      &lt;td&gt;0.331531&lt;/td&gt;
      &lt;td&gt;-2.179483&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;0.075664&lt;/td&gt;
      &lt;td&gt;-1.728785&lt;/td&gt;
      &lt;td&gt;0.852626&lt;/td&gt;
      &lt;td&gt;0.272695&lt;/td&gt;
      &lt;td&gt;-1.841370&lt;/td&gt;
      &lt;td&gt;0.327936&lt;/td&gt;
      &lt;td&gt;1.251219&lt;/td&gt;
      &lt;td&gt;0.771450&lt;/td&gt;
      &lt;td&gt;0.030917&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;-0.200404&lt;/td&gt;
      &lt;td&gt;0.061753&lt;/td&gt;
      &lt;td&gt;-2.273998&lt;/td&gt;
      &lt;td&gt;-0.039365&lt;/td&gt;
      &lt;td&gt;0.368801&lt;/td&gt;
      &lt;td&gt;-2.566551&lt;/td&gt;
      &lt;td&gt;-1.675044&lt;/td&gt;
      &lt;td&gt;-1.082050&lt;/td&gt;
      &lt;td&gt;-0.965218&lt;/td&gt;
      &lt;td&gt;-1.836966&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;5 rows × 2309 columns&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;The dataset has 83 rows and 2309 columns.&lt;/p&gt;
&lt;p&gt;Since it&amp;rsquo;s a very &lt;em&gt;wide&lt;/em&gt; dataset, selecting the right features is crucial.&lt;/p&gt;
&lt;p&gt;Also note that we cannot run linear regression on this dataset.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Reduce dataset size
gene_small = gene.iloc[:,0:202]
X = gene_small.iloc[:,1:]
y = gene_small.iloc[:,0]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s now cross-validate over number of trees and maximum number of features considered.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Init (takes a lot of time with J=30)
params = range(50,150,10)
m_scores = np.zeros((len(params),3))
p = np.shape(X)[1]
J = 30;

# Loop over parameters
for i, k in enumerate(params):
    
    # Array of features
    ms = [round(p/2), round(np.sqrt(p)), round(np.log(p))]
    
    # Repeat L times
    temp_scores = np.zeros((J,3))
    for j in range(J):
        print(&amp;quot;Computing k=%1.0f (iter=%1.0f)&amp;quot; % (k,j+1), end =&amp;quot;&amp;quot;)
    
        # Loop over values of m
        for index, m in enumerate(ms):
            forest = RandomForestClassifier(n_estimators=k, max_features=m, oob_score=True)
            forest.fit(X, y)
            temp_scores[j,index] = forest.oob_score_
        print(&amp;quot;&amp;quot;, end=&amp;quot;\r&amp;quot;)
            
    # Average
    m_scores[i,:] = np.mean(temp_scores, axis=0)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Computing k=140 (iter=30)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Figure 8.10
def make_figure_8_10():
    
    # Init
    fig, ax = plt.subplots(1,1,figsize=(10,6))
    ax.set_title(&#39;Figure 8.10&#39;);

    # Plot scores
    ax.plot(params, m_scores);
    ax.legend([&#39;m=p/2&#39;,&#39;m=sqrt(p)&#39;,&#39;m=log(p)&#39;]);
    ax.set_xlabel(&#39;Number of Trees&#39;); ax.set_ylabel(&#39;Test Classification Accuracy&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt; make_figure_8_10()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/07_trees_120_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;It seems that the best scores are achieved with few features and many trees.&lt;/p&gt;
&lt;h3 id=&#34;boosting&#34;&gt;Boosting&lt;/h3&gt;
&lt;p&gt;Like bagging, boosting is a general approach that can be applied to many statistical learning methods for regression or classification. Here we restrict our discussion of boosting to the context of decision trees.&lt;/p&gt;
&lt;p&gt;Boosting works similarly to bagging, except that the trees are grown sequentially: each tree is grown using information from previously grown trees. Boosting does not involve bootstrap sampling; instead each tree is fit on a modified version of the original data set.&lt;/p&gt;
&lt;p&gt;What is the idea behind this procedure? Given the current model, we fit a decision tree to the residuals from the model. That is, &lt;strong&gt;we fit a tree using the current residuals&lt;/strong&gt;, rather than the outcome $y$, as the response. We then add this new decision tree into the fitted function in order to update the residuals. Each of these trees can be rather small, with just a few terminal nodes, determined by the parameter $d$ in the algorithm. By fitting small trees to the residuals, &lt;strong&gt;we slowly improve $\hat f$ in areas where it does not perform well&lt;/strong&gt;. The shrinkage parameter λ slows the process down even further, allowing more and different shaped trees to attack the resid- uals. In general, statistical learning approaches that learn slowly tend to perform well.&lt;/p&gt;
&lt;h3 id=&#34;algorithm&#34;&gt;Algorithm&lt;/h3&gt;
&lt;p&gt;The boosting algorithm works as follows:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Set $\hat f(x)=0$ and $r_i=y_i$ for all $i$ in the training set.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;For $b=1,2,&amp;hellip;,B$ repeat:&lt;/p&gt;
&lt;p&gt;a. Fit a tree $\hat f^b $ with $d$ splits ($d+1$ terminal nodes) to the training data $(X,r)$.&lt;/p&gt;
&lt;p&gt;b. Update $\hat f$ by adding in a shrunken version of the new tree:
$$
\hat f(x) \leftarrow \hat f(x) + \lambda \hat f^b(x)
$$&lt;/p&gt;
&lt;p&gt;c. Update the residuals
$$
r_i = r_i - \lambda \hat f^b(x_i)
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Output the boosted model
$$
\hat{f}(x)=\sum_{b=1}^{B} \lambda \hat{f}^{b}(x)
$$&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Boosting has three tuning parameters:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;The &lt;strong&gt;number of trees&lt;/strong&gt; $B$&lt;/li&gt;
&lt;li&gt;The &lt;strong&gt;shrinkage parameter&lt;/strong&gt; $\lambda$. This controls the rate at which boosting learns.&lt;/li&gt;
&lt;li&gt;The &lt;strong&gt;number of splits in each tree&lt;/strong&gt; $d$ , which controls the complexity of the boosted ensemble. Often d = 1 works well, in which case each tree is a stump, consisting of a single split.&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Init , oob_score=True
params = range(50,150,10)
boost_scores = np.zeros((len(params),3))
p = np.shape(X)[1]
J = 30

# Loop over parameters
for i, k in enumerate(params):
    
    # Repeat L times
    temp_scores = np.zeros((J,3))
    for j in range(J):
        print(&amp;quot;Computing k=%1.0f (iter=%1.0f)&amp;quot; % (k,j+1), end =&amp;quot;&amp;quot;)
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=20, random_state=j)
    
        # First score: random forest
        forest = RandomForestClassifier(n_estimators=k, max_features=&amp;quot;sqrt&amp;quot;)
        forest.fit(X_train, y_train)
        temp_scores[j,0] = forest.score(X_test, y_test)

        # Second score: boosting with 1-split trees
        boost1 = GradientBoostingClassifier(learning_rate=0.01, max_depth=1, n_estimators=k, max_features=&amp;quot;sqrt&amp;quot;)
        boost1.fit(X_train, y_train)
        temp_scores[j,1] = boost1.score(X_test, y_test)

        # Third score: boosting with 1-split trees
        boost2 = GradientBoostingClassifier(learning_rate=0.01, max_depth=2, n_estimators=k, max_features=&amp;quot;sqrt&amp;quot;)
        boost2.fit(X_train, y_train)
        temp_scores[j,2] = boost2.score(X_test, y_test)
        print(&amp;quot;&amp;quot;, end=&amp;quot;\r&amp;quot;)
    
    # Average
    boost_scores[i,:] = np.mean(temp_scores, axis=0)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Computing k=140 (iter=30)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s compare boosting and forest.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Figure 8.11
def make_figure_8_11():
    
    # Init
    fig, ax = plt.subplots(1,1,figsize=(10,6))
    ax.set_title(&#39;Figure 8.11&#39;);

    # Plot scores
    ax.plot(params, m_scores);
    ax.legend([&#39;forest&#39;,&#39;boosting with d=1&#39;,&#39;boosting with d=2&#39;]);
    ax.set_xlabel(&#39;Number of Trees&#39;); ax.set_ylabel(&#39;Test Classification Accuracy&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;make_figure_8_11()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/07_trees_131_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Neural Networks</title>
      <link>https://matteocourthoud.github.io/course/ml-econ/08_neuralnets/</link>
      <pubDate>Wed, 09 Mar 2022 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/course/ml-econ/08_neuralnets/</guid>
      <description>&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Remove warnings
import warnings
warnings.filterwarnings(&#39;ignore&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Import everything
import pandas as pd
import numpy as np
import seaborn as sns
import statsmodels.api as sm

import copy
import torch 
import torch.nn as nn
import torch.utils.data as Data
from torch.autograd import Variable
from sklearn.linear_model import LinearRegression
from torchviz import make_dot
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Import matplotlib for graphs
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import axes3d
from IPython.display import clear_output

# Set global parameters
%matplotlib inline
plt.style.use(&#39;seaborn-white&#39;)
plt.rcParams[&#39;lines.linewidth&#39;] = 3
plt.rcParams[&#39;figure.figsize&#39;] = (10,6)
plt.rcParams[&#39;figure.titlesize&#39;] = 20
plt.rcParams[&#39;axes.titlesize&#39;] = 18
plt.rcParams[&#39;axes.labelsize&#39;] = 14
plt.rcParams[&#39;legend.fontsize&#39;] = 14
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;While &lt;code&gt;sklearn&lt;/code&gt; has a library for neural networks, it is very basic and not the standard in the industry. The most commonly used libraries as of 2020 are &lt;strong&gt;Tensorflow&lt;/strong&gt; and &lt;strong&gt;Pytorch&lt;/strong&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;TensorFlow is developed by Google Brain and actively used at Google both for research and production needs. Its closed-source predecessor is called DistBelief.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;PyTorch is a cousin of lua-based Torch framework which was developed and used at Facebook. However, PyTorch is not a simple set of wrappers to support popular language, it was rewritten and tailored to be fast and feel native.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Here is an article that explains very well the difference between the two libraries: &lt;a href=&#34;https://towardsdatascience.com/pytorch-vs-tensorflow-spotting-the-difference-25c75777377b&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;pytorch-vs-tensorflow&lt;/a&gt;. In short, pytorch is much more intuitive for a python programmer and more user friendly. It also has a superior development and debugging experience. However, if you want more control on the fundamentals, a better community support and you need to train large models, Tensorflow is better.&lt;/p&gt;
&lt;h2 id=&#34;81-introduction&#34;&gt;8.1 Introduction&lt;/h2&gt;
&lt;p&gt;The term neural network has evolved to encompass a large class of models and learning methods. Here I describe the most widely used “vanilla” neural net, sometimes called the single hidden layer back-propagation network, or single layer perceptron.&lt;/p&gt;
&lt;h3 id=&#34;regression&#34;&gt;Regression&lt;/h3&gt;
&lt;p&gt;Imagine a setting with two &lt;strong&gt;inputs&lt;/strong&gt; available (let’s denote these inputs $i_1$ and $i_2$), and no special knowledge about the relationship between these inputs and the &lt;strong&gt;output&lt;/strong&gt; that we want to predict (denoted by $o$) except that this relationship is, a priori, pretty complex and non-linear.&lt;/p&gt;
&lt;p&gt;So we want to learn the function $f$ such that f($i_1$, $i_2$) is a good estimator of $o$. We could then suggest the following first model:&lt;/p&gt;
&lt;p&gt;$$
o = w_{11} i_1 + w_{12} i_2
$$&lt;/p&gt;
&lt;p&gt;where $w_{11}$ and $w_{12}$ are just weights/coefficients (do not take care about the indices for now). Before going any further, we should notice that, here, there is no constant term in the model. However, we could have introduced such term by setting $f(i_1, i_2) = w_{11} i_1 + w_{12} i_2 + c$. The constant is often called &lt;strong&gt;bias&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;We can represent the setting as follows.&lt;/p&gt;
&lt;img src=&#34;../figures/nn1.jpeg&#34; alt=&#34;Drawing&#34; style=&#34;width: 600px;&#34;/&gt;
&lt;p&gt;In this case, the model is easy to understand and to fit but has a big drawback : there is no non-linearity! This obviously do not respect our non-linear assumption.&lt;/p&gt;
&lt;h3 id=&#34;activation-functions&#34;&gt;Activation Functions&lt;/h3&gt;
&lt;p&gt;In order to introduce a non-linearity, let us make a little modification in the previous model and suggest the following one.&lt;/p&gt;
&lt;p&gt;$$
o = a ( w_{11} i_1 + w_{12} i_2)
$$&lt;/p&gt;
&lt;p&gt;where $a$ is a function called &lt;strong&gt;activation function&lt;/strong&gt; which is non-linear.&lt;/p&gt;
&lt;img src=&#34;../figures/nn2.jpeg&#34; alt=&#34;Drawing&#34; style=&#34;width: 600px;&#34;/&gt;
&lt;p&gt;One activation function that is well known in economics (and other disciplines) is the &lt;em&gt;sigmoid&lt;/em&gt; function or logit function&lt;/p&gt;
&lt;p&gt;$$
a (w_{11} i_1 + w_{12} i_2) = \frac{1}{1 + e^{w_{11} i_1 + w_{12} i_2}}
$$&lt;/p&gt;
&lt;h3 id=&#34;layers&#34;&gt;Layers&lt;/h3&gt;
&lt;p&gt;However, even if better than multilinear model, this model is still too simple and can’t handle the assumed underlying complexity of the relationship between inputs and output. We can make a step further and enrich the model the following way.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;First we could consider that the quantity $a ( w_{11} i_1 + w_{12} i_2)$ is no longer the final output but instead a new intermediate feature of our function, called $l_1$, which stands for &lt;strong&gt;layer&lt;/strong&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;$$
l_1 = a ( w_{11} i_1 + w_{12} i_2)
$$&lt;/p&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;Second we could consider that we build several (3 in our example) such features in the same way, but possibly with different weights and different activation functions&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;$$
l_1 = a ( w_{11} i_1 + w_{12} i_2) \
l_2 = a ( w_{21} i_1 + w_{22} i_2) \
l_3 = a ( w_{31} i_1 + w_{32} i_2)
$$&lt;/p&gt;
&lt;p&gt;where the $a$’s are just activation functions and the $w$’s are weights.&lt;/p&gt;
&lt;ol start=&#34;3&#34;&gt;
&lt;li&gt;Finally, we can consider that our final output is build based on these intermediate features with the same “template”&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;$$
a_2 ( v_1 l_1 + v_2 l_2 + v_3 * l_3 )
$$&lt;/p&gt;
&lt;p&gt;If we aggregate all the pieces, we then get our &lt;strong&gt;prediction&lt;/strong&gt; $p$&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
p = f_{3}\left(i_{1}, i_{2}\right) &amp;amp;=a_{2}\left(v_{1} l_{1}+v_{2} l_{2}+v_{3} l_{3}\right) \
&amp;amp;=a_{2}\left(v_{1} \times a_{11}\left(w_{11} i_{1}+w_{12} i_{2}\right)+v_{2} \times a_{12}\left(w_{21} i_{1}+w_{22} i_{2}\right)+v_{3} \times a_{13}\left(w_{31} i_{1}+w_{32} i_{2}\right)\right)
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;where we should mainly keep in mind that $a$’s are non-linear activation functions and $w$’s and $v$’s are weights.&lt;/p&gt;
&lt;p&gt;Graphically:&lt;/p&gt;
&lt;img src=&#34;../figures/nn3.jpeg&#34; alt=&#34;Drawing&#34; style=&#34;width: 900px;&#34;/&gt;
&lt;p&gt;This last model is a basic feedforward neural network with:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;2 entries ($i_1$ and $i_2$)&lt;/li&gt;
&lt;li&gt;1 hidden layer with 3 hidden neurones (whose outputs are $l_1$, $l_2$ and $l_3$)&lt;/li&gt;
&lt;li&gt;1 final output ($p$)&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;pytorch&#34;&gt;Pytorch&lt;/h2&gt;
&lt;h3 id=&#34;tensors&#34;&gt;Tensors&lt;/h3&gt;
&lt;p&gt;We can express the data as a &lt;code&gt;numpy&lt;/code&gt; array.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;x_np = np.arange(6).reshape((3, 2))
x_np
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;array([[0, 1],
       [2, 3],
       [4, 5]])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Or equivalently as a &lt;code&gt;pytorch&lt;/code&gt; tensor.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;x_tensor = torch.from_numpy(x_np)
x_tensor
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;tensor([[0, 1],
        [2, 3],
        [4, 5]])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can also translate tensors back to arrays.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;tensor2array = x_tensor.numpy()
tensor2array
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;array([[0, 1],
       [2, 3],
       [4, 5]])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can make operations over this data. For example we can take the mean&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;try:
    torch.mean(x_tensor)
except Exception as e:
    print(e)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;mean(): input dtype should be either floating point or complex dtypes. Got Long instead.
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We first have to convert the data in float&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;x_tensor = torch.FloatTensor(x_np)
x_tensor
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;tensor([[0., 1.],
        [2., 3.],
        [4., 5.]])
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;print(np.mean(x_np), &#39;\n\n&#39;, torch.mean(x_tensor))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;2.5 

 tensor(2.5000)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can also apply compontent-wise functions&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;print(np.sin(x_np), &#39;\n\n&#39;, torch.sin(x_tensor))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[[ 0.          0.84147098]
 [ 0.90929743  0.14112001]
 [-0.7568025  -0.95892427]] 

 tensor([[ 0.0000,  0.8415],
        [ 0.9093,  0.1411],
        [-0.7568, -0.9589]])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can multiply tensors as we multiply matrices&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;print(np.matmul(x_np.T, x_np), &#39;\n\n&#39;, torch.mm(x_tensor.T, x_tensor))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[[20 26]
 [26 35]] 

 tensor([[20., 26.],
        [26., 35.]])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;But the element-wise multiplication does not work&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;try:
    x_tensor.dot(x_tensor)
except Exception as e:
    print(e)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;1D tensors expected, but got 2D and 2D tensors
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;variables&#34;&gt;Variables&lt;/h3&gt;
&lt;p&gt;Variable in torch is to build a computational graph, but this graph is dynamic compared with a static graph in Tensorflow or Theano. So torch does not have placeholder, torch can just pass variable to the computational graph.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# build a variable, usually for compute gradients
x_variable = Variable(x_tensor, requires_grad=True)   

x_variable
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;tensor([[0., 1.],
        [2., 3.],
        [4., 5.]], requires_grad=True)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Until now the tensor and variable seem the same. However, the variable is a part of the graph, it&amp;rsquo;s a part of the auto-gradient.&lt;/p&gt;
&lt;p&gt;Suppose we are interested in:&lt;/p&gt;
&lt;p&gt;$$
y = \text{mean} (x_1^2) = \frac{1}{6} x^2
$$&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;y = torch.mean(x_variable*x_variable)
print(y)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;tensor(9.1667, grad_fn=&amp;lt;MeanBackward0&amp;gt;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can compute the gradient by backpropagation&lt;/p&gt;
&lt;p&gt;$$
\nabla y(x) = \frac{2}{3} x
$$&lt;/p&gt;
&lt;p&gt;i.e. if we call the &lt;code&gt;backward&lt;/code&gt; method on our outcome &lt;code&gt;y&lt;/code&gt;, we see that the gradient of our variable &lt;code&gt;x&lt;/code&gt; gets updated.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;print(x_variable.grad)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;None
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;y.backward()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;print(x_variable.grad)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;tensor([[0.0000, 0.3333],
        [0.6667, 1.0000],
        [1.3333, 1.6667]])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;However, its value has not changed.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;print(x_variable)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;tensor([[0., 1.],
        [2., 3.],
        [4., 5.]], requires_grad=True)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can also access the &lt;code&gt;tensor&lt;/code&gt; part of the variable alone by calling the &lt;code&gt;data&lt;/code&gt; method.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;print(x_variable.data)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;tensor([[0., 1.],
        [2., 3.],
        [4., 5.]])
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;activation-function&#34;&gt;Activation Function&lt;/h3&gt;
&lt;p&gt;The main advantage of neural networks is that they introduce non-linearities among the layers. The standard non-linear function&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ReLu&lt;/li&gt;
&lt;li&gt;Sigmoid&lt;/li&gt;
&lt;li&gt;TanH&lt;/li&gt;
&lt;li&gt;Softmax&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# X grid
x_grid = torch.linspace(-5, 5, 200)  # x data (tensor), shape=(100, 1)
x_grid = Variable(x_grid)
x_grid_np = x_grid.data.numpy()   # numpy array for plotting

# Activation functions
y_relu = torch.relu(x_grid).data.numpy()
y_sigmoid = torch.sigmoid(x_grid).data.numpy()
y_tanh = torch.tanh(x_grid).data.numpy()
y_softmax = torch.softmax(x_grid, dim=0).data.numpy() 
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# New figure 1
def make_new_figure_1():

    # Init figure
    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2,2, figsize=(8,6))
    fig.suptitle(&#39;Activation Functions&#39;)

    # Relu
    ax1.plot(x_grid_np, y_relu, c=&#39;red&#39;, label=&#39;relu&#39;)
    ax1.set_ylim((-1, 6)); ax1.legend()

    # Sigmoid
    ax2.plot(x_grid_np, y_sigmoid, c=&#39;red&#39;, label=&#39;sigmoid&#39;)
    ax2.set_ylim((-0.2, 1.2)); ax2.legend()

    # Tanh
    ax3.plot(x_grid_np, y_tanh, c=&#39;red&#39;, label=&#39;tanh&#39;)
    ax3.set_ylim((-1.2, 1.2)); ax3.legend()

    # Softmax
    ax4.plot(x_grid_np, y_softmax, c=&#39;red&#39;, label=&#39;softmax&#39;)
    ax4.set_ylim((-0.01, 0.06)); ax4.legend();
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s compare the different activation functions.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;make_new_figure_1()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/08_neuralnets_56_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;ReLu is very popular since it&amp;rsquo;s non-linear.&lt;/p&gt;
&lt;h2 id=&#34;83-optimization-and-gradient-descent&#34;&gt;8.3 Optimization and Gradient Descent&lt;/h2&gt;
&lt;h3 id=&#34;gradient-descent&#34;&gt;Gradient Descent&lt;/h3&gt;
&lt;p&gt;Gradient descent works as follows:&lt;/p&gt;
&lt;ol start=&#34;0&#34;&gt;
&lt;li&gt;Initialize the parameters&lt;/li&gt;
&lt;li&gt;Compute the Loss&lt;/li&gt;
&lt;li&gt;Compute the Gradients&lt;/li&gt;
&lt;li&gt;Update the Parameters&lt;/li&gt;
&lt;li&gt;Repeat (1)-(3) until convergence&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;gradient-descent-in-linear-regression&#34;&gt;Gradient Descent in Linear Regression&lt;/h3&gt;
&lt;p&gt;In order to understand how are NN optimized, we start with a linear regression example. Remember that linear regression can be interpreted as the simplest possible NN.&lt;/p&gt;
&lt;p&gt;We generate the following data:&lt;/p&gt;
&lt;p&gt;$$
y = 1 + 2 x - 3 x^2 + \varepsilon
$$&lt;/p&gt;
&lt;p&gt;with $x \sim N(0,1)$ and $\varepsilon \sim N(0,0.1)$&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Data Generation
np.random.seed(42)
N = 100

x = np.sort(np.random.rand(N, 1), axis=0)
e = .1*np.random.randn(N, 1)
y_true = 1 + 2*x - 3*x**2
y = y_true + e
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s plot the data.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# New figure 2
def make_new_figure_2():
    
    # Init
    fig, ax = plt.subplots(figsize=(8,6))
    fig.suptitle(&#39;Activation Functions&#39;)

    # Scatter
    ax.scatter(x,y); 
    ax.plot(x,y_true,color=&#39;orange&#39;); 
    ax.set_xlabel(&#39;X&#39;); ax.set_ylabel(&#39;Y&#39;);
    ax.legend([&#39;y true&#39;,&#39;y&#39;]);
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;make_new_figure_2()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/08_neuralnets_66_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Suppose we try to fit the data with a linear model&lt;/p&gt;
&lt;p&gt;$$
y = a + b x
$$&lt;/p&gt;
&lt;p&gt;We proceed iteratively by gradient descent. Our objective function is the Mean Squared Error.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Algorithm&lt;/strong&gt;&lt;/p&gt;
&lt;ol start=&#34;0&#34;&gt;
&lt;li&gt;
&lt;p&gt;Take an initial guess of the parameters
$$
a = a_0 \
b = b_0
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Compute the Mean Squared Error
$$
\begin{array}
\text{MSE} &amp;amp;= \frac{1}{N} \sum_{i=1}^{N}\left(y_{i}-\hat{y}&lt;em&gt;{i}\right)^{2} \
&amp;amp;= \frac{1}{N} \sum&lt;/em&gt;{i=1}^{N}\left(y_{i}-a-b x_{i}\right)^{2}
\end{array}
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Compute its derivative
$$
\begin{array}{l}
\frac{\partial M S E}{\partial a}=\frac{\partial M S E}{\partial \hat{y}&lt;em&gt;{i}} \cdot \frac{\partial \hat{y}&lt;/em&gt;{i}}{\partial a}=\frac{1}{N} \sum_{i=1}^{N} 2\left(y_{i}-a-b x_{i}\right) \cdot(-1)=-2 \frac{1}{N} \sum_{i=1}^{N}\left(y_{i}-\hat{y}&lt;em&gt;{i}\right) \
\frac{\partial M S E}{\partial b}=\frac{\partial M S E}{\partial \hat{y}&lt;/em&gt;{i}} \cdot \frac{\partial \hat{y}&lt;em&gt;{i}}{\partial b}=\frac{1}{N} \sum&lt;/em&gt;{i=1}^{N} 2\left(y_{i}-a-b x_{i}\right) \cdot\left(-x_{i}\right)=-2 \frac{1}{N} \sum_{i=1}^{N} x_{i}\left(y_{i}-\hat{y}_{i}\right)
\end{array}
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Update the parameters
$$
\begin{array}{l}
a=a-\eta \frac{\partial M S E}{\partial a} \
b=b-\eta \frac{\partial M S E}{\partial b}
\end{array}
$$&lt;/p&gt;
&lt;p&gt;Where $\eta$ is the &lt;strong&gt;learning rate&lt;/strong&gt;. A lower learning rate makes learning more stable but slower.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Repeat (1)-(3) $T$ times, where the number of total iterations $T$ is called &lt;strong&gt;epochs&lt;/strong&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;We start by taking a random guess of $\alpha$ and $\beta$.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Initializes parameters &amp;quot;a&amp;quot; and &amp;quot;b&amp;quot; randomly
np.random.seed(42)
a = np.random.randn(1)
b = np.random.randn(1)

print(a, b)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[0.49671415] [-0.1382643]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Plot gradient 
def gradient_plot(x, y, y_hat, y_true, EPOCHS, losses):
    clear_output(wait=True)
    fig, (ax1, ax2) = plt.subplots(1,2, figsize=(12,5))
    
    # First figure
    ax1.clear()
    ax1.scatter(x, y)
    ax1.plot(x, y_true, &#39;orange&#39;)
    ax1.plot(x, y_hat, &#39;r-&#39;)
    ax1.set_title(&#39;Data and Fit&#39;)
    ax1.legend([&#39;True&#39;, &#39;Predicted&#39;])
    
    # Second figure
    ax2.clear()
    ax2.plot(range(len(losses)), losses, color=&#39;g&#39;)
    ax2.set_xlim(0,EPOCHS); ax2.set_ylim(0,1.1*np.max(losses))
    ax2.set_title(&#39;True MSE = %.4f&#39; % losses[-1])
    
    # Plot
    plt.show();
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We set the learning rate $\eta = 0.1$ and the number of epochs $T=200$&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# parameters
LR = 0.1        # learning rate
EPOCHS = 200    # number of epochs
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can now plot the training and the result.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# New figure 3
def make_new_figure_3(a, b):
    
    # Init
    losses = []

    # train
    for t in range(EPOCHS):

        # compute loss
        y_hat = a + b * x
        error = (y - y_hat)
        loss = (error**2).mean()

        # compute gradient
        a_grad = -2 * error.mean()
        b_grad = -2 * (x * error).mean()

        # update parameters
        a -= LR * a_grad
        b -= LR * b_grad

        # plot
        losses += [loss]
        if (t+1) % (EPOCHS/25) == 0: # print 25 times
            gradient_plot(x, y, y_hat, y_true, EPOCHS, losses)

    print(a, b)
    return a, b
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;a_fit, b_fit = make_new_figure_3(a, b)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/08_neuralnets_76_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[1.40589939] [-0.83739496]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Sanity Check: do we get the same results as our gradient descent?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# OLS estimates
ols = LinearRegression()
ols.fit(x, y)
print(ols.intercept_, ols.coef_[0])
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1.4345303] [-0.89397853]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Close enough!&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s plot both lines in the graph.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# New figure 4
def make_new_figure_4():
    
    # Init
    fig, ax = plt.subplots(figsize=(8,6))

    # Scatter
    ax.plot(x,y_true,color=&#39;orange&#39;); 
    ax.plot(x,a_fit + b_fit*x,color=&#39;red&#39;); 
    ax.plot(x,ols.predict(x),color=&#39;green&#39;); 
    ax.legend([&#39;y true&#39;,&#39;y gd&#39;, &#39;y ols&#39;])
    ax.scatter(x,y); 
    ax.set_xlabel(&#39;X&#39;); ax.set_ylabel(&#39;Y&#39;); ax.set_title(&amp;quot;Data&amp;quot;);
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;make_new_figure_4()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/08_neuralnets_82_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Now we are going to do exactly the same but with &lt;code&gt;pytorch&lt;/code&gt;.&lt;/p&gt;
&lt;h3 id=&#34;autograd&#34;&gt;Autograd&lt;/h3&gt;
&lt;p&gt;Autograd is PyTorch’s automatic differentiation package. Thanks to it, we don’t need to worry about partial derivatives, chain rule or anything like it.&lt;/p&gt;
&lt;p&gt;So, how do we tell PyTorch to do its thing and compute all gradients? That’s what &lt;code&gt;backward()&lt;/code&gt; is good for.
§
Do you remember the starting point for computing the gradients? It was the loss, as we computed its partial derivatives w.r.t. our parameters. Hence, we need to invoke the &lt;code&gt;backward()&lt;/code&gt; method from the corresponding Python variable, like, &lt;code&gt;loss.backward()&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;What about the actual values of the gradients? We can inspect them by looking at the grad attribute of a tensor.&lt;/p&gt;
&lt;p&gt;If you check the method’s documentation, it clearly states that gradients are accumulated. So, every time we use the
gradients to update the parameters, we need to zero the gradients afterwards. And that’s what zero_() is good for.&lt;/p&gt;
&lt;p&gt;What does the underscore (_) at the end of the method name mean? Do you remember? If not, scroll back to the previous section and find out.&lt;/p&gt;
&lt;p&gt;So, let’s ditch the manual computation of gradients and use both backward() and zero_() methods instead.&lt;/p&gt;
&lt;p&gt;First, we convert our variables to tensors.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Convert data to tensors
x_tensor = torch.from_numpy(x).float().to(&#39;cpu&#39;)
y_tensor = torch.from_numpy(y).float().to(&#39;cpu&#39;)
print(type(x), type(x_tensor))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;class &#39;numpy.ndarray&#39;&amp;gt; &amp;lt;class &#39;torch.Tensor&#39;&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We take the initial parameters guess&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# initial parameter guess
torch.manual_seed(42)
a = torch.randn(1, requires_grad=True, dtype=torch.float, device=&#39;cpu&#39;)
b = torch.randn(1, requires_grad=True, dtype=torch.float, device=&#39;cpu&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we are ready to fit the model.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# New figure 5
def make_new_figure_5(a, b):
    
    # Init
    losses = []

    # parameters
    LR = 0.1
    EPOCHS = 200

    # train
    for t in range(EPOCHS):

        # compute loss
        y_hat = a + b * x_tensor
        error = y_tensor - y_hat
        loss = (error ** 2).mean()

        # compute gradient
        loss.backward()

        # update parameters
        with torch.no_grad():
            a -= LR * a.grad
            b -= LR * b.grad

        # clear gradients
        a.grad.zero_()
        b.grad.zero_()

        # Plot
        losses += [((y_true - y_hat.detach().numpy())**2).mean()]
        if (t+1) % (EPOCHS/25) == 0: # print 25 times
            gradient_plot(x, y, y_hat.data.numpy(), y_true, EPOCHS, losses)

    print(a, b)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;make_new_figure_5(a, b)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/08_neuralnets_93_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;tensor([1.3978], requires_grad=True) tensor([-0.8214], requires_grad=True)
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;optimizer&#34;&gt;Optimizer&lt;/h3&gt;
&lt;p&gt;So far, we’ve been manually updating the parameters using the computed gradients. That’s probably fine for two parameters… but what if we had a whole lot of them?! We use one of PyTorch’s optimizers.&lt;/p&gt;
&lt;p&gt;An optimizer takes the following arguments:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the parameters we want to update&lt;/li&gt;
&lt;li&gt;he learning rate we want to use&lt;/li&gt;
&lt;li&gt;(possibly many other hyper-parameters)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Moreover, we can now call the function &lt;code&gt;zero_grad()&lt;/code&gt; to automatically update the parameters. In particular, we will need to perform the following steps at each iteration:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Clear the parameters: &lt;code&gt;optimizer.zero_grad()&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Compute the gradient: &lt;code&gt;loss.backward()&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Update the parameters: &lt;code&gt;optimizer.step()&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In the code below, we create a Stochastic Gradient Descent (&lt;code&gt;SGD&lt;/code&gt;) optimizer to update our parameters $a$ and $b$.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Init parameters
torch.manual_seed(42)
a = torch.randn(1, requires_grad=True, dtype=torch.float, device=&#39;cpu&#39;)
b = torch.randn(1, requires_grad=True, dtype=torch.float, device=&#39;cpu&#39;)

# Defines a SGD optimizer to update the parameters
optimizer = torch.optim.SGD([a, b], lr=LR)
print(optimizer)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;SGD (
Parameter Group 0
    dampening: 0
    lr: 0.1
    momentum: 0
    nesterov: False
    weight_decay: 0
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can also define a default loss function so that we don&amp;rsquo;t have to compute it by hand. We are going to use the &lt;code&gt;MSE&lt;/code&gt; loss function.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Define a loss function
loss_func = torch.nn.MSELoss()
print(loss_func)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;MSELoss()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s plot the estimator and the MSE.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# New figure 6
def make_new_figure_6(a, b):
    
    # parameters
    EPOCHS = 200

    # init 
    losses = []

    # train
    for t in range(EPOCHS):

        # compute loss
        y_hat = a + b * x_tensor
        error = y_tensor - y_hat
        loss = (error ** 2).mean()  

        # update parameters
        optimizer.zero_grad()   # clear gradients for next train
        loss.backward()         # backpropagation, compute gradients
        optimizer.step()        # apply gradients, update parameters

        # Plot
        losses += [((y_true - y_hat.detach().numpy())**2).mean()]
        if (t+1) % (EPOCHS/25) == 0:
            gradient_plot(x, y, y_hat.data.numpy(), y_true, EPOCHS, losses)

    print(a, b)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;make_new_figure_6(a, b)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/08_neuralnets_103_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;tensor([1.3978], requires_grad=True) tensor([-0.8214], requires_grad=True)
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;building-a-nn&#34;&gt;Building a NN&lt;/h3&gt;
&lt;p&gt;In our model, we manually created two parameters to perform a linear regression. Let’s use PyTorch’s &lt;code&gt;Sequential&lt;/code&gt; module to create our neural network.&lt;/p&gt;
&lt;p&gt;We first want to build the linear regression framework&lt;/p&gt;
&lt;p&gt;$$
y = a + b x
$$&lt;/p&gt;
&lt;p&gt;Which essentially is a network with&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;1 input&lt;/li&gt;
&lt;li&gt;no hidden layer&lt;/li&gt;
&lt;li&gt;no activation function&lt;/li&gt;
&lt;li&gt;1 output&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Let&amp;rsquo;s build the simplest possible neural network with &lt;code&gt;PyTorch&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Simplest possible neural network
linear_net = torch.nn.Sequential(
    torch.nn.Linear(1, 1)
)

print(linear_net)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Sequential(
  (0): Linear(in_features=1, out_features=1, bias=True)
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, if we call the &lt;code&gt;parameters()&lt;/code&gt; method of this model, PyTorch will figure the parameters of its attributes in a recursive way.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;[*linear_net.parameters()]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[Parameter containing:
 tensor([[-0.2191]], requires_grad=True),
 Parameter containing:
 tensor([0.2018], requires_grad=True)]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can now define the definitive training function.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def train_NN(x, y, y_true, net, optimizer, loss_func, EPOCHS):
    
    # transform variables
    x_tensor = torch.from_numpy(x).float().to(&#39;cpu&#39;)
    y_tensor = torch.from_numpy(y).float().to(&#39;cpu&#39;)

    # init 
    losses = []
    
    # train
    for t in range(EPOCHS):        

        # compute loss
        y_hat = net(x_tensor)     
        loss = loss_func(y_hat, y_tensor)    
        
        # update parameters
        optimizer.zero_grad()   # clear gradients for next train
        loss.backward()         # backpropagation, compute gradients
        optimizer.step()        # apply gradients, update parameters

        # plot
        losses += [((y_true - y_hat.detach().numpy())**2).mean()]
        if (t+1) % (EPOCHS/25) == 0: # print 25 times
            gradient_plot(x, y, y_hat.data.numpy(), y_true, EPOCHS, losses)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we are ready to train our neural network.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;optimizer = torch.optim.SGD(linear_net.parameters(), lr=LR)

# train
train_NN(x, y, y_true, linear_net, optimizer, loss_func, EPOCHS)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/08_neuralnets_113_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;We now define a more complicated NN. In particular we, build a neural network with&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;1 input&lt;/li&gt;
&lt;li&gt;1 hidden layer with 10 neurons and Relu activation function&lt;/li&gt;
&lt;li&gt;1 output layer&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Relu Net
relu_net = torch.nn.Sequential(
    torch.nn.Linear(1, 10),
    torch.nn.ReLU(),
    torch.nn.Linear(10, 1)
)

print(relu_net)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Sequential(
  (0): Linear(in_features=1, out_features=10, bias=True)
  (1): ReLU()
  (2): Linear(in_features=10, out_features=1, bias=True)
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This network has much more parameters.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;[*relu_net.parameters()]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[Parameter containing:
 tensor([[-0.4869],
         [ 0.5873],
         [ 0.8815],
         [-0.7336],
         [ 0.8692],
         [ 0.1872],
         [ 0.7388],
         [ 0.1354],
         [ 0.4822],
         [-0.1412]], requires_grad=True),
 Parameter containing:
 tensor([ 0.7709,  0.1478, -0.4668,  0.2549, -0.4607, -0.1173, -0.4062,  0.6634,
         -0.7894, -0.4610], requires_grad=True),
 Parameter containing:
 tensor([[-0.0893, -0.1901,  0.0298, -0.3123,  0.2856, -0.2686,  0.2441,  0.0526,
          -0.1027,  0.1954]], requires_grad=True),
 Parameter containing:
 tensor([0.0493], requires_grad=True)]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We are again using Stochastic Gradient Descent (&lt;code&gt;SGD&lt;/code&gt;) as optimization algorithm and Mean Squared Error (&lt;code&gt;MSELoss&lt;/code&gt;) as objective function.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# parameters
LR = 0.1
EPOCHS = 1000

# optimizer and loss function
optimizer = torch.optim.SGD(relu_net.parameters(), lr=LR)
loss_func = torch.nn.MSELoss()

# Train
train_NN(x, y, y_true, relu_net, optimizer, loss_func, EPOCHS)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/08_neuralnets_119_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;It seems that we can use fewer nodes to get the same result.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s make a smallet network.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Relu Net
relu_net2 = torch.nn.Sequential(
    torch.nn.Linear(1, 4),
    torch.nn.ReLU(),
    torch.nn.Linear(4, 1)
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And train it.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# parameters
LR = 0.1
EPOCHS = 1000

# optimizer and loss function
optimizer = torch.optim.SGD(relu_net2.parameters(), lr=LR)
loss_func = torch.nn.MSELoss()

# Train
train_NN(x, y, y_true, relu_net2, optimizer, loss_func, EPOCHS)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/08_neuralnets_124_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;We can try different activation functions.&lt;/p&gt;
&lt;p&gt;For example the tangent.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# TanH Net
tanh_net = torch.nn.Sequential(
    torch.nn.Linear(1, 10),
    torch.nn.Tanh(),
    torch.nn.Linear(10, 1)
)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# parameters
LR = 0.2
EPOCHS = 1000

# optimizer and loss function
optimizer = torch.optim.SGD(tanh_net.parameters(), lr=LR)
loss_func = torch.nn.MSELoss()

# train
train_NN(x, y, y_true, tanh_net, optimizer, loss_func, EPOCHS)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/08_neuralnets_127_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;loss-functions&#34;&gt;Loss functions&lt;/h3&gt;
&lt;p&gt;So far we have used the Stochastic  as loss function.&lt;/p&gt;
&lt;p&gt;Notice that &lt;code&gt;nn.MSELoss&lt;/code&gt; actually creates a loss function for us — it is NOT the loss function itself. Moreover, you can specify a reduction method to be applied, that is, how do you want to aggregate the results for individual points — you can average them (reduction=&lt;code&gt;mean&lt;/code&gt;) or simply sum them up (reduction=&lt;code&gt;sum&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;We are now going to use different ones.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# parameters
LR = 0.1
EPOCHS = 25

# nets
n = torch.nn.Sequential(torch.nn.Linear(1, 10), torch.nn.ReLU(), torch.nn.Linear(10, 1))
nets = [n,n,n,n]

# optimizers
optimizers = [torch.optim.SGD(n.parameters(), lr=LR) for n in nets]

# different loss functions
loss_MSE        = torch.nn.MSELoss()
loss_L1         = torch.nn.L1Loss()
loss_NLL        = torch.nn.NLLLoss()
loss_KLD        = torch.nn.KLDivLoss()
loss_funcs = [loss_MSE, loss_L1, loss_NLL, loss_KLD]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is the description of the loss functions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;MSELoss&lt;/code&gt;: Creates a criterion that measures the mean squared error (squared L2 norm) between each element in the input $x$ and target $y$.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;L1Loss&lt;/code&gt;: Creates a criterion that measures the mean absolute error (MAE) between each element in the input $x$ and target $y$.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;NLLLoss&lt;/code&gt;: The negative log likelihood loss.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;KLDivLoss&lt;/code&gt;: The Kullback-Leibler divergence loss measure&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Train multiple nets
def train_nets(x, y, y_true, nets, optimizers, loss_funcs, labels, EPOCHS):

    # Put dateset into torch dataset
    x_tensor = torch.from_numpy(x).float().to(&#39;cpu&#39;)
    y_tensor = torch.from_numpy(y).float().to(&#39;cpu&#39;)
    torch_dataset = Data.TensorDataset(x_tensor, y_tensor)
    
    # Init
    losses = np.zeros((0,4))
    
    # Train
    for epoch in range(EPOCHS): # for each epoch
        losses = np.vstack((losses, np.zeros((1,4))))
        for k, net, opt, lf in zip(range(4), nets, optimizers, loss_funcs):
            y_hat = net(x_tensor)              # get output for every net
            loss = loss_func(y_hat, y_tensor)  # compute loss for every net
            opt.zero_grad()                    # clear gradients for next train
            loss.backward()                    # backpropagation, compute gradients
            opt.step()                         # apply gradients
            losses[-1,k] = ((y_true - y_hat.detach().numpy())**2).mean()
        plot_losses(losses, labels, EPOCHS)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Plot losses 
def plot_losses(losses, labels, EPOCHS):
    clear_output(wait=True)
    fig, ax = plt.subplots(1,1, figsize=(10,6))
    
    # Plot
    ax.clear()
    ax.plot(range(len(losses)), losses)
    ax.set_xlim(0,EPOCHS-1); ax.set_ylim(0,1.1*np.max(losses))
    ax.set_title(&#39;Compare Losses&#39;); ax.set_ylabel(&#39;True MSE&#39;)
    legend_txt = [&#39;%s=%.4f&#39; % (label, loss) for label,loss in zip(labels, losses[-1,:])]
    ax.legend(legend_txt)
    
    # Shot
    plt.show();
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s compare them.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Train
labels = [&#39;MSE&#39;, &#39;L1&#39;, &#39;LogL&#39;, &#39;KLdiv&#39;]
train_nets(x, y, y_true, nets, optimizers, loss_funcs, labels, EPOCHS)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/08_neuralnets_135_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;In this very simple case, all loss functions are very similar.&lt;/p&gt;
&lt;h3 id=&#34;optimizers&#34;&gt;Optimizers&lt;/h3&gt;
&lt;p&gt;So far we have used the Stochastic Gradient Descent to fit the neural network. We are now going to use different ones.&lt;/p&gt;
&lt;p&gt;This is the &lt;a href=&#34;https://pytorch.org/docs/stable/optim.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;description of the optimizers&lt;/a&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;SGD&lt;/code&gt;: Implements stochastic gradient descent (optionally with momentum).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;Momentum&lt;/code&gt;: Nesterov momentum is based on the formula from &lt;a href=&#34;http://www.cs.toronto.edu/~hinton/absps/momentum.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;On the importance of initialization and momentum in deep learning&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;RMSprop&lt;/code&gt;: Proposed by G. Hinton in his &lt;a href=&#34;https://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;course&lt;/a&gt;. The centered version first appears in &lt;a href=&#34;https://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Generating Sequences With Recurrent Neural Networks&lt;/a&gt;. The implementation here takes the square root of the gradient average before adding epsilon (note that TensorFlow interchanges these two operations). The effective learning rate is thus $\frac{\alpha}{\sqrt{v} + \epsilon}$ where $\alpha$ is the scheduled learning rate and $v$ is the weighted moving average of the squared gradient.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;Adam&lt;/code&gt;: Proposed in &lt;a href=&#34;https://arxiv.org/abs/1412.6980&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Adam: A Method for Stochastic Optimization&lt;/a&gt;. The implementation of the L2 penalty follows changes proposed in &lt;a href=&#34;https://arxiv.org/abs/1711.05101&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Decoupled Weight Decay Regularization&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# parameters
LR = 0.1
EPOCHS = 25

# nets
n = torch.nn.Sequential(torch.nn.Linear(1, 10), torch.nn.ReLU(), torch.nn.Linear(10, 1))
nets = [n,n,n,n]

# different optimizers
opt_SGD         = torch.optim.SGD(nets[0].parameters(), lr=LR)
opt_Momentum    = torch.optim.SGD(nets[1].parameters(), lr=LR, momentum=0.8)
opt_RMSprop     = torch.optim.RMSprop(nets[2].parameters(), lr=LR, alpha=0.9)
opt_Adam        = torch.optim.Adam(nets[3].parameters(), lr=LR, betas=(0.9, 0.99))
optimizers = [opt_SGD, opt_Momentum, opt_RMSprop, opt_Adam]

# loss functions
l = torch.nn.MSELoss()
loss_funcs = [l,l,l,l]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s prot the loss functions over training, for different optimizers.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# train
labels = [&#39;SGD&#39;, &#39;Momentum&#39;, &#39;RMSprop&#39;, &#39;Adam&#39;]
train_nets(x, y, y_true, nets, optimizers, loss_funcs, labels, EPOCHS)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/08_neuralnets_141_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;training-on-batch&#34;&gt;Training on batch&lt;/h3&gt;
&lt;p&gt;Until now, we have used the whole training data at every training step. It has been batch gradient descent all along.&lt;/p&gt;
&lt;p&gt;This is fine for our ridiculously small dataset, sure, but if we want to go serious about all this, we must use mini-batch gradient descent. Thus, we need mini-batches. Thus, we need to slice our dataset accordingly. Do you want to do it manually?! Me neither!&lt;/p&gt;
&lt;p&gt;So we use PyTorch’s &lt;code&gt;DataLoader&lt;/code&gt; class for this job. We tell it which dataset to use (the one we just built in the previous section), the desired mini-batch size and if we’d like to shuffle it or not. That’s it!&lt;/p&gt;
&lt;p&gt;Our loader will behave like an iterator, so we can loop over it and fetch a different mini-batch every time.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Init data
x_tensor = torch.from_numpy(x).float().to(&#39;cpu&#39;)
y_tensor = torch.from_numpy(y).float().to(&#39;cpu&#39;)
torch_dataset = Data.TensorDataset(x_tensor, y_tensor)

# Build DataLoader
BATCH_SIZE = 25
loader = Data.DataLoader(
    dataset=torch_dataset,      # torch TensorDataset format
    batch_size=BATCH_SIZE,      # mini batch size
    shuffle=True,               # random shuffle for training
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s try using sub-samples of dimension &lt;code&gt;BATCH_SIZE = 25&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def train_NN_batch(loader, y_true, net, optimizer, loss_func, EPOCHS):
    
    # init
    losses = []

    # train
    for t in range(EPOCHS):   
        # train entire dataset 3 times
        for step, (batch_x, batch_y) in enumerate(loader):

            # compute loss
            y_hat = net(batch_x)     
            loss = loss_func(y_hat, batch_y)    

            # update parameters
            optimizer.zero_grad()   # clear gradients for next train
            loss.backward()         # backpropagation, compute gradients
            optimizer.step()        # apply gradients

        # plt every epoch
        y_hat = net(x_tensor)  
        losses += [((y_true - y_hat.detach().numpy())**2).mean()]
        if (t+1) % (EPOCHS/25) == 0:
            gradient_plot(x, y, y_hat.data.numpy(), y_true, EPOCHS, losses)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# parameters
LR = 0.1
EPOCHS = 1000
net = torch.nn.Sequential(torch.nn.Linear(1, 10), torch.nn.ReLU(), torch.nn.Linear(10, 1))
optimizer = torch.optim.SGD(net.parameters(), lr=LR)
loss_func = torch.nn.MSELoss()

# Train
train_NN_batch(loader, y_true, net, optimizer, loss_func, EPOCHS)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/08_neuralnets_147_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Two things are different now: not only we have an inner loop to load each and every mini-batch from our DataLoader but, more importantly, we are now sending only one mini-batch to the device.&lt;/p&gt;
&lt;p&gt;For bigger datasets, loading data sample by sample (into a CPU tensor) using Dataset’s &lt;code&gt;__get_item__&lt;/code&gt; and then sending all samples that belong to the same mini-batch at once to your GPU (device) is the way to go in order to make the best use of your graphics card’s RAM.&lt;/p&gt;
&lt;p&gt;Moreover, if you have many GPUs to train your model on, it is best to keep your dataset “agnostic” and assign the batches to different GPUs during training.&lt;/p&gt;
&lt;h2 id=&#34;84-advanced-topics&#34;&gt;8.4 Advanced Topics&lt;/h2&gt;
&lt;h3 id=&#34;issues&#34;&gt;Issues&lt;/h3&gt;
&lt;h4 id=&#34;starting-values&#34;&gt;Starting Values&lt;/h4&gt;
&lt;p&gt;Usually starting values for weights are chosen to be random values near zero. Hence the model starts out nearly linear, and becomes nonlinear as the weights increase.&lt;/p&gt;
&lt;h4 id=&#34;overfitting&#34;&gt;Overfitting&lt;/h4&gt;
&lt;p&gt;In early developments of neural networks, either by design or by accident, an early stopping rule was used to avoid overfitting.&lt;/p&gt;
&lt;p&gt;A more explicit method for regularization is &lt;em&gt;weight decay&lt;/em&gt;.&lt;/p&gt;
&lt;h4 id=&#34;scaling-of-the-inputs&#34;&gt;Scaling of the Inputs&lt;/h4&gt;
&lt;p&gt;Since the scaling of the inputs determines the effective scaling of the weights in the bottom layer, it can have a large effect on the quality of the final solution. At the outset it is best to standardize all inputs to have mean zero and standard deviation one.&lt;/p&gt;
&lt;h4 id=&#34;number-of-hidden-units-and-layers&#34;&gt;Number of Hidden Units and Layers&lt;/h4&gt;
&lt;p&gt;Generally speaking it is better to have too many hidden units than too few. With too few hidden units, the model might not have enough flexibility to capture the nonlinearities in the data; with too many hidden units, the extra weights can be shrunk toward zero if appropriate regularization is used.&lt;/p&gt;
&lt;p&gt;Choice of the number of hidden layers is guided by background knowledge and experimentation. Each layer extracts features of the input for regression or classification. Use of multiple hidden layers allows construction of hierarchical features at different levels of resolution.&lt;/p&gt;
&lt;p&gt;You can get an intuition on the role of hidden layers here: &lt;a href=&#34;https://playground.tensorflow.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://playground.tensorflow.org/&lt;/a&gt;&lt;/p&gt;
&lt;h4 id=&#34;multiple-minima&#34;&gt;Multiple Minima&lt;/h4&gt;
&lt;p&gt;The error function R(θ) is nonconvex, possessing many local minima. One approach is to use the average predictions over the collection of networks as the final prediction. Another approach is via &lt;em&gt;bagging&lt;/em&gt;.&lt;/p&gt;
&lt;h3 id=&#34;deep-neural-networks-and-deep-learning&#34;&gt;Deep Neural Networks and Deep Learning&lt;/h3&gt;
&lt;p&gt;Deep Neural Networks are just Neural Networks with more than one hidden layer.&lt;/p&gt;
&lt;h3 id=&#34;convolutional-neural-nets&#34;&gt;Convolutional Neural Nets&lt;/h3&gt;
&lt;p&gt;Convolutional Neural Nets are often applied when dealing with image/video data. They are usually coded with each feature being a pixel and its value is the pixel color (3 dimensional RGB array).&lt;/p&gt;
&lt;img src=&#34;../figures/cnn1.png&#34; style=&#34;width: 400px;&#34;/&gt;
&lt;p&gt;Videos and images have 2 main characteristics:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;have lots of features&lt;/li&gt;
&lt;li&gt;&amp;ldquo;close&amp;rdquo; features are often similar&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Convolutional Neural Nets exploit the second characteristic to alleviate the computational problems arising from the first. They do it by constructing a first layer that does not build on evey feature but only on adjacent ones.&lt;/p&gt;
&lt;img src=&#34;../figures/cnn1.gif&#34; style=&#34;width: 400px;&#34;/&gt;
&lt;p&gt;In this way, most of the information is preserved, on a lower dimensional representation.&lt;/p&gt;
&lt;h3 id=&#34;recurrent-neural-nets&#34;&gt;Recurrent Neural Nets&lt;/h3&gt;
&lt;p&gt;Recurrent Neural Networks are often applied in contexts in which the data generating process is dynamic. The most important example is Natural Language Processing. The idea is that you want to make predictions &amp;ldquo;live&amp;rdquo; as data comes in. Moreover, the order of the data is relevant, so that you also what to keep track of what the model has learned so far.&lt;/p&gt;
&lt;p&gt;While RNNs learn similarly while training, in addition, they remember things learnt from prior input(s) while generating output(s). It’s part of the network. RNNs can take one or more input vectors and produce one or more output vectors and the output(s) are influenced not just by weights applied on inputs like a regular NN, but also by a “hidden” state vector representing the context based on prior input(s)/output(s). So, the same input could produce a different output depending on previous inputs in the series.&lt;/p&gt;
&lt;p&gt;Grafically:&lt;/p&gt;
&lt;img src=&#34;../figures/rnn1.png&#34; style=&#34;width: 600px;&#34;/&gt;
&lt;p&gt;In summary, in a vanilla neural network, a fixed size input vector is transformed into a fixed size output vector. Such a network becomes “recurrent” when you repeatedly apply the transformations to a series of given input and produce a series of output vectors.&lt;/p&gt;
&lt;h3 id=&#34;bidirectional-rnn&#34;&gt;Bidirectional RNN&lt;/h3&gt;
&lt;p&gt;Sometimes it’s not just about learning from the past to predict the future, but we also need to look into the future to fix the past. In speech recognition and handwriting recognition tasks, where there could be considerable ambiguity given just one part of the input, we often need to know what’s coming next to better understand the context and detect the present.&lt;/p&gt;
&lt;img src=&#34;../figures/rnn2.png&#34; style=&#34;width: 600px;&#34;/&gt;
&lt;p&gt;This does introduce the obvious challenge of how much into the future we need to look into, because if we have to wait to see all inputs then the entire operation will become costly.&lt;/p&gt;
&lt;h3 id=&#34;recursive-neural-netw&#34;&gt;Recursive Neural Netw&lt;/h3&gt;
&lt;p&gt;A recurrent neural network parses the inputs in a sequential fashion. A recursive neural network is similar to the extent that the transitions are repeatedly applied to inputs, but not necessarily in a sequential fashion. Recursive Neural Networks are a more general form of Recurrent Neural Networks. It can operate on any hierarchical tree structure. Parsing through input nodes, combining child nodes into parent nodes and combining them with other child/parent nodes to create a tree like structure. Recurrent Neural Networks do the same, but the structure there is strictly linear. i.e. weights are applied on the first input node, then the second, third and so on.&lt;/p&gt;
&lt;img src=&#34;../figures/rnn3.png&#34; style=&#34;width: 600px;&#34;/&gt;
&lt;p&gt;But this raises questions pertaining to the structure. How do we decide that? If the structure is fixed like in Recurrent Neural Networks then the process of training, backprop etc makes sense in that they are similar to a regular neural network. But if the structure isn’t fixed, is that learnt as well?&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Post-Double Selection</title>
      <link>https://matteocourthoud.github.io/course/ml-econ/09_postdoubleselection/</link>
      <pubDate>Wed, 09 Mar 2022 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/course/ml-econ/09_postdoubleselection/</guid>
      <description>&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Remove warnings
import warnings
warnings.filterwarnings(&#39;ignore&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Import everything
import pandas as pd
import numpy as np
import seaborn as sns
import statsmodels.api as sm

from numpy.linalg import inv
from statsmodels.iolib.summary2 import summary_col
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression, Lasso, Ridge
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.preprocessing import PolynomialFeatures, StandardScaler
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Import matplotlib for graphs
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import axes3d

# Set global parameters
%matplotlib inline
plt.style.use(&#39;seaborn-white&#39;)
plt.rcParams[&#39;lines.linewidth&#39;] = 3
plt.rcParams[&#39;figure.figsize&#39;] = (10,6)
plt.rcParams[&#39;figure.titlesize&#39;] = 20
plt.rcParams[&#39;axes.titlesize&#39;] = 18
plt.rcParams[&#39;axes.labelsize&#39;] = 14
plt.rcParams[&#39;legend.fontsize&#39;] = 14
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;91-frisch-waugh-theorem&#34;&gt;9.1 Frisch-Waugh theorem&lt;/h2&gt;
&lt;p&gt;Consider the data $D = { x_i, y_i, z_i }_{i=1}^n$ with DGP:&lt;/p&gt;
&lt;p&gt;$$
y_i = x_i&amp;rsquo; \alpha_0+ z_i&amp;rsquo; \beta_0 + \varepsilon_i
$$&lt;/p&gt;
&lt;p&gt;. The following estimators of $\alpha$ are numerically equivalent (if $[x, z]$ has full rank):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;OLS: $\hat{\alpha}$ from regressing $y$ on $x, z$&lt;/li&gt;
&lt;li&gt;Partialling out: $\tilde{\alpha}$ from regressing $y$ on $\tilde{x}$&lt;/li&gt;
&lt;li&gt;&amp;ldquo;Double&amp;rdquo; partialling out: $\bar{\alpha}$ from regressing $\tilde{y}$ on $\tilde{x}$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;where the operation of passing to $y, x$ to $\tilde{y}, \tilde{x}$ is called &lt;em&gt;projection  out $z$&lt;/em&gt;, e.g. $\tilde{x}$ are the residuals from regressing $x$ on $z$.&lt;/p&gt;
&lt;p&gt;$$
\tilde{x} = x - \hat \gamma z = (I - z (z&amp;rsquo; z)^{-1} z&amp;rsquo; ) x = (I-P_z) x = M_z x
$$&lt;/p&gt;
&lt;p&gt;I.e we have done the following:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;regress $x$ on $z$&lt;/li&gt;
&lt;li&gt;compute $\hat x$&lt;/li&gt;
&lt;li&gt;compute the residuals $\tilde x = x - \hat x$&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;We now explore the theorem through simulation. In particular, we generate a sample from the following model:&lt;/p&gt;
&lt;p&gt;$$
y_i = x_i - 0.3 z_i + \varepsilon_i
$$&lt;/p&gt;
&lt;p&gt;where $x_i,z_i,\varepsilon_i \sim N(0,1)$ and $n=1000$.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;np.random.seed(1)

# Init
n = 1000
a = 1
b = -.3

# Generate data
x = np.random.uniform(0,1,n).reshape(-1,1)
z = np.random.uniform(0,1,n).reshape(-1,1)
e = np.random.normal(0,1,n).reshape(-1,1)
y = a*x + b*z + e
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s compute the value of the OLS estimator.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Estimate alpha by OLS
xz = np.concatenate([x,z], axis=1)
ols_coeff = inv(xz.T @ xz) @ xz.T @ y
alpha_ols = ols_coeff[0][0]

print(&#39;alpha OLS: %.4f (true=%1.0f)&#39; % (alpha_ols, a))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;alpha OLS: 1.0928 (true=1)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The partialling out estimator.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Partialling out
x_tilde = (np.eye(n) - z @ inv(z.T @ z) @ z.T ) @ x
alpha_po = inv(x_tilde.T @ x_tilde) @ x_tilde.T @ y

print(&#39;alpha partialling out: %.4f (true=%1.0f)&#39; % (alpha_po, a))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;alpha partialling out: 1.0928 (true=1)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And lastly, the double-partialling out estimator.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# &amp;quot;Double&amp;quot; partialling out
y_tilde = (np.eye(n) - z @ inv(z.T @ z) @ z.T ) @ y
alpha_po2 = inv(x_tilde.T @ x_tilde) @ x_tilde.T @ y_tilde

print(&#39;alpha double partialling out: %.4f (true=%1.0f)&#39; % (alpha_po2, a))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;alpha double partialling out: 1.0928 (true=1)
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;92-omitted-variable-bias&#34;&gt;9.2 Omitted Variable Bias&lt;/h2&gt;
&lt;p&gt;Consider two separate statistical models. Assume the following &lt;strong&gt;long regression&lt;/strong&gt; of interest:&lt;/p&gt;
&lt;p&gt;$$
y_i = x_i&amp;rsquo; \alpha_0+ z_i&amp;rsquo; \beta_0 + \varepsilon_i
$$&lt;/p&gt;
&lt;p&gt;Define the corresponding &lt;strong&gt;short regression&lt;/strong&gt; as&lt;/p&gt;
&lt;p&gt;$$
y_i = x_i&amp;rsquo; \alpha_0 + v_i \quad \text{ with } \quad x_i = z_i&amp;rsquo; \gamma_0 + u_i
$$&lt;/p&gt;
&lt;h4 id=&#34;ovb-theorem&#34;&gt;OVB Theorem&lt;/h4&gt;
&lt;p&gt;Suppose that the DGP for the long regression corresponds to $\alpha_0$, $\beta_0$. Suppose further that $\mathbb E[x_i] = 0$, $\mathbb E[z_i] = 0$, $\mathbb E[\varepsilon_i |x_i,z_i] = 0$. Then, unless $\beta_0 = 0$ or $z_i$ is orthogonal to $x_i$, the (sole) stochastic regressor $x_i$ is correlated with the error term in the short regression which implies that the OLS estimator of the short regression is inconsistent for $\alpha_0$ due to the omitted variable bias. In particular, one can show that the plim of the OLS estimator of $\hat{\alpha}_{SHORT}$ from the short regression is&lt;/p&gt;
&lt;p&gt;$$
\hat{\alpha}_{SHORT} \overset{p}{\to} \frac{Cov(y_i, x_i)}{Var(x_i)} = \alpha_0 + \beta_0 \frac{Cov(z_i, x_i)}{Var(x_i)}
$$&lt;/p&gt;
&lt;p&gt;Consider data $D= (y_i, x_i, z_i)_{i=1}^n$, where the true model is:&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
&amp;amp; y_i = x_i&amp;rsquo; \alpha_0  + z_i&amp;rsquo; \beta_0 + \varepsilon_i \
&amp;amp; x_i = z_i&amp;rsquo; \gamma_0 + u_i
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s investigate the Omitted Variable Bias by simulation. In particular, we generate a sample from the following model:&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
&amp;amp; y_i = x_i - 0.3 z_i + \varepsilon_i \
&amp;amp; x_i = 3 z_i + u_i \
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;where $z_i,\varepsilon_i,u_i \sim N(0,1)$ and $n=1000$.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def generate_data(a, b, c, n):

    # Generate data
    z = np.random.normal(0,1,n).reshape(-1,1)
    u = np.random.normal(0,1,n).reshape(-1,1)
    x = c*z + u
    e = np.random.normal(0,1,n).reshape(-1,1)
    y = a*x + b*z + e
    
    return x, y, z
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;First let&amp;rsquo;s compute the value of the OLS estimator.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Init
n = 1000
a = 1
b = -.3
c = 3
x, y, z = generate_data(a, b, c, n)

# Estimate alpha by OLS
ols_coeff = inv(x.T @ x) @ x.T @ y
alpha_short = ols_coeff[0][0]

print(&#39;alpha OLS: %.4f (true=%1.0f)&#39; % (alpha_short, a))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;alpha OLS: 0.9115 (true=1)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In our case the expected bias is:&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
Bias &amp;amp; = \beta_0 \frac{Cov(z_i, x_i)}{Var(x_i)} = \
&amp;amp; = \beta_0 \frac{Cov(z_i&amp;rsquo; \gamma_0 + u_i, x_i)}{Var(z_i&amp;rsquo; \gamma_0 + u_i)} = \
&amp;amp; = \beta_0 \frac{\gamma_0 Var(z_i)}{\gamma_0^2 Var(z_i) + Var(u_i)}
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;which in our case is $b \frac{c}{c^2 + 1}$.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Expected bias
bias = alpha_short - a
exp_bias = b * c / (c**2 + 1)

print(&#39;Empirical bias: %.4f \nExpected bias:  %.4f&#39; % (bias, exp_bias))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Empirical bias: -0.0885 
Expected bias:  -0.0900
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;93-pre-test-bias&#34;&gt;9.3 Pre-Test Bias&lt;/h2&gt;
&lt;p&gt;Consider data $D= (y_i, x_i, z_i)_{i=1}^n$, where the true model is:&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
&amp;amp; y_i = x_i&amp;rsquo; \alpha_0  + z_i&amp;rsquo; \beta_0 + \varepsilon_i \
&amp;amp; x_i = z_i&amp;rsquo; \gamma_0 + u_i
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;Where $x_i$ is the variable of interest (we want to make inference on $\alpha_0$) and $z_i$ is a high dimensional set of control variables.&lt;/p&gt;
&lt;p&gt;From now on, we will work under the following assumptions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\dim(x_i)=1$ for all $n$&lt;/li&gt;
&lt;li&gt;$\beta_0$ uniformely bounded in $n$&lt;/li&gt;
&lt;li&gt;Strict exogeneity: $\mathbb E[\varepsilon_i | x_i, z_i] = 0$ and $\mathbb E[u_i | z_i] = 0$&lt;/li&gt;
&lt;li&gt;$\beta_0$ and $\gamma_0$ have dimension (and hence value) that depend on $n$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Pre-Testing procedure:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Regress $y_i$ on $x_i$ and $z_i$&lt;/li&gt;
&lt;li&gt;For each $j = 1, &amp;hellip;, p = \dim(z_i)$ calculate a test statistic $t_j$&lt;/li&gt;
&lt;li&gt;Let $\hat{T} = { j: |t_j| &amp;gt; C &amp;gt; 0 }$ for some constant $C$ (set of statistically significant coefficients).&lt;/li&gt;
&lt;li&gt;Re-run the new &amp;ldquo;model&amp;rdquo; using $(x_i, z_{\hat{T},i})$ (i.e. using the selected covariates with statistically significant coefficients).&lt;/li&gt;
&lt;li&gt;Perform statistical inference (i.e. confidence intervals and hypothesis tests) as if no model selection had been done.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Pre-testing leads to incorrect inference. Why? Because of test errors in the first stage.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# T-test
def t_test(y, x, k):
    beta_hat = inv(x.T @ x) @ x.T @ y
    residuals = y - x @ beta_hat
    sigma2_hat = np.var(residuals)
    beta_std = np.sqrt(np.diag(inv(x.T @ x)) * sigma2_hat )
    return beta_hat[k,0]/beta_std[k]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;First of all the t-test for $H_0: \beta_0 = 0$:&lt;/p&gt;
&lt;p&gt;$$
t = \frac{\hat \beta_k}{\hat \sigma_{\beta_k}}
$$&lt;/p&gt;
&lt;p&gt;where the standard deviation of the ols coefficient is given by&lt;/p&gt;
&lt;p&gt;$$
\hat \sigma_{\beta_k} = \sqrt{ \hat \sigma^2 \cdot (X&amp;rsquo;X)^{-1}_{[k,k]} }
$$&lt;/p&gt;
&lt;p&gt;where we estimate the variance of the error term with the variance of the residuals&lt;/p&gt;
&lt;p&gt;$$
\hat \sigma^2 = Var \big( y - \hat y \big) = Var \big( y - X (X&amp;rsquo;X)^{-1}X&amp;rsquo;y \big)
$$&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Pre-testing
def pre_testing(a, b, c, n, simulations=1000):
    np.random.seed(1)
    
    # Init
    alpha = {&#39;Long&#39;: np.zeros((simulations,1)),
            &#39;Short&#39;: np.zeros((simulations,1)),
            &#39;Pre-test&#39;: np.zeros((simulations,1))}

    # Loop over simulations
    for i in range(simulations):
        
        # Generate data
        x, y, z = generate_data(a, b, c, n)
        xz = np.concatenate([x,z], axis=1)
        
        # Compute coefficients
        alpha[&#39;Long&#39;][i] = (inv(xz.T @ xz) @ xz.T @ y)[0][0]
        alpha[&#39;Short&#39;][i] = inv(x.T @ x) @ x.T @ y
        
        # Compute significance of z on y
        t = t_test(y, xz, 1)
        
        # Select specification based on test
        if np.abs(t)&amp;gt;1.96:
            alpha[&#39;Pre-test&#39;][i] = alpha[&#39;Long&#39;][i]
        else:
            alpha[&#39;Pre-test&#39;][i] = alpha[&#39;Short&#39;][i]
    
    return alpha
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s compare the different estimates.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Get pre_test alpha
alpha = pre_testing(a, b, c, n)

for key, value in alpha.items():
    print(&#39;Mean alpha %s = %.4f&#39; % (key, np.mean(value)))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Mean alpha Long = 0.9994
Mean alpha Short = 0.9095
Mean alpha Pre-test = 0.9925
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The pre-testing coefficient is very close to the true coefficient.&lt;/p&gt;
&lt;p&gt;However, the main effect of pre-testing is on inference. With pre-testing, the distribution of the estimator is not gaussian anymore.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def plot_alpha(alpha, a):
    
    fig = plt.figure(figsize=(17,6))

    # Plot distributions
    x_max = np.max([np.max(np.abs(x-a)) for x in alpha.values()])

    # All axes
    for i, key in enumerate(alpha.keys()):
        
        # Reshape exisiting subplots
        k = len(fig.axes)
        for i in range(k):
            fig.axes[i].change_geometry(1, k+1, i+1)
            
        # Add new plot
        ax = fig.add_subplot(1, k+1, k+1)
        ax.hist(alpha[key], bins=30)
        ax.set_title(key)
        ax.set_xlim([a-x_max, a+x_max])
        ax.axvline(a, c=&#39;r&#39;, ls=&#39;--&#39;)
        legend_text = [r&#39;$\alpha_0=%.0f$&#39; % a, r&#39;$\hat \alpha=%.4f$&#39; % np.mean(alpha[key])]
        ax.legend(legend_text, prop={&#39;size&#39;: 10})
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s compare the long, short and pre-test estimators.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Plot
plot_alpha(alpha, a)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/09_postdoubleselection_36_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;As we can see, the main problem of pre-testing is inference.&lt;/p&gt;
&lt;p&gt;Because of the testing procedure, the distribution of the estimator is a combination of tho different distributions: the one resulting from the long regression and the one resulting from the short regression. &lt;strong&gt;Pre-testing is not a problem in 3 cases&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;when $\beta_0$ is very large: in this case the test always rejects the null hypothesis $H_0 : \beta_0=0$ and we always run the correct specification, i.e. the long regression&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;when $\beta_0$ is very small: in this case the test has very little power. However, as we saw from the Omitted Variable Bias formula, the bias is small.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;when $\gamma_0$ is very small: also in this case the test has very little power. However, as we saw from the Omitted Variable Bias formula, the bias is small.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Let&amp;rsquo;s compare the pre-test estimates for different values of the true parameter $\beta$.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Case 1: different betas and same sample size
b_sequence = b*np.array([0.1,0.3,1,3])
alpha = {}

# Get sequence
for k, b_ in enumerate(b_sequence):
    label = &#39;beta = %.2f&#39; % b_
    alpha[label] = pre_testing(a, b_, c, n)[&#39;Pre-test&#39;]
    print(&#39;Mean alpha with beta=%.2f: %.4f&#39; % (b_, np.mean(alpha[label])))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Mean alpha with beta=-0.03: 0.9926
Mean alpha with beta=-0.09: 0.9826
Mean alpha with beta=-0.30: 0.9925
Mean alpha with beta=-0.90: 0.9994
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The means are similar, but let&amp;rsquo;s look at the distributions.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Plot
plot_alpha(alpha, a)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/09_postdoubleselection_41_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;When $\beta_0$ is &amp;ldquo;small&amp;rdquo;, the distribution of the pre-testing estimator for $\alpha$ is not normal.&lt;/p&gt;
&lt;p&gt;However, the magnitue of $\beta_0$ is a relative concept. For an infinite sample size, $\beta_0$ is always going to be &amp;ldquo;big enough&amp;rdquo;, in the sense that with an infinite sample size the probability fo false positives in testing $H_0: \beta_0 = 0$ is going to zero. I.e. we always select the correct model specification, the long regression.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s have a look at the distibution of $\hat \alpha_{\text{PRE-TEST}}$ when the sample size increaes.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Case 2: same beta and different sample sizes
n_sequence = [100,300,1000,3000]
alpha = {}

# Get sequence
for k, n_ in enumerate(n_sequence):
    label = &#39;n = %.0f&#39; % n_
    alpha[label] = pre_testing(a, b, c, n_)[&#39;Pre-test&#39;]
    print(&#39;Mean alpha with n=%.0f: %.4f&#39; % (n_, np.mean(alpha[label])))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Mean alpha with n=100: 0.9442
Mean alpha with n=300: 0.9635
Mean alpha with n=1000: 0.9925
Mean alpha with n=3000: 0.9989
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Plot
plot_alpha(alpha, a)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/09_postdoubleselection_44_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;As we can see, for large samples, $\beta_0$ is never &amp;ldquo;small&amp;rdquo;. In the limit, when $n \to \infty$, the probability of false positives while testing $H_0: \beta_0 = 0$ goes to zero.&lt;/p&gt;
&lt;p&gt;We face a dilemma:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;pre-testing is clearlly a problem in finite samples&lt;/li&gt;
&lt;li&gt;all our econometric results are based on the assumption that $n \to \infty$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The problem is solved by assuming that the value of $\beta_0$ depends on the sample size. This might seems like a weird assumption but is just to have an asymptotically meaningful concept of &amp;ldquo;big&amp;rdquo; and &amp;ldquo;small&amp;rdquo;.&lt;/p&gt;
&lt;p&gt;We now look at what happens in the simulations when $\beta_0$ is proportional to $\frac{1}{\sqrt{n}}$.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Case 3: beta proportional to 1/sqrt(n) and different sample sizes
beta =  b * 30 / np.sqrt(n_sequence)

# Get sequence
alpha = {}
for k, n_ in enumerate(n_sequence):
    label = &#39;n = %.0f&#39; % n_
    alpha[label] = pre_testing(a, beta[k], c, n_)[&#39;Pre-test&#39;]
    print(&#39;Mean alpha with n=%.0f: %.4f&#39; % (n_, np.mean(alpha[label])))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Mean alpha with n=100: 0.9703
Mean alpha with n=300: 0.9838
Mean alpha with n=1000: 0.9914
Mean alpha with n=3000: 0.9947
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Plot
plot_alpha(alpha, a)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/09_postdoubleselection_48_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Now the distribution of $\hat \alpha$ does not converge to a normal when the sample size increases.&lt;/p&gt;
&lt;h3 id=&#34;pre-testing-and-machine-learning&#34;&gt;Pre-Testing and Machine Learning&lt;/h3&gt;
&lt;p&gt;How are machine learning and pre-testing related? The best example is Lasso. Suppose you have a dataset with many variables. This means that you have very few degrees of freedom and your OLS estimates are going to be very imprecise. At the extreme, you have more variables than observations so that your OLS coefficient is undefined since you cannot invert the design matrix $X&amp;rsquo;X$.&lt;/p&gt;
&lt;p&gt;In this case, you might want to do variable selection. One way of doing variable selection is pre-testing. Another way is Lasso. A third alternative is to use machine learning methods that do not suffer this curse of dimensionality.&lt;/p&gt;
&lt;p&gt;The purpose and outcome of pre-testing and Lasso are the same:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;you have too many variables&lt;/li&gt;
&lt;li&gt;you exclude some of them from the regression / set their coefficients to zero&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;As a consequence, also the problems are the same, i.e. pre-test bias.&lt;/p&gt;
&lt;h2 id=&#34;94-post-double-selection&#34;&gt;9.4 Post-Double Selection&lt;/h2&gt;
&lt;p&gt;Consider again data $D= (y_i, x_i, z_i)_{i=1}^n$, where the true model is:&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
&amp;amp; y_i = x_i&amp;rsquo; \alpha_0  + z_i&amp;rsquo; \beta_0 + \varepsilon_i \
&amp;amp; x_i = z_i&amp;rsquo; \gamma_0 + u_i
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;We would like to guard against pretest bias if possible, in order to handle high dimensional models. A good pathway towards motivating procedures which guard against pretest bias is a discussion of classical partitioned regression.&lt;/p&gt;
&lt;p&gt;Consider a regression $y_i$ on $x_i$ and $z_i$. $x_i$ is the 1-dimensional variable of interest, $z_i$ is a high-dimensional set of control variables. We have the following procedure:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;First Stage&lt;/strong&gt; selection: regress $x_i$ on $z_i$. Select the statistically significant variables in the set $S_{FS} \subseteq z_i$&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Reduced Form&lt;/strong&gt; selection: lasso $y_i$ on $z_i$. Select the statistically significant variables in the set $S_{RF} \subseteq z_i$&lt;/li&gt;
&lt;li&gt;Regress $y_i$ on $x_i$ and $S_{FS} \cup S_{RF}$&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt;:
Let ${P^n}$ be a sequence of data-generating processes for $D_n = (y_i, x_i, z_i)^n_{i=1} \in (\mathbb R \times \mathbb R \times \mathbb R^p) ^n$ where $p$ depends on $n$. For each $n$, the data are iid with $yi =  x_i&amp;rsquo;\alpha_0^{(n)} + z_i&amp;rsquo; \beta_0^{(n)} + \varepsilon_i$ and $x_i = z_i&amp;rsquo; \gamma_0^{(n)} + u_i$ where $\mathbb E[\varepsilon_i | x_i,z_i] = 0$ and $\mathbb E[u_i|z_i] = 0$. The sparsity of the vectors  $\beta_0^{(n)}$, $\gamma_0^{(n)}$ is controlled by $|| \beta_0^{(n)} ||_0 \leq s$ with $s^2 (\log p)^2/n \to 0$. Suppose that additional regularity conditions on the model selection procedures and moments of the random variables $y_i$ , $x_i$ , $z_i$ as documented in Belloni et al. (2014). Then the confidence intervals, CI, from the post double selection procedure are uniformly valid. That is, for any confidence level $\xi \in (0, 1)$
$$
\Pr(\alpha_0 \in CI) \to 1- \xi
$$&lt;/p&gt;
&lt;p&gt;In order to have valid confidence intervals you want their bias to be negligibly. Since
$$
CI = \left[ \hat{\alpha} \pm \frac{1.96 \cdot \hat{\sigma}}{\sqrt{n}} \right]
$$&lt;/p&gt;
&lt;p&gt;If the bias is $o \left( \frac{1}{\sqrt{n}} \right)$ then there is no problem since it is asymptotically negligible w.r.t. the magnitude of the confidence interval. If however the the bias is $O \left( \frac{1}{\sqrt{n}} \right)$ then it has the same magnitude of the confidence interval and it does not asymptotically vanish.&lt;/p&gt;
&lt;p&gt;The idea of the proof is to use partitioned regression. An alternative way to think about the argument is: bound the omitted variables bias. Omitted variable bias comes from the product of 2 quantities related to the omitted variable:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Its partial correlation with the outcome, and&lt;/li&gt;
&lt;li&gt;Its partial correlation with the variable of interest.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;If both those partial correlations are $O( \sqrt{\log p/n})$, then the omitted variables bias is $(s \times O( \sqrt{\log p/n})^2 = o \left( \frac{1}{\sqrt{n}} \right)$, provided $s^2 (\log p)^2/n \to 0$. Relative to the $ \frac{1}{\sqrt{n}} $ convergence rate, the omitted variables bias is negligible.&lt;/p&gt;
&lt;p&gt;In our omitted variable bias case, we want $| \beta_0 \gamma_0 | = o \left( \frac{1}{\sqrt{n}} \right)$.  Post-double selection guarantees that&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Reduced form&lt;/em&gt; selection (pre-testing): any &amp;ldquo;missing&amp;rdquo; variable has $|\beta_{0j}| \leq \frac{c}{\sqrt{n}}$&lt;/li&gt;
&lt;li&gt;&lt;em&gt;First stage&lt;/em&gt; selection (additional): any &amp;ldquo;missing&amp;rdquo; variable has $|\gamma_{0j}| \leq \frac{c}{\sqrt{n}}$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;As a consequence, as long as the number of omitted variables is finite, the omitted variable bias is
$$
OVB(\alpha) = |\beta_{0j}| \cdot|\gamma_{0j}| \leq \frac{c}{\sqrt{n}} \cdot \frac{c}{\sqrt{n}} = \frac{c^2}{n} = o \left(\frac{1}{\sqrt{n}}\right)
$$&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Pre-testing code
def post_double_selection(a, b, c, n, simulations=1000):
    np.random.seed(1)
    
    # Init
    alpha = {&#39;Long&#39;: np.zeros((simulations,1)),
            &#39;Short&#39;: np.zeros((simulations,1)),
            &#39;Pre-test&#39;: np.zeros((simulations,1)),
            &#39;Post-double&#39;: np.zeros((simulations,1))}

    # Loop over simulations
    for i in range(simulations):
        
        # Generate data
        x, y, z = generate_data(a, b, c, n)
        
        # Compute coefficients
        xz = np.concatenate([x,z], axis=1)
        alpha[&#39;Long&#39;][i] = (inv(xz.T @ xz) @ xz.T @ y)[0][0]
        alpha[&#39;Short&#39;][i] = inv(x.T @ x) @ x.T @ y
        
        # Compute significance of z on y (beta hat)
        t1 = t_test(y, xz, 1)
        
        # Compute significance of z on x (gamma hat)
        t2 = t_test(x, z, 0)
        
        # Select specification based on first test
        if np.abs(t1)&amp;gt;1.96:
            alpha[&#39;Pre-test&#39;][i] = alpha[&#39;Long&#39;][i]
        else:
            alpha[&#39;Pre-test&#39;][i] = alpha[&#39;Short&#39;][i]
            
        # Select specification based on both tests
        if np.abs(t1)&amp;gt;1.96 or np.abs(t2)&amp;gt;1.96:
            alpha[&#39;Post-double&#39;][i] = alpha[&#39;Long&#39;][i]
        else:
            alpha[&#39;Post-double&#39;][i] = alpha[&#39;Short&#39;][i]
    
    return alpha
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s now repeat the same exercise as above, but with also post-double selection&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Get pre_test alpha
alpha = post_double_selection(a, b, c, n)

for key, value in alpha.items():
    print(&#39;Mean alpha %s = %.4f&#39; % (key, np.mean(value)))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Mean alpha Long = 0.9994
Mean alpha Short = 0.9095
Mean alpha Pre-test = 0.9925
Mean alpha Post-double = 0.9994
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Plot
plot_alpha(alpha, a)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/09_postdoubleselection_62_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;As we can see, post-double selection has solved the pre-testing problem. Does it work for any magnitude of $\beta$ (relative to the sample size)?&lt;/p&gt;
&lt;p&gt;We first have a look at the case in which the sample size is fixed and $\beta_0$ changes.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Case 1: different betas and same sample size
b_sequence = b*np.array([0.1,0.3,1,3])
alpha = {}

# Get sequence
for k, b_ in enumerate(b_sequence):
    label = &#39;beta = %.2f&#39; % b_
    alpha[label] = post_double_selection(a, b_, c, n)[&#39;Post-double&#39;]
    print(&#39;Mean alpha with beta=%.2f: %.4f&#39; % (b_, np.mean(alpha[label])))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Mean alpha with beta=-0.03: 0.9994
Mean alpha with beta=-0.09: 0.9994
Mean alpha with beta=-0.30: 0.9994
Mean alpha with beta=-0.90: 0.9994
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Plot
plot_alpha(alpha, a)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/09_postdoubleselection_65_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Post-double selection always selects the correct specification, the long regression, even when $\beta$ is very small.&lt;/p&gt;
&lt;p&gt;Now we check the same but for fixed $\beta_0$ and different sample sizes.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Case 2: same beta and different sample sizes
n_sequence = [100,300,1000,3000]
alpha = {}

# Get sequence
for k, n_ in enumerate(n_sequence):
    label = &#39;N = %.0f&#39; % n_
    alpha[label] = post_double_selection(a, b, c, n_)[&#39;Post-double&#39;]
    print(&#39;Mean alpha with n=%.0f: %.4f&#39; % (n_, np.mean(alpha[label])))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Mean alpha with n=100: 0.9964
Mean alpha with n=300: 0.9985
Mean alpha with n=1000: 0.9994
Mean alpha with n=3000: 0.9990
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Plot
plot_alpha(alpha, a)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/09_postdoubleselection_69_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Post-double selection always selects the correct specification, the long regression, even when the sample size is very small.&lt;/p&gt;
&lt;p&gt;Last, we check the case of $\beta_0$ proportional to $\frac{1}{\sqrt{n}}$.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Case 3: beta proportional to 1/sqrt(n) and different sample sizes
beta =  b * 30 / np.sqrt(n_sequence)

# Get sequence
alpha = {}
for k, n_ in enumerate(n_sequence):
    label = &#39;N = %.0f&#39; % n_
    alpha[label] = post_double_selection(a, beta[k], c, n_)[&#39;Post-double&#39;]
    print(&#39;Mean alpha with n=%.0f: %.4f&#39; % (n_, np.mean(alpha[label])))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Mean alpha with n=100: 0.9964
Mean alpha with n=300: 0.9985
Mean alpha with n=1000: 0.9994
Mean alpha with n=3000: 0.9990
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Plot
plot_alpha(alpha, a)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/09_postdoubleselection_73_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Once again post-double selection always selects the correct specification, the long regression.&lt;/p&gt;
&lt;h3 id=&#34;post-double-selection-and-machine-learning&#34;&gt;Post-double Selection and Machine Learning&lt;/h3&gt;
&lt;p&gt;As we have seen at the end of the previous section, Lasso can be used to perform variable selection in high dimensional settings. Therefore, post-double selection solves the pre-test bias problem in those settings. The post-double selection procedure with Lasso is:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;First Stage&lt;/strong&gt; selection: lasso $x_i$ on $z_i$. Let the selected variables be collected in the set $S_{FS} \subseteq z_i$&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Reduced Form&lt;/strong&gt; selection: lasso $y_i$ on $z_i$. Let the selected variables be collected in the set $S_{RF} \subseteq z_i$&lt;/li&gt;
&lt;li&gt;Regress $y_i$ on $x_i$ and $S_{FS} \cup S_{RF}$&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;95-doubledebiased-machine-learning&#34;&gt;9.5 Double/debiased Machine Learning&lt;/h2&gt;
&lt;p&gt;This section is taken from &lt;a href=&#34;https://onlinelibrary.wiley.com/doi/abs/10.1111/ectj.12097&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Chernozhukov, V., Chetverikov, D., Demirer, M., Duflo, E., Hansen, C., Newey, W., &amp;amp; Robins, J. (2018). &amp;ldquo;&lt;em&gt;Double/debiased machine learning for treatment and structural parameters&lt;/em&gt;&amp;rdquo;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Consider the following partially linear model&lt;/p&gt;
&lt;p&gt;$$
y = \beta_0 D + g_0(X) + u \
D = m_0(X) + v
$$&lt;/p&gt;
&lt;p&gt;where $y$ is the outcome variable, $D$ is the treatment to interest and $X$ is a potentially high-dimensional set of controls.&lt;/p&gt;
&lt;h4 id=&#34;naive-approach&#34;&gt;Naive approach&lt;/h4&gt;
&lt;p&gt;A naive approach to estimation of $\beta_0$ using ML methods would be, for example, to construct a sophisticated ML estimator $\beta_0 D + g_0(X)$ for learning the regression function $\beta_0 D$ + $g_0(X)$.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Split the sample in two: main sample and auxiliary sample&lt;/li&gt;
&lt;li&gt;Use the auxiliary sample to estimate $\hat g_0(X)$&lt;/li&gt;
&lt;li&gt;Use the main sample to compute the orthogonalized component of $Y$ on $X$: $\hat u = \left(Y_{i}-\hat{g}&lt;em&gt;{0}\left(X&lt;/em&gt;{i}\right)\right)$&lt;/li&gt;
&lt;li&gt;Use the main sample to estimate the residualized OLS estimator&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;$$
\hat{\beta}&lt;em&gt;{0}=\left(\frac{1}{n} \sum&lt;/em&gt;{i \in I} D_{i}^{2}\right)^{-1} \frac{1}{n} \sum_{i \in I} D_{i} \hat u_i
$$&lt;/p&gt;
&lt;p&gt;This estimator is going to have two problems:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Slow rate of convergence, i.e. slower than $\sqrt(n)$&lt;/li&gt;
&lt;li&gt;It will be biased because we are employing highdimensional regularized estimators (e.g. we are doing variable selection)&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;orthogonalization&#34;&gt;Orthogonalization&lt;/h4&gt;
&lt;p&gt;Now consider a second construction that employs an orthogonalized formulation obtained by directly partialling out the effect of $X$ from $D$ to obtain the orthogonalized regressor $v = D − m_0(X)$.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Split the sample in two: main sample and auxiliary sample&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Use the auxiliary sample to estimate $\hat g_0(X)$ from&lt;/p&gt;
&lt;p&gt;$$
y = \beta_0 D + g_0(X) + u \
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Use the auxiliary sample to estimate $\hat m_0(X)$ from&lt;/p&gt;
&lt;p&gt;$$
D = m_0(X) + v
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Use the main sample to compute the orthogonalized component of $D$ on $X$ as&lt;/p&gt;
&lt;p&gt;$$
\hat v = D - \hat m_0(X)
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Use the main sample to estimate the double-residualized OLS estimator as&lt;/p&gt;
&lt;p&gt;$$
\hat{\beta}&lt;em&gt;{0}=\left(\frac{1}{n} \sum&lt;/em&gt;{i \in I} \hat v_i D_{i} \right)^{-1} \frac{1}{n} \sum_{i \in I} \hat v_i \left( Y - \hat g_0(X) \right)
$$&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The estimator is unbiased but still has a lower rate of convergence because of sample splitting. The problem is solved by inverting the split sample, re-estimating the coefficient and averaging the two estimates. Note that this procedure is valid since the two estimates are independent by the sample splitting procedure.&lt;/p&gt;
&lt;h3 id=&#34;application-to-ajr02&#34;&gt;Application to AJR02&lt;/h3&gt;
&lt;p&gt;In this section we are going to replicate 6.3 of the &amp;ldquo;&lt;em&gt;Double/debiased machine learning&lt;/em&gt;&amp;rdquo; paper based on &lt;a href=&#34;https://economics.mit.edu/files/4123&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Acemoglu, Johnson, Robinson (2002), &amp;ldquo;&lt;em&gt;The Colonial Origins of Comparative Development&lt;/em&gt;&amp;rdquo;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;We first load the dataset&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Load Acemoglu Johnson Robinson Dataset
df = pd.read_csv(&#39;data/AJR02.csv&#39;,index_col=0)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;GDP&lt;/th&gt;
      &lt;th&gt;Exprop&lt;/th&gt;
      &lt;th&gt;Mort&lt;/th&gt;
      &lt;th&gt;Latitude&lt;/th&gt;
      &lt;th&gt;Neo&lt;/th&gt;
      &lt;th&gt;Africa&lt;/th&gt;
      &lt;th&gt;Asia&lt;/th&gt;
      &lt;th&gt;Namer&lt;/th&gt;
      &lt;th&gt;Samer&lt;/th&gt;
      &lt;th&gt;logMort&lt;/th&gt;
      &lt;th&gt;Latitude2&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;8.39&lt;/td&gt;
      &lt;td&gt;6.50&lt;/td&gt;
      &lt;td&gt;78.20&lt;/td&gt;
      &lt;td&gt;0.3111&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;4.359270&lt;/td&gt;
      &lt;td&gt;0.096783&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;7.77&lt;/td&gt;
      &lt;td&gt;5.36&lt;/td&gt;
      &lt;td&gt;280.00&lt;/td&gt;
      &lt;td&gt;0.1367&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;5.634790&lt;/td&gt;
      &lt;td&gt;0.018687&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;9.13&lt;/td&gt;
      &lt;td&gt;6.39&lt;/td&gt;
      &lt;td&gt;68.90&lt;/td&gt;
      &lt;td&gt;0.3778&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;4.232656&lt;/td&gt;
      &lt;td&gt;0.142733&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;9.90&lt;/td&gt;
      &lt;td&gt;9.32&lt;/td&gt;
      &lt;td&gt;8.55&lt;/td&gt;
      &lt;td&gt;0.3000&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;2.145931&lt;/td&gt;
      &lt;td&gt;0.090000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;5&lt;/th&gt;
      &lt;td&gt;9.29&lt;/td&gt;
      &lt;td&gt;7.50&lt;/td&gt;
      &lt;td&gt;85.00&lt;/td&gt;
      &lt;td&gt;0.2683&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;4.442651&lt;/td&gt;
      &lt;td&gt;0.071985&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df.info()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;class &#39;pandas.core.frame.DataFrame&#39;&amp;gt;
Int64Index: 64 entries, 1 to 64
Data columns (total 11 columns):
 #   Column     Non-Null Count  Dtype  
---  ------     --------------  -----  
 0   GDP        64 non-null     float64
 1   Exprop     64 non-null     float64
 2   Mort       64 non-null     float64
 3   Latitude   64 non-null     float64
 4   Neo        64 non-null     int64  
 5   Africa     64 non-null     int64  
 6   Asia       64 non-null     int64  
 7   Namer      64 non-null     int64  
 8   Samer      64 non-null     int64  
 9   logMort    64 non-null     float64
 10  Latitude2  64 non-null     float64
dtypes: float64(6), int64(5)
memory usage: 6.0 KB
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In their paper, AJR note that their IV strategy will be invalidated if other factors are also highly persistent and related to the development of institutions within a country and to the country’s GDP. A leading candidate for such a factor, as they discuss, is geography. AJR address this by assuming that the confounding effect of geography is adequately captured by a linear term in distance from the equator and a set of continent dummy variables.&lt;/p&gt;
&lt;p&gt;They inclue their results in table 2.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Add constant term to dataset
df[&#39;const&#39;] = 1

# Create lists of variables to be used in each regression
X1 = df[[&#39;const&#39;, &#39;Exprop&#39;]]
X2 = df[[&#39;const&#39;, &#39;Exprop&#39;, &#39;Latitude&#39;, &#39;Latitude2&#39;]]
X3 = df[[&#39;const&#39;, &#39;Exprop&#39;, &#39;Latitude&#39;, &#39;Latitude2&#39;, &#39;Asia&#39;, &#39;Africa&#39;, &#39;Namer&#39;, &#39;Samer&#39;]]
y = df[&#39;GDP&#39;]

# Estimate an OLS regression for each set of variables
reg1 = sm.OLS(y, X1, missing=&#39;drop&#39;).fit()
reg2 = sm.OLS(y, X2, missing=&#39;drop&#39;).fit()
reg3 = sm.OLS(y, X3, missing=&#39;drop&#39;).fit()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s replicate Table 2 in AJR.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Make table 2
def make_table_2():

    info_dict={&#39;No. observations&#39; : lambda x: f&amp;quot;{int(x.nobs):d}&amp;quot;}

    results_table = summary_col(results=[reg1,reg2,reg3],
                                float_format=&#39;%0.2f&#39;,
                                stars = True,
                                model_names=[&#39;Model 1&#39;,&#39;Model 2&#39;,&#39;Model 3&#39;],
                                info_dict=info_dict,
                                regressor_order=[&#39;const&#39;,&#39;Exprop&#39;,&#39;Latitude&#39;,&#39;Latitude2&#39;])
    return results_table
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;table_2 = make_table_2()
table_2
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
          &lt;td&gt;&lt;/td&gt;         &lt;th&gt;Model 1&lt;/th&gt; &lt;th&gt;Model 2&lt;/th&gt; &lt;th&gt;Model 3&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;const&lt;/th&gt;            &lt;td&gt;4.66***&lt;/td&gt; &lt;td&gt;4.55***&lt;/td&gt; &lt;td&gt;5.95***&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;&lt;/th&gt;                 &lt;td&gt;(0.41)&lt;/td&gt;  &lt;td&gt;(0.45)&lt;/td&gt;  &lt;td&gt;(0.68)&lt;/td&gt; 
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Exprop&lt;/th&gt;           &lt;td&gt;0.52***&lt;/td&gt; &lt;td&gt;0.49***&lt;/td&gt; &lt;td&gt;0.40***&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;&lt;/th&gt;                 &lt;td&gt;(0.06)&lt;/td&gt;  &lt;td&gt;(0.07)&lt;/td&gt;  &lt;td&gt;(0.06)&lt;/td&gt; 
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Latitude&lt;/th&gt;            &lt;td&gt;&lt;/td&gt;      &lt;td&gt;2.16&lt;/td&gt;    &lt;td&gt;0.42&lt;/td&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;&lt;/th&gt;                    &lt;td&gt;&lt;/td&gt;     &lt;td&gt;(1.68)&lt;/td&gt;  &lt;td&gt;(1.47)&lt;/td&gt; 
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Latitude2&lt;/th&gt;           &lt;td&gt;&lt;/td&gt;      &lt;td&gt;-2.12&lt;/td&gt;   &lt;td&gt;0.44&lt;/td&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;&lt;/th&gt;                    &lt;td&gt;&lt;/td&gt;     &lt;td&gt;(2.86)&lt;/td&gt;  &lt;td&gt;(2.48)&lt;/td&gt; 
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Africa&lt;/th&gt;              &lt;td&gt;&lt;/td&gt;        &lt;td&gt;&lt;/td&gt;     &lt;td&gt;-1.06**&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;&lt;/th&gt;                    &lt;td&gt;&lt;/td&gt;        &lt;td&gt;&lt;/td&gt;     &lt;td&gt;(0.41)&lt;/td&gt; 
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Asia&lt;/th&gt;                &lt;td&gt;&lt;/td&gt;        &lt;td&gt;&lt;/td&gt;     &lt;td&gt;-0.74*&lt;/td&gt; 
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;&lt;/th&gt;                    &lt;td&gt;&lt;/td&gt;        &lt;td&gt;&lt;/td&gt;     &lt;td&gt;(0.42)&lt;/td&gt; 
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Namer&lt;/th&gt;               &lt;td&gt;&lt;/td&gt;        &lt;td&gt;&lt;/td&gt;      &lt;td&gt;-0.17&lt;/td&gt; 
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;&lt;/th&gt;                    &lt;td&gt;&lt;/td&gt;        &lt;td&gt;&lt;/td&gt;     &lt;td&gt;(0.40)&lt;/td&gt; 
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Samer&lt;/th&gt;               &lt;td&gt;&lt;/td&gt;        &lt;td&gt;&lt;/td&gt;      &lt;td&gt;-0.12&lt;/td&gt; 
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;&lt;/th&gt;                    &lt;td&gt;&lt;/td&gt;        &lt;td&gt;&lt;/td&gt;     &lt;td&gt;(0.42)&lt;/td&gt; 
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;No. observations&lt;/th&gt;   &lt;td&gt;64&lt;/td&gt;      &lt;td&gt;64&lt;/td&gt;      &lt;td&gt;64&lt;/td&gt;   
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;Using DML allows us to relax this assumption and to replace it by a weaker assumption that geography can be sufficiently controlled by an unknown function of distance from the equator and continent dummies, which can be learned by ML methods.&lt;/p&gt;
&lt;p&gt;In particular, our framework is&lt;/p&gt;
&lt;p&gt;$$
{GDP} = \beta_0 \times {Exprop} + g_0({geography}) + u \
{Exprop} = m_0({geography}) + u
$$&lt;/p&gt;
&lt;p&gt;So that the double/debiased machine learning procedure is&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Split the sample in two: main sample and auxiliary sample&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Use the auxiliary sample to estimate $\hat g_0({geography})$ from&lt;/p&gt;
&lt;p&gt;$$
{GDP} = \beta_0 \times {Exprop} + g_0({geography}) + u
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Use the auxiliary sample to estimate $\hat m_0({geography})$ from&lt;/p&gt;
&lt;p&gt;$$
{Exprop} = m_0({geography}) + v
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Use the main sample to compute the orthogonalized component of ${Exprop}$ on ${geography}$ as&lt;/p&gt;
&lt;p&gt;$$
\hat v = {Exprop} - \hat m_0({geography})
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Use the main sample to estimate the double-residualized OLS estimator as&lt;/p&gt;
&lt;p&gt;$$
\hat{\beta}&lt;em&gt;{0}=\left(\frac{1}{n} \sum&lt;/em&gt;{i \in I} \hat v_i \times {Exprop}&lt;em&gt;{i} \right)^{-1} \frac{1}{n} \sum&lt;/em&gt;{i \in I} \hat v_i \times \left( {GDP} - \hat g_0({geography}) \right)
$$&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Since we employ an &lt;strong&gt;intrumental variable&lt;/strong&gt; strategy, we replace $m_0({geography})$ with $m_0({geography},{logMort})$ in the first stage.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Generate variables
D = df[&#39;Exprop&#39;].values.reshape(-1,1)
X = df[[&#39;const&#39;, &#39;Latitude&#39;, &#39;Latitude2&#39;, &#39;Asia&#39;, &#39;Africa&#39;, &#39;Namer&#39;, &#39;Samer&#39;]].values
y = df[&#39;GDP&#39;].values.reshape(-1,1)
Z = df[[&#39;const&#39;, &#39;Latitude&#39;, &#39;Latitude2&#39;, &#39;Asia&#39;, &#39;Africa&#39;, &#39;Namer&#39;, &#39;Samer&#39;,&#39;logMort&#39;]].values
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def estimate_beta(algorithm, alg_name, D, X, y, Z, sample):

    # Split sample
    D_main, D_aux = (D[sample==1], D[sample==0])
    X_main, X_aux = (X[sample==1], X[sample==0])
    y_main, y_aux = (y[sample==1], y[sample==0])
    Z_main, Z_aux = (Z[sample==1], Z[sample==0])

    # Residualize y on D
    b_hat = inv(D_aux.T @ D_aux) @ D_aux.T @ y_aux
    y_resid_aux = y_aux - D_aux @ b_hat
    
    # Estimate g0
    alg_fitted = algorithm.fit(X=X_aux, y=y_resid_aux.ravel())
    g0 = alg_fitted.predict(X_main).reshape(-1,1)

    # Compute v_hat
    u_hat = y_main - g0

    # Estimate m0
    alg_fitted = algorithm.fit(X=Z_aux, y=D_aux.ravel())
    m0 = algorithm.predict(Z_main).reshape(-1,1)
    
    # Compute u_hat
    v_hat = D_main - m0

    # Estimate beta
    beta = inv(v_hat.T @ D_main) @ v_hat.T @ u_hat
        
    return beta 
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def ddml(algorithm, alg_name, D, X, y, Z, p=0.5, verbose=False):
    
    # Expand X if Lasso or Ridge
    if alg_name in [&#39;Lasso   &#39;,&#39;Ridge   &#39;]:
        X = PolynomialFeatures(degree=2).fit_transform(X)

    # Generate split (fixed proportions)
    split = np.array([i in train_test_split(range(len(D)), test_size=p)[0] for i in range(len(D))])
    
    # Compute beta
    beta = [estimate_beta(algorithm, alg_name, D, X, y, Z, split==k) for k in range(2)]
    beta = np.mean(beta)
     
    # Print and return
    if verbose:
        print(&#39;%s : %.4f&#39; % (alg_name, beta))
    return beta
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Generate sample split
p = 0.5
split = np.random.binomial(1, p, len(D))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We inspect different algorithms. In particular, we consider:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Lasso Regression&lt;/li&gt;
&lt;li&gt;Ridge Regression&lt;/li&gt;
&lt;li&gt;Regression Trees&lt;/li&gt;
&lt;li&gt;Random Forest&lt;/li&gt;
&lt;li&gt;Boosted Forests&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# List all algorithms
algorithms = {&#39;Ridge   &#39;: Ridge(alpha=.1),
              &#39;Lasso   &#39;: Lasso(alpha=.01),
              &#39;Tree    &#39;: DecisionTreeRegressor(),
              &#39;Forest  &#39;: RandomForestRegressor(n_estimators=30),
              &#39;Boosting&#39;: GradientBoostingRegressor(n_estimators=30)}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s compare the results.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Loop over algorithms
for alg_name, algorithm in algorithms.items():
    ddml(algorithm, alg_name, D, X, y, Z, verbose=True)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Ridge    : 0.1289
Lasso    : -8.7963
Tree     : 1.2879
Forest   : 2.4938
Boosting : 0.5977
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The results are extremely volatile.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Repeat K times
def estimate_beta_median(algorithms, D, X, y, Z, K):
    
    # Loop over algorithms
    for alg_name, algorithm in algorithms.items():
        betas = []
            
        # Iterate n times
        for k in range(K):
            beta = ddml(algorithm, alg_name, D, X, y, Z)
            betas = np.append(betas, beta)
    
        print(&#39;%s : %.4f&#39; % (alg_name, np.median(betas)))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s try using the median to have a more stable estimator.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;np.random.seed(123)

# Repeat 100 times and take median
estimate_beta_median(algorithms, D, X, y, Z, 100)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Ridge    : 0.6670
Lasso    : 1.2511
Tree     : 0.9605
Forest   : 0.5327
Boosting : 1.0327
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The results differ slightly from the ones in the paper, but they are at least closer.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Unsupervised Learning</title>
      <link>https://matteocourthoud.github.io/course/ml-econ/10_unsupervised/</link>
      <pubDate>Wed, 09 Mar 2022 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/course/ml-econ/10_unsupervised/</guid>
      <description>&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Setup
from utils.lecture10 import *
%matplotlib inline
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;supervised-vs-unsupervised-learning&#34;&gt;Supervised vs Unsupervised Learning&lt;/h3&gt;
&lt;p&gt;The difference between &lt;em&gt;supervised learning&lt;/em&gt; and &lt;em&gt;unsupervised learning&lt;/em&gt; is that in the first case we have a variable $y$ which we want to predict, given a set of variables ${ X_1, . . . , X_p }$.&lt;/p&gt;
&lt;p&gt;In &lt;em&gt;unsupervised learning&lt;/em&gt; are not interested in prediction, because we do not have an associated response variable $y$. Rather, the goal is to discover interesting properties about the measurements on ${ X_1, . . . , X_p }$.&lt;/p&gt;
&lt;p&gt;Questions that we are usually interested in are&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Clustering&lt;/li&gt;
&lt;li&gt;Dimensionality reduction&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In general, unsupervised learning can be viewed as an extention of exploratory data analysis.&lt;/p&gt;
&lt;h3 id=&#34;dimensionality-reduction&#34;&gt;Dimensionality Reduction&lt;/h3&gt;
&lt;p&gt;Working in high-dimensional spaces can be undesirable for many reasons; raw data are often sparse as a consequence of the curse of dimensionality, and analyzing the data is usually computationally intractable (hard to control or deal with).&lt;/p&gt;
&lt;p&gt;Dimensionality reduction can also be useful to plot high-dimensional data.&lt;/p&gt;
&lt;h3 id=&#34;clustering&#34;&gt;Clustering&lt;/h3&gt;
&lt;p&gt;Clustering refers to a very broad set of techniques for finding subgroups, or clusters, in a data set. When we cluster the observations of a data set, we seek to partition them into distinct groups so that the observations within each group are quite similar to each other, while observations in different groups are quite different from each other.&lt;/p&gt;
&lt;p&gt;In this section we focus on the following algorithms:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;K-means clustering&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Hierarchical clustering&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Gaussian Mixture Models&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;principal-component-analysis&#34;&gt;Principal Component Analysis&lt;/h2&gt;
&lt;p&gt;Suppose that we wish to visualize $n$ observations with measurements on a set of $p$ features, ${X_1, . . . , X_p}$, as part of an exploratory data analysis.&lt;/p&gt;
&lt;p&gt;We could do this by examining two-dimensional scatterplots of the data, each of which contains the n observations’ measurements on two of the features. However, there are $p(p−1)/2$ such scatterplots; for example,
with $p = 10$ there are $45$ plots!&lt;/p&gt;
&lt;p&gt;PCA provides a tool to do just this. It finds a low-dimensional represen- tation of a data set that contains as much as possible of the variation.&lt;/p&gt;
&lt;h3 id=&#34;first-principal-component&#34;&gt;First Principal Component&lt;/h3&gt;
&lt;p&gt;The &lt;strong&gt;first principal component&lt;/strong&gt; of a set of features ${X_1, . . . , X_p}$ is the normalized linear combination of the features $Z_1$&lt;/p&gt;
&lt;p&gt;$$
Z_1 = \phi_{11} X_1 + \phi_{21} X_2 + &amp;hellip; + \phi_{p1} X_p
$$&lt;/p&gt;
&lt;p&gt;that has the largest variance.&lt;/p&gt;
&lt;p&gt;By normalized, we mean that $\sum_{i=1}^p \phi^2_{i1} = 1$.&lt;/p&gt;
&lt;h3 id=&#34;pca-computation&#34;&gt;PCA Computation&lt;/h3&gt;
&lt;p&gt;In other words, the first principal component loading vector solves the optimization problem&lt;/p&gt;
&lt;p&gt;$$
\underset{\phi_{11}, \ldots, \phi_{p 1}}{\max} \ \Bigg \lbrace \frac{1}{n} \sum _ {i=1}^{n}\left(\sum _ {j=1}^{p} \phi _ {j1} x _ {ij} \right)^{2} \Bigg \rbrace \quad \text { subject to } \quad \sum _ {j=1}^{p} \phi _ {j1}^{2}=1
$$&lt;/p&gt;
&lt;p&gt;The objective that we are maximizing is just the sample variance of the $n$ values of $z_{i1}$.&lt;/p&gt;
&lt;p&gt;After the first principal component $Z_1$ of the features has been determined, we can find the second principal component $Z_2$. The &lt;strong&gt;second principal component&lt;/strong&gt; is the linear combination of ${X_1, . . . , X_p}$ that has maximal variance out of all linear combinations that are &lt;em&gt;uncorrelated&lt;/em&gt; with $Z_1$.&lt;/p&gt;
&lt;h3 id=&#34;example&#34;&gt;Example&lt;/h3&gt;
&lt;p&gt;We illustrate the use of PCA on the &lt;code&gt;USArrests&lt;/code&gt; data set.&lt;/p&gt;
&lt;p&gt;For each of the 50 states in the United States, the data set contains the number of arrests per $100,000$ residents for each of three crimes: &lt;code&gt;Assault&lt;/code&gt;, &lt;code&gt;Murder&lt;/code&gt;, and &lt;code&gt;Rape.&lt;/code&gt; We also record the percent of the population in each state living in urban areas, &lt;code&gt;UrbanPop&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Load crime data
df = pd.read_csv(&#39;data/USArrests.csv&#39;, index_col=0)
df.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;Murder&lt;/th&gt;
      &lt;th&gt;Assault&lt;/th&gt;
      &lt;th&gt;UrbanPop&lt;/th&gt;
      &lt;th&gt;Rape&lt;/th&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;State&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;Alabama&lt;/th&gt;
      &lt;td&gt;13.2&lt;/td&gt;
      &lt;td&gt;236&lt;/td&gt;
      &lt;td&gt;58&lt;/td&gt;
      &lt;td&gt;21.2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;Alaska&lt;/th&gt;
      &lt;td&gt;10.0&lt;/td&gt;
      &lt;td&gt;263&lt;/td&gt;
      &lt;td&gt;48&lt;/td&gt;
      &lt;td&gt;44.5&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;Arizona&lt;/th&gt;
      &lt;td&gt;8.1&lt;/td&gt;
      &lt;td&gt;294&lt;/td&gt;
      &lt;td&gt;80&lt;/td&gt;
      &lt;td&gt;31.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;Arkansas&lt;/th&gt;
      &lt;td&gt;8.8&lt;/td&gt;
      &lt;td&gt;190&lt;/td&gt;
      &lt;td&gt;50&lt;/td&gt;
      &lt;td&gt;19.5&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;California&lt;/th&gt;
      &lt;td&gt;9.0&lt;/td&gt;
      &lt;td&gt;276&lt;/td&gt;
      &lt;td&gt;91&lt;/td&gt;
      &lt;td&gt;40.6&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;h3 id=&#34;data-scaling&#34;&gt;Data Scaling&lt;/h3&gt;
&lt;p&gt;To make all the features comparable, we first need to scale them. In this case, we use the &lt;code&gt;sklearn.preprocessing.scale()&lt;/code&gt; function to normalize each variable to have zero mean and unit variance.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Scale data
X_scaled = pd.DataFrame(scale(df), index=df.index, columns=df.columns).values
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We will see later what are the practical implications of (not) scaling.&lt;/p&gt;
&lt;h3 id=&#34;fitting&#34;&gt;Fitting&lt;/h3&gt;
&lt;p&gt;Let&amp;rsquo;s fit PCA with 2 components.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Fit PCA with 2 components
pca2 = PCA(n_components=2).fit(X_scaled)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Get weights
weights = pca2.components_.T
df_weights = pd.DataFrame(weights, index=df.columns, columns=[&#39;PC1&#39;, &#39;PC2&#39;])
df_weights
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;PC1&lt;/th&gt;
      &lt;th&gt;PC2&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;Murder&lt;/th&gt;
      &lt;td&gt;0.535899&lt;/td&gt;
      &lt;td&gt;0.418181&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;Assault&lt;/th&gt;
      &lt;td&gt;0.583184&lt;/td&gt;
      &lt;td&gt;0.187986&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;UrbanPop&lt;/th&gt;
      &lt;td&gt;0.278191&lt;/td&gt;
      &lt;td&gt;-0.872806&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;Rape&lt;/th&gt;
      &lt;td&gt;0.543432&lt;/td&gt;
      &lt;td&gt;-0.167319&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;h3 id=&#34;projecting-the-data&#34;&gt;Projecting the data&lt;/h3&gt;
&lt;p&gt;What does the trasformed data looks like?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Transform X to get the principal components
X_dim2 = pca2.transform(X_scaled)
df_dim2 = pd.DataFrame(X_dim2, columns=[&#39;PC1&#39;, &#39;PC2&#39;], index=df.index)
df_dim2.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;PC1&lt;/th&gt;
      &lt;th&gt;PC2&lt;/th&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;State&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;Alabama&lt;/th&gt;
      &lt;td&gt;0.985566&lt;/td&gt;
      &lt;td&gt;1.133392&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;Alaska&lt;/th&gt;
      &lt;td&gt;1.950138&lt;/td&gt;
      &lt;td&gt;1.073213&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;Arizona&lt;/th&gt;
      &lt;td&gt;1.763164&lt;/td&gt;
      &lt;td&gt;-0.745957&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;Arkansas&lt;/th&gt;
      &lt;td&gt;-0.141420&lt;/td&gt;
      &lt;td&gt;1.119797&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;California&lt;/th&gt;
      &lt;td&gt;2.523980&lt;/td&gt;
      &lt;td&gt;-1.542934&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;h3 id=&#34;visualization&#34;&gt;Visualization&lt;/h3&gt;
&lt;p&gt;The advantage og PCA is that it allows us to see the variation in lower dimesions.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;make_figure_10_1a(df_dim2, df_weights)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/10_unsupervised_30_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;pca-and-spectral-analysis&#34;&gt;PCA and Spectral Analysis&lt;/h3&gt;
&lt;p&gt;In case you haven&amp;rsquo;t noticed, calculating principal components, is equivalent to calculating the eigenvectors of the design matrix $X&amp;rsquo;X$, i.e. the variance-covariance matrix of $X$. Indeed what we performed above is a decomposition of the variance of $X$ into orthogonal components.&lt;/p&gt;
&lt;p&gt;The constrained maximization problem above can be re-written in matrix notation as&lt;/p&gt;
&lt;p&gt;$$
\max \ \phi&amp;rsquo; X&amp;rsquo;X \phi \quad \text{ s. t. } \quad \phi&amp;rsquo;\phi = 1
$$&lt;/p&gt;
&lt;p&gt;Which has the following dual representation&lt;/p&gt;
&lt;p&gt;$$
\mathcal L (\phi, \lambda) = \phi&amp;rsquo; X&amp;rsquo;X \phi - \lambda (\phi&amp;rsquo;\phi - 1)
$$&lt;/p&gt;
&lt;p&gt;If we take the first order conditions&lt;/p&gt;
&lt;p&gt;$$
\begin{align}
&amp;amp; \frac{\partial \mathcal L}{\partial \lambda} = \phi&amp;rsquo;\phi - 1 \
&amp;amp; \frac{\partial \mathcal L}{\partial \phi} = 2 X&amp;rsquo;X \phi - 2 \lambda \phi
\end{align}
$$&lt;/p&gt;
&lt;p&gt;Setting the derivatives to zero at the optimum, we get&lt;/p&gt;
&lt;p&gt;$$
\begin{align}
&amp;amp; \phi&amp;rsquo;\phi = 1 \
&amp;amp; X&amp;rsquo;X \phi = \lambda \phi
\end{align}
$$&lt;/p&gt;
&lt;p&gt;Thus, $\phi$ is an &lt;strong&gt;eigenvector&lt;/strong&gt; of the covariance matrix $X&amp;rsquo;X$, and the maximizing vector will be the one associated with the largest &lt;strong&gt;eigenvalue&lt;/strong&gt; $\lambda$.&lt;/p&gt;
&lt;h3 id=&#34;eigenvalues-and-eigenvectors&#34;&gt;Eigenvalues and eigenvectors&lt;/h3&gt;
&lt;p&gt;We can now double-check it using &lt;code&gt;numpy&lt;/code&gt; linear algebra package.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;eigenval, eigenvec = np.linalg.eig(X_scaled.T @ X_scaled)
data = np.concatenate((eigenvec,eigenval.reshape(1,-1)))
idx = list(df.columns) + [&#39;Eigenvalue&#39;]
df_eigen = pd.DataFrame(data, index=idx, columns=[&#39;PC1&#39;, &#39;PC2&#39;,&#39;PC3&#39;,&#39;PC4&#39;])

df_eigen
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;PC1&lt;/th&gt;
      &lt;th&gt;PC2&lt;/th&gt;
      &lt;th&gt;PC3&lt;/th&gt;
      &lt;th&gt;PC4&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;Murder&lt;/th&gt;
      &lt;td&gt;0.535899&lt;/td&gt;
      &lt;td&gt;0.418181&lt;/td&gt;
      &lt;td&gt;0.649228&lt;/td&gt;
      &lt;td&gt;-0.341233&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;Assault&lt;/th&gt;
      &lt;td&gt;0.583184&lt;/td&gt;
      &lt;td&gt;0.187986&lt;/td&gt;
      &lt;td&gt;-0.743407&lt;/td&gt;
      &lt;td&gt;-0.268148&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;UrbanPop&lt;/th&gt;
      &lt;td&gt;0.278191&lt;/td&gt;
      &lt;td&gt;-0.872806&lt;/td&gt;
      &lt;td&gt;0.133878&lt;/td&gt;
      &lt;td&gt;-0.378016&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;Rape&lt;/th&gt;
      &lt;td&gt;0.543432&lt;/td&gt;
      &lt;td&gt;-0.167319&lt;/td&gt;
      &lt;td&gt;0.089024&lt;/td&gt;
      &lt;td&gt;0.817778&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;Eigenvalue&lt;/th&gt;
      &lt;td&gt;124.012079&lt;/td&gt;
      &lt;td&gt;49.488258&lt;/td&gt;
      &lt;td&gt;8.671504&lt;/td&gt;
      &lt;td&gt;17.828159&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;The spectral decomposition of the variance of $X$ generates a set of orthogonal vectors (eigenvectors) with different magnitudes (eigenvalues). The eigenvalues tell us the amount of variance of the data in that direction.&lt;/p&gt;
&lt;p&gt;If we combine the eigenvectors together, we form a projection matrix $P$ that we can use to transform the original variables: $\tilde X = P X$&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;X_transformed = X_scaled @ eigenvec
df_transformed = pd.DataFrame(X_transformed, index=df.index, columns=[&#39;PC1&#39;, &#39;PC2&#39;,&#39;PC3&#39;,&#39;PC4&#39;])

df_transformed.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;PC1&lt;/th&gt;
      &lt;th&gt;PC2&lt;/th&gt;
      &lt;th&gt;PC3&lt;/th&gt;
      &lt;th&gt;PC4&lt;/th&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;State&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;Alabama&lt;/th&gt;
      &lt;td&gt;0.985566&lt;/td&gt;
      &lt;td&gt;1.133392&lt;/td&gt;
      &lt;td&gt;0.156267&lt;/td&gt;
      &lt;td&gt;-0.444269&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;Alaska&lt;/th&gt;
      &lt;td&gt;1.950138&lt;/td&gt;
      &lt;td&gt;1.073213&lt;/td&gt;
      &lt;td&gt;-0.438583&lt;/td&gt;
      &lt;td&gt;2.040003&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;Arizona&lt;/th&gt;
      &lt;td&gt;1.763164&lt;/td&gt;
      &lt;td&gt;-0.745957&lt;/td&gt;
      &lt;td&gt;-0.834653&lt;/td&gt;
      &lt;td&gt;0.054781&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;Arkansas&lt;/th&gt;
      &lt;td&gt;-0.141420&lt;/td&gt;
      &lt;td&gt;1.119797&lt;/td&gt;
      &lt;td&gt;-0.182811&lt;/td&gt;
      &lt;td&gt;0.114574&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;California&lt;/th&gt;
      &lt;td&gt;2.523980&lt;/td&gt;
      &lt;td&gt;-1.542934&lt;/td&gt;
      &lt;td&gt;-0.341996&lt;/td&gt;
      &lt;td&gt;0.598557&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;This is exactly the dataset that we obtained before.&lt;/p&gt;
&lt;h3 id=&#34;scaling-the-variables&#34;&gt;Scaling the Variables&lt;/h3&gt;
&lt;p&gt;The results obtained when we perform PCA will also depend on whether the variables have been individually scaled. In fact, the variance of a variable depends on its magnitude.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Variables variance
df.var(axis=0)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Murder        18.970465
Assault     6945.165714
UrbanPop     209.518776
Rape          87.729159
dtype: float64
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Consequently, if we perform PCA on the unscaled variables, then the first principal component loading vector will have a very large loading for &lt;code&gt;Assault&lt;/code&gt;, since that variable has by far the highest variance.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Fit PCA with unscaled varaibles
X = df.values
pca2_u = PCA(n_components=2).fit(X)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Get weights
weights_u = pca2_u.components_.T
df_weights_u = pd.DataFrame(weights_u, index=df.columns, columns=[&#39;PC1&#39;, &#39;PC2&#39;])
df_weights_u
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;PC1&lt;/th&gt;
      &lt;th&gt;PC2&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;Murder&lt;/th&gt;
      &lt;td&gt;0.041704&lt;/td&gt;
      &lt;td&gt;0.044822&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;Assault&lt;/th&gt;
      &lt;td&gt;0.995221&lt;/td&gt;
      &lt;td&gt;0.058760&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;UrbanPop&lt;/th&gt;
      &lt;td&gt;0.046336&lt;/td&gt;
      &lt;td&gt;-0.976857&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;Rape&lt;/th&gt;
      &lt;td&gt;0.075156&lt;/td&gt;
      &lt;td&gt;-0.200718&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Transform X to get the principal components
X_dim2_u = pca2_u.transform(X)
df_dim2_u = pd.DataFrame(X_dim2_u, columns=[&#39;PC1&#39;, &#39;PC2&#39;], index=df.index)
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;plotting&#34;&gt;Plotting&lt;/h3&gt;
&lt;p&gt;We can compare the lower dimensional representations with and without scaling.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;make_figure_10_1b(df_dim2, df_dim2_u, df_weights, df_weights_u)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/10_unsupervised_49_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;As predicted, the first principal component loading vector places almost all of its weight on &lt;code&gt;Assault&lt;/code&gt;, while the second principal component loading vector places almost all of its weight on &lt;code&gt;UrpanPop&lt;/code&gt;. Comparing this to the left-hand plot, we see that scaling does indeed have a substantial effect on the results obtained. However, this result is simply a consequence of the scales on which the variables were measured.&lt;/p&gt;
&lt;h3 id=&#34;the-proportion-of-variance-explained&#34;&gt;The Proportion of Variance Explained&lt;/h3&gt;
&lt;p&gt;We can now ask a natural question: how much of the information in a given data set is lost by projecting the observations onto the first few principal components? That is, how much of the variance in the data is not contained in the first few principal components? More generally, we are interested in knowing the &lt;strong&gt;proportion of variance explained (PVE)&lt;/strong&gt; by each principal component.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Four components
pca4 = PCA(n_components=4).fit(X_scaled)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Variance of the four principal components
pca4.explained_variance_
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;array([2.53085875, 1.00996444, 0.36383998, 0.17696948])
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;interpretation&#34;&gt;Interpretation&lt;/h3&gt;
&lt;p&gt;We can compute it in percentage of the total variance.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# As a percentage of the total variance
pca4.explained_variance_ratio_
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;array([0.62006039, 0.24744129, 0.0891408 , 0.04335752])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In the &lt;code&gt;Arrest&lt;/code&gt; dataset, the first principal component explains $62.0%$ of the variance in the data, and the next principal component explains $24.7%$ of the variance. Together, the first two principal components explain almost $87%$ of the variance in the data, and the last two principal components explain only $13%$ of the variance.&lt;/p&gt;
&lt;h3 id=&#34;plotting-1&#34;&gt;Plotting&lt;/h3&gt;
&lt;p&gt;We can plot in a graph the percentage of the variance explained, relative to the number of components.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;make_figure_10_2(pca4)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/10_unsupervised_61_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;how-many-principal-components&#34;&gt;How Many Principal Components?&lt;/h3&gt;
&lt;p&gt;In general, a $n \times p$ data matrix $X$ has $\min{n − 1, p}$ distinct principal components. However, we usually are not interested in all of them; rather, we would like to use just the first few principal components in order to visualize or interpret the data.&lt;/p&gt;
&lt;p&gt;We typically decide on the number of principal components required to visualize the data by examining a &lt;em&gt;scree plot&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;However, there is no well-accepted objective way to decide how many principal com- ponents are enough.&lt;/p&gt;
&lt;h2 id=&#34;k-means-clustering&#34;&gt;K-Means Clustering&lt;/h2&gt;
&lt;p&gt;The idea behind K-means clustering is that a good clustering is one for which the within-cluster variation is as small as possible. Hence we want to solve the problem&lt;/p&gt;
&lt;p&gt;$$
\underset{C_{1}, \ldots, C_{K}}{\operatorname{minimize}} \Bigg\lbrace \sum_{k=1}^{K} W\left(C_{k}\right) \Bigg\rbrace
$$&lt;/p&gt;
&lt;p&gt;where $C_k$ is a cluster and $ W(C_k)$ is a measure of the amount by which the observations within a cluster differ from each other.&lt;/p&gt;
&lt;p&gt;There are many possible ways to define this concept, but by far the most common choice involves &lt;strong&gt;squared Euclidean distance&lt;/strong&gt;. That is, we define&lt;/p&gt;
&lt;p&gt;$$
W\left(C_{k}\right)=\frac{1}{\left|C_{k}\right|} \sum_{i, i^{\prime} \in C_{k}} \sum_{j=1}^{p}\left(x_{i j}-x_{i^{\prime} j}\right)^2
$$&lt;/p&gt;
&lt;p&gt;where $|C_k|$ denotes the number of observations in the $k^{th}$ cluster.&lt;/p&gt;
&lt;h3 id=&#34;algorithm&#34;&gt;Algorithm&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Randomly assign a number, from $1$ to $K$, to each of the observations. These serve as initial cluster assignments for the observations.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Iterate until the cluster assignments stop changing:&lt;/p&gt;
&lt;p&gt;a) For each of the $K$ clusters, compute the cluster centroid. The kth cluster centroid is the vector of the $p$ feature means for the observations in the $k^{th}$ cluster.&lt;/p&gt;
&lt;p&gt;b) Assign each observation to the cluster whose centroid is closest (where closest is defined using Euclidean distance).&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;generate-the-data&#34;&gt;Generate the data&lt;/h3&gt;
&lt;p&gt;We first generate a 2-dimensional dataset.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Simulate data
np.random.seed(123)
X = np.random.randn(50,2)
X[0:25, 0] = X[0:25, 0] + 3
X[0:25, 1] = X[0:25, 1] - 4
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;make_new_figure_1(X)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/10_unsupervised_71_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;step-1-random-assignement&#34;&gt;Step 1: random assignement&lt;/h3&gt;
&lt;p&gt;Now let&amp;rsquo;s randomly assign the data to two clusters, at random.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Init clusters
K = 2
clusters0 = np.random.randint(K,size=(np.size(X,0)))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;make_new_figure_2(X, clusters0)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/10_unsupervised_75_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;step-2-estimate-distributions&#34;&gt;Step 2: estimate distributions&lt;/h3&gt;
&lt;p&gt;What are the new centroids?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Compute new centroids
def compute_new_centroids(X, clusters):
    K = len(np.unique(clusters))
    centroids = np.zeros((K,np.size(X,1)))
    for k in range(K):
        if sum(clusters==k)&amp;gt;0:
            centroids[k,:] = np.mean(X[clusters==k,:], axis=0)
        else:
            centroids[k,:] = np.mean(X, axis=0)
    return centroids
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Print
centroids0 = compute_new_centroids(X, clusters0)
print(centroids0)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[[ 1.54179703 -1.65922379]
 [ 1.67917325 -2.36272948]]
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;plotting-the-centroids&#34;&gt;Plotting the centroids&lt;/h3&gt;
&lt;p&gt;Let&amp;rsquo;s add the centroids to the graph.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Plot
plot_assignment(X, centroids0, clusters0, 0, 0)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/10_unsupervised_82_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;step-3-assign-data-to-clusters&#34;&gt;Step 3: assign data to clusters&lt;/h3&gt;
&lt;p&gt;Now we can assign the data to the clusters, according to the closest centroid.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Assign X to clusters
def assign_to_cluster(X, centroids):
    K = np.size(centroids,0)
    dist = np.zeros((np.size(X,0),K))
    for k in range(K):
        dist[:,k] = np.mean((X - centroids[k,:])**2, axis=1)
    clusters = np.argmin(dist, axis=1)
    
    # Compute inertia
    inertia = 0
    for k in range(K):
        if sum(clusters==k)&amp;gt;0:
            inertia += np.sum((X[clusters==k,:] - centroids[k,:])**2)
    return clusters, inertia
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;plotting-assigned-data&#34;&gt;Plotting assigned data&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Get cluster assignment
[clusters1,d] = assign_to_cluster(X, centroids0)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Plot
plot_assignment(X, centroids0, clusters1, d, 1)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/10_unsupervised_88_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;full-algorithm&#34;&gt;Full Algorithm&lt;/h3&gt;
&lt;p&gt;We now have all the components to proceed iteratively.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def kmeans_manual(X, K):

    # Init
    i = 0
    d0 = 1e4
    d1 = 1e5
    clusters = np.random.randint(K,size=(np.size(X,0)))

    # Iterate until convergence
    while np.abs(d0-d1) &amp;gt; 1e-10:
        d1 = d0
        centroids = compute_new_centroids(X, clusters)
        [clusters, d0] = assign_to_cluster(X, centroids)
        plot_assignment(X, centroids, clusters, d0, i)
        i+=1
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;plotting-k-means-clustering&#34;&gt;Plotting k-means clustering&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Test
kmeans_manual(X, K)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/10_unsupervised_93_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Here the observations can be easily plotted because they are two-dimensional.
If there were more than two variables then we could instead perform PCA
and plot the first two principal components score vectors.&lt;/p&gt;
&lt;h3 id=&#34;more-clusters&#34;&gt;More clusters&lt;/h3&gt;
&lt;p&gt;In the previous example, we knew that there really were two clusters because
we generated the data. However, for real data, in general we do not know
the true number of clusters. We could instead have performed K-means
clustering on this example with &lt;code&gt;K  =  3&lt;/code&gt;. If we do this, K-means clustering will split up the two &amp;ldquo;real&amp;rdquo; clusters, since it has no information about them:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# K=3
kmeans_manual(X, 3)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/10_unsupervised_97_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;sklearn-package&#34;&gt;Sklearn package&lt;/h3&gt;
&lt;p&gt;The automated function in &lt;code&gt;sklearn&lt;/code&gt; to persorm $K$-means clustering is &lt;code&gt;KMeans&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# SKlearn algorithm
km1 = KMeans(n_clusters=3, n_init=1, random_state=1)
km1.fit(X)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;KMeans(n_clusters=3, n_init=1, random_state=1)
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;plotting-2&#34;&gt;Plotting&lt;/h3&gt;
&lt;p&gt;We can plot the asssignment generated by the &lt;code&gt;KMeans&lt;/code&gt; function.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Plot
plot_assignment(X, km1.cluster_centers_, km1.labels_, km1.inertia_, km1.n_iter_)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/10_unsupervised_103_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;As we can see, the results are different in the two algorithms? Why? $K$-means is susceptible to the initial values. One way to solve this problem is to run the algorithm multiple times and report only the best results&lt;/p&gt;
&lt;h3 id=&#34;initial-assignment&#34;&gt;Initial Assignment&lt;/h3&gt;
&lt;p&gt;To run the &lt;code&gt;Kmeans()&lt;/code&gt; function in python with multiple initial cluster assignments, we use the &lt;code&gt;n_init&lt;/code&gt; argument (default: 10). If a value of &lt;code&gt;n_init&lt;/code&gt; greater than one is used, then K-means clustering will be performed using multiple random assignments, and the &lt;code&gt;Kmeans()&lt;/code&gt; function will report only the best results.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# 30 runs
km_30run = KMeans(n_clusters=3, n_init=30, random_state=1).fit(X)
plot_assignment(X, km_30run.cluster_centers_, km_30run.labels_, km_30run.inertia_, km_30run.n_iter_)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/10_unsupervised_107_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;best-practices&#34;&gt;Best Practices&lt;/h3&gt;
&lt;p&gt;It is generally recommended to always run K-means clustering with a large value of &lt;code&gt;n_init&lt;/code&gt;, such as 20 or 50 to avoid getting stuck in an undesirable local optimum.&lt;/p&gt;
&lt;p&gt;When performing K-means clustering, in addition to using multiple initial cluster assignments, it is also important to set a random seed using the &lt;code&gt;random_state&lt;/code&gt; parameter. This way, the initial cluster assignments can be replicated, and the K-means output will be fully reproducible.&lt;/p&gt;
&lt;h2 id=&#34;hierarchical-clustering&#34;&gt;Hierarchical Clustering&lt;/h2&gt;
&lt;p&gt;One potential disadvantage of K-means clustering is that it requires us to pre-specify the number of clusters $K$.&lt;/p&gt;
&lt;p&gt;Hierarchical clustering is an alternative approach which does not require that we commit to a particular choice of $K$.&lt;/p&gt;
&lt;h3 id=&#34;the-dendogram&#34;&gt;The Dendogram&lt;/h3&gt;
&lt;p&gt;Hierarchical clustering has an added advantage over K-means clustering in that it results in an attractive tree-based representation of the observations, called a &lt;strong&gt;dendrogram&lt;/strong&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;d = dendrogram(
        linkage(X, &amp;quot;complete&amp;quot;),
        leaf_rotation=90.,  # rotates the x axis labels
        leaf_font_size=8.,  # font size for the x axis labels
    )
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/10_unsupervised_114_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;interpretation-1&#34;&gt;Interpretation&lt;/h3&gt;
&lt;p&gt;Each leaf of the &lt;em&gt;dendrogram&lt;/em&gt; represents one observation.&lt;/p&gt;
&lt;p&gt;As we move up the tree, some leaves begin to fuse into branches. These correspond to observations that are similar to each other. As we move higher up the tree, branches themselves fuse, either with leaves or other branches. The earlier (lower in the tree) fusions occur, the more similar the groups of observations are to each other.&lt;/p&gt;
&lt;p&gt;We can use de &lt;em&gt;dendogram&lt;/em&gt; to understand how similar two observations are: we can look for the point in the tree where branches containing those two obse rvations are first fused. The height of this fusion, as measured on the vertical axis, indicates how different the two observations are. Thus, observations that fuse at the very bottom of the tree are quite similar to each other, whereas observations that fuse close to the top of the tree will tend to be quite different.&lt;/p&gt;
&lt;p&gt;The term &lt;strong&gt;hierarchical&lt;/strong&gt; refers to the fact that clusters obtained by cutting the dendrogram at a given height are necessarily nested within the clusters obtained by cutting the dendrogram at any greater height.&lt;/p&gt;
&lt;h3 id=&#34;the-hierarchical-clustering-algorithm&#34;&gt;The Hierarchical Clustering Algorithm&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Begin with $n$ observations and a measure (such as Euclidean distance) of all the $n(n − 1)/2$ pairwise dissimilarities. Treat each 2 observation as its own cluster.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;For $i=n,n−1,&amp;hellip;,2$&lt;/p&gt;
&lt;p&gt;a) Examine all pairwise inter-cluster dissimilarities among the $i$ clusters and identify the &lt;strong&gt;pair of clusters that are least dissimilar&lt;/strong&gt; (that is, most similar). Fuse these two clusters. The dissimilarity between these two clusters indicates the height in the dendrogram at which the fusion should be placed.&lt;/p&gt;
&lt;p&gt;b) Compute the new pairwise inter-cluster dissimilarities among the $i−1$ remaining clusters.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;the-linkage-function&#34;&gt;The Linkage Function&lt;/h3&gt;
&lt;p&gt;We have a concept of the dissimilarity between pairs of observations, but how do we define the dissimilarity between two clusters if one or both of the clusters contains multiple observations?&lt;/p&gt;
&lt;p&gt;The concept of dissimilarity between a pair of observations needs to be extended to a pair of groups of observations. This extension is achieved by developing the notion of &lt;strong&gt;linkage&lt;/strong&gt;, which defines the dissimilarity between two groups of observations.&lt;/p&gt;
&lt;h3 id=&#34;linkages&#34;&gt;Linkages&lt;/h3&gt;
&lt;p&gt;The four most common types of linkage are:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Complete&lt;/strong&gt;: Maximal intercluster dissimilarity. Compute all pairwise dissimilarities between the observations in cluster A and the observations in cluster B, and record the largest of these dissimilarities.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Single&lt;/strong&gt;: Minimal intercluster dissimilarity. Compute all pairwise dissimilarities between the observations in cluster A and the observations in cluster B, and record the smallest of these dissimilarities.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Average&lt;/strong&gt;: Mean intercluster dissimilarity. Compute all pairwise dissimilarities between the observations in cluster A and the observations in cluster B, and record the average of these dissimilarities.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Centroid&lt;/strong&gt;: Dissimilarity between the centroid for cluster A (a mean vector of length p) and the centroid for cluster B. Centroid linkage can result in undesirable inversions.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Average, complete, and single linkage are most popular among statisticians. Average and complete linkage are generally preferred over single linkage, as they tend to yield more balanced dendrograms. Centroid linkage is often used in genomics, but suffers from a major drawback in that an inversion can occur, whereby two clusters are fused at a height below either of the individual clusters in the dendrogram. This can lead to difficulties in visualization as well as in interpretation of the dendrogram.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Init
linkages = [hierarchy.complete(X), hierarchy.average(X), hierarchy.single(X)]
titles = [&#39;Complete Linkage&#39;, &#39;Average Linkage&#39;, &#39;Single Linkage&#39;]
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;plotting-3&#34;&gt;Plotting&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;make_new_figure_4(linkages, titles)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/10_unsupervised_126_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;For this data, both &lt;em&gt;complete&lt;/em&gt; and &lt;em&gt;average&lt;/em&gt; linkage generally separates the observations into their correct groups.&lt;/p&gt;
&lt;h2 id=&#34;gaussian-mixture-models&#34;&gt;Gaussian Mixture Models&lt;/h2&gt;
&lt;p&gt;Clustering methods such as hierarchical clustering and K-means are based on heuristics and rely primarily on finding clusters whose members are close to one another, as measured directly with the data (no probability model involved).&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Gaussian Mixture Models&lt;/em&gt; assume that the data was generated by multiple multivariate gaussian distributions. The objective of the algorithm is to recover these latent distributions.&lt;/p&gt;
&lt;p&gt;The advantages with respect to K-means are&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;a structural interpretaion of the parameters&lt;/li&gt;
&lt;li&gt;automatically generates class probabilities&lt;/li&gt;
&lt;li&gt;can generate clusters of observations that are not necessarily close to each other&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;algorithm-1&#34;&gt;Algorithm&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Randomly assign a number, from $1$ to $K$, to each of the observations. These serve as initial cluster assignments for the observations.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Iterate until the cluster assignments stop changing:&lt;/p&gt;
&lt;p&gt;a) For each of the $K$ clusters, compute its mean and variance. The main difference with K-means is that we also compute the variance matrix.&lt;/p&gt;
&lt;p&gt;b) Assign each observation to its most likely cluster.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;dataset&#34;&gt;Dataset&lt;/h3&gt;
&lt;p&gt;Let&amp;rsquo;s use the same data we have used for k-means, for a direct comparison.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;make_new_figure_1(X)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/10_unsupervised_134_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;step-1-random-assignement-1&#34;&gt;Step 1: random assignement&lt;/h3&gt;
&lt;p&gt;Let&amp;rsquo;s also use the same random assignment of the K-means algorithm.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;make_new_figure_2(X, clusters0)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/10_unsupervised_137_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;step-2-compute-distirbutions&#34;&gt;Step 2: compute distirbutions&lt;/h3&gt;
&lt;p&gt;What are the new distributions?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Compute new centroids
def compute_distributions(X, clusters):
    K = len(np.unique(clusters))
    distr = []
    for k in range(K):
        if sum(clusters==k)&amp;gt;0:
            distr += [multivariate_normal(np.mean(X[clusters==k,:], axis=0), np.cov(X[clusters==k,:].T))]
        else:
            distr += [multivariate_normal(np.mean(X, axis=0), np.cov(X.T))]
    return distr
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Print
distr0 = compute_distributions(X, clusters0)
print(&amp;quot;Mean of the first distribution: \n&amp;quot;, distr0[0].mean)
print(&amp;quot;\nVariance of the first distribution: \n&amp;quot;, distr0[0].cov)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Mean of the first distribution: 
 [ 1.54179703 -1.65922379]

Variance of the first distribution: 
 [[ 3.7160256  -2.27290036]
 [-2.27290036  4.67223237]]
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;plotting-the-distributions&#34;&gt;Plotting the distributions&lt;/h3&gt;
&lt;p&gt;Let&amp;rsquo;s add the distributions to the graph.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Plot
plot_assignment_gmm(X, clusters0, distr0, i=0, logL=0.0)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/10_unsupervised_144_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;likelihood&#34;&gt;Likelihood&lt;/h3&gt;
&lt;p&gt;The main difference with respect with K-means is that we can now compute the probability that each observation belongs to each cluster. This is the probability that each observation was generated by one of the two bi-variate normal distributions. These probabilities are called &lt;strong&gt;likelihoods&lt;/strong&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Print first 5 likelihoods
pdfs0 = np.stack([d.pdf(X) for d in distr0], axis=1)
pdfs0[:5]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;array([[0.03700522, 0.05086876],
       [0.00932081, 0.02117353],
       [0.04092453, 0.04480732],
       [0.00717854, 0.00835799],
       [0.01169199, 0.01847373]])
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;step-3-assign-data-to-clusters-1&#34;&gt;Step 3: assign data to clusters&lt;/h3&gt;
&lt;p&gt;Now we can assign the data to the clusters, via maximum likelihood.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Assign X to clusters
def assign_to_cluster_gmm(X, distr):
    pdfs = np.stack([d.pdf(X) for d in distr], axis=1)
    clusters = np.argmax(pdfs, axis=1)
    log_likelihood = 0
    for k, pdf in enumerate(pdfs):
        log_likelihood += np.log(pdf[clusters[k]])
    return clusters, log_likelihood
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Get cluster assignment
clusters1, logL1 = assign_to_cluster_gmm(X, distr0)
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;plotting-assigned-data-1&#34;&gt;Plotting assigned data&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Compute new distributions
distr1 = compute_distributions(X, clusters1)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Plot
plot_assignment_gmm(X, clusters1, distr1, 1, logL1);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/10_unsupervised_154_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;expectation---maximization&#34;&gt;Expectation - Maximization&lt;/h3&gt;
&lt;p&gt;The two steps we have just seen, are part of a broader family of algorithms to maximize likelihoods called &lt;strong&gt;expectation&lt;/strong&gt;-&lt;strong&gt;maximization&lt;/strong&gt; algorithms.&lt;/p&gt;
&lt;p&gt;In the expectation step, we computed the expectation of the parameters, given the current cluster assignment.&lt;/p&gt;
&lt;p&gt;In the maximization step, we assigned observations to the cluster that maximized the likelihood of the single observation.&lt;/p&gt;
&lt;p&gt;The alternative, and more computationally intensive procedure, would have been to specify a global likelihood function and find the mean and variance paramenters of the two normal distributions that maximized those likelihoods.&lt;/p&gt;
&lt;h3 id=&#34;full-algorithm-1&#34;&gt;Full Algorithm&lt;/h3&gt;
&lt;p&gt;We can now deploy the full algorithm.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def gmm_manual(X, K):

    # Init
    i = 0
    logL0 = 1e4
    logL1 = 1e5
    clusters = np.random.randint(K,size=(np.size(X,0)))

    # Iterate until convergence
    while np.abs(logL0-logL1) &amp;gt; 1e-10:
        logL1 = logL0
        distr = compute_distributions(X, clusters)
        clusters, logL0 = assign_to_cluster_gmm(X, distr)
        plot_assignment_gmm(X, clusters, distr, i, logL0)
        i+=1
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;plotting-k-means-clustering-1&#34;&gt;Plotting k-means clustering&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Test
gmm_manual(X, K)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/10_unsupervised_161_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;In this case, GMM does a very poor job identifying the original clusters.&lt;/p&gt;
&lt;h3 id=&#34;overlapping-clusters&#34;&gt;Overlapping Clusters&lt;/h3&gt;
&lt;p&gt;Let&amp;rsquo;s now try with a different dataset, where the data is drawn from two overlapping bi-variate gaussian distributions, forming a cross.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Simulate data
X = np.random.randn(50,2)
X[0:25, :] = np.random.multivariate_normal([0,0], [[50,0],[0,1]], size=25)
X[25:, :] = np.random.multivariate_normal([0,0], [[1,0],[0,50]], size=25)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;make_new_figure_1(X)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/10_unsupervised_166_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;gmm-with-overlapping-distributions&#34;&gt;GMM with overlapping distributions&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# GMM
gmm_manual(X, K)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/10_unsupervised_168_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;As we can see, GMM is able to correctly recover the original clusters.&lt;/p&gt;
&lt;h3 id=&#34;k-means-with-overlapping-distributions&#34;&gt;K-means with overlapping distributions&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# K-means
kmeans_manual(X, K)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../img/10_unsupervised_171_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;K-means generates completely different clusters.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
