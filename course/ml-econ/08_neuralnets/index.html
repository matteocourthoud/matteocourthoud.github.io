<!DOCTYPE html><html lang="en-us" >

<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  
  
  
    <meta name="generator" content="Wowchemy 5.1.0 for Hugo" />
  

  

  
  
  
  
  
    
    
    
  
  

  

  
  
  
    
  
  <meta name="description" content="# Remove warnings import warnings warnings.filterwarnings(&#39;ignore&#39;) # Import everything import pandas as pd import numpy as np import seaborn as sns import statsmodels.api as sm import copy import torch import torch." />

  
  <link rel="alternate" hreflang="en-us" href="https://matteocourthoud.github.io/course/ml-econ/08_neuralnets/" />

  







  




  
  
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
  

  
  
  
    <meta name="theme-color" content="#003f5c" />
  

  
  
    
    <script src="/js/mathjax-config.js"></script>
  

  
  
  
  
    
    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha256-FMvZuGapsJLjouA6k7Eo2lusoAX9i0ShlWFG6qt7SLc=" crossorigin="anonymous">

    
    

    
    
    
      
    
    
      
      
        
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/styles/github.min.css" crossorigin="anonymous" title="hl-light" media="print" onload="this.media='all'">
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/styles/dracula.min.css" crossorigin="anonymous" title="hl-dark" media="print" onload="this.media='all'" disabled>
        
      
    

    
    
    
      <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.7.1/leaflet.min.css" integrity="sha512-1xoFisiGdy9nvho8EgXuXvnpR5GAMSjFwp40gSRE3NwdUdIMIKuPa7bqoUhLD0O/5tPNhteAsE5XyyMi5reQVA==" crossorigin="anonymous" media="print" onload="this.media='all'">
    

    

    
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js" integrity="" crossorigin="anonymous" async></script>
      
    
      
      

      
      

      
    
      
      

      
      

      
    
  

  
  
  
    
      
      <link rel="preload" as="style" href="https://fonts.googleapis.com/css2?family=Roboto+Mono&family=Source+Sans+Pro:wght@200;300;400;700&display=swap">
      <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Roboto+Mono&family=Source+Sans+Pro:wght@200;300;400;700&display=swap" media="print" onload="this.media='all'">
    
  

  
  
  
  
  
  <link rel="stylesheet" href="/css/wowchemy.4f7182ca394d705ee32d9d7750e9aa1d.css" />

  




<script async src="https://www.googletagmanager.com/gtag/js?id=UA-144780600-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];

  function gtag() {
      dataLayer.push(arguments);
  }

  function trackOutboundLink(url, target) {
    gtag('event', 'click', {
         'event_category': 'outbound',
         'event_label': url,
         'transport_type': 'beacon',
         'event_callback': function () {
           if (target !== '_blank') {
             document.location = url;
           }
         }
    });
    console.debug("Outbound link clicked: " + url);
  }

  function onClickCallback(event) {
    if ((event.target.tagName !== 'A') || (event.target.host === window.location.host)) {
      return;
    }
    trackOutboundLink(event.target, event.target.getAttribute('target'));  
  }

  gtag('js', new Date());
  gtag('config', 'UA-144780600-1', { 'anonymize_ip': true });
  gtag('set', {'cookie_flags': 'SameSite=None;Secure'});

  
  document.addEventListener('click', onClickCallback, false);
</script>


  

  

  




  
  
  

  

  
    <link rel="manifest" href="/index.webmanifest" />
  

  <link rel="icon" type="image/png" href="/media/icon_hu03e9b3967b83fd39296ec9da5ff1ea05_201175_32x32_fill_lanczos_center_3.png" />
  <link rel="apple-touch-icon" type="image/png" href="/media/icon_hu03e9b3967b83fd39296ec9da5ff1ea05_201175_180x180_fill_lanczos_center_3.png" />

  <link rel="canonical" href="https://matteocourthoud.github.io/course/ml-econ/08_neuralnets/" />

  
  
  
  
  
  
  
  
    
    
  
  
  <meta property="twitter:card" content="summary" />
  
  <meta property="og:site_name" content="Matteo Courthoud" />
  <meta property="og:url" content="https://matteocourthoud.github.io/course/ml-econ/08_neuralnets/" />
  <meta property="og:title" content="Neural Networks | Matteo Courthoud" />
  <meta property="og:description" content="# Remove warnings import warnings warnings.filterwarnings(&#39;ignore&#39;) # Import everything import pandas as pd import numpy as np import seaborn as sns import statsmodels.api as sm import copy import torch import torch." /><meta property="og:image" content="https://matteocourthoud.github.io/media/icon_hu03e9b3967b83fd39296ec9da5ff1ea05_201175_512x512_fill_lanczos_center_3.png" />
    <meta property="twitter:image" content="https://matteocourthoud.github.io/media/icon_hu03e9b3967b83fd39296ec9da5ff1ea05_201175_512x512_fill_lanczos_center_3.png" /><meta property="og:locale" content="en-us" />
  
    
      <meta
        property="article:published_time"
        content="2022-03-09T00:00:00&#43;00:00"
      />
    
    <meta property="article:modified_time" content="2022-03-09T00:00:00&#43;00:00">
  

  



  

  
  
  
  
  
    <script src="https://cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.1.1/cookieconsent.min.js" integrity="sha256-5VhCqFam2Cn+yjw61zbBNrbHVJ6SRydPeKopYlngbiQ=" crossorigin="anonymous"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.1.1/cookieconsent.min.css" integrity="sha256-zQ0LblD/Af8vOppw18+2anxsuaz3pWYyVWi+bTvTH8Q=" crossorigin="anonymous">
  
  <script>
  window.addEventListener("load", function(){
    window.cookieconsent.initialise({
      "palette": {
        "popup": {
          "background": "#003f5c",
          "text": "rgb(255, 255, 255)"
        },
        "button": {
          "background": "rgb(255, 255, 255)",
          "text": "#003f5c"
        }
      },
      "theme": "classic",
      "content": {
        "message": "This website uses cookies to ensure you get the best experience on our website.",
        "dismiss": "Got it!",
        "link": "Learn more",
        "href": "/privacy/"
      }
    })});
  </script>


  





  <title>Neural Networks | Matteo Courthoud</title>
</head>


<body id="top" data-spy="scroll" data-offset="70" data-target="#TableOfContents" class="page-wrapper   " data-wc-page-id="9d0e7e97b8f20fca1686acda2d5705cf" >

  
  
  
  
  
  
  
  
  <script src="/js/wowchemy-init.min.66d3e0fff6d32c4ece05adee927fbd96.js"></script>

  


<aside class="search-modal" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Search</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#" aria-label="Close"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        
        <input name="q" id="search-query" placeholder="Search..." autocapitalize="off"
        autocomplete="off" autocorrect="off" spellcheck="false" type="search" class="form-control"
        aria-label="Search...">
        
      </div>

      
      

      

    </section>
    <section class="section-search-results">

      <div id="search-hits">
        
      </div>

    </section>
  </div>
</aside>



  <div class="page-header">
    












<nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id="navbar-main">
  <div class="container-xl">

    
    <div class="d-none d-lg-inline-flex">
      <a class="navbar-brand" href="/">Matteo Courthoud</a>
    </div>
    

    
    <button type="button" class="navbar-toggler" data-toggle="collapse"
            data-target="#navbar-content" aria-controls="navbar-content" aria-expanded="false" aria-label="Toggle navigation">
    <span><i class="fas fa-bars"></i></span>
    </button>
    

    
    <div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none">
      <a class="navbar-brand" href="/">Matteo Courthoud</a>
    </div>
    

    
    
    <div class="navbar-collapse main-menu-item collapse justify-content-end" id="navbar-content">

      
      <ul class="navbar-nav d-md-inline-flex">
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
            
            
            
              
            
            
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#about"><span>Home</span></a>
        </li>

        
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
            
            
            
              
            
            
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#posts"><span>Posts</span></a>
        </li>

        
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
            
            
            
              
            
            
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#projects"><span>Research</span></a>
        </li>

        
        

        
        <li class="nav-item dropdown">
          <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown" aria-haspopup="true"><span>Courses</span><span class="caret"></span>
          </a>
          <div class="dropdown-menu">
            
              <a class="dropdown-item" href="/course/ml-econ/"><span>Machine Learning for Economics</span></a>
            
              <a class="dropdown-item" href="/course/data-science/"><span>Data Science with Python</span></a>
            
              <a class="dropdown-item" href="/course/empirical-io/"><span>PhD Industrial Organization</span></a>
            
              <a class="dropdown-item" href="/course/metrics/"><span>PhD Econometrics</span></a>
            
              <a class="dropdown-item" href="https://pp4rs.github.io/"><span>Programming Practices for Research</span></a>
            
          </div>
        </li>

        
        

        

        
        
        

        

        
        
        
        

        
          
            
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/cv"><span>CV</span></a>
        </li>

        
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
            
            
            
              
            
            
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#contact"><span>Contact</span></a>
        </li>

        
        

      

        
      </ul>
    </div>

    <ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2">

      
      
        
      

      
      

      
      

      
      

    </ul>

  </div>
</nav>


  </div>

  <div class="page-body">
    

<div class="container-fluid docs">
  <div class="row flex-xl-nowrap">
    <div class="col-12 col-md-3 col-xl-2 docs-sidebar">
      
<nav class="collapse docs-links" id="docs-nav">
  
  
  
  
  
  

  
  
    

    
      

      <ul class="nav docs-sidenav">
        <li><a href="/course/"><i class="fas fa-arrow-left pr-1"></i>Courses</a></li>
      </ul>

      
      
        
          
        
      


  
    
    
    
    
      
    
    

    
      <div class="docs-toc-item">
        <a class="docs-toc-link " href="/course/ml-econ/">ML for Economics</a>
    
      
        <ul class="nav docs-sidenav">
      


  <li class=""><a href="/course/ml-econ/01_regression/">Linear Regression</a></li>



  <li class=""><a href="/course/ml-econ/02_iv/">Instrumental Variables</a></li>



  <li class=""><a href="/course/ml-econ/03_nonparametric/">Non-Parametric Regression</a></li>



  <li class=""><a href="/course/ml-econ/04_crossvalidation/">Resampling Methods</a></li>



  <li class=""><a href="/course/ml-econ/05_regularization/">Model Selection and Regularization</a></li>



  <li class=""><a href="/course/ml-econ/06_convexity/">Convexity and Optimization</a></li>



  <li class=""><a href="/course/ml-econ/07_trees/">Tree-based Methods</a></li>



  <li class="active"><a href="/course/ml-econ/08_neuralnets/">Neural Networks</a></li>



  <li class=""><a href="/course/ml-econ/09_postdoubleselection/">Post-Double Selection</a></li>



  <li class=""><a href="/course/ml-econ/10_unsupervised/">Unsupervised Learning</a></li>

      
        </ul>
      
    

    
      </div>
    

    
  
</nav>

    </div>

    
    
    <div class="d-none d-xl-block col-xl-2 docs-toc">
      

      <ul class="nav toc-top">
        <li><a href="#" id="back_to_top" class="docs-toc-title">Contents</a></li>
      </ul>

      <nav id="TableOfContents">
  <ul>
    <li><a href="#81-introduction">8.1 Introduction</a>
      <ul>
        <li><a href="#regression">Regression</a></li>
        <li><a href="#activation-functions">Activation Functions</a></li>
        <li><a href="#layers">Layers</a></li>
      </ul>
    </li>
    <li><a href="#pytorch">Pytorch</a>
      <ul>
        <li><a href="#tensors">Tensors</a></li>
        <li><a href="#variables">Variables</a></li>
        <li><a href="#activation-function">Activation Function</a></li>
      </ul>
    </li>
    <li><a href="#83-optimization-and-gradient-descent">8.3 Optimization and Gradient Descent</a>
      <ul>
        <li><a href="#gradient-descent">Gradient Descent</a></li>
        <li><a href="#gradient-descent-in-linear-regression">Gradient Descent in Linear Regression</a></li>
        <li><a href="#autograd">Autograd</a></li>
        <li><a href="#optimizer">Optimizer</a></li>
        <li><a href="#building-a-nn">Building a NN</a></li>
        <li><a href="#loss-functions">Loss functions</a></li>
        <li><a href="#optimizers">Optimizers</a></li>
        <li><a href="#training-on-batch">Training on batch</a></li>
      </ul>
    </li>
    <li><a href="#84-advanced-topics">8.4 Advanced Topics</a>
      <ul>
        <li><a href="#issues">Issues</a></li>
        <li><a href="#deep-neural-networks-and-deep-learning">Deep Neural Networks and Deep Learning</a></li>
        <li><a href="#convolutional-neural-nets">Convolutional Neural Nets</a></li>
        <li><a href="#recurrent-neural-nets">Recurrent Neural Nets</a></li>
        <li><a href="#bidirectional-rnn">Bidirectional RNN</a></li>
        <li><a href="#recursive-neural-netw">Recursive Neural Netw</a></li>
      </ul>
    </li>
  </ul>
</nav>

      
    </div>
    

    <main class="col-12 col-md-9 col-xl-8 py-md-3 pl-md-5 docs-content" role="main">

      <article class="article">

        <div class="docs-article-container">

          <h1>Neural Networks</h1>

          <p>Last updated on Mar 9, 2022</p>

          <div class="article-style">
            <pre><code class="language-python"># Remove warnings
import warnings
warnings.filterwarnings('ignore')
</code></pre>
<pre><code class="language-python"># Import everything
import pandas as pd
import numpy as np
import seaborn as sns
import statsmodels.api as sm

import copy
import torch 
import torch.nn as nn
import torch.utils.data as Data
from torch.autograd import Variable
from sklearn.linear_model import LinearRegression
from torchviz import make_dot
</code></pre>
<pre><code class="language-python"># Import matplotlib for graphs
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import axes3d
from IPython.display import clear_output

# Set global parameters
%matplotlib inline
plt.style.use('seaborn-white')
plt.rcParams['lines.linewidth'] = 3
plt.rcParams['figure.figsize'] = (10,6)
plt.rcParams['figure.titlesize'] = 20
plt.rcParams['axes.titlesize'] = 18
plt.rcParams['axes.labelsize'] = 14
plt.rcParams['legend.fontsize'] = 14
</code></pre>
<p>While <code>sklearn</code> has a library for neural networks, it is very basic and not the standard in the industry. The most commonly used libraries as of 2020 are <strong>Tensorflow</strong> and <strong>Pytorch</strong>.</p>
<ul>
<li>
<p>TensorFlow is developed by Google Brain and actively used at Google both for research and production needs. Its closed-source predecessor is called DistBelief.</p>
</li>
<li>
<p>PyTorch is a cousin of lua-based Torch framework which was developed and used at Facebook. However, PyTorch is not a simple set of wrappers to support popular language, it was rewritten and tailored to be fast and feel native.</p>
</li>
</ul>
<p>Here is an article that explains very well the difference between the two libraries: <a href="https://towardsdatascience.com/pytorch-vs-tensorflow-spotting-the-difference-25c75777377b" target="_blank" rel="noopener">pytorch-vs-tensorflow</a>. In short, pytorch is much more intuitive for a python programmer and more user friendly. It also has a superior development and debugging experience. However, if you want more control on the fundamentals, a better community support and you need to train large models, Tensorflow is better.</p>
<h2 id="81-introduction">8.1 Introduction</h2>
<p>The term neural network has evolved to encompass a large class of models and learning methods. Here I describe the most widely used “vanilla” neural net, sometimes called the single hidden layer back-propagation network, or single layer perceptron.</p>
<h3 id="regression">Regression</h3>
<p>Imagine a setting with two <strong>inputs</strong> available (let’s denote these inputs $i_1$ and $i_2$), and no special knowledge about the relationship between these inputs and the <strong>output</strong> that we want to predict (denoted by $o$) except that this relationship is, a priori, pretty complex and non-linear.</p>
<p>So we want to learn the function $f$ such that f($i_1$, $i_2$) is a good estimator of $o$. We could then suggest the following first model:</p>
<p>$$
o = w_{11} i_1 + w_{12} i_2
$$</p>
<p>where $w_{11}$ and $w_{12}$ are just weights/coefficients (do not take care about the indices for now). Before going any further, we should notice that, here, there is no constant term in the model. However, we could have introduced such term by setting $f(i_1, i_2) = w_{11} i_1 + w_{12} i_2 + c$. The constant is often called <strong>bias</strong>.</p>
<p>We can represent the setting as follows.</p>
<img src="../figures/nn1.jpeg" alt="Drawing" style="width: 600px;"/>
<p>In this case, the model is easy to understand and to fit but has a big drawback : there is no non-linearity! This obviously do not respect our non-linear assumption.</p>
<h3 id="activation-functions">Activation Functions</h3>
<p>In order to introduce a non-linearity, let us make a little modification in the previous model and suggest the following one.</p>
<p>$$
o = a ( w_{11} i_1 + w_{12} i_2)
$$</p>
<p>where $a$ is a function called <strong>activation function</strong> which is non-linear.</p>
<img src="../figures/nn2.jpeg" alt="Drawing" style="width: 600px;"/>
<p>One activation function that is well known in economics (and other disciplines) is the <em>sigmoid</em> function or logit function</p>
<p>$$
a (w_{11} i_1 + w_{12} i_2) = \frac{1}{1 + e^{w_{11} i_1 + w_{12} i_2}}
$$</p>
<h3 id="layers">Layers</h3>
<p>However, even if better than multilinear model, this model is still too simple and can’t handle the assumed underlying complexity of the relationship between inputs and output. We can make a step further and enrich the model the following way.</p>
<ol>
<li>First we could consider that the quantity $a ( w_{11} i_1 + w_{12} i_2)$ is no longer the final output but instead a new intermediate feature of our function, called $l_1$, which stands for <strong>layer</strong>.</li>
</ol>
<p>$$
l_1 = a ( w_{11} i_1 + w_{12} i_2)
$$</p>
<ol start="2">
<li>Second we could consider that we build several (3 in our example) such features in the same way, but possibly with different weights and different activation functions</li>
</ol>
<p>$$
l_1 = a ( w_{11} i_1 + w_{12} i_2) \
l_2 = a ( w_{21} i_1 + w_{22} i_2) \
l_3 = a ( w_{31} i_1 + w_{32} i_2)
$$</p>
<p>where the $a$’s are just activation functions and the $w$’s are weights.</p>
<ol start="3">
<li>Finally, we can consider that our final output is build based on these intermediate features with the same “template”</li>
</ol>
<p>$$
a_2 ( v_1 l_1 + v_2 l_2 + v_3 * l_3 )
$$</p>
<p>If we aggregate all the pieces, we then get our <strong>prediction</strong> $p$</p>
<p>$$
\begin{aligned}
p = f_{3}\left(i_{1}, i_{2}\right) &amp;=a_{2}\left(v_{1} l_{1}+v_{2} l_{2}+v_{3} l_{3}\right) \
&amp;=a_{2}\left(v_{1} \times a_{11}\left(w_{11} i_{1}+w_{12} i_{2}\right)+v_{2} \times a_{12}\left(w_{21} i_{1}+w_{22} i_{2}\right)+v_{3} \times a_{13}\left(w_{31} i_{1}+w_{32} i_{2}\right)\right)
\end{aligned}
$$</p>
<p>where we should mainly keep in mind that $a$’s are non-linear activation functions and $w$’s and $v$’s are weights.</p>
<p>Graphically:</p>
<img src="../figures/nn3.jpeg" alt="Drawing" style="width: 900px;"/>
<p>This last model is a basic feedforward neural network with:</p>
<ul>
<li>2 entries ($i_1$ and $i_2$)</li>
<li>1 hidden layer with 3 hidden neurones (whose outputs are $l_1$, $l_2$ and $l_3$)</li>
<li>1 final output ($p$)</li>
</ul>
<h2 id="pytorch">Pytorch</h2>
<h3 id="tensors">Tensors</h3>
<p>We can express the data as a <code>numpy</code> array.</p>
<pre><code class="language-python">x_np = np.arange(6).reshape((3, 2))
x_np
</code></pre>
<pre><code>array([[0, 1],
       [2, 3],
       [4, 5]])
</code></pre>
<p>Or equivalently as a <code>pytorch</code> tensor.</p>
<pre><code class="language-python">x_tensor = torch.from_numpy(x_np)
x_tensor
</code></pre>
<pre><code>tensor([[0, 1],
        [2, 3],
        [4, 5]])
</code></pre>
<p>We can also translate tensors back to arrays.</p>
<pre><code class="language-python">tensor2array = x_tensor.numpy()
tensor2array
</code></pre>
<pre><code>array([[0, 1],
       [2, 3],
       [4, 5]])
</code></pre>
<p>We can make operations over this data. For example we can take the mean</p>
<pre><code class="language-python">try:
    torch.mean(x_tensor)
except Exception as e:
    print(e)
</code></pre>
<pre><code>mean(): input dtype should be either floating point or complex dtypes. Got Long instead.
</code></pre>
<p>We first have to convert the data in float</p>
<pre><code class="language-python">x_tensor = torch.FloatTensor(x_np)
x_tensor
</code></pre>
<pre><code>tensor([[0., 1.],
        [2., 3.],
        [4., 5.]])
</code></pre>
<pre><code class="language-python">print(np.mean(x_np), '\n\n', torch.mean(x_tensor))
</code></pre>
<pre><code>2.5 

 tensor(2.5000)
</code></pre>
<p>We can also apply compontent-wise functions</p>
<pre><code class="language-python">print(np.sin(x_np), '\n\n', torch.sin(x_tensor))
</code></pre>
<pre><code>[[ 0.          0.84147098]
 [ 0.90929743  0.14112001]
 [-0.7568025  -0.95892427]] 

 tensor([[ 0.0000,  0.8415],
        [ 0.9093,  0.1411],
        [-0.7568, -0.9589]])
</code></pre>
<p>We can multiply tensors as we multiply matrices</p>
<pre><code class="language-python">print(np.matmul(x_np.T, x_np), '\n\n', torch.mm(x_tensor.T, x_tensor))
</code></pre>
<pre><code>[[20 26]
 [26 35]] 

 tensor([[20., 26.],
        [26., 35.]])
</code></pre>
<p>But the element-wise multiplication does not work</p>
<pre><code class="language-python">try:
    x_tensor.dot(x_tensor)
except Exception as e:
    print(e)
</code></pre>
<pre><code>1D tensors expected, but got 2D and 2D tensors
</code></pre>
<h3 id="variables">Variables</h3>
<p>Variable in torch is to build a computational graph, but this graph is dynamic compared with a static graph in Tensorflow or Theano. So torch does not have placeholder, torch can just pass variable to the computational graph.</p>
<pre><code class="language-python"># build a variable, usually for compute gradients
x_variable = Variable(x_tensor, requires_grad=True)   

x_variable
</code></pre>
<pre><code>tensor([[0., 1.],
        [2., 3.],
        [4., 5.]], requires_grad=True)
</code></pre>
<p>Until now the tensor and variable seem the same. However, the variable is a part of the graph, it&rsquo;s a part of the auto-gradient.</p>
<p>Suppose we are interested in:</p>
<p>$$
y = \text{mean} (x_1^2) = \frac{1}{6} x^2
$$</p>
<pre><code class="language-python">y = torch.mean(x_variable*x_variable)
print(y)
</code></pre>
<pre><code>tensor(9.1667, grad_fn=&lt;MeanBackward0&gt;)
</code></pre>
<p>We can compute the gradient by backpropagation</p>
<p>$$
\nabla y(x) = \frac{2}{3} x
$$</p>
<p>i.e. if we call the <code>backward</code> method on our outcome <code>y</code>, we see that the gradient of our variable <code>x</code> gets updated.</p>
<pre><code class="language-python">print(x_variable.grad)
</code></pre>
<pre><code>None
</code></pre>
<pre><code class="language-python">y.backward()
</code></pre>
<pre><code class="language-python">print(x_variable.grad)
</code></pre>
<pre><code>tensor([[0.0000, 0.3333],
        [0.6667, 1.0000],
        [1.3333, 1.6667]])
</code></pre>
<p>However, its value has not changed.</p>
<pre><code class="language-python">print(x_variable)
</code></pre>
<pre><code>tensor([[0., 1.],
        [2., 3.],
        [4., 5.]], requires_grad=True)
</code></pre>
<p>We can also access the <code>tensor</code> part of the variable alone by calling the <code>data</code> method.</p>
<pre><code class="language-python">print(x_variable.data)
</code></pre>
<pre><code>tensor([[0., 1.],
        [2., 3.],
        [4., 5.]])
</code></pre>
<h3 id="activation-function">Activation Function</h3>
<p>The main advantage of neural networks is that they introduce non-linearities among the layers. The standard non-linear function</p>
<ul>
<li>ReLu</li>
<li>Sigmoid</li>
<li>TanH</li>
<li>Softmax</li>
</ul>
<pre><code class="language-python"># X grid
x_grid = torch.linspace(-5, 5, 200)  # x data (tensor), shape=(100, 1)
x_grid = Variable(x_grid)
x_grid_np = x_grid.data.numpy()   # numpy array for plotting

# Activation functions
y_relu = torch.relu(x_grid).data.numpy()
y_sigmoid = torch.sigmoid(x_grid).data.numpy()
y_tanh = torch.tanh(x_grid).data.numpy()
y_softmax = torch.softmax(x_grid, dim=0).data.numpy() 
</code></pre>
<pre><code class="language-python"># New figure 1
def make_new_figure_1():

    # Init figure
    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2,2, figsize=(8,6))
    fig.suptitle('Activation Functions')

    # Relu
    ax1.plot(x_grid_np, y_relu, c='red', label='relu')
    ax1.set_ylim((-1, 6)); ax1.legend()

    # Sigmoid
    ax2.plot(x_grid_np, y_sigmoid, c='red', label='sigmoid')
    ax2.set_ylim((-0.2, 1.2)); ax2.legend()

    # Tanh
    ax3.plot(x_grid_np, y_tanh, c='red', label='tanh')
    ax3.set_ylim((-1.2, 1.2)); ax3.legend()

    # Softmax
    ax4.plot(x_grid_np, y_softmax, c='red', label='softmax')
    ax4.set_ylim((-0.01, 0.06)); ax4.legend();
</code></pre>
<p>Let&rsquo;s compare the different activation functions.</p>
<pre><code class="language-python">make_new_figure_1()
</code></pre>
<p><img src="../img/08_neuralnets_56_0.png" alt="png"></p>
<p>ReLu is very popular since it&rsquo;s non-linear.</p>
<h2 id="83-optimization-and-gradient-descent">8.3 Optimization and Gradient Descent</h2>
<h3 id="gradient-descent">Gradient Descent</h3>
<p>Gradient descent works as follows:</p>
<ol start="0">
<li>Initialize the parameters</li>
<li>Compute the Loss</li>
<li>Compute the Gradients</li>
<li>Update the Parameters</li>
<li>Repeat (1)-(3) until convergence</li>
</ol>
<h3 id="gradient-descent-in-linear-regression">Gradient Descent in Linear Regression</h3>
<p>In order to understand how are NN optimized, we start with a linear regression example. Remember that linear regression can be interpreted as the simplest possible NN.</p>
<p>We generate the following data:</p>
<p>$$
y = 1 + 2 x - 3 x^2 + \varepsilon
$$</p>
<p>with $x \sim N(0,1)$ and $\varepsilon \sim N(0,0.1)$</p>
<pre><code class="language-python"># Data Generation
np.random.seed(42)
N = 100

x = np.sort(np.random.rand(N, 1), axis=0)
e = .1*np.random.randn(N, 1)
y_true = 1 + 2*x - 3*x**2
y = y_true + e
</code></pre>
<p>Let&rsquo;s plot the data.</p>
<pre><code class="language-python"># New figure 2
def make_new_figure_2():
    
    # Init
    fig, ax = plt.subplots(figsize=(8,6))
    fig.suptitle('Activation Functions')

    # Scatter
    ax.scatter(x,y); 
    ax.plot(x,y_true,color='orange'); 
    ax.set_xlabel('X'); ax.set_ylabel('Y');
    ax.legend(['y true','y']);
</code></pre>
<pre><code class="language-python">make_new_figure_2()
</code></pre>
<p><img src="../img/08_neuralnets_66_0.png" alt="png"></p>
<p>Suppose we try to fit the data with a linear model</p>
<p>$$
y = a + b x
$$</p>
<p>We proceed iteratively by gradient descent. Our objective function is the Mean Squared Error.</p>
<p><strong>Algorithm</strong></p>
<ol start="0">
<li>
<p>Take an initial guess of the parameters
$$
a = a_0 \
b = b_0
$$</p>
</li>
<li>
<p>Compute the Mean Squared Error
$$
\begin{array}
\text{MSE} &amp;= \frac{1}{N} \sum_{i=1}^{N}\left(y_{i}-\hat{y}<em>{i}\right)^{2} \
&amp;= \frac{1}{N} \sum</em>{i=1}^{N}\left(y_{i}-a-b x_{i}\right)^{2}
\end{array}
$$</p>
</li>
<li>
<p>Compute its derivative
$$
\begin{array}{l}
\frac{\partial M S E}{\partial a}=\frac{\partial M S E}{\partial \hat{y}<em>{i}} \cdot \frac{\partial \hat{y}</em>{i}}{\partial a}=\frac{1}{N} \sum_{i=1}^{N} 2\left(y_{i}-a-b x_{i}\right) \cdot(-1)=-2 \frac{1}{N} \sum_{i=1}^{N}\left(y_{i}-\hat{y}<em>{i}\right) \
\frac{\partial M S E}{\partial b}=\frac{\partial M S E}{\partial \hat{y}</em>{i}} \cdot \frac{\partial \hat{y}<em>{i}}{\partial b}=\frac{1}{N} \sum</em>{i=1}^{N} 2\left(y_{i}-a-b x_{i}\right) \cdot\left(-x_{i}\right)=-2 \frac{1}{N} \sum_{i=1}^{N} x_{i}\left(y_{i}-\hat{y}_{i}\right)
\end{array}
$$</p>
</li>
<li>
<p>Update the parameters
$$
\begin{array}{l}
a=a-\eta \frac{\partial M S E}{\partial a} \
b=b-\eta \frac{\partial M S E}{\partial b}
\end{array}
$$</p>
<p>Where $\eta$ is the <strong>learning rate</strong>. A lower learning rate makes learning more stable but slower.</p>
</li>
<li>
<p>Repeat (1)-(3) $T$ times, where the number of total iterations $T$ is called <strong>epochs</strong>.</p>
</li>
</ol>
<p>We start by taking a random guess of $\alpha$ and $\beta$.</p>
<pre><code class="language-python"># Initializes parameters &quot;a&quot; and &quot;b&quot; randomly
np.random.seed(42)
a = np.random.randn(1)
b = np.random.randn(1)

print(a, b)
</code></pre>
<pre><code>[0.49671415] [-0.1382643]
</code></pre>
<pre><code class="language-python"># Plot gradient 
def gradient_plot(x, y, y_hat, y_true, EPOCHS, losses):
    clear_output(wait=True)
    fig, (ax1, ax2) = plt.subplots(1,2, figsize=(12,5))
    
    # First figure
    ax1.clear()
    ax1.scatter(x, y)
    ax1.plot(x, y_true, 'orange')
    ax1.plot(x, y_hat, 'r-')
    ax1.set_title('Data and Fit')
    ax1.legend(['True', 'Predicted'])
    
    # Second figure
    ax2.clear()
    ax2.plot(range(len(losses)), losses, color='g')
    ax2.set_xlim(0,EPOCHS); ax2.set_ylim(0,1.1*np.max(losses))
    ax2.set_title('True MSE = %.4f' % losses[-1])
    
    # Plot
    plt.show();
</code></pre>
<p>We set the learning rate $\eta = 0.1$ and the number of epochs $T=200$</p>
<pre><code class="language-python"># parameters
LR = 0.1        # learning rate
EPOCHS = 200    # number of epochs
</code></pre>
<p>We can now plot the training and the result.</p>
<pre><code class="language-python"># New figure 3
def make_new_figure_3(a, b):
    
    # Init
    losses = []

    # train
    for t in range(EPOCHS):

        # compute loss
        y_hat = a + b * x
        error = (y - y_hat)
        loss = (error**2).mean()

        # compute gradient
        a_grad = -2 * error.mean()
        b_grad = -2 * (x * error).mean()

        # update parameters
        a -= LR * a_grad
        b -= LR * b_grad

        # plot
        losses += [loss]
        if (t+1) % (EPOCHS/25) == 0: # print 25 times
            gradient_plot(x, y, y_hat, y_true, EPOCHS, losses)

    print(a, b)
    return a, b
</code></pre>
<pre><code class="language-python">a_fit, b_fit = make_new_figure_3(a, b)
</code></pre>
<p><img src="../img/08_neuralnets_76_0.png" alt="png"></p>
<pre><code>[1.40589939] [-0.83739496]
</code></pre>
<p>Sanity Check: do we get the same results as our gradient descent?</p>
<pre><code class="language-python"># OLS estimates
ols = LinearRegression()
ols.fit(x, y)
print(ols.intercept_, ols.coef_[0])
</code></pre>
<pre><code>[1.4345303] [-0.89397853]
</code></pre>
<p>Close enough!</p>
<p>Let&rsquo;s plot both lines in the graph.</p>
<pre><code class="language-python"># New figure 4
def make_new_figure_4():
    
    # Init
    fig, ax = plt.subplots(figsize=(8,6))

    # Scatter
    ax.plot(x,y_true,color='orange'); 
    ax.plot(x,a_fit + b_fit*x,color='red'); 
    ax.plot(x,ols.predict(x),color='green'); 
    ax.legend(['y true','y gd', 'y ols'])
    ax.scatter(x,y); 
    ax.set_xlabel('X'); ax.set_ylabel('Y'); ax.set_title(&quot;Data&quot;);
</code></pre>
<pre><code class="language-python">make_new_figure_4()
</code></pre>
<p><img src="../img/08_neuralnets_82_0.png" alt="png"></p>
<p>Now we are going to do exactly the same but with <code>pytorch</code>.</p>
<h3 id="autograd">Autograd</h3>
<p>Autograd is PyTorch’s automatic differentiation package. Thanks to it, we don’t need to worry about partial derivatives, chain rule or anything like it.</p>
<p>So, how do we tell PyTorch to do its thing and compute all gradients? That’s what <code>backward()</code> is good for.
§
Do you remember the starting point for computing the gradients? It was the loss, as we computed its partial derivatives w.r.t. our parameters. Hence, we need to invoke the <code>backward()</code> method from the corresponding Python variable, like, <code>loss.backward()</code>.</p>
<p>What about the actual values of the gradients? We can inspect them by looking at the grad attribute of a tensor.</p>
<p>If you check the method’s documentation, it clearly states that gradients are accumulated. So, every time we use the
gradients to update the parameters, we need to zero the gradients afterwards. And that’s what zero_() is good for.</p>
<p>What does the underscore (_) at the end of the method name mean? Do you remember? If not, scroll back to the previous section and find out.</p>
<p>So, let’s ditch the manual computation of gradients and use both backward() and zero_() methods instead.</p>
<p>First, we convert our variables to tensors.</p>
<pre><code class="language-python"># Convert data to tensors
x_tensor = torch.from_numpy(x).float().to('cpu')
y_tensor = torch.from_numpy(y).float().to('cpu')
print(type(x), type(x_tensor))
</code></pre>
<pre><code>&lt;class 'numpy.ndarray'&gt; &lt;class 'torch.Tensor'&gt;
</code></pre>
<p>We take the initial parameters guess</p>
<pre><code class="language-python"># initial parameter guess
torch.manual_seed(42)
a = torch.randn(1, requires_grad=True, dtype=torch.float, device='cpu')
b = torch.randn(1, requires_grad=True, dtype=torch.float, device='cpu')
</code></pre>
<p>Now we are ready to fit the model.</p>
<pre><code class="language-python"># New figure 5
def make_new_figure_5(a, b):
    
    # Init
    losses = []

    # parameters
    LR = 0.1
    EPOCHS = 200

    # train
    for t in range(EPOCHS):

        # compute loss
        y_hat = a + b * x_tensor
        error = y_tensor - y_hat
        loss = (error ** 2).mean()

        # compute gradient
        loss.backward()

        # update parameters
        with torch.no_grad():
            a -= LR * a.grad
            b -= LR * b.grad

        # clear gradients
        a.grad.zero_()
        b.grad.zero_()

        # Plot
        losses += [((y_true - y_hat.detach().numpy())**2).mean()]
        if (t+1) % (EPOCHS/25) == 0: # print 25 times
            gradient_plot(x, y, y_hat.data.numpy(), y_true, EPOCHS, losses)

    print(a, b)
</code></pre>
<pre><code class="language-python">make_new_figure_5(a, b)
</code></pre>
<p><img src="../img/08_neuralnets_93_0.png" alt="png"></p>
<pre><code>tensor([1.3978], requires_grad=True) tensor([-0.8214], requires_grad=True)
</code></pre>
<h3 id="optimizer">Optimizer</h3>
<p>So far, we’ve been manually updating the parameters using the computed gradients. That’s probably fine for two parameters… but what if we had a whole lot of them?! We use one of PyTorch’s optimizers.</p>
<p>An optimizer takes the following arguments:</p>
<ul>
<li>the parameters we want to update</li>
<li>he learning rate we want to use</li>
<li>(possibly many other hyper-parameters)</li>
</ul>
<p>Moreover, we can now call the function <code>zero_grad()</code> to automatically update the parameters. In particular, we will need to perform the following steps at each iteration:</p>
<ul>
<li>Clear the parameters: <code>optimizer.zero_grad()</code></li>
<li>Compute the gradient: <code>loss.backward()</code></li>
<li>Update the parameters: <code>optimizer.step()</code></li>
</ul>
<p>In the code below, we create a Stochastic Gradient Descent (<code>SGD</code>) optimizer to update our parameters $a$ and $b$.</p>
<pre><code class="language-python"># Init parameters
torch.manual_seed(42)
a = torch.randn(1, requires_grad=True, dtype=torch.float, device='cpu')
b = torch.randn(1, requires_grad=True, dtype=torch.float, device='cpu')

# Defines a SGD optimizer to update the parameters
optimizer = torch.optim.SGD([a, b], lr=LR)
print(optimizer)
</code></pre>
<pre><code>SGD (
Parameter Group 0
    dampening: 0
    lr: 0.1
    momentum: 0
    nesterov: False
    weight_decay: 0
)
</code></pre>
<p>We can also define a default loss function so that we don&rsquo;t have to compute it by hand. We are going to use the <code>MSE</code> loss function.</p>
<pre><code class="language-python"># Define a loss function
loss_func = torch.nn.MSELoss()
print(loss_func)
</code></pre>
<pre><code>MSELoss()
</code></pre>
<p>Let&rsquo;s plot the estimator and the MSE.</p>
<pre><code class="language-python"># New figure 6
def make_new_figure_6(a, b):
    
    # parameters
    EPOCHS = 200

    # init 
    losses = []

    # train
    for t in range(EPOCHS):

        # compute loss
        y_hat = a + b * x_tensor
        error = y_tensor - y_hat
        loss = (error ** 2).mean()  

        # update parameters
        optimizer.zero_grad()   # clear gradients for next train
        loss.backward()         # backpropagation, compute gradients
        optimizer.step()        # apply gradients, update parameters

        # Plot
        losses += [((y_true - y_hat.detach().numpy())**2).mean()]
        if (t+1) % (EPOCHS/25) == 0:
            gradient_plot(x, y, y_hat.data.numpy(), y_true, EPOCHS, losses)

    print(a, b)
</code></pre>
<pre><code class="language-python">make_new_figure_6(a, b)
</code></pre>
<p><img src="../img/08_neuralnets_103_0.png" alt="png"></p>
<pre><code>tensor([1.3978], requires_grad=True) tensor([-0.8214], requires_grad=True)
</code></pre>
<h3 id="building-a-nn">Building a NN</h3>
<p>In our model, we manually created two parameters to perform a linear regression. Let’s use PyTorch’s <code>Sequential</code> module to create our neural network.</p>
<p>We first want to build the linear regression framework</p>
<p>$$
y = a + b x
$$</p>
<p>Which essentially is a network with</p>
<ul>
<li>1 input</li>
<li>no hidden layer</li>
<li>no activation function</li>
<li>1 output</li>
</ul>
<p>Let&rsquo;s build the simplest possible neural network with <code>PyTorch</code>.</p>
<pre><code class="language-python"># Simplest possible neural network
linear_net = torch.nn.Sequential(
    torch.nn.Linear(1, 1)
)

print(linear_net)
</code></pre>
<pre><code>Sequential(
  (0): Linear(in_features=1, out_features=1, bias=True)
)
</code></pre>
<p>Now, if we call the <code>parameters()</code> method of this model, PyTorch will figure the parameters of its attributes in a recursive way.</p>
<pre><code class="language-python">[*linear_net.parameters()]
</code></pre>
<pre><code>[Parameter containing:
 tensor([[-0.2191]], requires_grad=True),
 Parameter containing:
 tensor([0.2018], requires_grad=True)]
</code></pre>
<p>We can now define the definitive training function.</p>
<pre><code class="language-python">def train_NN(x, y, y_true, net, optimizer, loss_func, EPOCHS):
    
    # transform variables
    x_tensor = torch.from_numpy(x).float().to('cpu')
    y_tensor = torch.from_numpy(y).float().to('cpu')

    # init 
    losses = []
    
    # train
    for t in range(EPOCHS):        

        # compute loss
        y_hat = net(x_tensor)     
        loss = loss_func(y_hat, y_tensor)    
        
        # update parameters
        optimizer.zero_grad()   # clear gradients for next train
        loss.backward()         # backpropagation, compute gradients
        optimizer.step()        # apply gradients, update parameters

        # plot
        losses += [((y_true - y_hat.detach().numpy())**2).mean()]
        if (t+1) % (EPOCHS/25) == 0: # print 25 times
            gradient_plot(x, y, y_hat.data.numpy(), y_true, EPOCHS, losses)
</code></pre>
<p>Now we are ready to train our neural network.</p>
<pre><code class="language-python">optimizer = torch.optim.SGD(linear_net.parameters(), lr=LR)

# train
train_NN(x, y, y_true, linear_net, optimizer, loss_func, EPOCHS)
</code></pre>
<p><img src="../img/08_neuralnets_113_0.png" alt="png"></p>
<p>We now define a more complicated NN. In particular we, build a neural network with</p>
<ul>
<li>1 input</li>
<li>1 hidden layer with 10 neurons and Relu activation function</li>
<li>1 output layer</li>
</ul>
<pre><code class="language-python"># Relu Net
relu_net = torch.nn.Sequential(
    torch.nn.Linear(1, 10),
    torch.nn.ReLU(),
    torch.nn.Linear(10, 1)
)

print(relu_net)
</code></pre>
<pre><code>Sequential(
  (0): Linear(in_features=1, out_features=10, bias=True)
  (1): ReLU()
  (2): Linear(in_features=10, out_features=1, bias=True)
)
</code></pre>
<p>This network has much more parameters.</p>
<pre><code class="language-python">[*relu_net.parameters()]
</code></pre>
<pre><code>[Parameter containing:
 tensor([[-0.4869],
         [ 0.5873],
         [ 0.8815],
         [-0.7336],
         [ 0.8692],
         [ 0.1872],
         [ 0.7388],
         [ 0.1354],
         [ 0.4822],
         [-0.1412]], requires_grad=True),
 Parameter containing:
 tensor([ 0.7709,  0.1478, -0.4668,  0.2549, -0.4607, -0.1173, -0.4062,  0.6634,
         -0.7894, -0.4610], requires_grad=True),
 Parameter containing:
 tensor([[-0.0893, -0.1901,  0.0298, -0.3123,  0.2856, -0.2686,  0.2441,  0.0526,
          -0.1027,  0.1954]], requires_grad=True),
 Parameter containing:
 tensor([0.0493], requires_grad=True)]
</code></pre>
<p>We are again using Stochastic Gradient Descent (<code>SGD</code>) as optimization algorithm and Mean Squared Error (<code>MSELoss</code>) as objective function.</p>
<pre><code class="language-python"># parameters
LR = 0.1
EPOCHS = 1000

# optimizer and loss function
optimizer = torch.optim.SGD(relu_net.parameters(), lr=LR)
loss_func = torch.nn.MSELoss()

# Train
train_NN(x, y, y_true, relu_net, optimizer, loss_func, EPOCHS)
</code></pre>
<p><img src="../img/08_neuralnets_119_0.png" alt="png"></p>
<p>It seems that we can use fewer nodes to get the same result.</p>
<p>Let&rsquo;s make a smallet network.</p>
<pre><code class="language-python"># Relu Net
relu_net2 = torch.nn.Sequential(
    torch.nn.Linear(1, 4),
    torch.nn.ReLU(),
    torch.nn.Linear(4, 1)
)
</code></pre>
<p>And train it.</p>
<pre><code class="language-python"># parameters
LR = 0.1
EPOCHS = 1000

# optimizer and loss function
optimizer = torch.optim.SGD(relu_net2.parameters(), lr=LR)
loss_func = torch.nn.MSELoss()

# Train
train_NN(x, y, y_true, relu_net2, optimizer, loss_func, EPOCHS)
</code></pre>
<p><img src="../img/08_neuralnets_124_0.png" alt="png"></p>
<p>We can try different activation functions.</p>
<p>For example the tangent.</p>
<pre><code class="language-python"># TanH Net
tanh_net = torch.nn.Sequential(
    torch.nn.Linear(1, 10),
    torch.nn.Tanh(),
    torch.nn.Linear(10, 1)
)
</code></pre>
<pre><code class="language-python"># parameters
LR = 0.2
EPOCHS = 1000

# optimizer and loss function
optimizer = torch.optim.SGD(tanh_net.parameters(), lr=LR)
loss_func = torch.nn.MSELoss()

# train
train_NN(x, y, y_true, tanh_net, optimizer, loss_func, EPOCHS)
</code></pre>
<p><img src="../img/08_neuralnets_127_0.png" alt="png"></p>
<h3 id="loss-functions">Loss functions</h3>
<p>So far we have used the Stochastic  as loss function.</p>
<p>Notice that <code>nn.MSELoss</code> actually creates a loss function for us — it is NOT the loss function itself. Moreover, you can specify a reduction method to be applied, that is, how do you want to aggregate the results for individual points — you can average them (reduction=<code>mean</code>) or simply sum them up (reduction=<code>sum</code>).</p>
<p>We are now going to use different ones.</p>
<pre><code class="language-python"># parameters
LR = 0.1
EPOCHS = 25

# nets
n = torch.nn.Sequential(torch.nn.Linear(1, 10), torch.nn.ReLU(), torch.nn.Linear(10, 1))
nets = [n,n,n,n]

# optimizers
optimizers = [torch.optim.SGD(n.parameters(), lr=LR) for n in nets]

# different loss functions
loss_MSE        = torch.nn.MSELoss()
loss_L1         = torch.nn.L1Loss()
loss_NLL        = torch.nn.NLLLoss()
loss_KLD        = torch.nn.KLDivLoss()
loss_funcs = [loss_MSE, loss_L1, loss_NLL, loss_KLD]
</code></pre>
<p>This is the description of the loss functions:</p>
<ul>
<li>
<p><code>MSELoss</code>: Creates a criterion that measures the mean squared error (squared L2 norm) between each element in the input $x$ and target $y$.</p>
</li>
<li>
<p><code>L1Loss</code>: Creates a criterion that measures the mean absolute error (MAE) between each element in the input $x$ and target $y$.</p>
</li>
<li>
<p><code>NLLLoss</code>: The negative log likelihood loss.</p>
</li>
<li>
<p><code>KLDivLoss</code>: The Kullback-Leibler divergence loss measure</p>
</li>
</ul>
<pre><code class="language-python"># Train multiple nets
def train_nets(x, y, y_true, nets, optimizers, loss_funcs, labels, EPOCHS):

    # Put dateset into torch dataset
    x_tensor = torch.from_numpy(x).float().to('cpu')
    y_tensor = torch.from_numpy(y).float().to('cpu')
    torch_dataset = Data.TensorDataset(x_tensor, y_tensor)
    
    # Init
    losses = np.zeros((0,4))
    
    # Train
    for epoch in range(EPOCHS): # for each epoch
        losses = np.vstack((losses, np.zeros((1,4))))
        for k, net, opt, lf in zip(range(4), nets, optimizers, loss_funcs):
            y_hat = net(x_tensor)              # get output for every net
            loss = loss_func(y_hat, y_tensor)  # compute loss for every net
            opt.zero_grad()                    # clear gradients for next train
            loss.backward()                    # backpropagation, compute gradients
            opt.step()                         # apply gradients
            losses[-1,k] = ((y_true - y_hat.detach().numpy())**2).mean()
        plot_losses(losses, labels, EPOCHS)
</code></pre>
<pre><code class="language-python"># Plot losses 
def plot_losses(losses, labels, EPOCHS):
    clear_output(wait=True)
    fig, ax = plt.subplots(1,1, figsize=(10,6))
    
    # Plot
    ax.clear()
    ax.plot(range(len(losses)), losses)
    ax.set_xlim(0,EPOCHS-1); ax.set_ylim(0,1.1*np.max(losses))
    ax.set_title('Compare Losses'); ax.set_ylabel('True MSE')
    legend_txt = ['%s=%.4f' % (label, loss) for label,loss in zip(labels, losses[-1,:])]
    ax.legend(legend_txt)
    
    # Shot
    plt.show();
</code></pre>
<p>Let&rsquo;s compare them.</p>
<pre><code class="language-python"># Train
labels = ['MSE', 'L1', 'LogL', 'KLdiv']
train_nets(x, y, y_true, nets, optimizers, loss_funcs, labels, EPOCHS)
</code></pre>
<p><img src="../img/08_neuralnets_135_0.png" alt="png"></p>
<p>In this very simple case, all loss functions are very similar.</p>
<h3 id="optimizers">Optimizers</h3>
<p>So far we have used the Stochastic Gradient Descent to fit the neural network. We are now going to use different ones.</p>
<p>This is the <a href="https://pytorch.org/docs/stable/optim.html" target="_blank" rel="noopener">description of the optimizers</a>:</p>
<ul>
<li>
<p><code>SGD</code>: Implements stochastic gradient descent (optionally with momentum).</p>
</li>
<li>
<p><code>Momentum</code>: Nesterov momentum is based on the formula from <a href="http://www.cs.toronto.edu/~hinton/absps/momentum.pdf" target="_blank" rel="noopener">On the importance of initialization and momentum in deep learning</a>.</p>
</li>
<li>
<p><code>RMSprop</code>: Proposed by G. Hinton in his <a href="https://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf" target="_blank" rel="noopener">course</a>. The centered version first appears in <a href="https://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf" target="_blank" rel="noopener">Generating Sequences With Recurrent Neural Networks</a>. The implementation here takes the square root of the gradient average before adding epsilon (note that TensorFlow interchanges these two operations). The effective learning rate is thus $\frac{\alpha}{\sqrt{v} + \epsilon}$ where $\alpha$ is the scheduled learning rate and $v$ is the weighted moving average of the squared gradient.</p>
</li>
<li>
<p><code>Adam</code>: Proposed in <a href="https://arxiv.org/abs/1412.6980" target="_blank" rel="noopener">Adam: A Method for Stochastic Optimization</a>. The implementation of the L2 penalty follows changes proposed in <a href="https://arxiv.org/abs/1711.05101" target="_blank" rel="noopener">Decoupled Weight Decay Regularization</a>.</p>
</li>
</ul>
<pre><code class="language-python"># parameters
LR = 0.1
EPOCHS = 25

# nets
n = torch.nn.Sequential(torch.nn.Linear(1, 10), torch.nn.ReLU(), torch.nn.Linear(10, 1))
nets = [n,n,n,n]

# different optimizers
opt_SGD         = torch.optim.SGD(nets[0].parameters(), lr=LR)
opt_Momentum    = torch.optim.SGD(nets[1].parameters(), lr=LR, momentum=0.8)
opt_RMSprop     = torch.optim.RMSprop(nets[2].parameters(), lr=LR, alpha=0.9)
opt_Adam        = torch.optim.Adam(nets[3].parameters(), lr=LR, betas=(0.9, 0.99))
optimizers = [opt_SGD, opt_Momentum, opt_RMSprop, opt_Adam]

# loss functions
l = torch.nn.MSELoss()
loss_funcs = [l,l,l,l]
</code></pre>
<p>Let&rsquo;s prot the loss functions over training, for different optimizers.</p>
<pre><code class="language-python"># train
labels = ['SGD', 'Momentum', 'RMSprop', 'Adam']
train_nets(x, y, y_true, nets, optimizers, loss_funcs, labels, EPOCHS)
</code></pre>
<p><img src="../img/08_neuralnets_141_0.png" alt="png"></p>
<h3 id="training-on-batch">Training on batch</h3>
<p>Until now, we have used the whole training data at every training step. It has been batch gradient descent all along.</p>
<p>This is fine for our ridiculously small dataset, sure, but if we want to go serious about all this, we must use mini-batch gradient descent. Thus, we need mini-batches. Thus, we need to slice our dataset accordingly. Do you want to do it manually?! Me neither!</p>
<p>So we use PyTorch’s <code>DataLoader</code> class for this job. We tell it which dataset to use (the one we just built in the previous section), the desired mini-batch size and if we’d like to shuffle it or not. That’s it!</p>
<p>Our loader will behave like an iterator, so we can loop over it and fetch a different mini-batch every time.</p>
<pre><code class="language-python"># Init data
x_tensor = torch.from_numpy(x).float().to('cpu')
y_tensor = torch.from_numpy(y).float().to('cpu')
torch_dataset = Data.TensorDataset(x_tensor, y_tensor)

# Build DataLoader
BATCH_SIZE = 25
loader = Data.DataLoader(
    dataset=torch_dataset,      # torch TensorDataset format
    batch_size=BATCH_SIZE,      # mini batch size
    shuffle=True,               # random shuffle for training
)
</code></pre>
<p>Let&rsquo;s try using sub-samples of dimension <code>BATCH_SIZE = 25</code>.</p>
<pre><code class="language-python">def train_NN_batch(loader, y_true, net, optimizer, loss_func, EPOCHS):
    
    # init
    losses = []

    # train
    for t in range(EPOCHS):   
        # train entire dataset 3 times
        for step, (batch_x, batch_y) in enumerate(loader):

            # compute loss
            y_hat = net(batch_x)     
            loss = loss_func(y_hat, batch_y)    

            # update parameters
            optimizer.zero_grad()   # clear gradients for next train
            loss.backward()         # backpropagation, compute gradients
            optimizer.step()        # apply gradients

        # plt every epoch
        y_hat = net(x_tensor)  
        losses += [((y_true - y_hat.detach().numpy())**2).mean()]
        if (t+1) % (EPOCHS/25) == 0:
            gradient_plot(x, y, y_hat.data.numpy(), y_true, EPOCHS, losses)
</code></pre>
<pre><code class="language-python"># parameters
LR = 0.1
EPOCHS = 1000
net = torch.nn.Sequential(torch.nn.Linear(1, 10), torch.nn.ReLU(), torch.nn.Linear(10, 1))
optimizer = torch.optim.SGD(net.parameters(), lr=LR)
loss_func = torch.nn.MSELoss()

# Train
train_NN_batch(loader, y_true, net, optimizer, loss_func, EPOCHS)
</code></pre>
<p><img src="../img/08_neuralnets_147_0.png" alt="png"></p>
<p>Two things are different now: not only we have an inner loop to load each and every mini-batch from our DataLoader but, more importantly, we are now sending only one mini-batch to the device.</p>
<p>For bigger datasets, loading data sample by sample (into a CPU tensor) using Dataset’s <code>__get_item__</code> and then sending all samples that belong to the same mini-batch at once to your GPU (device) is the way to go in order to make the best use of your graphics card’s RAM.</p>
<p>Moreover, if you have many GPUs to train your model on, it is best to keep your dataset “agnostic” and assign the batches to different GPUs during training.</p>
<h2 id="84-advanced-topics">8.4 Advanced Topics</h2>
<h3 id="issues">Issues</h3>
<h4 id="starting-values">Starting Values</h4>
<p>Usually starting values for weights are chosen to be random values near zero. Hence the model starts out nearly linear, and becomes nonlinear as the weights increase.</p>
<h4 id="overfitting">Overfitting</h4>
<p>In early developments of neural networks, either by design or by accident, an early stopping rule was used to avoid overfitting.</p>
<p>A more explicit method for regularization is <em>weight decay</em>.</p>
<h4 id="scaling-of-the-inputs">Scaling of the Inputs</h4>
<p>Since the scaling of the inputs determines the effective scaling of the weights in the bottom layer, it can have a large effect on the quality of the final solution. At the outset it is best to standardize all inputs to have mean zero and standard deviation one.</p>
<h4 id="number-of-hidden-units-and-layers">Number of Hidden Units and Layers</h4>
<p>Generally speaking it is better to have too many hidden units than too few. With too few hidden units, the model might not have enough flexibility to capture the nonlinearities in the data; with too many hidden units, the extra weights can be shrunk toward zero if appropriate regularization is used.</p>
<p>Choice of the number of hidden layers is guided by background knowledge and experimentation. Each layer extracts features of the input for regression or classification. Use of multiple hidden layers allows construction of hierarchical features at different levels of resolution.</p>
<p>You can get an intuition on the role of hidden layers here: <a href="https://playground.tensorflow.org/" target="_blank" rel="noopener">https://playground.tensorflow.org/</a></p>
<h4 id="multiple-minima">Multiple Minima</h4>
<p>The error function R(θ) is nonconvex, possessing many local minima. One approach is to use the average predictions over the collection of networks as the final prediction. Another approach is via <em>bagging</em>.</p>
<h3 id="deep-neural-networks-and-deep-learning">Deep Neural Networks and Deep Learning</h3>
<p>Deep Neural Networks are just Neural Networks with more than one hidden layer.</p>
<h3 id="convolutional-neural-nets">Convolutional Neural Nets</h3>
<p>Convolutional Neural Nets are often applied when dealing with image/video data. They are usually coded with each feature being a pixel and its value is the pixel color (3 dimensional RGB array).</p>
<img src="../figures/cnn1.png" style="width: 400px;"/>
<p>Videos and images have 2 main characteristics:</p>
<ul>
<li>have lots of features</li>
<li>&ldquo;close&rdquo; features are often similar</li>
</ul>
<p>Convolutional Neural Nets exploit the second characteristic to alleviate the computational problems arising from the first. They do it by constructing a first layer that does not build on evey feature but only on adjacent ones.</p>
<img src="../figures/cnn1.gif" style="width: 400px;"/>
<p>In this way, most of the information is preserved, on a lower dimensional representation.</p>
<h3 id="recurrent-neural-nets">Recurrent Neural Nets</h3>
<p>Recurrent Neural Networks are often applied in contexts in which the data generating process is dynamic. The most important example is Natural Language Processing. The idea is that you want to make predictions &ldquo;live&rdquo; as data comes in. Moreover, the order of the data is relevant, so that you also what to keep track of what the model has learned so far.</p>
<p>While RNNs learn similarly while training, in addition, they remember things learnt from prior input(s) while generating output(s). It’s part of the network. RNNs can take one or more input vectors and produce one or more output vectors and the output(s) are influenced not just by weights applied on inputs like a regular NN, but also by a “hidden” state vector representing the context based on prior input(s)/output(s). So, the same input could produce a different output depending on previous inputs in the series.</p>
<p>Grafically:</p>
<img src="../figures/rnn1.png" style="width: 600px;"/>
<p>In summary, in a vanilla neural network, a fixed size input vector is transformed into a fixed size output vector. Such a network becomes “recurrent” when you repeatedly apply the transformations to a series of given input and produce a series of output vectors.</p>
<h3 id="bidirectional-rnn">Bidirectional RNN</h3>
<p>Sometimes it’s not just about learning from the past to predict the future, but we also need to look into the future to fix the past. In speech recognition and handwriting recognition tasks, where there could be considerable ambiguity given just one part of the input, we often need to know what’s coming next to better understand the context and detect the present.</p>
<img src="../figures/rnn2.png" style="width: 600px;"/>
<p>This does introduce the obvious challenge of how much into the future we need to look into, because if we have to wait to see all inputs then the entire operation will become costly.</p>
<h3 id="recursive-neural-netw">Recursive Neural Netw</h3>
<p>A recurrent neural network parses the inputs in a sequential fashion. A recursive neural network is similar to the extent that the transitions are repeatedly applied to inputs, but not necessarily in a sequential fashion. Recursive Neural Networks are a more general form of Recurrent Neural Networks. It can operate on any hierarchical tree structure. Parsing through input nodes, combining child nodes into parent nodes and combining them with other child/parent nodes to create a tree like structure. Recurrent Neural Networks do the same, but the structure there is strictly linear. i.e. weights are applied on the first input node, then the second, third and so on.</p>
<img src="../figures/rnn3.png" style="width: 600px;"/>
<p>But this raises questions pertaining to the structure. How do we decide that? If the structure is fixed like in Recurrent Neural Networks then the process of training, backprop etc makes sense in that they are similar to a regular neural network. But if the structure isn’t fixed, is that learnt as well?</p>

          </div>

          



          
          
          <div class="article-widget">
            
<div class="post-nav">
  
  
  
  <div class="post-nav-item">
    <div class="meta-nav">Previous</div>
    <a href="/course/ml-econ/07_trees/" rel="next">Tree-based Methods</a>
  </div>
  
  
  
  <div class="post-nav-item">
    <div class="meta-nav">Next</div>
    <a href="/course/ml-econ/09_postdoubleselection/" rel="prev">Post-Double Selection</a>
  </div>
  
</div>

          </div>
          
        </div>

        <div class="body-footer">

          





          




          


  
  



        </div>

      </article>

      <footer class="site-footer">

  

  
  <p class="powered-by">
    
      <a href="/privacy/">Privacy Policy</a>
    
    
       &middot; 
      <a href="/terms/">Terms</a>
    
  </p>
  

  

  
  







</footer>


    </main>
  </div>
</div>

  </div>

  <div class="page-footer">
    
    
  </div>

  
<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        <pre><code class="tex hljs"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

      

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/instant.page/5.1.0/instantpage.min.js" integrity="sha512-1+qUtKoh9XZW7j+6LhRMAyOrgSQKenQ4mluTR+cvxXjP1Z54RxZuzstR/H9kgPXQsVB8IW7DMDFUJpzLjvhGSQ==" crossorigin="anonymous"></script>

      
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js" integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin="anonymous"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js" integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin="anonymous"></script>
      

      
      

      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mermaid/8.8.4/mermaid.min.js" integrity="sha512-as1BF4+iHZ3BVO6LLDQ7zrbvTXM+c/1iZ1qII/c3c4L8Rn5tHLpFUtpaEtBNS92f+xGsCzsD7b62XP3XYap6oA==" crossorigin="anonymous" title="mermaid"></script>
      

      
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/highlight.min.js" integrity="sha512-TDKKr+IvoqZnPzc3l35hdjpHD0m+b2EC2SrLEgKDRWpxf2rFCxemkgvJ5kfU48ip+Y+m2XVKyOCD85ybtlZDmw==" crossorigin="anonymous"></script>
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/languages/r.min.js"></script>
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/languages/python.min.js"></script>
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/languages/julia.min.js"></script>
        
      

    

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.7.1/leaflet.min.js" integrity="sha512-SeiQaaDh73yrb56sTW/RgVdi/mMqNeM2oBwubFHagc5BkixSpP1fvqF47mKzPGWYSSy4RwbBunrJBQ4Co8fRWA==" crossorigin="anonymous"></script>
    

    
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/anchor-js/4.2.2/anchor.min.js" integrity="sha512-I7w3ZdSFzw5j3jU3ZkNikBNeIrl3i+hEuEdwNmqUJvwNcaBUNcijnP2gd9DtGlgVYDplfjGoD8vTNsID+lCjqg==" crossorigin="anonymous"></script>
    <script>
      anchors.add();
    </script>
    

    
    
    
      
      <script id="search-hit-fuse-template" type="text/x-template">
        <div class="search-hit" id="summary-{{key}}">
          <div class="search-hit-content">
            <div class="search-hit-name">
              <a href="{{relpermalink}}">{{title}}</a>
              <div class="article-metadata search-hit-type">{{type}}</div>
              <p class="search-hit-description">{{snippet}}</p>
            </div>
          </div>
        </div>
      </script>
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js" integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin="anonymous"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js" integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin="anonymous"></script>
      
    

    
    

    
    
    
    

    
    <script src="/js/bootstrap.bundle.min.6aed84840afc03ab4d5750157f69c120.js"></script>

    
    
      
      
      
      
      
      
      
    

    
    
    
    
    
    
    
    
      
      
    
    
    <script src="/en/js/wowchemy.min.4ea9cc8d09c5c158656ac1a804743b34.js"></script>

    
  <script async defer src="https://buttons.github.io/buttons.js"></script>




</body>
</html>
