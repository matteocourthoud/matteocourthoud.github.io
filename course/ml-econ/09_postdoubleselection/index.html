<!DOCTYPE html><html lang="en-us" >

<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  
  
  
    <meta name="generator" content="Wowchemy 5.1.0 for Hugo" />
  

  

  
  
  
  
  
    
    
    
  
  

  

  
  
  
    
  
  <meta name="description" content="# Remove warnings import warnings warnings.filterwarnings(&#39;ignore&#39;)  # Import everything import pandas as pd import numpy as np import seaborn as sns import statsmodels.api as sm from numpy.linalg import inv from statsmodels." />

  
  <link rel="alternate" hreflang="en-us" href="https://matteocourthoud.github.io/course/ml-econ/09_postdoubleselection/" />

  







  




  
  
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
  

  
  
  
    <meta name="theme-color" content="#003f5c" />
  

  
  
    
    <script src="/js/mathjax-config.js"></script>
  

  
  
  
  
    
    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha256-FMvZuGapsJLjouA6k7Eo2lusoAX9i0ShlWFG6qt7SLc=" crossorigin="anonymous">

    
    

    
    
    
      
    
    
      
      
        
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/styles/github.min.css" crossorigin="anonymous" title="hl-light" media="print" onload="this.media='all'">
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/styles/dracula.min.css" crossorigin="anonymous" title="hl-dark" media="print" onload="this.media='all'" disabled>
        
      
    

    
    
    
      <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.7.1/leaflet.min.css" integrity="sha512-1xoFisiGdy9nvho8EgXuXvnpR5GAMSjFwp40gSRE3NwdUdIMIKuPa7bqoUhLD0O/5tPNhteAsE5XyyMi5reQVA==" crossorigin="anonymous" media="print" onload="this.media='all'">
    

    

    
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js" integrity="" crossorigin="anonymous" async></script>
      
    
      
      

      
      

      
    
      
      

      
      

      
    
  

  
  
  
    
      
      <link rel="preload" as="style" href="https://fonts.googleapis.com/css2?family=Roboto+Mono&family=Source+Sans+Pro:wght@200;300;400;700&display=swap">
      <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Roboto+Mono&family=Source+Sans+Pro:wght@200;300;400;700&display=swap" media="print" onload="this.media='all'">
    
  

  
  
  
  
  
  <link rel="stylesheet" href="/css/wowchemy.5c4def4f00a521426f4eb098155f3342.css" />

  




<script async src="https://www.googletagmanager.com/gtag/js?id=UA-144780600-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];

  function gtag() {
      dataLayer.push(arguments);
  }

  function trackOutboundLink(url, target) {
    gtag('event', 'click', {
         'event_category': 'outbound',
         'event_label': url,
         'transport_type': 'beacon',
         'event_callback': function () {
           if (target !== '_blank') {
             document.location = url;
           }
         }
    });
    console.debug("Outbound link clicked: " + url);
  }

  function onClickCallback(event) {
    if ((event.target.tagName !== 'A') || (event.target.host === window.location.host)) {
      return;
    }
    trackOutboundLink(event.target, event.target.getAttribute('target'));  
  }

  gtag('js', new Date());
  gtag('config', 'UA-144780600-1', { 'anonymize_ip': true });
  gtag('set', {'cookie_flags': 'SameSite=None;Secure'});

  
  document.addEventListener('click', onClickCallback, false);
</script>


  

  

  




  
  
  

  

  
    <link rel="manifest" href="/index.webmanifest" />
  

  <link rel="icon" type="image/png" href="/media/icon_hu03e9b3967b83fd39296ec9da5ff1ea05_201175_32x32_fill_lanczos_center_3.png" />
  <link rel="apple-touch-icon" type="image/png" href="/media/icon_hu03e9b3967b83fd39296ec9da5ff1ea05_201175_180x180_fill_lanczos_center_3.png" />

  <link rel="canonical" href="https://matteocourthoud.github.io/course/ml-econ/09_postdoubleselection/" />

  
  
  
  
  
  
  
  
    
    
  
  
  <meta property="twitter:card" content="summary" />
  
  <meta property="og:site_name" content="Matteo Courthoud" />
  <meta property="og:url" content="https://matteocourthoud.github.io/course/ml-econ/09_postdoubleselection/" />
  <meta property="og:title" content="Post-Double Selection | Matteo Courthoud" />
  <meta property="og:description" content="# Remove warnings import warnings warnings.filterwarnings(&#39;ignore&#39;)  # Import everything import pandas as pd import numpy as np import seaborn as sns import statsmodels.api as sm from numpy.linalg import inv from statsmodels." /><meta property="og:image" content="https://matteocourthoud.github.io/media/icon_hu03e9b3967b83fd39296ec9da5ff1ea05_201175_512x512_fill_lanczos_center_3.png" />
    <meta property="twitter:image" content="https://matteocourthoud.github.io/media/icon_hu03e9b3967b83fd39296ec9da5ff1ea05_201175_512x512_fill_lanczos_center_3.png" /><meta property="og:locale" content="en-us" />
  
    
      <meta
        property="article:published_time"
        content="2022-03-09T00:00:00&#43;00:00"
      />
    
    <meta property="article:modified_time" content="2022-03-09T00:00:00&#43;00:00">
  

  



  

  
  
  
  
  
    <script src="https://cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.1.1/cookieconsent.min.js" integrity="sha256-5VhCqFam2Cn+yjw61zbBNrbHVJ6SRydPeKopYlngbiQ=" crossorigin="anonymous"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.1.1/cookieconsent.min.css" integrity="sha256-zQ0LblD/Af8vOppw18+2anxsuaz3pWYyVWi+bTvTH8Q=" crossorigin="anonymous">
  
  <script>
  window.addEventListener("load", function(){
    window.cookieconsent.initialise({
      "palette": {
        "popup": {
          "background": "#003f5c",
          "text": "rgb(255, 255, 255)"
        },
        "button": {
          "background": "rgb(255, 255, 255)",
          "text": "#003f5c"
        }
      },
      "theme": "classic",
      "content": {
        "message": "This website uses cookies to ensure you get the best experience on our website.",
        "dismiss": "Got it!",
        "link": "Learn more",
        "href": "/privacy/"
      }
    })});
  </script>


  





  <title>Post-Double Selection | Matteo Courthoud</title>
</head>


<body id="top" data-spy="scroll" data-offset="70" data-target="#TableOfContents" class="page-wrapper   " data-wc-page-id="18f7a71852cf7865ca2bcf5562ea90ac" >

  
  
  
  
  
  
  
  
  <script src="/js/wowchemy-init.min.6edaf3b475ce43de30d98828aea698be.js"></script>

  


<aside class="search-modal" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Search</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#" aria-label="Close"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        
        <input name="q" id="search-query" placeholder="Search..." autocapitalize="off"
        autocomplete="off" autocorrect="off" spellcheck="false" type="search" class="form-control"
        aria-label="Search...">
        
      </div>

      
      

      

    </section>
    <section class="section-search-results">

      <div id="search-hits">
        
      </div>

    </section>
  </div>
</aside>



  <div class="page-header">
    












<nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id="navbar-main">
  <div class="container-xl">

    
    <div class="d-none d-lg-inline-flex">
      <a class="navbar-brand" href="/">Matteo Courthoud</a>
    </div>
    

    
    <button type="button" class="navbar-toggler" data-toggle="collapse"
            data-target="#navbar-content" aria-controls="navbar-content" aria-expanded="false" aria-label="Toggle navigation">
    <span><i class="fas fa-bars"></i></span>
    </button>
    

    
    <div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none">
      <a class="navbar-brand" href="/">Matteo Courthoud</a>
    </div>
    

    
    
    <div class="navbar-collapse main-menu-item collapse justify-content-end" id="navbar-content">

      
      <ul class="navbar-nav d-md-inline-flex">
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
            
            
            
              
            
            
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#about"><span>Home</span></a>
        </li>

        
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
            
            
            
              
            
            
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#projects"><span>Research</span></a>
        </li>

        
        

        
        <li class="nav-item dropdown">
          <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown" aria-haspopup="true"><span>Courses</span><span class="caret"></span>
          </a>
          <div class="dropdown-menu">
            
              <a class="dropdown-item" href="/course/ml-econ/"><span>Machine Learning for Economics</span></a>
            
              <a class="dropdown-item" href="/course/data-science/"><span>Data Science with Python</span></a>
            
              <a class="dropdown-item" href="/course/empirical-io/"><span>PhD Industrial Organization</span></a>
            
              <a class="dropdown-item" href="/course/metrics/"><span>PhD Econometrics</span></a>
            
              <a class="dropdown-item" href="https://pp4rs.github.io/"><span>Programming Practices for Research</span></a>
            
          </div>
        </li>

        
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/post/"><span>Posts</span></a>
        </li>

        
        

        

        
        
        

        

        
        
        
        

        
          
            
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/cv"><span>CV</span></a>
        </li>

        
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
            
            
            
              
            
            
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#contact"><span>Contact</span></a>
        </li>

        
        

      

        
      </ul>
    </div>

    <ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2">

      
      
        
      

      
      

      
      

      
      

    </ul>

  </div>
</nav>


  </div>

  <div class="page-body">
    

<div class="container-fluid docs">
  <div class="row flex-xl-nowrap">
    <div class="col-12 col-md-3 col-xl-2 docs-sidebar">
      
<nav class="collapse docs-links" id="docs-nav">
  
  
  
  
  
  

  
  
    

    
      

      <ul class="nav docs-sidenav">
        <li><a href="/course/"><i class="fas fa-arrow-left pr-1"></i>Courses</a></li>
      </ul>

      
      
        
          
        
      


  
    
    
    
    
      
    
    

    
      <div class="docs-toc-item">
        <a class="docs-toc-link " href="/course/ml-econ/">ML for Economics</a>
    
      
        <ul class="nav docs-sidenav">
      


  <li class=""><a href="/course/ml-econ/01_regression/">Linear Regression</a></li>



  <li class=""><a href="/course/ml-econ/02_iv/">Instrumental Variables</a></li>



  <li class=""><a href="/course/ml-econ/03_nonparametric/">Non-Parametric Regression</a></li>



  <li class=""><a href="/course/ml-econ/04_crossvalidation/">Resampling Methods</a></li>



  <li class=""><a href="/course/ml-econ/05_regularization/">Model Selection and Regularization</a></li>



  <li class=""><a href="/course/ml-econ/06_convexity/">Convexity and Optimization</a></li>



  <li class=""><a href="/course/ml-econ/07_trees/">Tree-based Methods</a></li>



  <li class=""><a href="/course/ml-econ/08_neuralnets/">Neural Networks</a></li>



  <li class="active"><a href="/course/ml-econ/09_postdoubleselection/">Post-Double Selection</a></li>



  <li class=""><a href="/course/ml-econ/10_unsupervised/">Unsupervised Learning</a></li>

      
        </ul>
      
    

    
      </div>
    

    
  
</nav>

    </div>

    
    
    <div class="d-none d-xl-block col-xl-2 docs-toc">
      

      <ul class="nav toc-top">
        <li><a href="#" id="back_to_top" class="docs-toc-title">Contents</a></li>
      </ul>

      <nav id="TableOfContents">
  <ul>
    <li><a href="#91-frisch-waugh-theorem">9.1 Frisch-Waugh theorem</a></li>
    <li><a href="#92-omitted-variable-bias">9.2 Omitted Variable Bias</a>
      <ul>
        <li></li>
      </ul>
    </li>
    <li><a href="#93-pre-test-bias">9.3 Pre-Test Bias</a>
      <ul>
        <li><a href="#pre-testing-and-machine-learning">Pre-Testing and Machine Learning</a></li>
      </ul>
    </li>
    <li><a href="#94-post-double-selection">9.4 Post-Double Selection</a>
      <ul>
        <li><a href="#post-double-selection-and-machine-learning">Post-double Selection and Machine Learning</a></li>
      </ul>
    </li>
    <li><a href="#95-doubledebiased-machine-learning">9.5 Double/debiased Machine Learning</a>
      <ul>
        <li></li>
        <li><a href="#application-to-ajr02">Application to AJR02</a></li>
      </ul>
    </li>
  </ul>
</nav>

      
    </div>
    

    <main class="col-12 col-md-9 col-xl-8 py-md-3 pl-md-5 docs-content" role="main">

      <article class="article">

        <div class="docs-article-container">

          <h1>Post-Double Selection</h1>

          <p>Last updated on Mar 9, 2022</p>

          <div class="article-style">
            <pre><code class="language-python"># Remove warnings
import warnings
warnings.filterwarnings('ignore')
</code></pre>
<pre><code class="language-python"># Import everything
import pandas as pd
import numpy as np
import seaborn as sns
import statsmodels.api as sm

from numpy.linalg import inv
from statsmodels.iolib.summary2 import summary_col
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression, Lasso, Ridge
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.preprocessing import PolynomialFeatures, StandardScaler
</code></pre>
<pre><code class="language-python"># Import matplotlib for graphs
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import axes3d

# Set global parameters
%matplotlib inline
plt.style.use('seaborn-white')
plt.rcParams['lines.linewidth'] = 3
plt.rcParams['figure.figsize'] = (10,6)
plt.rcParams['figure.titlesize'] = 20
plt.rcParams['axes.titlesize'] = 18
plt.rcParams['axes.labelsize'] = 14
plt.rcParams['legend.fontsize'] = 14
</code></pre>
<h2 id="91-frisch-waugh-theorem">9.1 Frisch-Waugh theorem</h2>
<p>Consider the data $D = { x_i, y_i, z_i }_{i=1}^n$ with DGP:</p>
<p>$$
y_i = x_i' \alpha_0+ z_i' \beta_0 + \varepsilon_i
$$</p>
<p>. The following estimators of $\alpha$ are numerically equivalent (if $[x, z]$ has full rank):</p>
<ul>
<li>OLS: $\hat{\alpha}$ from regressing $y$ on $x, z$</li>
<li>Partialling out: $\tilde{\alpha}$ from regressing $y$ on $\tilde{x}$</li>
<li>&ldquo;Double&rdquo; partialling out: $\bar{\alpha}$ from regressing $\tilde{y}$ on $\tilde{x}$</li>
</ul>
<p>where the operation of passing to $y, x$ to $\tilde{y}, \tilde{x}$ is called <em>projection  out $z$</em>, e.g. $\tilde{x}$ are the residuals from regressing $x$ on $z$.</p>
<p>$$
\tilde{x} = x - \hat \gamma z = (I - z (z' z)^{-1} z' ) x = (I-P_z) x = M_z x
$$</p>
<p>I.e we have done the following:</p>
<ol>
<li>regress $x$ on $z$</li>
<li>compute $\hat x$</li>
<li>compute the residuals $\tilde x = x - \hat x$</li>
</ol>
<p>We now explore the theorem through simulation. In particular, we generate a sample from the following model:</p>
<p>$$
y_i = x_i - 0.3 z_i + \varepsilon_i
$$</p>
<p>where $x_i,z_i,\varepsilon_i \sim N(0,1)$ and $n=1000$.</p>
<pre><code class="language-python">np.random.seed(1)

# Init
n = 1000
a = 1
b = -.3

# Generate data
x = np.random.uniform(0,1,n).reshape(-1,1)
z = np.random.uniform(0,1,n).reshape(-1,1)
e = np.random.normal(0,1,n).reshape(-1,1)
y = a*x + b*z + e
</code></pre>
<p>Let&rsquo;s compute the value of the OLS estimator.</p>
<pre><code class="language-python"># Estimate alpha by OLS
xz = np.concatenate([x,z], axis=1)
ols_coeff = inv(xz.T @ xz) @ xz.T @ y
alpha_ols = ols_coeff[0][0]

print('alpha OLS: %.4f (true=%1.0f)' % (alpha_ols, a))
</code></pre>
<pre><code>alpha OLS: 1.0928 (true=1)
</code></pre>
<p>The partialling out estimator.</p>
<pre><code class="language-python"># Partialling out
x_tilde = (np.eye(n) - z @ inv(z.T @ z) @ z.T ) @ x
alpha_po = inv(x_tilde.T @ x_tilde) @ x_tilde.T @ y

print('alpha partialling out: %.4f (true=%1.0f)' % (alpha_po, a))
</code></pre>
<pre><code>alpha partialling out: 1.0928 (true=1)
</code></pre>
<p>And lastly, the double-partialling out estimator.</p>
<pre><code class="language-python"># &quot;Double&quot; partialling out
y_tilde = (np.eye(n) - z @ inv(z.T @ z) @ z.T ) @ y
alpha_po2 = inv(x_tilde.T @ x_tilde) @ x_tilde.T @ y_tilde

print('alpha double partialling out: %.4f (true=%1.0f)' % (alpha_po2, a))
</code></pre>
<pre><code>alpha double partialling out: 1.0928 (true=1)
</code></pre>
<h2 id="92-omitted-variable-bias">9.2 Omitted Variable Bias</h2>
<p>Consider two separate statistical models. Assume the following <strong>long regression</strong> of interest:</p>
<p>$$
y_i = x_i' \alpha_0+ z_i' \beta_0 + \varepsilon_i
$$</p>
<p>Define the corresponding <strong>short regression</strong> as</p>
<p>$$
y_i = x_i' \alpha_0 + v_i \quad \text{ with } \quad x_i = z_i' \gamma_0 + u_i
$$</p>
<h4 id="ovb-theorem">OVB Theorem</h4>
<p>Suppose that the DGP for the long regression corresponds to $\alpha_0$, $\beta_0$. Suppose further that $\mathbb E[x_i] = 0$, $\mathbb E[z_i] = 0$, $\mathbb E[\varepsilon_i |x_i,z_i] = 0$. Then, unless $\beta_0 = 0$ or $z_i$ is orthogonal to $x_i$, the (sole) stochastic regressor $x_i$ is correlated with the error term in the short regression which implies that the OLS estimator of the short regression is inconsistent for $\alpha_0$ due to the omitted variable bias. In particular, one can show that the plim of the OLS estimator of $\hat{\alpha}_{SHORT}$ from the short regression is</p>
<p>$$
\hat{\alpha}_{SHORT} \overset{p}{\to} \frac{Cov(y_i, x_i)}{Var(x_i)} = \alpha_0 + \beta_0 \frac{Cov(z_i, x_i)}{Var(x_i)}
$$</p>
<p>Consider data $D= (y_i, x_i, z_i)_{i=1}^n$, where the true model is:</p>
<p>$$
\begin{aligned}
&amp; y_i = x_i' \alpha_0  + z_i' \beta_0 + \varepsilon_i \
&amp; x_i = z_i' \gamma_0 + u_i
\end{aligned}
$$</p>
<p>Let&rsquo;s investigate the Omitted Variable Bias by simulation. In particular, we generate a sample from the following model:</p>
<p>$$
\begin{aligned}
&amp; y_i = x_i - 0.3 z_i + \varepsilon_i \
&amp; x_i = 3 z_i + u_i \
\end{aligned}
$$</p>
<p>where $z_i,\varepsilon_i,u_i \sim N(0,1)$ and $n=1000$.</p>
<pre><code class="language-python">def generate_data(a, b, c, n):

    # Generate data
    z = np.random.normal(0,1,n).reshape(-1,1)
    u = np.random.normal(0,1,n).reshape(-1,1)
    x = c*z + u
    e = np.random.normal(0,1,n).reshape(-1,1)
    y = a*x + b*z + e
    
    return x, y, z
</code></pre>
<p>First let&rsquo;s compute the value of the OLS estimator.</p>
<pre><code class="language-python"># Init
n = 1000
a = 1
b = -.3
c = 3
x, y, z = generate_data(a, b, c, n)

# Estimate alpha by OLS
ols_coeff = inv(x.T @ x) @ x.T @ y
alpha_short = ols_coeff[0][0]

print('alpha OLS: %.4f (true=%1.0f)' % (alpha_short, a))
</code></pre>
<pre><code>alpha OLS: 0.9115 (true=1)
</code></pre>
<p>In our case the expected bias is:</p>
<p>$$
\begin{aligned}
Bias &amp; = \beta_0 \frac{Cov(z_i, x_i)}{Var(x_i)} = \
&amp; = \beta_0 \frac{Cov(z_i' \gamma_0 + u_i, x_i)}{Var(z_i' \gamma_0 + u_i)} = \
&amp; = \beta_0 \frac{\gamma_0 Var(z_i)}{\gamma_0^2 Var(z_i) + Var(u_i)}
\end{aligned}
$$</p>
<p>which in our case is $b \frac{c}{c^2 + 1}$.</p>
<pre><code class="language-python"># Expected bias
bias = alpha_short - a
exp_bias = b * c / (c**2 + 1)

print('Empirical bias: %.4f \nExpected bias:  %.4f' % (bias, exp_bias))
</code></pre>
<pre><code>Empirical bias: -0.0885 
Expected bias:  -0.0900
</code></pre>
<h2 id="93-pre-test-bias">9.3 Pre-Test Bias</h2>
<p>Consider data $D= (y_i, x_i, z_i)_{i=1}^n$, where the true model is:</p>
<p>$$
\begin{aligned}
&amp; y_i = x_i' \alpha_0  + z_i' \beta_0 + \varepsilon_i \
&amp; x_i = z_i' \gamma_0 + u_i
\end{aligned}
$$</p>
<p>Where $x_i$ is the variable of interest (we want to make inference on $\alpha_0$) and $z_i$ is a high dimensional set of control variables.</p>
<p>From now on, we will work under the following assumptions:</p>
<ul>
<li>$\dim(x_i)=1$ for all $n$</li>
<li>$\beta_0$ uniformely bounded in $n$</li>
<li>Strict exogeneity: $\mathbb E[\varepsilon_i | x_i, z_i] = 0$ and $\mathbb E[u_i | z_i] = 0$</li>
<li>$\beta_0$ and $\gamma_0$ have dimension (and hence value) that depend on $n$</li>
</ul>
<p>Pre-Testing procedure:</p>
<ol>
<li>Regress $y_i$ on $x_i$ and $z_i$</li>
<li>For each $j = 1, &hellip;, p = \dim(z_i)$ calculate a test statistic $t_j$</li>
<li>Let $\hat{T} = { j: |t_j| &gt; C &gt; 0 }$ for some constant $C$ (set of statistically significant coefficients).</li>
<li>Re-run the new &ldquo;model&rdquo; using $(x_i, z_{\hat{T},i})$ (i.e. using the selected covariates with statistically significant coefficients).</li>
<li>Perform statistical inference (i.e. confidence intervals and hypothesis tests) as if no model selection had been done.</li>
</ol>
<p>Pre-testing leads to incorrect inference. Why? Because of test errors in the first stage.</p>
<pre><code class="language-python"># T-test
def t_test(y, x, k):
    beta_hat = inv(x.T @ x) @ x.T @ y
    residuals = y - x @ beta_hat
    sigma2_hat = np.var(residuals)
    beta_std = np.sqrt(np.diag(inv(x.T @ x)) * sigma2_hat )
    return beta_hat[k,0]/beta_std[k]
</code></pre>
<p>First of all the t-test for $H_0: \beta_0 = 0$:</p>
<p>$$
t = \frac{\hat \beta_k}{\hat \sigma_{\beta_k}}
$$</p>
<p>where the standard deviation of the ols coefficient is given by</p>
<p>$$
\hat \sigma_{\beta_k} = \sqrt{ \hat \sigma^2 \cdot (X&rsquo;X)^{-1}_{[k,k]} }
$$</p>
<p>where we estimate the variance of the error term with the variance of the residuals</p>
<p>$$
\hat \sigma^2 = Var \big( y - \hat y \big) = Var \big( y - X (X&rsquo;X)^{-1}X&rsquo;y \big)
$$</p>
<pre><code class="language-python"># Pre-testing
def pre_testing(a, b, c, n, simulations=1000):
    np.random.seed(1)
    
    # Init
    alpha = {'Long': np.zeros((simulations,1)),
            'Short': np.zeros((simulations,1)),
            'Pre-test': np.zeros((simulations,1))}

    # Loop over simulations
    for i in range(simulations):
        
        # Generate data
        x, y, z = generate_data(a, b, c, n)
        xz = np.concatenate([x,z], axis=1)
        
        # Compute coefficients
        alpha['Long'][i] = (inv(xz.T @ xz) @ xz.T @ y)[0][0]
        alpha['Short'][i] = inv(x.T @ x) @ x.T @ y
        
        # Compute significance of z on y
        t = t_test(y, xz, 1)
        
        # Select specification based on test
        if np.abs(t)&gt;1.96:
            alpha['Pre-test'][i] = alpha['Long'][i]
        else:
            alpha['Pre-test'][i] = alpha['Short'][i]
    
    return alpha
</code></pre>
<p>Let&rsquo;s compare the different estimates.</p>
<pre><code class="language-python"># Get pre_test alpha
alpha = pre_testing(a, b, c, n)

for key, value in alpha.items():
    print('Mean alpha %s = %.4f' % (key, np.mean(value)))
</code></pre>
<pre><code>Mean alpha Long = 0.9994
Mean alpha Short = 0.9095
Mean alpha Pre-test = 0.9925
</code></pre>
<p>The pre-testing coefficient is very close to the true coefficient.</p>
<p>However, the main effect of pre-testing is on inference. With pre-testing, the distribution of the estimator is not gaussian anymore.</p>
<pre><code class="language-python">def plot_alpha(alpha, a):
    
    fig = plt.figure(figsize=(17,6))

    # Plot distributions
    x_max = np.max([np.max(np.abs(x-a)) for x in alpha.values()])

    # All axes
    for i, key in enumerate(alpha.keys()):
        
        # Reshape exisiting subplots
        k = len(fig.axes)
        for i in range(k):
            fig.axes[i].change_geometry(1, k+1, i+1)
            
        # Add new plot
        ax = fig.add_subplot(1, k+1, k+1)
        ax.hist(alpha[key], bins=30)
        ax.set_title(key)
        ax.set_xlim([a-x_max, a+x_max])
        ax.axvline(a, c='r', ls='--')
        legend_text = [r'$\alpha_0=%.0f$' % a, r'$\hat \alpha=%.4f$' % np.mean(alpha[key])]
        ax.legend(legend_text, prop={'size': 10})
</code></pre>
<p>Let&rsquo;s compare the long, short and pre-test estimators.</p>
<pre><code class="language-python"># Plot
plot_alpha(alpha, a)
</code></pre>
<p><img src="../img/09_postdoubleselection_36_0.png" alt="png"></p>
<p>As we can see, the main problem of pre-testing is inference.</p>
<p>Because of the testing procedure, the distribution of the estimator is a combination of tho different distributions: the one resulting from the long regression and the one resulting from the short regression. <strong>Pre-testing is not a problem in 3 cases</strong>:</p>
<ul>
<li>
<p>when $\beta_0$ is very large: in this case the test always rejects the null hypothesis $H_0 : \beta_0=0$ and we always run the correct specification, i.e. the long regression</p>
</li>
<li>
<p>when $\beta_0$ is very small: in this case the test has very little power. However, as we saw from the Omitted Variable Bias formula, the bias is small.</p>
</li>
<li>
<p>when $\gamma_0$ is very small: also in this case the test has very little power. However, as we saw from the Omitted Variable Bias formula, the bias is small.</p>
</li>
</ul>
<p>Let&rsquo;s compare the pre-test estimates for different values of the true parameter $\beta$.</p>
<pre><code class="language-python"># Case 1: different betas and same sample size
b_sequence = b*np.array([0.1,0.3,1,3])
alpha = {}

# Get sequence
for k, b_ in enumerate(b_sequence):
    label = 'beta = %.2f' % b_
    alpha[label] = pre_testing(a, b_, c, n)['Pre-test']
    print('Mean alpha with beta=%.2f: %.4f' % (b_, np.mean(alpha[label])))
</code></pre>
<pre><code>Mean alpha with beta=-0.03: 0.9926
Mean alpha with beta=-0.09: 0.9826
Mean alpha with beta=-0.30: 0.9925
Mean alpha with beta=-0.90: 0.9994
</code></pre>
<p>The means are similar, but let&rsquo;s look at the distributions.</p>
<pre><code class="language-python"># Plot
plot_alpha(alpha, a)
</code></pre>
<p><img src="../img/09_postdoubleselection_41_0.png" alt="png"></p>
<p>When $\beta_0$ is &ldquo;small&rdquo;, the distribution of the pre-testing estimator for $\alpha$ is not normal.</p>
<p>However, the magnitue of $\beta_0$ is a relative concept. For an infinite sample size, $\beta_0$ is always going to be &ldquo;big enough&rdquo;, in the sense that with an infinite sample size the probability fo false positives in testing $H_0: \beta_0 = 0$ is going to zero. I.e. we always select the correct model specification, the long regression.</p>
<p>Let&rsquo;s have a look at the distibution of $\hat \alpha_{\text{PRE-TEST}}$ when the sample size increaes.</p>
<pre><code class="language-python"># Case 2: same beta and different sample sizes
n_sequence = [100,300,1000,3000]
alpha = {}

# Get sequence
for k, n_ in enumerate(n_sequence):
    label = 'n = %.0f' % n_
    alpha[label] = pre_testing(a, b, c, n_)['Pre-test']
    print('Mean alpha with n=%.0f: %.4f' % (n_, np.mean(alpha[label])))
</code></pre>
<pre><code>Mean alpha with n=100: 0.9442
Mean alpha with n=300: 0.9635
Mean alpha with n=1000: 0.9925
Mean alpha with n=3000: 0.9989
</code></pre>
<pre><code class="language-python"># Plot
plot_alpha(alpha, a)
</code></pre>
<p><img src="../img/09_postdoubleselection_44_0.png" alt="png"></p>
<p>As we can see, for large samples, $\beta_0$ is never &ldquo;small&rdquo;. In the limit, when $n \to \infty$, the probability of false positives while testing $H_0: \beta_0 = 0$ goes to zero.</p>
<p>We face a dilemma:</p>
<ul>
<li>pre-testing is clearlly a problem in finite samples</li>
<li>all our econometric results are based on the assumption that $n \to \infty$</li>
</ul>
<p>The problem is solved by assuming that the value of $\beta_0$ depends on the sample size. This might seems like a weird assumption but is just to have an asymptotically meaningful concept of &ldquo;big&rdquo; and &ldquo;small&rdquo;.</p>
<p>We now look at what happens in the simulations when $\beta_0$ is proportional to $\frac{1}{\sqrt{n}}$.</p>
<pre><code class="language-python"># Case 3: beta proportional to 1/sqrt(n) and different sample sizes
beta =  b * 30 / np.sqrt(n_sequence)

# Get sequence
alpha = {}
for k, n_ in enumerate(n_sequence):
    label = 'n = %.0f' % n_
    alpha[label] = pre_testing(a, beta[k], c, n_)['Pre-test']
    print('Mean alpha with n=%.0f: %.4f' % (n_, np.mean(alpha[label])))
</code></pre>
<pre><code>Mean alpha with n=100: 0.9703
Mean alpha with n=300: 0.9838
Mean alpha with n=1000: 0.9914
Mean alpha with n=3000: 0.9947
</code></pre>
<pre><code class="language-python"># Plot
plot_alpha(alpha, a)
</code></pre>
<p><img src="../img/09_postdoubleselection_48_0.png" alt="png"></p>
<p>Now the distribution of $\hat \alpha$ does not converge to a normal when the sample size increases.</p>
<h3 id="pre-testing-and-machine-learning">Pre-Testing and Machine Learning</h3>
<p>How are machine learning and pre-testing related? The best example is Lasso. Suppose you have a dataset with many variables. This means that you have very few degrees of freedom and your OLS estimates are going to be very imprecise. At the extreme, you have more variables than observations so that your OLS coefficient is undefined since you cannot invert the design matrix $X&rsquo;X$.</p>
<p>In this case, you might want to do variable selection. One way of doing variable selection is pre-testing. Another way is Lasso. A third alternative is to use machine learning methods that do not suffer this curse of dimensionality.</p>
<p>The purpose and outcome of pre-testing and Lasso are the same:</p>
<ul>
<li>you have too many variables</li>
<li>you exclude some of them from the regression / set their coefficients to zero</li>
</ul>
<p>As a consequence, also the problems are the same, i.e. pre-test bias.</p>
<h2 id="94-post-double-selection">9.4 Post-Double Selection</h2>
<p>Consider again data $D= (y_i, x_i, z_i)_{i=1}^n$, where the true model is:</p>
<p>$$
\begin{aligned}
&amp; y_i = x_i' \alpha_0  + z_i' \beta_0 + \varepsilon_i \
&amp; x_i = z_i' \gamma_0 + u_i
\end{aligned}
$$</p>
<p>We would like to guard against pretest bias if possible, in order to handle high dimensional models. A good pathway towards motivating procedures which guard against pretest bias is a discussion of classical partitioned regression.</p>
<p>Consider a regression $y_i$ on $x_i$ and $z_i$. $x_i$ is the 1-dimensional variable of interest, $z_i$ is a high-dimensional set of control variables. We have the following procedure:</p>
<ol>
<li><strong>First Stage</strong> selection: regress $x_i$ on $z_i$. Select the statistically significant variables in the set $S_{FS} \subseteq z_i$</li>
<li><strong>Reduced Form</strong> selection: lasso $y_i$ on $z_i$. Select the statistically significant variables in the set $S_{RF} \subseteq z_i$</li>
<li>Regress $y_i$ on $x_i$ and $S_{FS} \cup S_{RF}$</li>
</ol>
<p><strong>Theorem</strong>:
Let ${P^n}$ be a sequence of data-generating processes for $D_n = (y_i, x_i, z_i)^n_{i=1} \in (\mathbb R \times \mathbb R \times \mathbb R^p) ^n$ where $p$ depends on $n$. For each $n$, the data are iid with $yi =  x_i'\alpha_0^{(n)} + z_i' \beta_0^{(n)} + \varepsilon_i$ and $x_i = z_i' \gamma_0^{(n)} + u_i$ where $\mathbb E[\varepsilon_i | x_i,z_i] = 0$ and $\mathbb E[u_i|z_i] = 0$. The sparsity of the vectors  $\beta_0^{(n)}$, $\gamma_0^{(n)}$ is controlled by $|| \beta_0^{(n)} ||_0 \leq s$ with $s^2 (\log p)^2/n \to 0$. Suppose that additional regularity conditions on the model selection procedures and moments of the random variables $y_i$ , $x_i$ , $z_i$ as documented in Belloni et al. (2014). Then the confidence intervals, CI, from the post double selection procedure are uniformly valid. That is, for any confidence level $\xi \in (0, 1)$
$$
\Pr(\alpha_0 \in CI) \to 1- \xi
$$</p>
<p>In order to have valid confidence intervals you want their bias to be negligibly. Since
$$
CI = \left[ \hat{\alpha} \pm \frac{1.96 \cdot \hat{\sigma}}{\sqrt{n}} \right]
$$</p>
<p>If the bias is $o \left( \frac{1}{\sqrt{n}} \right)$ then there is no problem since it is asymptotically negligible w.r.t. the magnitude of the confidence interval. If however the the bias is $O \left( \frac{1}{\sqrt{n}} \right)$ then it has the same magnitude of the confidence interval and it does not asymptotically vanish.</p>
<p>The idea of the proof is to use partitioned regression. An alternative way to think about the argument is: bound the omitted variables bias. Omitted variable bias comes from the product of 2 quantities related to the omitted variable:</p>
<ol>
<li>Its partial correlation with the outcome, and</li>
<li>Its partial correlation with the variable of interest.</li>
</ol>
<p>If both those partial correlations are $O( \sqrt{\log p/n})$, then the omitted variables bias is $(s \times O( \sqrt{\log p/n})^2 = o \left( \frac{1}{\sqrt{n}} \right)$, provided $s^2 (\log p)^2/n \to 0$. Relative to the $ \frac{1}{\sqrt{n}} $ convergence rate, the omitted variables bias is negligible.</p>
<p>In our omitted variable bias case, we want $| \beta_0 \gamma_0 | = o \left( \frac{1}{\sqrt{n}} \right)$.  Post-double selection guarantees that</p>
<ul>
<li><em>Reduced form</em> selection (pre-testing): any &ldquo;missing&rdquo; variable has $|\beta_{0j}| \leq \frac{c}{\sqrt{n}}$</li>
<li><em>First stage</em> selection (additional): any &ldquo;missing&rdquo; variable has $|\gamma_{0j}| \leq \frac{c}{\sqrt{n}}$</li>
</ul>
<p>As a consequence, as long as the number of omitted variables is finite, the omitted variable bias is
$$
OVB(\alpha) = |\beta_{0j}| \cdot|\gamma_{0j}| \leq \frac{c}{\sqrt{n}} \cdot \frac{c}{\sqrt{n}} = \frac{c^2}{n} = o \left(\frac{1}{\sqrt{n}}\right)
$$</p>
<pre><code class="language-python"># Pre-testing code
def post_double_selection(a, b, c, n, simulations=1000):
    np.random.seed(1)
    
    # Init
    alpha = {'Long': np.zeros((simulations,1)),
            'Short': np.zeros((simulations,1)),
            'Pre-test': np.zeros((simulations,1)),
            'Post-double': np.zeros((simulations,1))}

    # Loop over simulations
    for i in range(simulations):
        
        # Generate data
        x, y, z = generate_data(a, b, c, n)
        
        # Compute coefficients
        xz = np.concatenate([x,z], axis=1)
        alpha['Long'][i] = (inv(xz.T @ xz) @ xz.T @ y)[0][0]
        alpha['Short'][i] = inv(x.T @ x) @ x.T @ y
        
        # Compute significance of z on y (beta hat)
        t1 = t_test(y, xz, 1)
        
        # Compute significance of z on x (gamma hat)
        t2 = t_test(x, z, 0)
        
        # Select specification based on first test
        if np.abs(t1)&gt;1.96:
            alpha['Pre-test'][i] = alpha['Long'][i]
        else:
            alpha['Pre-test'][i] = alpha['Short'][i]
            
        # Select specification based on both tests
        if np.abs(t1)&gt;1.96 or np.abs(t2)&gt;1.96:
            alpha['Post-double'][i] = alpha['Long'][i]
        else:
            alpha['Post-double'][i] = alpha['Short'][i]
    
    return alpha
</code></pre>
<p>Let&rsquo;s now repeat the same exercise as above, but with also post-double selection</p>
<pre><code class="language-python"># Get pre_test alpha
alpha = post_double_selection(a, b, c, n)

for key, value in alpha.items():
    print('Mean alpha %s = %.4f' % (key, np.mean(value)))
</code></pre>
<pre><code>Mean alpha Long = 0.9994
Mean alpha Short = 0.9095
Mean alpha Pre-test = 0.9925
Mean alpha Post-double = 0.9994
</code></pre>
<pre><code class="language-python"># Plot
plot_alpha(alpha, a)
</code></pre>
<p><img src="../img/09_postdoubleselection_62_0.png" alt="png"></p>
<p>As we can see, post-double selection has solved the pre-testing problem. Does it work for any magnitude of $\beta$ (relative to the sample size)?</p>
<p>We first have a look at the case in which the sample size is fixed and $\beta_0$ changes.</p>
<pre><code class="language-python"># Case 1: different betas and same sample size
b_sequence = b*np.array([0.1,0.3,1,3])
alpha = {}

# Get sequence
for k, b_ in enumerate(b_sequence):
    label = 'beta = %.2f' % b_
    alpha[label] = post_double_selection(a, b_, c, n)['Post-double']
    print('Mean alpha with beta=%.2f: %.4f' % (b_, np.mean(alpha[label])))
</code></pre>
<pre><code>Mean alpha with beta=-0.03: 0.9994
Mean alpha with beta=-0.09: 0.9994
Mean alpha with beta=-0.30: 0.9994
Mean alpha with beta=-0.90: 0.9994
</code></pre>
<pre><code class="language-python"># Plot
plot_alpha(alpha, a)
</code></pre>
<p><img src="../img/09_postdoubleselection_65_0.png" alt="png"></p>
<p>Post-double selection always selects the correct specification, the long regression, even when $\beta$ is very small.</p>
<p>Now we check the same but for fixed $\beta_0$ and different sample sizes.</p>
<pre><code class="language-python"># Case 2: same beta and different sample sizes
n_sequence = [100,300,1000,3000]
alpha = {}

# Get sequence
for k, n_ in enumerate(n_sequence):
    label = 'N = %.0f' % n_
    alpha[label] = post_double_selection(a, b, c, n_)['Post-double']
    print('Mean alpha with n=%.0f: %.4f' % (n_, np.mean(alpha[label])))
</code></pre>
<pre><code>Mean alpha with n=100: 0.9964
Mean alpha with n=300: 0.9985
Mean alpha with n=1000: 0.9994
Mean alpha with n=3000: 0.9990
</code></pre>
<pre><code class="language-python"># Plot
plot_alpha(alpha, a)
</code></pre>
<p><img src="../img/09_postdoubleselection_69_0.png" alt="png"></p>
<p>Post-double selection always selects the correct specification, the long regression, even when the sample size is very small.</p>
<p>Last, we check the case of $\beta_0$ proportional to $\frac{1}{\sqrt{n}}$.</p>
<pre><code class="language-python"># Case 3: beta proportional to 1/sqrt(n) and different sample sizes
beta =  b * 30 / np.sqrt(n_sequence)

# Get sequence
alpha = {}
for k, n_ in enumerate(n_sequence):
    label = 'N = %.0f' % n_
    alpha[label] = post_double_selection(a, beta[k], c, n_)['Post-double']
    print('Mean alpha with n=%.0f: %.4f' % (n_, np.mean(alpha[label])))
</code></pre>
<pre><code>Mean alpha with n=100: 0.9964
Mean alpha with n=300: 0.9985
Mean alpha with n=1000: 0.9994
Mean alpha with n=3000: 0.9990
</code></pre>
<pre><code class="language-python"># Plot
plot_alpha(alpha, a)
</code></pre>
<p><img src="../img/09_postdoubleselection_73_0.png" alt="png"></p>
<p>Once again post-double selection always selects the correct specification, the long regression.</p>
<h3 id="post-double-selection-and-machine-learning">Post-double Selection and Machine Learning</h3>
<p>As we have seen at the end of the previous section, Lasso can be used to perform variable selection in high dimensional settings. Therefore, post-double selection solves the pre-test bias problem in those settings. The post-double selection procedure with Lasso is:</p>
<ol>
<li><strong>First Stage</strong> selection: lasso $x_i$ on $z_i$. Let the selected variables be collected in the set $S_{FS} \subseteq z_i$</li>
<li><strong>Reduced Form</strong> selection: lasso $y_i$ on $z_i$. Let the selected variables be collected in the set $S_{RF} \subseteq z_i$</li>
<li>Regress $y_i$ on $x_i$ and $S_{FS} \cup S_{RF}$</li>
</ol>
<h2 id="95-doubledebiased-machine-learning">9.5 Double/debiased Machine Learning</h2>
<p>This section is taken from <a href="https://onlinelibrary.wiley.com/doi/abs/10.1111/ectj.12097" target="_blank" rel="noopener">Chernozhukov, V., Chetverikov, D., Demirer, M., Duflo, E., Hansen, C., Newey, W., &amp; Robins, J. (2018). &ldquo;<em>Double/debiased machine learning for treatment and structural parameters</em>&quot;</a>.</p>
<p>Consider the following partially linear model</p>
<p>$$
y = \beta_0 D + g_0(X) + u \
D = m_0(X) + v
$$</p>
<p>where $y$ is the outcome variable, $D$ is the treatment to interest and $X$ is a potentially high-dimensional set of controls.</p>
<h4 id="naive-approach">Naive approach</h4>
<p>A naive approach to estimation of $\beta_0$ using ML methods would be, for example, to construct a sophisticated ML estimator $\beta_0 D + g_0(X)$ for learning the regression function $\beta_0 D$ + $g_0(X)$.</p>
<ol>
<li>Split the sample in two: main sample and auxiliary sample</li>
<li>Use the auxiliary sample to estimate $\hat g_0(X)$</li>
<li>Use the main sample to compute the orthogonalized component of $Y$ on $X$: $\hat u = \left(Y_{i}-\hat{g}<em>{0}\left(X</em>{i}\right)\right)$</li>
<li>Use the main sample to estimate the residualized OLS estimator</li>
</ol>
<p>$$
\hat{\beta}<em>{0}=\left(\frac{1}{n} \sum</em>{i \in I} D_{i}^{2}\right)^{-1} \frac{1}{n} \sum_{i \in I} D_{i} \hat u_i
$$</p>
<p>This estimator is going to have two problems:</p>
<ol>
<li>Slow rate of convergence, i.e. slower than $\sqrt(n)$</li>
<li>It will be biased because we are employing highdimensional regularized estimators (e.g. we are doing variable selection)</li>
</ol>
<h4 id="orthogonalization">Orthogonalization</h4>
<p>Now consider a second construction that employs an orthogonalized formulation obtained by directly partialling out the effect of $X$ from $D$ to obtain the orthogonalized regressor $v = D − m_0(X)$.</p>
<ol>
<li>
<p>Split the sample in two: main sample and auxiliary sample</p>
</li>
<li>
<p>Use the auxiliary sample to estimate $\hat g_0(X)$ from</p>
<p>$$
y = \beta_0 D + g_0(X) + u \
$$</p>
</li>
<li>
<p>Use the auxiliary sample to estimate $\hat m_0(X)$ from</p>
<p>$$
D = m_0(X) + v
$$</p>
</li>
<li>
<p>Use the main sample to compute the orthogonalized component of $D$ on $X$ as</p>
<p>$$
\hat v = D - \hat m_0(X)
$$</p>
</li>
<li>
<p>Use the main sample to estimate the double-residualized OLS estimator as</p>
<p>$$
\hat{\beta}<em>{0}=\left(\frac{1}{n} \sum</em>{i \in I} \hat v_i D_{i} \right)^{-1} \frac{1}{n} \sum_{i \in I} \hat v_i \left( Y - \hat g_0(X) \right)
$$</p>
</li>
</ol>
<p>The estimator is unbiased but still has a lower rate of convergence because of sample splitting. The problem is solved by inverting the split sample, re-estimating the coefficient and averaging the two estimates. Note that this procedure is valid since the two estimates are independent by the sample splitting procedure.</p>
<h3 id="application-to-ajr02">Application to AJR02</h3>
<p>In this section we are going to replicate 6.3 of the &ldquo;<em>Double/debiased machine learning</em>&rdquo; paper based on <a href="https://economics.mit.edu/files/4123" target="_blank" rel="noopener">Acemoglu, Johnson, Robinson (2002), &ldquo;<em>The Colonial Origins of Comparative Development</em>&quot;</a>.</p>
<p>We first load the dataset</p>
<pre><code class="language-python"># Load Acemoglu Johnson Robinson Dataset
df = pd.read_csv('data/AJR02.csv',index_col=0)
</code></pre>
<pre><code class="language-python">df.head()
</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
</code></pre>
<p></style></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>GDP</th>
      <th>Exprop</th>
      <th>Mort</th>
      <th>Latitude</th>
      <th>Neo</th>
      <th>Africa</th>
      <th>Asia</th>
      <th>Namer</th>
      <th>Samer</th>
      <th>logMort</th>
      <th>Latitude2</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>8.39</td>
      <td>6.50</td>
      <td>78.20</td>
      <td>0.3111</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>4.359270</td>
      <td>0.096783</td>
    </tr>
    <tr>
      <th>2</th>
      <td>7.77</td>
      <td>5.36</td>
      <td>280.00</td>
      <td>0.1367</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>5.634790</td>
      <td>0.018687</td>
    </tr>
    <tr>
      <th>3</th>
      <td>9.13</td>
      <td>6.39</td>
      <td>68.90</td>
      <td>0.3778</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>4.232656</td>
      <td>0.142733</td>
    </tr>
    <tr>
      <th>4</th>
      <td>9.90</td>
      <td>9.32</td>
      <td>8.55</td>
      <td>0.3000</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>2.145931</td>
      <td>0.090000</td>
    </tr>
    <tr>
      <th>5</th>
      <td>9.29</td>
      <td>7.50</td>
      <td>85.00</td>
      <td>0.2683</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>4.442651</td>
      <td>0.071985</td>
    </tr>
  </tbody>
</table>
</div>
<pre><code class="language-python">df.info()
</code></pre>
<pre><code>&lt;class 'pandas.core.frame.DataFrame'&gt;
Int64Index: 64 entries, 1 to 64
Data columns (total 11 columns):
 #   Column     Non-Null Count  Dtype  
---  ------     --------------  -----  
 0   GDP        64 non-null     float64
 1   Exprop     64 non-null     float64
 2   Mort       64 non-null     float64
 3   Latitude   64 non-null     float64
 4   Neo        64 non-null     int64  
 5   Africa     64 non-null     int64  
 6   Asia       64 non-null     int64  
 7   Namer      64 non-null     int64  
 8   Samer      64 non-null     int64  
 9   logMort    64 non-null     float64
 10  Latitude2  64 non-null     float64
dtypes: float64(6), int64(5)
memory usage: 6.0 KB
</code></pre>
<p>In their paper, AJR note that their IV strategy will be invalidated if other factors are also highly persistent and related to the development of institutions within a country and to the country’s GDP. A leading candidate for such a factor, as they discuss, is geography. AJR address this by assuming that the confounding effect of geography is adequately captured by a linear term in distance from the equator and a set of continent dummy variables.</p>
<p>They inclue their results in table 2.</p>
<pre><code class="language-python"># Add constant term to dataset
df['const'] = 1

# Create lists of variables to be used in each regression
X1 = df[['const', 'Exprop']]
X2 = df[['const', 'Exprop', 'Latitude', 'Latitude2']]
X3 = df[['const', 'Exprop', 'Latitude', 'Latitude2', 'Asia', 'Africa', 'Namer', 'Samer']]
y = df['GDP']

# Estimate an OLS regression for each set of variables
reg1 = sm.OLS(y, X1, missing='drop').fit()
reg2 = sm.OLS(y, X2, missing='drop').fit()
reg3 = sm.OLS(y, X3, missing='drop').fit()
</code></pre>
<p>Let&rsquo;s replicate Table 2 in AJR.</p>
<pre><code class="language-python"># Make table 2
def make_table_2():

    info_dict={'No. observations' : lambda x: f&quot;{int(x.nobs):d}&quot;}

    results_table = summary_col(results=[reg1,reg2,reg3],
                                float_format='%0.2f',
                                stars = True,
                                model_names=['Model 1','Model 2','Model 3'],
                                info_dict=info_dict,
                                regressor_order=['const','Exprop','Latitude','Latitude2'])
    return results_table
</code></pre>
<pre><code class="language-python">table_2 = make_table_2()
table_2
</code></pre>
<table class="simpletable">
<tr>
          <td></td>         <th>Model 1</th> <th>Model 2</th> <th>Model 3</th>
</tr>
<tr>
  <th>const</th>            <td>4.66***</td> <td>4.55***</td> <td>5.95***</td>
</tr>
<tr>
  <th></th>                 <td>(0.41)</td>  <td>(0.45)</td>  <td>(0.68)</td> 
</tr>
<tr>
  <th>Exprop</th>           <td>0.52***</td> <td>0.49***</td> <td>0.40***</td>
</tr>
<tr>
  <th></th>                 <td>(0.06)</td>  <td>(0.07)</td>  <td>(0.06)</td> 
</tr>
<tr>
  <th>Latitude</th>            <td></td>      <td>2.16</td>    <td>0.42</td>  
</tr>
<tr>
  <th></th>                    <td></td>     <td>(1.68)</td>  <td>(1.47)</td> 
</tr>
<tr>
  <th>Latitude2</th>           <td></td>      <td>-2.12</td>   <td>0.44</td>  
</tr>
<tr>
  <th></th>                    <td></td>     <td>(2.86)</td>  <td>(2.48)</td> 
</tr>
<tr>
  <th>Africa</th>              <td></td>        <td></td>     <td>-1.06**</td>
</tr>
<tr>
  <th></th>                    <td></td>        <td></td>     <td>(0.41)</td> 
</tr>
<tr>
  <th>Asia</th>                <td></td>        <td></td>     <td>-0.74*</td> 
</tr>
<tr>
  <th></th>                    <td></td>        <td></td>     <td>(0.42)</td> 
</tr>
<tr>
  <th>Namer</th>               <td></td>        <td></td>      <td>-0.17</td> 
</tr>
<tr>
  <th></th>                    <td></td>        <td></td>     <td>(0.40)</td> 
</tr>
<tr>
  <th>Samer</th>               <td></td>        <td></td>      <td>-0.12</td> 
</tr>
<tr>
  <th></th>                    <td></td>        <td></td>     <td>(0.42)</td> 
</tr>
<tr>
  <th>No. observations</th>   <td>64</td>      <td>64</td>      <td>64</td>   
</tr>
</table>
<p>Using DML allows us to relax this assumption and to replace it by a weaker assumption that geography can be sufficiently controlled by an unknown function of distance from the equator and continent dummies, which can be learned by ML methods.</p>
<p>In particular, our framework is</p>
<p>$$
{GDP} = \beta_0 \times {Exprop} + g_0({geography}) + u \
{Exprop} = m_0({geography}) + u
$$</p>
<p>So that the double/debiased machine learning procedure is</p>
<ol>
<li>
<p>Split the sample in two: main sample and auxiliary sample</p>
</li>
<li>
<p>Use the auxiliary sample to estimate $\hat g_0({geography})$ from</p>
<p>$$
{GDP} = \beta_0 \times {Exprop} + g_0({geography}) + u
$$</p>
</li>
<li>
<p>Use the auxiliary sample to estimate $\hat m_0({geography})$ from</p>
<p>$$
{Exprop} = m_0({geography}) + v
$$</p>
</li>
<li>
<p>Use the main sample to compute the orthogonalized component of ${Exprop}$ on ${geography}$ as</p>
<p>$$
\hat v = {Exprop} - \hat m_0({geography})
$$</p>
</li>
<li>
<p>Use the main sample to estimate the double-residualized OLS estimator as</p>
<p>$$
\hat{\beta}<em>{0}=\left(\frac{1}{n} \sum</em>{i \in I} \hat v_i \times {Exprop}<em>{i} \right)^{-1} \frac{1}{n} \sum</em>{i \in I} \hat v_i \times \left( {GDP} - \hat g_0({geography}) \right)
$$</p>
</li>
</ol>
<p>Since we employ an <strong>intrumental variable</strong> strategy, we replace $m_0({geography})$ with $m_0({geography},{logMort})$ in the first stage.</p>
<pre><code class="language-python"># Generate variables
D = df['Exprop'].values.reshape(-1,1)
X = df[['const', 'Latitude', 'Latitude2', 'Asia', 'Africa', 'Namer', 'Samer']].values
y = df['GDP'].values.reshape(-1,1)
Z = df[['const', 'Latitude', 'Latitude2', 'Asia', 'Africa', 'Namer', 'Samer','logMort']].values
</code></pre>
<pre><code class="language-python">def estimate_beta(algorithm, alg_name, D, X, y, Z, sample):

    # Split sample
    D_main, D_aux = (D[sample==1], D[sample==0])
    X_main, X_aux = (X[sample==1], X[sample==0])
    y_main, y_aux = (y[sample==1], y[sample==0])
    Z_main, Z_aux = (Z[sample==1], Z[sample==0])

    # Residualize y on D
    b_hat = inv(D_aux.T @ D_aux) @ D_aux.T @ y_aux
    y_resid_aux = y_aux - D_aux @ b_hat
    
    # Estimate g0
    alg_fitted = algorithm.fit(X=X_aux, y=y_resid_aux.ravel())
    g0 = alg_fitted.predict(X_main).reshape(-1,1)

    # Compute v_hat
    u_hat = y_main - g0

    # Estimate m0
    alg_fitted = algorithm.fit(X=Z_aux, y=D_aux.ravel())
    m0 = algorithm.predict(Z_main).reshape(-1,1)
    
    # Compute u_hat
    v_hat = D_main - m0

    # Estimate beta
    beta = inv(v_hat.T @ D_main) @ v_hat.T @ u_hat
        
    return beta 
</code></pre>
<pre><code class="language-python">def ddml(algorithm, alg_name, D, X, y, Z, p=0.5, verbose=False):
    
    # Expand X if Lasso or Ridge
    if alg_name in ['Lasso   ','Ridge   ']:
        X = PolynomialFeatures(degree=2).fit_transform(X)

    # Generate split (fixed proportions)
    split = np.array([i in train_test_split(range(len(D)), test_size=p)[0] for i in range(len(D))])
    
    # Compute beta
    beta = [estimate_beta(algorithm, alg_name, D, X, y, Z, split==k) for k in range(2)]
    beta = np.mean(beta)
     
    # Print and return
    if verbose:
        print('%s : %.4f' % (alg_name, beta))
    return beta
</code></pre>
<pre><code class="language-python"># Generate sample split
p = 0.5
split = np.random.binomial(1, p, len(D))
</code></pre>
<p>We inspect different algorithms. In particular, we consider:</p>
<ol>
<li>Lasso Regression</li>
<li>Ridge Regression</li>
<li>Regression Trees</li>
<li>Random Forest</li>
<li>Boosted Forests</li>
</ol>
<pre><code class="language-python"># List all algorithms
algorithms = {'Ridge   ': Ridge(alpha=.1),
              'Lasso   ': Lasso(alpha=.01),
              'Tree    ': DecisionTreeRegressor(),
              'Forest  ': RandomForestRegressor(n_estimators=30),
              'Boosting': GradientBoostingRegressor(n_estimators=30)}
</code></pre>
<p>Let&rsquo;s compare the results.</p>
<pre><code class="language-python"># Loop over algorithms
for alg_name, algorithm in algorithms.items():
    ddml(algorithm, alg_name, D, X, y, Z, verbose=True)
</code></pre>
<pre><code>Ridge    : 0.1289
Lasso    : -8.7963
Tree     : 1.2879
Forest   : 2.4938
Boosting : 0.5977
</code></pre>
<p>The results are extremely volatile.</p>
<pre><code class="language-python"># Repeat K times
def estimate_beta_median(algorithms, D, X, y, Z, K):
    
    # Loop over algorithms
    for alg_name, algorithm in algorithms.items():
        betas = []
            
        # Iterate n times
        for k in range(K):
            beta = ddml(algorithm, alg_name, D, X, y, Z)
            betas = np.append(betas, beta)
    
        print('%s : %.4f' % (alg_name, np.median(betas)))
</code></pre>
<p>Let&rsquo;s try using the median to have a more stable estimator.</p>
<pre><code class="language-python">np.random.seed(123)

# Repeat 100 times and take median
estimate_beta_median(algorithms, D, X, y, Z, 100)
</code></pre>
<pre><code>Ridge    : 0.6670
Lasso    : 1.2511
Tree     : 0.9605
Forest   : 0.5327
Boosting : 1.0327
</code></pre>
<p>The results differ slightly from the ones in the paper, but they are at least closer.</p>

          </div>

          



          
          
          <div class="article-widget">
            
<div class="post-nav">
  
  
  
  <div class="post-nav-item">
    <div class="meta-nav">Previous</div>
    <a href="/course/ml-econ/08_neuralnets/" rel="next">Neural Networks</a>
  </div>
  
  
  
  <div class="post-nav-item">
    <div class="meta-nav">Next</div>
    <a href="/course/ml-econ/10_unsupervised/" rel="prev">Unsupervised Learning</a>
  </div>
  
</div>

          </div>
          
        </div>

        <div class="body-footer">

          





          




          


  
  



        </div>

      </article>

      <footer class="site-footer">

  

  
  <p class="powered-by">
    
      <a href="/privacy/">Privacy Policy</a>
    
    
       &middot; 
      <a href="/terms/">Terms</a>
    
  </p>
  

  

  
  







</footer>


    </main>
  </div>
</div>

  </div>

  <div class="page-footer">
    
    
  </div>

  
<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        <pre><code class="tex hljs"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

      

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/instant.page/5.1.0/instantpage.min.js" integrity="sha512-1+qUtKoh9XZW7j+6LhRMAyOrgSQKenQ4mluTR+cvxXjP1Z54RxZuzstR/H9kgPXQsVB8IW7DMDFUJpzLjvhGSQ==" crossorigin="anonymous"></script>

      
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js" integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin="anonymous"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js" integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin="anonymous"></script>
      

      
      

      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mermaid/8.8.4/mermaid.min.js" integrity="sha512-as1BF4+iHZ3BVO6LLDQ7zrbvTXM+c/1iZ1qII/c3c4L8Rn5tHLpFUtpaEtBNS92f+xGsCzsD7b62XP3XYap6oA==" crossorigin="anonymous" title="mermaid"></script>
      

      
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/highlight.min.js" integrity="sha512-TDKKr+IvoqZnPzc3l35hdjpHD0m+b2EC2SrLEgKDRWpxf2rFCxemkgvJ5kfU48ip+Y+m2XVKyOCD85ybtlZDmw==" crossorigin="anonymous"></script>
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/languages/r.min.js"></script>
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/languages/python.min.js"></script>
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/languages/julia.min.js"></script>
        
      

    

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.7.1/leaflet.min.js" integrity="sha512-SeiQaaDh73yrb56sTW/RgVdi/mMqNeM2oBwubFHagc5BkixSpP1fvqF47mKzPGWYSSy4RwbBunrJBQ4Co8fRWA==" crossorigin="anonymous"></script>
    

    
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/anchor-js/4.2.2/anchor.min.js" integrity="sha512-I7w3ZdSFzw5j3jU3ZkNikBNeIrl3i+hEuEdwNmqUJvwNcaBUNcijnP2gd9DtGlgVYDplfjGoD8vTNsID+lCjqg==" crossorigin="anonymous"></script>
    <script>
      anchors.add();
    </script>
    

    
    
    
      
      <script id="search-hit-fuse-template" type="text/x-template">
        <div class="search-hit" id="summary-{{key}}">
          <div class="search-hit-content">
            <div class="search-hit-name">
              <a href="{{relpermalink}}">{{title}}</a>
              <div class="article-metadata search-hit-type">{{type}}</div>
              <p class="search-hit-description">{{snippet}}</p>
            </div>
          </div>
        </div>
      </script>
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js" integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin="anonymous"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js" integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin="anonymous"></script>
      
    

    
    

    
    
    
    

    
    <script src="/js/bootstrap.bundle.min.6aed84840afc03ab4d5750157f69c120.js"></script>

    
    
      
      
      
      
      
      
      
    

    
    
    
    
    
    
    
    
      
      
    
    
    <script src="/en/js/wowchemy.min.cf8ca859a9b74f8b1cd804621b13e5f1.js"></script>

    
  <script async defer src="https://buttons.github.io/buttons.js"></script>




</body>
</html>
