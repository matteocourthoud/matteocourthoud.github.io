<!DOCTYPE html><html lang="en-us" >

<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  
  
  
    <meta name="generator" content="Wowchemy 5.1.0 for Hugo" />
  

  

  
  
  
  
  
    
    
    
  
  

  

  
  
  
    
  
  <meta name="description" content="A complete guide to comparing distributions, from visualization to statistical tests
Comparing the empirical distribution of a variable across different groups is a common problem in data science. In particular, in causal inference the problem often arises when we have to assess the quality of randomization." />

  
  <link rel="alternate" hreflang="en-us" href="https://matteocourthoud.github.io/post/distr/" />

  







  




  
  
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
  

  
  
  
    <meta name="theme-color" content="#003f5c" />
  

  
  
    
    <script src="/js/mathjax-config.js"></script>
  

  
  
  
  
    
    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha256-FMvZuGapsJLjouA6k7Eo2lusoAX9i0ShlWFG6qt7SLc=" crossorigin="anonymous">

    
    

    
    
    
      
    
    
      
      
        
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/styles/github.min.css" crossorigin="anonymous" title="hl-light" media="print" onload="this.media='all'">
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/styles/dracula.min.css" crossorigin="anonymous" title="hl-dark" media="print" onload="this.media='all'" disabled>
        
      
    

    
    
    
      <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.7.1/leaflet.min.css" integrity="sha512-1xoFisiGdy9nvho8EgXuXvnpR5GAMSjFwp40gSRE3NwdUdIMIKuPa7bqoUhLD0O/5tPNhteAsE5XyyMi5reQVA==" crossorigin="anonymous" media="print" onload="this.media='all'">
    

    

    
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js" integrity="" crossorigin="anonymous" async></script>
      
    
      
      

      
      

      
    
      
      

      
      

      
    
  

  
  
  
    
      
      <link rel="preload" as="style" href="https://fonts.googleapis.com/css2?family=Roboto+Mono&family=Source+Sans+Pro:wght@200;300;400;700&display=swap">
      <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Roboto+Mono&family=Source+Sans+Pro:wght@200;300;400;700&display=swap" media="print" onload="this.media='all'">
    
  

  
  
  
  
  
  <link rel="stylesheet" href="/css/wowchemy.4f7182ca394d705ee32d9d7750e9aa1d.css" />

  




<script async src="https://www.googletagmanager.com/gtag/js?id=UA-144780600-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];

  function gtag() {
      dataLayer.push(arguments);
  }

  function trackOutboundLink(url, target) {
    gtag('event', 'click', {
         'event_category': 'outbound',
         'event_label': url,
         'transport_type': 'beacon',
         'event_callback': function () {
           if (target !== '_blank') {
             document.location = url;
           }
         }
    });
    console.debug("Outbound link clicked: " + url);
  }

  function onClickCallback(event) {
    if ((event.target.tagName !== 'A') || (event.target.host === window.location.host)) {
      return;
    }
    trackOutboundLink(event.target, event.target.getAttribute('target'));  
  }

  gtag('js', new Date());
  gtag('config', 'UA-144780600-1', { 'anonymize_ip': true });
  gtag('set', {'cookie_flags': 'SameSite=None;Secure'});

  
  document.addEventListener('click', onClickCallback, false);
</script>


  

  

  




  
  
  

  

  
    <link rel="manifest" href="/index.webmanifest" />
  

  <link rel="icon" type="image/png" href="/media/icon_hu03e9b3967b83fd39296ec9da5ff1ea05_201175_32x32_fill_lanczos_center_3.png" />
  <link rel="apple-touch-icon" type="image/png" href="/media/icon_hu03e9b3967b83fd39296ec9da5ff1ea05_201175_180x180_fill_lanczos_center_3.png" />

  <link rel="canonical" href="https://matteocourthoud.github.io/post/distr/" />

  
  
  
  
  
  
  
  
    
  
  
  <meta property="twitter:card" content="summary_large_image" />
  
  <meta property="og:site_name" content="Matteo Courthoud" />
  <meta property="og:url" content="https://matteocourthoud.github.io/post/distr/" />
  <meta property="og:title" content="How to Compare Two or More Distributions | Matteo Courthoud" />
  <meta property="og:description" content="A complete guide to comparing distributions, from visualization to statistical tests
Comparing the empirical distribution of a variable across different groups is a common problem in data science. In particular, in causal inference the problem often arises when we have to assess the quality of randomization." /><meta property="og:image" content="https://matteocourthoud.github.io/post/distr/featured.png" />
    <meta property="twitter:image" content="https://matteocourthoud.github.io/post/distr/featured.png" /><meta property="og:locale" content="en-us" />
  
    
      <meta
        property="article:published_time"
        content="2022-06-22T00:00:00&#43;00:00"
      />
    
    <meta property="article:modified_time" content="2022-06-22T00:00:00&#43;00:00">
  

  


    






  




<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://matteocourthoud.github.io/post/distr/"
  },
  "headline": "How to Compare Two or More Distributions",
  
  "image": [
    "https://matteocourthoud.github.io/post/distr/featured.png"
  ],
  
  "datePublished": "2022-06-22T00:00:00Z",
  "dateModified": "2022-06-22T00:00:00Z",
  
  "publisher": {
    "@type": "Organization",
    "name": "Matteo Courthoud",
    "logo": {
      "@type": "ImageObject",
      "url": "https://matteocourthoud.github.io/media/icon_hu03e9b3967b83fd39296ec9da5ff1ea05_201175_192x192_fill_lanczos_center_3.png"
    }
  },
  "description": "A complete guide to comparing distributions, from visualization to statistical tests\nComparing the empirical distribution of a variable across different groups is a common problem in data science. In particular, in causal inference the problem often arises when we have to assess the quality of randomization."
}
</script>

  

  

  
  
  
  
  
    <script src="https://cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.1.1/cookieconsent.min.js" integrity="sha256-5VhCqFam2Cn+yjw61zbBNrbHVJ6SRydPeKopYlngbiQ=" crossorigin="anonymous"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.1.1/cookieconsent.min.css" integrity="sha256-zQ0LblD/Af8vOppw18+2anxsuaz3pWYyVWi+bTvTH8Q=" crossorigin="anonymous">
  
  <script>
  window.addEventListener("load", function(){
    window.cookieconsent.initialise({
      "palette": {
        "popup": {
          "background": "#003f5c",
          "text": "rgb(255, 255, 255)"
        },
        "button": {
          "background": "rgb(255, 255, 255)",
          "text": "#003f5c"
        }
      },
      "theme": "classic",
      "content": {
        "message": "This website uses cookies to ensure you get the best experience on our website.",
        "dismiss": "Got it!",
        "link": "Learn more",
        "href": "/privacy/"
      }
    })});
  </script>


  





  <title>How to Compare Two or More Distributions | Matteo Courthoud</title>
</head>


<body id="top" data-spy="scroll" data-offset="70" data-target="#TableOfContents" class="page-wrapper   " data-wc-page-id="e99f68079316990ae82671527ce10872" >

  
  
  
  
  
  
  
  
  <script src="/js/wowchemy-init.min.66d3e0fff6d32c4ece05adee927fbd96.js"></script>

  


<aside class="search-modal" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Search</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#" aria-label="Close"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        
        <input name="q" id="search-query" placeholder="Search..." autocapitalize="off"
        autocomplete="off" autocorrect="off" spellcheck="false" type="search" class="form-control"
        aria-label="Search...">
        
      </div>

      
      

      

    </section>
    <section class="section-search-results">

      <div id="search-hits">
        
      </div>

    </section>
  </div>
</aside>



  <div class="page-header">
    












<nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id="navbar-main">
  <div class="container-xl">

    
    <div class="d-none d-lg-inline-flex">
      <a class="navbar-brand" href="/">Matteo Courthoud</a>
    </div>
    

    
    <button type="button" class="navbar-toggler" data-toggle="collapse"
            data-target="#navbar-content" aria-controls="navbar-content" aria-expanded="false" aria-label="Toggle navigation">
    <span><i class="fas fa-bars"></i></span>
    </button>
    

    
    <div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none">
      <a class="navbar-brand" href="/">Matteo Courthoud</a>
    </div>
    

    
    
    <div class="navbar-collapse main-menu-item collapse justify-content-end" id="navbar-content">

      
      <ul class="navbar-nav d-md-inline-flex">
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
            
            
            
              
            
            
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#about"><span>Home</span></a>
        </li>

        
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
            
            
            
              
            
            
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#posts"><span>Posts</span></a>
        </li>

        
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
            
            
            
              
            
            
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#projects"><span>Research</span></a>
        </li>

        
        

        
        <li class="nav-item dropdown">
          <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown" aria-haspopup="true"><span>Courses</span><span class="caret"></span>
          </a>
          <div class="dropdown-menu">
            
              <a class="dropdown-item" href="/course/ml-econ/"><span>Machine Learning for Economics</span></a>
            
              <a class="dropdown-item" href="/course/data-science/"><span>Data Science with Python</span></a>
            
              <a class="dropdown-item" href="/course/empirical-io/"><span>PhD Industrial Organization</span></a>
            
              <a class="dropdown-item" href="/course/metrics/"><span>PhD Econometrics</span></a>
            
              <a class="dropdown-item" href="https://pp4rs.github.io/"><span>Programming Practices for Research</span></a>
            
          </div>
        </li>

        
        

        

        
        
        

        

        
        
        
        

        
          
            
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/cv"><span>CV</span></a>
        </li>

        
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
            
            
            
              
            
            
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#contact"><span>Contact</span></a>
        </li>

        
        

      

        
      </ul>
    </div>

    <ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2">

      
      
        
      

      
      

      
      

      
      

    </ul>

  </div>
</nav>


  </div>

  <div class="page-body">
    <div class="container-fluid docs">
  <div class="row">

    <div class="col-xl-2 col-lg-2 d-none d-xl-block d-lg-block empty">
    </div>

    <div class="col-2 col-xl-2 col-lg-2 d-none d-lg-block docs-toc">
      <ul class="nav toc-top">
        <li><a href="#" id="back_to_top" class="docs-toc-title">Contents</a></li>
      </ul>
      <nav id="TableOfContents">
  <ul>
    <li><a href="#example">Example</a></li>
    <li><a href="#two-groups---plots">Two Groups - Plots</a>
      <ul>
        <li><a href="#boxplot">Boxplot</a></li>
        <li><a href="#histogram">Histogram</a></li>
        <li><a href="#kernel-density">Kernel Density</a></li>
        <li><a href="#cumulative-distribution">Cumulative Distribution</a></li>
        <li><a href="#qq-plot">QQ Plot</a></li>
      </ul>
    </li>
    <li><a href="#two-groups---tests">Two Groups - Tests</a>
      <ul>
        <li><a href="#t-test">T-test</a></li>
        <li><a href="#standardized-mean-difference-smd">Standardized Mean Difference (SMD)</a></li>
        <li><a href="#mannwhitney-u-test">Mann–Whitney U Test</a></li>
        <li><a href="#permutation-tests">Permutation Tests</a></li>
        <li><a href="#chi-squared-test">Chi-Squared Test</a></li>
        <li><a href="#kolmogorov-smirnov-test">Kolmogorov-Smirnov Test</a></li>
      </ul>
    </li>
    <li><a href="#multiple-groups---plots">Multiple Groups - Plots</a>
      <ul>
        <li><a href="#boxplot-1">Boxplot</a></li>
        <li><a href="#violin-plot">Violin Plot</a></li>
        <li><a href="#ridgeline-plot">Ridgeline Plot</a></li>
      </ul>
    </li>
    <li><a href="#multiple-groups---tests">Multiple Groups - Tests</a>
      <ul>
        <li><a href="#f-test">F-test</a></li>
      </ul>
    </li>
    <li><a href="#conclusion">Conclusion</a>
      <ul>
        <li><a href="#references">References</a></li>
        <li><a href="#related-articles">Related Articles</a></li>
        <li><a href="#code">Code</a></li>
      </ul>
    </li>
  </ul>
</nav>
    </div>

    <main class="col-xl-8 col-lg-8 docs-content" role="main">
        <article class="article">
        




















  


<div class="article-container pt-3">
  <h1>How to Compare Two or More Distributions</h1>

  

  


<div class="article-metadata">

  
  

  
  <span class="article-date">
    
    
      
    
    Jun 22, 2022
  </span>
  

  

  
  <span class="middot-divider"></span>
  <span class="article-reading-time">
    19 min read
  </span>
  

  
  
  
  
  
  

  
  

</div>

  





</div>


<div class="article-header article-container featured-image-wrapper mt-4 mb-4" style="max-width: 1334px; max-height: 739px;">
  <div style="position: relative">
    <img src="/post/distr/featured.png" alt="" class="featured-image">
    
  </div>
</div>


        <div class="article-container">
          <div class="article-style" align="justify">
            <p><em>A complete guide to comparing distributions, from visualization to statistical tests</em></p>
<p>Comparing the empirical distribution of a variable across different groups is a common problem in data science. In particular, in causal inference the problem often arises when we have to <strong>assess the quality of randomization</strong>.</p>
<p>When we want to assess the causal effect of a policy (or UX feature, ad campaign, drug, &hellip;), the golden standard in causal inference are <a href="https://en.wikipedia.org/wiki/Randomized_controlled_trial" target="_blank" rel="noopener"><strong>randomized control trials</strong></a>, also known as <a href="https://de.wikipedia.org/wiki/A/B-Test" target="_blank" rel="noopener"><strong>A/B tests</strong></a>. In practice, we select a sample for the study and we randomly split it into a <strong>control</strong> and a <strong>treatment</strong> group, and we compare the outcomes between the two groups. Randomization ensures that only difference between the two groups is the treatment, on average, so that we can attribute outcome differences to the treatment effect.</p>
<p>The <strong>problem</strong> is that, despite randomization, the two groups are never identical. However, sometimes, they are not even &ldquo;similar&rdquo;. For example, we might have more males in one group, or older people, etc.. (we usually call these characteristics, <em>covariates</em> or <em>control variables</em>). When it happens, we cannot be certain anymore that the difference in the outcome is only due to the treatment and cannot be attributed to the <strong>inbalanced covariates</strong> instead. Therefore, it is always important, after randomization, to check whether all observed variables are balanced across groups and whether there are no systematic differences. Another option, to be certain ex-ante that certain covariates are balanced, is <a href="https://en.wikipedia.org/wiki/Stratified_sampling" target="_blank" rel="noopener">stratified sampling</a>.</p>
<p>In this blog post, we are going to see different ways to compare two (or more) distributions and assess the magnitude and significance of their difference. We are going to consider two different approaches, <strong>visual</strong> and <strong>statistical</strong>. The two approaches generally trade-off <strong>intuition</strong> with <strong>rigor</strong>: from plots we can quickly assess and explore differences, but it&rsquo;s hard to tell whether these differences are systematic or due to noise.</p>
<h2 id="example">Example</h2>
<p>Let&rsquo;s assume we need to perform an <strong>experiment</strong> on a group of individuals and we have randomized them into a <strong>treatment and control</strong> group. We would like them to be <strong>as comparable as possible</strong>, in order to attribute any difference between the two groups to the treatment effect alone. We also have divided the treatment group in different <em>arms</em> for testing different treatments (e.g. slight variations of the same drug).</p>
<p>For this example, I have simulated a dataset of 1000 individuals, for whom we observe a set of characteristics. I import the data generating process <code>dgp_rnd_assignment()</code> from <a href="https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/src/dgp.py" target="_blank" rel="noopener"><code>src.dgp</code></a> and some plotting functions and libraries from <a href="https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/src/utils.py" target="_blank" rel="noopener"><code>src.utils</code></a>.</p>
<pre><code class="language-python">%matplotlib inline
%config InlineBackend.figure_format = 'retina'
</code></pre>
<pre><code class="language-python">from src.utils import *
from src.dgp import dgp_rnd_assignment

df = dgp_rnd_assignment().generate_data()
df.head()
</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
</code></pre>
<p></style></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Group</th>
      <th>Arm</th>
      <th>Gender</th>
      <th>Age</th>
      <th>Income</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>control</td>
      <td>NaN</td>
      <td>0</td>
      <td>29.0</td>
      <td>568.44</td>
    </tr>
    <tr>
      <th>1</th>
      <td>control</td>
      <td>NaN</td>
      <td>1</td>
      <td>32.0</td>
      <td>596.45</td>
    </tr>
    <tr>
      <th>2</th>
      <td>treatment</td>
      <td>arm 3</td>
      <td>0</td>
      <td>29.0</td>
      <td>380.86</td>
    </tr>
    <tr>
      <th>3</th>
      <td>control</td>
      <td>NaN</td>
      <td>0</td>
      <td>25.0</td>
      <td>476.38</td>
    </tr>
    <tr>
      <th>4</th>
      <td>treatment</td>
      <td>arm 4</td>
      <td>1</td>
      <td>32.0</td>
      <td>628.28</td>
    </tr>
  </tbody>
</table>
</div>
<p>We have information on $1000$ individuals, for which we observe <code>gender</code>, <code>age</code> and weekly <code>income</code>. Each individual is assigned either to the treatment or control <code>group</code> and treated individuals are distributed across four treatment <code>arms</code>.</p>
<h2 id="two-groups---plots">Two Groups - Plots</h2>
<p>Let&rsquo;s start with the simplest setting: we want to compare the distribution of income across the <code>treatment</code> and <code>control</code> group. We first explore <strong>visual</strong> approaches and the <strong>statistical</strong> approaches. The advantage of the first is <strong>intuition</strong> while the advantage of the second is <strong>rigor</strong>.</p>
<p>For most visualizations I am going to use Python&rsquo;s <a href="https://seaborn.pydata.org/" target="_blank" rel="noopener"><code>seaborn</code></a> library.</p>
<h3 id="boxplot">Boxplot</h3>
<p>A first visual approach is the <a href="https://en.wikipedia.org/wiki/Box_plot" target="_blank" rel="noopener"><strong>boxplot</strong></a>. The boxplot is a good trade-off between summary statistics and data visualization. The center of the <strong>box</strong> represents the <em>median</em> while the borders represent the first (Q1) and third <a href="https://en.wikipedia.org/wiki/Quartile" target="_blank" rel="noopener">quartile</a> (Q3), respectively. The <strong>whiskers</strong> instead, extend to the first data points that are more than 1.5 times the <em>interquartile range</em> (Q3 - Q1) outside the box. The points that fall outside of the whiskers are plotted individually and are usually considered <a href="https://en.wikipedia.org/wiki/Outlier" target="_blank" rel="noopener"><strong>outliers</strong></a>.</p>
<p>Therefore, the boxplot provides both summary statistics (the box and the whiskers) and direct data visualization (the outliers).</p>
<pre><code class="language-python">sns.boxplot(data=df, x='Group', y='Income');
plt.title(&quot;Boxplot&quot;);
</code></pre>
<p><img src="img/distr_12_0.png" alt="png"></p>
<p>It seems that the <code>income</code> distribution in the <code>treatment</code> group is slightly more dispersed: the orange box is larger and its whiskers cover a wider range. However, the <strong>issue</strong> with the boxplot is that it hides the shape of the data, telling us some summary statistics but not showing us the actual data distribution.</p>
<h3 id="histogram">Histogram</h3>
<p>The most intuitive way to plot a distribution is the <strong>histogram</strong>. The histogram groups the data into equally wide <strong>bins</strong> and plots the number of observations within each bin.</p>
<pre><code class="language-python">sns.histplot(data=df, x='Income', hue='Group', bins=50);
plt.title(&quot;Histogram&quot;);
</code></pre>
<p><img src="img/distr_16_0.png" alt="png"></p>
<p>There are multiple <strong>issues</strong> with this plot:</p>
<ul>
<li>Since the two groups have a different number of observations, the two histograms are not comparable</li>
<li>The number of bins is arbitrary</li>
</ul>
<p>We can solve the first issue using the <code>stat</code> option to plot the <code>density</code> instead of the count and setting the <code>common_norm</code> option to <code>False</code> to use the same normalization.</p>
<pre><code class="language-python">sns.histplot(data=df, x='Income', hue='Group', bins=50, stat='density', common_norm=False);
plt.title(&quot;Density Histogram&quot;);
</code></pre>
<p><img src="img/distr_18_0.png" alt="png"></p>
<p>Now the two histograms are comparable!</p>
<p>However, an important <strong>issue</strong> remains: the size of the bins is arbitrary. In the extreme, if we bunch the data less, we end up with bins with at most one observation, if we bunch the data more, we end up with a single bin. In both cases, if we exaggerate, the plot loses informativeness. This is a classical <a href="https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff" target="_blank" rel="noopener">bias-variance trade-off</a>.</p>
<h3 id="kernel-density">Kernel Density</h3>
<p>One possible solution is to use a <a href="https://en.wikipedia.org/wiki/Kernel_density_estimation" target="_blank" rel="noopener"><strong>kernel density function</strong></a> that tries to approximate the histogram with a continuous function, using <a href="https://en.wikipedia.org/wiki/Kernel_density_estimation" target="_blank" rel="noopener">kernel density estimation (KDE)</a>.</p>
<pre><code class="language-python">sns.kdeplot(x='Income', data=df, hue='Group', common_norm=False);
plt.title(&quot;Kernel Density Function&quot;);
</code></pre>
<p><img src="img/distr_22_0.png" alt="png"></p>
<p>From the plot, it seems that the estimated kernel density of <code>income</code> has &ldquo;fatter tails&rdquo; (i.e. higher variance) in the <code>treatment</code> group, while the average seems similar across groups.</p>
<p>The <strong>issue</strong> with kernel density estimation is that it is a bit of a  black-box and might mask relevant features of the data.</p>
<h3 id="cumulative-distribution">Cumulative Distribution</h3>
<p>A more transparent representation of the two distribution is their <a href="https://en.wikipedia.org/wiki/Cumulative_distribution_function" target="_blank" rel="noopener"><strong>cumulative distribution function</strong></a>. At each point of the x axis (<code>income</code>) we plot the percentage of data points that have an equal or lower value. The main <strong>advantages</strong> of the cumulative distribution function are that</p>
<ul>
<li>we do not need to make any arbitrary choice (e.g. number of bins)</li>
<li>we do not need to perform any approximation (e.g. with KDE), but we represent all data points</li>
</ul>
<pre><code class="language-python">sns.histplot(x='Income', data=df, hue='Group', bins=len(df), stat=&quot;density&quot;,
             element=&quot;step&quot;, fill=False, cumulative=True, common_norm=False);
plt.title(&quot;Cumulative distribution function&quot;);
</code></pre>
<p><img src="img/distr_26_0.png" alt="png"></p>
<p>How should we interpret the graph?</p>
<ul>
<li>
<p>Since the two lines cross more or less at 0.5 (y axis), it means that their median is similar</p>
</li>
<li>
<p>Since the orange line is above the blue line on the left and below the blue line on the left, it means that the distribution of the <code>treatment</code> group as fatter tails</p>
</li>
</ul>
<h3 id="qq-plot">QQ Plot</h3>
<p>A related method is the <strong>qq-plot</strong>, where <em>q</em> stands for quantile. The qq-plot plots the quantiles of the two distributions against each other. If the distributions are the same, we should get a 45 degree line.</p>
<p>There is no native qq-plot function in Python and, while the <code>statsmodels</code> package provides a <a href="https://www.statsmodels.org/dev/generated/statsmodels.graphics.gofplots.qqplot.html" target="_blank" rel="noopener"><code>qqplot</code> function</a>, it is quite cumbersome. Therefore, we will do it by hand.</p>
<p>First, we need to compute the quartiles of the two groups, using the <code>percentile</code> function.</p>
<pre><code class="language-python">income = df['Income'].values
income_t = df.loc[df.Group=='treatment', 'Income'].values
income_c = df.loc[df.Group=='control', 'Income'].values

df_pct = pd.DataFrame()
df_pct['q_treatment'] = np.percentile(income_t, range(100))
df_pct['q_control'] = np.percentile(income_c, range(100))
</code></pre>
<p>Now we can plot the two quantile distributions against each other, plus the 45-degree line, representing the benchmark perfect fit.</p>
<pre><code class="language-python">plt.figure(figsize=(8, 8))
plt.scatter(x='q_control', y='q_treatment', data=df_pct, label='Actual fit');
sns.lineplot(x='q_control', y='q_control', data=df_pct, color='r', label='Line of perfect fit');
plt.xlabel('Quantile of income, control group')
plt.ylabel('Quantile of income, treatment group')
plt.legend()
plt.title(&quot;QQ plot&quot;);
</code></pre>
<p><img src="img/distr_32_0.png" alt="png"></p>
<p>The qq-plot delivers a very <strong>similar insight</strong> with respect to the cumulative distribution plot: income in the treatment group has the same median (lines cross in the center) but wider tails (dots are below the line on the left end and above on the right end).</p>
<h2 id="two-groups---tests">Two Groups - Tests</h2>
<p>So far, we have seen different ways to <em>visualize</em> differences between distributions. The main advantage of visualization is <strong>intuition</strong>: we can eyeball the differences and intuitively assess them.</p>
<p>However, we might want to be more <strong>rigorous</strong> and try to assess the <strong>statistical significance</strong> of the difference between the distributions, i.e. answer the question &ldquo;<em>is the observed difference systematic or due to sampling noise?</em>&rdquo;.</p>
<p>We are now going to analyze different tests to discern two distributions from each other.</p>
<h3 id="t-test">T-test</h3>
<p>The first and most common test is the <a href="https://en.wikipedia.org/wiki/Student%27s_t-test" target="_blank" rel="noopener">student t-test</a>. T-tests are generally used to <strong>compare means</strong>. In this case, we want to test whether the means of the <code>income</code> distribution is the same across the two groups. The test statistic for the two-means comparison test is given by:</p>
<p>$$
stat = \frac{|\bar x_1 - \bar x_2|}{\sqrt{s^2 / n }}
$$</p>
<p>Where $\bar x$ is the sample mean and $s$ is the sample standard deviation. Under mild conditions, the test statistic is asymptotically distributed as a <a href="https://en.wikipedia.org/wiki/Student%27s_t-distribution" target="_blank" rel="noopener">student t</a> distribution.</p>
<p>We use the <code>ttest_ind</code> function from <code>scipy</code> to perform the t-test. The function returns both the test statistic and the implied <a href="https://en.wikipedia.org/wiki/P-value" target="_blank" rel="noopener">p-value</a>.</p>
<pre><code class="language-python">from scipy.stats import ttest_ind

stat, p_value = ttest_ind(income_c, income_t)
print(f&quot;t-test: statistic={stat:.4f}, p-value={p_value:.4f}&quot;)
</code></pre>
<pre><code>t-test: statistic=-1.5549, p-value=0.1203
</code></pre>
<p>The p-value of the test is $0.12$, therefore we do <strong>not reject</strong> the null hypothesis of no difference in <em>means</em> across treatment and control groups.</p>
<blockquote>
<p><strong>Note</strong>: the t-test assumes that the variance in the two samples is the same so that its estimate is computed on the joint sample. <a href="https://en.wikipedia.org/wiki/Welch%27s_t-test" target="_blank" rel="noopener">Welch’s t-test</a> allows for unequal variances in the two samples.</p>
</blockquote>
<h3 id="standardized-mean-difference-smd">Standardized Mean Difference (SMD)</h3>
<p>In general, it is good practice to always perform a test for difference in means on <strong>all variables</strong> across the treatment and control group, when we are running a randomized control trial or A/B test.</p>
<p>However, since the denominator of the t-test statistic depends on the sample size, the t-test has been <strong>criticized</strong> for making p-values hard to compare across studies. In fact, we may obtain a significant result in an experiment with very small magnitude of difference but large sample size while we may obtain a non-significant result in an experiment with large magnitude of difference but small sample size.</p>
<p>One solution that has been proposed is the <strong>standardized mean difference (SMD)</strong>. As the name suggests, this is not a proper test statistic, but just a standardized difference, which can be computed as:</p>
<p>$$
SMD = \frac{|\bar x_1 - \bar x_2|}{\sqrt{(s^2_1 + s^2_2) / 2}}
$$</p>
<p>Usually a value below $0.1$ is considered a &ldquo;small&rdquo; difference.</p>
<p>It is good practice to collect average values of all variables across treatment and control group and a measure of distance between the two — either the t-test or the SMD — into a table that is called <strong>balance table</strong>. We can use the <a href="https://causalml.readthedocs.io/en/latest/causalml.html#module-causalml.match" target="_blank" rel="noopener"><code>create_table_one</code></a> function from the <a href="https://causalml.readthedocs.io/en/latest/about.html" target="_blank" rel="noopener"><code>causalml</code></a> library to generate it. As the name of the function suggests, the balance table should always be the <strong>first table</strong> you present when performing an A/B test.</p>
<pre><code class="language-python">from causalml.match import create_table_one

df['treatment'] = df['Group']=='treatment'
create_table_one(df, 'treatment', ['Gender', 'Age', 'Income'])
</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
</code></pre>
<p></style></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Control</th>
      <th>Treatment</th>
      <th>SMD</th>
    </tr>
    <tr>
      <th>Variable</th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>n</th>
      <td>704</td>
      <td>296</td>
      <td></td>
    </tr>
    <tr>
      <th>Age</th>
      <td>32.40 (8.54)</td>
      <td>36.42 (7.76)</td>
      <td>0.4928</td>
    </tr>
    <tr>
      <th>Gender</th>
      <td>0.51 (0.50)</td>
      <td>0.58 (0.49)</td>
      <td>0.1419</td>
    </tr>
    <tr>
      <th>Income</th>
      <td>524.59 (117.35)</td>
      <td>538.75 (160.15)</td>
      <td>0.1009</td>
    </tr>
  </tbody>
</table>
</div>
<p>In the first two columns, we can see the average of the different variables across the treatment and control groups, with standard errors in parenthesis. In the <strong>last column</strong>, the values of the SMD indicate a standardized difference of more than $0.1$ for all variables, suggesting that the two groups are probably different.</p>
<h3 id="mannwhitney-u-test">Mann–Whitney U Test</h3>
<p>An alternative test is the <a href="https://en.wikipedia.org/wiki/Mann%E2%80%93Whitney_U_test" target="_blank" rel="noopener">Mann–Whitney U test</a>. The null hypothesis for this test is that the two groups have the same distribution, while the alternative hypothesis is that one group has larger (or smaller) values than the other.</p>
<p>Differently from the other tests we have seen so far, the Mann–Whitney U test is agnostic to outliers and concentrates on the center of the distribution.</p>
<p>The test <strong>procedure</strong> is the following.</p>
<ol>
<li>
<p>Combine all data points and rank them (in increasing or decreasing order)</p>
</li>
<li>
<p>Compute $U_1 = R_1 - n_1(n_1 + 1)/2$, where $R_1$ is the sum of the ranks for data points in the first group and $n_1$ is the number of points in the first group.</p>
</li>
<li>
<p>Compute $U_2$ similarly for the second group.</p>
</li>
<li>
<p>The test statistic is given by $stat = min(U_1, U_2)$.</p>
</li>
</ol>
<p>Under the null hypothesis of no systematic rank differences between the two distributions (i.e. same median), the test statistic is asymptotically normally distributed with known mean and variance.</p>
<p>The <strong>intuition</strong> behind the computation of $R$ and $U$ is the following: if the values in the first sample were all bigger than the values in the second sample, then $R_1 = n_1(n_1 + 1)/2$ and, as a consequence, $U_1$ would then be zero (minimum attainable value). Otherwise, if the two samples were similar, $U_1$ and $U_2$ would be very close to $n_1 n_2 / 2$ (maximum attainable value).</p>
<p>We perform the test using the <code>mannwhitneyu</code> function from <code>scipy</code>.</p>
<pre><code class="language-python">from scipy.stats import mannwhitneyu

stat, p_value = mannwhitneyu(income_t, income_c)
print(f&quot; Mann–Whitney U Test: statistic={stat:.4f}, p-value={p_value:.4f}&quot;)
</code></pre>
<pre><code> Mann–Whitney U Test: statistic=106371.5000, p-value=0.6012
</code></pre>
<p>We get a p-value of 0.6 which implies that we do <strong>not reject</strong> the null hypothesis of no difference between the two distributions.</p>
<blockquote>
<p><strong>Note</strong>: as for the t-test, there exists a version of the Mann–Whitney U test for unequal variances in the two samples, the <a href="https://www.statisticshowto.com/brunner-munzel-test-generalized-wilcoxon-test/" target="_blank" rel="noopener">Brunner-Munzel test</a>.</p>
</blockquote>
<h3 id="permutation-tests">Permutation Tests</h3>
<p>A non-parametric alternative is permutation testing. The idea is that, under the null hypothesis, the two distributions should be the same, therefore <strong>shuffling</strong> the <code>group</code> labels should not significantly alter any statistic.</p>
<p>We can chose any statistic and check how its value in the original sample compares with its distribution across <code>group</code> label permutations. For example, let&rsquo;s use as a test statistic the <strong>difference of sample means</strong> between the treatment and control group.</p>
<pre><code class="language-python">sample_stat = np.mean(income_t) - np.mean(income_c)
</code></pre>
<pre><code class="language-python">stats = np.zeros(1000)
for k in range(1000):
    labels = np.random.permutation((df['Group'] == 'treatment').values)
    stats[k] = np.mean(income[labels]) - np.mean(income[labels==False])
p_value = np.mean(stats &gt; sample_stat)

print(f&quot;Permutation test: p-value={p_value:.4f}&quot;)
</code></pre>
<pre><code>Permutation test: p-value=0.0530
</code></pre>
<p>The permutation test gives us a p-value of $0.056$, implying a weak <strong>non-rejection</strong> of the null hypothesis at the 5% level.</p>
<p>How do we <strong>interpret</strong> the p-value? It means that the difference in means in the data is larger than $1 - 0.0560 = 94.4%$ of the differences in means across the permuted samples.</p>
<p>We can <strong>visualize</strong> the test, by plotting the distribution of the test statistic across permutations against its sample value.</p>
<pre><code class="language-python">plt.hist(stats, label='Permutation Statistics', bins=30);
plt.axvline(x=sample_stat, c='r', ls='--', label='Sample Statistic');
plt.legend();
plt.xlabel('Income difference between treatment and control group')
plt.title('Permutation Test');
</code></pre>
<p><img src="img/distr_55_0.png" alt="png"></p>
<p>As we can see, the sample statistic is quite extreme with respect to the values in the permuted samples, but not excessively.</p>
<h3 id="chi-squared-test">Chi-Squared Test</h3>
<p>The <a href="https://matteocourthoud.github.io/post/chisquared/" target="_blank" rel="noopener">chi-squared test</a> is a very powerful test that is mostly used to test differences in frequencies.</p>
<p>One of the <strong>least known applications</strong> of the chi-squared test, is testing the similarity between two distributions. The <strong>idea</strong> is to bin the observations of the two groups. If the two distributions were the same, we would expect the same frequency of observations in each bin. Importantly, we need enough observations in each bin, in order for the test to be valid.</p>
<p>I generate bins corresponding to deciles of the distribution of <code>income</code> in the <em>control</em> group and then I compute the expected number of observations in each bin in the <em>treatment</em> group, if the two distributions were the same.</p>
<pre><code class="language-python"># Init dataframe
df_bins = pd.DataFrame()

# Generate bins from control group
_, bins = pd.qcut(income_c, q=10, retbins=True)
df_bins['bin'] = pd.cut(income_c, bins=bins).value_counts().index

# Apply bins to both groups
df_bins['income_c_observed'] = pd.cut(income_c, bins=bins).value_counts().values
df_bins['income_t_observed'] = pd.cut(income_t, bins=bins).value_counts().values

# Compute expected frequency in the treatment group
df_bins['income_t_expected'] = df_bins['income_c_observed'] / np.sum(df_bins['income_c_observed']) * np.sum(df_bins['income_t_observed'])

df_bins
</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
</code></pre>
<p></style></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>bin</th>
      <th>income_c_observed</th>
      <th>income_t_observed</th>
      <th>income_t_expected</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>(232.26, 380.496]</td>
      <td>70</td>
      <td>46</td>
      <td>29.075391</td>
    </tr>
    <tr>
      <th>1</th>
      <td>(380.496, 425.324]</td>
      <td>70</td>
      <td>30</td>
      <td>29.075391</td>
    </tr>
    <tr>
      <th>2</th>
      <td>(425.324, 456.795]</td>
      <td>70</td>
      <td>24</td>
      <td>29.075391</td>
    </tr>
    <tr>
      <th>3</th>
      <td>(456.795, 488.83]</td>
      <td>71</td>
      <td>26</td>
      <td>29.490754</td>
    </tr>
    <tr>
      <th>4</th>
      <td>(488.83, 513.66]</td>
      <td>70</td>
      <td>18</td>
      <td>29.075391</td>
    </tr>
    <tr>
      <th>5</th>
      <td>(513.66, 540.048]</td>
      <td>70</td>
      <td>19</td>
      <td>29.075391</td>
    </tr>
    <tr>
      <th>6</th>
      <td>(540.048, 576.664]</td>
      <td>71</td>
      <td>21</td>
      <td>29.490754</td>
    </tr>
    <tr>
      <th>7</th>
      <td>(576.664, 621.022]</td>
      <td>70</td>
      <td>25</td>
      <td>29.075391</td>
    </tr>
    <tr>
      <th>8</th>
      <td>(621.022, 682.003]</td>
      <td>70</td>
      <td>42</td>
      <td>29.075391</td>
    </tr>
    <tr>
      <th>9</th>
      <td>(682.003, 973.46]</td>
      <td>71</td>
      <td>41</td>
      <td>29.490754</td>
    </tr>
  </tbody>
</table>
</div>
<p>We can now perform the test by comparing the expected (E) and observed (O) number of observations in the treatment group, across bins. The test statistic is given by</p>
<p>$$
stat = \sum _{i=1}^{n} \frac{(O_i - E_i)^{2}}{E_i}
$$</p>
<p>where the bins are indexed by $i$ and $O$ is the observed number of data points in bin $i$ and $E$ is the expected number of data points in bin $i$. Since we generated the bins using deciles of the distribution of <code>income</code> in the control group, we expect the number of observations per bin in the treatment group to be the same across bins. The test statistic is asymptocally distributed as a <a href="https://en.wikipedia.org/wiki/Chi-squared_distribution" target="_blank" rel="noopener">chi-squared</a> distribution.</p>
<p>To compute the test statistic and the p-value of the test, we use the <code>chisquare</code> function from <code>scipy</code>.</p>
<pre><code class="language-python">from scipy.stats import chisquare

stat, p_value = chisquare(df_bins['income_t_observed'], df_bins['income_t_expected'])
print(f&quot;Chi-squared Test: statistic={stat:.4f}, p-value={p_value:.4f}&quot;)
</code></pre>
<pre><code>Chi-squared Test: statistic=32.1432, p-value=0.0002
</code></pre>
<p>Differently from all other tests so far, the chi-squared test <strong>strongly rejects</strong> the null hypothesis that the two distributions are the same. Why?</p>
<p>The reason lies in the fact that the two distributions have a similar center but different tails and the chi-squared test tests the similarity along the <strong>whole distribution</strong> and not only in the center, as we were doing with the previous tests.</p>
<p>This result tells a <strong>cautionary tale</strong>: it is very important to understand <em>what</em> you are actually testing before drawing blind conclusions from a p-value!</p>
<h3 id="kolmogorov-smirnov-test">Kolmogorov-Smirnov Test</h3>
<p>The idea of the <a href="https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test" target="_blank" rel="noopener">Kolmogorov-Smirnov test</a>, is to <strong>compare the cumulative distributions</strong> of the two groups. In particular, the Kolmogorov-Smirnov test statistic is the maximum absolute difference between the two cumulative distributions.</p>
<p>$$
stat = \sup _{x} \ \Big| \ F_1(x) - F_2(x) \ \Big|
$$</p>
<p>Where $F_1$ and $F_2$ are the two cumulative distribution functions and $x$ are the values of the underlying variable. The asymptotic distribution of the Kolmogorov-Smirnov test statistic is <a href="https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test#Kolmogorov_distribution" target="_blank" rel="noopener">Kolmogorov distributed</a>.</p>
<p>To better understand the test, let&rsquo;s plot the cumulative distribution functions and the test statistic. First, we compute the cumulative distribution functions.</p>
<pre><code class="language-python">df_ks = pd.DataFrame()
df_ks['Income'] = np.sort(df['Income'].unique())
df_ks['F_control'] = df_ks['Income'].apply(lambda x: np.mean(income_c&lt;=x))
df_ks['F_treatment'] = df_ks['Income'].apply(lambda x: np.mean(income_t&lt;=x))
df_ks.head()
</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
</code></pre>
<p></style></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Income</th>
      <th>F_control</th>
      <th>F_treatment</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>216.36</td>
      <td>0.000000</td>
      <td>0.003378</td>
    </tr>
    <tr>
      <th>1</th>
      <td>232.26</td>
      <td>0.001420</td>
      <td>0.003378</td>
    </tr>
    <tr>
      <th>2</th>
      <td>243.15</td>
      <td>0.001420</td>
      <td>0.006757</td>
    </tr>
    <tr>
      <th>3</th>
      <td>259.88</td>
      <td>0.002841</td>
      <td>0.006757</td>
    </tr>
    <tr>
      <th>4</th>
      <td>262.82</td>
      <td>0.002841</td>
      <td>0.010135</td>
    </tr>
  </tbody>
</table>
</div>
<p>We now need to find the point where the absolute distance between the cumulative distribution functions is largest.</p>
<pre><code class="language-python">k = np.argmax( np.abs(df_ks['F_control'] - df_ks['F_treatment']))
ks_stat = np.abs(df_ks['F_treatment'][k] - df_ks['F_control'][k])
</code></pre>
<p>We can visualize the value of the test statistic, by plotting the two cumulative distribution functions and the value of the test statistic.</p>
<pre><code class="language-python">y = (df_ks['F_treatment'][k] + df_ks['F_control'][k])/2
plt.plot('Income', 'F_control', data=df_ks, label='Control')
plt.plot('Income', 'F_treatment', data=df_ks, label='Treatment')
plt.errorbar(x=df_ks['Income'][k], y=y, yerr=ks_stat/2, color='k',
             capsize=5, mew=3, label=f&quot;Test statistic: {ks_stat:.4f}&quot;)
plt.legend(loc='center right');
plt.title(&quot;Kolmogorov-Smirnov Test&quot;);
</code></pre>
<p><img src="img/distr_69_0.png" alt="png"></p>
<p>From the plot, we can see that the value of the test statistic corresponds to the distance between the two cumulative distributions at <code>income</code>~650. For that value of <code>income</code>, we have the largest imbalance between the two groups.</p>
<p>We can now perform the actual test using the <code>kstest</code> function from <code>scipy</code>.</p>
<pre><code class="language-python">from scipy.stats import kstest

stat, p_value = kstest(income_t, income_c)
print(f&quot; Kolmogorov-Smirnov Test: statistic={stat:.4f}, p-value={p_value:.4f}&quot;)
</code></pre>
<pre><code> Kolmogorov-Smirnov Test: statistic=0.0974, p-value=0.0355
</code></pre>
<p>The p-value is below 5%: we <strong>reject</strong> the null hypothesis that the two distributions are the same, with 95% confidence.</p>
<blockquote>
<p><strong>Note 1</strong>: The KS test is too conservative and rejects the null hypothesis too rarely. Lilliefors test corrects this bias using a different distribution for the test statistic, the Lilliefors distribution.</p>
</blockquote>
<blockquote>
<p><strong>Note 2</strong>: the KS test uses very little information since it only compares the two cumulative distributions at one point: the one of maximum distance. The <a href="https://en.wikipedia.org/wiki/Anderson%E2%80%93Darling_test" target="_blank" rel="noopener">Anderson-Darling test</a> and the <a href="https://en.wikipedia.org/wiki/Cram%C3%A9r%E2%80%93von_Mises_criterion" target="_blank" rel="noopener">Cramér-von Mises test</a> instead compare the two distributions along the whole domain, by integration (the difference between the two lies in the weighting of the squared distances).</p>
</blockquote>
<h2 id="multiple-groups---plots">Multiple Groups - Plots</h2>
<p>So far we have only considered the case of two groups: treatment and control. But that if we had <strong>multiple groups</strong>? Some of the methods we have seen above scale well, while others don&rsquo;t.</p>
<p>As a working example, we are now going to check whether the distribution of <code>income</code> is the same across treatment <code>arms</code>.</p>
<h3 id="boxplot-1">Boxplot</h3>
<p>The <strong>boxplot</strong> scales very well, when we have a number of groups in the single-digits, since we can put the different boxes side-by-side.</p>
<pre><code class="language-python">sns.boxplot(x='Arm', y='Income', data=df.sort_values('Arm'));
plt.title(&quot;Boxplot, multiple groups&quot;);
</code></pre>
<p><img src="img/distr_78_0.png" alt="png"></p>
<p>From the plot, it looks like the distribution of <code>income</code> is different across treatment arms, with higher numbered arms having a higher average income.</p>
<h3 id="violin-plot">Violin Plot</h3>
<p>A very nice extension of the boxplot that combines summary statistics and kernel density estimation is the  <strong>violinplot</strong>. The violinplot plots separate densities along the y axis so that they don&rsquo;t overlap. By default, it also adds a miniature boxplot inside.</p>
<pre><code class="language-python">sns.violinplot(x='Arm', y='Income', data=df.sort_values('Arm'));
plt.title(&quot;Violin Plot, multiple groups&quot;);
</code></pre>
<p><img src="img/distr_82_0.png" alt="png"></p>
<p>As for the boxplot, the violin plot suggests that income is different across treatment arms.</p>
<h3 id="ridgeline-plot">Ridgeline Plot</h3>
<p>Lastly, the <strong>ridgeline plot</strong> plots multiple kernel density distributions along the x-axis, making them more intuitive than the violin plot but partially overlapping them. Unfortunately, there is no default ridgeline plot neither in <code>matplotlib</code> nor in <code>seaborn</code>. We need to import it from <a href="https://github.com/leotac/joypy" target="_blank" rel="noopener"><code>joypy</code></a>.</p>
<pre><code class="language-python">from joypy import joyplot

joyplot(df, by='Arm', column='Income', colormap=sns.color_palette(&quot;crest&quot;, as_cmap=True));
plt.xlabel('Income');
plt.title(&quot;Ridgeline Plot, multiple groups&quot;);
</code></pre>
<p><img src="img/distr_86_0.png" alt="png"></p>
<p>Again, the ridgeline plot suggests that higher numbered treatment arms have higher income. From this plot it is also easier to appreciate the different shapes of the distributions.</p>
<h2 id="multiple-groups---tests">Multiple Groups - Tests</h2>
<p>Lastly, let&rsquo;s consider hypothesis tests to compare multiple groups. For simplicity, we will concentrate on the most popular one: the F-test.</p>
<h3 id="f-test">F-test</h3>
<p>With multiple groups, the most popular test is the <a href="https://en.wikipedia.org/wiki/F-test" target="_blank" rel="noopener"><strong>F-test</strong></a>. The F-test compares the variance of a variable across different groups. This analysis is also called <a href="https://en.wikipedia.org/wiki/Analysis_of_variance" target="_blank" rel="noopener">analysis of variance, or <strong>ANOVA</strong></a>.</p>
<p>In practice, the F-test statistic is</p>
<p>$$
\text{stat} = \frac{\text{between-group variance}}{\text{within-group variance}} = \frac{\sum_{g} \big( \bar x_g - \bar x \big) / (G-1)}{\sum_{g} \sum_{i \in g} \big( \bar x_i - \bar x_g \big) / (N-G)}
$$</p>
<p>Where $G$ is the number of groups, $N$ is the number of observations, $\bar x$ is the overall mean and $\bar x_g$ is the mean within group $g$. Under the null hypothesis of group independence, the f-statistic is <a href="https://en.wikipedia.org/wiki/F-distribution" target="_blank" rel="noopener">F-distributed</a>.</p>
<pre><code class="language-python">from scipy.stats import f_oneway

income_groups = [df.loc[df['Arm']==arm, 'Income'].values for arm in df['Arm'].dropna().unique()]
stat, p_value = f_oneway(*income_groups)
print(f&quot;F Test: statistic={stat:.4f}, p-value={p_value:.4f}&quot;)
</code></pre>
<pre><code>F Test: statistic=9.0911, p-value=0.0000
</code></pre>
<p>The test p-value is basically zero, implying a <strong>strong rejection</strong> of the null hypothesis of no differences in the <code>income</code> distribution across treatment arms.</p>
<h2 id="conclusion">Conclusion</h2>
<p>In this post we have see a ton of different ways to <strong>compare two or more distributions</strong>, both visually and statistically. This is a primary concern in many applications, but especially in causal inference where we use randomization to make treatment and control group as comparable as possible.</p>
<p>We have also seen how different methods might be better suited for <strong>different situations</strong>. Visual methods are great to build intuition, but statistical methods are essential for decision-making, since we need to be able to assess the magnitude and statistical significance of the differences.</p>
<h3 id="references">References</h3>
<p>[1] Student, <a href="https://www.jstor.org/stable/2331554" target="_blank" rel="noopener">The Probable Error of a Mean</a> (1908), <em>Biometrika</em>.</p>
<p>[2] F. Wilcoxon, <a href="https://www.jstor.org/stable/3001968" target="_blank" rel="noopener">Individual Comparisons by Ranking Methods</a> (1945), <em>Biometrics Bulletin</em>.</p>
<p>[3] B. L. Welch, <a href="https://academic.oup.com/biomet/article/34/1-2/28/210174" target="_blank" rel="noopener">The generalization of &ldquo;Student&rsquo;s&rdquo; problem when several different population variances are involved</a> (1947), <em>Biometrika</em>.</p>
<p>[4] H. B. Mann, D. R. Whitney, <a href="https://www.jstor.org/stable/2236101" target="_blank" rel="noopener">On a Test of Whether one of Two Random Variables is Stochastically Larger than the Other</a> (1947), <em>The Annals of Mathematical Statistics</em>.</p>
<p>[5] E. Brunner, U. Munzen, <a href="https://onlinelibrary.wiley.com/doi/abs/10.1002/%28SICI%291521-4036%28200001%2942:1%3C17::AID-BIMJ17%3E3.0.CO;2-U" target="_blank" rel="noopener">The Nonparametric Behrens-Fisher Problem: Asymptotic Theory and a Small-Sample Approximation</a> (2000), <em>Biometrical Journal</em>.</p>
<p>[6] A. N. Kolmogorov, <a href="https://link.springer.com/chapter/10.1007/978-94-011-2260-3_15" target="_blank" rel="noopener">Sulla determinazione empirica di una legge di distribuzione</a> (1933), <em>Giorn. Ist. Ital. Attuar.</em>.</p>
<p>[7] H. Cramér, <a href="https://www.tandfonline.com/doi/abs/10.1080/03461238.1928.10416862" target="_blank" rel="noopener">On the composition of elementary errors</a> (1928), <em>Scandinavian Actuarial Journal</em>.</p>
<p>[8] R. von Mises, <a href="https://www.ams.org/journals/bull/1937-43-05/S0002-9904-1937-06520-7/" target="_blank" rel="noopener">Wahrscheinlichkeit statistik und wahrheit</a> (1936), <em>Bulletin of the American Mathematical Society</em>.</p>
<p>[9] T. W. Anderson, D. A. Darling, <a href="https://projecteuclid.org/journals/annals-of-mathematical-statistics/volume-23/issue-2/Asymptotic-Theory-of-Certain-Goodness-of-Fit-Criteria-Based-on/10.1214/aoms/1177729437.full" target="_blank" rel="noopener">Asymptotic Theory of Certain &ldquo;Goodness of Fit&rdquo; Criteria Based on Stochastic Processes</a> (1953), <em>The Annals of Mathematical Statistics</em>.</p>
<h3 id="related-articles">Related Articles</h3>
<ul>
<li><a href="https://towardsdatascience.com/b63dc69e3d8c" target="_blank" rel="noopener">Goodbye Scatterplot, Welcome Binned Scatterplot</a></li>
</ul>
<h3 id="code">Code</h3>
<p>You can find the original Jupyter Notebook here:</p>
<p><a href="https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/distr.ipynb" target="_blank" rel="noopener">https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/distr.ipynb</a></p>

          </div>
          


















  
  



  
  
  
    
  
  
  
  <div class="media author-card content-widget-hr">
    
      
      <a href="https://matteocourthoud.github.io/"><img class="avatar mr-3 avatar-circle" src="/author/avatar_hu365eedc833ccd5578a90de7c849ec45e_385094_270x270_fill_q75_lanczos_center.jpg" alt=""></a>
    

    <div class="media-body">
      <h5 class="card-title"><a href="https://matteocourthoud.github.io/"></a></h5>
      
      <p class="card-text">I hold a PhD in economics from the University of Zurich. Now I work at the intersection of economics, data science and statistics. I regularly write about causal inference on <a href="https://medium.com/@matteo.courthoud" target="_blank" rel="noopener">Medium</a>.</p>
      <ul class="network-icon" aria-hidden="true">
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://medium.com/@matteo.courthoud" target="_blank" rel="noopener">
        <i class="fab fa-medium"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://www.linkedin.com/in/matteo-courthoud-7335198a/" target="_blank" rel="noopener">
        <i class="fab fa-linkedin"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://twitter.com/MatteoCourthoud/" target="_blank" rel="noopener">
        <i class="fab fa-twitter"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://github.com/matteocourthoud" target="_blank" rel="noopener">
        <i class="fab fa-github"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://open.spotify.com/user/1180947523" target="_blank" rel="noopener">
        <i class="fab fa-spotify"></i>
      </a>
    </li>
  
</ul>

    </div>
  </div>
















  
  




        </div>
        </article>
    </main>
  </div>
</div>
  </div>

  <div class="page-footer">
    
    
    <div class="container">
      <footer class="site-footer">

  

  
  <p class="powered-by">
    
      <a href="/privacy/">Privacy Policy</a>
    
    
       &middot; 
      <a href="/terms/">Terms</a>
    
  </p>
  

  

  
  







</footer>

    </div>
    
  </div>

  
<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        <pre><code class="tex hljs"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

      

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/instant.page/5.1.0/instantpage.min.js" integrity="sha512-1+qUtKoh9XZW7j+6LhRMAyOrgSQKenQ4mluTR+cvxXjP1Z54RxZuzstR/H9kgPXQsVB8IW7DMDFUJpzLjvhGSQ==" crossorigin="anonymous"></script>

      
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js" integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin="anonymous"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js" integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin="anonymous"></script>
      

      
      

      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mermaid/8.8.4/mermaid.min.js" integrity="sha512-as1BF4+iHZ3BVO6LLDQ7zrbvTXM+c/1iZ1qII/c3c4L8Rn5tHLpFUtpaEtBNS92f+xGsCzsD7b62XP3XYap6oA==" crossorigin="anonymous" title="mermaid"></script>
      

      
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/highlight.min.js" integrity="sha512-TDKKr+IvoqZnPzc3l35hdjpHD0m+b2EC2SrLEgKDRWpxf2rFCxemkgvJ5kfU48ip+Y+m2XVKyOCD85ybtlZDmw==" crossorigin="anonymous"></script>
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/languages/r.min.js"></script>
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/languages/python.min.js"></script>
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/languages/julia.min.js"></script>
        
      

    

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.7.1/leaflet.min.js" integrity="sha512-SeiQaaDh73yrb56sTW/RgVdi/mMqNeM2oBwubFHagc5BkixSpP1fvqF47mKzPGWYSSy4RwbBunrJBQ4Co8fRWA==" crossorigin="anonymous"></script>
    

    
    

    
    
    
      
      <script id="search-hit-fuse-template" type="text/x-template">
        <div class="search-hit" id="summary-{{key}}">
          <div class="search-hit-content">
            <div class="search-hit-name">
              <a href="{{relpermalink}}">{{title}}</a>
              <div class="article-metadata search-hit-type">{{type}}</div>
              <p class="search-hit-description">{{snippet}}</p>
            </div>
          </div>
        </div>
      </script>
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js" integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin="anonymous"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js" integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin="anonymous"></script>
      
    

    
    

    
    
    
    

    
    <script src="/js/bootstrap.bundle.min.6aed84840afc03ab4d5750157f69c120.js"></script>

    
    
      
      
      
      
      
      
      
    

    
    
    
    
    
    
    
    
      
      
    
    
    <script src="/en/js/wowchemy.min.4ea9cc8d09c5c158656ac1a804743b34.js"></script>

    
  <script async defer src="https://buttons.github.io/buttons.js"></script>




</body>
</html>
