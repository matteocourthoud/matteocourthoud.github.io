<!DOCTYPE html><html lang="en-us" >

<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  
  
  
    <meta name="generator" content="Wowchemy 5.1.0 for Hugo" />
  

  

  
  
  
  
  
    
    
    
  
  

  

  
  
  
    
  
  <meta name="description" content="Causal inference, machine learning and regularization bias
In causal inference, we often estimate causal effects by conditioning the analysis on other variables. We usually refer to these variables as control variables or confounders." />

  
  <link rel="alternate" hreflang="en-us" href="https://matteocourthoud.github.io/post/pretest/" />

  







  




  
  
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
  

  
  
  
    <meta name="theme-color" content="#003f5c" />
  

  
  
    
    <script src="/js/mathjax-config.js"></script>
  

  
  
  
  
    
    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha256-FMvZuGapsJLjouA6k7Eo2lusoAX9i0ShlWFG6qt7SLc=" crossorigin="anonymous">

    
    

    
    
    
      
    
    
      
      
        
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/styles/github.min.css" crossorigin="anonymous" title="hl-light" media="print" onload="this.media='all'">
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/styles/dracula.min.css" crossorigin="anonymous" title="hl-dark" media="print" onload="this.media='all'" disabled>
        
      
    

    
    
    
      <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.7.1/leaflet.min.css" integrity="sha512-1xoFisiGdy9nvho8EgXuXvnpR5GAMSjFwp40gSRE3NwdUdIMIKuPa7bqoUhLD0O/5tPNhteAsE5XyyMi5reQVA==" crossorigin="anonymous" media="print" onload="this.media='all'">
    

    

    
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js" integrity="" crossorigin="anonymous" async></script>
      
    
      
      

      
      

      
    
      
      

      
      

      
    
  

  
  
  
    
      
      <link rel="preload" as="style" href="https://fonts.googleapis.com/css2?family=Roboto+Mono&family=Source+Sans+Pro:wght@200;300;400;700&display=swap">
      <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Roboto+Mono&family=Source+Sans+Pro:wght@200;300;400;700&display=swap" media="print" onload="this.media='all'">
    
  

  
  
  
  
  
  <link rel="stylesheet" href="/css/wowchemy.4f7182ca394d705ee32d9d7750e9aa1d.css" />

  




<script async src="https://www.googletagmanager.com/gtag/js?id=UA-144780600-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];

  function gtag() {
      dataLayer.push(arguments);
  }

  function trackOutboundLink(url, target) {
    gtag('event', 'click', {
         'event_category': 'outbound',
         'event_label': url,
         'transport_type': 'beacon',
         'event_callback': function () {
           if (target !== '_blank') {
             document.location = url;
           }
         }
    });
    console.debug("Outbound link clicked: " + url);
  }

  function onClickCallback(event) {
    if ((event.target.tagName !== 'A') || (event.target.host === window.location.host)) {
      return;
    }
    trackOutboundLink(event.target, event.target.getAttribute('target'));  
  }

  gtag('js', new Date());
  gtag('config', 'UA-144780600-1', { 'anonymize_ip': true });
  gtag('set', {'cookie_flags': 'SameSite=None;Secure'});

  
  document.addEventListener('click', onClickCallback, false);
</script>


  

  

  




  
  
  

  

  
    <link rel="manifest" href="/index.webmanifest" />
  

  <link rel="icon" type="image/png" href="/media/icon_hu03e9b3967b83fd39296ec9da5ff1ea05_201175_32x32_fill_lanczos_center_3.png" />
  <link rel="apple-touch-icon" type="image/png" href="/media/icon_hu03e9b3967b83fd39296ec9da5ff1ea05_201175_180x180_fill_lanczos_center_3.png" />

  <link rel="canonical" href="https://matteocourthoud.github.io/post/pretest/" />

  
  
  
  
  
  
  
  
    
  
  
  <meta property="twitter:card" content="summary_large_image" />
  
  <meta property="og:site_name" content="Matteo Courthoud" />
  <meta property="og:url" content="https://matteocourthoud.github.io/post/pretest/" />
  <meta property="og:title" content="Double Debiased Machine Learning (part 1) | Matteo Courthoud" />
  <meta property="og:description" content="Causal inference, machine learning and regularization bias
In causal inference, we often estimate causal effects by conditioning the analysis on other variables. We usually refer to these variables as control variables or confounders." /><meta property="og:image" content="https://matteocourthoud.github.io/post/pretest/featured.png" />
    <meta property="twitter:image" content="https://matteocourthoud.github.io/post/pretest/featured.png" /><meta property="og:locale" content="en-us" />
  
    
      <meta
        property="article:published_time"
        content="2022-06-04T00:00:00&#43;00:00"
      />
    
    <meta property="article:modified_time" content="2022-06-04T00:00:00&#43;00:00">
  

  


    






  




<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://matteocourthoud.github.io/post/pretest/"
  },
  "headline": "Double Debiased Machine Learning (part 1)",
  
  "image": [
    "https://matteocourthoud.github.io/post/pretest/featured.png"
  ],
  
  "datePublished": "2022-06-04T00:00:00Z",
  "dateModified": "2022-06-04T00:00:00Z",
  
  "publisher": {
    "@type": "Organization",
    "name": "Matteo Courthoud",
    "logo": {
      "@type": "ImageObject",
      "url": "https://matteocourthoud.github.io/media/icon_hu03e9b3967b83fd39296ec9da5ff1ea05_201175_192x192_fill_lanczos_center_3.png"
    }
  },
  "description": "Causal inference, machine learning and regularization bias\nIn causal inference, we often estimate causal effects by conditioning the analysis on other variables. We usually refer to these variables as control variables or confounders."
}
</script>

  

  

  
  
  
  
  
    <script src="https://cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.1.1/cookieconsent.min.js" integrity="sha256-5VhCqFam2Cn+yjw61zbBNrbHVJ6SRydPeKopYlngbiQ=" crossorigin="anonymous"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.1.1/cookieconsent.min.css" integrity="sha256-zQ0LblD/Af8vOppw18+2anxsuaz3pWYyVWi+bTvTH8Q=" crossorigin="anonymous">
  
  <script>
  window.addEventListener("load", function(){
    window.cookieconsent.initialise({
      "palette": {
        "popup": {
          "background": "#003f5c",
          "text": "rgb(255, 255, 255)"
        },
        "button": {
          "background": "rgb(255, 255, 255)",
          "text": "#003f5c"
        }
      },
      "theme": "classic",
      "content": {
        "message": "This website uses cookies to ensure you get the best experience on our website.",
        "dismiss": "Got it!",
        "link": "Learn more",
        "href": "/privacy/"
      }
    })});
  </script>


  





  <title>Double Debiased Machine Learning (part 1) | Matteo Courthoud</title>
</head>


<body id="top" data-spy="scroll" data-offset="70" data-target="#TableOfContents" class="page-wrapper   " data-wc-page-id="1c03618e28411fd9e7c78ea524ff7f59" >

  
  
  
  
  
  
  
  
  <script src="/js/wowchemy-init.min.66d3e0fff6d32c4ece05adee927fbd96.js"></script>

  


<aside class="search-modal" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Search</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#" aria-label="Close"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        
        <input name="q" id="search-query" placeholder="Search..." autocapitalize="off"
        autocomplete="off" autocorrect="off" spellcheck="false" type="search" class="form-control"
        aria-label="Search...">
        
      </div>

      
      

      

    </section>
    <section class="section-search-results">

      <div id="search-hits">
        
      </div>

    </section>
  </div>
</aside>



  <div class="page-header">
    












<nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id="navbar-main">
  <div class="container-xl">

    
    <div class="d-none d-lg-inline-flex">
      <a class="navbar-brand" href="/">Matteo Courthoud</a>
    </div>
    

    
    <button type="button" class="navbar-toggler" data-toggle="collapse"
            data-target="#navbar-content" aria-controls="navbar-content" aria-expanded="false" aria-label="Toggle navigation">
    <span><i class="fas fa-bars"></i></span>
    </button>
    

    
    <div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none">
      <a class="navbar-brand" href="/">Matteo Courthoud</a>
    </div>
    

    
    
    <div class="navbar-collapse main-menu-item collapse justify-content-end" id="navbar-content">

      
      <ul class="navbar-nav d-md-inline-flex">
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
            
            
            
              
            
            
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#about"><span>Home</span></a>
        </li>

        
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
            
            
            
              
            
            
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#posts"><span>Posts</span></a>
        </li>

        
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
            
            
            
              
            
            
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#projects"><span>Research</span></a>
        </li>

        
        

        
        <li class="nav-item dropdown">
          <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown" aria-haspopup="true"><span>Courses</span><span class="caret"></span>
          </a>
          <div class="dropdown-menu">
            
              <a class="dropdown-item" href="/course/ml-econ/"><span>Machine Learning for Economics</span></a>
            
              <a class="dropdown-item" href="/course/data-science/"><span>Data Science with Python</span></a>
            
              <a class="dropdown-item" href="/course/empirical-io/"><span>PhD Industrial Organization</span></a>
            
              <a class="dropdown-item" href="/course/metrics/"><span>PhD Econometrics</span></a>
            
              <a class="dropdown-item" href="https://pp4rs.github.io/"><span>Programming Practices for Research</span></a>
            
          </div>
        </li>

        
        

        

        
        
        

        

        
        
        
        

        
          
            
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/cv"><span>CV</span></a>
        </li>

        
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
            
            
            
              
            
            
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#contact"><span>Contact</span></a>
        </li>

        
        

      

        
      </ul>
    </div>

    <ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2">

      
      
        
      

      
      

      
      

      
      

    </ul>

  </div>
</nav>


  </div>

  <div class="page-body">
    <div class="container-fluid docs">
  <div class="row">

    <div class="col-xl-2 col-lg-2 d-none d-xl-block d-lg-block empty">
    </div>

    <div class="col-2 col-xl-2 col-lg-2 d-none d-lg-block docs-toc">
      <ul class="nav toc-top">
        <li><a href="#" id="back_to_top" class="docs-toc-title">Contents</a></li>
      </ul>
      <nav id="TableOfContents">
  <ul>
    <li><a href="#pre-testing">Pre-Testing</a></li>
    <li><a href="#the-bias">The Bias</a></li>
    <li><a href="#when-is-pre-testing-a-problem">When is pre-testing a problem?</a></li>
    <li><a href="#pre-testing-and-machine-learning">Pre-Testing and Machine Learning</a></li>
    <li><a href="#conclusion">Conclusion</a>
      <ul>
        <li><a href="#references">References</a></li>
        <li><a href="#related-articles">Related Articles</a></li>
        <li><a href="#code">Code</a></li>
      </ul>
    </li>
  </ul>
</nav>
    </div>

    <main class="col-xl-8 col-lg-8 docs-content" role="main">
        <article class="article">
        




















  


<div class="article-container pt-3">
  <h1>Double Debiased Machine Learning (part 1)</h1>

  

  


<div class="article-metadata">

  
  

  
  <span class="article-date">
    
    
      
    
    Jun 4, 2022
  </span>
  

  

  
  <span class="middot-divider"></span>
  <span class="article-reading-time">
    11 min read
  </span>
  

  
  
  
  
  
  

  
  

</div>

  





</div>


<div class="article-header article-container featured-image-wrapper mt-4 mb-4" style="max-width: 1372px; max-height: 700px;">
  <div style="position: relative">
    <img src="/post/pretest/featured.png" alt="" class="featured-image">
    
  </div>
</div>


        <div class="article-container">
          <div class="article-style" align="justify">
            <p><em>Causal inference, machine learning and regularization bias</em></p>
<p>In causal inference, we often estimate causal effects by conditioning the analysis on other variables. We usually refer to these variables as <strong>control variables</strong> or <strong>confounders</strong>. In randomized control trials or AB tests, conditioning can increase the power of the analysis, by reducing imbalances that have emerged despite randomization. However, conditioning is even more important in observational studies, where, absent randomization, it might be <a href="https://towardsdatascience.com/b63dc69e3d8c" target="_blank" rel="noopener">essential to recover causal effects</a>.</p>
<p>When we have many control variables, we might want to <strong>select the most relevant ones</strong>, ppossibly capturing nonlinearities and interactions. Machine learning algorithms are perfect for this task. However, in these cases, we are introducing a bias that is called <strong>regularization or pre-test, or feature selection bias</strong>. In this and the next blog post, I try to explain the source of the bias and a very poweful solution called <strong>double debiased machine learning</strong>, which has been probably one of the most relevant advancement at the intersection of machine learning and causal inference of the last decade.</p>
<h2 id="pre-testing">Pre-Testing</h2>
<p>Since this is a complex topic, let&rsquo;s start with a simple example.</p>
<p>Suppose we were a firm and we are interested in the <strong>effect of advertisement spending on revenue</strong>: is advertisement worth the money? There are also a lot of other things that might influence sales, therefore, we are thinking of controlling for past sales in the analysis, in order to increase the power of our analysis.</p>
<p>Assume the data generating process can be represented with the following <a href="https://towardsdatascience.com/b63dc69e3d8c" target="_blank" rel="noopener"><strong>Directed Acyclic Graph (DAG)</strong></a>. If you are not familiar with DAGs, I have written a short <a href="https://towardsdatascience.com/b63dc69e3d8c" target="_blank" rel="noopener">introduction here</a>.</p>
<pre><code class="language-mermaid">flowchart LR
classDef included fill:#DCDCDC,stroke:#000000,stroke-width:2px;
classDef excluded fill:#ffffff,stroke:#000000,stroke-width:2px;
classDef unobserved fill:#ffffff,stroke:#000000,stroke-width:2px,stroke-dasharray: 5 5;

D((ad spend))
Z((past sales))
Y((sales))

D --&gt; Y
Z -- ??? --&gt; Y
Z --&gt; D

class D,Y included;
class Z excluded;

linkStyle 0 stroke:#00ff00,stroke-width:4px;
</code></pre>
<p>I import the data generating process <code>dgp_tbd()</code> from <a href="https://github.com/matteocourthoud/Blog-Posts/blob/main/src/dgp.py" target="_blank" rel="noopener"><code>src.dgp</code></a> and some plotting functions and libraries from <a href="https://github.com/matteocourthoud/Blog-Posts/blob/main/src/utils.py" target="_blank" rel="noopener"><code>src.utils</code></a>.</p>
<pre><code class="language-python">%matplotlib inline
%config InlineBackend.figure_format = 'retina'
</code></pre>
<pre><code class="language-python">from src.utils import *
from src.dgp import dgp_pretest

df = dgp_pretest().generate_data()
df.head()
</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
</code></pre>
<p></style></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>ads</th>
      <th>sales</th>
      <th>past_sales</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>16.719800</td>
      <td>19.196620</td>
      <td>6.624345</td>
    </tr>
    <tr>
      <th>1</th>
      <td>7.732222</td>
      <td>9.287491</td>
      <td>4.388244</td>
    </tr>
    <tr>
      <th>2</th>
      <td>10.923469</td>
      <td>11.816906</td>
      <td>4.471828</td>
    </tr>
    <tr>
      <th>3</th>
      <td>8.457062</td>
      <td>9.024376</td>
      <td>3.927031</td>
    </tr>
    <tr>
      <th>4</th>
      <td>13.085146</td>
      <td>12.814823</td>
      <td>5.865408</td>
    </tr>
  </tbody>
</table>
</div>
<p>We have data on $1000$ different markets, in which we observe current <code>sales</code>, the amount spent in <code>advertisement</code> and <code>past sales</code>.</p>
<p>We want to understand <code>ads</code> spending is effective in increasing <code>sales</code>. One possibility is to regress the latter on the former, using the following regression model, also called the <strong>short model</strong>.</p>
<p>$$
\text{sales} = \alpha \cdot \text{ads} + \varepsilon
$$</p>
<p>Should we also include <code>past sales</code> in the regression? Then the regression model would be the following, also called <strong>long model</strong>.</p>
<p>$$
\text{sales} = \alpha \cdot \text{ads} + \beta \cdot \text{past sales} + \varepsilon
$$</p>
<p>Since we are not sure whether to condition the analysis on <code>past sales</code>, we could <strong>let the data decide</strong>: we could run the second regression and, if the effect of <code>past sales</code>, $\beta$, is statistically significant, we are good with the long model, otherwise we run the short model.</p>
<pre><code class="language-python">smf.ols('sales ~ ads + past_sales', df).fit().summary().tables[1]
</code></pre>
<table class="simpletable">
<tr>
       <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  
</tr>
<tr>
  <th>Intercept</th>  <td>    0.1405</td> <td>    0.185</td> <td>    0.758</td> <td> 0.448</td> <td>   -0.223</td> <td>    0.504</td>
</tr>
<tr>
  <th>ads</th>        <td>    0.9708</td> <td>    0.030</td> <td>   32.545</td> <td> 0.000</td> <td>    0.912</td> <td>    1.029</td>
</tr>
<tr>
  <th>past_sales</th> <td>    0.3381</td> <td>    0.095</td> <td>    3.543</td> <td> 0.000</td> <td>    0.151</td> <td>    0.525</td>
</tr>
</table>
<p>It seems that the effect of <code>past sales</code> on current <code>sales</code> is positive and significant. Therefore, we are happy with our specification and we conclude that the effect of <code>ads</code> on <code>sales</code> is positive and significant with a 95% confidence interval of $[0.912, 1.029]$.</p>
<h2 id="the-bias">The Bias</h2>
<p>There is an <strong>issue</strong> with this procedure: we are not taking into account the fact that we have run a test to decide whether to include <code>past_sales</code> in the regression. The fact that we have decided to include <code>past_sales</code> because its coefficient is significant <em>does</em> have an effect on the inference on the effect of <code>ads</code> on <code>sales</code>, $\alpha$.</p>
<p>The best way to understand the problem is through <strong>simulations</strong>. Since we have access to the data generating process <code>dgp_pretest()</code> (unlike in real life), we can just test what would happen if we were repeating this procedure multiple times:</p>
<ol>
<li>We draw a new sample from the data generating process.</li>
<li>We regress <code>sales</code> on <code>ads</code> and <code>past_sales</code>.</li>
<li>If the coefficient of <code>past_sales</code> is significant at the 95% level, we keep $\hat \alpha_{long}$ from (2).</li>
<li>Otherwise, we regress <code>sales</code> on <code>ads</code> only, and we keep that coefficient $\hat \alpha_{short}$.</li>
</ol>
<p>I write a <code>pre_test</code> function to implement the procedure above. I also save the coefficients from both regressions, long and short, and the chosen one, called the <strong>pre-test coefficient</strong>.</p>
<p><strong>Reminder</strong>: we are pre-testing the effect of <code>past_sales</code> on <code>sales</code> but the coefficient of interest is the one of <code>ads</code> on <code>sales</code>.</p>
<pre><code class="language-python">def pre_testing(d='ads', y='sales', x='past_sales', K=1000, **kwargs):
    
    # Init
    alpha = {'Long': np.zeros(K), 'Short': np.zeros(K), 'Pre-test': np.zeros(K)}

    # Loop over simulations
    for k in range(K):
        
        # Generate data
        df = dgp_pretest().generate_data(seed=k, **kwargs)
        
        # Compute coefficients
        alpha['Long'][k] = smf.ols(f'{y} ~ {d} + {x}', df).fit().params[1]
        alpha['Short'][k] = smf.ols(f'{y} ~ {d}', df).fit().params[1]
    
        # Compute significance of beta
        p_value = smf.ols(f'{y} ~ {d} + {x}', df).fit().pvalues[2]
        
        # Select specification based on p-value
        if p_value&lt;0.05:
            alpha['Pre-test'][k] = alpha['Long'][k]
        else:
            alpha['Pre-test'][k] = alpha['Short'][k]
    
    return alpha
</code></pre>
<pre><code class="language-python">alphas = pre_testing()
</code></pre>
<p>We can now plot the distributions (over simulations) of the estimated coefficients.</p>
<pre><code class="language-python">def plot_alphas(alphas, true_alpha):
    
    # Init plot
    fig, axes = plt.subplots(1, len(alphas), figsize=(4*len(alphas), 5), sharey=True, sharex=True)

    # Make one plot for each set of coefficients
    for i, key in enumerate(alphas.keys()):
        axes[i].hist(alphas[key], bins=30, lw=.1)
        axes[i].set_title(key)
        axes[i].axvline(true_alpha, c='r', ls='--')
        legend_text = [r'$\alpha=%.0f$' % true_alpha, r'$\hat \alpha=%.4f$' % np.mean(alphas[key])]
        axes[i].legend(legend_text, prop={'size': 10}, loc='upper right')
</code></pre>
<pre><code class="language-python">plot_alphas(alphas, true_alpha=1)
</code></pre>
<p><img src="img/pretest_16_0.png" alt="png"></p>
<p>In the plot above, I have depicted the estimated coefficients, across simulations, for the different regression specifications.</p>
<p>As we can see from the first plot, if we were always running the <strong>long regression</strong>, our estimator $\hat \alpha_{long}$ would be unbiased and normally distributed. However, if we were always running the <strong>short regression</strong> (second plot), our estimator $\hat \alpha_{short}$ would be <strong>biased</strong>.</p>
<p>The <strong>pre-testing</strong> procedure generates an estimator $\hat \alpha_{pretest}$ that is a mix of the two: most of the times we select the correct specification, the long regression, but sometimes the pre-test fails to reject the null hypothesis of no effect of <code>past sales</code> on <code>sales</code>, $H_0 : \beta = 0$, and we select the incorrect specification, running the short regression.</p>
<p>Importantly, the pre-testing procedure <strong>does not generate a biased estimator</strong>. As we can see in the last plot, the estimated coefficient is very close to the true value, 1. The reason is that most of the time, the number of times we select the <em>short</em> regression is sufficiently small not to introduce bias, but not small enough to have valid inference.</p>
<p>Indeed, <strong>pre-testing distorts inference</strong>: the distribution of the estimator $\hat \alpha_{pretest}$ is not normal anymore, but bimodal. The <strong>consequence</strong> is that our confidence intervals for $\alpha$ are going to have the wrong coverage (contain the true effect with a different probability than the claimed one).</p>
<h2 id="when-is-pre-testing-a-problem">When is pre-testing a problem?</h2>
<p>The problem of pre-testing arises because of the bias generated by running the short regression: <a href="https://towardsdatascience.com/344ac1477699" target="_blank" rel="noopener"><strong>omitted variable bias (OVB)</strong></a>. In you are not familiar with OVB, I have written a <a href="https://towardsdatascience.com/344ac1477699" target="_blank" rel="noopener">short introduction here</a>. In general however, we can express the omitted variable bias introduced by regressing $Y$ on $D$ ignoring $X$ as</p>
<p>$$
\text{OVB} = \beta \delta \qquad \text{ where } \qquad \beta := \frac{Cov(X, Y)}{Var(X)}, \quad \delta := \frac{Cov(D, X)}{Var(D)}
$$</p>
<p>Where $\beta$ is the effect of $X$ (<code>past sales</code> in our example) on $Y$ (<code>sales</code>) and $\delta$ is the effect of $D$ (<code>ads</code>) on $X$.</p>
<p>Pre-testing is a <strong>problem</strong> if</p>
<ol>
<li>We run the short regression instead of the long one <em>and</em></li>
<li>The effect of the bias is sensible</li>
</ol>
<p>What can help improving (1), i.e. the probability of correctly rejecting the null hypothesis of zero effect of <code>past sales</code>, $H_0 : \beta = 0$? The answer is simple: a <strong>bigger sample size</strong>. If we have more observations, we can more precisely estimate $\beta$ and it is going to be less likely that we commit a <a href="https://en.wikipedia.org/wiki/Type_I_and_type_II_errors" target="_blank" rel="noopener">type 2 error</a> and run the short regression instead of the long one.</p>
<p>Let&rsquo;s simulate the estimated coefficient $\hat \alpha$ under different sample sizes. Remember that the sample size used until now is $N=1000$.</p>
<pre><code class="language-python">Ns = [100,300,1000,3000]
alphas = {f'N = {n:.0f}':  pre_testing(N=n)['Pre-test'] for n in Ns}
plot_alphas(alphas, true_alpha=1)
</code></pre>
<p><img src="img/pretest_20_0.png" alt="png"></p>
<p>As we can see from the plots, as the sample size increases (left to right), the bias decreases and the distribution of the estimator $\hat \alpha_{pretest}$ converges to a normal distribution.</p>
<p>What happens instead if the value of $\beta$ was different? It is probably going to affect point (2) in the previous paragraph, but how?</p>
<ul>
<li>
<p>If $\beta$ is <strong>very small</strong>, it is going to be hard to detect it, and we will often end up running the <em>short</em> regression, introducing a bias. However, if $\beta$ is very small, it also implies that the <strong>magnitude of the bias</strong> is small and therefore it is not going to affect our estimate of $\alpha$ much</p>
</li>
<li>
<p>If $\beta$ is <strong>very big</strong>, it is going to be easy to detect and we will often end up running the <em>long</em> regression, avoiding the bias (which would have been very big though).</p>
</li>
</ul>
<p>Let&rsquo;s simulate the estimated coefficient $\hat \alpha$ under different values of $\beta$. The true value used until now was $\beta = 0.3$.</p>
<pre><code class="language-python">betas = 0.3 * np.array([0.1,0.3,1,3])
alphas = {f'beta = {b:.2f}':  pre_testing(b=b)['Pre-test'] for b in betas}
plot_alphas(alphas, true_alpha=1)
</code></pre>
<p><img src="img/pretest_22_0.png" alt="png"></p>
<p>As we can see from the plots, as the value of $\beta$ increases, the bias first appears and then disappears. When $\beta$ is small (left plot), we often choose the short regression, but the bias is small and the average estimate is very close to the true value. For intermediate values of $\beta$, the bias is sensible and it has a clear effect on inference. Lastly, for large values of $\beta$ instead (right plot), we always run the long regression and the bias disappears.</p>
<p>But <strong>when is a coefficient big or small</strong>? And big or small with respect to what? The answer is simple: with respect to the <strong>sample size</strong>, or more accurately, with respect to the inverse of the square root of the sample size, $1 / \sqrt{n}$. The reason is deeply rooted in the <a href="https://en.wikipedia.org/wiki/Central_limit_theorem" target="_blank" rel="noopener">Central Limit Theorem</a>, but I won&rsquo;t cover it here.</p>
<p>The idea is easier to show than to explain, so let&rsquo;s repeat the same simulation as above, but now we will increase both the coefficient and the sample size at the same time.</p>
<pre><code class="language-python">betas = 0.3 * 30 / np.sqrt(Ns)
alphas = {f'N = {n:.0f}':  pre_testing(b=b, N=n)['Pre-test'] for n,b in zip(Ns,betas)}
plot_alphas(alphas, true_alpha=1)
</code></pre>
<p><img src="img/pretest_24_0.png" alt="png"></p>
<p>As we can see, now that $\beta$ is proportional to $1 / \sqrt{n}$, the distortion is not going away, not matter the sample size. Therefore, inference will always be wrong.</p>
<p>While a coefficient that depends on the sample size might sound <strong>not intuitive</strong>, it captures well the idea of <strong>magnitude</strong> in a world where we do inference relying on asymptotic results, first among all the <a href="https://en.wikipedia.org/wiki/Central_limit_theorem" target="_blank" rel="noopener">Central Limit Theorem</a>. In fact, the Central Limit Theorem relieas on an infinitely large sample size. However, with an infinite amount of data, no coefficient is small and any non-zero effect is detected with certainty.</p>
<h2 id="pre-testing-and-machine-learning">Pre-Testing and Machine Learning</h2>
<p>So far we talked about a linear regression with only 2 variables. Where is the <strong>machine learning</strong> we were promised?</p>
<p>Usually we do not have just one control variable (or confounder), but many. Moreover, we might want to be flexible with respect to the functional form through which these control variables enter the model. In general, we will assume the following model:</p>
<p>$$
Y = \alpha D + g_0(X) + u
\newline
D = m_0(X) + v
$$</p>
<p>Where the effect of interest is still $\alpha$, $X$ is potentially high dimensional and we do not take a stand on the functional form through which $X$ influences $D$ or $Y$.</p>
<p>In this setting, it is natural to use a machine learning algorithm to estimate $g_0$ and $m_0$. However, machine learning algorithms usually introduce a <strong>regularization bias</strong> that is comparable to pre-testing.</p>
<p>Possibly, the &ldquo;simplest&rdquo; way to think about it is <a href="https://en.wikipedia.org/wiki/Lasso_%28statistics%29" target="_blank" rel="noopener">Lasso</a>. Lasso is linear in $X$, with a penalization term that effectively just performs the variable selection we discussed above. Therefore, if we were to use Lasso of $X$ and $D$ on $Y$ we would be introducing regularization bias and inference would be distorted. The same goes for more complex algorithms.</p>
<p>Lastly, you might still wonder &ldquo;why is the model linear in the treatment variable $D$?&rdquo;. Doing inference is much easier in linear model, not only for computational reasons but also for interpretation. Moreover, if the treatment $D$ is binary, the linear functional form is without loss of generality. A stronger assumption is the additive separability of $D$ and $g(X)$.</p>
<h2 id="conclusion">Conclusion</h2>
<p>In this post, I have tried to explain how does regularization bias emerges and why it can the an issue in causal inference. This problem is inherently related to settings with many control variables or where we would like to have a model-free (i.e. non-parametric) when controlling for confounders. These are exactly the settings in which machine learning algorithms can be useful.</p>
<p>In the next post, I will cover a simple and yet incredibly powerful solution to this problem: double-debiased machine learning.</p>
<h3 id="references">References</h3>
<p>[1] A. Belloni, D. Chen, V. Chernozhukov, C. Hansen, <a href="https://onlinelibrary.wiley.com/doi/abs/10.3982/ECTA9626" target="_blank" rel="noopener">Sparse Models and Methods for Optimal Instruments With an Application to Eminent Domain</a> (2012), <em>Econometrica</em>.</p>
<p>[2] A. Belloni, V. Chernozhukov, C. Hansen, <a href="https://academic.oup.com/restud/article-abstract/81/2/608/1523757" target="_blank" rel="noopener">Inference on treatment effects after selection among high-dimensional controls</a> (2014), <em>The Review of Economic Studies</em>.</p>
<p>[3] V. Chernozhukov, D. Chetverikov, M. Demirer, E. Duflo, C. Hansen, W. Newey, J. Robins, <a href="https://academic.oup.com/ectj/article/21/1/C1/5056401" target="_blank" rel="noopener">Double/debiased machine learning for treatment and structural parameters</a> (2018), <em>The Econometrics Journal</em>.</p>
<h3 id="related-articles">Related Articles</h3>
<ul>
<li><a href="https://towardsdatascience.com/344ac1477699" target="_blank" rel="noopener">Understanding Omitted Variable Bias</a></li>
<li><a href="https://towardsdatascience.com/59f801eb3299" target="_blank" rel="noopener">Understanding The Frisch-Waugh-Lovell Theorem</a></li>
<li><a href="https://towardsdatascience.com/b63dc69e3d8c" target="_blank" rel="noopener">DAGs and Control Variables</a></li>
</ul>
<h3 id="code">Code</h3>
<p>You can find the original Jupyter Notebook here:</p>
<p><a href="https://github.com/matteocourthoud/Blog-Posts/blob/main/pretest.ipynb" target="_blank" rel="noopener">https://github.com/matteocourthoud/Blog-Posts/blob/main/pretest.ipynb</a></p>

          </div>
          


















  
  



  
  
  
    
  
  
  
  <div class="media author-card content-widget-hr">
    
      
      <a href="https://matteocourthoud.github.io/"><img class="avatar mr-3 avatar-circle" src="/author/avatar_hu365eedc833ccd5578a90de7c849ec45e_385094_270x270_fill_q75_lanczos_center.jpg" alt=""></a>
    

    <div class="media-body">
      <h5 class="card-title"><a href="https://matteocourthoud.github.io/"></a></h5>
      
      <p class="card-text">I hold a PhD in economics from the University of Zurich. Now I work at the intersection of economics, data science and statistics. I regularly write about causal inference on <a href="https://medium.com/@matteo.courthoud" target="_blank" rel="noopener">Medium</a>.</p>
      <ul class="network-icon" aria-hidden="true">
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://medium.com/@matteo.courthoud" target="_blank" rel="noopener">
        <i class="fab fa-medium"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://www.linkedin.com/in/matteo-courthoud-7335198a/" target="_blank" rel="noopener">
        <i class="fab fa-linkedin"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://twitter.com/MatteoCourthoud/" target="_blank" rel="noopener">
        <i class="fab fa-twitter"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://github.com/matteocourthoud" target="_blank" rel="noopener">
        <i class="fab fa-github"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://open.spotify.com/user/1180947523" target="_blank" rel="noopener">
        <i class="fab fa-spotify"></i>
      </a>
    </li>
  
</ul>

    </div>
  </div>
















  
  




        </div>
        </article>
    </main>
  </div>
</div>
  </div>

  <div class="page-footer">
    
    
    <div class="container">
      <footer class="site-footer">

  

  
  <p class="powered-by">
    
      <a href="/privacy/">Privacy Policy</a>
    
    
       &middot; 
      <a href="/terms/">Terms</a>
    
  </p>
  

  

  
  







</footer>

    </div>
    
  </div>

  
<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        <pre><code class="tex hljs"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

      

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/instant.page/5.1.0/instantpage.min.js" integrity="sha512-1+qUtKoh9XZW7j+6LhRMAyOrgSQKenQ4mluTR+cvxXjP1Z54RxZuzstR/H9kgPXQsVB8IW7DMDFUJpzLjvhGSQ==" crossorigin="anonymous"></script>

      
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js" integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin="anonymous"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js" integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin="anonymous"></script>
      

      
      

      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mermaid/8.8.4/mermaid.min.js" integrity="sha512-as1BF4+iHZ3BVO6LLDQ7zrbvTXM+c/1iZ1qII/c3c4L8Rn5tHLpFUtpaEtBNS92f+xGsCzsD7b62XP3XYap6oA==" crossorigin="anonymous" title="mermaid"></script>
      

      
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/highlight.min.js" integrity="sha512-TDKKr+IvoqZnPzc3l35hdjpHD0m+b2EC2SrLEgKDRWpxf2rFCxemkgvJ5kfU48ip+Y+m2XVKyOCD85ybtlZDmw==" crossorigin="anonymous"></script>
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/languages/r.min.js"></script>
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/languages/python.min.js"></script>
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/languages/julia.min.js"></script>
        
      

    

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.7.1/leaflet.min.js" integrity="sha512-SeiQaaDh73yrb56sTW/RgVdi/mMqNeM2oBwubFHagc5BkixSpP1fvqF47mKzPGWYSSy4RwbBunrJBQ4Co8fRWA==" crossorigin="anonymous"></script>
    

    
    

    
    
    
      
      <script id="search-hit-fuse-template" type="text/x-template">
        <div class="search-hit" id="summary-{{key}}">
          <div class="search-hit-content">
            <div class="search-hit-name">
              <a href="{{relpermalink}}">{{title}}</a>
              <div class="article-metadata search-hit-type">{{type}}</div>
              <p class="search-hit-description">{{snippet}}</p>
            </div>
          </div>
        </div>
      </script>
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js" integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin="anonymous"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js" integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin="anonymous"></script>
      
    

    
    

    
    
    
    

    
    <script src="/js/bootstrap.bundle.min.6aed84840afc03ab4d5750157f69c120.js"></script>

    
    
      
      
      
      
      
      
      
    

    
    
    
    
    
    
    
    
      
      
    
    
    <script src="/en/js/wowchemy.min.4ea9cc8d09c5c158656ac1a804743b34.js"></script>

    
  <script async defer src="https://buttons.github.io/buttons.js"></script>




</body>
</html>
