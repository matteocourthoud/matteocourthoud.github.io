<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts | Matteo Courthoud</title>
    <link>https://matteocourthoud.github.io/post/</link>
      <atom:link href="https://matteocourthoud.github.io/post/index.xml" rel="self" type="application/rss+xml" />
    <description>Posts</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><copyright>Theme edited by Matteo CourthoudÂ© - Want to have a similar website? [Guide here](https://matteocourthoud.github.io/post/website/).</copyright><lastBuildDate>Fri, 08 Apr 2022 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://matteocourthoud.github.io/media/icon_hu03e9b3967b83fd39296ec9da5ff1ea05_201175_512x512_fill_lanczos_center_3.png</url>
      <title>Posts</title>
      <link>https://matteocourthoud.github.io/post/</link>
    </image>
    
    <item>
      <title>AIPW</title>
      <link>https://matteocourthoud.github.io/post/aipw/</link>
      <pubDate>Fri, 08 Apr 2022 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/post/aipw/</guid>
      <description>&lt;p&gt;In this tutorial, we are going to see how to robustly estimate treatment effects when treatment is conditionally randomly assigned, using the &lt;strong&gt;Augmented Inverse Propensity Weighted&lt;/strong&gt; estimator, also known as &lt;strong&gt;doubly-robust&lt;/strong&gt; estimator.&lt;/p&gt;
&lt;p&gt;For this tutorial, I assume you are familiar with the following concepts:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Rubin&amp;rsquo;s potential outcome framework&lt;/li&gt;
&lt;li&gt;Propensity score weighting&lt;/li&gt;
&lt;li&gt;Basic machine learning&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;setting&#34;&gt;Setting&lt;/h2&gt;
&lt;p&gt;We assume that for a set of i.i.d. subjects $i = 1, &amp;hellip;, n$ we observed a tuple $(X_i, T_i, Y_i)$ comprised of&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;a feature vector $X_i \in \mathbb R^n$&lt;/li&gt;
&lt;li&gt;a treatment assignment $T_i \in \lbrace 0, 1 \rbrace$&lt;/li&gt;
&lt;li&gt;a response $Y_i \in \mathbb R$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Assumption 1 : unconfoundedness&lt;/strong&gt; (or ignorability, or selection on observables)&lt;/p&gt;
&lt;p&gt;$$
\big \lbrace Y_i^{(1)} , Y_i^{(0)} \big \rbrace \ \perp \ T_i \ | \ X_i
$$&lt;/p&gt;
&lt;p&gt;i.e. conditional on observable characteristics $X$, the treatment assignment $T$ is as good as random.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Assumption 2: overlap&lt;/strong&gt; (or bounded support)&lt;/p&gt;
&lt;p&gt;$$
\exists \eta &amp;gt; 0 \ : \ \eta \leq \mathbb E \left[ T_i = 1 \ \big | \ X_i = x \right] \leq 1-\eta
$$&lt;/p&gt;
&lt;p&gt;i.e. no observation is deterministically assigned to the treatment or control group.&lt;/p&gt;
&lt;h2 id=&#34;the-ipw-estimator&#34;&gt;The IPW Estimator&lt;/h2&gt;
&lt;p&gt;We want to estimate the &lt;strong&gt;average treatment effect&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;$$
\tau(x) = \mathbb E \left[ Y^{(1)} - Y^{(0)} \ \big| \ X = x \right]
$$&lt;/p&gt;
&lt;p&gt;We would like to obtain an unbiased estimator that satifies a central limit theorem of the form&lt;/p&gt;
&lt;p&gt;$$
\sqrt{n} ( \hat \tau - \tau) \ \overset{d}{\to} \ N(0, V)
$$&lt;/p&gt;
&lt;p&gt;thus enabling us to construct &lt;strong&gt;confidence intervals&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Under &lt;strong&gt;unconfoundedness&lt;/strong&gt;, we can rewrite the average treatment effect as&lt;/p&gt;
&lt;p&gt;$$
\tau(x) = \mathbb E \left[ Y^{(1)} - Y^{(0)} \ \big| \ X = x \right] = \mathbb E \left[ \frac{T_i Y_i}{e(X_i)} - \frac{(1-T_i) Y_i}{1-e(X_i)} \right]
$$&lt;/p&gt;
&lt;p&gt;where $e(X_i)$ is the &lt;strong&gt;propensity score&lt;/strong&gt; of observation $i$,&lt;/p&gt;
&lt;p&gt;$$
e(x) = \mathbb P \left[ T_i = 1 \ \big | \ X_i = x \right]
$$&lt;/p&gt;
&lt;p&gt;i.e. its probability of being treated.&lt;/p&gt;
&lt;p&gt;Note that this formulation of the average treatment effect does not depend on the potential outcomes $Y_i^{(1)}$ and $Y_i^{(0)}$, but only on the observed outcomes $Y_i$.&lt;/p&gt;
&lt;p&gt;This formulation of the average treatment effect implies the &lt;strong&gt;Inverse Propensity Weighted&lt;/strong&gt; estimator which is an unbiased estimator for the average treatment effect $\tau$&lt;/p&gt;
&lt;p&gt;$$
\hat \tau^{*}_{IPW} = \frac{1}{n} \sum _ {i=1}^{n} \left( \frac{T_i Y_i}{e(X_i)} - \frac{(1-T_i) Y_i}{1-e(X_i)} \right)
$$&lt;/p&gt;
&lt;p&gt;However, this estimator is &lt;strong&gt;unfeasible&lt;/strong&gt; since we do not observe the propensity scores $e(X_i)$.&lt;/p&gt;
&lt;h2 id=&#34;the-aipw-estimator&#34;&gt;The AIPW Estimator&lt;/h2&gt;
&lt;p&gt;A feasible estimator of the average treatment effect $\tau$ is&lt;/p&gt;
&lt;p&gt;$$
\hat \tau_{IPW} = \frac{1}{n} \sum_{i=1}^{n} \left( \frac{T_i Y_i}{\hat e(X_i)} - \frac{(1-T_i) Y_i}{1-\hat e(X_i)} \right)
$$&lt;/p&gt;
&lt;p&gt;where we have replaced the propensity scores $e (X_i)$ with their estimates $\hat e (X_i)$.&lt;/p&gt;
&lt;p&gt;With a linear model, $Y_i = \alpha T_i + \beta X_i + \varepsilon_i$, we can get a central limit theorem for $\hat \tau$. However, we want to take a non-parametric / &lt;strong&gt;machine learning&lt;/strong&gt; approach with respect to the relationship between $Y$ and $X$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Advantages&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;flexible functional form for $\mathbb E[Y | X]$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Disadvantages&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;machine learning methods have slow convergence rates&lt;/li&gt;
&lt;li&gt;it impacts inference, i.e. we cannot easily get a central limit theorem result to build confidence intervals&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Is there a way to get around the slow rate of convergence of machine learning methods?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Yes!&lt;/strong&gt; Idea: combine two predicton problems instead of one.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;Augmented Inverse Propensity Weighted&lt;/strong&gt; estimator is given by&lt;/p&gt;
&lt;p&gt;$$
\hat \tau_{IPW} = \frac{1}{n} \sum_{i=1}^{n} \left( \hat \mu^{(1)}(X_i) - \hat \mu^{(0)}(X_i) + \frac{T_i }{\hat e(X_i)} \left( Y_i - \hat \mu^{(1)}(X_i) \right) - \frac{(1-T_i) }{1-\hat e(X_i)} \left( Y_i - \hat \mu^{(0)}(X_i) \right) \right)
$$&lt;/p&gt;
&lt;p&gt;where&lt;/p&gt;
&lt;p&gt;$$
\mu^{(t)}(x) = \mathbb E \left[ Y_i \ \big | \ X_i = x, T_i = t \right] \qquad ; \qquad e(x) = \mathbb E \left[ T_i = 1 \ \big | \ X_i = x \right]
$$&lt;/p&gt;
&lt;p&gt;The formula of the AIPW estimator might seem scary at first, so let&amp;rsquo;s &lt;strong&gt;decompose&lt;/strong&gt; it into two parts.&lt;/p&gt;
&lt;p&gt;First,&lt;/p&gt;
&lt;p&gt;$$
D = \frac{1}{n} \sum_{i=1}^{n} \left( \hat \mu^{(1)}(X_i) - \hat \mu^{(0)}(X_i) \right)
$$&lt;/p&gt;
&lt;p&gt;is basically the &lt;strong&gt;direct estimate&lt;/strong&gt; of the average treatment effect. This is a consistent estimator of the ATE but, since machine learning estimators&#39; rate of convergence is too slow, it does not provide correct confidence intervals.&lt;/p&gt;
&lt;p&gt;Instead,&lt;/p&gt;
&lt;p&gt;$$
R = \frac{1}{n} \sum_{i=1}^{n} \left(\frac{T_i }{\hat e(X_i)} \left( Y_i - \hat \mu^{(1)}(X_i) \right) - \frac{(1-T_i) }{1-\hat e(X_i)} \left( Y_i - \hat \mu^{(0)}(X_i) \right) \right)
$$&lt;/p&gt;
&lt;p&gt;is basically &lt;strong&gt;IPW applied to the residuals&lt;/strong&gt; $Y_i - \hat \mu^{(t)}(X_i)$ instead of $Y_i$.&lt;/p&gt;
&lt;h3 id=&#34;double-robustness&#34;&gt;Double Robustness&lt;/h3&gt;
&lt;p&gt;Why is the AIPW estimator so &lt;strong&gt;compelling&lt;/strong&gt;? It just needs one of the two predictions, $\hat \mu$ and $\hat e$, to be right in order to be unbiased. Let&amp;rsquo;s check it.&lt;/p&gt;
&lt;p&gt;If $\hat \mu$ is correctly specified, i.e. $\mathbb E \left[ \hat \mu^{(t)}(x) \right] = \mathbb E \left[ Y_i \ \big | \ X_i = x, T_i = t \right]$, then&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
\hat \tau_{IPW} &amp;amp;\overset{p}{\to} \mathbb E \left[ \hat \mu^{(1)}(X_i) - \hat \mu^{(0)}(X_i) + \frac{T_i \left( Y_i - \hat \mu^{(1)}(X_i) \right)}{\hat e(X_i)} - \frac{(1-T_i) \left( Y_i - \hat \mu^{(0)}(X_i) \right)}{1-\hat e(X_i)} \right] =
\newline
&amp;amp;= \mathbb E \left[ \hat \mu^{(1)}(X_i) - \hat \mu^{(0)}(X_i) \right] =
\newline
&amp;amp;= \mathbb E \left[ Y^{(1)} - Y^{(0)} \right] =
\newline
&amp;amp;= \tau
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;even if $\hat e$ is misspecified.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;intuition&lt;/strong&gt; is that the residuals $\left( Y_i - \hat \mu^{(t)}(X_i) \right)$ converge to zero and therefore IPW has no relevance.&lt;/p&gt;
&lt;p&gt;On the other hand, if $\hat e$ is correctly specified, i.e. $\mathbb E \left[\hat e(x) \right] = \mathbb E \left[ T_i = 1 \ \big | \ X_i = x \right]$, then&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
\hat \tau_{IPW} &amp;amp;\overset{p}{\to} \mathbb E \left[ \hat \mu^{(1)}(X_i) - \hat \mu^{(0)}(X_i) + \frac{T_i \left( Y_i - \hat \mu^{(1)}(X_i) \right)}{\hat e(X_i)} - \frac{(1-T_i) \left( Y_i - \hat \mu^{(0)}(X_i) \right)}{1-\hat e(X_i)} \right] =
\newline
&amp;amp;= \mathbb E \left[ \frac{T_i Y_i}{\hat e(X_i)} - \frac{(1-T_i) Y_i }{1-\hat e(X_i)} + \left(1 - \frac{T_i}{\hat e(X_i)} \right) \hat \mu^{(1)}(X_i) - \left(1 - \frac{1-T_i}{1-\hat e(X_i)} \right) \hat \mu^{(0)}(X_i)  \right] =
\newline
&amp;amp;= \mathbb E \left[ \frac{T_i Y_i}{\hat e(X_i)} - \frac{(1-T_i) Y_i }{1-\hat e(X_i)}\right] =
\newline
&amp;amp;= \mathbb E \left[ Y^{(1)} - Y^{(0)} \right] =
\newline
&amp;amp;= \tau
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;even if $\hat \mu$ is misspecified.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;intuition&lt;/strong&gt; is that, if we have a wrong model of $\mu(x)$ for $\mathbb E \left[ Y_i \ \big | \ X_i = x, T_i = t \right]$, it will not capture differences between treatment and control group or, even worse, it will capture wrong ones. Running IPW on the residuals we can not only recover the treatment effect but also compensate for eventual biases introduced by $\mu(x)$.&lt;/p&gt;
&lt;h2 id=&#34;best-practices&#34;&gt;Best Practices&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;1. Check Covariate Balance&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Both IPW and AIPW were built for settings in which the treatment $T$ is not uconditionally randomly assigned, but might depend on some observables $X$. This information can be checked in two ways:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Produce a balance table, summarizing the covariates across treatment arms. If undonditional randomization does not hold, we expect to see significant differences across some observables&lt;/li&gt;
&lt;li&gt;Plot the estimated propensity scores. If undonditional randomization holds, we expect the propensity scores to be constant&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;2. Check the Overlap Assumption&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Another assumption that we can check is the &lt;strong&gt;overlap&lt;/strong&gt; assumption, i.e. $\exists \eta \ : \ \eta \leq \mathbb E \left[ T_i = 1 \ \big | \ X_i = x \right] \leq 1-\eta$. To check this assumption we can simply check the bounds of the predicted propensity scores. If the overlap assumption is violated, we end up dividing some term of the estimator by zero.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;3. Use LOO Predictors&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;It is best practice, whenever we build a prediction it is best pratice to exclude observation $i$ when fitting the algorithm for predicting $\hat \mu^{(t)} (X_i)$ or $\hat e (X_i)$. These predictors are called &lt;strong&gt;Leave One Out&lt;/strong&gt; predictors.&lt;/p&gt;
&lt;h2 id=&#34;example&#34;&gt;Example&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;%matplotlib inline
%config InlineBackend.figure_format = &#39;retina&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from src.utils import *
from src.dgp import dgp_aipw
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In this example, we are going to use the following &lt;strong&gt;data generating process&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$N = 1000$&lt;/li&gt;
&lt;li&gt;$p = 20$&lt;/li&gt;
&lt;li&gt;$X_i \sim N(0, I_p)$&lt;/li&gt;
&lt;li&gt;$e(x) = 1 / (1 + e^{-x_1})$&lt;/li&gt;
&lt;li&gt;$\mu^{(0)}(x) = (x_1 + x_2)_+$&lt;/li&gt;
&lt;li&gt;$\mu^{(1)}(x) = (x_1 + x_3)_+ \mathbf{- 0.05}$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;So that the average treatment effect is $- 0.05$.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;dgp = dgp_aipw()
df = dgp.generate_data()
df.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;x1&lt;/th&gt;
      &lt;th&gt;x2&lt;/th&gt;
      &lt;th&gt;x3&lt;/th&gt;
      &lt;th&gt;x4&lt;/th&gt;
      &lt;th&gt;x5&lt;/th&gt;
      &lt;th&gt;x6&lt;/th&gt;
      &lt;th&gt;x7&lt;/th&gt;
      &lt;th&gt;x8&lt;/th&gt;
      &lt;th&gt;x9&lt;/th&gt;
      &lt;th&gt;x10&lt;/th&gt;
      &lt;th&gt;...&lt;/th&gt;
      &lt;th&gt;x16&lt;/th&gt;
      &lt;th&gt;x17&lt;/th&gt;
      &lt;th&gt;x18&lt;/th&gt;
      &lt;th&gt;x19&lt;/th&gt;
      &lt;th&gt;x20&lt;/th&gt;
      &lt;th&gt;e&lt;/th&gt;
      &lt;th&gt;T&lt;/th&gt;
      &lt;th&gt;Y0&lt;/th&gt;
      &lt;th&gt;Y1&lt;/th&gt;
      &lt;th&gt;Y&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;1.624345&lt;/td&gt;
      &lt;td&gt;-0.611756&lt;/td&gt;
      &lt;td&gt;-0.528172&lt;/td&gt;
      &lt;td&gt;-1.072969&lt;/td&gt;
      &lt;td&gt;0.865408&lt;/td&gt;
      &lt;td&gt;-2.301539&lt;/td&gt;
      &lt;td&gt;1.744812&lt;/td&gt;
      &lt;td&gt;-0.761207&lt;/td&gt;
      &lt;td&gt;0.319039&lt;/td&gt;
      &lt;td&gt;-0.249370&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;-1.099891&lt;/td&gt;
      &lt;td&gt;-0.172428&lt;/td&gt;
      &lt;td&gt;-0.877858&lt;/td&gt;
      &lt;td&gt;0.042214&lt;/td&gt;
      &lt;td&gt;0.582815&lt;/td&gt;
      &lt;td&gt;0.835394&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0.962589&lt;/td&gt;
      &lt;td&gt;0.996174&lt;/td&gt;
      &lt;td&gt;0.996174&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;-1.100619&lt;/td&gt;
      &lt;td&gt;1.144724&lt;/td&gt;
      &lt;td&gt;0.901591&lt;/td&gt;
      &lt;td&gt;0.502494&lt;/td&gt;
      &lt;td&gt;0.900856&lt;/td&gt;
      &lt;td&gt;-0.683728&lt;/td&gt;
      &lt;td&gt;-0.122890&lt;/td&gt;
      &lt;td&gt;-0.935769&lt;/td&gt;
      &lt;td&gt;-0.267888&lt;/td&gt;
      &lt;td&gt;0.530355&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;-0.012665&lt;/td&gt;
      &lt;td&gt;-1.117310&lt;/td&gt;
      &lt;td&gt;0.234416&lt;/td&gt;
      &lt;td&gt;1.659802&lt;/td&gt;
      &lt;td&gt;0.742044&lt;/td&gt;
      &lt;td&gt;0.249624&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0.044105&lt;/td&gt;
      &lt;td&gt;-0.050000&lt;/td&gt;
      &lt;td&gt;0.044105&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;-0.191836&lt;/td&gt;
      &lt;td&gt;-0.887629&lt;/td&gt;
      &lt;td&gt;-0.747158&lt;/td&gt;
      &lt;td&gt;1.692455&lt;/td&gt;
      &lt;td&gt;0.050808&lt;/td&gt;
      &lt;td&gt;-0.636996&lt;/td&gt;
      &lt;td&gt;0.190915&lt;/td&gt;
      &lt;td&gt;2.100255&lt;/td&gt;
      &lt;td&gt;0.120159&lt;/td&gt;
      &lt;td&gt;0.617203&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;0.586623&lt;/td&gt;
      &lt;td&gt;0.838983&lt;/td&gt;
      &lt;td&gt;0.931102&lt;/td&gt;
      &lt;td&gt;0.285587&lt;/td&gt;
      &lt;td&gt;0.885141&lt;/td&gt;
      &lt;td&gt;0.452188&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;-0.050000&lt;/td&gt;
      &lt;td&gt;-0.100000&lt;/td&gt;
      &lt;td&gt;-0.100000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;-0.754398&lt;/td&gt;
      &lt;td&gt;1.252868&lt;/td&gt;
      &lt;td&gt;0.512930&lt;/td&gt;
      &lt;td&gt;-0.298093&lt;/td&gt;
      &lt;td&gt;0.488518&lt;/td&gt;
      &lt;td&gt;-0.075572&lt;/td&gt;
      &lt;td&gt;1.131629&lt;/td&gt;
      &lt;td&gt;1.519817&lt;/td&gt;
      &lt;td&gt;2.185575&lt;/td&gt;
      &lt;td&gt;-1.396496&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;-2.022201&lt;/td&gt;
      &lt;td&gt;-0.306204&lt;/td&gt;
      &lt;td&gt;0.827975&lt;/td&gt;
      &lt;td&gt;0.230095&lt;/td&gt;
      &lt;td&gt;0.762011&lt;/td&gt;
      &lt;td&gt;0.319864&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0.448470&lt;/td&gt;
      &lt;td&gt;-0.100000&lt;/td&gt;
      &lt;td&gt;-0.100000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;-0.222328&lt;/td&gt;
      &lt;td&gt;-0.200758&lt;/td&gt;
      &lt;td&gt;0.186561&lt;/td&gt;
      &lt;td&gt;0.410052&lt;/td&gt;
      &lt;td&gt;0.198300&lt;/td&gt;
      &lt;td&gt;0.119009&lt;/td&gt;
      &lt;td&gt;-0.670662&lt;/td&gt;
      &lt;td&gt;0.377564&lt;/td&gt;
      &lt;td&gt;0.121821&lt;/td&gt;
      &lt;td&gt;1.129484&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;0.077340&lt;/td&gt;
      &lt;td&gt;-0.343854&lt;/td&gt;
      &lt;td&gt;0.043597&lt;/td&gt;
      &lt;td&gt;-0.620001&lt;/td&gt;
      &lt;td&gt;0.698032&lt;/td&gt;
      &lt;td&gt;0.444646&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;-0.050000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;5 rows Ã 25 columns&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;First, we check for covariate imbalance across treatment arms, with a &lt;strong&gt;balance table&lt;/strong&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df.groupby(&#39;T&#39;).agg([&#39;mean&#39;, &#39;std&#39;]).T.unstack(1).head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead tr th {
    text-align: left;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;T&lt;/th&gt;
      &lt;th colspan=&#34;2&#34; halign=&#34;left&#34;&gt;0&lt;/th&gt;
      &lt;th colspan=&#34;2&#34; halign=&#34;left&#34;&gt;1&lt;/th&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;mean&lt;/th&gt;
      &lt;th&gt;std&lt;/th&gt;
      &lt;th&gt;mean&lt;/th&gt;
      &lt;th&gt;std&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;x1&lt;/th&gt;
      &lt;td&gt;-0.462644&lt;/td&gt;
      &lt;td&gt;0.920626&lt;/td&gt;
      &lt;td&gt;0.413837&lt;/td&gt;
      &lt;td&gt;0.882116&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;x2&lt;/th&gt;
      &lt;td&gt;0.007900&lt;/td&gt;
      &lt;td&gt;1.010610&lt;/td&gt;
      &lt;td&gt;-0.054072&lt;/td&gt;
      &lt;td&gt;0.991680&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;x3&lt;/th&gt;
      &lt;td&gt;-0.005718&lt;/td&gt;
      &lt;td&gt;0.950997&lt;/td&gt;
      &lt;td&gt;-0.042624&lt;/td&gt;
      &lt;td&gt;1.059308&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;x4&lt;/th&gt;
      &lt;td&gt;0.122930&lt;/td&gt;
      &lt;td&gt;0.936591&lt;/td&gt;
      &lt;td&gt;0.095082&lt;/td&gt;
      &lt;td&gt;1.004048&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;x5&lt;/th&gt;
      &lt;td&gt;0.032637&lt;/td&gt;
      &lt;td&gt;0.955479&lt;/td&gt;
      &lt;td&gt;0.119788&lt;/td&gt;
      &lt;td&gt;1.039616&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;We can also get a first (wrong) estimate of the treatment effect as a difference in means.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df.loc[df[&#39;T&#39;]==1, &#39;Y&#39;].mean() - df.loc[df[&#39;T&#39;]==0, &#39;Y&#39;].mean()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;0.29838749886286686
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We know this estimate is wrong since the treatment is not unconditionally randomized. Therefore, we estimate the average treatment effect using the &lt;strong&gt;AIPW estimator&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;First, we estimate the &lt;strong&gt;propensity scores&lt;/strong&gt; $e(x)$ using &lt;code&gt;LogisticRegression&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def estimate_e(df, X, model_e):
    e = model_e.fit(df[dgp.X], df[&#39;T&#39;]).predict_proba(df[dgp.X])[:,1]
    return e
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from sklearn.linear_model import LogisticRegression as logit

e = estimate_e(df, dgp.X, logit())
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;A best practice is to use the &lt;code&gt;LeaveOneOut&lt;/code&gt; estimator to make predictions, i.e., for each observations, build a model using the remaining observations. This procedure helps preventing overfitting bias. The main issue is that it&amp;rsquo;s particularly slow since we need to fit one model per observation. However, it is parallelizable.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from sklearn.model_selection import cross_val_predict, LeaveOneOut

e = cross_val_predict(estimator=logit(), 
                      X=df[dgp.X], 
                      y=df[&#39;T&#39;],
                      cv=LeaveOneOut(),
                      method=&#39;predict_proba&#39;,
                      n_jobs=-1)[:,1]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s check if the &lt;strong&gt;bounded support&lt;/strong&gt; assumption is satisfied.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;print(f&#39;Support of e is [{min(e):.2}, {max(e):.2}]&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Support of e is [0.019, 0.97]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The support assumption is satisfied. Note that this is guaranteed with logistic regression.&lt;/p&gt;
&lt;p&gt;We can further plot the distribution of scores. They definitely do not seem constant.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sns.histplot(e, bins=100).set(title=&#39;Propensity Scores&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/aipw_29_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;We can now estimate the average treatment effect using the AIPW estimator. We also separately compute its components, $D$ and $R$.&lt;/p&gt;
&lt;p&gt;We use &lt;code&gt;RandomForestRegressor&lt;/code&gt; to estimate the conditional expectation of $Y$, $\hat \mu^{(t)}(x)$.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def estimate_mu(df, X, model_mu):
    mu = model_mu.fit(df[X + [&#39;T&#39;]], df[&#39;Y&#39;])
    mu0 = mu.predict(df[X + [&#39;T&#39;]].assign(T=0))
    mu1 = mu.predict(df[X + [&#39;T&#39;]].assign(T=1))
    return mu0, mu1
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from sklearn.ensemble import RandomForestRegressor as rfr

mu0, mu1 = estimate_mu(df, dgp.X, rfr(max_features=5))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We now have all the elements to estimate AIPW&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def AIPW(df, X, e, mu0, mu1):
    D = mu1 - mu0
    R = df[&#39;T&#39;] / e * (df[&#39;Y&#39;] - mu1) - (1-df[&#39;T&#39;]) / (1-e) * (df[&#39;Y&#39;] - mu0)
    tau_AIPW = D + R
    return D, R, tau_AIPW
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;D, R, hat_tau_AIPW = AIPW(df, dgp.X, e, mu0, mu1)
print(f&amp;quot;Mean: {np.mean(hat_tau_AIPW):.2} and var {np.var(hat_tau_AIPW):.2}&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Mean: -0.02 and var 0.48
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Our estimate is now closer to the true value, $-0.05$. We plot the distribution of $\hat \tau_i^{AIPW}$ below.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sns.histplot(hat_tau_AIPW, bins=20).set(title=&#39;Estimated $Ï_{AIPW}$&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/aipw_37_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;To visualize the impact of the AIPW correction, we simulate the distribution of the AIPW estimator and its components.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def simulate_AIPW(k):
    df = dgp_aipw().generate_data(seed=k)
    e = estimate_e(df, dgp.X, logit())
    mu0, mu1 = estimate_mu(df, dgp.X, rfr())
    aipw = AIPW(df, dgp.X, e, mu0, mu1)
    return np.mean(aipw, axis=1)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from joblib import Parallel, delayed

def distribution_AIPW(t):
    r = Parallel(n_jobs=8)(delayed(simulate_AIPW)(i) for i in range(100))
    sim_tau_AIPW = pd.DataFrame(r, columns=[&#39;Direct&#39;, &#39;Correction&#39;, &#39;$Ï_{AIPW}$&#39;])
    plot = sns.boxplot(data=pd.melt(sim_tau_AIPW), x=&#39;variable&#39;, y=&#39;value&#39;);
    plot.set(title=t, xlabel=&#39;&#39;, ylabel=&#39;&#39;)
    plot.axhline(-0.05, c=&#39;r&#39;, ls=&#39;:&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Below we plot the AIPW estimator and its components.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;distribution_AIPW(&amp;quot;Distribution of $\hat Ï_{AIPW}$ and its components&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/aipw_42_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Since the model is well specified, the correction is close to zero and the final estimates are very close to the direct estimates.&lt;/p&gt;
&lt;p&gt;We now turn into checking the &lt;strong&gt;double-robustness&lt;/strong&gt; of the estimator.&lt;/p&gt;
&lt;p&gt;What happens if we misspecify the propensity score? Let&amp;rsquo;s assume now that the propensity score is $\hat e(X_i) = 1 /(1 + e^{x_4})$.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def estimate_e(df, X, model_e):
    e = 1 / (1 + np.exp(df[&#39;x4&#39;]))
    return e
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;distribution_AIPW(&amp;quot;Distribution of $\hat Ï_{AIPW}$ with misspecified $\hat e$&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/aipw_45_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Not much changes. In fact, since we have a good direct model of $\mu(x)$, the residuals are small and the correction does not play a big role.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s now misspecify $\mu(x)$ and assume it is equal to $|x_5|$.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def estimate_e(df, X, model_e):
    e = model_e.fit(df[dgp.X], df[&#39;T&#39;]).predict_proba(df[dgp.X])[:,1]
    return e

def estimate_mu(df, X, model_mu):
    mu0 = 0
    mu1 = np.abs(df[&#39;x6&#39;])
    return mu0, mu1
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;distribution_AIPW(&amp;quot;Distribution of $\hat Ï_{AIPW}$ with misspecified $\hat Î¼$&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/aipw_48_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;In this case, the correction term is crucial and fully offsets the bias of the original estimator.&lt;/p&gt;
&lt;p&gt;The &lt;a href=&#34;https://econml.azurewebsites.net/index.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;EconML&lt;/code&gt;&lt;/a&gt; library offers a plug and play estimator, &lt;code&gt;LinearDRLearner&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from econml.drlearner import LinearDRLearner

model = LinearDRLearner(model_propensity=logit(), model_regression=rfr())
model.fit(Y=df[dgp.Y], X=df[dgp.X], T=df[dgp.T])
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;econml.drlearner.LinearDRLearner at 0x14bdb2610&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;model.ate_inference(X=df[dgp.X].values, T0=0, T1=1)
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;caption&gt;Uncertainty of Mean Point Estimate&lt;/caption&gt;
&lt;tr&gt;
  &lt;th&gt;mean_point&lt;/th&gt; &lt;th&gt;stderr_mean&lt;/th&gt;  &lt;th&gt;zstat&lt;/th&gt; &lt;th&gt;pvalue&lt;/th&gt; &lt;th&gt;ci_mean_lower&lt;/th&gt; &lt;th&gt;ci_mean_upper&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
    &lt;td&gt;-0.081&lt;/td&gt;      &lt;td&gt;0.036&lt;/td&gt;    &lt;td&gt;-2.236&lt;/td&gt;  &lt;td&gt;0.025&lt;/td&gt;    &lt;td&gt;-0.152&lt;/td&gt;         &lt;td&gt;-0.01&lt;/td&gt;    
&lt;/tr&gt;
&lt;/table&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;caption&gt;Distribution of Point Estimate&lt;/caption&gt;
&lt;tr&gt;
  &lt;th&gt;std_point&lt;/th&gt; &lt;th&gt;pct_point_lower&lt;/th&gt; &lt;th&gt;pct_point_upper&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
    &lt;td&gt;0.692&lt;/td&gt;       &lt;td&gt;-1.531&lt;/td&gt;           &lt;td&gt;1.205&lt;/td&gt;     
&lt;/tr&gt;
&lt;/table&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;caption&gt;Total Variance of Point Estimate&lt;/caption&gt;
&lt;tr&gt;
  &lt;th&gt;stderr_point&lt;/th&gt; &lt;th&gt;ci_point_lower&lt;/th&gt; &lt;th&gt;ci_point_upper&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
      &lt;td&gt;0.692&lt;/td&gt;        &lt;td&gt;-1.543&lt;/td&gt;          &lt;td&gt;1.278&lt;/td&gt;    
&lt;/tr&gt;
&lt;/table&gt;
&lt;h2 id=&#34;research-paper-replication&#34;&gt;Research Paper Replication&lt;/h2&gt;
&lt;p&gt;TBD&lt;/p&gt;
&lt;h2 id=&#34;business-case&#34;&gt;Business Case&lt;/h2&gt;
&lt;p&gt;TBD&lt;/p&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Original paper: &lt;a href=&#34;https://www.cambridge.org/core/journals/political-analysis/article/abs/an-introduction-to-the-augmented-inverse-propensity-weighted-estimator/4B1B8301E46F4432C4DCC91FE20780DB&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;An Introduction to the Augmented Inverse Propensity Weighted Estimator&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=IfZHUFFlsGc&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Video lecture&lt;/a&gt; by Prof. Stefan Wager (Stanford)&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Causal Trees</title>
      <link>https://matteocourthoud.github.io/post/causal_trees/</link>
      <pubDate>Fri, 08 Apr 2022 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/post/causal_trees/</guid>
      <description>&lt;p&gt;In this tutorial, we are going to see how to estimate heterogeneous treatment effects using regression trees.&lt;/p&gt;
&lt;p&gt;For this tutorial, I assume you are familiar with the following concepts:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Rubin&amp;rsquo;s potential outcome framework&lt;/li&gt;
&lt;li&gt;Decision tree methods&lt;/li&gt;
&lt;li&gt;Propensity score matching&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;setting&#34;&gt;Setting&lt;/h2&gt;
&lt;p&gt;We assume that for a set of i.i.d. subjects $i = 1, &amp;hellip;, n$ we observed a tuple $(X_i, D_i, Y_i)$ comprised of&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;a feature vector $X_i \in \mathbb R^n$&lt;/li&gt;
&lt;li&gt;a treatment assignment $T_i \in \lbrace 0, 1 \rbrace$&lt;/li&gt;
&lt;li&gt;a response $Y_i \in \mathbb R$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Our goal is to estimate the &lt;strong&gt;conditional average treatment effect&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;$$
\tau(x) = \mathbb E \Big [ Y^{(1)} - Y^{(0)} \ \Big| \ X = x \Big ]
$$&lt;/p&gt;
&lt;p&gt;Crucially, we only get to observe $Y_i = Y_i^{(T_i)}$.&lt;/p&gt;
&lt;p&gt;Wihtout further assumptions, we cannot estimate $\tau(x)$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Assumption: unconfoundedness&lt;/strong&gt; (or ignorability, or selection on observables)&lt;/p&gt;
&lt;p&gt;$$
\lbrace Y_i^{(1)} , Y^{(0)} \rbrace \perp W_i \ | \ X_i
$$&lt;/p&gt;
&lt;p&gt;i.e. conditional on observable characteristics the treatment assignment is as good as random.&lt;/p&gt;
&lt;p&gt;When unconfoundedness holds, matching methods usually provide consistent estimates of the conditional average treatment effect.&lt;/p&gt;
&lt;h2 id=&#34;prediction-problem&#34;&gt;Prediction Problem&lt;/h2&gt;
&lt;p&gt;How can we make the inference problem a &lt;strong&gt;prediction problem&lt;/strong&gt;?&lt;/p&gt;
&lt;p&gt;In principle, we would like to divide the population in subgroups in order to minimize the mean squared error (MSE) of treatment effects.&lt;/p&gt;
&lt;p&gt;The objective function is&lt;/p&gt;
&lt;p&gt;$$
\sum_i \Big [ ( \tau_i - \hat \tau_i(X))^2 \Big ]
$$&lt;/p&gt;
&lt;p&gt;However, this objective function is &lt;strong&gt;unfeasible&lt;/strong&gt; since we do not observe $\tau_i$.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;idea&lt;/strong&gt; is to transform our outcome variable as&lt;/p&gt;
&lt;p&gt;$$
Y_i^* = \frac{Y_i}{D_i * p(X_i) - (1-D_i) * (1-p(X_i))}
$$&lt;/p&gt;
&lt;p&gt;where $p_i$ is the &lt;strong&gt;propensity score&lt;/strong&gt; of observation $i$, i.e. its probability of being treated.&lt;/p&gt;
&lt;p&gt;It&amp;rsquo;s intuitive to verify that, given this specification, the expected value of $Y_i^*$ is the &lt;strong&gt;conditional average treatment effect&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Here is a proof:&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
\mathbb E \left[ Y_i^{*} \mid X_i = x \right] &amp;amp;= \mathbb E \left[ \frac{Y_i}{T_i * p(X_i) - (1-T_i) * (1-p(X_i))} \ \Big | \ X_i = x \right]
\newline
&amp;amp;= \mathbb E \left[ Y_i * \frac{T_i - p(X_i)}{p(X_i) (1-p(X_i))} \ \Big | \ X_i = x \right]
\newline
&amp;amp;= \mathbb E \left[ Y_i T_i * \frac{T_i - p(X_i)}{p(X_i) (1 - p(X_i))} + Y_i (1-T_i) * \frac{T_i - p(X_i)}{p(X_i) (1 - p(X_i))} \ \Big | \ X_i = x \right]
\newline
&amp;amp;= \mathbb E \Big[ Y^{(1)}_i * \frac{D_i (1 - p(X_i))}{p(X_i) (1 - p(X_i))} \ \Big | \ X_i = x \Big] - \mathbb E \left[Y^{(0)}_i * \frac{(1 - T_i) p(X_i)}{p(X_i) (1-p(X_i))} \ \Big | \ X_i = x \right]
\newline
&amp;amp;= \frac{1}{p(X_i)} \mathbb E \Big[ Y^{(1)}_i * T_i \ \Big | \ X_i = x \Big] - \frac{1}{1-p(X_i)} \mathbb E \left[ Y^{(0)}_i * (1 - T_i) \ \Big | \ X_i = x \right]
\newline
&amp;amp;= \frac{1}{p(X_i)} \mathbb E \left[ Y^{(1)}_i \ \Big | \ X_i = x \right] * \mathbb E \Big[ T_i \ \Big | \ X_i = x \Big] - \frac{1}{1 - p(X_i)} \mathbb E \left[ Y^{(0)}_i \ \Big | \ X_i = x \right] * \mathbb E \left[ (1 - T_i) \ \Big | \ X_i = x \right]
\newline
&amp;amp;= \mathbb E \Big[ Y^{(1)}_i \ \Big | \ X_i = x \Big] - \mathbb E \Big[Y^{(0)}_i \ \Big | \ X_i = x \Big]
\newline
&amp;amp;= \tau_i(x)
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;How can &lt;strong&gt;regression trees&lt;/strong&gt; help estimate heterogeneous treatment effects?&lt;/p&gt;
&lt;p&gt;If we fit a tree model on the modified outcome $Y^*$, we will get a partition of the data that minimizes the expected mean squared error of the conditional treatment effect. While the individual estimates are going to be inaccurate, within each leaf, we can estimate &lt;strong&gt;heterogeneous treatment effects&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;In order to get an &lt;strong&gt;unbiased estimate&lt;/strong&gt; however, we need to use different data to build the tree and to estimate the effect. This procedure comes at the cost of increased variance.&lt;/p&gt;
&lt;h2 id=&#34;example-1-simulated-data&#34;&gt;Example 1: Simulated Data&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s start with an example on synthetic data. We have the following individual characteristics:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;male&lt;/code&gt;: gender&lt;/li&gt;
&lt;li&gt;&lt;code&gt;black&lt;/code&gt;: race&lt;/li&gt;
&lt;li&gt;&lt;code&gt;age&lt;/code&gt;: age&lt;/li&gt;
&lt;li&gt;&lt;code&gt;educ&lt;/code&gt;: education, which depends on age and race&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Moreover, we maketreatment status &lt;code&gt;D&lt;/code&gt; depend on both gender and race so that it will be important to condition on observables.&lt;/p&gt;
&lt;p&gt;Our outcome variable &lt;code&gt;y&lt;/code&gt; depends on the treatment assignment differently according to education and age.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;%matplotlib inline
%config InlineBackend.figure_format = &#39;retina&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from src.utils import *
from src.dgp import dgp_ad
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We generate a dataset out of our DGP.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;dgp = dgp_ad()
df = dgp.generate_data()
df.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;male&lt;/th&gt;
      &lt;th&gt;black&lt;/th&gt;
      &lt;th&gt;age&lt;/th&gt;
      &lt;th&gt;educ&lt;/th&gt;
      &lt;th&gt;ad_exposure&lt;/th&gt;
      &lt;th&gt;revenue&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;55.0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;-0.327221&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;47.0&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;0.659393&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;31.0&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;True&lt;/td&gt;
      &lt;td&gt;2.805178&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;51.0&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;-0.508548&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;48.0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;True&lt;/td&gt;
      &lt;td&gt;0.762280&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;We can check the distribution of variables across treatment assignment.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df.groupby(dgp.T).agg([&#39;mean&#39;, &#39;std&#39;]).T.unstack(1)
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead tr th {
    text-align: left;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;ad_exposure&lt;/th&gt;
      &lt;th colspan=&#34;2&#34; halign=&#34;left&#34;&gt;False&lt;/th&gt;
      &lt;th colspan=&#34;2&#34; halign=&#34;left&#34;&gt;True&lt;/th&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;mean&lt;/th&gt;
      &lt;th&gt;std&lt;/th&gt;
      &lt;th&gt;mean&lt;/th&gt;
      &lt;th&gt;std&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;male&lt;/th&gt;
      &lt;td&gt;0.42000&lt;/td&gt;
      &lt;td&gt;0.494053&lt;/td&gt;
      &lt;td&gt;0.592000&lt;/td&gt;
      &lt;td&gt;0.491955&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;black&lt;/th&gt;
      &lt;td&gt;0.58800&lt;/td&gt;
      &lt;td&gt;0.492688&lt;/td&gt;
      &lt;td&gt;0.470000&lt;/td&gt;
      &lt;td&gt;0.499599&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;age&lt;/th&gt;
      &lt;td&gt;44.57600&lt;/td&gt;
      &lt;td&gt;10.350386&lt;/td&gt;
      &lt;td&gt;44.970000&lt;/td&gt;
      &lt;td&gt;9.961102&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;educ&lt;/th&gt;
      &lt;td&gt;2.19200&lt;/td&gt;
      &lt;td&gt;1.824773&lt;/td&gt;
      &lt;td&gt;1.968000&lt;/td&gt;
      &lt;td&gt;1.724216&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;revenue&lt;/th&gt;
      &lt;td&gt;-0.11697&lt;/td&gt;
      &lt;td&gt;1.057941&lt;/td&gt;
      &lt;td&gt;1.156387&lt;/td&gt;
      &lt;td&gt;2.147342&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;As we can see, &lt;code&gt;male&lt;/code&gt; and &lt;code&gt;black&lt;/code&gt; are now balanced across groups.&lt;/p&gt;
&lt;p&gt;We can get a first estimate of the &lt;strong&gt;average treatment effect&lt;/strong&gt; as a simple comparison of means.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df.loc[df[dgp.T]==1, dgp.Y].mean() - df.loc[df[dgp.T]==0, dgp.Y].mean()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;1.2733576182695898
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can visualize the difference with a barplot.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sns.barplot(x=dgp.T, y=dgp.Y, data=df);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/causal_trees_18_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;It seems there is a significant difference between the two groups. We can get a standard error around the estimate by regressing $Y$ on $D$.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;est = smf.ols(f&#39;{dgp.Y} ~ {dgp.T}&#39;, df).fit()
est.summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
           &lt;td&gt;&lt;/td&gt;              &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt;           &lt;td&gt;   -0.1170&lt;/td&gt; &lt;td&gt;    0.076&lt;/td&gt; &lt;td&gt;   -1.545&lt;/td&gt; &lt;td&gt; 0.123&lt;/td&gt; &lt;td&gt;   -0.266&lt;/td&gt; &lt;td&gt;    0.032&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;ad_exposure[T.True]&lt;/th&gt; &lt;td&gt;    1.2734&lt;/td&gt; &lt;td&gt;    0.107&lt;/td&gt; &lt;td&gt;   11.894&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    1.063&lt;/td&gt; &lt;td&gt;    1.483&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;The coefficient is statistically significant. However, we know that the treatment assignment is not &lt;strong&gt;unconditionally exogenous&lt;/strong&gt;. We need to condition on observables $X$.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;est = smf.ols(f&#39;{dgp.Y} ~ {dgp.T} +&#39; + &#39; + &#39;.join(dgp.X), df).fit()
est.summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
           &lt;td&gt;&lt;/td&gt;              &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt;           &lt;td&gt;   -0.1026&lt;/td&gt; &lt;td&gt;    0.247&lt;/td&gt; &lt;td&gt;   -0.416&lt;/td&gt; &lt;td&gt; 0.678&lt;/td&gt; &lt;td&gt;   -0.587&lt;/td&gt; &lt;td&gt;    0.382&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;ad_exposure[T.True]&lt;/th&gt; &lt;td&gt;    1.0342&lt;/td&gt; &lt;td&gt;    0.103&lt;/td&gt; &lt;td&gt;   10.044&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    0.832&lt;/td&gt; &lt;td&gt;    1.236&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;male&lt;/th&gt;                &lt;td&gt;    0.9987&lt;/td&gt; &lt;td&gt;    0.102&lt;/td&gt; &lt;td&gt;    9.765&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    0.798&lt;/td&gt; &lt;td&gt;    1.199&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;black&lt;/th&gt;               &lt;td&gt;   -0.8234&lt;/td&gt; &lt;td&gt;    0.126&lt;/td&gt; &lt;td&gt;   -6.555&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;   -1.070&lt;/td&gt; &lt;td&gt;   -0.577&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;age&lt;/th&gt;                 &lt;td&gt;   -0.0050&lt;/td&gt; &lt;td&gt;    0.005&lt;/td&gt; &lt;td&gt;   -1.001&lt;/td&gt; &lt;td&gt; 0.317&lt;/td&gt; &lt;td&gt;   -0.015&lt;/td&gt; &lt;td&gt;    0.005&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;educ&lt;/th&gt;                &lt;td&gt;    0.1240&lt;/td&gt; &lt;td&gt;    0.035&lt;/td&gt; &lt;td&gt;    3.533&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    0.055&lt;/td&gt; &lt;td&gt;    0.193&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;We do not need to actually condition on the full vector of observables $X$. It is sufficient to condition on the &lt;strong&gt;propensity score&lt;/strong&gt; $p(X)$, i.e. the conditional probability of treatment.&lt;/p&gt;
&lt;p&gt;We can estimate the propensity score with any method we want. The more flexible, the better.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df[&#39;pscore&#39;] = RandomForestRegressor().fit(df[dgp.X], df[dgp.T]).predict(df[dgp.X])
df[&#39;pscore&#39;].head()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;0    0.368726
1    0.036667
2    0.675000
3    0.701702
4    0.307456
Name: pscore, dtype: float64
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We now estiamte the conditional average treatment effect regressing $Y$ on $D$ and on the p-score.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;est = smf.ols(f&#39;{dgp.Y} ~ {dgp.T} + pscore&#39;, df).fit()
est.summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
           &lt;td&gt;&lt;/td&gt;              &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt;           &lt;td&gt;   -0.3402&lt;/td&gt; &lt;td&gt;    0.103&lt;/td&gt; &lt;td&gt;   -3.315&lt;/td&gt; &lt;td&gt; 0.001&lt;/td&gt; &lt;td&gt;   -0.542&lt;/td&gt; &lt;td&gt;   -0.139&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;ad_exposure[T.True]&lt;/th&gt; &lt;td&gt;    0.8957&lt;/td&gt; &lt;td&gt;    0.159&lt;/td&gt; &lt;td&gt;    5.638&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    0.584&lt;/td&gt; &lt;td&gt;    1.208&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;pscore&lt;/th&gt;              &lt;td&gt;    0.8235&lt;/td&gt; &lt;td&gt;    0.257&lt;/td&gt; &lt;td&gt;    3.204&lt;/td&gt; &lt;td&gt; 0.001&lt;/td&gt; &lt;td&gt;    0.319&lt;/td&gt; &lt;td&gt;    1.328&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;We are now ready to estimate &lt;strong&gt;heterogeneous treatment effects&lt;/strong&gt;. First, we need to compute the transformed outcome&lt;/p&gt;
&lt;p&gt;$$
Y_i^* = \frac{Y_i}{T_i * e_i - (1-T_i) * (1-e_i)}
$$&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df[&#39;y_star&#39;] = df[dgp.Y] /(df[dgp.T] * df[&#39;pscore&#39;] - (1-df[dgp.T]) * (1-df[&#39;pscore&#39;]))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we train a small tree on the transformed outcome $Y^*$.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;tree = DecisionTreeRegressor(max_depth=2, min_samples_leaf=30).fit(df[dgp.X], df[&#39;y_star&#39;])
df[&#39;y_hat&#39;] = tree.predict(df[dgp.X])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can plot the tree and visualize the estimated groups and treatment effects.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Plot tree
plot_tree(tree, filled=True, fontsize=12, feature_names=dgp.X, impurity=False);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/causal_trees_32_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;We still have one issue: we have trained our tree model and estimated the treatment effects using the same data. This introduces bias in the estimates.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from econml.causal_forest import CausalForest

model = CausalForest(max_depth=2).fit(Y=df[dgp.Y], X=df[dgp.X], T=df[dgp.T])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can plot a tree&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from econml.cate_interpreter import SingleTreeCateInterpreter

intrp = SingleTreeCateInterpreter(max_depth=2).interpret(model, df[dgp.X])
intrp.plot(feature_names=dgp.X, fontsize=10)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/causal_trees_36_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;example-2-lalonde-data&#34;&gt;Example 2: Lalonde Data&lt;/h2&gt;
&lt;p&gt;For this tutorial, we are goind to use the data from Lalonde (1981). You can find the data here: &lt;a href=&#34;https://users.nber.org/~rdehejia/nswdata.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://users.nber.org/~rdehejia/nswdata.html&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df = pd.read_csv(&#39;data/lalonde86.csv&#39;)
df.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;treat&lt;/th&gt;
      &lt;th&gt;age&lt;/th&gt;
      &lt;th&gt;education&lt;/th&gt;
      &lt;th&gt;black&lt;/th&gt;
      &lt;th&gt;hispanic&lt;/th&gt;
      &lt;th&gt;married&lt;/th&gt;
      &lt;th&gt;nodegree&lt;/th&gt;
      &lt;th&gt;re74&lt;/th&gt;
      &lt;th&gt;re75&lt;/th&gt;
      &lt;th&gt;re78&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;37.0&lt;/td&gt;
      &lt;td&gt;11.0&lt;/td&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;9930.0460&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;22.0&lt;/td&gt;
      &lt;td&gt;9.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;3595.8940&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;30.0&lt;/td&gt;
      &lt;td&gt;12.0&lt;/td&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;24909.4500&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;27.0&lt;/td&gt;
      &lt;td&gt;11.0&lt;/td&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;7506.1460&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;33.0&lt;/td&gt;
      &lt;td&gt;8.0&lt;/td&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;289.7899&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;We can summarize each variable by its treatment status.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df.groupby(&#39;treat&#39;).agg([&#39;mean&#39;, &#39;std&#39;]).T.unstack(1)
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead tr th {
    text-align: left;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;treat&lt;/th&gt;
      &lt;th colspan=&#34;2&#34; halign=&#34;left&#34;&gt;0.0&lt;/th&gt;
      &lt;th colspan=&#34;2&#34; halign=&#34;left&#34;&gt;1.0&lt;/th&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;mean&lt;/th&gt;
      &lt;th&gt;std&lt;/th&gt;
      &lt;th&gt;mean&lt;/th&gt;
      &lt;th&gt;std&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;age&lt;/th&gt;
      &lt;td&gt;25.053846&lt;/td&gt;
      &lt;td&gt;7.057745&lt;/td&gt;
      &lt;td&gt;25.816216&lt;/td&gt;
      &lt;td&gt;7.155019&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;education&lt;/th&gt;
      &lt;td&gt;10.088462&lt;/td&gt;
      &lt;td&gt;1.614325&lt;/td&gt;
      &lt;td&gt;10.345946&lt;/td&gt;
      &lt;td&gt;2.010650&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;black&lt;/th&gt;
      &lt;td&gt;0.826923&lt;/td&gt;
      &lt;td&gt;0.379043&lt;/td&gt;
      &lt;td&gt;0.843243&lt;/td&gt;
      &lt;td&gt;0.364558&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;hispanic&lt;/th&gt;
      &lt;td&gt;0.107692&lt;/td&gt;
      &lt;td&gt;0.310589&lt;/td&gt;
      &lt;td&gt;0.059459&lt;/td&gt;
      &lt;td&gt;0.237124&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;married&lt;/th&gt;
      &lt;td&gt;0.153846&lt;/td&gt;
      &lt;td&gt;0.361497&lt;/td&gt;
      &lt;td&gt;0.189189&lt;/td&gt;
      &lt;td&gt;0.392722&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;nodegree&lt;/th&gt;
      &lt;td&gt;0.834615&lt;/td&gt;
      &lt;td&gt;0.372244&lt;/td&gt;
      &lt;td&gt;0.708108&lt;/td&gt;
      &lt;td&gt;0.455867&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;re74&lt;/th&gt;
      &lt;td&gt;2107.026658&lt;/td&gt;
      &lt;td&gt;5687.905694&lt;/td&gt;
      &lt;td&gt;2095.573689&lt;/td&gt;
      &lt;td&gt;4886.620353&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;re75&lt;/th&gt;
      &lt;td&gt;1266.909002&lt;/td&gt;
      &lt;td&gt;3102.982044&lt;/td&gt;
      &lt;td&gt;1532.055314&lt;/td&gt;
      &lt;td&gt;3219.250870&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;re78&lt;/th&gt;
      &lt;td&gt;4554.801126&lt;/td&gt;
      &lt;td&gt;5483.835991&lt;/td&gt;
      &lt;td&gt;6349.143530&lt;/td&gt;
      &lt;td&gt;7867.402218&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;y = &#39;re78&#39;
D = &#39;treat&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;est = smf.ols(&#39;re78 ~ treat&#39;, df).fit()
est.summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
      &lt;td&gt;&lt;/td&gt;         &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt; &lt;td&gt; 4554.8011&lt;/td&gt; &lt;td&gt;  408.046&lt;/td&gt; &lt;td&gt;   11.162&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt; 3752.855&lt;/td&gt; &lt;td&gt; 5356.747&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;treat&lt;/th&gt;     &lt;td&gt; 1794.3424&lt;/td&gt; &lt;td&gt;  632.853&lt;/td&gt; &lt;td&gt;    2.835&lt;/td&gt; &lt;td&gt; 0.005&lt;/td&gt; &lt;td&gt;  550.574&lt;/td&gt; &lt;td&gt; 3038.110&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;X = [&#39;age&#39;, &#39;education&#39;, &#39;black&#39;, &#39;hispanic&#39;, &#39;married&#39;, &#39;nodegree&#39;]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;est = smf.ols(&#39;re78 ~ treat +&#39; + &#39; + &#39;.join(X), df).fit()
est.summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
      &lt;td&gt;&lt;/td&gt;         &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt; &lt;td&gt; 1168.0035&lt;/td&gt; &lt;td&gt; 3360.588&lt;/td&gt; &lt;td&gt;    0.348&lt;/td&gt; &lt;td&gt; 0.728&lt;/td&gt; &lt;td&gt;-5436.921&lt;/td&gt; &lt;td&gt; 7772.928&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;treat&lt;/th&gt;     &lt;td&gt; 1671.1304&lt;/td&gt; &lt;td&gt;  637.973&lt;/td&gt; &lt;td&gt;    2.619&lt;/td&gt; &lt;td&gt; 0.009&lt;/td&gt; &lt;td&gt;  417.254&lt;/td&gt; &lt;td&gt; 2925.007&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;age&lt;/th&gt;       &lt;td&gt;   52.8219&lt;/td&gt; &lt;td&gt;   45.255&lt;/td&gt; &lt;td&gt;    1.167&lt;/td&gt; &lt;td&gt; 0.244&lt;/td&gt; &lt;td&gt;  -36.123&lt;/td&gt; &lt;td&gt;  141.767&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;education&lt;/th&gt; &lt;td&gt;  393.8213&lt;/td&gt; &lt;td&gt;  227.114&lt;/td&gt; &lt;td&gt;    1.734&lt;/td&gt; &lt;td&gt; 0.084&lt;/td&gt; &lt;td&gt;  -52.549&lt;/td&gt; &lt;td&gt;  840.192&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;black&lt;/th&gt;     &lt;td&gt;-2220.2622&lt;/td&gt; &lt;td&gt; 1168.317&lt;/td&gt; &lt;td&gt;   -1.900&lt;/td&gt; &lt;td&gt; 0.058&lt;/td&gt; &lt;td&gt;-4516.480&lt;/td&gt; &lt;td&gt;   75.956&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;hispanic&lt;/th&gt;  &lt;td&gt;   83.7193&lt;/td&gt; &lt;td&gt; 1550.348&lt;/td&gt; &lt;td&gt;    0.054&lt;/td&gt; &lt;td&gt; 0.957&lt;/td&gt; &lt;td&gt;-2963.346&lt;/td&gt; &lt;td&gt; 3130.785&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;married&lt;/th&gt;   &lt;td&gt;  158.2084&lt;/td&gt; &lt;td&gt;  850.326&lt;/td&gt; &lt;td&gt;    0.186&lt;/td&gt; &lt;td&gt; 0.852&lt;/td&gt; &lt;td&gt;-1513.029&lt;/td&gt; &lt;td&gt; 1829.446&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;nodegree&lt;/th&gt;  &lt;td&gt; -128.2203&lt;/td&gt; &lt;td&gt;  995.416&lt;/td&gt; &lt;td&gt;   -0.129&lt;/td&gt; &lt;td&gt; 0.898&lt;/td&gt; &lt;td&gt;-2084.617&lt;/td&gt; &lt;td&gt; 1828.177&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df[&#39;pscore&#39;] = RandomForestRegressor().fit(df[X], df[D]).predict(df[X])
df[&#39;pscore&#39;].head()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;0    0.810000
1    0.752000
2    0.485786
3    0.670023
4    0.731667
Name: pscore, dtype: float64
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;est = smf.ols(&#39;re78 ~ treat + pscore&#39;, df).fit()
est.summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
      &lt;td&gt;&lt;/td&gt;         &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt; &lt;td&gt; 4313.9001&lt;/td&gt; &lt;td&gt;  591.433&lt;/td&gt; &lt;td&gt;    7.294&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt; 3151.530&lt;/td&gt; &lt;td&gt; 5476.270&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;treat&lt;/th&gt;     &lt;td&gt; 1461.6324&lt;/td&gt; &lt;td&gt;  866.171&lt;/td&gt; &lt;td&gt;    1.687&lt;/td&gt; &lt;td&gt; 0.092&lt;/td&gt; &lt;td&gt; -240.692&lt;/td&gt; &lt;td&gt; 3163.957&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;pscore&lt;/th&gt;    &lt;td&gt;  894.6072&lt;/td&gt; &lt;td&gt; 1588.766&lt;/td&gt; &lt;td&gt;    0.563&lt;/td&gt; &lt;td&gt; 0.574&lt;/td&gt; &lt;td&gt;-2227.867&lt;/td&gt; &lt;td&gt; 4017.082&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;Estimate CATE&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from econml.causal_forest import CausalForest

model = CausalForest(max_depth=2).fit(Y=df[y], X=df[X], T=df[D])
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from econml.cate_interpreter import SingleTreeCateInterpreter

intrp = SingleTreeCateInterpreter(max_depth=2).interpret(model, df[X])
intrp.plot(feature_names=X, fontsize=10)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/causal_trees_50_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Original paper: &lt;a href=&#34;https://www.pnas.org/doi/abs/10.1073/pnas.1510489113&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Recursive partitioning for heterogeneous causal effects&lt;/a&gt; (2016) by Athey and Imbens&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=JtDRpM6Mnnw&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Video lecture&lt;/a&gt; by Prof. Susan Athey (Stanford)&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Chi-Squared Test for Dummies</title>
      <link>https://matteocourthoud.github.io/post/chisquared/</link>
      <pubDate>Fri, 08 Apr 2022 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/post/chisquared/</guid>
      <description>&lt;p&gt;If you search the Wikipedia definition of Chi-Squared test, you get the following definition:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Pearson&amp;rsquo;s chi-squared test $\chi^2$ is a statistical test applied to sets of categorical data to evaluate how likely it is that any observed difference between the sets arose by chance.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;What does it mean? Let&amp;rsquo;s see it together.&lt;/p&gt;
&lt;h2 id=&#34;example-1-is-a-dice-fair&#34;&gt;Example 1: is a dice fair?&lt;/h2&gt;
&lt;p&gt;Suppose you want to test whether a dice is fair.&lt;/p&gt;
&lt;p&gt;You throw the dice 60 times and you count the number of times you get each outcome.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s simulate some data (from a fair dice).&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import numpy as np
import pandas as pd

# Data generating process
def generate_data_dice(N=60, seed=1):
    np.random.seed(1) # Set seed for replicability
    dice_numbers = [1,2,3,4,5,6]  # Dice numbers
    dice_throws = np.random.choice(dice_numbers, size=N)  # Actual dice throws
    data = pd.DataFrame({&amp;quot;dice number&amp;quot;: dice_numbers,
                         &amp;quot;observed&amp;quot;: [sum(dice_throws==n) for n in dice_numbers],
                         &amp;quot;expected&amp;quot;: N / 6})
    return data
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Generate data
data_dice = generate_data_dice()
data_dice
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;dice number&lt;/th&gt;
      &lt;th&gt;observed&lt;/th&gt;
      &lt;th&gt;expected&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;10&lt;/td&gt;
      &lt;td&gt;10.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;14&lt;/td&gt;
      &lt;td&gt;10.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;6&lt;/td&gt;
      &lt;td&gt;10.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;8&lt;/td&gt;
      &lt;td&gt;10.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;12&lt;/td&gt;
      &lt;td&gt;10.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;5&lt;/th&gt;
      &lt;td&gt;6&lt;/td&gt;
      &lt;td&gt;10&lt;/td&gt;
      &lt;td&gt;10.0&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;If we were throwing the dice &lt;strong&gt;a lot&lt;/strong&gt; of times, we would expect the same number of observations for each outcome. However, there is inherent noise in the process. How can we tell whether the fact that we didn&amp;rsquo;t get exactly 10 observations for each outcome is just due to randomness or it&amp;rsquo;s because the dice is unfair?&lt;/p&gt;
&lt;p&gt;The idea is to compute some statistic whose distribution is known under the assumption that the dice is fair, and then check if its value is &amp;ldquo;unusual&amp;rdquo; or not. If the value is particularly &amp;ldquo;unusual&amp;rdquo;, we reject the null hypothesis that the dice is fair.&lt;/p&gt;
&lt;p&gt;In our case, the statistic we choose is the chi-squared $\chi^{2}$ test-statistic.&lt;/p&gt;
&lt;p&gt;The value of the &lt;a href=&#34;https://en.wikipedia.org/wiki/Pearson%27s_chi-squared_test&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Pearson&amp;rsquo;s chi-squared test-statistic&lt;/a&gt; is&lt;/p&gt;
&lt;p&gt;$$
T_{\chi^2} = \sum _{i=1}^{n} \frac{(O_i - E_i)^{2}}{E_i} = N \sum _{i=1}^{n} \frac{\left(O_i/N - p_i \right)^2 }{p_i}
$$&lt;/p&gt;
&lt;p&gt;where&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$O_{i}$ = the number of observations of type i.&lt;/li&gt;
&lt;li&gt;$N$ = total number of observations&lt;/li&gt;
&lt;li&gt;$E_{i}=Np_{i}$ = the expected (theoretical) count of type $i$, asserted by the null hypothesis that the fraction of type $i$ in the population is $p_{i}$&lt;/li&gt;
&lt;li&gt;$n$ = the number of cells in the table.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Compute value of the statistic
def compute_chi2_stat(data):
    return sum( (data.observed - data.expected)**2 / data.expected )

chi2_stat = compute_chi2_stat(data_dice)
chi2_stat
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;4.0
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What do we make of this number? Is it &amp;ldquo;unusual&amp;rdquo;?&lt;/p&gt;
&lt;p&gt;If the dice were fair, the test Pearson&amp;rsquo;s chi-squared test-statistic $T_{\chi^2}$ is distributed as a chi-squared distribution with $k-1$ degrees of freedom, $\chi^2_{k-1}$. For the moment, take this claim at face value, we will verify it later, both empirically and theoretically. We will also discuss the degrees of freedom in detail later on.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Important!&lt;/strong&gt; Do not confuse the chi-squared test statistic (a number) with the chi-squared distribution (a distribution).&lt;/p&gt;
&lt;p&gt;What does a chi-squared distribution with $n-1$ degrees of freedom, $\chi^2_{k-1}$, look like?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;%matplotlib inline
%config InlineBackend.figure_format = &#39;retina&#39;
from src.utils import *
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from scipy.stats import chi2

# x-axis ranges from 0 to 20 with .001 steps
x = np.arange(0, 30, 0.001)

# Chi-square distribution with 5 degrees of freedom
chi2_5_pdf = chi2.pdf(x, df=5)

# Plot 
plt.plot(x, chi2_5_pdf);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/chisquared_11_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;How does the value of the statistic we have observed compares with its the distribution under the null hypothesis of a fair dice?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# plot Chi-square distribution with 5 degrees of freedom
plt.plot(x, chi2.pdf(x, df=5));
plt.vlines(chi2_stat, ymin=0, ymax=plt.ylim()[1], color=&#39;k&#39;, label=&#39;chi2 statistic&#39;)
plt.legend();
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/chisquared_13_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;The test statistic seems to fall well within the distribution, i.e. it does not seem to be an unusual event. Indeed, the question we want to answer is: &amp;ldquo;&lt;em&gt;under the null hypothesis that the dice is fair, how unlikely is the statistic we have observed?&lt;/em&gt;&amp;rdquo;.&lt;/p&gt;
&lt;p&gt;The last component we need in order to build a hypothesis test is a level of confidence, i.e. a threshold of &amp;ldquo;unlikeliness&amp;rdquo; of an event, below which we declare that the event is too unlikely under the model, for the model to be true. Let&amp;rsquo;s say we decide to set that threshold at 5%.&lt;/p&gt;
&lt;p&gt;If the likelihood of observing an even that (or more) extreme than the one we have actually observed is less than 5%, we reject the null hypothesis that the dice is fair.&lt;/p&gt;
&lt;p&gt;What is this value for a chi-squared distribution with 5 degrees of freedom?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Compute the Percent Point Function of the chi-squared distribution
z95 = chi2.ppf(0.95, df=5)
z95
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;11.070497693516351
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Since our value is smaller, we do not reject the null.&lt;/p&gt;
&lt;p&gt;We can plot the rejection and non-rejection areas in a plot.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Visualize test
def plot_test(x, stat, df):
    z95 = chi2.ppf(0.95, df=df)
    chi2_pdf = chi2.pdf(x, df=df)
    plt.plot(x, chi2_pdf);
    plt.fill_between(x[x&amp;gt;z95], chi2_pdf[x&amp;gt;z95], color=&#39;r&#39;, alpha=0.4, label=&#39;rejection area&#39;)
    plt.fill_between(x[x&amp;lt;z95], chi2_pdf[x&amp;lt;z95], color=&#39;g&#39;, alpha=0.4, label=&#39;non-rejection area&#39;)
    plt.vlines(chi2_stat, ymin=0, ymax=plt.ylim()[1], color=&#39;k&#39;, label=&#39;chi2 statistic&#39;)
    plt.legend();
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Plot
plot_test(x, chi2_stat, 5)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/chisquared_18_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;From the plot, we can clearly see that we do not reject the null hypothesis that the dice is fair.&lt;/p&gt;
&lt;h2 id=&#34;why-the-chi-squared-distribution&#34;&gt;Why the Chi-squared Distribution?&lt;/h2&gt;
&lt;p&gt;How do we know that that particular statistic has that particular distribution?&lt;/p&gt;
&lt;p&gt;Before digging into the math, we can check this claim via &lt;strong&gt;simulation&lt;/strong&gt;. We will repeat the procedure above many times, i.e.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;roll a (fair) dice 60 times&lt;/li&gt;
&lt;li&gt;compute the chi-square statistic&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;and then plot the distribution of chi square statistics.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Function to simulate data and compute chi2 stats
def simulate_chi2stats(K, N, dgp):
    chi2_stats = []
    for i in range(K):
        data = dgp()
        chi2_stats += [compute_chi2_stat(data)]
    return np.array(chi2_stats)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;chi2_stats = simulate_chi2stats(K=100, N=60, dgp=generate_data_dice)

# Plot data
plt.hist(chi2_stats, density=True, bins=30, alpha=0.3, color=&#39;C0&#39;);
plt.plot(x, chi2_5_pdf);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/chisquared_23_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Since we only did it 100 times, the distribution looks pretty coarse but vaguely close. Let&amp;rsquo;s now try 1000 times.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;chi2_stats = simulate_chi2stats(K=1000, N=60, dgp=generate_data_dice)

# Plot data
plt.hist(chi2_stats, density=True, bins=30, alpha=0.3, color=&#39;C0&#39;);
plt.plot(x, chi2_5_pdf);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/chisquared_25_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;The empirical distribution of the test statistic is indeed very close to its theoretical counterpart.&lt;/p&gt;
&lt;h2 id=&#34;some-statistics&#34;&gt;Some Statistics&lt;/h2&gt;
&lt;p&gt;Why does the distribution of the test statistic look like that? Let&amp;rsquo;s now dig deeper into the math.&lt;/p&gt;
&lt;p&gt;There are two things we need to know in order to understand the answer:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the Central Limit Theorem&lt;/li&gt;
&lt;li&gt;the relationship between a chi-squared and a normal distribution&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The &lt;a href=&#34;https://en.wikipedia.org/wiki/Central_limit_theorem&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Central Limit Theorem&lt;/a&gt; says that&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;In probability theory, the central limit theorem (CLT) establishes that, in many situations, when independent random variables are summed up, their properly normalized sum tends toward a normal distribution (informally a bell curve) even if the original variables themselves are not normally distributed.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Where does a normal distribution come up in our case? If we look at a single row in our data, i.e. the occurrences of a specific dice throw, it can be interpreted as the sum of realization from a &lt;a href=&#34;https://en.wikipedia.org/wiki/Bernoulli_distribution&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Bernoulli distribution&lt;/a&gt; with probability 1/6.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;In probability theory and statistics, the Bernoulli distribution is the discrete probability distribution of a random variable which takes the value $1$ with probability $p$ and the value $0$ with probability $q=1-p$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;In our case, the probability of getting a particular number is exactly 1/6. What is the distribution of the sum of its realizations? The Central Limit Theorem also tells us that:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;If $X_1, X_2, \dots , X_n, \dots$ are random samples drawn from a population with overall mean $\mu$ and finite variance $\sigma^2$, and if $\bar X_n$ is the sample mean of the first $n$ samples, then the limiting form of the distribution,&lt;/p&gt;
&lt;p&gt;$$
Z = \lim_{n \to \infty} \sqrt{n} \left( \frac{\bar X_n - \mu }{\sigma} \right)
$$&lt;/p&gt;
&lt;p&gt;is a standard normal distribution.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Therefore, in our case, the distribution of the sum of Bernoulli distributions with mean $p$ is distributed as a normal distribution with&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;mean $p$&lt;/li&gt;
&lt;li&gt;variance $p * (1-p)$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Therefore, we can obtain a random variable that is asymptotically standard normal distributed as&lt;/p&gt;
&lt;p&gt;$$
\lim_{n \to \infty} \ \sqrt{n} \left( \frac {\bar X_n - p}{\sqrt{p * (1-p)}} \right) \sim N(0,1)
$$&lt;/p&gt;
&lt;p&gt;Our last piece: what is a &lt;a href=&#34;https://en.wikipedia.org/wiki/Chi-squared_distribution&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;chi-squared distribution&lt;/a&gt;?&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;If $Z_1, &amp;hellip;, Z_k$ are independent, standard normal random variables, then the sum of their squares,&lt;/p&gt;
&lt;p&gt;$$
Q = \sum_{i=1}^k Z_i^2
$$
is distributed according to the chi-squared distribution with $k$ degrees of freedom.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;I.e. the sum of standard normal distributions is a chi-squared distribution, where the &lt;strong&gt;degrees of freedom&lt;/strong&gt; indicate the number of normal distributions we are summing over. Since the normalized sum of realizations of each dice number should converge to a standard normal distribution, their sum of squares should converge to a chi-squared distribution. I.e.&lt;/p&gt;
&lt;p&gt;$$
\lim_{n \to \infty} \ \sum_k n \frac{(\bar X_n - p)^2}{p * (1-p)} \sim \chi^2_k
$$&lt;/p&gt;
&lt;p&gt;There is just one issue: the last distribution is not really independent from the others. In fact, as soon as we know that we have thrown 60 dices and how many 1s, 2s, 3s, 4s, and 5s we got, we can compute the number of 6s. Therefore, we should exclude one distribution since only 5 (or, in general, $k-1$) are truly independent.&lt;/p&gt;
&lt;p&gt;In practice, however, we sum all distributions, but then we scale them down by multiplying them by $(1-p)$ so that we have&lt;/p&gt;
&lt;p&gt;$$
\lim_{n \to \infty} \ \sum_k n \frac{(\bar X_n - p)^2}{p} \sim \chi^2_{k-1}
$$&lt;/p&gt;
&lt;p&gt;which is exactly the formula we used to compute the test statistic:&lt;/p&gt;
&lt;p&gt;$$
T_{\chi^2} = \sum _{i=1}^{n} \frac{(O_i - E_i)^{2}}{E_i} = N \sum _{i=1}^{n} \frac{\left(O_i/N - p_i \right)^2 }{p_i}
$$&lt;/p&gt;
&lt;h2 id=&#34;example-2-are-grades-independent-from-gender&#34;&gt;Example 2: are grades independent from gender?&lt;/h2&gt;
&lt;p&gt;Chi-squared tests can also be used to test independence between 2 variables. The idea is fundamentally the same as the test in the previous section: checking systematic differences between observed and expected values, across different variables.&lt;/p&gt;
&lt;p&gt;Suppose you have data on grades in a classroom, by gender. Grades go from $1$ to $4$. Assuming males and females are equally prepared for the test, you want to test whether there has been discrimination in grading.&lt;/p&gt;
&lt;p&gt;The problem is again asserting whether the observed differences are random or systematic.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s generate some data (under the no discrimination assumption).&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Data generating process for grades
def generate_data_grades(N_male=60, N_female=40):
    grade_scale = [1,2,3,4]
    p = [0.1, 0.2, 0.5, 0.2]
    grades_male = np.random.choice(grade_scale, size=N_male, p=p)
    grades_female = np.random.choice(grade_scale, size=N_female, p=p)
    data = pd.DataFrame({&amp;quot;grade&amp;quot;: grade_scale + grade_scale,
                          &amp;quot;gender&amp;quot;: [&amp;quot;male&amp;quot; for i in grade_scale] + [&amp;quot;female&amp;quot; for i in grade_scale],
                          &amp;quot;observed&amp;quot;: [sum(grades_male==n) for n in grade_scale] + [sum(grades_female==n) for n in grade_scale],
                        })  
    data[&#39;expected gender&#39;] = data.groupby(&amp;quot;gender&amp;quot;)[&amp;quot;observed&amp;quot;].transform(&amp;quot;mean&amp;quot;) 
    data[&#39;expected grade&#39;] = data.groupby(&amp;quot;grade&amp;quot;)[&amp;quot;observed&amp;quot;].transform(&amp;quot;mean&amp;quot;) 
    data[&#39;expected&#39;] = data[&#39;expected gender&#39;] * data[&#39;expected grade&#39;] / data[&#39;observed&#39;].mean()
    return data
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Set seed for replicability
np.random.seed(1)

# Generate data
data_grades = generate_data_grades()
data_grades
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;grade&lt;/th&gt;
      &lt;th&gt;gender&lt;/th&gt;
      &lt;th&gt;observed&lt;/th&gt;
      &lt;th&gt;expected gender&lt;/th&gt;
      &lt;th&gt;expected grade&lt;/th&gt;
      &lt;th&gt;expected&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;male&lt;/td&gt;
      &lt;td&gt;9&lt;/td&gt;
      &lt;td&gt;15.0&lt;/td&gt;
      &lt;td&gt;5.5&lt;/td&gt;
      &lt;td&gt;6.6&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;male&lt;/td&gt;
      &lt;td&gt;14&lt;/td&gt;
      &lt;td&gt;15.0&lt;/td&gt;
      &lt;td&gt;10.5&lt;/td&gt;
      &lt;td&gt;12.6&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;male&lt;/td&gt;
      &lt;td&gt;27&lt;/td&gt;
      &lt;td&gt;15.0&lt;/td&gt;
      &lt;td&gt;24.5&lt;/td&gt;
      &lt;td&gt;29.4&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;male&lt;/td&gt;
      &lt;td&gt;10&lt;/td&gt;
      &lt;td&gt;15.0&lt;/td&gt;
      &lt;td&gt;9.5&lt;/td&gt;
      &lt;td&gt;11.4&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;female&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;10.0&lt;/td&gt;
      &lt;td&gt;5.5&lt;/td&gt;
      &lt;td&gt;4.4&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;5&lt;/th&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;female&lt;/td&gt;
      &lt;td&gt;7&lt;/td&gt;
      &lt;td&gt;10.0&lt;/td&gt;
      &lt;td&gt;10.5&lt;/td&gt;
      &lt;td&gt;8.4&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;6&lt;/th&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;female&lt;/td&gt;
      &lt;td&gt;22&lt;/td&gt;
      &lt;td&gt;10.0&lt;/td&gt;
      &lt;td&gt;24.5&lt;/td&gt;
      &lt;td&gt;19.6&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;7&lt;/th&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;female&lt;/td&gt;
      &lt;td&gt;9&lt;/td&gt;
      &lt;td&gt;10.0&lt;/td&gt;
      &lt;td&gt;9.5&lt;/td&gt;
      &lt;td&gt;7.6&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;Has there been discrimination?&lt;/p&gt;
&lt;p&gt;The value of the test-statistic is&lt;/p&gt;
&lt;p&gt;$$
T_{\chi^2} = \sum_{i=1}^r \sum_{j=1}^c \frac{(O_{i,j} - E_{i,j})^2 }{ E_{i,j} } = N \sum_{i,j} p_{i \cdot} p_{\cdot j} \left( \frac{O_{i,j}/N - p_{i \cdot} p_{\cdot j} }{ p_{i \cdot} p_{\cdot j}} \right)^2
$$&lt;/p&gt;
&lt;p&gt;where&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;$N$ is the total sample size (the sum of all cells in the table)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$p_{i \cdot} = \frac{O_{i\cdot }}{N} = \sum_{j=1}^{c} \frac{O_{i,j}}{N}$ is the fraction of observations of type i ignoring the column attribute (fraction of row totals), and&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$p_{\cdot j} = \frac{O_{\cdot j}}{N} = \sum_{i=1}^{r} \frac{O_{i,j}}{N}$ is the fraction of observations of type j ignoring the row attribute (fraction of column totals).&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;So the formula for the test statistic is the same&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;chi2_stat = compute_chi2_stat(data_grades)
chi2_stat
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;3.490327550477927
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As before, we can double-check whether the statistic is indeed distributed as a chi-squared with $k-1$ degrees of freedom by simulating the data generating process.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Plot data
chi2_stats = simulate_chi2stats(K=1000, N=60, dgp=generate_data_grades)
plt.hist(chi2_stats, density=True, bins=30, alpha=0.3, color=&#39;C0&#39;);
plt.plot(x, chi2_5_pdf);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/chisquared_37_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;What happened? We forgot to change the degrees of freedom! The general formula for the degrees of freedom when testing the independence of variables is $(N_i - 1) \times (N_j - 1)$. So in our case, it&amp;rsquo;s $(4-1) \times (2-1) = 3$.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;chi2_3_pdf = chi2.pdf(x, df=3)

# Plot data
chi2_stats = simulate_chi2stats(K=1000, N=60, dgp=generate_data_grades)
plt.hist(chi2_stats, density=True, bins=30, alpha=0.3, color=&#39;C0&#39;);
plt.plot(x, chi2_3_pdf);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/chisquared_39_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Do we reject the null hypothesis of independent distributions of gender and grades?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Plot
plot_test(x, chi2_stat, df=3)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/chisquared_41_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;No, we &lt;strong&gt;do not reject&lt;/strong&gt; the null hypothesis of independend distributions of gender and grades.&lt;/p&gt;
&lt;h2 id=&#34;example-3-testing-a-specific-data-generating-process&#34;&gt;Example 3: testing a specific data generating process&lt;/h2&gt;
&lt;p&gt;As we have seen, the chi-square test can be used to compare observed means/frequencies against a null hypothesis. How can we use this statistic to &lt;strong&gt;test a distributional assumption&lt;/strong&gt;?&lt;/p&gt;
&lt;p&gt;The answer is simple: we can construct conditional means. The easiest way to do it is to bin the data into equally sized bins and then check if the observed frequencies match the expected probabilities.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: we don&amp;rsquo;t need to have equally sized bins, but it&amp;rsquo;s useful since it ensures that we have as many observations in each bin as possible.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from scipy.stats import expon

# Data generating process
def generate_poisson_data(N=60, cuts=4):
    poisson_draws = np.random.exponential(size=N)
    cat, bins = pd.qcut(poisson_draws, cuts, retbins=True)
    p = [expon.cdf(bins[n+1]) - expon.cdf(bins[n]) for n in range(len(bins)-1)]
    data = pd.DataFrame({&amp;quot;bin&amp;quot;: cat.unique(),
                         &amp;quot;observed&amp;quot;: [sum(cat==n) for n in cat.unique()],
                         &amp;quot;expected&amp;quot;: np.dot(p, N)})
    return data, poisson_draws
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Set seed for replicability
np.random.seed(2)

# Generate data
data_poisson, poisson_draws = generate_poisson_data()
data_poisson
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;bin&lt;/th&gt;
      &lt;th&gt;observed&lt;/th&gt;
      &lt;th&gt;expected&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;(0.254, 0.573]&lt;/td&gt;
      &lt;td&gt;15&lt;/td&gt;
      &lt;td&gt;11.919795&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;(0.0253, 0.254]&lt;/td&gt;
      &lt;td&gt;15&lt;/td&gt;
      &lt;td&gt;12.706904&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;(0.573, 0.923]&lt;/td&gt;
      &lt;td&gt;15&lt;/td&gt;
      &lt;td&gt;9.967649&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;(0.923, 5.092]&lt;/td&gt;
      &lt;td&gt;15&lt;/td&gt;
      &lt;td&gt;23.481200&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;We can also plot the observed and realized distribution of the data.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Plot data
plt.hist(poisson_draws, density=True, bins=30, alpha=0.3, color=&#39;C0&#39;);
exp_pdf = expon.pdf(x)
plt.plot(x, exp_pdf);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/chisquared_48_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;The two distributions seem close but we need a test statistic in order to assess whether the dgp is indeed an exponential distribution&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;chi2_stat = compute_chi2_stat(data_poisson)
chi2_stat
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;6.813781416601728
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Do we reject the null hypothesis that the data is drawn from an exponential distribution?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Plot
plot_test(x, chi2_stat, df=3)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/chisquared_52_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;No, we &lt;strong&gt;do not reject&lt;/strong&gt; the null hypothesis that the data is drawn from an exponential distribution.&lt;/p&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;In this tutorial, we have seen how to perform 3 hypoteses tests&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;testing if a set of means or sums is coming from the expected distribution&lt;/li&gt;
&lt;li&gt;testing if two distributions are independent or not&lt;/li&gt;
&lt;li&gt;testing a specific data generating process&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The underlying principle is the same: testing discrepancies between expected and observed count data.&lt;/p&gt;
&lt;p&gt;The key statistic is Pearson&amp;rsquo;s chi-square statistic and the key distribution is the chi-squared distribution. We have seen how to compute the statistic, why it has a chi-squared distribution, and how to use this information to perform a statistical hypothesis test.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Instrumental Variables</title>
      <link>https://matteocourthoud.github.io/post/iv/</link>
      <pubDate>Fri, 08 Apr 2022 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/post/iv/</guid>
      <description>&lt;p&gt;In this tutorial, we are going to see how to estimate causal effects when the treatment is not randomly assigned, but we have access to a third variable that is as good as randomly assigned and is correlated (only) with the treatment. These variables are called instrumental variables and are a powerful tool for causal inference, especially in observational studies.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Requisites&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;For this tutorial, I assume you are familiar with the following concepts:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Rubin&amp;rsquo;s potential outcome framework&lt;/li&gt;
&lt;li&gt;Ordinary least squares regression&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Academic Application 1&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;As a first academic application, we are going to replicate &lt;a href=&#34;https://www.jstor.org/stable/2937954&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Does Compulsory School Attendance Affect Schooling and Earnings?&lt;/a&gt; (1991) by Angrist and Krueger. The authors study the effect of education on wages.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Academic Application 2&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;As a further academic application, we are going to replicate &lt;a href=&#34;https://economics.mit.edu/files/4123&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The Colonial Origins of Comparative Development&lt;/a&gt; (2002) by Acemoglu, Johnson, Robinson. The authors study the effect of institutions on economic development.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Business Case&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;As a business case, we are going to study a company that wants to find out whether subscribing to its newsletter has an effect on revenues. Since the travel agency cannot force customers to subscribing to the newsletter, it randomly sends reminder emails to infer the effect of the newsletter on revenues.&lt;/p&gt;
&lt;h2 id=&#34;setting&#34;&gt;Setting&lt;/h2&gt;
&lt;p&gt;We assume that for a set of i.i.d. subjects $i = 1, &amp;hellip;, n$ we observed a tuple $(X_i, D_i, Y_i)$ comprised of&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;a feature vector $X_i \in \mathbb R^n$&lt;/li&gt;
&lt;li&gt;a treatment variable $T_i \in \lbrace 0, 1 \rbrace$&lt;/li&gt;
&lt;li&gt;a response $Y_i \in \mathbb R$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Crucially, we do not assume unconfoundedness / strong ignorability hence&lt;/p&gt;
&lt;p&gt;$$
\big \lbrace Y_i^{(1)} , Y_i^{(0)} \big \rbrace \ \not \perp \ T_i \ | \ X_i
$$&lt;/p&gt;
&lt;h2 id=&#34;instrumental-variables&#34;&gt;Instrumental Variables&lt;/h2&gt;
&lt;p&gt;The &lt;strong&gt;standard linear IV model&lt;/strong&gt; is the following&lt;/p&gt;
&lt;p&gt;$$
Y_i = T_i \alpha + X_i \beta_1 + \varepsilon_i
\newline
T_i = Z_i \gamma + X_i \beta_2 + u_i
$$&lt;/p&gt;
&lt;p&gt;We assume there exists an &lt;strong&gt;instrumental variable&lt;/strong&gt; $Z_i \in \mathbb R^k$ that satisfies the following assumptions.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Assumption 1: Exclusion&lt;/strong&gt;: $\mathbb E [Z \varepsilon] = 0$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Assumption 2: Relevance&lt;/strong&gt;: $\mathbb E [Z T] \neq 0$&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The model can be represented by a DAG.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from src.plots import dag_iv
dag_iv()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/iv_6_0.svg&#34; alt=&#34;svg&#34;&gt;&lt;/p&gt;
&lt;p&gt;The IV estimator is instead unbiased&lt;/p&gt;
&lt;p&gt;$$
\hat \beta_{IV} = (Z&amp;rsquo;X)^{-1}(Z&amp;rsquo;Y)
$$&lt;/p&gt;
&lt;h3 id=&#34;potential-outcomes-perspective&#34;&gt;Potential Outcomes Perspective&lt;/h3&gt;
&lt;p&gt;We need to extend the potential outcomes framework in order to allow for the instrumental variable $Z$. First we define the potential outcomes as $Y^{(D(Z_i))}(Z_i)$&lt;/p&gt;
&lt;p&gt;The assumptions become&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Exclusion: $Y^{(D(Z_i))}(Z_i) = Y^{(T(Z_i))}$&lt;/li&gt;
&lt;li&gt;Relevance: $P(z) = \mathbb E [T, Z=z]$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We assume that $Z$ is fully randomly assigned (while $T$ is not).&lt;/p&gt;
&lt;p&gt;What does IV estimate?&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
\mathbb E[Y_i | Z_i = 1] - \mathbb E[Y_i | Z_i = 0] &amp;amp;= \Pr (D_i^{(1)} - D_i^{(0)} = 1) \times \mathbb E \Big[ Y_i^{(1)} - Y_i^{(0)} = 1 \ \Big | \ D_i^{(1)} - D_i^{(0)} = 1 \Big] -
\newline
&amp;amp;- \Pr (D_i^{(1)} - D_i^{(0)} = -1) \times \mathbb E \Big[ Y_i^{(1)} - Y_i^{(0)} = 1 \ \Big | \ D_i^{(1)} - D_i^{(0)} = -1 \Big]
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;Is this a quantity of interest? Almost. There are &lt;strong&gt;two issues&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;First, the first term is the treatment effect, but only for those individuals for whom $D_i^{(1)} - D_i^{(0)} = 1$, i.e. those that are induced into treatment by $Z_i$. These individuals are referred to as &lt;strong&gt;compliers&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Second, the second term is problematic since it removes from the first effect, the local effect of another subpopulation: $D_i^{(1)} - D_i^{(0)} = -1$, i.e. those that are induced out of treatment by $Z_i$. These individuals are referred to as &lt;strong&gt;defiers&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;We can get rid of defiers with a simple assumption.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Assumption 3: monotonocity&lt;/strong&gt;: $D_i^{(1)} \geq D_i^{(0)}$ (or viceversa)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;All effects must be monotone in the same direction&lt;/li&gt;
&lt;li&gt;Fundamentally untestable&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Then, the IV estimator can be expressed as a ration between two differences in means&lt;/p&gt;
&lt;p&gt;$$
\hat \beta_{IV} = \frac{\mathbb E[Y_i | Z_i = 1] - \mathbb E[Y_i | Z_i = 0]}{\mathbb E[T_i | Z_i = 1] - \mathbb E[T_i | Z_i = 0]}
$$&lt;/p&gt;
&lt;h3 id=&#34;structural-perspective&#34;&gt;Structural Perspective&lt;/h3&gt;
&lt;p&gt;One can interpret the IV estimator as a GMM estimator that uses the exclusion restriction as estimating equation.&lt;/p&gt;
&lt;p&gt;$$
\hat \beta_{GMM} = \arg \min_{\beta} \mathbb E \Big[ Z (Y - \alpha T - \beta X) \Big]^2
$$&lt;/p&gt;
&lt;h2 id=&#34;the-algebra-of-iv&#34;&gt;The Algebra of IV&lt;/h2&gt;
&lt;p&gt;TBD&lt;/p&gt;
&lt;h2 id=&#34;demand-and-supply&#34;&gt;Demand and Supply&lt;/h2&gt;
&lt;p&gt;TBD&lt;/p&gt;
&lt;h2 id=&#34;academic-application-1&#34;&gt;Academic Application 1&lt;/h2&gt;
&lt;p&gt;As an research paper replication, we are going to replicate &lt;a href=&#34;https://www.jstor.org/stable/2937954&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Does compulsory school attendance affect schooling and earnings?&lt;/a&gt; (1991) by Angrist and Krueger. The authors study the effect of education on wages.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;problem&lt;/strong&gt; of studying the relationship of education on wages is that there might be factors that influence both education and wages but we do not observe, for example ability. Students that have higher ability might decide to stay longer in school and also get higher wages afterwards.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;idea&lt;/strong&gt; of the authors is to use the quarter of birth as an instrument for education. In fact, quarter of birth is plausibly exogenous with respect to wages while, on the other hand, is correlated with education. Why? Students that are both in the last quarter of the year cannot drop out as early as other students and therefore are exposed to more eduction.&lt;/p&gt;
&lt;p&gt;We can represent the DAG of their model as follows.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;%matplotlib inline
%config InlineBackend.figure_format = &#39;retina&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from src.utils import *
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;dag_iv(Y=&amp;quot;wage&amp;quot;, T=&amp;quot;education&amp;quot;, Z=&amp;quot;quarter of birth&amp;quot;, U=&amp;quot;ability&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/iv_20_0.svg&#34; alt=&#34;svg&#34;&gt;&lt;/p&gt;
&lt;p&gt;A shortcoming of this instrument comes out of the fact that the population of &lt;strong&gt;compliers&lt;/strong&gt; is students that drop out of school as soon as possible, we will know the treatment effect only for this population. It&amp;rsquo;s important to keep this in mind when interpreting the results.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s load the data, freely available &lt;a href=&#34;https://economics.mit.edu/faculty/angrist/data1/data/angkru1991&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df = pd.read_csv(&#39;data/ak91.csv&#39;)
df.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;log_wage&lt;/th&gt;
      &lt;th&gt;years_of_schooling&lt;/th&gt;
      &lt;th&gt;date_of_birth&lt;/th&gt;
      &lt;th&gt;year_of_birth&lt;/th&gt;
      &lt;th&gt;quarter_of_birth&lt;/th&gt;
      &lt;th&gt;state_of_birth&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;5.790019&lt;/td&gt;
      &lt;td&gt;12.0&lt;/td&gt;
      &lt;td&gt;1930.0&lt;/td&gt;
      &lt;td&gt;1930.0&lt;/td&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;45.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;5.952494&lt;/td&gt;
      &lt;td&gt;11.0&lt;/td&gt;
      &lt;td&gt;1930.0&lt;/td&gt;
      &lt;td&gt;1930.0&lt;/td&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;45.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;5.315949&lt;/td&gt;
      &lt;td&gt;12.0&lt;/td&gt;
      &lt;td&gt;1930.0&lt;/td&gt;
      &lt;td&gt;1930.0&lt;/td&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;45.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;5.595926&lt;/td&gt;
      &lt;td&gt;12.0&lt;/td&gt;
      &lt;td&gt;1930.0&lt;/td&gt;
      &lt;td&gt;1930.0&lt;/td&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;45.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;6.068915&lt;/td&gt;
      &lt;td&gt;12.0&lt;/td&gt;
      &lt;td&gt;1930.0&lt;/td&gt;
      &lt;td&gt;1930.0&lt;/td&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;37.0&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;We have the variables of interest, &lt;code&gt;log_wage&lt;/code&gt;, &lt;code&gt;years_of_schooling&lt;/code&gt; and &lt;code&gt;quarter_of_birth&lt;/code&gt;, together with a set of controls.&lt;/p&gt;
&lt;h3 id=&#34;ols&#34;&gt;OLS&lt;/h3&gt;
&lt;p&gt;If we were to ignore the endogeneity problem we would estimate a linear regression of &lt;code&gt;log_wage&lt;/code&gt; on &lt;code&gt;years_of_schooling&lt;/code&gt;, plus control dummy variables for the &lt;code&gt;state_of_birth&lt;/code&gt; and &lt;code&gt;year_of_birth&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;smf.ols(&#39;log_wage ~ years_of_schooling&#39;, df).fit().summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
           &lt;td&gt;&lt;/td&gt;             &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt;          &lt;td&gt;    4.9952&lt;/td&gt; &lt;td&gt;    0.004&lt;/td&gt; &lt;td&gt; 1118.882&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    4.986&lt;/td&gt; &lt;td&gt;    5.004&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;years_of_schooling&lt;/th&gt; &lt;td&gt;    0.0709&lt;/td&gt; &lt;td&gt;    0.000&lt;/td&gt; &lt;td&gt;  209.243&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    0.070&lt;/td&gt; &lt;td&gt;    0.072&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;h3 id=&#34;iv&#34;&gt;IV&lt;/h3&gt;
&lt;p&gt;We now use &lt;code&gt;quarter_of_birth&lt;/code&gt; as an instrument for &lt;code&gt;years_of_schooling&lt;/code&gt;. We cannot check the exclusion restriction condition, but we can check the &lt;strong&gt;relevance&lt;/strong&gt; condition.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s start first by plotting average &lt;code&gt;years_of_schooling&lt;/code&gt; by date of birth.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;group_df = df.groupby(&amp;quot;date_of_birth&amp;quot;).mean().reset_index()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plt.figure(figsize=(15,6))
sns.lineplot(data=group_df, x=&amp;quot;date_of_birth&amp;quot;, y=&amp;quot;years_of_schooling&amp;quot;, zorder=1)\
.set(title=&amp;quot;First Stage&amp;quot;, xlabel=&amp;quot;Year of Birth&amp;quot;, ylabel=&amp;quot;Years of Schooling&amp;quot;);

for q in range(1, 5):
    x = group_df.loc[group_df[&#39;quarter_of_birth&#39;]==q, &amp;quot;date_of_birth&amp;quot;]
    y = group_df.loc[group_df[&#39;quarter_of_birth&#39;]==q, &amp;quot;years_of_schooling&amp;quot;]
    plt.scatter(x, y, marker=&amp;quot;s&amp;quot;, s=200, c=f&amp;quot;C{q}&amp;quot;)
    plt.scatter(x, y, marker=f&amp;quot;${q}$&amp;quot;, s=100, c=f&amp;quot;white&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/iv_31_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;As we can see, there is an upward trend but, within each year, people both in the last quarter usually have more years of schooling than people born in other quarters of the year.&lt;/p&gt;
&lt;p&gt;We can check this correlation more formally by regressing &lt;code&gt;years_of_schooling&lt;/code&gt; of a set of dummies for &lt;code&gt;quarter_of_birth&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;smf.ols(&#39;years_of_schooling ~ C(quarter_of_birth)&#39;, df).fit().summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
               &lt;td&gt;&lt;/td&gt;                 &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt;                  &lt;td&gt;   12.6881&lt;/td&gt; &lt;td&gt;    0.011&lt;/td&gt; &lt;td&gt; 1105.239&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;   12.666&lt;/td&gt; &lt;td&gt;   12.711&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;C(quarter_of_birth)[T.2.0]&lt;/th&gt; &lt;td&gt;    0.0566&lt;/td&gt; &lt;td&gt;    0.016&lt;/td&gt; &lt;td&gt;    3.473&lt;/td&gt; &lt;td&gt; 0.001&lt;/td&gt; &lt;td&gt;    0.025&lt;/td&gt; &lt;td&gt;    0.089&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;C(quarter_of_birth)[T.3.0]&lt;/th&gt; &lt;td&gt;    0.1173&lt;/td&gt; &lt;td&gt;    0.016&lt;/td&gt; &lt;td&gt;    7.338&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    0.086&lt;/td&gt; &lt;td&gt;    0.149&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;C(quarter_of_birth)[T.4.0]&lt;/th&gt; &lt;td&gt;    0.1514&lt;/td&gt; &lt;td&gt;    0.016&lt;/td&gt; &lt;td&gt;    9.300&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    0.119&lt;/td&gt; &lt;td&gt;    0.183&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;The relationship between &lt;code&gt;years_of_schooling&lt;/code&gt; and &lt;code&gt;quarter_of_birth&lt;/code&gt; is indeed statistically significant.&lt;/p&gt;
&lt;p&gt;Does it translate it into higher wages? We can have a first glimpse of potential IV effects by plotting wages against the date of birth.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plt.figure(figsize=(15,6))
sns.lineplot(data=group_df, x=&amp;quot;date_of_birth&amp;quot;, y=&amp;quot;log_wage&amp;quot;, zorder=1)\
.set(title=&amp;quot;Reduced Form&amp;quot;, xlabel=&amp;quot;Year of Birth&amp;quot;, ylabel=&amp;quot;Log Wage&amp;quot;);

for q in range(1, 5):
    x = group_df.loc[group_df[&#39;quarter_of_birth&#39;]==q, &amp;quot;date_of_birth&amp;quot;]
    y = group_df.loc[group_df[&#39;quarter_of_birth&#39;]==q, &amp;quot;log_wage&amp;quot;]
    plt.scatter(x, y, marker=&amp;quot;s&amp;quot;, s=200, c=f&amp;quot;C{q}&amp;quot;)
    plt.scatter(x, y, marker=f&amp;quot;${q}$&amp;quot;, s=100, c=f&amp;quot;white&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/iv_35_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;It seems that indeed people both in later quarters earn higher wages later in life.&lt;/p&gt;
&lt;p&gt;We now turn into the estimation of the causal effect of education on wages.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df[[&#39;q1&#39;, &#39;q2&#39;, &#39;q3&#39;, &#39;q4&#39;]] = pd.get_dummies(df[&#39;quarter_of_birth&#39;])
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from linearmodels.iv import IV2SLS

IV2SLS.from_formula(&#39;log_wage ~ 1 + [years_of_schooling ~ q1 + q2 + q3]&#39;, data=df).fit().summary.tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;caption&gt;Parameter Estimates&lt;/caption&gt;
&lt;tr&gt;
           &lt;td&gt;&lt;/td&gt;          &lt;th&gt;Parameter&lt;/th&gt; &lt;th&gt;Std. Err.&lt;/th&gt; &lt;th&gt;T-stat&lt;/th&gt; &lt;th&gt;P-value&lt;/th&gt; &lt;th&gt;Lower CI&lt;/th&gt; &lt;th&gt;Upper CI&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt;           &lt;td&gt;4.5898&lt;/td&gt;    &lt;td&gt;0.2494&lt;/td&gt;   &lt;td&gt;18.404&lt;/td&gt; &lt;td&gt;0.0000&lt;/td&gt;   &lt;td&gt;4.1010&lt;/td&gt;   &lt;td&gt;5.0786&lt;/td&gt; 
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;years_of_schooling&lt;/th&gt;  &lt;td&gt;0.1026&lt;/td&gt;    &lt;td&gt;0.0195&lt;/td&gt;   &lt;td&gt;5.2539&lt;/td&gt; &lt;td&gt;0.0000&lt;/td&gt;   &lt;td&gt;0.0643&lt;/td&gt;   &lt;td&gt;0.1409&lt;/td&gt; 
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;The coefficient is slightly higher than the OLS coefficient. It&amp;rsquo;s important to remember that the estimated effect is specific to the subpopulation of people that drop out of school as soon as they can.&lt;/p&gt;
&lt;h2 id=&#34;research-paper-replication-2&#34;&gt;Research Paper Replication 2&lt;/h2&gt;
&lt;p&gt;In &lt;a href=&#34;https://economics.mit.edu/files/4123&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The Colonial Origins of Comparative Development&lt;/a&gt; (2002) by Acemoglu, Johnson, Robinson, the authors wish to determine whether or not differences in institutions can help to explain observed economic outcomes.&lt;/p&gt;
&lt;p&gt;How do we measure &lt;em&gt;institutional differences&lt;/em&gt; and &lt;em&gt;economic outcomes&lt;/em&gt;?&lt;/p&gt;
&lt;p&gt;In this paper,&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;economic outcomes are proxied by log GDP per capita in 1995, adjusted for exchange rates.&lt;/li&gt;
&lt;li&gt;institutional differences are proxied by an index of protection against expropriation on average over 1985-95, constructed by the &lt;a href=&#34;https://www.prsgroup.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Political Risk Services Group&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The &lt;strong&gt;problem&lt;/strong&gt; is that there might exist other factors that affects both the quality of institutions and GDP. The authors suggest the following problems as sources of endogeneity:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;richer countries may be able to afford or prefer better institutions&lt;/li&gt;
&lt;li&gt;variables that affect income may also be correlated with institutional differences&lt;/li&gt;
&lt;li&gt;the construction of the index may be biased; analysts may be biased towards seeing countries with higher income having better institutions&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The &lt;strong&gt;idea&lt;/strong&gt; of the authors is to use settler&amp;rsquo;s mortality during the colonization period as an instrument for the quality of institutions. They hypothesize that higher mortality rates of colonizers led to the establishment of institutions that were more extractive in nature (less protection against expropriation), and these institutions still persist today.&lt;/p&gt;
&lt;p&gt;We can represent their DAG as follows.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;dag_iv(Y=&amp;quot;GDP&amp;quot;, T=&amp;quot;institutions&amp;quot;, Z=&amp;quot;settlers&#39; mortality&amp;quot;, U=&amp;quot;tons of stuff&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/iv_42_0.svg&#34; alt=&#34;svg&#34;&gt;&lt;/p&gt;
&lt;p&gt;First, let&amp;rsquo;s load the data (available &lt;a href=&#34;https://economics.mit.edu/faculty/acemoglu/data/ajr2001&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;) and have a look at it.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df = pd.read_csv(&#39;data/ajr02.csv&#39;,index_col=0)
df.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;GDP&lt;/th&gt;
      &lt;th&gt;Exprop&lt;/th&gt;
      &lt;th&gt;Mort&lt;/th&gt;
      &lt;th&gt;Latitude&lt;/th&gt;
      &lt;th&gt;Neo&lt;/th&gt;
      &lt;th&gt;Africa&lt;/th&gt;
      &lt;th&gt;Asia&lt;/th&gt;
      &lt;th&gt;Namer&lt;/th&gt;
      &lt;th&gt;Samer&lt;/th&gt;
      &lt;th&gt;logMort&lt;/th&gt;
      &lt;th&gt;Latitude2&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;8.39&lt;/td&gt;
      &lt;td&gt;6.50&lt;/td&gt;
      &lt;td&gt;78.20&lt;/td&gt;
      &lt;td&gt;0.3111&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;4.359270&lt;/td&gt;
      &lt;td&gt;0.096783&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;7.77&lt;/td&gt;
      &lt;td&gt;5.36&lt;/td&gt;
      &lt;td&gt;280.00&lt;/td&gt;
      &lt;td&gt;0.1367&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;5.634790&lt;/td&gt;
      &lt;td&gt;0.018687&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;9.13&lt;/td&gt;
      &lt;td&gt;6.39&lt;/td&gt;
      &lt;td&gt;68.90&lt;/td&gt;
      &lt;td&gt;0.3778&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;4.232656&lt;/td&gt;
      &lt;td&gt;0.142733&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;9.90&lt;/td&gt;
      &lt;td&gt;9.32&lt;/td&gt;
      &lt;td&gt;8.55&lt;/td&gt;
      &lt;td&gt;0.3000&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;2.145931&lt;/td&gt;
      &lt;td&gt;0.090000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;5&lt;/th&gt;
      &lt;td&gt;9.29&lt;/td&gt;
      &lt;td&gt;7.50&lt;/td&gt;
      &lt;td&gt;85.00&lt;/td&gt;
      &lt;td&gt;0.2683&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;4.442651&lt;/td&gt;
      &lt;td&gt;0.071985&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;The data contains the main variables, &lt;code&gt;DGP&lt;/code&gt;, &lt;code&gt;Exprop&lt;/code&gt; and &lt;code&gt;Mort&lt;/code&gt;, plus some geographical information.&lt;/p&gt;
&lt;h3 id=&#34;ols-1&#34;&gt;OLS&lt;/h3&gt;
&lt;p&gt;What would we get if we were to ignore the endogeneity problem? We estimate the following misspecified model by OLS&lt;/p&gt;
&lt;p&gt;$$
{GDP}_i = \beta_0 + \beta_1 {Exprop}_i + \varepsilon_i
$$&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;reg1 = smf.ols(&#39;GDP ~ Exprop&#39;, df).fit()
reg1.summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
      &lt;td&gt;&lt;/td&gt;         &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt; &lt;td&gt;    4.6609&lt;/td&gt; &lt;td&gt;    0.409&lt;/td&gt; &lt;td&gt;   11.402&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    3.844&lt;/td&gt; &lt;td&gt;    5.478&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Exprop&lt;/th&gt;    &lt;td&gt;    0.5220&lt;/td&gt; &lt;td&gt;    0.061&lt;/td&gt; &lt;td&gt;    8.527&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    0.400&lt;/td&gt; &lt;td&gt;    0.644&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;The coefficient of &lt;code&gt;Exprop&lt;/code&gt; is positive and significant but we know it is a biased estimate of the causal effect.&lt;/p&gt;
&lt;p&gt;One direction we could take in addressing the endogeneity problem could be to control for any factor that affects both &lt;code&gt;GDP&lt;/code&gt; and &lt;code&gt;Exprop&lt;/code&gt;. In particular, the authors consider the following sets of variables:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;climat; proxied by latitude&lt;/li&gt;
&lt;li&gt;differences that affect both economic performance and institutions, eg. cultural, historical, etc.; controlled for with the use of continent dummies&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;reg2 = smf.ols(&#39;GDP ~ Exprop + Latitude + Latitude2&#39;, df).fit()
reg3 = smf.ols(&#39;GDP ~ Exprop + Latitude + Latitude2 + Asia + Africa + Namer + Samer&#39;, df).fit()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from statsmodels.iolib.summary2 import summary_col

summary_col(results=[reg1,reg2,reg3],
            float_format=&#39;%0.2f&#39;,
            stars = True,
            info_dict={&#39;No. observations&#39; : lambda x: f&amp;quot;{int(x.nobs):d}&amp;quot;},
            regressor_order=[&#39;Intercept&#39;,&#39;Exprop&#39;,&#39;Latitude&#39;,&#39;Latitude2&#39;])
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
          &lt;td&gt;&lt;/td&gt;          &lt;th&gt;GDP I&lt;/th&gt;  &lt;th&gt;GDP II&lt;/th&gt;  &lt;th&gt;GDP III&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt;        &lt;td&gt;4.66***&lt;/td&gt; &lt;td&gt;4.55***&lt;/td&gt; &lt;td&gt;5.95***&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;&lt;/th&gt;                 &lt;td&gt;(0.41)&lt;/td&gt;  &lt;td&gt;(0.45)&lt;/td&gt;  &lt;td&gt;(0.68)&lt;/td&gt; 
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Exprop&lt;/th&gt;           &lt;td&gt;0.52***&lt;/td&gt; &lt;td&gt;0.49***&lt;/td&gt; &lt;td&gt;0.40***&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;&lt;/th&gt;                 &lt;td&gt;(0.06)&lt;/td&gt;  &lt;td&gt;(0.07)&lt;/td&gt;  &lt;td&gt;(0.06)&lt;/td&gt; 
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Latitude&lt;/th&gt;            &lt;td&gt;&lt;/td&gt;      &lt;td&gt;2.16&lt;/td&gt;    &lt;td&gt;0.42&lt;/td&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;&lt;/th&gt;                    &lt;td&gt;&lt;/td&gt;     &lt;td&gt;(1.68)&lt;/td&gt;  &lt;td&gt;(1.47)&lt;/td&gt; 
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Latitude2&lt;/th&gt;           &lt;td&gt;&lt;/td&gt;      &lt;td&gt;-2.12&lt;/td&gt;   &lt;td&gt;0.44&lt;/td&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;&lt;/th&gt;                    &lt;td&gt;&lt;/td&gt;     &lt;td&gt;(2.86)&lt;/td&gt;  &lt;td&gt;(2.48)&lt;/td&gt; 
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Africa&lt;/th&gt;              &lt;td&gt;&lt;/td&gt;        &lt;td&gt;&lt;/td&gt;     &lt;td&gt;-1.06**&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;&lt;/th&gt;                    &lt;td&gt;&lt;/td&gt;        &lt;td&gt;&lt;/td&gt;     &lt;td&gt;(0.41)&lt;/td&gt; 
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Asia&lt;/th&gt;                &lt;td&gt;&lt;/td&gt;        &lt;td&gt;&lt;/td&gt;     &lt;td&gt;-0.74*&lt;/td&gt; 
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;&lt;/th&gt;                    &lt;td&gt;&lt;/td&gt;        &lt;td&gt;&lt;/td&gt;     &lt;td&gt;(0.42)&lt;/td&gt; 
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Namer&lt;/th&gt;               &lt;td&gt;&lt;/td&gt;        &lt;td&gt;&lt;/td&gt;      &lt;td&gt;-0.17&lt;/td&gt; 
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;&lt;/th&gt;                    &lt;td&gt;&lt;/td&gt;        &lt;td&gt;&lt;/td&gt;     &lt;td&gt;(0.40)&lt;/td&gt; 
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Samer&lt;/th&gt;               &lt;td&gt;&lt;/td&gt;        &lt;td&gt;&lt;/td&gt;      &lt;td&gt;-0.12&lt;/td&gt; 
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;&lt;/th&gt;                    &lt;td&gt;&lt;/td&gt;        &lt;td&gt;&lt;/td&gt;     &lt;td&gt;(0.42)&lt;/td&gt; 
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;R-squared&lt;/th&gt;         &lt;td&gt;0.54&lt;/td&gt;    &lt;td&gt;0.56&lt;/td&gt;    &lt;td&gt;0.71&lt;/td&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;R-squared Adj.&lt;/th&gt;    &lt;td&gt;0.53&lt;/td&gt;    &lt;td&gt;0.54&lt;/td&gt;    &lt;td&gt;0.67&lt;/td&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;No. observations&lt;/th&gt;   &lt;td&gt;64&lt;/td&gt;      &lt;td&gt;64&lt;/td&gt;      &lt;td&gt;64&lt;/td&gt;   
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;The coefficient of &lt;code&gt;Expropr&lt;/code&gt; decreases in magnitude but remains positive and significant after the addition of geographical control variables. This might suggest that the endogeneity problem is not very pronounced. However, it&amp;rsquo;s hard to say given the large number of factors that could affect both institutions and GDP.&lt;/p&gt;
&lt;h3 id=&#34;iv-1&#34;&gt;IV&lt;/h3&gt;
&lt;p&gt;In order for &lt;code&gt;Mort&lt;/code&gt; to be a valid instrument it needs to satisfy the two IV conditions:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Exclusion&lt;/strong&gt;: &lt;code&gt;Mort&lt;/code&gt; must be correlated to &lt;code&gt;GDP&lt;/code&gt; only through &lt;code&gt;Exprop&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Relevance&lt;/strong&gt;: &lt;code&gt;Mort&lt;/code&gt; must be correlated with &lt;code&gt;Exprop&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The &lt;strong&gt;exclusion restriction&lt;/strong&gt; condition is untestable, however, we may not be satisfied if settler mortality rates in the 17th to 19th centuries have a direct effect on current GDP (in addition to their indirect effect through institutions).&lt;/p&gt;
&lt;p&gt;For example, settler mortality rates may be related to the current disease environment in a country, which could affect current economic performance.&lt;/p&gt;
&lt;p&gt;The authors argue this is unlikely because:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The majority of settler deaths were due to malaria and yellow fever and had a limited effect on local people.&lt;/li&gt;
&lt;li&gt;The disease burden on local people in Africa or India, for example, did not appear to be higher than average, supported by relatively high population densities in these areas before colonization.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The &lt;strong&gt;relevance&lt;/strong&gt; condition is testable and we can check it by computing the partial correlation between &lt;code&gt;Mort&lt;/code&gt; and &lt;code&gt;Exprop&lt;/code&gt;. Let&amp;rsquo;s start by visual inspection first.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sns.scatterplot(data=df, x=&#39;Mort&#39;, y=&#39;Exprop&#39;)\
.set(title=&#39;First Stage&#39;,
    xlabel=&#39;Settler mortality&#39;,
    ylabel=&#39;Risk of expropriation&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/iv_57_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Visually, the first stage seems weak, at best. However, a regression of &lt;code&gt;Exprop&lt;/code&gt; on &lt;code&gt;Mort&lt;/code&gt; can help us better assess whether the relationship is significant or not.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;smf.ols(&#39;Exprop ~ Mort&#39;, df).fit().summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
      &lt;td&gt;&lt;/td&gt;         &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt; &lt;td&gt;    6.7094&lt;/td&gt; &lt;td&gt;    0.202&lt;/td&gt; &lt;td&gt;   33.184&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    6.305&lt;/td&gt; &lt;td&gt;    7.114&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Mort&lt;/th&gt;      &lt;td&gt;   -0.0008&lt;/td&gt; &lt;td&gt;    0.000&lt;/td&gt; &lt;td&gt;   -2.059&lt;/td&gt; &lt;td&gt; 0.044&lt;/td&gt; &lt;td&gt;   -0.002&lt;/td&gt; &lt;td&gt;-2.28e-05&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;The coefficient is negative, as expected, and statistically significant.&lt;/p&gt;
&lt;p&gt;The second-stage regression results give us an unbiased and consistent estimate of the effect of institutions on economic outcomes.&lt;/p&gt;
&lt;p&gt;$$
{GDP}_i = \beta_0 + \beta_1 {Exprop}_i + \varepsilon_i \
{Exprop}_i = \delta_0 + \delta_1 {logMort}_i + v_i
$$&lt;/p&gt;
&lt;p&gt;Note that while our parameter estimates are correct, our standard errors
are not and for this reason, computing 2SLS âmanuallyâ (in stages with
OLS) is not recommended.&lt;/p&gt;
&lt;p&gt;We can correctly estimate a 2SLS regression in one step using the
&lt;a href=&#34;https://github.com/bashtage/linearmodels&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;linearmodels&lt;/a&gt; package, an extension of &lt;code&gt;statsmodels&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Note that when using &lt;code&gt;IV2SLS&lt;/code&gt;, the exogenous and instrument variables
are split up in the function arguments (whereas before the instrument
included exogenous variables)&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;IV2SLS.from_formula(&#39;GDP ~ 1 + [Exprop ~ logMort]&#39;, data=df).fit().summary.tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;caption&gt;Parameter Estimates&lt;/caption&gt;
&lt;tr&gt;
      &lt;td&gt;&lt;/td&gt;      &lt;th&gt;Parameter&lt;/th&gt; &lt;th&gt;Std. Err.&lt;/th&gt; &lt;th&gt;T-stat&lt;/th&gt; &lt;th&gt;P-value&lt;/th&gt; &lt;th&gt;Lower CI&lt;/th&gt; &lt;th&gt;Upper CI&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt;  &lt;td&gt;2.0448&lt;/td&gt;    &lt;td&gt;1.1273&lt;/td&gt;   &lt;td&gt;1.8139&lt;/td&gt; &lt;td&gt;0.0697&lt;/td&gt;   &lt;td&gt;-0.1647&lt;/td&gt;  &lt;td&gt;4.2542&lt;/td&gt; 
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Exprop&lt;/th&gt;     &lt;td&gt;0.9235&lt;/td&gt;    &lt;td&gt;0.1691&lt;/td&gt;   &lt;td&gt;5.4599&lt;/td&gt; &lt;td&gt;0.0000&lt;/td&gt;   &lt;td&gt;0.5920&lt;/td&gt;   &lt;td&gt;1.2550&lt;/td&gt; 
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;The result suggests a stronger positive relationship than what the OLS results indicated.&lt;/p&gt;
&lt;h2 id=&#34;business-case&#34;&gt;Business Case&lt;/h2&gt;
&lt;p&gt;We are given the following problem:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;A firm would like to understand whether its newsletter is working to increase revenue. However, it cannot force customers to subscribe to the newsletter. Instead, the firm sends a reminder email to a random sample of customers for the newsletter. Estimate the effect of the newsletter on revenue.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;We start by drawing a sample from the data generating process.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from src.dgp import dgp_newsletter
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;dgp = dgp_newsletter()
df = dgp.generate_data()
df.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;reminder&lt;/th&gt;
      &lt;th&gt;subscribe&lt;/th&gt;
      &lt;th&gt;revenue&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0.582809&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;3.427162&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1.953731&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;2.902038&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0.826724&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;From the data, we know the &lt;code&gt;revenue&lt;/code&gt; per customer, whether it was sent a &lt;code&gt;reminder&lt;/code&gt; for the newsletter and whether it actually decided to &lt;code&gt;subscribe&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;If we to estimate the effect of &lt;code&gt;subscribe&lt;/code&gt; on &lt;code&gt;revenue&lt;/code&gt;, we might get a biased estimate because the decision of subscribing is endogenous. For example, we can imagine that wealthier customers are generating more revenue but are also less likely to subscribe.&lt;/p&gt;
&lt;p&gt;We can represent the model with a DAG.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;dag_iv(Y=&amp;quot;revenue&amp;quot;, T=&amp;quot;subscribe&amp;quot;, Z=&amp;quot;reminder&amp;quot;, U=&amp;quot;income&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/iv_69_0.svg&#34; alt=&#34;svg&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;ols-2&#34;&gt;OLS&lt;/h3&gt;
&lt;p&gt;By directly inspecting the data, it seems that subscribed members actually generate less revenue than normal customers.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sns.barplot(x=&#39;subscribe&#39;, y=&#39;revenue&#39;, data=df);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/iv_72_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;A linear regression confirms the graphical intuition.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;smf.ols(&#39;revenue ~ 1 + subscribe&#39;, df).fit().summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
      &lt;td&gt;&lt;/td&gt;         &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt; &lt;td&gt;    1.7752&lt;/td&gt; &lt;td&gt;    0.086&lt;/td&gt; &lt;td&gt;   20.697&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    1.607&lt;/td&gt; &lt;td&gt;    1.943&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;subscribe&lt;/th&gt; &lt;td&gt;   -0.7441&lt;/td&gt; &lt;td&gt;    0.140&lt;/td&gt; &lt;td&gt;   -5.334&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;   -1.018&lt;/td&gt; &lt;td&gt;   -0.470&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;However, if indeed wealthier customers generate more revenue and are less likely to subscribe, we have a negative omitted variable bias and we can expect the true effect of the newsletter to be bigger than the OLS estimate.&lt;/p&gt;
&lt;h3 id=&#34;iv-2&#34;&gt;IV&lt;/h3&gt;
&lt;p&gt;Let&amp;rsquo;s now exploit the random variation induced by the discount. In order for our instrument to be valid, we need it to be exogenous (untestable) and relevant. We can test the relevance with the &lt;strong&gt;first stage&lt;/strong&gt; regresssion of &lt;code&gt;reminder&lt;/code&gt; on &lt;code&gt;subscribe&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;smf.ols(&#39;subscribe ~ 1 + reminder&#39;, df).fit().summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
      &lt;td&gt;&lt;/td&gt;         &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt; &lt;td&gt;    0.2368&lt;/td&gt; &lt;td&gt;    0.021&lt;/td&gt; &lt;td&gt;   11.324&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    0.196&lt;/td&gt; &lt;td&gt;    0.278&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;reminder&lt;/th&gt;  &lt;td&gt;    0.2790&lt;/td&gt; &lt;td&gt;    0.029&lt;/td&gt; &lt;td&gt;    9.488&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    0.221&lt;/td&gt; &lt;td&gt;    0.337&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;It seems that the instrument is relevant. We can now estimate the IV regression.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;IV2SLS.from_formula(&#39;revenue ~ 1 + [subscribe ~ reminder]&#39;, data=df).fit().summary.tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;caption&gt;Parameter Estimates&lt;/caption&gt;
&lt;tr&gt;
      &lt;td&gt;&lt;/td&gt;      &lt;th&gt;Parameter&lt;/th&gt; &lt;th&gt;Std. Err.&lt;/th&gt; &lt;th&gt;T-stat&lt;/th&gt; &lt;th&gt;P-value&lt;/th&gt; &lt;th&gt;Lower CI&lt;/th&gt; &lt;th&gt;Upper CI&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt;  &lt;td&gt;0.9485&lt;/td&gt;    &lt;td&gt;0.2147&lt;/td&gt;   &lt;td&gt;4.4184&lt;/td&gt; &lt;td&gt;0.0000&lt;/td&gt;   &lt;td&gt;0.5278&lt;/td&gt;   &lt;td&gt;1.3693&lt;/td&gt; 
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;subscribe&lt;/th&gt;  &lt;td&gt;1.4428&lt;/td&gt;    &lt;td&gt;0.5406&lt;/td&gt;   &lt;td&gt;2.6689&lt;/td&gt; &lt;td&gt;0.0076&lt;/td&gt;   &lt;td&gt;0.3832&lt;/td&gt;   &lt;td&gt;2.5023&lt;/td&gt; 
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;The estimated coefficient has now flipped sign and turned positive! Ignoring the endogeneity problem would have lead us to the wrong conclusion.&lt;/p&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=LEAx0He_KBI&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Instrumental Variables&lt;/a&gt; video lecture by Paul Goldsmith-Pinkham (Yale)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://matheusfacure.github.io/python-causality-handbook/08-Instrumental-Variables.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Instrumental Variables&lt;/a&gt; section from Matheus Facure&amp;rsquo;s &lt;a href=&#34;https://matheusfacure.github.io/python-causality-handbook/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Causal Inference for The Brave and The True&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.jstor.org/stable/2937954&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Does compulsory school attendance affect schooling and earnings?&lt;/a&gt; (1991) by Angrist and Krueger&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://economics.mit.edu/files/4123&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The Colonial Origins of Comparative Development&lt;/a&gt; (2002) by Acemoglu, Johnson, Robinson&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Meta Learners</title>
      <link>https://matteocourthoud.github.io/post/meta_learners/</link>
      <pubDate>Fri, 08 Apr 2022 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/post/meta_learners/</guid>
      <description>&lt;p&gt;In this tutorial, we are going to explore and compare different methods that leverage machine learning to estimate heterogeneous treatment effects.&lt;/p&gt;
&lt;p&gt;For this tutorial, I assume you are familiar with the following concepts:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Rubin&amp;rsquo;s potential outcome framework&lt;/li&gt;
&lt;li&gt;Propensity score weighting&lt;/li&gt;
&lt;li&gt;Basic machine learning models&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;setting&#34;&gt;Setting&lt;/h2&gt;
&lt;p&gt;We assume that for a set of i.i.d. subjects $i = 1, &amp;hellip;, n$ we observed a tuple $(X_i, D_i, Y_i)$ comprised of&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;a feature vector $X_i \in \mathbb R^n$&lt;/li&gt;
&lt;li&gt;a treatment assignment $T_i \in \lbrace 0, 1 \rbrace$&lt;/li&gt;
&lt;li&gt;a response $Y_i \in \mathbb R$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Assumption 1 : unconfoundedness&lt;/strong&gt; (or ignorability, or selection on observables)&lt;/p&gt;
&lt;p&gt;$$
\big \lbrace Y_i^{(1)} , Y_i^{(0)} \big \rbrace \ \perp \ T_i \ | \ X_i
$$&lt;/p&gt;
&lt;p&gt;i.e. conditional on observable characteristics $X$, the treatment assignment $T$ is as good as random.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Assumption 2: overlap&lt;/strong&gt; (or bounded support)&lt;/p&gt;
&lt;p&gt;$$
\exists \eta &amp;gt; 0 \ : \ \eta \leq \mathbb E \left[ T_i = 1 \ \big | \ X_i = x \right] \leq 1-\eta
$$&lt;/p&gt;
&lt;p&gt;i.e. no observation is deterministically assigned to the treatment or control group.&lt;/p&gt;
&lt;h2 id=&#34;meta-learners&#34;&gt;Meta Learners&lt;/h2&gt;
&lt;h3 id=&#34;s-learner&#34;&gt;S-Learner&lt;/h3&gt;
&lt;p&gt;The simplest meta-algorithm is the single learner or &lt;strong&gt;S-learner&lt;/strong&gt;. To build the S-learner estimator, we fit a single model for all observations.&lt;/p&gt;
&lt;p&gt;$$
\mu(z) = \mathbb E \left[ Y_i \ \big | \ (X_i, T_i) = z \right]
$$&lt;/p&gt;
&lt;p&gt;the estimator is given by the difference between the predicted values evaluated at $t=1$ and $t=0$.&lt;/p&gt;
&lt;p&gt;$$
\hat \tau_{S} (x) = \hat \mu(x,1) - \hat \mu(x,0)
$$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Problems&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;We are learning a single model so we hope that the model uncovers heterogeneity in $T$ but it might not be the case&lt;/li&gt;
&lt;li&gt;If the model is heavily regularized because of the high dimensionality of $X$, it might not recover any treatment effect
&lt;ul&gt;
&lt;li&gt;e.g. with trees, it might not split on $T$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;t-learner&#34;&gt;T-learner&lt;/h3&gt;
&lt;p&gt;To build the two-learner or &lt;strong&gt;T-learner&lt;/strong&gt; estimator, we fit two different models, one for treated units and one for control units.&lt;/p&gt;
&lt;p&gt;$$
\mu^{(1)}(x) = \mathbb E \left[ Y_i \ \big | \ X_i = x, T_i = 1 \right] \qquad ; \qquad \mu^{(0)}(x) = \mathbb E \left[ Y_i \ \big | \ X_i = x, T_i = 0 \right]
$$&lt;/p&gt;
&lt;p&gt;the estimator is given by the difference between the predicted values of the two algorithms.&lt;/p&gt;
&lt;p&gt;$$
\hat \tau_{T} (x) = \hat \mu^{(1)}(x) - \hat \mu^{(0)}(x)
$$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Problems&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;We are using just a fraction of the data for each prediction problem
&lt;ul&gt;
&lt;li&gt;S-learner was using all the data&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;We might get heterogeneity where there is none, just because we are forcing different models
&lt;ul&gt;
&lt;li&gt;E.g. if trees split differently, to compute the two potential outcomes we use different populations&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;x-learner&#34;&gt;X-learner&lt;/h3&gt;
&lt;p&gt;The cross-learner or &lt;strong&gt;X-learner&lt;/strong&gt; estimator is an extension of the T-learner estimator. It is built in the following way:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;As for the T-learner, compute separate models for $\mu^{(1)}(x)$ and $\mu^{(0)}(x)$ using the treated and control units, respectively&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Compute the estimated treatment effects as&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;$$
\Delta_i (x) =
\begin{cases}
Y_i - \hat \mu^{(0)}(x) &amp;amp;\quad \text{ if } T_i = 1
\newline
\hat \mu^{(1)}(x) - Y_i &amp;amp;\quad \text{ if } T_i = 0
\end{cases}
$$&lt;/p&gt;
&lt;ol start=&#34;3&#34;&gt;
&lt;li&gt;
&lt;p&gt;Predicting $\Delta$ from $X$, compute $\hat \tau^{(0)}(x)$ from treated units and  $\hat \tau^{(1)}(x)$ from control units&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Estimate $e(x) = \mathbb E \left[ T_i = 1 \ \big | \ X_i = x \right]$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Compute&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;$$
\hat \tau_X(x) = \hat \tau^{(0)}(x) \hat e(x) + \hat \tau^{(1)}(x) (1 - \hat e(x))
$$&lt;/p&gt;
&lt;h2 id=&#34;example&#34;&gt;Example&lt;/h2&gt;
&lt;p&gt;In this example, we are going to use the following &lt;strong&gt;data generating process&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$N = 4000$&lt;/li&gt;
&lt;li&gt;$p = 10$&lt;/li&gt;
&lt;li&gt;$X_i \sim N(0, I_p)$&lt;/li&gt;
&lt;li&gt;$e(x) = 0.3$&lt;/li&gt;
&lt;li&gt;$\varepsilon_i \sim N(0, 1)$&lt;/li&gt;
&lt;li&gt;$\mu^{(0)}(x) = (x_1 + x_2)_{+} + \varepsilon$&lt;/li&gt;
&lt;li&gt;$\mu^{(1)}(x) = (x_1 + x_2)_{+} + \frac{1}{1 + e^{-x_3}} + \varepsilon$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;So that the propensity score is constant $e(x) = 0.3$, the treatment effect is $\frac{1}{1 + e^{-x_3}}$ and the average treatment effect is $0.5$.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;%matplotlib inline
%config InlineBackend.figure_format = &#39;retina&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from src.utils import *
from src.dgp import dgp4
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We generate a dataset out of our DGP.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;dgp = dgp4()
df = dgp.generate_data()
df.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;x1&lt;/th&gt;
      &lt;th&gt;x2&lt;/th&gt;
      &lt;th&gt;x3&lt;/th&gt;
      &lt;th&gt;x4&lt;/th&gt;
      &lt;th&gt;x5&lt;/th&gt;
      &lt;th&gt;x6&lt;/th&gt;
      &lt;th&gt;x7&lt;/th&gt;
      &lt;th&gt;x8&lt;/th&gt;
      &lt;th&gt;x9&lt;/th&gt;
      &lt;th&gt;x10&lt;/th&gt;
      &lt;th&gt;e&lt;/th&gt;
      &lt;th&gt;T&lt;/th&gt;
      &lt;th&gt;tau&lt;/th&gt;
      &lt;th&gt;Y&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;1.624345&lt;/td&gt;
      &lt;td&gt;-0.611756&lt;/td&gt;
      &lt;td&gt;-0.528172&lt;/td&gt;
      &lt;td&gt;-1.072969&lt;/td&gt;
      &lt;td&gt;0.865408&lt;/td&gt;
      &lt;td&gt;-2.301539&lt;/td&gt;
      &lt;td&gt;1.744812&lt;/td&gt;
      &lt;td&gt;-0.761207&lt;/td&gt;
      &lt;td&gt;0.319039&lt;/td&gt;
      &lt;td&gt;-0.249370&lt;/td&gt;
      &lt;td&gt;0.3&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0.370943&lt;/td&gt;
      &lt;td&gt;1.116502&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;1.462108&lt;/td&gt;
      &lt;td&gt;-2.060141&lt;/td&gt;
      &lt;td&gt;-0.322417&lt;/td&gt;
      &lt;td&gt;-0.384054&lt;/td&gt;
      &lt;td&gt;1.133769&lt;/td&gt;
      &lt;td&gt;-1.099891&lt;/td&gt;
      &lt;td&gt;-0.172428&lt;/td&gt;
      &lt;td&gt;-0.877858&lt;/td&gt;
      &lt;td&gt;0.042214&lt;/td&gt;
      &lt;td&gt;0.582815&lt;/td&gt;
      &lt;td&gt;0.3&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0.420087&lt;/td&gt;
      &lt;td&gt;-0.248671&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;-1.100619&lt;/td&gt;
      &lt;td&gt;1.144724&lt;/td&gt;
      &lt;td&gt;0.901591&lt;/td&gt;
      &lt;td&gt;0.502494&lt;/td&gt;
      &lt;td&gt;0.900856&lt;/td&gt;
      &lt;td&gt;-0.683728&lt;/td&gt;
      &lt;td&gt;-0.122890&lt;/td&gt;
      &lt;td&gt;-0.935769&lt;/td&gt;
      &lt;td&gt;-0.267888&lt;/td&gt;
      &lt;td&gt;0.530355&lt;/td&gt;
      &lt;td&gt;0.3&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0.711276&lt;/td&gt;
      &lt;td&gt;0.651441&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;-0.691661&lt;/td&gt;
      &lt;td&gt;-0.396754&lt;/td&gt;
      &lt;td&gt;-0.687173&lt;/td&gt;
      &lt;td&gt;-0.845206&lt;/td&gt;
      &lt;td&gt;-0.671246&lt;/td&gt;
      &lt;td&gt;-0.012665&lt;/td&gt;
      &lt;td&gt;-1.117310&lt;/td&gt;
      &lt;td&gt;0.234416&lt;/td&gt;
      &lt;td&gt;1.659802&lt;/td&gt;
      &lt;td&gt;0.742044&lt;/td&gt;
      &lt;td&gt;0.3&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0.334662&lt;/td&gt;
      &lt;td&gt;-0.913644&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;-0.191836&lt;/td&gt;
      &lt;td&gt;-0.887629&lt;/td&gt;
      &lt;td&gt;-0.747158&lt;/td&gt;
      &lt;td&gt;1.692455&lt;/td&gt;
      &lt;td&gt;0.050808&lt;/td&gt;
      &lt;td&gt;-0.636996&lt;/td&gt;
      &lt;td&gt;0.190915&lt;/td&gt;
      &lt;td&gt;2.100255&lt;/td&gt;
      &lt;td&gt;0.120159&lt;/td&gt;
      &lt;td&gt;0.617203&lt;/td&gt;
      &lt;td&gt;0.3&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0.321441&lt;/td&gt;
      &lt;td&gt;0.121779&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;First, we implement the simplest machine learning method for learning heterogeneous treatment effects: the &lt;strong&gt;S-learner&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;We fit a single &lt;code&gt;RandomForestRegressor&lt;/code&gt; method to all the data.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from sklearn.ensemble import RandomForestRegressor as rfr

mu_S = rfr(min_samples_leaf=30)
mu_S.fit(df[dgp.X + [&#39;T&#39;]], df[&#39;Y&#39;]);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then, we use it to predict $\mu^{(1)}(x)$ and $\mu^{(0)}(x)$.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df[&#39;hat_mu0_S&#39;] = mu_S.predict(df[dgp.X + [&#39;T&#39;]].assign(T=0))
df[&#39;hat_mu1_S&#39;] = mu_S.predict(df[dgp.X + [&#39;T&#39;]].assign(T=1))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We estimate the average treatment effect as the difference between the two predictions.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df[&#39;hat_tau_S&#39;] = df[&#39;hat_mu1_S&#39;] - df[&#39;hat_mu0_S&#39;]
print(f&amp;quot;S-learner estimate : {np.mean(df[&#39;hat_tau_S&#39;]):.4}&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;S-learner estimate : 0.3657
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;How close are we to the true treatment effect?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sns.scatterplot(data=df, x=&#39;x3&#39;, y=&#39;hat_tau_S&#39;, alpha=0.3);
sns.scatterplot(data=df, x=&#39;x3&#39;, y=&#39;tau&#39;, color=&#39;C2&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/meta_learners_24_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;T-learner&lt;/strong&gt; method instead fits different model for treated and control units. The advantage is that it can&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;mu0_T = rfr(min_samples_leaf=30)
mu0_T.fit(df.loc[df[&#39;T&#39;]==0, dgp.X + [&#39;T&#39;]], df.loc[df[&#39;T&#39;]==0, &#39;Y&#39;])
mu1_T = rfr(min_samples_leaf=30)
mu1_T.fit(df.loc[df[&#39;T&#39;]==1, dgp.X + [&#39;T&#39;]], df.loc[df[&#39;T&#39;]==1, &#39;Y&#39;]);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then, we use it to predict $\mu^{(1)}(x)$ and $\mu^{(0)}(x)$.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df[&#39;hat_mu0_T&#39;] = mu0_T.predict(df[dgp.X + [&#39;T&#39;]])
df[&#39;hat_mu1_T&#39;] = mu1_T.predict(df[dgp.X + [&#39;T&#39;]])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We estimate the average treatment effect as the difference between the two predictions.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df[&#39;hat_tau_T&#39;] = df[&#39;hat_mu1_T&#39;] - df[&#39;hat_mu0_T&#39;]
print(f&amp;quot;S-learner estimate : {np.mean(df[&#39;hat_tau_T&#39;]):.4}&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;S-learner estimate : 0.5231
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can plot the distribution of treatment effect estimates against the true values.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sns.scatterplot(data=df, x=&#39;x3&#39;, y=&#39;hat_tau_T&#39;, alpha=0.3);
sns.scatterplot(data=df, x=&#39;x3&#39;, y=&#39;tau&#39;, color=&#39;C2&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/meta_learners_32_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s now estimate the &lt;strong&gt;X-learner&lt;/strong&gt;. The first step is exactly the same as for the T-learner: estimate $\hat \mu^{(1)}(x)$ and $\hat \mu^{(0)}(x)$ using the treated and control group, respectively.&lt;/p&gt;
&lt;p&gt;Afterwards, we compute the estimated treatment effect on the treated using the the estimated counterfactual outcome estimated on the control group $\hat \mu^{(0)}(x)$, and viceversa.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df[&#39;Delta&#39;] = 0
df.loc[df[&#39;T&#39;]==0, &#39;Delta&#39;] = (df[&#39;hat_mu1_T&#39;] - df[&#39;Y&#39;])[df[&#39;T&#39;]==0]
df.loc[df[&#39;T&#39;]==1, &#39;Delta&#39;] = (df[&#39;Y&#39;] - df[&#39;hat_mu0_T&#39;])[df[&#39;T&#39;]==1]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then, we basically repeat the process for the T-learner, but using Delta as outcome variable.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;tau0_X = rfr(min_samples_leaf=30)
tau0_X.fit(df.loc[df[&#39;T&#39;]==0, dgp.X + [&#39;T&#39;]], df.loc[df[&#39;T&#39;]==0, &#39;Delta&#39;])
tau1_X = rfr(min_samples_leaf=30)
tau1_X.fit(df.loc[df[&#39;T&#39;]==1, dgp.X + [&#39;T&#39;]], df.loc[df[&#39;T&#39;]==1, &#39;Delta&#39;]);
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df[&#39;hat_tau0_X&#39;] = tau0_X.predict(df[dgp.X + [&#39;T&#39;]])
df[&#39;hat_tau1_X&#39;] = tau1_X.predict(df[dgp.X + [&#39;T&#39;]])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally, we estimate the propensity score.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from sklearn.linear_model import LogisticRegression as lr

df[&#39;hat_e&#39;] = lr().fit(df[dgp.X], df[&#39;T&#39;]).predict_proba(df[dgp.X])[:,1]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we can compute the &lt;strong&gt;X-learner estimate&lt;/strong&gt; as&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df[&#39;hat_tau_X&#39;] = df[&#39;hat_e&#39;] * df[&#39;hat_tau0_X&#39;] + (1-df[&#39;hat_e&#39;]) * df[&#39;hat_tau1_X&#39;]
print(f&amp;quot;X-learner estimate : {np.mean(df[&#39;hat_tau_X&#39;]):.4}&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;X-learner estimate : 0.5253
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can plot the distribution of treatment effect estimates against the true values.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;X_plot = sns.scatterplot(data=df, x=&#39;x3&#39;, y=&#39;hat_tau_X&#39;, alpha=0.3);
sns.scatterplot(data=df, x=&#39;x3&#39;, y=&#39;tau&#39;, color=&#39;C2&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/meta_learners_43_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;The X-learner estimator is heavily superior to both the S-learner and the T-learner. This is particularly evident if we combine all the plots.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;fig, axs = plt.subplots(1,3, sharex=True, sharey=True, figsize=(20,6))
for i, l in enumerate([&#39;S&#39;, &#39;T&#39;, &#39;X&#39;]):
    sns.scatterplot(data=df, x=&#39;x3&#39;, y=f&amp;quot;hat_tau_{l}&amp;quot;, alpha=0.3, ax=axs[i]);
    sns.scatterplot(data=df, x=&#39;x3&#39;, y=&#39;tau&#39;, color=&#39;C2&#39;, ax=axs[i]).\
    set(title=f&amp;quot;{l}-learner&amp;quot;, ylabel=&#39;&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/meta_learners_45_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Meta learners: &lt;a href=&#34;https://www.pnas.org/doi/abs/10.1073/pnas.1804597116&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Metalearners for estimating heterogeneous treatment effects using machine learning&lt;/a&gt; (2019) by KÃ¼nzel, Sekhon, Bickel, and Yu&lt;/li&gt;
&lt;li&gt;Taxonomy of methods: &lt;a href=&#34;https://www.pnas.org/doi/abs/10.1073/pnas.1510489113&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Recursive partitioning for heterogeneous causal effects&lt;/a&gt; (2016) by Athey and Imbens&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=N9ThAs7NS0g&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Video lecture&lt;/a&gt; by Stefan Wager (Stanford)&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Permutation Tests for Dummies</title>
      <link>https://matteocourthoud.github.io/post/permutation_test/</link>
      <pubDate>Fri, 08 Apr 2022 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/post/permutation_test/</guid>
      <description>&lt;p&gt;If you search &amp;ldquo;permutation test&amp;rdquo; on Wikipedia, you get the following definition:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;A permutation test (also called re-randomization test) is an exact statistical hypothesis test making use of the proof by contradiction in which the distribution of the test statistic under the null hypothesis is obtained by calculating all possible values of the test statistic under possible rearrangements of the observed data.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;What does it mean? In this tutorial we are going to see in detail what this definition means, how to implement permutation tests, and their pitfalls.&lt;/p&gt;
&lt;h2 id=&#34;example-1-is-a-coin-fair&#34;&gt;Example 1: is a coin fair?&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s start with an example: suppose you wanted to test whether a coin is fair. You throw the coin 10 times and you count the number of times you get heads. Let&amp;rsquo;s simulate the outcome.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import numpy as np

np.random.seed(1)
np.random.binomial(1, 0.5, 10)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;array([0, 1, 0, 0, 0, 0, 0, 0, 0, 1])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Out of 10 coin throws, we got only 2 heads. Does it mean that the coin is not fair?&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;question&lt;/strong&gt; that permutation testing is trying to answer is &amp;ldquo;&lt;em&gt;how unlikely is the observed outcome under the null hypothesis that the coin is fair?&lt;/em&gt;&amp;rdquo;.&lt;/p&gt;
&lt;p&gt;In this case we can directly compute this answer since we have a very little number of throws. The total number of outcomes is $2^{10}$. The number of as or more extreme outcomes, under the assumption that the coin is fair (50-50) is&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;0 heads: ${10 \choose 0} = 1$&lt;/li&gt;
&lt;li&gt;1 head: ${10 \choose 1} = 10$&lt;/li&gt;
&lt;li&gt;2 heads: ${10 \choose 2} = 45$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;So that the probability of getting the same or a more extreme outcome is&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from scipy.special import comb

(comb(10, 0) + comb(10, 1) + comb(10, 2)) / 2**10
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;0.0546875
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This probability seems low but not too low.&lt;/p&gt;
&lt;p&gt;However, we have forgot one thing. We want to test whether the coin is fair in &lt;strong&gt;either&lt;/strong&gt; direction. We would suspect that the coin is unfair if we were getting few heads (as we did), but also if we were getting many heads. Therefore, we should account for both extremes.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sum([comb(10, i) for i in [0, 1, 2, 8, 9, 10]]) / 2**10
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;0.109375
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This number should not be surprising since it&amp;rsquo;s exactly double the previous one.&lt;/p&gt;
&lt;p&gt;It is common in statistics to say that an event is unusual if its probability is less than 1 in 20, i.e. $5%$. If we were adopting that threshold, we would not conclude that getting 2 heads in 10 trows is so unusual. However, getting just one, would be.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sum([comb(10, i) for i in [0, 1, 9, 10]]) / 2**10
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;0.021484375
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;hypothesis-testing&#34;&gt;Hypothesis Testing&lt;/h2&gt;
&lt;p&gt;The process we just went through is called &lt;strong&gt;hypothesis testing&lt;/strong&gt;. The components of an hypothesis test are:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A null hypothesis $H_0$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;in our case, that the coin war fair&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;A test statistic $t$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;in our case, the number of zeros&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;A level of significance $\alpha$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;it is common to choose 5%&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The &lt;strong&gt;idea&lt;/strong&gt; behind &lt;strong&gt;permutation testing&lt;/strong&gt; is the following: in a setting in which we are checking whether one variable has an effect on another variable, the two variables should not be correlated, under the null hypothesis . Therefore, we could re-shuffle the treatment variable and re-compute the test statistic. Lastly, we can compute the p-value as the fraction of as or more extremes outcomes under re-shuffling of the data.&lt;/p&gt;
&lt;h2 id=&#34;example-2-are-women-smarter&#34;&gt;Example 2: are women smarter?&lt;/h2&gt;
&lt;p&gt;Suppose now we were interested in knowing whether females perform better in a test than men. Let&amp;rsquo;s start by writing the data generating process under the assumption of no difference in scores. However, only 30% of the sample will be female.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import pandas as pd

# Data generating process
def generate_data_gender(N=100, seed=1):
    np.random.seed(seed) # Set seed for replicability
    data = pd.DataFrame({&amp;quot;female&amp;quot;: np.random.binomial(1, 0.3, N),
                         &amp;quot;test_score&amp;quot;: np.random.exponential(3, N)})
    return data
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s now generate a sample of size 100.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Generate data
data_gender = generate_data_gender()
data_gender.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;female&lt;/th&gt;
      &lt;th&gt;test_score&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1.186447&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;2.246348&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;6.513147&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1.326091&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;7.175402&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;We can compute the treatment effect by computing the difference in mean outcomes between male and females.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def compute_score_diff(data):
    T = np.mean(data.loc[data[&#39;female&#39;]==1, &#39;test_score&#39;]) - np.mean(data.loc[data[&#39;female&#39;]==0, &#39;test_score&#39;])
    return T
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;T = compute_score_diff(data_gender)
print(f&amp;quot;The estimated treatment effect is {T}&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;The estimated treatment effect is -1.3612262580563321
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It looks that females actually did worse than males. But is the difference statistically significant? We can perform a randomization test and compute the probability of observing a more extreme outcome.&lt;/p&gt;
&lt;p&gt;First, let&amp;rsquo;s write the permutation routine that takes a variable in the data and permutes it.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def permute(data, var, r):
    temp_data = data.copy()
    temp_data[var] = np.random.choice(data[var], size=len(data), replace=r)
    return temp_data
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can now write the permutation test. It spits out a vector of statistics and prints the implied p-value.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def permutation_test(data, permute, var, compute_stat, K=1000, r=False):
    T = compute_stat(data)
    T_perm = []
    for k in range(K):
        temp_data = permute(data, var, r)
        T_perm += [compute_stat(temp_data)]
    print(f&amp;quot;The p-value is {sum(np.abs(T_perm) &amp;gt;= np.abs(T))/K}&amp;quot;)
    return T_perm
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;Ts = permutation_test(data_gender, permute, &#39;test_score&#39;, compute_score_diff)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;The p-value is 0.063
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Apparently the result we have observed was quite unusual, but not at the 5% level. We can plot the distribution of statistics to visualize this result.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;%matplotlib inline
%config InlineBackend.figure_format = &#39;retina&#39;
from src.utils import *
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def plot_test(T, Ts, title):
    plt.hist(Ts, density=True, bins=30, alpha=0.7, color=&#39;C0&#39;)
    plt.vlines([-T, T], ymin=plt.ylim()[0], ymax=plt.ylim()[1], color=&#39;C2&#39;)
    plt.title(title);
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plot_test(T, Ts, &#39;Distribution of score differences under permutation&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/permutation_test_29_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;As we can see, the observed difference in scores is quite extreme with respect the distribution generate by the permutation.&lt;/p&gt;
&lt;p&gt;One &lt;strong&gt;issue&lt;/strong&gt; with the permutation test we just ran is that it is computationally expensive to draw without replacement. The standard and much faster procedure is to draw without replacement.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;Ts_repl = permutation_test(data_gender, permute, &#39;test_score&#39;, compute_score_diff, r=True)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;The p-value is 0.052
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The p-value is virtually the same.&lt;/p&gt;
&lt;p&gt;How &lt;strong&gt;accurate&lt;/strong&gt; is the test? Since we have access to the data generating process, we can compute the true p-value via simulation. We draw many samples from the true data generating process and, for each, compute the difference in scores. The simulated p-value is going to be the frequency of more extreme statistics.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Function to simulate data and compute pvalue
def simulate_stat(dgp, compute_stat, K=1000):
    T = compute_stat(dgp())
    T_sim = []
    for k in range(K):
        data = dgp(seed=k)
        T_sim += [compute_stat(data)]
    print(f&amp;quot;The p-value is {sum(np.abs(T_sim) &amp;gt;= np.abs(T))/K}&amp;quot;)
    return np.array(T_sim)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;T_sim = simulate_stat(generate_data_gender, compute_score_diff)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;The p-value is 0.038
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Again, we can plot the distribution of simulated statistics to understand the computed p-value.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plot_test(T, T_sim, &#39;Distribution of score differences under simulation&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/permutation_test_36_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;As expected, most of the mass lies within the interval, indicating a relatively extreme result. We have just been &amp;ldquo;unlucky&amp;rdquo; with the draw, but the permutation test was accurate.&lt;/p&gt;
&lt;h2 id=&#34;permutation-tests-vs-t-tests&#34;&gt;Permutation tests vs t-tests&lt;/h2&gt;
&lt;p&gt;What is the difference between a t-test and a permutation test?&lt;/p&gt;
&lt;p&gt;Permutation test &lt;strong&gt;advantages&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;does not make distributional assumptions&lt;/li&gt;
&lt;li&gt;not sensible to outliers&lt;/li&gt;
&lt;li&gt;can be computed also for statistics whose distribution is not known&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Permutation test &lt;strong&gt;disadvantages&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;computationally intense&lt;/li&gt;
&lt;li&gt;very sample-dependent&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;example-3-is-university-worth&#34;&gt;Example 3: is university worth?&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s now switch to a new example to compare t-tests and permutation tests.&lt;/p&gt;
&lt;p&gt;Assume we want to check whether university is a worthy investment. We have information about whether individuals attended university and their future salary. The problem here is that income is a particularly &lt;strong&gt;skewed&lt;/strong&gt; variable.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Data generating process
def generate_data_income(N=1000, seed=1):
    np.random.seed(seed) # Set seed for replicability
    university = np.random.binomial(1, 0.5, N) # Treatment
    data = pd.DataFrame({&amp;quot;university&amp;quot;: university,
                         &amp;quot;income&amp;quot;: np.random.lognormal(university, 2.3, N)})
    return data
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;data_income = generate_data_income()
data_income.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;university&lt;/th&gt;
      &lt;th&gt;income&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;5.305618&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1.289598&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;6.507720&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;6.019961&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0.034482&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;The distribution of income is very heavy tailed. Let&amp;rsquo;s plot its density across the two groups.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sns.kdeplot(data=data_income, x=&amp;quot;income&amp;quot;, hue=&amp;quot;university&amp;quot;)\
.set(title=&#39;Income density by group&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/permutation_test_45_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;The distribution is so skewed that we cannot actually visually perceive differences between the two groups. Let&amp;rsquo;s compute the expected difference.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def compute_income_diff(data):
    T = np.mean(data.loc[data[&#39;university&#39;]==1, &#39;income&#39;]) - np.mean(data.loc[data[&#39;university&#39;]==0, &#39;income&#39;])
    return T
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;T = compute_income_diff(data_income)
T
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;23.546974435985444
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It looks like university graduates have higher income. Is this difference statistically different from zero? Let&amp;rsquo;s perform a permutation test.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;T_perm = permutation_test(data_income, permute, &#39;university&#39;, compute_income_diff)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;The p-value is 0.011
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The permutation test is telling us that the difference is extremely unusual under the null hypothesis. In other words, it is very unlikely that university graduates earn the same income of non-university graduates.&lt;/p&gt;
&lt;p&gt;What would be the outcome of a standard t-test?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from scipy.stats import ttest_ind

ttest_ind(data_income.query(&#39;university==1&#39;)[&#39;income&#39;], data_income.query(&#39;university==0&#39;)[&#39;income&#39;])
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Ttest_indResult(statistic=1.5589492598056494, pvalue=0.1193254252009701)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As we can see, the two tests provide extremely different results. The t-test is much more conservative, telling us that the unlikeliness of the data is just $12%$ compared to the $1.1%$ of the permutation test.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;reason&lt;/strong&gt; is that we have extremely skewed data. The t-test is very sensible to extreme observation and will therefore compute a very high variance because of very few data points.&lt;/p&gt;
&lt;p&gt;The permutation test can further address the problem of a skewed outcome distribution by using a test statistic that is more &lt;strong&gt;sensible to outliers&lt;/strong&gt;. Let&amp;rsquo;s perform the permutation test using the &lt;strong&gt;trimmed mean&lt;/strong&gt; instead of the mean.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from scipy.stats import trim_mean

def compute_income_mediandiff(data):
    T = np.median(data.loc[data[&#39;university&#39;]==1, &#39;income&#39;]) - np.median(data.loc[data[&#39;university&#39;]==0, &#39;income&#39;])
    return T
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;T_perm = permutation_test(data_income, permute, &#39;university&#39;, compute_income_mediandiff)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;The p-value is 0.0
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In this case, the permutation test is extremely confident that the trimmed mean of the two groups is different.&lt;/p&gt;
&lt;p&gt;However, an advantage of the t-test is &lt;strong&gt;speed&lt;/strong&gt;. Let&amp;rsquo;s compare the two tests by computing their execution time. Note that this is just a rough approximation since the permutation test could be sensible optimized.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import time

# No replacement
start = time.time()
permutation_test(data_income, permute, &#39;university&#39;, compute_income_diff)
print(f&amp;quot;Elapsed time without replacement: {time.time() - start}&amp;quot;)

# Replacement
start = time.time()
ttest_ind(data_income.query(&#39;university==1&#39;)[&#39;income&#39;], data_income.query(&#39;university==0&#39;)[&#39;income&#39;])
print(f&amp;quot;Elapsed time with replacement: {time.time() - start}&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;The p-value is 0.016
Elapsed time without replacement: 0.28911614418029785
Elapsed time with replacement: 0.00125885009765625
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The permutation test is 300 times slower. This can be a particularly relevant difference for larger sample sizes.&lt;/p&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;In this tutorial, we have seen how to perform permutation tests across different data generating processes.&lt;/p&gt;
&lt;p&gt;The underlying principle is the same: permute an variable that is assumed to be random under the null hypothesis and re-compute the test statistic. Then check how unusual was the test statistic in the original dataset.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Policy Learning</title>
      <link>https://matteocourthoud.github.io/post/policy_learning/</link>
      <pubDate>Fri, 08 Apr 2022 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/post/policy_learning/</guid>
      <description>&lt;p&gt;In this tutorial, we are going to see how to design the most welfare-improving policy in presence of treatment effect heterogeneity and treatment costs or budget constraints.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Requisites&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;For this tutorial, I assume you are familiar with the following concepts:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Rubin&amp;rsquo;s potential outcome framework&lt;/li&gt;
&lt;li&gt;Propensity weighting or uplifting&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://matteocourthoud.github.io/post/aipw/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;AIPW or Double Robust Estimators&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://matteocourthoud.github.io/post/causal_trees/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Causal Trees&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Academic Replication&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;We are going to replicate the paper by &lt;a href=&#34;https://www.aeaweb.org/articles?id=10.1257/jep.32.4.201&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Hanna and Olken (2018)&lt;/a&gt; in which the authors study the optimal allocation of a cash transfer to households in Peru. We slightly twist the original paper by actually assigning the transfer and assuming 100% consumption.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Business Case&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;We are going to study a company that has to decide which consumers to target with ads.&lt;/p&gt;
&lt;h2 id=&#34;setting&#34;&gt;Setting&lt;/h2&gt;
&lt;p&gt;We assume that for a set of i.i.d. subjects $i = 1, &amp;hellip;, n$ we observed a tuple $(X_i, T_i, Y_i)$ comprised of&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;a feature vector $X_i \in \mathbb R^n$&lt;/li&gt;
&lt;li&gt;a treatment variable $T_i \in \lbrace 0, 1 \rbrace$&lt;/li&gt;
&lt;li&gt;a response $Y_i \in \mathbb R$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Assumption 1 : unconfoundedness&lt;/strong&gt; (or ignorability, or selection on observables)&lt;/p&gt;
&lt;p&gt;$$
\big \lbrace Y_i^{(1)} , Y_i^{(0)} \big \rbrace \ \perp \ T_i \ | \ X_i
$$&lt;/p&gt;
&lt;p&gt;i.e. conditional on observable characteristics $X$, the treatment assignment $T$ is as good as random.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Assumption 2: overlap&lt;/strong&gt; (or bounded support)&lt;/p&gt;
&lt;p&gt;$$
\exists \eta &amp;gt; 0 \ : \ \eta \leq \mathbb E \left[ T_i = 1 \ \big | \ X_i = x \right] \leq 1-\eta
$$&lt;/p&gt;
&lt;p&gt;i.e. no observation is deterministically assigned to the treatment or control group.&lt;/p&gt;
&lt;h2 id=&#34;policy-learning&#34;&gt;Policy Learning&lt;/h2&gt;
&lt;p&gt;The objective of policy learning is to decide which people to treat. More explicitly, we want to learn a map from observable characteristics to a (usually binary) policy space.&lt;/p&gt;
&lt;p&gt;$$
\pi : \mathcal X \to \lbrace 0, 1 \rbrace
$$&lt;/p&gt;
&lt;p&gt;Policy learning is closely related to the &lt;strong&gt;estimation of heterogeneous treatment effects&lt;/strong&gt;. In fact, in both settings, we want to investigate how the treatment affects different individuals in different ways.&lt;/p&gt;
&lt;p&gt;The main &lt;strong&gt;difference&lt;/strong&gt; between policy learning and the estimation of heterogeneous treatment effects is the objective function. In policy learning, we are acting in a limited resources setting where providing treatment is costly and the cost could depend on individual characteristics. For example, it might be more costly to vaccinate individuals that live in remote areas. Therefore, one might not just want to treat individuals with the largest expected treatment effect, but the ones for whom treatment is most cost-effective.&lt;/p&gt;
&lt;p&gt;The utilitarian &lt;strong&gt;value&lt;/strong&gt; of a policy $\pi$&lt;/p&gt;
&lt;p&gt;$$
V(\pi) = \mathbb E \Big[ Y_i(\pi(X_i)) \Big] = \mathbb E \big[ Y^{(0)}_i \big] + \mathbb E \big[ \tau(X_i) \pi(X_i) \big]
$$&lt;/p&gt;
&lt;p&gt;measures the expectation of the potential outcome $Y$ if we were to &lt;strong&gt;assign&lt;/strong&gt; treatment $T$ according to policy $\pi$. This expectation can be split into &lt;strong&gt;two parts&lt;/strong&gt;:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;The baseline expected potential outcome $\mathbb E \big[ Y^{(0)}_i \big]$&lt;/li&gt;
&lt;li&gt;The expected effect of the policy $\mathbb E \big[ \tau(X_i) \pi(X_i) \big]$&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The &lt;strong&gt;objective&lt;/strong&gt; of policy learning is to learn a policy with high value $V(\pi)$. As part (2) of the formula makes clear, you get a higher value if you treat the people with a high treatment effect $\tau(x)$.&lt;/p&gt;
&lt;p&gt;A &lt;strong&gt;simple approach&lt;/strong&gt; could be to assign treatment according to a &lt;strong&gt;thresholding rule&lt;/strong&gt; $\tau(x) &amp;gt; c$, where $c$ is some cost below which is not worth treating individuals (or there is not enough budget).&lt;/p&gt;
&lt;p&gt;However, estimating the conditional average treatment effect (CATE) function $\tau(x)$ and learning a good policy $\pi(x)$ are different &lt;strong&gt;problems&lt;/strong&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the correct loss function for policy learning is not the mean squared error (MSE) on $\tau(x)$
&lt;ul&gt;
&lt;li&gt;we want to &lt;strong&gt;maximize welfare&lt;/strong&gt;!&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;the CATE function $\tau(x)$ might not use some features for targeting
&lt;ul&gt;
&lt;li&gt;e.g. &lt;strong&gt;cannot discriminate&lt;/strong&gt; based on race or gender&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;you don&amp;rsquo;t want to have feature that people can influence
&lt;ul&gt;
&lt;li&gt;e.g. use a self-reported measure that people can distort&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We would like to find a &lt;strong&gt;loss function&lt;/strong&gt; $L(\pi ; Y_i, X_i, T_i)$ such that&lt;/p&gt;
&lt;p&gt;$$
\mathbb E \big[ L(\pi ; Y_i, X_i, T_i) \big] = - V(\pi)
$$&lt;/p&gt;
&lt;h3 id=&#34;ipw-loss&#34;&gt;IPW Loss&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://onlinelibrary.wiley.com/doi/abs/10.3982/ECTA13288&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Kitagawa and Tenenov (2018)&lt;/a&gt; propose to learn an empirical estimate of the value function using inverse propensity weighting (IPW).&lt;/p&gt;
&lt;p&gt;$$
\hat \pi = \arg \max_{\pi} \Big\lbrace \hat V(\pi) : \pi \in \Pi \Big\rbrace
$$&lt;/p&gt;
&lt;p&gt;where&lt;/p&gt;
&lt;p&gt;$$
\hat V(\pi) = \frac{ \mathbb I \big(\lbrace T_i = \pi(X_i) \rbrace \big) }{ \mathbb P \big[ \lbrace T_i = \pi(X_i) \rbrace \ \big| \ X_i \big] } Y_i
$$&lt;/p&gt;
&lt;p&gt;The authors show that under &lt;strong&gt;unconfoundedness&lt;/strong&gt;, if the propensity score $e(x)$ is known and $\Pi$ is not too complex, the value of the estimated policy converges to the optimal value.&lt;/p&gt;
&lt;p&gt;Note that this is a &lt;strong&gt;very different problem&lt;/strong&gt; from the normal optimization problem with a MSE loss. In fact, we now have a binary argument in the loss function which makes the problem similar to a classification problem, in which we want to classify people into &lt;em&gt;high gain&lt;/em&gt; and &lt;em&gt;low gain&lt;/em&gt; categories.&lt;/p&gt;
&lt;h3 id=&#34;aipw-loss&#34;&gt;AIPW Loss&lt;/h3&gt;
&lt;p&gt;If propensity score $e(x)$ is not known, we can use a &lt;strong&gt;doubly robust estimator&lt;/strong&gt;, exactly as for the average treatment effect.&lt;/p&gt;
&lt;p&gt;$$
\hat V = \frac{1}{n} \sum_{i=1}^{n}
\begin{cases}
\hat \Gamma_i \quad &amp;amp;\text{if} \quad \pi(X_i) = 1
\newline&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;\hat \Gamma_i \quad &amp;amp;\text{if} \quad \pi(X_i) = 0
\end{cases}
$$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;where&lt;/p&gt;
&lt;p&gt;$$
\hat \Gamma_i = \hat \mu^{(1)}(X_i) - \hat \mu^{(0)}(X_i) + \frac{T_i }{\hat e(X_i)} \left( Y_i - \hat \mu^{(1)}(X_i) \right) - \frac{(1-T_i) }{1-\hat e(X_i)} \left( Y_i - \hat \mu^{(0)}(X_i) \right)
$$&lt;/p&gt;
&lt;p&gt;The relationship with AIPW is that $\hat \tau_{AIPW} = \frac{1}{n} \sum_{i=1}^{n} \hat \Gamma_i$. Therefore, the objective function $V(\pi)$ is build so that when we assign treatment to a unit we &amp;ldquo;gain&amp;rdquo; the double-robust score $\hat \tau_{AIPW}$, while, if we do not assign treatment, we &amp;ldquo;pay&amp;rdquo; the double-robust score $\hat \tau_{AIPW}$.&lt;/p&gt;
&lt;h2 id=&#34;academic-application&#34;&gt;Academic Application&lt;/h2&gt;
&lt;p&gt;For the academic applicaiton, we are going to replicate the paper by &lt;a href=&#34;https://www.aeaweb.org/articles?id=10.1257/jep.32.4.201&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Hanna and Olken (2018)&lt;/a&gt; in which the authors study the optimal allocation of a cash transfer to households in Peru. We slightly twist the original paper by actually assigning the transfer and assuming 100% consumption.&lt;/p&gt;
&lt;p&gt;First, let&amp;rsquo;s load the modified dataset.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from src.utils import *
from src.dgp import dgp_ao18
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;%matplotlib inline
%config InlineBackend.figure_format = &#39;retina&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;dgp = dgp_ao18()
df = dgp.import_data()
df.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;Unnamed: 0&lt;/th&gt;
      &lt;th&gt;d_fuel_other&lt;/th&gt;
      &lt;th&gt;d_fuel_wood&lt;/th&gt;
      &lt;th&gt;d_fuel_coal&lt;/th&gt;
      &lt;th&gt;d_fuel_kerosene&lt;/th&gt;
      &lt;th&gt;d_fuel_gas&lt;/th&gt;
      &lt;th&gt;d_fuel_electric&lt;/th&gt;
      &lt;th&gt;d_fuel_none&lt;/th&gt;
      &lt;th&gt;d_water_other&lt;/th&gt;
      &lt;th&gt;d_water_river&lt;/th&gt;
      &lt;th&gt;...&lt;/th&gt;
      &lt;th&gt;d_lux_1&lt;/th&gt;
      &lt;th&gt;d_lux_2&lt;/th&gt;
      &lt;th&gt;d_lux_3&lt;/th&gt;
      &lt;th&gt;d_lux_4&lt;/th&gt;
      &lt;th&gt;d_lux_5&lt;/th&gt;
      &lt;th&gt;training&lt;/th&gt;
      &lt;th&gt;h_hhsize&lt;/th&gt;
      &lt;th&gt;cash_transfer&lt;/th&gt;
      &lt;th&gt;consumption&lt;/th&gt;
      &lt;th&gt;welfare&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;211.0000&lt;/td&gt;
      &lt;td&gt;5.351858&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;420.1389&lt;/td&gt;
      &lt;td&gt;6.040585&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;390.8318&lt;/td&gt;
      &lt;td&gt;5.968277&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;9&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;285.6018&lt;/td&gt;
      &lt;td&gt;5.654599&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;8&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;118.0713&lt;/td&gt;
      &lt;td&gt;4.771289&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;5 rows Ã 78 columns&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;As we can see, we have a lot of information about individuals in Peru. Crucially for the research question, we observe&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;whether the household received a cash transfer, &lt;code&gt;cash_transfer&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;the household&amp;rsquo;s welfare afterwards, &lt;code&gt;welfare_post&lt;/code&gt;
&lt;ul&gt;
&lt;li&gt;assuming
$$
\text{welfare} = \log (\text{consumption})
$$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We would like to understand which individuals should be given a transfer, given that the transfer is costly. Let&amp;rsquo;s assume the transfer costs $0.3$ units of welfare.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from econml.policy import DRPolicyForest

cost = 0.3
policy = DRPolicyForest(random_state=1).fit(Y=df[dgp.Y] - cost*df[dgp.T], T=df[dgp.T], X=df[dgp.X])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can partially visualize the policy by plotting a regression tree for the most important features.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;%matplotlib inline
policy.plot(tree_id=1, max_depth=2, feature_names=dgp.X, fontsize=8)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/policy_learning_18_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;To understand if the estimated policy was effective, we can load the oracle dataset, with the potential outcomes.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_oracle = dgp.import_data(oracle=True)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;From the oracle dataset, we can compute the actual value of the policy.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;T_hat = policy.predict(df[dgp.X])
V_policy = (df_oracle[&#39;welfare_1&#39;].values - cost - df_oracle[&#39;welfare_0&#39;].values) * T_hat
print(f&#39;Estimated policy value (N_T={sum(T_hat)}): {np.mean(V_policy) :.4}&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Estimated policy value (N_T=21401): 0.05897
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The value is positive, indicating that the treatment was effective. But how well did we do? We can compare the estimated policy with the oracle policy that assign treatment to each cost-effective unit.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;T_oracle = (df_oracle[&#39;welfare_1&#39;] - df_oracle[&#39;welfare_0&#39;]) &amp;gt; cost
V_oracle = (df_oracle[&#39;welfare_1&#39;] - cost - df_oracle[&#39;welfare_0&#39;] ) * T_oracle
print(f&#39;Oracle policy value (N_T={sum(T_oracle)}): {np.mean(V_oracle) :.4}&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Oracle policy value (N_T=17630): 0.07494
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We actually achieved 79% of the potential policy gains! Also note that our policy is too generous, treating more units than optimal. But how well would we have done if the same amount of cash transfers were given at random?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;T_rand = np.random.binomial(1, sum(T_hat)/len(df), len(df))
V_rand = (df_oracle[&#39;welfare_1&#39;] - cost - df_oracle[&#39;welfare_0&#39;] ) * T_rand
print(f&#39;Random policy value (N_T={sum(T_rand)}): {np.mean(V_rand) :.4}&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Random policy value (N_T=21359): 0.0002698
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;A random assignment of the same amount of cash transfers would not achieve any effect. However, this assumes that we already know the optimal amount of funds to distribute. What if instead we had treated everyone?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;V_all = (df_oracle[&#39;welfare_1&#39;] - cost - df_oracle[&#39;welfare_0&#39;] )
print(f&#39;All-treated policy value (N_T={len(df)}): {np.mean(V_all) :.4}&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;All-treated policy value (N_T=45378): 0.0004019
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Indiscriminate treatment would again not achieve any effect. Lastly, what if we had just estimated the treatment effect using AIPW and used it as a threshold?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from econml.dr import LinearDRLearner

model = LinearDRLearner(random_state=1).fit(Y=df[dgp.Y], T=df[dgp.T], X=df[dgp.X])
T_ipw = model.effect(X=df[dgp.X], T0=0, T1=1) &amp;gt; cost
V_ipw = (df_oracle[&#39;welfare_1&#39;] - cost - df_oracle[&#39;welfare_0&#39;] ) * T_ipw
print(f&#39;IPW policy value (N_T={sum(T_ipw)}): {np.mean(V_ipw) :.4}&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;IPW policy value (N_T=21003): 0.06293
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We are actually doing better! Weird&amp;hellip;&lt;/p&gt;
&lt;h2 id=&#34;business-case&#34;&gt;Business Case&lt;/h2&gt;
&lt;p&gt;We are given the following problem:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;A firm would like to understand which customers to show an ad, in order to increase revenue. The firm ran a A/B test showing a random sample of customers an ad. First, try to understand if there is heterogeneity in treatment. Then, decide which customers to show the ad, given that ads are costly (1$ each). Further suppose that you cannot discriminate on gender. How do the results change?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;We start by drawing a sample from the data generating process.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from src.utils import *
from src.dgp import dgp_ad
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;dgp = dgp_ad()
df = dgp.generate_data()
df.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;male&lt;/th&gt;
      &lt;th&gt;black&lt;/th&gt;
      &lt;th&gt;age&lt;/th&gt;
      &lt;th&gt;educ&lt;/th&gt;
      &lt;th&gt;ad&lt;/th&gt;
      &lt;th&gt;revenue&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;55.0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;-0.327221&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;47.0&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;0.659393&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;31.0&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;True&lt;/td&gt;
      &lt;td&gt;2.805178&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;51.0&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;-0.508548&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;48.0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;True&lt;/td&gt;
      &lt;td&gt;0.762280&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;We have information on the number of pages visited in the previous month, whether the user is located in the US, whether it connects by mobile and the revenue pre-intervention.&lt;/p&gt;
&lt;p&gt;We are going to use the &lt;a href=&#34;https://econml.azurewebsites.net&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;econml&lt;/code&gt;&lt;/a&gt; library to estimate the treatment effects. First, we use the &lt;code&gt;DRLearner&lt;/code&gt; library to estimate heterogeneous treatment effects using a double robust estimator. We can specify both the &lt;code&gt;model_propensity&lt;/code&gt; for $e(x)$ and the &lt;code&gt;model_regression&lt;/code&gt; for $\mu(x)$.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from econml.dr import DRLearner

model = DRLearner(random_state=1).fit(Y=df[dgp.Y], T=df[dgp.T], X=df[dgp.X]);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can plot a visual representation of the treatment effect heterogeneity using the &lt;code&gt;SingleTreePolicyInterpreter&lt;/code&gt; function, which infers a tree representation of the treatment effects learned from another model.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from econml.cate_interpreter import SingleTreeCateInterpreter

SingleTreeCateInterpreter(max_depth=2, random_state=1).interpret(model, X=df[dgp.X]).plot(feature_names=dgp.X)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/policy_learning_39_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;It seems that the most relevant dimension of treatment heterogeneity is &lt;code&gt;education&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;We can now use policy learning to estimate a treatment policy. We use the &lt;code&gt;DRPolicyTree&lt;/code&gt; from the &lt;code&gt;econml&lt;/code&gt; package.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from econml.policy import DRPolicyTree

policy = DRPolicyTree(random_state=1, max_depth=2).fit(Y=df[dgp.Y], T=df[dgp.T], X=df[dgp.X])
policy.plot(feature_names=dgp.X)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/policy_learning_41_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;We will now assume that the treatment is costly.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;cost = 1
policy = DRPolicyTree(random_state=1, max_depth=2).fit(Y=df[dgp.Y]-cost*df[dgp.T], T=df[dgp.T], X=df[dgp.X])
policy.plot(feature_names=dgp.X)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/policy_learning_43_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;As we can see, the model decides to use race to discriminate treatment. However, let&amp;rsquo;s now suppose we cannot discriminate on race and gender.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;X_short = [&#39;age&#39;, &#39;educ&#39;]
policy = DRPolicyTree(random_state=1, max_depth=2).fit(Y=df[dgp.Y]-cost*df[dgp.T], T=df[dgp.T], X=df[X_short])
policy.plot(feature_names=X_short)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/policy_learning_45_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;In this case, the model uses education instead of race in order to assign treatment.&lt;/p&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://onlinelibrary.wiley.com/doi/abs/10.3982/ECTA13288&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Who Should Be Treated? Empirical Welfare Maximization Methods for Treatment Choice&lt;/a&gt; (2018) by Kitagawa and Tetenov&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://ideas.repec.org/p/ecl/stabus/3506.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Efficient Policy Learning&lt;/a&gt; (2017) by Athey and Wager&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=YQXRwvFQOPk&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Policy Learning&lt;/a&gt; video lecture by Stefan Wager (Stanford)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/microsoft/EconML/blob/main/notebooks/CustomerScenarios/Case%20Study%20-%20Customer%20Segmentation%20at%20An%20Online%20Media%20Company.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Customer Segmentation&lt;/a&gt; case study by EconML&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Propensity Score Matching</title>
      <link>https://matteocourthoud.github.io/post/propensity_score/</link>
      <pubDate>Fri, 08 Apr 2022 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/post/propensity_score/</guid>
      <description>&lt;p&gt;In this tutorial, we are going to see how to estimate causal effects when the treatment is not &lt;em&gt;unconditionally&lt;/em&gt; randomly assigned, but we need to condition on observable features in order to assume treatment exogeneity. This might happen either when an experiment is stratified or in observational studies.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Requisites&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;For this tutorial, I assume you are familiar with the following concepts:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Rubin&amp;rsquo;s potential outcome framework&lt;/li&gt;
&lt;li&gt;Ordinary least squares regression&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Academic Application&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;As an academic application, we are going to replicate &lt;a href=&#34;https://www.jstor.org/stable/2937954&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Evaluating the Econometric Evaluations of Training Programs with Experimental Data&lt;/a&gt; (1986) by Lalonde and the followup paper &lt;a href=&#34;https://www.tandfonline.com/doi/abs/10.1080/01621459.1999.10473858&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Causal Effects in Non-Experimental Studies: Reevaluating the Evaluation of Training Programs&lt;/a&gt; (1999) by Dahejia and Wahba. These papers study a randomized intervention providing work experienced to improve labor market outcomes.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Business Case&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;TBD&lt;/p&gt;
&lt;h2 id=&#34;setting&#34;&gt;Setting&lt;/h2&gt;
&lt;p&gt;We assume that for a set of i.i.d. subjects $i = 1, &amp;hellip;, n$ we observed a tuple $(X_i, T_i, Y_i)$ comprised of&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;a feature vector $X_i \in \mathbb R^n$&lt;/li&gt;
&lt;li&gt;a treatment assignment $T_i \in \lbrace 0, 1 \rbrace$&lt;/li&gt;
&lt;li&gt;a response $Y_i \in \mathbb R$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Assumption 1 : unconfoundedness&lt;/strong&gt; (or ignorability, or selection on observables, or conditional independence)&lt;/p&gt;
&lt;p&gt;$$
\big \lbrace Y_i^{(1)} , Y_i^{(0)} \big \rbrace \ \perp \ T_i \ | \ X_i
$$&lt;/p&gt;
&lt;p&gt;i.e. conditional on observable characteristics $X$, the treatment assignment $T$ is as good as random.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Assumption 2: overlap&lt;/strong&gt; (or bounded support)&lt;/p&gt;
&lt;p&gt;$$
\exists \eta &amp;gt; 0 \ : \ \eta \leq \mathbb E \left[ T_i = 1 \ \big | \ X_i = x \right] \leq 1-\eta
$$&lt;/p&gt;
&lt;p&gt;i.e. no observation is deterministically assigned to the treatment or control group.&lt;/p&gt;
&lt;h2 id=&#34;propensity-scores&#34;&gt;Propensity Scores&lt;/h2&gt;
&lt;h3 id=&#34;exogenous-treatment&#34;&gt;Exogenous Treatment&lt;/h3&gt;
&lt;p&gt;The &lt;strong&gt;fundamental problem of causal inference&lt;/strong&gt; is that we do not observe counterfactual outcomes, i.e. we do not observe what would have happened to treated units if they had not received the treatment and viceversa.&lt;/p&gt;
&lt;p&gt;If treatment is exogenous, we know that the difference in means identifies the average treatment effect $\mathbb E[\tau]$.&lt;/p&gt;
&lt;p&gt;$$
\mathbb E[\tau] = \mathbb E \big[ Y_i \ \big| \ T_i = 1 \big] - \mathbb E \big[ Y_i \ \big| \ T_i = 0 \big] = \mathbb E \big[ Y_{i}^{(1)} - Y_{i}^{(0)} \big]
$$&lt;/p&gt;
&lt;p&gt;Therefore, we can build an unbiased estimator of the average treatment effect as the empirical counterpart of the expression above&lt;/p&gt;
&lt;p&gt;$$
\hat \tau(Y, T) = \frac{1}{n} \sum_{i=1}^{n} \big( T_i Y_i - (1-T_i) Y_i \big)
$$&lt;/p&gt;
&lt;p&gt;In case treatment is not randomly assigned, we use the Thompson Horowitz (1952) estimator&lt;/p&gt;
&lt;p&gt;$$
\hat \tau(Y, T) = \frac{1}{n} \sum_{i=1}^{n} \left( \frac{T_i Y_i}{\pi_{i}} - \frac{(1-T_i) Y_i}{1 - \pi_{i}} \right)
$$&lt;/p&gt;
&lt;p&gt;where $\pi_{i} = \Pr(T_i=1)$ is the probability of being treated, also known as &lt;strong&gt;propensity score&lt;/strong&gt;. Sometimes the propensity score is known, for example when treatment is stratified. However, in general, it is not.&lt;/p&gt;
&lt;h3 id=&#34;conditionally-exogenous-treatment&#34;&gt;Conditionally Exogenous Treatment&lt;/h3&gt;
&lt;p&gt;In many cases and especially in observational studies, treatment is not unconditionally exogenous, but it&amp;rsquo;s exogenous only after we condition on some characteristic $X$. If these characteristics are observables, we have the &lt;strong&gt;unconfoundedness&lt;/strong&gt; assumption.&lt;/p&gt;
&lt;p&gt;Under unconfoundedness, we can still identify the average treatment effect, as a &lt;em&gt;conditional&lt;/em&gt; difference in means:&lt;/p&gt;
&lt;p&gt;$$
\mathbb E[\tau] = \mathbb E \big[ Y_{i}^{(1)} - Y_{i}^{(0)} \ \big| \ X_i \big]
$$&lt;/p&gt;
&lt;p&gt;The main problem is that we need to condition of the observables that actually make the unconfoundedness assumption hold. This might be tricky in two cases:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;when we have many observables&lt;/li&gt;
&lt;li&gt;when we do not know the functional form of the observables that we need to condition on&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The main contribution of Rosenbaum and Rubin (1983) is to show that if &lt;strong&gt;unconfoundedness&lt;/strong&gt; holds, then&lt;/p&gt;
&lt;p&gt;$$
\big \lbrace Y_i^{(1)} , Y_i^{(0)} \big \rbrace \ \perp \ T_i \ | \ \pi(X_i)
$$&lt;/p&gt;
&lt;p&gt;i.e. you only need to condition on $\pi(X)$ in order to recover the average treatment effect.&lt;/p&gt;
&lt;p&gt;$$
\mathbb E[\tau] = \mathbb E \big[ Y_{i}^{(1)} - Y_{i}^{(0)} \ \big| \ \pi(X_i) \big]
$$&lt;/p&gt;
&lt;p&gt;This implies the following &lt;strong&gt;inverse propensity-weighted&lt;/strong&gt; estimator:&lt;/p&gt;
&lt;p&gt;$$
\hat \tau^{IPW} = \frac{1}{n} \sum_{i=1}^{n} \left( \frac{T_i Y_i}{\hat \pi(X_i)} - \frac{(1-T_i) Y_i}{1 - \hat \pi(X_i)} \right)
$$&lt;/p&gt;
&lt;p&gt;which, under &lt;em&gt;unconfoundedness&lt;/em&gt; is an &lt;strong&gt;unbiased&lt;/strong&gt; estimator of the average treatment effect, $\mathbb E \left[\hat \tau^{IPW} \right] = \tau$.&lt;/p&gt;
&lt;p&gt;This is a very practically relevant result since it tells us that we need to condition on a single variable instead of a potentially infinite dimensional array. The only thing we need to do is to estimate $\pi(X_i)$.&lt;/p&gt;
&lt;h3 id=&#34;comments&#34;&gt;Comments&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Actual vs Estimated Scores&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Hirano and Ridder (2002) show that even when you know the true propensity score $\pi(X)$, it&amp;rsquo;s better to plug in the estimated propensity score $\hat \pi(X)$. Why? The idea is that the deviation between the actual and the estimated propensity score is providing some additional information. Therefore, it is best to use the actual fraction of treated rather than the theoretical one.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Propensity Scores and Regression&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;What is the difference between running a regression with controls vs doing propensity score matching?&lt;/p&gt;
&lt;p&gt;Aranow and Miller (2015) investigate this comparison in depth. First of all, whenever you are inserting &lt;strong&gt;control variables&lt;/strong&gt; in a regression, you are implicitly thinking about propensity scores. Both approaches are implicitly estimating counterfactual outcomes. Usually OLS extrapolates further away from the actual support than propensity score does.&lt;/p&gt;
&lt;p&gt;In the tweet (and its comments) below you can find further discussion and comments.&lt;/p&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;Thank you for tolerating such a vague poll question. &lt;br&gt;&lt;br&gt;Let me explain why I think this is a useful thing to bring front and certain, and highlight what I think is a flaw in how much of econometrics is taught, currently. &lt;br&gt;&lt;br&gt;1/n &lt;a href=&#34;https://t.co/Wm2jFereYO&#34;&gt;pic.twitter.com/Wm2jFereYO&lt;/a&gt;&lt;/p&gt;&amp;mdash; Paul Goldsmith-Pinkham (@paulgp) &lt;a href=&#34;https://twitter.com/paulgp/status/1470787510091632651?ref_src=twsrc%5Etfw&#34;&gt;December 14, 2021&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;

&lt;h2 id=&#34;academic-application&#34;&gt;Academic Application&lt;/h2&gt;
&lt;p&gt;As an academic application, we are going to replicate &lt;a href=&#34;https://www.tandfonline.com/doi/abs/10.1080/01621459.1999.10473858&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Causal Effects in Non-Experimental Studies: Reevaluating the Evaluation of Training Programs&lt;/a&gt; (1999) by Dahejia and Wahba.&lt;/p&gt;
&lt;p&gt;This study builds on a previous study: &lt;a href=&#34;https://www.jstor.org/stable/2937954&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Evaluating the Econometric Evaluations of Training Programs with Experimental Data&lt;/a&gt; (1986) by Lalonde. In this study, the author compares observational and experimental methods. In particular, he studies an experimental intervention called the NSW (National Supported Work demonstration). The NSW is a temporary training program to give work experience to unemployed people.&lt;/p&gt;
&lt;p&gt;The exogenous variation allows us to estimate the treatment effect as a difference in means. The author then asks: what if we didn&amp;rsquo;t have access to an experiment? In particular, what if we did not have information on the control group? He takes a sample of untreated people from the PSID panel and use them as a control group.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s start by loading the NSW data.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;%matplotlib inline
%config InlineBackend.figure_format = &#39;retina&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from src.utils import *
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_nsw = pd.read_csv(&#39;data/l86_nsw.csv&#39;)
df_nsw.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;treat&lt;/th&gt;
      &lt;th&gt;age&lt;/th&gt;
      &lt;th&gt;education&lt;/th&gt;
      &lt;th&gt;black&lt;/th&gt;
      &lt;th&gt;hispanic&lt;/th&gt;
      &lt;th&gt;married&lt;/th&gt;
      &lt;th&gt;nodegree&lt;/th&gt;
      &lt;th&gt;re74&lt;/th&gt;
      &lt;th&gt;re75&lt;/th&gt;
      &lt;th&gt;re78&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;37&lt;/td&gt;
      &lt;td&gt;11&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;9930.0460&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;22&lt;/td&gt;
      &lt;td&gt;9&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;3595.8940&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;30&lt;/td&gt;
      &lt;td&gt;12&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;24909.4500&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;27&lt;/td&gt;
      &lt;td&gt;11&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;7506.1460&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;33&lt;/td&gt;
      &lt;td&gt;8&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;289.7899&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;The treatment variable is &lt;code&gt;treat&lt;/code&gt; and the outcome of interest is &lt;code&gt;re78&lt;/code&gt;, the income in 1978. We also have access to a bunch of covariates.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;y = &#39;re78&#39;
T = &#39;treat&#39;
X = [&#39;age&#39;, &#39;education&#39;, &#39;black&#39;, &#39;hispanic&#39;, &#39;married&#39;, &#39;nodegree&#39;, &#39;re74&#39;, &#39;re75&#39;]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Was there selection on observables? Let&amp;rsquo;s summarize the data, according to treatment status.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_nsw.groupby(&#39;treat&#39;).agg([&#39;mean&#39;, &#39;std&#39;]).T.unstack(1)
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead tr th {
    text-align: left;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;treat&lt;/th&gt;
      &lt;th colspan=&#34;2&#34; halign=&#34;left&#34;&gt;0&lt;/th&gt;
      &lt;th colspan=&#34;2&#34; halign=&#34;left&#34;&gt;1&lt;/th&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;mean&lt;/th&gt;
      &lt;th&gt;std&lt;/th&gt;
      &lt;th&gt;mean&lt;/th&gt;
      &lt;th&gt;std&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;age&lt;/th&gt;
      &lt;td&gt;25.053846&lt;/td&gt;
      &lt;td&gt;7.057745&lt;/td&gt;
      &lt;td&gt;25.816216&lt;/td&gt;
      &lt;td&gt;7.155019&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;education&lt;/th&gt;
      &lt;td&gt;10.088462&lt;/td&gt;
      &lt;td&gt;1.614325&lt;/td&gt;
      &lt;td&gt;10.345946&lt;/td&gt;
      &lt;td&gt;2.010650&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;black&lt;/th&gt;
      &lt;td&gt;0.826923&lt;/td&gt;
      &lt;td&gt;0.379043&lt;/td&gt;
      &lt;td&gt;0.843243&lt;/td&gt;
      &lt;td&gt;0.364558&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;hispanic&lt;/th&gt;
      &lt;td&gt;0.107692&lt;/td&gt;
      &lt;td&gt;0.310589&lt;/td&gt;
      &lt;td&gt;0.059459&lt;/td&gt;
      &lt;td&gt;0.237124&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;married&lt;/th&gt;
      &lt;td&gt;0.153846&lt;/td&gt;
      &lt;td&gt;0.361497&lt;/td&gt;
      &lt;td&gt;0.189189&lt;/td&gt;
      &lt;td&gt;0.392722&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;nodegree&lt;/th&gt;
      &lt;td&gt;0.834615&lt;/td&gt;
      &lt;td&gt;0.372244&lt;/td&gt;
      &lt;td&gt;0.708108&lt;/td&gt;
      &lt;td&gt;0.455867&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;re74&lt;/th&gt;
      &lt;td&gt;2107.026658&lt;/td&gt;
      &lt;td&gt;5687.905694&lt;/td&gt;
      &lt;td&gt;2095.573689&lt;/td&gt;
      &lt;td&gt;4886.620353&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;re75&lt;/th&gt;
      &lt;td&gt;1266.909002&lt;/td&gt;
      &lt;td&gt;3102.982044&lt;/td&gt;
      &lt;td&gt;1532.055314&lt;/td&gt;
      &lt;td&gt;3219.250870&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;re78&lt;/th&gt;
      &lt;td&gt;4554.801126&lt;/td&gt;
      &lt;td&gt;5483.835991&lt;/td&gt;
      &lt;td&gt;6349.143530&lt;/td&gt;
      &lt;td&gt;7867.402218&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;It seems that covariates are balanced across treatment arms. Nothing seems to point towards selection on observables. Therefore, we can compute the average treatment effect as a simple difference in means&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_nsw.loc[df_nsw[T]==1, y].mean() - df_nsw.loc[df_nsw[T]==0, y].mean()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;1794.342404270271
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Or equivalently in a regression&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;est = smf.ols(&#39;re78 ~ treat&#39;, df_nsw).fit()
est.summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
      &lt;td&gt;&lt;/td&gt;         &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt; &lt;td&gt; 4554.8011&lt;/td&gt; &lt;td&gt;  408.046&lt;/td&gt; &lt;td&gt;   11.162&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt; 3752.855&lt;/td&gt; &lt;td&gt; 5356.747&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;treat&lt;/th&gt;     &lt;td&gt; 1794.3424&lt;/td&gt; &lt;td&gt;  632.853&lt;/td&gt; &lt;td&gt;    2.835&lt;/td&gt; &lt;td&gt; 0.005&lt;/td&gt; &lt;td&gt;  550.574&lt;/td&gt; &lt;td&gt; 3038.110&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;It looks like the effect is positive and significant.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s now load a different dataset in which we have replaced the true control units with observations from the PSID sample.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_psid = pd.read_csv(&#39;data/l86_psid.csv&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Is this dataset balanced?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_psid.groupby(&#39;treat&#39;).agg([&#39;mean&#39;, &#39;std&#39;]).T.unstack(1)
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead tr th {
    text-align: left;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;treat&lt;/th&gt;
      &lt;th colspan=&#34;2&#34; halign=&#34;left&#34;&gt;0&lt;/th&gt;
      &lt;th colspan=&#34;2&#34; halign=&#34;left&#34;&gt;1&lt;/th&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;mean&lt;/th&gt;
      &lt;th&gt;std&lt;/th&gt;
      &lt;th&gt;mean&lt;/th&gt;
      &lt;th&gt;std&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;age&lt;/th&gt;
      &lt;td&gt;36.094862&lt;/td&gt;
      &lt;td&gt;12.081030&lt;/td&gt;
      &lt;td&gt;25.816216&lt;/td&gt;
      &lt;td&gt;7.155019&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;education&lt;/th&gt;
      &lt;td&gt;10.766798&lt;/td&gt;
      &lt;td&gt;3.176827&lt;/td&gt;
      &lt;td&gt;10.345946&lt;/td&gt;
      &lt;td&gt;2.010650&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;black&lt;/th&gt;
      &lt;td&gt;0.391304&lt;/td&gt;
      &lt;td&gt;0.489010&lt;/td&gt;
      &lt;td&gt;0.843243&lt;/td&gt;
      &lt;td&gt;0.364558&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;hispanic&lt;/th&gt;
      &lt;td&gt;0.067194&lt;/td&gt;
      &lt;td&gt;0.250853&lt;/td&gt;
      &lt;td&gt;0.059459&lt;/td&gt;
      &lt;td&gt;0.237124&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;married&lt;/th&gt;
      &lt;td&gt;0.735178&lt;/td&gt;
      &lt;td&gt;0.442113&lt;/td&gt;
      &lt;td&gt;0.189189&lt;/td&gt;
      &lt;td&gt;0.392722&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;nodegree&lt;/th&gt;
      &lt;td&gt;0.486166&lt;/td&gt;
      &lt;td&gt;0.500799&lt;/td&gt;
      &lt;td&gt;0.708108&lt;/td&gt;
      &lt;td&gt;0.455867&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;re74&lt;/th&gt;
      &lt;td&gt;11027.303390&lt;/td&gt;
      &lt;td&gt;10814.670751&lt;/td&gt;
      &lt;td&gt;2095.573689&lt;/td&gt;
      &lt;td&gt;4886.620353&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;re75&lt;/th&gt;
      &lt;td&gt;7569.222058&lt;/td&gt;
      &lt;td&gt;9041.944403&lt;/td&gt;
      &lt;td&gt;1532.055314&lt;/td&gt;
      &lt;td&gt;3219.250870&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;re78&lt;/th&gt;
      &lt;td&gt;9995.949977&lt;/td&gt;
      &lt;td&gt;11184.450050&lt;/td&gt;
      &lt;td&gt;6349.143530&lt;/td&gt;
      &lt;td&gt;7867.402218&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;People in the PSID control group are older, more educated, white, married and generally have considerably higher pre-intervention earnings (&lt;code&gt;re74&lt;/code&gt;). This makes sense since the people selected for the NSW program are people that are younger, less experienced and unemployed.&lt;/p&gt;
&lt;p&gt;Lalonde (1986) argues in favor of experimental approaches by showing that using a non-experimental setting, one would not be able to estimate the true treatment effect. Actually, one could even get statistically significant results of the opposite sign.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s repeat the regression exercise for the PSID data.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;smf.ols(&#39;re78 ~ treat&#39;, df_psid).fit().summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
      &lt;td&gt;&lt;/td&gt;         &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt; &lt;td&gt; 9995.9500&lt;/td&gt; &lt;td&gt;  623.715&lt;/td&gt; &lt;td&gt;   16.026&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt; 8770.089&lt;/td&gt; &lt;td&gt; 1.12e+04&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;treat&lt;/th&gt;     &lt;td&gt;-3646.8064&lt;/td&gt; &lt;td&gt;  959.704&lt;/td&gt; &lt;td&gt;   -3.800&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;-5533.027&lt;/td&gt; &lt;td&gt;-1760.586&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;The estimated coefficient is negative and significant. The conclusion from Lalonde (1986) is&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;This comparison shows that many of the econometric procedures d not replicate the experimentally determined results&amp;rdquo;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Dahejia and Wahba (1999) argue that with appropriate matching one would still be able to get a relatively precise estimate of the treatment effect. In particular, the argue in favor of controlling for pre-intervention income, &lt;code&gt;re74&lt;/code&gt; and &lt;code&gt;re75&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s just linearly insert the control variables in the regression.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;smf.ols(&#39;re78 ~ treat + &#39; + &#39; + &#39;.join(X), df_psid).fit().summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
      &lt;td&gt;&lt;/td&gt;         &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt; &lt;td&gt; 5576.0002&lt;/td&gt; &lt;td&gt; 3627.428&lt;/td&gt; &lt;td&gt;    1.537&lt;/td&gt; &lt;td&gt; 0.125&lt;/td&gt; &lt;td&gt;-1553.789&lt;/td&gt; &lt;td&gt; 1.27e+04&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;treat&lt;/th&gt;     &lt;td&gt; 1873.7655&lt;/td&gt; &lt;td&gt; 1060.564&lt;/td&gt; &lt;td&gt;    1.767&lt;/td&gt; &lt;td&gt; 0.078&lt;/td&gt; &lt;td&gt; -210.797&lt;/td&gt; &lt;td&gt; 3958.328&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;age&lt;/th&gt;       &lt;td&gt; -127.8870&lt;/td&gt; &lt;td&gt;   42.166&lt;/td&gt; &lt;td&gt;   -3.033&lt;/td&gt; &lt;td&gt; 0.003&lt;/td&gt; &lt;td&gt; -210.765&lt;/td&gt; &lt;td&gt;  -45.009&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;education&lt;/th&gt; &lt;td&gt;  309.8708&lt;/td&gt; &lt;td&gt;  218.249&lt;/td&gt; &lt;td&gt;    1.420&lt;/td&gt; &lt;td&gt; 0.156&lt;/td&gt; &lt;td&gt; -119.103&lt;/td&gt; &lt;td&gt;  738.845&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;black&lt;/th&gt;     &lt;td&gt;-1432.1626&lt;/td&gt; &lt;td&gt; 1005.876&lt;/td&gt; &lt;td&gt;   -1.424&lt;/td&gt; &lt;td&gt; 0.155&lt;/td&gt; &lt;td&gt;-3409.235&lt;/td&gt; &lt;td&gt;  544.910&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;hispanic&lt;/th&gt;  &lt;td&gt;-1107.9730&lt;/td&gt; &lt;td&gt; 1711.488&lt;/td&gt; &lt;td&gt;   -0.647&lt;/td&gt; &lt;td&gt; 0.518&lt;/td&gt; &lt;td&gt;-4471.940&lt;/td&gt; &lt;td&gt; 2255.994&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;married&lt;/th&gt;   &lt;td&gt; 1776.1337&lt;/td&gt; &lt;td&gt;  967.008&lt;/td&gt; &lt;td&gt;    1.837&lt;/td&gt; &lt;td&gt; 0.067&lt;/td&gt; &lt;td&gt; -124.542&lt;/td&gt; &lt;td&gt; 3676.809&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;nodegree&lt;/th&gt;  &lt;td&gt;-1733.3380&lt;/td&gt; &lt;td&gt; 1158.403&lt;/td&gt; &lt;td&gt;   -1.496&lt;/td&gt; &lt;td&gt; 0.135&lt;/td&gt; &lt;td&gt;-4010.204&lt;/td&gt; &lt;td&gt;  543.528&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;re74&lt;/th&gt;      &lt;td&gt;    0.2088&lt;/td&gt; &lt;td&gt;    0.058&lt;/td&gt; &lt;td&gt;    3.625&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    0.096&lt;/td&gt; &lt;td&gt;    0.322&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;re75&lt;/th&gt;      &lt;td&gt;    0.4715&lt;/td&gt; &lt;td&gt;    0.070&lt;/td&gt; &lt;td&gt;    6.710&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    0.333&lt;/td&gt; &lt;td&gt;    0.610&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;The treatment effect is now positive, borderline significant, and close to the experimental estimate of 1794$. Moreover, it&amp;rsquo;s hard to tell whether this is the correct functional form for the control variables.&lt;/p&gt;
&lt;p&gt;Another option is to use &lt;strong&gt;propensity score matching&lt;/strong&gt;. First, we need to estimate the treatment probability. Let&amp;rsquo;s start with a very simple standard model to predict binary outcomes.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from sklearn.linear_model import LogisticRegressionCV

pi = LogisticRegressionCV().fit(y=df_psid[T], X=df_psid[X])
df_psid[&#39;pscore&#39;] = pi.predict_proba(df_psid[X])[:,1]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;How does the distribution of the propensity scores look like?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sns.histplot(data=df_psid, x=&#39;pscore&#39;, hue=T, bins=20)\
.set(title=&#39;Distribution of propensity scores, PSID data&#39;, xlabel=&#39;&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/propensity_score_35_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;It seems that indeed we predict higher propensity scores for treated people, and viceversa, indicating a strong selection on observable. However, there is also a considerable amount of overlap.&lt;/p&gt;
&lt;p&gt;We can now estimate the treatment effect controlling for the propensity score.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;est = smf.ols(&#39;re78 ~ treat + pscore&#39;, df_psid).fit()
est.summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
      &lt;td&gt;&lt;/td&gt;         &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt; &lt;td&gt; 1.209e+04&lt;/td&gt; &lt;td&gt;  723.351&lt;/td&gt; &lt;td&gt;   16.719&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt; 1.07e+04&lt;/td&gt; &lt;td&gt; 1.35e+04&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;treat&lt;/th&gt;     &lt;td&gt; 1570.5620&lt;/td&gt; &lt;td&gt; 1355.839&lt;/td&gt; &lt;td&gt;    1.158&lt;/td&gt; &lt;td&gt; 0.247&lt;/td&gt; &lt;td&gt;-1094.249&lt;/td&gt; &lt;td&gt; 4235.372&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;pscore&lt;/th&gt;    &lt;td&gt;-1.016e+04&lt;/td&gt; &lt;td&gt; 1918.498&lt;/td&gt; &lt;td&gt;   -5.295&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;-1.39e+04&lt;/td&gt; &lt;td&gt;-6386.994&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;The effect is positive but not significant.&lt;/p&gt;
&lt;p&gt;What would have been the propensity scores if we had used the NSW experimental sample? If it&amp;rsquo;s a well done experiment with a sufficiently large sample, we would expect the propensity scores to concentrate around the percentage of people treated, 0.42 in our data.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;pi = LogisticRegressionCV().fit(y=df_nsw[T], X=df_nsw[X])
df_nsw[&#39;pscore&#39;] = pi.predict_proba(df_nsw[X])[:,1]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sns.histplot(data=df_nsw, x=&#39;pscore&#39;, hue=T, bins=20)\
.set(title=&#39;Distribution of propensity scores, NSW data&#39;, xlabel=&#39;&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/propensity_score_41_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Indeed, now the distribution of the p-scores is concentrated around the treatment frequency in the data. Remarkably, the standard deviation is extremely tight.&lt;/p&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://academic.oup.com/biomet/article/70/1/41/240879&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The central role of the propensity score in observational studies for causal effects&lt;/a&gt; (1983) by Rosenbaum and Rubin&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=8gWctYvRzk4&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Propensity Scores&lt;/a&gt; video lecture by Paul Goldsmith-Pinkham (Yale)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=m3Y8heXoDxE&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Propensity Scores&lt;/a&gt; video lecture by Stefan Wager (Stanford)&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Synthetic Control</title>
      <link>https://matteocourthoud.github.io/post/synthetic_control/</link>
      <pubDate>Fri, 08 Apr 2022 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/post/synthetic_control/</guid>
      <description>&lt;p&gt;In this tutorial, we are going to see how to estimate causal effects when we do not have access to a control group. These settings are extremely common in observational studies and, in these cases, claims of causality are relatively weak. However, there still exist methods to make causal claims, under certain assumptions.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Requisites&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;For this tutorial, I assume you are familiar with the following concepts:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Rubin&amp;rsquo;s potential outcome framework&lt;/li&gt;
&lt;li&gt;Ordinary least squares regression&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://matteocourthoud.github.io/post/permutation_test/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Randomization/permutation inference&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Academic Application&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;As an academic application, we are going to replicate &lt;a href=&#34;https://www.tandfonline.com/doi/abs/10.1198/jasa.2009.ap08746&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Synthetic Control Methods for Comparative Case Studies: Estimating the Effect of Californiaâs Tobacco Control Program&lt;/a&gt; (2010) by Abadie, Diamond and Hainmueller. The authors study the effect of a tobacco control program in California onsmoking habits.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Business Case&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;TBD&lt;/p&gt;
&lt;h2 id=&#34;setting&#34;&gt;Setting&lt;/h2&gt;
&lt;p&gt;We assume that for a panel of i.i.d. subjects $i = 1, &amp;hellip;, n$ over time $t=1, &amp;hellip;,$ we observed a tuple $(X_{i,t}, Y_{i,t})$ comprised of&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;a feature vector $X_{i,t} \in \mathbb R^p$&lt;/li&gt;
&lt;li&gt;a response $Y_{i,t} \in \mathbb R$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Moreover, one unit is treated at time $\tau$. We distinguish time periods in pre-treatment periods and post-treatment periods.&lt;/p&gt;
&lt;p&gt;Crucially, treatment is not randomly assignment, therefore a difference in means between the treated unit and the control group is not an unbiased estimator of the average treatment effect.&lt;/p&gt;
&lt;h2 id=&#34;synthetic-control&#34;&gt;Synthetic Control&lt;/h2&gt;
&lt;p&gt;The problem is that, as usual, we do not observe the counterfactual outcome for treated units, i.e. we do not know what would have happened to them, if they had not been treated. This is known as the &lt;strong&gt;fundamental problem of causal inference&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;The simplest approach, would be just to compare pre and post periods. This is called the &lt;strong&gt;event study&lt;/strong&gt; approach.&lt;/p&gt;
&lt;p&gt;However, we can do better than this. In fact, even though treatment was not randomly assigned, we still have access to some units that were not treated.&lt;/p&gt;
&lt;p&gt;For the outcome variable we have the following setup&lt;/p&gt;
&lt;h1 id=&#34;endarray-right&#34;&gt;$$
Y =
\left[ \begin{array}{l}
Y_{t, post} \ &amp;amp; Y_{c, post}
\newline
Y_{t, pre} \ &amp;amp; Y_{c, pre}
\end{array} \right]&lt;/h1&gt;
&lt;p&gt;\left[ \begin{array}{l}
Y^{(1)}&lt;em&gt;{t, post} \ &amp;amp; Y^{(0)}&lt;/em&gt;{c, post}
\newline
Y^{(0)}&lt;em&gt;{t, pre} \ &amp;amp; Y^{(0)}&lt;/em&gt;{c, pre}
\end{array} \right]
$$&lt;/p&gt;
&lt;p&gt;We basically have a &lt;strong&gt;missing data problem&lt;/strong&gt; since we do not observe $Y^{(0)}_{t, post}$.&lt;/p&gt;
&lt;p&gt;Following Doudchenko and Inbens (2018), we can formulate an estimate of the counterfactual outcome for the treated unit as a linear combination of the observed outcomes for the control units.&lt;/p&gt;
&lt;p&gt;$$
\hat Y^{(0)}&lt;em&gt;{t, post} = \alpha + \sum&lt;/em&gt;{i \in c} \beta_i Y^{(0)}_{i, post}
$$&lt;/p&gt;
&lt;p&gt;where&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the constant $\alpha$ allows for different averages between the two groups&lt;/li&gt;
&lt;li&gt;the weights $\beta_i$ are allowed to vary across control units $i$
&lt;ul&gt;
&lt;li&gt;otherwise it would be a diff-in-diff&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;academic-application&#34;&gt;Academic Application&lt;/h2&gt;
&lt;p&gt;As an academic application, we are going to replicate &lt;a href=&#34;https://www.tandfonline.com/doi/abs/10.1198/jasa.2009.ap08746&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Synthetic Control Methods for Comparative Case Studies: Estimating the Effect of Californiaâs Tobacco Control Program&lt;/a&gt; (2010) by Abadie, Diamond and Hainmueller. The authors study the effect of a tobacco control program in California on smoking habits.&lt;/p&gt;
&lt;p&gt;In particular, in 1989, California passes a tobacco control program that significantly reduces the sales of cigarettes. Was this program effective? Unfortunately, there is neither a counterfactual scenario, nor a randomized experiment.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s start by loading the data.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;%matplotlib inline
%config InlineBackend.figure_format = &#39;retina&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from src.utils import *
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df = pd.read_csv(&#39;data/adh10.csv&#39;)
df.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;state&lt;/th&gt;
      &lt;th&gt;year&lt;/th&gt;
      &lt;th&gt;cost_per_pack&lt;/th&gt;
      &lt;th&gt;cig_sales&lt;/th&gt;
      &lt;th&gt;tax&lt;/th&gt;
      &lt;th&gt;tax_revenue&lt;/th&gt;
      &lt;th&gt;california&lt;/th&gt;
      &lt;th&gt;post&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;AK&lt;/td&gt;
      &lt;td&gt;1970&lt;/td&gt;
      &lt;td&gt;0.418&lt;/td&gt;
      &lt;td&gt;121.3&lt;/td&gt;
      &lt;td&gt;0.16&lt;/td&gt;
      &lt;td&gt;38.3&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;AK&lt;/td&gt;
      &lt;td&gt;1971&lt;/td&gt;
      &lt;td&gt;0.421&lt;/td&gt;
      &lt;td&gt;123.0&lt;/td&gt;
      &lt;td&gt;0.16&lt;/td&gt;
      &lt;td&gt;38.0&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;AK&lt;/td&gt;
      &lt;td&gt;1972&lt;/td&gt;
      &lt;td&gt;0.420&lt;/td&gt;
      &lt;td&gt;130.0&lt;/td&gt;
      &lt;td&gt;0.16&lt;/td&gt;
      &lt;td&gt;38.1&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;AK&lt;/td&gt;
      &lt;td&gt;1973&lt;/td&gt;
      &lt;td&gt;0.438&lt;/td&gt;
      &lt;td&gt;125.8&lt;/td&gt;
      &lt;td&gt;0.16&lt;/td&gt;
      &lt;td&gt;36.5&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;AK&lt;/td&gt;
      &lt;td&gt;1974&lt;/td&gt;
      &lt;td&gt;0.449&lt;/td&gt;
      &lt;td&gt;130.4&lt;/td&gt;
      &lt;td&gt;0.16&lt;/td&gt;
      &lt;td&gt;35.6&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;We have information on cigarette costs, sales, tax rate and revenues for all US states for years from 1970 to 2014.&lt;/p&gt;
&lt;p&gt;We are interested in the sales of cigarettes &lt;code&gt;cig_sales&lt;/code&gt; over time. Let&amp;rsquo;s start by reshaping the dataset so that each state is a time series.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df = df.pivot(index=&#39;year&#39;, columns=&#39;state&#39;, values=&#39;cig_sales&#39;).reset_index()
df.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;state&lt;/th&gt;
      &lt;th&gt;year&lt;/th&gt;
      &lt;th&gt;AK&lt;/th&gt;
      &lt;th&gt;AL&lt;/th&gt;
      &lt;th&gt;AR&lt;/th&gt;
      &lt;th&gt;AZ&lt;/th&gt;
      &lt;th&gt;CA&lt;/th&gt;
      &lt;th&gt;CO&lt;/th&gt;
      &lt;th&gt;CT&lt;/th&gt;
      &lt;th&gt;DC&lt;/th&gt;
      &lt;th&gt;DE&lt;/th&gt;
      &lt;th&gt;...&lt;/th&gt;
      &lt;th&gt;SD&lt;/th&gt;
      &lt;th&gt;TN&lt;/th&gt;
      &lt;th&gt;TX&lt;/th&gt;
      &lt;th&gt;UT&lt;/th&gt;
      &lt;th&gt;VA&lt;/th&gt;
      &lt;th&gt;VT&lt;/th&gt;
      &lt;th&gt;WA&lt;/th&gt;
      &lt;th&gt;WI&lt;/th&gt;
      &lt;th&gt;WV&lt;/th&gt;
      &lt;th&gt;WY&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;1970&lt;/td&gt;
      &lt;td&gt;121.3&lt;/td&gt;
      &lt;td&gt;89.8&lt;/td&gt;
      &lt;td&gt;100.3&lt;/td&gt;
      &lt;td&gt;115.2&lt;/td&gt;
      &lt;td&gt;123.0&lt;/td&gt;
      &lt;td&gt;124.8&lt;/td&gt;
      &lt;td&gt;120.0&lt;/td&gt;
      &lt;td&gt;200.4&lt;/td&gt;
      &lt;td&gt;155.0&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;92.7&lt;/td&gt;
      &lt;td&gt;99.8&lt;/td&gt;
      &lt;td&gt;106.4&lt;/td&gt;
      &lt;td&gt;65.5&lt;/td&gt;
      &lt;td&gt;124.3&lt;/td&gt;
      &lt;td&gt;122.6&lt;/td&gt;
      &lt;td&gt;96.7&lt;/td&gt;
      &lt;td&gt;106.4&lt;/td&gt;
      &lt;td&gt;114.5&lt;/td&gt;
      &lt;td&gt;132.2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;1971&lt;/td&gt;
      &lt;td&gt;123.0&lt;/td&gt;
      &lt;td&gt;95.4&lt;/td&gt;
      &lt;td&gt;104.1&lt;/td&gt;
      &lt;td&gt;109.6&lt;/td&gt;
      &lt;td&gt;121.0&lt;/td&gt;
      &lt;td&gt;125.5&lt;/td&gt;
      &lt;td&gt;117.6&lt;/td&gt;
      &lt;td&gt;213.0&lt;/td&gt;
      &lt;td&gt;161.1&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;96.7&lt;/td&gt;
      &lt;td&gt;106.3&lt;/td&gt;
      &lt;td&gt;108.9&lt;/td&gt;
      &lt;td&gt;67.7&lt;/td&gt;
      &lt;td&gt;128.4&lt;/td&gt;
      &lt;td&gt;124.4&lt;/td&gt;
      &lt;td&gt;97.0&lt;/td&gt;
      &lt;td&gt;105.4&lt;/td&gt;
      &lt;td&gt;111.5&lt;/td&gt;
      &lt;td&gt;131.7&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;1972&lt;/td&gt;
      &lt;td&gt;130.0&lt;/td&gt;
      &lt;td&gt;101.1&lt;/td&gt;
      &lt;td&gt;103.9&lt;/td&gt;
      &lt;td&gt;125.0&lt;/td&gt;
      &lt;td&gt;123.5&lt;/td&gt;
      &lt;td&gt;134.3&lt;/td&gt;
      &lt;td&gt;110.8&lt;/td&gt;
      &lt;td&gt;220.6&lt;/td&gt;
      &lt;td&gt;156.3&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;103.0&lt;/td&gt;
      &lt;td&gt;111.5&lt;/td&gt;
      &lt;td&gt;108.6&lt;/td&gt;
      &lt;td&gt;71.3&lt;/td&gt;
      &lt;td&gt;137.0&lt;/td&gt;
      &lt;td&gt;138.0&lt;/td&gt;
      &lt;td&gt;88.5&lt;/td&gt;
      &lt;td&gt;108.8&lt;/td&gt;
      &lt;td&gt;117.5&lt;/td&gt;
      &lt;td&gt;140.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;1973&lt;/td&gt;
      &lt;td&gt;125.8&lt;/td&gt;
      &lt;td&gt;102.9&lt;/td&gt;
      &lt;td&gt;108.0&lt;/td&gt;
      &lt;td&gt;128.3&lt;/td&gt;
      &lt;td&gt;124.4&lt;/td&gt;
      &lt;td&gt;137.9&lt;/td&gt;
      &lt;td&gt;109.3&lt;/td&gt;
      &lt;td&gt;209.4&lt;/td&gt;
      &lt;td&gt;154.7&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;103.5&lt;/td&gt;
      &lt;td&gt;109.7&lt;/td&gt;
      &lt;td&gt;110.4&lt;/td&gt;
      &lt;td&gt;72.7&lt;/td&gt;
      &lt;td&gt;143.1&lt;/td&gt;
      &lt;td&gt;146.8&lt;/td&gt;
      &lt;td&gt;91.0&lt;/td&gt;
      &lt;td&gt;109.5&lt;/td&gt;
      &lt;td&gt;116.6&lt;/td&gt;
      &lt;td&gt;141.2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;1974&lt;/td&gt;
      &lt;td&gt;130.4&lt;/td&gt;
      &lt;td&gt;108.2&lt;/td&gt;
      &lt;td&gt;109.7&lt;/td&gt;
      &lt;td&gt;133.1&lt;/td&gt;
      &lt;td&gt;126.7&lt;/td&gt;
      &lt;td&gt;132.8&lt;/td&gt;
      &lt;td&gt;112.4&lt;/td&gt;
      &lt;td&gt;182.7&lt;/td&gt;
      &lt;td&gt;151.3&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;108.4&lt;/td&gt;
      &lt;td&gt;114.8&lt;/td&gt;
      &lt;td&gt;114.7&lt;/td&gt;
      &lt;td&gt;75.6&lt;/td&gt;
      &lt;td&gt;149.6&lt;/td&gt;
      &lt;td&gt;151.8&lt;/td&gt;
      &lt;td&gt;98.6&lt;/td&gt;
      &lt;td&gt;111.8&lt;/td&gt;
      &lt;td&gt;119.9&lt;/td&gt;
      &lt;td&gt;145.8&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;5 rows Ã 52 columns&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;In an ideal world, one could use the average of all the other states in the United States as a control. Let&amp;rsquo;s plot what that would look like.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;states = [c for c in df.columns if c not in [&#39;year&#39;]]
df[&#39;not CA&#39;] = df[[s for s in states if s != &#39;CA&#39;]].mean(axis=1)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sns.lineplot(x=df[&#39;year&#39;], y=df[&#39;CA&#39;].values, label=&#39;California&#39;)
sns.lineplot(x=df[&#39;year&#39;], y=df[&#39;not CA&#39;].values, label=&#39;Other States&#39;)
plt.vlines(x=1988, ymin=plt.ylim()[0], ymax=plt.ylim()[1], linestyle=&amp;quot;:&amp;quot;, color=&#39;C2&#39;)
plt.title(&amp;quot;Per-capita cigarette sales (in packs)&amp;quot;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/synthetic_control_15_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;It looks like the trends are slightly diverging after 1989, the year of Proposition 99. However, it also looks like the trends are sensibly different before 1989. Therefore, it feels quite a stretch to attribute the differences in cigarette sales post 1989 to Proposition 99 alone.&lt;/p&gt;
&lt;p&gt;The idea is to &lt;strong&gt;build a synthetic control&lt;/strong&gt; state for California. Maybe no single state is a good control, but a combination of them could actually provide a good approximation to California. Intuitively, one could think that better approximations are states that are&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;larger&lt;/li&gt;
&lt;li&gt;more democratic&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In practice, we are going to try to build the synthetic control state such as it approximates pre-trend, the sales of cigarettes pre 1989. Let&amp;rsquo;s start by using &lt;code&gt;LinearRegression&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from sklearn.linear_model import LinearRegression

def synth_predict(df, state, model):
    y = df.loc[df[&#39;year&#39;] &amp;lt;= 1988, state]
    other_states = [c for c in states if c not in [&#39;year&#39;, state]]
    X = df.loc[df[&#39;year&#39;] &amp;lt;= 1988, other_states]
    df[f&#39;s_{state}&#39;] = model.fit(X, y).predict(df[other_states])
    return model
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;model = synth_predict(df, &#39;CA&#39;, LinearRegression())
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s now plot the predicted against the forea&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sns.lineplot(x=df[&#39;year&#39;], y=df[&#39;CA&#39;].values, label=&amp;quot;California&amp;quot;)
sns.lineplot(x=df[&#39;year&#39;], y=df[f&#39;s_CA&#39;].values, label=&#39;Synthetic California&#39;)
plt.vlines(x=1988, ymin=plt.ylim()[0], ymax=plt.ylim()[1], ls=&amp;quot;:&amp;quot;, color=&#39;C2&#39;, label=&#39;Proposition 99&#39;, zorder=-1)
plt.legend();
plt.title(&amp;quot;Per-capita cigarette sales (in packs)&amp;quot;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/synthetic_control_20_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def plot_normalized(df, state, vline=True, hline=True, **kwargs):
    sns.lineplot(x=df[&#39;year&#39;], y=df[state] - df[f&#39;s_{state}&#39;], **kwargs)
    if vline: 
        plt.vlines(x=1988, ymin=0.9*plt.ylim()[0], ymax=0.9*plt.ylim()[1], ls=&amp;quot;:&amp;quot;, color=&#39;C2&#39;, label=&#39;Proposition 99&#39;, zorder=-3);
        plt.legend()
    if hline: sns.lineplot(x=df[&#39;year&#39;], y=0, lw=1, color=&#39;black&#39;, zorder=1)
    plt.title(&amp;quot;Normalized per-capita cigarette sales (in packs)&amp;quot;);
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plot_normalized(df, &#39;CA&#39;, label=&amp;quot;California&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/synthetic_control_23_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Now we can clearly see the negative effect of the policy and how it rebounds back over time.&lt;/p&gt;
&lt;p&gt;We can also observe that we are clearly &lt;strong&gt;overfitting&lt;/strong&gt;: the pre-policy predicted cigarette consumption line is exactly flat. One way to avoid overfitting is to use a penalized estimator.&lt;/p&gt;
&lt;p&gt;Another problem concerns the &lt;strong&gt;weights&lt;/strong&gt;. We have not set any constraint on the weights. Let&amp;rsquo;s see what they look like.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sns.histplot(model.coef_, bins=30).set(title=&#39;Coefficients&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/synthetic_control_25_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;We have many &lt;strong&gt;negative weights&lt;/strong&gt;, which do not make much sense from a causal inference perspective. In principle all weights should be between zero and one.&lt;/p&gt;
&lt;p&gt;To adress both concerns, we are going to use the &lt;code&gt;Lasso&lt;/code&gt; estimator, which uses a norm-1 penalty for parameters in linear regression. Moreover, we will constrain it to have only positive weights.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from sklearn.linear_model import Lasso

synth_predict(df, &#39;CA&#39;, Lasso(positive=True, alpha=5))
plot_normalized(df, &#39;CA&#39;, label=&#39;California&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/synthetic_control_27_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;The normalized estimator gives a very similar prediction, however it does not blatantly overfit the pre-period.&lt;/p&gt;
&lt;p&gt;Is the estimate significant? The question we are trying to answer is &amp;ldquo;&lt;em&gt;how unusual is this estimate under the null hypothesis of no policy effect&lt;/em&gt;?&amp;rdquo;. We are going to perform a randomization/permutation test in order to answer this question. The &lt;strong&gt;idea&lt;/strong&gt; is that if the policy has no effect, the effect we observe for California should not be significantly different from the effect we observe for any other state.&lt;/p&gt;
&lt;p&gt;Therefore, we are going to replicate the procedure above, but for all other states and observe how unusual is California.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;for state in states:
    synth_predict(df, state, Lasso(positive=True, alpha=5))
    plot_normalized(df, state, vline=False, alpha=0.2, color=&#39;grey&#39;)
plot_normalized(df, &#39;CA&#39;, vline=True, label=&#39;California&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/synthetic_control_29_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;It looks like the effect for California is quite extreme, especially if we consider a one-sided hypothesis test (it feels weird to assume that the policy could ever increase cigarette sales). We get more extreme only for a couple of states, especially in the first 10 years. Therefore, we conclude that the effect of the Proposition 99 policy is statistically different from zero.&lt;/p&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=BsRqy7juMus&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Synthetic Control&lt;/a&gt; video lecture by Paul Goldsmith-Pinkham (Yale)&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Free PhD Courses in Economics and Data Science</title>
      <link>https://matteocourthoud.github.io/post/courses/</link>
      <pubDate>Thu, 10 Feb 2022 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/post/courses/</guid>
      <description>&lt;p&gt;In this page I will collect lectures and materials for graduate courses in Economics and Social Sciences.&lt;/p&gt;
&lt;p&gt;I will only link to lectures and materials that are freely available. I will not link to courses hosted on MOOC websites or that require university credentials to access.&lt;/p&gt;
&lt;p&gt;A special mention goes to the following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The &lt;strong&gt;NBER&lt;/strong&gt; that during each Summer Institute has a &lt;a href=&#34;https://www.nber.org/research/lectures&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;lecture series&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;The &lt;strong&gt;Chamberlain Seminar&lt;/strong&gt; that since 2021 started hosting and recording &lt;a href=&#34;https://www.chamberlainseminar.org/past-seminars&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;tutorial sessions&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;video-lectures&#34;&gt;Video Lectures&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Course Title&lt;/th&gt;
&lt;th&gt;Author&lt;/th&gt;
&lt;th&gt;University&lt;/th&gt;
&lt;th&gt;Year&lt;/th&gt;
&lt;th&gt;Material&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://www.youtube.com/playlist?list=PLxq_lXOUlvQAoWZEqhRqHNezS30lI49G-&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Machine Learning and Causal Inference&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Susan Athey et al.&lt;/td&gt;
&lt;td&gt;Stanford&lt;/td&gt;
&lt;td&gt;2022&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://www.gsb.stanford.edu/faculty-research/centers-initiatives/sil/research/methods/ai-machine-learning/short-course&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Yes&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://chrisconlon.github.io/gradio.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Industrial Organization&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Chris Conlon&lt;/td&gt;
&lt;td&gt;NYU&lt;/td&gt;
&lt;td&gt;2021&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://chrisconlon.github.io/gradio.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Yes&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://chrisconlon.github.io/metrics.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Panel Data Econometrics&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Chris Conlon&lt;/td&gt;
&lt;td&gt;NYU&lt;/td&gt;
&lt;td&gt;2021&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://chrisconlon.github.io/metrics.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Yes&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://www.youtube.com/playlist?list=PLoROMvodv4rPLKxIpqhjhPgdQy7imNkDn&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Machine Learning with Graphs&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Yure Leskovec&lt;/td&gt;
&lt;td&gt;Stanford&lt;/td&gt;
&lt;td&gt;2021&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;http://web.stanford.edu/class/cs224w/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Yes&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://www.youtube.com/playlist?list=PLWWcL1M3lLlojLTSVf2gGYQ_9TlPyPbiJ&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Applied Methods&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Paul Goldsmith-Pinkham&lt;/td&gt;
&lt;td&gt;Yale&lt;/td&gt;
&lt;td&gt;2021&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://github.com/paulgp/applied-methods-phd&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Yes&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://taylorjwright.github.io/did-reading-group/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;DiD Reading Group&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;misc&lt;/td&gt;
&lt;td&gt;misc&lt;/td&gt;
&lt;td&gt;2021&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://taylorjwright.github.io/did-reading-group/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Yes&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://vimeo.com/user108848900&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Computational Economics&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Kenneth Judd&lt;/td&gt;
&lt;td&gt;Stanford&lt;/td&gt;
&lt;td&gt;2020&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://github.com/KennethJudd/CompEcon2020&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Yes&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://www.youtube.com/playlist?list=PLoROMvodv4rOSOPzutgyCTapiGlY2Nd8u&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Reinforcement Learning&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Emma Brunskill&lt;/td&gt;
&lt;td&gt;Stanford&lt;/td&gt;
&lt;td&gt;2020&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;http://web.stanford.edu/class/cs234/index.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Yes&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://www.youtube.com/playlist?list=PLoROMvodv4rObpMCir6rNNUlFAn56Js20&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Natural Language Understanding&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Christopher Potts&lt;/td&gt;
&lt;td&gt;Stanford&lt;/td&gt;
&lt;td&gt;2019&lt;/td&gt;
&lt;td&gt;No&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;material&#34;&gt;Material&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Course Title&lt;/th&gt;
&lt;th&gt;Author&lt;/th&gt;
&lt;th&gt;University&lt;/th&gt;
&lt;th&gt;Year&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://floswald.github.io/NumericalMethods/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Computational Economics&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://floswald.github.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Florial Oswald&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Bocconi&lt;/td&gt;
&lt;td&gt;2021&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://github.com/uo-ec607/lectures&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Data Science for Economists&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://grantmcdermott.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Grant McDermott&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Oregon&lt;/td&gt;
&lt;td&gt;2020&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;http://www.johnasker.com/IO.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Industrial Organization&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;John Asker&lt;/td&gt;
&lt;td&gt;UCLA&lt;/td&gt;
&lt;td&gt;2020&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://kohei-kawaguchi.github.io/EmpiricalIO/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Topics in Empirical Industrial Organization&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Kohei Kawaguchi&lt;/td&gt;
&lt;td&gt;Hong Kong&lt;/td&gt;
&lt;td&gt;2020&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;http://individual.utoronto.ca/vaguirre/courses/eco2901/teaching_io_toronto.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Industrial Organization&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Victor Aguirregabiria&lt;/td&gt;
&lt;td&gt;Toronto&lt;/td&gt;
&lt;td&gt;2019&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://github.com/OU-PhD-Econometrics/fall-2020&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Econometrics&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Tyler Ransom&lt;/td&gt;
&lt;td&gt;Oklahoma&lt;/td&gt;
&lt;td&gt;2020&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://github.com/MartinSpindler/Machine-Learning-in-Econometrics&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Machine Learning in Econometrics&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Martin Spindler&lt;/td&gt;
&lt;td&gt;Munich&lt;/td&gt;
&lt;td&gt;2020&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;http://comlabgames.com/structuraleconometrics/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Structural Econometrics&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Robert Miller&lt;/td&gt;
&lt;td&gt;Carnegie Mellon&lt;/td&gt;
&lt;td&gt;2019&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;iframe src=&#34;https://docs.google.com/forms/d/e/1FAIpQLSf672Vwguhe9GAmFGqtPMeGWzTamwzU2VcMtEFgaJWnP3YtBw/viewform?embedded=true&#34; width=&#34;640&#34; height=&#34;439&#34; frameborder=&#34;0&#34; marginheight=&#34;0&#34; marginwidth=&#34;0&#34;&gt;Loadingâ¦&lt;/iframe&gt;
</description>
    </item>
    
    <item>
      <title>Conferences in Economics and Finance</title>
      <link>https://matteocourthoud.github.io/post/conferences/</link>
      <pubDate>Mon, 10 Jan 2022 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/post/conferences/</guid>
      <description>&lt;p&gt;I will use this page to collect information about conferences in Economics and Finance.&lt;/p&gt;
&lt;p&gt;If you know about public conferences or meetings that are missing from this list, please either &lt;a href=&#34;mailto:matteo.courthoud@econ.uzh.ch&#34;&gt;contact me&lt;/a&gt; or &lt;a href=&#34;https://github.com/matteocourthoud/website/blob/master/content/post/conferences/index.md&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;edit the table on Github&lt;/a&gt;!&lt;/p&gt;
&lt;p&gt;Note that conferences are ordered by deadline and not by conference date.&lt;/p&gt;
&lt;h2 id=&#34;january&#34;&gt;January&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Conference&lt;/th&gt;
&lt;th&gt;Organizer&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Field&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Deadline&lt;/th&gt;
&lt;th&gt;Target&lt;/th&gt;
&lt;th&gt;Date&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://www.sgvs.ch/conferences/sses2022&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Annual Congress of the Swiss Society of Economics and Statistics&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;SSES&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;All&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://www.sgvs.ch/conferences/sses2022/call_for_papers&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;January 31&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;23/06/21&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;br&gt;
&lt;h2 id=&#34;february&#34;&gt;February&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Conference&lt;/th&gt;
&lt;th&gt;Organizer&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Field&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Deadline&lt;/th&gt;
&lt;th&gt;Target&lt;/th&gt;
&lt;th&gt;Date&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://beccle.no/event/bergen-competition-policy-conference-2020/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Bergen Competition Policy Conference&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;NHH&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Comp policy&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://beccle.no/event/bergen-competition-policy-conference-2020/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;February 03&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;23/04/20&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://cepr.org/6754/cfp-mainconference&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CEPR/JIE Conference on Applied IO&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;CEPR&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;IO&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;&#34;&gt;February 10&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Junior&lt;/td&gt;
&lt;td&gt;08/06/21&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;http://ec22.sigecom.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Economics and Computation&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;ACM SIGecom&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Theory&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://ec22.sigecom.org/call-for-contributions-acm/papers/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;February 10&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;11/07/21&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://www.econometricsociety.org/meetings/schedule/2022/06/16/2022-north-america-summer-meeting&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ES North American Summer Meeting&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Econometric Society&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;All&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://www.econometricsociety.org/meetings/schedule/2022/06/16/2022-north-america-summer-meeting&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;February 12&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Senior&lt;/td&gt;
&lt;td&gt;16/06/21&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://www.eea-esem-congresses.org/index.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;EEA Summer Meeting&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;European Economic Association&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;All&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://www.eea-esem-congresses.org/important-dates.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;February 15&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;23/08/21&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://www.eea-esem-congresses.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ES European Summer Meeting&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Econometric Society&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;All&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://www.eea-esem-congresses.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;February 15&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Senior&lt;/td&gt;
&lt;td&gt;23/08/21&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://www.economicdynamics.org/sedam_2021/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Annual Meeting of the Society for Economic Dynamics&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;SED&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Macro&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://www.economicdynamics.org/sedam_2021/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;February 15&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;01/07/21&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://games2020.hu/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GAMES 2020&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Game Theory Society&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Game Theory&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://games2020.hu/registration/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;February 20&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;19/07/21&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://www.nottingham.ac.uk/gep/news-events/conferences/2020-21/postgrad-conference-2021.aspx&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Annual GEP/CEPR Postgraduate Conference&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;University of Nottingham&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Policy&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://www.nottingham.ac.uk/gep/documents/conferences/2020-21/pg-conf-cfp-2021.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;February 26&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Junior&lt;/td&gt;
&lt;td&gt;06/05/21&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://www.digital-economics.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Doctoral Workshop on the Economics of Digitization&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;TSE&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Digitalization&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://www.digital-economics.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;February 28&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Junior&lt;/td&gt;
&lt;td&gt;12/05/22&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;br&gt;
&lt;h2 id=&#34;march&#34;&gt;March&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Conference&lt;/th&gt;
&lt;th&gt;Organizer&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Field&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Deadline&lt;/th&gt;
&lt;th&gt;Target&lt;/th&gt;
&lt;th&gt;Date&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://cssh.northeastern.edu/economics/iioc/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Annual IIOC&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Northeastern University&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;IO&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://cssh.northeastern.edu/economics/iioc/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;March 01&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;30/04/21&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://yem2020.econ.muni.cz/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Young Economists&#39; Meeting&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;University fo Munich&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;All&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://yem2020.econ.muni.cz/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;March 08&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Junior&lt;/td&gt;
&lt;td&gt;01/10/20&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://www.barcelonagse.eu/summer-forum&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GSE Summer Forum&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Barcelona GSE&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;All&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://www.barcelonagse.eu/summer-forum&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;March 14&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;07/06/21&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://www.nhh.no/en/calendar/conferences/2021/earie-2021/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;EARIE&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;NHH&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;IO&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://www.nhh.no/en/calendar/conferences/2021/earie-2021/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;March 15&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;27/08/21&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://www.sioe.org/news/economics-media-workshop-call-paper-poster-presentations&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Economics of Media Workshop&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Queenâs University&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;IO&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://www.sioe.org/news/economics-media-workshop-call-paper-poster-presentations&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;March 15&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Junior&lt;/td&gt;
&lt;td&gt;12/06/20&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://www.qmul.ac.uk/sef/events/conferences/items/3rd-qmul-economics-and-finance-workshop-for-phd--post-doctoral-students.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;QMUL Economics and Finance Workshop&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Queen Mary University&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;All&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://econ.columbia.edu/call-for-papers-3rd-qm-phd-workshop/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;March 20&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Junior&lt;/td&gt;
&lt;td&gt;26/05/20&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://static1.squarespace.com/static/56086d00e4b0fb7874bc2d42/t/5e753140c2225859fa93ba1e/1584738624656/callforpapers.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Virtual Finance and Economics Conference&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Yale University&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Finecon&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://static1.squarespace.com/static/56086d00e4b0fb7874bc2d42/t/5e753140c2225859fa93ba1e/1584738624656/callforpapers.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;March 25&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;17/04/20&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://sites.google.com/view/dc-io-day&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;DC IO Day 2020&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Georgetown University&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;IO&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://sites.google.com/view/dc-io-day&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;March 31&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;15/05/20&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;br&gt;
&lt;h2 id=&#34;april&#34;&gt;April&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Conference&lt;/th&gt;
&lt;th&gt;Organizer&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Field&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Deadline&lt;/th&gt;
&lt;th&gt;Target&lt;/th&gt;
&lt;th&gt;Date&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://economics.stanford.edu/site/site-2021&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;SITE&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Stanford University&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Theory&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://economics.stanford.edu/site/site-2021&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;April 01&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;12/06/21&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://www.nber.org/conferences/summer-institute&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;NBER Summer Meeting Workshop&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;NBER&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;All&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://www.nber.org/conferences/summer-institute&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;April 05&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;12/07/21&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://www.aeaweb.org/conference/submissions&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;AEA Annual Meeting&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;AEA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;All&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://www.aeaweb.org/conference/submissions&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;April 15&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;07/01/22&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;files/swissIOday2021_CallForPapers.pdf&#34;&gt;Swiss IO Day&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;University of Bern&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;IO&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;files/swissIOday2021_CallForPapers.pdf&#34;&gt;April 16&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;11/06/21&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://www.econometricsociety.org/meetings/schedule/2022/01/06/2022-north-american-winter-meeting&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Econometric Society - North American Winter Meetings&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;ES&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;All&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://www.econometricsociety.org/meetings/schedule/2022/01/06/2022-north-american-winter-meeting&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;April 21&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;06/01/22&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://www.cresse.info/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CRESSE&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;CRESSE&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;IO&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://www.cresse.info/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;April 30&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;26/07/21&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;br&gt;
&lt;h2 id=&#34;may&#34;&gt;May&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Conference&lt;/th&gt;
&lt;th&gt;Organizer&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Field&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Deadline&lt;/th&gt;
&lt;th&gt;Target&lt;/th&gt;
&lt;th&gt;Date&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://mailchi.mp/cepr/european-research-workshop-in-international-trade-erwit-506353&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;European Research Workshop in International Trade&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;CEPR&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Trade&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://mailchi.mp/cepr/european-research-workshop-in-international-trade-erwit-506353&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;May 02&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;22/10/21&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;http://coin.wne.uw.edu.pl/wiem/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Warsaw International Economic Meeting&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Warsaw University&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;All&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;http://coin.wne.uw.edu.pl/wiem/wiem2020-cfp.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;May 03&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;01/07/20&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://warwick.ac.uk/fac/soc/economics/events/2021/6/economics_phd_conference/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Warwick Economics PhD Conference&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;University of Warwick&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;All&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://warwick.ac.uk/fac/soc/economics/events/2021/6/economics_phd_conference/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;May 09&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;PhD&lt;/td&gt;
&lt;td&gt;24/06/21&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://www.law.northwestern.edu/research-faculty/clbe/events/antitrust/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Annual Conference on Antitrust Economics and Competition Policy&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Northwestern University&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Comp policy&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://www.law.northwestern.edu/research-faculty/clbe/events/antitrust/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;May 17&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;17/09/21&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://www.economicsofai.com/blog/2021/2/2/2021-nber-economics-of-ai-conference-call-for-papers&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;NBER Economics of AI Conference&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;NBER&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;AI&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://www.economicsofai.com/blog/2021/2/2/2021-nber-economics-of-ai-conference-call-for-papers&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;May 31&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;23/09/21&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;br&gt;
&lt;h2 id=&#34;june&#34;&gt;June&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Conference&lt;/th&gt;
&lt;th&gt;Organizer&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Field&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Deadline&lt;/th&gt;
&lt;th&gt;Target&lt;/th&gt;
&lt;th&gt;Date&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://eaamo.org/cfp/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Equity and Access in Algorithms, Mechanisms, and Optimization (EAAMO)&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;ACM&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;AI&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://eaamo.org/cfp/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;June 14&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;05/10/21&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://www.ftc.gov/news-events/events-calendar/fourteenth-annual-federal-trade-commission-microeconomics-conference&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;FTC Micro Conference&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;FTC&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;IO&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://www.ftc.gov/system/files/documents/public_events/1588356/20210326_-_micro_conf_call_for_papers.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;June 23&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Senior&lt;/td&gt;
&lt;td&gt;04/11/21&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;br&gt;
&lt;h2 id=&#34;july&#34;&gt;July&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Conference&lt;/th&gt;
&lt;th&gt;Organizer&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Field&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Deadline&lt;/th&gt;
&lt;th&gt;Target&lt;/th&gt;
&lt;th&gt;Date&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://www.emconference.net/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Empirics and Methods in Economics Conference&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Northwestern &amp;amp; Chicago&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Empirical&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://www.emconference.net/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;July 30&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Junior&lt;/td&gt;
&lt;td&gt;22/10/20&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;br&gt;
&lt;h2 id=&#34;august&#34;&gt;August&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Conference&lt;/th&gt;
&lt;th&gt;Organizer&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Field&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Deadline&lt;/th&gt;
&lt;th&gt;Target&lt;/th&gt;
&lt;th&gt;Date&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://docs.google.com/document/d/1BjHI_6HyL7pk2UYUNDlDY971B9AO83wD8DfEF6KNDfs/edit&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;AI Policy Conference&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;ETH Zurich&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;AI&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://docs.google.com/document/d/1BjHI_6HyL7pk2UYUNDlDY971B9AO83wD8DfEF6KNDfs/edit&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;August 1&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;14/09/21&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://sites.google.com/site/uscfom/finance-organizations-and-markets-fom-research-group&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Finance, Organizations and Markets (FOM) Conference&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Dartmouth College&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Finance, IO&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://sites.google.com/site/uscfom/finance-organizations-and-markets-fom-research-group&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;August 14&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;28/10/21&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;br&gt;
&lt;h2 id=&#34;september&#34;&gt;September&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Conference&lt;/th&gt;
&lt;th&gt;Organizer&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Field&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Deadline&lt;/th&gt;
&lt;th&gt;Target&lt;/th&gt;
&lt;th&gt;Date&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://why21.causalai.net/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Causal Inference &amp;amp; Machine Learning: Why now?&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;NeurIPS&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Econometrics&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://why21.causalai.net/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;September 18&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;13/12/21&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://www.ub.edu/school-economics/ewmes&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ES European Winter Meeting&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Econometricc Society&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;All&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://www.ub.edu/school-economics/ewmes&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;September 19&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Senior&lt;/td&gt;
&lt;td&gt;13/12/21&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://www.causalscience.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Causal Data Science Conference&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;causalscience&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;All&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://www.causalscience.org/blog/causal-data-science-meeting-2021-call-for-papers/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;September 30&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;15/11/21&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;br&gt;
&lt;h2 id=&#34;october&#34;&gt;October&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Conference&lt;/th&gt;
&lt;th&gt;Organizer&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Field&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Deadline&lt;/th&gt;
&lt;th&gt;Target&lt;/th&gt;
&lt;th&gt;Date&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://www.tse-fr.eu/conferences/2022-15th-digital-economics-conference&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Digital Economics Conference&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;TSE&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Digital&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://www.tse-fr.eu/sites/default/files/TSE/documents/conf/2022/Digital_Economics/call_for_papers_digital_conf_2022.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;October 3&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;13/01/22&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://apios.org.au/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Asia-Pacific IO Conference&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Asia-Pacific IO Society&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;IO&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://apios.org.au/submission/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;October 22&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;13/12/21&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://www.sgvs.ch/conferences/ysem2021&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Young Swiss Economists Meeting&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;SSES&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;All&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://www.sgvs.ch/files/Call_for_Papers_YSEM_2021.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;October 25&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Junior&lt;/td&gt;
&lt;td&gt;11/02/21&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;br&gt;
&lt;h2 id=&#34;november&#34;&gt;November&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Conference&lt;/th&gt;
&lt;th&gt;Organizer&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Field&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Deadline&lt;/th&gt;
&lt;th&gt;Target&lt;/th&gt;
&lt;th&gt;Date&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://www.law.nyu.edu/conferences/2022-NextGen-Antitrust-Conference&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Next Generation of Antitrust, Data Privacy and Data Protection Scholars Conference&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;NYU&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;IO&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://www.law.nyu.edu/conferences/2022-NextGen-Antitrust-Conference&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;November 1&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;28/01/22&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://smye2021.weebly.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Spring Meeting of Young Economists&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;University of Bologna&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;All&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://smye2021.weebly.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;November 12&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Junior&lt;/td&gt;
&lt;td&gt;17/06/21&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://www.nber.org/conferences/industrial-organization-program-meeting-spring-2021&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;NBER IO Winter Meeting&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;NBER&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;IO&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://conference.nber.org/confsubmit/backend/cfp?id=IOs21&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;November 20&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Senior&lt;/td&gt;
&lt;td&gt;12/02/21&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://www.zew.de/en/events-and-professional-training/detail/2021-macci-annual-conference/3320&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;MaCCI Annual Conference&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;University of Mannheim&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;IO&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://www.zew.de/en/events-and-professional-training/detail/2021-macci-annual-conference/3320&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;November 30&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;12/03/21&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://www.tse-fr.eu/sites/default/files/TSE/documents/conf/2022/postal/call_postal_2022_v2.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Postal Economics Conference&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;TSE&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Digital&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://www.tse-fr.eu/sites/default/files/TSE/documents/conf/2022/postal/call_postal_2022_v2.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;November 30&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;07/04/22&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;br&gt;
&lt;h2 id=&#34;december&#34;&gt;December&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Conference&lt;/th&gt;
&lt;th&gt;Organizer&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Field&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Deadline&lt;/th&gt;
&lt;th&gt;Target&lt;/th&gt;
&lt;th&gt;Date&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://sites.google.com/site/ecbeconference&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Early-Career Behavioral Economics Conference&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Princeton University&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Behavioral&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://sites.google.com/site/ecbeconference/call&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;December 15&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;junior&lt;/td&gt;
&lt;td&gt;03/06/21&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;br&gt;
&lt;h2 id=&#34;undefined&#34;&gt;Undefined&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Conference&lt;/th&gt;
&lt;th&gt;Organizer&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Field&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Deadline&lt;/th&gt;
&lt;th&gt;Target&lt;/th&gt;
&lt;th&gt;Date&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://www.law.northwestern.edu/research-faculty/clbe/events/innovation/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Annual Conference on Innovation Economics&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Northwestern University&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Innovation&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://www.law.northwestern.edu/research-faculty/clbe/callforpapers/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;forthcoming&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;27/08/20&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://conference2.aau.at/event/4/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Conference on Mechanism and Institution Design&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;UniversitÃ¤t Klagenfurt&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Market Design&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;closed&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;11/06/20&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://sites.google.com/site/dteaworkshop/home&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;D-TEA Conference&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;HEC Paris&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Theory&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;closed&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;16/06/20&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://sites.wustl.edu/egsc/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Economics Graduate Student Conference&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Washington University&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;All&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;closed&lt;/td&gt;
&lt;td&gt;Junior&lt;/td&gt;
&lt;td&gt;07/11/20&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;http://conference.nber.org/confer/2020/SI2020/SI2020.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;NBER Summer Institute&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;NBER&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;All&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;invitation&lt;/td&gt;
&lt;td&gt;Senior&lt;/td&gt;
&lt;td&gt;06/07/20&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;http://competitionpolicy.ac.uk/events/annual-conferences&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CCP Annual Conference&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Centre for Competition Policy&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Comp policy&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;closed&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;24/06/21&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://www.res.org.uk/event-listing/2021-annual-conference.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;RES Annual Conference&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Royal Economics Society&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;All&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;closed&lt;/td&gt;
&lt;td&gt;Senior&lt;/td&gt;
&lt;td&gt;12/04/21&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://www.emerginginvestigators.org/conference&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;JEI Student Conference&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Harvard University&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;All&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;canceled&lt;/td&gt;
&lt;td&gt;Junior&lt;/td&gt;
&lt;td&gt;20/06/20&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://bfi.uchicago.edu/event/sixth-annual-conference-on-network-science-and-economics/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Annual Conference on Network Science and Economics&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Becker Friedman Institute&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Networks&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;canceled&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;27/03/20&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://saet.uiowa.edu/2021-annual-saet-conference/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Annual SAET Conference&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Society for the Advancement of Economic Theory&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Theory&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;closed&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;13/06/21&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
</description>
    </item>
    
    <item>
      <title>How to Work on a Remote Machine</title>
      <link>https://matteocourthoud.github.io/post/remote/</link>
      <pubDate>Mon, 06 Dec 2021 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/post/remote/</guid>
      <description>&lt;p&gt;In this page I will share tips on how to set up a remote machine and deploy your code there. I will first analyze SSH and then look at two specific applications: coding in Python and Julia.&lt;/p&gt;
&lt;h2 id=&#34;setup&#34;&gt;Setup&lt;/h2&gt;
&lt;p&gt;In order to start working on a remote server you need&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the server&lt;/li&gt;
&lt;li&gt;local shell&lt;/li&gt;
&lt;li&gt;SSH installed&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;SSH, or Secure Shell, is a protocol designed to transfer data between a client and a server (two computers basically) over an untrusted network.&lt;/p&gt;
&lt;p&gt;The way SSH works is it encrypts the connection using a pair of keys and the server, which is the computer you would connect to, is usually waiting for an SSH connection on Port 22.&lt;/p&gt;
&lt;p&gt;SSH is normally installed by default. To check if you have SSH installed, open the terminal and write &lt;code&gt;ssh&lt;/code&gt;. You should receive a message that looks like this&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;usage: ssh [-1246AaCfGgKkMNnqsTtVvXxYy] [-b bind_address] [-c cipher_spec]
[-D [bind_address:]port] [-E log_file] [-e escape_char]
[-F configfile] [-I pkcs11] [-i identity_file]
[-J [user@]host[:port]] [-L address] [-l login_name] [-m mac_spec] [-O ctl_cmd] [-o option] [-p port] [-Q query_option] [-R address] [-S ctl_path] [-W host:port] [-w local_tun[:remote_tun]]
[user@]hostname [command]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If SSH is not installed, you can install it using the following commands.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;sudo apt-get install openssh-server
sudo systemctl enable ssh
sudo systemctl start ssh
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now that you have installed SSH, we are ready to setup a remote connection.&lt;/p&gt;
&lt;p&gt;From the computer you want to access remotey, generate the public key.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;ssh-keygen -t rsa
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You will be asked for a location. If you decide to enter one manually then that will be the pairâs location, if you leave the default one it will be inside the &lt;code&gt;.ssh&lt;/code&gt; hidden folder in your home directory.&lt;/p&gt;
&lt;p&gt;Now you will be prompted for a password. If you enter one you will be asked for it every time you use the key, this works for added security. If you donât want a password just press enter and continue without one.&lt;/p&gt;
&lt;p&gt;Two files were created. One file ends with the â.pubâ extension and the other one doesnât. The file that ends with â.pubâ is your public key. This key needs to be in the computer you want to connect to (the server) inside a file called &lt;code&gt;authorized_keys&lt;/code&gt; . You can accomplish this with the following command:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;ssh-copy-id username@ip
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For example in my case to send the key to my computer it would be:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;ssh-copy-id sergiop@132.132.132.132
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you have MacOS thereâs a chance you donât have ssh-copy-id installed, in that case you can install it using&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;brew install ssh-copy-id
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you haven&amp;rsquo;t installed &lt;code&gt;brew&lt;/code&gt;, you can install it by following &lt;a href=&#34;https://brew.sh/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;this guide&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;connect&#34;&gt;Connect&lt;/h2&gt;
&lt;p&gt;To permanently add the SSH key, you can use the follwing command&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;ssh-add directory\key.pem
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Lastly, to connect, just type the following command.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;ssh username@ip
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Where &lt;code&gt;username&lt;/code&gt; is the server name and &lt;code&gt;ip&lt;/code&gt; is the public IP adress, e.g. 132.132.132.132.&lt;/p&gt;
&lt;p&gt;If your server is not public, you will not be able to access it.&lt;/p&gt;
&lt;p&gt;If your server is password protected, you will be prompted to insert a password when you connect. If not, you should protect it with a password.&lt;/p&gt;
&lt;h2 id=&#34;managing-screens&#34;&gt;Managing screens&lt;/h2&gt;
&lt;p&gt;While you are connected to the remote terminal, any disturbance to your connection will interrupt the code. In order to avoid that, you want to create separate screens. This will allow your code to run remotely undisturbed, irrespectively of your connection.&lt;/p&gt;
&lt;p&gt;First, you need to install &lt;code&gt;screen&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;brew install screen
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To create a new screen, just type&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;screen
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now you can lunch your code.&lt;/p&gt;
&lt;p&gt;After that, you want to detach from that screen so that the code can run remotely undisturbed.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;screen -d
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Another option is to use &lt;code&gt;ctrl+a&lt;/code&gt; followed by &lt;code&gt;ctrl+d&lt;/code&gt;. This will detach the screen without the need to type anythin in the terminal, in case the terminal is busy (most likely).&lt;/p&gt;
&lt;p&gt;To list the current active screens type&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;screen -ls
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you want to check at any time that your code is running, without re-attaching to the screen, you can just type&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;top
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;which is the general command to check active processes. To exit, use &lt;code&gt;ctrl+z&lt;/code&gt;, which generally terminates processes in the terminal.&lt;/p&gt;
&lt;p&gt;To reattach to your screen, type&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;screen -r
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In case you have multiple screens (you can check with &lt;code&gt;screen -ls&lt;/code&gt;), you can reattach to a specific one by typing&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;screen -r 12345
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;where &lt;code&gt;12345&lt;/code&gt; is the id of the screen.&lt;/p&gt;
&lt;p&gt;To kill a screen, type&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;screen -XS 12345 quit
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;where again &lt;code&gt;12345&lt;/code&gt; is the id of the screen.&lt;/p&gt;
&lt;h2 id=&#34;python-and-pycharm&#34;&gt;Python and Pycharm&lt;/h2&gt;
&lt;p&gt;If you are coding in Python, &lt;a href=&#34;https://www.jetbrains.com/pycharm/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PyCharm&lt;/a&gt; is one of the best IDEs. Among many features, it offers the possibility to set a remote compiler for your pthon console and to sync input and output files automatically.&lt;/p&gt;
&lt;p&gt;First, you need to have setup a remote SSH connection following the steps above. Importantly, you need to have added the public key to your machine using the &lt;code&gt;ssh-add&lt;/code&gt; command, as explained above.&lt;/p&gt;
&lt;p&gt;Then open Pytharm, go to the lower-right corner, where the current interpreter is listed (e.g. Pytohn 3.8), click it and select &lt;code&gt;interpreter settings&lt;/code&gt;.&lt;/p&gt;
&lt;img src=&#34;files/interpreter_settings.png&#34; alt=&#34;interpreter_settings&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;Click on the gear icon âï¸ on the top-right corner and select &lt;code&gt;add&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;files/add.png&#34; alt=&#34;add&#34;&gt;&lt;/p&gt;
&lt;p&gt;Insert the server &lt;code&gt;host&lt;/code&gt; (IP address, e.g. 132.132.132.132) and &lt;code&gt;username&lt;/code&gt; (e.g. sergiop).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;files/configuration.png&#34; alt=&#34;configuration&#34;&gt;&lt;/p&gt;
&lt;p&gt;Next, you have to insert your credentials. If you have a password, insert it, otherwise you have to insert the path to your SSH key file.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;files/password.png&#34; alt=&#34;configuration&#34;&gt;&lt;/p&gt;
&lt;p&gt;Lastly, select the remote interpreter. If you are using a python version that is not default, browse to the preferred python installation folder. Also, check the box for &lt;code&gt;execute code giving this interpreter with root privileges via sudo&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;You can also select which remote folder to sync with your local project. By default, you are given a &lt;code&gt;tmp/pycharm_project_XX&lt;/code&gt; folder. You can change it if you want. I recommend also to have the last option checked: &lt;code&gt;automatically sync project files to the server&lt;/code&gt;. This will automatically synch all remote changes with your local machine, in your local project folder.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;files/folder.png&#34; alt=&#34;configuration&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;julia-and-juno&#34;&gt;Julia and Juno&lt;/h2&gt;
&lt;p&gt;If you are coding in Julia, &lt;a href=&#34;https://junolab.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Juno&lt;/a&gt; is the best IDE around. It&amp;rsquo;s an integration with &lt;a href=&#34;https://atom.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Atom&lt;/a&gt; with a dedicated compiler, local variables, syntax highlight, autocompletion.&lt;/p&gt;
&lt;p&gt;On Atom, you first need to install the &lt;a href=&#34;https://github.com/h3imdall/ftp-remote-edit&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;ftp-remote-edit&lt;/code&gt;&lt;/a&gt; package.&lt;/p&gt;
&lt;p&gt;Then go to the menu item &lt;code&gt;Packages &amp;gt; Ftp-Remote-Edit &amp;gt; Toggle&lt;/code&gt;.&lt;/p&gt;
&lt;img src=&#34;files/toggle.png&#34; alt=&#34;julia&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;A new &lt;code&gt;Remote&lt;/code&gt; panel will open with the default button to &lt;code&gt;Edit a new server&lt;/code&gt;.&lt;/p&gt;
&lt;img src=&#34;files/edit.png&#34; alt=&#34;julia&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;Click it and you will be able to set up your remote connection.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Press &lt;code&gt;New&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Insert your username in &lt;code&gt;The name of the server&lt;/code&gt;, for example &lt;code&gt;sergiop&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Insert your ip adress  in &lt;code&gt;The hostname or IP adress of the server&lt;/code&gt;, for example &lt;code&gt;123.123.123.123&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Select &lt;code&gt;SFTP - SSH File Transfer Protocol&lt;/code&gt; under &lt;code&gt;Protocol&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Select your &lt;code&gt;Logon&lt;/code&gt; option. You can either insert your password every time, just once, or use a keyfile.&lt;/li&gt;
&lt;li&gt;Insert again your username in &lt;code&gt;Username for autentication&lt;/code&gt;, again for example &lt;code&gt;sergiop&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;If you don&amp;rsquo;t want to start from the root folder, you can change the &lt;code&gt;Initial Directory&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;files/julia.png&#34; alt=&#34;julia&#34;&gt;&lt;/p&gt;
&lt;p&gt;Now you will be able to see your remote directory (named for example &lt;code&gt;sergiop&lt;/code&gt;) in the &lt;code&gt;Remote&lt;/code&gt; panel.&lt;/p&gt;
&lt;p&gt;To start using Julia remotely, just start a new remote Julia process from the menu on the left.&lt;/p&gt;
&lt;img src=&#34;files/remote.png&#34; alt=&#34;julia&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;Now you are ready to deploy your Julia code on your remote server!&lt;/p&gt;
&lt;h2 id=&#34;jupyter-notebooks&#34;&gt;Jupyter Notebooks&lt;/h2&gt;
&lt;p&gt;If you want to have a &lt;a href=&#34;https://jupyter.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Jupyter Notebook&lt;/a&gt; running remotely, the steps are the following. The main advantage of a Jupyter Notebook is that it allows you to mix text and code in a single file, similarly to &lt;a href=&#34;https://rmarkdown.rstudio.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;RMarkdown&lt;/a&gt;, with the advantage of not being contrained to use a R (or Python) kernel. For example, I often use Jupyter Notebook with Julia or Matlab Kernels. Moreover, you can also make nice slides out of it!&lt;/p&gt;
&lt;p&gt;First, connect to the remote machine. Look at &lt;a href=&#34;#setup&#34;&gt;section 1&lt;/a&gt; to set up your SSH connection.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;ssh username@ip
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Start a Jupyter Notebook in the remote machine.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;jupyter notebook --no-browser
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The command will open a jupyter notebook in the remote machine. To connect to it, we need to know which port it used. The default port is &lt;code&gt;8888&lt;/code&gt;. If that port is busy, it will look for another available one. We can see the port from the output in terminal.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Jupyter Notebook is running at:  http://localhost:XXXX/&amp;hellip;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Where &lt;code&gt;XXXX&lt;/code&gt; is the repote port used.&lt;/p&gt;
&lt;p&gt;Now we need to forward the remote port &lt;code&gt;XXXX&lt;/code&gt; to our local &lt;code&gt;YYYY&lt;/code&gt; port.&lt;/p&gt;
&lt;p&gt;Open a new &lt;em&gt;local&lt;/em&gt; shell. Type&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;ssh -L localhost:YYYY:localhost:XXXX username@ip
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Where &lt;code&gt;YYYY&lt;/code&gt; can be anything. I&amp;rsquo;d use the default port &lt;code&gt;8888&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;ssh -L localhost:8889:localhost:8888 username@ip
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now go to your browser and type&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;localhost:YYYY
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Which in my case is&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;localhost:8889
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This will open the remote Jupyter Notebook.&lt;/p&gt;
&lt;p&gt;Done!&lt;/p&gt;
&lt;p&gt;In case you want to check which Jupiter notebooks are running, type&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;jupyter notebook list
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To kill a notebook use&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;jupyter notebook stop XXXX
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;sources&#34;&gt;Sources&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://medium.com/@SergioPietri/how-to-setup-and-use-ssh-for-remote-connections-e86556d804dd&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;How To Setup And Use SSH For Remote Connections&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.junolab.org/stable/man/remote/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Connecting to a Julia session on a remote machine&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://ljvmiranda921.github.io/notebook/2018/01/31/running-a-jupyter-notebook/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Running a Jupyter notebook from a remote server&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>My Color Palette</title>
      <link>https://matteocourthoud.github.io/post/colors/</link>
      <pubDate>Sun, 24 Oct 2021 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/post/colors/</guid>
      <description>&lt;p&gt;Ok, this is a fun post. I am choosing&amp;hellip; &lt;strong&gt;my color palette&lt;/strong&gt;!&lt;/p&gt;
&lt;p&gt;I have decided to unify all the color palettes I have on my website, slides, graphs, etc&amp;hellip; into a unique universal color palette.&lt;/p&gt;
&lt;h2 id=&#34;main-color&#34;&gt;Main Color&lt;/h2&gt;
&lt;p&gt;First of all, I have to choose my main color.&lt;/p&gt;
&lt;!-- Coolors Palette Widget --&gt;   
&lt;script src=&#34;https://coolors.co/palette-widget/widget.js&#34;&gt;&lt;/script&gt;     
&lt;script data-id=&#34;038499353489456634&#34;&gt;new CoolorsPaletteWidget(&#34;038499353489456634&#34;, [&#34;003f5c&#34;]); &lt;/script&gt;
&lt;br&gt;
&lt;p&gt;Here are some shades of it.&lt;/p&gt;
&lt;!-- Coolors Palette Widget --&gt;
&lt;script src=&#34;https://coolors.co/palette-widget/widget.js&#34;&gt;&lt;/script&gt;
&lt;script data-id=&#34;030474315233368743&#34;&gt;new CoolorsPaletteWidget(&#34;030474315233368743&#34;, [&#34;002637&#34;,&#34;00324a&#34;,&#34;003f5c&#34;,&#34;17506b&#34;,&#34;2c6078&#34;]); &lt;/script&gt;
&lt;br&gt;
&lt;h2 id=&#34;related-palettes&#34;&gt;Related Palettes&lt;/h2&gt;
&lt;p&gt;Now I will build a couple of colors palettes based on it.&lt;/p&gt;
&lt;p&gt;The first one, is red oriented.&lt;/p&gt;
&lt;!-- Coolors Palette Widget --&gt;
&lt;script src=&#34;https://coolors.co/palette-widget/widget.js&#34;&gt;&lt;/script&gt;
&lt;script data-id=&#34;04738379522618503&#34;&gt;new CoolorsPaletteWidget(&#34;04738379522618503&#34;, [&#34;003f5c&#34;,&#34;444e86&#34;,&#34;955196&#34;,&#34;dd5182&#34;,&#34;ff6e54&#34;,&#34;ffa600&#34;]); &lt;/script&gt;
&lt;br&gt;
&lt;p&gt;Second one, is green oriented.&lt;/p&gt;
&lt;!-- Coolors Palette Widget --&gt;
&lt;script src=&#34;https://coolors.co/palette-widget/widget.js&#34;&gt;&lt;/script&gt;
&lt;script data-id=&#34;041398899421411417&#34;&gt;new CoolorsPaletteWidget(&#34;041398899421411417&#34;, [&#34;003f5c&#34;,&#34;00677f&#34;,&#34;00908f&#34;,&#34;2db88b&#34;,&#34;94dc7b&#34;,&#34;f9f871&#34;]); &lt;/script&gt;
&lt;br&gt;
&lt;h2 id=&#34;color-sequence&#34;&gt;Color Sequence&lt;/h2&gt;
&lt;p&gt;Now I need a high contrast scheme for graphs. I add one color at the time to check that contrast is always maximized.&lt;/p&gt;
&lt;!-- Coolors Palette Widget --&gt;
&lt;script src=&#34;https://coolors.co/palette-widget/widget.js&#34;&gt;&lt;/script&gt;
&lt;script data-id=&#34;09725467508229444&#34;&gt;new CoolorsPaletteWidget(&#34;09725467508229444&#34;, [&#34;003f5c&#34;,&#34;ff6e54&#34;,&#34;f9f871&#34;,&#34;2db88b&#34;,&#34;955196&#34;]); &lt;/script&gt;
&lt;br&gt;
&lt;p&gt;A milder version of the same palette is:&lt;/p&gt;
&lt;!-- Coolors Palette Widget --&gt;
&lt;script src=&#34;https://coolors.co/palette-widget/widget.js&#34;&gt;&lt;/script&gt;
&lt;script data-id=&#34;09460303923704312&#34;&gt;new CoolorsPaletteWidget(&#34;09460303923704312&#34;, [&#34;00798c&#34;,&#34;d1495b&#34;,&#34;edae49&#34;,&#34;52a369&#34;,&#34;756ab2&#34;]); &lt;/script&gt;
&lt;h2 id=&#34;resources&#34;&gt;Resources&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;To generate the palette: &lt;a href=&#34;https://learnui.design/tools/data-color-picker.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://learnui.design/tools/data-color-picker.html&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;To generate the palette (2): &lt;a href=&#34;https://mycolor.space/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://mycolor.space/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;To get different shades of the same color: &lt;a href=&#34;https://www.tutorialrepublic.com/html-reference/html-color-picker.php&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.tutorialrepublic.com&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;To build the palette: &lt;a href=&#34;https://htmlcolorcodes.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://htmlcolorcodes.com/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;To test the palette: &lt;a href=&#34;http://colormind.io/bootstrap/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://colormind.io/bootstrap/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;To embed the palette in a HTML page: &lt;a href=&#34;https://coolors.co/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://coolors.co/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Online Seminars in Economics and Finance</title>
      <link>https://matteocourthoud.github.io/post/seminars/</link>
      <pubDate>Sat, 09 Oct 2021 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/post/seminars/</guid>
      <description>&lt;p&gt;I will use this page to collect information about online seminars in Economics and Finance.&lt;/p&gt;
&lt;p&gt;If you know about public seminars that are missing from this list, please either &lt;a href=&#34;mailto:matteo.courthoud@econ.uzh.ch&#34;&gt;contact me&lt;/a&gt; or &lt;a href=&#34;https://github.com/matteocourthoud/website/blob/master/content/post/seminars/index.md&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;edit the table on Github&lt;/a&gt;!&lt;/p&gt;
&lt;h2 id=&#34;monday&#34;&gt;Monday&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Seminar&lt;/th&gt;
&lt;th&gt;Field&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Time (CET)&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Registration&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Recorded&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://gametheorynetwork.com/one-world-game-theory-seminar/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;One World Mathematical Game Theory Seminar&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Theory&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;15:00&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;No&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;No&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;http://virtual-md-seminar.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Virtual Market Design Seminar Series&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Theory&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;16:00&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;http://virtual-md-seminar.com/registration.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Yes&lt;/a&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;No&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://sites.google.com/site/berlinappliedmicroseminar/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Berlin Applied Micro Seminar (BAMS)&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Applied&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;16:00&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://sites.google.com/site/berlinappliedmicroseminar/co_hosted_events&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Yes&lt;/a&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;No&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://sites.google.com/view/vibesecon/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Virtual Behavioral Economics Seminar (VIBES)&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Behavioral&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;17:00&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://forms.gle/nAE6VLEZQqeRkSuK6&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Yes&lt;/a&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;No&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://vquantmarketing.substack.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Virtual Quantitative Marketing Seminar&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Marketing&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;17:30&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://vquantmarketing.substack.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Yes&lt;/a&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;No&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://sammf.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Search and Matching in Macro and Finance Virtual Seminar Series&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Macro&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;18:00&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://sammf.com/sign-up/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Yes&lt;/a&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;No&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;tuesday&#34;&gt;Tuesday&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Seminar&lt;/th&gt;
&lt;th&gt;Field&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Time (CET)&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Registration&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Recorded&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://www.tse-fr.eu/new-online-seminar-economics-platforms&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Online Economics of Platforms Seminar&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;IO&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;14:00&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;mailto:marie-helene.dufour@tse-fr.eu&#34;&gt;Yes&lt;/a&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://www.youtube.com/channel/UCQLTomj3LkQ_8rKGfxxmYvw&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Yes&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://persuasion.wp.st-andrews.ac.uk/seminars/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Seminar Series on Communication and Persuasion&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Theory&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;16:00&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://docs.google.com/forms/d/e/1FAIpQLSc3Sq3GGNDEIa5lcbda9a45sjVldHZrJlRKH-jyPobZ1oE2Aw/viewform?usp=sf_link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Yes&lt;/a&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;No&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://sites.google.com/view/ocis/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Online Causal Inference Seminar&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Econometrics&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;16:30&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://www.google.com/url?q=https%3A%2F%2Fmailman.stanford.edu%2Fmailman%2Flistinfo%2Fonline-causal-inference-seminar&amp;amp;sa=D&amp;amp;sntz=1&amp;amp;usg=AFQjCNGgPMLB-5Iv0SRBiJHXlIhxo2ta2A&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Yes&lt;/a&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://sites.google.com/view/ocis/past-talks-and-recordings?authuser=0&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Yes&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://vdevecon.wixsite.com/website&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Virtual Development Economics Seminar Series (VDEV)&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Development&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;17:00&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://us02web.zoom.us/webinar/register/WN_m4Ws1VxXRry_kwZoT8T5WA&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Yes&lt;/a&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://www.youtube.com/channel/UC9NMehzZBlChKSiie1DFCaA/featured?view_as=public&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Yes&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://sites.google.com/view/virtualmacro/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Virtual Macro Seminar Series&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Macro&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;18:00&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://www.google.com/url?q=https%3A%2F%2Fstockholmuniversity.zoom.us%2Fwebinar%2Fregister%2F7815862675026%2FWN_yNkq5FpES9yJjpjdqYWFbA&amp;amp;sa=D&amp;amp;sntz=1&amp;amp;usg=AFQjCNHWMu_cBPhLdq_uqy1eugAzEy2bzg&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Yes&lt;/a&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://sites.google.com/view/virtualmacro/past-seminars&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Yes&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://sites.google.com/view/lfos&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Labor &amp;amp; Finance Online Seminar&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Labor&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;19:15&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://sites.google.com/view/lfos/register&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Yes&lt;/a&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;No&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://web.stanford.edu/~leinav/teaching/IOIOspring2020.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Interactive Online IO Seminar (IO^2)&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;IO&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;21:00&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://stanford.zoom.us/webinar/register/WN_A85qA0DSQmmJeHBimyw2MQ&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Yes&lt;/a&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;No&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;wednesday&#34;&gt;Wednesday&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Seminar&lt;/th&gt;
&lt;th&gt;Field&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Time (CET)&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Registration&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Recorded&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://sites.google.com/view/virtual-io-seminar/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CEPR Virtual IO Seminar (VIOS)&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;IO&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;16:00&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://drive.google.com/open?id=1-dtGhiM2EqtXnkTztoLzDO7cNB_ekOFfbuckSSIotxU&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Yes&lt;/a&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;No&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://mailchi.mp/cepr/cepr-webinar-polecon-series-reminder&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CEPR Political Economy Seminar Series&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Political&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;16:00&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://zoom.us/webinar/register/WN_3MqzgQR2RQ-1nu8DD2LxeA&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Yes&lt;/a&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;No&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://www.jku.at/en/department-of-economics/research/research-events/online-economics-research-seminar/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ONLINE Economics Research Seminar (JKU)&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Applied&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;16:00&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;mailto:alexander.ahammer@jku.at&#34;&gt;Yes&lt;/a&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://www.jku.at/en/department-of-economics/research/research-events/online-economics-research-seminar/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Yes&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://economics.uchicago.edu/content/afe-seminar-series&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Virtual AFE Seminar Series&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Experimental&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;18:00&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://economics.uchicago.edu/content/afe-2020&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Yes&lt;/a&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;No&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://drive.google.com/file/d/1FEnt42opuzpQiJtPUF1lHqN3Zdp9mdhZ/view&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Virtual Finance Theory Seminar&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Finance&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;18:00&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;mailto:[virtualfinancetheoryseminar.com]%28mailto:mail@virtualfinancetheoryseminar.com%29&#34;&gt;Yes&lt;/a&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;No&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://sites.google.com/site/plustcs/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;TCS+&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Computer Science&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;19:00&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://sites.google.com/site/plustcs/livetalk/live-seat-reservation&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Yes&lt;/a&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://sites.google.com/site/plustcs/past-talks&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Yes&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;http://tamuz.caltech.edu/cettc/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Caltech Economic Theory at the Time of Cholera&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Theory&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;21:00&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;No&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;http://tamuz.caltech.edu/cettc/#x1-50003&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Yes&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;thursday&#34;&gt;Thursday&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Seminar&lt;/th&gt;
&lt;th&gt;Field&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Time (CET)&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Registration&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Recorded&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://sites.google.com/view/euro-quant-marketing-seminar/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;European Quant Marketing Seminar&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Marketing&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;14:00&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;No&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://sites.google.com/view/euro-quant-marketing-seminar/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Yes&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://sites.google.com/view/macci-epos-virtual-io-seminar&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;MaCCI EPoS Virtual IO Seminar&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;IO&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;15:00&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://sites.google.com/view/macci-epos-virtual-io-seminar/program/registration&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Yes&lt;/a&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;No&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://www.digitalecon.org/seminar&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Virtual Digital Economy Seminar (VIDE)&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Media&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;16:00&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://www.digitalecon.org/seminar&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Yes&lt;/a&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://www.digitalecon.org/seminar/past-seminars&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Yes&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;http://middexlab.weebly.com/virtual-seminar-series.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;MiddExLab Virtual Seminar Series&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Experimental&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;18:00&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;http://middexlab.weebly.com/virtual-seminar-series.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Yes&lt;/a&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;No&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://www.tradedynamics.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Virtual International Trade and Macro Seminar&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Macro&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;20:00&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://forms.gle/uiNjSptWjvucDbHSA&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Yes&lt;/a&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://www.tradedynamics.org/video-archive&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Yes&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;friday&#34;&gt;Friday&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Seminar&lt;/th&gt;
&lt;th&gt;Field&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Time (CET)&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Registration&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Recorded&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://sites.google.com/view/comsoc-seminar/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;International Seminar Series on Social Choice (COMSOC)&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Theory&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;15:00&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://www.google.com/url?q=https%3A%2F%2Flist.uva.nl%2Fmailman%2Flistinfo%2Fcomsoc-video-seminar&amp;amp;sa=D&amp;amp;sntz=1&amp;amp;usg=AFQjCNEAx47kVC25VUer3fp05Mw7vcZwuA&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Yes&lt;/a&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://sites.google.com/view/comsoc-seminar/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Yes&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://web.stanford.edu/~leinav/teaching/IOIOspring2020.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Interactive Online IO Seminar (IO^2)&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;IO&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;16:00&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://stanford.zoom.us/webinar/register/WN_Tb_FyMJ1RCeNF1HmCVvjMQ&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Yes&lt;/a&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;No&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://sites.google.com/view/dcpec/events/webinar&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Political Economy Webinar Series&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Political&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;17:00&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://forms.gle/yohLo3pk898Yq9Sf9&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Yes&lt;/a&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;No&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://www.chamberlainseminar.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The Gary Chamberlain Online Seminar&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Econometrics&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;18:00&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://www.google.com/url?q=https%3A%2F%2Fmailman.stanford.edu%2Fmailman%2Flistinfo%2Fchamberlainseminar&amp;amp;sa=D&amp;amp;sntz=1&amp;amp;usg=AFQjCNHJFMMiJMowt_vAtuBWbrK-4PA2IA&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Yes&lt;/a&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://www.chamberlainseminar.org/past-seminars&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Yes&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://www.virtualfinance.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Virtual Finance Workshop&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Finance&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;18:30&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;http://www.google.com/url?q=http%3A%2F%2Feepurl.com%2FgYBYvP&amp;amp;sa=D&amp;amp;sntz=1&amp;amp;usg=AFQjCNGNgL-zAqniptGfniTAeZ_cTkOyLg&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Yes&lt;/a&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;No&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;http://www.rohitlamba.com/penntheon&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Penn State Theory Online&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Theory&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;20:00&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;mailto:l-micropenntheon-subscribe-request@lists.psu.edu&#34;&gt;Yes&lt;/a&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;http://www.rohitlamba.com/penntheon&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Yes&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://sites.google.com/view/dstheory/home&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Foundations of Data Science Virtual Talk Series&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Computer Science&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;20:00&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;No&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://sites.google.com/view/dstheory/past-talks&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Yes&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
</description>
    </item>
    
    <item>
      <title>Frequently Asked Questions in a PhD</title>
      <link>https://matteocourthoud.github.io/post/faq/</link>
      <pubDate>Tue, 05 Oct 2021 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/post/faq/</guid>
      <description>&lt;p&gt;In this page I will collect anquestions that I frequently asked myself during my PhD, possibly with answers.&lt;/p&gt;
&lt;p&gt;Personally, the article for PhD students that helped me the most is &lt;a href=&#34;https://medium.com/@paul.niehaus/doing-research-18cb310529e0&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;âDoing researchâ&lt;/a&gt; by Paul Niehaus. But beware, it might not work for everyone.&lt;/p&gt;
&lt;h2 id=&#34;starting-the-phd&#34;&gt;Starting the PhD&lt;/h2&gt;
&lt;h3 id=&#34;information&#34;&gt;Information&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.nature.com/articles/d41586-019-03459-7?sf223557541=1&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;ldquo;PhDs: the tortuous truth&amp;rdquo;&lt;/a&gt;, Chris Woolston, 2019.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://medium.economist.com/why-doing-a-phd-is-often-a-waste-of-time-349206f9addb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;ldquo;Why doing a PhD is often a waste of time&amp;rdquo;&lt;/a&gt;, The Economist, 2016&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.theguardian.com/careers/phd-right-career-option&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;ldquo;Should you do a PhD?&amp;quot;&lt;/a&gt;, Daniel K. Sokol, 2012.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://tertilt.vwl.uni-mannheim.de/bachelor/GradSchoolGuide.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;ldquo;So, you want to go to a grad school in economics?&amp;quot;&lt;/a&gt;, Ceyhun Elgin and Mario Solis-Garcia, 2007.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;applying&#34;&gt;Applying&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://james-tierney.medium.com/how-to-ask-your-professor-for-a-letter-of-recommendation-f06e8b2f2c64&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;ldquo;How to Ask Your Professor for a Letter of Recommendation&amp;rdquo;&lt;/a&gt;, James Tierney, 2020.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://drive.google.com/file/d/16eUvtahziPyBTpX_ZeyXjPck2OyinfHH/view&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;ldquo;Pre-Doc Guide&amp;rdquo;&lt;/a&gt;, Alvin Christian, 2019.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://athey.people.stanford.edu/professional-advice&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;ldquo;Advice for Applying to Grad School in Economics&amp;rdquo;&lt;/a&gt;, Susan Athey, 2016.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://qz.com/116081/the-complete-guide-to-getting-into-an-economics-phd-program/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;ldquo;The complete guide to getting into an economics PhD program&amp;rdquo;&lt;/a&gt;, Miles Kimball, 2013.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.ericzwick.com/public_goods/twelve_steps.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;ldquo;The 12 Step Program for Grad School&amp;rdquo;&lt;/a&gt;, Erik Zwick.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;starting&#34;&gt;Starting&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/hagertynw/grad-school-reflections/blob/master/grad_school_reflections.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;ldquo;Reflections on Grad School in Economics&amp;rdquo;&lt;/a&gt;, Nick Hagerty, 2020.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://law.vanderbilt.edu/phd/How_to_Survive_1st_Year.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;ldquo;How to survive your first year of graduate school in economics&amp;rdquo;&lt;/a&gt;, Matthew Pearson, 2005.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;during-the-phd&#34;&gt;During the PhD&lt;/h2&gt;
&lt;h3 id=&#34;mental-health&#34;&gt;Mental Health&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://scholar.harvard.edu/files/bolotnyy/files/bbb_mentalhealth_paper.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;ldquo;Graduate Student Mental Health: Lessons from American Economics Departments&amp;rdquo;&lt;/a&gt;, Bolotnyy, Valentin, Matthew Basilico, and Paul Barreira, 2021.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.insidehighered.com/news/2019/11/14/phd-student-poll-finds-mental-health-bullying-and-career-uncertainty-are-top&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;ldquo;Mental Health, Bullying, Career Uncertainty&amp;rdquo;&lt;/a&gt;, Colleen Flahert, 2019.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.sciencemag.org/careers/2019/03/how-mindfulness-can-help-phd-students-deal-mental-health-challenges&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;ldquo;How mindfulness can help Ph.D. students deal with mental health challenges&amp;rdquo;&lt;/a&gt;, Katie Langin, 2019.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.phdstudies.com/article/managing-your-mental-health-as-a-phd-student/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;ldquo;Managing Your Mental Health as a PhD Student&amp;rdquo;&lt;/a&gt;, Joanna Hughes, 2019.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.psychologytoday.com/us/blog/emotional-mastery/201904/what-makes-it-so-hard-ask-help&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;ldquo;What Makes It So Hard to Ask for Help?&amp;quot;&lt;/a&gt;, Joan Rosenberg, 2019.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.sciencemag.org/careers/2018/11/grad-school-depression-almost-took-me-end-road-i-found-new-start&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;ldquo;Grad school depression almost took me to the end of the roadâbut I found a new start&amp;rdquo;&lt;/a&gt;, Francis Aguisanda, 2018.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.nature.com/articles/nj7587-555a.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;ldquo;Faking it&amp;rdquo;&lt;/a&gt;, Chris Woolston, 2016.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://blogs.nature.com/naturejobs/2016/09/14/panic-and-a-phd/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;ldquo;Panic and a PhD&amp;rdquo;&lt;/a&gt;, Jack Leeming, 2016.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://qz.com/547641/theres-an-awful-cost-to-getting-a-phd-that-no-one-talks-about/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;ldquo;Thereâs an awful cost to getting a PhD that no one talks about&amp;rdquo;&lt;/a&gt;, Jennifer Walker, 2015.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;research-and-ideas&#34;&gt;Research and Ideas&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://www.ricardodahis.com/files/papers/Dahis_Advice_Research.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;ldquo;Advice for Academic Research&amp;rdquo;&lt;/a&gt;, Ricardo Dahis, 2021.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.aeaweb.org/articles?id=10.1257/jel.20191573&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;ldquo;Sins of Omission and the Practice of Economics&amp;rdquo;&lt;/a&gt;, George A. Akerlof, 2020.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://medium.com/@paul.niehaus/doing-research-18cb310529e0&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;ldquo;Doing research&amp;rdquo;&lt;/a&gt;, Paul Niehaus, 2019.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://static1.squarespace.com/static/55c143d9e4b0cb07521c6d17/t/5b4f409f575d1ff83c2f12d8/1531920545061/PhDGuidebook.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;ldquo;An unofficial guidebook for PhD students in economics and education&amp;rdquo;&lt;/a&gt;, Alex Eble, 2018.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://iu.box.com/s/0ha9gcq0t22kyyy1rqv15mkmauw1py18&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;ldquo;The Research Productivity of New PhDs in Economics: The Surprisingly High Non-Success of the Successful&amp;rdquo;&lt;/a&gt;, John P. Conley and Ali Sina Ãnder, 2014.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://econ.lse.ac.uk/staff/spischke/phds/How%20to%20start.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;ldquo;How to get started on research in economics?&amp;quot;&lt;/a&gt;, Steve Pischke, 2009.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://iu.box.com/s/km7cxhcxgfcdpk4cp38b47x7is7lum11&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;ldquo;The Importance of Stupidity in Scientific Research&amp;rdquo;&lt;/a&gt;, Martin A. Schwartz, 2008.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://stevepavlina.com/blog/2007/01/7-rules-for-maximizing-your-creative-output/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;ldquo;7 Rules for Maximizing Your Creative Output&amp;rdquo;&lt;/a&gt;, Steve Pavlina, 2007.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://people.ischool.berkeley.edu/~hal/Papers/how.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;ldquo;How To Build An Economic Model in Your Spare Time&amp;rdquo;&lt;/a&gt;, Hal. R. Varian, 1998.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.columbia.edu/~drd28/Thesis%20Research.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;ldquo;Ph.D. Thesis Research: Where do I Start?&amp;quot;&lt;/a&gt;, Don Davis.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;presenting&#34;&gt;Presenting&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://david-schindler.de/unfair-questions/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;ldquo;Unfair Questions&amp;rdquo;&lt;/a&gt;, David Schindler, 2021.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/paulgp/beamer-tips/blob/master/slides.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;ldquo;Beamer Tips for Presentations&amp;rdquo;&lt;/a&gt;, Paul Goldsmith-Pinkham, 2020.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.princeton.edu/~reddings/tradephd/public_speaking_for_academic_economists.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;ldquo;Public Speaking for Academic Economists&amp;rdquo;&lt;/a&gt;, Rachel Meager, 2017.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.europeanjobmarketofeconomists.org/uploads/HowToPresent_LaFerrara.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;ldquo;How to present your job market paper&amp;rdquo;&lt;/a&gt;, Eliana La Ferrara, 2018.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://people.bu.edu/guren/Guren_HowToGiveALunchTalk.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;ldquo;How To Give a Lunch Talk&amp;rdquo;&lt;/a&gt;, Adam Guren, 2018.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://chrisblattman.com/2010/02/22/the-discussants-art/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;ldquo;The Discussant&amp;rsquo;s Art&amp;rdquo;&lt;/a&gt;, Chris Blattman, 2010.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://papers.ssrn.com/sol3/papers.cfm?abstract_id=1332144&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;ldquo;How to be a Great Conference Participants&amp;rdquo;&lt;/a&gt;, Art Carden, 2009.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://econ.lse.ac.uk/staff/spischke/phds/The%20Big%205.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;ldquo;The &amp;ldquo;Big 5&amp;rdquo; and Other Ideas For Presentations&amp;rdquo;&lt;/a&gt;, Cox, Donald,  2000.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://iu.box.com/s/aw92d7kl7xh5s4zsub8jq3qnknq9zcsi&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;ldquo;How to Give an Applied Micro Talk&amp;rdquo;&lt;/a&gt;, Jesse M. Shapiro.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://iu.box.com/s/37j3eip7x9fdg30n4eeepu92228eb999&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;ldquo;Tips on How to Avoid Disaster in Presentations&amp;rdquo;&lt;/a&gt;, Monika Piazzesi.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.ssc.wisc.edu/~bhansen/placement/Seminar%20Slides.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;ldquo;Seminar Slides &amp;ldquo;&lt;/a&gt;, Bruce Hansen.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;writing&#34;&gt;Writing&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.google.com/url?q=https%3A%2F%2Fwww.dropbox.com%2Fs%2Fq7wjaidl5w91srt%2FGuest%20lecture%20FS.pdf%3Fdl%3D0&amp;amp;sa=D&amp;amp;sntz=1&amp;amp;usg=AFQjCNG_nRs6QlkZzWBHAy0PjF4jfEYBAw&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;ldquo;5 Steps Toward a Paper&amp;rdquo;&lt;/a&gt;, Frank Schilbach, 2019.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.nature.com/articles/d41586-019-02918-5&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;ldquo;Novelist Cormac McCarthyâs tips on how to write a great science paper&amp;rdquo;&lt;/a&gt;, Van Savage and Pamela Yeh, 2019.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://marcfbellemare.com/wordpress/12797&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;ldquo;The âMiddle Bitsâ Formula for Applied Papers&amp;rdquo;&lt;/a&gt;, Marc Bellamare, 2018.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://marcfbellemare.com/wordpress/12060&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;ldquo;The Conclusion Formula&amp;rdquo;&lt;/a&gt;, Marc Bellamare, 2018.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.albany.edu/spatial/training/5-The%20Introduction%20Formula.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;ldquo;The Introduction Formula&amp;rdquo;&lt;/a&gt;, Keith Head, 2015.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.people.fas.harvard.edu/~pnikolov/resources/writingtips.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;ldquo;Writing Tips For Economics Research Papers&amp;rdquo;&lt;/a&gt;, Plamen Nikolov, 2013.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://economics.harvard.edu/files/economics/files/tenruleswriting.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;ldquo;The Ten Most Important Rules of Writing Your Job Market Paper&amp;rdquo;&lt;/a&gt;, Goldin, Claudia and Lawrence Katz, 2008.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://schwert.ssb.rochester.edu/aec510/phd_paper_writing.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;ldquo;Writing Tips for Ph.D. Students&amp;rdquo;&lt;/a&gt;, John Cochrane, 2005.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://qed.econ.queensu.ca/pub/faculty/sumon/mkremer_checklist_paper.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;ldquo;Writing Papers: A Checklist&amp;rdquo;&lt;/a&gt;, Michael Kremer.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;referiing&#34;&gt;Referiing&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://blog.academicsequitur.com/2019/06/30/how-to-write-a-good-referee-report/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;ldquo;How To Write A Good Referee Report&amp;rdquo;&lt;/a&gt;, Tatyana Deryugina, 2019.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://iu.box.com/s/lgmhqw5uxvrb7qdrhxxskzki9pcwx7o6&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;ldquo;How to Review Manuscripts&amp;rdquo;&lt;/a&gt;, Elsevier, 2015.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://marcfbellemare.com/wordpress/5542&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;ldquo;Contributing to Public Goods: My 20 Rules for Refereeing&amp;rdquo;&lt;/a&gt;, Marc F. Bellemare, 2012&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;finishing-the-phd&#34;&gt;Finishing the PhD&lt;/h2&gt;
&lt;h3 id=&#34;the-job-market&#34;&gt;The Job Market&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.aeaweb.org/content/file?id=869&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;ldquo;A Guide and Advice for Economists on the U.S. Junior Academic Job Market 2018-2019 Edition&amp;rdquo;&lt;/a&gt;, John Cawley, 2018.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://chrisblattman.com/job-market/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;ldquo;Academic job market advice for economics, political science, public policy, and other professional schools&amp;rdquo;&lt;/a&gt;, Blattman, Christopher,  2015.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.ericzwick.com/public_goods/love_the_market.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;ldquo;How I Learned to Stop Worrying and Love the Job Market&amp;rdquo;&lt;/a&gt;, Erik Zwick, 2014.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;the-private-sector&#34;&gt;The Private Sector&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.linkedin.com/pulse/my-journey-from-economics-phd-data-scientist-tech-rose-tan&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;ldquo;My Journey from Economics PhD to Data Scientist in Tech&amp;rdquo;&lt;/a&gt;, Rose Tan, 2021.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://scarlet-chen.medium.com/tech-industry-jobs-for-econ-phds-54a276dda80b&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;ldquo;Tech Industry Jobs for Econ PhDs&amp;rdquo;&lt;/a&gt;, Scarlet Chen, 2020.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://scarlet-chen.medium.com/my-journey-from-econ-phd-to-tech-part-1-interview-prep-networking-d256918410a2&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;ldquo;My Journey from Econ PhD to Tech&amp;rdquo;&lt;/a&gt;, Scarlet Chen, 2020.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.nature.com/articles/d41586-018-05838-y&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;ldquo;Why it is not a âfailureâ to leave academia&amp;rdquo;&lt;/a&gt;, Philipp Kruger, 2018.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;the-tenure-track&#34;&gt;The Tenure Track&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://blogs.scientificamerican.com/guest-blog/the-awesomest-7-year-postdoc-or-how-i-learned-to-stop-worrying-and-love-the-tenure-track-faculty-life/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;ldquo;The Awesomest 7-Year Postdoc or: How I Learned to Stop Worrying and Love the Tenure-Track Faculty Life&amp;rdquo;&lt;/a&gt;, Radhika Nagpal, 2013.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;more&#34;&gt;More&lt;/h2&gt;
&lt;p&gt;You can find more resources here:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;AEA &lt;a href=&#34;https://www.aeaweb.org/about-aea/committees/cswep/mentoring/reading&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Mentoring Reading Materials&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Johannes Pfeifer &lt;a href=&#34;https://sites.google.com/site/pfeiferecon/job-market-resources&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Job Market Resources&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Kristoph Kronenberg &lt;a href=&#34;https://sites.google.com/view/christoph-kronenberg/home/resources&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Resources&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Patrick Button &lt;a href=&#34;https://www.patrickbutton.com/resources&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Resources&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Ryan Edwards &lt;a href=&#34;http://www.ryanbedwards.com/resources&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Resources&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Jennifer Doleac &lt;a href=&#34;http://jenniferdoleac.com/resources/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Resources&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Amanda Agan &lt;a href=&#34;https://sites.google.com/site/amandayagan/writingadvice&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Writing and Presentation Advice&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Random forum &lt;a href=&#34;http://www.inhe365.com/thread-17506-1-1.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Resource Collection&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>How To Make A Personal Website with Hugo</title>
      <link>https://matteocourthoud.github.io/post/website/</link>
      <pubDate>Thu, 23 Sep 2021 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/post/website/</guid>
      <description>&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;In this tutorial, I am going to explain how to build a personal website on &lt;a href=&#34;https://www.github.com&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt; using the &lt;strong&gt;&lt;a href=&#34;https://wowchemy.com/docs/getting-started/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Wowchemy&lt;/a&gt;&lt;/strong&gt; theme from &lt;a href=&#34;https://gohugo.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Hugo&lt;/a&gt;, also known as &lt;a href=&#34;https://themes.gohugo.io/academic/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Hugo Academic&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Note that Hugo offers &lt;a href=&#34;https://themes.gohugo.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;many other website templates&lt;/a&gt;. I suggest checking them out. Some interesting alternatives are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://themes.gohugo.io/theme/hugo-story/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://themes.gohugo.io/theme/hugo-story/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://themes.gohugo.io/theme/somrat/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://themes.gohugo.io/theme/somrat/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://themes.gohugo.io/theme/hugo-uilite/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://themes.gohugo.io/theme/hugo-uilite/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://themes.gohugo.io/theme/story/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://themes.gohugo.io/theme/story/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;However, I will concentrate on the Hugo Academic theme, since it&amp;rsquo;s the one I used for this website and it&amp;rsquo;s the the best one for building academic profile pages. I also suggest checking out &lt;a href=&#34;https://sites.google.com/new&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Google Sites&lt;/a&gt; as they are a valid alternative.&lt;/p&gt;
&lt;p&gt;The first part of this guide is general and valid for any Hugo theme. The original guide can be found &lt;a href=&#34;https://wowchemy.com/docs/getting-started/install/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;create-website&#34;&gt;Create Website&lt;/h2&gt;
&lt;h3 id=&#34;0-prerequisites&#34;&gt;0. Prerequisites&lt;/h3&gt;
&lt;p&gt;Before we start, I will take for granted the following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;that you have an account on &lt;a href=&#34;https://www.github.com&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;that you have &lt;a href=&#34;https://www.r-project.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;R&lt;/a&gt; installed&lt;/li&gt;
&lt;li&gt;that you have  &lt;a href=&#34;https://www.rstudio.com&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;RStudio&lt;/a&gt; installed&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;1-create-github-repository&#34;&gt;1. Create Github Repository&lt;/h3&gt;
&lt;p&gt;First, go to your &lt;a href=&#34;https://www.github.com&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt; page and create a new repository (&lt;code&gt;+&lt;/code&gt; button in the top-right corner).&lt;/p&gt;
&lt;img src=&#34;img/new_repo.png&#34; alt=&#34;new_repo&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;Name the repository &lt;code&gt;username.github.io&lt;/code&gt; where &lt;code&gt;username&lt;/code&gt; is your Github username.&lt;/p&gt;
&lt;img src=&#34;img/name_repo.png&#34; alt=&#34;name_repo&#34; style=&#34;zoom:40%;&#34; /&gt;
&lt;p&gt;In my case, my github username is &lt;code&gt;matteocourthoud&lt;/code&gt;, therefore the repository is &lt;code&gt;matteocourthoud.github.io&lt;/code&gt; and my personal website is &lt;a href=&#34;https://matteocourthoud.github.io&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://matteocourthoud.github.io&lt;/a&gt;. Use the default settings when creating the repository.&lt;/p&gt;
&lt;h3 id=&#34;2-install-blogdown-and-hugo&#34;&gt;2. Install Blogdown and Hugo&lt;/h3&gt;
&lt;p&gt;Now you need to install Blogdown, which is the program what will allow you to build and deploy your website, and Hugo, which is the template generator.&lt;/p&gt;
&lt;p&gt;Switch to RStudio and type the following commands&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Install blogdown
install.packages(&amp;quot;blogdown&amp;quot;)

# Install Hugo
blogdown::install_hugo()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now everything should be ready.&lt;/p&gt;
&lt;h3 id=&#34;3-setup-folder&#34;&gt;3. Setup folder&lt;/h3&gt;
&lt;p&gt;Open RStudio and select &lt;code&gt;New Project&lt;/code&gt;&lt;/p&gt;
&lt;img src=&#34;img/new_project.png&#34; alt=&#34;new_project&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;Select &lt;code&gt;New Directory&lt;/code&gt; when asked where to create the project&lt;/p&gt;
&lt;img src=&#34;img/new_project2.png&#34; alt=&#34;new_project2&#34; style=&#34;zoom:40%;&#34; /&gt;
&lt;p&gt;Then select &lt;code&gt;Website using blogdown&lt;/code&gt; as project type&lt;/p&gt;
&lt;img src=&#34;img/new_project3.png&#34; alt=&#34;new_project3&#34; style=&#34;zoom:40%;&#34; /&gt;
&lt;p&gt;Now you have to select a couple of options&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;Directory name&lt;/code&gt;: here input the name of the folder which will contain all the website files. The name is irrelevant. I called mine &lt;code&gt;website&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Create project as a subdirectory of&lt;/code&gt;: select the directory in which you want to put the website folder.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Theme&lt;/code&gt;: input &lt;code&gt;wowchemy/starter-academic&lt;/code&gt; instead of the default theme.&lt;/li&gt;
&lt;/ul&gt;
&lt;img src=&#34;img/new_project4.png&#34; alt=&#34;new_project4&#34; style=&#34;zoom:40%;&#34; /&gt;
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: if you want to install a different theme, just go on the corresponding Github page (for example &lt;a href=&#34;https://github.com/caressofsteel/hugo-story&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/caressofsteel/hugo-story&lt;/a&gt;) and instead of &lt;code&gt;gcushen/hugo-academic&lt;/code&gt;, insert the corresponding Github repository (for example &lt;code&gt;caressofsteel/hugo-story&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;If you go into the website folder, it should look something like this.&lt;/p&gt;
&lt;img src=&#34;img/folder.png&#34; alt=&#34;folder&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;h3 id=&#34;4-build-website&#34;&gt;4. Build website&lt;/h3&gt;
&lt;p&gt;To build the website, open the RProject file &lt;code&gt;website.Rproj&lt;/code&gt; in RStudio and type&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;blogdown::hugo_build(local=TRUE)
&lt;/code&gt;&lt;/pre&gt;
&lt;img src=&#34;img/hugo_build.png&#34; alt=&#34;hugo_build&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;This command will generate a &lt;code&gt;public/&lt;/code&gt; subfolder in which the actual code of the website is stored.&lt;/p&gt;
&lt;p&gt;Don&amp;rsquo;t ask me why, but the option &lt;code&gt;local=TRUE&lt;/code&gt; seems to make a difference. Updating without it sometimes does not change the content in the &lt;code&gt;\public&lt;/code&gt; subfolder.&lt;/p&gt;
&lt;img src=&#34;img/folder2.png&#34; alt=&#34;folder2&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;To preview the website, type in RStudio&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;blogdown::serve_site()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The following preview should automatically open in your browser.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;img/prevew.png&#34; alt=&#34;prevew&#34;&gt;&lt;/p&gt;
&lt;p&gt;Previewing the website is very useful as it allows you to see live changes locally inside RStudio, before publishing them. This is the &lt;strong&gt;main advantage of working in RStudio&lt;/strong&gt;.&lt;/p&gt;
&lt;h3 id=&#34;5-publish-website&#34;&gt;5. Publish website&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Importantly&lt;/strong&gt;, before pushing the code online, you need to open the file &lt;code&gt;config.yaml&lt;/code&gt; and change the &lt;code&gt;baseurl&lt;/code&gt; to your future website url, which will be &lt;code&gt;https://username.github.io/&lt;/code&gt;, where &lt;code&gt;username&lt;/code&gt; is our Github username.&lt;/p&gt;
&lt;img src=&#34;img/username.png&#34; alt=&#34;username&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;Now that you have set the correct url, you have to push the changes from the &lt;code&gt;public/&lt;/code&gt; folder to your &lt;code&gt;username.github.io&lt;/code&gt; repository on Github.&lt;/p&gt;
&lt;p&gt;To do that, you need to get to the website folder. Let&amp;rsquo;s assume that the path to your folder is &lt;code&gt;Documents/website&lt;/code&gt;. Open the Terminal and type&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;cd Documents/website/public
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The following code will link the &lt;code&gt;public/&lt;/code&gt; folder, containing the actual code of the website, to your  &lt;code&gt;username.github.io&lt;/code&gt; repository.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Init git in the /website/public/ folder
git init

# Add and commit the changes
git add .
git commit -m &amp;quot;first commit&amp;quot;

# Set origin
git remote add origin https://github.com/username/username.github.io.git

# Rename local branch
git branch -M main

# And push your updates online
git push -u origin main
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Wait a few seconds (or minutes for heavy changes) and your website should be online!&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;If the website is not working&lt;/strong&gt;, you can check the following:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Is there anything in your &lt;code&gt;\public&lt;/code&gt; folder? (does it even exist?) If not, something went wrong when compiling the website with &lt;code&gt;blogdown::hugo_build()&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Inside your &lt;code&gt;\public&lt;/code&gt; folder, there should be an &lt;code&gt;index.html&lt;/code&gt; file. If you double-click on it, you should see a local preview of your website in your browser. If not, something in the website code is wrong.&lt;/li&gt;
&lt;li&gt;Is the content of your &lt;code&gt;\public&lt;/code&gt; folder exactly the same of the content of your Github repository? If not, something went wrong when pushing to Github.&lt;/li&gt;
&lt;li&gt;Did you name your Github repository &lt;code&gt;username.github.io&lt;/code&gt;, where &lt;code&gt;username&lt;/code&gt; is your Github username?&lt;/li&gt;
&lt;li&gt;Did you change the &lt;code&gt;baseurl&lt;/code&gt; option in the file &lt;code&gt;config.yaml&lt;/code&gt; to &lt;code&gt;https://username.github.io/&lt;/code&gt;, where &lt;code&gt;username&lt;/code&gt; is your Github username?&lt;/li&gt;
&lt;li&gt;You can check the list of websites deployments at &lt;code&gt;https://github.com/username/username.github.io/deployments&lt;/code&gt;. Control that they correspond with your commits.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;If all the conditions are satisfied, but the website is still not online, maybe it&amp;rsquo;s just a matter of time. Have some patience.&lt;/p&gt;
&lt;h2 id=&#34;basic-customization&#34;&gt;Basic Customization&lt;/h2&gt;
&lt;p&gt;The basic files that you want to modify to customize your website are the following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;config.yaml&lt;/code&gt;: general website information&lt;/li&gt;
&lt;li&gt;&lt;code&gt;config/_default/params.yaml&lt;/code&gt;: website customization&lt;/li&gt;
&lt;li&gt;&lt;code&gt;config/_default/menus.yaml&lt;/code&gt;: top bar / menu customization&lt;/li&gt;
&lt;li&gt;&lt;code&gt;content/authors/admin/_index.md&lt;/code&gt;: personal information&lt;/li&gt;
&lt;/ul&gt;
&lt;img src=&#34;img/files.png&#34; alt=&#34;files&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;For what concerns &lt;strong&gt;images&lt;/strong&gt;, there are two main things you might want to modify:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Profile picture: change the &lt;code&gt;content/authors/admin/avatar.jpg&lt;/code&gt; picture&lt;/li&gt;
&lt;li&gt;Website icon: change the &lt;code&gt;assets/media/icon.png&lt;/code&gt; picture&lt;/li&gt;
&lt;/ul&gt;
&lt;img src=&#34;img/images.png&#34; alt=&#34;images&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;In order to modify the &lt;strong&gt;widgets&lt;/strong&gt; on your homepage, go to &lt;code&gt;content/home/&lt;/code&gt; and modify the files inside. If you want to remove a section, just open the corresponding file and select &lt;code&gt;active: false&lt;/code&gt;. If there is no &lt;code&gt;active&lt;/code&gt; option, just copy the line &lt;code&gt;active: false&lt;/code&gt; in the corresponding file.&lt;/p&gt;
&lt;img src=&#34;img/widgets.png&#34; alt=&#34;widgets&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;In my website I have only the following sections set to true:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;about&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;projects&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;posts&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;contact&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;To change the &lt;strong&gt;color palette&lt;/strong&gt; of the website, go to &lt;code&gt;data\theme&lt;/code&gt; and generate a &lt;code&gt;custom.toml&lt;/code&gt; file with the following content:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-toml&#34;&gt;# Theme metadata
name = &amp;quot;Custom&amp;quot;

# Is theme light or dark?
light = true

# Primary
primary = &amp;quot;#284f7a&amp;quot;

# Menu
menu_primary = &amp;quot;#fff&amp;quot;
menu_text = &amp;quot;#34495e&amp;quot;
menu_text_active = &amp;quot;#284f7a&amp;quot;
menu_title = &amp;quot;#2b2b2b&amp;quot;

# Home sections
home_section_odd = &amp;quot;rgb(255, 255, 255)&amp;quot;
home_section_even = &amp;quot;rgb(247, 247, 247)&amp;quot;

[dark]
  link = &amp;quot;#bbdefb&amp;quot;
  link_hover = &amp;quot;#bbdefb&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then go to the &lt;code&gt;config/_default/params.yaml&lt;/code&gt; file and set the &lt;code&gt;theme&lt;/code&gt; to &lt;code&gt;custom&lt;/code&gt;.&lt;/p&gt;
&lt;img src=&#34;img/custom_theme.png&#34; alt=&#34;custom_theme&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;You can get more information on how to personalize it &lt;a href=&#34;https://wowchemy.com/docs/getting-started/customization/#custom-theme&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;To change the &lt;strong&gt;font&lt;/strong&gt;, go to &lt;code&gt;data\fonts&lt;/code&gt; and generate a &lt;code&gt;custom.toml&lt;/code&gt; file with the following content:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-toml&#34;&gt;# Font style metadata
name = &amp;quot;Custom&amp;quot;

# Optional Google font URL
google_fonts = &amp;quot;family=Roboto+Mono&amp;amp;family=Source+Sans+Pro:wght@200;300;400;700&amp;quot;

# Font families
heading_font = &amp;quot;Source Sans Pro&amp;quot;
body_font = &amp;quot;Source Sans Pro&amp;quot;
nav_font = &amp;quot;Source Sans Pro&amp;quot;
mono_font = &amp;quot;Roboto Mono&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then go to the &lt;code&gt;config/_default/params.yaml&lt;/code&gt; file and set the &lt;code&gt;font&lt;/code&gt; to &lt;code&gt;Custom&lt;/code&gt;.&lt;/p&gt;
&lt;img src=&#34;img/custom_font.png&#34; alt=&#34;custom_font&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;You can get more information on how to personalize it &lt;a href=&#34;https://wowchemy.com/docs/getting-started/customization/#custom-font&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;. Importantly, by default the website supports only fonts of weight 400 and 700. If you want a lighter font, as the &lt;a href=&#34;https://fonts.google.com/specimen/Source&amp;#43;Sans&amp;#43;Pro&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Source Sans Pro&lt;/a&gt; I use for my website, you have to dig into the advanced customization (which requires HTML and CSS skills).&lt;/p&gt;
&lt;h2 id=&#34;advanced-customization&#34;&gt;Advanced Customization&lt;/h2&gt;
&lt;p&gt;Advanced customization is possible but &lt;strong&gt;it&amp;rsquo;s a pain&lt;/strong&gt;. You basically want to go inside &lt;code&gt;themes\github.com\wowchemy\wowchemy-hugo-modules\wowchemy&lt;/code&gt; and start digging. Tip: you want to start digging in the folowing places:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;In &lt;code&gt;layouts\partials&lt;/code&gt; to edit the HTML files&lt;/li&gt;
&lt;li&gt;In &lt;code&gt;assets\scss&lt;/code&gt; to edit the SCSS code&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If you want to copy my exact theme, I have published my custom theme here: &lt;a href=&#34;https://github.com/matteocourthoud/custom-wowchemy-settings&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/matteocourthoud/custom-wowchemy-settings&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;You have to do the following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;go inside the &lt;code&gt;theme&lt;/code&gt; folder&lt;/li&gt;
&lt;li&gt;copy the content of the &lt;code&gt;custom-wowchemy-theme&lt;/code&gt; repository in a folder there&lt;/li&gt;
&lt;li&gt;go to the &lt;code&gt;custom_toml&lt;/code&gt; file into the MODULES section&lt;/li&gt;
&lt;/ul&gt;
&lt;img src=&#34;img/config_before.png&#34; alt=&#34;config_before&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;ul&gt;
&lt;li&gt;change the second link to the folder with the custom settings&lt;/li&gt;
&lt;/ul&gt;
&lt;img src=&#34;img/config_after.png&#34; alt=&#34;config_after&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;Now your website should look quite similar to mine! :)&lt;/p&gt;
&lt;h2 id=&#34;google-analytics&#34;&gt;Google Analytics&lt;/h2&gt;
&lt;p&gt;In order for the website to be displayed in Google searches, you need to ask Google to track it.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Go to the &lt;a href=&#34;https://search.google.com/search-console&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Google Search Console website&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Use the &lt;a href=&#34;https://search.google.com/search-console?action=inspect&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;URL Inspection tool&lt;/a&gt; to inspect the URL of your personal website: &lt;code&gt;https://username.github.io&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Use &lt;strong&gt;Request indexing&lt;/strong&gt; to request Google to index your website so that it will apprear in Google searches.&lt;/li&gt;
&lt;li&gt;Under &lt;strong&gt;Sitemap&lt;/strong&gt; provide the link to your website sitemap to Google. It should be &lt;code&gt;https://username.github.io/sitemap.xml&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In order to receive statistics on your website, you first need to get your associated tracking code.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Go to the &lt;a href=&#34;https://www.google.com/analytics/web/#home/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Google Analytics website&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Click &lt;a href=&#34;https://support.google.com/analytics/answer/6132368&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Admin&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Select an account from the menu in the &lt;strong&gt;ACCOUNT&lt;/strong&gt; column.&lt;/li&gt;
&lt;li&gt;Select a property from the menu in the &lt;strong&gt;PROPERTY&lt;/strong&gt; column.&lt;/li&gt;
&lt;li&gt;Under &lt;strong&gt;PROPERTY&lt;/strong&gt;, click Tracking Info &amp;gt; Tracking Code.&lt;/li&gt;
&lt;li&gt;Your tracking ID and property number are displayed at the top of the page. It should have the form &lt;code&gt;UA-xxxxxxxxx-1&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Now that we have the website tracking code, we need to insert it into the &lt;code&gt;googleAnalytics&lt;/code&gt; section of the &lt;code&gt;config/_default/params.yaml&lt;/code&gt; file.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;marketing:
  google_analytics: &#39;UA-xxxxxxxxx-1&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The mobile application of &lt;a href=&#34;https://analytics.google.com/analytics/web/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Google Analytics&lt;/a&gt; is particular intuitive and allows you to monitor your website traffic in detail. You just need to link the website from the &lt;a href=&#34;https://search.google.com/search-console&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Google Sesarch Console&lt;/a&gt; and then you can motitor you website from this platform. There is also a very nice mobile app for both &lt;a href=&#34;https://play.google.com/store/apps/details?id=com.google.android.apps.giant&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Android&lt;/a&gt; and &lt;a href=&#34;https://apps.apple.com/us/app/google-analytics/id881599038&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;iOS&lt;/a&gt; to monitor your website from your smartphone.&lt;/p&gt;
&lt;p&gt;Another good free tool to analyze the &amp;ldquo;quality&amp;rdquo; of your website is &lt;a href=&#34;https://www.seomechanic.com/seo-analyzer/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;SEO Mechanic&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;p&gt;Here are the main resources I used to write this guide:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Wowchemy website: &lt;a href=&#34;https://wowchemy.com/docs/getting-started/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://wowchemy.com/docs/getting-started/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Old Academic website: &lt;a href=&#34;https://sourcethemes.com/academic/docs/install/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://sourcethemes.com/academic/docs/install/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Guide for the Terminal: &lt;a href=&#34;https://github.com/fliptanedo/FlipWebsite2017&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/fliptanedo/FlipWebsite2017&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;iframe src=&#34;https://docs.google.com/forms/d/e/1FAIpQLSdKqWZQAbHkcNLYvjxZ_fVrAZCJkNmsBtUBCCcfZxYzJCaqIQ/viewform?embedded=true&#34; width=&#34;640&#34; height=&#34;530&#34; frameborder=&#34;0&#34; marginheight=&#34;0&#34; marginwidth=&#34;0&#34;&gt;Loadingâ¦&lt;/iframe&gt;
</description>
    </item>
    
    <item>
      <title>Coding Resources for Social Sciences</title>
      <link>https://matteocourthoud.github.io/post/coding/</link>
      <pubDate>Tue, 24 Aug 2021 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/post/coding/</guid>
      <description>&lt;p&gt;In this page I will collect useful resources for coding for researchers in social sciences. A mention goes to &lt;a href=&#34;https://maxkasy.github.io/home/computationlinks/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Maximilian Kasy&lt;/a&gt; that inspired me to build this page.&lt;/p&gt;
&lt;p&gt;A quick legend:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ð book&lt;/li&gt;
&lt;li&gt;ð webpage&lt;/li&gt;
&lt;li&gt;ð charts&lt;/li&gt;
&lt;li&gt;ð¥ videos&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;econometrics-and-statistics&#34;&gt;Econometrics and Statistics&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;ð&lt;a href=&#34;https://www.ssc.wisc.edu/~bhansen/econometrics/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Bruce Hansen&amp;rsquo;s Econometrics&lt;/strong&gt;&lt;/a&gt;:
By far the best freely available and regularly updated resource for Econometrics&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;machine-learning&#34;&gt;Machine Learning&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;ð&lt;a href=&#34;https://web.stanford.edu/~hastie/Papers/ESLII.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;The Elements of Statistical Learning&lt;/strong&gt;&lt;/a&gt;:
General introduction to machine learning&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;ð&lt;a href=&#34;http://www.gaussianprocess.org/gpml/chapters/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Gaussian Processes for Machine Learning&lt;/strong&gt;&lt;/a&gt;:
Extremely useful tools for nonparametric Bayesian modeling&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;ð&lt;a href=&#34;https://www.deeplearningbook.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Deep Learning&lt;/strong&gt;&lt;/a&gt;:
The theory and implementation of neural nets&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;ð&lt;a href=&#34;https://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning/understanding-machine-learning-theory-algorithms.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Understanding Machine Learning: From Theory to Algorithms&lt;/strong&gt;&lt;/a&gt;:
An introduction to statistical learning theory in the tradition of Vapnik&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;ð&lt;a href=&#34;http://www.incompleteideas.net/book/RLbook2018.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Reinforcement Learning - An Introduction&lt;/strong&gt;&lt;/a&gt;:
Adaptive learning for Markov decision problems&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;ð&lt;a href=&#34;http://jeffe.cs.illinois.edu/teaching/algorithms/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Algorithms&lt;/strong&gt;&lt;/a&gt;:
Introduction to the theory of algorithms&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;ð&lt;a href=&#34;https://playground.tensorflow.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Tensorflow Playground&lt;/strong&gt;&lt;/a&gt;:
Visualisation tool for neural networks&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;ð&lt;a href=&#34;https://www.fast.ai/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Artificial Intelligence&lt;/strong&gt;&lt;/a&gt;:
Online lectures on AI&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;ð&lt;a href=&#34;https://global.oup.com/academic/product/the-ethical-algorithm-9780190948207&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;The Ethical Algorithm&lt;/strong&gt;&lt;/a&gt;:
How to impose normative constraints on ML and other algorithms&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;python&#34;&gt;Python&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;ð&lt;a href=&#34;https://realpython.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;RealPython&lt;/strong&gt;&lt;/a&gt;:
Collection of Python tutorials, from introductory to advanced. Also contains &lt;a href=&#34;https://realpython.com/learning-paths/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;learning paths&lt;/a&gt; for specific topics&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;ð&lt;a href=&#34;https://python.quantecon.org/intro.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;QuantEcon Python&lt;/strong&gt;&lt;/a&gt;
Tutorials and economic applications in Python, especially for macroeconomics&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;ð&lt;a href=&#34;https://blog.finxter.com/python-cheat-sheets/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Cheat Sheets&lt;/strong&gt;&lt;/a&gt;:
Collection of cheat sheets for python&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;ð&lt;a href=&#34;https://docs.python-guide.org/writing/structure/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Structuring a Python project&lt;/strong&gt;&lt;/a&gt;:
Advanced tutorial on how to structure a Python program&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;ð&lt;a href=&#34;https://www.softwaretestinghelp.com/python-ide-code-editors/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;IDE Guide&lt;/strong&gt;&lt;/a&gt;:
Comparison of IDEs for Python. Suggested: &lt;a href=&#34;https://www.jetbrains.com/pycharm/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PyCharm&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;ð&lt;a href=&#34;https://www.jetbrains.com/help/pycharm/configuring-remote-interpreters-via-ssh.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Configuring remote interpreters via SSH&lt;/strong&gt;&lt;/a&gt;:
How to use Python remotely via SSH via PyCharm&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;ð&lt;a href=&#34;https://www.kaggle.com/maheshdadhich/strength-of-visualization-python-visuals-tutorial/notebook&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Visualization in Python&lt;/strong&gt;&lt;/a&gt;:
How to make nice graphs in Python, with a dedicated jupyter notebook&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;ð&lt;a href=&#34;https://python-graph-gallery.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Python Graph Gallery&lt;/strong&gt;&lt;/a&gt;:
Graph examples in Python&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;matlab&#34;&gt;Matlab&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;ð&lt;a href=&#34;https://mathworks.com/help/matlab/matlab_oop/user-defined-classes.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;User defined classes in Matlab&lt;/strong&gt;&lt;/a&gt;:
How to work with classes in Matlab&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;ð&lt;a href=&#34;https://am111.readthedocs.io/en/latest/jmatlab_use.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Julyter Notebooks&lt;/strong&gt;&lt;/a&gt;:
How to run a jupyter notebook with Matlab kernel&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;ð&lt;a href=&#34;https://www.bradleymonk.com/wp/how-to-make-professional-looking-plots-for-journal-publication-using-matlab-r2014a-and-r2014b/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Graph Tips in Matlab&lt;/strong&gt;&lt;/a&gt; and &lt;a href=&#34;https://mathworks.com/matlabcentral/answers/133372-how-to-make-nice-plots&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;link2&lt;/a&gt;:
Suggestions on how to make pretty graphs in Matlab&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;julia&#34;&gt;Julia&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;ð&lt;a href=&#34;https://docs.julialang.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Julia Manual&lt;/strong&gt;&lt;/a&gt;:
Julia unfortunately lacks a big community and tutorials, but it has a very good manual&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;ð&lt;a href=&#34;https://julia.quantecon.org/index_toc.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;QuantEcon Julia&lt;/strong&gt;&lt;/a&gt;
Tutorials and economic applications in Julia, especially for macroeconomics&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;ð&lt;a href=&#34;https://medium.com/dev-genius/what-is-the-best-ide-for-developing-in-the-programming-language-julia-484c913f07bc&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;IDE Guide&lt;/strong&gt;&lt;/a&gt;:
Guide for IDEs for Julia. Suggested: Juno for Atom.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;r&#34;&gt;R&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;ð&lt;a href=&#34;https://cran.r-project.org/doc/manuals/r-release/R-intro.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;An Introduction to R&lt;/strong&gt;&lt;/a&gt;:
Complete introduction to base R&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;ð&lt;a href=&#34;http://r4ds.had.co.nz/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;R for Data Science&lt;/strong&gt;&lt;/a&gt;
Introduction to data analysis using R, focused on the tidyverse packages&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;ð&lt;a href=&#34;https://adv-r.hadley.nz/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Advanced R&lt;/strong&gt;&lt;/a&gt;:
In depth discussion of programming in R&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;ð&lt;a href=&#34;https://bradleyboehmke.github.io/HOML/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Hands-On Machine Learning with R&lt;/strong&gt;&lt;/a&gt;:
Fitting ML models in R&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;ð&lt;a href=&#34;https://github.com/stan-dev/rstan/wiki/RStan-Getting-Started&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Bayesian statistics using Stan&lt;/strong&gt;&lt;/a&gt; and &lt;a href=&#34;https://mc-stan.org/docs/2_20/stan-users-guide/index.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;link&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;ð&lt;a href=&#34;https://www.rstudio.com/resources/cheatsheets/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;RStudio Cheat Sheets&lt;/strong&gt;&lt;/a&gt;
for various extensions, including data processing, visualization, writing web apps, â¦&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;ð&lt;a href=&#34;https://www.r-graph-gallery.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;R Graph Gallery&lt;/strong&gt;&lt;/a&gt;:
Graph examples in R&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;ð&lt;a href=&#34;https://www.christophenicault.com/pages/visualizations/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Nice Graphs with code&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;A collection of elaborate graphs with code in R&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;others&#34;&gt;Others&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;ð&lt;a href=&#34;https://git-scm.com/book/en/v2&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Github Advanced&lt;/strong&gt;&lt;/a&gt;:
Advanced guide for version control with Github&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;ð¥&lt;a href=&#34;https://missing.csail.mit.edu/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;The Missing Semester of Your CS Education&lt;/strong&gt;&lt;/a&gt;
Video lectures and notes on tools for computer scientists (version control, debugging, &amp;hellip;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;ð&lt;a href=&#34;http://pgfplots.sourceforge.net/gallery.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;PGF plots in Latex&lt;/strong&gt;&lt;/a&gt;:
Gallery and examples to make plots directly in Latex&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;ð&lt;a href=&#34;https://medium.com/@SergioPietri/how-to-setup-and-use-ssh-for-remote-connections-e86556d804dd&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Work remotely from server&lt;/strong&gt;&lt;/a&gt;:
How to setup SSH for remote computing&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>How to access WRDS in Python</title>
      <link>https://matteocourthoud.github.io/post/wrds/</link>
      <pubDate>Tue, 09 Mar 2021 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/post/wrds/</guid>
      <description>&lt;p&gt;In this page, I explain how to work with the WRDS database using Python.&lt;/p&gt;
&lt;h2 id=&#34;setup&#34;&gt;Setup&lt;/h2&gt;
&lt;p&gt;The first thing we need to do, is to set up a connection to the &lt;a href=&#34;https://wrds-www.wharton.upenn.edu/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;WRDS database&lt;/a&gt;. I am assuming you have credentials to log in. Check the &lt;a href=&#34;https://wrds-www.wharton.upenn.edu/login/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;log in page&lt;/a&gt; to make sure.&lt;/p&gt;
&lt;p&gt;The second requirement is the &lt;a href=&#34;https://pypi.org/project/wrds/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;wrds&lt;/a&gt; Python package.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pip3 install wrds
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, in order to connect to the WRDS database, you just need to run the following commang in Python.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import wrds
db = wrds.Connection() 
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then, you will be propted to input your WRDS username and password.&lt;/p&gt;
&lt;p&gt;However, if you are using a Python IDE such as &lt;a href=&#34;https://www.jetbrains.com/pycharm/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PyCharm&lt;/a&gt;, you cannot run the command from the Python Console. Moreover, you might want to save your credentials once and for all, so that you don&amp;rsquo;t have to log in every time.&lt;/p&gt;
&lt;p&gt;First, walk to your home directory from the Terminal (&lt;code&gt;/Users/username&lt;/code&gt;).&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;cd
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now create an empty &lt;code&gt;.pgpass&lt;/code&gt; file.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;touch .pgpass
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now you write &lt;code&gt;your_username&lt;/code&gt; and &lt;code&gt;your_password&lt;/code&gt; into the &lt;code&gt;.pgpass&lt;/code&gt; file.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;echo &amp;quot;wrds-pgdata.wharton.upenn.edu:9737:wrds:your_username:your_password&amp;quot; &amp;gt;&amp;gt; .pgpass
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You also need to restrict permissions to the file.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;chmod 600 ~/.pgpass
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now you can go back to your Python IDE and access the database by just inputing your username.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import wrds
db = wrds.Connection(wrds_username=&#39;your_username&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If everything works, you should see the following output.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Loading library list...
Done
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;query&#34;&gt;Query&lt;/h2&gt;
&lt;p&gt;The available functions are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;db.connection() &lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;db.list_libraries()&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;db.list_tables() &lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;db.get_table() &lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;db.describe_table() &lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;db.raw_sql() &lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;db.close()&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I make a simple example of how they work. Suppose first you want to list all the libraries in the WRDS database.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;db.list_libraries()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then you can list all the datasets within a given library.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;db.list_tables(library=&#39;comp&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Before downloading a table, you can describe it.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;df = db.describe_table(library=&#39;comp&#39;, table=&#39;funda&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To download the dataset you can use the &lt;code&gt;get_table()&lt;/code&gt; function.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;df = db.get_table(library=&#39;comp&#39;, table=&#39;funda&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can restrict both the rows and the columns you want to query.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;df_short = db.get_table(library=&#39;comp&#39;, table=&#39;funda&#39;, columns = [&#39;conm&#39;, &#39;gvkey&#39;, &#39;cik&#39;], obs=5)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can also query the database directly using SQL.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;df_sql = db.raw_sql(&#39;&#39;&#39;select conm, gvkey, cik FROM comp.funda WHERE fyear&amp;gt;2010 AND (indfmt=&#39;INDL&#39;)&#39;&#39;&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;sources&#34;&gt;Sources&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://wrds-www.wharton.upenn.edu/pages/support/programming-wrds/programming-python/querying-wrds-data-python/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Querying WRDS Data using Python&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://wrds-www.wharton.upenn.edu/documents/1443/wrds_connection.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Using Python on WRDS Platform&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://sites.duke.edu/kevinstandridge/2020/03/07/introduction-to-the-wrds-python-package/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Introduction to the WRDS Python Package&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://wizardkingz.github.io/wrdsdataaccesspython-tutorial/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;WRDS Data Access Via Python API&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Summer Schools in Economics and Finance</title>
      <link>https://matteocourthoud.github.io/post/summerschools/</link>
      <pubDate>Mon, 15 Feb 2021 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/post/summerschools/</guid>
      <description>&lt;p&gt;I will use this page to collect information about summer schools in Economics.&lt;/p&gt;
&lt;p&gt;If you know about summer schools that are missing from this list, please either &lt;a href=&#34;mailto:matteo.courthoud@econ.uzh.ch&#34;&gt;contact me&lt;/a&gt; or &lt;a href=&#34;https://github.com/matteocourthoud/website/blob/master/content/post/summerschools/index.md&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;edit the table on Github&lt;/a&gt;!&lt;/p&gt;
&lt;h2 id=&#34;2020&#34;&gt;2020&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Name and Link&lt;/th&gt;
&lt;th&gt;Field&lt;/th&gt;
&lt;th&gt;Organizer&lt;/th&gt;
&lt;th&gt;Location&lt;/th&gt;
&lt;th&gt;Instructor(s)&lt;/th&gt;
&lt;th&gt;Dates&lt;/th&gt;
&lt;th&gt;Deadline&lt;/th&gt;
&lt;th&gt;Fee&lt;/th&gt;
&lt;th&gt;Aid&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://dseconf.org/dse2020&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Dynamic Structural Econometrics&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Econometrics, IO&lt;/td&gt;
&lt;td&gt;Econometrics Society&lt;/td&gt;
&lt;td&gt;Zurich&lt;/td&gt;
&lt;td&gt;John Rust et al.&lt;/td&gt;
&lt;td&gt;June 15-21&lt;/td&gt;
&lt;td&gt;March 15&lt;/td&gt;
&lt;td&gt;500$&lt;/td&gt;
&lt;td&gt;no&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://www.cresse.info/default.aspx?articleID=3398&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CRESSE&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Comp. Policy, IO&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;Crete&lt;/td&gt;
&lt;td&gt;various&lt;/td&gt;
&lt;td&gt;June 20 - July 02&lt;/td&gt;
&lt;td&gt;FCFS&lt;/td&gt;
&lt;td&gt;3200â¬&lt;/td&gt;
&lt;td&gt;-30%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://www.barcelonagse.eu/study/summer-school/digital-economy&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Digital Economy&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Comp. Policy, IO&lt;/td&gt;
&lt;td&gt;Barcelona GSE&lt;/td&gt;
&lt;td&gt;Barcelona&lt;/td&gt;
&lt;td&gt;Martin Peitz&lt;/td&gt;
&lt;td&gt;July 13-17&lt;/td&gt;
&lt;td&gt;March 10&lt;/td&gt;
&lt;td&gt;550â¬&lt;/td&gt;
&lt;td&gt;maybe&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://www.parisschoolofeconomics.eu/en/teaching/pse-summer-school/social-networks-platforms/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Social Networks, Platforms&amp;hellip;&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Comp. Policy, IO&lt;/td&gt;
&lt;td&gt;PSE&lt;/td&gt;
&lt;td&gt;Paris&lt;/td&gt;
&lt;td&gt;from Paris&lt;/td&gt;
&lt;td&gt;June 15-19&lt;/td&gt;
&lt;td&gt;March 31&lt;/td&gt;
&lt;td&gt;1200â¬&lt;/td&gt;
&lt;td&gt;no&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;2019&#34;&gt;2019&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Name and Link&lt;/th&gt;
&lt;th&gt;Field&lt;/th&gt;
&lt;th&gt;Organizer&lt;/th&gt;
&lt;th&gt;Location&lt;/th&gt;
&lt;th&gt;Instructor(s)&lt;/th&gt;
&lt;th&gt;Dates&lt;/th&gt;
&lt;th&gt;Deadline&lt;/th&gt;
&lt;th&gt;Fee&lt;/th&gt;
&lt;th&gt;Aid&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://dseconf.org/dse2019&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Dynamic Structural Econometrics&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Econometrics, IO&lt;/td&gt;
&lt;td&gt;Econometrics Society&lt;/td&gt;
&lt;td&gt;Chicago&lt;/td&gt;
&lt;td&gt;John Rust et al.&lt;/td&gt;
&lt;td&gt;July 08-14&lt;/td&gt;
&lt;td&gt;March 15&lt;/td&gt;
&lt;td&gt;500$&lt;/td&gt;
&lt;td&gt;no&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;CRESSE&lt;/td&gt;
&lt;td&gt;Competition Policy, IO&lt;/td&gt;
&lt;td&gt;various&lt;/td&gt;
&lt;td&gt;Crete&lt;/td&gt;
&lt;td&gt;various&lt;/td&gt;
&lt;td&gt;June 20 - July 02&lt;/td&gt;
&lt;td&gt;FCFS&lt;/td&gt;
&lt;td&gt;3200â¬&lt;/td&gt;
&lt;td&gt;-30%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://www.cemfi.es/studies/css/course.asp?cu=10&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Empirical Analysis of Firm Performance&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;IO, Trade&lt;/td&gt;
&lt;td&gt;CEMFI&lt;/td&gt;
&lt;td&gt;Madrid&lt;/td&gt;
&lt;td&gt;Jan de Loecker&lt;/td&gt;
&lt;td&gt;August 19-23&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://www.cemfi.es/studies/css/course.asp?cu=16&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Panel Data Econometrics&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Econometrics&lt;/td&gt;
&lt;td&gt;CEMFI&lt;/td&gt;
&lt;td&gt;Madrid&lt;/td&gt;
&lt;td&gt;Steve Bond&lt;/td&gt;
&lt;td&gt;September 02-06&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;2018&#34;&gt;2018&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Name and Link&lt;/th&gt;
&lt;th&gt;Field&lt;/th&gt;
&lt;th&gt;Organizer&lt;/th&gt;
&lt;th&gt;Location&lt;/th&gt;
&lt;th&gt;Instructor(s)&lt;/th&gt;
&lt;th&gt;Dates&lt;/th&gt;
&lt;th&gt;Deadline&lt;/th&gt;
&lt;th&gt;Fee&lt;/th&gt;
&lt;th&gt;Aid&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://www.econ.ku.dk/cce/events/summerschool/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Dynamic Structural Models&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Econometrics, IO&lt;/td&gt;
&lt;td&gt;University of Copenhagen&lt;/td&gt;
&lt;td&gt;Copenhagen&lt;/td&gt;
&lt;td&gt;John Rust et al.&lt;/td&gt;
&lt;td&gt;May 28 - Jun 03&lt;/td&gt;
&lt;td&gt;March 15&lt;/td&gt;
&lt;td&gt;600â¬&lt;/td&gt;
&lt;td&gt;no&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://www.cemfi.es/studies/css/course_previous_years.asp?c=12&amp;amp;y=2018&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Empirical Analysis of Innovation in Oligopoly Industries&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Econometrics, IO&lt;/td&gt;
&lt;td&gt;CEMFI&lt;/td&gt;
&lt;td&gt;Madrid&lt;/td&gt;
&lt;td&gt;Victor Aguirregabiria&lt;/td&gt;
&lt;td&gt;September 03-07&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
</description>
    </item>
    
  </channel>
</rss>
