<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts | Matteo Courthoud</title>
    <link>https://matteocourthoud.github.io/post/</link>
      <atom:link href="https://matteocourthoud.github.io/post/index.xml" rel="self" type="application/rss+xml" />
    <description>Posts</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><copyright>Theme edited by Matteo Courthoud© - Want to have a similar website? [Guide here](https://matteocourthoud.github.io/post/website/).</copyright><lastBuildDate>Sun, 20 Aug 2023 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://matteocourthoud.github.io/media/icon_hu03e9b3967b83fd39296ec9da5ff1ea05_201175_512x512_fill_lanczos_center_3.png</url>
      <title>Posts</title>
      <link>https://matteocourthoud.github.io/post/</link>
    </image>
    
    <item>
      <title>XYZ</title>
      <link>https://matteocourthoud.github.io/post/cin-2023-08-20/</link>
      <pubDate>Sun, 20 Aug 2023 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/post/cin-2023-08-20/</guid>
      <description>&lt;p&gt;&lt;em&gt;A causal inference newsletter&lt;/em&gt;&lt;/p&gt;
&lt;h2 id=&#34;news&#34;&gt;News&lt;/h2&gt;
&lt;h3 id=&#34;speeding-up-hellofreshs-bayesian-ab-testinghttpsmediumcomsocial_68653bayes-is-slow-speeding-up-hellofreshs-bayesian-ab-tests-by-60x-fc431a327cd8&#34;&gt;&lt;a href=&#34;https://medium.com/@social_68653/bayes-is-slow-speeding-up-hellofreshs-bayesian-ab-tests-by-60x-fc431a327cd8&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Speeding up HelloFresh’s Bayesian AB Testing&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;PyMC researchers tested different ideas to speed up thousands of concurrent AB tests. First, they decreased model parametrization, and sampling length, leading to marginal speed increases. However, the most gains were achieved by defining a single unpooled model made of many statistically independent models, leading to 60x speed gains.&lt;/p&gt;
&lt;h3 id=&#34;a-tidy-package-for-hte-estimationhttpstwittercomdrewdimstatus1691095106886242304&#34;&gt;&lt;a href=&#34;https://twitter.com/DrewDim/status/1691095106886242304&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;A &lt;code&gt;tidy&lt;/code&gt; Package for HTE Estimation&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;A new R package, &lt;code&gt;tidyhte&lt;/code&gt;, estimates Heterogeneous Treatment Effects with a &lt;a href=&#34;https://www.tidyverse.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;tidy&lt;/a&gt; syntax. The estimator is essentially an R-learner in the spirit of &lt;a href=&#34;https://arxiv.org/abs/2004.14497&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Kennedy (2022)&lt;/a&gt;. It uses &lt;a href=&#34;https://github.com/ecpolley/SuperLearner&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;SuperLearner&lt;/code&gt;&lt;/a&gt; for fitting nuisance parameters and &lt;a href=&#34;http://bdwilliamson.github.io/vimp/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;vimp&lt;/code&gt;&lt;/a&gt; for variable importance. Official website &lt;a href=&#34;https://ddimmery.github.io/tidyhte/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;spotify-shares-some-experimentation-lessonshttpsengineeringatspotifycom202308experimentation-at-spotify-three-lessons-for-maximizing-impact-in-innovation&#34;&gt;&lt;a href=&#34;https://engineering.atspotify.com/2023/08/experimentation-at-spotify-three-lessons-for-maximizing-impact-in-innovation&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Spotify Shares Some Experimentation Lessons&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;First, the researchers suggest thinking backward, starting from the decision that needs to be informed and designing the experiment accordingly. Second, they suggest running local experiments to discover local effects, an effective strategy for their expansion in the Japanese market. Last, they recommend testing changes incrementally and not in bunches.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;old-reads&#34;&gt;Old Reads&lt;/h2&gt;
&lt;h3 id=&#34;experimentation-with-resource-constraintshttpsmultithreadedstitchfixcomblog20201118virtual-warehouse&#34;&gt;&lt;a href=&#34;https://multithreaded.stitchfix.com/blog/2020/11/18/virtual-warehouse/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Experimentation with Resource Constraints&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Researchers at Stitchfix show how to deal with interference bias from budget constraints: enforce virtual constraints, preventing isolated groups from competing for resources. The solution is effective in solving the bias but opens new challenges in how to enforce the new constraints and scale the estimates to the full market.&lt;/p&gt;
&lt;h3 id=&#34;r-guide-for-tmle-in-medical-researchhttpsehsanxgithubiotmleworkshop&#34;&gt;&lt;a href=&#34;https://ehsanx.github.io/TMLEworkshop/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;R Guide for TMLE in Medical Research&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;This code-first guide introduces Targeted Maximum Likelihood Estimation (TMLE) through a medical application on real data: the effects of right heart catheterization on critically ill patients in the intensive care unit. The guide starts with data exploration, then introduces outcome models (G-computation), exposure models (IPW), and lastly combines them into TMLE.&lt;/p&gt;
&lt;h3 id=&#34;an-overview-of-netlix-experimentation-platformhttpsarxivorgabs191003878&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/1910.03878&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;An Overview of Netlix Experimentation Platform&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Netflix&amp;rsquo;s experimentation platform is built on three pillars. The first pillar is a metrics repository where statistics are stored and shared across projects. The second pillar is a causal models library that collects causal inference methods. Lastly, a lightweight interactive visualization library to explore and report estimates. A global causal graph is not mentioned.&lt;/p&gt;
&lt;h3 id=&#34;thumbnack-explains-the-efficiency-gains-of-interleavinghttpsmediumcomthumbtack-engineeringaccelerating-ranking-experimentation-at-thumbtack-with-interleaving-20cbe7837edf&#34;&gt;&lt;a href=&#34;https://medium.com/thumbtack-engineering/accelerating-ranking-experimentation-at-thumbtack-with-interleaving-20cbe7837edf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Thumbnack Explains the Efficiency Gains of Interleaving&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;First, interleaving is a one-sample test. Second, interleaving controls for customer variation since each customer sees both rankings. Third, signals are stronger since consumers have to make a choice (revealed preference). The downside is generalization since the rankings during the experiment differ from the deployed ones.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;For more causal inference resources:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Subscribe to my stories on &lt;a href=&#34;https://medium.com/@matteo.courthoud&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Medium&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Follow me on &lt;a href=&#34;https://www.linkedin.com/in/matteo-courthoud/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;LinkedIn&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Star &lt;a href=&#34;https://github.com/matteocourthoud/awesome-causal-inference&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;awesome-causal-inference&lt;/code&gt;&lt;/a&gt;, where I collect all the articles and additional causal inference resources&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>XYZ</title>
      <link>https://matteocourthoud.github.io/post/cin-2023-08-06/</link>
      <pubDate>Sun, 06 Aug 2023 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/post/cin-2023-08-06/</guid>
      <description>&lt;p&gt;&lt;em&gt;A causal inference newsletter&lt;/em&gt;&lt;/p&gt;
&lt;h2 id=&#34;news&#34;&gt;News&lt;/h2&gt;
&lt;h3 id=&#34;spotify-releases-its-experimentation-platform-confidencehttpsengineeringatspotifycom202308coming-soon-confidence-an-experimentation-platform-from-spotify&#34;&gt;&lt;a href=&#34;https://engineering.atspotify.com/2023/08/coming-soon-confidence-an-experimentation-platform-from-spotify&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Spotify Releases its Experimentation Platform: Confidence&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Spotify has released its internal experimentation platform, used for 10+ years to run and evaluate A/B tests at scale. Currently, it is in closed beta, but it promises large-scale performance, usability, flexibility, but also integration with existing A/B testing tools. A &lt;a href=&#34;https://github.com/spotify/confidence&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;confidence&lt;/code&gt;&lt;/a&gt; Github repo has been existing of a while.&lt;/p&gt;
&lt;h3 id=&#34;microsoft-calls-for-not-worrying-about-interaction-effectshttpswwwmicrosoftcomen-usresearchgroupexperimentation-platform-exparticlesa-b-interactions-a-call-to-relax&#34;&gt;&lt;a href=&#34;https://www.microsoft.com/en-us/research/group/experimentation-platform-exp/articles/a-b-interactions-a-call-to-relax/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Microsoft Calls for not Worrying About Interaction Effects&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Microsoft researchers have looked at four major products, each running hundreds of A/B tests. They could not find any evidence of interaction effects. This does not mean interaction effects should be ignored, but rather that, at least in this scenario, they are of sufficiently low magnitude not to worry about them.&lt;/p&gt;
&lt;h3 id=&#34;updated-version-of-statistical-challenges-in-online-controlled-experimentshttpsarxivorgabs221211366&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2212.11366&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Updated Version of &amp;ldquo;Statistical Challenges in Online Controlled Experiments&amp;rdquo;&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Ron Kohavi and co-authors have updated their paper on A/B test challenges, splitting the paper into a main paper and supplementary material. The main paper covers variance reduction, heterogeneous treatment effects, long-term effects, optimal stopping, and interference. The appendix covers stratified sampling, surrogate outcomes, and network A/B tests.&lt;/p&gt;
&lt;h3 id=&#34;pymc-adds-the-do-operatorhttpswwwpymc-labsioblog-postscausal-analysis-with-pymc-answering-what-if-with-the-new-do-operator&#34;&gt;&lt;a href=&#34;https://www.pymc-labs.io/blog-posts/causal-analysis-with-pymc-answering-what-if-with-the-new-do-operator&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PyMC Adds the do-Operator&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Python&amp;rsquo;s main probabilistic programming library has added a do-operator to do Bayesian Causal Analysis. If you know and specify the whole causal graph, it allows you to do Bayesian inference on causal quantities of interest. The repo is still experimental.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;old-reads&#34;&gt;Old Reads&lt;/h2&gt;
&lt;h3 id=&#34;embrace-overlapping-ab-tests-and-avoid-the-dangers-of-isolating-experimentshttpsblogstatsigcomembracing-overlapping-a-b-tests-and-the-danger-of-isolating-experiments-cb0a69e09d3&#34;&gt;&lt;a href=&#34;https://blog.statsig.com/embracing-overlapping-a-b-tests-and-the-danger-of-isolating-experiments-cb0a69e09d3&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Embrace Overlapping A/B Tests and Avoid the Dangers of Isolating Experiments&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;From the experience of data scientists at Meta, strong interaction effects are rare. It&amp;rsquo;s worth isolating experiments only when it&amp;rsquo;s mechanically necessary, or they might break the user experience, or you need a very precise measurement.&lt;/p&gt;
&lt;h3 id=&#34;an-analysts-guide-to-mmmhttpsfacebookexperimentalgithubiorobyndocsanalysts-guide-to-mmm&#34;&gt;&lt;a href=&#34;https://facebookexperimental.github.io/Robyn/docs/analysts-guide-to-MMM/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;An Analysts guide to MMM&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;While not being a standalone publication, this MMM guide in Meta&amp;rsquo;s &lt;a href=&#34;https://facebookexperimental.github.io/Robyn/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Robyn&lt;/a&gt; documentation is comprehensive and detailed. It goes through all the phases of building an MMM model. Causality is mentioned shortly but vehemently: &amp;ldquo;&lt;em&gt;we strongly recommend using experimental and causal results to calibrate MMM&lt;/em&gt;&amp;rdquo;.&lt;/p&gt;
&lt;h3 id=&#34;an-intuitive-explanation-of-why-π-is-in-the-normal-distributionhttpswwwyoutubecomwatchvcy8r7wsut1i&#34;&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=cy8r7WSuT1I&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;An Intuitive Explanation of Why π is in the Normal Distribution&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;In two dimensions, the normal distribution can be defined by two properties: the pdf depends only on the distance from the origin, and the two dimensions are independent. From here, the relationship with a circle (and hence π) but also the relationship between the normal distribution and the Central Limit Theorem (covered in &lt;a href=&#34;https://www.youtube.com/watch?v=d_qvLDhkg00&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;another video&lt;/a&gt;).&lt;/p&gt;
&lt;h3 id=&#34;a-list-of-company-experimentation-platformshttpswwwlinkedincompulsein-house-experimentation-platforms-denise-visser&#34;&gt;&lt;a href=&#34;https://www.linkedin.com/pulse/in-house-experimentation-platforms-denise-visser/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;A List of Company Experimentation Platforms&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Denise Visser lists the different in-house experimentation platforms of different companies as of 2020.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;For more causal inference resources:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Subscribe to my stories on &lt;a href=&#34;https://medium.com/@matteo.courthoud&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Medium&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Follow me on &lt;a href=&#34;https://www.linkedin.com/in/matteo-courthoud/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;LinkedIn&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Star &lt;a href=&#34;https://github.com/matteocourthoud/awesome-causal-inference&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;awesome-causal-inference&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Beyond Churn Prediction and Churn Uplift</title>
      <link>https://matteocourthoud.github.io/post/beyond_churn/</link>
      <pubDate>Tue, 25 Jul 2023 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/post/beyond_churn/</guid>
      <description>&lt;p&gt;A very common task in data science is &lt;em&gt;churn prediction&lt;/em&gt;. However, predicting churn is often just an intermediate step and rarely the final objective. Usually, what we actually care about is &lt;strong&gt;reducing churn&lt;/strong&gt;, which is a separate objective, not necessarily related. In fact, for example, knowing that long-term customers are less likely to churn than new customers is not an actionable insight since we cannot increase customers&amp;rsquo; tenure. What we would like to know instead is how one (or more) treatment impacts churn. This is often referred to as &lt;strong&gt;churn uplift&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;In this article, we will be going &lt;strong&gt;beyond&lt;/strong&gt; both churn prediction and churn uplift and consider instead the ultimate goal of churn-prevention campaigns: &lt;strong&gt;increasing revenue&lt;/strong&gt;. First of all, a policy that reduces churn might also have an impact on revenue, which should be taken into account. However, and more importantly, increasing revenue is relevant only if the customer does not churn. Vice-versa, decreasing churn is more relevant for high-revenue customers. This &lt;strong&gt;interaction&lt;/strong&gt; between churn and revenue is critical in understanding the profitability of any treatment campaign and should not be overlooked.&lt;/p&gt;
&lt;h2 id=&#34;gifts-and-subscriptions&#34;&gt;Gifts and Subscriptions&lt;/h2&gt;
&lt;p&gt;For the rest of the article, we are going to use a &lt;strong&gt;toy example&lt;/strong&gt; to illustrate the main idea. Suppose we were a company interested in reducing our customer&amp;rsquo;s churn and ultimately increasing our revenue. Suppose we have decided to test a new idea: sending a &lt;strong&gt;gift&lt;/strong&gt; of &lt;em&gt;1\$&lt;/em&gt; to our users. In order to test whether the treatment works, we have randomly sent it only to a subsample of our customer base.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;cost = 1
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s have a look at the data we have at our disposal. I import the data-generating process &lt;code&gt;dgp_gift()&lt;/code&gt; from &lt;code&gt;src.dgp&lt;/code&gt;. I also import some plotting functions and libraries from &lt;code&gt;src.utils&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;%matplotlib inline
%config InlineBackend.figure_format = &#39;retina&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from src.utils import *
from src.dgp import dgp_gift
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;dgp = dgp_gift(n=100_000)
df = dgp.generate_data()
df.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;months&lt;/th&gt;
      &lt;th&gt;rev_old&lt;/th&gt;
      &lt;th&gt;rev_change&lt;/th&gt;
      &lt;th&gt;gift&lt;/th&gt;
      &lt;th&gt;churn&lt;/th&gt;
      &lt;th&gt;revenue&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;3.98&lt;/td&gt;
      &lt;td&gt;3.36&lt;/td&gt;
      &lt;td&gt;0.86&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;7.28&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;6.28&lt;/td&gt;
      &lt;td&gt;14.41&lt;/td&gt;
      &lt;td&gt;-2.77&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;11.60&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;4.62&lt;/td&gt;
      &lt;td&gt;2.89&lt;/td&gt;
      &lt;td&gt;-2.21&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;3.59&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;3.94&lt;/td&gt;
      &lt;td&gt;0.00&lt;/td&gt;
      &lt;td&gt;-3.26&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0.00&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;2.76&lt;/td&gt;
      &lt;td&gt;3.25&lt;/td&gt;
      &lt;td&gt;-3.43&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;5.33&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;We have information on &lt;code&gt;100_000&lt;/code&gt; customers for which we observe the number of &lt;code&gt;months&lt;/code&gt; they have been active customers, the revenue they generated in the last month (&lt;code&gt;rev_old&lt;/code&gt;), the change in revenue between the last month and the previous one (&lt;code&gt;rev_change&lt;/code&gt;), whether they were randomly sent a &lt;code&gt;gift&lt;/code&gt; and the two outcomes of interest: &lt;code&gt;churn&lt;/code&gt;, i.e. whether they are not active customers anymore and the &lt;code&gt;revenue&lt;/code&gt; they have generated in the current month. We denote the outcomes with the letter &lt;em&gt;Y&lt;/em&gt;, the treatment with the letter &lt;em&gt;W&lt;/em&gt; and the other variables with the letter &lt;em&gt;X&lt;/em&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;Y = [&#39;churn&#39;, &#39;revenue&#39;]
W = &#39;gift&#39;
X = [&#39;months&#39;, &#39;rev_old&#39;, &#39;rev_change&#39;]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt; that, for simplicity, we are considering a single-period snapshot of the data and summarizing the panel structure of the data in just a couple of variables. Usually we would have a longer time series but also a longer time horizon for what concerns the outcome (e.g. &lt;a href=&#34;https://en.wikipedia.org/wiki/Customer_lifetime_value&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;customer lifetime value&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;We can represent the underlying data generating process with the following &lt;strong&gt;Directed Acyclic Graph (DAG)&lt;/strong&gt;. Nodes represent variables and arrows represent potential causal relationships. I have highlighted in green the two relationships of interest: the effect of the &lt;code&gt;gift&lt;/code&gt; on &lt;code&gt;churn&lt;/code&gt; and &lt;code&gt;revenue&lt;/code&gt;. Note that &lt;code&gt;churn&lt;/code&gt; is related to revenue since churned customers by definition generate zero revenue.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-mermaid&#34;&gt;flowchart TD
classDef included fill:#DCDCDC,stroke:#000000,stroke-width:3px;
classDef empty width:-10px,height:-10px,fill:#000000,stroke-width:0px;

W((1$ gift))
D1(( ))
D2(( ))
Y1((churn))
Y2((revenue))
X1((months))
X2((revenue change))
X3((revenue old))

W --- D1
X1 --- D1
D1 --&amp;gt; Y1
W --- D2
X1 --- D2
D2 --&amp;gt; Y2
Y1 --&amp;gt; Y2
X2 --&amp;gt; Y1
X3 --&amp;gt; Y1
X3 --&amp;gt; Y2

class W,Y1,Y2,X1,X2,X3 included;
class D1,D2 empty;

linkStyle 0,2,3,5 stroke:#2db88b,stroke-width:6px;
linkStyle 1,4,6,7,8,9 stroke:#003f5c,stroke-width:6px;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Importantly, past revenue and the revenue change are predictors of &lt;code&gt;churn&lt;/code&gt; and &lt;code&gt;revenue&lt;/code&gt; but are not related to our intervention. On the contrary, the intervention affects &lt;code&gt;churn&lt;/code&gt; and &lt;code&gt;revenue&lt;/code&gt; differentially depending on the customers total active &lt;code&gt;months&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;While simplistic, this data generating process aims at captiring an important &lt;strong&gt;insight&lt;/strong&gt;: variables that are good predictors of &lt;code&gt;churn&lt;/code&gt; or &lt;code&gt;revenue&lt;/code&gt;, are not necessarily variables that predict &lt;code&gt;churn&lt;/code&gt; or &lt;code&gt;revenue&lt;/code&gt; &lt;strong&gt;lift&lt;/strong&gt;. We will see later how this impacts our analysis.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s start first by exploring the data.&lt;/p&gt;
&lt;h2 id=&#34;exploratory-data-analysis&#34;&gt;Exploratory Data Analysis&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s start with &lt;code&gt;churn&lt;/code&gt;. How many customers did the company lose last month?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df.churn.mean()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;0.19767
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The company lost almost &lt;em&gt;20%&lt;/em&gt; of customers last month! Did the &lt;code&gt;gift&lt;/code&gt; help in preventing churn?&lt;/p&gt;
&lt;p&gt;We want to compare the churn frequency of customers that received the gift with the churn frequency of customers that did not receive the gift. Since the gift was randomized, the difference-in-means estimator is an unbiased estimator for the &lt;a href=&#34;https://en.wikipedia.org/wiki/Average_treatment_effect&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;average treatment effect (ATE)&lt;/a&gt; of the &lt;code&gt;gift&lt;/code&gt; on &lt;code&gt;churn&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;$$
ATE = \mathbb{E} \Big[ Y \ \Big| \ W = 1 \Big] - \mathbb{E} \Big[ Y \ \Big| \ W = 0 \Big]
$$&lt;/p&gt;
&lt;p&gt;We compute the difference-in-means estimate by linear regression. We also include other covariates to improve the efficiency of the estimator.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;smf.ols(&amp;quot;churn ~ &amp;quot; + W + &amp;quot; + &amp;quot; + &amp;quot; + &amp;quot;.join(X), data=df).fit().summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
       &lt;td&gt;&lt;/td&gt;         &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt;  &lt;td&gt;    0.3271&lt;/td&gt; &lt;td&gt;    0.002&lt;/td&gt; &lt;td&gt;  151.440&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    0.323&lt;/td&gt; &lt;td&gt;    0.331&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;gift&lt;/th&gt;       &lt;td&gt;   -0.1173&lt;/td&gt; &lt;td&gt;    0.002&lt;/td&gt; &lt;td&gt;  -51.521&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;   -0.122&lt;/td&gt; &lt;td&gt;   -0.113&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;months&lt;/th&gt;     &lt;td&gt;    0.0050&lt;/td&gt; &lt;td&gt;    0.000&lt;/td&gt; &lt;td&gt;   21.832&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    0.005&lt;/td&gt; &lt;td&gt;    0.005&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;rev_old&lt;/th&gt;    &lt;td&gt;   -0.0181&lt;/td&gt; &lt;td&gt;    0.000&lt;/td&gt; &lt;td&gt; -108.061&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;   -0.018&lt;/td&gt; &lt;td&gt;   -0.018&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;rev_change&lt;/th&gt; &lt;td&gt;   -0.0497&lt;/td&gt; &lt;td&gt;    0.001&lt;/td&gt; &lt;td&gt;  -87.412&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;   -0.051&lt;/td&gt; &lt;td&gt;   -0.049&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;It looks like the &lt;code&gt;gift&lt;/code&gt; decreased churn by around &lt;em&gt;11&lt;/em&gt; percentage points, i.e. almost one-third of the baseline level of &lt;em&gt;32%&lt;/em&gt;! Did it also have an impact on &lt;code&gt;revenue&lt;/code&gt;?&lt;/p&gt;
&lt;p&gt;As for churn, we regress &lt;code&gt;revenue&lt;/code&gt; on &lt;code&gt;gift&lt;/code&gt;, our treatment variable, to estimate the average treatment effect.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;smf.ols(&amp;quot;revenue ~ &amp;quot; + W + &amp;quot; + &amp;quot; + &amp;quot; + &amp;quot;.join(X), data=df).fit().summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
       &lt;td&gt;&lt;/td&gt;         &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt;  &lt;td&gt;    0.3691&lt;/td&gt; &lt;td&gt;    0.010&lt;/td&gt; &lt;td&gt;   37.910&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    0.350&lt;/td&gt; &lt;td&gt;    0.388&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;gift&lt;/th&gt;       &lt;td&gt;    0.6317&lt;/td&gt; &lt;td&gt;    0.010&lt;/td&gt; &lt;td&gt;   61.560&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    0.612&lt;/td&gt; &lt;td&gt;    0.652&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;months&lt;/th&gt;     &lt;td&gt;    0.0120&lt;/td&gt; &lt;td&gt;    0.001&lt;/td&gt; &lt;td&gt;   11.768&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    0.010&lt;/td&gt; &lt;td&gt;    0.014&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;rev_old&lt;/th&gt;    &lt;td&gt;    0.8251&lt;/td&gt; &lt;td&gt;    0.001&lt;/td&gt; &lt;td&gt; 1092.846&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    0.824&lt;/td&gt; &lt;td&gt;    0.827&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;rev_change&lt;/th&gt; &lt;td&gt;    0.1457&lt;/td&gt; &lt;td&gt;    0.003&lt;/td&gt; &lt;td&gt;   56.777&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    0.141&lt;/td&gt; &lt;td&gt;    0.151&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;It looks like the &lt;code&gt;gift&lt;/code&gt; on average increased revenue by &lt;em&gt;0.63\$&lt;/em&gt;, which means that it was &lt;strong&gt;not profitable&lt;/strong&gt;. Does it mean that we should stop sending gifts to our customers? It depends. In fact, the gift might be effective for certain customer segments. We just need to identify them.&lt;/p&gt;
&lt;h2 id=&#34;targeting-policies&#34;&gt;Targeting Policies&lt;/h2&gt;
&lt;p&gt;In this section, we will try to understand if there is a data-informed way to send the &lt;code&gt;gift&lt;/code&gt; to customers that is profitable. In particular, we will c&lt;strong&gt;compare&lt;/strong&gt; different data-informed targeting &lt;strong&gt;policies&lt;/strong&gt; with the objective of increasing revenue.&lt;/p&gt;
&lt;p&gt;Throughout this section, we will need some algorithms to either predict &lt;code&gt;revenue&lt;/code&gt;, or &lt;code&gt;churn&lt;/code&gt;, or the probability of receiving the &lt;code&gt;gift&lt;/code&gt;. We use gradient-boosted tree models from the &lt;a href=&#34;https://lightgbm.readthedocs.io/en/latest/index.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;lightgbm&lt;/code&gt;&lt;/a&gt; library  We use the same models for all policies, so that we cannot attribute differences in performance to prediction accuracy.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from lightgbm import LGBMClassifier, LGBMRegressor
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To &lt;strong&gt;evaluate&lt;/strong&gt; each policy denoted with &lt;em&gt;τ&lt;/em&gt;, we compare its profits with the policy &lt;em&gt;Π⁽¹⁾&lt;/em&gt;, with its profits without the policy &lt;em&gt;Π⁽⁰⁾&lt;/em&gt;, over every single individual, in a separate validation dataset. Note that this is usually not possible, since, for each customer, we only observe one of the two &lt;strong&gt;potential outcomes&lt;/strong&gt;, with or without the &lt;code&gt;gift&lt;/code&gt;. However, this is synthetic data, so we can do oracle evaluation. If you want to know more about how to evaluate uplift models with real data, I wrote an article about it.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://towardsdatascience.com/8a078996a113&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://towardsdatascience.com/8a078996a113&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;First of all, let&amp;rsquo;s define profits &lt;em&gt;Π&lt;/em&gt; as the net revenue &lt;em&gt;R&lt;/em&gt; when the customer does not churn &lt;em&gt;C&lt;/em&gt;.
$$
\Pi = R (1-C)
$$&lt;/p&gt;
&lt;p&gt;Therefore, the overall effect on profits for treated individuals is given by the difference between the profits when treated &lt;em&gt;Π⁽¹⁾&lt;/em&gt; minus the profits when not treated &lt;em&gt;Π⁽⁰⁾&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;$$
\tau_{\pi} = R_1 (1 - C_1) - R_0 (1 - C_0)
$$&lt;/p&gt;
&lt;p&gt;The effect for untreated individuals is zero.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def evaluate_policy(policy):
    data = dgp.generate_data(seed_data=4, seed_assignment=5, keep_po=True)
    data[&#39;profits&#39;] = (1 - data.churn) * data.revenue
    baseline = (1-data.churn_c) * data.revenue_c
    effect = policy(data) * (1-data.churn_t) * (data.revenue_t-cost) + (1-policy(data)) * (1-data.churn_c) * data.revenue_c
    return np.sum(effect - baseline)
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;1-target-churning-customers&#34;&gt;1. Target Churning Customers&lt;/h3&gt;
&lt;p&gt;A first policy could be to just target &lt;strong&gt;churning customers&lt;/strong&gt;. Let&amp;rsquo;s say, we send the &lt;code&gt;gift&lt;/code&gt; only to customers with above-average redicted churn.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;model_churn = LGBMClassifier().fit(X=df[X], y=df[&#39;churn&#39;])

policy_churn = lambda df : (model_churn.predict_proba(df[X])[:,1] &amp;gt; df.churn.mean())
evaluate_policy(policy_churn)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;-5497.46
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The policy is not profitable and would lead to an aggregate &lt;strong&gt;loss&lt;/strong&gt; of more than &lt;em&gt;5000\$&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;You might think that the problem is the &lt;strong&gt;arbitrary threshold&lt;/strong&gt;, but this is not the case. Below I plot the aggregate effect for all possible policy thresholds.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;x = np.linspace(0, 1, 100)
y = [evaluate_policy(lambda df : (model_churn.predict_proba(df[X])[:,1] &amp;gt; k)) for k in x]

fig, ax = plt.subplots(figsize=(10, 3))
sns.lineplot(x=x, y=y).set(xlabel=&#39;Churn Policy Threshold&#39;, title=&#39;Aggregate Effect&#39;);
ax.axhline(y=0, c=&#39;k&#39;, lw=3, ls=&#39;--&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/beyond_churn_33_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;As we can see, at best we can make zero losses when we decide not to give the gift to any customer.&lt;/p&gt;
&lt;p&gt;The problem is that the fact that a customer is likely to churn does not imply that the &lt;code&gt;gift&lt;/code&gt; will have any impact on their churn probability. The two measures are not completely unrelated (e.g. we cannot decrease the churning probability of customers that have 0% probability of churning), but they are not the same thing.&lt;/p&gt;
&lt;h3 id=&#34;2-target-revenue-customers&#34;&gt;2. Target revenue customers&lt;/h3&gt;
&lt;p&gt;Let&amp;rsquo;s now try a different policy: we send the gift only to &lt;strong&gt;high-revenue customers&lt;/strong&gt;. For example, we might send the gift only to the top-10% of customers by revenue. The idea is that, if the policy indeed decreases churn, these are the customers for whom decreasing churn is more profitable.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;model_revenue = LGBMRegressor().fit(X=df[X], y=df[&#39;revenue&#39;])

policy_revenue = lambda df : (model_revenue.predict(df[X]) &amp;gt; np.quantile(df.revenue, 0.9))
evaluate_policy(policy_revenue)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;-4730.8200000000015
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The policy is again unprofitable, leading to substantial &lt;strong&gt;losses&lt;/strong&gt;. As before, this is not a problem of selecting the threshold as we can see in the plot below. The best we can do is set a threshold so high that we do not treat anyone and we make zero profits.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;x = np.linspace(0, 100, 100)
y = [evaluate_policy(lambda df : (model_revenue.predict(df[X]) &amp;gt; k*cost)) for k in x]

fig, ax = plt.subplots(figsize=(10, 3))
sns.lineplot(x=x, y=y).set(xlabel=&#39;Revenue Policy Threshold&#39;, title=&#39;Aggregate Effect&#39;);
ax.axhline(y=0, c=&#39;k&#39;, lw=3, ls=&#39;--&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/beyond_churn_39_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;The problem is that, in our setting, the churn probability of high-revenue customers does not decrease enough to make the &lt;code&gt;gift&lt;/code&gt; profitable. This is also partially due to the fact, often observed in reality, that high-revenue customers are also the least likely to churn, to begin with.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s now consider a more relevant set of policies: policies based on &lt;strong&gt;uplift&lt;/strong&gt;.&lt;/p&gt;
&lt;h3 id=&#34;3-target-churn-uplift-customers&#34;&gt;3. Target churn uplift customers&lt;/h3&gt;
&lt;p&gt;A more sensible approach would be to target customers whose &lt;code&gt;churn&lt;/code&gt; probability decreases the most when receiving the &lt;em&gt;1\$&lt;/em&gt; &lt;code&gt;gift&lt;/code&gt;. We estimate churn uplift using the &lt;a href=&#34;https://arxiv.org/abs/2004.14497&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;double-robust estimator&lt;/a&gt;, one of the best performing uplift models. If you are unfamiliar with meta-learners, I recommend starting from my introductory article.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://towardsdatascience.com/8a9c1e340832&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://towardsdatascience.com/8a9c1e340832&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;We import the doubly-robust learner from &lt;a href=&#34;https://econml.azurewebsites.net/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;econml&lt;/a&gt;, a Microsoft library.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from econml.dr import DRLearner

DR_learner_churn = DRLearner(model_regression=LGBMRegressor(), model_propensity=LGBMClassifier(), model_final=LGBMRegressor())
DR_learner_churn.fit(df[&#39;churn&#39;], df[W], X=df[X]);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now that we have estimated churn uplift, we might be tempted to just target customers with a high negative uplift (negative, since we want to &lt;em&gt;decrease&lt;/em&gt; churn). For example, we might send the &lt;code&gt;gift&lt;/code&gt; to all customers with an estimated uplift larger than the average churn.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;policy_churn_lift = lambda df : DR_learner_churn.effect(df[X]) &amp;lt; - np.mean(df.churn)
evaluate_policy(policy_churn_lift)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;-3925.2400000000002
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The policy is still unprofitable, leading to almost &lt;em&gt;4000\$&lt;/em&gt; in losses.&lt;/p&gt;
&lt;p&gt;The problem is that we haven&amp;rsquo;t considered the cost of the policy. In fact, decreasing the churn probability is &lt;strong&gt;only profitable for high-revenue customers&lt;/strong&gt;. Take the extreme case: avoiding churn of a customer that does not generate any revenue is not worth any intervention.&lt;/p&gt;
&lt;p&gt;Therefore, let&amp;rsquo;s only send the &lt;code&gt;gift&lt;/code&gt; to customers whose churn probability weighted by revenue decreases more than the cost of the gift.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;model_revenue_1 = LGBMRegressor().fit(X=df.loc[df[W] == 1, X], y=df.loc[df[W] == 1, &#39;revenue&#39;])

policy_churn_lift = lambda df : - DR_learner_churn.effect(df[X]) * model_revenue_1.predict(df[X]) &amp;gt; cost
evaluate_policy(policy_churn_lift)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;318.03000000000003
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This policy is finally profitable!&lt;/p&gt;
&lt;p&gt;However, we still have not considered one channel: the intervention might also affect the revenue of existing customers.&lt;/p&gt;
&lt;h3 id=&#34;4-target-revenue-uplift-customers&#34;&gt;4. Target revenue uplift customers&lt;/h3&gt;
&lt;p&gt;A symmetric approach to the previous one would be to consider only the impact on &lt;code&gt;revenue&lt;/code&gt;, ignoring the impact on churn. We could estimate the &lt;code&gt;revenue&lt;/code&gt; uplift for non-churning customers and treat only customers whose incremental effect on revenue, net of churn, is greater than the cost of the &lt;code&gt;gift&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;DR_learner_netrevenue = DRLearner(model_regression=LGBMRegressor(), model_propensity=LGBMClassifier(), model_final=LGBMRegressor())
DR_learner_netrevenue.fit(df.loc[df.churn==0, &#39;revenue&#39;], df.loc[df.churn==0, W], X=df.loc[df.churn==0, X]);
model_churn_1 = LGBMClassifier().fit(X=df.loc[df[W] == 1, X], y=df.loc[df[W] == 1, &#39;churn&#39;])

policy_netrevenue_lift = lambda df : DR_learner_netrevenue.effect(df[X]) * (1-model_churn_1.predict(df[X])) &amp;gt; cost
evaluate_policy(policy_netrevenue_lift)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;50.800000000000004
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This policy is profitable as well, but ignores the effect on churn. How do we combine this poilcy with the previous one?&lt;/p&gt;
&lt;h3 id=&#34;5-target-revenue-uplift-customers&#34;&gt;5. Target revenue uplift customers&lt;/h3&gt;
&lt;p&gt;The best way to efficiently &lt;strong&gt;combine&lt;/strong&gt; both the effect on churn and the effect on net revenue, is simply to estimate &lt;strong&gt;total revenue uplift&lt;/strong&gt;. The implied optimal policy is to treat customers whose total revenue uplift is greater than the cost of the &lt;code&gt;gift&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;DR_learner_revenue = DRLearner(model_regression=LGBMRegressor(), model_propensity=LGBMClassifier(), model_final=LGBMRegressor())
DR_learner_revenue.fit(df[&#39;revenue&#39;], df[W], X=df[X]);

policy_revenue_lift = lambda df : (DR_learner_revenue.effect(df[X]) &amp;gt; cost)
evaluate_policy(policy_revenue_lift)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;2028.2100000000003
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It looks like this is by far the best policy, generating an aggregate profit of more than &lt;em&gt;2000\$&lt;/em&gt;!&lt;/p&gt;
&lt;p&gt;The result is starking if we compare all the different policies.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;policies = [policy_churn, policy_revenue, policy_churn_lift, policy_netrevenue_lift, policy_revenue_lift] 
df_results = pd.DataFrame()
df_results[&#39;policy&#39;] = [&#39;churn&#39;, &#39;revenue&#39;, &#39;churn_L&#39;, &#39;netrevenue_L&#39;, &#39;revenue_L&#39;]
df_results[&#39;value&#39;] = [evaluate_policy(policy) for policy in policies]

fig, ax = plt.subplots()
sns.barplot(df_results, x=&#39;policy&#39;, y=&#39;value&#39;)
plt.axhline(0, c=&#39;k&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/beyond_churn_59_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;intuition-and-decomposition&#34;&gt;Intuition and Decomposition&lt;/h2&gt;
&lt;p&gt;If we compare the different policies, it is clear that targeting high-revenue or high-churn probability customers directly were the &lt;strong&gt;worst choices&lt;/strong&gt;. This is not necessarily always the case, but it happened in our simulated data because of two facts that are also common in many real scenarios:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Revenue and churn probability are negatively correlated&lt;/li&gt;
&lt;li&gt;The effect of the &lt;code&gt;gift&lt;/code&gt; on &lt;code&gt;churn&lt;/code&gt; (or &lt;code&gt;revenue&lt;/code&gt;) was not strongly negatively (or positively for &lt;code&gt;revenue&lt;/code&gt;) correlated with the baseline values&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Either one of these two facts can be enough to make targeting revenue or churn a bad strategy. What one should target instead is customers with a high &lt;strong&gt;incremental&lt;/strong&gt; effect. And it&amp;rsquo;s best to directly use as outcome the variable of interest, &lt;code&gt;revenue&lt;/code&gt; in this case, whenever available.&lt;/p&gt;
&lt;p&gt;To better understand the mechanism, we can &lt;strong&gt;decompose&lt;/strong&gt; the aggregate effect of a policy on profits into three parts.&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
\tau_{\pi} &amp;amp;= R_1 * (1 - C_1) - R_0 * (1 - C_0) = \newline
&amp;amp;= R_1 * (1 - C_1) - \underbrace{\hat{r}_1 * (1 - \hat{c}_0) + R_1 * (1 - C_0)} _ {\text{add and subtract}} - R_0 * (1 - C_0) = \newline
&amp;amp;= - R_1 * \tau_c + R_1 * (1 - C_0) - R_0 * (1 - C_0) = \newline
&amp;amp;= - R_1 * \tau_c + \tau_r * (1 - C_0) = \newline
&amp;amp;= \underbrace{- R_0 * \tau_c} _ {\text{incremental effect on churn}} + \underbrace{\tau_r * (1 - C_0)} _ {\text{incremental effect on revenue}} + \underbrace{\tau_r * \tau_c} _ {\text{interaction effect}}
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;This implies that there are &lt;strong&gt;three channels&lt;/strong&gt; that make treating a customer profitable.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;If it&amp;rsquo;s a &lt;em&gt;high-revenue&lt;/em&gt; customer and the treatment &lt;em&gt;decreases&lt;/em&gt; its churn probability&lt;/li&gt;
&lt;li&gt;If it&amp;rsquo;s a &lt;em&gt;non-churning&lt;/em&gt; customer and the treatment &lt;em&gt;increases&lt;/em&gt; its revenue&lt;/li&gt;
&lt;li&gt;It the treatment has a strong impact on &lt;em&gt;both&lt;/em&gt; its revenue and churn probability&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Targeting by churn uplift exploits only the first channel, targeting by net revenue uplift exploits only the second channel, and targeting by total revenue uplift exploits all three channels, making it the &lt;strong&gt;most effective&lt;/strong&gt; method.&lt;/p&gt;
&lt;h3 id=&#34;bonus-weighting&#34;&gt;Bonus: weighting&lt;/h3&gt;
&lt;p&gt;As highlighted by &lt;a href=&#34;https://www.hbs.edu/ris/Publication%20Files/14-020_2d6c9da0-94d3-4dd5-9952-d81feb432f61.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Lemmens, Gupta (2020)&lt;/a&gt;, sometimes it might be worth &lt;strong&gt;weighting&lt;/strong&gt; observations when estimating model uplift. In particular, it might be worth giving more weight to observations close to the treatment policy threshold.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;idea&lt;/strong&gt; is that weighting generally decreases the efficiency of the estimator. However, we are not interested in having correct estimates for all the observations, but rather we are interested in estimating the &lt;strong&gt;policy threshold&lt;/strong&gt; correctly. In fact, whether you estimate a net profit of &lt;em&gt;1\$&lt;/em&gt; or &lt;em&gt;1000\$&lt;/em&gt; it does not matter: the implied policy is the same: send the &lt;code&gt;gift&lt;/code&gt;. However, estimating a net profit of &lt;em&gt;1\$&lt;/em&gt; rather than &lt;em&gt;-1\$&lt;/em&gt; reverses the policy implications. Therefore, a large loss in accuracy away from the threshold sometimes is worth a small gain in accuracy at the threshold.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s try using negative exponential weights, decreasing in distance from the threshold.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;DR_learner_revenue_w = DRLearner(model_regression=LGBMRegressor(), model_propensity=LGBMClassifier(), model_final=LGBMRegressor())
w = np.exp(1 + np.abs(DR_learner_revenue.effect(df[X]) - cost))
DR_learner_revenue_w.fit(df[&#39;revenue&#39;], df[W], X=df[X], sample_weight=w);

policy_revenue_lift_w = lambda df : (DR_learner_revenue_w.effect(df[X]) &amp;gt; cost)
evaluate_policy(policy_revenue_lift_w)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;1398.19
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In our case, weighting is not worth: the implied policy is still profitable, but less than the one obtained with the unweighted model, &lt;em&gt;2028\$&lt;/em&gt;.&lt;/p&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;In this article, we have why and how one should go &lt;strong&gt;beyond&lt;/strong&gt; churn prediction and churn uplift modeling. In particular, one should concentrate on the final business objective of increasing profitability. This implies shifting the focus from &lt;em&gt;prediction&lt;/em&gt; to &lt;em&gt;uplift&lt;/em&gt; but also combining churn and revenue into a single outcome.&lt;/p&gt;
&lt;p&gt;An important caveat concerns the dimension of the data available. We have used a toy dataset that highly simplifies the problem in at least &lt;strong&gt;two dimensions&lt;/strong&gt;. First of all, backwards, we normally have longer time series that can (and should) be used for both prediction and modeling purposes. Second, forward, one should combine churn with a longer-run estimate of customer profitability, usually referred to as &lt;em&gt;customer lifetime value&lt;/em&gt;.&lt;/p&gt;
&lt;h3 id=&#34;references&#34;&gt;References&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Kennedy (2022), &lt;a href=&#34;https://arxiv.org/abs/2004.14497&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;ldquo;Towards Optimal Doubly Robust Estimation of Heterogeneous Causal Effects&amp;rdquo;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Bonvini, Kennedy, Keele (2021), &lt;a href=&#34;https://arxiv.org/abs/2306.17464&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;ldquo;Minimax Optimal Subgroup Identification&amp;rdquo;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Lemmens, Gupta (2020), &lt;a href=&#34;https://www.hbs.edu/ris/Publication%20Files/14-020_2d6c9da0-94d3-4dd5-9952-d81feb432f61.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;ldquo;Managing Churn to Maximize Profits&amp;rdquo;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;related-articles&#34;&gt;Related Articles&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://towardsdatascience.com/8a078996a113&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Evaluating Uplift Models&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://towardsdatascience.com/8a9c1e340832&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Understanding Meta Learners&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://towardsdatascience.com/ed4097dab27a&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Understanding AIPW, the Doubly-Robust Estimator&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;code&#34;&gt;Code&lt;/h3&gt;
&lt;p&gt;You can find the original Jupyter Notebook here:&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/beyond_churn.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/beyond_churn.ipynb&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Evaluating Uplift Models</title>
      <link>https://matteocourthoud.github.io/post/evaluate_uplift/</link>
      <pubDate>Wed, 12 Jul 2023 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/post/evaluate_uplift/</guid>
      <description>&lt;p&gt;One of the most widespread applications of causal inference in the industry is &lt;strong&gt;uplift modeling&lt;/strong&gt;, a.k.a. the estimation of Conditional Average Treatment Effects.&lt;/p&gt;
&lt;p&gt;When estimating the causal effect of a &lt;strong&gt;treatment&lt;/strong&gt; (a drug, ad, product, &amp;hellip;) on an &lt;strong&gt;outcome&lt;/strong&gt; of interest (a disease, firm revenue, customer satisfaction, &amp;hellip;), we are often not only interested in understanding whether the treatment works on average, but we would like to know for which &lt;strong&gt;subjects&lt;/strong&gt; (patients, users, customers, &amp;hellip;) it works better or worse.&lt;/p&gt;
&lt;p&gt;Estimating heterogeneous incremental effects, or uplift, is an essential intermediate step to improve &lt;strong&gt;targeting&lt;/strong&gt; of the policy of interest. For example, we might want to warn certain people that they are more likely to experience side effects from a drug or show an advertisement only to a specific set of customers.&lt;/p&gt;
&lt;p&gt;While there exist many methods to model uplift, it is not always clear which one to use in a specific application. Crucially, because of the &lt;strong&gt;fundamental problem of causal inference&lt;/strong&gt;, the objective of interest, the uplift, is never observed, and therefore we cannot validate our estimators as we would do with a machine learning prediction algorithm. We cannot set aside a validation set and pick the best-performing model since we have &lt;strong&gt;no ground truth&lt;/strong&gt;, not even in the validation set, and not even if we ran a randomized experiment.&lt;/p&gt;
&lt;p&gt;What can we do then? In this article, I try to cover the most popular methods used to &lt;strong&gt;evaluate uplift models&lt;/strong&gt;. If you are unfamiliar with uplift models, I suggest first reading my introductory article.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://towardsdatascience.com/8a9c1e340832&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://towardsdatascience.com/8a9c1e340832&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;uplift-and-promotional-emails&#34;&gt;Uplift and Promotional Emails&lt;/h2&gt;
&lt;p&gt;Imagine we were working in the marketing department of a product company interested in improving our &lt;strong&gt;email marketing campaign&lt;/strong&gt;. Historically, we mostly sent emails to new customers. However, now we would like to adopt a data-driven approach and target customers for whom the email has the highest positive impact on revenue. This impact is also called &lt;strong&gt;uplift&lt;/strong&gt; or &lt;strong&gt;incrementality&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s have a look at the data we have at our disposal. I import the data-generating process &lt;code&gt;dgp_promotional_email()&lt;/code&gt; from &lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/src/dgp.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;src.dgp&lt;/code&gt;&lt;/a&gt;. I also import some plotting functions and libraries from &lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/src/utils.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;src.utils&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;%matplotlib inline
%config InlineBackend.figure_format = &#39;retina&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from src.utils import *
from src.dgp import dgp_promotional_email
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;dgp = dgp_promotional_email(n=500)
df = dgp.generate_data()
df.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;new&lt;/th&gt;
      &lt;th&gt;age&lt;/th&gt;
      &lt;th&gt;sales_old&lt;/th&gt;
      &lt;th&gt;mail&lt;/th&gt;
      &lt;th&gt;sales&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;32.42&lt;/td&gt;
      &lt;td&gt;0.11&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0.03&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;34.92&lt;/td&gt;
      &lt;td&gt;0.36&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0.47&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;41.00&lt;/td&gt;
      &lt;td&gt;0.49&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0.40&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;50.02&lt;/td&gt;
      &lt;td&gt;0.35&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0.53&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;33.34&lt;/td&gt;
      &lt;td&gt;0.12&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0.02&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;We have information on 500 customers, for whom we observe whether they are &lt;code&gt;new&lt;/code&gt; customers, their &lt;code&gt;age&lt;/code&gt;, the sales they generated before the email campaign (&lt;code&gt;sales_old&lt;/code&gt;), whether they were sent the &lt;code&gt;mail&lt;/code&gt;, and the &lt;code&gt;sales&lt;/code&gt; after the email campaign.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;outcome&lt;/strong&gt; of interest is &lt;code&gt;sales&lt;/code&gt;, which we denote with the letter &lt;em&gt;Y&lt;/em&gt;. The &lt;strong&gt;treatment&lt;/strong&gt; or policy that we would like to improve is the &lt;code&gt;mail&lt;/code&gt; campaign, which we denote with the letter &lt;em&gt;W&lt;/em&gt;. We call all the remaining variables &lt;strong&gt;confounders&lt;/strong&gt; or control variables and we denote them with &lt;em&gt;X&lt;/em&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;Y = &#39;sales&#39;
W = &#39;mail&#39;
X = [&#39;age&#39;, &#39;sales_old&#39;, &#39;new&#39;]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The Dyrected Acyclic Graph (DAG) representing the causal relationships between the variables is the following. The causal relationship of interest is depicted in green.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-mermaid&#34;&gt;flowchart TD
classDef included fill:#DCDCDC,stroke:#000000,stroke-width:3px;

W((mail))
Y((sales))
X1((new))
X2((age))
X3((sales old))

W --&amp;gt; Y
X1 --&amp;gt; W
X1 --&amp;gt; Y
X2 --&amp;gt; Y
X3 --&amp;gt; Y

class W,Y,X1,X2,X3 included;

linkStyle 0 stroke:#2db88b,stroke-width:6px;
linkStyle 1,2,3,4 stroke:#003f5c,stroke-width:6px;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;From the DAG we see that the &lt;code&gt;new&lt;/code&gt; customer indicator is a confounder and needs to be controlled for in order to identify the effect of &lt;code&gt;mail&lt;/code&gt; on &lt;code&gt;sales.&lt;/code&gt; &lt;code&gt;age&lt;/code&gt; and &lt;code&gt;sales_old&lt;/code&gt; instead are not essential for estimation but could be helpful for identification. For more information on DAGs and control variables, you can check my introductory article.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://towardsdatascience.com/b63dc69e3d8c&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://towardsdatascience.com/b63dc69e3d8c&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The objective of uplift modeling is to recover the &lt;strong&gt;Individual Treatment Effects (ITE)&lt;/strong&gt; $\tau_i$, i.e. the incremental effect on &lt;code&gt;sales&lt;/code&gt; of sending the promotional &lt;code&gt;mail&lt;/code&gt;. We can express the ITE as the difference between two hypothetical quantities: the potential outcome of the customer if they had received the email, $Y_i^{(1)}$, minus the potential outcome of the customer if they had &lt;em&gt;not&lt;/em&gt; received the email, $Y_i^{(0)}$.
$$
\tau_i = Y_i^{(1)} - Y_i^{(0)}
$$&lt;/p&gt;
&lt;p&gt;Note that for each customer, we only observe one of the two realized outcomes, depending on whether they actually received the &lt;code&gt;mail&lt;/code&gt; or not. Therefore, the ITE are inherently unobservable. What can be estimated instead is the &lt;strong&gt;Conditional Average Treatment Effect (CATE)&lt;/strong&gt; i.e., the expected individual treatment effect $\tau_i$, conditional on covariates &lt;em&gt;X&lt;/em&gt;. For example, the average effect of the &lt;code&gt;mail&lt;/code&gt; on &lt;code&gt;sales&lt;/code&gt; for older customers (&lt;code&gt;age&lt;/code&gt; &amp;gt; 50).
$$
\tau(x) = \mathbb{E} \Big[ \ \tau_i \ \Big| \ X_i = x \Big]
$$&lt;/p&gt;
&lt;p&gt;In order to be able to recover the CATE, we need to make three assumptions.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Unconfoundedness&lt;/strong&gt;: $Y^{(0)}, Y^{(1)} \perp W \ | \ X$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Overlap&lt;/strong&gt;: $0 &amp;lt; e(X) &amp;lt; 1$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Consistency&lt;/strong&gt;: $Y = W \cdot Y^{(1)} + (1-W) \cdot Y^{(0)}$&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Where $e(X)$ is the &lt;strong&gt;propensity score&lt;/strong&gt; i.e., the expected probability of being treated, conditional on covariates &lt;em&gt;X&lt;/em&gt;.
$$
e(x) = \mathbb{E} \Big[ \ W_i \ \Big| \ X_i = x \Big]
$$&lt;/p&gt;
&lt;p&gt;In what follows, we will use machine learning methods to estimate the CATE $\tau(x)$, the propensity scores $e(x)$, and the conditional expectation function (CEF) of the outcome, $\mu(x)$
$$
\mu(x) = \mathbb{E} \Big[ \ Y_i \ \Big| \ X_i = x \Big]
$$&lt;/p&gt;
&lt;p&gt;We use &lt;a href=&#34;https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Random Forest Regression&lt;/a&gt; algorithms to model the CATE and the outcome CEF, while we use &lt;a href=&#34;https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Logistic Regression&lt;/a&gt; to model the propensity score.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from sklearn.ensemble import RandomForestRegressor
from sklearn.linear_model import LogisticRegressionCV

model_tau = RandomForestRegressor(max_depth=2)
model_y = RandomForestRegressor(max_depth=2)
model_e = LogisticRegressionCV()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In this article, we do not fine-tune the underlying machine learning models, but fine-tuning is strongly recommended to improve the accuracy of uplift models (for example, with auto-ml libraries like &lt;a href=&#34;https://microsoft.github.io/FLAML/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;FLAML&lt;/a&gt;).&lt;/p&gt;
&lt;h2 id=&#34;uplift-models&#34;&gt;Uplift Models&lt;/h2&gt;
&lt;p&gt;There exist &lt;strong&gt;many methods&lt;/strong&gt; to model uplift or, in other words, to estimate Conditional Average Treatment Effects (CATE). Since the objective of this article is to compare methods to &lt;em&gt;evaluate&lt;/em&gt; uplift models, we will not explain the methods in detail. For a gentle introduction, you can check &lt;a href=&#34;https://medium.com/towards-data-science/understanding-meta-learners-8a9c1e340832&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;my introductory article on meta learners&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The learners that we will consider are the following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;S-learner or single-learner, introduced by &lt;a href=&#34;https://arxiv.org/abs/1706.03461&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Kunzel, Sekhon, Bickel, Yu (2017)&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;T-learner or two-learner, introduced by &lt;a href=&#34;https://arxiv.org/abs/1706.03461&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Kunzel, Sekhon, Bickel, Yu (2017)&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;X-learner or cross-learner, introduced by &lt;a href=&#34;https://arxiv.org/abs/1706.03461&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Kunzel, Sekhon, Bickel, Yu (2017)&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;R-learner or &lt;a href=&#34;https://www.jstor.org/stable/1912705&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Robinson&lt;/a&gt;-learner introduced by &lt;a href=&#34;https://arxiv.org/abs/1712.04912&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Nie, Wager (2017)&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;DR-learner or doubly-robust-learner, introduced by &lt;a href=&#34;https://arxiv.org/abs/2004.14497&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Kennedy (2022)&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We import all the model from Microsoft&amp;rsquo;s &lt;a href=&#34;https://econml.azurewebsites.net/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;econml&lt;/a&gt; library.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from src.learners_utils import *
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from econml.metalearners import SLearner, TLearner, XLearner
from econml.dml import NonParamDML
from econml.dr import DRLearner

S_learner = SLearner(overall_model=model_y)
T_learner = TLearner(models=clone(model_y))
X_learner = XLearner(models=model_y, propensity_model=model_e, cate_models=model_tau)
R_learner = NonParamDML(model_y=model_y, model_t=model_e, model_final=model_tau, discrete_treatment=True)
DR_learner = DRLearner(model_regression=model_y, model_propensity=model_e, model_final=model_tau)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We &lt;code&gt;fit()&lt;/code&gt; the models on the data, specifying the outcome variable &lt;em&gt;Y&lt;/em&gt;, the treatment variable &lt;em&gt;W&lt;/em&gt; and covariates &lt;em&gt;X&lt;/em&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;names = [&#39;SL&#39;, &#39;TL&#39;, &#39;XL&#39;, &#39;RL&#39;, &#39;DRL&#39;]
learners = [S_learner, T_learner, X_learner, R_learner, DR_learner]
for learner in learners:
    learner.fit(df[Y], df[W], X=df[X])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We are now ready to evaluate the models! Which model should we choose?&lt;/p&gt;
&lt;h2 id=&#34;oracle-loss-functions&#34;&gt;Oracle Loss Functions&lt;/h2&gt;
&lt;p&gt;The main problem of evaluating uplift models is that, even with a validation set and even with a randomized experiment or AB test, we do &lt;strong&gt;not observe&lt;/strong&gt; our metric of interest: the Individual Treatment Effects. In fact, we only observe the realized outcomes, $Y_i^{(0)}$ for untreated customers and $Y_i^{(1)}$ for treated customers. Therefore, for no customer we can compute the individual treatment effect in the validation data, $\tau_i = Y_i^{(1)} - Y_i^{(0)}$.&lt;/p&gt;
&lt;p&gt;Can we still do something to &lt;strong&gt;evaluate&lt;/strong&gt; our estimators?&lt;/p&gt;
&lt;p&gt;The answer is yes, but before giving more details, let&amp;rsquo;s first understand what we would do if we &lt;strong&gt;could observe&lt;/strong&gt; the Individual Treatment Effects $\tau_i$.&lt;/p&gt;
&lt;h3 id=&#34;oracle-mse-loss&#34;&gt;Oracle MSE Loss&lt;/h3&gt;
&lt;p&gt;If we could observe the individual treatment effects (but we don&amp;rsquo;t, hence the &amp;ldquo;oracle&amp;rdquo; attribute), we could try to measure how far our estimates $\hat{\tau}(X_i)$ are from the true values $\tau_i$. This is what we normally do in machine learning when we want to evaluate a prediction method: we set aside a validation dataset and we compare predicted and true values on that data. There exist plenty of loss functions to evaluate prediction accurary, so let&amp;rsquo;s concentrate on the most popular one: the &lt;strong&gt;Mean Squared Error (MSE) loss&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;$$
\mathcal{L} _ {oracle-MSE}(\hat{\tau}) = \frac{1}{n} \sum _ {i=1}^{n} \left( \hat{\tau}(X_i) - \tau(X_i) \right)^2
$$&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def loss_oracle_mse(data, learner):
    tau = learner.effect(data[X])
    return np.mean((tau - data[&#39;effect_on_sales&#39;])**2)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The function &lt;code&gt;compare_methods&lt;/code&gt; prints and plots evaluation metrics computed on a separate validation dataset.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def compare_methods(learners, names, loss, title=None, subtitle=&#39;lower is better&#39;):
    data = dgp.generate_data(seed_data=1, seed_assignment=1, keep_po=True)
    results = pd.DataFrame({
        &#39;learner&#39;: names,
        &#39;loss&#39;: [loss(data.copy(), learner) for learner in learners]
    })
    fig, ax = plt.subplots(1, 1, figsize=(6, 4))
    sns.barplot(data=results, x=&amp;quot;learner&amp;quot;, y=&#39;loss&#39;).set(ylabel=&#39;&#39;)
    plt.suptitle(title, y=1.02)
    plt.title(subtitle, fontsize=12, fontweight=None, y=0.94)
    return results
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;compare_methods(learners, names, loss_oracle_mse, title=&#39;Oracle MSE Loss&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;learner&lt;/th&gt;
      &lt;th&gt;loss&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;SL&lt;/td&gt;
      &lt;td&gt;0.002932&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;TL&lt;/td&gt;
      &lt;td&gt;0.004637&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;XL&lt;/td&gt;
      &lt;td&gt;0.000936&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;RL&lt;/td&gt;
      &lt;td&gt;0.000990&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;DRL&lt;/td&gt;
      &lt;td&gt;0.000577&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;&lt;img src=&#34;img/evaluate_uplift_31_1.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;In this case, we see that the T-learner clearly performs worst, with the S-learner just behind. On the other hand, the X-, R- and DR-learners perform significantly better, with the &lt;strong&gt;DR-learner winning&lt;/strong&gt; the race.&lt;/p&gt;
&lt;p&gt;However, this might &lt;em&gt;not&lt;/em&gt; be the best loss function to evaluate our uplift model. In fact, uplift modeling is just an intermediate step towards our ultimate goal: improving revenue.&lt;/p&gt;
&lt;h3 id=&#34;oracle-policy-gain&#34;&gt;Oracle Policy Gain&lt;/h3&gt;
&lt;p&gt;Since our ultimate goal is to &lt;strong&gt;improve revenue&lt;/strong&gt;, we could evaluate estimators by how much they increase revenue, given a certain policy function. Suppose, for example, that we had a $0.01$\$ cost of sending an email. Then, our policy would be to treat each costumer that has a predicted Conditional Average Treatment Effect above $0.01$\$.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;cost = 0.01
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;How much would our revenue actually increase? Let&amp;rsquo;s define with $d(\hat{\tau})$ our policy function, such that $d=1$ if $\tau &amp;gt;= 0.1$ and $d=0$ otherwise. Then our &lt;em&gt;gain&lt;/em&gt; (higher is better) function is:
$$
\mathcal{G} _ {oracle-POLICY}(\hat{\tau}) = \frac{1}{n} \sum _ {i=1}^{n} d(\hat{\tau}) (\tau_i - c)
$$&lt;/p&gt;
&lt;p&gt;Again, this is an &amp;ldquo;oracle&amp;rdquo; loss function that &lt;strong&gt;cannot be computed&lt;/strong&gt; in reality since we do not observe the individual treatment effects.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def gain_oracle_policy(data, learner):
    tau_hat = learner.effect(data[X])
    return np.sum((data[&#39;effect_on_sales&#39;] - cost) * (tau_hat &amp;gt; cost))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;compare_methods(learners, names, gain_oracle_policy, title=&#39;Oracle Policy Gain&#39;, subtitle=&#39;higher is better&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;learner&lt;/th&gt;
      &lt;th&gt;loss&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;SL&lt;/td&gt;
      &lt;td&gt;0.00&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;TL&lt;/td&gt;
      &lt;td&gt;4.23&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;XL&lt;/td&gt;
      &lt;td&gt;10.98&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;RL&lt;/td&gt;
      &lt;td&gt;10.43&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;DRL&lt;/td&gt;
      &lt;td&gt;12.33&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;&lt;img src=&#34;img/evaluate_uplift_38_1.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;In this case, the S-learner is clearly the worst performer, leading to no effect on revenues. The T-learner leads to modest gains while the X-, R- and DR- learners all lead to aggregate gains, with the &lt;strong&gt;X-learner slightly ahead&lt;/strong&gt;.&lt;/p&gt;
&lt;h2 id=&#34;practical-loss-functions&#34;&gt;Practical Loss Functions&lt;/h2&gt;
&lt;p&gt;In the previous section, we have seen two examples of loss functions that we would like to compute if we could observe the Individual Treatment Effects $\tau_i$. However, in practice, even with a randomized experiment and even with a validation set, we do not observe the ITE,our object of interest. We will now cover some measures that try to evaluate uplift models, given this practical constraint.&lt;/p&gt;
&lt;h3 id=&#34;outcome-loss&#34;&gt;Outcome Loss&lt;/h3&gt;
&lt;p&gt;The first and simplest approach is to switch to a different loss variable. While we cannot observe the Individual Treatment Effects, $\tau_i$, we can still observe our outcome $y_i$. This is not exactly our object of interest, but we might expect an uplift model that performs well in terms of predicting $y$ to also produce good estimates of $\tau$.&lt;/p&gt;
&lt;p&gt;One such loss function could be the &lt;strong&gt;Outcome MSE loss&lt;/strong&gt;, which is the usual MSE loss function for prediction methods.
$$
\mathcal{L}_{Y}(\hat{\mu}) = \frac{1}{n} \sum _ {i=1}^{n} \Big( \hat{\mu}(X_i, W_i) - Y_i \Big)^2
$$&lt;/p&gt;
&lt;p&gt;The problem here is that not all models directly produce an estimate of $\mu(x)$ and, even when they do, it is not the object of interest.&lt;/p&gt;
&lt;h3 id=&#34;prediction-to-prediction-loss&#34;&gt;Prediction to Prediction Loss&lt;/h3&gt;
&lt;p&gt;Another very simple approach could be to compare the predictions of the model trained on the training set with the predictions of another model trained on the validation set. While intuitive, this appraoch could be &lt;strong&gt;extremely misleading&lt;/strong&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def loss_pred(data, learner):
    tau = learner.effect(data[X])
    learner2 = copy.deepcopy(learner).fit(data[Y], data[W], X=data[X])
    tau2 = learner2.effect(data[X])
    return np.mean((tau - tau2)**2)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;compare_methods(learners, names, loss_pred, &#39;Prediction to Prediction Loss&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;learner&lt;/th&gt;
      &lt;th&gt;loss&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;SL&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;TL&lt;/td&gt;
      &lt;td&gt;0.007342&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;XL&lt;/td&gt;
      &lt;td&gt;0.000366&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;RL&lt;/td&gt;
      &lt;td&gt;0.134137&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;DRL&lt;/td&gt;
      &lt;td&gt;0.000933&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;&lt;img src=&#34;img/evaluate_uplift_47_1.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Unsurprisingly, this metric performs extremely bad, and you should &lt;strong&gt;never use it&lt;/strong&gt;, since it rewards models that are consistent, irrespectively of their quality. A model that always predicts a random constant CATE for each observations would obtain a perfect score.&lt;/p&gt;
&lt;h3 id=&#34;distribution-loss&#34;&gt;Distribution Loss&lt;/h3&gt;
&lt;p&gt;A different approach is to ask: how well can we match the distribution of potential outcomes? We can do this exarcise for either the &lt;em&gt;treated&lt;/em&gt; or &lt;em&gt;untreated&lt;/em&gt; potential outcomes. Let&amp;rsquo;s take the last case. Suppose we take the observed &lt;code&gt;sales&lt;/code&gt; for customers that did &lt;em&gt;not&lt;/em&gt; receive the &lt;code&gt;mail&lt;/code&gt; and the observed &lt;code&gt;sales&lt;/code&gt; &lt;em&gt;minus&lt;/em&gt; the estimated CATE $\hat{\tau}(x)$ for customers that did receive the &lt;code&gt;mail&lt;/code&gt;. By the &lt;strong&gt;unconfoundedness&lt;/strong&gt; assumption, these two distributions of the untreated potential outcome should be similar, conditional on covariates $X$.&lt;/p&gt;
&lt;p&gt;Therefore, we expect the distance between the two distributions to be close if we correctly estimated the treatment effects.
$$
dist \ \Big( \ {Y_i, X_i | W_i=0 } \ , \ {Y_i - \hat{\tau}(X_i), X_i | W_i=1 } \ \Big)
$$&lt;/p&gt;
&lt;p&gt;We can also do the same exercise for the &lt;em&gt;treated&lt;/em&gt; potential outcome.&lt;/p&gt;
&lt;p&gt;$$
dist \ \Big( \ {Y_i + \hat{\tau}(X_i), X_i | W_i=0 } \ , \ {Y_i, X_i | W_i=1 } \ \Big)
$$&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from dcor import energy_distance

def loss_dist(data, learner):
    tau = learner.effect(data[X])
    data.loc[data.mail==1, &#39;sales&#39;] -= tau[data.mail==1]
    return energy_distance(data.loc[data.mail==0, [Y] + X], data.loc[data.mail==1, [Y] + X], exponent=2)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;compare_methods(learners, names, loss_dist, &#39;Distribution Loss&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;learner&lt;/th&gt;
      &lt;th&gt;loss&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;SL&lt;/td&gt;
      &lt;td&gt;1.728523&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;TL&lt;/td&gt;
      &lt;td&gt;1.733941&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;XL&lt;/td&gt;
      &lt;td&gt;1.733993&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;RL&lt;/td&gt;
      &lt;td&gt;1.736704&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;DRL&lt;/td&gt;
      &lt;td&gt;1.735105&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;&lt;img src=&#34;img/evaluate_uplift_52_1.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;This measure is extremely noisy and rewards the S-learner followed by the T-learner which are actually the two worst performing models.&lt;/p&gt;
&lt;h3 id=&#34;above-below-median-difference&#34;&gt;Above-below Median Difference&lt;/h3&gt;
&lt;p&gt;The above-below median loss tries to answer the question: is our uplift model detecting &lt;strong&gt;any heterogeneity&lt;/strong&gt;? In particular, if we take the validation set and we split the sample into above-median and below median predicted uplift $\hat{\tau}(x)$, how big is the actual difference in average effect, estimated with a difference-in-means estimator? We would expect better estimators to better split the sample into high-effects and low-effects.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from statsmodels.formula.api import ols 

def loss_ab(data, learner):
    tau = learner.effect(data[X]) + np.random.normal(0, 1e-8, len(data))
    data[&#39;above_median&#39;] = tau &amp;gt;= np.median(tau)
    param = ols(&#39;sales ~ mail * above_median&#39;, data=data).fit().params[-1]
    return param
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;compare_methods(learners, names, loss_ab, title=&#39;Above-below Median Difference&#39;, subtitle=&#39;higher is better&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;learner&lt;/th&gt;
      &lt;th&gt;loss&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;SL&lt;/td&gt;
      &lt;td&gt;-0.008835&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;TL&lt;/td&gt;
      &lt;td&gt;0.221423&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;XL&lt;/td&gt;
      &lt;td&gt;0.093177&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;RL&lt;/td&gt;
      &lt;td&gt;0.134629&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;DRL&lt;/td&gt;
      &lt;td&gt;0.075319&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;&lt;img src=&#34;img/evaluate_uplift_57_1.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Unfortunately, the above-below median difference rewards the T-learner, which is among the worst performing models.&lt;/p&gt;
&lt;p&gt;It&amp;rsquo;s important to note that the difference-in-means estimators in the two groups (above- and below- median $\hat{\tau}(x)$) are &lt;strong&gt;not guaranteed to be unbiased&lt;/strong&gt;, even if the data came from a randomized experiment. In fact, we have split the two groups on a variable, $\hat{\tau}(x)$, that is highly endogenous. Therefore, the method should be used with a grain of salt.&lt;/p&gt;
&lt;h3 id=&#34;uplift-curve&#34;&gt;Uplift Curve&lt;/h3&gt;
&lt;p&gt;An extension of the above-below median test is the &lt;strong&gt;uplift curve&lt;/strong&gt;. The idea is simple: instead of splitting the sample into two groups based on the median (0.5 quantile), why not split the data into more groups (more quantiles)?&lt;/p&gt;
&lt;p&gt;For each group, we compute the difference-in-means estimate, and we plot its cumulative sum against the corresponding quantile. The result is called &lt;strong&gt;uplift curve&lt;/strong&gt;. The interpretation is simple: the higher the curve, the better we are able to separate high- from low-effect observations. However, also the same &lt;strong&gt;disclaimer&lt;/strong&gt; applies: the difference-in-means estimates are not unbiased. Therefore, they should be used with a grain of salt.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def generate_uplift_curve(df):
    Q = 20
    df_q = pd.DataFrame()
    data = dgp.generate_data(seed_data=1, seed_assignment=1, keep_po=True)
    ate = np.mean(data[Y][data[W]==1]) - np.mean(data[Y][data[W]==0])
    for learner, name in zip(learners, names):
        data[&#39;tau_hat&#39;] = learner.effect(data[X])
        data[&#39;q&#39;] = pd.qcut(-data.tau_hat + np.random.normal(0, 1e-8, len(data)), q=Q, labels=False)
        for q in range(Q):
            temp = data[data.q &amp;lt;= q]
            uplift = (np.mean(temp[Y][temp[W]==1]) - np.mean(temp[Y][temp[W]==0])) * q / (Q-1)
            df_q = pd.concat([df_q, pd.DataFrame({&#39;q&#39;: [q], &#39;uplift&#39;: [uplift], &#39;learner&#39;: [name]})], ignore_index=True)
    
    fig, ax = plt.subplots(1, 1, figsize=(8, 5))
    sns.lineplot(x=range(Q), y=ate*range(Q)/(Q-1), color=&#39;k&#39;, ls=&#39;--&#39;, lw=3)
    sns.lineplot(x=&#39;q&#39;, y=&#39;uplift&#39;, hue=&#39;learner&#39;, data=df_q);
    plt.suptitle(&#39;Uplift Curve&#39;, y=1.02, fontsize=28, fontweight=&#39;bold&#39;)
    plt.title(&#39;higher is better&#39;, fontsize=14, fontweight=None, y=0.96)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;generate_uplift_curve(df)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/evaluate_uplift_62_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;While probably not the best method to &lt;em&gt;evaluate&lt;/em&gt; uplift models, the uplift curve is very important in &lt;strong&gt;understanding&lt;/strong&gt; and &lt;strong&gt;implementing&lt;/strong&gt; them. In fact, for each model, it tells us that is the expected average treatment effect (y-axis) as we increase the share of the treated population (x-axis).&lt;/p&gt;
&lt;h3 id=&#34;nearest-neighbor-match&#34;&gt;Nearest Neighbor Match&lt;/h3&gt;
&lt;p&gt;The last couple of methods we analyzed, aggregated data in order to understand whether the methods work on larger groups. The nearest neighbor match tries instead to understand how well an uplift model predicts individual treatment effects. However, since the ITEs are not observable, it tries to build a &lt;strong&gt;proxy by matching&lt;/strong&gt; treated and control observations on observable characteristics $X$.&lt;/p&gt;
&lt;p&gt;For example, if we take all treated observations ($i: W_i=1$), and we find the nearest neighbor in the control group ($NN_0(X_i)$), the corresponding MSE loss function is
$$
\mathcal{L} _ {NN}(\hat{\tau}) = \frac{1}{n} \sum _ {i: W_i=1} \Big( \hat{\tau}(X_i) - (Y_i - NN_0(X_i)) \Big)^2
$$&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from scipy.spatial import KDTree

def loss_nn(data, learner):
    tau_hat = learner.effect(data[X])
    nn0 = KDTree(data.loc[data[W]==0, X].values)
    control_index = nn0.query(data.loc[data[W]==1, X], k=1)[-1]
    tau_nn = data.loc[data[W]==1, Y].values - data.iloc[control_index, :][Y].values
    return np.mean((tau_hat[data[W]==1] - tau_nn)**2)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;compare_methods(learners, names, loss_nn, title=&#39;Nearest Neighbor Loss&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;learner&lt;/th&gt;
      &lt;th&gt;loss&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;SL&lt;/td&gt;
      &lt;td&gt;0.050478&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;TL&lt;/td&gt;
      &lt;td&gt;0.051301&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;XL&lt;/td&gt;
      &lt;td&gt;0.047102&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;RL&lt;/td&gt;
      &lt;td&gt;0.046684&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;DRL&lt;/td&gt;
      &lt;td&gt;0.046652&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;&lt;img src=&#34;img/evaluate_uplift_67_1.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;In this case, the nearest neighbor loss performs quite well, identifying the two worse performing methods, the S- and T-learner.&lt;/p&gt;
&lt;h3 id=&#34;ipw-loss&#34;&gt;IPW Loss&lt;/h3&gt;
&lt;p&gt;The Inverse Probability Weighting (IPW) loss function was first proposed by &lt;a href=&#34;https://proceedings.mlr.press/v67/gutierrez17a/gutierrez17a.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Gutierrez, Gerardy (2017)&lt;/a&gt;, and it is the first of three metrics that we are going to see that uses a &lt;strong&gt;pseudo-outcome&lt;/strong&gt; $Y^{*}$ to evaluate the estimator. Pseudo-outcomes are variables whose expected value is the Conditional Average Treatment Effect, but that are too volatile to be directly used as estimates. For a more detailed explanation of pseudo-outcomes, I suggest &lt;a href=&#34;https://towardsdatascience.com/920177462149&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;my article on causal regression trees&lt;/a&gt;. The pseudo-outcome corresponding to the IPW loss is
$$
Y^* _ {IPW} = Y_i \frac{W_i - \hat{e}(X_i)}{\hat{e}(X_i)(1 - \hat{e}(X_i))}
$$&lt;/p&gt;
&lt;p&gt;so that the corresponding loss function is
$$
\mathcal{L} _ {IPW} = \frac{1}{n} \sum_{i=1}^{n} \left( \hat{\tau}(X_i) - Y_i \ \frac{W_i - \hat{e}(X_i)}{\hat{e}(X_i)(1 - \hat{e}(X_i))} \right)^2
$$&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def loss_ipw(data, learner):
    tau_hat = learner.effect(data[X])
    e_hat = clone(model_e).fit(data[X], data[W]).predict_proba(data[X])[:,1]
    tau_gg = data[Y] * (data[W] - e_hat) / (e_hat * (1 - e_hat))
    return np.mean((tau_hat - tau_gg)**2)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;compare_methods(learners, names, loss_ipw, title=&#39;IPW Loss&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;learner&lt;/th&gt;
      &lt;th&gt;loss&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;SL&lt;/td&gt;
      &lt;td&gt;1.170917&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;TL&lt;/td&gt;
      &lt;td&gt;1.153752&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;XL&lt;/td&gt;
      &lt;td&gt;1.172517&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;RL&lt;/td&gt;
      &lt;td&gt;1.172934&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;DRL&lt;/td&gt;
      &lt;td&gt;1.171769&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;&lt;img src=&#34;img/evaluate_uplift_72_1.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;The IPW loss is extremely noisy. A solution is to use its more robust variations, the R-loss or the DR-loss which we present next.&lt;/p&gt;
&lt;h3 id=&#34;r-loss&#34;&gt;R Loss&lt;/h3&gt;
&lt;p&gt;The R-loss was introduced together with the R-learner by &lt;a href=&#34;https://arxiv.org/abs/1712.04912&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Nie, Wager (2017)&lt;/a&gt;, and it is essentially the &lt;strong&gt;objective function&lt;/strong&gt; of the R-learner. As for the IPW-loss, the idea is to try to match a pseudo outcome whose expected value is the Conditional Average Treatment Effect.
$$
Y^* _ {R} = \frac{Y_i - \hat{\mu}_W(X_i)}{W_i - \hat{e}(X_i)}
$$&lt;/p&gt;
&lt;p&gt;The corresponding loss function is
$$
\mathcal{L}_{R} = \frac{1}{n} \sum _ {i=1}^{n} \left( \hat{\tau}(X_i) -  \frac{Y_i - \hat{\mu}_W(X_i)}{W_i - \hat{e}(X_i)} \right)^2
$$&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def loss_r(data, learner):
    tau_hat = learner.effect(data[X])
    y_hat = clone(model_y).fit(df[X + [W]], df[Y]).predict(data[X + [W]])
    e_hat = clone(model_e).fit(df[X], df[W]).predict_proba(data[X])[:,1]
    tau_nw = (data[Y] - y_hat) / (data[W] - e_hat)
    return np.mean((tau_hat - tau_nw)**2)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;results = compare_methods(learners, names, loss_r, title=&#39;R Loss&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/evaluate_uplift_77_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;The R-loss is sensibly less noisy than the IPW loss and it clearly isolates the S-learner. However, it tends to favor its corresponding learner, the R-learner.&lt;/p&gt;
&lt;h3 id=&#34;dr-loss&#34;&gt;DR Loss&lt;/h3&gt;
&lt;p&gt;The DR-loss is the &lt;strong&gt;objective function&lt;/strong&gt; of the DR-learner, and it was first introduced by &lt;a href=&#34;https://arxiv.org/abs/1909.05299&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Saito, Yasui (2020)&lt;/a&gt;. As for the IPW- and the R-loss, the idea is to try to match a pseudo outcome, whose expected value is the Conditional Average Treatment Effect. The DR pseudo-outcome is strongly related to the &lt;a href=&#34;https://towardsdatascience.com/ed4097dab27a&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;AIPW estimator&lt;/a&gt;, also known as doubly-robust estimator, hence the DR name.
$$
Y^* _ {DR} = \hat{\mu}_1(X_i) - \hat{\mu}_0(X_i) + (Y_i - \hat{\mu}_W(X_i)) \ \frac{W_i - \hat{e}(X_i)}{\hat{e}(X_i)(1 - \hat{e}(X_i))}
$$&lt;/p&gt;
&lt;p&gt;The corresponding loss function is
$$
\mathcal{L} _ {DR} = \frac{1}{n} \sum _ {i=1}^{n} \left( \hat{\tau}(X_i) - \hat{\mu}_1(X_i) + \hat{\mu}_0(X_i) - (Y_i - \hat{\mu}_W(X_i)) \ \frac{W_i - \hat{e}(X_i)}{\hat{e}(X_i)(1 - \hat{e}(X_i))} \right)^2
$$&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def loss_dr(data, learner):
    tau_hat = learner.effect(data[X])
    y_hat = clone(model_y).fit(df[X + [W]], df[Y]).predict(data[X + [W]])
    mu1 = clone(model_y).fit(df[X + [W]], df[Y]).predict(data[X + [W]].assign(mail=1))
    mu0 = clone(model_y).fit(df[X + [W]], df[Y]).predict(data[X + [W]].assign(mail=0))
    e_hat = clone(model_e).fit(df[X], df[W]).predict_proba(data[X])[:,1]
    tau_nw = mu1 - mu0 + (data[Y] - y_hat) * (data[W] - e_hat) / (e_hat * (1 - e_hat))
    return np.mean((tau_hat - tau_nw)**2)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;results = compare_methods(learners, names, loss_dr, title=&#39;DR Loss&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/evaluate_uplift_82_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;As for the R-loss, the DR-loss tends to favor its corresponding learner, the DR-learner. However, it provides a more accurate ranking in terms of algorithms&amp;rsquo; accuracy.&lt;/p&gt;
&lt;h3 id=&#34;empirical-policy-gain&#34;&gt;Empirical Policy Gain&lt;/h3&gt;
&lt;p&gt;The last loss function that we are going to analyze is different from all the others we have seen so far since it does &lt;em&gt;not&lt;/em&gt; focus on how well we are able to estimate the treatment effects but rather on how well would the corresponding &lt;strong&gt;optimal treatment policy&lt;/strong&gt; performs. In particular, &lt;a href=&#34;https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3111957&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Hitsch, Misra, Zhang (2023)&lt;/a&gt; propose the following gain function:&lt;/p&gt;
&lt;p&gt;$$
\mathcal{G} _ {HMZ} = \sum _ {i=1}^{n} \left( W_i \cdot d(\hat{\tau}) \cdot \frac{Y_i - c}{\hat{e}(X_i)} + (1-W_i) \cdot (1-d(\hat{\tau})) \cdot \frac{Y_i}{1-\hat{e}(X_i)} \right)
$$&lt;/p&gt;
&lt;p&gt;where $c$ is the treatment cost and $d$ is the optimal treatment policy given the estimated CATE $\hat{\tau}(X_i)$. In our case, we assume an individual treatment cost of $c=0.01$\$, so that the optimal policy is to treat every customer with an estimated CATE larger than 0.01.&lt;/p&gt;
&lt;p&gt;The terms $W_i \cdot d(X_i)$ and $(1-W_i) \cdot (1-d(X_i))$ imply that we use for the calculation only individuals for whom the actual treatment &lt;em&gt;W&lt;/em&gt; corresponds with the optimal one, &lt;em&gt;d&lt;/em&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def gain_policy(data, learner):
    tau_hat = learner.effect(data[X])
    e_hat = clone(model_e).fit(data[X], data[W]).predict_proba(data[X])[:,1]
    d = tau_hat &amp;gt; cost
    return np.sum((d * data[W] * (data[Y] - cost)/ e_hat + (1-d) * (1-data[W]) * data[Y] / (1-e_hat)))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;results = compare_methods(learners, names, gain_policy, title=&#39;Empirical Policy Gain&#39;, subtitle=&#39;higher is better&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/evaluate_uplift_87_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;The empirical policy gain performs very well, isolating the two worst performing methods, the S- and T-learners.&lt;/p&gt;
&lt;h2 id=&#34;meta-studies&#34;&gt;Meta Studies&lt;/h2&gt;
&lt;p&gt;In this article we have introduced a wide variety of methods to evaluate uplift models, a.k.a. Conditional Average Treatment Effect estimators. We have also tested in our simulated dataset, which is a very special and limited example. How do these metrics &lt;strong&gt;perform&lt;/strong&gt; in general?&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/1804.05146&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Schuler, Baiocchi, Tibshirani, Shah (2018)&lt;/a&gt; compares the S-loss, T-loss, R-loss, on &lt;strong&gt;simulated data&lt;/strong&gt;, for the corresponding estimators. They find that the R-loss &amp;ldquo;&lt;em&gt;is the validation set metric that, when optimized, most consistently leads to the selection of a high-performing model&lt;/em&gt;&amp;rdquo;. The authors also detect the so-called &lt;strong&gt;congeniality bias&lt;/strong&gt;: metrics such as the R- or DR-loss tend to be biased towards the corresponding learner.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2302.02923&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Curth, van der Schaar (2023)&lt;/a&gt; studies a broader array of learners from a &lt;strong&gt;theoretical perspective&lt;/strong&gt;. They find that &amp;ldquo;&lt;em&gt;no existing selection criterion is globally best across all experimental conditions we consider&lt;/em&gt;&amp;rdquo;.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2211.01939&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Mahajan, Mitliagkas, Neal, Syrgkanis (2023)&lt;/a&gt; is the &lt;strong&gt;most comprehensive&lt;/strong&gt; study in terms of scope. The authors compare many metrics on 144 datasets and 415 estimators. They find that “&lt;em&gt;no metric significantly dominates the rest&lt;/em&gt;” but “&lt;em&gt;metrics that use DR elements seem to always be among the candidate winners&lt;/em&gt;”.&lt;/p&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;In this article, we have explored multiple methods to evaluate uplift models. The &lt;strong&gt;main challenge&lt;/strong&gt; is the unobservability of the variable of interest, the Individual Treatment Effects. Therefore, different methods try to evaluate uplift models either using other variables, using proxy outcomes, or approximating the effect of implied optimal policies.&lt;/p&gt;
&lt;p&gt;It is hard to recommend using a single method since there is &lt;strong&gt;no consensus&lt;/strong&gt; on which one performs best, neither from a theoretical nor from an empirical perspective. Loss functions that use R- and DR- elements tend to perform &lt;strong&gt;consistently better&lt;/strong&gt;, but are also biased towards the corresponding learners. Understanding how these metrics work, however, can help in understanding their biases and limitations in order to make the most appropriate decisions depending on the specific scenario.&lt;/p&gt;
&lt;h3 id=&#34;references&#34;&gt;References&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Curth, van der Schaar (2023), &lt;a href=&#34;https://arxiv.org/abs/2302.02923&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;ldquo;In Search of Insights, Not Magic Bullets: Towards Demystification of the Model Selection Dilemma in Heterogeneous Treatment Effect Estimation&amp;rdquo;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Gutierrez, Gerardy (2017), &lt;a href=&#34;https://proceedings.mlr.press/v67/gutierrez17a/gutierrez17a.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;ldquo;Causal Inference and Uplift Modeling: A review of the literature&amp;rdquo;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Hitsch, Misra, Zhang (2023), &lt;a href=&#34;https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3111957&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;ldquo;Heterogeneous Treatment Effects and Optimal Targeting Policy Evaluation&amp;rdquo;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Kennedy (2022), &lt;a href=&#34;https://arxiv.org/abs/2004.14497&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;ldquo;Towards optimal doubly robust estimation of heterogeneous causal effects&amp;rdquo;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Kunzel, Sekhon, Bickel, Yu (2017), &lt;a href=&#34;https://arxiv.org/abs/1706.03461&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;ldquo;Meta-learners for Estimating Heterogeneous Treatment Effects using Machine Learning&amp;rdquo;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Mahajan, Mitliagkas, Neal, Syrgkanis (2023), &lt;a href=&#34;https://arxiv.org/abs/2211.01939&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;ldquo;Empirical Analysis of Model Selection for Heterogeneous Causal Effect Estimation&amp;rdquo;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Nie, Wager (2017), &lt;a href=&#34;https://arxiv.org/abs/1712.04912&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;ldquo;Quasi-Oracle Estimation of Heterogeneous Treatment Effects&amp;rdquo;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Saito, Yasui (2020), &lt;a href=&#34;https://arxiv.org/abs/1909.05299&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;ldquo;Counterfactual Cross-Validation: Stable Model Selection Procedure for Causal Inference Models&amp;rdquo;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Schuler, Baiocchi, Tibshirani, Shah (2018), &lt;a href=&#34;https://arxiv.org/abs/1804.05146&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;ldquo;A comparison of methods for model selection when estimating individual treatment effects&amp;rdquo;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;related-articles&#34;&gt;Related Articles&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://towardsdatascience.com/8a9c1e340832&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Understanding Meta Learners&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://towardsdatascience.com/ed4097dab27a&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Understanding AIPW, the Doubly-Robust Estimator&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://towardsdatascience.com/920177462149&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Understanding Causal Trees&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://towardsdatascience.com/43c4536f1481&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;From Causal Trees to Forests&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;code&#34;&gt;Code&lt;/h3&gt;
&lt;p&gt;You can find the original Jupyter Notebook here:&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/evaluate_uplift.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/evaluate_uplift.ipynb&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Bayesian AB Testing</title>
      <link>https://matteocourthoud.github.io/post/bayesian_ab_test/</link>
      <pubDate>Tue, 11 Jul 2023 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/post/bayesian_ab_test/</guid>
      <description>&lt;p&gt;&lt;em&gt;An introduction to the Bayesian approach to randomized experiments.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Randomized experiments, a.k.a. &lt;strong&gt;AB tests&lt;/strong&gt;, are now the established gold standard in the industry to estimate causal effects. Randomly assigning the treatment (new product, feature, UI, &amp;hellip;) to a subset of the population (users, patients, customers, &amp;hellip;) we ensure that, on average, the difference in outcomes (revenue, visits, clicks, &amp;hellip;) can be attributed to the treatment. Established companies like &lt;a href=&#34;https://partner.booking.com/en-gb/click-magazine/industry-perspectives/role-experimentation-bookingcom&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Booking.com&lt;/a&gt; report constantly running thousands of AB tests at the same time. And newer growing companies like &lt;a href=&#34;https://blog.duolingo.com/improving-duolingo-one-experiment-at-a-time/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Duolingo&lt;/a&gt; attribute a large chunk of their success to their culture of experimentation at scale.&lt;/p&gt;
&lt;p&gt;With so many experiments, one question comes natural: in one specific experiment, can you leverage information from previous tests? How? In this post, I will try to answer these questions by introducing the &lt;strong&gt;Bayesian approach to AB testing&lt;/strong&gt;. The Bayesian framework is well suited for this type of task because it naturally allows for the updating of existing knowledge (the prior) using new data. However, the method is particularly sensitive to functional form assumptions and apparently innocuous model choices can translate in sensible differences in the estimates, especially when the data is very skewed.&lt;/p&gt;
&lt;h2 id=&#34;search-and-infinite-scrolling&#34;&gt;Search and Infinite Scrolling&lt;/h2&gt;
&lt;p&gt;For the rest of the article, we are going to use a toy example, loosely inspired by &lt;a href=&#34;https://www.aeaweb.org/articles?id=10.1257/pandp.20191003&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Azavedo et al. (2019)&lt;/a&gt;: a &lt;strong&gt;search engine&lt;/strong&gt; that wants to increase its &lt;strong&gt;ad revenue&lt;/strong&gt;, without sacrificing search quality. We are a company with an established experimentation culture and we continuously test new ideas on how to rank results, how to select the most relevant ads for consumers, and the user interface (UI) of the results page. Suppose that, in this specific case, we came up with a new brilliant idea: &lt;a href=&#34;https://blog.google/products/search/continuous-scrolling-mobile/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;infinite scrolling&lt;/a&gt;! Instead of having a discrete sequence of pages, we allow users to keep scrolling down if they want to see more results.&lt;/p&gt;
&lt;img src=&#34;fig/phones.jpg&#34; width=&#34;300px&#34;/&gt;
&lt;p&gt;To understand whether infinite scrolling works, we ran an &lt;strong&gt;AB test&lt;/strong&gt;: we randomize users into a treatment and a control group. We implement infinite scrolling only for users in the treatment group. I import the data generating process &lt;code&gt;dgp_infinite_scroll()&lt;/code&gt; from &lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/src/dgp.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;src.dgp&lt;/code&gt;&lt;/a&gt;. With respect to previous articles, I generated a new DGP parent class that handles randomization and data generation, while its children classes contain the specific use-cases. I also import some plotting functions and libraries from &lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/src/utils.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;src.utils&lt;/code&gt;&lt;/a&gt;.  To include not only code but also data and tables, I use &lt;a href=&#34;https://deepnote.com&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Deepnote&lt;/a&gt;, a Jupyter-like web-based collaborative notebook environment.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;%matplotlib inline
%config InlineBackend.figure_format = &#39;retina&#39;

from src.utils import *
from src.dgp import DGP, dgp_infinite_scroll
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;dgp = dgp_infinite_scroll(n=10_000)
df = dgp.generate_data(true_effect=0.14)
df.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;past_revenue&lt;/th&gt;
      &lt;th&gt;infinite_scroll&lt;/th&gt;
      &lt;th&gt;ad_revenue&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;3.76&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;3.70&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;2.40&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1.71&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;2.98&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;4.85&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;4.24&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;4.57&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;3.87&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;3.69&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;We have information on $10.000$ website visitors for which we observe the monthly &lt;code&gt;ad_revenue&lt;/code&gt; they generated, whether they were assigned to the treatment group and were using the &lt;code&gt;infinite_scroll&lt;/code&gt;, and also the average monthly &lt;code&gt;past_revenue&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;The random treatment assignment makes the &lt;strong&gt;difference-in-means&lt;/strong&gt; estimator &lt;a href=&#34;https://en.wikipedia.org/wiki/Bias_of_an_estimator&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;unbiased&lt;/strong&gt;&lt;/a&gt;: we expect the treatment and control group to be comparable on average, so we can causal attribute the average observed difference in outcomes to the treatment effect. We estimate the treatment effect by linear regression. We can interpret the coefficient of &lt;code&gt;infinite_scroll&lt;/code&gt; as the estimated treatment effect.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;smf.ols(&#39;ad_revenue ~ infinite_scroll&#39;, df).fit().summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
         &lt;td&gt;&lt;/td&gt;            &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt;       &lt;td&gt;    1.9865&lt;/td&gt; &lt;td&gt;    0.020&lt;/td&gt; &lt;td&gt;  101.320&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    1.948&lt;/td&gt; &lt;td&gt;    2.025&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;infinite_scroll&lt;/th&gt; &lt;td&gt;    0.1524&lt;/td&gt; &lt;td&gt;    0.028&lt;/td&gt; &lt;td&gt;    5.461&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    0.098&lt;/td&gt; &lt;td&gt;    0.207&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;It seems that the &lt;code&gt;infinite_scroll&lt;/code&gt; was indeed a good idea and it increase the average monthly revenue by $0.1524$$. Moreover, the effect is significantly different from zero at the 1% confidence level.&lt;/p&gt;
&lt;p&gt;We could further improve the precision of the estimator by controlling for &lt;code&gt;past_revenue&lt;/code&gt; in the regression. We do not expect a sensible change in the estimated coefficient, but the precision should improve (if you want to know more on out control variables, check my other articles on &lt;a href=&#34;https://towardsdatascience.com/understanding-cuped-a822523641af&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CUPED&lt;/a&gt; and &lt;a href=&#34;https://towardsdatascience.com/controls-b63dc69e3d8c&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;DAGs&lt;/a&gt;).&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;reg = smf.ols(&#39;ad_revenue ~ infinite_scroll + past_revenue&#39;, df).fit()
reg.summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
         &lt;td&gt;&lt;/td&gt;            &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt;       &lt;td&gt;    0.0181&lt;/td&gt; &lt;td&gt;    0.024&lt;/td&gt; &lt;td&gt;    0.741&lt;/td&gt; &lt;td&gt; 0.459&lt;/td&gt; &lt;td&gt;   -0.030&lt;/td&gt; &lt;td&gt;    0.066&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;infinite_scroll&lt;/th&gt; &lt;td&gt;    0.1571&lt;/td&gt; &lt;td&gt;    0.020&lt;/td&gt; &lt;td&gt;    7.910&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    0.118&lt;/td&gt; &lt;td&gt;    0.196&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;past_revenue&lt;/th&gt;    &lt;td&gt;    0.9922&lt;/td&gt; &lt;td&gt;    0.010&lt;/td&gt; &lt;td&gt;   98.655&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    0.972&lt;/td&gt; &lt;td&gt;    1.012&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;Indeed, &lt;code&gt;past_revenue&lt;/code&gt; is highly predictive of current &lt;code&gt;ad_revenue&lt;/code&gt; and the precision of the estimated coefficient for &lt;code&gt;infinite_scroll&lt;/code&gt; decreases by one-third.&lt;/p&gt;
&lt;p&gt;So far, everything has been very standard. However, as we said at the beginning, suppose this is not the only experiment we ran trying to improve our browser (and ultimately ad revenue). The infinite scroll is just one idea among thousands of others that we have tested in the past. Is there a way to efficiently use this additional &lt;strong&gt;information&lt;/strong&gt;?&lt;/p&gt;
&lt;h2 id=&#34;bayesian-statistics&#34;&gt;Bayesian Statistics&lt;/h2&gt;
&lt;p&gt;One of the main advantages of Bayesian statistics over the frequentist approach is that it easily allows to incorporate additional information into a model. The idea directly follows from the main results behind all Bayesian statistics: &lt;a href=&#34;https://en.wikipedia.org/wiki/Bayes%27_theorem&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Bayes Theorem&lt;/strong&gt;&lt;/a&gt;. Bayes theorem, allows you to do inference on a model by &lt;strong&gt;inverting the inference problem&lt;/strong&gt;: from the probability of the model given the data, to the probability of the data given the model, a much easier object to deal with.&lt;/p&gt;
&lt;p&gt;$$
\underbrace{ \Pr \big( \text{model} \ \big| \ \text{data} \big) }&lt;em&gt;{\text{posterior}} = \underbrace{ \Pr(\text{model}) }&lt;/em&gt;{\text{prior}} \ \underbrace{ \frac{ \Pr \big( \text{data} \ \big| \ \text{model} \big) }{ \Pr(\text{data}) } }_{\text{likelihood}}
$$&lt;/p&gt;
&lt;p&gt;We can split the right-hand side of Bayes Theorem (or Rule) into two components: the &lt;strong&gt;prior&lt;/strong&gt; and the &lt;strong&gt;likelihood&lt;/strong&gt;. The likelihood is the information about the model that comes from the data, the prior instead is any additional information about the model.&lt;/p&gt;
&lt;p&gt;First of all, let&amp;rsquo;s map Bayes theorem into our context. What is the data, what is the model and what is our object of interest?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the &lt;strong&gt;data&lt;/strong&gt; which consists in our outcome variable &lt;code&gt;ad_revenue&lt;/code&gt;, $y$, the treatment &lt;code&gt;infinite_scroll&lt;/code&gt;, $D$ and the other variables, &lt;code&gt;past_revenue&lt;/code&gt; and a constant, which we jointly denote as $X$&lt;/li&gt;
&lt;li&gt;the &lt;strong&gt;model&lt;/strong&gt; is the distribution of &lt;code&gt;ad_revenue&lt;/code&gt;, given &lt;code&gt;past_revenue&lt;/code&gt; and the &lt;code&gt;infinite_scroll&lt;/code&gt; feature, $y | D, X$&lt;/li&gt;
&lt;li&gt;our &lt;strong&gt;object of interest&lt;/strong&gt; is the posterior $\Pr \big( \text{model} \ \big| \ \text{data} \big)$, in particular the relationship between &lt;code&gt;ad_revenue&lt;/code&gt; and &lt;code&gt;infinite_scroll&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;X = sm.add_constant(df[[&#39;past_revenue&#39;]].values)
D = df[&#39;infinite_scroll&#39;].values
y = df[&#39;ad_revenue&#39;].values
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;How do we use prior information in the context of AB testing, potentially including additional covariates?&lt;/p&gt;
&lt;h3 id=&#34;bayesian-regression&#34;&gt;Bayesian Regression&lt;/h3&gt;
&lt;p&gt;Let&amp;rsquo;s use a linear model to make it directly comparable with the frequentist approach:&lt;/p&gt;
&lt;p&gt;$$
y_i = \beta X_i + \tau D_i + \varepsilon_i \qquad \text{where} \quad \varepsilon_i \sim N \big( 0, \sigma^2 \big)
$$&lt;/p&gt;
&lt;p&gt;This is a parametric model with &lt;strong&gt;two sets of parameters&lt;/strong&gt;: the linear coefficients $\beta$ and $\tau$, and the variance of the residuals $\sigma$. An equivalent, but more Bayesian, way to write the model is:&lt;/p&gt;
&lt;p&gt;$$
y \ | \ X, D; \beta, \tau, \sigma \sim N \Big( \beta X + \tau D \ , \sigma^2 \Big) ,
$$&lt;/p&gt;
&lt;p&gt;where the semi-column separates the data from the model parameters. Differently from the frequentist approach, in Bayesian regressions we do not rely on the &lt;a href=&#34;https://en.wikipedia.org/wiki/Central_limit_theorem&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;central limit theorem&lt;/a&gt; to approximate the conditional distribution of $y$, but we directly &lt;strong&gt;assume&lt;/strong&gt; it is normal. Is it just a formality? Not really, but a proper comparison between the frequentist and Bayesian approach is beyond the scope of this article.&lt;/p&gt;
&lt;p&gt;We are interested in doing inference on the model parameters, $\beta$, $\tau$, and $\sigma$. Another &lt;strong&gt;core difference&lt;/strong&gt; between the frequentist and the Bayesian approach is that the the first assumes that the model parameters are fixed (scalars), while the latter allows them to be stochastic (random variables).&lt;/p&gt;
&lt;p&gt;This assumption has a very practical &lt;strong&gt;implication&lt;/strong&gt;: you can easily incorporate previous information about the model parameters in the form of &lt;strong&gt;prior&lt;/strong&gt; distributions. As the name says, priors contain information that was available even &lt;em&gt;before&lt;/em&gt; looking at the data. This leads to one of the most relevant questions in Bayesian statistics: &lt;strong&gt;how do you chose a prior&lt;/strong&gt;?&lt;/p&gt;
&lt;h2 id=&#34;priors&#34;&gt;Priors&lt;/h2&gt;
&lt;p&gt;When choosing a prior, one analytically appealing restriction is to have a prior distribution such that the posterior belongs to the same family. These priors are called &lt;strong&gt;conjugate priors&lt;/strong&gt;. For example, before seeing the data, I assume my treatment effect is normally distributed and I would like it to be normally distributed also after incorporating the information contained in the data.&lt;/p&gt;
&lt;p&gt;In the case of Bayesian linear regression, the conjugate priors for $\beta$ and $\sigma$ are normally and inverse-gamma distributed. Let&amp;rsquo;s start a bit blindly, by taking a standard normal and inverse gamma distribution as prior.&lt;/p&gt;
&lt;p&gt;$$
\beta_i \sim N(\boldsymbol 0, \boldsymbol 1) \
\tau_i \sim N(0,1) \
\sigma^2 \sim \Gamma^{-1} (1, 1)
$$&lt;/p&gt;
&lt;p&gt;We use the package &lt;a href=&#34;https://www.pymc.io/projects/docs/en/stable/learn.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PyMC&lt;/a&gt; to do inference. First we need to specify the model: what are the distributions of the different parameters (priors) and what is the likelihood of the data.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import pymc as pm
with pm.Model() as baseline_model:

    # Priors
    beta = pm.MvNormal(&#39;beta&#39;, mu=np.ones(np.shape(X)[1]), cov=np.eye(np.shape(X)[1]))
    tau = pm.Normal(&#39;tau&#39;, mu=0, sigma=1)
    sigma = pm.InverseGamma(&#39;sigma&#39;, mu=1, sigma=1, initval=1)
    
    # Likelihood 
    Ylikelihood = pm.Normal(&#39;y&#39;, mu=(X@beta + D@tau).flatten(), sigma=sigma, observed=y)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;PyMC has an extremely nice function that allows us to visualize the model as a graph, &lt;code&gt;model_to_graphviz&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;pm.model_to_graphviz(baseline_model)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/bayesian_ab_test_25_0.svg&#34; alt=&#34;svg&#34;&gt;&lt;/p&gt;
&lt;p&gt;From the graphical representation, we can see the various model components, their distributions, and how they interact with each other.&lt;/p&gt;
&lt;p&gt;We are now ready to &lt;strong&gt;compute&lt;/strong&gt; the model posterior. How does it work? In short, we sample realizations of model parameters, we compute the likelihood of the data given those values and the compute the corresponding posterior.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;idata = pm.sample(model=baseline_model, draws=1000)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Auto-assigning NUTS sampler...
Initializing NUTS using jitter+adapt_diag...
Multiprocess sampling (4 chains in 4 jobs)
NUTS: [beta, tau, sigma]
&lt;/code&gt;&lt;/pre&gt;
&lt;style&gt;
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
&lt;/style&gt;
&lt;div&gt;
  &lt;progress value=&#39;8000&#39; class=&#39;&#39; max=&#39;8000&#39; style=&#39;width:300px; height:20px; vertical-align: middle;&#39;&gt;&lt;/progress&gt;
  100.00% [8000/8000 00:04&amp;lt;00:00 Sampling 4 chains, 0 divergences]
&lt;/div&gt;
&lt;pre&gt;&lt;code&gt;Sampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 4_000 draws total) took 19 seconds.
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The fact that Bayesian inference requires sampling, has been historically one of the main bottlenecks of Bayesian statistics, since it makes it sensibly slower than the frequentist approach. However, this is less and less of a problem with the increased computational power of model computers.&lt;/p&gt;
&lt;p&gt;We are now ready to print out results. First, with the &lt;code&gt;summary()&lt;/code&gt; method, we can print a model summary very similar to those produced by the &lt;code&gt;statsmodels&lt;/code&gt; package.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;pm.summary(idata, hdi_prob=0.95).round(4)
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;mean&lt;/th&gt;
      &lt;th&gt;sd&lt;/th&gt;
      &lt;th&gt;hdi_2.5%&lt;/th&gt;
      &lt;th&gt;hdi_97.5%&lt;/th&gt;
      &lt;th&gt;mcse_mean&lt;/th&gt;
      &lt;th&gt;mcse_sd&lt;/th&gt;
      &lt;th&gt;ess_bulk&lt;/th&gt;
      &lt;th&gt;ess_tail&lt;/th&gt;
      &lt;th&gt;r_hat&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;beta[0]&lt;/th&gt;
      &lt;td&gt;0.019&lt;/td&gt;
      &lt;td&gt;0.025&lt;/td&gt;
      &lt;td&gt;-0.031&lt;/td&gt;
      &lt;td&gt;0.068&lt;/td&gt;
      &lt;td&gt;0.001&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;1943.0&lt;/td&gt;
      &lt;td&gt;1866.0&lt;/td&gt;
      &lt;td&gt;1.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;beta[1]&lt;/th&gt;
      &lt;td&gt;0.992&lt;/td&gt;
      &lt;td&gt;0.010&lt;/td&gt;
      &lt;td&gt;0.970&lt;/td&gt;
      &lt;td&gt;1.011&lt;/td&gt;
      &lt;td&gt;0.000&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;2239.0&lt;/td&gt;
      &lt;td&gt;1721.0&lt;/td&gt;
      &lt;td&gt;1.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;tau&lt;/th&gt;
      &lt;td&gt;0.157&lt;/td&gt;
      &lt;td&gt;0.021&lt;/td&gt;
      &lt;td&gt;0.117&lt;/td&gt;
      &lt;td&gt;0.197&lt;/td&gt;
      &lt;td&gt;0.000&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;2770.0&lt;/td&gt;
      &lt;td&gt;2248.0&lt;/td&gt;
      &lt;td&gt;1.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;sigma&lt;/th&gt;
      &lt;td&gt;0.993&lt;/td&gt;
      &lt;td&gt;0.007&lt;/td&gt;
      &lt;td&gt;0.980&lt;/td&gt;
      &lt;td&gt;1.007&lt;/td&gt;
      &lt;td&gt;0.000&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;3473.0&lt;/td&gt;
      &lt;td&gt;2525.0&lt;/td&gt;
      &lt;td&gt;1.0&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;The estimated parameters are extremely close to the ones we got with the frequentist approach, with an estimated effect of the &lt;code&gt;infinite_scroll&lt;/code&gt; equal to $0.157$.&lt;/p&gt;
&lt;p&gt;If sampling had the disadvantage of being slow, it has the advantage of being very &lt;strong&gt;transparent&lt;/strong&gt;. We can directly plot the distribution of the posterior. Let&amp;rsquo;s do it for the treatment effect $\tau$. The PyMC function &lt;code&gt;plot_posterior&lt;/code&gt; plots the distribution of the posterior, with a black bar for the Bayesian equivalent of a 95% confidence interval.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;pm.plot_posterior(idata, kind=&amp;quot;hist&amp;quot;, var_names=(&#39;tau&#39;), hdi_prob=0.95, figsize=(6, 3), bins=30); 
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/bayesian_ab_test_31_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;As expected, since we chose conjugate priors, the posterior distribution looks gaussian.&lt;/p&gt;
&lt;p&gt;So far we have chosen the prior without much guidance. However, suppose we had access to past experiments. How do we incorporate this specific information?&lt;/p&gt;
&lt;h2 id=&#34;past-experiments&#34;&gt;Past Experiments&lt;/h2&gt;
&lt;p&gt;Suppose the idea of the infinite scroll, was just one among a ton of other ones that we tried and tested in the past. For each idea we have the data for the corresponding experiment, with the corresponding estimated coefficient. Suppose we had a thousand of them.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;past_experiments = [dgp.generate_data(seed_data=i) for i in range(1000)]
taus = [smf.ols(&#39;ad_revenue ~ infinite_scroll + past_revenue&#39;, pe).fit().params.values for pe in past_experiments]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;How do we use this additional information?&lt;/p&gt;
&lt;h3 id=&#34;normal-prior&#34;&gt;Normal Prior&lt;/h3&gt;
&lt;p&gt;The first idea could be to calibrate our prior to reflect the data distribution in the past. Keeping the normality assumption, we use the estimated average and standard deviations of the estimates from past experiments.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;taus_mean = np.mean(taus, axis=0)[1]
taus_mean
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;0.0009094486420266667
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;On average, had practically no effect on &lt;code&gt;ad_revenue&lt;/code&gt;, with a average effect of $0.0009$.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;taus_std = np.sqrt(np.cov(taus, rowvar=0)[1,1])
taus_std
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;0.029014447772168384
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;However, there was sensible variation across experiments, with a standard deviation of $0.029$.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s now estimate the&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;with pm.Model() as model_normal_prior:
    k = np.shape(X)[1]
    beta = pm.MvNormal(&#39;beta&#39;, mu=np.ones(k), cov=np.eye(k))
    tau = pm.Normal(&#39;tau&#39;, mu=taus_mean, sigma=taus_std)
    sigma = pm.InverseGamma(&#39;sigma&#39;, mu=1, sigma=1, initval=1)
    Ylikelihood = pm.Normal(&#39;y&#39;, mu=(X@beta + D@tau).flatten(), sigma=sigma, observed=y)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s sample from the model.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;idata_normal_prior = pm.sample(model=model_normal_prior, draws=1000)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Auto-assigning NUTS sampler...
Initializing NUTS using jitter+adapt_diag...
Multiprocess sampling (4 chains in 4 jobs)
NUTS: [beta, tau, sigma]
&lt;/code&gt;&lt;/pre&gt;
&lt;style&gt;
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
&lt;/style&gt;
&lt;div&gt;
  &lt;progress value=&#39;8000&#39; class=&#39;&#39; max=&#39;8000&#39; style=&#39;width:300px; height:20px; vertical-align: middle;&#39;&gt;&lt;/progress&gt;
  100.00% [8000/8000 00:04&amp;lt;00:00 Sampling 4 chains, 0 divergences]
&lt;/div&gt;
&lt;pre&gt;&lt;code&gt;Sampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 4_000 draws total) took 19 seconds.
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And plot the sample posterior distribution of the treatment effect parameter $\tau$.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;pm.plot_posterior(idata_normal_prior, kind=&amp;quot;hist&amp;quot;, var_names=(&#39;tau&#39;), hdi_prob=0.95, figsize=(6, 3), bins=30); 
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/bayesian_ab_test_47_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;The estimated coefficient is sensibly smaller: $0.08$ instead of the previous estimate of $0.12$. Why is it the case?&lt;/p&gt;
&lt;p&gt;The fact is that the previous coefficient of $0.12$ is extremely unlikey, given our prior. We can compute the probability of getting the same or a more extreme value, given the prior.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;1 - sp.stats.norm(taus_mean, taus_std).cdf(0.12)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;2.025724712373389e-05
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The probability of such value is almost zero. Therefore, the estimated coefficient has moved towards the prior mean of $0.0009$.&lt;/p&gt;
&lt;h3 id=&#34;student-t-prior&#34;&gt;Student t Prior&lt;/h3&gt;
&lt;p&gt;So far, we have assumed a normal distribution for all linear coefficients. Is it appropriate? Let&amp;rsquo;s check it visually (check &lt;a href=&#34;https://medium.com/towards-data-science/how-to-compare-two-or-more-distributions-9b06ee4d30bf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt; for other methods on how to compare distributions).&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sns.histplot([tau[0] for tau in taus]).set(title=r&#39;Distribution of $\hat{\beta}_0$ in past experiments&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/bayesian_ab_test_53_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;The distribution seems pretty normal. What the treatment effect paramenter $\tau$?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;fig, ax = plt.subplots()
sns.histplot([tau[1] for tau in taus], label=&#39;past experiments&#39;);
ax.axvline(reg.params[&#39;infinite_scroll&#39;], lw=2, c=&#39;C3&#39;, ls=&#39;--&#39;, label=&#39;current experiment&#39;)
plt.legend();
plt.title(r&#39;Distribution of $\hat{\tau}$ in past experiments&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/bayesian_ab_test_55_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;The distribution very &lt;strong&gt;heavy tailed&lt;/strong&gt;! While at the center it looks like a normal distributions, the tails are much &amp;ldquo;fatter&amp;rdquo; and we have a couple of very extreme values. excluding the case of measurement error, this is a setting that happens often in the industry, where most ideas have extremely small or null effects and very rarely an idea is actually a breakthrough.&lt;/p&gt;
&lt;p&gt;One way to model this distribution is a &lt;a href=&#34;&#34;&gt;student-t distribution&lt;/a&gt;. In particular, we use a t-student with mean $0.0009$, variance $0.003$ and $1.3$ degrees of freedom to match the moments of the empirical distributions of past estimates.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;with pm.Model() as model_studentt_prior:

    # Priors
    k = np.shape(X)[1]
    beta = pm.MvNormal(&#39;beta&#39;, mu=np.ones(k), cov=np.eye(k))
    tau = pm.StudentT(&#39;tau&#39;, mu=taus_mean, sigma=0.003, nu=1.3)
    sigma = pm.InverseGamma(&#39;sigma&#39;, mu=1, sigma=1, initval=1)
    
    # Likelihood 
    Ylikelihood = pm.Normal(&#39;y&#39;, mu=(X@beta + D@tau).flatten(), sigma=sigma, observed=y)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s sample from the model.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;idata_studentt_priors = pm.sample(model=model_studentt_prior, draws=1000)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Auto-assigning NUTS sampler...
Initializing NUTS using jitter+adapt_diag...
Multiprocess sampling (4 chains in 4 jobs)
NUTS: [beta, tau, sigma]
&lt;/code&gt;&lt;/pre&gt;
&lt;style&gt;
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
&lt;/style&gt;
&lt;div&gt;
  &lt;progress value=&#39;8000&#39; class=&#39;&#39; max=&#39;8000&#39; style=&#39;width:300px; height:20px; vertical-align: middle;&#39;&gt;&lt;/progress&gt;
  100.00% [8000/8000 00:04&amp;lt;00:00 Sampling 4 chains, 0 divergences]
&lt;/div&gt;
&lt;pre&gt;&lt;code&gt;Sampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 4_000 draws total) took 18 seconds.
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And plot the sample posterior distribution of the treatment effect parameter $\tau$.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;pm.plot_posterior(idata_studentt_priors, kind=&amp;quot;hist&amp;quot;, var_names=(&#39;tau&#39;), hdi_prob=0.95, figsize=(6, 3), bins=30); 
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/bayesian_ab_test_61_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;The estimated coefficient is now again similar to the one we got with the standard normal prior, $0.11$. However, the estimate is more precise since the confidence interval has shrunk from $[0.077, 0.016]$ to $[0.065, 0.015]$.&lt;/p&gt;
&lt;p&gt;What has happened?&lt;/p&gt;
&lt;h3 id=&#34;shrinking&#34;&gt;Shrinking&lt;/h3&gt;
&lt;p&gt;The answer lies in the shape of the different &lt;strong&gt;prior distributions&lt;/strong&gt; that we have used:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;standard normal, $N(0,1)$&lt;/li&gt;
&lt;li&gt;normal with matched moments, $N(0, 0.03)$&lt;/li&gt;
&lt;li&gt;t-student with matched moments, $t_{1.3}$(0, 0.003)$&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;t_hats = np.linspace(-0.3, 0.3, 1_000)
distributions = {
    &#39;N(0,1)&#39;: sp.stats.norm(0, 1).pdf(t_hats),
    &#39;N(0, 0.03)&#39;: sp.stats.norm(0, 0.03).pdf(t_hats),
    &#39;$t_{1.3}$(0, 0.003)&#39;: sp.stats.t(df=1.3).pdf(t_hats / 0.003)*300,
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s plot all of them together.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;for i, (label, y) in enumerate(distributions.items()):
    sns.lineplot(x=t_hats, y=y, color=f&#39;C{i}&#39;, label=label);
plt.legend(); 
plt.title(&#39;Prior Distributions&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/bayesian_ab_test_67_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;As we can see, all distributions are centered on zero, but they have very different shapes. The standard normal distribution is essentially flat over the $[-0.15, 0.15]$ interval. Every value has basically the same probability. The last two instead, even though they have the same mean and variance, have very different shapes.&lt;/p&gt;
&lt;p&gt;How does it translate into our estimation? We can plot the implied posterior for different estimates, for each prior distribution.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def compute_posterior(b, prior):
    likelihood = sp.stats.norm(b, taus_std).pdf(t_hats)
    return np.average(t_hats, weights=prior*likelihood)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;fig, ax = plt.subplots(figsize=(7,6))
ax.axvline(0, lw=1.5, c=&#39;k&#39;);
ax.axhline(0, lw=1.5, c=&#39;k&#39;);
ax.axvline(reg.params[&#39;infinite_scroll&#39;], lw=2, ls=&#39;--&#39;, c=&#39;darkgray&#39;);
for i, (label, y) in enumerate(distributions.items()):
    sns.lineplot(x=t_hats, y=[compute_posterior(t, y) for t in t_hats] , color=f&#39;C{i}&#39;, label=label);
ax.set_xlim(-0.17, 0.17);
ax.set_ylim(-0.17, 0.17);
plt.legend(); 
ax.set_xlabel(&#39;Experiment Estimate&#39;);
ax.set_ylabel(&#39;Posterior&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/bayesian_ab_test_70_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;As we can see, the different priors transform the experimental estimates in very different ways. The standard normal prior essentially has no effect for estimates in the $[-0.15, 0.15]$ interval. The normal prior with matched moments instead shrinks each estimate by approximately 2/3. The effect of the t-student prior is instead &lt;strong&gt;non-linear&lt;/strong&gt;: it shrinks small estimates towards zero, while it keeps large estimates as they are.&lt;/p&gt;
&lt;p&gt;My &lt;strong&gt;intuition&lt;/strong&gt; is the following. A prior distribution very skewed or with &amp;ldquo;fat tails&amp;rdquo; means that large values are rare but not impossible. In practice, it means accepting that breakthrough improvements are possible. On the other hand, for the same variance, the distribution is more concentrated around zero than a standard normal so that small values are shrunk even more.&lt;/p&gt;
&lt;img src=&#34;fig/scroll.jpg&#34; width=&#34;300px&#34;/&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;In this article we have seen how to extend the analysis of AB test to incorporate &lt;strong&gt;information from past experiments&lt;/strong&gt;. In particular, we have seen the importance of choosing a prior. Selecting the distribution function is just as important as tuning its parameters. The shape of the prior distribution can drastically affect our inference, especially in a world with skewed distributions.&lt;/p&gt;
&lt;p&gt;Despite the length of the article, this was just a glimpse in the world of &lt;strong&gt;AB testing and Bayesian statistics&lt;/strong&gt;. While being computationally more intensive and requiring additional assumptions, the Bayesian approach is often more natural, powerful and flexible than the frequentist one. Knowing pros and cons of both approaches is crucial to get the best of both worlds, picking the approach that work best or combining them efficiently.&lt;/p&gt;
&lt;h3 id=&#34;references&#34;&gt;References&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;E. Azevedo, A. Deng, J. Olea, G. Weyl, &lt;a href=&#34;https://www.aeaweb.org/articles?id=10.1257/pandp.20191003&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Empirical Bayes Estimation of Treatment Effects with Many A/B Tests: An Overview&lt;/a&gt; (2019). &lt;em&gt;AEA Papers and Proceedings&lt;/em&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;A. Deng, &lt;a href=&#34;https://dl.acm.org/doi/abs/10.1145/2740908.2742563&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Objective Bayesian Two Sample Hypothesis Testing for Online Controlled Experiments&lt;/a&gt; (2018), &lt;em&gt;WWW15&lt;/em&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;related-articles&#34;&gt;Related Articles&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://medium.com/towards-data-science/the-bayesian-bootstrap-6ca4a1d45148&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The Bayesian Bootstrap&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://towardsdatascience.com/understanding-cuped-a822523641af&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Understanding CUPED&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://towardsdatascience.com/controls-b63dc69e3d8c&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;DAGs and Control Variables&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;code&#34;&gt;Code&lt;/h3&gt;
&lt;p&gt;You can find the original Jupyter Notebook here:&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/bayes_ab.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/bayes_ab.ipynb&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Outliers, Leverage, and Influential Observations</title>
      <link>https://matteocourthoud.github.io/post/outliers_leverage/</link>
      <pubDate>Tue, 11 Jul 2023 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/post/outliers_leverage/</guid>
      <description>&lt;p&gt;&lt;em&gt;What makes an observation &amp;ldquo;unusual&amp;rdquo;?&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;In data science, one common task is outlier detection. This is a broad term that is often misused or misunderstood. More broadly, we are often interested in understanding any observation is &lt;strong&gt;&amp;ldquo;unusual&amp;rdquo;&lt;/strong&gt;. First of all, what does it mean to be unusual? In this article we are going to inspect three different ways in which an observation can be unusual: it can be unusual characteristics, it might not fit the model or it might be particularly influential in fitting the model. We will see that in linear regression the latter characteristics is a byproduct of the first two.&lt;/p&gt;
&lt;p&gt;Importantly, being unusual is &lt;strong&gt;not necessarily bad&lt;/strong&gt;. Observations that have different characteristics from all others usually carry more information. We also expect some observations not to fit the model well, otherwise the model is likely biased (overfitting). However, &amp;ldquo;unusual&amp;rdquo; observations are also more likely to be generated by a different process. Extreme cases include measurement error or fraud, but differences can be more nuanced. Domain knowledge is always kind and dropping observations only for for statistical reasons is never wise.&lt;/p&gt;
&lt;p&gt;That said, let&amp;rsquo;s have a look at some different ways in which observations can be &amp;ldquo;unusual&amp;rdquo;.&lt;/p&gt;
&lt;h1 id=&#34;example&#34;&gt;Example&lt;/h1&gt;
&lt;p&gt;Suppose we are an &lt;strong&gt;peer-to-peer online platform&lt;/strong&gt; and we are interested in understanding if there is anything suspicious going on with our business. We have information about how much time our customers spend on the platform and the total value of their transactions.&lt;/p&gt;
&lt;p&gt;First, let&amp;rsquo;s have a look at the data. I import the data generating process &lt;code&gt;dgp_p2p()&lt;/code&gt; from &lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/src/dgp.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;src.dgp&lt;/code&gt;&lt;/a&gt; and some plotting functions and libraries from &lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/src/utils.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;src.utils&lt;/code&gt;&lt;/a&gt;. I include code snippets from &lt;a href=&#34;https://deepnote.com&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Deepnote&lt;/a&gt;, a Jupyter-like web-based collaborative notebook environment. For our purpose, Deepnote is very handy because it allows me not only to include code but also output, like data and tables.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;%matplotlib inline
%config InlineBackend.figure_format = &#39;retina&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from src.utils import *
from src.dgp import dgp_p2p
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df = dgp_p2p().generate_data()
df.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;hours&lt;/th&gt;
      &lt;th&gt;transactions&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;2.6&lt;/td&gt;
      &lt;td&gt;8.30&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;2.0&lt;/td&gt;
      &lt;td&gt;8.00&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;7.0&lt;/td&gt;
      &lt;td&gt;21.00&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;6.7&lt;/td&gt;
      &lt;td&gt;18.00&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;1.2&lt;/td&gt;
      &lt;td&gt;3.82&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;We have information on 50 clients for which we observe &lt;code&gt;hours&lt;/code&gt; spent on the website and total &lt;code&gt;transactions&lt;/code&gt; amount. Since we only have two variables we can easily inspect them using a scatterplot.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sns.scatterplot(data=df, x=&#39;hours&#39;, y=&#39;transactions&#39;).set(title=&#39;Data Scatterplot&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/outliers_leverage_9_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;The relationship between &lt;code&gt;hours&lt;/code&gt; and &lt;code&gt;transactions&lt;/code&gt; seems to follow a clear linear relationship. If we fit a linear model, we observe a particularly tight fit.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;smf.ols(&#39;hours ~ transactions&#39;, data=df).fit().summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
        &lt;td&gt;&lt;/td&gt;          &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt;    &lt;td&gt;   -0.0975&lt;/td&gt; &lt;td&gt;    0.084&lt;/td&gt; &lt;td&gt;   -1.157&lt;/td&gt; &lt;td&gt; 0.253&lt;/td&gt; &lt;td&gt;   -0.267&lt;/td&gt; &lt;td&gt;    0.072&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;transactions&lt;/th&gt; &lt;td&gt;    0.3452&lt;/td&gt; &lt;td&gt;    0.009&lt;/td&gt; &lt;td&gt;   39.660&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    0.328&lt;/td&gt; &lt;td&gt;    0.363&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;Does any data point look suspiciously different from the others? How?&lt;/p&gt;
&lt;h2 id=&#34;leverage&#34;&gt;Leverage&lt;/h2&gt;
&lt;p&gt;The first metric that we are going to use to evaluate &amp;ldquo;unusual&amp;rdquo; observations is the &lt;strong&gt;leverage&lt;/strong&gt;, which was first introduced by &lt;a href=&#34;https://www.jstor.org/stable/1268249&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Cook (1980)&lt;/a&gt;. The objective of the leverage is to capture how much a single point is different with respect to other data points. These data points are often called &lt;strong&gt;outliers&lt;/strong&gt; and there exist a nearly amount of algorithms and rules of thumb to flag them.However the idea is the same: flagging observations that are unusual in terms of features.&lt;/p&gt;
&lt;p&gt;The leverage of an observation $i$ is defined as&lt;/p&gt;
&lt;p&gt;$$
h_{ii} := x_i&amp;rsquo; (X&amp;rsquo;X)^{-1} x_i
$$&lt;/p&gt;
&lt;p&gt;One interpretation of the leverage is as a &lt;strong&gt;measure of distance&lt;/strong&gt; where individual observations are compared against the average of all observations.&lt;/p&gt;
&lt;p&gt;Another interpretation of the leverage is as the influence of the outcome of observation $i$, $y_i$, on the corresponding fitted value $\hat{y_i}$.&lt;/p&gt;
&lt;p&gt;$$
h_{ii} = \frac{\partial \hat{y}_i}{\partial y_i}
$$&lt;/p&gt;
&lt;p&gt;Algebraically, the leverage of observation $i$ is the $i^{th}$ element of the &lt;strong&gt;design matrix&lt;/strong&gt; $X&amp;rsquo; (X&amp;rsquo;X)^{-1} X$. Among the many properties of the leverages, is the fact that they are non-negative and their values sum to 1.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s compute the leverage of the observations in our dataset. We also flag observations that have unusual leverages (which we arbitrarily define as more than two standard deviations away from the average leverage).&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;X = np.reshape(df[&#39;hours&#39;].values, (-1, 1))
Y = np.reshape(df[&#39;transactions&#39;].values, (-1, 1))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df[&#39;leverage&#39;] = np.diagonal(X @ np.linalg.inv(X.T @ X) @ X.T)
df[&#39;high_leverage&#39;] = df[&#39;leverage&#39;] &amp;gt; (np.mean(df[&#39;leverage&#39;]) + 2*np.std(df[&#39;leverage&#39;]))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s plot the distribution of leverage values in our data.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;fix, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))
sns.histplot(data=df, x=&#39;leverage&#39;, hue=&#39;high_leverage&#39;, alpha=1, bins=30, ax=ax1).set(title=&#39;Distribution of Leverages&#39;);
sns.scatterplot(data=df, x=&#39;hours&#39;, y=&#39;transactions&#39;, hue=&#39;high_leverage&#39;, ax=ax2).set(title=&#39;Data Scatterplot&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/outliers_leverage_18_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;As we can see, the distribution is skewed with two observations having a unusually high leverage. Indeed, in the scatterplot these two observations are slightly separated from the rest of the distribution.&lt;/p&gt;
&lt;p&gt;Is this bad news? It depends. Outliers are &lt;strong&gt;not a problem per se&lt;/strong&gt;. Actually, if they are genuine observations, they might carry much more information than other observations. On the other hand, they are also more likely &lt;em&gt;not&lt;/em&gt; to be genuine observations (e.g. fraud, measurement error, &amp;hellip;) or to be inherently different from the other ones (e.g. professional users vs amateurs). In any case, we might want to investigate further and use as much context-specific information as we can.&lt;/p&gt;
&lt;p&gt;Importantly, the fact that an observation has a high leverage tells us information about the features of the model but nothing about the model itself. Are these users just different observations or they also behave differently?&lt;/p&gt;
&lt;h2 id=&#34;residuals&#34;&gt;Residuals&lt;/h2&gt;
&lt;p&gt;So far we have only talked about unusual features, but what about &lt;strong&gt;unusual behavior&lt;/strong&gt;? This is what regression residuals measure.&lt;/p&gt;
&lt;p&gt;Regression residuals are the difference between the predicted outcome values and the observed outcome values. In a sense, they capture what the model cannot explain: the higher the residual of one observation the more it is unusual in the sense that the model cannot explain it.&lt;/p&gt;
&lt;p&gt;In the case of linear regression, residuals can be written as&lt;/p&gt;
&lt;p&gt;$$
\hat{e} = y - \hat{y} = y - \hat \beta X
$$&lt;/p&gt;
&lt;p&gt;In our case, since $X$ is one dimensional (&lt;code&gt;hours&lt;/code&gt;), we can easily visualize them.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;Y_hat = X @ np.linalg.inv(X.T @ X) @ X.T @ Y
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plt.scatter(X, Y, s=50, label=&#39;data&#39;)
plt.plot(X, Y_hat, c=&#39;k&#39;, lw=2, label=&#39;prediction&#39;)
plt.vlines(X, np.minimum(Y, Y_hat), np.maximum(Y, Y_hat), color=&#39;r&#39;, lw=3, label=&amp;quot;residuals&amp;quot;);
plt.legend()
plt.title(f&amp;quot;Regression prediction and residuals&amp;quot;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/outliers_leverage_23_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Do some observations have unusually high residuals? Let&amp;rsquo;s plot their distribution.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df[&#39;residual&#39;] = np.abs(Y - X @ np.linalg.inv(X.T @ X) @ X.T @ Y)
df[&#39;high_residual&#39;] = df[&#39;residual&#39;] &amp;gt; (np.mean(df[&#39;residual&#39;]) + 2*np.std(df[&#39;residual&#39;]))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;fix, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))
sns.histplot(data=df, x=&#39;residual&#39;, hue=&#39;high_residual&#39;, alpha=1, bins=30, ax=ax1).set(title=&#39;Distribution of Residuals&#39;);
sns.scatterplot(data=df, x=&#39;hours&#39;, y=&#39;transactions&#39;, hue=&#39;high_residual&#39;, ax=ax2).set(title=&#39;Data Scatterplot&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/outliers_leverage_26_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Two observations have particularly high residuals. This means that for these observations, the model is not good at predicting the observed outcomes.&lt;/p&gt;
&lt;p&gt;Is this bad news? Not necessarily. A model that fits the observations too well is likely to be &lt;a href=&#34;https://en.wikipedia.org/wiki/Bias_%28statistics%29&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;biased&lt;/strong&gt;&lt;/a&gt;. However, it might still be important to understand why some users have a different relationship between hours spent and total transactions. As usual, information on the specific context is key.&lt;/p&gt;
&lt;p&gt;So far we have looked at observations with &amp;ldquo;unusual&amp;rdquo; characteristics and &amp;ldquo;unusual&amp;rdquo; model fit, but what is the observation itself is distorting the model? How much our model is driven by a handful of observations?&lt;/p&gt;
&lt;h2 id=&#34;influence&#34;&gt;Influence&lt;/h2&gt;
&lt;p&gt;The concept of &lt;strong&gt;influence and influence functions&lt;/strong&gt; was developed precisely to answer this question: what are influential observations? This questions were very popular in the 80&amp;rsquo;s and lost appeal for a long time until the recent need of explaining complex machine learning and AI models.&lt;/p&gt;
&lt;p&gt;The general idea is to define an observation as &lt;strong&gt;influential&lt;/strong&gt; if removing it significantly changes the estimated model. In linear regression, we define the influence of observation $i$ as:&lt;/p&gt;
&lt;p&gt;$$
\hat{\beta} - \hat{\beta}_{-i} = (X&amp;rsquo;X)^{-1} x_i e_i
$$&lt;/p&gt;
&lt;p&gt;Where $\hat{\beta}_{-i}$ is the OLS coefficient estimated omitting observation $i$.&lt;/p&gt;
&lt;p&gt;As you can see, there is a tight connection to both leverage $h_{ii}$ and residuals $e_i$: influence is almost the product of the two. Indeed, in linear regression, observations with high leverage are observations that are both outliers and have high residuals. None of the two conditions alone is sufficient for an observation to have an influence on the model.&lt;/p&gt;
&lt;p&gt;We can see it best in the data.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df[&#39;influence&#39;] = (np.linalg.inv(X.T @ X) @ X.T).T * np.abs(Y - Y_hat)
df[&#39;high_influence&#39;] = df[&#39;influence&#39;] &amp;gt; (np.mean(df[&#39;influence&#39;]) + 2*np.std(df[&#39;influence&#39;]))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;fix, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))
sns.histplot(data=df, x=&#39;influence&#39;, hue=&#39;high_influence&#39;, alpha=1, bins=30, ax=ax1).set(title=&#39;Distribution of Influences&#39;);
sns.scatterplot(data=df, x=&#39;hours&#39;, y=&#39;transactions&#39;, hue=&#39;high_influence&#39;, ax=ax2).set(title=&#39;Data Scatterplot&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/outliers_leverage_31_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;In our dataset, there is only one observation with high influence, and it is disproportionally larger than the influence of all other observations.&lt;/p&gt;
&lt;p&gt;We can now plot all &amp;ldquo;unusual&amp;rdquo; points in the same plot. I also report residuals and leverage of each point in a separate plot.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def plot_leverage_residuals(df):

    # Hue
    df[&#39;type&#39;] = &#39;Normal&#39;
    df.loc[df[&#39;high_residual&#39;], &#39;type&#39;] = &#39;High Residual&#39;
    df.loc[df[&#39;high_leverage&#39;], &#39;type&#39;] = &#39;High Leverage&#39;
    df.loc[df[&#39;high_influence&#39;], &#39;type&#39;] = &#39;High Influence&#39;

    # Init figure
    fig, (ax1,ax2) = plt.subplots(1,2, figsize=(12,5)) 
    ax1.plot(X, Y_hat, lw=1, c=&#39;grey&#39;, zorder=0.5)
    sns.scatterplot(data=df, x=&#39;hours&#39;, y=&#39;transactions&#39;, ax=ax1, hue=&#39;type&#39;).set(title=&#39;Data&#39;)
    sns.scatterplot(data=df, x=&#39;residual&#39;, y=&#39;leverage&#39;, hue=&#39;type&#39;, ax=ax2).set(title=&#39;Metrics&#39;)
    ax1.get_legend().remove()
    sns.move_legend(ax2, &amp;quot;upper left&amp;quot;, bbox_to_anchor=(1.05, 0.8));
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plot_leverage_residuals(df)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/outliers_leverage_34_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;As we can see, we have one point with high residual and low leverage, one with high leverage and low residual and only one point with both high leverage and high residual: the only influential point.&lt;/p&gt;
&lt;p&gt;From the plot it is also clear why none of the two conditions alone is sufficient for an observation to rive the model. The orange point has high residual but it lies right in the middle of the distribution and therefore cannot tilt the line of best fit. The green point instead has high leverage and lies far from the center of the distribution but its perfectly aligned with the line of fit. Removing it would not change anything. The red dot instead is different from the others in terms of &lt;strong&gt;both characteristics and behavior&lt;/strong&gt; and therefore tilts the fit line towards itself.&lt;/p&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;In this post, we have seen some different ways in which observations can be &amp;ldquo;unusual&amp;rdquo;: they can have either unusual characteristics or unusual behavior. In linear regression, when an observation has both it is also influential: it tilts the model towards itself.&lt;/p&gt;
&lt;p&gt;In the example of the article, we concentrated on a univariate linear regression. However, research on influence functions has recently become a hot topic because of the need to make black-box machine learning algorithms understandable. With models with millions of parameters, billions of observations and wild non-linearities, it can be very hard to establish whether a single observation is influential and how.&lt;/p&gt;
&lt;h3 id=&#34;references&#34;&gt;References&lt;/h3&gt;
&lt;p&gt;[1] D. Cook, &lt;a href=&#34;https://www.jstor.org/stable/1268249&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Detection of Influential Observation in Linear Regression&lt;/a&gt; (1980), &lt;em&gt;Technometrics&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;[2] D. Cook, S. Weisberg, &lt;a href=&#34;https://www.jstor.org/stable/1268187&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Characterizations of an Empirical Influence Function for Detecting Influential Cases in
Regression&lt;/a&gt; (1980), &lt;em&gt;Technometrics&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;[2] P. W. Koh, P. Liang, &lt;a href=&#34;http://proceedings.mlr.press/v70/koh17a&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Understanding Black-box Predictions via Influence Functions&lt;/a&gt; (2017), &lt;em&gt;ICML Proceedings&lt;/em&gt;.&lt;/p&gt;
&lt;h3 id=&#34;code&#34;&gt;Code&lt;/h3&gt;
&lt;p&gt;You can find the original Jupyter Notebook here:&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/outliers.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/outliers.ipynb&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>A/B Tests, Privacy and Online Regression</title>
      <link>https://matteocourthoud.github.io/post/online_regression/</link>
      <pubDate>Mon, 10 Jul 2023 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/post/online_regression/</guid>
      <description>&lt;p&gt;&lt;em&gt;How to run experiments without storing individual data&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;AB tests, a.k.a. randomized controlled trials, are widely recognized as the gold standard technique to compute the causal effect of a treatment (a drug, ad, product, &amp;hellip;) on an outcome of interest (a disease, firm revenue, customer satisfaction, &amp;hellip;). We randomly split a set of subjects (patients, users, customers, &amp;hellip;) into a treatment and a control group and give the treatment to the treatment group. This procedure ensures that ex-ante, the only expected difference between the two groups is caused by the treatment.&lt;/p&gt;
&lt;p&gt;One potential &lt;strong&gt;privacy concern&lt;/strong&gt; is that one needs to store data about many users for the whole duration of the experiment in order to estimate the effect of the treatment. This is not a problem if we can run the experiment instantaneusly, but can become an issue when the experiment duration is long. In this post, we are going to explore one solution to this problem: &lt;strong&gt;online regression&lt;/strong&gt;. We will see how to estimate (conditional) average treatment effects and how to do inference, using both asymptotic approximations and bootstrapping.&lt;/p&gt;
&lt;p&gt;⚠️ Some parts are algebra-intense, but you can skip them if you are only interested in the intuition.&lt;/p&gt;
&lt;h2 id=&#34;credit-cards-and-donations&#34;&gt;Credit Cards and Donations&lt;/h2&gt;
&lt;p&gt;Suppose, for example, that we were a fin-tech company. We have designed a new user interface (UI) for our mobile application and we would like to understand whether it slows down our transaction. In order to estimate the causal effect of the new UI on transaction speed, we plan to run an A/B test or randomized controlled trial.&lt;/p&gt;
&lt;p&gt;We have one major problem: we should not store user-level information for privacy reasons.&lt;/p&gt;
&lt;p&gt;First, let&amp;rsquo;s have a look at the data. I import the data generating process &lt;code&gt;dgp_credit()&lt;/code&gt; from &lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/src/dgp.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;src.dgp&lt;/code&gt;&lt;/a&gt; and some plotting functions and libraries from &lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/src/utils.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;src.utils&lt;/code&gt;&lt;/a&gt;. I include code snippets from &lt;a href=&#34;https://deepnote.com&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Deepnote&lt;/a&gt;, a Jupyter-like web-based collaborative notebook environment. For our purpose, Deepnote is very handy because it allows me not only to include code but also output, like data and tables.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;%matplotlib inline
%config InlineBackend.figure_format = &#39;retina&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from src.utils import *
from src.dgp import dgp_credit
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I first generate the whole dataset in one-shot. We will then investigate how to perform the experimental analysis in case the data was arriving dynamically.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def generate_data(self, N=100, seed=0):
        np.random.seed(seed)
        
        # Connection speed
        connection = np.random.lognormal(3, 1, N)
        
        # Treatment assignment
        treated = np.random.binomial(1, 0.5, N)
        
        # Transfer speed
        #spend = np.minimum(np.random.lognormal(1 + treated + 0.1*np.sqrt(balance), 2, N), balance)
        speed = np.minimum(np.random.exponential(10 + 4*treated - 0.5*np.sqrt(connection), N), connection)
        
        # Generate the dataframe
        df = pd.DataFrame({&#39;c&#39;: [1]*N, &#39;treated&#39;: treated,  
                           &#39;connection&#39;: np.round(connection,2), 
                           &#39;speed&#39;: np.round(speed,2)})

        return df
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;N = 100
df = generate_data(N)
df.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;c&lt;/th&gt;
      &lt;th&gt;treated&lt;/th&gt;
      &lt;th&gt;connection&lt;/th&gt;
      &lt;th&gt;speed&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;117.22&lt;/td&gt;
      &lt;td&gt;0.94&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;29.97&lt;/td&gt;
      &lt;td&gt;29.97&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;53.45&lt;/td&gt;
      &lt;td&gt;7.38&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;188.84&lt;/td&gt;
      &lt;td&gt;0.76&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;130.00&lt;/td&gt;
      &lt;td&gt;24.44&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;We have information on 100 users, for whom we observe&amp;hellip;&lt;/p&gt;
&lt;p&gt;First, let&amp;rsquo;s have a look&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;model = smf.ols(&#39;speed ~ treated + connection&#39;, data=df).fit()
model.summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
       &lt;td&gt;&lt;/td&gt;         &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt;  &lt;td&gt;    6.0740&lt;/td&gt; &lt;td&gt;    1.079&lt;/td&gt; &lt;td&gt;    5.630&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    3.933&lt;/td&gt; &lt;td&gt;    8.215&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;treated&lt;/th&gt;    &lt;td&gt;    1.3939&lt;/td&gt; &lt;td&gt;    1.297&lt;/td&gt; &lt;td&gt;    1.075&lt;/td&gt; &lt;td&gt; 0.285&lt;/td&gt; &lt;td&gt;   -1.180&lt;/td&gt; &lt;td&gt;    3.968&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;connection&lt;/th&gt; &lt;td&gt;   -0.0033&lt;/td&gt; &lt;td&gt;    0.017&lt;/td&gt; &lt;td&gt;   -0.197&lt;/td&gt; &lt;td&gt; 0.844&lt;/td&gt; &lt;td&gt;   -0.037&lt;/td&gt; &lt;td&gt;    0.030&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;In order to understand how we can make linear regression one data point at the time, we first need a brief linear algebra recap.&lt;/p&gt;
&lt;p&gt;First of all, let&amp;rsquo;s define $y$ the dependent variable, &lt;code&gt;spend&lt;/code&gt; in our case, and $X$ the explanatory variable, the &lt;code&gt;treated&lt;/code&gt; indicator, the account &lt;code&gt;balance&lt;/code&gt; and a constant.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def xy_from_df(df, r0, r1):
    return df.iloc[r0:r1,:3].to_numpy(), df.iloc[r0:r1,3].to_numpy()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The  estimator is given by
$$
\hat{\beta}_{OLS} = (X&amp;rsquo;X)^{-1}X&amp;rsquo;y
$$&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from numpy.linalg import inv

X, Y = xy_from_df(df, 0, 100)
inv(X.T @ X) @ X.T @ Y
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;array([ 6.07404291e+00,  1.39385101e+00, -3.33599131e-03])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We get indeed the same exact number as with the &lt;code&gt;smf.ols&lt;/code&gt; command!&lt;/p&gt;
&lt;p&gt;Can we compute $\beta$ one observation at the time?&lt;/p&gt;
&lt;p&gt;The answer is yes! Assume we had $n$ observations and we just received the $n+1$th observation: the pair $(x_{n+1}, y_{n+1})$. In order to compute $\hat{\beta}_{n+1}$ we need to have stored only two objects in memory&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\hat{\beta}_{n}$, the previous estimate of $\beta$&lt;/li&gt;
&lt;li&gt;$(X_n&amp;rsquo; X_n)^{-1}$, the previous value of $(X&amp;rsquo; X)^{-1}$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;First of all, how do we update $(X&amp;rsquo; X)^{-1}$?
$$
\begin{align*}
(X_{n+1}&amp;rsquo; X_{n+1})^{-1} = (X_n&amp;rsquo; X_n)^{-1} - \frac{(X_n&amp;rsquo; X_n)^{-1} x_{n+1} x_{n+1}&amp;rsquo; (X_n&amp;rsquo; X_n)^{-1}}{1 + x_{n+1}&amp;rsquo; (X_n&amp;rsquo; X_n)^{-1} x_{n+1}}
\end{align*}
$$&lt;/p&gt;
&lt;p&gt;After having updated $(X&amp;rsquo; X)^{-1}$, we can update $\hat{\beta}$.
$$
\hat{\beta}&lt;em&gt;{n+1} = \hat{\beta}&lt;/em&gt;{n} + (X_n&amp;rsquo; X_n)^{-1} x_{n} (y_n - x_n&amp;rsquo; \hat{\beta}_{n})
$$&lt;/p&gt;
&lt;p&gt;Note that this procedure is not only privacy friendly but also &lt;strong&gt;memory-friendly&lt;/strong&gt;. Our dataset is a $100 \times 4$ matrix while $(X&amp;rsquo; X)^{-1}$ is $3 \times 3$ matrix and $\beta$ is a $3 \times 1$ matrix. We are storing only 12 numbers instead of up to 400!&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def update_xb(XiX, beta, x, y):
    XiX -= (XiX @ x.T @ x @ XiX) / (1 + x @ XiX @ x.T )
    beta += XiX @ x.T @ (y - x @ beta)
    return XiX, beta
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We are now ready to estimate our OLS coefficient, one data point at the time. However, we cannot really start from the first observation, because we would be unable to invert the matrix $X&amp;rsquo;X$. We need at least $k+1$ observations, where $k$ is the number of variables in $X$.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s use a warm start of 10 observations to be safe.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Initialize XiX and beta from first 10 observations
x, y = xy_from_df(df, 0, 10)
XiX = inv(x.T @ x)
beta = XiX @ x.T @ y

# Update estimate live
for n in range(10, N):
    x, y = xy_from_df(df, n, n+1)
    XiX, beta = update_xb(XiX, beta, x, y)
    
# Print result
print(beta)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[ 6.07404291e+00  1.39385101e+00 -3.33599131e-03]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We got exactly the same coefficient! Nice!&lt;/p&gt;
&lt;p&gt;How did we get there? We can plot the evolution of out estimate $\hat{\beta}$ as we accumulate data. The dynamic plotting function is a bit more cumbersome, but you can find it in &lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/src/figures.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;src.figures&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from src.figures import online_regression

online_regression(df, &amp;quot;fig/online_reg1.gif&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;fig/online_reg1.gif&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;As we can see, as the number of data points increases, the estimate seems to less and less volatile.&lt;/p&gt;
&lt;p&gt;But is it really the case? As usual, we are not just interested in the point estimate of the effect of the &lt;code&gt;coupon&lt;/code&gt; on spending, we would also like to understand how precise this estimate is.&lt;/p&gt;
&lt;h2 id=&#34;inference&#34;&gt;Inference&lt;/h2&gt;
&lt;p&gt;We have seen how to estimate the treatment effect &amp;ldquo;online&amp;rdquo;: one observation at the time. Can we also compute the variance of the estimator in the same manner?&lt;/p&gt;
&lt;p&gt;First of all, let&amp;rsquo;s review what the variance of the OLS estimator looks like. Under baseline assumptions, the variance of the OLS estimator is given by:
$$
\text{Var}(\hat{\beta}_{OLS}) = (X&amp;rsquo;X)^{-1} \hat{\sigma}^2
$$&lt;/p&gt;
&lt;p&gt;where $\hat{\sigma}^2$ is the variance of the residuals $e = (y - X&amp;rsquo;\hat{\beta})$.&lt;/p&gt;
&lt;p&gt;The regression table reports the standard errors of the coefficients, which are the squared elements on the diagonal of $\text{Var}(\hat{\beta})$.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;model.summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
       &lt;td&gt;&lt;/td&gt;         &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt;  &lt;td&gt;    6.0740&lt;/td&gt; &lt;td&gt;    1.079&lt;/td&gt; &lt;td&gt;    5.630&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    3.933&lt;/td&gt; &lt;td&gt;    8.215&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;treated&lt;/th&gt;    &lt;td&gt;    1.3939&lt;/td&gt; &lt;td&gt;    1.297&lt;/td&gt; &lt;td&gt;    1.075&lt;/td&gt; &lt;td&gt; 0.285&lt;/td&gt; &lt;td&gt;   -1.180&lt;/td&gt; &lt;td&gt;    3.968&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;connection&lt;/th&gt; &lt;td&gt;   -0.0033&lt;/td&gt; &lt;td&gt;    0.017&lt;/td&gt; &lt;td&gt;   -0.197&lt;/td&gt; &lt;td&gt; 0.844&lt;/td&gt; &lt;td&gt;   -0.037&lt;/td&gt; &lt;td&gt;    0.030&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;Let&amp;rsquo;s check that we would obtain the same numbers using matrix algebra.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;beta = inv(X.T @ X) @ X.T @ Y
np.sqrt(np.diag(inv(X.T @ X) * np.var(Y - X @ beta)))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;array([1.06261376, 1.27718352, 0.01669716])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Indeed, we get exactly the same numbers!&lt;/p&gt;
&lt;p&gt;We already have a method to part of $\text{Var}(\hat{\beta}&lt;em&gt;{OLS})$ online: $(X&amp;rsquo;X)^{-1}$ update the matrix $(X&amp;rsquo;X)^{-1}$ online. How do we update $\hat{\sigma}^2$? This is the formula to update the sum of squared residuals $S$.
$$
S&lt;/em&gt;{n+1} = S_{n} + \frac{(y_{n+1} - x_{n+1}\hat{\beta}&lt;em&gt;n)}{1 + x&lt;/em&gt;{n+1}&amp;rsquo; (X_n&amp;rsquo; X_n)^{-1} x_{n+1}}
$$&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def update_xbs(XiX, beta, S, x, y):
    S += (y - x @ beta)**2 / (1 + x @ XiX @ x.T )
    XiX -= (XiX @ x.T @ x @ XiX) / (1 + x @ XiX @ x.T )
    beta += XiX @ x.T @ (y - x @ beta)
    return XiX, beta, S[0,0]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note, the order here is very important!&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Inizialize XiX, beta, and sigma from the first 10 observations
x, y = xy_from_df(df, 0, 10)
XiX = inv(x.T @ x)
beta = XiX @ x.T @ y
S = np.sum((y - x @ beta)**2)

# Update XiX, beta, and sigma online
for n in range(10, N):
    x, y = xy_from_df(df, n, n+1)
    XiX, beta, S = update_xbs(XiX, beta, S, x, y)
    
# Print result
print(np.sqrt(np.diag(XiX * S / (N - 3))))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1.0789208  1.29678338 0.0169534 ]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We indeed got the same result! Note that to get from the sum of squared residuals $S$ to the residuals variance $\hat{\sigma}^2$ we need to divide by the degrees of freedom: $n - k = 100 - 3$.&lt;/p&gt;
&lt;p&gt;As before we have plotted the evolution of the estimate of the OLS coefficient over time, we can now augment that plot with a confidence band of +- one standard deviation.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;online_regression(df, &amp;quot;fig/online_reg2.gif&amp;quot;, ci=True)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;fig/online_reg2.gif&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;As we can see, the estimated variance of the OLS estimator indeed decreases as the sample size increases.&lt;/p&gt;
&lt;h2 id=&#34;bootstrap&#34;&gt;Bootstrap&lt;/h2&gt;
&lt;p&gt;So far we have used the asymptotic assumptions behind the Central Limit Theorem to compute the standard errors of the estimator. However, we have a particularly small sample. We further check the empirical distribution of the model residuals.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sns.histplot(model.resid, bins=30);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/online_regression_39_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;The residuals seem to be particularly &lt;strong&gt;skewed&lt;/strong&gt;! This might be a problem in such a small sample.&lt;/p&gt;
&lt;p&gt;One alternative is &lt;strong&gt;the bootstrap&lt;/strong&gt;. Instead of relying on asymptotics, we approximate the distribution of our estimator by resampling our dataset with replacement. Can we bootstrap online?&lt;/p&gt;
&lt;p&gt;The answer is once again yes! They key is to weight each observation with an integer weight drawn from a Poisson distribution with mean (and variance) equal to 1. We repeat this process multiple times, in parallel and then we&lt;/p&gt;
&lt;p&gt;The updating rules for $(X&amp;rsquo;X)^{-1}$ and $\hat{beta}$ become the following.
$$
\begin{align*}
(X_{n+1}&amp;rsquo; X_{n+1})^{-1} = (X_n&amp;rsquo; X_n)^{-1} - \frac{w (X_n&amp;rsquo; X_n)^{-1} x_{n+1} x_{n+1}&amp;rsquo; (X_n&amp;rsquo; X_n)^{-1}}{1 + w x_{n+1}&amp;rsquo; (X_n&amp;rsquo; X_n)^{-1} x_{n+1}}
\end{align*}
$$&lt;/p&gt;
&lt;p&gt;and
$$
\hat{\beta}&lt;em&gt;{n+1} = \hat{\beta}&lt;/em&gt;{n} + w (X_n&amp;rsquo; X_n)^{-1} x_{n} (y_n - x_n&amp;rsquo; \hat{\beta}_{n})
$$&lt;/p&gt;
&lt;p&gt;where $w$ are Poisson weights. First, let&amp;rsquo;s update the updating function for $(X&amp;rsquo;X)^{-1}$ and $\hat{beta}$.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def update_xbw(XiX, beta, w, x, y):
    XiX -= (w * XiX @ x.T @ x @ XiX) / (1 + w * x @ XiX @ x.T )
    beta += w * XiX @ x.T @ (y - x @ beta)
    return XiX, beta
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can now run the online estimation. We bootstrap 1000 different estimates of $\hat{\beta}$.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Inizialize a vector of XiXs and betas 
np.random.seed(0)
K = 1000
x, y = xy_from_df(df, 0, 10)
XiXs = [inv(x.T @ x) for k in range(K)]
betas = [xix @ x.T @ y for xix in XiXs]

# Update the vector of XiXs and betas online
for n in range(10, N):
    x, y = xy_from_df(df, n, n+1)
    for k in range(K):
        w = np.random.poisson(1)
        XiXs[k], betas[k] = update_xbw(XiXs[k], betas[k], w, x, y)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can compute the estimated standard deviation of the treatment effect, simply by computing the standard deviation of the vector of bootstrapped coefficients.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;np.std(betas, axis=0)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;array([0.95301002, 1.14186364, 0.01207962])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The estimated standard errors are slightly different from the previous values of $[1.275, 1.532, 0.020]$, but not very far apart.&lt;/p&gt;
&lt;p&gt;Lastly, some of you might have wondered &amp;ldquo;&lt;em&gt;why sampling discrete weights and not continuous ones?&lt;/em&gt;&amp;rdquo;. Indeed, we can. This procedure is called the &lt;strong&gt;Bayesian Bootstrap&lt;/strong&gt; and you can find a more detailed explanation &lt;a href=&#34;https://towardsdatascience.com/6ca4a1d45148&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;In this post, we have seen hot to run an experiment without storing individual-level data. How are we able to do it? In order to compute the average treatment effect, we do not need every single observation but it&amp;rsquo;s sufficient to store just a more compact representation of it.&lt;/p&gt;
&lt;h3 id=&#34;references&#34;&gt;References&lt;/h3&gt;
&lt;p&gt;[1] W. Chou, &lt;a href=&#34;https://arxiv.org/abs/2102.03316&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Randomized Controlled Trials without Data Retention&lt;/a&gt; (2021), &lt;em&gt;Working Paper&lt;/em&gt;.&lt;/p&gt;
&lt;h3 id=&#34;related-articles&#34;&gt;Related Articles&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/954506cec665&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Experiments, Peeking, and Optimal Stopping&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/6ca4a1d45148&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The Bayesian Bootstrap&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;code&#34;&gt;Code&lt;/h3&gt;
&lt;p&gt;You can find the original Jupyter Notebook here:&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/online_reg.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/online_reg.ipynb&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>DAGs and Control Variables</title>
      <link>https://matteocourthoud.github.io/post/good_bad_controls/</link>
      <pubDate>Mon, 10 Jul 2023 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/post/good_bad_controls/</guid>
      <description>&lt;p&gt;When analyzing causal relationships, it is very hard to understand which variables to &lt;strong&gt;condition the analysis on&lt;/strong&gt;, i.e. how to &amp;ldquo;split&amp;rdquo; the data so that we are &lt;strong&gt;comparing apples to apples&lt;/strong&gt;. For example, if you want to understand the effect of having a tablet in class on studenta&amp;rsquo; performance, it makes sense to compare schools where students have similar socio-economic backgrounds. Otherwise, the risk is that only wealthier students can afford a tablet and, without controlling for it, we might attribute the effect to tablets instead of the socio-economic background.&lt;/p&gt;
&lt;p&gt;When the treatment of interest comes from a proper &lt;strong&gt;randomized experiment&lt;/strong&gt;, we do not need to worry about conditioning on other variables. If tablets are distributed randomly across schools, and we have enough schools in the experiment, we do not have to worry about the socio-economic background of students. The only advantage of conditioning the analysis on some so-called &amp;ldquo;control variable&amp;rdquo; could be an increase in power. However, this is a different story.&lt;/p&gt;
&lt;p&gt;In this post, we are going to have a brief introduction to Directed Acyclic Graphs and how they can be useful to select variables to condition a causal analysis on. Not only DAGs provide visual intuition on which variables we need to &lt;em&gt;include&lt;/em&gt; in the analysis, but also on which variables we should &lt;em&gt;not include&lt;/em&gt;, and why.&lt;/p&gt;
&lt;h2 id=&#34;directed-acyclic-graphs&#34;&gt;Directed Acyclic Graphs&lt;/h2&gt;
&lt;h3 id=&#34;definitions&#34;&gt;Definitions&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Directed acyclic graphs&lt;/strong&gt; (&lt;strong&gt;DAG&lt;/strong&gt;s) provide a visual representation of the data generating process. Random variables are represented with letters (e.g. $X$) and causal relationships are represented with arrows (e.g. $\to$). For example, we interpret&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-mermaid&#34;&gt;flowchart LR
classDef white fill:#FFFFFF,stroke:#000000,stroke-width:2px
X((X)):::white --&amp;gt; Y((Y)):::white
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;as $X$ (possibly) causes $Y$. We call a &lt;strong&gt;path&lt;/strong&gt; between two variables $X$ and $Y$ any connection, &lt;em&gt;independently of the direction of the arrows&lt;/em&gt;. If all arrows point forward, we call it a &lt;strong&gt;causal path&lt;/strong&gt;, otherwise we call it a &lt;strong&gt;spurious path&lt;/strong&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-mermaid&#34;&gt;flowchart LR
classDef included fill:#DCDCDC,stroke:#000000,stroke-width:2px;
classDef excluded fill:#ffffff,stroke:#000000,stroke-width:2px;

X((X))
Y((Y))
Z1((Z1))
Z2((Z2))
Z3((Z3))

X --&amp;gt; Z1
Z1 --&amp;gt; Z2
Z3 --&amp;gt; Z2
Z3 --&amp;gt; Y

class X,Y included;
class Z1,Z2,Z3 excluded;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In the example above, we have a path between $X$ and $Y$ passing through the variables $Z_1$, $Z_2$, and $Z_3$. Since not all arrows point forward, the path is &lt;em&gt;spurious&lt;/em&gt; and there is no causal relationship of $X$ on $Y$. In fact, variable $Z_2$ is caused by both $Z_1$ and $Z_3$ and therefore &lt;strong&gt;blocks&lt;/strong&gt; the path.&lt;/p&gt;
&lt;p&gt;$Z_2$ is called a &lt;strong&gt;collider&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;The purpose of our analysis is to assess the &lt;strong&gt;causal relationship&lt;/strong&gt; between two variables $X$ and $Y$. Directed acyclic graphs are useful because they provide us instructions on which other variables $Z$ we need to &lt;strong&gt;condition&lt;/strong&gt; our analysis on. Conditioning the analysis on a variable means that we keep it fixed and we draw our conclusions &lt;em&gt;ceteris paribus&lt;/em&gt;. For example, in a linear regression framework, inserting another regressor $Z$ means that we are computing the best linear approximation of the conditional expectation function of $Y$ given $X$, &lt;em&gt;conditional&lt;/em&gt; on the observed values of $Z$.&lt;/p&gt;
&lt;h3 id=&#34;causality&#34;&gt;Causality&lt;/h3&gt;
&lt;p&gt;In order to assess causality, we want to &lt;strong&gt;close all spurious paths&lt;/strong&gt; between $X$ and $Y$. The &lt;strong&gt;questions&lt;/strong&gt; now are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;When is a path &lt;strong&gt;open&lt;/strong&gt;?
&lt;ul&gt;
&lt;li&gt;If it does not contain &lt;em&gt;colliders&lt;/em&gt;. Otherwise, it is &lt;em&gt;closed&lt;/em&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;How do you &lt;strong&gt;close an open path&lt;/strong&gt;?
&lt;ul&gt;
&lt;li&gt;You condition on &lt;em&gt;at least one&lt;/em&gt; intermediate variable.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;How do you &lt;strong&gt;open a closed path&lt;/strong&gt;?
&lt;ul&gt;
&lt;li&gt;You condition on &lt;em&gt;all&lt;/em&gt; colliders along the path.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Suppose we are again interested in the causal relationship of $X$ on $Y$. Let&amp;rsquo;s consider the following graph&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-mermaid&#34;&gt;flowchart LR
classDef included fill:#DCDCDC,stroke:#000000,stroke-width:2px;
classDef excluded fill:#ffffff,stroke:#000000,stroke-width:2px;

X((X))
Y((Y))
Z1((Z1))
Z2((Z2))
Z3((Z3))

X --&amp;gt; Y
X --&amp;gt; Z2
Z2 --&amp;gt; Y
Z1 --&amp;gt; X
Z1 --&amp;gt; Y
X --&amp;gt; Z3
Y --&amp;gt; Z3

class X,Y included;
class Z1,Z2,Z3 excluded;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In this case, apart from the direct path, there are &lt;strong&gt;three non-direct paths&lt;/strong&gt; between $X$ and $Y$ through the variables $Z_1$, $Z_2$, and $Z_3$.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s consider the case in which we analyze the relationship between $X$ and $Y$, ignoring all other variables.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;The path through $Z_1$ is &lt;strong&gt;open&lt;/strong&gt; but it is &lt;strong&gt;spurious&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;The path through $Z_2$ is &lt;strong&gt;open&lt;/strong&gt; and &lt;strong&gt;causal&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;The path through $Z_3$ is &lt;strong&gt;closed&lt;/strong&gt; since $Z_3$ is a &lt;em&gt;collider&lt;/em&gt; and it is &lt;strong&gt;spurious&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Let&amp;rsquo;s draw the same graph indicating in &lt;em&gt;grey&lt;/em&gt; variables that we are conditioning on, with &lt;em&gt;dotted lines&lt;/em&gt; closed paths, with &lt;em&gt;red lines&lt;/em&gt; spurious open paths, and with &lt;em&gt;green lines&lt;/em&gt; causal open paths.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-mermaid&#34;&gt;flowchart LR
classDef included fill:#DCDCDC,stroke:#000000,stroke-width:2px;
classDef excluded fill:#ffffff,stroke:#000000,stroke-width:2px;

X((X))
Y((Y))
Z1((Z1))
Z2((Z2))
Z3((Z3))

X --&amp;gt; Y
X --&amp;gt; Z2
Z2 --&amp;gt; Y
Z1 --&amp;gt; X
Z1 --&amp;gt; Y
X -.-&amp;gt; Z3
Y -.-&amp;gt; Z3

linkStyle 0,1,2 stroke:#00ff00,stroke-width:4px;
linkStyle 3,4 stroke:#ff0000,stroke-width:4px;
class X,Y included;
class Z1,Z2,Z3 excluded;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In this case, to assess the &lt;strong&gt;causal&lt;/strong&gt; relationship between $X$ and $Y$ we need to &lt;strong&gt;close&lt;/strong&gt; the path that passes through $Z_1$. We can do that by conditioning the analysis on $Z_1$.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-mermaid&#34;&gt;flowchart LR
classDef included fill:#DCDCDC,stroke:#000000,stroke-width:2px;
classDef excluded fill:#ffffff,stroke:#000000,stroke-width:2px;

X((X))
Y((Y))
Z1((Z1))
Z2((Z2))
Z3((Z3))

X --&amp;gt; Y
X --&amp;gt; Z2
Z2 --&amp;gt; Y
Z1 -.-&amp;gt; X
Z1 -.-&amp;gt; Y
X -.-&amp;gt; Z3
Y -.-&amp;gt; Z3

linkStyle 0,1,2 stroke:#00ff00,stroke-width:4px;
class X,Y,Z1 included;
class Z2,Z3 excluded;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we are able to recover the causal relationship between $X$ and $Y$ by conditioning on $Z_1$.&lt;/p&gt;
&lt;p&gt;What would happen if we were also &lt;strong&gt;conditioning on $Z_2$&lt;/strong&gt;? In this case, we would &lt;strong&gt;close&lt;/strong&gt; the path passing through $Z_2$ leaving only the &lt;em&gt;direct&lt;/em&gt; path between $X$ and $Y$ open. We would then recover only the &lt;strong&gt;direct effect&lt;/strong&gt; of $X$ on $Y$ and not the &lt;em&gt;indirect&lt;/em&gt; one.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-mermaid&#34;&gt;flowchart LR
classDef included fill:#DCDCDC,stroke:#000000,stroke-width:2px;
classDef excluded fill:#ffffff,stroke:#000000,stroke-width:2px;

X((X))
Y((Y))
Z1((Z1))
Z2((Z2))
Z3((Z3))

X --&amp;gt; Y
X -.-&amp;gt; Z2
Z2 -.-&amp;gt; Y
Z1 -.-&amp;gt; X
Z1 -.-&amp;gt; Y
X -.-&amp;gt; Z3
Y -.-&amp;gt; Z3

linkStyle 0 stroke:#00ff00,stroke-width:4px;
class X,Y,Z1,Z2 included;
class Z3 excluded;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What would happen if we were also &lt;strong&gt;conditioning on $Z_3$&lt;/strong&gt;? In this case, we would &lt;strong&gt;open&lt;/strong&gt; the path passing through $Z_3$ which is a &lt;strong&gt;spurious&lt;/strong&gt; path. We would then &lt;strong&gt;not&lt;/strong&gt; be able to recover the causal effect of $X$ on $Y$.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-mermaid&#34;&gt;flowchart LR
classDef included fill:#DCDCDC,stroke:#000000,stroke-width:2px;

X((X))
Y((Y))
Z1((Z1))
Z2((Z2))
Z3((Z3))

X --&amp;gt; Y
X -.-&amp;gt; Z2
Z2 -.-&amp;gt; Y
Z1 -.-&amp;gt; X
Z1 -.-&amp;gt; Y
X --&amp;gt; Z3
Y --&amp;gt; Z3

linkStyle 0 stroke:#00ff00,stroke-width:4px;
linkStyle 5,6 stroke:#ff0000,stroke-width:4px;
class X,Y,Z1,Z2,Z3 included;
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;example-class-size-and-math-scores&#34;&gt;Example: Class Size and Math Scores&lt;/h2&gt;
&lt;p&gt;Suppose you are interested in the &lt;strong&gt;effect of class size on math scores&lt;/strong&gt;. Are bigger classes better or worse for students&amp;rsquo; performance?&lt;/p&gt;
&lt;p&gt;Assume that the data generating process can be represented with the following &lt;strong&gt;DAG&lt;/strong&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-mermaid&#34;&gt;flowchart TB
classDef included fill:#DCDCDC,stroke:#000000,stroke-width:2px;
classDef excluded fill:#ffffff,stroke:#000000,stroke-width:2px;
classDef unobserved fill:#ffffff,stroke:#000000,stroke-width:2px,stroke-dasharray: 5 5;


X((class size))
Y((math score))
Z1((class year))
Z2((good school))
Z3((math hours))
Z4((hist score))
U((ability))

X --&amp;gt; Y
Z1 --&amp;gt; X
X --&amp;gt; Z4
U --&amp;gt; Y
U --&amp;gt; Z4
Z2 --&amp;gt; X
Z2 --&amp;gt; Y
Z2 --&amp;gt; Z4
Z3 --&amp;gt; Y

class X,Y included;
class Z1,Z2,Z3,Z4 excluded;
class U unobserved;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The variables of interest are highlighted. Moreover, the dotted line around &lt;code&gt;ability&lt;/code&gt; indicates that this is a variable that we do not observe in the data.&lt;/p&gt;
&lt;p&gt;We can now load the data and check what it looks like.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;%matplotlib inline
%config InlineBackend.figure_format = &#39;retina&#39;
from src.utils import *
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from src.dgp import dgp_school

df = dgp_school().generate_data()
df.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;math_hours&lt;/th&gt;
      &lt;th&gt;history_hours&lt;/th&gt;
      &lt;th&gt;good_school&lt;/th&gt;
      &lt;th&gt;class_year&lt;/th&gt;
      &lt;th&gt;class_size&lt;/th&gt;
      &lt;th&gt;math_score&lt;/th&gt;
      &lt;th&gt;hist_score&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;15&lt;/td&gt;
      &lt;td&gt;13.009309&lt;/td&gt;
      &lt;td&gt;15.167024&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;19&lt;/td&gt;
      &lt;td&gt;13.047033&lt;/td&gt;
      &lt;td&gt;13.387456&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;25&lt;/td&gt;
      &lt;td&gt;8.330311&lt;/td&gt;
      &lt;td&gt;10.824070&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;22&lt;/td&gt;
      &lt;td&gt;11.322190&lt;/td&gt;
      &lt;td&gt;14.594394&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;15&lt;/td&gt;
      &lt;td&gt;12.338458&lt;/td&gt;
      &lt;td&gt;11.871626&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;What variables should we condition our regression on, in order to estimate the causal effect of &lt;code&gt;class size&lt;/code&gt; on &lt;code&gt;math scores&lt;/code&gt;?&lt;/p&gt;
&lt;p&gt;First of all, let&amp;rsquo;s look at what happens if we do not condition our analysis on any variable and we just regress &lt;code&gt;math score&lt;/code&gt; on &lt;code&gt;class size&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;smf.ols(&#39;math_score ~ class_size&#39;, df).fit().summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
       &lt;td&gt;&lt;/td&gt;         &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt;  &lt;td&gt;   12.0421&lt;/td&gt; &lt;td&gt;    0.259&lt;/td&gt; &lt;td&gt;   46.569&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;   11.535&lt;/td&gt; &lt;td&gt;   12.550&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;class_size&lt;/th&gt; &lt;td&gt;   -0.0399&lt;/td&gt; &lt;td&gt;    0.013&lt;/td&gt; &lt;td&gt;   -3.025&lt;/td&gt; &lt;td&gt; 0.003&lt;/td&gt; &lt;td&gt;   -0.066&lt;/td&gt; &lt;td&gt;   -0.014&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;The effect of &lt;code&gt;class_size&lt;/code&gt; is negative and statistically different from zero.&lt;/p&gt;
&lt;p&gt;But should we believe this estimated effect? Without controlling for anything, this is &lt;strong&gt;DAG representation&lt;/strong&gt; of the effect we are capturing.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-mermaid&#34;&gt;flowchart TB
classDef included fill:#DCDCDC,stroke:#000000,stroke-width:2px;
classDef excluded fill:#ffffff,stroke:#000000,stroke-width:2px;
classDef unobserved fill:#ffffff,stroke:#000000,stroke-width:2px,stroke-dasharray: 5 5;


X((class size))
Y((math score))
Z1((class year))
Z2((good school))
Z3((math hours))
Z4((hist score))
U((ability))

X --&amp;gt; Y
Z1 --&amp;gt; X
X -.-&amp;gt; Z4
U --&amp;gt; Y
U -.-&amp;gt; Z4
Z2 --&amp;gt; X
Z2 --&amp;gt; Y
Z2 --&amp;gt; Z4
Z3 --&amp;gt; Y

linkStyle 0 stroke:#00ff00,stroke-width:4px;
linkStyle 5,6 stroke:#ff0000,stroke-width:4px;

class X,Y included;
class Z1,Z2,Z3,Z4 excluded;
class U unobserved;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There is a &lt;strong&gt;spurious&lt;/strong&gt; path passing through &lt;code&gt;good school&lt;/code&gt; that &lt;strong&gt;biases&lt;/strong&gt; our estimated coefficient. Intuitively, being enrolled in a better school improves the students&amp;rsquo; math scores and better schools might have smaller class sizes. We need to control for the quality of the school.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;smf.ols(&#39;math_score ~ class_size + good_school&#39;, df).fit().summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
       &lt;td&gt;&lt;/td&gt;          &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt;   &lt;td&gt;    4.7449&lt;/td&gt; &lt;td&gt;    0.247&lt;/td&gt; &lt;td&gt;   19.176&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    4.259&lt;/td&gt; &lt;td&gt;    5.230&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;class_size&lt;/th&gt;  &lt;td&gt;    0.2095&lt;/td&gt; &lt;td&gt;    0.010&lt;/td&gt; &lt;td&gt;   20.020&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    0.189&lt;/td&gt; &lt;td&gt;    0.230&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;good_school&lt;/th&gt; &lt;td&gt;    5.0807&lt;/td&gt; &lt;td&gt;    0.130&lt;/td&gt; &lt;td&gt;   39.111&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    4.826&lt;/td&gt; &lt;td&gt;    5.336&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;Now the estimate of the effect of &lt;code&gt;class size&lt;/code&gt; on &lt;code&gt;math score&lt;/code&gt; is &lt;strong&gt;unbiased&lt;/strong&gt;! Indeed, the true coefficient in the data generating process was $0.2$.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-mermaid&#34;&gt;flowchart TB
classDef included fill:#DCDCDC,stroke:#000000,stroke-width:2px;
classDef excluded fill:#ffffff,stroke:#000000,stroke-width:2px;
classDef unobserved fill:#ffffff,stroke:#000000,stroke-width:2px,stroke-dasharray: 5 5;


X((class size))
Y((math score))
Z1((class year))
Z2((good school))
Z3((math hours))
Z4((hist score))
U((ability))

X --&amp;gt; Y
Z1 --&amp;gt; X
X -.-&amp;gt; Z4
U --&amp;gt; Y
U -.-&amp;gt; Z4
Z2 -.-&amp;gt; X
Z2 -.-&amp;gt; Y
Z2 --&amp;gt; Z4
Z3 --&amp;gt; Y

linkStyle 0 stroke:#00ff00,stroke-width:4px;

class X,Y,Z2 included;
class Z1,Z3,Z4 excluded;
class U unobserved;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What would happen if we were to instead &lt;strong&gt;control for all variables&lt;/strong&gt;?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;smf.ols(&#39;math_score ~ class_size + good_school + math_hours + class_year + hist_score&#39;, df).fit().summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
       &lt;td&gt;&lt;/td&gt;          &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt;   &lt;td&gt;   -0.7847&lt;/td&gt; &lt;td&gt;    0.310&lt;/td&gt; &lt;td&gt;   -2.529&lt;/td&gt; &lt;td&gt; 0.012&lt;/td&gt; &lt;td&gt;   -1.394&lt;/td&gt; &lt;td&gt;   -0.176&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;class_size&lt;/th&gt;  &lt;td&gt;    0.1292&lt;/td&gt; &lt;td&gt;    0.010&lt;/td&gt; &lt;td&gt;   13.054&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    0.110&lt;/td&gt; &lt;td&gt;    0.149&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;good_school&lt;/th&gt; &lt;td&gt;    2.9815&lt;/td&gt; &lt;td&gt;    0.170&lt;/td&gt; &lt;td&gt;   17.533&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    2.648&lt;/td&gt; &lt;td&gt;    3.315&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;math_hours&lt;/th&gt;  &lt;td&gt;    1.0516&lt;/td&gt; &lt;td&gt;    0.048&lt;/td&gt; &lt;td&gt;   21.744&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    0.957&lt;/td&gt; &lt;td&gt;    1.147&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;class_year&lt;/th&gt;  &lt;td&gt;    0.0424&lt;/td&gt; &lt;td&gt;    0.037&lt;/td&gt; &lt;td&gt;    1.130&lt;/td&gt; &lt;td&gt; 0.259&lt;/td&gt; &lt;td&gt;   -0.031&lt;/td&gt; &lt;td&gt;    0.116&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;hist_score&lt;/th&gt;  &lt;td&gt;    0.4116&lt;/td&gt; &lt;td&gt;    0.027&lt;/td&gt; &lt;td&gt;   15.419&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    0.359&lt;/td&gt; &lt;td&gt;    0.464&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;The coefficient is again &lt;strong&gt;biased&lt;/strong&gt;. Why?&lt;/p&gt;
&lt;p&gt;We have opened a new spurious path by controlling for &lt;code&gt;hist score&lt;/code&gt;. In fact, &lt;code&gt;hist score&lt;/code&gt; is a &lt;strong&gt;collider&lt;/strong&gt; and controlling for it has opened a path through &lt;code&gt;hist score&lt;/code&gt; and &lt;code&gt;ability&lt;/code&gt; that was otherwise closed.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-mermaid&#34;&gt;flowchart TB
classDef included fill:#DCDCDC,stroke:#000000,stroke-width:2px;
classDef excluded fill:#ffffff,stroke:#000000,stroke-width:2px;
classDef unobserved fill:#ffffff,stroke:#000000,stroke-width:2px,stroke-dasharray: 5 5;


X((class size))
Y((math score))
Z1((class year))
Z2((good school))
Z3((math hours))
Z4((hist score))
U((ability))

X --&amp;gt; Y
Z1 --&amp;gt; X
X --&amp;gt; Z4
U --&amp;gt; Y
U --&amp;gt; Z4
Z2 -.-&amp;gt; X
Z2 -.-&amp;gt; Y
Z2 --&amp;gt; Z4
Z3 --&amp;gt; Y

linkStyle 0 stroke:#00ff00,stroke-width:4px;
linkStyle 2,3,4 stroke:#ff0000,stroke-width:4px;

class X,Y,Z1,Z2,Z3,Z4 included;
class U unobserved;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The example was inspired by the following tweet.&lt;/p&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;We can illustrate this with Model 16 of the &amp;quot;Crash Course in Good and Bad Controls&amp;quot; (&lt;a href=&#34;https://t.co/GcSNzhuVt2&#34;&gt;https://t.co/GcSNzhuVt2&lt;/a&gt;). Here X = class size, Y = math4, Z = read4, and U = student&amp;#39;s ability. Conditioning on Z opens the path X -&amp;gt; Z &amp;lt;- U -&amp;gt; Y and it is thus a &amp;quot;bad control.&amp;quot; &lt;a href=&#34;https://t.co/KNfqtsMWwB&#34;&gt;https://t.co/KNfqtsMWwB&lt;/a&gt; &lt;a href=&#34;https://t.co/lUSigNYSJj&#34;&gt;pic.twitter.com/lUSigNYSJj&lt;/a&gt;&lt;/p&gt;&amp;mdash; Análise Real (@analisereal) &lt;a href=&#34;https://twitter.com/analisereal/status/1502793254592401409?ref_src=twsrc%5Etfw&#34;&gt;March 12, 2022&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;

&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;In this post, we have seen how to use Directed Acyclic Graphs to select control variables in a causal analysis. DAGs are very helpful tools since they provide an intuitive graphical representation of causal relationships between random variables. Contrary to common intuition that &amp;ldquo;the more information the better&amp;rdquo;, sometimes including extra variables might bias the analysis, preventing a causal interpretation of the results. In particular, we must pay attention not to include &lt;em&gt;colliders&lt;/em&gt; that open &lt;em&gt;spurious&lt;/em&gt; paths that would otherwise be closed.&lt;/p&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;p&gt;[1] C. Cinelli, A. Forney, J. Pearl, &lt;a href=&#34;https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3689437&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;A Crash Course in Good and Bad Controls&lt;/a&gt; (2018), working paper.&lt;/p&gt;
&lt;p&gt;[2] J. Pearl, &lt;a href=&#34;http://bayes.cs.ucla.edu/BOOK-2K/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Causality&lt;/a&gt; (2009), Cambridge University Press.&lt;/p&gt;
&lt;p&gt;[3] S. Cunningham, Chapter 3 of &lt;a href=&#34;https://mixtape.scunning.com/dag.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The Causal Inference Mixtape&lt;/a&gt; (2021), Yale University Press.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Double Debiased Machine Learning (part 1)</title>
      <link>https://matteocourthoud.github.io/post/regularization_bias/</link>
      <pubDate>Mon, 10 Jul 2023 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/post/regularization_bias/</guid>
      <description>&lt;p&gt;&lt;em&gt;Causal inference, machine learning and regularization bias&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;In causal inference, we often estimate causal effects by conditioning the analysis on other variables. We usually refer to these variables as &lt;strong&gt;control variables&lt;/strong&gt; or &lt;strong&gt;confounders&lt;/strong&gt;. In randomized control trials or AB tests, conditioning can increase the power of the analysis, by reducing imbalances that have emerged despite randomization. However, conditioning is even more important in observational studies, where, absent randomization, it might be &lt;a href=&#34;https://towardsdatascience.com/b63dc69e3d8c&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;essential to recover causal effects&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;When we have many control variables, we might want to &lt;strong&gt;select the most relevant ones&lt;/strong&gt;, ppossibly capturing nonlinearities and interactions. Machine learning algorithms are perfect for this task. However, in these cases, we are introducing a bias that is called &lt;strong&gt;regularization or pre-test, or feature selection bias&lt;/strong&gt;. In this and the next blog post, I try to explain the source of the bias and a very poweful solution called &lt;strong&gt;double debiased machine learning&lt;/strong&gt;, which has been probably one of the most relevant advancement at the intersection of machine learning and causal inference of the last decade.&lt;/p&gt;
&lt;h2 id=&#34;pre-testing&#34;&gt;Pre-Testing&lt;/h2&gt;
&lt;p&gt;Since this is a complex topic, let&amp;rsquo;s start with a simple example.&lt;/p&gt;
&lt;p&gt;Suppose we were a firm and we are interested in the &lt;strong&gt;effect of advertisement spending on revenue&lt;/strong&gt;: is advertisement worth the money? There are also a lot of other things that might influence sales, therefore, we are thinking of controlling for past sales in the analysis, in order to increase the power of our analysis.&lt;/p&gt;
&lt;p&gt;Assume the data generating process can be represented with the following &lt;a href=&#34;https://towardsdatascience.com/b63dc69e3d8c&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Directed Acyclic Graph (DAG)&lt;/strong&gt;&lt;/a&gt;. If you are not familiar with DAGs, I have written a short &lt;a href=&#34;https://towardsdatascience.com/b63dc69e3d8c&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;introduction here&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-mermaid&#34;&gt;flowchart LR
classDef included fill:#DCDCDC,stroke:#000000,stroke-width:2px;
classDef excluded fill:#ffffff,stroke:#000000,stroke-width:2px;
classDef unobserved fill:#ffffff,stroke:#000000,stroke-width:2px,stroke-dasharray: 5 5;

D((ad spend))
Z((past sales))
Y((sales))

D --&amp;gt; Y
Z -- ??? --&amp;gt; Y
Z --&amp;gt; D

class D,Y included;
class Z excluded;

linkStyle 0 stroke:#00ff00,stroke-width:4px;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I import the data generating process &lt;code&gt;dgp_tbd()&lt;/code&gt; from &lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/src/dgp.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;src.dgp&lt;/code&gt;&lt;/a&gt; and some plotting functions and libraries from &lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/src/utils.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;src.utils&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;%matplotlib inline
%config InlineBackend.figure_format = &#39;retina&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from src.utils import *
from src.dgp import dgp_pretest

df = dgp_pretest().generate_data()
df.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;ads&lt;/th&gt;
      &lt;th&gt;sales&lt;/th&gt;
      &lt;th&gt;past_sales&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;16.719800&lt;/td&gt;
      &lt;td&gt;19.196620&lt;/td&gt;
      &lt;td&gt;6.624345&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;7.732222&lt;/td&gt;
      &lt;td&gt;9.287491&lt;/td&gt;
      &lt;td&gt;4.388244&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;10.923469&lt;/td&gt;
      &lt;td&gt;11.816906&lt;/td&gt;
      &lt;td&gt;4.471828&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;8.457062&lt;/td&gt;
      &lt;td&gt;9.024376&lt;/td&gt;
      &lt;td&gt;3.927031&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;13.085146&lt;/td&gt;
      &lt;td&gt;12.814823&lt;/td&gt;
      &lt;td&gt;5.865408&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;We have data on $1000$ different markets, in which we observe current &lt;code&gt;sales&lt;/code&gt;, the amount spent in &lt;code&gt;advertisement&lt;/code&gt; and &lt;code&gt;past sales&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;We want to understand &lt;code&gt;ads&lt;/code&gt; spending is effective in increasing &lt;code&gt;sales&lt;/code&gt;. One possibility is to regress the latter on the former, using the following regression model, also called the &lt;strong&gt;short model&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;$$
\text{sales} = \alpha \cdot \text{ads} + \varepsilon
$$&lt;/p&gt;
&lt;p&gt;Should we also include &lt;code&gt;past sales&lt;/code&gt; in the regression? Then the regression model would be the following, also called &lt;strong&gt;long model&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;$$
\text{sales} = \alpha \cdot \text{ads} + \beta \cdot \text{past sales} + \varepsilon
$$&lt;/p&gt;
&lt;p&gt;Since we are not sure whether to condition the analysis on &lt;code&gt;past sales&lt;/code&gt;, we could &lt;strong&gt;let the data decide&lt;/strong&gt;: we could run the second regression and, if the effect of &lt;code&gt;past sales&lt;/code&gt;, $\beta$, is statistically significant, we are good with the long model, otherwise we run the short model.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;smf.ols(&#39;sales ~ ads + past_sales&#39;, df).fit().summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
       &lt;td&gt;&lt;/td&gt;         &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt;  &lt;td&gt;    0.1405&lt;/td&gt; &lt;td&gt;    0.185&lt;/td&gt; &lt;td&gt;    0.758&lt;/td&gt; &lt;td&gt; 0.448&lt;/td&gt; &lt;td&gt;   -0.223&lt;/td&gt; &lt;td&gt;    0.504&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;ads&lt;/th&gt;        &lt;td&gt;    0.9708&lt;/td&gt; &lt;td&gt;    0.030&lt;/td&gt; &lt;td&gt;   32.545&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    0.912&lt;/td&gt; &lt;td&gt;    1.029&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;past_sales&lt;/th&gt; &lt;td&gt;    0.3381&lt;/td&gt; &lt;td&gt;    0.095&lt;/td&gt; &lt;td&gt;    3.543&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    0.151&lt;/td&gt; &lt;td&gt;    0.525&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;It seems that the effect of &lt;code&gt;past sales&lt;/code&gt; on current &lt;code&gt;sales&lt;/code&gt; is positive and significant. Therefore, we are happy with our specification and we conclude that the effect of &lt;code&gt;ads&lt;/code&gt; on &lt;code&gt;sales&lt;/code&gt; is positive and significant with a 95% confidence interval of $[0.912, 1.029]$.&lt;/p&gt;
&lt;h2 id=&#34;the-bias&#34;&gt;The Bias&lt;/h2&gt;
&lt;p&gt;There is an &lt;strong&gt;issue&lt;/strong&gt; with this procedure: we are not taking into account the fact that we have run a test to decide whether to include &lt;code&gt;past_sales&lt;/code&gt; in the regression. The fact that we have decided to include &lt;code&gt;past_sales&lt;/code&gt; because its coefficient is significant &lt;em&gt;does&lt;/em&gt; have an effect on the inference on the effect of &lt;code&gt;ads&lt;/code&gt; on &lt;code&gt;sales&lt;/code&gt;, $\alpha$.&lt;/p&gt;
&lt;p&gt;The best way to understand the problem is through &lt;strong&gt;simulations&lt;/strong&gt;. Since we have access to the data generating process &lt;code&gt;dgp_pretest()&lt;/code&gt; (unlike in real life), we can just test what would happen if we were repeating this procedure multiple times:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;We draw a new sample from the data generating process.&lt;/li&gt;
&lt;li&gt;We regress &lt;code&gt;sales&lt;/code&gt; on &lt;code&gt;ads&lt;/code&gt; and &lt;code&gt;past_sales&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;If the coefficient of &lt;code&gt;past_sales&lt;/code&gt; is significant at the 95% level, we keep $\hat \alpha_{long}$ from (2).&lt;/li&gt;
&lt;li&gt;Otherwise, we regress &lt;code&gt;sales&lt;/code&gt; on &lt;code&gt;ads&lt;/code&gt; only, and we keep that coefficient $\hat \alpha_{short}$.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;I write a &lt;code&gt;pre_test&lt;/code&gt; function to implement the procedure above. I also save the coefficients from both regressions, long and short, and the chosen one, called the &lt;strong&gt;pre-test coefficient&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Reminder&lt;/strong&gt;: we are pre-testing the effect of &lt;code&gt;past_sales&lt;/code&gt; on &lt;code&gt;sales&lt;/code&gt; but the coefficient of interest is the one of &lt;code&gt;ads&lt;/code&gt; on &lt;code&gt;sales&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def pre_testing(d=&#39;ads&#39;, y=&#39;sales&#39;, x=&#39;past_sales&#39;, K=1000, **kwargs):
    
    # Init
    alpha = {&#39;Long&#39;: np.zeros(K), &#39;Short&#39;: np.zeros(K), &#39;Pre-test&#39;: np.zeros(K)}

    # Loop over simulations
    for k in range(K):
        
        # Generate data
        df = dgp_pretest().generate_data(seed=k, **kwargs)
        
        # Compute coefficients
        alpha[&#39;Long&#39;][k] = smf.ols(f&#39;{y} ~ {d} + {x}&#39;, df).fit().params[1]
        alpha[&#39;Short&#39;][k] = smf.ols(f&#39;{y} ~ {d}&#39;, df).fit().params[1]
    
        # Compute significance of beta
        p_value = smf.ols(f&#39;{y} ~ {d} + {x}&#39;, df).fit().pvalues[2]
        
        # Select specification based on p-value
        if p_value&amp;lt;0.05:
            alpha[&#39;Pre-test&#39;][k] = alpha[&#39;Long&#39;][k]
        else:
            alpha[&#39;Pre-test&#39;][k] = alpha[&#39;Short&#39;][k]
    
    return alpha
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;alphas = pre_testing()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can now plot the distributions (over simulations) of the estimated coefficients.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def plot_alphas(alphas, true_alpha):
    
    # Init plot
    fig, axes = plt.subplots(1, len(alphas), figsize=(4*len(alphas), 5), sharey=True, sharex=True)

    # Make one plot for each set of coefficients
    for i, key in enumerate(alphas.keys()):
        axes[i].hist(alphas[key], bins=30, lw=.1)
        axes[i].set_title(key)
        axes[i].axvline(true_alpha, c=&#39;r&#39;, ls=&#39;--&#39;)
        legend_text = [r&#39;$\alpha=%.0f$&#39; % true_alpha, r&#39;$\hat \alpha=%.4f$&#39; % np.mean(alphas[key])]
        axes[i].legend(legend_text, prop={&#39;size&#39;: 10}, loc=&#39;upper right&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plot_alphas(alphas, true_alpha=1)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/regularization_bias_16_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;In the plot above, I have depicted the estimated coefficients, across simulations, for the different regression specifications.&lt;/p&gt;
&lt;p&gt;As we can see from the first plot, if we were always running the &lt;strong&gt;long regression&lt;/strong&gt;, our estimator $\hat \alpha_{long}$ would be unbiased and normally distributed. However, if we were always running the &lt;strong&gt;short regression&lt;/strong&gt; (second plot), our estimator $\hat \alpha_{short}$ would be &lt;strong&gt;biased&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;pre-testing&lt;/strong&gt; procedure generates an estimator $\hat \alpha_{pretest}$ that is a mix of the two: most of the times we select the correct specification, the long regression, but sometimes the pre-test fails to reject the null hypothesis of no effect of &lt;code&gt;past sales&lt;/code&gt; on &lt;code&gt;sales&lt;/code&gt;, $H_0 : \beta = 0$, and we select the incorrect specification, running the short regression.&lt;/p&gt;
&lt;p&gt;Importantly, the pre-testing procedure &lt;strong&gt;does not generate a biased estimator&lt;/strong&gt;. As we can see in the last plot, the estimated coefficient is very close to the true value, 1. The reason is that most of the time, the number of times we select the &lt;em&gt;short&lt;/em&gt; regression is sufficiently small not to introduce bias, but not small enough to have valid inference.&lt;/p&gt;
&lt;p&gt;Indeed, &lt;strong&gt;pre-testing distorts inference&lt;/strong&gt;: the distribution of the estimator $\hat \alpha_{pretest}$ is not normal anymore, but bimodal. The &lt;strong&gt;consequence&lt;/strong&gt; is that our confidence intervals for $\alpha$ are going to have the wrong coverage (contain the true effect with a different probability than the claimed one).&lt;/p&gt;
&lt;h2 id=&#34;when-is-pre-testing-a-problem&#34;&gt;When is pre-testing a problem?&lt;/h2&gt;
&lt;p&gt;The problem of pre-testing arises because of the bias generated by running the short regression: &lt;a href=&#34;https://towardsdatascience.com/344ac1477699&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;omitted variable bias (OVB)&lt;/strong&gt;&lt;/a&gt;. In you are not familiar with OVB, I have written a &lt;a href=&#34;https://towardsdatascience.com/344ac1477699&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;short introduction here&lt;/a&gt;. In general however, we can express the omitted variable bias introduced by regressing $Y$ on $D$ ignoring $X$ as&lt;/p&gt;
&lt;p&gt;$$
\text{OVB} = \beta \delta \qquad \text{ where } \qquad \beta := \frac{Cov(X, Y)}{Var(X)}, \quad \delta := \frac{Cov(D, X)}{Var(D)}
$$&lt;/p&gt;
&lt;p&gt;Where $\beta$ is the effect of $X$ (&lt;code&gt;past sales&lt;/code&gt; in our example) on $Y$ (&lt;code&gt;sales&lt;/code&gt;) and $\delta$ is the effect of $D$ (&lt;code&gt;ads&lt;/code&gt;) on $X$.&lt;/p&gt;
&lt;p&gt;Pre-testing is a &lt;strong&gt;problem&lt;/strong&gt; if&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;We run the short regression instead of the long one &lt;em&gt;and&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;The effect of the bias is sensible&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;What can help improving (1), i.e. the probability of correctly rejecting the null hypothesis of zero effect of &lt;code&gt;past sales&lt;/code&gt;, $H_0 : \beta = 0$? The answer is simple: a &lt;strong&gt;bigger sample size&lt;/strong&gt;. If we have more observations, we can more precisely estimate $\beta$ and it is going to be less likely that we commit a &lt;a href=&#34;https://en.wikipedia.org/wiki/Type_I_and_type_II_errors&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;type 2 error&lt;/a&gt; and run the short regression instead of the long one.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s simulate the estimated coefficient $\hat \alpha$ under different sample sizes. Remember that the sample size used until now is $N=1000$.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;Ns = [100,300,1000,3000]
alphas = {f&#39;N = {n:.0f}&#39;:  pre_testing(N=n)[&#39;Pre-test&#39;] for n in Ns}
plot_alphas(alphas, true_alpha=1)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/regularization_bias_20_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;As we can see from the plots, as the sample size increases (left to right), the bias decreases and the distribution of the estimator $\hat \alpha_{pretest}$ converges to a normal distribution.&lt;/p&gt;
&lt;p&gt;What happens instead if the value of $\beta$ was different? It is probably going to affect point (2) in the previous paragraph, but how?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;If $\beta$ is &lt;strong&gt;very small&lt;/strong&gt;, it is going to be hard to detect it, and we will often end up running the &lt;em&gt;short&lt;/em&gt; regression, introducing a bias. However, if $\beta$ is very small, it also implies that the &lt;strong&gt;magnitude of the bias&lt;/strong&gt; is small and therefore it is not going to affect our estimate of $\alpha$ much&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;If $\beta$ is &lt;strong&gt;very big&lt;/strong&gt;, it is going to be easy to detect and we will often end up running the &lt;em&gt;long&lt;/em&gt; regression, avoiding the bias (which would have been very big though).&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Let&amp;rsquo;s simulate the estimated coefficient $\hat \alpha$ under different values of $\beta$. The true value used until now was $\beta = 0.3$.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;betas = 0.3 * np.array([0.1,0.3,1,3])
alphas = {f&#39;beta = {b:.2f}&#39;:  pre_testing(b=b)[&#39;Pre-test&#39;] for b in betas}
plot_alphas(alphas, true_alpha=1)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/regularization_bias_22_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;As we can see from the plots, as the value of $\beta$ increases, the bias first appears and then disappears. When $\beta$ is small (left plot), we often choose the short regression, but the bias is small and the average estimate is very close to the true value. For intermediate values of $\beta$, the bias is sensible and it has a clear effect on inference. Lastly, for large values of $\beta$ instead (right plot), we always run the long regression and the bias disappears.&lt;/p&gt;
&lt;p&gt;But &lt;strong&gt;when is a coefficient big or small&lt;/strong&gt;? And big or small with respect to what? The answer is simple: with respect to the &lt;strong&gt;sample size&lt;/strong&gt;, or more accurately, with respect to the inverse of the square root of the sample size, $1 / \sqrt{n}$. The reason is deeply rooted in the &lt;a href=&#34;https://en.wikipedia.org/wiki/Central_limit_theorem&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Central Limit Theorem&lt;/a&gt;, but I won&amp;rsquo;t cover it here.&lt;/p&gt;
&lt;p&gt;The idea is easier to show than to explain, so let&amp;rsquo;s repeat the same simulation as above, but now we will increase both the coefficient and the sample size at the same time.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;betas = 0.3 * 30 / np.sqrt(Ns)
alphas = {f&#39;N = {n:.0f}&#39;:  pre_testing(b=b, N=n)[&#39;Pre-test&#39;] for n,b in zip(Ns,betas)}
plot_alphas(alphas, true_alpha=1)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/regularization_bias_24_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;As we can see, now that $\beta$ is proportional to $1 / \sqrt{n}$, the distortion is not going away, not matter the sample size. Therefore, inference will always be wrong.&lt;/p&gt;
&lt;p&gt;While a coefficient that depends on the sample size might sound &lt;strong&gt;not intuitive&lt;/strong&gt;, it captures well the idea of &lt;strong&gt;magnitude&lt;/strong&gt; in a world where we do inference relying on asymptotic results, first among all the &lt;a href=&#34;https://en.wikipedia.org/wiki/Central_limit_theorem&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Central Limit Theorem&lt;/a&gt;. In fact, the Central Limit Theorem relieas on an infinitely large sample size. However, with an infinite amount of data, no coefficient is small and any non-zero effect is detected with certainty.&lt;/p&gt;
&lt;h2 id=&#34;pre-testing-and-machine-learning&#34;&gt;Pre-Testing and Machine Learning&lt;/h2&gt;
&lt;p&gt;So far we talked about a linear regression with only 2 variables. Where is the &lt;strong&gt;machine learning&lt;/strong&gt; we were promised?&lt;/p&gt;
&lt;p&gt;Usually we do not have just one control variable (or confounder), but many. Moreover, we might want to be flexible with respect to the functional form through which these control variables enter the model. In general, we will assume the following model:&lt;/p&gt;
&lt;p&gt;$$
Y = \alpha D + g_0(X) + u
\newline
D = m_0(X) + v
$$&lt;/p&gt;
&lt;p&gt;Where the effect of interest is still $\alpha$, $X$ is potentially high dimensional and we do not take a stand on the functional form through which $X$ influences $D$ or $Y$.&lt;/p&gt;
&lt;p&gt;In this setting, it is natural to use a machine learning algorithm to estimate $g_0$ and $m_0$. However, machine learning algorithms usually introduce a &lt;strong&gt;regularization bias&lt;/strong&gt; that is comparable to pre-testing.&lt;/p&gt;
&lt;p&gt;Possibly, the &amp;ldquo;simplest&amp;rdquo; way to think about it is &lt;a href=&#34;https://en.wikipedia.org/wiki/Lasso_%28statistics%29&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Lasso&lt;/a&gt;. Lasso is linear in $X$, with a penalization term that effectively just performs the variable selection we discussed above. Therefore, if we were to use Lasso of $X$ and $D$ on $Y$ we would be introducing regularization bias and inference would be distorted. The same goes for more complex algorithms.&lt;/p&gt;
&lt;p&gt;Lastly, you might still wonder &amp;ldquo;why is the model linear in the treatment variable $D$?&amp;rdquo;. Doing inference is much easier in linear model, not only for computational reasons but also for interpretation. Moreover, if the treatment $D$ is binary, the linear functional form is without loss of generality. A stronger assumption is the additive separability of $D$ and $g(X)$.&lt;/p&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;In this post, I have tried to explain how does regularization bias emerges and why it can the an issue in causal inference. This problem is inherently related to settings with many control variables or where we would like to have a model-free (i.e. non-parametric) when controlling for confounders. These are exactly the settings in which machine learning algorithms can be useful.&lt;/p&gt;
&lt;p&gt;In the next post, I will cover a simple and yet incredibly powerful solution to this problem: double-debiased machine learning.&lt;/p&gt;
&lt;h3 id=&#34;references&#34;&gt;References&lt;/h3&gt;
&lt;p&gt;[1] A. Belloni, D. Chen, V. Chernozhukov, C. Hansen, &lt;a href=&#34;https://onlinelibrary.wiley.com/doi/abs/10.3982/ECTA9626&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Sparse Models and Methods for Optimal Instruments With an Application to Eminent Domain&lt;/a&gt; (2012), &lt;em&gt;Econometrica&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;[2] A. Belloni, V. Chernozhukov, C. Hansen, &lt;a href=&#34;https://academic.oup.com/restud/article-abstract/81/2/608/1523757&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Inference on treatment effects after selection among high-dimensional controls&lt;/a&gt; (2014), &lt;em&gt;The Review of Economic Studies&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;[3] V. Chernozhukov, D. Chetverikov, M. Demirer, E. Duflo, C. Hansen, W. Newey, J. Robins, &lt;a href=&#34;https://academic.oup.com/ectj/article/21/1/C1/5056401&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Double/debiased machine learning for treatment and structural parameters&lt;/a&gt; (2018), &lt;em&gt;The Econometrics Journal&lt;/em&gt;.&lt;/p&gt;
&lt;h3 id=&#34;related-articles&#34;&gt;Related Articles&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/344ac1477699&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Understanding Omitted Variable Bias&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/59f801eb3299&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Understanding The Frisch-Waugh-Lovell Theorem&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/b63dc69e3d8c&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;DAGs and Control Variables&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;code&#34;&gt;Code&lt;/h3&gt;
&lt;p&gt;You can find the original Jupyter Notebook here:&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/pretest.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/matteocourthoud/Blog-Posts/blob/main/pretest.ipynb&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Double Debiased Machine Learning (part 2)</title>
      <link>https://matteocourthoud.github.io/post/double_ml/</link>
      <pubDate>Mon, 10 Jul 2023 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/post/double_ml/</guid>
      <description>&lt;p&gt;In the &lt;a href=&#34;https://towardsdatascience.com/eb767a59975b&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;previous part of this blog post&lt;/a&gt;, we have seen how pre-testing can distort inference, i.e., how selecting control variables depending on their statistical significance results in wrong confidence intervals for the variable of interest. This bias is generally called &lt;strong&gt;regularization bias&lt;/strong&gt; and also emerges in machine learning algorithms.&lt;/p&gt;
&lt;p&gt;In blog post, we are going to explore a solution to the simple selection example, &lt;strong&gt;post-double selection&lt;/strong&gt;, and a more general approach when we have many control variables and we do not want to assume linearity, &lt;strong&gt;double-debiased machine learning&lt;/strong&gt;.&lt;/p&gt;
&lt;h2 id=&#34;recap&#34;&gt;Recap&lt;/h2&gt;
&lt;p&gt;To better understand the source of the bias, in the first part of this post, we have explored the example of a firm that is interested in testing the effectiveness of an a campaign. The firm has information on its current ad spending and on the level of sales. The problem arises because the firm is uncertain on whether it should condition its analysis on the level of past sales.&lt;/p&gt;
&lt;p&gt;The following &lt;a href=&#34;https://towardsdatascience.com/b63dc69e3d8c&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Directed Acyclic Graph (DAG)&lt;/strong&gt;&lt;/a&gt; summarizes the data generating process.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-mermaid&#34;&gt;flowchart LR
classDef included fill:#DCDCDC,stroke:#000000,stroke-width:2px;
classDef excluded fill:#ffffff,stroke:#000000,stroke-width:2px;
classDef unobserved fill:#ffffff,stroke:#000000,stroke-width:2px,stroke-dasharray: 5 5;

D((ad spend))
Z((past sales))
Y((sales))

D --&amp;gt; Y
Z -- ??? --&amp;gt; Y
Z --&amp;gt; D

class D,Y included;
class Z excluded;

linkStyle 0 stroke:#00ff00,stroke-width:4px;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I import the data generating process &lt;code&gt;dgp_tbd()&lt;/code&gt; from &lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/src/dgp.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;src.dgp&lt;/code&gt;&lt;/a&gt; and some plotting functions and libraries from &lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/src/utils.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;src.utils&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;%matplotlib inline
%config InlineBackend.figure_format = &#39;retina&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from src.utils import *
from src.dgp import dgp_pretest

df = dgp_pretest().generate_data()
df.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;ads&lt;/th&gt;
      &lt;th&gt;sales&lt;/th&gt;
      &lt;th&gt;past_sales&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;16.719800&lt;/td&gt;
      &lt;td&gt;19.196620&lt;/td&gt;
      &lt;td&gt;6.624345&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;7.732222&lt;/td&gt;
      &lt;td&gt;9.287491&lt;/td&gt;
      &lt;td&gt;4.388244&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;10.923469&lt;/td&gt;
      &lt;td&gt;11.816906&lt;/td&gt;
      &lt;td&gt;4.471828&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;8.457062&lt;/td&gt;
      &lt;td&gt;9.024376&lt;/td&gt;
      &lt;td&gt;3.927031&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;13.085146&lt;/td&gt;
      &lt;td&gt;12.814823&lt;/td&gt;
      &lt;td&gt;5.865408&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;We have data on $1000$ different markets, in which we observe current &lt;code&gt;sales&lt;/code&gt;, the amount spent in &lt;code&gt;advertisement&lt;/code&gt; and &lt;code&gt;past sales&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;We want to understand &lt;code&gt;ads&lt;/code&gt; spending is effective in increasing &lt;code&gt;sales&lt;/code&gt;. One possibility is to regress the latter on the former, using the following regression model, also called the &lt;strong&gt;short model&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;$$
\text{sales} = \alpha \cdot \text{ads} + \varepsilon
$$&lt;/p&gt;
&lt;p&gt;Should we also include &lt;code&gt;past sales&lt;/code&gt; in the regression? Then the regression model would be the following, also called &lt;strong&gt;long model&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;$$
\text{sales} = \alpha \cdot \text{ads} + \beta \cdot \text{past sales} + \varepsilon
$$&lt;/p&gt;
&lt;p&gt;One naive approach would be to &lt;strong&gt;let the data decide&lt;/strong&gt;: we could run the second regression and, if the effect of &lt;code&gt;past sales&lt;/code&gt;, $\beta$, is statistically significant, we are good with the long model, otherwise we run the short model. This procedure is called &lt;strong&gt;pre-testing&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;The problem with this procedure is that it introduces a bias that is called &lt;strong&gt;regularization or pre-test bias&lt;/strong&gt;. Pre-testing ensures that this bias is small enough not to distort the estimated coefficient. However, it does not ensure that it is small enough not to distort the confidence intervals around the estimated coefficient.&lt;/p&gt;
&lt;p&gt;Is there a solution? Yes!&lt;/p&gt;
&lt;h2 id=&#34;post-double-selection&#34;&gt;Post-Double Selection&lt;/h2&gt;
&lt;p&gt;The solution is called &lt;strong&gt;post-double selection&lt;/strong&gt;. The method was first introduced in &lt;a href=&#34;https://academic.oup.com/restud/article-abstract/81/2/608/1523757&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Belloni, Chernozhukov, Hansen (2014)&lt;/a&gt; and later expanded in a variety of papers.&lt;/p&gt;
&lt;p&gt;The authors assume the following &lt;strong&gt;data generating process&lt;/strong&gt;:&lt;/p&gt;
&lt;p&gt;$$
y = \alpha D + \beta X + u
\newline
D = \delta X + v
$$&lt;/p&gt;
&lt;p&gt;In our example, $Y$ corresponds to &lt;code&gt;sales&lt;/code&gt;, $D$ corresponds to &lt;code&gt;ads&lt;/code&gt;, $X$ corresponds to &lt;code&gt;past_sales&lt;/code&gt; and the effect of interest is $\alpha$. In our example, $X$ is 1-dimensional for simplicity, but generally we are interested in cases where X is high-dimensional, potentially even having more dimensions than the number of observations. In that case, variable selection is &lt;strong&gt;essential&lt;/strong&gt; in linear regression since we cannot have more features than variables (the OLS coefficients are not uniquely determined anymore).&lt;/p&gt;
&lt;p&gt;Post-double selection consists in the following procedure.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Reduced Form&lt;/strong&gt; selection: lasso $Y$ on $X$. Select the statistically significant variables in the set $S_{RF} \subseteq X$&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;First Stage&lt;/strong&gt; selection: regress $D$ on $X$. Select the statistically significant variables in the set $S_{FS} \subseteq X$&lt;/li&gt;
&lt;li&gt;Regress $Y$ on $D$ and the &lt;strong&gt;union&lt;/strong&gt; of the selected variables in the first two steps, $S_{FS} \cup S_{RF}$&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The authors show that this procedure produces confidence intervals for the coefficient of interest $\alpha$ that have the correct coverage, i.e. the correct probability of type 1 error.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Note (1)&lt;/strong&gt;: this procedure is always less parsimonious, in terms of variable selection, than pre-testing. In fact, we still select all the variables we would have selected with pre-testing but, in the first stage, we might select additional variables.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Note (2)&lt;/strong&gt;: the terms &lt;em&gt;first stage&lt;/em&gt; and &lt;em&gt;reduced form&lt;/em&gt; come from the &lt;a href=&#34;https://en.wikipedia.org/wiki/Instrumental_variables_estimation&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;intrumental variables&lt;/a&gt; literature in econometrics. Indeed, the first application of post-double selection was to select instrumental variables in &lt;a href=&#34;https://onlinelibrary.wiley.com/doi/abs/10.3982/ECTA9626&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Belloni, Chen, Chernozhukov, Hansen (2012)&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Note (3)&lt;/strong&gt;: the name post-double selection comes from the fact that now we are not performing variable selection once but &lt;em&gt;twice&lt;/em&gt;.&lt;/p&gt;
&lt;h3 id=&#34;intuition&#34;&gt;Intuition&lt;/h3&gt;
&lt;p&gt;The idea behind post-double selection is: bound the &lt;a href=&#34;https://towardsdatascience.com/344ac1477699&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;omitted variables bias&lt;/a&gt;. In case you are not familiar with it, I wrote a separate &lt;a href=&#34;https://towardsdatascience.com/344ac1477699&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;blog post on omitted variable bias&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;In our setting, we can express the omitted variable bias as&lt;/p&gt;
&lt;p&gt;$$
\text{OVB} = \beta \delta \qquad \text{ where } \qquad \beta := \frac{Cov(X, Y)}{Var(X)}, \quad \delta := \frac{Cov(D, X)}{Var(D)}
$$&lt;/p&gt;
&lt;p&gt;As we can see, the omitted variable bias comes from the product of two quantities related to the omitted variable $X$:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Its partial correlation with the outcome $Y$, $\beta$&lt;/li&gt;
&lt;li&gt;Its partial correlation with the variable of interest $D$, $\delta$&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;With pre-testing, we ensure that the partial correlation between $X$ the outcome $Y$, $\beta$, is &lt;strong&gt;small&lt;/strong&gt;. In fact, we omit $Z$ when we shouldn&amp;rsquo;t (i.e. we commit a type 2 error) rarely. What do &lt;em&gt;small&lt;/em&gt; and &lt;em&gt;rarely&lt;/em&gt; mean?&lt;/p&gt;
&lt;p&gt;When we are selecting a variable because of its significance, we ensure that it dimension is smaller than $\frac{c}{\sqrt{n}}$ for some number $c$, where $n$ is the sample size.&lt;/p&gt;
&lt;p&gt;Therefore, with pre-testing, we ensure that, no matter what the value of $\delta$ is, the dimension of the bias is smaller than $\frac{c}{\sqrt{n}}$ which means that it converges to zero for sufficiently large $n$. This is why the pre-testing estimator is still &lt;strong&gt;consistent&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;However, in order for our confidence intervals to have the right coverage, this is &lt;strong&gt;not enough&lt;/strong&gt;. In practice, we need the bias to converge to zero &lt;strong&gt;faster&lt;/strong&gt; than $\frac{1}{\sqrt{n}}$. Why?&lt;/p&gt;
&lt;p&gt;To get an &lt;strong&gt;intuition&lt;/strong&gt; for this result, we need to turn to the &lt;a href=&#34;https://en.wikipedia.org/wiki/Central_limit_theorem&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Central Limit Theorem&lt;/a&gt;. The CLT tells us that for large $n$ the distribution of the sample average of a random variable $X$ converges to a normal distribution with mean $\mu$ and standard deviation $\frac{\sigma}{\sqrt{n}}$, where $\mu$ and $\sigma$ are the mean and standard deviation of $X$. To do inference, we usually apply the Central Limit Theorem to our estimator to get its asymptotic distribution, which in turn allows us to build confidence intervals (using the mean and the standard deviation). Therefore, if the bias is not sensibly smaller than the standard deviation of the estimator, the confidence intervals are going to be wrong. Therefore, we need the bias to converge to zero &lt;strong&gt;faster&lt;/strong&gt; than the standard deviation, i.e. faster than $\frac{1}{\sqrt{n}}$.&lt;/p&gt;
&lt;p&gt;In our setting, the omitted variable bias is $\beta \gamma$ and we want it to converge to zero faster than $\frac{1}{\sqrt{n}}$.  Post-double selection guarantees that&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Reduced form&lt;/em&gt; selection (pre-testing): any &amp;ldquo;missing&amp;rdquo; variable $j$ has $|\beta_j| \leq \frac{c}{\sqrt{n}}$&lt;/li&gt;
&lt;li&gt;&lt;em&gt;First stage&lt;/em&gt; selection (additional): any &amp;ldquo;missing&amp;rdquo; variable $j$ has $|\delta_j| \leq \frac{c}{\sqrt{n}}$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;As a consequence, as long as the number of omitted variables is finite, the omitted variable bias is going to converge to zero at a rate $\frac{1}{n}$, which is faster than $\frac{1}{\sqrt{n}}$. &lt;strong&gt;Problem solved&lt;/strong&gt;!&lt;/p&gt;
&lt;h3 id=&#34;application&#34;&gt;Application&lt;/h3&gt;
&lt;p&gt;Let&amp;rsquo;s now go back to our example and test the post-double selection procedure. In practice, we want to do the following:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;First Stage&lt;/strong&gt; selection: regress &lt;code&gt;ads&lt;/code&gt; on &lt;code&gt;past_sales&lt;/code&gt;. Check if &lt;code&gt;past_sales&lt;/code&gt; is statistically significant&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Reduced Form&lt;/strong&gt; selection: regress &lt;code&gt;sales&lt;/code&gt; on &lt;code&gt;past_sales&lt;/code&gt;. Check if &lt;code&gt;past_sales&lt;/code&gt; is statistically significant&lt;/li&gt;
&lt;li&gt;Regress &lt;code&gt;sales&lt;/code&gt; on &lt;code&gt;ads&lt;/code&gt; and include &lt;code&gt;past_sales&lt;/code&gt; &lt;strong&gt;only if&lt;/strong&gt; it was significant in &lt;em&gt;either&lt;/em&gt; one of the two previous regressions&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;I update the &lt;code&gt;pre_test&lt;/code&gt; function from the first part of the post to compute also the post-double selection estimator.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def pre_test(d=&#39;ads&#39;, y=&#39;sales&#39;, x=&#39;past_sales&#39;, K=1000, **kwargs):
    
    # Init
    alphas = pd.DataFrame({&#39;Long&#39;: np.zeros(K), 
             &#39;Short&#39;: np.zeros(K), 
             &#39;Pre-test&#39;: np.zeros(K),
             &#39;Post-double&#39;: np.zeros(K)})

    # Loop over simulations
    for k in range(K):
        
        # Generate data
        df = dgp_pretest().generate_data(seed=k, **kwargs)
        
        # Compute coefficients
        alphas[&#39;Long&#39;][k] = smf.ols(f&#39;{y} ~ {d} + {x}&#39;, df).fit().params[1]
        alphas[&#39;Short&#39;][k] = smf.ols(f&#39;{y} ~ {d}&#39;, df).fit().params[1]
    
        # Compute significance of beta and gamma
        p_value_ydx = smf.ols(f&#39;{y} ~ {d} + {x}&#39;, df).fit().pvalues[2]
        p_value_yx = smf.ols(f&#39;{y} ~ {x}&#39;, df).fit().pvalues[1]
        p_value_dx = smf.ols(f&#39;{d} ~ {x}&#39;, df).fit().pvalues[1]
        
        # Select pre-test specification based on regression of y on d and x
        if p_value_ydx&amp;lt;0.05:
            alphas[&#39;Pre-test&#39;][k] = alphas[&#39;Long&#39;][k]
        else:
            alphas[&#39;Pre-test&#39;][k] = alphas[&#39;Short&#39;][k]
            
        # Select post-double specification based on regression of y on d and x
        if p_value_yx&amp;lt;0.05 or p_value_dx&amp;lt;0.05:
            alphas[&#39;Post-double&#39;][k] = alphas[&#39;Long&#39;][k]
        else:
            alphas[&#39;Post-double&#39;][k] = alphas[&#39;Short&#39;][k]
    
    return alphas
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;alphas = pre_test()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can now plot the distributions (over simulations) of the estimated coefficients.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def plot_alphas(alphas, true_alpha):
    
    # Init plot
    K = len(alphas.columns)
    fig, axes = plt.subplots(1, K, figsize=(4*K, 5), sharey=True, sharex=True)

    # Make one plot for each set of coefficients
    for i, key in enumerate(alphas.columns):
        axes[i].hist(alphas[key].values, bins=30, lw=.1, color=f&#39;C{int(i==3)*2}&#39;)
        axes[i].set_title(key)
        axes[i].axvline(true_alpha, c=&#39;r&#39;, ls=&#39;--&#39;)
        legend_text = [rf&#39;$\alpha=${true_alpha}&#39;, rf&#39;$\hat \alpha=${np.mean(alphas[key]):.4f}&#39;]
        axes[i].legend(legend_text, prop={&#39;size&#39;: 10}, loc=&#39;upper right&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plot_alphas(alphas, true_alpha=1)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/double_ml_17_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;As we can see, the post-double selection estimator always correctly selects the long regression and therefore has the correct distribution.&lt;/p&gt;
&lt;h3 id=&#34;double-checks&#34;&gt;Double-checks&lt;/h3&gt;
&lt;p&gt;In the last post, we ran some simulations in order to investigate when pre-testing bias emerges. We saw that pre-testing is a problem for&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Small sample sizes $n$&lt;/li&gt;
&lt;li&gt;Intermediate values of $\beta$&lt;/li&gt;
&lt;li&gt;When the value of $\beta$ depends on the sample size&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Let&amp;rsquo;s check that post-double selection removes regularization bias in &lt;strong&gt;all&lt;/strong&gt; the previous cases.&lt;/p&gt;
&lt;p&gt;First, let&amp;rsquo;s simulate the distribution of the post-double selection estimator $\hat \alpha_{postdouble}$ for different sample sizes.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;Ns = [100,300,1000,3000]
alphas = {f&#39;N = {n:.0f}&#39;:  pre_test(N=n) for n in Ns}
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def compare_alphas(alphas, true_alpha):
    
    # Init plot
    fig, axes = plt.subplots(1, len(alphas), figsize=(4*len(alphas), 5), sharey=True, sharex=True)

    # Make one plot for each set of coefficients
    for i, key in enumerate(alphas.keys()):
        axes[i].hist(alphas[key][&#39;Pre-test&#39;], bins=30, lw=.1, alpha=0.5)
        axes[i].hist(alphas[key][&#39;Post-double&#39;], bins=30, lw=.1, alpha=0.5, color=&#39;C2&#39;)
        axes[i].set_title(key)
        axes[i].axvline(true_alpha, c=&#39;r&#39;, ls=&#39;--&#39;)
        axes[i].legend([rf&#39;$\alpha=${true_alpha}&#39;, &#39;Pre-test&#39;, &#39;Post-double&#39;], 
                       prop={&#39;size&#39;: 10}, loc=&#39;upper right&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;compare_alphas(alphas, true_alpha=1)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/double_ml_23_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;For small samples, the distribution of the pre-testing estimator is not normal but rather bimodal. From the plots we can see that the post-double estimator is gaussian also in small sample sizes.&lt;/p&gt;
&lt;p&gt;Now we repeat the same exercise, but for different values of $\beta$, the coefficient of &lt;code&gt;past_sales&lt;/code&gt; on &lt;code&gt;sales&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;betas = 0.3 * np.array([0.1,0.3,1,3])
alphas = {f&#39;beta = {b:.2f}&#39;: pre_test(b=b) for b in betas}
compare_alphas(alphas, true_alpha=1)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/double_ml_25_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Again, the post-double selection estimator has a gaussian distribution irrespectively of the value of $\beta$, while he pre-testing estimator suffers from regularization bias.&lt;/p&gt;
&lt;p&gt;For the last simulation, we change both the coefficient and the sample size at the same time.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;betas = 0.3 * 30 / np.sqrt(Ns)
alphas = {f&#39;N = {n:.0f}&#39;:  pre_test(b=b, N=n) for n,b in zip(Ns,betas)}
compare_alphas(alphas, true_alpha=1)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/double_ml_27_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Also in this last case, the post-double selection estimator performs well and inference is not distorted.&lt;/p&gt;
&lt;h2 id=&#34;double-debiased-machine-learning&#34;&gt;Double Debiased Machine Learning&lt;/h2&gt;
&lt;p&gt;So far, we only have analyzed a linear, univariate example. What happens if the dimension of $X$ increases and we do not know the functional form through which $X$ affects $Y$ and $D$? In these cases, we can use machine learning algorithms to uncover these high-dimensional non-linear relationships.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://academic.oup.com/ectj/article/21/1/C1/5056401&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Chernozhukov, Chetverikov, Demirer, Duflo, Hansen, Newey, and Robins (2018)&lt;/a&gt; investigate this setting. In particular, the authors consider the following partially linear model.&lt;/p&gt;
&lt;p&gt;$$
Y = \alpha D + g(X) + u \
D = m(X) + v
$$&lt;/p&gt;
&lt;p&gt;where $Y$ is the outcome variable, $D$ is the treatment to interest and $X$ is a potentially high-dimensional set of control variables.&lt;/p&gt;
&lt;h3 id=&#34;naive-approach&#34;&gt;Naive approach&lt;/h3&gt;
&lt;p&gt;A naive approach to estimation of $\alpha$ using machine learning methods would be, for example, to construct a sophisticated machine learning estimator for learning the regression function $\alpha D$ + $g(X)$.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Split the sample in two: main sample and auxiliary sample [why? see note below]&lt;/li&gt;
&lt;li&gt;Use the auxiliary sample to estimate $\hat g(X)$&lt;/li&gt;
&lt;li&gt;Use the main sample to compute the orthogonalized component of $Y$ on $X$: $\ \hat u = Y - \hat{g} (X)$&lt;/li&gt;
&lt;li&gt;Use the main sample to estimate the residualized OLS estimator from regressing $\hat u$ on $D$&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;$$
\hat \alpha = \left( D&amp;rsquo; D \right) ^{-1} D&amp;rsquo; \hat u
$$&lt;/p&gt;
&lt;p&gt;This estimator is going to have &lt;strong&gt;two problems&lt;/strong&gt;:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Slow rate of convergence, i.e. slower than $\sqrt(n)$&lt;/li&gt;
&lt;li&gt;It will be biased because we are employing high dimensional regularized estimators (e.g. we are doing variable selection)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;Note (1)&lt;/strong&gt;: so far we have not talked about it, but variable selection procedure also introduce another type of bias: &lt;strong&gt;overfitting bias&lt;/strong&gt;. This bias emerges because of the fact that the sample used to select the variables is the same that is used to estimate the coefficient of interest. This bias is &lt;strong&gt;easily accounted for&lt;/strong&gt; with sample splitting: using different sub-samples for the selection and the estimation procedures.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Note (2)&lt;/strong&gt;: why can we use the residuals from step 3 to estimate $\alpha$ in step 4? Because of the &lt;a href=&#34;https://towardsdatascience.com/59f801eb3299&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Frisch-Waugh-Lovell theorem&lt;/a&gt;. If you are not familiar with it, I have written a &lt;a href=&#34;https://towardsdatascience.com/59f801eb3299&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;blog post on the Frisch-Waugh-Lovell theorem here&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;orthogonalization&#34;&gt;Orthogonalization&lt;/h3&gt;
&lt;p&gt;Now consider a second construction that employs an orthogonalized formulation obtained by directly partialling out the effect of $X$ from $D$ to obtain the orthogonalized regressor $v = D − m(X)$.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Split the sample in two: main sample and auxiliary sample&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Use the auxiliary sample to estimate $\hat g(X)$ from&lt;/p&gt;
&lt;p&gt;$$
Y = \alpha D + g(X) + u \
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Use the auxiliary sample to estimate $\hat m(X)$ from&lt;/p&gt;
&lt;p&gt;$$
D = m(X) + v
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Use the main sample to compute the orthogonalized component of $D$ on $X$ as&lt;/p&gt;
&lt;p&gt;$$
\hat v = D - \hat m(X)
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Use the main sample to estimate the double-residualized OLS estimator as&lt;/p&gt;
&lt;p&gt;$$
\hat \alpha = \left( \hat{v}&amp;rsquo; D \right) ^{-1} \hat{v}&amp;rsquo; \left( Y - \hat g(X) \right)
$$&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The estimator is &lt;strong&gt;root-N consistent&lt;/strong&gt;! This means that not only the estimator converges to the true value as the sample sizes increases (i.e. it&amp;rsquo;s consistent), but also its standard deviation does (i.e. it&amp;rsquo;s root-N consistent).&lt;/p&gt;
&lt;p&gt;However, the estimator still has a lower rate of convergence because of sample splitting. The problem is solved by inverting the split sample, re-estimating the coefficient and averaging the two estimates. Note that this procedure is valid since the two estimates are independent by the sample splitting procedure.&lt;/p&gt;
&lt;h3 id=&#34;a-cautionary-tale&#34;&gt;A Cautionary Tale&lt;/h3&gt;
&lt;p&gt;Before we conclude, I have to mention a recent research paper by &lt;a href=&#34;https://arxiv.org/abs/2108.11294&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Hünermund, Louw, and Caspi (2022)&lt;/a&gt;, in which the authors show that double-debiased machine learning can easily &lt;strong&gt;backfire&lt;/strong&gt;, if we apply blindly.&lt;/p&gt;
&lt;p&gt;The problem is related to &lt;strong&gt;bad control variables&lt;/strong&gt;. If you have never heard this term, I have written an introductory &lt;a href=&#34;https://towardsdatascience.com/b63dc69e3d8c&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;blog post on good and bad control variables here&lt;/a&gt;. In short, conditioning the analysis on additional features is not always good for causal inference. Depending on the setting, there might exist variables that we want to leave out of our analysis since their &lt;strong&gt;inclusion&lt;/strong&gt; can bias the coefficient of interest, preventing a causal interpretation. The simplest example is variables that are common outcomes, of both the treatment $D$ and outcome variable $Y$.&lt;/p&gt;
&lt;p&gt;The double-debiased machine learning model implicitly assumes that the control variables $X$ are (weakly) &lt;strong&gt;common causes&lt;/strong&gt; to both the outcome $Y$ and the treatment $D$. If this is the case, and no further mediated/indirect relationship exists between $X$ and $Y$, there is no problem. However, if, for example, some variable among the controls $X$ is a common effect instead of a common cause, its inclusion will bias the coefficient of interest. Moreover, this variable is likely to be highly correlated either with the outcome $Y$ or with the treatment $D$. In the latter case, this implies that post-double selection might include it in cases in which simple selection would have not. Therefore, in presence of bad control variables, doule-debiased machine learning might be &lt;strong&gt;even worse&lt;/strong&gt; than simple pre-testing.&lt;/p&gt;
&lt;p&gt;In short, as for any method, it is &lt;strong&gt;crucial&lt;/strong&gt; to have a clear understanding of the method&amp;rsquo;s assumptions and to always check for potential violations.&lt;/p&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;In this post, we have seen how to use post-double selection and, more generally, double debiased machine learning to get rid of an important source of bias: regularization bias.&lt;/p&gt;
&lt;p&gt;This contribution by Victor Chernozhukov and co-authors has been undoubtedly one of the most relevant advances in causal inferences in the last decade. It is now widely employed in the industry and included in the most used causal inference packages, such as &lt;a href=&#34;https://econml.azurewebsites.net/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;EconML&lt;/a&gt; (Microsoft) and &lt;a href=&#34;https://causalml.readthedocs.io/en/latest/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;causalml&lt;/a&gt; (Uber).&lt;/p&gt;
&lt;p&gt;If you (understandably) feel the need for more material on double-debiased machine learning, but you do not feel like reading academic papers (also very understandable), here is a good compromise.&lt;/p&gt;
&lt;br&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/eHOjmyoPCFU&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;br&gt;
&lt;p&gt;In this video lecture, Victor Chernozhukov himself presents the idea. The video lecture is relatively heavy on math and statistics, but you cannot get a more qualified and direct source than this!&lt;/p&gt;
&lt;h3 id=&#34;references&#34;&gt;References&lt;/h3&gt;
&lt;p&gt;[1] A. Belloni, D. Chen, V. Chernozhukov, C. Hansen, &lt;a href=&#34;https://onlinelibrary.wiley.com/doi/abs/10.3982/ECTA9626&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Sparse Models and Methods for Optimal Instruments With an Application to Eminent Domain&lt;/a&gt; (2012), &lt;em&gt;Econometrica&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;[2] A. Belloni, V. Chernozhukov, C. Hansen, &lt;a href=&#34;https://academic.oup.com/restud/article-abstract/81/2/608/1523757&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Inference on treatment effects after selection among high-dimensional controls&lt;/a&gt; (2014), &lt;em&gt;The Review of Economic Studies&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;[3] V. Chernozhukov, D. Chetverikov, M. Demirer, E. Duflo, C. Hansen, W. Newey, J. Robins, &lt;a href=&#34;https://academic.oup.com/ectj/article/21/1/C1/5056401&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Double/debiased machine learning for treatment and structural parameters&lt;/a&gt; (2018), &lt;em&gt;The Econometrics Journal&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;[4] P. Hünermund, B. Louw, I. Caspi, &lt;a href=&#34;https://arxiv.org/abs/2108.11294&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Double Machine Learning and Automated Confounder Selection - A Cautionary Tale&lt;/a&gt; (2022), &lt;em&gt;working paper&lt;/em&gt;.&lt;/p&gt;
&lt;h3 id=&#34;related-articles&#34;&gt;Related Articles&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/eb767a59975b&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Double Debiased Machine Learning (part 1)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/344ac1477699&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Understanding Omitted Variable Bias&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/59f801eb3299&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Understanding The Frisch-Waugh-Lovell Theorem&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/b63dc69e3d8c&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;DAGs and Control Variables&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;code&#34;&gt;Code&lt;/h3&gt;
&lt;p&gt;You can find the original Jupyter Notebook here:&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/pds.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/matteocourthoud/Blog-Posts/blob/main/pds.ipynb&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Experiments on Returns on Investment</title>
      <link>https://matteocourthoud.github.io/post/delta_method/</link>
      <pubDate>Mon, 10 Jul 2023 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/post/delta_method/</guid>
      <description>&lt;p&gt;&lt;em&gt;An introduction to the delta method for inference on ratio metrics.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;When we run an experiment, we are often not only interested in the effect of a treatment (new product, new feature, new interface, &amp;hellip;) on revenue, but in it&amp;rsquo;s &lt;strong&gt;cost-effectiveness&lt;/strong&gt;. In other words, is the investment worth the cost? Common examples include investments in computing resources, returns on advertisement, but also click-through rates and other ratio metrics.&lt;/p&gt;
&lt;p&gt;When we investigate causal effects, the gold standard is randomized control trials, a.k.a. &lt;strong&gt;AB tests&lt;/strong&gt;. Randomly assigning the treatment to a subset of the population (users, patients, customers, &amp;hellip;) we ensure that, on average, the difference in outcomes can be attributed to the treatment. However, when the object of interest is cost-effectiveness, AB tests present some additional problems since we are not just interested in one treatment effect, but in the &lt;strong&gt;ratio of two treatment effects&lt;/strong&gt;, the outcome of the investment over its cost.&lt;/p&gt;
&lt;p&gt;In this post we are going to see how to analyze randomized experiments when the object of interest is the &lt;strong&gt;return on investment (ROI)&lt;/strong&gt;. We are going to explore alternative metrics to measure whether an investment paid off. We will also introduce a very powerful tool for inference with complex metrics: the &lt;strong&gt;delta method&lt;/strong&gt;. While the algebra can be intense, the result is simple: we can compute the confidence interval for our ratio estimator using a simple linear regression.&lt;/p&gt;
&lt;h2 id=&#34;investing-in-cloud-computing&#34;&gt;Investing in Cloud Computing&lt;/h2&gt;
&lt;p&gt;To better illustrate the concepts, we are going to use a toy example throughout the article: suppose we were an &lt;strong&gt;online marketplace&lt;/strong&gt; and we wanted to &lt;strong&gt;invest in cloud computing&lt;/strong&gt;: we want to increase the computing power behind our internal search engine, by switching to a higher tier server. The idea is that the faster search will improve the user experience, potentially leading to higher sales. Therefore, the question is: is the investment worth the cost? The object of interest is the &lt;strong&gt;return on investment (ROI)&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Differently from usual AB tests or randomized experiments, we are not interested in a single causal effect, but in the &lt;strong&gt;ratio&lt;/strong&gt; of two metrics: the effect on revenue and the effect on cost. We will still use a &lt;strong&gt;randomized control trial&lt;/strong&gt; or &lt;strong&gt;AB test&lt;/strong&gt; to estimate the ROI: we randomly assign groups of users to either the treatment or the control group. The treated users will benefit from the faster cloud machines, while the control users will use the old slower machines. Randomization ensures that we can estimate the impact of the new machines on either cost or revenue by comparing users in the treatment and control group: the difference in their average is an unbiased estimator of the average treatment effect. However, things are more complicated for their ratio.&lt;/p&gt;
&lt;p&gt;I import the data generating process &lt;code&gt;dgp_cloud()&lt;/code&gt; from &lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/src/dgp.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;src.dgp&lt;/code&gt;&lt;/a&gt;. With respect to previous articles, I generated a new DGP parent class that handles randomization and data generation, while its children classes contain the specific use-cases. I also import some plotting functions and libraries from &lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/src/utils.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;src.utils&lt;/code&gt;&lt;/a&gt;.  To include not only code but also data and tables, I use &lt;a href=&#34;https://deepnote.com&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Deepnote&lt;/a&gt;, a Jupyter-like web-based collaborative notebook environment.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;%matplotlib inline
%config InlineBackend.figure_format = &#39;retina&#39;

from src.utils import *
from src.dgp import dgp_cloud, DGP
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;dgp = dgp_cloud(n=10_000)
df = dgp.generate_data(seed_assignment=6)
df.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;new_machine&lt;/th&gt;
      &lt;th&gt;cost&lt;/th&gt;
      &lt;th&gt;revenue&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;3.14&lt;/td&gt;
      &lt;td&gt;20.90&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;3.77&lt;/td&gt;
      &lt;td&gt;33.57&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;3.16&lt;/td&gt;
      &lt;td&gt;24.31&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;2.36&lt;/td&gt;
      &lt;td&gt;20.35&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1.65&lt;/td&gt;
      &lt;td&gt;12.60&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;The data contains information on the total &lt;code&gt;cost&lt;/code&gt; and &lt;code&gt;revenue&lt;/code&gt; for a set of $10.000$ users over a period of a month. We also have information on the treatment: whether the search engine was running on the old or &lt;code&gt;new machines&lt;/code&gt;.  As it often happens with business metrics, both distributions of cost and revenues are very &lt;strong&gt;skewed&lt;/strong&gt;. Moreover, most people do not buy anything and therefore generate zero revenue, even though they still use the platform, generating positive costs.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;fix, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))
sns.histplot(df.cost, ax=ax1, color=&#39;C0&#39;).set(title=&#39;Distribution of Cost&#39;)
sns.histplot(df.revenue, ax=ax2, color=&#39;C1&#39;).set(title=&#39;Distribution of Revenue&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/delta_method_8_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;We can compute the &lt;strong&gt;difference-in-means&lt;/strong&gt; estimate for &lt;code&gt;cost&lt;/code&gt; and &lt;code&gt;revenue&lt;/code&gt; by regressing the outcome on the treatment indicator.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;smf.ols(&#39;cost ~ new_machine&#39;, data=df).fit().summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
       &lt;td&gt;&lt;/td&gt;          &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt;   &lt;td&gt;    2.9617&lt;/td&gt; &lt;td&gt;    0.043&lt;/td&gt; &lt;td&gt;   69.034&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    2.878&lt;/td&gt; &lt;td&gt;    3.046&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;new_machine&lt;/th&gt; &lt;td&gt;    0.5152&lt;/td&gt; &lt;td&gt;    0.060&lt;/td&gt; &lt;td&gt;    8.563&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    0.397&lt;/td&gt; &lt;td&gt;    0.633&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;The average &lt;code&gt;cost&lt;/code&gt; has increased by $0.5152$$ per user. What about revenue?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;smf.ols(&#39;revenue ~ new_machine&#39;, data=df).fit().summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
       &lt;td&gt;&lt;/td&gt;          &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt;   &lt;td&gt;   25.9172&lt;/td&gt; &lt;td&gt;    0.425&lt;/td&gt; &lt;td&gt;   60.950&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;   25.084&lt;/td&gt; &lt;td&gt;   26.751&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;new_machine&lt;/th&gt; &lt;td&gt;    1.0664&lt;/td&gt; &lt;td&gt;    0.596&lt;/td&gt; &lt;td&gt;    1.788&lt;/td&gt; &lt;td&gt; 0.074&lt;/td&gt; &lt;td&gt;   -0.103&lt;/td&gt; &lt;td&gt;    2.235&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;The average &lt;code&gt;revenue&lt;/code&gt; per user has also increased, by $1.0664$$. So, was the investment &lt;strong&gt;profitable&lt;/strong&gt;?&lt;/p&gt;
&lt;p&gt;To answer this question, we first have to decide which metric to use as our &lt;strong&gt;outcome metric&lt;/strong&gt;. In case of ratio metrics, this is not trivial.&lt;/p&gt;
&lt;h2 id=&#34;average-return-or-return-of-the-average&#34;&gt;Average Return or Return of the Average?&lt;/h2&gt;
&lt;p&gt;It is very tempting to approach this problem saying: it is true that we have two variables, by we can just compute their ratio, and then analyze everything as usual, using a &lt;strong&gt;single variable&lt;/strong&gt;: the individual level return.&lt;/p&gt;
&lt;p&gt;$$
\rho_i = \frac{\text{individual revenue}}{\text{individual cost}} = \frac{R_i}{C_i}
$$&lt;/p&gt;
&lt;p&gt;What happens if we analyze the experiment using this single metric?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df[&amp;quot;rho&amp;quot;] = df[&amp;quot;revenue&amp;quot;] / df[&amp;quot;cost&amp;quot;]
smf.ols(&amp;quot;rho ~ new_machine&amp;quot;, df).fit().summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
       &lt;td&gt;&lt;/td&gt;          &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt;   &lt;td&gt;    6.6898&lt;/td&gt; &lt;td&gt;    0.044&lt;/td&gt; &lt;td&gt;  150.832&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    6.603&lt;/td&gt; &lt;td&gt;    6.777&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;new_machine&lt;/th&gt; &lt;td&gt;   -0.7392&lt;/td&gt; &lt;td&gt;    0.062&lt;/td&gt; &lt;td&gt;  -11.893&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;   -0.861&lt;/td&gt; &lt;td&gt;   -0.617&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;The estimated effect is &lt;strong&gt;negative and significant&lt;/strong&gt;, $-0.7392$! It seems like the the new machines were not a good investment, and the returns have decreased by $74%$.&lt;/p&gt;
&lt;p&gt;This result seems to contradict our previous estimates. We have seen before that the revenue has increased on average more than the cost ($0.9505$ vs $0.5076$). Why is it the case? The problem is that we are giving the same weight to heavy users and light users. Let&amp;rsquo;s use a simple example with two users. The first one (blue) is a light user and before was costing $1$ $ and returning $10$ $, while now is costing $4$ $ and returning $20$ $. The other user (violet) is a heavy user and before was costing $10$ $ and returning $100$ $ and now is costing $20$ $ and returning $220$ $.&lt;/p&gt;
&lt;img src=&#34;fig/return.png&#34; width=&#34;700px&#34;/&gt;
&lt;p&gt;The average return is -3x: on average the return per user has decreased by $300%$. However, the total return per user is $1000%$: the increase in cost of $13$$ has generated $130$$ in revenue! The results are wildly different and entirely driven by the weight of the two users: the effect of the heavy user is low in relative terms but high in absolute terms, while it&amp;rsquo;s the opposite for the light user. The average relative effect is therefore mostly driven by the light user, while the relative average effect is mostly driven by the heavy user.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Which metric&lt;/strong&gt; is more relevant in our setting? We talking about return on investment, we are usually interested in understanding whether we got a return on the money we spend. Therefore, the &lt;strong&gt;total return&lt;/strong&gt; is more interesting than the average return.&lt;/p&gt;
&lt;p&gt;From now on, the object of interest will be the &lt;strong&gt;return on investment (ROI)&lt;/strong&gt;, given by the expected increase in revenue over the expected increase in cost, and we will denote it with the greek letter rho, $\rho$.&lt;/p&gt;
&lt;p&gt;$$
\rho = \frac{\text{incremental revenue}}{\text{incremental cost}} = \frac{\mathbb E [\Delta R]}{\mathbb E [\Delta C]}
$$&lt;/p&gt;
&lt;p&gt;We can estimate the ROI as the ratio of the two previous estimates: the average difference in revenue between the treatment and control group, over the average difference in cost between the treatment and control group.&lt;/p&gt;
&lt;p&gt;$$
\hat{\rho} = \frac{\mathbb E_n [\Delta R]}{\mathbb E_n [\Delta C]}
$$&lt;/p&gt;
&lt;p&gt;Note a subtle but crucial difference with respect to the previous formula: we have replaced the &lt;a href=&#34;https://en.wikipedia.org/wiki/Expected_value&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;expected values&lt;/a&gt; $\mathbb E$ with the empirical expectation operators $\mathbb E_n$, also known as the &lt;a href=&#34;https://en.wikipedia.org/wiki/Arithmetic_mean&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;sample average&lt;/a&gt;. The difference in notation is minimal, but the conceptual difference is huge. The first, $\mathbb E$, is a &lt;strong&gt;theoretical&lt;/strong&gt; concept, while the second, $\mathbb E_n$, is &lt;strong&gt;empirical&lt;/strong&gt;: it is a number that depends on the actual data. I personally like the notation since it highlights the close link between the two concepts (the second is the empirical counterpart of the first), while also making it clear that the second crucially depends on the sample size $n$.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def estimate_roi(df):
    Delta_C = df.loc[df.new_machine==1, &amp;quot;cost&amp;quot;].mean() - df.loc[df.new_machine==0, &amp;quot;cost&amp;quot;].mean()
    Delta_R = df.loc[df.new_machine==1, &amp;quot;revenue&amp;quot;].mean() - df.loc[df.new_machine==0, &amp;quot;revenue&amp;quot;].mean()
    return Delta_R / Delta_C

estimate_roi(df)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;2.0698235970047887
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The estimate is $2.0698$: each additional dollar spent in the new machines translated in $2.0698$ extra dollars in revenue. Sounds great!&lt;/p&gt;
&lt;p&gt;But how much should we trust this number? Is it significantly different form one, or it is just driven by noise?&lt;/p&gt;
&lt;h2 id=&#34;inference&#34;&gt;Inference&lt;/h2&gt;
&lt;p&gt;To answer this question, we would like to compute a &lt;a href=&#34;https://en.wikipedia.org/wiki/Confidence_interval&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;confidence interval&lt;/strong&gt;&lt;/a&gt; for our estimate. How do we compute a confidence interval for a ratio metric? The first step is to compute the standard deviation of the estimator. One method that is always available is the &lt;a href=&#34;https://en.wikipedia.org/wiki/Bootstrapping_%28statistics%29&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;bootstrap&lt;/strong&gt;&lt;/a&gt;: resample the data with replacement multiple times and use the distribution of the estimates over samples to compute the standard deviation of the estimator.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s try it in our case. I compute the standard deviation over $10.000$ bootstrapped samples, using the function &lt;code&gt;pd.DataFrame().sample()&lt;/code&gt; with the options &lt;code&gt;frac=1&lt;/code&gt; to obtain a dataset of the same size and &lt;code&gt;replace=True&lt;/code&gt; to sample with replacement.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;boot_estimates = [estimate_roi(df.sample(frac=1, replace=True, random_state=i)) for i in range(10_000)]
np.std(boot_estimates)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;0.9790730538161984
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The bootstrap estimate of the standard deviation is equal to $0.979$. How good is it?&lt;/p&gt;
&lt;p&gt;Since we fully control the data generating process, we can simulate the &amp;ldquo;true&amp;rdquo; distribution of the estimator. We do that for $10.000$ simulations and we compute the resulting standard deviation of the estimator.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;np.std(dgp.evaluate_f_redrawing_outcomes(estimate_roi, 10_000))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;1.0547776958025372
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The estimated variance of the estimator using the &amp;ldquo;true&amp;rdquo; data generating process is slightly higher but very similar, around $1.055$.&lt;/p&gt;
&lt;p&gt;The issue with the bootstrap is that it is very computational intense since it requires repeating the estimating procedure thousands of times. We are now going to explore another &lt;em&gt;extremely&lt;/em&gt; powerful alternative that requires a single estimation step, the &lt;a href=&#34;https://en.wikipedia.org/wiki/Delta_method&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;delta method&lt;/strong&gt;&lt;/a&gt;. The delta method generally allows us to do inference on functions of random variable, therefore its applications are broader than ratios.&lt;/p&gt;
&lt;p&gt;⚠️ &lt;strong&gt;Warning&lt;/strong&gt;: the next section is going to be algebra-intense. If you want, you can skip it and go straight to the last section.&lt;/p&gt;
&lt;h2 id=&#34;the-delta-method&#34;&gt;The Delta Method&lt;/h2&gt;
&lt;p&gt;What is the &lt;strong&gt;delta method&lt;/strong&gt;? In short, it is an incredibly powerful &lt;strong&gt;asymptotic inference&lt;/strong&gt; method for functions of random variables, that exploits Taylor expansions. In short, the delta method requires four ingredients&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;One or more &lt;a href=&#34;https://en.wikipedia.org/wiki/Random_variable&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;random variables&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;A function&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Central_limit_theorem&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The Central Limit Theorem&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Taylor_series&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Taylor expansions&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I will assume some basic knowledge of all four concepts. Suppose we had a set of realizations $X_1$, &amp;hellip;, $X_n$ of a random variable that satisfy the requirements for the Central Limit Theorem (CLT): independence, identically distributions with expected value $\mu$, and finite variance $\sigma^2$. Under these conditions, the CLT tells us that the sample average $\mathbb E_n[X]$ converges in distribution to a normal distribution, or more precisely&lt;/p&gt;
&lt;p&gt;$$
\sqrt{n} \ \frac{ \mathbb E_n[X] - \mu}{\sigma} \ \overset{D}{\to} \ N(0, 1)
$$&lt;/p&gt;
&lt;p&gt;What does the equation mean? It reads &amp;ldquo;the normalized sample average, scaled by a factor $\sqrt{n}$, converges in distribution to a standard normal distribution, i.e. it is approximately Gaussian for a sufficiently large sample.&lt;/p&gt;
&lt;p&gt;Now, suppose we were interested in a &lt;strong&gt;function&lt;/strong&gt; of the sample average $f\big(\mathbb E_n[X]\big)$. Note that this is different from the sample average of the function $\mathbb E_n\big[f(X)\big]$. The delta method tells us what the function of the sample average converges to.&lt;/p&gt;
&lt;p&gt;$$
\sqrt{n} \ \frac{ f\big(\mathbb E_n[X]\big) - f(\mu)}{\sigma} \ \overset{D}{\to} \ N \big(0, f&amp;rsquo;(\mu)^2 \big)
$$&lt;/p&gt;
&lt;p&gt;, where $f&amp;rsquo;(\mu)^2$ is the derivative of the function $f$, evaluated at $\mu$.&lt;/p&gt;
&lt;p&gt;What is the &lt;strong&gt;intuition&lt;/strong&gt; behind this formula? We now have a new term inside the expression of the variance, the squared first derivative $f&amp;rsquo;(\mu)^2$ ($\neq$ second derivative). If the derivative of the function is low, the variance decreases since different inputs translate into similar outputs. On the contrary, if the derivative of the function is high, the variance of the distribution is amplified, since different inputs translate into even more different outputs.&lt;/p&gt;
&lt;img src=&#34;fig/delta_intuition.png&#34; width=&#34;700px&#34;/&gt;
&lt;p&gt;The result directly follows from the Taylor approximation of $f \big(\mathbb E_n[X]\big)$&lt;/p&gt;
&lt;p&gt;$$
f\big(\mathbb E_n[X]\big) = f(\mu) + f&amp;rsquo;(\mu) (\mathbb E_n[X] - \mu) + \text{residual}
$$&lt;/p&gt;
&lt;p&gt;Importantly, asymptotically, the last term disappears and the linear approximation holds exactly!&lt;/p&gt;
&lt;p&gt;How is this connected to the ratio estimator? We need a bit more math and to switch from a single dimension to two dimensions in order to understand that. In our case, we have a bivariate function of two random variables, $\Delta R$ and $\Delta C$, which returns their ratio. In the case of a multivariate function $f$, the asymptotic variance of the estimator is given by&lt;/p&gt;
&lt;p&gt;$$
\text{AVar} \big( \hat{\rho} \big) = \nabla \hat{\rho}&amp;rsquo; \Sigma_n \nabla \hat{\rho}
$$&lt;/p&gt;
&lt;p&gt;where, $\nabla$ indicates the &lt;a href=&#34;https://en.wikipedia.org/wiki/Gradient&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;gradient&lt;/a&gt; of the function, i.e. the vector of directional derivatives, and $\Sigma_n$ is the empirical variance-covariance matrix of $X$. In our case, they correspond to&lt;/p&gt;
&lt;p&gt;$$
\nabla \hat{\rho} =
\begin{bmatrix}
\frac{1}{\mathbb E_n [\Delta C]} \newline - \frac{\mathbb E_n [\Delta R]}{\mathbb E_n [\Delta C]^2}
\end{bmatrix}
$$&lt;/p&gt;
&lt;p&gt;and&lt;/p&gt;
&lt;p&gt;$$
\Sigma_n =
\begin{bmatrix}
\text{Var}_n (\Delta R) &amp;amp; \text{Cov}_n (\Delta R, \Delta C) \newline
\text{Cov}_n (\Delta R, \Delta C) &amp;amp; \text{Var}_n (\Delta C) \newline
\end{bmatrix}
$$&lt;/p&gt;
&lt;p&gt;, where the subscripts $n$ indicate the empirical counterparts, as for the expected value.&lt;/p&gt;
&lt;p&gt;Combining the previous three equations together with a little matrix algebra, we get the formula of the asymptotic variance of the return on investment estimator.&lt;/p&gt;
&lt;p&gt;$$
\begin{align*}
\text{AVar} \big( \hat{\rho} \big) &amp;amp;= \frac{1}{\mathbb E_n[\Delta C]^2} \text{Var}_n(\Delta R) - 2 \frac{\mathbb E_n[\Delta R]}{\mathbb E_n[\Delta C]^3} \text{Cov}_n(\Delta R, \Delta C) + \frac{\mathbb E_n[\Delta R]^2}{\mathbb E_n[\Delta C]^4} \text{Var}_n(\Delta C)
\end{align*}
$$&lt;/p&gt;
&lt;p&gt;Since the estimator is given by $\hat{\rho} = \frac{\mathbb E_n[\Delta R]}{\mathbb E_n[\Delta C]}$, we can rewrite the asymptotic variance as&lt;/p&gt;
&lt;p&gt;$$
\begin{align*}
\text{AVar} \big( \hat{\rho} \big) = \frac{1}{\mathbb E_n[\Delta C]^2} \text{Var}_n \Big( \Delta R - \hat{\rho} \Delta C \Big)
\end{align*}
$$&lt;/p&gt;
&lt;p&gt;The last expression is very interesting because it suggests that we can rewrite the asymptotic variance of our estimator as the &lt;strong&gt;variance of a difference-in-means estimator&lt;/strong&gt; for a new auxiliary variable. In fact, we can rewrite the above expression as&lt;/p&gt;
&lt;p&gt;$$
\begin{align*}
\text{AVar} \big( \hat{\rho} \big) = \text{Var}_n \Big( \Delta \tilde R \Big) \qquad \text{where} \quad \tilde R = \frac{R - \hat{\rho} \ C}{| \mathbb E [\Delta C] |}
\end{align*}
$$&lt;/p&gt;
&lt;p&gt;This expression is incredibly useful because it gives us intuition and allows us to estimate the standard deviation of our estimator by &lt;strong&gt;linear regression&lt;/strong&gt;.&lt;/p&gt;
&lt;h2 id=&#34;inference-with-linear-regression&#34;&gt;Inference with Linear Regression&lt;/h2&gt;
&lt;p&gt;Did you skip the previous section? No problem!&lt;/p&gt;
&lt;p&gt;After some algebra, we concluded that we can estimate the variance of a difference-in-means estimator for an &lt;strong&gt;auxiliary variable&lt;/strong&gt; defined as&lt;/p&gt;
&lt;p&gt;$$
\tilde R = \frac{R - \hat{\rho} \ C}{| \mathbb E_n [\Delta C] |}
$$&lt;/p&gt;
&lt;p&gt;This expression might seem obscure at first, but it is incredibly useful. In fact, it gives us (1) an intuitive &lt;strong&gt;interpretation&lt;/strong&gt; of the variance of the estimator and (2) a &lt;strong&gt;practical&lt;/strong&gt; way to estimate it.&lt;/p&gt;
&lt;p&gt;Interpretation first! How should we read the above expression? We can estimate the variance of the empirical estimator as the variance of a difference-in-means estimator, for a new variable $\tilde R$ that we can easily compute from the data. We just need to take the revenue $R$, subtract the cost $C$ multiplied by the estimated ROI $\rho$ and scale it down by the expected cost difference $|\mathbb E_n[\Delta C]|$. We can interpret this variable as the &lt;strong&gt;baseline revenue&lt;/strong&gt;, i.e. the revenue not affected by the investment. The fact that it is scaled by the expected cost difference tells us that its variance will be &lt;strong&gt;decreasing in the total investment&lt;/strong&gt;: the more we spend, the more precisely we can estimate the return on that expenditure.&lt;/p&gt;
&lt;p&gt;Now, let&amp;rsquo;s estimate the variance of the ROI estimator, in &lt;strong&gt;four steps&lt;/strong&gt;.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;We need to estimate the return on investment $\hat \rho$.&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;rho_hat = estimate_roi(df)
&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;The term $| \mathbb E_n[\Delta C] |$ is the absolute difference in average cost between the treatment and control group.&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;abs_Delta_C = np.abs(df.loc[df.new_machine==1, &amp;quot;cost&amp;quot;].mean() - df.loc[df.new_machine==0, &amp;quot;cost&amp;quot;].mean())
&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;3&#34;&gt;
&lt;li&gt;We now have all the ingredients to generate the auxiliary variable $\tilde R$.&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df[&#39;revenue_tilde&#39;] = (df[&#39;revenue&#39;] - rho_hat * df[&#39;cost&#39;]) / abs_Delta_C
&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;4&#34;&gt;
&lt;li&gt;The variance of the treatment-control difference $\Delta \tilde R$ can be directly computed by linear regression, as in randomized controlled trials for difference-in-means estimators (see Agrist and Pischke, 2008).&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;smf.ols(&#39;revenue_tilde ~ new_machine&#39;, df).fit().summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
       &lt;td&gt;&lt;/td&gt;          &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt;   &lt;td&gt;   38.4067&lt;/td&gt; &lt;td&gt;    0.653&lt;/td&gt; &lt;td&gt;   58.771&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;   37.126&lt;/td&gt; &lt;td&gt;   39.688&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;new_machine&lt;/th&gt; &lt;td&gt; -2.01e-14&lt;/td&gt; &lt;td&gt;    0.917&lt;/td&gt; &lt;td&gt;-2.19e-14&lt;/td&gt; &lt;td&gt; 1.000&lt;/td&gt; &lt;td&gt;   -1.797&lt;/td&gt; &lt;td&gt;    1.797&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;The estimated standard error of the ROI is $0.917$, very close to the bootstrap estimate of $0.979$ and the simulated value of $1.055$. However, with respect to bootstrapping, the delta method allowed us to compute it in a single step, making it sensibly &lt;strong&gt;faster&lt;/strong&gt; (around $1000$ times on my local machine).&lt;/p&gt;
&lt;p&gt;Note that this estimated standard deviation implies a 95% confidence interval of $2.0698 +- 1.96 \times 0.917$, equal to $[-0.2735, 3.8671]$. This might seem like good news since the confidence interval does not cover zero. However, note that in this case, a more interesting &lt;strong&gt;null hypothesis&lt;/strong&gt; is that the ROI is equal to 1: we are breaking even. A value larger than 1 implies profits, while a value lower than 1 implies losses. In our case, we cannot reject the null hypothesis that the investment in new machines was not profitable.&lt;/p&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;In this article, we have explored a very common causal inference problem: assessing the &lt;strong&gt;return on investment&lt;/strong&gt;. Whether it&amp;rsquo;s a physical investment in new hardware, a virtual cost, or advertisement expenditure, we are interested in understanding whether this incremental cost has paid off. The additional complications come from the fact that we are studying not one, but two causal quantities, intertwined.&lt;/p&gt;
&lt;p&gt;We have first explored and compared different outcome metrics to assess whether the investment paid off. Then, we have introduced an incredibly powerful method to do inference with complex random variables: the &lt;strong&gt;delta method&lt;/strong&gt;. In the particular case of ratios, the delta method delivers a very insightful and practical functional form for the asymptotic variance of the estimator that can be estimated with a simple linear regression.&lt;/p&gt;
&lt;h3 id=&#34;references&#34;&gt;References&lt;/h3&gt;
&lt;p&gt;[1] A. Deng, U. Knoblich, J. Lu, &lt;a href=&#34;https://arxiv.org/pdf/1803.06336.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Applying the Delta Method in Metric Analytics: A Practical Guide with Novel Ideas&lt;/a&gt; (2018).&lt;/p&gt;
&lt;p&gt;[2] R. Budylin, A. Drutsa, I. Katsev, V. Tsoy, &lt;a href=&#34;https://dl.acm.org/doi/abs/10.1145/3159652.3159699&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Consistent Transformation of Ratio Metrics for Efficient Online Controlled Experiments&lt;/a&gt; (2018). &lt;em&gt;ACM&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;[3] J. Angrist, J. Pischke, &lt;a href=&#34;https://www.mostlyharmlesseconometrics.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Mostly harmless econometrics: An empiricist&amp;rsquo;s companion&lt;/a&gt; (2009). &lt;em&gt;Princeton university press&lt;/em&gt;.&lt;/p&gt;
&lt;h3 id=&#34;related-articles&#34;&gt;Related Articles&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://medium.com/towards-data-science/the-bayesian-bootstrap-6ca4a1d45148&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The Bayesian Bootstrap&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://towardsdatascience.com/df3065a0388e&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Outliers, Leverage, Residuals, and Influential Observations&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://towardsdatascience.com/b07ab46aa782&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;A/B Tests, Privacy, and Online Regression&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;code&#34;&gt;Code&lt;/h3&gt;
&lt;p&gt;You can find the original Jupyter Notebook here:&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/delta.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/delta.ipynb&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Experiments, Peeking, and Optimal Stopping</title>
      <link>https://matteocourthoud.github.io/post/optimal_stopping/</link>
      <pubDate>Mon, 10 Jul 2023 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/post/optimal_stopping/</guid>
      <description>&lt;p&gt;In the decade preceding the Second World War, there was a massive increase in industrial production of war materials, so there was a need to ensure that products, especially munitions, were reliable. The &lt;strong&gt;testing&lt;/strong&gt; of war materials is not only &lt;strong&gt;expensive&lt;/strong&gt; but also &lt;strong&gt;destructive&lt;/strong&gt; since, for example, bullets need to be fired in order to be tested.&lt;/p&gt;
&lt;p&gt;Therefore, the U.S. government was presented with the following &lt;strong&gt;dilemma&lt;/strong&gt;: how many bullets should one fire out of a batch before declaring the batch reliable? Clearly, if we were to fire all the bullets, we would know the exact amount of functioning bullets in a crate. However, there would be no bullets left to use.&lt;/p&gt;
&lt;p&gt;Because of the growing relevance of these statistical problems, in 1939, a group of prominent statisticians and economists joined forces at Columbia University&amp;rsquo;s &lt;a href=&#34;https://en.wikipedia.org/wiki/The_Statistical_Research_Group_of_World_War_II&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Statistical Research Group (SGR)&lt;/strong&gt;&lt;/a&gt;. The group included, among others, &lt;a href=&#34;https://en.wikipedia.org/wiki/W._Allen_Wallis&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;W. Allen Wallis&lt;/a&gt;, &lt;a href=&#34;https://en.wikipedia.org/wiki/Jacob_Wolfowitz&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Jacob Wolfowitz&lt;/a&gt; and &lt;a href=&#34;https://en.wikipedia.org/wiki/Abraham_Wald&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Abraham Wald&lt;/a&gt;. According to Wallis himself the SGR group was &amp;ldquo;&lt;em&gt;composed of what surely must be the most extraordinary group of statisticians ever organized, taking into account both number and quality&lt;/em&gt;&amp;rdquo;[&lt;a href=&#34;#References&#34;&gt;2&lt;/a&gt;].&lt;/p&gt;
&lt;p&gt;Their work was of first order importance and &lt;strong&gt;classified&lt;/strong&gt;, to the point that Wallis reports:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;It is said that as Wald worked on sequential analysis his pages were snatched away and given a security classification. Being still an &amp;ldquo;enemy alien&amp;rdquo;, he did not have a security clearance so, the story has it, he was not allowed to know of his results.&lt;/em&gt; [&lt;a href=&#34;https://www.jstor.org/stable/2287451&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Wallis (1980)&lt;/a&gt;]&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Indeed, the group worked under the pressure from the U.S. Army to deliver &lt;strong&gt;fast practical solutions&lt;/strong&gt; that could be readily deployed on the field. For example, Wallis reports that&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;during the &lt;a href=&#34;https://en.wikipedia.org/wiki/Battle_of_the_Bulge&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Battle of the Bulge&lt;/a&gt; in December 1944, several high-ranking Army officers flew to Washington from the battle, spent a day discussing the best settings on proximity fuzes for air bursts of artillery shells against ground troops, and flew back to the battle to put into effect advice from, among others, &lt;a href=&#34;https://en.wikipedia.org/wiki/Milton_Friedman&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Milton Friedman&lt;/a&gt;, whose earlier studies of the fuzes had given him extensive and accurate knowledge of the way the fuzes actually performed.&lt;/em&gt;  [&lt;a href=&#34;https://www.jstor.org/stable/2287451&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Wallis (1980)&lt;/a&gt;]&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The most prominent &lt;strong&gt;result&lt;/strong&gt; that came out of the SGR experience was undoubtedly the &lt;a href=&#34;https://en.wikipedia.org/wiki/Sequential_probability_ratio_test&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Sequential Probability Ratio Test&lt;/a&gt;. The idea first came to Wallis and Friedman that realized that&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;it might pay to use a test which would not be as efficient as the classical tests if a sample of exactly N were to be taken, but which would more than offset this disadvantage by providing a good chance of terminating early when used sequentially.&lt;/em&gt; [&lt;a href=&#34;https://www.jstor.org/stable/2287451&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Wallis (1980)&lt;/a&gt;]&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The two economists exposed the idea to the statistician Jacob Wolfowitz who initially&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;seemed to be something distasteful about the idea of people so ignorant of mathematics as Milton and I venturing to meddle with such sacred ideas as those of most powerful statistics, etc. No doubt this antipathy was strengthened by our calling the new tests &amp;ldquo;supercolossal&amp;rdquo; on the grounds that they are more powerful than &amp;ldquo;most powerful&amp;rdquo; tests.&lt;/em&gt; [&lt;a href=&#34;https://www.jstor.org/stable/2287451&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Wallis (1980)&lt;/a&gt;]&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Ultimately, the two economists managed to draw the attention of both Wolfowitz and Wald that started to formally work on the idea. The results remained top secret until the end of the war when Wald published his &lt;a href=&#34;https://www.jstor.org/stable/2235829&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Sequential Tests of Statistical Hypotheses&lt;/a&gt; article.&lt;/p&gt;
&lt;p&gt;In this post, after a quick introduction to hypothesis testing, we are going to explore the Sequential Probability Ratio Test and implement it in Python.&lt;/p&gt;
&lt;h2 id=&#34;hypothesis-testing&#34;&gt;Hypothesis Testing&lt;/h2&gt;
&lt;p&gt;When we design an A/B test or, more generally, an experiment, the standard steps are the following&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Define a &lt;strong&gt;null hypothesis&lt;/strong&gt; $H_0$, usually a zero effect of the experiment on a metric of interest&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;for example, no effect of a drug on mortality&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Define a &lt;strong&gt;significance level&lt;/strong&gt; $\alpha$, usually equal to 0.05, it represents the maximum probability of rejecting the null hypothesis when it is true&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;for example, the probability of claiming that the drug is effective in reducing mortality, when it&amp;rsquo;s not effective&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Define an &lt;strong&gt;alternative hypothesis&lt;/strong&gt; $H_1$, usually the minimum effect size that we would like to detect&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;for example, a decrease in mortality by 1%&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Define a &lt;strong&gt;power level&lt;/strong&gt; $1-\beta$, usually equal to 0.8 ($\beta=0.2$), it represents the minimum probability of rejecting the null hypothesis $H_0$, when the alternative $H_1$ is true&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;for example, the probability of claiming that the drug is ineffective, when it&amp;rsquo;s effective&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Pick a &lt;strong&gt;test statistic&lt;/strong&gt; whose distribution is known under both hypotheses, usually the sample average of the metric of interest&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;for example, the average mortality rate of patients&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Compute the minimum &lt;strong&gt;sample size&lt;/strong&gt;, in order to achieve the desired power level $1-\beta$, given all the test parameters&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Then, we &lt;strong&gt;run the test&lt;/strong&gt; and, depending on the realized value of the test statistic, we decide whether to &lt;strong&gt;reject&lt;/strong&gt; the null hypothesis or not. In particular, we reject the null hypothesis if the &lt;strong&gt;p-value&lt;/strong&gt;, i.e. the probability of observing under the null hypothesis a statistic as or more extreme than the sample statistic, is lower than the significance level $\alpha$.&lt;/p&gt;
&lt;p&gt;Remember that rejecting the null hypothesis does not imply accepting the alternative hypothesis.&lt;/p&gt;
&lt;h2 id=&#34;peeking&#34;&gt;Peeking&lt;/h2&gt;
&lt;p&gt;Suppose that halfway through the experiment we were to &lt;strong&gt;peek at the data&lt;/strong&gt; and notice that, for that intermediate value of the test statistic, we would reject the null hypothesis. Should we stop the experiment? If we do, what happens?&lt;/p&gt;
&lt;p&gt;The answer is that we &lt;strong&gt;should not stop&lt;/strong&gt; the experiment. If we do, the test would not achieve the desired significance level or, in other terms, our confidence intervals would have the &lt;strong&gt;wrong coverage&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s see what I mean with an &lt;strong&gt;example&lt;/strong&gt;. Suppose our &lt;strong&gt;data generating process&lt;/strong&gt; is a &lt;a href=&#34;https://en.wikipedia.org/wiki/Normal_distribution&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;standard normal distribution&lt;/a&gt; with unknown mean $\mu$ and known variance $\sigma=1$: $X \sim N(\mu,1)$.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;hypothesis&lt;/strong&gt; that we wish to test is&lt;/p&gt;
&lt;p&gt;$$
\begin{align}
H_0: \quad &amp;amp; \mu = 0
\newline
H_1: \quad &amp;amp; \mu = 0.1
\end{align}
$$&lt;/p&gt;
&lt;p&gt;After each observation $n$, we compute the &lt;a href=&#34;https://en.wikipedia.org/wiki/Z-test&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;z test statistic&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;$$
z = \frac{\bar X_n - \mu_0}{\frac{\sigma}{\sqrt{n}}} = \frac{\bar X_n - 0}{\frac{1}{\sqrt{n}}} = \bar X_n * \sqrt{n}
$$&lt;/p&gt;
&lt;p&gt;where $\bar X_n$ is the &lt;a href=&#34;https://en.wikipedia.org/wiki/Sample_mean_and_covariance&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;sample mean&lt;/a&gt; from a sample $X_1, X_2, &amp;hellip;, X_n$, of size $n$, $\sigma$ is the standard deviation of the population, and $\mu_0$ is the population mean, under the null hypothesis. The term in the denominator, $\frac{\sigma}{\sqrt{n}}$, is the variance of the sample mean. Under the null hypothesis of zero mean, the test statistic is distributed as a standard normal distribution with zero mean and unit variance, $N(0,1)$.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s code the test in Python. I import some code from &lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/src/utils.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;utils&lt;/code&gt;&lt;/a&gt; to make the plots prettier.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;%matplotlib inline
%config InlineBackend.figure_format = &#39;retina&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from src.utils import *

zstat = lambda x: np.mean(x) * np.sqrt(len(x))
zstat.__name__ = &#39;z-statistic&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Suppose we want a test with significance level $\alpha=0.05$ and power $1-\beta=0.8$. What sample size $n$ do we need?&lt;/p&gt;
&lt;p&gt;We need a sample size such that&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;The probability of rejecting the null hypothesis $H_0$, when $H_0$ is &lt;em&gt;true&lt;/em&gt;, is at most $\alpha=0.05$&lt;/li&gt;
&lt;li&gt;The probability of &lt;em&gt;not&lt;/em&gt; rejecting the null hypothesis $H_0$, when $H_0$ is &lt;em&gt;false&lt;/em&gt; (i.e. $H_1$ is true), is at most $\beta=0.2$&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;I.e. we need to find a &lt;strong&gt;critical value&lt;/strong&gt; $c$ such that&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;$c = \mu_0 + z_{0.95} * \frac{\sigma}{\sqrt{n}}$&lt;/li&gt;
&lt;li&gt;$c = \mu_1 - z_{0.8} * \frac{\sigma}{\sqrt{n}}$&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;where $z_{p}$ is the CDF inverse (or &lt;a href=&#34;https://en.wikipedia.org/wiki/Quantile_function&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;percent point function&lt;/a&gt;) at $p$, and $\mu_i$ are the values of the mean under the different hypotheses.&lt;/p&gt;
&lt;p&gt;If we do not know the &lt;strong&gt;sign&lt;/strong&gt; of the unknown mean $\mu$, we have to run a &lt;strong&gt;two-sided test&lt;/strong&gt;. This means that the maximum probability of type 1 error on each side of the distribution has to be $\alpha/2 = 0.025$, implying $z_{0.975} = 1.96$.&lt;/p&gt;
&lt;p&gt;Combining the two expressions together we can solve for the &lt;strong&gt;required minimum sample size&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;$$
n : \mu_0 + z_{0.975} * \frac{\sigma}{\sqrt{n}} = \mu_1 - z_{0.8} * \frac{\sigma}{\sqrt{n}}
$$&lt;/p&gt;
&lt;p&gt;so that&lt;/p&gt;
&lt;p&gt;$$
n = \left( \sigma * \frac{z_{0.975} + z_{0.8}}{\mu_0 + \mu_1} \right)^2 = \left( 1 * \frac{1.96 + 0.84}{0 + 0.1} \right)^2 = 784.89
$$&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from scipy.stats import norm

n = ( (norm.ppf(0.975) + norm.ppf(0.8)) / 0.1 )**2
print(f&amp;quot;Minimum sample size: {n}&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Minimum sample size: 784.8879734349086
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We need at least 785 observations.&lt;/p&gt;
&lt;p&gt;We can get a better &lt;strong&gt;intuition&lt;/strong&gt; by graphically plotting the two distributions with the critical value. I wrote a function &lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/src/figures.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;plot_test&lt;/code&gt;&lt;/a&gt; to draw a standard hypothesis testing setting.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from src.figures import plot_test

plot_test(mu0=0, mu1=0.1, alpha=0.05, n=n)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/optimal_stopping_11_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;The critical value is such that, given the distributions under the two hypothesis, the &lt;strong&gt;rejection area&lt;/strong&gt; in red is equal to $\alpha$. The sample size $n$ is such that it shrinks the variance of the two distributions so that the area in green is equal to $\beta$.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s now &lt;strong&gt;simulate an experiment&lt;/strong&gt; in which we draw an ordered sequence of observations and, after each observation, we compute the value of the test statistic.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def experiment(f_stat, mu=0, n=785, seed=1):
    np.random.seed(seed) # Set seed
    I = np.arange(1, n+1) # Observation index
    x = np.random.normal(mu, 1, n) # Observation value
    stat = [f_stat(x[:i]) for i in I] # Value of the test statistic so far
    df = pd.DataFrame({&#39;i&#39;: I, &#39;x&#39;: x, f_stat.__name__: stat}) # Generate dataframe
    return df
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s have a look at what a sample looks like.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df = experiment(zstat)
df.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;i&lt;/th&gt;
      &lt;th&gt;x&lt;/th&gt;
      &lt;th&gt;z-statistic&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1.624345&lt;/td&gt;
      &lt;td&gt;1.624345&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;-0.611756&lt;/td&gt;
      &lt;td&gt;0.716009&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;-0.528172&lt;/td&gt;
      &lt;td&gt;0.279678&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;-1.072969&lt;/td&gt;
      &lt;td&gt;-0.294276&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;0.865408&lt;/td&gt;
      &lt;td&gt;0.123814&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;We can now plot the time trend of the test statistic as we accumulate observations during the sampling process. I also mark with horizontal lines the values for rejection of the null hypothesis of a test with $\alpha = 0.05$: $z_{0.025} = -1.96$ and $z_{0.975} = 1.96$.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def plot_experiment(df, ybounds, **kwargs):
    sns.lineplot(data=df, x=&#39;i&#39;, y=df.columns[2], **kwargs)
    for ybound in ybounds:
        sns.lineplot(x=df[&#39;i&#39;], y=ybound, lw=1.5, color=&#39;black&#39;)
    plt.title(f&#39;{df.columns[2]} with sequential sampling&#39;)
    plt.yticks([0, ybounds[0], ybounds[1]])
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plot_experiment(df, ybounds=[-1.96, 1.96])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/optimal_stopping_18_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;In this case, the test never crosses the critical values. Therefore, peeking does not have an effect. We would not have stopped the experiment prematurely.&lt;/p&gt;
&lt;p&gt;What would happen if we were &lt;strong&gt;repeating&lt;/strong&gt; the experiment many times? Since the data is generated under the null hypothesis, $H_0: \mu = 0$, we expect to reject it only $\alpha=5%$ of the times.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s simulate the data-generating process $K=100$ times.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def simulate_experiments(f_stat, ybounds, xmin=0, early_stop=False, mu=0, K=100, n=785, **kwargs):
    # Count experiment durations
    stops = np.zeros(K) * n
    
    # Perform K simulations
    for k in range(K):
        # Draw data
        df = experiment(f_stat, mu=mu, seed=k, n=n)
        vals = df[f_stat.__name__].values
        
        # If early stop, check early violations (during sampling)
        if early_stop:
            violations = (vals[xmin:] &amp;gt; max(ybounds)) + (vals[xmin:] &amp;lt; min(ybounds))
        if early_stop and any(violations):
            end = 1 + xmin + np.where(violations)[0][0]
            plot_experiment(df.iloc[:end, :], ybounds, **kwargs)
            stops[k] = end * np.sign(df[f_stat.__name__].values[end])
        
        # Otherwise, only check violations of last value
        elif (vals[-1] &amp;gt; max(ybounds)) or (vals[-1] &amp;lt; min(ybounds)):
            plot_experiment(df, ybounds, **kwargs)
            stops[k] = len(df) * np.sign(vals[-1])
        
        # Plot all other observations in grey
        else: 
            plot_experiment(df, ybounds, color=&#39;grey&#39;, alpha=0.1, lw=1)
    
    # Print diagnostics
    pct_up = sum(stops&amp;gt;0)/sum(stops!=0)*100
    print(f&#39;Bounds crossed: {sum(stops!=0)} ({pct_up:.0f}% upper, {100-pct_up:.0f}% lower)&#39;)
    print(f&#39;Average experiment duration: {(sum(np.abs(stops)) + n*sum(stops==0))/ len(stops) :.0f}&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We plot the distribution of the z-statistic over samples .&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;simulate_experiments(zstat, ybounds=[-1.96, 1.96], early_stop=False);
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Bounds crossed: 3 (67% upper, 33% lower)
Average experiment duration: 785
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/optimal_stopping_22_1.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;In the figure above, I have highlighted the experiments for which we reject the null hypothesis &lt;strong&gt;without peeking&lt;/strong&gt;, i.e. given the value of the z test statistic &lt;strong&gt;at the end of the sampling&lt;/strong&gt; process. Only in 3 experiments the final value lies outside the critical values, so that we reject the null hypothesis. This means a &lt;strong&gt;rejection rate&lt;/strong&gt; of 3% which is very close to the expected rejection rate of $\alpha=0.05$ (under the null).&lt;/p&gt;
&lt;p&gt;What if instead we were &lt;strong&gt;impatient&lt;/strong&gt; and, after collecting the first 100 observations, we were stopping &lt;strong&gt;as soon as&lt;/strong&gt; we saw the z-statistic crossing the boundaries?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;stops_zstat_h0 = simulate_experiments(zstat, xmin=99, ybounds=[-1.96, 1.96], early_stop=True, lw=2);
plt.vlines(100, ymin=plt.ylim()[0], ymax=plt.ylim()[1], color=&#39;k&#39;, lw=1, ls=&#39;--&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Bounds crossed: 29 (45% upper, 55% lower)
Average experiment duration: 644
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/optimal_stopping_24_1.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;In the figure above, I have highlighted the experiments in which the values of the z-statistic crosses one of the boundaries, from the 100th observation onwards. This happens in 29 simulations out of 100, which implies a &lt;strong&gt;rejection rate&lt;/strong&gt; of 25%, which is very far from the expected rejection rate of $\alpha=0.05$ (under the null hypothesis). Peaking &lt;strong&gt;distorts&lt;/strong&gt; the significance level of the test.&lt;/p&gt;
&lt;p&gt;Potential solutions are:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;sequential probability ratio tests&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;sequential triangular testing&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;group sequential testing&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Before analyzing these sequential testing procedures, we first need to introduce the &lt;strong&gt;likelihood ratio test&lt;/strong&gt;.&lt;/p&gt;
&lt;h2 id=&#34;likelihood-ratio-test&#34;&gt;Likelihood Ratio Test&lt;/h2&gt;
&lt;p&gt;The likelihood ratio test is a test that tries to assess the likelihood that the observed data was generated by either one of two competing statistical models. &lt;/p&gt;
&lt;p&gt;In order to perform the likelihood ratio test for hypothesis testing, we need the data generating process to be fully specified under both hypotheses. For example, this would be the case with the following hypotheses:&lt;/p&gt;
&lt;p&gt;$$
\begin{align}
H_0: \quad &amp;amp; \mu=0
\newline
H_1: \quad &amp;amp; \mu=0.1
\end{align}
$$&lt;/p&gt;
&lt;p&gt;In this case, we say that the statistical test is fully specified. If the alternative hypothesis was $H_1: \mu \neq 0$, then the data generating process would not be specified under the alternative hypothesis. &lt;/p&gt;
&lt;p&gt;When a statistical test is fully specified, we can compute the likelihood ratio as the the ratio of the &lt;a href=&#34;https://en.wikipedia.org/wiki/Likelihood_function&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;likelihood function&lt;/a&gt; under the two hypotheses.&lt;/p&gt;
&lt;p&gt;$$
\Lambda (X) = \frac{\mathcal L (\theta_1 \ | \ X)}{\mathcal L (\theta_0 \ | \ X)}
$$&lt;/p&gt;
&lt;p&gt;The likelihood-ratio test provides a decision rule as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;If $\Lambda&amp;gt;c$, reject $H_{0}$;&lt;/li&gt;
&lt;li&gt;If $\Lambda&amp;lt;c$, do not reject $H_{0}$;&lt;/li&gt;
&lt;li&gt;If $\Lambda =c$, reject with probability $q$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The values $c$ and $q$ are usually chosen to obtain a specified significance level $\alpha$.&lt;/p&gt;
&lt;p&gt;The &lt;a href=&#34;https://en.wikipedia.org/wiki/Neyman%E2%80%93Pearson_lemma&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Neyman–Pearson lemma&lt;/strong&gt;&lt;/a&gt; states that this likelihood-ratio test is the most powerful among all level $\alpha$ tests for this case.&lt;/p&gt;
&lt;h3 id=&#34;special-case-testing-mean-of-normal-distribution&#34;&gt;Special Case: testing mean of normal distribution&lt;/h3&gt;
&lt;p&gt;Let&amp;rsquo;s go back to our example where data is coming from a normal distribution with unknown mean $\mu$ and known variance $\sigma^2$ and we want to perform the following test&lt;/p&gt;
&lt;p&gt;$$
\begin{align}
H_0: \quad &amp;amp; \mu = 0 ,
\newline
H_1: \quad &amp;amp; \mu = 0.1
\end{align}
$$&lt;/p&gt;
&lt;p&gt;The likelihood of the normal distribution with unknown mean $\mu$ and known variance $\sigma^2$ is&lt;/p&gt;
&lt;p&gt;$$
\mathcal L(\mu) = \left( \frac{1}{\sqrt{2 \pi} \sigma } \right)^n e^{- \sum_{i=1}^{n} \frac{(X_i - \mu)^2}{2 \sigma^2}}
$$&lt;/p&gt;
&lt;p&gt;So that the likelihood ratio under the two hypotheses is&lt;/p&gt;
&lt;p&gt;$$
\Lambda(X) = \frac{\mathcal L (0.1, \sigma^2)}{\mathcal L (0, \sigma^2)} = \frac{e^{- \sum_{i=1}^{n} \frac{(X_i - 0.1)^2}{2 \sigma^2}}}{e^{- \sum_{i=1}^{n} \frac{(X_i)^2}{2 \sigma^2}}}
$$&lt;/p&gt;
&lt;p&gt;We now have all the ingredients to move on to the final purpose of this blog post: the Sequential Probability Ratio Test.&lt;/p&gt;
&lt;h2 id=&#34;sequential-probability-ratio-test&#34;&gt;Sequential Probability Ratio Test&lt;/h2&gt;
&lt;p&gt;Given a pair of fully specified hypotheses, say $H_{0}$ and $H_{1}$, the &lt;strong&gt;first step&lt;/strong&gt; of the sequential probability ratio test is to calculate the log-likelihood ratio test $\log (\Lambda_{i})$, as new data arrive: with $S_{0}=0$, then, for $i=1,2,&amp;hellip;,$&lt;/p&gt;
&lt;p&gt;$$
S_{i} = S_{i-1} + \log(\Lambda_{i})
$$&lt;/p&gt;
&lt;p&gt;The stopping rule is a simple thresholding scheme:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$S_{i}\geq b$: Accept $H_{1}$&lt;/li&gt;
&lt;li&gt;$S_{i}\leq a$: Accept $H_{0}$&lt;/li&gt;
&lt;li&gt;$a&amp;lt;S_{i}&amp;lt;b$: continue monitoring (critical inequality)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;where $a$ and $b$ ($-\infty&amp;lt;a&amp;lt;0&amp;lt;b&amp;lt;\infty$) depend on the desired type I and type II errors, $\alpha$  and $\beta$.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.jstor.org/stable/2235829&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Wald (1945)&lt;/a&gt; shows that the choice of the following boundaries delivers a test with expected probability of type 1 and 2 error not greater than $\alpha$ and $\beta$, respectively.&lt;/p&gt;
&lt;p&gt;$$
a \approx \log {\frac  {\beta }{1-\alpha }} \quad \text{and} \quad  b \approx \log {\frac  {1-\beta }{\alpha }}
$$&lt;/p&gt;
&lt;p&gt;The equations are approximations because of the discrete nature of the data generating process.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.jstor.org/stable/2235638&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Wald and Wolfowitz (1948)&lt;/a&gt; have proven that a test with these boundaries is the most powerful sequential probability ratio test, i.e. all SPR tests with the same power and significance require at least the same amount of observations.&lt;/p&gt;
&lt;h3 id=&#34;special-case-testing-null-effect&#34;&gt;Special Case: testing null effect&lt;/h3&gt;
&lt;p&gt;Let&amp;rsquo;s go back to our example where data is coming from a normal distribution with unknown mean $\mu$ and known variance $\sigma^2$ and hypotheses $H_0: \ \mu = 0$ and $H_1: \ \mu = 0.1$.&lt;/p&gt;
&lt;p&gt;We have seen that the likelihood ratio with a sample of size $n$ is&lt;/p&gt;
&lt;p&gt;$$
\Lambda(X) = \frac{\mathcal L (0.1, \sigma^2)}{\mathcal L (0, \sigma^2)} = \frac{e^{- \sum_{i=1}^{n} \frac{(X_i - 0.1)^2}{2 \sigma^2}}}{e^{- \sum_{i=1}^{n} \frac{(X_i)^2}{2 \sigma^2}}}
$$&lt;/p&gt;
&lt;p&gt;Therefore, the log-likelihood (easier to compute) is&lt;/p&gt;
&lt;p&gt;$$
\log (\Lambda(X)) = \left( \sum_{i=1}^{n} \frac{(X_i)^2}{2 \sigma^2} \right) - \left( \sum_{i=1}^{n} \frac{(X_i - 0.1)^2}{2 \sigma^2} \right)
$$&lt;/p&gt;
&lt;h3 id=&#34;simulation&#34;&gt;Simulation&lt;/h3&gt;
&lt;p&gt;We are now ready to perform some simulations. First, let&amp;rsquo;s code the &lt;strong&gt;log likelihood ratio test statistic&lt;/strong&gt; that we have just computed.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;log_lr = lambda x: (np.sum((x)**2) - np.sum((x-0.1)**2) ) / 2
log_lr.__name__ = &#39;log likelihood-ratio&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We now repeat the same experiment we did at the beginning, with one difference: we will compute the log likelihood ratio as a statistic. The data generating process has $\mu=0$, as under the null hypothesis.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df = experiment(log_lr, )
df.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;i&lt;/th&gt;
      &lt;th&gt;x&lt;/th&gt;
      &lt;th&gt;log likelihood-ratio&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1.624345&lt;/td&gt;
      &lt;td&gt;0.157435&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;-0.611756&lt;/td&gt;
      &lt;td&gt;0.091259&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;-0.528172&lt;/td&gt;
      &lt;td&gt;0.033442&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;-1.072969&lt;/td&gt;
      &lt;td&gt;-0.078855&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;0.865408&lt;/td&gt;
      &lt;td&gt;0.002686&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;Let&amp;rsquo;s now compute the optimal bounds, given significance level $\alpha=0.05$ and power $1-\beta=0.8$.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;alpha = 0.05
beta = 0.2

a = np.log( beta / (1-alpha) )
b = np.log( (1-beta) / alpha )
print(f&#39;Optimal bounds : [{a:.3f}, {b:.3f}]&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Optimal bounds : [-1.558, 2.773]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Since significance and (one minus) power are different, the bound for the null hypothesis is much &lt;strong&gt;closer&lt;/strong&gt; than the bound for the alternative hypothesis. This means that, in case of an intermediate effect of $\mu=0.05$, we will be more likely to accept the null hypothesis $H_0: \mu = 0$ than the alternative $H_1: \mu = 0.1$.&lt;/p&gt;
&lt;p&gt;We can plot the distribution of the likelihood ratio over samples drawn under the null hypothesis $H_0: \mu = 0$.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plot_experiment(df, ybounds=[a,b])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/optimal_stopping_43_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;In this particular case, the test is inconclusive within our sampling framework. We need to &lt;strong&gt;collect more data&lt;/strong&gt; in order to come to a decision.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plot_experiment(experiment(log_lr, n=789), ybounds=[a,b]);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/optimal_stopping_45_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;It takes 789 observations to reach to a conclusion, while before the sample size was 785. This test procedure can require a &lt;strong&gt;larger sample size&lt;/strong&gt; than the previous one. Is it true on average?&lt;/p&gt;
&lt;p&gt;What would happen if we were to repeat the experiment $K=100$ times?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;simulate_experiments(log_lr, ybounds=[a, b], early_stop=True, lw=1.5);
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Bounds crossed: 96 (4% upper, 96% lower)
Average experiment duration: 264
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/optimal_stopping_47_1.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;We get a decision for 96 simulations out of 100 and for 96% of them, it&amp;rsquo;s the correct decision. Therefore, our rejection rate is very close to the expected $\alpha=0.05$ (under the null hypothesis).&lt;/p&gt;
&lt;p&gt;However, for 4 experiments, the test is inconclusive. What would happen if we were to sample until we reach a conclusion in each experiment?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;simulate_experiments(log_lr, ybounds=[a,b], early_stop=True, lw=1.5, n=1900);
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Bounds crossed: 100 (4% upper, 96% lower)
Average experiment duration: 275
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/optimal_stopping_49_1.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;As we can see from the plot, in one particularly unlucky experiment, we need to collect 1900 observations before coming to a conclusion. However, despite this outlier, the &lt;strong&gt;average experiment duration&lt;/strong&gt; is an astounding 275 samples, almost a third of the original sample size of 785.&lt;/p&gt;
&lt;p&gt;What would happen if instead the alternative hypothesis $H_1: \mu = 0.1$ was true?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;simulate_experiments(log_lr, ybounds=[a,b], early_stop=True, mu=0.1, lw=1, n=2100);
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Bounds crossed: 100 (84% upper, 16% lower)
Average experiment duration: 443
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/optimal_stopping_51_1.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;In this case, we make the correct decision in only 84% of the simulations, which is very close to the expected value of 80% (under the alternative hypothesis), i.e. the power of the experiment, 1-β.&lt;/p&gt;
&lt;p&gt;Moreover, also under the alternative hypothesis we need a significantly lower sample size: just 443 observation, on average.h a conclusion in 78/100 experiments we need just 1/3 of the samples!&lt;/p&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;In this post, we have seen the dangers of &lt;strong&gt;peeking&lt;/strong&gt; during a randomized experiment. Prematurely stopping a test can be dangerous since it distorts inference, biasing the expected rejection rates.&lt;/p&gt;
&lt;p&gt;Does it mean that we always need to perform tests with a pre-specified sample size? No! There exist procedures that allow for optimal stopping. These procedures were born for a specific purpose: reducing the sample size as much as possible, without sacrificing accuracy. The first and most known is the Sequential Probability Ratio Test, defined by Wallis as &amp;ldquo;&lt;em&gt;the most powerful and
seminal statistical ideas of the past third of a century&lt;/em&gt;&amp;rdquo; (in 1980).&lt;/p&gt;
&lt;p&gt;The SPRT was not only a powerful tool during war time but keeps being used today for very practical purposes (see for example &lt;a href=&#34;https://netflixtechblog.com/improving-experimentation-efficiency-at-netflix-with-meta-analysis-and-optimal-stopping-d8ec290ae5be&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Netflix&lt;/a&gt;, &lt;a href=&#34;https://eng.uber.com/xp/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Uber&lt;/a&gt;).&lt;/p&gt;
&lt;h3 id=&#34;references&#34;&gt;References&lt;/h3&gt;
&lt;p&gt;[1] A. Wald, &lt;a href=&#34;https://www.jstor.org/stable/2235829&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Sequential tests of statistical hypotheses&lt;/a&gt; (1945), &lt;em&gt;The Annal of Mathematical Statistics&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;[2] A. Wald and J Wolfowitz, &lt;a href=&#34;https://www.jstor.org/stable/2235638&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Optimum character of the sequential probability ratio test&lt;/a&gt; (1948), &lt;em&gt;The Annals of Mathematical Statistics&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;[3] W. A. Wallis, &lt;a href=&#34;https://www.tandfonline.com/doi/abs/10.1080/01621459.1980.10477469&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The Statistical Research Group, 1942–1945&lt;/a&gt; (1980), &lt;em&gt;Journal of the American Statistical Association&lt;/em&gt;.&lt;/p&gt;
&lt;h3 id=&#34;code&#34;&gt;Code&lt;/h3&gt;
&lt;p&gt;You can find the original Jupyter Notebook here:&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/optimal_stopping.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/matteocourthoud/Blog-Posts/blob/main/optimal_stopping.ipynb&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>From Causal Trees to Forests</title>
      <link>https://matteocourthoud.github.io/post/causal_forests/</link>
      <pubDate>Mon, 10 Jul 2023 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/post/causal_forests/</guid>
      <description>&lt;p&gt;&lt;em&gt;How to use regression trees to do policy targeting.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;In my &lt;a href=&#34;https://medium.com/towards-data-science/understanding-causal-trees-920177462149&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;previous blog post&lt;/a&gt;, we have seen how to use &lt;strong&gt;causal trees&lt;/strong&gt; to estimate heterogeneous treatment effects of a policy. If you haven&amp;rsquo;t read it, I recommend starting there first, since we are going to take the content of that article for granted and start from there.&lt;/p&gt;
&lt;p&gt;Why heterogenous treatment effects (HTE)? The estimation of heterogeneous treatments effects is important because it allows us to do &lt;strong&gt;targeting&lt;/strong&gt;. Knowing which customers are more likely to react to a discount allows a company to spend less money by offering fewer but better targeted discounts. This works also for negative effects: knowing for which patients a certain drug has side effects allows a pharmaceutical company to warn or exclude them from the treatment. There is also a more subtle advantage of estimating heterogeneous treatment effects: knowing &lt;strong&gt;for whom&lt;/strong&gt; a treatment works allows us to better understand &lt;strong&gt;how&lt;/strong&gt; a treatment works. Knowing that the effect of a discount does not depend on the income of its recipient but rather by its buying habits  tells us that maybe it is not a matter of money, but rather a matter of attention or loyalty.&lt;/p&gt;
&lt;p&gt;In this article, we will explore an extention of causal trees: causal forests. Exactly as random forests extend regression trees by averaging multiple bootstrapped trees together, causal forests extend causal trees. The main difference comes from the inference perspective, which is less straighforward. We are also going to see how to compare outputs of different HTE estimation algorithms and how to use them for &lt;strong&gt;policy targeting&lt;/strong&gt;.&lt;/p&gt;
&lt;h2 id=&#34;online-discounts&#34;&gt;Online Discounts&lt;/h2&gt;
&lt;p&gt;For the rest of the article, we resume the toy example used in the &lt;a href=&#34;https://medium.com/towards-data-science/understanding-causal-trees-920177462149&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;causal trees article&lt;/a&gt;: we assume we are an &lt;strong&gt;online store&lt;/strong&gt; and we are interested in understanding whether offering discounts to new customers increases their expenditure in the store.&lt;/p&gt;
&lt;img src=&#34;fig/causal_forests1.jpg&#34; width=&#34;300px&#34;/&gt;
&lt;p&gt;To understand whether and how much the discounts are effective we run an &lt;strong&gt;A/B test&lt;/strong&gt;: whenever a new user visits our online store, we randomly decide whether to offer them the discount or not. I import the data-generating process &lt;code&gt;dgp_online_discounts()&lt;/code&gt; from &lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/src/dgp.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;src.dgp&lt;/code&gt;&lt;/a&gt;. I also import some plotting functions and libraries from &lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/src/utils.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;src.utils&lt;/code&gt;&lt;/a&gt;. To include not only code but also data and tables, I use &lt;a href=&#34;https://deepnote.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Deepnote&lt;/a&gt;, a Jupyter-like web-based collaborative notebook environment.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;%matplotlib inline
%config InlineBackend.figure_format = &#39;retina&#39;

from src.utils import *
from src.dgp import dgp_online_discounts
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;dgp = dgp_online_discounts(n=100_000)
df = dgp.generate_data()
df.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;time&lt;/th&gt;
      &lt;th&gt;device&lt;/th&gt;
      &lt;th&gt;browser&lt;/th&gt;
      &lt;th&gt;region&lt;/th&gt;
      &lt;th&gt;discount&lt;/th&gt;
      &lt;th&gt;spend&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;10.78&lt;/td&gt;
      &lt;td&gt;mobile&lt;/td&gt;
      &lt;td&gt;edge&lt;/td&gt;
      &lt;td&gt;9&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0.46&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;0.57&lt;/td&gt;
      &lt;td&gt;desktop&lt;/td&gt;
      &lt;td&gt;firefox&lt;/td&gt;
      &lt;td&gt;9&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;11.04&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;3.74&lt;/td&gt;
      &lt;td&gt;mobile&lt;/td&gt;
      &lt;td&gt;safari&lt;/td&gt;
      &lt;td&gt;7&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1.81&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;13.37&lt;/td&gt;
      &lt;td&gt;desktop&lt;/td&gt;
      &lt;td&gt;other&lt;/td&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;31.90&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;0.71&lt;/td&gt;
      &lt;td&gt;mobile&lt;/td&gt;
      &lt;td&gt;explorer&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;15.42&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;We have data on 100.000 store visitors, for whom we observe the &lt;code&gt;time&lt;/code&gt; of the day the acessed the website, the &lt;code&gt;device&lt;/code&gt; they use, their &lt;code&gt;browser&lt;/code&gt;, and their geographical &lt;code&gt;region&lt;/code&gt;. We also see whether they were offered the &lt;code&gt;discount&lt;/code&gt;, our treatment, and what is their &lt;code&gt;spend&lt;/code&gt;, the outcome of interest.&lt;/p&gt;
&lt;p&gt;Since the treatment was randomly assigned, we can use a simple &lt;strong&gt;difference-in-means&lt;/strong&gt; estimator to estimate the treatment effect. We expect the treatment and control group to be similar, except for the &lt;code&gt;discount&lt;/code&gt;, therefore we can causally attribute any difference in &lt;code&gt;spend&lt;/code&gt; to the &lt;code&gt;discount&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;smf.ols(&#39;spend ~ discount&#39;, df).fit().summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
      &lt;td&gt;&lt;/td&gt;         &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt; &lt;td&gt;    5.0306&lt;/td&gt; &lt;td&gt;    0.045&lt;/td&gt; &lt;td&gt;  110.772&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    4.942&lt;/td&gt; &lt;td&gt;    5.120&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;discount&lt;/th&gt;  &lt;td&gt;    1.9492&lt;/td&gt; &lt;td&gt;    0.064&lt;/td&gt; &lt;td&gt;   30.346&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    1.823&lt;/td&gt; &lt;td&gt;    2.075&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;The discount seems to be effective: on average the spend in the treatment group increases by 1.95$. But are all customers equally affected?&lt;/p&gt;
&lt;p&gt;To answer this question, we would like to estimate &lt;strong&gt;heterogeneous treatment effects&lt;/strong&gt;, possibly at the individual level.&lt;/p&gt;
&lt;h2 id=&#34;causal-forests&#34;&gt;Causal Forests&lt;/h2&gt;
&lt;p&gt;There are many different options to compute heterogeneous treatment effects. The simplest one is to interact the outcome of interest with a dimension of heterogeneity. The problem with this approach is which variable to pick. Sometimes we have prior information that might guide out actions; for example, we might know that &lt;code&gt;mobile&lt;/code&gt; users on average spend more than &lt;code&gt;desktop&lt;/code&gt; users. Other times, we might be interested in one dimension for business reasons; for example we might want to invest more in a certain &lt;code&gt;region&lt;/code&gt;. However, when we do not extra information we would like this process to be data-driven.&lt;/p&gt;
&lt;p&gt;In the &lt;a href=&#34;https://medium.com/towards-data-science/understanding-causal-trees-920177462149&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;previous article&lt;/a&gt; we have explored one data-drive approach to estimate heterogeneous treatment effects: &lt;strong&gt;causal trees&lt;/strong&gt;. We will now expand them to causal forests. However, before we start, we have to give an introduction to its non-causal cousing: random forests.&lt;/p&gt;
&lt;img src=&#34;fig/causal_forests2.jpg&#34; width=&#34;300px&#34;/&gt;
&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Random_forest&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Random forests&lt;/strong&gt;&lt;/a&gt;, as the name suggests, are an extension of regression trees, adding two separate sources of randomness of top of them. In particular, a random forest algorithm takes the predictions of many different regression trees, each trained on a bootstrapped sample of the data, and averages them together. This procedure is generally known as &lt;a href=&#34;https://en.wikipedia.org/wiki/Bootstrap_aggregating&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;bagging&lt;/strong&gt;&lt;/a&gt;, boostrap-aggregating, and can be applied to any prediction algorithm and is not specific to random forest. The additional source of randomness comes from feature selection since at each split, only a random subset of all the features $X$ is considered for the optimal split.&lt;/p&gt;
&lt;p&gt;These two extra sources of randomness are extremely important and controbute to a superior performance of random forests. First of all, bagging allows random forests to &lt;strong&gt;produce smoother&lt;/strong&gt; prediction than regression trees by averaging multiple discrete predictions. Random feature selection instead allows random forests to &lt;strong&gt;explore the feature space&lt;/strong&gt; more in depth, allowing it to discover more interations than simple regression trees. In fact, there might be interactions between variables that are on their own not very predictive (and therefore would not generate splits) but together very powerful.&lt;/p&gt;
&lt;p&gt;Causal Forests are the equivalent of random forests, but for the estimation of heterogeneous treatment effects, exaxtly as for causal trees and regression trees. Exactly as for Causal Trees, we have a fundamental problem: we are interested in predicting an object that we do not observe: the individua treatment effects $\tau_i$. The solution is to create an auxiliary outcome variable $Y^*$ whose expected value for each single observation is exactly the treatment effect.&lt;/p&gt;
&lt;p&gt;$$
Y_i^* = \frac{Y_i}{D_i \cdot p(X_i) - (1-D_i) \cdot (1-p(X_i))}
$$&lt;/p&gt;
&lt;p&gt;If you want to know more details on why this variable is unbiased for the individual treatment effect, have a look at my &lt;a href=&#34;https://towardsdatascience.com/920177462149&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;previous post&lt;/a&gt; where I go more in detail. In short, you can interpret $Y_i^*$ as the difference-in-means estimator for a single observation.&lt;/p&gt;
&lt;p&gt;Once we have an outcome variable, there are still a couple of things we need to do in order to use Random Forests to estimate heterogeneous treatment effects. First, we need to build trees that have an equal number of treated and control units in each leaf. Second, we need to use different samples to build the tree and evaluate it, i.e. compute the average outcome per leaf. This procedure is often referred to as &lt;strong&gt;honest trees&lt;/strong&gt; and it&amp;rsquo;s extremely helpful for inference, since we can treat the sample of each leaf as independent from the tree structure.&lt;/p&gt;
&lt;p&gt;Before we go into the estimation, let&amp;rsquo;s first generate dummy variables for our categorical variables, &lt;code&gt;device&lt;/code&gt;, &lt;code&gt;browser&lt;/code&gt; and &lt;code&gt;region&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_dummies = pd.get_dummies(df[dgp.X[1:]], drop_first=True)
df = pd.concat([df, df_dummies], axis=1)
X = [&#39;time&#39;] + list(df_dummies.columns)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can now estimate the heterogeneous treatment effects using the Random Forest algorithm. Luckily, we don&amp;rsquo;t have to do all this by hand, but there is a great implementation of Causal Trees and Forests in Microsoft&amp;rsquo;s &lt;a href=&#34;https://econml.azurewebsites.net/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;EconML&lt;/a&gt; package. We will use the &lt;code&gt;CausalForestDML&lt;/code&gt; function. We set a seed for reproducibility.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from econml.dml import CausalForestDML

np.random.seed(0)
forest_model = CausalForestDML(max_depth=3)
forest_model = forest_model.fit(Y=df[dgp.Y], X=df[X], T=df[dgp.D])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Differently from Causal Trees, Causal Forests are harder to interpret since we cannot visualize every single tree. We can use the &lt;code&gt;SingleTreeCateInterpreter&lt;/code&gt; function to plot an equivalent representation of the Causal Forest algorithm.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from econml.cate_interpreter import SingleTreeCateInterpreter
%matplotlib inline

intrp = SingleTreeCateInterpreter(max_depth=2).interpret(forest_model, df[X])
intrp.plot(feature_names=X, fontsize=12)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/causal_forests_20_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;We can interpret the tree diagram exactly as for the Causal Tree model. On the top, we can see the average $Y^*$ in the data, $1.917$. Starting from there, the data gets split into different branches, according to the rules highlighted at the top of each node. For example, the first node splits the data into two groups of size $46878$ and $53122$ depending on whether the &lt;code&gt;time&lt;/code&gt; is later than $11.295$. At the bottom, we have our final partitions, with the predicted values. For example, the leftmost leaf contains $40191$ observation with &lt;code&gt;time&lt;/code&gt; earlier than $11.295$ and non-Safari &lt;code&gt;browser&lt;/code&gt;, for which we predict a spend of $0.264$. Darker node colors indicate higher prediction values.&lt;/p&gt;
&lt;p&gt;The problem with this representation is that, differently from the case of Causal Trees, it is only an interpretation of the model. Since Causal Forests are made of many bootstrapped trees, there is no way to directly inspect each decision tree. One way to understand which feature is most important in detemining the tree split is the so-called &lt;a href=&#34;https://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_importances.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;feature importance&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;fig, ax = plt.subplots()
sns.barplot(x=X, y=forest_model.feature_importances()[0], color=&#39;C0&#39;).set(
    title=&#39;Feature Importances&#39;, ylabel=&#39;Importance&#39;)
ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha=&amp;quot;right&amp;quot;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/causal_forests_22_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Clearly &lt;code&gt;time&lt;/code&gt; is the first dimension of heterogeneity, followed by &lt;code&gt;device&lt;/code&gt; (mobile in particular) and &lt;code&gt;browser&lt;/code&gt; (safari in particular). Other dimensions do not matter much.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s now check the model performance.&lt;/p&gt;
&lt;h3 id=&#34;performance&#34;&gt;Performance&lt;/h3&gt;
&lt;p&gt;Since we control the data generating process, we can do something that is not possible with real data: check the predicted treatment effects against the true ones. The &lt;code&gt;generate_potential_outcomes()&lt;/code&gt; function loads the data with both potential outcomes for each observation, under both treatment (&lt;code&gt;outcome_t&lt;/code&gt;) and control (&lt;code&gt;outcome_c&lt;/code&gt;). Let&amp;rsquo;s start first by evaluating how well the algorithm predicts the effects along the discrete dimensions of the data.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def compute_discrete_effects(df, hte_model):
    temp_df = df.copy()
    temp_df.time = 0
    temp_df = dgp.add_treatment_effect(temp_df)
    temp_df = temp_df.rename(columns={&#39;effect_on_spend&#39;: &#39;True&#39;})
    temp_df[&#39;Predicted&#39;] = hte_model.effect(temp_df[X])
    df_effects = pd.DataFrame()
    for var in X[1:]:
        for effect in [&#39;True&#39;, &#39;Predicted&#39;]:
            v = temp_df.loc[temp_df[var]==1, effect].mean() - temp_df[effect][temp_df[var]==0].mean()
            effect_var = {&#39;Variable&#39;: [var], &#39;Effect&#39;: [effect], &#39;Value&#39;: [v]}
            df_effects = pd.concat([df_effects, pd.DataFrame(effect_var)]).reset_index(drop=True)
    return df_effects, temp_df[&#39;Predicted&#39;].mean()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_effects, avg_effect_notime = compute_discrete_effects(df, forest_model)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;fig, ax = plt.subplots()
sns.barplot(data=df_effects, x=&amp;quot;Variable&amp;quot;, y=&amp;quot;Value&amp;quot;, hue=&amp;quot;Effect&amp;quot;, ax=ax).set(
    xlabel=&#39;&#39;, ylabel=&#39;&#39;, title=&#39;Heterogeneous Treatment Effects&#39;)
ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha=&amp;quot;right&amp;quot;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/causal_forests_28_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;The Causal Forest algorithm is pretty good at predicting the treatment effects related to the categorical variables. As for Causal Trees, this is expected since the algorithm has a very discrete nature. However, differently from Causal Trees, the predictions are more nuanced.&lt;/p&gt;
&lt;p&gt;We can now do a more relevant test: how well the algorithm performs with a continuous variable such as &lt;code&gt;time&lt;/code&gt;? First, let&amp;rsquo;s again isolate the predicted treatment effects on &lt;code&gt;time&lt;/code&gt; and ignore the other covariates.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def compute_time_effect(df, hte_model, avg_effect_notime):
    df_time = df.copy()
    df_time[[X[1:]] + [&#39;device&#39;, &#39;browser&#39;, &#39;region&#39;]] = 0
    df_time = dgp.add_treatment_effect(df_time)
    df_time[&#39;predicted&#39;] = hte_model.effect(df_time[X]) + avg_effect_notime
    return df_time
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_time = compute_time_effect(df, forest_model, avg_effect_notime)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We now plot the predicted treatment effects against the true ones, along the &lt;code&gt;time&lt;/code&gt; dimension.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sns.scatterplot(x=&#39;time&#39;, y=&#39;effect_on_spend&#39;, data=df_time, label=&#39;True&#39;)
sns.scatterplot(x=&#39;time&#39;, y=&#39;predicted&#39;, data=df_time, label=&#39;Predicted&#39;).set(
    ylabel=&#39;&#39;, title=&#39;Heterogeneous Treatment Effects&#39;)
plt.legend(title=&#39;Effect&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/causal_forests_33_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;We can now fully appreciate the difference between Causal Trees and Forests: while in the case of Causal Trees the estimates were essentially a very coarse step function, we can now see how Causal Forests produce &lt;strong&gt;smoother estimates&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;We have now explored the model, it&amp;rsquo;s time to use it!&lt;/p&gt;
&lt;h2 id=&#34;policy&#34;&gt;Policy&lt;/h2&gt;
&lt;p&gt;Suppose that we were considering offering a 4$ discount to new customers that visit our online store.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;cost = 4
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For which customers is the discount effective? We have estimated an average treatment effect of 1.9492$ which means that the discount is not really profitable on average. However, we are now able to target single individuals and we can offer the discount only to a subset of the incoming customers. We will now explore how to do &lt;strong&gt;policy targeting&lt;/strong&gt; and in order to get a better understanding of the quality of the targeting, we will use the Causal Tree model as a reference point.&lt;/p&gt;
&lt;p&gt;We build a Causal Tree using the same &lt;code&gt;CausalForestDML&lt;/code&gt; function but restricting the number of estimators and the forest size to 1.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from econml.dml import CausalForestDML

np.random.seed(0)
tree_model = CausalForestDML(n_estimators=1, subforest_size=1, inference=False, max_depth=3)
tree_model = tree_model.fit(Y=df[dgp.Y], X=df[X], T=df[dgp.D])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next, we split the dataset into a train and a test set. The idea is very similar to &lt;a href=&#34;https://en.wikipedia.org/wiki/Cross-validation_%28statistics%29&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;cross-validation&lt;/strong&gt;&lt;/a&gt;: we use the training set to train the model - in our case the estimator for the heterogeneous treatment effects - and the test set to assess its quality. The main difference is that we do not observe the true outcome in the test dataset. But we can still use the train-test split to compare in-sample predictions with out-of-sample predictions.&lt;/p&gt;
&lt;p&gt;We put 80% of all observations in the training set and 20% in the test set.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_train, df_test = df.iloc[:80_000, :], df.iloc[20_000:,]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;First, let&amp;rsquo;s retrain the models only on the training sample.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;np.random.seed(0)
tree_model = tree_model.fit(Y=df_train[dgp.Y], X=df_train[X], T=df_train[dgp.D])
forest_model = forest_model.fit(Y=df_train[dgp.Y], X=df_train[X], T=df_train[dgp.D])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we can decide on a targeting policy, i.e. decide to which customers we offer the discount. The answer seems simple: we offer the discount to all the customers for whom we anticipate a treatment effect larger than the cost, 4$.&lt;/p&gt;
&lt;p&gt;A visualization tool that allows us to understand on whom the treatment is effective and how, is the so-called &lt;strong&gt;Treatment Operative Characteristic (TOC)&lt;/strong&gt; curve. The name is remindful of the much more famous &lt;a href=&#34;https://en.wikipedia.org/wiki/Receiver_operating_characteristic&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;receiver operating characteristic (ROC)&lt;/a&gt; curve that plots the true positive rate against the false positive rate for different thresholds of a binary classifier. The idea is similar: we plot the average treatment effect for different shares of the treated population. At one extreme, when all customers are treated, and the curve takes value equal to the average treatement effect, while at the other extreme, when only one customer is treated, and the curve takes value equal to the maximum treatment effect.&lt;/p&gt;
&lt;p&gt;Now let&amp;rsquo;s compute the curve.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def compute_toc(df, hte_model, cost, truth=False):
    df_toc = pd.DataFrame()
    for q in np.linspace(0, 1, 101):
        if truth:
            df = dgp.add_treatment_effect(df_test)
            effect = df[&#39;effect_on_spend&#39;]
        else:
            effect = hte_model.effect(df[X])
        ate = np.mean(effect[effect &amp;gt;= np.quantile(effect, 1-q)])
        temp = pd.DataFrame({&#39;q&#39;: [q], &#39;ate&#39;: [ate]})
        df_toc = pd.concat([df_toc, temp]).reset_index(drop=True)
    return df_toc
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_toc_tree = compute_toc(df_train, tree_model, cost)
df_toc_forest = compute_toc(df_train, forest_model, cost)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we can plot the Treatment Operating Curves for the two CATE estimators.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def plot_toc(df_toc, cost, ax, color, title):
    ax.axhline(y=cost, lw=2, c=&#39;k&#39;)
    ax.fill_between(x=df_toc.q, y1=cost, y2=df_toc.ate, where=(df_toc.ate &amp;gt; cost), color=color, alpha=0.3)
    if any(df_toc.ate &amp;gt; cost):
        q = df_toc_tree.loc[df_toc.ate &amp;gt; cost, &#39;q&#39;].values[-1]
    else: 
        q = 0
    ax.axvline(x=q, ymin=0, ymax=0.36, lw=2, c=&#39;k&#39;, ls=&#39;--&#39;)
    sns.lineplot(data=df_toc, x=&#39;q&#39;, y=&#39;ate&#39;, ax=ax, color=color).set(
        title=title, ylabel=&#39;ATT&#39;, xlabel=&#39;Share of treated&#39;, ylim=[1.5, 8.5]) 
    ax.text(0.7, cost+0.1, f&#39;Discount cost: {cost:.0f}$&#39;, fontsize=12)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;fix, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))
plot_toc(df_toc_tree, cost, ax1, &#39;C0&#39;, &#39;TOC - Causal Tree&#39;)
plot_toc(df_toc_forest, cost, ax2, &#39;C1&#39;, &#39;TOC - Causal Forest&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/causal_forests_49_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;As expected, the TOC curve is decreasing for both estimators since the average effect decreases as we increase the share of treated customers. In other words, the more selective we are in releasing discounts, the higher the effect of the coupon, per customer. I have also plotted an horizontal line with the discount cost so that we can interpret the shaded area below the TOC curve and above the cost line as the &lt;strong&gt;expected profits&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;The two algorims predict a similar share of treated, around 20%, with the Causal Forest algorithm targeting slightly more customers. However, they predict very different profits. The Causal Tree algorithm predicts a small and constant margin, while the Causal Forest algorithm predicts a larger and steeper margin. Which algorithm is more accurate?&lt;/p&gt;
&lt;p&gt;In order to compare them, we can evaluate them in the test set. We take the model trained on the training set, we predict the treatment effects and we compare them with the predictions from a model trained on the test set. Note that, differently from machine learning standard testing procedures, there is a substantial &lt;strong&gt;difference&lt;/strong&gt;: in our case, we cannot evaluate our predictions against the ground truth, since the treatment effects are not observed. We can only compare two predictions with each other.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def compute_effect_test(df_test, hte_model, cost, ax, title, truth=False):
    df_test[&#39;Treated&#39;] = hte_model.effect(df_test[X]) &amp;gt; cost
    if truth:
        df_test = dgp.add_treatment_effect(df_test)
        df_test[&#39;Effect&#39;] = df_test[&#39;effect_on_spend&#39;]
    else:
        np.random.seed(0)
        hte_model_test = copy.deepcopy(hte_model).fit(Y=df_test[dgp.Y], X=df_test[X], T=df_test[dgp.D])
        df_test[&#39;Effect&#39;] = hte_model_test.effect(df_test[X])
    df_test[&#39;Cost Effective&#39;] = df_test[&#39;Effect&#39;] &amp;gt; cost
    tot_effect = ((df_test[&#39;Effect&#39;] - cost) * df_test[&#39;Treated&#39;]).sum()
    sns.barplot(data=df_test, x=&#39;Cost Effective&#39;, y=&#39;Treated&#39;, errorbar=None, width=0.5, ax=ax, palette=[&#39;C3&#39;, &#39;C2&#39;]).set(
        title=title + &#39;\n&#39;, ylim=[0,1])
    ax.text(0.5, 1.08, f&#39;Total effect: {tot_effect:.2f}&#39;, fontsize=14, ha=&#39;center&#39;)
    return 
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;fix, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))
compute_effect_test(df_test, tree_model, cost, ax1, &#39;Causal Tree&#39;)
compute_effect_test(df_test, forest_model, cost, ax2, &#39;Causal Forest&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/causal_forests_52_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;It seems that the Causal Tree model performs better than the Causal Forest model, with a total net effect of $8386$$ against $4948$$. From the plot we can also understand the source of the discrepancy. The Causal Forest algorithm  tends to be more restrictive and treat fewer customers, making no false positives but also having a lot of false negatives. On the other hand, the Causal Tree algorithm, is much more generous and distributes the &lt;code&gt;discount&lt;/code&gt; to mamy more new customers. This translates in both more true positives but also false positives. The net effect seem to favor the causal tree algorithm.&lt;/p&gt;
&lt;p&gt;Normally, we would stop here since there is not much more we can do. However, in our case, we have access to the &lt;strong&gt;true data generating process&lt;/strong&gt;. Therefore we can check the ground-truth accuracy of the two algorithms.&lt;/p&gt;
&lt;p&gt;First, let&amp;rsquo;s compare them in terms of prediction error of the treatment effects. For each algorithm we compute the &lt;a href=&#34;https://en.wikipedia.org/wiki/Mean_squared_error&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;mean squared error&lt;/a&gt; of the treatment effects.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from sklearn.metrics import mean_squared_error as mse

def compute_mse_test(df_test, hte_model):
    df_test = dgp.add_treatment_effect(df_test)
    print(f&amp;quot;MSE = {mse(df_test[&#39;effect_on_spend&#39;], hte_model.effect(df_test[X])):.4f}&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;compute_mse_test(df_test, tree_model)
compute_mse_test(df_test, forest_model)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;MSE = 0.9035
MSE = 0.5555
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The Random Forest model better predicts the average treatment effect, with a mean squared error of $0.5555$ instead of $0.9035$.&lt;/p&gt;
&lt;p&gt;Does this map into a &lt;strong&gt;better targeting&lt;/strong&gt;? We can now replicate the same barplot we did above, to understand how well the two algorithms perform in terms of policy targeting.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;fix, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))
compute_effect_test(df_test, tree_model, cost, ax1, &#39;Causal Tree&#39;, True)
compute_effect_test(df_test, forest_model, cost, ax2, &#39;Causal Forest&#39;, True)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/causal_forests_57_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;The plot is very similar, but the result differ substantially. In fact, the Causal Forest algorithm now outperforms the Causal Tree algorithm with a total effect of $10395$$ compared to $8828$$. Why this sudden difference?&lt;/p&gt;
&lt;p&gt;To better understand the source of the discrepancy let&amp;rsquo;s plot the TOC based on the ground truth.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_toc = compute_toc(df_test, tree_model, cost, True)

fix, ax = plt.subplots(1, 1, figsize=(7, 5))
plot_toc(df_toc, cost, ax, &#39;C2&#39;, &#39;TOC - Ground Truth&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/causal_forests_59_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;As we can see, the TOC is very skewed and there exist a few customers with very high average treatment effects. The Random Forest algorothm is better able to indentify them and therefore is overall more effective, despite targeting fewer customers.&lt;/p&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;In this post, we have seen a very powerful algorithm for the estimation of heterogeneous treatment effects: &lt;strong&gt;causal forests&lt;/strong&gt;. Causal forests are built on the same principle of causal trees, but benefit from a much deeper exploration of the parameter space and bagging.&lt;/p&gt;
&lt;p&gt;We have also seen how to use the estimates of the heterogeneous treatment effects to perform policy &lt;strong&gt;targeting&lt;/strong&gt;. By identifying users with the highest treatment effects, we are able to make profitable a policy that wouldn&amp;rsquo;t be otherwise. We have also see how the objective of policy targeting might differ from the objective of heterogeneous treatment effect estimation since the tails of the distribution might be more relevant than the average.&lt;/p&gt;
&lt;h3 id=&#34;references&#34;&gt;References&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;S. Athey, G. Imbens, &lt;a href=&#34;https://www.pnas.org/doi/abs/10.1073/pnas.1510489113&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Recursive partitioning for heterogeneous causal effects&lt;/a&gt; (2016), &lt;em&gt;PNAS&lt;/em&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;S. Wager, S. Athey, &lt;a href=&#34;https://www.tandfonline.com/doi/full/10.1080/01621459.2017.1319839&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Estimation and Inference of Heterogeneous Treatment Effects using Random Forests&lt;/a&gt; (2018), &lt;em&gt;Journal of the American Statistical Association&lt;/em&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;S. Athey, J. Tibshirani, S. Wager, &lt;a href=&#34;https://projecteuclid.org/journals/annals-of-statistics/volume-47/issue-2/Generalized-random-forests/10.1214/18-AOS1709.full&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Generalized Random Forests&lt;/a&gt; (2019). &lt;em&gt;The Annals of Statistics&lt;/em&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;M. Oprescu, V. Syrgkanis, Z. Wu, &lt;a href=&#34;http://proceedings.mlr.press/v97/oprescu19a.html?ref=https://githubhelp.com&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Orthogonal Random Forest for Causal Inference&lt;/a&gt; (2019). &lt;em&gt;Proceedings of the 36th International Conference on Machine Learning&lt;/em&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;related-articles&#34;&gt;Related Articles&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://towardsdatascience.com/b63dc69e3d8c&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;DAGs and Control Variables&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://towardsdatascience.com/99bf5cffa0d9&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Matching, Weighting, or Regression?&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://towardsdatascience.com/8a9c1e340832&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Understanding Meta Learners&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://towardsdatascience.com/ed4097dab27a&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Understanding AIPW, the Doubly-Robust Estimator&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://towardsdatascience.com/920177462149&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Understanding Causal Trees&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;code&#34;&gt;Code&lt;/h3&gt;
&lt;p&gt;You can find the original Jupyter Notebook here:&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/causal_forests.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/causal_forests.ipynb&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Goodbye Scatterplot, Welcome Binned Scatterplot</title>
      <link>https://matteocourthoud.github.io/post/binned_scatterplot/</link>
      <pubDate>Mon, 10 Jul 2023 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/post/binned_scatterplot/</guid>
      <description>&lt;p&gt;When we want to visualize the relationship between two continuous variables, the go-to plot is the &lt;strong&gt;scatterplot&lt;/strong&gt;. It&amp;rsquo;s a very intuitive visualization tool that allows us to directly look at the data. However, when we have a lot of data and/or when the data is skewed, scatterplots can be too noisy to be informative.&lt;/p&gt;
&lt;p&gt;In this blog post, I am going to review a very powerful alternative to the scatterplot to visualize correlations between two variables: the &lt;strong&gt;binned scatterplot&lt;/strong&gt;. Binned scatterplots are not only a great visualization tool, but they can also be used to do inference on the conditional distribution of the dependent variable.&lt;/p&gt;
&lt;h2 id=&#34;the-scatterplot&#34;&gt;The Scatterplot&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s start with an example. Suppose we are an &lt;strong&gt;online marketplace&lt;/strong&gt; where multiple firms offer goods that consumer can efficiently browse, compare and buy. Our &lt;strong&gt;dataset&lt;/strong&gt; consists in a snapshot of the firms active on the marketplace.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s load the data and have a look at it. You can find the code for the data generating process &lt;a href=&#34;&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;%matplotlib inline
%config InlineBackend.figure_format = &#39;retina&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from src.utils import *
from src.dgp import dgp_marketplace

df = dgp_marketplace().generate_data(N=10_000)
df.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;age&lt;/th&gt;
      &lt;th&gt;sales&lt;/th&gt;
      &lt;th&gt;online&lt;/th&gt;
      &lt;th&gt;products&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;0.312777&lt;/td&gt;
      &lt;td&gt;450.858091&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;1.176221&lt;/td&gt;
      &lt;td&gt;1121.882449&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;1.764048&lt;/td&gt;
      &lt;td&gt;2698.714549&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;1.082742&lt;/td&gt;
      &lt;td&gt;1627.746386&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;3.156503&lt;/td&gt;
      &lt;td&gt;1464.593939&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;We have information on 10.000 firms. For each firm we know:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;age&lt;/code&gt;: the age of the firm&lt;/li&gt;
&lt;li&gt;&lt;code&gt;sales&lt;/code&gt;: the monthly sales from last month&lt;/li&gt;
&lt;li&gt;&lt;code&gt;online&lt;/code&gt;: whether the firm is only active online&lt;/li&gt;
&lt;li&gt;&lt;code&gt;products&lt;/code&gt;: the number of products that the firm offers&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Suppose we are interested in understanding the relationship between &lt;code&gt;age&lt;/code&gt; and &lt;code&gt;sales&lt;/code&gt;. What is the &lt;strong&gt;life-cycle&lt;/strong&gt; of sales?&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s start with a simple &lt;strong&gt;scatterplot&lt;/strong&gt; of &lt;code&gt;sales&lt;/code&gt; over &lt;code&gt;age&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sns.scatterplot(x=&#39;age&#39;, y=&#39;sales&#39;, data=df);
plt.title(&amp;quot;Sales by firm&#39;s age&amp;quot;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/binned_scatterplot_7_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;The plot is extremely &lt;strong&gt;noisy&lt;/strong&gt;. We have a lot of observations, therefore, it is very difficult to visualize them all. If we had to guess, we could say that the relationship looks negative (&lt;code&gt;sales&lt;/code&gt; decrease with &lt;code&gt;age&lt;/code&gt;), but it would be a very uninformed guess.&lt;/p&gt;
&lt;p&gt;We are now going to explore some plausible tweaks and alternatives.&lt;/p&gt;
&lt;h2 id=&#34;scatterplot-alternatives&#34;&gt;Scatterplot Alternatives&lt;/h2&gt;
&lt;p&gt;What can we do when we have an extremely dense scatterplot? One solution could be to plot the &lt;strong&gt;density&lt;/strong&gt; of the observations, instead of the observations themselves.&lt;/p&gt;
&lt;p&gt;There are multiple solutions in Python to visualize the density of a 2-dimensional distribution. A very useful one is &lt;a href=&#34;https://seaborn.pydata.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;seaborn&lt;/a&gt; &lt;a href=&#34;https://seaborn.pydata.org/generated/seaborn.jointplot.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;jointplot&lt;/a&gt;. &lt;code&gt;jointplot&lt;/code&gt; plots the joint distribution of two variables, together with the marginal distributions along the axis. The default option is the scatterplot, but one can also choose to add a regression line (&lt;code&gt;reg&lt;/code&gt;), change the plot to a histogram (&lt;code&gt;hist&lt;/code&gt;), a hexplot (&lt;code&gt;hex&lt;/code&gt;), or a &lt;a href=&#34;https://en.wikipedia.org/wiki/Kernel_density_estimation&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;kernel density estimate&lt;/a&gt; (&lt;code&gt;kde&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s try the &lt;strong&gt;hexplot&lt;/strong&gt;, which is basically a histogram of the data, where the bins are hexagons, in the 2-dimensional space.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;s = sns.jointplot(x=&#39;age&#39;, y=&#39;sales&#39;, data=df, kind=&#39;hex&#39;, );
s.ax_joint.grid(False);
s.ax_marg_y.grid(False);
s.fig.suptitle(&amp;quot;Sales by firm&#39;s age&amp;quot;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/binned_scatterplot_11_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Not much has changed. It looks like the distributions of &lt;code&gt;age&lt;/code&gt; and &lt;code&gt;sales&lt;/code&gt; are both very &lt;strong&gt;skewed&lt;/strong&gt; and, therefore, most of the action is concentrated in a very small subspace.&lt;/p&gt;
&lt;p&gt;Maybe we could remove &lt;strong&gt;outliers&lt;/strong&gt; and zoom-in on the area where most of the data is located. Let&amp;rsquo;s zoom-in on the bottom-left corner, on observations what have &lt;code&gt;age &amp;lt; 3&lt;/code&gt; and &lt;code&gt;sales &amp;lt; 3000&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;s = sns.jointplot(x=&#39;age&#39;, y=&#39;sales&#39;, data=df.query(&amp;quot;age &amp;lt; 3 &amp;amp; sales &amp;lt; 3000&amp;quot;), kind=&amp;quot;hex&amp;quot;);
s.ax_joint.grid(False);
s.ax_marg_y.grid(False);
s.fig.suptitle(&amp;quot;Sales by firm&#39;s age&amp;quot;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/binned_scatterplot_13_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Now there is much less empty space, but it does not look like we are going far. The joint distribution is &lt;strong&gt;still too skewed&lt;/strong&gt;. This is the case when the data follows some power distribution, as it&amp;rsquo;s often the case with business data.&lt;/p&gt;
&lt;p&gt;One solution is to &lt;strong&gt;transform&lt;/strong&gt; the variable, by taking the &lt;a href=&#34;https://en.wikipedia.org/wiki/Natural_logarithm&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;natural logarithm&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df[&#39;log_age&#39;] = np.log(df[&#39;age&#39;])
df[&#39;log_sales&#39;] = np.log(df[&#39;sales&#39;])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can now plot the relationship between the logarithms of &lt;code&gt;age&lt;/code&gt; and &lt;code&gt;sales&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;s = sns.jointplot(x=&#39;log_age&#39;, y=&#39;log_sales&#39;, data=df, kind=&#39;hex&#39;);
s.ax_joint.grid(False);
s.ax_marg_y.grid(False);
s.fig.suptitle(&amp;quot;Sales by firm&#39;s age&amp;quot;, y=1.02);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/binned_scatterplot_17_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;The logarithm definitely helped. Now the data is more spread across space, which means that the visualization is more informative. Moreover, it looks like there is &lt;strong&gt;no relationship&lt;/strong&gt; between the two variables.&lt;/p&gt;
&lt;p&gt;However, there is still &lt;strong&gt;too much noise&lt;/strong&gt;. Maybe data visualization alone is not sufficient do draw a conclusion.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s swap to a more structured approach: &lt;a href=&#34;https://en.wikipedia.org/wiki/Linear_regression&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;linear regression&lt;/strong&gt;&lt;/a&gt;. Let&amp;rsquo;s linearly regress &lt;code&gt;log_sales&lt;/code&gt; on &lt;code&gt;log_age&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;smf.ols(&#39;log_sales ~ log_age&#39;, df).fit().summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
      &lt;td&gt;&lt;/td&gt;         &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt; &lt;td&gt;    7.3971&lt;/td&gt; &lt;td&gt;    0.015&lt;/td&gt; &lt;td&gt;  478.948&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    7.367&lt;/td&gt; &lt;td&gt;    7.427&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;log_age&lt;/th&gt;   &lt;td&gt;    0.1690&lt;/td&gt; &lt;td&gt;    0.010&lt;/td&gt; &lt;td&gt;   16.888&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    0.149&lt;/td&gt; &lt;td&gt;    0.189&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;The regression coefficient for &lt;code&gt;log_age&lt;/code&gt; is &lt;strong&gt;positive&lt;/strong&gt; and statistically significant (i.e. different from zero). It seems that all previous visualizations were very &lt;strong&gt;misleading&lt;/strong&gt;. From none of the graphs above we could have guessed such a strong positive relationship.&lt;/p&gt;
&lt;p&gt;However, maybe this relationship is different for &lt;code&gt;online&lt;/code&gt;-only firms and the rest of the sample. We need to control for this variable in order to avoid &lt;a href=&#34;https://en.wikipedia.org/wiki/Simpson%27s_paradox&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Simpson&amp;rsquo;s Paradox&lt;/a&gt; and, more generally, bias.&lt;/p&gt;
&lt;p&gt;With linear regression, we can &lt;strong&gt;condition the analysis on covariates&lt;/strong&gt;. Let&amp;rsquo;s add the binary indicator for &lt;code&gt;online&lt;/code&gt;-only firms and the variable counting the number of &lt;code&gt;products&lt;/code&gt; to the regression.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;smf.ols(&#39;log_sales ~ log_age + online + products&#39;, df).fit().summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
      &lt;td&gt;&lt;/td&gt;         &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt; &lt;td&gt;    6.5717&lt;/td&gt; &lt;td&gt;    0.037&lt;/td&gt; &lt;td&gt;  176.893&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    6.499&lt;/td&gt; &lt;td&gt;    6.644&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;log_age&lt;/th&gt;   &lt;td&gt;    0.0807&lt;/td&gt; &lt;td&gt;    0.010&lt;/td&gt; &lt;td&gt;    7.782&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    0.060&lt;/td&gt; &lt;td&gt;    0.101&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;online&lt;/th&gt;    &lt;td&gt;    0.1447&lt;/td&gt; &lt;td&gt;    0.027&lt;/td&gt; &lt;td&gt;    5.433&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    0.092&lt;/td&gt; &lt;td&gt;    0.197&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;products&lt;/th&gt;  &lt;td&gt;    0.3456&lt;/td&gt; &lt;td&gt;    0.014&lt;/td&gt; &lt;td&gt;   24.110&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    0.317&lt;/td&gt; &lt;td&gt;    0.374&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;The coefficient for &lt;code&gt;log_age&lt;/code&gt; is still positive and statistically significant, but its &lt;strong&gt;magnitude&lt;/strong&gt; has halved.&lt;/p&gt;
&lt;p&gt;What should we conclude? It seems that &lt;code&gt;sales&lt;/code&gt; increase over age, on average. However, this pattern might be very &lt;strong&gt;non-linear&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Within the linear regression framework, one approach could be to &lt;strong&gt;add extra terms&lt;/strong&gt; such as polynomials (&lt;code&gt;age^2&lt;/code&gt;) or categorical features (e.g. &lt;code&gt;age &amp;lt; 2&lt;/code&gt;). However, it would be really cool if there was a more &lt;strong&gt;flexible&lt;/strong&gt; (i.e. &lt;a href=&#34;https://en.wikipedia.org/wiki/Nonparametric_statistics&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;non-parametric&lt;/a&gt;) approach that could inform us on the relationship between firm &lt;code&gt;age&lt;/code&gt; and &lt;code&gt;sales&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;If only&amp;hellip;&lt;/p&gt;
&lt;h2 id=&#34;the-binned-scatterplot&#34;&gt;The Binned Scatterplot&lt;/h2&gt;
&lt;p&gt;The &lt;strong&gt;binned scatterplot&lt;/strong&gt; is a very powerful tool that provides a &lt;strong&gt;flexible&lt;/strong&gt; and &lt;strong&gt;parsimonious&lt;/strong&gt; way of visualizing and summarizing conditional means (and not only) in large datasets.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;idea&lt;/strong&gt; behind the binned scatterplot is to divide the conditioning variable, &lt;code&gt;age&lt;/code&gt; in our example, into &lt;strong&gt;equally sized bins or quantiles&lt;/strong&gt;, and then plot the conditional mean of the dependent variable, &lt;code&gt;sales&lt;/code&gt; in our example, within each bin.&lt;/p&gt;
&lt;h3 id=&#34;details&#34;&gt;Details&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/1902.09608&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Cattaneo, Crump, Farrell, Feng (2021)&lt;/a&gt; have built an extremely good package for binned scatterplots in R, &lt;a href=&#34;https://cran.r-project.org/web/packages/binsreg/index.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;binsreg&lt;/a&gt;. Moreover, they have ported the package to Python. We can install &lt;code&gt;binsreg&lt;/code&gt; directly from pip using &lt;code&gt;pip install binsreg&lt;/code&gt;. You can find more information on the Python package &lt;a href=&#34;https://pypi.org/project/binsreg/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;, while the original and detailed R package documentation can be found &lt;a href=&#34;https://www.rdocumentation.org/packages/binsreg/versions/0.7/topics/binsreg&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The most important choice when building a binned scatterplot is the &lt;strong&gt;number of bins&lt;/strong&gt;. The trade-off is the usual &lt;a href=&#34;https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;bias-variance trade-off&lt;/strong&gt;&lt;/a&gt;. By picking a higher number of bins, we have more points in the graph. In the extreme, we end up having a standard &lt;strong&gt;scatterplot&lt;/strong&gt; (assuming the conditioning variable is continuous). On the other hand, by decreasing the number bins, the plot will be more stable. However, in the extreme, we will have a &lt;strong&gt;single point&lt;/strong&gt; representing the sample mean.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/1902.09608&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Cattaneo, Crump, Farrell, Feng (2021)&lt;/a&gt; prove that, in the basic binned scatterplot, the number of bins that minimizes the mean squared error is proportional to $n^{1/3}$, where $n$ is the number of observations. Therefore, in general, more observations lead to more bins.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://onlinelibrary.wiley.com/doi/abs/10.1002/smj.3199&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Starr and Goldfarb (2020)&lt;/a&gt; add the following consideration:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;&lt;em&gt;However other elements are also important. For example, holding the distribution of x constant, the more curvilinear the true relationship between x and y is, the more bins the algorithm will select (otherwise mean squared error will increase). This implies that even with large n, few bins will be chosen for relatively flat relationships. The calculation of the optimal number of bins in a basic binned scatterplot thus takes into account the amount and location of variation in the data available to identify the relationship between x and y.&lt;/em&gt;&amp;rdquo;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;It is strongly recommended to use the default optimal number of bins. However, one can also set a customized number of bins in &lt;code&gt;binsreg&lt;/code&gt; with the &lt;code&gt;nbins&lt;/code&gt; option.&lt;/p&gt;
&lt;p&gt;Binned scatterplots however, do not just compute conditional means, for optimally chosen intervals, but they can also provide &lt;strong&gt;inference&lt;/strong&gt; for these means. In particular, we can build &lt;strong&gt;confidence intervals&lt;/strong&gt; around each data point. In the &lt;code&gt;binsreg&lt;/code&gt; package, the option &lt;code&gt;ci&lt;/code&gt; adds confidence intervals to the estimation results. The option takes as input a tuple of parameters &lt;code&gt;(p, s)&lt;/code&gt; and uses a piecewise polynomial of degree &lt;code&gt;p&lt;/code&gt; with &lt;code&gt;s&lt;/code&gt; smoothness constraints to construct the confidence intervals. By default, the confidence intervals are not included in the plot. For what concerns the choice of &lt;code&gt;p&lt;/code&gt; and &lt;code&gt;s&lt;/code&gt;, the &lt;a href=&#34;https://www.rdocumentation.org/packages/binsreg/versions/0.7/topics/binsreg&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;package documentation&lt;/a&gt; reports:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;&lt;em&gt;Recommended specification is ci=c(3,3), which adds confidence intervals based on cubic B-spline estimate of the regression function of interest to the binned scatter plot.&lt;/em&gt;&amp;rdquo;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;binsreg&#34;&gt;Binsreg&lt;/h3&gt;
&lt;p&gt;One problem with the Python version of the package, is that is not very Python-ish. Therefore, I have wrapped the &lt;code&gt;binsreg&lt;/code&gt; package into a function &lt;code&gt;binscatter&lt;/code&gt; that takes care of cleaning and formatting the output in a nicely readable &lt;a href=&#34;https://pandas.pydata.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Pandas&lt;/a&gt; DataFrame.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import binsreg

def binscatter(**kwargs):
    # Estimate binsreg
    est = binsreg.binsreg(**kwargs)
    
    # Retrieve estimates
    df_est = pd.concat([d.dots for d in est.data_plot])
    df_est = df_est.rename(columns={&#39;x&#39;: kwargs.get(&amp;quot;x&amp;quot;), &#39;fit&#39;: kwargs.get(&amp;quot;y&amp;quot;)})
    
    # Add confidence intervals
    if &amp;quot;ci&amp;quot; in kwargs:
        df_est = pd.merge(df_est, pd.concat([d.ci for d in est.data_plot]))
        df_est = df_est.drop(columns=[&#39;x&#39;])
        df_est[&#39;ci&#39;] = df_est[&#39;ci_r&#39;] - df_est[&#39;ci_l&#39;]
    
    # Rename groups
    if &amp;quot;by&amp;quot; in kwargs:
        df_est[&#39;group&#39;] = df_est[&#39;group&#39;].astype(df[kwargs.get(&amp;quot;by&amp;quot;)].dtype)
        df_est = df_est.rename(columns={&#39;group&#39;: kwargs.get(&amp;quot;by&amp;quot;)})

    return df_est
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can now proceed to &lt;strong&gt;estimate&lt;/strong&gt; and &lt;strong&gt;visualize&lt;/strong&gt; the binned scatterplot for &lt;code&gt;age&lt;/code&gt; based on &lt;code&gt;sales&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Estimate binsreg
df_est = binscatter(x=&#39;age&#39;, y=&#39;sales&#39;, data=df, ci=(3,3))
df_est.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/binned_scatterplot_31_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;group&lt;/th&gt;
      &lt;th&gt;age&lt;/th&gt;
      &lt;th&gt;bin&lt;/th&gt;
      &lt;th&gt;isknot&lt;/th&gt;
      &lt;th&gt;mid&lt;/th&gt;
      &lt;th&gt;sales&lt;/th&gt;
      &lt;th&gt;ci_l&lt;/th&gt;
      &lt;th&gt;ci_r&lt;/th&gt;
      &lt;th&gt;ci&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;Full Sample&lt;/td&gt;
      &lt;td&gt;0.016048&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1653.865445&lt;/td&gt;
      &lt;td&gt;1362.722061&lt;/td&gt;
      &lt;td&gt;1893.998686&lt;/td&gt;
      &lt;td&gt;531.276626&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;Full Sample&lt;/td&gt;
      &lt;td&gt;0.049295&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1666.329034&lt;/td&gt;
      &lt;td&gt;1486.504692&lt;/td&gt;
      &lt;td&gt;1890.922562&lt;/td&gt;
      &lt;td&gt;404.417871&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;Full Sample&lt;/td&gt;
      &lt;td&gt;0.086629&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1937.095012&lt;/td&gt;
      &lt;td&gt;1727.248438&lt;/td&gt;
      &lt;td&gt;2124.811346&lt;/td&gt;
      &lt;td&gt;397.562909&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;Full Sample&lt;/td&gt;
      &lt;td&gt;0.125955&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1972.484136&lt;/td&gt;
      &lt;td&gt;1801.125187&lt;/td&gt;
      &lt;td&gt;2243.034755&lt;/td&gt;
      &lt;td&gt;441.909568&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;Full Sample&lt;/td&gt;
      &lt;td&gt;0.167636&lt;/td&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;2142.560866&lt;/td&gt;
      &lt;td&gt;1937.677738&lt;/td&gt;
      &lt;td&gt;2405.785562&lt;/td&gt;
      &lt;td&gt;468.107824&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;The &lt;code&gt;binscatter&lt;/code&gt; function outputs a dataset in which, for each bin of the conditioning variable, &lt;code&gt;age&lt;/code&gt;, we have values and confidence intervals for the outcome variable, &lt;code&gt;sales&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;We can now plot the estimates.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Plot binned scatterplot
sns.scatterplot(x=&#39;age&#39;, y=&#39;sales&#39;, data=df_est);
plt.errorbar(&#39;age&#39;, &#39;sales&#39;, yerr=&#39;ci&#39;, data=df_est, ls=&#39;&#39;, lw=3, alpha=0.2);
plt.title(&amp;quot;Sales by firm&#39;s age&amp;quot;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/binned_scatterplot_33_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;The plot is quite revealing. Now the relationship looks extremely &lt;strong&gt;non-linear&lt;/strong&gt; with a sharp increase in &lt;code&gt;sales&lt;/code&gt; at the beginning of the lifetime of a firm, followed by a plateau.&lt;/p&gt;
&lt;p&gt;Moreover, the plot is also telling us information regarding the distributions of &lt;code&gt;age&lt;/code&gt; and &lt;code&gt;sales&lt;/code&gt;. In fact, the plot is more dense on the left, where the distribution of &lt;code&gt;age&lt;/code&gt; is concentrated. Also, confidence intervals are tighter on the left, where most of the conditional distribution of &lt;code&gt;sales&lt;/code&gt; lies.&lt;/p&gt;
&lt;p&gt;As we already discussed in the previous section, it might be important to control for other variables. For example, the number of &lt;code&gt;products&lt;/code&gt;, since firms that sell more products probably survive longer in the markets and also make more sales.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;binsreg&lt;/code&gt; allows to &lt;strong&gt;condition&lt;/strong&gt; the analysis on any number of variables, with the &lt;code&gt;w&lt;/code&gt; option.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Estimate binsreg
df_est = binscatter(x=&#39;age&#39;, y=&#39;sales&#39;, w=[&#39;products&#39;], data=df, ci=(3,3))

# Plot binned scatterplot
sns.scatterplot(x=&#39;age&#39;, y=&#39;sales&#39;, data=df_est);
plt.errorbar(&#39;age&#39;, &#39;sales&#39;, yerr=&#39;ci&#39;, data=df_est, ls=&#39;&#39;, lw=3, alpha=0.2);
plt.title(&amp;quot;Sales by firm&#39;s age&amp;quot;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/binned_scatterplot_35_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;img/binned_scatterplot_35_2.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Conditional on number of &lt;code&gt;products&lt;/code&gt;, the shape of the &lt;code&gt;sales&lt;/code&gt; life-cycle changes further. Now, after an initial increase in sales, we observe a gradual decrease over time.&lt;/p&gt;
&lt;p&gt;Do &lt;code&gt;online&lt;/code&gt;-only firms have different &lt;code&gt;sales&lt;/code&gt; life-cycles with respect to mixed online-offline firms? We can produce different binned scatterplots &lt;strong&gt;by group&lt;/strong&gt; using the option &lt;code&gt;by&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Estimate binsreg
df_est = binscatter(x=&#39;age&#39;, y=&#39;sales&#39;, by=&#39;online&#39;, w=[&#39;products&#39;], data=df, ci=(3,3))

# Plot binned scatterplot
sns.scatterplot(x=&#39;age&#39;, y=&#39;sales&#39;, data=df_est, hue=&#39;online&#39;);
plt.errorbar(&#39;age&#39;, &#39;sales&#39;, yerr=&#39;ci&#39;, data=df_est.query(&amp;quot;online==0&amp;quot;), ls=&#39;&#39;, lw=3, alpha=0.2);
plt.errorbar(&#39;age&#39;, &#39;sales&#39;, yerr=&#39;ci&#39;, data=df_est.query(&amp;quot;online==1&amp;quot;), ls=&#39;&#39;, lw=3, alpha=0.2);
plt.title(&amp;quot;Sales by firm&#39;s age&amp;quot;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/binned_scatterplot_37_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;img/binned_scatterplot_37_2.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;From the binned scatterplot, we can see that &lt;code&gt;online&lt;/code&gt; products have on average shorter lifetimes, with a higher initial peak in &lt;code&gt;sales&lt;/code&gt;, followed by a sharper decline.&lt;/p&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;In this blog post, we have analyzed a very powerful data visualization tool: the &lt;strong&gt;binned scatterplot&lt;/strong&gt;. In particular, we have seen how to use the &lt;code&gt;binsreg&lt;/code&gt; package to automatically pick the optimal number of bins and perform non-parametric inference on conditional means. However, the &lt;code&gt;binsreg&lt;/code&gt; package offers much more than that and I strongly recommend checking &lt;a href=&#34;https://cran.r-project.org/web/packages/binsreg/binsreg.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;its manual&lt;/a&gt; more in depth.&lt;/p&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;p&gt;[1] E Starr, B Goldfarb, &lt;a href=&#34;https://onlinelibrary.wiley.com/doi/abs/10.1002/smj.3199&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Binned Scatterplots: A Simple Tool to Make Research Easier and Better&lt;/a&gt; (2020), Strategic Management Journal.&lt;/p&gt;
&lt;p&gt;[2] M. D. Cattaneo, R. K. Crump, M. H. Farrell, Y. Feng, &lt;a href=&#34;https://arxiv.org/abs/1902.09608&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;On Binscatter&lt;/a&gt; (2021), working paper.&lt;/p&gt;
&lt;p&gt;[3] P. Goldsmith-Pinkham, &lt;a href=&#34;https://www.youtube.com/watch?v=fg9T2gPZCIs&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Lecture 6. Linear Regression II: Semiparametrics and Visualization&lt;/a&gt;, Applied Metrics PhD Course.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>How to Compare Two or More Distributions</title>
      <link>https://matteocourthoud.github.io/post/comparing_distributions/</link>
      <pubDate>Mon, 10 Jul 2023 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/post/comparing_distributions/</guid>
      <description>&lt;p&gt;&lt;em&gt;A complete guide to comparing distributions, from visualization to statistical tests&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Comparing the empirical distribution of a variable across different groups is a common problem in data science. In particular, in causal inference the problem often arises when we have to &lt;strong&gt;assess the quality of randomization&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;When we want to assess the causal effect of a policy (or UX feature, ad campaign, drug, &amp;hellip;), the golden standard in causal inference are &lt;a href=&#34;https://en.wikipedia.org/wiki/Randomized_controlled_trial&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;randomized control trials&lt;/strong&gt;&lt;/a&gt;, also known as &lt;a href=&#34;https://de.wikipedia.org/wiki/A/B-Test&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;A/B tests&lt;/strong&gt;&lt;/a&gt;. In practice, we select a sample for the study and we randomly split it into a &lt;strong&gt;control&lt;/strong&gt; and a &lt;strong&gt;treatment&lt;/strong&gt; group, and we compare the outcomes between the two groups. Randomization ensures that only difference between the two groups is the treatment, on average, so that we can attribute outcome differences to the treatment effect.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;problem&lt;/strong&gt; is that, despite randomization, the two groups are never identical. However, sometimes, they are not even &amp;ldquo;similar&amp;rdquo;. For example, we might have more males in one group, or older people, etc.. (we usually call these characteristics, &lt;em&gt;covariates&lt;/em&gt; or &lt;em&gt;control variables&lt;/em&gt;). When it happens, we cannot be certain anymore that the difference in the outcome is only due to the treatment and cannot be attributed to the &lt;strong&gt;inbalanced covariates&lt;/strong&gt; instead. Therefore, it is always important, after randomization, to check whether all observed variables are balanced across groups and whether there are no systematic differences. Another option, to be certain ex-ante that certain covariates are balanced, is &lt;a href=&#34;https://en.wikipedia.org/wiki/Stratified_sampling&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;stratified sampling&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;In this blog post, we are going to see different ways to compare two (or more) distributions and assess the magnitude and significance of their difference. We are going to consider two different approaches, &lt;strong&gt;visual&lt;/strong&gt; and &lt;strong&gt;statistical&lt;/strong&gt;. The two approaches generally trade-off &lt;strong&gt;intuition&lt;/strong&gt; with &lt;strong&gt;rigor&lt;/strong&gt;: from plots we can quickly assess and explore differences, but it&amp;rsquo;s hard to tell whether these differences are systematic or due to noise.&lt;/p&gt;
&lt;h2 id=&#34;example&#34;&gt;Example&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s assume we need to perform an &lt;strong&gt;experiment&lt;/strong&gt; on a group of individuals and we have randomized them into a &lt;strong&gt;treatment and control&lt;/strong&gt; group. We would like them to be &lt;strong&gt;as comparable as possible&lt;/strong&gt;, in order to attribute any difference between the two groups to the treatment effect alone. We also have divided the treatment group in different &lt;em&gt;arms&lt;/em&gt; for testing different treatments (e.g. slight variations of the same drug).&lt;/p&gt;
&lt;p&gt;For this example, I have simulated a dataset of 1000 individuals, for whom we observe a set of characteristics. I import the data generating process &lt;code&gt;dgp_rnd_assignment()&lt;/code&gt; from &lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/src/dgp.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;src.dgp&lt;/code&gt;&lt;/a&gt; and some plotting functions and libraries from &lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/src/utils.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;src.utils&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;%matplotlib inline
%config InlineBackend.figure_format = &#39;retina&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from src.utils import *
from src.dgp import dgp_rnd_assignment

df = dgp_rnd_assignment().generate_data()
df.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;Group&lt;/th&gt;
      &lt;th&gt;Arm&lt;/th&gt;
      &lt;th&gt;Gender&lt;/th&gt;
      &lt;th&gt;Age&lt;/th&gt;
      &lt;th&gt;Income&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;control&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;29.0&lt;/td&gt;
      &lt;td&gt;568.44&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;control&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;32.0&lt;/td&gt;
      &lt;td&gt;596.45&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;treatment&lt;/td&gt;
      &lt;td&gt;arm 3&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;29.0&lt;/td&gt;
      &lt;td&gt;380.86&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;control&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;25.0&lt;/td&gt;
      &lt;td&gt;476.38&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;treatment&lt;/td&gt;
      &lt;td&gt;arm 4&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;32.0&lt;/td&gt;
      &lt;td&gt;628.28&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;We have information on $1000$ individuals, for which we observe &lt;code&gt;gender&lt;/code&gt;, &lt;code&gt;age&lt;/code&gt; and weekly &lt;code&gt;income&lt;/code&gt;. Each individual is assigned either to the treatment or control &lt;code&gt;group&lt;/code&gt; and treated individuals are distributed across four treatment &lt;code&gt;arms&lt;/code&gt;.&lt;/p&gt;
&lt;h2 id=&#34;two-groups---plots&#34;&gt;Two Groups - Plots&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s start with the simplest setting: we want to compare the distribution of income across the &lt;code&gt;treatment&lt;/code&gt; and &lt;code&gt;control&lt;/code&gt; group. We first explore &lt;strong&gt;visual&lt;/strong&gt; approaches and the &lt;strong&gt;statistical&lt;/strong&gt; approaches. The advantage of the first is &lt;strong&gt;intuition&lt;/strong&gt; while the advantage of the second is &lt;strong&gt;rigor&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;For most visualizations I am going to use Python&amp;rsquo;s &lt;a href=&#34;https://seaborn.pydata.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;seaborn&lt;/code&gt;&lt;/a&gt; library.&lt;/p&gt;
&lt;h3 id=&#34;boxplot&#34;&gt;Boxplot&lt;/h3&gt;
&lt;p&gt;A first visual approach is the &lt;a href=&#34;https://en.wikipedia.org/wiki/Box_plot&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;boxplot&lt;/strong&gt;&lt;/a&gt;. The boxplot is a good trade-off between summary statistics and data visualization. The center of the &lt;strong&gt;box&lt;/strong&gt; represents the &lt;em&gt;median&lt;/em&gt; while the borders represent the first (Q1) and third &lt;a href=&#34;https://en.wikipedia.org/wiki/Quartile&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;quartile&lt;/a&gt; (Q3), respectively. The &lt;strong&gt;whiskers&lt;/strong&gt; instead, extend to the first data points that are more than 1.5 times the &lt;em&gt;interquartile range&lt;/em&gt; (Q3 - Q1) outside the box. The points that fall outside of the whiskers are plotted individually and are usually considered &lt;a href=&#34;https://en.wikipedia.org/wiki/Outlier&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;outliers&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Therefore, the boxplot provides both summary statistics (the box and the whiskers) and direct data visualization (the outliers).&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sns.boxplot(data=df, x=&#39;Group&#39;, y=&#39;Income&#39;);
plt.title(&amp;quot;Boxplot&amp;quot;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/comparing_distributions_12_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;It seems that the &lt;code&gt;income&lt;/code&gt; distribution in the &lt;code&gt;treatment&lt;/code&gt; group is slightly more dispersed: the orange box is larger and its whiskers cover a wider range. However, the &lt;strong&gt;issue&lt;/strong&gt; with the boxplot is that it hides the shape of the data, telling us some summary statistics but not showing us the actual data distribution.&lt;/p&gt;
&lt;h3 id=&#34;histogram&#34;&gt;Histogram&lt;/h3&gt;
&lt;p&gt;The most intuitive way to plot a distribution is the &lt;strong&gt;histogram&lt;/strong&gt;. The histogram groups the data into equally wide &lt;strong&gt;bins&lt;/strong&gt; and plots the number of observations within each bin.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sns.histplot(data=df, x=&#39;Income&#39;, hue=&#39;Group&#39;, bins=50);
plt.title(&amp;quot;Histogram&amp;quot;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/comparing_distributions_16_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;There are multiple &lt;strong&gt;issues&lt;/strong&gt; with this plot:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Since the two groups have a different number of observations, the two histograms are not comparable&lt;/li&gt;
&lt;li&gt;The number of bins is arbitrary&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We can solve the first issue using the &lt;code&gt;stat&lt;/code&gt; option to plot the &lt;code&gt;density&lt;/code&gt; instead of the count and setting the &lt;code&gt;common_norm&lt;/code&gt; option to &lt;code&gt;False&lt;/code&gt; to use the same normalization.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sns.histplot(data=df, x=&#39;Income&#39;, hue=&#39;Group&#39;, bins=50, stat=&#39;density&#39;, common_norm=False);
plt.title(&amp;quot;Density Histogram&amp;quot;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/comparing_distributions_18_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Now the two histograms are comparable!&lt;/p&gt;
&lt;p&gt;However, an important &lt;strong&gt;issue&lt;/strong&gt; remains: the size of the bins is arbitrary. In the extreme, if we bunch the data less, we end up with bins with at most one observation, if we bunch the data more, we end up with a single bin. In both cases, if we exaggerate, the plot loses informativeness. This is a classical &lt;a href=&#34;https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;bias-variance trade-off&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;kernel-density&#34;&gt;Kernel Density&lt;/h3&gt;
&lt;p&gt;One possible solution is to use a &lt;a href=&#34;https://en.wikipedia.org/wiki/Kernel_density_estimation&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;kernel density function&lt;/strong&gt;&lt;/a&gt; that tries to approximate the histogram with a continuous function, using &lt;a href=&#34;https://en.wikipedia.org/wiki/Kernel_density_estimation&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;kernel density estimation (KDE)&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sns.kdeplot(x=&#39;Income&#39;, data=df, hue=&#39;Group&#39;, common_norm=False);
plt.title(&amp;quot;Kernel Density Function&amp;quot;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/comparing_distributions_22_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;From the plot, it seems that the estimated kernel density of &lt;code&gt;income&lt;/code&gt; has &amp;ldquo;fatter tails&amp;rdquo; (i.e. higher variance) in the &lt;code&gt;treatment&lt;/code&gt; group, while the average seems similar across groups.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;issue&lt;/strong&gt; with kernel density estimation is that it is a bit of a  black-box and might mask relevant features of the data.&lt;/p&gt;
&lt;h3 id=&#34;cumulative-distribution&#34;&gt;Cumulative Distribution&lt;/h3&gt;
&lt;p&gt;A more transparent representation of the two distribution is their &lt;a href=&#34;https://en.wikipedia.org/wiki/Cumulative_distribution_function&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;cumulative distribution function&lt;/strong&gt;&lt;/a&gt;. At each point of the x axis (&lt;code&gt;income&lt;/code&gt;) we plot the percentage of data points that have an equal or lower value. The main &lt;strong&gt;advantages&lt;/strong&gt; of the cumulative distribution function are that&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;we do not need to make any arbitrary choice (e.g. number of bins)&lt;/li&gt;
&lt;li&gt;we do not need to perform any approximation (e.g. with KDE), but we represent all data points&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sns.histplot(x=&#39;Income&#39;, data=df, hue=&#39;Group&#39;, bins=len(df), stat=&amp;quot;density&amp;quot;,
             element=&amp;quot;step&amp;quot;, fill=False, cumulative=True, common_norm=False);
plt.title(&amp;quot;Cumulative distribution function&amp;quot;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/comparing_distributions_26_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;How should we interpret the graph?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Since the two lines cross more or less at 0.5 (y axis), it means that their median is similar&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Since the orange line is above the blue line on the left and below the blue line on the left, it means that the distribution of the &lt;code&gt;treatment&lt;/code&gt; group as fatter tails&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;qq-plot&#34;&gt;QQ Plot&lt;/h3&gt;
&lt;p&gt;A related method is the &lt;strong&gt;qq-plot&lt;/strong&gt;, where &lt;em&gt;q&lt;/em&gt; stands for quantile. The qq-plot plots the quantiles of the two distributions against each other. If the distributions are the same, we should get a 45 degree line.&lt;/p&gt;
&lt;p&gt;There is no native qq-plot function in Python and, while the &lt;code&gt;statsmodels&lt;/code&gt; package provides a &lt;a href=&#34;https://www.statsmodels.org/dev/generated/statsmodels.graphics.gofplots.qqplot.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;qqplot&lt;/code&gt; function&lt;/a&gt;, it is quite cumbersome. Therefore, we will do it by hand.&lt;/p&gt;
&lt;p&gt;First, we need to compute the quartiles of the two groups, using the &lt;code&gt;percentile&lt;/code&gt; function.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;income = df[&#39;Income&#39;].values
income_t = df.loc[df.Group==&#39;treatment&#39;, &#39;Income&#39;].values
income_c = df.loc[df.Group==&#39;control&#39;, &#39;Income&#39;].values

df_pct = pd.DataFrame()
df_pct[&#39;q_treatment&#39;] = np.percentile(income_t, range(100))
df_pct[&#39;q_control&#39;] = np.percentile(income_c, range(100))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we can plot the two quantile distributions against each other, plus the 45-degree line, representing the benchmark perfect fit.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plt.figure(figsize=(8, 8))
plt.scatter(x=&#39;q_control&#39;, y=&#39;q_treatment&#39;, data=df_pct, label=&#39;Actual fit&#39;);
sns.lineplot(x=&#39;q_control&#39;, y=&#39;q_control&#39;, data=df_pct, color=&#39;r&#39;, label=&#39;Line of perfect fit&#39;);
plt.xlabel(&#39;Quantile of income, control group&#39;)
plt.ylabel(&#39;Quantile of income, treatment group&#39;)
plt.legend()
plt.title(&amp;quot;QQ plot&amp;quot;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/comparing_distributions_32_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;The qq-plot delivers a very &lt;strong&gt;similar insight&lt;/strong&gt; with respect to the cumulative distribution plot: income in the treatment group has the same median (lines cross in the center) but wider tails (dots are below the line on the left end and above on the right end).&lt;/p&gt;
&lt;h2 id=&#34;two-groups---tests&#34;&gt;Two Groups - Tests&lt;/h2&gt;
&lt;p&gt;So far, we have seen different ways to &lt;em&gt;visualize&lt;/em&gt; differences between distributions. The main advantage of visualization is &lt;strong&gt;intuition&lt;/strong&gt;: we can eyeball the differences and intuitively assess them.&lt;/p&gt;
&lt;p&gt;However, we might want to be more &lt;strong&gt;rigorous&lt;/strong&gt; and try to assess the &lt;strong&gt;statistical significance&lt;/strong&gt; of the difference between the distributions, i.e. answer the question &amp;ldquo;&lt;em&gt;is the observed difference systematic or due to sampling noise?&lt;/em&gt;&amp;rdquo;.&lt;/p&gt;
&lt;p&gt;We are now going to analyze different tests to discern two distributions from each other.&lt;/p&gt;
&lt;h3 id=&#34;t-test&#34;&gt;T-test&lt;/h3&gt;
&lt;p&gt;The first and most common test is the &lt;a href=&#34;https://en.wikipedia.org/wiki/Student%27s_t-test&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;student t-test&lt;/a&gt;. T-tests are generally used to &lt;strong&gt;compare means&lt;/strong&gt;. In this case, we want to test whether the means of the &lt;code&gt;income&lt;/code&gt; distribution is the same across the two groups. The test statistic for the two-means comparison test is given by:&lt;/p&gt;
&lt;p&gt;$$
stat = \frac{|\bar x_1 - \bar x_2|}{\sqrt{s^2 / n }}
$$&lt;/p&gt;
&lt;p&gt;Where $\bar x$ is the sample mean and $s$ is the sample standard deviation. Under mild conditions, the test statistic is asymptotically distributed as a &lt;a href=&#34;https://en.wikipedia.org/wiki/Student%27s_t-distribution&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;student t&lt;/a&gt; distribution.&lt;/p&gt;
&lt;p&gt;We use the &lt;code&gt;ttest_ind&lt;/code&gt; function from &lt;code&gt;scipy&lt;/code&gt; to perform the t-test. The function returns both the test statistic and the implied &lt;a href=&#34;https://en.wikipedia.org/wiki/P-value&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;p-value&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from scipy.stats import ttest_ind

stat, p_value = ttest_ind(income_c, income_t)
print(f&amp;quot;t-test: statistic={stat:.4f}, p-value={p_value:.4f}&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;t-test: statistic=-1.5549, p-value=0.1203
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The p-value of the test is $0.12$, therefore we do &lt;strong&gt;not reject&lt;/strong&gt; the null hypothesis of no difference in &lt;em&gt;means&lt;/em&gt; across treatment and control groups.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: the t-test assumes that the variance in the two samples is the same so that its estimate is computed on the joint sample. &lt;a href=&#34;https://en.wikipedia.org/wiki/Welch%27s_t-test&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Welch’s t-test&lt;/a&gt; allows for unequal variances in the two samples.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;standardized-mean-difference-smd&#34;&gt;Standardized Mean Difference (SMD)&lt;/h3&gt;
&lt;p&gt;In general, it is good practice to always perform a test for difference in means on &lt;strong&gt;all variables&lt;/strong&gt; across the treatment and control group, when we are running a randomized control trial or A/B test.&lt;/p&gt;
&lt;p&gt;However, since the denominator of the t-test statistic depends on the sample size, the t-test has been &lt;strong&gt;criticized&lt;/strong&gt; for making p-values hard to compare across studies. In fact, we may obtain a significant result in an experiment with very small magnitude of difference but large sample size while we may obtain a non-significant result in an experiment with large magnitude of difference but small sample size.&lt;/p&gt;
&lt;p&gt;One solution that has been proposed is the &lt;strong&gt;standardized mean difference (SMD)&lt;/strong&gt;. As the name suggests, this is not a proper test statistic, but just a standardized difference, which can be computed as:&lt;/p&gt;
&lt;p&gt;$$
SMD = \frac{|\bar x_1 - \bar x_2|}{\sqrt{(s^2_1 + s^2_2) / 2}}
$$&lt;/p&gt;
&lt;p&gt;Usually a value below $0.1$ is considered a &amp;ldquo;small&amp;rdquo; difference.&lt;/p&gt;
&lt;p&gt;It is good practice to collect average values of all variables across treatment and control group and a measure of distance between the two — either the t-test or the SMD — into a table that is called &lt;strong&gt;balance table&lt;/strong&gt;. We can use the &lt;a href=&#34;https://causalml.readthedocs.io/en/latest/causalml.html#module-causalml.match&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;create_table_one&lt;/code&gt;&lt;/a&gt; function from the &lt;a href=&#34;https://causalml.readthedocs.io/en/latest/about.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;causalml&lt;/code&gt;&lt;/a&gt; library to generate it. As the name of the function suggests, the balance table should always be the &lt;strong&gt;first table&lt;/strong&gt; you present when performing an A/B test.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from causalml.match import create_table_one

df[&#39;treatment&#39;] = df[&#39;Group&#39;]==&#39;treatment&#39;
create_table_one(df, &#39;treatment&#39;, [&#39;Gender&#39;, &#39;Age&#39;, &#39;Income&#39;])
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;Control&lt;/th&gt;
      &lt;th&gt;Treatment&lt;/th&gt;
      &lt;th&gt;SMD&lt;/th&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;Variable&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;n&lt;/th&gt;
      &lt;td&gt;704&lt;/td&gt;
      &lt;td&gt;296&lt;/td&gt;
      &lt;td&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;Age&lt;/th&gt;
      &lt;td&gt;32.40 (8.54)&lt;/td&gt;
      &lt;td&gt;36.42 (7.76)&lt;/td&gt;
      &lt;td&gt;0.4928&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;Gender&lt;/th&gt;
      &lt;td&gt;0.51 (0.50)&lt;/td&gt;
      &lt;td&gt;0.58 (0.49)&lt;/td&gt;
      &lt;td&gt;0.1419&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;Income&lt;/th&gt;
      &lt;td&gt;524.59 (117.35)&lt;/td&gt;
      &lt;td&gt;538.75 (160.15)&lt;/td&gt;
      &lt;td&gt;0.1009&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;In the first two columns, we can see the average of the different variables across the treatment and control groups, with standard errors in parenthesis. In the &lt;strong&gt;last column&lt;/strong&gt;, the values of the SMD indicate a standardized difference of more than $0.1$ for all variables, suggesting that the two groups are probably different.&lt;/p&gt;
&lt;h3 id=&#34;mannwhitney-u-test&#34;&gt;Mann–Whitney U Test&lt;/h3&gt;
&lt;p&gt;An alternative test is the &lt;a href=&#34;https://en.wikipedia.org/wiki/Mann%E2%80%93Whitney_U_test&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Mann–Whitney U test&lt;/a&gt;. The null hypothesis for this test is that the two groups have the same distribution, while the alternative hypothesis is that one group has larger (or smaller) values than the other.&lt;/p&gt;
&lt;p&gt;Differently from the other tests we have seen so far, the Mann–Whitney U test is agnostic to outliers and concentrates on the center of the distribution.&lt;/p&gt;
&lt;p&gt;The test &lt;strong&gt;procedure&lt;/strong&gt; is the following.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Combine all data points and rank them (in increasing or decreasing order)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Compute $U_1 = R_1 - n_1(n_1 + 1)/2$, where $R_1$ is the sum of the ranks for data points in the first group and $n_1$ is the number of points in the first group.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Compute $U_2$ similarly for the second group.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The test statistic is given by $stat = min(U_1, U_2)$.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Under the null hypothesis of no systematic rank differences between the two distributions (i.e. same median), the test statistic is asymptotically normally distributed with known mean and variance.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;intuition&lt;/strong&gt; behind the computation of $R$ and $U$ is the following: if the values in the first sample were all bigger than the values in the second sample, then $R_1 = n_1(n_1 + 1)/2$ and, as a consequence, $U_1$ would then be zero (minimum attainable value). Otherwise, if the two samples were similar, $U_1$ and $U_2$ would be very close to $n_1 n_2 / 2$ (maximum attainable value).&lt;/p&gt;
&lt;p&gt;We perform the test using the &lt;code&gt;mannwhitneyu&lt;/code&gt; function from &lt;code&gt;scipy&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from scipy.stats import mannwhitneyu

stat, p_value = mannwhitneyu(income_t, income_c)
print(f&amp;quot; Mann–Whitney U Test: statistic={stat:.4f}, p-value={p_value:.4f}&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt; Mann–Whitney U Test: statistic=106371.5000, p-value=0.6012
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We get a p-value of 0.6 which implies that we do &lt;strong&gt;not reject&lt;/strong&gt; the null hypothesis of no difference between the two distributions.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: as for the t-test, there exists a version of the Mann–Whitney U test for unequal variances in the two samples, the &lt;a href=&#34;https://www.statisticshowto.com/brunner-munzel-test-generalized-wilcoxon-test/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Brunner-Munzel test&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;permutation-tests&#34;&gt;Permutation Tests&lt;/h3&gt;
&lt;p&gt;A non-parametric alternative is permutation testing. The idea is that, under the null hypothesis, the two distributions should be the same, therefore &lt;strong&gt;shuffling&lt;/strong&gt; the &lt;code&gt;group&lt;/code&gt; labels should not significantly alter any statistic.&lt;/p&gt;
&lt;p&gt;We can chose any statistic and check how its value in the original sample compares with its distribution across &lt;code&gt;group&lt;/code&gt; label permutations. For example, let&amp;rsquo;s use as a test statistic the &lt;strong&gt;difference of sample means&lt;/strong&gt; between the treatment and control group.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sample_stat = np.mean(income_t) - np.mean(income_c)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;stats = np.zeros(1000)
for k in range(1000):
    labels = np.random.permutation((df[&#39;Group&#39;] == &#39;treatment&#39;).values)
    stats[k] = np.mean(income[labels]) - np.mean(income[labels==False])
p_value = np.mean(stats &amp;gt; sample_stat)

print(f&amp;quot;Permutation test: p-value={p_value:.4f}&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Permutation test: p-value=0.0530
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The permutation test gives us a p-value of $0.056$, implying a weak &lt;strong&gt;non-rejection&lt;/strong&gt; of the null hypothesis at the 5% level.&lt;/p&gt;
&lt;p&gt;How do we &lt;strong&gt;interpret&lt;/strong&gt; the p-value? It means that the difference in means in the data is larger than $1 - 0.0560 = 94.4%$ of the differences in means across the permuted samples.&lt;/p&gt;
&lt;p&gt;We can &lt;strong&gt;visualize&lt;/strong&gt; the test, by plotting the distribution of the test statistic across permutations against its sample value.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plt.hist(stats, label=&#39;Permutation Statistics&#39;, bins=30);
plt.axvline(x=sample_stat, c=&#39;r&#39;, ls=&#39;--&#39;, label=&#39;Sample Statistic&#39;);
plt.legend();
plt.xlabel(&#39;Income difference between treatment and control group&#39;)
plt.title(&#39;Permutation Test&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/comparing_distributions_55_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;As we can see, the sample statistic is quite extreme with respect to the values in the permuted samples, but not excessively.&lt;/p&gt;
&lt;h3 id=&#34;chi-squared-test&#34;&gt;Chi-Squared Test&lt;/h3&gt;
&lt;p&gt;The &lt;a href=&#34;https://matteocourthoud.github.io/post/chisquared/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;chi-squared test&lt;/a&gt; is a very powerful test that is mostly used to test differences in frequencies.&lt;/p&gt;
&lt;p&gt;One of the &lt;strong&gt;least known applications&lt;/strong&gt; of the chi-squared test, is testing the similarity between two distributions. The &lt;strong&gt;idea&lt;/strong&gt; is to bin the observations of the two groups. If the two distributions were the same, we would expect the same frequency of observations in each bin. Importantly, we need enough observations in each bin, in order for the test to be valid.&lt;/p&gt;
&lt;p&gt;I generate bins corresponding to deciles of the distribution of &lt;code&gt;income&lt;/code&gt; in the &lt;em&gt;control&lt;/em&gt; group and then I compute the expected number of observations in each bin in the &lt;em&gt;treatment&lt;/em&gt; group, if the two distributions were the same.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Init dataframe
df_bins = pd.DataFrame()

# Generate bins from control group
_, bins = pd.qcut(income_c, q=10, retbins=True)
df_bins[&#39;bin&#39;] = pd.cut(income_c, bins=bins).value_counts().index

# Apply bins to both groups
df_bins[&#39;income_c_observed&#39;] = pd.cut(income_c, bins=bins).value_counts().values
df_bins[&#39;income_t_observed&#39;] = pd.cut(income_t, bins=bins).value_counts().values

# Compute expected frequency in the treatment group
df_bins[&#39;income_t_expected&#39;] = df_bins[&#39;income_c_observed&#39;] / np.sum(df_bins[&#39;income_c_observed&#39;]) * np.sum(df_bins[&#39;income_t_observed&#39;])

df_bins
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;bin&lt;/th&gt;
      &lt;th&gt;income_c_observed&lt;/th&gt;
      &lt;th&gt;income_t_observed&lt;/th&gt;
      &lt;th&gt;income_t_expected&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;(232.26, 380.496]&lt;/td&gt;
      &lt;td&gt;70&lt;/td&gt;
      &lt;td&gt;46&lt;/td&gt;
      &lt;td&gt;29.075391&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;(380.496, 425.324]&lt;/td&gt;
      &lt;td&gt;70&lt;/td&gt;
      &lt;td&gt;30&lt;/td&gt;
      &lt;td&gt;29.075391&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;(425.324, 456.795]&lt;/td&gt;
      &lt;td&gt;70&lt;/td&gt;
      &lt;td&gt;24&lt;/td&gt;
      &lt;td&gt;29.075391&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;(456.795, 488.83]&lt;/td&gt;
      &lt;td&gt;71&lt;/td&gt;
      &lt;td&gt;26&lt;/td&gt;
      &lt;td&gt;29.490754&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;(488.83, 513.66]&lt;/td&gt;
      &lt;td&gt;70&lt;/td&gt;
      &lt;td&gt;18&lt;/td&gt;
      &lt;td&gt;29.075391&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;5&lt;/th&gt;
      &lt;td&gt;(513.66, 540.048]&lt;/td&gt;
      &lt;td&gt;70&lt;/td&gt;
      &lt;td&gt;19&lt;/td&gt;
      &lt;td&gt;29.075391&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;6&lt;/th&gt;
      &lt;td&gt;(540.048, 576.664]&lt;/td&gt;
      &lt;td&gt;71&lt;/td&gt;
      &lt;td&gt;21&lt;/td&gt;
      &lt;td&gt;29.490754&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;7&lt;/th&gt;
      &lt;td&gt;(576.664, 621.022]&lt;/td&gt;
      &lt;td&gt;70&lt;/td&gt;
      &lt;td&gt;25&lt;/td&gt;
      &lt;td&gt;29.075391&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;8&lt;/th&gt;
      &lt;td&gt;(621.022, 682.003]&lt;/td&gt;
      &lt;td&gt;70&lt;/td&gt;
      &lt;td&gt;42&lt;/td&gt;
      &lt;td&gt;29.075391&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;9&lt;/th&gt;
      &lt;td&gt;(682.003, 973.46]&lt;/td&gt;
      &lt;td&gt;71&lt;/td&gt;
      &lt;td&gt;41&lt;/td&gt;
      &lt;td&gt;29.490754&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;We can now perform the test by comparing the expected (E) and observed (O) number of observations in the treatment group, across bins. The test statistic is given by&lt;/p&gt;
&lt;p&gt;$$
stat = \sum _{i=1}^{n} \frac{(O_i - E_i)^{2}}{E_i}
$$&lt;/p&gt;
&lt;p&gt;where the bins are indexed by $i$ and $O$ is the observed number of data points in bin $i$ and $E$ is the expected number of data points in bin $i$. Since we generated the bins using deciles of the distribution of &lt;code&gt;income&lt;/code&gt; in the control group, we expect the number of observations per bin in the treatment group to be the same across bins. The test statistic is asymptocally distributed as a &lt;a href=&#34;https://en.wikipedia.org/wiki/Chi-squared_distribution&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;chi-squared&lt;/a&gt; distribution.&lt;/p&gt;
&lt;p&gt;To compute the test statistic and the p-value of the test, we use the &lt;code&gt;chisquare&lt;/code&gt; function from &lt;code&gt;scipy&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from scipy.stats import chisquare

stat, p_value = chisquare(df_bins[&#39;income_t_observed&#39;], df_bins[&#39;income_t_expected&#39;])
print(f&amp;quot;Chi-squared Test: statistic={stat:.4f}, p-value={p_value:.4f}&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Chi-squared Test: statistic=32.1432, p-value=0.0002
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Differently from all other tests so far, the chi-squared test &lt;strong&gt;strongly rejects&lt;/strong&gt; the null hypothesis that the two distributions are the same. Why?&lt;/p&gt;
&lt;p&gt;The reason lies in the fact that the two distributions have a similar center but different tails and the chi-squared test tests the similarity along the &lt;strong&gt;whole distribution&lt;/strong&gt; and not only in the center, as we were doing with the previous tests.&lt;/p&gt;
&lt;p&gt;This result tells a &lt;strong&gt;cautionary tale&lt;/strong&gt;: it is very important to understand &lt;em&gt;what&lt;/em&gt; you are actually testing before drawing blind conclusions from a p-value!&lt;/p&gt;
&lt;h3 id=&#34;kolmogorov-smirnov-test&#34;&gt;Kolmogorov-Smirnov Test&lt;/h3&gt;
&lt;p&gt;The idea of the &lt;a href=&#34;https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Kolmogorov-Smirnov test&lt;/a&gt;, is to &lt;strong&gt;compare the cumulative distributions&lt;/strong&gt; of the two groups. In particular, the Kolmogorov-Smirnov test statistic is the maximum absolute difference between the two cumulative distributions.&lt;/p&gt;
&lt;p&gt;$$
stat = \sup _{x} \ \Big| \ F_1(x) - F_2(x) \ \Big|
$$&lt;/p&gt;
&lt;p&gt;Where $F_1$ and $F_2$ are the two cumulative distribution functions and $x$ are the values of the underlying variable. The asymptotic distribution of the Kolmogorov-Smirnov test statistic is &lt;a href=&#34;https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test#Kolmogorov_distribution&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Kolmogorov distributed&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;To better understand the test, let&amp;rsquo;s plot the cumulative distribution functions and the test statistic. First, we compute the cumulative distribution functions.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_ks = pd.DataFrame()
df_ks[&#39;Income&#39;] = np.sort(df[&#39;Income&#39;].unique())
df_ks[&#39;F_control&#39;] = df_ks[&#39;Income&#39;].apply(lambda x: np.mean(income_c&amp;lt;=x))
df_ks[&#39;F_treatment&#39;] = df_ks[&#39;Income&#39;].apply(lambda x: np.mean(income_t&amp;lt;=x))
df_ks.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;Income&lt;/th&gt;
      &lt;th&gt;F_control&lt;/th&gt;
      &lt;th&gt;F_treatment&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;216.36&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.003378&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;232.26&lt;/td&gt;
      &lt;td&gt;0.001420&lt;/td&gt;
      &lt;td&gt;0.003378&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;243.15&lt;/td&gt;
      &lt;td&gt;0.001420&lt;/td&gt;
      &lt;td&gt;0.006757&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;259.88&lt;/td&gt;
      &lt;td&gt;0.002841&lt;/td&gt;
      &lt;td&gt;0.006757&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;262.82&lt;/td&gt;
      &lt;td&gt;0.002841&lt;/td&gt;
      &lt;td&gt;0.010135&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;We now need to find the point where the absolute distance between the cumulative distribution functions is largest.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;k = np.argmax( np.abs(df_ks[&#39;F_control&#39;] - df_ks[&#39;F_treatment&#39;]))
ks_stat = np.abs(df_ks[&#39;F_treatment&#39;][k] - df_ks[&#39;F_control&#39;][k])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can visualize the value of the test statistic, by plotting the two cumulative distribution functions and the value of the test statistic.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;y = (df_ks[&#39;F_treatment&#39;][k] + df_ks[&#39;F_control&#39;][k])/2
plt.plot(&#39;Income&#39;, &#39;F_control&#39;, data=df_ks, label=&#39;Control&#39;)
plt.plot(&#39;Income&#39;, &#39;F_treatment&#39;, data=df_ks, label=&#39;Treatment&#39;)
plt.errorbar(x=df_ks[&#39;Income&#39;][k], y=y, yerr=ks_stat/2, color=&#39;k&#39;,
             capsize=5, mew=3, label=f&amp;quot;Test statistic: {ks_stat:.4f}&amp;quot;)
plt.legend(loc=&#39;center right&#39;);
plt.title(&amp;quot;Kolmogorov-Smirnov Test&amp;quot;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/comparing_distributions_69_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;From the plot, we can see that the value of the test statistic corresponds to the distance between the two cumulative distributions at &lt;code&gt;income&lt;/code&gt;~650. For that value of &lt;code&gt;income&lt;/code&gt;, we have the largest imbalance between the two groups.&lt;/p&gt;
&lt;p&gt;We can now perform the actual test using the &lt;code&gt;kstest&lt;/code&gt; function from &lt;code&gt;scipy&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from scipy.stats import kstest

stat, p_value = kstest(income_t, income_c)
print(f&amp;quot; Kolmogorov-Smirnov Test: statistic={stat:.4f}, p-value={p_value:.4f}&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt; Kolmogorov-Smirnov Test: statistic=0.0974, p-value=0.0355
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The p-value is below 5%: we &lt;strong&gt;reject&lt;/strong&gt; the null hypothesis that the two distributions are the same, with 95% confidence.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Note 1&lt;/strong&gt;: The KS test is too conservative and rejects the null hypothesis too rarely. Lilliefors test corrects this bias using a different distribution for the test statistic, the Lilliefors distribution.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Note 2&lt;/strong&gt;: the KS test uses very little information since it only compares the two cumulative distributions at one point: the one of maximum distance. The &lt;a href=&#34;https://en.wikipedia.org/wiki/Anderson%E2%80%93Darling_test&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Anderson-Darling test&lt;/a&gt; and the &lt;a href=&#34;https://en.wikipedia.org/wiki/Cram%C3%A9r%E2%80%93von_Mises_criterion&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Cramér-von Mises test&lt;/a&gt; instead compare the two distributions along the whole domain, by integration (the difference between the two lies in the weighting of the squared distances).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;multiple-groups---plots&#34;&gt;Multiple Groups - Plots&lt;/h2&gt;
&lt;p&gt;So far we have only considered the case of two groups: treatment and control. But that if we had &lt;strong&gt;multiple groups&lt;/strong&gt;? Some of the methods we have seen above scale well, while others don&amp;rsquo;t.&lt;/p&gt;
&lt;p&gt;As a working example, we are now going to check whether the distribution of &lt;code&gt;income&lt;/code&gt; is the same across treatment &lt;code&gt;arms&lt;/code&gt;.&lt;/p&gt;
&lt;h3 id=&#34;boxplot-1&#34;&gt;Boxplot&lt;/h3&gt;
&lt;p&gt;The &lt;strong&gt;boxplot&lt;/strong&gt; scales very well, when we have a number of groups in the single-digits, since we can put the different boxes side-by-side.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sns.boxplot(x=&#39;Arm&#39;, y=&#39;Income&#39;, data=df.sort_values(&#39;Arm&#39;));
plt.title(&amp;quot;Boxplot, multiple groups&amp;quot;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/comparing_distributions_78_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;From the plot, it looks like the distribution of &lt;code&gt;income&lt;/code&gt; is different across treatment arms, with higher numbered arms having a higher average income.&lt;/p&gt;
&lt;h3 id=&#34;violin-plot&#34;&gt;Violin Plot&lt;/h3&gt;
&lt;p&gt;A very nice extension of the boxplot that combines summary statistics and kernel density estimation is the  &lt;strong&gt;violinplot&lt;/strong&gt;. The violinplot plots separate densities along the y axis so that they don&amp;rsquo;t overlap. By default, it also adds a miniature boxplot inside.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sns.violinplot(x=&#39;Arm&#39;, y=&#39;Income&#39;, data=df.sort_values(&#39;Arm&#39;));
plt.title(&amp;quot;Violin Plot, multiple groups&amp;quot;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/comparing_distributions_82_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;As for the boxplot, the violin plot suggests that income is different across treatment arms.&lt;/p&gt;
&lt;h3 id=&#34;ridgeline-plot&#34;&gt;Ridgeline Plot&lt;/h3&gt;
&lt;p&gt;Lastly, the &lt;strong&gt;ridgeline plot&lt;/strong&gt; plots multiple kernel density distributions along the x-axis, making them more intuitive than the violin plot but partially overlapping them. Unfortunately, there is no default ridgeline plot neither in &lt;code&gt;matplotlib&lt;/code&gt; nor in &lt;code&gt;seaborn&lt;/code&gt;. We need to import it from &lt;a href=&#34;https://github.com/leotac/joypy&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;joypy&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from joypy import joyplot

joyplot(df, by=&#39;Arm&#39;, column=&#39;Income&#39;, colormap=sns.color_palette(&amp;quot;crest&amp;quot;, as_cmap=True));
plt.xlabel(&#39;Income&#39;);
plt.title(&amp;quot;Ridgeline Plot, multiple groups&amp;quot;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/comparing_distributions_86_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Again, the ridgeline plot suggests that higher numbered treatment arms have higher income. From this plot it is also easier to appreciate the different shapes of the distributions.&lt;/p&gt;
&lt;h2 id=&#34;multiple-groups---tests&#34;&gt;Multiple Groups - Tests&lt;/h2&gt;
&lt;p&gt;Lastly, let&amp;rsquo;s consider hypothesis tests to compare multiple groups. For simplicity, we will concentrate on the most popular one: the F-test.&lt;/p&gt;
&lt;h3 id=&#34;f-test&#34;&gt;F-test&lt;/h3&gt;
&lt;p&gt;With multiple groups, the most popular test is the &lt;a href=&#34;https://en.wikipedia.org/wiki/F-test&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;F-test&lt;/strong&gt;&lt;/a&gt;. The F-test compares the variance of a variable across different groups. This analysis is also called &lt;a href=&#34;https://en.wikipedia.org/wiki/Analysis_of_variance&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;analysis of variance, or &lt;strong&gt;ANOVA&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;In practice, the F-test statistic is&lt;/p&gt;
&lt;p&gt;$$
\text{stat} = \frac{\text{between-group variance}}{\text{within-group variance}} = \frac{\sum_{g} \big( \bar x_g - \bar x \big) / (G-1)}{\sum_{g} \sum_{i \in g} \big( \bar x_i - \bar x_g \big) / (N-G)}
$$&lt;/p&gt;
&lt;p&gt;Where $G$ is the number of groups, $N$ is the number of observations, $\bar x$ is the overall mean and $\bar x_g$ is the mean within group $g$. Under the null hypothesis of group independence, the f-statistic is &lt;a href=&#34;https://en.wikipedia.org/wiki/F-distribution&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;F-distributed&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from scipy.stats import f_oneway

income_groups = [df.loc[df[&#39;Arm&#39;]==arm, &#39;Income&#39;].values for arm in df[&#39;Arm&#39;].dropna().unique()]
stat, p_value = f_oneway(*income_groups)
print(f&amp;quot;F Test: statistic={stat:.4f}, p-value={p_value:.4f}&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;F Test: statistic=9.0911, p-value=0.0000
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The test p-value is basically zero, implying a &lt;strong&gt;strong rejection&lt;/strong&gt; of the null hypothesis of no differences in the &lt;code&gt;income&lt;/code&gt; distribution across treatment arms.&lt;/p&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;In this post we have see a ton of different ways to &lt;strong&gt;compare two or more distributions&lt;/strong&gt;, both visually and statistically. This is a primary concern in many applications, but especially in causal inference where we use randomization to make treatment and control group as comparable as possible.&lt;/p&gt;
&lt;p&gt;We have also seen how different methods might be better suited for &lt;strong&gt;different situations&lt;/strong&gt;. Visual methods are great to build intuition, but statistical methods are essential for decision-making, since we need to be able to assess the magnitude and statistical significance of the differences.&lt;/p&gt;
&lt;h3 id=&#34;references&#34;&gt;References&lt;/h3&gt;
&lt;p&gt;[1] Student, &lt;a href=&#34;https://www.jstor.org/stable/2331554&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The Probable Error of a Mean&lt;/a&gt; (1908), &lt;em&gt;Biometrika&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;[2] F. Wilcoxon, &lt;a href=&#34;https://www.jstor.org/stable/3001968&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Individual Comparisons by Ranking Methods&lt;/a&gt; (1945), &lt;em&gt;Biometrics Bulletin&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;[3] B. L. Welch, &lt;a href=&#34;https://academic.oup.com/biomet/article/34/1-2/28/210174&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The generalization of &amp;ldquo;Student&amp;rsquo;s&amp;rdquo; problem when several different population variances are involved&lt;/a&gt; (1947), &lt;em&gt;Biometrika&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;[4] H. B. Mann, D. R. Whitney, &lt;a href=&#34;https://www.jstor.org/stable/2236101&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;On a Test of Whether one of Two Random Variables is Stochastically Larger than the Other&lt;/a&gt; (1947), &lt;em&gt;The Annals of Mathematical Statistics&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;[5] E. Brunner, U. Munzen, &lt;a href=&#34;https://onlinelibrary.wiley.com/doi/abs/10.1002/%28SICI%291521-4036%28200001%2942:1%3C17::AID-BIMJ17%3E3.0.CO;2-U&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The Nonparametric Behrens-Fisher Problem: Asymptotic Theory and a Small-Sample Approximation&lt;/a&gt; (2000), &lt;em&gt;Biometrical Journal&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;[6] A. N. Kolmogorov, &lt;a href=&#34;https://link.springer.com/chapter/10.1007/978-94-011-2260-3_15&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Sulla determinazione empirica di una legge di distribuzione&lt;/a&gt; (1933), &lt;em&gt;Giorn. Ist. Ital. Attuar.&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;[7] H. Cramér, &lt;a href=&#34;https://www.tandfonline.com/doi/abs/10.1080/03461238.1928.10416862&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;On the composition of elementary errors&lt;/a&gt; (1928), &lt;em&gt;Scandinavian Actuarial Journal&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;[8] R. von Mises, &lt;a href=&#34;https://www.ams.org/journals/bull/1937-43-05/S0002-9904-1937-06520-7/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Wahrscheinlichkeit statistik und wahrheit&lt;/a&gt; (1936), &lt;em&gt;Bulletin of the American Mathematical Society&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;[9] T. W. Anderson, D. A. Darling, &lt;a href=&#34;https://projecteuclid.org/journals/annals-of-mathematical-statistics/volume-23/issue-2/Asymptotic-Theory-of-Certain-Goodness-of-Fit-Criteria-Based-on/10.1214/aoms/1177729437.full&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Asymptotic Theory of Certain &amp;ldquo;Goodness of Fit&amp;rdquo; Criteria Based on Stochastic Processes&lt;/a&gt; (1953), &lt;em&gt;The Annals of Mathematical Statistics&lt;/em&gt;.&lt;/p&gt;
&lt;h3 id=&#34;related-articles&#34;&gt;Related Articles&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/b63dc69e3d8c&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Goodbye Scatterplot, Welcome Binned Scatterplot&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;code&#34;&gt;Code&lt;/h3&gt;
&lt;p&gt;You can find the original Jupyter Notebook here:&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/distr.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/distr.ipynb&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Mean vs Median Causal Effect</title>
      <link>https://matteocourthoud.github.io/post/quantile_regression/</link>
      <pubDate>Mon, 10 Jul 2023 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/post/quantile_regression/</guid>
      <description>&lt;p&gt;&lt;em&gt;An introduction to quantile regression.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;In A/B tests, a.k.a. &lt;a href=&#34;https://en.wikipedia.org/wiki/Randomized_controlled_trial&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;randomized controlled trials&lt;/a&gt;, we usually estimate the &lt;strong&gt;average treatment effect (ATE)&lt;/strong&gt;: effect of a treatment (a drug, ad, product, &amp;hellip;) on an outcome of interest (a disease, firm revenue, customer satisfaction, &amp;hellip;), where the &amp;ldquo;average&amp;rdquo; is taken over the test subjects (patients, users, customers, &amp;hellip;). The ATE is a very useful quantity since it tells us the effect that we can expect if we were to treat a new subject with the same treatment.&lt;/p&gt;
&lt;p&gt;However, sometimes we might be interested in quantities different from the average, such as the &lt;a href=&#34;https://en.wikipedia.org/wiki/Median&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;median&lt;/strong&gt;&lt;/a&gt;. The median is an alternative measure of &lt;em&gt;central tendency&lt;/em&gt; that is more robust to outliers and is often more informative with skewed distributions. More generally, we might want to estimate the effect for different &lt;a href=&#34;https://en.wikipedia.org/wiki/Quantile&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;quantiles&lt;/a&gt; of the outcome distribution. A &lt;strong&gt;common use-case&lt;/strong&gt; is studying the impact of a UI change on the loading time of a website: a slightly heavier website might translate in an imperceptible change for most users, but a big change for a few users with very slow connections. Another common use-case is studying the impact of a product change on a product that is bought by few people: do existing customers buy it more or are we attracting new customers?&lt;/p&gt;
&lt;p&gt;These questions are hard to answer with linear regression that estimates the &lt;em&gt;average treatment effect&lt;/em&gt;. A more suitable tool is &lt;strong&gt;quantile regression&lt;/strong&gt; that can instead estimate the &lt;em&gt;median treatment effect&lt;/em&gt;. In this article we are going to cover a brief introduction to quantile regression and the estimation of quantile treatment effects.&lt;/p&gt;
&lt;h2 id=&#34;loyalty-cards-and-spending&#34;&gt;Loyalty Cards and Spending&lt;/h2&gt;
&lt;p&gt;Suppose we were an &lt;strong&gt;online store&lt;/strong&gt; and we wanted to increase sales. We decide to offer our customers a &lt;strong&gt;loyalty card&lt;/strong&gt; that grants them discounts as they increase their spend in the store. We would like to assess if the loyalty card is effective in increasing sales so we run an &lt;strong&gt;A/B test&lt;/strong&gt;: we offer the loyalty card only to a subset of our customers, at random.&lt;/p&gt;
&lt;p&gt;I import the data generating process &lt;code&gt;dgp_loyalty()&lt;/code&gt; from &lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/src/dgp.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;src.dgp&lt;/code&gt;&lt;/a&gt;. I also import some plotting functions and libraries from &lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/src/utils.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;src.utils&lt;/code&gt;&lt;/a&gt;.  To include not only code but also data and tables, I use &lt;a href=&#34;https://deepnote.com&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Deepnote&lt;/a&gt;, a Jupyter-like web-based collaborative notebook environment.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;%matplotlib inline
%config InlineBackend.figure_format = &#39;retina&#39;

from src.utils import *
from src.dgp import dgp_loyalty
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, let&amp;rsquo;s have a look at the data. We have information on $10.000$ customers, for whom we observe their &lt;code&gt;spend&lt;/code&gt; and whether they were offered the &lt;code&gt;loyalty&lt;/code&gt; card. We also observe some demographics, like &lt;code&gt;age&lt;/code&gt; and &lt;code&gt;gender&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df = dgp_loyalty().generate_data()
df.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;loyalty&lt;/th&gt;
      &lt;th&gt;spend&lt;/th&gt;
      &lt;th&gt;age&lt;/th&gt;
      &lt;th&gt;gender&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;30&lt;/td&gt;
      &lt;td&gt;Male&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;26&lt;/td&gt;
      &lt;td&gt;Male&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;27&lt;/td&gt;
      &lt;td&gt;Male&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;29&lt;/td&gt;
      &lt;td&gt;Male&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;23&lt;/td&gt;
      &lt;td&gt;Female&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;Interestingly, we notice that the outcome of interest, &lt;code&gt;spend&lt;/code&gt;, seems to have a lot of zeros. Let&amp;rsquo;s dig deeper.&lt;/p&gt;
&lt;h2 id=&#34;mean-vs-median&#34;&gt;Mean vs Median&lt;/h2&gt;
&lt;p&gt;Before analyzing our experiment, let&amp;rsquo;s have a look at our outcome variable, &lt;code&gt;spend&lt;/code&gt;. We first inspect it using centrality measures. We have two main options: the &lt;strong&gt;mean&lt;/strong&gt; and the &lt;strong&gt;median&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;First of all, what are they? The mean captures the average value, while the median captures the value in the middle of the distribution. In general, the mean is mathematically more tractable and easier to interpret, while the median is more robust to outliers. You can find plenty of articles online comparing the two measures and suggesting which one is more appropriate and when. Let&amp;rsquo;s have a look at the mean and median &lt;code&gt;spend&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;np.mean(df[&#39;spend&#39;])
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;28.136224
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;np.median(df[&#39;spend&#39;])
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;0.0
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;How do we interpret these two numbers? People spend on average 28\$ on our store. However, more than 50% of people don&amp;rsquo;t spend anything. As we can see, both measures are very informative and, to a certain extent, complementary. We can better understand the distribution of &lt;code&gt;spend&lt;/code&gt; by plotting its histogram.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sns.histplot(data=df, x=&amp;quot;spend&amp;quot;).set(ylabel=&#39;&#39;, title=&#39;Spending Distribution&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/quantile_regression_14_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;As previewed by the values of the mean and the median, the distribution of &lt;code&gt;spend&lt;/code&gt; is very skewed, with more than 5000 customers (out of 10000) not spending anything.&lt;/p&gt;
&lt;p&gt;One natural question then is: are we interested in the effect of the &lt;code&gt;loyalty&lt;/code&gt; card on average &lt;code&gt;spend&lt;/code&gt; or on median &lt;code&gt;spend&lt;/code&gt;? The first would tell us if customers spend more on average, while the second would tell us if the average customer spends more.&lt;/p&gt;
&lt;p&gt;Linear regression can tell us the effect of the &lt;code&gt;loyalty&lt;/code&gt; card on average &lt;code&gt;spend&lt;/code&gt;. However, what can we do if we were interested in the effect of the &lt;code&gt;loyalty&lt;/code&gt; card on median &lt;code&gt;spend&lt;/code&gt; (or other quantiles)? The answer is &lt;strong&gt;quantile regression&lt;/strong&gt;.&lt;/p&gt;
&lt;h2 id=&#34;quantile-regression&#34;&gt;Quantile Regression&lt;/h2&gt;
&lt;p&gt;With &lt;strong&gt;linear regression&lt;/strong&gt;, we try to estimate the &lt;em&gt;conditional expectation function&lt;/em&gt; of an outcome variable $Y$ (&lt;code&gt;spend&lt;/code&gt; in our example) with respect to one or more explanatory variables $X$ (&lt;code&gt;loyalty&lt;/code&gt; in our example).&lt;/p&gt;
&lt;p&gt;$$
\mathbb E \big[ Y \big| X \big]
$$&lt;/p&gt;
&lt;p&gt;In other words, we want to find a function $f$ such that $f(X) = \mathbb E[Y|X]$. We do so, by solving the following minimization problem:&lt;/p&gt;
&lt;p&gt;$$
\hat f(X) = \arg \min_{f} \mathbb E \big[ Y - f(X) \big]^2
$$&lt;/p&gt;
&lt;p&gt;It can be shown that the function $f$ that solves this minimization is indeed the conditional expectation of $Y$, with respect to $X$.&lt;/p&gt;
&lt;p&gt;Since $f(X)$ can be infinite dimensional, we usually estimate a parametric form of $f(X)$. The most common one is the linear form $f(X) = \beta X$, where $\beta$ is estimated by solving the corresponding minimization problem:&lt;/p&gt;
&lt;p&gt;$$
\hat \beta = \arg \min_{\beta} \mathbb E \big[ Y - \beta X \big]^2
$$&lt;/p&gt;
&lt;p&gt;The linear form is not just convenient, but it can be interpreted as the best local approximation of $f(X)$, referring to Taylor&amp;rsquo;s expansion.&lt;/p&gt;
&lt;p&gt;With &lt;strong&gt;quantile regression&lt;/strong&gt;, we do the &lt;strong&gt;same&lt;/strong&gt;. The only difference is that, instead of estimating the conditional expectation of $Y$ with respect to $X$, we want to estimate the $q$-&lt;a href=&#34;https://en.wikipedia.org/wiki/Quantile&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;quantile&lt;/a&gt; of $Y$ with respect to $X$.&lt;/p&gt;
&lt;p&gt;$$
\mathbb Q_q \big[ Y \big| X \big]
$$&lt;/p&gt;
&lt;p&gt;First of all, what is a &lt;strong&gt;quantile&lt;/strong&gt;? The &lt;a href=&#34;https://en.wikipedia.org/wiki/Quantile&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Wikipedia definition&lt;/a&gt; says&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;In statistics and probability, quantiles are cut points dividing the range of a probability distribution into continuous intervals with equal probabilities, or dividing the observations in a sample in the same way. Common quantiles have special names, such as quartiles (four groups), deciles (ten groups), and percentiles (100 groups).&amp;rdquo;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;For example, the 0.1-quantile represents the value that sits on the right of 10% of the mass of the distribution. The &lt;strong&gt;median&lt;/strong&gt; is the 0.5-quantile (or, equivalently, the $50^{th}$ percentile or the $5^{th}$ decile) and corresponds with the value in the center of the distribution. Let&amp;rsquo;s see a simple example with a &lt;a href=&#34;https://en.wikipedia.org/wiki/Log-normal_distribution&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;log-normal distribution&lt;/a&gt;. I plot the three quartiles that divide the data in four equally sized bins.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;data = np.random.lognormal(0, 1, 1_000_000);
sns.histplot(data).set(title=&#39;Lognormal Distribution&#39;, xlim=(0,10))
plt.axvline(x=np.percentile(data, 25), c=&#39;C8&#39;, label=&#39;25th percentile&#39;)
plt.axvline(x=np.median(data), c=&#39;C1&#39;, label=&#39;Median (50th pct)&#39;)
plt.axvline(x=np.percentile(data, 75), c=&#39;C3&#39;, label=&#39;75th percentile&#39;)
plt.legend();
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/quantile_regression_18_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;As we can see, the three quartiles divide the data into four bins, of equal size.&lt;/p&gt;
&lt;p&gt;So, what is the &lt;strong&gt;objective&lt;/strong&gt; of quantile regression? The objective is to find a function $f$ such that $f(X) = F^{-1}(y_q)$, where $F$ is the &lt;a href=&#34;https://en.wikipedia.org/wiki/Cumulative_distribution_function&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;cumulative distribution function&lt;/a&gt; of $Y$ and $y_q$ is the $q$-quantile of the distribution of $Y$.&lt;/p&gt;
&lt;p&gt;How do we do this? It can be shown with a little linear algebra that we can obtain the conditional quantile as the solution of the following minimization problem:&lt;/p&gt;
&lt;p&gt;$$
\hat f(X) = \arg \min_{f} \mathbb E \big[ \rho_q (Y - f(X)) \big] = \arg \min_{f} \ (1-q) \int_{-\infty}^{f(x)} (y - f(x)) \text{d} F(y) + q \int_{f(x)}^{\infty} (y - f(x)) \text{d} F(y)
$$&lt;/p&gt;
&lt;p&gt;Where $\rho_q$ is an auxiliary weighting function with the following shape.&lt;/p&gt;
&lt;img src=&#34;fig/rho.png&#34; width=&#34;400px&#34;/&gt;
&lt;p&gt;What is the &lt;strong&gt;intuition&lt;/strong&gt; behind the objective function?&lt;/p&gt;
&lt;p&gt;The idea is that we can interpret the equation as follows&lt;/p&gt;
&lt;p&gt;$$
\mathbb E \big[ \rho_q (Y - f(X)) \big] = (1-q) \underset{\color{red}{\text{mass of distribution below }f(x)}}{\int_{-\infty}^{f(x)} (y - f(x)) \text{d} F(y)} + q \underset{\color{red}{\text{mass of distribution above }f(x)}}{\int_{f(x)}^{\infty} (y - f(x)) \text{d} F(y)} \overset{\color{blue}{\text{if } f(x) = y_q}}{=} - (1-q) q + q (1-q) = 0
$$&lt;/p&gt;
&lt;p&gt;So that, when $f(X)$ corresponds with the quantile $y_q$, the value of the objective function is zero.&lt;/p&gt;
&lt;p&gt;Exactly as before, we can estimate a &lt;strong&gt;parametric form&lt;/strong&gt; of $f$ and, exactly as before, we can interpret it as a best local approximation (not trivially though, see &lt;a href=&#34;https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1468-0262.2006.00671.x&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Angrist, Chernozhukov, and Fernández-Val (2006)&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;$$
\hat \beta_q = \arg \min_{\beta} \mathbb E \big[ \rho_q (Y - \beta X ) \big]
$$&lt;/p&gt;
&lt;p&gt;We wrote $\hat \beta_q$ to indicate that this is the coefficient for the best linear approximation of the conditional $q$-quantile function.&lt;/p&gt;
&lt;p&gt;How do we &lt;strong&gt;estimate&lt;/strong&gt; a quantile regression?&lt;/p&gt;
&lt;h2 id=&#34;estimation&#34;&gt;Estimation&lt;/h2&gt;
&lt;p&gt;The &lt;a href=&#34;https://www.statsmodels.org/stable/index.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;statsmodels&lt;/a&gt; package allows us to estimate quantile regression with the the &lt;a href=&#34;https://www.statsmodels.org/dev/generated/statsmodels.regression.quantile_regression.QuantReg.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;quantreg()&lt;/code&gt;&lt;/a&gt; function. We just need to specify the quantile $q$ when we fit the model. Let&amp;rsquo;s use $q=0.5$, which corresponds with the median.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;smf.quantreg(&amp;quot;spend ~ loyalty&amp;quot;, data=df).fit(q=0.5).summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
      &lt;td&gt;&lt;/td&gt;         &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt; &lt;td&gt; 8.668e-07&lt;/td&gt; &lt;td&gt;    0.153&lt;/td&gt; &lt;td&gt; 5.66e-06&lt;/td&gt; &lt;td&gt; 1.000&lt;/td&gt; &lt;td&gt;   -0.300&lt;/td&gt; &lt;td&gt;    0.300&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;loyalty&lt;/th&gt;   &lt;td&gt;    3.4000&lt;/td&gt; &lt;td&gt;    0.217&lt;/td&gt; &lt;td&gt;   15.649&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    2.974&lt;/td&gt; &lt;td&gt;    3.826&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;Quantile regression estimates a positive coefficient for &lt;code&gt;loyalty&lt;/code&gt;. How does this estimate compare with linear regression?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;smf.ols(&amp;quot;spend ~ loyalty&amp;quot;, data=df).fit().summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
      &lt;td&gt;&lt;/td&gt;         &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt; &lt;td&gt;   25.6583&lt;/td&gt; &lt;td&gt;    0.564&lt;/td&gt; &lt;td&gt;   45.465&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;   24.552&lt;/td&gt; &lt;td&gt;   26.765&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;loyalty&lt;/th&gt;   &lt;td&gt;    4.9887&lt;/td&gt; &lt;td&gt;    0.801&lt;/td&gt; &lt;td&gt;    6.230&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    3.419&lt;/td&gt; &lt;td&gt;    6.558&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;The estimated coefficient with linear regression is higher. What does it mean? We will spend more time on the &lt;em&gt;interpretation&lt;/em&gt; of quantile regression coefficients later.&lt;/p&gt;
&lt;p&gt;Can we condition the analysis on other variables? We suspect that &lt;code&gt;spend&lt;/code&gt; is also affected by other variables and we want to increase the precision of our estimate by also conditioning the analysis on &lt;code&gt;age&lt;/code&gt; and &lt;code&gt;gender&lt;/code&gt;. We can just add the variables to the &lt;code&gt;quantreg()&lt;/code&gt; model.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;smf.quantreg(&amp;quot;spend ~ loyalty + age + gender&amp;quot;, data=df).fit(q=0.5).summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
         &lt;td&gt;&lt;/td&gt;           &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt;      &lt;td&gt;  -50.5353&lt;/td&gt; &lt;td&gt;    1.053&lt;/td&gt; &lt;td&gt;  -47.977&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;  -52.600&lt;/td&gt; &lt;td&gt;  -48.471&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;gender[T.Male]&lt;/th&gt; &lt;td&gt;  -20.2963&lt;/td&gt; &lt;td&gt;    0.557&lt;/td&gt; &lt;td&gt;  -36.410&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;  -21.389&lt;/td&gt; &lt;td&gt;  -19.204&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;loyalty&lt;/th&gt;        &lt;td&gt;    4.5747&lt;/td&gt; &lt;td&gt;    0.546&lt;/td&gt; &lt;td&gt;    8.374&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    3.504&lt;/td&gt; &lt;td&gt;    5.646&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;age&lt;/th&gt;            &lt;td&gt;    2.3663&lt;/td&gt; &lt;td&gt;    0.026&lt;/td&gt; &lt;td&gt;   92.293&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    2.316&lt;/td&gt; &lt;td&gt;    2.417&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;The coefficient of &lt;code&gt;loyalty&lt;/code&gt; increases when we condition the analysis on &lt;code&gt;age&lt;/code&gt; and &lt;code&gt;gender&lt;/code&gt;. This is true also for linear regresssion.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;smf.ols(&amp;quot;spend ~ loyalty + age + gender&amp;quot;, data=df).fit().summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
         &lt;td&gt;&lt;/td&gt;           &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt;      &lt;td&gt;  -57.4466&lt;/td&gt; &lt;td&gt;    0.911&lt;/td&gt; &lt;td&gt;  -63.028&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;  -59.233&lt;/td&gt; &lt;td&gt;  -55.660&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;gender[T.Male]&lt;/th&gt; &lt;td&gt;  -26.3170&lt;/td&gt; &lt;td&gt;    0.482&lt;/td&gt; &lt;td&gt;  -54.559&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;  -27.262&lt;/td&gt; &lt;td&gt;  -25.371&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;loyalty&lt;/th&gt;        &lt;td&gt;    3.9101&lt;/td&gt; &lt;td&gt;    0.473&lt;/td&gt; &lt;td&gt;    8.272&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    2.983&lt;/td&gt; &lt;td&gt;    4.837&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;age&lt;/th&gt;            &lt;td&gt;    2.7688&lt;/td&gt; &lt;td&gt;    0.022&lt;/td&gt; &lt;td&gt;  124.800&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    2.725&lt;/td&gt; &lt;td&gt;    2.812&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;There are a couple of things that we haven&amp;rsquo;t mentioned yet. The first one is &lt;strong&gt;inference&lt;/strong&gt;. How do we compute confidence intervals and p-values for our estimates in quantile regression?&lt;/p&gt;
&lt;h2 id=&#34;inference&#34;&gt;Inference&lt;/h2&gt;
&lt;p&gt;The asymptotic variance of the estimate $a$ of the quantile $q$ of a distribution $F$ is given by&lt;/p&gt;
&lt;p&gt;$$
AVar(y) = q(1-q) f^{-2}(y)
$$&lt;/p&gt;
&lt;p&gt;where $f$ is the &lt;a href=&#34;https://en.wikipedia.org/wiki/Probability_density_function&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;density function&lt;/a&gt; of $F$. This expression can be decomposed into &lt;strong&gt;two components&lt;/strong&gt;: $q(1-q)$ and $f^{-2}(y)$.&lt;/p&gt;
&lt;p&gt;The first component, $q(1-q)$, basically tells us that the variance of a quantile is higher the more the quantile is closer to the center of the distribution. Why is that so? First, we need to think about when the quantile of a point changes in response to a change in the value of a second point. The quantile changes when the second point swaps from left to right (or viceversa) of the first point. This is intuitively very easy if the first point lies in the middle of the distribution, but very hard if it lies at the extreme.&lt;/p&gt;
&lt;p&gt;The second component, $f^{-2}(a)$, instead tells us that this side swapping is more likely if the first point is surrounded by a lot of points.&lt;/p&gt;
&lt;p&gt;Importantly, estimating the variance of a quantile requires an estimate of the whole distribution of $Y$. This is done via approximation and it can be computationally very intensive. However, alternative procedures like the bootstrap or the &lt;a href=&#34;https://towardsdatascience.com/6ca4a1d45148&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;bayesian bootstrap&lt;/a&gt; are always available.&lt;/p&gt;
&lt;p&gt;The second thing that we haven&amp;rsquo;t talked about yet is the &lt;strong&gt;interpretation&lt;/strong&gt; of the estimated coefficients. We got a lower coefficient of &lt;code&gt;loyalty&lt;/code&gt; on &lt;code&gt;spend&lt;/code&gt; with median regression. What does it mean?&lt;/p&gt;
&lt;h2 id=&#34;interpretation&#34;&gt;Interpretation&lt;/h2&gt;
&lt;p&gt;The &lt;strong&gt;interpretation&lt;/strong&gt; of linear regression coefficients is straightforward: each coefficient is the derivative of the conditional expectation function $\mathbb E[Y|X]$ with respect to one dimension of $X$. In our case, we can interpret the regression coefficient of &lt;code&gt;loyalty&lt;/code&gt; as the average &lt;code&gt;spend&lt;/code&gt; increase from being offered a loyalty card. Crucially, here &amp;ldquo;average&amp;rdquo; means that this holds true for &lt;em&gt;each customer&lt;/em&gt;, on average.&lt;/p&gt;
&lt;p&gt;However, the interpretation of quantile regression coefficients is &lt;strong&gt;tricky&lt;/strong&gt;. Before, we were tempted to say that the &lt;code&gt;loyalty&lt;/code&gt; card increases the spend of the median customer by 3.4\$. But &lt;strong&gt;what does it mean&lt;/strong&gt;? Is it the same median customer that spends more or do we have a different median customer? This might seem like a philosophical question but it has important implications on reporting of quantile regression results. In the first case, we are making a statement that, as for the interpretation of linear regression coefficients, applies to a &lt;em&gt;single individual&lt;/em&gt;. In the second case, we are making a statement about the &lt;em&gt;distribution&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1468-0262.2005.00570.x&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Chernozhukov and Hansen (2005)&lt;/a&gt; show that a strong but helpful assumption is &lt;strong&gt;rank invariance&lt;/strong&gt;: assuming that the treatment &lt;strong&gt;does not shift&lt;/strong&gt; the relative composition of the distribution. In other words, if we rank people by &lt;code&gt;spend&lt;/code&gt; before the experiment, we assume that this ranking is not affected by the introduction of the &lt;code&gt;loyalty&lt;/code&gt; card. If I was spending less than you before, I might spend more afterwards, but still less than you (for any two people).&lt;/p&gt;
&lt;p&gt;Under this assumption, we can interpret the quantile coefficients as &lt;strong&gt;marginal effects for single individuals&lt;/strong&gt; sitting at different points of the outcome distribution, as in the first interpretation provided above. Moreover, we can report the treatment effect for many quantiles and interpret each one of them as a local effect for a different individual. Let&amp;rsquo;s plot the distribution of treatment effects, for different quantiles of &lt;code&gt;spend&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def plot_quantile_TE(df, formula, q, varname):
    df_results = pd.DataFrame()
    for q in np.arange(q, 1-q, q):
        qreg = smf.quantreg(formula, data=df).fit(q=q)
        temp = pd.DataFrame({&#39;q&#39;: [q],
                             &#39;coeff&#39;: [qreg.params[varname]], 
                             &#39;std&#39;: [qreg.bse[varname]],
                             &#39;ci_lower&#39;: [qreg.conf_int()[0][varname]],
                             &#39;ci_upper&#39;: [qreg.conf_int()[1][varname]]})
        df_results = pd.concat([df_results, temp]).reset_index(drop=True)
    
    # Plot
    fig, ax = plt.subplots()
    sns.lineplot(data=df_results, x=&#39;q&#39;, y=&#39;coeff&#39;)
    ax.fill_between(data=df_results, x=&#39;q&#39;, y1=&#39;ci_lower&#39;, y2=&#39;ci_upper&#39;, alpha=0.1);
    plt.axhline(y=0, c=&amp;quot;k&amp;quot;, lw=2, zorder=1)
    ols_coeff = smf.ols(formula, data=df).fit().params[varname]
    plt.axhline(y=ols_coeff, ls=&amp;quot;--&amp;quot;, c=&amp;quot;C1&amp;quot;, label=&amp;quot;OLS coefficient&amp;quot;, zorder=1)
    plt.legend()
    plt.title(&amp;quot;Estimated coefficient, by quantile&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plot_quantile_TE(df, formula=&amp;quot;spend ~ loyalty&amp;quot;, varname=&#39;loyalty&#39;, q=0.05)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/quantile_regression_37_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;This plot is &lt;strong&gt;extremely insightful&lt;/strong&gt;: for almost half of the customers, the &lt;code&gt;loyalty&lt;/code&gt; card has no effect. On the other hand, customers that were already spending something end up spending even more (around 10/12\$ more). This is a very powerful insight that we would have missed with linear regression that estimated an average effect of 5\$.&lt;/p&gt;
&lt;p&gt;We can repeat the same exercise, conditioning the analysis on &lt;code&gt;gender&lt;/code&gt; and &lt;code&gt;age&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plot_quantile_TE(df, formula=&amp;quot;spend ~ loyalty + age + gender&amp;quot;, varname=&#39;loyalty&#39;, q=0.05)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/quantile_regression_39_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Conditioning on other covariates removes a lot of the heterogeneity in treatment effects. The &lt;code&gt;loyalty&lt;/code&gt; card increases spending for most people, it&amp;rsquo;s demographic characteristics that are responsible for no spending to begin with.&lt;/p&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;In this article, we have explored a different &lt;strong&gt;causal estimand&lt;/strong&gt;: median treatment effects. How does it compare with the average treatment effect that we usually estimate? The pros and cons are closely related to the pros and cons of the median with respect to the mean as a measure of &lt;em&gt;central tendency&lt;/em&gt;. Median treatment effects are more informative on what is the effect on the average subject and are more robust to outliers. However, they are much more computationally intensive and they require strong assumptions for identification, such as rank invariance.&lt;/p&gt;
&lt;h3 id=&#34;references&#34;&gt;References&lt;/h3&gt;
&lt;p&gt;[1] R. Koenker, &lt;a href=&#34;https://www.cambridge.org/core/books/quantile-regression/C18AE7BCF3EC43C16937390D44A328B1&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Quantile Regression&lt;/a&gt; (1996), Cambridge University Press.&lt;/p&gt;
&lt;p&gt;[1] R. Koenker, K. Hallock, &lt;a href=&#34;https://www.aeaweb.org/articles?id=10.1257/jep.15.4.143&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Quantile Regression&lt;/a&gt;, (2001), &lt;em&gt;Journal of Economic Perspectives&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;[2] V. Chernozhukov, C. Hansen, &lt;a href=&#34;https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1468-0262.2005.00570.x&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;An IV Model of Quantile Treatment Effects&lt;/a&gt; (2005), &lt;em&gt;Econometrica&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;[3] J. Angrist, V. Chernozhukov, I. Fernández-Val, &lt;a href=&#34;https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1468-0262.2006.00671.x&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Quantile Regression under Misspecification, with an Application to the U.S. Wage Structure&lt;/a&gt; (2006), &lt;em&gt;Econometrica&lt;/em&gt;.&lt;/p&gt;
&lt;h3 id=&#34;related-articles&#34;&gt;Related Articles&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/b63dc69e3d8c&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;DAGs and Control Variables&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/6ca4a1d45148&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The Bayesian Bootstrap&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/a928f67413e4&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Goodbye Scatterplot, Welcome Binned Scatterplot&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;code&#34;&gt;Code&lt;/h3&gt;
&lt;p&gt;You can find the original Jupyter Notebook here:&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/quantile_reg.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/quantile_reg.ipynb&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Omitted Variable Bias And How To Deal With It</title>
      <link>https://matteocourthoud.github.io/post/omitted_variable_bias/</link>
      <pubDate>Mon, 10 Jul 2023 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/post/omitted_variable_bias/</guid>
      <description>&lt;p&gt;In causal inference, &lt;strong&gt;bias&lt;/strong&gt; is extremely problematic because it makes inference not valid. Bias generally means that an estimator will not deliver the estimate of the true effect, on average.&lt;/p&gt;
&lt;p&gt;This is why, in general, we prefer estimators that are &lt;strong&gt;unbiased&lt;/strong&gt;, at the cost of a higher variance, i.e. more noise. Does it mean that every biased estimator is useless? Actually no. Sometimes, with domain knowledge, we can still draw causal conclusions even with a biased estimator.&lt;/p&gt;
&lt;p&gt;In this post, we are going to review a specific but frequent source of bias, &lt;strong&gt;omitted variable bias (OVB)&lt;/strong&gt;. We are going to explore the causes of the bias and leverage these insights to make causal statements, despite the bias.&lt;/p&gt;
&lt;h2 id=&#34;theory&#34;&gt;Theory&lt;/h2&gt;
&lt;p&gt;Suppose we are interested in the effect of a variable $D$ on a variable $y$. However, there is a third variable $Z$ that we do not observe and that is correlated with both $D$ and $Y$. Assume the data generating process can be represented with the following &lt;a href=&#34;https://towardsdatascience.com/b63dc69e3d8c&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Directed Acyclic Graph (DAG)&lt;/strong&gt;&lt;/a&gt;. If you are not familiar with DAGs, I have written a short &lt;a href=&#34;https://towardsdatascience.com/b63dc69e3d8c&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;introduction here&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-mermaid&#34;&gt;flowchart LR
classDef included fill:#DCDCDC,stroke:#000000,stroke-width:2px;
classDef excluded fill:#ffffff,stroke:#000000,stroke-width:2px;
classDef unobserved fill:#ffffff,stroke:#000000,stroke-width:2px,stroke-dasharray: 5 5;

D((D))
Z((Z))
Y((Y))

D --&amp;gt; Y
Z --&amp;gt; D
Z --&amp;gt; Y

class D,Y excluded;
class Z unobserved;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Since there is a &lt;a href=&#34;https://towardsdatascience.com/b63dc69e3d8c&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;backdoor path&lt;/strong&gt;&lt;/a&gt; from $D$ to $y$ passing through $Z$, we need to condition our analysis on $Z$ in order to recover the causal effect of $D$ on $y$. If we could observe $Z$, we would run a linear regression of $y$ on $D$ and $Z$ to estimate the following model:&lt;/p&gt;
&lt;p&gt;$$
y = \alpha D + \gamma Z + \varepsilon
$$&lt;/p&gt;
&lt;p&gt;where $\alpha$ is the effect of interest. This regression is usually referred to as the &lt;strong&gt;long regression&lt;/strong&gt; since it includes all variables of the model.&lt;/p&gt;
&lt;p&gt;However, since we do not observe $Z$, we have to estimate the following model:&lt;/p&gt;
&lt;p&gt;$$
y = \alpha D + u
$$&lt;/p&gt;
&lt;p&gt;The corresponding regression is usually referred to as the &lt;strong&gt;short regression&lt;/strong&gt; since it does not include all the variables of the model&lt;/p&gt;
&lt;p&gt;What is the &lt;strong&gt;consequence&lt;/strong&gt; of estimating the short regression when the true model is the long one?&lt;/p&gt;
&lt;p&gt;In that case, the OLS estimator of $\alpha$ is&lt;/p&gt;
&lt;p&gt;$$
\begin{align}
\hat \alpha &amp;amp;= \frac{Cov(D, y)}{Var(D)} =
\newline
&amp;amp;= \frac{Cov(D, \alpha D + \gamma Z + \varepsilon)}{Var(D)} =
\newline
&amp;amp;= \frac{Cov(D, \alpha D)}{Var(D)} + \frac{Cov(D, \gamma Z)}{Var(D)} + \frac{Cov(D, \varepsilon)}{Var(D)} =
\newline
&amp;amp;= \alpha + \underbrace{ \gamma \frac{Cov(D, Z)}{Var(D)} }_{\text{omitted variable bias}}
\end{align}
$$&lt;/p&gt;
&lt;p&gt;Therefore, we can write the &lt;strong&gt;omitted variable bias&lt;/strong&gt; as&lt;/p&gt;
&lt;p&gt;$$
\text{OVB} = \gamma \delta \qquad \text{ where } \qquad \delta := \frac{Cov(D, Z)}{Var(D)}
$$&lt;/p&gt;
&lt;p&gt;The beauty of this formula is its &lt;strong&gt;interpretability&lt;/strong&gt;: the omitted variable bias consists in just &lt;strong&gt;two components&lt;/strong&gt;, both extremely easy to interpret.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\gamma$: the effect of $Z$ on $y$&lt;/li&gt;
&lt;li&gt;$\delta$: the effect of $D$ on $Z$&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;additional-controls&#34;&gt;Additional Controls&lt;/h3&gt;
&lt;p&gt;What happens if we had &lt;strong&gt;additional control variables&lt;/strong&gt; in the regression? For example, assume that besides the variable of interest $D$, we also observe a vector of other variables $X$ so that the &lt;strong&gt;long regression&lt;/strong&gt; is&lt;/p&gt;
&lt;p&gt;$$
y = \alpha D + \beta X + \gamma Z + \varepsilon
$$&lt;/p&gt;
&lt;p&gt;Thanks to the &lt;a href=&#34;https://towardsdatascience.com/59f801eb3299&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Frisch-Waugh-Lowell theorem&lt;/strong&gt;&lt;/a&gt;, we can simply &lt;strong&gt;partial-out&lt;/strong&gt; $X$ and express the omitted variable bias in terms of $D$ and $Z$.&lt;/p&gt;
&lt;p&gt;$$
\text{OVB} = \gamma \times \frac{Cov(D^{\perp X}, Z^{\perp X})}{Var(D^{\perp X})}
$$&lt;/p&gt;
&lt;p&gt;where $D^{\perp X}$ are the residuals from regressing $D$ on $X$ and $Z^{\perp X}$ are the residuals from regressing $Z$ on $X$. If you are not familiar with Frisch-Waugh-Lowell theorem, I have written a short &lt;a href=&#34;https://towardsdatascience.com/59f801eb3299&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;note here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2112.13398&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Chernozhukov, Cinelli, Newey, Sharma, Syrgkanis (2022)&lt;/a&gt; further generalize to analysis the the setting in which the control variables $X$ and the unobserved variables $Z$ enter the long model with a general functional form $f$&lt;/p&gt;
&lt;p&gt;$$
y = \alpha D + f(Z, X) + \varepsilon
$$&lt;/p&gt;
&lt;p&gt;You can find more details in their paper, but the underlying idea remains the same.&lt;/p&gt;
&lt;h2 id=&#34;example&#34;&gt;Example&lt;/h2&gt;
&lt;p&gt;Suppose we were a researcher interested in the relationship between &lt;strong&gt;education&lt;/strong&gt; and &lt;strong&gt;wages&lt;/strong&gt;. Does investing in education pay off in terms of future wages? Suppose we had data on wages for people with different years of education. Why not looking at the correlation between years of education and wages?&lt;/p&gt;
&lt;p&gt;The problem is that there might be many &lt;strong&gt;unobserved variables&lt;/strong&gt; that are correlated with both education and wages. For simplicity, let&amp;rsquo;s concentrate on &lt;strong&gt;ability&lt;/strong&gt;. People of higher ability might decide to invest more in education just because they are better in school and they get more opportunities. On the other hand, they might also get higher wages afterwards, purely because of their innate ability.&lt;/p&gt;
&lt;p&gt;We can represent the data generating process with the following &lt;strong&gt;Directed Acyclic Graph&lt;/strong&gt; (DAG).&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-mermaid&#34;&gt;flowchart TD
classDef included fill:#DCDCDC,stroke:#000000,stroke-width:2px;
classDef excluded fill:#ffffff,stroke:#000000,stroke-width:2px;
classDef unobserved fill:#ffffff,stroke:#000000,stroke-width:2px,stroke-dasharray: 5 5;

D((education))
Z((ability))
Y((wage))
X1((age))
X2((gender))

D --&amp;gt; Y
Z --&amp;gt; D
Z --&amp;gt; Y
X1 --&amp;gt; Y
X2 --&amp;gt; Y

class D,Y included;
class X1,X2 excluded;
class Z unobserved;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s load and inspect the &lt;strong&gt;data&lt;/strong&gt;. I import the data generating process from &lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/src/dgp.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;src.dgp&lt;/code&gt;&lt;/a&gt; and some plotting functions and libraries from &lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/src/utils.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;src.utils&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;%matplotlib inline
%config InlineBackend.figure_format = &#39;retina&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from src.utils import *
from src.dgp import dgp_educ_wages

df = dgp_educ_wages().generate_data(N=50)
df.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;age&lt;/th&gt;
      &lt;th&gt;gender&lt;/th&gt;
      &lt;th&gt;education&lt;/th&gt;
      &lt;th&gt;wage&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;62&lt;/td&gt;
      &lt;td&gt;male&lt;/td&gt;
      &lt;td&gt;6.0&lt;/td&gt;
      &lt;td&gt;3800.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;44&lt;/td&gt;
      &lt;td&gt;male&lt;/td&gt;
      &lt;td&gt;8.0&lt;/td&gt;
      &lt;td&gt;4500.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;63&lt;/td&gt;
      &lt;td&gt;male&lt;/td&gt;
      &lt;td&gt;8.0&lt;/td&gt;
      &lt;td&gt;4700.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;33&lt;/td&gt;
      &lt;td&gt;male&lt;/td&gt;
      &lt;td&gt;7.0&lt;/td&gt;
      &lt;td&gt;3500.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;57&lt;/td&gt;
      &lt;td&gt;female&lt;/td&gt;
      &lt;td&gt;6.0&lt;/td&gt;
      &lt;td&gt;4000.0&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;We have information on &lt;strong&gt;300 individuals&lt;/strong&gt;, for which we observe their &lt;code&gt;age&lt;/code&gt;, their &lt;code&gt;gender&lt;/code&gt;, the years of &lt;code&gt;education&lt;/code&gt;, and the current monthly &lt;code&gt;wage&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Suppose we were directly regressing &lt;code&gt;wage&lt;/code&gt; on &lt;code&gt;education&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;short_model = smf.ols(&#39;wage ~ education + gender + age&#39;, df).fit()
short_model.summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
         &lt;td&gt;&lt;/td&gt;           &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt;      &lt;td&gt; 2657.8864&lt;/td&gt; &lt;td&gt;  444.996&lt;/td&gt; &lt;td&gt;    5.973&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt; 1762.155&lt;/td&gt; &lt;td&gt; 3553.618&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;gender[T.male]&lt;/th&gt; &lt;td&gt;  335.1075&lt;/td&gt; &lt;td&gt;  132.685&lt;/td&gt; &lt;td&gt;    2.526&lt;/td&gt; &lt;td&gt; 0.015&lt;/td&gt; &lt;td&gt;   68.027&lt;/td&gt; &lt;td&gt;  602.188&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;education&lt;/th&gt;      &lt;td&gt;   95.9437&lt;/td&gt; &lt;td&gt;   38.752&lt;/td&gt; &lt;td&gt;    2.476&lt;/td&gt; &lt;td&gt; 0.017&lt;/td&gt; &lt;td&gt;   17.940&lt;/td&gt; &lt;td&gt;  173.948&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;age&lt;/th&gt;            &lt;td&gt;   12.3120&lt;/td&gt; &lt;td&gt;    6.110&lt;/td&gt; &lt;td&gt;    2.015&lt;/td&gt; &lt;td&gt; 0.050&lt;/td&gt; &lt;td&gt;    0.013&lt;/td&gt; &lt;td&gt;   24.611&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;The coefficient of &lt;code&gt;education&lt;/code&gt; is positive and significant. However, we know there might be an &lt;strong&gt;omitted variable bias&lt;/strong&gt;, because we do not observe &lt;code&gt;ability&lt;/code&gt;. In terms of DAGs, there is a &lt;strong&gt;backdoor path&lt;/strong&gt; from &lt;code&gt;education&lt;/code&gt; to &lt;code&gt;wage&lt;/code&gt; passing through &lt;code&gt;ability&lt;/code&gt; that is not blocked and therefore biases our estimate.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-mermaid&#34;&gt;flowchart TD
classDef included fill:#DCDCDC,stroke:#000000,stroke-width:2px;
classDef excluded fill:#ffffff,stroke:#000000,stroke-width:2px;
classDef unobserved fill:#ffffff,stroke:#000000,stroke-width:2px,stroke-dasharray: 5 5;

D((education))
Z((ability))
Y((wage))
X1((age))
X2((gender))

D --&amp;gt; Y
Z --&amp;gt; D
Z --&amp;gt; Y
X1 --&amp;gt; Y
X2 --&amp;gt; Y

class D,Y included;
class X1,X2 excluded;
class Z unobserved;

linkStyle 0 stroke:#00ff00,stroke-width:4px;
linkStyle 1,2 stroke:#ff0000,stroke-width:4px;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Does it mean that all our analysis is &lt;strong&gt;garbage&lt;/strong&gt;? Can we still draw some causal conclusion from the regression results?&lt;/p&gt;
&lt;h2 id=&#34;direction-of-the-bias&#34;&gt;Direction of the Bias&lt;/h2&gt;
&lt;p&gt;If we knew the signs of $\gamma$ and $\delta$, we could infer the sign of the bias, since it&amp;rsquo;s the product of the two signs.&lt;/p&gt;
&lt;p&gt;$$
\text{OVB} = \gamma \delta \qquad \text{ where } \qquad \gamma := \frac{Cov(Z, y)}{Var(Z)}, \quad \delta := \frac{Cov(D, Z)}{Var(D)}
$$&lt;/p&gt;
&lt;p&gt;which in our example is&lt;/p&gt;
&lt;p&gt;$$
\text{OVB} = \gamma \delta \qquad \text{ where } \qquad \gamma := \frac{Cov(\text{ability}, \text{wage})}{Var(\text{ability})}, \quad \delta := \frac{Cov(\text{education}, \text{ability})}{Var(\text{education})}
$$&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s analyze the two correlations separately:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The correlation between &lt;code&gt;ability&lt;/code&gt; and &lt;code&gt;wage&lt;/code&gt; is most likely positive&lt;/li&gt;
&lt;li&gt;The correlation between &lt;code&gt;ability&lt;/code&gt; and &lt;code&gt;education&lt;/code&gt; is most likely positive&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Therefore, the bias is most likely &lt;strong&gt;positive&lt;/strong&gt;. From this, we can conclude that our estimate from the regression on &lt;code&gt;wage&lt;/code&gt; on &lt;code&gt;education&lt;/code&gt; is most likely an &lt;strong&gt;overestimate&lt;/strong&gt; of the true effect, which is most likely smaller.&lt;/p&gt;
&lt;p&gt;This might seem like a small insight, but it&amp;rsquo;s actually huge. Now we can say with confidence that one year of &lt;code&gt;education&lt;/code&gt; increases &lt;code&gt;wages&lt;/code&gt; by &lt;strong&gt;at most&lt;/strong&gt; 95 dollars per month, which is a much more informative statement than just saying that the estimate is biased.&lt;/p&gt;
&lt;p&gt;In general, we can summarize the different possible effects of the bias in a 2-by-2 &lt;strong&gt;table&lt;/strong&gt;.&lt;/p&gt;
&lt;img src=&#34;other/ovb.png&#34; width=80% /&gt;
&lt;h2 id=&#34;further-sensitivity-analysis&#34;&gt;Further Sensitivity Analysis&lt;/h2&gt;
&lt;p&gt;Can we say &lt;strong&gt;more&lt;/strong&gt; about the omitted variable bias without making strong assumptions?&lt;/p&gt;
&lt;p&gt;The answer is yes! In particular, we can ask ourselves: how strong should the partial correlations $\gamma$ and $\delta$ be in order to &lt;strong&gt;overturn&lt;/strong&gt; our conclusion?&lt;/p&gt;
&lt;p&gt;In our example, we found a positive correlation between &lt;code&gt;education&lt;/code&gt; and &lt;code&gt;wages&lt;/code&gt; in the data. However, we know that we are omitting &lt;code&gt;ability&lt;/code&gt; in the regression. The question is: how strong should the correlation between &lt;code&gt;ability&lt;/code&gt; and &lt;code&gt;wage&lt;/code&gt;, $\gamma$, and between &lt;code&gt;ability&lt;/code&gt; and &lt;code&gt;education&lt;/code&gt;, $\delta$, be in order to make the effect not significant or even negative?&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://rss.onlinelibrary.wiley.com/doi/full/10.1111/rssb.12348&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Cinelli and Hazlett (2020)&lt;/a&gt; show that we can transform this question in terms of residual variation explained, i.e. the &lt;a href=&#34;https://en.wikipedia.org/wiki/Coefficient_of_determination&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;coefficient of determination, $R^2$&lt;/a&gt;. The advantage of this approach is &lt;strong&gt;interpretability&lt;/strong&gt;. It is much easier to make a guess about the percentage of variance explained than to make a guess about the magnitude of a conditional correlation.&lt;/p&gt;
&lt;p&gt;The authors wrote a companion package &lt;a href=&#34;https://github.com/carloscinelli/sensemakr&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;sensemakr&lt;/code&gt;&lt;/a&gt; to conduct the sensitivity analysis. You can find a detailed description of the package &lt;a href=&#34;https://cran.r-project.org/web/packages/sensemakr/vignettes/sensemakr.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;We will now use the &lt;code&gt;Sensemakr&lt;/code&gt; function. The main &lt;strong&gt;arguments&lt;/strong&gt; of the &lt;code&gt;Sensemakr&lt;/code&gt; function are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;model&lt;/code&gt;: the regression model we want to analyze&lt;/li&gt;
&lt;li&gt;&lt;code&gt;treatment&lt;/code&gt;: the feature/covariate of interest, in our case &lt;code&gt;education&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The question we will try to answer is the following:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;How much of the residual variation in &lt;code&gt;education&lt;/code&gt; (x axis) and &lt;code&gt;wage&lt;/code&gt; (y axis) does &lt;code&gt;ability&lt;/code&gt; need to explain in order for the effect of &lt;code&gt;education&lt;/code&gt; on &lt;code&gt;wages&lt;/code&gt; to &lt;strong&gt;change sign&lt;/strong&gt;?&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import sensemakr

sensitivity = sensemakr.Sensemakr(model = short_model, treatment = &amp;quot;education&amp;quot;)
sensitivity.plot()
plt.xlabel(&amp;quot;Partial $R^2$ of ability with education&amp;quot;);
plt.ylabel(&amp;quot;Partial $R^2$ of ability with wage&amp;quot;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/omitted_variable_bias_17_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;In the &lt;strong&gt;plot&lt;/strong&gt;, we see how the partial (because conditional on &lt;code&gt;age&lt;/code&gt; and &lt;code&gt;gender&lt;/code&gt;) $R^2$ of &lt;code&gt;ability&lt;/code&gt; with &lt;code&gt;education&lt;/code&gt; and &lt;code&gt;wage&lt;/code&gt; affects the estimated coefficient of &lt;code&gt;education&lt;/code&gt; on &lt;code&gt;wage&lt;/code&gt;. The $(0,0)$ coordinate, marked with a &lt;strong&gt;triangle&lt;/strong&gt;, corresponds to the current estimate and reflects what would happen if &lt;code&gt;ability&lt;/code&gt; had no explanatory power for both &lt;code&gt;wage&lt;/code&gt; with &lt;code&gt;education&lt;/code&gt;: nothing. As the explanatory power of &lt;code&gt;ability&lt;/code&gt; grows (moving upwards and rightwards from the triangle), the estimated coefficient decreases, as marked by the &lt;strong&gt;level curves&lt;/strong&gt;, until it becomes zero at the &lt;strong&gt;dotted red line&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;How should we &lt;strong&gt;interpret&lt;/strong&gt; the plot? We can see that we need &lt;code&gt;ability&lt;/code&gt; to explain around 30% of the residual variation in both &lt;code&gt;education&lt;/code&gt; and &lt;code&gt;wage&lt;/code&gt; in order for the effect of &lt;code&gt;education&lt;/code&gt; on &lt;code&gt;wages&lt;/code&gt; to disappear, corresponding to the red line.&lt;/p&gt;
&lt;p&gt;One question that you might (legitimately) have now is: what is 30%? Is it big or is it small? We can get a sense of the &lt;strong&gt;magnitude&lt;/strong&gt; of the partial $R^2$ by &lt;strong&gt;benchmarking&lt;/strong&gt; the results with the residual variance explained by another &lt;em&gt;observed&lt;/em&gt; variable. Let&amp;rsquo;s use &lt;code&gt;age&lt;/code&gt; for example.&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;Sensemakr&lt;/code&gt; function accepts the following optional arguments:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;benchmark_covariates&lt;/code&gt;: the covariate to use as a benchmark&lt;/li&gt;
&lt;li&gt;&lt;code&gt;kd&lt;/code&gt; and &lt;code&gt;ky&lt;/code&gt;: these arguments parameterize how many times stronger the unobserved variable (&lt;code&gt;ability&lt;/code&gt;) is related to the treatment (&lt;code&gt;kd&lt;/code&gt;) and to the outcome (&lt;code&gt;ky&lt;/code&gt;) in comparison to the observed benchmark covariate (&lt;code&gt;age&lt;/code&gt;). In our example, setting &lt;code&gt;kd&lt;/code&gt; and &lt;code&gt;ky&lt;/code&gt; equal to $[0.5, 1, 2]$ means we want to investigate the maximum strength of a variable half, same, or twice as strong as &lt;code&gt;age&lt;/code&gt; (in explaining &lt;code&gt;education&lt;/code&gt; and &lt;code&gt;wage&lt;/code&gt; variation).&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sensitivity = sensemakr.Sensemakr(model = short_model, 
                                  treatment = &amp;quot;education&amp;quot;,
                                  benchmark_covariates = &amp;quot;age&amp;quot;,
                                  kd = [0.5, 1, 2],
                                  ky = [0.5, 1, 2])
sensitivity.plot()
plt.xlabel(&amp;quot;Partial $R^2$ of ability with education&amp;quot;);
plt.ylabel(&amp;quot;Partial $R^2$ of ability with wage&amp;quot;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/omitted_variable_bias_19_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;It looks like even if &lt;code&gt;ability&lt;/code&gt; had twice as much explanatory power as &lt;code&gt;age&lt;/code&gt;, the effect of &lt;code&gt;education&lt;/code&gt; on &lt;code&gt;wage&lt;/code&gt; would still be positive. But would it be &lt;strong&gt;statistically significant&lt;/strong&gt;?&lt;/p&gt;
&lt;p&gt;We can repeat the same exercise, looking at the t-statistic instead of the magnitude of the coefficient. We just need to set the &lt;code&gt;sensitivity_of&lt;/code&gt; option in the plotting function equal to &lt;code&gt;t-value&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;The question that we are trying to answer in this case is:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;How much of the residual variation in &lt;code&gt;education&lt;/code&gt; (x axis) and &lt;code&gt;wage&lt;/code&gt; (y axis) does &lt;code&gt;ability&lt;/code&gt; need to explain in order for the effect of &lt;code&gt;education&lt;/code&gt; on &lt;code&gt;wages&lt;/code&gt; to &lt;strong&gt;become not significant&lt;/strong&gt;?&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sensitivity.plot(sensitivity_of = &#39;t-value&#39;)
plt.xlabel(&amp;quot;Partial $R^2$ of ability with education&amp;quot;);
plt.ylabel(&amp;quot;Partial $R^2$ of ability with wage&amp;quot;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/omitted_variable_bias_21_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;From the plot, we can see, we need &lt;code&gt;ability&lt;/code&gt; to explain around 5% to 10% of the residual variation in both &lt;code&gt;education&lt;/code&gt; and &lt;code&gt;wage&lt;/code&gt; in order for the effect of &lt;code&gt;education&lt;/code&gt; on &lt;code&gt;wage&lt;/code&gt; not to be significant. In particular, the red line plots the level curve for the t-statistic equal to 2.01, corresponding to a 5% significance level. From the comparison with &lt;code&gt;age&lt;/code&gt;, we see that a slightly stronger explanatory power (bigger than &lt;code&gt;1.0x age&lt;/code&gt;) would be sufficient to make the coefficient of &lt;code&gt;education&lt;/code&gt; on &lt;code&gt;wage&lt;/code&gt; not statistically significant.&lt;/p&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;In this post, I have introduced the concept of &lt;strong&gt;omitted variable bias&lt;/strong&gt;. We have seen how it&amp;rsquo;s computed in a simple linear model and how we can exploit qualitative information about the variables to make inference in presence of omitted variable bias.&lt;/p&gt;
&lt;p&gt;These tools are extremely useful since omitted variable bias is essentially &lt;strong&gt;everywhere&lt;/strong&gt;. First of all, there are always factors that we do not observe, such as ability in our toy example. However, even if we could observe everything, omitted variable bias can also emerge in the form of &lt;strong&gt;model misspecification&lt;/strong&gt;. Suppose that &lt;code&gt;wages&lt;/code&gt; depended on &lt;code&gt;age&lt;/code&gt; in a quadratic way. Then, omitting the quadratic term from the regression introduces bias, which can be analyzed with the same tools we have used for &lt;code&gt;ability&lt;/code&gt;.&lt;/p&gt;
&lt;h3 id=&#34;references&#34;&gt;References&lt;/h3&gt;
&lt;p&gt;[1] C. Cinelli, C. Hazlett, &lt;a href=&#34;https://rss.onlinelibrary.wiley.com/doi/full/10.1111/rssb.12348&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Making Sense of Sensitivity: Extending Omitted Variable Bias&lt;/a&gt; (2019), &lt;em&gt;Journal of the Royal Statistical Society&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;[2] V. Chernozhukov, C. Cinelli, W. Newey, A. Sharma, V. Syrgkanis, &lt;a href=&#34;https://arxiv.org/abs/2112.13398&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Long Story Short: Omitted Variable Bias in Causal Machine Learning&lt;/a&gt; (2022), working paper.&lt;/p&gt;
&lt;h3 id=&#34;related-articles&#34;&gt;Related Articles&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/59f801eb3299&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The FWL Theorem, Or How To Make Regressions Intuitive&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/b63dc69e3d8c&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;DAGs and Control Variables&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;code&#34;&gt;Code&lt;/h3&gt;
&lt;p&gt;You can find the original Jupyter Notebook here:&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/ovb.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/matteocourthoud/Blog-Posts/blob/main/ovb.ipynb&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Outliers, Leverage, and Influential Observations</title>
      <link>https://matteocourthoud.github.io/post/outliers_levarage/</link>
      <pubDate>Mon, 10 Jul 2023 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/post/outliers_levarage/</guid>
      <description>&lt;p&gt;&lt;em&gt;What makes an observation &amp;ldquo;unusual&amp;rdquo;?&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;In data science, one common task is outlier detection. This is a broad term that is often misused or misunderstood. More broadly, we are often interested in understanding any observation is &lt;strong&gt;&amp;ldquo;unusual&amp;rdquo;&lt;/strong&gt;. First of all, what does it mean to be unusual? In this article we are going to inspect three different ways in which an observation can be unusual: it can be unusual characteristics, it might not fit the model or it might be particularly influential in fitting the model. We will see that in linear regression the latter characteristics is a byproduct of the first two.&lt;/p&gt;
&lt;p&gt;Importantly, being unusual is &lt;strong&gt;not necessarily bad&lt;/strong&gt;. Observations that have different characteristics from all others usually carry more information. We also expect some observations not to fit the model well, otherwise the model is likely biased (overfitting). However, &amp;ldquo;unusual&amp;rdquo; observations are also more likely to be generated by a different process. Extreme cases include measurement error or fraud, but differences can be more nuanced. Domain knowledge is always kind and dropping observations only for for statistical reasons is never wise.&lt;/p&gt;
&lt;p&gt;That said, let&amp;rsquo;s have a look at some different ways in which observations can be &amp;ldquo;unusual&amp;rdquo;.&lt;/p&gt;
&lt;h1 id=&#34;example&#34;&gt;Example&lt;/h1&gt;
&lt;p&gt;Suppose we are an &lt;strong&gt;peer-to-peer online platform&lt;/strong&gt; and we are interested in understanding if there is anything suspicious going on with our business. We have information about how much time our customers spend on the platform and the total value of their transactions.&lt;/p&gt;
&lt;p&gt;First, let&amp;rsquo;s have a look at the data. I import the data generating process &lt;code&gt;dgp_p2p()&lt;/code&gt; from &lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/src/dgp.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;src.dgp&lt;/code&gt;&lt;/a&gt; and some plotting functions and libraries from &lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/src/utils.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;src.utils&lt;/code&gt;&lt;/a&gt;. I include code snippets from &lt;a href=&#34;https://deepnote.com&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Deepnote&lt;/a&gt;, a Jupyter-like web-based collaborative notebook environment. For our purpose, Deepnote is very handy because it allows me not only to include code but also output, like data and tables.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;%matplotlib inline
%config InlineBackend.figure_format = &#39;retina&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from src.utils import *
from src.dgp import dgp_p2p
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df = dgp_p2p().generate_data()
df.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;hours&lt;/th&gt;
      &lt;th&gt;transactions&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;2.6&lt;/td&gt;
      &lt;td&gt;8.30&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;2.0&lt;/td&gt;
      &lt;td&gt;8.00&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;7.0&lt;/td&gt;
      &lt;td&gt;21.00&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;6.7&lt;/td&gt;
      &lt;td&gt;18.00&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;1.2&lt;/td&gt;
      &lt;td&gt;3.82&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;We have information on 50 clients for which we observe &lt;code&gt;hours&lt;/code&gt; spent on the website and total &lt;code&gt;transactions&lt;/code&gt; amount. Since we only have two variables we can easily inspect them using a scatterplot.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sns.scatterplot(data=df, x=&#39;hours&#39;, y=&#39;transactions&#39;).set(title=&#39;Data Scatterplot&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/outliers_levarage_9_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;The relationship between &lt;code&gt;hours&lt;/code&gt; and &lt;code&gt;transactions&lt;/code&gt; seems to follow a clear linear relationship. If we fit a linear model, we observe a particularly tight fit.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;smf.ols(&#39;hours ~ transactions&#39;, data=df).fit().summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
        &lt;td&gt;&lt;/td&gt;          &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt;    &lt;td&gt;   -0.0975&lt;/td&gt; &lt;td&gt;    0.084&lt;/td&gt; &lt;td&gt;   -1.157&lt;/td&gt; &lt;td&gt; 0.253&lt;/td&gt; &lt;td&gt;   -0.267&lt;/td&gt; &lt;td&gt;    0.072&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;transactions&lt;/th&gt; &lt;td&gt;    0.3452&lt;/td&gt; &lt;td&gt;    0.009&lt;/td&gt; &lt;td&gt;   39.660&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    0.328&lt;/td&gt; &lt;td&gt;    0.363&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;Does any data point look suspiciously different from the others? How?&lt;/p&gt;
&lt;h2 id=&#34;leverage&#34;&gt;Leverage&lt;/h2&gt;
&lt;p&gt;The first metric that we are going to use to evaluate &amp;ldquo;unusual&amp;rdquo; observations is the &lt;strong&gt;leverage&lt;/strong&gt;, which was first introduced by &lt;a href=&#34;https://www.jstor.org/stable/1268249&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Cook (1980)&lt;/a&gt;. The objective of the leverage is to capture how much a single point is different with respect to other data points. These data points are often called &lt;strong&gt;outliers&lt;/strong&gt; and there exist a nearly amount of algorithms and rules of thumb to flag them.However the idea is the same: flagging observations that are unusual in terms of features.&lt;/p&gt;
&lt;p&gt;The leverage of an observation $i$ is defined as&lt;/p&gt;
&lt;p&gt;$$
h_{ii} := x_i&amp;rsquo; (X&amp;rsquo;X)^{-1} x_i
$$&lt;/p&gt;
&lt;p&gt;One interpretation of the leverage is as a &lt;strong&gt;measure of distance&lt;/strong&gt; where individual observations are compared against the average of all observations.&lt;/p&gt;
&lt;p&gt;Another interpretation of the leverage is as the influence of the outcome of observation $i$, $y_i$, on the corresponding fitted value $\hat{y_i}$.&lt;/p&gt;
&lt;p&gt;$$
h_{ii} = \frac{\partial \hat{y}_i}{\partial y_i}
$$&lt;/p&gt;
&lt;p&gt;Algebraically, the leverage of observation $i$ is the $i^{th}$ element of the &lt;strong&gt;design matrix&lt;/strong&gt; $X&amp;rsquo; (X&amp;rsquo;X)^{-1} X$. Among the many properties of the leverages, is the fact that they are non-negative and their values sum to 1.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s compute the leverage of the observations in our dataset. We also flag observations that have unusual leverages (which we arbitrarily define as more than two standard deviations away from the average leverage).&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;X = np.reshape(df[&#39;hours&#39;].values, (-1, 1))
Y = np.reshape(df[&#39;transactions&#39;].values, (-1, 1))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df[&#39;leverage&#39;] = np.diagonal(X @ np.linalg.inv(X.T @ X) @ X.T)
df[&#39;high_leverage&#39;] = df[&#39;leverage&#39;] &amp;gt; (np.mean(df[&#39;leverage&#39;]) + 2*np.std(df[&#39;leverage&#39;]))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s plot the distribution of leverage values in our data.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;fix, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))
sns.histplot(data=df, x=&#39;leverage&#39;, hue=&#39;high_leverage&#39;, alpha=1, bins=30, ax=ax1).set(title=&#39;Distribution of Leverages&#39;);
sns.scatterplot(data=df, x=&#39;hours&#39;, y=&#39;transactions&#39;, hue=&#39;high_leverage&#39;, ax=ax2).set(title=&#39;Data Scatterplot&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/outliers_levarage_18_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;As we can see, the distribution is skewed with two observations having a unusually high leverage. Indeed, in the scatterplot these two observations are slightly separated from the rest of the distribution.&lt;/p&gt;
&lt;p&gt;Is this bad news? It depends. Outliers are &lt;strong&gt;not a problem per se&lt;/strong&gt;. Actually, if they are genuine observations, they might carry much more information than other observations. On the other hand, they are also more likely &lt;em&gt;not&lt;/em&gt; to be genuine observations (e.g. fraud, measurement error, &amp;hellip;) or to be inherently different from the other ones (e.g. professional users vs amateurs). In any case, we might want to investigate further and use as much context-specific information as we can.&lt;/p&gt;
&lt;p&gt;Importantly, the fact that an observation has a high leverage tells us information about the features of the model but nothing about the model itself. Are these users just different observations or they also behave differently?&lt;/p&gt;
&lt;h2 id=&#34;residuals&#34;&gt;Residuals&lt;/h2&gt;
&lt;p&gt;So far we have only talked about unusual features, but what about &lt;strong&gt;unusual behavior&lt;/strong&gt;? This is what regression residuals measure.&lt;/p&gt;
&lt;p&gt;Regression residuals are the difference between the predicted outcome values and the observed outcome values. In a sense, they capture what the model cannot explain: the higher the residual of one observation the more it is unusual in the sense that the model cannot explain it.&lt;/p&gt;
&lt;p&gt;In the case of linear regression, residuals can be written as&lt;/p&gt;
&lt;p&gt;$$
\hat{e} = y - \hat{y} = y - \hat \beta X
$$&lt;/p&gt;
&lt;p&gt;In our case, since $X$ is one dimensional (&lt;code&gt;hours&lt;/code&gt;), we can easily visualize them.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;Y_hat = X @ np.linalg.inv(X.T @ X) @ X.T @ Y
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plt.scatter(X, Y, s=50, label=&#39;data&#39;)
plt.plot(X, Y_hat, c=&#39;k&#39;, lw=2, label=&#39;prediction&#39;)
plt.vlines(X, np.minimum(Y, Y_hat), np.maximum(Y, Y_hat), color=&#39;r&#39;, lw=3, label=&amp;quot;residuals&amp;quot;);
plt.legend()
plt.title(f&amp;quot;Regression prediction and residuals&amp;quot;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/outliers_levarage_23_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Do some observations have unusually high residuals? Let&amp;rsquo;s plot their distribution.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df[&#39;residual&#39;] = np.abs(Y - X @ np.linalg.inv(X.T @ X) @ X.T @ Y)
df[&#39;high_residual&#39;] = df[&#39;residual&#39;] &amp;gt; (np.mean(df[&#39;residual&#39;]) + 2*np.std(df[&#39;residual&#39;]))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;fix, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))
sns.histplot(data=df, x=&#39;residual&#39;, hue=&#39;high_residual&#39;, alpha=1, bins=30, ax=ax1).set(title=&#39;Distribution of Residuals&#39;);
sns.scatterplot(data=df, x=&#39;hours&#39;, y=&#39;transactions&#39;, hue=&#39;high_residual&#39;, ax=ax2).set(title=&#39;Data Scatterplot&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/outliers_levarage_26_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Two observations have particularly high residuals. This means that for these observations, the model is not good at predicting the observed outcomes.&lt;/p&gt;
&lt;p&gt;Is this bad news? Not necessarily. A model that fits the observations too well is likely to be &lt;a href=&#34;https://en.wikipedia.org/wiki/Bias_%28statistics%29&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;biased&lt;/strong&gt;&lt;/a&gt;. However, it might still be important to understand why some users have a different relationship between hours spent and total transactions. As usual, information on the specific context is key.&lt;/p&gt;
&lt;p&gt;So far we have looked at observations with &amp;ldquo;unusual&amp;rdquo; characteristics and &amp;ldquo;unusual&amp;rdquo; model fit, but what is the observation itself is distorting the model? How much our model is driven by a handful of observations?&lt;/p&gt;
&lt;h2 id=&#34;influence&#34;&gt;Influence&lt;/h2&gt;
&lt;p&gt;The concept of &lt;strong&gt;influence and influence functions&lt;/strong&gt; was developed precisely to answer this question: what are influential observations? This questions were very popular in the 80&amp;rsquo;s and lost appeal for a long time until the recent need of explaining complex machine learning and AI models.&lt;/p&gt;
&lt;p&gt;The general idea is to define an observation as &lt;strong&gt;influential&lt;/strong&gt; if removing it significantly changes the estimated model. In linear regression, we define the influence of observation $i$ as:&lt;/p&gt;
&lt;p&gt;$$
\hat{\beta} - \hat{\beta}_{-i} = (X&amp;rsquo;X)^{-1} x_i e_i
$$&lt;/p&gt;
&lt;p&gt;Where $\hat{\beta}_{-i}$ is the OLS coefficient estimated omitting observation $i$.&lt;/p&gt;
&lt;p&gt;As you can see, there is a tight connection to both leverage $h_{ii}$ and residuals $e_i$: influence is almost the product of the two. Indeed, in linear regression, observations with high leverage are observations that are both outliers and have high residuals. None of the two conditions alone is sufficient for an observation to have an influence on the model.&lt;/p&gt;
&lt;p&gt;We can see it best in the data.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df[&#39;influence&#39;] = (np.linalg.inv(X.T @ X) @ X.T).T * np.abs(Y - Y_hat)
df[&#39;high_influence&#39;] = df[&#39;influence&#39;] &amp;gt; (np.mean(df[&#39;influence&#39;]) + 2*np.std(df[&#39;influence&#39;]))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;fix, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))
sns.histplot(data=df, x=&#39;influence&#39;, hue=&#39;high_influence&#39;, alpha=1, bins=30, ax=ax1).set(title=&#39;Distribution of Influences&#39;);
sns.scatterplot(data=df, x=&#39;hours&#39;, y=&#39;transactions&#39;, hue=&#39;high_influence&#39;, ax=ax2).set(title=&#39;Data Scatterplot&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/outliers_levarage_31_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;In our dataset, there is only one observation with high influence, and it is disproportionally larger than the influence of all other observations.&lt;/p&gt;
&lt;p&gt;We can now plot all &amp;ldquo;unusual&amp;rdquo; points in the same plot. I also report residuals and leverage of each point in a separate plot.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def plot_leverage_residuals(df):

    # Hue
    df[&#39;type&#39;] = &#39;Normal&#39;
    df.loc[df[&#39;high_residual&#39;], &#39;type&#39;] = &#39;High Residual&#39;
    df.loc[df[&#39;high_leverage&#39;], &#39;type&#39;] = &#39;High Leverage&#39;
    df.loc[df[&#39;high_influence&#39;], &#39;type&#39;] = &#39;High Influence&#39;

    # Init figure
    fig, (ax1,ax2) = plt.subplots(1,2, figsize=(12,5)) 
    ax1.plot(X, Y_hat, lw=1, c=&#39;grey&#39;, zorder=0.5)
    sns.scatterplot(data=df, x=&#39;hours&#39;, y=&#39;transactions&#39;, ax=ax1, hue=&#39;type&#39;).set(title=&#39;Data&#39;)
    sns.scatterplot(data=df, x=&#39;residual&#39;, y=&#39;leverage&#39;, hue=&#39;type&#39;, ax=ax2).set(title=&#39;Metrics&#39;)
    ax1.get_legend().remove()
    sns.move_legend(ax2, &amp;quot;upper left&amp;quot;, bbox_to_anchor=(1.05, 0.8));
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plot_leverage_residuals(df)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/outliers_levarage_34_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;As we can see, we have one point with high residual and low leverage, one with high leverage and low residual and only one point with both high leverage and high residual: the only influential point.&lt;/p&gt;
&lt;p&gt;From the plot it is also clear why none of the two conditions alone is sufficient for an observation to rive the model. The orange point has high residual but it lies right in the middle of the distribution and therefore cannot tilt the line of best fit. The green point instead has high leverage and lies far from the center of the distribution but its perfectly aligned with the line of fit. Removing it would not change anything. The red dot instead is different from the others in terms of &lt;strong&gt;both characteristics and behavior&lt;/strong&gt; and therefore tilts the fit line towards itself.&lt;/p&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;In this post, we have seen some different ways in which observations can be &amp;ldquo;unusual&amp;rdquo;: they can have either unusual characteristics or unusual behavior. In linear regression, when an observation has both it is also influential: it tilts the model towards itself.&lt;/p&gt;
&lt;p&gt;In the example of the article, we concentrated on a univariate linear regression. However, research on influence functions has recently become a hot topic because of the need to make black-box machine learning algorithms understandable. With models with millions of parameters, billions of observations and wild non-linearities, it can be very hard to establish whether a single observation is influential and how.&lt;/p&gt;
&lt;h3 id=&#34;references&#34;&gt;References&lt;/h3&gt;
&lt;p&gt;[1] D. Cook, &lt;a href=&#34;https://www.jstor.org/stable/1268249&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Detection of Influential Observation in Linear Regression&lt;/a&gt; (1980), &lt;em&gt;Technometrics&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;[2] D. Cook, S. Weisberg, &lt;a href=&#34;https://www.jstor.org/stable/1268187&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Characterizations of an Empirical Influence Function for Detecting Influential Cases in
Regression&lt;/a&gt; (1980), &lt;em&gt;Technometrics&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;[2] P. W. Koh, P. Liang, &lt;a href=&#34;http://proceedings.mlr.press/v70/koh17a&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Understanding Black-box Predictions via Influence Functions&lt;/a&gt; (2017), &lt;em&gt;ICML Proceedings&lt;/em&gt;.&lt;/p&gt;
&lt;h3 id=&#34;code&#34;&gt;Code&lt;/h3&gt;
&lt;p&gt;You can find the original Jupyter Notebook here:&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/outliers.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/outliers.ipynb&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Synthetic Control</title>
      <link>https://matteocourthoud.github.io/post/synthetic_control/</link>
      <pubDate>Mon, 10 Jul 2023 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/post/synthetic_control/</guid>
      <description>&lt;p&gt;INTRO&lt;/p&gt;
&lt;h2 id=&#34;self-driving-cars&#34;&gt;Self-Driving Cars&lt;/h2&gt;
&lt;p&gt;Suppose you were a &lt;strong&gt;ride-sharing platform&lt;/strong&gt; and you want to test the effect of self-driving cars in your fleet.&lt;/p&gt;
&lt;p&gt;As you can imagine, there are many &lt;strong&gt;limitations&lt;/strong&gt; to running an AB/test for this type of feature. First of all, it&amp;rsquo;s complicated to randomize individual rides. Second, it&amp;rsquo;s a very expensive intervention. Third, and statistically most important, you cannot run this intervention at the ride level. The problem is that there are &lt;strong&gt;spillover&lt;/strong&gt; effects from treated to control units: if indeed self-driving cars are more efficient, it means that they can serve more customers in the same amount of time, reducing the customers available to normal drivers (the control group). This spillover &lt;strong&gt;contaminates&lt;/strong&gt; the experiment and prevents a causal interpretation of the results.&lt;/p&gt;
&lt;p&gt;For all these reasons, we select only one city at random to run this experiment&amp;hellip; (drum roll)&amp;hellip; Miami!&lt;/p&gt;
&lt;img src=&#34;fig/miami2.jpg&#34; width=&#34;700px&#34;/&gt;
&lt;p&gt;I generate a simulated dataset in which we observe a panel of U.S. cities over time. The revenue data is made up, while the socio-economic variables are taken from the &lt;a href=&#34;https://stats.oecd.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;OECD database&lt;/a&gt;. I import the data generating process &lt;code&gt;dgp_selfdriving()&lt;/code&gt; from &lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/src/dgp.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;src.dgp&lt;/code&gt;&lt;/a&gt;. I also import some plotting functions and libraries from &lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/src/utils.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;src.utils&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;%matplotlib inline
%config InlineBackend.figure_format = &#39;retina&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from src.utils import *
from src.dgp import dgp_selfdriving

treatment_year = 2013
treated_city = &#39;Miami&#39;
df = dgp_selfdriving().generate_data(year=treatment_year, city=treated_city)
df.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;city&lt;/th&gt;
      &lt;th&gt;year&lt;/th&gt;
      &lt;th&gt;density&lt;/th&gt;
      &lt;th&gt;employment&lt;/th&gt;
      &lt;th&gt;gdp&lt;/th&gt;
      &lt;th&gt;population&lt;/th&gt;
      &lt;th&gt;treated&lt;/th&gt;
      &lt;th&gt;post&lt;/th&gt;
      &lt;th&gt;revenue&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;Atlanta&lt;/td&gt;
      &lt;td&gt;2003&lt;/td&gt;
      &lt;td&gt;290&lt;/td&gt;
      &lt;td&gt;0.629761&lt;/td&gt;
      &lt;td&gt;6.4523&lt;/td&gt;
      &lt;td&gt;4.267538&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;25.713947&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;Atlanta&lt;/td&gt;
      &lt;td&gt;2004&lt;/td&gt;
      &lt;td&gt;295&lt;/td&gt;
      &lt;td&gt;0.635595&lt;/td&gt;
      &lt;td&gt;6.5836&lt;/td&gt;
      &lt;td&gt;4.349712&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;23.852279&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;Atlanta&lt;/td&gt;
      &lt;td&gt;2005&lt;/td&gt;
      &lt;td&gt;302&lt;/td&gt;
      &lt;td&gt;0.645614&lt;/td&gt;
      &lt;td&gt;6.6998&lt;/td&gt;
      &lt;td&gt;4.455273&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;24.332397&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;Atlanta&lt;/td&gt;
      &lt;td&gt;2006&lt;/td&gt;
      &lt;td&gt;313&lt;/td&gt;
      &lt;td&gt;0.648573&lt;/td&gt;
      &lt;td&gt;6.5653&lt;/td&gt;
      &lt;td&gt;4.609096&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;23.816017&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;Atlanta&lt;/td&gt;
      &lt;td&gt;2007&lt;/td&gt;
      &lt;td&gt;321&lt;/td&gt;
      &lt;td&gt;0.650976&lt;/td&gt;
      &lt;td&gt;6.4184&lt;/td&gt;
      &lt;td&gt;4.737037&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;25.786902&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;len(df.city.unique())
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;46
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We have information on the largest 46 U.S. cities for the period 2002-2019. The panel is &lt;strong&gt;balanced&lt;/strong&gt;, which means that we observe all cities for all time periods.&lt;/p&gt;
&lt;p&gt;Is the &lt;strong&gt;treated&lt;/strong&gt; unit, Miami, comparable to the rest of the sample? Let&amp;rsquo;s use the &lt;code&gt;create_table_one&lt;/code&gt; function from Uber&amp;rsquo;s &lt;a href=&#34;https://causalml.readthedocs.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;causalml&lt;/code&gt;&lt;/a&gt; package to produce a &lt;strong&gt;covariate balance table&lt;/strong&gt;, containing the average value of our observable characteristics, across treatment and control groups. As the name suggests, this should always be the first table you present in causal inference analysis.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from causalml.match import create_table_one

create_table_one(df, &#39;treated&#39;, [&#39;density&#39;, &#39;employment&#39;, &#39;gdp&#39;, &#39;population&#39;, &#39;revenue&#39;])
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;Control&lt;/th&gt;
      &lt;th&gt;Treatment&lt;/th&gt;
      &lt;th&gt;SMD&lt;/th&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;Variable&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;n&lt;/th&gt;
      &lt;td&gt;765&lt;/td&gt;
      &lt;td&gt;17&lt;/td&gt;
      &lt;td&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;density&lt;/th&gt;
      &lt;td&gt;256.63 (172.90)&lt;/td&gt;
      &lt;td&gt;364.94 (19.61)&lt;/td&gt;
      &lt;td&gt;0.8802&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;employment&lt;/th&gt;
      &lt;td&gt;0.63 (0.05)&lt;/td&gt;
      &lt;td&gt;0.60 (0.04)&lt;/td&gt;
      &lt;td&gt;-0.5266&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;gdp&lt;/th&gt;
      &lt;td&gt;6.07 (1.16)&lt;/td&gt;
      &lt;td&gt;5.12 (0.29)&lt;/td&gt;
      &lt;td&gt;-1.1124&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;population&lt;/th&gt;
      &lt;td&gt;3.53 (3.81)&lt;/td&gt;
      &lt;td&gt;5.85 (0.31)&lt;/td&gt;
      &lt;td&gt;0.861&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;revenue&lt;/th&gt;
      &lt;td&gt;25.25 (2.45)&lt;/td&gt;
      &lt;td&gt;23.86 (2.39)&lt;/td&gt;
      &lt;td&gt;-0.5737&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;As expected, the groups are &lt;strong&gt;not balanced&lt;/strong&gt;: Miami is more densely populated, poorer, larger and has lower employment rate than the other cities in the US in our sample.&lt;/p&gt;
&lt;p&gt;We are interested in understanding the impact of the introduction of &lt;strong&gt;self-driving cars&lt;/strong&gt; on &lt;code&gt;revenue&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;One initial idea could be to analyze the data as we would in an A/B test, comparing control and treatment group. We can estimate the treatment effect as a difference in means in &lt;code&gt;revenue&lt;/code&gt; between the treatment and control group, after the introduction of self-driving cars.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;smf.ols(&#39;revenue ~ treated&#39;, data=df[df[&#39;post&#39;]==True]).fit().summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
         &lt;td&gt;&lt;/td&gt;            &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt;       &lt;td&gt;   26.6006&lt;/td&gt; &lt;td&gt;    0.127&lt;/td&gt; &lt;td&gt;  210.061&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;   26.351&lt;/td&gt; &lt;td&gt;   26.850&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;treated[T.True]&lt;/th&gt; &lt;td&gt;   -0.7156&lt;/td&gt; &lt;td&gt;    0.859&lt;/td&gt; &lt;td&gt;   -0.833&lt;/td&gt; &lt;td&gt; 0.405&lt;/td&gt; &lt;td&gt;   -2.405&lt;/td&gt; &lt;td&gt;    0.974&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;The effect of self-driving cars seems to be negative but not significant.&lt;/p&gt;
&lt;p&gt;The main &lt;strong&gt;problem&lt;/strong&gt; here is that we have a &lt;strong&gt;single treated unit&lt;/strong&gt;: Miami. It&amp;rsquo;s very hard to argue that Miami is comparable to other cities. Randomization ensures that this simple estimator is unbiased, ex-ante. However, with a single treated unit, the estimator suffers from severe small sample bias .&lt;/p&gt;
&lt;p&gt;One alternative procedure, is to compare revenue &lt;strong&gt;before and after&lt;/strong&gt; the treatment, within the city of Miami.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;smf.ols(&#39;revenue ~ post&#39;, data=df[df[&#39;city&#39;]==treated_city]).fit().summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
        &lt;td&gt;&lt;/td&gt;          &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt;    &lt;td&gt;   22.4485&lt;/td&gt; &lt;td&gt;    0.534&lt;/td&gt; &lt;td&gt;   42.044&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;   21.310&lt;/td&gt; &lt;td&gt;   23.587&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;post[T.True]&lt;/th&gt; &lt;td&gt;    3.4364&lt;/td&gt; &lt;td&gt;    0.832&lt;/td&gt; &lt;td&gt;    4.130&lt;/td&gt; &lt;td&gt; 0.001&lt;/td&gt; &lt;td&gt;    1.663&lt;/td&gt; &lt;td&gt;    5.210&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;The effect of self-driving cars seems to be positive and statistically significant.&lt;/p&gt;
&lt;p&gt;However, the &lt;strong&gt;problem&lt;/strong&gt; of this procedure is that there might have been many &lt;strong&gt;other things happening&lt;/strong&gt; after 2016. It&amp;rsquo;s a very strong stretch to attribute all differences to self-driving cars.&lt;/p&gt;
&lt;p&gt;We can better understand this concern if we plot the time trend of revenue over cities. First, we need to reshape the data into a &lt;strong&gt;wide format&lt;/strong&gt;, with one column per city and one row per year.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df = df.pivot(index=&#39;year&#39;, columns=&#39;city&#39;, values=&#39;revenue&#39;).reset_index()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, let&amp;rsquo;s plot the revenue over time for Miami and for the other cities.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;cities = [c for c in df.columns if c!=&#39;year&#39;]
df[&#39;Other Cities&#39;] = df[[c for c in cities if c != treated_city]].mean(axis=1)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def plot_lines(df, line1, line2, year, hline=True):
    sns.lineplot(x=df[&#39;year&#39;], y=df[line1].values, label=line1)
    sns.lineplot(x=df[&#39;year&#39;], y=df[line2].values, label=line2)
    plt.axvline(x=year, ls=&amp;quot;:&amp;quot;, color=&#39;C2&#39;, label=&#39;Self-Driving Cars&#39;, zorder=1)
    plt.legend();
    plt.title(&amp;quot;Average revenue per day (in M$)&amp;quot;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Since we are talking about Miami, let&amp;rsquo;s use an appropriate color palette.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sns.set_palette(sns.color_palette([&#39;#f14db3&#39;, &#39;#0dc3e2&#39;, &#39;#443a84&#39;]))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plot_lines(df, treated_city, &#39;Other Cities&#39;, treatment_year)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/synthetic_control_22_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;As we can see, revenue seems to be increasing after the treatment in Miami. But it&amp;rsquo;s a very volatile time series. And revenue was increasing also in the rest of the country. It&amp;rsquo;s very hard from this plot to attribute the change to self-driving case.&lt;/p&gt;
&lt;p&gt;Can we do better?&lt;/p&gt;
&lt;h2 id=&#34;synthetic-control&#34;&gt;Synthetic Control&lt;/h2&gt;
&lt;p&gt;The answer is yes! Synthetic control methods were first introduced by &lt;a href=&#34;https://www.tandfonline.com/doi/abs/10.1198/jasa.2009.ap08746&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Abadie, Diamond and Hainmueller (2010)&lt;/a&gt; and allow us to do causal inference when we have as few as one treated unit and many control units and we observe them over time.&lt;/p&gt;
&lt;h3 id=&#34;setting&#34;&gt;Setting&lt;/h3&gt;
&lt;p&gt;We assume that for a panel of i.i.d. subjects $i = 1, &amp;hellip;, n$ over time $t=1, &amp;hellip;,T$ we observed a set of variables $(X_{it}, D_i, Y_{it})$ that includes&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;a treatment assignment $D_i \in \lbrace 0, 1 \rbrace$ (&lt;code&gt;treated&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;a response $Y_{i,t} \in \mathbb R$ (&lt;code&gt;revenue&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;a feature vector $X_{i,t} \in \mathbb R^n$ (&lt;code&gt;population&lt;/code&gt;, &lt;code&gt;density&lt;/code&gt;, &lt;code&gt;employment&lt;/code&gt; and &lt;code&gt;GDP&lt;/code&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Moreover, one unit (Miami in our case) is treated at time $t^*$ (2016 in our case). We distinguish time periods before treatment and time periods after treatment.&lt;/p&gt;
&lt;p&gt;Crucially, treatment $D_i$ is not randomly assigned, therefore a difference in means between the treated unit(s) and the control group is not an unbiased estimator of the average treatment effect.&lt;/p&gt;
&lt;h3 id=&#34;the-problem&#34;&gt;The Problem&lt;/h3&gt;
&lt;p&gt;The problem is that, as usual, we do not observe the counterfactual outcome for treated units, i.e. we do not know what would have happened to them, if they had not been treated. This is known as the &lt;strong&gt;fundamental problem of causal inference&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;The simplest approach, would be just to compare pre and post periods. This is called the &lt;strong&gt;event study&lt;/strong&gt; approach.&lt;/p&gt;
&lt;p&gt;However, we can do better than this. In fact, even though treatment was not randomly assigned, we still have access to some units that were not treated.&lt;/p&gt;
&lt;p&gt;For the outcome variable we have the following setup&lt;/p&gt;
&lt;p&gt;$$
Y =
\begin{bmatrix}
Y_{t, post} \ &amp;amp; Y_{c, post} \newline
Y_{t, pre} \ &amp;amp; Y_{c, pre}
\end{bmatrix}
$$&lt;/p&gt;
&lt;p&gt;which we can rewrite as&lt;/p&gt;
&lt;p&gt;$$
Y =
\begin{bmatrix}
Y^{(1)} _ {t, post} \ &amp;amp; Y^{(0)} _ {c, post} \newline
Y^{(0)} _ {t, pre} \ &amp;amp; Y^{(0)} _ {c, pre}
\end{bmatrix}
$$&lt;/p&gt;
&lt;p&gt;We basically have a &lt;strong&gt;missing data problem&lt;/strong&gt; since we do not observe $Y^{(0)} _ {t, post}$.&lt;/p&gt;
&lt;h3 id=&#34;the-solution&#34;&gt;The Solution&lt;/h3&gt;
&lt;p&gt;Following &lt;a href=&#34;https://arxiv.org/pdf/1610.07748.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Doudchenko and Inbens (2018)&lt;/a&gt;, we can formulate an estimate of the counterfactual outcome for the treated unit as a linear combination of the observed outcomes for the control units.&lt;/p&gt;
&lt;p&gt;$$
\hat Y^{(0)} _ {t, post} = \alpha + \sum_{i \in c} \beta_{i} Y^{(0)} _ {i, post}
$$&lt;/p&gt;
&lt;p&gt;where&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the constant $\alpha$ allows for different averages between the two groups&lt;/li&gt;
&lt;li&gt;the weights $\beta_i$ are allowed to vary across control units $i$ (otherwise, it would be a difference-in-differences)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;How should we &lt;strong&gt;choose which weights&lt;/strong&gt; to use? We want our synthetic control to approximate the outcome as closely as possible, before the treatment. The first approach could be to define the weights as&lt;/p&gt;
&lt;p&gt;$$
\hat \beta = \arg \min_{\beta} || \boldsymbol X_{t, pre} - \boldsymbol \beta \boldsymbol X_{c, pre} || = \sqrt{ \sum_{p} \left( X_{t, p, pre}  - \sum_{i \in c} \beta_{p} X_{c, p, pre} \right)^2 }
$$&lt;/p&gt;
&lt;p&gt;I.e. the weights are such that they minimize the distance between observable characteristics of control units $X_c$ and the treated unit $X_t$ before the treatment.&lt;/p&gt;
&lt;p&gt;You might notice a very close similarity to &lt;strong&gt;linear regression&lt;/strong&gt;.  Indeed, we are doing something very similar.&lt;/p&gt;
&lt;p&gt;In linear regression, we usually have &lt;strong&gt;many units&lt;/strong&gt; (observations), &lt;strong&gt;few exogenous features&lt;/strong&gt; and &lt;strong&gt;one endogenous feature&lt;/strong&gt; and we try to express the endogenous feature as a linear combination of the endogenous features, for each unit.&lt;/p&gt;
&lt;img src=&#34;fig/synth1.png&#34; width=&#34;500px&#34;/&gt;
&lt;p&gt;With synthetic control, we instead have &lt;strong&gt;many time periods&lt;/strong&gt; (features), few &lt;strong&gt;control units&lt;/strong&gt; and a single &lt;strong&gt;treated unit&lt;/strong&gt; and we try to express the treated unit as a linear combination of the control units, for each time period&lt;/p&gt;
&lt;img src=&#34;fig/synth2.png&#34; width=&#34;500px&#34;/&gt;
&lt;p&gt;In the end, we are doing the same thing, but with a transposed dataset.&lt;/p&gt;
&lt;img src=&#34;fig/synth3.png&#34; width=&#34;500px&#34;/&gt;
&lt;h3 id=&#34;back-to-self-driving-cars&#34;&gt;Back to self-driving cars&lt;/h3&gt;
&lt;p&gt;Let&amp;rsquo;s go back to the data now! First, we write a &lt;code&gt;synth_predict&lt;/code&gt; function that takes as input a model that is trained on control cities and tries to predict the outcome of the treated city, Miami, before the introduction of self-driving cars.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def synth_predict(df, model, city, year):
    other_cities = [c for c in cities if c not in [&#39;year&#39;, city]]
    y = df.loc[df[&#39;year&#39;] &amp;lt;= year, city]
    X = df.loc[df[&#39;year&#39;] &amp;lt;= year, other_cities]
    df[f&#39;Synthetic {city}&#39;] = model.fit(X, y).predict(df[other_cities])
    return model
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s estimate the model via linear regression.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from sklearn.linear_model import LinearRegression

coef = synth_predict(df, LinearRegression(), treated_city, treatment_year).coef_
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;How well did we &lt;strong&gt;match&lt;/strong&gt; pre-self-driving cars &lt;code&gt;revenue&lt;/code&gt; in Miami? What is the implied &lt;strong&gt;effect&lt;/strong&gt; of self-driving cars?&lt;/p&gt;
&lt;p&gt;We can visually answer both questions by plotting the actual revenue in Miami against the predicted one.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plot_lines(df, treated_city, f&#39;Synthetic {treated_city}&#39;, treatment_year)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/synthetic_control_43_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;It looks like self-driving cars had a sensible &lt;strong&gt;positive effect&lt;/strong&gt; on &lt;code&gt;revenue&lt;/code&gt; in Miami: the predicted trend is lower than the actual data and diverges right after the introduction of self-driving cars.&lt;/p&gt;
&lt;p&gt;On the other hand, we are clearly &lt;strong&gt;overfitting&lt;/strong&gt;: the pre-treatment predicted &lt;code&gt;revenue&lt;/code&gt; line is perfectly overlapping with the actual data. Given the high variability of &lt;code&gt;revenue&lt;/code&gt; in Miami, this is suspicious, to say the least.&lt;/p&gt;
&lt;p&gt;Another problem concerns the &lt;strong&gt;weights&lt;/strong&gt;. Let&amp;rsquo;s plot them.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_states = pd.DataFrame({&#39;city&#39;: [c for c in cities if c!=treated_city], &#39;ols_coef&#39;: coef})
plt.figure(figsize=(10, 10))
sns.barplot(data=df_states, x=&#39;ols_coef&#39;, y=&#39;city&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/synthetic_control_45_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;We have many &lt;strong&gt;negative weights&lt;/strong&gt;, which do not make much sense from a causal inference perspective. I can understand that Miami can be expressed as a combination of 0.2 St. Louis, 0.15 Oklahoma and 0.15 Hartford. But what does it mean that Miami is -0.15 Milwaukee?&lt;/p&gt;
&lt;p&gt;Since we would like to interpret our synthetic control as a &lt;strong&gt;weighted average&lt;/strong&gt; of untreated states, all weights should be positive  and they should sum to one.&lt;/p&gt;
&lt;p&gt;To address both concerns (weighting and overfitting), we need to impose some &lt;strong&gt;restrictions on the weights&lt;/strong&gt;.&lt;/p&gt;
&lt;h2 id=&#34;extensions&#34;&gt;Extensions&lt;/h2&gt;
&lt;h3 id=&#34;weights&#34;&gt;Weights&lt;/h3&gt;
&lt;p&gt;To solve the problems of overweighting and negative weights, &lt;a href=&#34;https://www.tandfonline.com/doi/abs/10.1198/jasa.2009.ap08746&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Abadie, Diamond and Hainmueller (2010)&lt;/a&gt; propose the following weights:&lt;/p&gt;
&lt;p&gt;$$
\hat \beta = \arg \min_{\beta} || \boldsymbol X_t - \boldsymbol \beta \boldsymbol X_c || = \sqrt{ \sum_{p} \left( X_{t, p}  - \sum_{i \in c} \beta_{p} X_{c, p} \right)^2 }
\quad \text{s.t.} \quad \sum_{p} \beta_p = 1 \quad \text{and} \quad \beta_p \geq 0 \quad \forall p
$$&lt;/p&gt;
&lt;p&gt;Which means, a set of weights $\beta$ such that&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;weighted observable characteristics of the control group $X_c$, match the observable characteristics of the treatment group $X_t$, before the treatment&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;they sum to 1&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;and are not negative.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;With this approach we get an &lt;strong&gt;interpretable counterfactual&lt;/strong&gt; as a weighted avarage of untreated units.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s write now our own objective function. I create a new class &lt;code&gt;SyntheticControl()&lt;/code&gt; which has both a &lt;code&gt;loss&lt;/code&gt; function, as described above, a method to &lt;code&gt;fit&lt;/code&gt; it and &lt;code&gt;predict&lt;/code&gt; the values for the treated unit.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from toolz import partial
from scipy.optimize import fmin_slsqp

class SyntheticControl():
    
    # Loss function
    def loss(self, W, X, y) -&amp;gt; float:
        return np.sqrt(np.mean((y - X.dot(W))**2))

    # Fit model
    def fit(self, X, y):
        w_start = [1/X.shape[1]]*X.shape[1]
        self.coef_ = fmin_slsqp(partial(self.loss, X=X, y=y),
                         np.array(w_start),
                         f_eqcons=lambda x: np.sum(x) - 1,
                         bounds=[(0.0, 1.0)]*len(w_start),
                         disp=False)
        self.mse = self.loss(W=self.coef_, X=X, y=y)
        return self
    
    # Predict 
    def predict(self, X):
        return X.dot(self.coef_)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can now repeat the same procedure as before, but using the &lt;code&gt;SyntheticControl&lt;/code&gt; method instead of the simple, unconstrained &lt;code&gt;LinearRegression&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_states[&#39;coef_synth&#39;] = synth_predict(df, SyntheticControl(), treated_city, treatment_year).coef_
plot_lines(df, treated_city, f&#39;Synthetic {treated_city}&#39;, treatment_year)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/synthetic_control_52_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;As we can see, now we are &lt;strong&gt;not overfitting&lt;/strong&gt; anymore. The actual and predicted &lt;code&gt;revenue&lt;/code&gt; pre-treatment are close but not identical. The reason is that the non-negativity constraint is constraining most coefficients to be zero (as &lt;a href=&#34;https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Lasso&lt;/a&gt; does).&lt;/p&gt;
&lt;p&gt;It looks like the effect is again negative. However, let&amp;rsquo;s plot the &lt;strong&gt;difference&lt;/strong&gt; between the two lines to better visualize the magnitude.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def plot_difference(df, city, year, vline=True, hline=True, **kwargs):
    sns.lineplot(x=df[&#39;year&#39;], y=df[city] - df[f&#39;Synthetic {city}&#39;], **kwargs)
    if vline: 
        plt.axvline(x=year, ls=&amp;quot;:&amp;quot;, color=&#39;C2&#39;, lw=3, label=&#39;Self-driving Cars&#39;, zorder=100)
        plt.legend()
    if hline: sns.lineplot(x=df[&#39;year&#39;], y=0, lw=3, color=&#39;k&#39;, zorder=1)
    plt.title(&amp;quot;Estimated effect of self-driving cars&amp;quot;);
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plot_difference(df, treated_city, treatment_year)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/synthetic_control_55_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;The difference is clearly positive and slightly increasing over time.&lt;/p&gt;
&lt;p&gt;We can also visualize the &lt;strong&gt;weights&lt;/strong&gt; to interpret the estimated counterfactual (what would have happened in Miami, without self-driving cars).&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plt.figure(figsize=(10, 10))
sns.barplot(data=df_states, x=&#39;coef_synth&#39;, y=&#39;city&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/synthetic_control_57_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;As we can see, now we are expressing &lt;code&gt;revenue&lt;/code&gt; in Miami as a linear combination of just a couple of cities: Tampa, St. Louis and, to a lower extent, Las Vegas. This makes the whole procedure very &lt;strong&gt;transparent&lt;/strong&gt;.&lt;/p&gt;
&lt;h3 id=&#34;inference&#34;&gt;Inference&lt;/h3&gt;
&lt;p&gt;What about &lt;strong&gt;inference&lt;/strong&gt;? Is the estimate significantly different from zero? Or, more practically, &amp;ldquo;&lt;em&gt;how unusual is this estimate under the null hypothesis of no policy effect&lt;/em&gt;?&amp;rdquo;.&lt;/p&gt;
&lt;p&gt;We are going to perform a &lt;a href=&#34;https://en.wikipedia.org/wiki/Permutation_test&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;randomization/permutation test&lt;/strong&gt;&lt;/a&gt; in order to answer this question. The &lt;strong&gt;idea&lt;/strong&gt; is that if the policy has no effect, the effect we observe for Miami should not be significantly different from the effect we observe for any other city.&lt;/p&gt;
&lt;p&gt;Therefore, we are going to replicate the procedure above, but for all other cities and compare them with the estimate for Miami.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from matplotlib.offsetbox import OffsetImage, AnnotationBbox

fig, ax = plt.subplots()
for city in cities:
    synth_predict(df, SyntheticControl(), city, treatment_year)
    plot_difference(df, city, treatment_year, vline=False, alpha=0.2, color=&#39;C1&#39;, lw=3)
plot_difference(df, treated_city, treatment_year)
ax.add_artist(AnnotationBbox(OffsetImage(mpimg.imread(&#39;fig/miami.png&#39;), zoom=0.25), (2015, 2.7), frameon=False));
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;---------------------------------------------------------------------------

NameError                                 Traceback (most recent call last)

Input In [21], in &amp;lt;module&amp;gt;
      6     plot_difference(df, city, treatment_year, vline=False, alpha=0.2, color=&#39;C1&#39;, lw=3)
      7 plot_difference(df, treated_city, treatment_year)
----&amp;gt; 8 ax.add_artist(AnnotationBbox(OffsetImage(mpimg.imread(&#39;fig/miami.png&#39;), zoom=0.25), (2015, 2.7), frameon=False))


NameError: name &#39;mpimg&#39; is not defined
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/synthetic_control_61_1.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;From the graph we notice two things. First, the effect for California is quite extreme and therefore likely not to be driven by random noise.&lt;/p&gt;
&lt;p&gt;Second, we also notice that there are a couple of states for which we cannot fit the pre-trend very well. This is expected since, for each state, we are building the counterfactual trend as a &lt;strong&gt;convex combination&lt;/strong&gt; of all other states. States that are quite extreme in terms of cigarette consumpion are very useful to build the counterfactuals of other states, but it&amp;rsquo;s hard to build a counterfactual for them. Not to bias the analysis, let&amp;rsquo;s exclude states for which we cannot build a &amp;ldquo;good enough&amp;rdquo; counterfectual, in terms of pre-treatment MSE.&lt;/p&gt;
&lt;p&gt;$$
MSE_{pre} = \frac{1}{n} \sum_{t \in \text{pre}} \left( Y_t - \hat Y_t \right)^2
$$&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;mse_treated = synth_predict(df, SyntheticControl(), treated_city, treatment_year).mse

fig, ax = plt.subplots()
for city in cities:
    mse = synth_predict(df, SyntheticControl(), city, treatment_year).mse
    if mse &amp;lt; 2 * mse_treated:
        plot_difference(df, city, treatment_year, vline=False, alpha=0.2, color=&#39;C1&#39;, lw=3)
plot_difference(df, treated_city, treatment_year)
ax.add_artist(AnnotationBbox(OffsetImage(mpimg.imread(&#39;fig/miami.png&#39;), zoom=0.25), (2015, 2.7), frameon=False));
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;After exluding extreme observations, it looks like the effect for California is very unusual, especially if we consider a one-sided hypothesis test (it feels weird to assume that the policy could ever increase cigarette sales).&lt;/p&gt;
&lt;p&gt;One &lt;strong&gt;statistic&lt;/strong&gt; that the authors suggest to perform a randomization test is the ratio between pre-treatment MSE and post-treatment MSE.&lt;/p&gt;
&lt;p&gt;$$
\lambda = \frac{MSE_{post}}{MSE_{pre}} = \frac{\frac{1}{n} \sum_{t \in \text{post}} \left( Y_t - \hat Y_t \right)^2 }{\frac{1}{n} \sum_{t \in \text{pre}} \left( Y_t - \hat Y_t \right)^2 }
$$&lt;/p&gt;
&lt;p&gt;We can compute a p-value as the number of observations with higher ratio.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;lambdas = {}
for city in cities:
    mse_pre = synth_predict(df, SyntheticControl(), city, treatment_year).mse
    mse_tot = np.mean((df[f&#39;Synthetic {city}&#39;] - df[city])**2)
    lambdas[city] = (mse_tot - mse_pre) / mse_pre
    
print(f&amp;quot;p-value: {np.mean(np.fromiter(lambdas.values(), dtype=&#39;float&#39;) &amp;gt; lambdas[treated_city]):.4}&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It seems that only $4.3%$ of the cities had a larger MSE ratio. We can visualize the distribution of the statistic under permutation with a histogram.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;fig, ax = plt.subplots()
_, bins, _ = plt.hist(lambdas.values(), bins=20, color=&amp;quot;C1&amp;quot;);
plt.hist([lambdas[treated_city]], bins=bins)
plt.title(&#39;Ratio of $MSE_{post}$ and $MSE_{pre}$ across cities&#39;);
ax.add_artist(AnnotationBbox(OffsetImage(plt.imread(&#39;fig/miami.png&#39;), zoom=0.25), (2.7, 1.6), frameon=False));
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Indeed, the statistic for Miami is quite extreme.&lt;/p&gt;
&lt;h3 id=&#34;synthetic-control-vs-other-methods&#34;&gt;Synthetic Control vs Other Methods&lt;/h3&gt;
&lt;p&gt;What are the advantages and disadvantages of synthetic control methods with respect to other methods?&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;As long as we use positive weights that are constrained to sum to one, the method &lt;strong&gt;avoids extrapolation&lt;/strong&gt;: we will never go out of the support of the data&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;It can be &lt;strong&gt;&amp;ldquo;pre-registered&amp;rdquo;&lt;/strong&gt; in the sense that you don&amp;rsquo;t need post-treatment observations to build the method: could avoid p-hacking and cherry picking&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Weights make the counterfactual analysis &lt;strong&gt;explicit&lt;/strong&gt;: one can look at the weights and understand which comparison we are making&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;It&amp;rsquo;s a &lt;strong&gt;bridge&lt;/strong&gt; between quantitative and qualitative research: can be used to inspect single-treated unit cases&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;TBD&lt;/p&gt;
&lt;h3 id=&#34;references&#34;&gt;References&lt;/h3&gt;
&lt;p&gt;[1] A. Abadie, A. Diamond and J. Hainmueller, &lt;a href=&#34;https://www.tandfonline.com/doi/abs/10.1198/jasa.2009.ap08746&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Synthetic Control Methods for Comparative Case Studies: Estimating the Effect of California’s Tobacco Control Program&lt;/a&gt; (2010), &lt;em&gt;Journal of the American Statistical Association&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;[2] A. Abadie, J. L&amp;rsquo;Hour, &lt;a href=&#34;https://www.tandfonline.com/doi/full/10.1080/01621459.2021.1971535&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;A Penalized Synthetic Control Estimator for Disaggregated Data&lt;/a&gt; (2020), &lt;em&gt;Journal of the American Statistical Association&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;[3] N. Doudchenko, G. Imbens, &lt;a href=&#34;https://arxiv.org/pdf/1610.07748.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Balancing, Regression, Difference-In-Differences and Synthetic Control Methods: A Synthesis&lt;/a&gt; (2017), &lt;em&gt;working paper&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;[] Matrix completion methods for causal panel data models&lt;/p&gt;
&lt;p&gt;[] Using Synthetic Controls: Feasibility, Data Requirements, and Methodological Aspects&lt;/p&gt;
&lt;h3 id=&#34;code&#34;&gt;Code&lt;/h3&gt;
&lt;p&gt;You can find the original Jupyter Notebook here:&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/synthetic_control.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/synthetic_control.ipynb&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>The Bayesian Bootstrap</title>
      <link>https://matteocourthoud.github.io/post/bayesian_bootstrap/</link>
      <pubDate>Mon, 10 Jul 2023 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/post/bayesian_bootstrap/</guid>
      <description>&lt;p&gt;&lt;em&gt;A short guide to a simple and powerful alternative to the bootstrap&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;In causal inference we do not want just to compute treatment effect, we also want to do &lt;strong&gt;inference&lt;/strong&gt; (duh!). In some cases, it&amp;rsquo;s very easy to compute the asymptotic difference of an estimator, thanks to the &lt;a href=&#34;https://en.wikipedia.org/wiki/Central_limit_theorem&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;central limit theorem&lt;/strong&gt;&lt;/a&gt;. This is the case of computing the average treatment effect in AB tests or randomized controlled trials, for example. However, in other settings, inference is more &lt;strong&gt;complicated&lt;/strong&gt;. The most frequent setting is the computation of quantities that are not sums or averages, such as the median treatment effect, for example. In these cases, we cannot rely on the central limit theorem. What can we do then?&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;bootstrap&lt;/strong&gt; is the answer! It is a very powerful procedure to compute the distribution of an estimator, without needing any knowledge of the data generating process. It is also very &lt;strong&gt;intuitive and simple&lt;/strong&gt; to implement: just re-sample your data with replacement a lot of times and compute your estimator on the re-computed sample.&lt;/p&gt;
&lt;p&gt;Can we do better? The answer is yes! The &lt;strong&gt;Bayesian Bootstrap&lt;/strong&gt; is a powerful procedure that in a lot of setting performs &lt;strong&gt;better&lt;/strong&gt; than the bootstrap. In particular, it&amp;rsquo;s usually faster, can give tighter confidence intervals and prevents a lot of corner cases of the bootstrap. In this article we are going to explore this simple but powerful procedure more in detail.&lt;/p&gt;
&lt;h2 id=&#34;the-bootstrap&#34;&gt;The Bootstrap&lt;/h2&gt;
&lt;p&gt;Bootstrap is a procedure to compute properties of an estimator by random &lt;strong&gt;re-sampling with replacement&lt;/strong&gt; from the data. It was first introduced by &lt;a href=&#34;https://www.jstor.org/stable/2958830&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Efron (1979)&lt;/a&gt;. The procedure is very simple and consists in the following steps.&lt;/p&gt;
&lt;p&gt;Suppose you have access to an i.i.d. sample $\lbrace X_i \rbrace_{i=1}^n$ and you want to compute a statistic $\theta$ using an estimator $\hat \theta(X)$. You can approximate the distribution of $\hat \theta$ by&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Sample $n$ observations with replacement from your sample $\lbrace \tilde X_i \rbrace_{i=1}^n$&lt;/li&gt;
&lt;li&gt;Compute the estimator $\hat \theta_{bootstrap}(\tilde X)$&lt;/li&gt;
&lt;li&gt;Repeat steps 1 and 2 a large number of times&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The distribution of $\hat \theta_{bootstrap}$ is a good approximation of the distribution of $\hat \theta$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Why is the bootstrap so powerful?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;First of all, it&amp;rsquo;s &lt;strong&gt;easy to implement&lt;/strong&gt;. It does not require you to do anything more than what you were already doing: estimating $\theta$. You just need to do it &lt;em&gt;a lot of times&lt;/em&gt;. Indeed, the main disadvantage of the bootstrap is its &lt;strong&gt;computational speed&lt;/strong&gt;. If estimating $\theta$ once is slow, bootstrapping it is prohibitive.&lt;/p&gt;
&lt;p&gt;Second, the bootstrap makes &lt;strong&gt;no distributional assumption&lt;/strong&gt;. It only assumes a representative sample from your population, where observations are independent from each other. This assumption might be violated when observations are tightly connected with each other, such when studying social networks.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Is bootstrap just weighting?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In the end, what we are doing is assigning &lt;strong&gt;integer weights&lt;/strong&gt; to our observations, such that their sum adds up to $n$. Such distribution is the &lt;a href=&#34;https://en.wikipedia.org/wiki/Multinomial_distribution&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;multinomial distribution&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s have a look at what a multinomial distribution look like by drawing a sample of size 10.000.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;%matplotlib inline
%config InlineBackend.figure_format = &#39;retina&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from src.utils import *
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;N = 10_000
np.random.seed(1)
bootstrap_weights = np.random.multinomial(N, np.ones(N)/N)
np.sum(bootstrap_weights)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;10000
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;First of all, we check that indeed the weights sum up to 1000, or equivalently, we generated a re-sample of the same size of the data.&lt;/p&gt;
&lt;p&gt;We can now plot the &lt;strong&gt;distribution of weights&lt;/strong&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sns.countplot(bootstrap_weights, color=&#39;C0&#39;).set(title=&#39;Bootstrap Weights&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/bayesian_bootstrap_9_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;As we can see, around 3600 observations got zero weight, however a couple of observations got a weights of 6. Or equivalently, around 3600 observations did not get re-sampled while a couple of observations got samples as many as 6 times.&lt;/p&gt;
&lt;p&gt;Now you might have a spontaneous question: why not use &lt;strong&gt;continuous weights&lt;/strong&gt; instead of discrete ones?&lt;/p&gt;
&lt;p&gt;Very good question! The &lt;strong&gt;Bayesian Bootstrap&lt;/strong&gt; is the answer.&lt;/p&gt;
&lt;h2 id=&#34;the-bayesian-bootstrap&#34;&gt;The Bayesian Bootstrap&lt;/h2&gt;
&lt;p&gt;The Bayesian bootstrap was introduced by &lt;a href=&#34;https://www.jstor.org/stable/2240875&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Rubin (1981)&lt;/a&gt; and it&amp;rsquo;s based on a very simple &lt;strong&gt;idea&lt;/strong&gt;: why not draw a smoother distribution of weights? The continuous equivalent of the multinomial distribution is the &lt;a href=&#34;https://en.wikipedia.org/wiki/Dirichlet_distribution&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Dirichelet distribution&lt;/strong&gt;&lt;/a&gt;. Below I plot the probability distribution of Multinomial and Dirichelet weights for a single observation (they are Poisson and Gamma distributed, respectively).&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from scipy.stats import gamma, poisson

x1 = np.arange(0, 8, 0.001)
x2 = np.arange(0, 8, 1)
sns.barplot(x2, poisson.pmf(x2, mu=1), color=&#39;C0&#39;, label=&#39;Multinomial Weights&#39;); 
plt.plot(x1, gamma.pdf(x1, a=1.0001), color=&#39;C1&#39;, label=&#39;Dirichlet Weights&#39;);
plt.legend()
plt.title(&#39;Distribution of Bootstrap Weights&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/bayesian_bootstrap_13_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;The Bayesian Bootstrap has &lt;strong&gt;many advantages&lt;/strong&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The first and most intuitive one is that it delivers estimates that are much more &lt;strong&gt;smooth&lt;/strong&gt; than the normal bootstrap, because of its continuous weighting scheme.&lt;/li&gt;
&lt;li&gt;Moreover, the continuous weighting scheme &lt;strong&gt;prevents corner cases&lt;/strong&gt; from emerging, since no observation will ever receive zero weight. For example, in linear regression, no problem of &lt;a href=&#34;https://en.wikipedia.org/wiki/Multicollinearity&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;collinearity&lt;/a&gt; emerges, if there wasn&amp;rsquo;t one in the original sample.&lt;/li&gt;
&lt;li&gt;Lastly, being a Bayesian method, we gain &lt;strong&gt;interpretation&lt;/strong&gt;: the estimated distribution of the estimator can be interpreted as the &lt;a href=&#34;https://en.wikipedia.org/wiki/Posterior_probability&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;posterior distribution&lt;/a&gt; with an &lt;a href=&#34;https://en.wikipedia.org/wiki/Prior_probability&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;uninformative prior&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Let&amp;rsquo;s now draw a set a Dirichlet weights.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;bayesian_weights = np.random.dirichlet(alpha=np.ones(N), size=1)[0] * N
np.sum(bayesian_weights)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;10000.000000000005
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The weights naturally sum to (approximately) 1, so we have to scale them by a factor N.&lt;/p&gt;
&lt;p&gt;As before, we can plot the distribution of weights, with the difference that now we have continuous weights, so we have to approximate the distribution.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sns.histplot(bayesian_weights, color=&#39;C1&#39;).set(title=&#39;Dirichlet Weights&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/bayesian_bootstrap_17_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;As you might have noticed, the Dirichelet distirbution has a parameter $\alpha$ that we have set to 1 for all observations. What does it do?&lt;/p&gt;
&lt;p&gt;The $\alpha$ parameter essentially governs both the absolute and relative probability of being samples. Increasing $\alpha$ for all observations makes the distribution less skewed so that all observations have a more similar weight. For $\alpha \to \infty$, all observations receiver the same weight and we are back to the original sample.&lt;/p&gt;
&lt;p&gt;How should we pick $\alpha$? &lt;a href=&#34;https://link.springer.com/book/10.1007/978-1-4612-0795-5&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Shao and Tu (1995)&lt;/a&gt; suggest the following.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;The distribution of the random weight vector does not have to be restricted to the Diri(l, &amp;hellip; , 1). Later investigations found that the weights having a scaled Diri(4, &amp;hellip; ,4) distribution give better approximations (Tu and Zheng, 1987)&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Let&amp;rsquo;s have a look at how a Dirichelet distribution with $\alpha = 4$ for all observations compare to our previous distribution with $\alpha = 1$ for all observations.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;bayesian_weights2 = np.random.dirichlet(np.ones(N) * 4, 1)[0] * N
sns.histplot(bayesian_weights, color=&#39;C1&#39;)
sns.histplot(bayesian_weights2, color=&#39;C2&#39;).set(title=&#39;Comparing Dirichlet Weights&#39;);
plt.legend([r&#39;$\alpha = 1$&#39;, r&#39;$\alpha = 4$&#39;]);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/bayesian_bootstrap_19_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;The new distribution is much less skewed and more concentrated around the average value of 1.&lt;/p&gt;
&lt;h2 id=&#34;examples&#34;&gt;Examples&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s have a look at a couple of examples, where we compare both inference procedures.&lt;/p&gt;
&lt;h3 id=&#34;mean-of-a-skewed-distribution&#34;&gt;Mean of a Skewed Distribution&lt;/h3&gt;
&lt;p&gt;First, let&amp;rsquo;s have a look at one of the simplest and most common estimators: the &lt;strong&gt;sample mean&lt;/strong&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;np.random.seed(2)
X = pd.Series(np.random.pareto(2, 100))
sns.histplot(X).set(title=&#39;Sample from Pareto Distribution&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/bayesian_bootstrap_25_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def classic_boot(df, estimator, seed=1):
    df_boot = df.sample(n=len(df), replace=True, random_state=seed)
    estimate = estimator(df_boot)
    return estimate
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;classic_boot(X, np.mean)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;0.7079805545831946
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def bayes_boot(df, estimator, seed=1):
    np.random.seed(seed)
    w = np.random.dirichlet(np.ones(len(df)), 1)[0]
    result = estimator(df, weights=w)
    return result
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;bayes_boot(X, np.average)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;1.0378495251293498
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from joblib import Parallel, delayed

def bootstrap(boot_method, df, estimator, K):
    r = Parallel(n_jobs=8)(delayed(boot_method)(df, estimator, seed=i) for i in range(K))
    return r
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def compare_boot(df, boot1, boot2, estimator, title, K=1000):
    s1 = bootstrap(boot1, df, estimator, K)
    s2 = bootstrap(boot2, df, estimator, K)
    df = pd.DataFrame({&#39;Estimate&#39;: s1 + s2,
                       &#39;Estimator&#39;: [&#39;Classic&#39;]*K + [&#39;Bayes&#39;]*K})
    sns.histplot(data=df, x=&#39;Estimate&#39;, hue=&#39;Estimator&#39;)
    plt.legend([f&#39;Bayes:   {np.mean(s2):.2f} ({np.std(s2):.2f})&#39;,
                f&#39;Classic: {np.mean(s1):.2f} ({np.std(s1):.2f})&#39;])
    plt.title(f&#39;Bootstrap Estimates of {title}&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;compare_boot(X, classic_boot, bayes_boot, np.average, &#39;Sample Mean&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/bayesian_bootstrap_32_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;In this setting, both procedures give a very similar answer.&lt;/p&gt;
&lt;p&gt;Which one is faster?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import time

def compare_time(df, boot1, boot2, estimator, K=1000):
    t1, t2 = np.zeros(K), np.zeros(K)
    for k in range(K):
        
        # Classic bootstrap
        start = time.time()
        boot1(df, estimator)
        t1[k] = time.time() - start
    
        # Bayesian bootstrap
        start = time.time()
        boot2(df, estimator)
        t2[k] = time.time() - start
    
    print(f&amp;quot;Bayes wins {np.mean(t1 &amp;gt; t2)*100}% of the time (by {np.mean((t1 - t2)/t1*100):.2f}%)&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;compare_time(X, classic_boot, bayes_boot, np.average)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Bayes wins 99.8% of the time (by 82.89%)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The Bayesian bootstrap is faster than the classical bootstrap 100% of the simulations, and by an impressive 83%!&lt;/p&gt;
&lt;h3 id=&#34;no-weighting-no-problem&#34;&gt;No Weighting? No Problem&lt;/h3&gt;
&lt;p&gt;What if we have an estimator that does not accept weights, such as the median? We can do &lt;strong&gt;two-level sampling&lt;/strong&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def twolv_boot(df, estimator, seed=1):
    np.random.seed(seed)
    w = np.random.dirichlet(np.ones(len(df))*4, 1)[0]
    df_boot = df.sample(n=len(df)*10, replace=True, weights=w, random_state=seed)
    result = estimator(df_boot)
    return result
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;np.random.seed(1)
X = pd.Series(np.random.normal(0, 10, 1000))
compare_boot(X, classic_boot, twolv_boot, np.median, &#39;Sample Median&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/bayesian_bootstrap_40_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;In this setting, the Bayesian Bootstrap is also &lt;strong&gt;more precise&lt;/strong&gt; than the classical bootstrap.&lt;/p&gt;
&lt;h3 id=&#34;logistic-regression-with-rare-outcome&#34;&gt;Logistic Regression with Rare Outcome&lt;/h3&gt;
&lt;p&gt;Let&amp;rsquo;s now explore the first of two settings in which the classical bootstrap might fall into &lt;strong&gt;corner cases&lt;/strong&gt;. Suppose we observed a feature $x$, normally distributed, and a binary outcome $y$. We are interested in the relationship between the two variables.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;N = 100
np.random.seed(1)
x = np.random.normal(0, 1, N)
y = np.rint(np.random.normal(x, 1, N) &amp;gt; 2)
df = pd.DataFrame({&#39;x&#39;: x, &#39;y&#39;: y})
df.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;x&lt;/th&gt;
      &lt;th&gt;y&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;1.624345&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;-0.611756&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;-0.528172&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;-1.072969&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;0.865408&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;In this case, we observe a positive outcome only in 10 observations out of 100.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;np.sum(df[&#39;y&#39;])
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;10.0
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Since the outcome is binary, we fit a &lt;a href=&#34;https://en.wikipedia.org/wiki/Logistic_regression&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;logistic regression&lt;/strong&gt;&lt;/a&gt; model.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;smf.logit(&#39;y ~ x&#39;, data=df).fit(disp=False).summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
      &lt;td&gt;&lt;/td&gt;         &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;z&lt;/th&gt;      &lt;th&gt;P&gt;|z|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt; &lt;td&gt;   -4.0955&lt;/td&gt; &lt;td&gt;    0.887&lt;/td&gt; &lt;td&gt;   -4.618&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;   -5.834&lt;/td&gt; &lt;td&gt;   -2.357&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;x&lt;/th&gt;         &lt;td&gt;    2.7664&lt;/td&gt; &lt;td&gt;    0.752&lt;/td&gt; &lt;td&gt;    3.677&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    1.292&lt;/td&gt; &lt;td&gt;    4.241&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;Can we bootstrap the distribution of our estimator? Let&amp;rsquo;s try to compute the logistic regression coefficient over 1000 bootstrap samples.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;estimate_logit = lambda df: smf.logit(&#39;y ~ x&#39;, data=df).fit(disp=False).params[1]
for i in range(1000):
    try:
        classic_boot(df, estimate_logit, seed=i)
    except Exception as e:
        print(f&#39;Error for bootstrap number {i}: {e}&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Error for bootstrap number 92: Perfect separation detected, results not available
Error for bootstrap number 521: Perfect separation detected, results not available
Error for bootstrap number 545: Perfect separation detected, results not available
Error for bootstrap number 721: Perfect separation detected, results not available
Error for bootstrap number 835: Perfect separation detected, results not available
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For 5 samples out of 1000, we are &lt;strong&gt;unable&lt;/strong&gt; to compute the estimate. This would not have happened with then bayesian bootstrap.&lt;/p&gt;
&lt;p&gt;This might seem like an innocuous issue in this case: we can just drop those observations. Let&amp;rsquo;s conclude with a much more dangerous example.&lt;/p&gt;
&lt;p&gt;Suppose we observed a binary feature $x$ and a continuous outcome $y$. We are again interested in the relationship between the two variables.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;N = 100
np.random.seed(1)
x = np.random.binomial(1, 5/N, N)
y = np.random.normal(1 + 2*x, 1, N)
df = pd.DataFrame({&#39;x&#39;: x, &#39;y&#39;: y})
df.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;x&lt;/th&gt;
      &lt;th&gt;y&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1.315635&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;-1.022201&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0.693796&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1.827975&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1.230095&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;Let&amp;rsquo;s compare the two bootstrap estimators of the regression coefficient of $y$ on $x$.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;estimate_beta = lambda df, **kwargs: smf.wls(&#39;y ~ x&#39;, data=df, **kwargs).fit().params[1]
compare_boot(df, classic_boot, bayes_boot, estimate_beta, &#39;beta&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/bayesian_bootstrap_55_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;The classic bootstrap procedure estimates a 50% larger variance of our estimator. Why? If we look more closely, we seen that in almost 20 re-samples, we get a very unusual estimate of zero!&lt;/p&gt;
&lt;p&gt;The problem is that in some samples we might not have have &lt;strong&gt;any observations&lt;/strong&gt; with $x=1$. Therefore, in these re-samples, the estimated coefficient is zero. This does not happen with the Bayesian bootstrap, since it does not drop any observation.&lt;/p&gt;
&lt;p&gt;The problematic part here is that we are not getting any error message or warning. This bias is very sneaky and could easily go &lt;strong&gt;unnoticed&lt;/strong&gt;!&lt;/p&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;The article was inspired by the following tweet by Brown University professor &lt;a href=&#34;https://sites.google.com/site/aboutpeterhull/home&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Peter Hull&lt;/a&gt;&lt;/p&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;Ok, so I come bearing good news for ~93% of you: esp. those bootstraping complex models (e.g. w/many FEs)&lt;br&gt;&lt;br&gt;Instead of resampling, which can be seen as reweighting by a random integer W that may be zero, you can reweight by a random non-zero non-integer W &lt;a href=&#34;https://t.co/Rpm1GmomHg&#34;&gt;https://t.co/Rpm1GmomHg&lt;/a&gt;&lt;/p&gt;&amp;mdash; Peter Hull (@instrumenthull) &lt;a href=&#34;https://twitter.com/instrumenthull/status/1487469316010389516?ref_src=twsrc%5Etfw&#34;&gt;January 29, 2022&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;

&lt;p&gt;Indeed, besides being a simple and intuitive procedure, the Bayesian Bootstrap is not part of the standard econometrics curriculum in economic graduate schools.&lt;/p&gt;
&lt;h3 id=&#34;references&#34;&gt;References&lt;/h3&gt;
&lt;p&gt;[1] B. Efron &lt;a href=&#34;https://www.jstor.org/stable/2958830&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Bootstrap Methods: Another Look at the Jackknife&lt;/a&gt; (1979), &lt;em&gt;The Annals of Statistics&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;[2] D. Rubin, &lt;a href=&#34;https://www.jstor.org/stable/2240875&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The Bayesian Bootstrap&lt;/a&gt; (1981), &lt;em&gt;The Annals of Statistics&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;[3] A. Lo, &lt;a href=&#34;https://www.jstor.org/stable/2241087&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;A Large Sample Study of the Bayesian Bootstrap&lt;/a&gt; (1987), &lt;em&gt;The Annals of Statistics&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;[4] J. Shao, D. Tu, &lt;a href=&#34;https://link.springer.com/book/10.1007/978-1-4612-0795-5&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Jacknife and Bootstrap&lt;/a&gt; (1995), &lt;em&gt;Springer&lt;/em&gt;.&lt;/p&gt;
&lt;h3 id=&#34;code&#34;&gt;Code&lt;/h3&gt;
&lt;p&gt;You can find the original Jupyter Notebook here:&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/bayes_boot.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/bayes_boot.ipynb&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>The FWL Theorem, Or How To Make All Regressions Intuitive</title>
      <link>https://matteocourthoud.github.io/post/fwl_theorem/</link>
      <pubDate>Mon, 10 Jul 2023 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/post/fwl_theorem/</guid>
      <description>&lt;p&gt;&lt;em&gt;An introduction to the Frisch-Waugh-Lowell theorem and how to use it to gain intuition in linear regressions&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;The Frisch-Waugh-Lowell theorem is a &lt;strong&gt;simple&lt;/strong&gt; but yet &lt;strong&gt;powerful&lt;/strong&gt; theorem that allows us to reduce multivariate regressions to &lt;strong&gt;univariate&lt;/strong&gt; ones. This is extremely useful when we are interested in the relationship between two variables, but we still need to control for other factors, as it is often the case in &lt;strong&gt;causal inference&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;In this blog post, I am going to introduce the Frisch-Waugh-Lowell theorem and illustrate some interesting applications.&lt;/p&gt;
&lt;h2 id=&#34;the-theorem&#34;&gt;The Theorem&lt;/h2&gt;
&lt;p&gt;The theorem was first published by &lt;a href=&#34;https://www.jstor.org/stable/1907330&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Ragnar Frisch and Frederick Waugh in 1933&lt;/a&gt;. However, since its proof was lengthy and cumbersome, &lt;a href=&#34;https://www.tandfonline.com/doi/abs/10.1080/01621459.1963.10480682&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Michael Lovell in 1963&lt;/a&gt; provided a very simple and intuitive proof and his name was added to the theorem name.&lt;/p&gt;
&lt;p&gt;The theorem states that, when estimating a model of the form&lt;/p&gt;
&lt;p&gt;$$
y_i = \beta_1 x_{i,1} + \beta_2 x_{i,2} + \varepsilon_i
$$&lt;/p&gt;
&lt;p&gt;then, the following estimators of $\beta_1$ are equivalent:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the OLS estimator obtained by regressing $y$ on $x_1$ and $x_2$&lt;/li&gt;
&lt;li&gt;the OLS estimator obtained by regressing $y$ on $\tilde x_1$
&lt;ul&gt;
&lt;li&gt;where $\tilde x_1$ is the residual from the regression of $x_1$ on $x_2$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;the OLS estimator obtained by regressing $\tilde y$ on $\tilde x_1$
&lt;ul&gt;
&lt;li&gt;where $\tilde y$ is the residual from the regression of $y$ on $x_2$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;interpretation&#34;&gt;Interpretation&lt;/h3&gt;
&lt;p&gt;What did we actually &lt;strong&gt;learn&lt;/strong&gt;?&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;Frisch-Waugh-Lowell theorem&lt;/strong&gt; is telling us that there are multiple ways to estimate a single regression coefficient. One possibility is to run the full regression of $y$ on $x$, as usual.&lt;/p&gt;
&lt;p&gt;However, we can also regress $x_1$ on $x_2$, take the residuals, and regress $y$ only those residuals. The first part of this process is sometimes referred to as &lt;strong&gt;partialling-out&lt;/strong&gt; (or &lt;em&gt;orthogonalization&lt;/em&gt;, or &lt;em&gt;residualization&lt;/em&gt;) of $x_1$ with respect to $x_2$. The idea is that we are isolating the variation in $x_1$ that is &lt;em&gt;orthogonal&lt;/em&gt; to $x_2$. Note that $x_2$ can be also be multi-dimensional (i.e. include multiple variables and not just one).&lt;/p&gt;
&lt;p&gt;Why would one ever do that?&lt;/p&gt;
&lt;p&gt;This seems like a way more &lt;strong&gt;complicated&lt;/strong&gt; procedure. Instead of simply doing the regression in 1 step, now we need to do 2 or even 3 steps. It&amp;rsquo;s not intuitive at all. The main advantage comes from the fact that we have reduced a multivariate regression to a univariate one, making more tractable and more intuitive.&lt;/p&gt;
&lt;p&gt;We will later explore more in detail three &lt;strong&gt;applications&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;data visualization&lt;/li&gt;
&lt;li&gt;computational speed&lt;/li&gt;
&lt;li&gt;further applications for inference&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;However, let&amp;rsquo;s first explore the theorem more in detail with an example.&lt;/p&gt;
&lt;h2 id=&#34;example&#34;&gt;Example&lt;/h2&gt;
&lt;p&gt;Suppose we were a retail chain, owning many different stores in different locations. We come up with a brilliant &lt;strong&gt;idea to increase sales&lt;/strong&gt;: give away discounts in the form of &lt;strong&gt;coupons&lt;/strong&gt;. We print a lot of coupons and we distribute them around.&lt;/p&gt;
&lt;p&gt;To understand whether our marketing strategy worked, in each store, we check the average daily &lt;code&gt;sales&lt;/code&gt; and which percentage of shoppers used a &lt;code&gt;coupon&lt;/code&gt;. However, there is one &lt;strong&gt;problem&lt;/strong&gt;: we are worried that higher income people are less likely to use the discount, but usually they spend more. To be safe, we also record the average &lt;code&gt;income&lt;/code&gt; in the neighborhood of each store.&lt;/p&gt;
&lt;p&gt;We can represent the data generating process with a &lt;strong&gt;Directed Acyclic Graph&lt;/strong&gt; (DAG). If you are not familiar with DAGs, I have written a short introduction to &lt;a href=&#34;https://medium.com/towards-data-science/controls-b63dc69e3d8c&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Directed Acyclic Graphs here&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-mermaid&#34;&gt;flowchart LR
classDef included fill:#DCDCDC,stroke:#000000,stroke-width:2px;
classDef excluded fill:#ffffff,stroke:#000000,stroke-width:2px;
classDef unobserved fill:#ffffff,stroke:#000000,stroke-width:2px,stroke-dasharray: 5 5;

X1((coupons))
X2((income))
X3((weekday))
Y((sales))


X1 --&amp;gt; Y
X2 --&amp;gt; X1
X2 --&amp;gt; Y
X3 --&amp;gt; Y

class X1,X2,X3,Y excluded;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s load and inspect the &lt;strong&gt;data&lt;/strong&gt;. I import the data generating process from &lt;code&gt;src.dgp&lt;/code&gt; and some plotting functions and libraries from &lt;code&gt;src.utils&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;%matplotlib inline
%config InlineBackend.figure_format = &#39;retina&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from src.utils import *
from src.dgp import dgp_store_coupons

df = dgp_store_coupons().generate_data(N=50)
df.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;sales&lt;/th&gt;
      &lt;th&gt;coupons&lt;/th&gt;
      &lt;th&gt;income&lt;/th&gt;
      &lt;th&gt;dayofweek&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;821.7&lt;/td&gt;
      &lt;td&gt;0.199&lt;/td&gt;
      &lt;td&gt;66.243&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;602.3&lt;/td&gt;
      &lt;td&gt;0.245&lt;/td&gt;
      &lt;td&gt;43.882&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;655.1&lt;/td&gt;
      &lt;td&gt;0.162&lt;/td&gt;
      &lt;td&gt;44.718&lt;/td&gt;
      &lt;td&gt;5&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;625.8&lt;/td&gt;
      &lt;td&gt;0.269&lt;/td&gt;
      &lt;td&gt;39.270&lt;/td&gt;
      &lt;td&gt;4&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;696.6&lt;/td&gt;
      &lt;td&gt;0.186&lt;/td&gt;
      &lt;td&gt;58.654&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;We have information on &lt;strong&gt;50 stores&lt;/strong&gt;, for which we observe the percentage of customers that use &lt;code&gt;coupons&lt;/code&gt;, daily &lt;code&gt;sales&lt;/code&gt; (in thousand $), average &lt;code&gt;income&lt;/code&gt; of the neighborhood (in thousand $), and &lt;code&gt;day of the week&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Suppose we were directly regressing &lt;code&gt;sales&lt;/code&gt; on &lt;code&gt;coupon&lt;/code&gt; usage. What would we get? I represent the &lt;strong&gt;result&lt;/strong&gt; of the regression graphically, using &lt;code&gt;seaborn&lt;/code&gt; &lt;code&gt;regplot&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sns.regplot(x=&amp;quot;coupons&amp;quot;, y=&amp;quot;sales&amp;quot;, data=df, ci=False, line_kws={&#39;color&#39;:&#39;r&#39;, &#39;label&#39;:&#39;linear fit&#39;})
plt.legend()
plt.title(f&amp;quot;Sales and coupon usage&amp;quot;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/fwl_theorem_12_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;It looks like coupons were a &lt;strong&gt;bad idea&lt;/strong&gt;: in stores where coupons are used more, we observe lower sales.&lt;/p&gt;
&lt;p&gt;However, it might just be that people with higher income are using less coupons, while also spending more. If this was true, it could &lt;strong&gt;bias&lt;/strong&gt; our results. In terms of the DAG, it means that we have a &lt;strong&gt;backdoor path&lt;/strong&gt; passing through &lt;code&gt;income&lt;/code&gt;, generating a non-causal relationship.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-mermaid&#34;&gt;flowchart LR
classDef included fill:#DCDCDC,stroke:#000000,stroke-width:2px;
classDef excluded fill:#ffffff,stroke:#000000,stroke-width:2px;
classDef unobserved fill:#ffffff,stroke:#000000,stroke-width:2px,stroke-dasharray: 5 5;

X1((coupons))
X2((income))
X3((weekday))
Y((sales))


X1 --&amp;gt; Y
X2 --&amp;gt; X1
X2 --&amp;gt; Y
X3 --&amp;gt; Y

class X1,Y included;
class X2,X3 excluded;

linkStyle 1,2 stroke:#ff0000,stroke-width:4px;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In order to recover the causal effect of &lt;code&gt;coupons&lt;/code&gt; on &lt;code&gt;sales&lt;/code&gt; we need to &lt;strong&gt;condition&lt;/strong&gt; our analysis on &lt;code&gt;income&lt;/code&gt;. This will &lt;strong&gt;block&lt;/strong&gt; the non-causal path passing through &lt;code&gt;income&lt;/code&gt;, leaving only the direct path from &lt;code&gt;coupons&lt;/code&gt; to &lt;code&gt;sales&lt;/code&gt; open, allowing us to estimate the causal effect.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-mermaid&#34;&gt;flowchart LR
classDef included fill:#DCDCDC,stroke:#000000,stroke-width:2px;
classDef excluded fill:#ffffff,stroke:#000000,stroke-width:2px;
classDef unobserved fill:#ffffff,stroke:#000000,stroke-width:2px,stroke-dasharray: 5 5;


X1((coupons))
X2((income))
X3((weekday))
Y((sales))


X1 --&amp;gt; Y
X2 -.-&amp;gt; X1
X2 -.-&amp;gt; Y
X3 --&amp;gt; Y

class X1,X2,Y included;
class X3 excluded;

linkStyle 0 stroke:#00ff00,stroke-width:4px;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s implement this, by including &lt;code&gt;income&lt;/code&gt; in the regression.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;smf.ols(&#39;sales ~ coupons + income&#39;, df).fit().summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
      &lt;td&gt;&lt;/td&gt;         &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt; &lt;td&gt;  161.4982&lt;/td&gt; &lt;td&gt;   33.253&lt;/td&gt; &lt;td&gt;    4.857&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;   94.601&lt;/td&gt; &lt;td&gt;  228.395&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;coupons&lt;/th&gt;   &lt;td&gt;  218.7548&lt;/td&gt; &lt;td&gt;   50.058&lt;/td&gt; &lt;td&gt;    4.370&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;  118.052&lt;/td&gt; &lt;td&gt;  319.458&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;income&lt;/th&gt;    &lt;td&gt;    9.5094&lt;/td&gt; &lt;td&gt;    0.480&lt;/td&gt; &lt;td&gt;   19.818&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    8.544&lt;/td&gt; &lt;td&gt;   10.475&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;Now the estimated effect of &lt;code&gt;coupons&lt;/code&gt; on &lt;code&gt;sales&lt;/code&gt; is positive and significant. Coupons were actually a &lt;strong&gt;good idea&lt;/strong&gt; after all.&lt;/p&gt;
&lt;h3 id=&#34;verifying-the-theorem&#34;&gt;Verifying the Theorem&lt;/h3&gt;
&lt;p&gt;Let&amp;rsquo;s now verify that the Frisch-Waugh-Lowell theorem actually holds. In particular, we want to check whether we get the &lt;strong&gt;same coefficient&lt;/strong&gt; if, instead of regressing &lt;code&gt;sales&lt;/code&gt; on &lt;code&gt;coupons&lt;/code&gt; and &lt;code&gt;income&lt;/code&gt;, we were&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;regressing &lt;code&gt;coupons&lt;/code&gt; on &lt;code&gt;income&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;computing the residuals &lt;code&gt;coupons_tilde&lt;/code&gt;, i.e. the variation in &lt;code&gt;coupons&lt;/code&gt; &lt;strong&gt;not&lt;/strong&gt; explained by &lt;code&gt;income&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;regressing &lt;code&gt;sales&lt;/code&gt; on &lt;code&gt;coupons_tilde&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df[&#39;coupons_tilde&#39;] = smf.ols(&#39;coupons ~ income&#39;, df).fit().resid

smf.ols(&#39;sales ~ coupons_tilde - 1&#39;, df).fit().summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
        &lt;td&gt;&lt;/td&gt;           &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;coupons_tilde&lt;/th&gt; &lt;td&gt;  218.7548&lt;/td&gt; &lt;td&gt; 1275.236&lt;/td&gt; &lt;td&gt;    0.172&lt;/td&gt; &lt;td&gt; 0.865&lt;/td&gt; &lt;td&gt;-2343.929&lt;/td&gt; &lt;td&gt; 2781.438&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;Yes, the coefficient is the same! However, the &lt;strong&gt;standard errors&lt;/strong&gt; now have increased a lot and the estimated coefficient is not significantly different from zero anymore.&lt;/p&gt;
&lt;p&gt;A better approach is to add a further step and repeat the same procedure also for &lt;code&gt;sales&lt;/code&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;regressing &lt;code&gt;sales&lt;/code&gt; on &lt;code&gt;income&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;computing the residuals &lt;code&gt;sales_tilde&lt;/code&gt;, i.e. the variation in &lt;code&gt;sales&lt;/code&gt; &lt;strong&gt;not&lt;/strong&gt; explained by &lt;code&gt;income&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;and finally regress &lt;code&gt;sales_tilde&lt;/code&gt; on &lt;code&gt;coupons_tilde&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df[&#39;sales_tilde&#39;] = smf.ols(&#39;sales ~ income&#39;, df).fit().resid

smf.ols(&#39;sales_tilde ~ coupons_tilde - 1&#39;, df).fit().summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
        &lt;td&gt;&lt;/td&gt;           &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;coupons_tilde&lt;/th&gt; &lt;td&gt;  218.7548&lt;/td&gt; &lt;td&gt;   49.025&lt;/td&gt; &lt;td&gt;    4.462&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;  120.235&lt;/td&gt; &lt;td&gt;  317.275&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;The coefficient is still exactly the same, but now also the standard errors are almost identical.&lt;/p&gt;
&lt;h3 id=&#34;projection&#34;&gt;Projection&lt;/h3&gt;
&lt;p&gt;What is &lt;strong&gt;partialling-out&lt;/strong&gt; (or residualization, or orthogonalization) actually doing? What is happening when we take the residuals of &lt;code&gt;coupons&lt;/code&gt; with respect to &lt;code&gt;income&lt;/code&gt;?&lt;/p&gt;
&lt;p&gt;We can &lt;strong&gt;visualize&lt;/strong&gt; the procedure in a plot. First, let&amp;rsquo;s actually display the &lt;strong&gt;residuals&lt;/strong&gt; of &lt;code&gt;coupons&lt;/code&gt; with respect to income.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df[&amp;quot;coupons_hat&amp;quot;] = smf.ols(&#39;coupons ~ income&#39;, df).fit().predict()
ax = sns.regplot(x=&amp;quot;income&amp;quot;, y=&amp;quot;coupons&amp;quot;, data=df, ci=False, line_kws={&#39;color&#39;:&#39;r&#39;, &#39;label&#39;:&#39;linear fit&#39;})
ax.vlines(df[&amp;quot;income&amp;quot;], np.minimum(df[&amp;quot;coupons&amp;quot;], df[&amp;quot;coupons_hat&amp;quot;]), np.maximum(df[&amp;quot;coupons&amp;quot;], df[&amp;quot;coupons_hat&amp;quot;]), 
           linestyle=&#39;--&#39;, color=&#39;k&#39;, alpha=0.5, linewidth=1, label=&amp;quot;residuals&amp;quot;);
plt.legend()
plt.title(f&amp;quot;Coupons usage, income and residuals&amp;quot;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/fwl_theorem_24_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;residuals&lt;/strong&gt; are the vertical dotted lines between the data and the linear fit, i.e. the part of the variation in &lt;code&gt;coupons&lt;/code&gt; unexplained by &lt;code&gt;income&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;By &lt;strong&gt;partialling-out&lt;/strong&gt;, we are removing the linear fit from the data and keeping only the residuals. We can visualize this procedure with a gif. I import the code from the &lt;code&gt;src.figures&lt;/code&gt; file that you can find &lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/src/figures.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from src.figures import gif_projection

gif_projection(x=&#39;income&#39;, y=&#39;coupons&#39;, df=df, gifname=&amp;quot;gifs/fwl.gif&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;gifs/fwl.gif&#34; alt=&#34;fwl&#34;&gt;&lt;/p&gt;
&lt;p&gt;The original distribution of the data is on the left in &lt;em&gt;blue&lt;/em&gt;, the partialled-out data in on the right in &lt;em&gt;green&lt;/em&gt;. As we can see, partialling-out removes both the level and the trend in &lt;code&gt;coupons&lt;/code&gt; that is explained by &lt;code&gt;income&lt;/code&gt;.&lt;/p&gt;
&lt;h3 id=&#34;multiple-controls&#34;&gt;Multiple Controls&lt;/h3&gt;
&lt;p&gt;We can use the Frisch-Waugh-Theorem also when we have &lt;strong&gt;multiple control variables&lt;/strong&gt;. Suppose that we wanted to also include &lt;code&gt;day of the week&lt;/code&gt; in the regression, to increase precision.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;smf.ols(&#39;sales ~ coupons + income + dayofweek&#39;, df).fit().summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
         &lt;td&gt;&lt;/td&gt;           &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt;      &lt;td&gt;  124.2721&lt;/td&gt; &lt;td&gt;   28.764&lt;/td&gt; &lt;td&gt;    4.320&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;   66.182&lt;/td&gt; &lt;td&gt;  182.362&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;dayofweek[T.2]&lt;/th&gt; &lt;td&gt;    7.7703&lt;/td&gt; &lt;td&gt;   14.607&lt;/td&gt; &lt;td&gt;    0.532&lt;/td&gt; &lt;td&gt; 0.598&lt;/td&gt; &lt;td&gt;  -21.729&lt;/td&gt; &lt;td&gt;   37.270&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;dayofweek[T.3]&lt;/th&gt; &lt;td&gt;   15.0895&lt;/td&gt; &lt;td&gt;   11.678&lt;/td&gt; &lt;td&gt;    1.292&lt;/td&gt; &lt;td&gt; 0.204&lt;/td&gt; &lt;td&gt;   -8.495&lt;/td&gt; &lt;td&gt;   38.674&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;dayofweek[T.4]&lt;/th&gt; &lt;td&gt;   28.2762&lt;/td&gt; &lt;td&gt;    9.868&lt;/td&gt; &lt;td&gt;    2.866&lt;/td&gt; &lt;td&gt; 0.007&lt;/td&gt; &lt;td&gt;    8.348&lt;/td&gt; &lt;td&gt;   48.204&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;dayofweek[T.5]&lt;/th&gt; &lt;td&gt;   44.0937&lt;/td&gt; &lt;td&gt;   10.214&lt;/td&gt; &lt;td&gt;    4.317&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;   23.467&lt;/td&gt; &lt;td&gt;   64.720&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;dayofweek[T.6]&lt;/th&gt; &lt;td&gt;   50.7664&lt;/td&gt; &lt;td&gt;   13.130&lt;/td&gt; &lt;td&gt;    3.866&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;   24.249&lt;/td&gt; &lt;td&gt;   77.283&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;dayofweek[T.7]&lt;/th&gt; &lt;td&gt;   57.3142&lt;/td&gt; &lt;td&gt;   12.413&lt;/td&gt; &lt;td&gt;    4.617&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;   32.245&lt;/td&gt; &lt;td&gt;   82.383&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;coupons&lt;/th&gt;        &lt;td&gt;  192.0262&lt;/td&gt; &lt;td&gt;   39.140&lt;/td&gt; &lt;td&gt;    4.906&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;  112.981&lt;/td&gt; &lt;td&gt;  271.071&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;income&lt;/th&gt;         &lt;td&gt;    9.8152&lt;/td&gt; &lt;td&gt;    0.404&lt;/td&gt; &lt;td&gt;   24.314&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    9.000&lt;/td&gt; &lt;td&gt;   10.630&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;We can perform the same procedure as before, but instead of &lt;strong&gt;partialling-out&lt;/strong&gt; only &lt;code&gt;income&lt;/code&gt;, now we partial out both &lt;code&gt;income&lt;/code&gt; and &lt;code&gt;day of the week&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df[&#39;coupons_tilde&#39;] = smf.ols(&#39;coupons ~ income + dayofweek&#39;, df).fit().resid
df[&#39;sales_tilde&#39;] = smf.ols(&#39;sales ~ income + dayofweek&#39;, df).fit().resid
smf.ols(&#39;sales_tilde ~ coupons_tilde - 1&#39;, df).fit().summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
        &lt;td&gt;&lt;/td&gt;           &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;coupons_tilde&lt;/th&gt; &lt;td&gt;  192.0262&lt;/td&gt; &lt;td&gt;   35.803&lt;/td&gt; &lt;td&gt;    5.363&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;  120.078&lt;/td&gt; &lt;td&gt;  263.974&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;We still get exactly the same coefficient!&lt;/p&gt;
&lt;h2 id=&#34;applications&#34;&gt;Applications&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s now inspect some useful applications of the FWL theorem.&lt;/p&gt;
&lt;h3 id=&#34;data-visualization&#34;&gt;Data Visualization&lt;/h3&gt;
&lt;p&gt;One of the advantages of the Frisch-Waugh-Theorem is that it allows us to estimate the coefficient of interest from a &lt;strong&gt;univariate&lt;/strong&gt; regression, i.e. with a single explanatory variable (or feature).&lt;/p&gt;
&lt;p&gt;Therefore, we can now represent the relationship of interest &lt;strong&gt;graphically&lt;/strong&gt;. Let&amp;rsquo;s plot the residual &lt;code&gt;sales&lt;/code&gt; against the residual &lt;code&gt;coupons&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sns.regplot(x=&amp;quot;coupons_tilde&amp;quot;, y=&amp;quot;sales_tilde&amp;quot;, data=df, ci=False, line_kws={&#39;color&#39;:&#39;r&#39;, &#39;label&#39;:&#39;linear fit&#39;})
plt.legend()
plt.title(f&amp;quot;Residual sales and residual coupons&amp;quot;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/fwl_theorem_39_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Now it&amp;rsquo;s evident from the graph that the &lt;strong&gt;conditional relationship&lt;/strong&gt; between &lt;code&gt;sales&lt;/code&gt; and &lt;code&gt;coupons&lt;/code&gt; is positive.&lt;/p&gt;
&lt;p&gt;One problem with this approach is that the variables are &lt;strong&gt;hard to interpret&lt;/strong&gt;: we now have negative values for both &lt;code&gt;sales&lt;/code&gt; and &lt;code&gt;coupons&lt;/code&gt;. Weird.&lt;/p&gt;
&lt;p&gt;Why did it happen? It happened because when we partialled-out the variables, we included the &lt;strong&gt;intercept&lt;/strong&gt; in the regression, effectively de-meaning the variables (i.e. normalizing their values so that their mean is zero).&lt;/p&gt;
&lt;p&gt;We can &lt;strong&gt;solve&lt;/strong&gt; this problem by &lt;strong&gt;scaling&lt;/strong&gt; both variables, adding their mean.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df[&#39;coupons_tilde_scaled&#39;] = df[&#39;coupons_tilde&#39;] + np.mean(df[&#39;coupons&#39;])
df[&#39;sales_tilde_scaled&#39;] = df[&#39;sales_tilde&#39;] + np.mean(df[&#39;sales&#39;])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now the magnitudes of the two variables are interpretable again.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sns.regplot(x=&amp;quot;coupons_tilde_scaled&amp;quot;, y=&amp;quot;sales_tilde_scaled&amp;quot;, data=df, ci=False, line_kws={&#39;color&#39;:&#39;r&#39;, &#39;label&#39;:&#39;linear fit&#39;})
plt.legend()
plt.title(f&amp;quot;Residual sales scaled and residual coupons scaled&amp;quot;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/fwl_theorem_43_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Is this a &lt;strong&gt;valid&lt;/strong&gt; approach or did it alter our estimates? We can can check it by running the regression with the scaled partialled-out variables.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;smf.ols(&#39;sales_tilde_scaled ~ coupons_tilde_scaled&#39;, df).fit().summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
            &lt;td&gt;&lt;/td&gt;              &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt;            &lt;td&gt;  641.6486&lt;/td&gt; &lt;td&gt;   10.017&lt;/td&gt; &lt;td&gt;   64.054&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;  621.507&lt;/td&gt; &lt;td&gt;  661.790&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;coupons_tilde_scaled&lt;/th&gt; &lt;td&gt;  192.0262&lt;/td&gt; &lt;td&gt;   36.174&lt;/td&gt; &lt;td&gt;    5.308&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;  119.294&lt;/td&gt; &lt;td&gt;  264.758&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;The coefficient is exactly the same as before!&lt;/p&gt;
&lt;h3 id=&#34;computational-speed&#34;&gt;Computational Speed&lt;/h3&gt;
&lt;p&gt;Another application of the Frisch-Waugh-Lovell theorem is to increase the computational speed of linear estimators. For example it is used to compute efficient linear estimators in presence of high-dimensional fixed effects (&lt;code&gt;day of the week&lt;/code&gt; in our example).&lt;/p&gt;
&lt;p&gt;Some packages that exploit the Frisch-Waugh-Lovell theorem include&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://scorreia.com/software/reghdfe/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;reghdfe in Stata&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://pyhdfe.readthedocs.io/en/stable/index.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;pyhdfe in Python&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;However, it&amp;rsquo;s important to also mention the &lt;a href=&#34;https://cran.r-project.org/web/packages/fixest/index.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;fixest&lt;/a&gt; package in R, which is also exceptionally efficient in running regressions with high dimensional fixed effects.&lt;/p&gt;
&lt;h3 id=&#34;inference-and-machine-learning&#34;&gt;Inference and Machine Learning&lt;/h3&gt;
&lt;p&gt;Another important application of the FWL theorem sits at the intersection of &lt;strong&gt;machine learning&lt;/strong&gt; and &lt;strong&gt;causal inference&lt;/strong&gt;. I am referring to the work on post-double selection by &lt;a href=&#34;https://academic.oup.com/restud/article-abstract/81/2/608/1523757&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Belloni, Chernozhukov, Hansen (2013)&lt;/a&gt; and the follow up work on &amp;ldquo;double machine learning&amp;rdquo; by &lt;a href=&#34;https://academic.oup.com/ectj/article/21/1/C1/5056401&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Chernozhukov, Chetverikov, Demirer, Duflo, Hansen, Newey, Robins (2018)&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;I plan to cover both applications in future posts, but I wanted to start with the basics. Stay tuned!&lt;/p&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;p&gt;[1] R. Frisch and F. V. Waugh, &lt;a href=&#34;https://www.jstor.org/stable/1907330&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Partial Time Regressions as Compared with Individual Trends&lt;/a&gt; (1933), &lt;em&gt;Econometrica&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;[2] M. C. Lowell, &lt;a href=&#34;https://www.tandfonline.com/doi/abs/10.1080/01621459.1963.10480682&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Seasonal Adjustment of Economic Time Series and Multiple Regression Analysis&lt;/a&gt; (1963), &lt;em&gt;Journal of the American Statistical Association&lt;/em&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Understanding AIPW</title>
      <link>https://matteocourthoud.github.io/post/aipw/</link>
      <pubDate>Mon, 10 Jul 2023 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/post/aipw/</guid>
      <description>&lt;p&gt;&lt;em&gt;An introduction to doubly-robust estimation of conditional average treatment effects (CATE)&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;When estimating causal effects, the gold standard is &lt;strong&gt;randomized controlled trials or AB tests&lt;/strong&gt;. By randomly exposing units to a treatment we make sure that individuals in both groups are comparable, on average, and any difference we observe can be attributed to the treatment effect alone.&lt;/p&gt;
&lt;p&gt;However, often the treatment and control groups are &lt;strong&gt;not perfectly comparable&lt;/strong&gt;. This could be due to the fact that randomization was not perfect or available. Not always we can randomize a treatment, for ethical or practical reasons. And even when we can, sometimes we do not have enough individuals or units so that differences between groups are seizable. This happens often, for example, when randomization is not done at the individual level, but at a higher level of aggregation, for example zipcodes, counties or even states.&lt;/p&gt;
&lt;p&gt;In a &lt;a href=&#34;https://towardsdatascience.com/99bf5cffa0d9&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;previous post&lt;/a&gt;, I have introduced and compared a series of methods that compute &lt;strong&gt;conditional average treatment effects (CATE)&lt;/strong&gt; from observational or experimental data. Some of these methods require the researcher to specify and estimate the distribution of the outcome of interest, given the treatment and the observable characteristics (e.g. &lt;a href=&#34;https://towardsdatascience.com/8a9c1e340832&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;meta learners&lt;/a&gt;). Other methods require the researcher to specify and estimate the probability of being treated, given the observable characteristics (e.g. &lt;a href=&#34;https://towardsdatascience.com/99bf5cffa0d9&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;IPW&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;In this post, we are going to see a procedure that &lt;strong&gt;combines&lt;/strong&gt; both methods and is &lt;strong&gt;robust&lt;/strong&gt; to misspecification of either method&amp;rsquo;s model: the Augmented Inverse Probability Weighted estimator (AIPW).&lt;/p&gt;
&lt;img src=&#34;fig/fusion.gif&#34; width=&#34;400px&#34;/&gt;
&lt;p&gt;&lt;strong&gt;TLDR;&lt;/strong&gt; AIPW greatly improves both IPW and meta-learners, and you should always use it!&lt;/p&gt;
&lt;h2 id=&#34;example&#34;&gt;Example&lt;/h2&gt;
&lt;p&gt;Assume we had blog on statistics and causal inference 😇. To improve user experience, we are considering &lt;strong&gt;releasing a dark mode&lt;/strong&gt;, and we would like to understand whether this new feature increases the time users spend on our blog.&lt;/p&gt;
&lt;p&gt;This example is borrowed from my last post on the estimation of conditional average treatment effects (CATE). You can find the &lt;a href=&#34;https://towardsdatascience.com/99bf5cffa0d9&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;original post here&lt;/a&gt;. If you remember the setting, you can skip this introduction.&lt;/p&gt;
&lt;img src=&#34;fig/modes.png&#34; width=&#34;600px&#34;/&gt;
&lt;p&gt;We are not a sophisticated company, therefore we do not run an AB test but we simply release the dark mode and we observe whether users select it or not and the time they spend on the blog. We know that there might be &lt;strong&gt;selection&lt;/strong&gt;:  users that prefer the dark mode could have different reading preferences and this might complicate our causal analysis.&lt;/p&gt;
&lt;p&gt;We can represent the data generating process with the following &lt;a href=&#34;https://towardsdatascience.com/b63dc69e3d8c&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Directed Acyclic Graph (DAG)&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-mermaid&#34;&gt;flowchart TB
classDef included fill:#DCDCDC,stroke:#000000,stroke-width:2px;
classDef excluded fill:#ffffff,stroke:#000000,stroke-width:2px;
classDef unobserved fill:#ffffff,stroke:#000000,stroke-width:2px,stroke-dasharray: 5 5;

X1((gender))
X2((age))
X3((hours))
D((dark mode))
Y((read time))

D --&amp;gt; Y
X1 --&amp;gt; Y
X1 --&amp;gt; D
X2 --&amp;gt; D
X3 --&amp;gt; Y

class D,Y included;
class X1,X2,X3 excluded;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We generate the simulated data using the data generating process &lt;code&gt;dgp_darkmode()&lt;/code&gt; from &lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/src/dgp.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;src.dgp&lt;/code&gt;&lt;/a&gt;. I also import some plotting functions and libraries from &lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/src/utils.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;src.utils&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;%matplotlib inline
%config InlineBackend.figure_format = &#39;retina&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from src.utils import *
from src.dgp import dgp_darkmode
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;dgp = dgp_darkmode()
df = dgp.generate_data()
df.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;read_time&lt;/th&gt;
      &lt;th&gt;dark_mode&lt;/th&gt;
      &lt;th&gt;male&lt;/th&gt;
      &lt;th&gt;age&lt;/th&gt;
      &lt;th&gt;hours&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;14.4&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;43.0&lt;/td&gt;
      &lt;td&gt;65.6&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;15.4&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;55.0&lt;/td&gt;
      &lt;td&gt;125.4&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;20.9&lt;/td&gt;
      &lt;td&gt;True&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;23.0&lt;/td&gt;
      &lt;td&gt;642.6&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;20.0&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;41.0&lt;/td&gt;
      &lt;td&gt;129.1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;21.5&lt;/td&gt;
      &lt;td&gt;True&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;29.0&lt;/td&gt;
      &lt;td&gt;190.2&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;We have informations on 300 users for whom we observe whether they select the &lt;code&gt;dark_mode&lt;/code&gt; (the treatment), their weekly &lt;code&gt;read_time&lt;/code&gt; (the outcome of interest) and some characteristics like &lt;code&gt;gender&lt;/code&gt;, &lt;code&gt;age&lt;/code&gt; and total &lt;code&gt;hours&lt;/code&gt; previously spend on the blog.&lt;/p&gt;
&lt;p&gt;We would like to estimate the effect of the new &lt;code&gt;dark_mode&lt;/code&gt; on users&amp;rsquo; &lt;code&gt;read_time&lt;/code&gt;. As a first approach, we might naively compute the effect as a difference in means, assuming that the treatment and control sample are comparable. We can estimate the difference in means by regressing &lt;code&gt;read_time&lt;/code&gt; on &lt;code&gt;dark_mode&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;smf.ols(&amp;quot;read_time ~ dark_mode&amp;quot;, data=df).fit().summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
          &lt;td&gt;&lt;/td&gt;             &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt;         &lt;td&gt;   19.1748&lt;/td&gt; &lt;td&gt;    0.402&lt;/td&gt; &lt;td&gt;   47.661&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;   18.383&lt;/td&gt; &lt;td&gt;   19.967&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;dark_mode[T.True]&lt;/th&gt; &lt;td&gt;   -0.4446&lt;/td&gt; &lt;td&gt;    0.571&lt;/td&gt; &lt;td&gt;   -0.779&lt;/td&gt; &lt;td&gt; 0.437&lt;/td&gt; &lt;td&gt;   -1.568&lt;/td&gt; &lt;td&gt;    0.679&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;Individuals that select the &lt;code&gt;dark_mode&lt;/code&gt; spend on average 0.44 hours less on the blog, per week. Should we conclude that &lt;code&gt;dark_mode&lt;/code&gt; is a &lt;strong&gt;bad idea&lt;/strong&gt;? Is this a causal effect?&lt;/p&gt;
&lt;p&gt;The problem is that we did &lt;strong&gt;not&lt;/strong&gt; run an &lt;a href=&#34;https://de.wikipedia.org/wiki/A/B-Test&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;AB test&lt;/strong&gt;&lt;/a&gt; or randomized control trial, therefore users that selected the &lt;code&gt;dark_mode&lt;/code&gt; might not be directly &lt;strong&gt;comparable&lt;/strong&gt; with users that didn&amp;rsquo;t. Can we verify this concern? Partially. We can only check characteristics that we observe, &lt;code&gt;gender&lt;/code&gt;, &lt;code&gt;age&lt;/code&gt; and total &lt;code&gt;hours&lt;/code&gt; in our setting. We cannot check if users differ along other dimensions that we don&amp;rsquo;t observe.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s use the &lt;code&gt;create_table_one&lt;/code&gt; function from Uber&amp;rsquo;s &lt;a href=&#34;https://causalml.readthedocs.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;causalml&lt;/code&gt;&lt;/a&gt; package to produce a &lt;strong&gt;covariate balance table&lt;/strong&gt;, containing the average value of our observable characteristics, across treatment and control groups. As the name suggests, this should always be the first table you present in causal inference analysis.&lt;/p&gt;
&lt;p&gt;We did not randomize the &lt;code&gt;dark_mode&lt;/code&gt; so that users that selected it might not be directly &lt;strong&gt;comparable&lt;/strong&gt; with users that didn&amp;rsquo;t. Can we verify this concern? Partially. We can only check characteristics that we observe, &lt;code&gt;gender&lt;/code&gt;, &lt;code&gt;age&lt;/code&gt; and total &lt;code&gt;hours&lt;/code&gt; in our setting. We cannot check if users differ along other dimensions that we don&amp;rsquo;t observe.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s use the &lt;code&gt;create_table_one&lt;/code&gt; function from Uber&amp;rsquo;s &lt;a href=&#34;https://causalml.readthedocs.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;causalml&lt;/code&gt;&lt;/a&gt; package to produce a &lt;strong&gt;covariate balance table&lt;/strong&gt;, containing the average value of our observable characteristics, across treatment and control groups. As the name suggests, this should always be the first table you present in causal inference analysis.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from causalml.match import create_table_one

X = [&#39;male&#39;, &#39;age&#39;, &#39;hours&#39;]
table1 = create_table_one(df, &#39;dark_mode&#39;, X)
table1
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;Control&lt;/th&gt;
      &lt;th&gt;Treatment&lt;/th&gt;
      &lt;th&gt;SMD&lt;/th&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;Variable&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;n&lt;/th&gt;
      &lt;td&gt;151&lt;/td&gt;
      &lt;td&gt;149&lt;/td&gt;
      &lt;td&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;age&lt;/th&gt;
      &lt;td&gt;46.01 (9.79)&lt;/td&gt;
      &lt;td&gt;39.09 (11.53)&lt;/td&gt;
      &lt;td&gt;-0.6469&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;hours&lt;/th&gt;
      &lt;td&gt;337.78 (464.00)&lt;/td&gt;
      &lt;td&gt;328.57 (442.12)&lt;/td&gt;
      &lt;td&gt;-0.0203&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;male&lt;/th&gt;
      &lt;td&gt;0.34 (0.47)&lt;/td&gt;
      &lt;td&gt;0.66 (0.48)&lt;/td&gt;
      &lt;td&gt;0.6732&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;There seems to be &lt;strong&gt;some difference&lt;/strong&gt; between treatment (&lt;code&gt;dark_mode&lt;/code&gt;) and control group. In particular, users that select the &lt;code&gt;dark_mode&lt;/code&gt; are older, have spent less hours on the blog and they are more likely to be males.&lt;/p&gt;
&lt;p&gt;What can we do? If we assume that all differences between treatment and control group are &lt;strong&gt;observable&lt;/strong&gt;, we can solve the problem by performing &lt;strong&gt;conditional analysis&lt;/strong&gt;.&lt;/p&gt;
&lt;h2 id=&#34;conditional-analysis&#34;&gt;Conditional Analysis&lt;/h2&gt;
&lt;p&gt;We assume that for a set of subjects $i = 1, &amp;hellip;, n$ we observed a set of variables $(D_i, Y_i, X_i)$ that includes&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;a treatment assignment $D_i \in \lbrace 0, 1 \rbrace$ (&lt;code&gt;dark_mode&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;a response $Y_i \in \mathbb R$ (&lt;code&gt;read_time&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;a feature vector $X_i \in \mathbb R^n$ (&lt;code&gt;gender&lt;/code&gt;, &lt;code&gt;age&lt;/code&gt; and &lt;code&gt;hours&lt;/code&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We are interested in &lt;strong&gt;estimating the conditional average treatment effect (CATE)&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;$$
\tau(x) = \mathbb E \Big[ Y_i^{(1)} - Y_i^{(0)} \ \Big| \ X_i = x \Big]
$$&lt;/p&gt;
&lt;p&gt;Where $Y_i^{(d)}$ indicates the potential outcome of individual $i$ under treatment status $d$. We also make the following assumptions.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Assumption 1 : unconfoundedness&lt;/strong&gt; (or ignorability, or selection on observables)&lt;/p&gt;
&lt;p&gt;$$
\big \lbrace Y_i^{(1)} , Y_i^{(0)} \big \rbrace \ \perp \ D_i \ | \ X_i
$$&lt;/p&gt;
&lt;p&gt;i.e. conditional on observable characteristics $X$, the treatment assignment $D$ is as good as random. What we are effectively assuming is that there is no other characteristics that we do not observe that could impact both whether a user selects the &lt;code&gt;dark_mode&lt;/code&gt; and their &lt;code&gt;read_time&lt;/code&gt;. This is a &lt;strong&gt;strong assumption&lt;/strong&gt; that is more likely to be satisfied the more individual characteristics we observe.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Assumption 2: overlap&lt;/strong&gt; (or common support)&lt;/p&gt;
&lt;p&gt;$$
\exists \eta &amp;gt; 0 \ : \ \eta \leq \mathbb E \left[ D_i = 1 \ \big | \ X_i = x \right] \leq 1-\eta
$$&lt;/p&gt;
&lt;p&gt;i.e. no observation is deterministically assigned to the treatment or control group. This is a more technical assumption that basically means that for any level of &lt;code&gt;gender&lt;/code&gt;, &lt;code&gt;age&lt;/code&gt; or &lt;code&gt;hours&lt;/code&gt;, there could exist an individual that select the &lt;code&gt;dark_mode&lt;/code&gt; and one that doesn&amp;rsquo;t. Differently from the unconfoundedness assumption, the overal assumption is &lt;strong&gt;testable&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Assumption 3: stable unit treatment value (SUTVA)&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;$$
Y^{(d)} \perp D
$$&lt;/p&gt;
&lt;p&gt;i.e. the potential outcome does not depend on the treatment status. In our case, we are ruling out the fact that another user selecting &lt;code&gt;dark_mode&lt;/code&gt; might affect my effect of &lt;code&gt;dark_mode&lt;/code&gt; on &lt;code&gt;read_time&lt;/code&gt;. The most common setting where SUTVA is violated is in presence of &lt;strong&gt;network effects&lt;/strong&gt;: if a friend of mine uses a social network increases my utility from using it.&lt;/p&gt;
&lt;h3 id=&#34;ipw-and-meta-learners&#34;&gt;IPW and Meta-Learners&lt;/h3&gt;
&lt;p&gt;Two alternative ways to perform conditional analysis are&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/99bf5cffa0d9&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;IPW&lt;/strong&gt;&lt;/a&gt;: balance observations by their conditional treatment assignment probability and then estimate the treatment effect as a weighted difference in means&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/8a9c1e340832&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Meta Learners&lt;/strong&gt;&lt;/a&gt;: predict the potential outcomes from observable characteristics and estimate treatment effects as the difference between observed and counterfactual outcomes&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;These two alternative procedures exploit the fact that we observe individual characteristics $X$ in different ways:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;IPW exploits $X$ to predict the treatment assignment $D$ and estimate the &lt;strong&gt;propensity scores&lt;/strong&gt; $e(X) = \mathbb{E} [D | X]$&lt;/li&gt;
&lt;li&gt;Meta Learners exploit $X$ to predict the counterfactual outcomes $Y^{(d)}$ and estimate the &lt;strong&gt;response function&lt;/strong&gt; $\mu(X)^{(d)} = \mathbb{E} [Y | D, X]$&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Can we &lt;strong&gt;combine&lt;/strong&gt; the two procedures and get the best of both worlds?&lt;/p&gt;
&lt;p&gt;Yes, with the &lt;strong&gt;AIPW or double-robust estimator&lt;/strong&gt;.&lt;/p&gt;
&lt;h2 id=&#34;the-aipw-estimator&#34;&gt;The AIPW Estimator&lt;/h2&gt;
&lt;p&gt;The &lt;strong&gt;Augmented Inverse Propensity Weighted&lt;/strong&gt; estimator is given by&lt;/p&gt;
&lt;p&gt;$$
\hat \tau_{AIPW} = \frac{1}{n} \sum_{i=1}^{n} \left( \hat \mu^{(1)}(X_i) - \hat \mu^{(0)}(X_i) + \frac{D_i }{\hat e(X_i)} \left( Y_i - \hat \mu^{(1)}(X_i) \right) - \frac{(1-D_i) }{1-\hat e(X_i)} \left( Y_i - \hat \mu^{(0)}(X_i) \right) \right)
$$&lt;/p&gt;
&lt;p&gt;where $\mu^{(d)}(x)$ is the &lt;strong&gt;response function&lt;/strong&gt;, i.e. the expected value of the outcome, conditional on observable characteristics $x$ and treatment status $d$, and $e(X)$ is the &lt;strong&gt;propensity score&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;$$
\mu^{(d)}(x) = \mathbb E \left[ Y_i \ \big | \ X_i = x, D_i = d \right] \qquad ; \qquad e(x) = \mathbb E \left[ D_i = 1 \ \big | \ X_i = x \right]
$$&lt;/p&gt;
&lt;p&gt;The formula of the AIPW estimator seems very cryptic at first, so let&amp;rsquo;s dig deeper and try to understand it.&lt;/p&gt;
&lt;h3 id=&#34;decomposition&#34;&gt;Decomposition&lt;/h3&gt;
&lt;p&gt;The best way to understand the AIPW formula is to &lt;strong&gt;decompose&lt;/strong&gt; it into two parts.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;first way&lt;/strong&gt; is to decompose the AIPW estimator into a &lt;a href=&#34;https://towardsdatascience.com/8a9c1e340832&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;S-learner estimator&lt;/strong&gt;&lt;/a&gt; and an adjustment factor.&lt;/p&gt;
&lt;p&gt;$$
\hat \tau_{AIPW} = \hat \tau_{S-learn} + \widehat{\text{adj}}_{S-learn}
$$&lt;/p&gt;
&lt;p&gt;where&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
\hat \tau_{S-learn} =&amp;amp; \frac{1}{n} \sum_{i=1}^{n} \left( \hat \mu^{(1)}(X_i) - \hat \mu^{(0)}(X_i) \right) \newline
\widehat {adj} _ {S-learn} =&amp;amp; \frac{1}{n} \sum_{i=1}^{n} \left(\frac{D_i }{\hat e(X_i)} \left( Y_i - \hat \mu^{(1)}(X_i) \right) - \frac{(1-D_i) }{1-\hat e(X_i)} \left( Y_i - \hat \mu^{(0)}(X_i) \right) \right)
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;The adjustment is essentially an IPW estimator performed on the &lt;strong&gt;residuals&lt;/strong&gt; of the S-learner.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;second way&lt;/strong&gt; to decompose the AIPW estimator into the &lt;strong&gt;IPW estimator&lt;/strong&gt; and an adjustment factor.&lt;/p&gt;
&lt;p&gt;$$
\hat \tau_{AIPW} = \hat \tau_{IPW} + \widehat{\text{adj}}_{IPW}
$$&lt;/p&gt;
&lt;p&gt;where&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
\hat \tau_{IPW} &amp;amp;= \frac{1}{n} \sum _ {i=1}^{n} \left( \frac{D_i Y_i}{\hat e(X_i)} - \frac{(1-D_i) Y_i}{1-\hat e(X_i)} \right) \newline
\widehat {adj} _ {IPW} &amp;amp;= \frac{1}{n} \sum_{i=1}^{n} \left( \frac{\hat e(X_i) - D_i}{\hat e(X_i)} \hat \mu^{(1)}(X_i) - \frac{(1-\hat e(X_i)) - (1-D_i)}{1-\hat e(X_i)} \hat \mu^{(0)}(X_i) \right)
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;The adjustment is essentially an S-learner estimator weighted by the residual treatment probabilities.&lt;/p&gt;
&lt;h3 id=&#34;double-robustness&#34;&gt;Double Robustness&lt;/h3&gt;
&lt;p&gt;Why is the AIPW estimator so &lt;strong&gt;compelling&lt;/strong&gt;? The reason is that it just needs one of the two predictions, $\hat \mu$ or $\hat e$, to be right in order to be &lt;strong&gt;unbiased&lt;/strong&gt; (i.e. correct on average). Let&amp;rsquo;s check it.&lt;/p&gt;
&lt;p&gt;If $\hat \mu$ is correctly specified, i.e. $\mathbb E \left[ \hat \mu^{(d)}(x) \right] = \mathbb E \left[ Y_i \ \big | \ X_i = x, D_i = d \right]$, then $\hat \tau_{AIPW}$ is unbiased, &lt;strong&gt;even if&lt;/strong&gt; $\hat e$ is misspecified.&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
\hat \tau_{AIPW} &amp;amp;\overset{p}{\to} \mathbb E \Big[ \hat \tau_{S-learn} + \widehat{\text{adj}}_{S-learn} \Big] = \newline
&amp;amp;= \mathbb E \left[ \hat \mu^{(1)}(X_i) - \hat \mu^{(0)}(X_i) + \frac{D_i \left( Y_i - \hat \mu^{(1)}(X_i) \right)}{\hat e(X_i)} - \frac{(1-D_i) \left( Y_i - \hat \mu^{(0)}(X_i) \right)}{1-\hat e(X_i)} \right] = \newline
&amp;amp;= \mathbb E \left[ \hat \mu^{(1)}(X_i) - \hat \mu^{(0)}(X_i) \right] = \newline
&amp;amp;= \mathbb E \left[ Y^{(1)} - Y^{(0)} \right] = \newline
&amp;amp;= \tau
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;intuition&lt;/strong&gt; is that, if $\hat \mu$ is correctly specified, $\hat \tau_{S-learn}$ is &lt;strong&gt;unbiased&lt;/strong&gt; and the adjustment factor &lt;strong&gt;vanishes&lt;/strong&gt;, since the residuals $\left( Y_i - \hat \mu^{(t)}(X_i) \right)$ converge to zero.&lt;/p&gt;
&lt;p&gt;On the other hand, if $\hat e$ is correctly specified, i.e. $\mathbb E \left[\hat e(x) \right] = \mathbb E \left[ D_i = 1 \ \big | \ X_i = x \right]$, then $\hat \tau_{AIPW}$ is unbiased, &lt;strong&gt;even if&lt;/strong&gt; $\hat \mu$ is misspecified.&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
\hat \tau_{AIPW} &amp;amp; \overset{p}{\to} \mathbb E \Big[ \hat \tau_{IPW} + \widehat{\text{adj}}_{IPW} \Big] = \newline
&amp;amp;= \mathbb E \left[ \frac{D_i Y_i}{\hat e(X_i)} - \frac{(1-D_i) Y_i }{1-\hat e(X_i)} + \frac{\hat e(X_i) - D_i}{\hat e(X_i)} \hat \mu^{(1)}(X_i) - \frac{(1-\hat e(X_i)) - (1-D_i)}{1-\hat e(X_i)} \hat \mu^{(0)}(X_i) \right] = \newline
&amp;amp;= \mathbb E \left[ \frac{D_i Y_i}{\hat e(X_i)} - \frac{(1-D_i) Y_i }{1-\hat e(X_i)}\right] = \newline
&amp;amp;= \mathbb E \left[ Y^{(1)} - Y^{(0)} \right] = \newline
&amp;amp;= \tau
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;intuition&lt;/strong&gt; is that, if $\hat e$ is correctly specified, $\hat \tau_{IPW}$ is &lt;strong&gt;unbiased&lt;/strong&gt; and the adjustment factor &lt;strong&gt;vanishes&lt;/strong&gt;, since the residuals $\left( D_i - \hat e (X_i) \right)$ converge to zero.&lt;/p&gt;
&lt;h3 id=&#34;best-practices&#34;&gt;Best Practices&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;1. Check Covariate Balance&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Both IPW and AIPW were built for settings in which the treatment $D$ is not unconditionally randomly assigned, but might depend on some observables $X$. This information can be checked in two ways:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Produce a balance table, summarizing the covariates across treatment arms. If unconditional randomization does not hold, we expect to see significant differences across some observables&lt;/li&gt;
&lt;li&gt;Plot the estimated propensity scores. If unconditional randomization holds, we expect the propensity scores to be constant&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;2. Check the Overlap Assumption&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Another assumption that we can check is the &lt;strong&gt;overlap&lt;/strong&gt; assumption, i.e. $\exists \eta \ : \ \eta \leq \mathbb E \left[ D_i = 1 \ \big | \ X_i = x \right] \leq 1-\eta$. To check this assumption we can simply check the bounds of the predicted propensity scores. If the overlap assumption is violated, we end up dividing some term of the estimator by zero.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;3. Use Cross-Fitting&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Whenever we build a prediction, it is best practice to exclude observation $i$ when estimating $\hat \mu^{(d)} (X_i)$ or $\hat e (X_i)$. This procedure is generally known as &lt;a href=&#34;https://en.wikipedia.org/wiki/Cross-validation_%28statistics%29&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;cross-fitting&lt;/strong&gt;&lt;/a&gt; in the machine learning literature. While there are many possible ways to perform cross-fitting, the simplest one is the following:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Split the sample in two at random&lt;/li&gt;
&lt;li&gt;Use sample 1 to estimate $\hat \mu^{(d)} (X_i)$ and $\hat e (X_i)$&lt;/li&gt;
&lt;li&gt;Use sample 2 to estimate $\hat{\tau}_{AIPW, 1}$&lt;/li&gt;
&lt;li&gt;Repeat (2) and (3) swapping samples to estimate $\hat{\tau}_{AIPW, 2}$&lt;/li&gt;
&lt;li&gt;Compute $\hat{\tau}_{AIPW}$ as the average of the two estimates&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Steps (2) and (3) ensure that the estimator is &lt;strong&gt;not overfitting&lt;/strong&gt;. Steps (4) and (5) ensure that the estimator is &lt;strong&gt;efficient&lt;/strong&gt;, using all the data for all steps and not just half. &lt;a href=&#34;https://arxiv.org/abs/2004.14497&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Kennedy (2022)&lt;/a&gt; shows that this procedure produces much more precise estimates than existing methods and provide formal results on error bounds. In particular, their main result is the following:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;&lt;em&gt;The bound on the DR-Learner error given in Theorem 2 shows that it can only deviate from the oracle error by at most a (smoothed) product of errors in the propensity score and regression estimators, thus allowing faster rates for estimating the CATE even when the nui- sance estimates converge at slower rates. Importantly the result is agnostic about the methods used, and requires no special tuning or undersmoothing.&lt;/em&gt;&amp;rdquo;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;back-to-the-data&#34;&gt;Back to the Data&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s now build and explore the AIPW estimator in our dataset on blog reading time and dark mode.&lt;/p&gt;
&lt;h3 id=&#34;propensity-scores&#34;&gt;Propensity Scores&lt;/h3&gt;
&lt;p&gt;First, let&amp;rsquo;s estimate the &lt;strong&gt;propensity scores&lt;/strong&gt; $e(X)$.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def estimate_e(df, X, D, model_e):
    e = model_e.fit(df[X], df[D]).predict_proba(df[X])[:,1]
    return e
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We estimate them by &lt;a href=&#34;https://en.wikipedia.org/wiki/Logistic_regression&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;logistic regression&lt;/a&gt; using the &lt;code&gt;LogisticRegression&lt;/code&gt; methods from the &lt;code&gt;sklearn&lt;/code&gt; package.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from sklearn.linear_model import LogisticRegression

df[&#39;e&#39;] = estimate_e(df, X, &amp;quot;dark_mode&amp;quot;, LogisticRegression())
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s check if the &lt;strong&gt;bounded support&lt;/strong&gt; assumption is satisfied, by plotting the estimated propensity scores, across treatment and control groups.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sns.histplot(data=df, x=&#39;e&#39;, hue=&#39;dark_mode&#39;, bins=30, stat=&#39;density&#39;, common_norm=False).\
    set(ylabel=&amp;quot;&amp;quot;, title=&amp;quot;Distribution of Propensity Scores&amp;quot;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/aipw_38_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;The distribution of propensity scores is different between two groups, but it&amp;rsquo;s generally overlapping.&lt;/p&gt;
&lt;p&gt;We can now use the propensity scores to build the IPW estimator.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;w = 1 / (e * df[&amp;quot;dark_mode&amp;quot;] + (1-e) * (1-df[&amp;quot;dark_mode&amp;quot;]))
smf.wls(&amp;quot;read_time ~ dark_mode&amp;quot;, weights=w, data=df).fit().summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
          &lt;td&gt;&lt;/td&gt;             &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt;         &lt;td&gt;   18.6099&lt;/td&gt; &lt;td&gt;    0.412&lt;/td&gt; &lt;td&gt;   45.159&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;   17.799&lt;/td&gt; &lt;td&gt;   19.421&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;dark_mode[T.True]&lt;/th&gt; &lt;td&gt;    1.0620&lt;/td&gt; &lt;td&gt;    0.582&lt;/td&gt; &lt;td&gt;    1.826&lt;/td&gt; &lt;td&gt; 0.069&lt;/td&gt; &lt;td&gt;   -0.083&lt;/td&gt; &lt;td&gt;    2.207&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;Note that the computed standard errors are not exact, since we are ignoring the extra uncertainty that comes from the estimation of the propensity scores $e(X)$.&lt;/p&gt;
&lt;h3 id=&#34;response-function&#34;&gt;Response Function&lt;/h3&gt;
&lt;p&gt;Let&amp;rsquo;s now estimate the second building block of the AIPW estimator: the &lt;strong&gt;response function&lt;/strong&gt; $\mu(X)$.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def estimate_mu(df, X, D, y, model_mu):
    mu = model_mu.fit(df[X + [D]], df[y])
    mu0 = mu.predict(df[X + [D]].assign(dark_mode=0))
    mu1 = mu.predict(df[X + [D]].assign(dark_mode=1))
    return mu0, mu1
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s start by estimating $\mu(X)$ with linear regression.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from sklearn.linear_model import LinearRegression

mu0, mu1 = estimate_mu(df, X, &amp;quot;dark_mode&amp;quot;, &amp;quot;read_time&amp;quot;, LinearRegression())
print(np.mean(mu1-mu0))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;1.3858099131476969
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We have computed the meta learner estimate of the average treatment effect as the difference in means between the two estimated response functions, $\mu^{(1)}(X)$ and $\mu^{(0)}(X)$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt; that we can use any estimator to get the response function, I used linear regression for simplicity.&lt;/p&gt;
&lt;h3 id=&#34;estimating-aipw&#34;&gt;Estimating AIPW&lt;/h3&gt;
&lt;p&gt;We now have &lt;strong&gt;all the building blocks&lt;/strong&gt; to compute the AIPW estimator!&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;aipw = mu1 - mu0 + df[&amp;quot;dark_mode&amp;quot;] / e * (df[&amp;quot;read_time&amp;quot;] - mu1) - (1-df[&amp;quot;dark_mode&amp;quot;]) / (1-e) * (df[&amp;quot;read_time&amp;quot;] - mu0)
print(np.mean(aipw))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;1.3153774511905783
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can also compute it directly using the &lt;code&gt;LinearDRLearner&lt;/code&gt; function from Microsoft&amp;rsquo;s &lt;a href=&#34;https://econml.azurewebsites.net/index.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;EconML&lt;/code&gt;&lt;/a&gt; library.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from econml.drlearner import LinearDRLearner

model = LinearDRLearner(model_propensity=LogisticRegression(), 
                        model_regression=LinearRegression(),
                        random_state=1)
model.fit(Y=df[&amp;quot;read_time&amp;quot;], T=df[&amp;quot;dark_mode&amp;quot;], X=df[X]);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The model directly gives us the average treatment effect.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;model.ate_inference(X=df[X].values, T0=0, T1=1).summary().tables[0]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;caption&gt;Uncertainty of Mean Point Estimate&lt;/caption&gt;
&lt;tr&gt;
  &lt;th&gt;mean_point&lt;/th&gt; &lt;th&gt;stderr_mean&lt;/th&gt; &lt;th&gt;zstat&lt;/th&gt; &lt;th&gt;pvalue&lt;/th&gt; &lt;th&gt;ci_mean_lower&lt;/th&gt; &lt;th&gt;ci_mean_upper&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
     &lt;td&gt;1.417&lt;/td&gt;      &lt;td&gt;0.541&lt;/td&gt;    &lt;td&gt;2.621&lt;/td&gt;  &lt;td&gt;0.009&lt;/td&gt;     &lt;td&gt;0.358&lt;/td&gt;         &lt;td&gt;2.477&lt;/td&gt;    
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;The estimate is statistically different from zero and the confidence interval includes the true value of 2.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt; that we got a different estimate because the &lt;code&gt;LinearDRLearner&lt;/code&gt; function also performed &lt;strong&gt;cross-fitting&lt;/strong&gt; in the background, which we did not before.&lt;/p&gt;
&lt;h3 id=&#34;assessment&#34;&gt;Assessment&lt;/h3&gt;
&lt;p&gt;Let&amp;rsquo;s now assess the main property of the AIPW estimator: its &lt;strong&gt;double robustness&lt;/strong&gt;. To do so, we compare it with its two parents: the IPW estimator and the S-learner.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def compare_estimators(X_e, X_mu, D, y, seed):
    df = dgp_darkmode().generate_data(seed=seed)
    e = estimate_e(df, X_e, D, LogisticRegression())
    mu0, mu1 = estimate_mu(df, X_mu, D, y, LinearRegression())
    slearn = mu1 - mu0
    ipw = (df[D] / e - (1-df[D]) / (1-e)) * df[y]
    aipw = slearn + df[D] / e * (df[y] - mu1) - (1-df[D]) / (1-e) * (df[y] - mu0)
    return np.mean((slearn, ipw, aipw), axis=1)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We use the &lt;a href=&#34;https://joblib.readthedocs.io/en/latest/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;joblib&lt;/code&gt;&lt;/a&gt; library to run the simulations in parallel and speed up the process.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from joblib import Parallel, delayed

def simulate_estimators(X_e, X_mu, D, y):
    r = Parallel(n_jobs=8)(delayed(compare_estimators)(X_e, X_mu, D, y, i) for i in range(100))
    df_tau = pd.DataFrame(r, columns=[&#39;S-learn&#39;, &#39;IPW&#39;, &#39;AIPW&#39;])
    plot = sns.boxplot(data=pd.melt(df_tau), x=&#39;variable&#39;, y=&#39;value&#39;, linewidth=2);
    plot.set(title=&amp;quot;Distribution of $\hat τ$ and its components&amp;quot;, xlabel=&#39;&#39;, ylabel=&#39;&#39;)
    plot.axhline(2, c=&#39;r&#39;, ls=&#39;:&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;First, let&amp;rsquo;s assume that we use &lt;strong&gt;all variables&lt;/strong&gt; for both models, $\mu(X)$ and $e(X)$. In this case, both models are &lt;strong&gt;well specified&lt;/strong&gt; and we expect all estimators to perform well.&lt;/p&gt;
&lt;p&gt;We plot the distribution of the three estimators across 100 simulations.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;simulate_estimators(X_e=X, X_mu=X, D=&amp;quot;dark_mode&amp;quot;, y=&amp;quot;read_time&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/aipw_62_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Indeed, all estimator are &lt;strong&gt;unbiased&lt;/strong&gt; and deliver very similar estimates.&lt;/p&gt;
&lt;p&gt;What happens if we &lt;strong&gt;misspecify&lt;/strong&gt; one of the two models? Let&amp;rsquo;s start by (correctly) assuming that &lt;code&gt;gender&lt;/code&gt; and &lt;code&gt;age&lt;/code&gt; influence the probability of selecting &lt;code&gt;dark_mode&lt;/code&gt; and (wrongly) assuming that only previous &lt;code&gt;hours&lt;/code&gt; influence the weekly &lt;code&gt;read_time&lt;/code&gt;. In this case, the propensity score $e(X)$ is well specified, while the response function $\mu(X)$ is misspecified.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;simulate_estimators(X_e=[&#39;male&#39;, &#39;age&#39;], X_mu=[&#39;hours&#39;], D=&amp;quot;dark_mode&amp;quot;, y=&amp;quot;read_time&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/aipw_64_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;As expected, the S-learner is biased since we have misspecified $\mu(X)$, while IPW isn&amp;rsquo;t. AIPW picks the &lt;strong&gt;best of both worlds&lt;/strong&gt; and is unbiased.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s now explore the alternative &lt;strong&gt;misspecification&lt;/strong&gt;. We (wrongly) assume that only &lt;code&gt;age&lt;/code&gt; influences the probability of selecting &lt;code&gt;dark_mode&lt;/code&gt; and (correctly) assume that both &lt;code&gt;gender&lt;/code&gt; and previous &lt;code&gt;hours&lt;/code&gt; influence the weekly &lt;code&gt;read_time&lt;/code&gt;. In this case, the propensity score $e(X)$ is misspecified, while the response function $\mu(X)$ is correctly specified.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;simulate_estimators([&#39;age&#39;], [&#39;male&#39;, &#39;hours&#39;], D=&amp;quot;dark_mode&amp;quot;, y=&amp;quot;read_time&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/aipw_66_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;In this case, the S-learner is unbiased, while IPW isn&amp;rsquo;t, since we have misspecified $e(X)$. Again, AIPW picks the &lt;strong&gt;best of both worlds&lt;/strong&gt; and is unbiased.&lt;/p&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;In this article we have seen a method to estimate conditional average treatment effects (CATE), that is &lt;strong&gt;robust to model misspecification&lt;/strong&gt;: the Augmented Inverse Propensity Weighted (AIPW) estimator. The AIPW estimator takes the best out of two existing estimators: the &lt;a href=&#34;&#34;&gt;IPW estimator&lt;/a&gt; and the &lt;a href=&#34;&#34;&gt;S-learner&lt;/a&gt;. It requires the estimation of both the propensity score function $\mathbb{E} [ D | X ]$ and the response function $\mathbb{E} [ Y | D, X ]$ and it is &lt;strong&gt;unbiased&lt;/strong&gt; even if one of the two functions is misspecified.&lt;/p&gt;
&lt;p&gt;This estimator is now a standard and it is included all the most important causal inference packages such as Microsoft&amp;rsquo;s &lt;a href=&#34;https://econml.azurewebsites.net/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;EconML&lt;/a&gt;, Uber&amp;rsquo;s &lt;a href=&#34;https://causalml.readthedocs.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;causalml&lt;/a&gt; and Stanford researchers&amp;rsquo; R package &lt;a href=&#34;https://grf-labs.github.io/grf/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;grf&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;references&#34;&gt;References&lt;/h3&gt;
&lt;p&gt;[1] J. Robins, A. Rotzniski, J. P. Zhao, &lt;a href=&#34;https://www.tandfonline.com/doi/abs/10.1080/01621459.1994.10476818&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Estimation of regression coefficients when some regressors are not always observed&lt;/a&gt; (1994), &lt;em&gt;Journal of the American Statistical Associations&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;[2] A. Glyn, K. Quinn, &lt;a href=&#34;https://www.cambridge.org/core/journals/political-analysis/article/abs/an-introduction-to-the-augmented-inverse-propensity-weighted-estimator/4B1B8301E46F4432C4DCC91FE20780DB&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;An Introduction to the Augmented Inverse Propensity Weighted Estimator&lt;/a&gt; (2010), &lt;em&gt;Political Analysis&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;[3] E. Kennedy, &lt;a href=&#34;https://arxiv.org/abs/2004.14497&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Towards optimal doubly robust estimation of heterogeneous causal effects&lt;/a&gt; (2022), &lt;em&gt;working paper&lt;/em&gt;.&lt;/p&gt;
&lt;h3 id=&#34;related-articles&#34;&gt;Related Articles&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/b63dc69e3d8c&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;DAGs and Control Variables&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/99bf5cffa0d9&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Matching, Weighting, or Regression?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/8a9c1e340832&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Understanding Meta Learners&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;code&#34;&gt;Code&lt;/h3&gt;
&lt;p&gt;You can find the original Jupyter Notebook here:&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/aipw.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/aipw.ipynb&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Understanding Causal Trees</title>
      <link>https://matteocourthoud.github.io/post/causal_trees/</link>
      <pubDate>Mon, 10 Jul 2023 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/post/causal_trees/</guid>
      <description>&lt;p&gt;&lt;em&gt;How to use regression trees to estimate heterogeneous treatment effects.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;In causal inference we are usually interested in estimating the causal effect of a treatment (a drug, ad, product, &amp;hellip;) on an outcome of interest (a disease, firm revenue, customer satisfaction, &amp;hellip;). However, knowing that a treatment works on average is often not sufficient and we would like to know for which subjects (patients, users, customers, &amp;hellip;) it works better or worse, i.e. we would like to estimate &lt;strong&gt;heterogeneous treatment effects&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Estimating heterogeneous treatments effects allows us to do &lt;strong&gt;targeting&lt;/strong&gt;. Knowing which customers are more likely to react to a discount allows a company to spend less money by offering fewer but better targeted discounts. This works also for negative effects: knowing for which patients a certain drug has side effects allows a pharmaceutical company to warn or exclude them from the treatment. There is also a more subtle advantage of estimating heterogeneous treatment effects: knowing &lt;strong&gt;for whom&lt;/strong&gt; a treatment works allows us to better understand &lt;strong&gt;how&lt;/strong&gt; a treatment works. Knowing that the effect of a discount does not depend on the income of its recipient but rather by its buying habits  tells us that maybe it is not a matter of money, but rather a matter of attention or loyalty.&lt;/p&gt;
&lt;p&gt;In this article, we will explore the estimation of heterogeneous treatment effects using a modified version of regression trees (and forests). From a machine learning perspective, there are two fundamental &lt;strong&gt;differences between causal trees and predictive trees&lt;/strong&gt;. First of all, the target is the treatment effect, which is an inherently unobservable object. Second, we are interested in doing inference, which means quantifying the uncertainty of our estimates.&lt;/p&gt;
&lt;h2 id=&#34;online-discounts&#34;&gt;Online Discounts&lt;/h2&gt;
&lt;p&gt;For the rest of the article, we are going to use a toy example, for the sake of exposition: suppose we were an &lt;strong&gt;online shop&lt;/strong&gt; and we are interested in understanding whether offering discounts to new customers increases their expenditure. In particular, we would like to know if offering discounts is more effective for some customers with respect to others, since we would prefer not to give discounts to customers that would spend anyways. Moreover, it could also be that spamming customers with pop-ups could deter them from buying, having the opposite effect.&lt;/p&gt;
&lt;img src=&#34;fig/causal_trees1.jpg&#34; width=&#34;300px&#34;/&gt;
&lt;p&gt;To understand whether and how much the discounts are effective we run an &lt;strong&gt;A/B test&lt;/strong&gt;: whenever a new user visits our online shop, we randomly decide whether to offer them the discount or not. I import the data-generating process &lt;code&gt;dgp_online_discounts()&lt;/code&gt; from &lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/src/dgp.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;src.dgp&lt;/code&gt;&lt;/a&gt;. With respect to previous articles, I generated a new DGP parent class that handles randomization and data generation, while its children classes contain specific use cases. I also import some plotting functions and libraries from &lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/src/utils.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;src.utils&lt;/code&gt;&lt;/a&gt;. To include not only code but also data and tables, I use &lt;a href=&#34;https://deepnote.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Deepnote&lt;/a&gt;, a Jupyter-like web-based collaborative notebook environment.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;%matplotlib inline
%config InlineBackend.figure_format = &#39;retina&#39;

from src.utils import *
from src.dgp import dgp_online_discounts
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;dgp = dgp_online_discounts(n=100_000)
df = dgp.generate_data()
df.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;time&lt;/th&gt;
      &lt;th&gt;device&lt;/th&gt;
      &lt;th&gt;browser&lt;/th&gt;
      &lt;th&gt;region&lt;/th&gt;
      &lt;th&gt;discount&lt;/th&gt;
      &lt;th&gt;spend&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;10.78&lt;/td&gt;
      &lt;td&gt;mobile&lt;/td&gt;
      &lt;td&gt;edge&lt;/td&gt;
      &lt;td&gt;9&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0.46&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;0.57&lt;/td&gt;
      &lt;td&gt;desktop&lt;/td&gt;
      &lt;td&gt;firefox&lt;/td&gt;
      &lt;td&gt;9&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;11.04&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;3.74&lt;/td&gt;
      &lt;td&gt;mobile&lt;/td&gt;
      &lt;td&gt;safari&lt;/td&gt;
      &lt;td&gt;7&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1.81&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;13.37&lt;/td&gt;
      &lt;td&gt;desktop&lt;/td&gt;
      &lt;td&gt;other&lt;/td&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;31.90&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;0.71&lt;/td&gt;
      &lt;td&gt;mobile&lt;/td&gt;
      &lt;td&gt;explorer&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;15.42&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;We have data on 100.000 website visitors, for whom we observe the &lt;code&gt;time&lt;/code&gt; of the day, the &lt;code&gt;device&lt;/code&gt; they use, their &lt;code&gt;browser&lt;/code&gt; and their geographical &lt;code&gt;region&lt;/code&gt;. We also see whether they were offered the &lt;code&gt;discount&lt;/code&gt;, our treatment, and what is their &lt;code&gt;spend&lt;/code&gt;, the outcome of interest.&lt;/p&gt;
&lt;p&gt;Since the treatment was randomly assigned, we can use a simple &lt;strong&gt;difference-in-means&lt;/strong&gt; estimator to estimate the treatment effect. We expect the treatment and control group to be similar, except for the &lt;code&gt;discount&lt;/code&gt;, therefore we can causally attribute any difference in &lt;code&gt;spend&lt;/code&gt; to the &lt;code&gt;discount&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;smf.ols(&#39;spend ~ discount&#39;, df).fit().summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
      &lt;td&gt;&lt;/td&gt;         &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt; &lt;td&gt;    5.0306&lt;/td&gt; &lt;td&gt;    0.045&lt;/td&gt; &lt;td&gt;  110.772&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    4.942&lt;/td&gt; &lt;td&gt;    5.120&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;discount&lt;/th&gt;  &lt;td&gt;    1.9492&lt;/td&gt; &lt;td&gt;    0.064&lt;/td&gt; &lt;td&gt;   30.346&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    1.823&lt;/td&gt; &lt;td&gt;    2.075&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;The discount seems to be effective: on average the spend in the treatment group increases by 3.86$. But are all customers equally affected?&lt;/p&gt;
&lt;p&gt;To answer this question, we would like to estimate &lt;strong&gt;heterogeneous treatment effects&lt;/strong&gt;, possibly at the individual level.&lt;/p&gt;
&lt;h2 id=&#34;conditional-average-treatment-effects&#34;&gt;Conditional Average Treatment Effects&lt;/h2&gt;
&lt;p&gt;There are many possible ways to estimate heterogenous treatment effects. The most common is to split the population in groups based on some observable characteristic, which in our case could be the &lt;code&gt;device&lt;/code&gt;, the &lt;code&gt;browser&lt;/code&gt; or the geographical &lt;code&gt;region&lt;/code&gt;. Once you have decided which variable to split your data on, you can simply interact the treatment variable (&lt;code&gt;discount&lt;/code&gt;) with the dimension of treatment heterogeneity. Let&amp;rsquo;s take &lt;code&gt;device&lt;/code&gt; for example.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;smf.ols(&#39;spend ~ discount * device&#39;, df).fit().summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
              &lt;td&gt;&lt;/td&gt;                 &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt;                 &lt;td&gt;    5.0006&lt;/td&gt; &lt;td&gt;    0.064&lt;/td&gt; &lt;td&gt;   78.076&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    4.875&lt;/td&gt; &lt;td&gt;    5.126&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;device[T.mobile]&lt;/th&gt;          &lt;td&gt;    0.0602&lt;/td&gt; &lt;td&gt;    0.091&lt;/td&gt; &lt;td&gt;    0.664&lt;/td&gt; &lt;td&gt; 0.507&lt;/td&gt; &lt;td&gt;   -0.118&lt;/td&gt; &lt;td&gt;    0.238&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;discount&lt;/th&gt;                  &lt;td&gt;    1.2264&lt;/td&gt; &lt;td&gt;    0.091&lt;/td&gt; &lt;td&gt;   13.527&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    1.049&lt;/td&gt; &lt;td&gt;    1.404&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;discount:device[T.mobile]&lt;/th&gt; &lt;td&gt;    1.4447&lt;/td&gt; &lt;td&gt;    0.128&lt;/td&gt; &lt;td&gt;   11.261&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    1.193&lt;/td&gt; &lt;td&gt;    1.696&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;How do we interpret the regression results? The effect of the &lt;code&gt;discount&lt;/code&gt; on customers&amp;rsquo; &lt;code&gt;spend&lt;/code&gt; is $1.22$$ but it increases by a further $1.44$$ if the customer is accessing the website from a mobile &lt;code&gt;device&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Splitting is easy for categorical variables, but for a continuous variable like &lt;code&gt;time&lt;/code&gt; it is not intuitive where to split. Every hour? And which dimension is more informative? It would be temping to try all possible splits, but the more we split the data, the more it is likely that we find spurious results (i.e. we overfit, in machine learning lingo). It would be great if we could &lt;strong&gt;let the data speak&lt;/strong&gt; and select the minimum and most informative splits.&lt;/p&gt;
&lt;p&gt;In a &lt;a href=&#34;https://medium.com/towards-data-science/understanding-meta-learners-8a9c1e340832&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;separate post&lt;/a&gt;, I have shown how the so-called &lt;strong&gt;meta-learners&lt;/strong&gt; take this approach to causal inference. The idea is to predict the outcome conditional on the treatment status for each observation, and then compare the predicted conditional on treatment, with the predicted outcome conditional on control. The difference is the individual treatment effect.&lt;/p&gt;
&lt;p&gt;The problem with meta-learners is that they use all their &lt;a href=&#34;https://en.wikipedia.org/wiki/Degrees_of_freedom_%28statistics%29&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;degrees of freedom&lt;/a&gt; in predicting the outcome. However, we are interested to predict treatment effect heterogeneity. If most of the variation in the outcome is &lt;em&gt;not&lt;/em&gt; in the treatment dimension, we will get very poor estimates of the treatment effects.&lt;/p&gt;
&lt;p&gt;Is it possible to instead directly concentrate on the &lt;strong&gt;prediction of individual treatment effects&lt;/strong&gt;? Let&amp;rsquo;s define $Y$ the outcome of interest, &lt;code&gt;spend&lt;/code&gt; in our case, $D$ the treatment, the &lt;code&gt;discount&lt;/code&gt;, and $X$ other observable characteristics. The &lt;em&gt;ideal&lt;/em&gt; objective function is&lt;/p&gt;
&lt;p&gt;$$
\sum_i \Big [ ( \tau_i - \hat \tau_i(X))^2 \Big ]
$$&lt;/p&gt;
&lt;p&gt;where $\tau_i$ is the treatment effect of individual $i$. However, this objective function is &lt;strong&gt;unfeasible&lt;/strong&gt; since we do not observe $\tau_i$.&lt;/p&gt;
&lt;p&gt;But, turns out that there is a way to get an unbiased estimate of the &lt;strong&gt;individual treatment effect&lt;/strong&gt;. The &lt;strong&gt;idea&lt;/strong&gt; is to use an auxiliary outcome variable, whose expected value for each individual is the individual treatment effect. This variable is&lt;/p&gt;
&lt;p&gt;$$
Y_i^* = \frac{Y_i}{D_i \cdot p(X_i) - (1-D_i) \cdot (1-p(X_i))}
$$&lt;/p&gt;
&lt;p&gt;where $p(X_i)$ is the &lt;a href=&#34;https://en.wikipedia.org/wiki/Propensity_score_matching&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;propensity score&lt;/strong&gt;&lt;/a&gt; of observation $i$, i.e. its probability of being treated. A crucial assuption here is &lt;strong&gt;unconfoundedness&lt;/strong&gt;, also known as ignorability or selection on observables. In short, we will assume that, conditional on some observables $X$ the treatment assignment is as good as random.&lt;/p&gt;
&lt;p&gt;$$
\left\lbrace Y_i^{(0)}, Y_i^{(1)} \right \rbrace \ \perp \ D_i | X_i
$$&lt;/p&gt;
&lt;p&gt;where $Y_i^{(0)}$ and $Y_i^{(1)}$ denote the control and treated potential outcomes, respectively. In our case, we have randomized assignment, therefore we do not have to worry about unconfoundedness, unless the randomization went wrong.&lt;/p&gt;
&lt;p&gt;In randomized experiments, the propensity score is known since randomization is fully under control of the experimenter. For example, in our case, the probability of treatment was 50%. In quasi-experimental studies instead, when the treatment probability is not known, it has to be estimated. Even in randomized experiments, it is always better to estimate rather than inpute the propensity scores, since it guards against sampling variation in the randomization. For more details on the propensity scores and how they are used in causal inference, I have a separate post &lt;a href=&#34;https://medium.com/towards-data-science/matching-weighting-or-regression-99bf5cffa0d9&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s first generate dummy variables for our categorical variables, &lt;code&gt;device&lt;/code&gt;, &lt;code&gt;browser&lt;/code&gt; and &lt;code&gt;region&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_dummies = pd.get_dummies(df[dgp.X[1:]], drop_first=True)
df = pd.concat([df, df_dummies], axis=1)
X = [&#39;time&#39;] + list(df_dummies.columns)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We fit a &lt;code&gt;LogisticRegression&lt;/code&gt; and use it to predict the treatment probability, i.e. construct the propensity score.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from sklearn.linear_model import LogisticRegression

df[&#39;pscore&#39;] = LogisticRegression().fit(df[X], df[dgp.D]).predict_proba(df[X])[:,1]
sns.histplot(data=df, x=&#39;pscore&#39;, hue=&#39;discount&#39;).set(
    title=&#39;Predicted propensity scores&#39;, xlim=[0,1], xlabel=&#39;Propensity Score&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/causal_trees_18_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;As expected, most propensity scores are very close to 0.5, the probability of treatment used in randomization.&lt;/p&gt;
&lt;p&gt;We now have all the elements to compute our auxiliary outcome variable $Y^*$.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df[&#39;y_star&#39;] = df[dgp.Y[0]] / (df[dgp.D] * df[&#39;pscore&#39;] - (1-df[dgp.D]) * (1-df[&#39;pscore&#39;]))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As we said before, the idea is to use $Y^*$ as the target of a prediction problem, since the expected value is exactly the individual treatment effect. Let&amp;rsquo;s check its average in the data.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df[&#39;y_star&#39;].mean()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;1.94501174385229
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Indeed its average is almost identical to the previously estimated average treatment effect of 3.85.&lt;/p&gt;
&lt;p&gt;How is it possible that, with a single observation and an estimate of the propensity score, we can estimate the individual treatment effect? What are the drawbacks?&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;intuition&lt;/strong&gt; is to approach the problem from a different perspective: &lt;em&gt;ex-ante&lt;/em&gt;, before the experiment. Imagine that our dataset had a single observation, $i$. We know that the treatment probability is $p(X_i)$, the propensity score. Therefore, in expectation, our dataset has $p(X_i)$ observations in the treatment group and $1 - p(X_i)$ observations in the control group. The rest is business as usual: we estimate the treatment effect as the difference in average outcomes between the two groups! And indeed that is what we would do:&lt;/p&gt;
&lt;p&gt;$$
Y_i^* = \frac{Y_i D_i}{p(X_i)} - \frac{Y_i (1-D_i)}{1-p(X_i)}
$$&lt;/p&gt;
&lt;p&gt;The only difference is that we have a single observation.&lt;/p&gt;
&lt;p&gt;This trick comes at a cost: $Y_i^*$ is an unbiased estimator for the individual treatment effect, but has a very &lt;strong&gt;high variance&lt;/strong&gt;. This is immediately visible by plotting its distribution.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;fig, ax = plt.subplots()
sns.histplot(df[&#39;y_star&#39;], ax=ax).set(title=&#39;Distribution of Auxiliary Variable&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/causal_trees_24_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;We are now ready to estimate &lt;strong&gt;heterogeneous treatment effects&lt;/strong&gt;, by translating the causal inference problem into a prediction problem, predicting the auxiliary outcome $Y^*$, given observable characteristics $X$.&lt;/p&gt;
&lt;img src=&#34;fig/causal_trees2.jpg&#34; width=&#34;300px&#34;/&gt;
&lt;h2 id=&#34;causal-trees&#34;&gt;Causal Trees&lt;/h2&gt;
&lt;p&gt;In the previous section, we have see that we can transform the estimation of &lt;strong&gt;heterogeneous treatment effects&lt;/strong&gt; into a prediction problem, where the outcome is the auxiliary outcome variable&lt;/p&gt;
&lt;p&gt;$$
Y_i^* = \frac{Y_i}{T_i * e_i - (1-T_i) * (1-e_i)}
$$&lt;/p&gt;
&lt;p&gt;We can in principle use any machine learning algorithm at this point to estimate individual treatment effects. However, &lt;a href=&#34;https://en.wikipedia.org/wiki/Decision_tree_learning&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;regression trees&lt;/strong&gt;&lt;/a&gt; have particularly convenient characteristics.&lt;/p&gt;
&lt;p&gt;First of all, how do regression trees work? Without going too much in detail, they are an algorithm that recursively &lt;strong&gt;partitions the data in bins&lt;/strong&gt; such that the outcome $Y$ &lt;em&gt;within&lt;/em&gt; each bin is as homogeneous as possible and the outcome &lt;em&gt;across&lt;/em&gt; bins is as heterogeneous as possible. The predicted values are simply the averages within each bin.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;averaging&lt;/strong&gt; part is one of the big advantages of regression trees for inference since we know very well how to do inference with averages, with the &lt;a href=&#34;https://en.wikipedia.org/wiki/Central_limit_theorem&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Central Limit Theorem&lt;/strong&gt;&lt;/a&gt;. The second advantage is that trees are very &lt;strong&gt;interpretable&lt;/strong&gt;, since we can directly plot the data partition as a tree structure. We will see more of this later. Last but not least, regression trees are still at the core the &lt;a href=&#34;https://arxiv.org/abs/2207.08815&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;best performing predictive algorithms&lt;/a&gt; with tabular data, as of 2022.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s use the &lt;a href=&#34;https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;DecisionTreeRegressor&lt;/code&gt;&lt;/a&gt; function from &lt;code&gt;sklearn&lt;/code&gt; to fit our regression tree and estimate heterogeneous treatment effects of &lt;code&gt;discounts&lt;/code&gt; on customers&amp;rsquo; &lt;code&gt;spend&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from sklearn.tree import DecisionTreeRegressor

tree = DecisionTreeRegressor(max_depth=2).fit(df[X], df[&#39;y_star&#39;])
df[&#39;y_hat&#39;] = tree.predict(df[X])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We have restricted the tree to have a maximum depth of 2 and at least 30 observation per partition (also called &lt;em&gt;leaf&lt;/em&gt;) so that we can easily plot the tree and visualize the estimated groups and treatment effects.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from sklearn.tree import plot_tree

plot_tree(tree, filled=True, fontsize=12, feature_names=X, impurity=False, rounded=True);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/causal_trees_31_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;How should we &lt;strong&gt;interpret&lt;/strong&gt; the tree? On the top, we can see the average $Y^*$ in the data, 3.851. Starting from there, the data gets split into different branches, according to the rules highlighted at the top of each node. For example, the first node splits the data into two groups of size 42970 and 57030 depending on whether the &lt;code&gt;time&lt;/code&gt; is later than 10.365. At the bottom, we have our final partitions, with the predicted values. For example, the leftmost leaf contains 36846 observation with &lt;code&gt;time&lt;/code&gt; earlier than 10.365 and non-Safari &lt;code&gt;browser&lt;/code&gt;, for which we predict a spend of 1.078. Darker node colors indicate higher prediction values.&lt;/p&gt;
&lt;p&gt;Should we believe these estimates? Not really, because of a couple of reasons. The &lt;strong&gt;first problem&lt;/strong&gt; is that we have an unbiased estimate of the average treatment effect only if, &lt;em&gt;within each leaf&lt;/em&gt;, we have the same number of treated and control units. This is not automatically the case with an off-the-shelf &lt;code&gt;DecisionTreeRegressor()&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Moreover, we have used the &lt;strong&gt;same data&lt;/strong&gt; to generate the tree and evaluate it. This generates some bias because of overfitting. We can split the sample in 2 and use different data to generate the tree and compute the predictions. These trees are called &lt;strong&gt;honest trees&lt;/strong&gt;.&lt;/p&gt;
&lt;h3 id=&#34;generating-splits&#34;&gt;Generating Splits&lt;/h3&gt;
&lt;p&gt;Last but not least, how should the tree be generated? The default rule to generate splits with the &lt;code&gt;DecisionTreeRegressor&lt;/code&gt; function is the &lt;code&gt;squared_error&lt;/code&gt; and there is no restriction on the minimum number of observations per leaf. Other commonly used rules include, mean absolute error, Gini&amp;rsquo;s impurity, and Shannon&amp;rsquo;s information. Which one performs better depends on the specific application, but the general objective is always prediction accuracy, broadly defined.&lt;/p&gt;
&lt;p&gt;In our case instead, the objective is inference: we want to uncover heterogeneous  treatment effects that are statistically different from each other. There is no value in generating different treatment effects if they are statistically indistinguishable. Moreover (but strongly related), when building the tree and generating the data partitions, we have to take into account that, since we use honest trees, we will use different data to estimate the within-leaf treatment effects.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.pnas.org/doi/abs/10.1073/pnas.1510489113&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Athey and Imbens (2016)&lt;/a&gt; use an modified version of the &lt;a href=&#34;https://en.wikipedia.org/wiki/Mean_squared_error&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Mean Squared Error (MSE)&lt;/a&gt; as splitting criterion, the &lt;strong&gt;Expanded Mean Squared Error (EMSE)&lt;/strong&gt;:&lt;/p&gt;
&lt;p&gt;$$
EMSE = \mathbb{E} \Big[ \big( Y_i - \hat \mu(X_i)\big)^2 - Y_i^2 \Big]
$$&lt;/p&gt;
&lt;p&gt;where the main difference is given by the additional term $Y_i^2$, the squared outcome variable. In our case, we can rewrite it as&lt;/p&gt;
&lt;p&gt;$$
EMSE = \mathbb{E} \Big[ \big( Y^&lt;em&gt;_i - \hat \tau(X_i)\big)^2 - {Y^&lt;/em&gt;_i}^2 \Big]
$$&lt;/p&gt;
&lt;p&gt;Why is this a sensible error loss? Because we can rewrite it as the sum of the squared mean μ and the estimator&amp;rsquo;s variance.&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
EMSE &amp;amp;= \mathbb{E} \Big[ \big( Y^&lt;em&gt;_i - \hat \tau(X_i)\big)^2 - {Y^&lt;/em&gt;_i}^2 \Big] = \newline
&amp;amp;= \mathbb{E} \Big[ \big( Y^&lt;em&gt;_i - \tau(X_i)\big)^2 - {Y^&lt;/em&gt;_i}^2 \Big] - \mathbb{E} \Big[ \big( \hat \tau(X_i) - \tau(X_i)\big)^2 \Big] = \newline
&amp;amp;= \mathbb{E} \Big[ \mathbb{V} \big (\hat \tau(X_i)^2 \big) \Big] - \mathbb{E} \Big[ \tau(X_i)^2 \Big]
\end{aligned}
$$&lt;/p&gt;
&lt;h3 id=&#34;implementation&#34;&gt;Implementation&lt;/h3&gt;
&lt;p&gt;Luckily, there are multiple libraries where the so-called &lt;a href=&#34;https://www.pnas.org/doi/abs/10.1073/pnas.1510489113&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;causal trees&lt;/strong&gt;&lt;/a&gt; are implemented. We import &lt;code&gt;CausalForestDML&lt;/code&gt; from Microsoft&amp;rsquo;s &lt;a href=&#34;https://econml.azurewebsites.net/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;EconML&lt;/a&gt; library, one of the best libraries for causal inference.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from econml.dml import CausalForestDML

np.random.seed(0)
tree_model = CausalForestDML(n_estimators=1, subforest_size=1, inference=False, max_depth=3)
tree_model = tree_model.fit(Y=df[dgp.Y], X=df[X], T=df[dgp.D])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We have restricted the number of estimators to 1 to have a single tree instead of multiple ones, the so-called &lt;a href=&#34;https://en.wikipedia.org/wiki/Random_forest&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;random forests&lt;/strong&gt;&lt;/a&gt; that we will cover in a separate article.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from econml.cate_interpreter import SingleTreeCateInterpreter
%matplotlib inline

intrp = SingleTreeCateInterpreter(max_depth=2).interpret(tree_model, df[X])
intrp.plot(feature_names=X, fontsize=12)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/causal_trees_39_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;As we can see, the tree representation looks extremely similar to the one we got before using the &lt;code&gt;DecisionTreeRegressor&lt;/code&gt; function. However, now the model not only reports estimates of the conditional average treatment effects, but also the standard errors of the estimates (at the bottom). How were they computed?&lt;/p&gt;
&lt;h3 id=&#34;inference&#34;&gt;Inference&lt;/h3&gt;
&lt;p&gt;Honest trees, besides improving the out-of-sample prediction accuracy of the model, have another great implication: they allow us to &lt;strong&gt;compute standard errors as if the tree structure was exogenous&lt;/strong&gt;. In fact, since the data used to compute the predictions is independent from the data used to build the tree (split the data), we can just treat the tree structure as independent from the estimated treatment effects. As a consequence, we can estimate the standard errors of the the estimates as standard errors of difference between sample averages, as in a standard AB test.&lt;/p&gt;
&lt;p&gt;If we had used the same data to build the tree and estimate the treatment effects, we would have introduced &lt;strong&gt;bias&lt;/strong&gt;, because of the spurious correlation between the covariates and the outcomes. This bias usually disappears for very large sample sizes, but honest trees do not require than.&lt;/p&gt;
&lt;h3 id=&#34;performance&#34;&gt;Performance&lt;/h3&gt;
&lt;p&gt;How well does the model perform? Since we control the data generating process, we can do something that is not possible with real data: check the predicted treatment effects against the true ones. The &lt;code&gt;generate_potential_outcomes()&lt;/code&gt; function loads the data with both potential outcomes for each observation, under both treatment (&lt;code&gt;outcome_t&lt;/code&gt;) and control (&lt;code&gt;outcome_c&lt;/code&gt;).&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def compute_discrete_effects(df, hte_model):
    temp_df = df.copy()
    temp_df.time = 0
    temp_df = dgp.add_treatment_effect(temp_df)
    temp_df = temp_df.rename(columns={&#39;effect_on_spend&#39;: &#39;True&#39;})
    temp_df[&#39;Predicted&#39;] = hte_model.effect(temp_df[X])
    df_effects = pd.DataFrame()
    for var in X[1:]:
        for effect in [&#39;True&#39;, &#39;Predicted&#39;]:
            v = temp_df[effect][temp_df[var]==1].mean() - temp_df[effect][temp_df[var]==0].mean()
            effect_var = {&#39;Variable&#39;: [var], &#39;Effect&#39;: [effect], &#39;Value&#39;: [v]}
            df_effects = pd.concat([df_effects, pd.DataFrame(effect_var)]).reset_index(drop=True)
    return df_effects, temp_df[&#39;Predicted&#39;].mean()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_effects_tree, avg_effect_notime_tree = compute_discrete_effects(df, tree_model)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;fig, ax = plt.subplots()
sns.barplot(data=df_effects_tree, x=&amp;quot;Variable&amp;quot;, y=&amp;quot;Value&amp;quot;, hue=&amp;quot;Effect&amp;quot;, ax=ax).set(
    xlabel=&#39;&#39;, ylabel=&#39;&#39;, title=&#39;Heterogeneous Treatment Effects&#39;)
ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha=&amp;quot;right&amp;quot;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/causal_trees_48_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;The causal tree is pretty good at detecting the heterogeneous treatment effects for the categorical variables. It only missed the heterogeneity in the third region.&lt;/p&gt;
&lt;p&gt;However, this is also where we expect a tree model to perform particularly well: where the effects are &lt;strong&gt;discrete&lt;/strong&gt;. How well does it do on our continuous variable, time? First, let&amp;rsquo;s again isolate the predicted treatment effects on &lt;code&gt;time&lt;/code&gt; and ignore the other covariates.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def compute_time_effect(df, hte_model, avg_effect_notime):
    df_time = df.copy()
    df_time[[X[1:]] + [&#39;device&#39;, &#39;browser&#39;, &#39;region&#39;]] = 0
    df_time = dgp.add_treatment_effect(df_time)
    df_time[&#39;predicted&#39;] = hte_model.effect(df_time[X]) + avg_effect_notime
    return df_time
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_time_tree = compute_time_effect(df, tree_model, avg_effect_notime_tree)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We now plot the predicted treatment effects against the true ones, along the &lt;code&gt;time&lt;/code&gt; dimension.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sns.scatterplot(x=&#39;time&#39;, y=&#39;effect_on_spend&#39;, data=df_time_tree, label=&#39;True&#39;)
sns.scatterplot(x=&#39;time&#39;, y=&#39;predicted&#39;, data=df_time_tree, label=&#39;Predicted&#39;).set(
    ylabel=&#39;&#39;, title=&#39;Heterogeneous Treatment Effects&#39;)
plt.legend(title=&#39;Effect&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/causal_trees_53_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;From the plot, we can appreciate the discrete nature of causal trees: the model is only able to split the continuous variable into 5 bins. These bins are close to the true treatment effects, but they fail to capture a big chunk of the treatment effect heterogeneity.&lt;/p&gt;
&lt;p&gt;Can these predictions be improved? The answer is yes, and we will explore how in the next post.&lt;/p&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;In this article, we have seen how to use causal trees to estimate &lt;strong&gt;heterogeneous treatment effects&lt;/strong&gt;. The main insight comes from the definition of an auxiliary outcome variable that allows us to frame the inference problem as a prediction problem. While we can then use any algorithm to predict treatment effects, regression trees are particularly useful because of their interpretability, prediction accuracy, and feature of generating prediction as subsample averages.&lt;/p&gt;
&lt;p&gt;The work by &lt;a href=&#34;https://www.pnas.org/doi/abs/10.1073/pnas.1510489113&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Athey and Imbens (2016)&lt;/a&gt; on regression trees to compute heterogeneous treatment effects brought together two separate literatures, causal inference and machine learning in a very fruitful &lt;strong&gt;synergy&lt;/strong&gt;. The causal inference literature (re)discovered the inference benefits of sample splitting, that allows us to do correct inference even when the data partition is complex and hard to analyze. On the other hand, splitting the tree generation phase from the within-leaf prediction phase has strong benefits in terms of prediction accuracy, by safeguarding against overfitting.&lt;/p&gt;
&lt;h3 id=&#34;references&#34;&gt;References&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;S. Athey, G. Imbens, &lt;a href=&#34;https://www.pnas.org/doi/abs/10.1073/pnas.1510489113&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Recursive partitioning for heterogeneous causal effects&lt;/a&gt; (2016), &lt;em&gt;PNAS&lt;/em&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;S. Wager, S. Athey, &lt;a href=&#34;https://www.tandfonline.com/doi/full/10.1080/01621459.2017.1319839&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Estimation and Inference of Heterogeneous Treatment Effects using Random Forests&lt;/a&gt; (2018), &lt;em&gt;Journal of the American Statistical Association&lt;/em&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;S. Athey, J. Tibshirani, S. Wager, &lt;a href=&#34;https://projecteuclid.org/journals/annals-of-statistics/volume-47/issue-2/Generalized-random-forests/10.1214/18-AOS1709.full&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Generalized Random Forests&lt;/a&gt; (2019). &lt;em&gt;The Annals of Statistics&lt;/em&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;M. Oprescu, V. Syrgkanis, Z. Wu, &lt;a href=&#34;http://proceedings.mlr.press/v97/oprescu19a.html?ref=https://githubhelp.com&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Orthogonal Random Forest for Causal Inference&lt;/a&gt; (2019). &lt;em&gt;Proceedings of the 36th International Conference on Machine Learning&lt;/em&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;related-articles&#34;&gt;Related Articles&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://towardsdatascience.com/b63dc69e3d8c&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;DAGs and Control Variables&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://towardsdatascience.com/99bf5cffa0d9&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Matching, Weighting, or Regression?&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://towardsdatascience.com/8a9c1e340832&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Understanding Meta Learners&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://towardsdatascience.com/ed4097dab27a&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Understanding AIPW, the Doubly-Robust Estimator&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;code&#34;&gt;Code&lt;/h3&gt;
&lt;p&gt;You can find the original Jupyter Notebook here:&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/causal_trees.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/causal_trees.ipynb&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Understanding Contamination Bias</title>
      <link>https://matteocourthoud.github.io/post/contamination_bias/</link>
      <pubDate>Mon, 10 Jul 2023 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/post/contamination_bias/</guid>
      <description>&lt;p&gt;&lt;em&gt;Problems and solutions of linear regression with multiple treatments&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;In many causal inference settings, we might be interested in the effect of not just one treatment, but &lt;strong&gt;many mutually exclusive treatments&lt;/strong&gt;. For example, we might want to test alternative UX designs, or drugs, or policies. Depending on the context, there might be many reasons why we want to test different treatments at the same time, but generally it can help &lt;em&gt;reducing the sample size&lt;/em&gt;, as we need just a single control group. A simple way to recover the different treatment effects is a linear regression of the outcome of interest on the different treatment indicators.&lt;/p&gt;
&lt;p&gt;However, in causal inference, we often &lt;strong&gt;condition the analysis&lt;/strong&gt; on other observable variables (often called control variables), either to increase power or, especially in quasi-experimental settings, to identify a causal parameter instead of a simple correlation. There are &lt;a href=&#34;https://towardsdatascience.com/b63dc69e3d8c&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;cases in which adding control variables can backfire&lt;/a&gt;, but otherwise, we usually think that the regression framework is still able to recover the average treatment effect.&lt;/p&gt;
&lt;p&gt;In a breakthrough paper, &lt;a href=&#34;https://www.nber.org/papers/w30108&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Goldsmith-Pinkham, Hull and Kolesár (2022)&lt;/a&gt; have recently shown that in case of &lt;em&gt;multiple and mutually-exclusive&lt;/em&gt; treatments with &lt;em&gt;control variables&lt;/em&gt;, the &lt;strong&gt;regression coefficients do not identify a causal effect&lt;/strong&gt;. However, not everything is lost: the authors propose a simple solution to this problem that still makes use of linear regression.&lt;/p&gt;
&lt;p&gt;In this blog post, I am going to go through a &lt;strong&gt;simple example&lt;/strong&gt; illustrating the nature of the problem and the solution proposed by the authors.&lt;/p&gt;
&lt;h2 id=&#34;multiple-treatments-example&#34;&gt;Multiple Treatments Example&lt;/h2&gt;
&lt;p&gt;Suppose we are an online store and we are not satisfied with our current &lt;em&gt;checkout page&lt;/em&gt;. In particular, we would like to change our &lt;strong&gt;checkout button&lt;/strong&gt; to increase the probability of a purchase. Our UX designer comes up with two alternative checkout buttons, which are displayed below.&lt;/p&gt;
&lt;img src=&#34;fig/buttons.png&#34; width=&#34;800px&#34;/&gt;
&lt;p&gt;In order to understand which button to use, we run an &lt;a href=&#34;https://en.wikipedia.org/wiki/A/B_testing&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;A/B test&lt;/strong&gt;&lt;/a&gt;, or randomized control trial. In particular, when people arrive at the checkout page, we show them one of the three options, at random. Then, for each user, we record the revenue generated which is our outcome of interest.&lt;/p&gt;
&lt;p&gt;I generate a synthetic dataset using &lt;code&gt;dgp_buttons()&lt;/code&gt; from &lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/src/dgp.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;src.dgp&lt;/code&gt;&lt;/a&gt; as data generating process. I also import plotting functions and standard libraries from &lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/src/utils.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;src.utils&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;%matplotlib inline
%config InlineBackend.figure_format = &#39;retina&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from src.utils import *
from src.dgp import dgp_buttons
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;dgp = dgp_buttons()
df = dgp.generate_data()
df.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;group&lt;/th&gt;
      &lt;th&gt;revenue&lt;/th&gt;
      &lt;th&gt;mobile&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;button1&lt;/td&gt;
      &lt;td&gt;8.927335&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;default&lt;/td&gt;
      &lt;td&gt;13.613456&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;button2&lt;/td&gt;
      &lt;td&gt;4.777628&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;default&lt;/td&gt;
      &lt;td&gt;8.909049&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;default&lt;/td&gt;
      &lt;td&gt;10.160347&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;We have information on 2000 users, for which we observe their checkout button (&lt;code&gt;default&lt;/code&gt;, &lt;code&gt;button1&lt;/code&gt; or &lt;code&gt;button2&lt;/code&gt;), the &lt;code&gt;revenue&lt;/code&gt; they generate and whether they connected from desktop or &lt;code&gt;mobile&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;We notice too late that we have a &lt;strong&gt;problem with randomization&lt;/strong&gt;. We showed &lt;code&gt;button1&lt;/code&gt; more frequently to desktop users and &lt;code&gt;button2&lt;/code&gt; more frequently to mobile users. The control group that sees the &lt;code&gt;default&lt;/code&gt; button instead is balanced.&lt;/p&gt;
&lt;img src=&#34;fig/randomization.png&#34; width=&#34;600px&#34;/&gt;
&lt;p&gt;What should we do? What happens if we simply compare &lt;code&gt;revenue&lt;/code&gt; across &lt;code&gt;groups&lt;/code&gt;? Let&amp;rsquo;s do it by regressing &lt;code&gt;revenue&lt;/code&gt; on &lt;code&gt;group&lt;/code&gt; dummy variables.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;smf.ols(&#39;revenue ~ group&#39;, data=df).fit().summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
          &lt;td&gt;&lt;/td&gt;            &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt;        &lt;td&gt;   11.6553&lt;/td&gt; &lt;td&gt;    0.149&lt;/td&gt; &lt;td&gt;   78.250&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;   11.363&lt;/td&gt; &lt;td&gt;   11.948&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;group[T.button1]&lt;/th&gt; &lt;td&gt;   -0.5802&lt;/td&gt; &lt;td&gt;    0.227&lt;/td&gt; &lt;td&gt;   -2.556&lt;/td&gt; &lt;td&gt; 0.011&lt;/td&gt; &lt;td&gt;   -1.026&lt;/td&gt; &lt;td&gt;   -0.135&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;group[T.button2]&lt;/th&gt; &lt;td&gt;   -0.5958&lt;/td&gt; &lt;td&gt;    0.218&lt;/td&gt; &lt;td&gt;   -2.727&lt;/td&gt; &lt;td&gt; 0.006&lt;/td&gt; &lt;td&gt;   -1.024&lt;/td&gt; &lt;td&gt;   -0.167&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;From the regression results we estimate a negative and significant effect for both buttons. Should we believe these estimates? Are they &lt;strong&gt;causal&lt;/strong&gt;?&lt;/p&gt;
&lt;p&gt;It is unlikely that what we have estimated are the true treatment effects. In fact, there might be substantial &lt;strong&gt;differences in purchase attitudes&lt;/strong&gt; between desktop and mobile users. Since we do not have a comparable number of mobile and desktop users across treatment arms, it might be that the observed differences in &lt;code&gt;revenue&lt;/code&gt; are due to the &lt;em&gt;device&lt;/em&gt; used and not the &lt;em&gt;button design&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Because of this, we decide to &lt;strong&gt;condition&lt;/strong&gt; our analysis on the device used and we include the &lt;code&gt;mobile&lt;/code&gt; dummy variable in the regression.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;smf.ols(&#39;revenue ~ group + mobile&#39;, data=df).fit().summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
          &lt;td&gt;&lt;/td&gt;            &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt;        &lt;td&gt;    9.1414&lt;/td&gt; &lt;td&gt;    0.110&lt;/td&gt; &lt;td&gt;   82.905&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    8.925&lt;/td&gt; &lt;td&gt;    9.358&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;group[T.button1]&lt;/th&gt; &lt;td&gt;    0.3609&lt;/td&gt; &lt;td&gt;    0.141&lt;/td&gt; &lt;td&gt;    2.558&lt;/td&gt; &lt;td&gt; 0.011&lt;/td&gt; &lt;td&gt;    0.084&lt;/td&gt; &lt;td&gt;    0.638&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;group[T.button2]&lt;/th&gt; &lt;td&gt;   -1.0326&lt;/td&gt; &lt;td&gt;    0.134&lt;/td&gt; &lt;td&gt;   -7.684&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;   -1.296&lt;/td&gt; &lt;td&gt;   -0.769&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;mobile&lt;/th&gt;           &lt;td&gt;    4.7181&lt;/td&gt; &lt;td&gt;    0.116&lt;/td&gt; &lt;td&gt;   40.691&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    4.491&lt;/td&gt; &lt;td&gt;    4.946&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;Now the coefficient of &lt;code&gt;button1&lt;/code&gt; is positive and significant. Should we recommend its implementation?&lt;/p&gt;
&lt;p&gt;The answer is &lt;strong&gt;surprisingly no&lt;/strong&gt;. &lt;a href=&#34;https://www.nber.org/papers/w30108&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Goldsmith-Pinkham, Hull, Kolesár (2022)&lt;/a&gt; show that this type of regression does not identify the average treatment effect when:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;there are mutually exclusive treatment arms (in our case, &lt;code&gt;groups&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;we are controlling for some variable $X$ (in our case, &lt;code&gt;mobile&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;there treatment effects are heterogeneous in $X$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This is true &lt;strong&gt;even if&lt;/strong&gt; the treatment is &amp;ldquo;as good as random&amp;rdquo; once we condition on $X$.&lt;/p&gt;
&lt;p&gt;Indeed, in our case, the true treatment effects are the ones reported in the following table.&lt;/p&gt;
&lt;img src=&#34;fig/effects.png&#34; width=&#34;500px&#34;/&gt;
&lt;p&gt;The first button has &lt;strong&gt;no effect&lt;/strong&gt; on revenue, irrespectively of the device, while the second button has a &lt;strong&gt;positive effect&lt;/strong&gt; for mobile users and a &lt;strong&gt;negative effect&lt;/strong&gt; for desktop users. Our (wrong) regression specification instead estimates a positive effect of the first button.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s now dig more in detail into the math, to understand why this is happening.&lt;/p&gt;
&lt;h2 id=&#34;theory&#34;&gt;Theory&lt;/h2&gt;
&lt;p&gt;This section borrows heavily from &lt;a href=&#34;https://www.nber.org/papers/w30108&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Goldsmith-Pinkham, Hull, Kolesár (2022)&lt;/a&gt;. For a great summary of the paper, I recommend this excellent Twitter thread by one of the authors, Paul Goldsmith-Pinkham.&lt;/p&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;Economists love using linear regression to estimate treatment effects — it turns out that there are perils to this method, but also amazing perks&lt;br&gt;&lt;br&gt;Come with me in this 🧵 if you want to learn… &lt;a href=&#34;https://t.co/eDsRLkZFZe&#34;&gt;https://t.co/eDsRLkZFZe&lt;/a&gt;&lt;/p&gt;&amp;mdash; Paul Goldsmith-Pinkham (@paulgp) &lt;a href=&#34;https://twitter.com/paulgp/status/1534169803388293120?ref_src=twsrc%5Etfw&#34;&gt;June 7, 2022&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;

&lt;h3 id=&#34;single-treatment-arm&#34;&gt;Single Treatment Arm&lt;/h3&gt;
&lt;p&gt;Suppose we are interested in the effect of a treatment $D$ on an outcome $Y$. First, let&amp;rsquo;s consider the standard case of a &lt;strong&gt;single treatment arm&lt;/strong&gt; so that the treatment variable is binary, $D \in \lbrace 0 , 1 \rbrace$. Also consider a single &lt;strong&gt;binary control variable&lt;/strong&gt; $X \in \lbrace 0 , 1 \rbrace$. We also assume that treatment assignment is as good as random, conditionally on $X$. This means that there might be systematic differences between the treatment and control group, however, these differences are fully accounted for by $X$. Formally we write&lt;/p&gt;
&lt;p&gt;$$
\left( Y_i^{(0)}, Y_i^{(1)} \right) \ \perp \ D_i \ | \ X_i
$$&lt;/p&gt;
&lt;p&gt;Where $Y_i^{(d)}$ denotes the potential outcome of individual $i$ when its treatment status is $d$. For example, $Y_i^{(0)}$ indicates the outcome of individual $i$ in case it is not treated. This notation comes from &lt;a href=&#34;https://www.tandfonline.com/doi/abs/10.1198/016214504000001880&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Rubin&amp;rsquo;s potential outcomes framework&lt;/a&gt;. We can write the &lt;strong&gt;individual treatment effect&lt;/strong&gt; of individual $i$ as&lt;/p&gt;
&lt;p&gt;$$
\tau_i = Y_i^{(1)} - Y_i^{(0)}
$$&lt;/p&gt;
&lt;p&gt;In this setting, the &lt;strong&gt;regression of interest&lt;/strong&gt; is&lt;/p&gt;
&lt;p&gt;$$
Y_i = \alpha + \beta D_i + \gamma X_i + u_i
$$&lt;/p&gt;
&lt;p&gt;The coefficient of interest is $\beta$.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.jstor.org/stable/2998558&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Angrist (1998)&lt;/a&gt; shows that &lt;strong&gt;the regression coefficient $\beta$ identifies the average treatment effect&lt;/strong&gt;. In particular, $\beta$ identifies a weighted average of the within-group $x$ average treatment effect $\tau (x)$ with convex weights. In this particular setting, we can write it as&lt;/p&gt;
&lt;p&gt;$$
\beta = \lambda \tau(0) + (1 - \lambda) \tau(1) \qquad \text{where} \qquad \tau (x) = \mathbb E \big[ Y_i^{(1)} - Y_i^{(0)} \ \big| \ X_i = x \big]
$$&lt;/p&gt;
&lt;p&gt;The weights $\lambda$ and $(1-\lambda)$ are given by the within-group treatment variance. Hence, the OLS estimator gives &lt;strong&gt;less weight to groups where we have less treatment variance&lt;/strong&gt;, i.e., where treatment is more imbalanced. Groups where treatment is distributed 50-50 get the most weight.&lt;/p&gt;
&lt;p&gt;$$
\lambda = \frac{ \text{Var} \big(D_i \ \big| \ X_i = 0 \big) \Pr \big(X_i=0 \big)}{\sum_{x \in \lbrace 0 , 1 \rbrace} \text{Var} \big(D_i \ \big| \ X_i = x \big) \Pr \big( X_i=x \big)} \in [0, 1]
$$&lt;/p&gt;
&lt;p&gt;The weights can be derived using the &lt;a href=&#34;https://towardsdatascience.com/59f801eb3299&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Frisch-Waugh-Lowell theorem&lt;/a&gt; to express $\beta_1$ as the OLS coefficient of a univariate regression of $Y$ on $D_{i, \perp X}$, where $D_{i, \perp X}$ are the residuals from regressing $D$ on $X$. If you are not familiar with the Frisch-Waugh-Lowell theorem, I wrote an &lt;a href=&#34;https://towardsdatascience.com/59f801eb3299&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;introductory blog post here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;$$
\beta_1 = \frac{ \mathbb E \big[ D_{i, \perp X} Y_i \big] }{ \mathbb E \big[ D_{i, \perp X}^2 \big] } =
\underbrace{ \frac{\mathbb E \big[ D_{i, \perp X} Y_i(0) \big]}{\mathbb E \big[ D_{i, \perp X}^2 \big]} } _ {=0} + \frac{\mathbb E \big[ D_{i, \perp X} D_i \tau_i \big]}{\mathbb E \big[ D_{i, \perp X}^2 \big]} =
\frac{\mathbb E \big[ \text{Var} (D_i | X_i) \ \tau(X_i) \big]}{\mathbb E \big[ \text{Var}(D_i | X_i) \big]}
$$&lt;/p&gt;
&lt;p&gt;The first term of the central expression disappears because the residual $D_{i, \perp X}$ is by construction &lt;strong&gt;mean independent&lt;/strong&gt; of the control variable $X_i$, i.e.&lt;/p&gt;
&lt;p&gt;$$
\mathbb E \big[ D_{i, \perp X} | X_i \big] = 0
$$&lt;/p&gt;
&lt;p&gt;This mean independence property is crucial to obtain an unbiased estimate and its failure in the multiple-treatment case is the source of the &lt;em&gt;contamination bias&lt;/em&gt;.&lt;/p&gt;
&lt;h3 id=&#34;multiple-treatment-arms&#34;&gt;Multiple Treatment Arms&lt;/h3&gt;
&lt;p&gt;Let&amp;rsquo;s now consider the case of multiple treatment arms, $D \in \lbrace 0, 1, 2 \rbrace$, where $1$ and $2$ indicate two mutually-exclusive treatments. We still assume &lt;strong&gt;conditional ignorability&lt;/strong&gt;, i.e., treatment assignment is as good as random, conditional on $X$.&lt;/p&gt;
&lt;p&gt;$$
\left( Y_i^{(0)}, Y_i^{(1)}, Y_i^{(2)} \right) \ \perp \ D_i \ | \ X_i
$$&lt;/p&gt;
&lt;p&gt;In this case, we have two different &lt;strong&gt;individual treatment effects&lt;/strong&gt;, one per treatment.&lt;/p&gt;
&lt;p&gt;$$
\tau_{i1} = Y_i^{(1)} - Y_i^{(0)} \qquad \text{and} \qquad \tau_{i2} = Y_i^{(2)} - Y_i^{(0)}
$$&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;regression of interest&lt;/strong&gt; is&lt;/p&gt;
&lt;p&gt;$$
Y_i = \alpha + \beta_1 D_{i1} + \beta_2 D_{i2} + \gamma X_i + u_i
$$&lt;/p&gt;
&lt;p&gt;Does the OLS estimator of $\beta_1$ and $\beta_2$ &lt;strong&gt;identify&lt;/strong&gt; an average treatment effect?&lt;/p&gt;
&lt;p&gt;It would be very tempting to say yes. In fact, it looks like not much has changed with respect to the previous setup. We just have one extra treatment, but the potential outcomes are still conditionally independent of it. Where is the &lt;strong&gt;issue&lt;/strong&gt;?&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s concentrate on $\beta_1$ (the same applies to $\beta_2$). As before, can rewrite $\beta_1$ using the &lt;a href=&#34;https://towardsdatascience.com/59f801eb3299&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Frisch-Waugh-Lowell theorem&lt;/a&gt; as the OLS coefficient of a univariate regression of $Y_i$ on $D_{i1, \perp X, D_2}$, where $D_{i1, \perp X, D_2}$ are the residuals from regressing $D_1$ on $D_2$ and $X$.&lt;/p&gt;
&lt;p&gt;$$
\beta_1 = \frac{ \mathbb E \big[D_{i1, \perp X, D_2} Y_i \big] }{ \mathbb E \big[ D_{i1, \perp X, D_2}^2 \big]} = \underbrace{ \frac{ \mathbb E \big[ D_{i1, \perp X, D_2} Y_i(0) \big] }{\mathbb E \big[ D_{i1, \perp X, D_2}^2 \big]} } _ {=0} + \frac{ \mathbb E \big[ D_{i1, \perp X, D_2} D_{i1} \tau_{i1} \big] }{ \mathbb E \big[ D_{i1, \perp X, D_2}^2 \big]} + \color{red}{ \underbrace{ \color{black}{ \frac{ \mathbb E \big[ D_{i1, \perp X, D_2} D_{i2} \tau_{i2} \big] }{ \mathbb E \big[ D_{i1, \perp X, D_2}^2 \big]}} } _ { \neq 0} }
$$&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;problem&lt;/strong&gt; is the last term. Without the last term, we could still write $\beta_1$ as a convex combination of the individual treatment effects. However, the last term biases the estimator by adding a component that depends on &lt;strong&gt;the treatment effect of $D_2$&lt;/strong&gt;, $\tau_2$. Why does this term not disappear?&lt;/p&gt;
&lt;p&gt;The problem is that $D_{i1, \perp X, D_2}$ is not mean independent of $D_{i2}$, i.e.&lt;/p&gt;
&lt;p&gt;$$
\mathbb E \big[ D_{i1, \perp X, D_2} D_{i2} \ \big| \ X_i \big] \neq 0
$$&lt;/p&gt;
&lt;p&gt;The reason lies in the fact that the treatments are &lt;strong&gt;mutually exclusive&lt;/strong&gt;. This implies that when $D_{i1}=1$, $D_{i2}$ must be zero, regardless of the value of $X_i$. Therefore, the last term does not cancel out and it introduces a &lt;strong&gt;contamination bias&lt;/strong&gt;.&lt;/p&gt;
&lt;h3 id=&#34;solution&#34;&gt;Solution&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://www.nber.org/papers/w30108&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Goldsmith-Pinkham, Hull, Kolesár (2022)&lt;/a&gt; show that a &lt;strong&gt;simple estimator&lt;/strong&gt;, first proposed by &lt;a href=&#34;https://www.aeaweb.org/articles?id=10.1257/jel.47.1.5&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Imbens and Wooldridge (2009)&lt;/a&gt;, is able to remove the bias. The procedure is the following.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;De-mean the control variable: $\tilde X = X - \bar X$&lt;/li&gt;
&lt;li&gt;Regress $Y$ on the interaction between the treatment indicators $D$ and the demeaned control variable $\tilde X$&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The OLS estimators of $\beta_1$ and $\beta_2$ are &lt;strong&gt;unbiased&lt;/strong&gt; estimators of the average treatment effects. It also just requires a linear regression. Moreover, this estimator is unbiased also for continuous control variables $X$, not only for a binary one as we have considered so far.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Why&lt;/strong&gt; was this estimator initially proposed by &lt;a href=&#34;https://www.aeaweb.org/articles?id=10.1257/jel.47.1.5&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Imbens and Wooldridge (2009)&lt;/a&gt;? Let&amp;rsquo;s analyze two parts separately: the interaction term between $D$ and $X$ and the fact that $X$ is de-meaned in the interaction term.&lt;/p&gt;
&lt;p&gt;First, the &lt;strong&gt;interaction term&lt;/strong&gt; $D X$ allows us to control for different effects and/or distributions of $X$ across treatment and control group.&lt;/p&gt;
&lt;p&gt;Second, &lt;strong&gt;de-meaning&lt;/strong&gt; $X$ in the interaction term allows us to &lt;strong&gt;interpret&lt;/strong&gt; the estimated coefficient $\hat \beta$ as the average treatment effect. In fact, assume we were estimating the following linear model, where $X$ is &lt;em&gt;not&lt;/em&gt; de-meaned in the interaction term.&lt;/p&gt;
&lt;p&gt;$$
Y_i = \alpha + \beta D_i + \gamma X_i + \delta D_i X_i + u_i
$$&lt;/p&gt;
&lt;p&gt;In this case, the marginal effect of $D$ on $Y$ is $\beta + \delta X_i$ so that the &lt;em&gt;average&lt;/em&gt; marginal effect is $\beta + \delta \bar X$ which is different from $\beta$.&lt;/p&gt;
&lt;p&gt;If instead we use the de-meaned value of $X$ in the interaction term, the marginal effect of $D$ on $Y$ is $\beta + \delta (X_i - \bar X)$ so that the &lt;em&gt;average&lt;/em&gt; marginal effect is $\beta + \delta (\bar X - \bar X) = \beta$. Now we can interpret $\beta$ as the average marginal effect of $D$ on $Y$.&lt;/p&gt;
&lt;h2 id=&#34;simulations&#34;&gt;Simulations&lt;/h2&gt;
&lt;p&gt;In order to better understand both the problem and the solution, let&amp;rsquo;s run some &lt;strong&gt;simulations&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;We run an estimator over different draws from the data generating process &lt;code&gt;dgp_buttons()&lt;/code&gt;. This is only possible with synthetic data and we do not have this luxury in reality. For each sample, we record the estimated coefficient and the corresponding &lt;a href=&#34;https://en.wikipedia.org/wiki/P-value&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;p-value&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def simulate(dgp, estimator, K=1000):
    
    # Initialize coefficients
    results = pd.DataFrame({&#39;Coefficient&#39;: np.zeros(K), &#39;pvalue&#39;: np.zeros(K)})
    
    # Compute coefficients
    for k in range(K):
        df = dgp.generate_data(seed=k)
        results.Coefficient[k] = estimator(df).params[1]
        results.pvalue[k] = estimator(df).pvalues[1]
    
    results[&#39;Significant&#39;] = results[&#39;pvalue&#39;] &amp;lt; 0.05
    return results
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;First, let&amp;rsquo;s try it with the old estimator that regresses &lt;code&gt;revenue&lt;/code&gt; on both &lt;code&gt;group&lt;/code&gt; and &lt;code&gt;mobile&lt;/code&gt; dummy variables.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;ols_estimator = lambda x: smf.ols(&#39;revenue ~ group + mobile&#39;, data=x).fit()
results = simulate(dgp, ols_estimator)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I &lt;strong&gt;plot&lt;/strong&gt; the distribution of the coefficient estimates of &lt;code&gt;button1&lt;/code&gt; over 1000 simulations, highlighting the statistically significant ones at the 5% level. I also highlight the true value of the coefficient, zero, with a vertical dotted bar.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def plot_results(results):
    p_sig = sum(results[&#39;Significant&#39;]) / len(results) * 100
    sns.histplot(data=results, x=&amp;quot;Coefficient&amp;quot;, hue=&amp;quot;Significant&amp;quot;, multiple=&amp;quot;stack&amp;quot;, 
                 palette = [&#39;tab:red&#39;, &#39;tab:green&#39;]);
    plt.axvline(x=0, c=&#39;k&#39;, ls=&#39;--&#39;, label=&#39;truth&#39;)
    plt.title(rf&amp;quot;Estimated $\beta_1$ ({p_sig:.0f}% significant)&amp;quot;);
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plot_results(results)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/contamination_bias_35_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;As we can see, we reject the null hypothesis of no effect of &lt;code&gt;button1&lt;/code&gt; in 45% of the simulations. Since we set a confidence level of 5%, we would have expected at most around 5% of rejections. Our estimator is &lt;strong&gt;biased&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;As we have seen above, the problem is that the estimator is not just a convex combination of the effect of &lt;code&gt;button1&lt;/code&gt; across mobile and desktop users (it&amp;rsquo;s zero for both), but it is &lt;strong&gt;contaminated&lt;/strong&gt; by the effect of &lt;code&gt;button2&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s now try the estimator proposed from &lt;a href=&#34;https://www.aeaweb.org/articles?id=10.1257/jel.47.1.5&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Imbens and Wooldridge (2009)&lt;/a&gt;. First, we need do de-mean our control variable, &lt;code&gt;mobile&lt;/code&gt;. Then, we regress &lt;code&gt;revenue&lt;/code&gt; on the interaction between &lt;code&gt;group&lt;/code&gt; and the de-meaned control variable, &lt;code&gt;mobile_res&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df[&#39;mobile_res&#39;] = df[&#39;mobile&#39;] - np.mean(df[&#39;mobile&#39;])
smf.ols(&#39;revenue ~ group * mobile_res&#39;, data=df).fit().summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
               &lt;td&gt;&lt;/td&gt;                  &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt;                   &lt;td&gt;   11.5773&lt;/td&gt; &lt;td&gt;    0.067&lt;/td&gt; &lt;td&gt;  172.864&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;   11.446&lt;/td&gt; &lt;td&gt;   11.709&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;group[T.button1]&lt;/th&gt;            &lt;td&gt;    0.0281&lt;/td&gt; &lt;td&gt;    0.106&lt;/td&gt; &lt;td&gt;    0.266&lt;/td&gt; &lt;td&gt; 0.790&lt;/td&gt; &lt;td&gt;   -0.180&lt;/td&gt; &lt;td&gt;    0.236&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;group[T.button2]&lt;/th&gt;            &lt;td&gt;   -1.5071&lt;/td&gt; &lt;td&gt;    0.100&lt;/td&gt; &lt;td&gt;  -15.112&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;   -1.703&lt;/td&gt; &lt;td&gt;   -1.311&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;mobile_res&lt;/th&gt;                  &lt;td&gt;    2.9107&lt;/td&gt; &lt;td&gt;    0.134&lt;/td&gt; &lt;td&gt;   21.715&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    2.648&lt;/td&gt; &lt;td&gt;    3.174&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;group[T.button1]:mobile_res&lt;/th&gt; &lt;td&gt;    0.1605&lt;/td&gt; &lt;td&gt;    0.211&lt;/td&gt; &lt;td&gt;    0.760&lt;/td&gt; &lt;td&gt; 0.448&lt;/td&gt; &lt;td&gt;   -0.254&lt;/td&gt; &lt;td&gt;    0.575&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;group[T.button2]:mobile_res&lt;/th&gt; &lt;td&gt;    5.3771&lt;/td&gt; &lt;td&gt;    0.200&lt;/td&gt; &lt;td&gt;   26.905&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    4.985&lt;/td&gt; &lt;td&gt;    5.769&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;The estimated coefficients are now &lt;strong&gt;close to their true values&lt;/strong&gt;. The estimated coefficient for &lt;code&gt;button1&lt;/code&gt; is not significant, while the estimated coefficient for &lt;code&gt;button2&lt;/code&gt; is negative and significant.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s check whether this results holds &lt;strong&gt;across samples&lt;/strong&gt; by running a simulation. We repeat the estimation procedure 1000 times and we plot the distribution of estimated coefficients for &lt;code&gt;button1&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;new_estimator = lambda x: smf.ols(&#39;revenue ~ group * mobile&#39;, data=x).fit()
new_results = simulate(dgp, new_estimator)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plot_results(new_results)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/contamination_bias_40_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Now the distribution of the estimated coefficient for &lt;code&gt;button1&lt;/code&gt; is centered around the true value of zero. Moreover, we reject the null hypothesis of no effect only in 1% of the simulations, consistently with the chosen confidence level of 95%.&lt;/p&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;In this post, we have seen the dangers of running a factor regression model with multiple &lt;em&gt;mutually exclusive&lt;/em&gt; treatment arms and treatment effect heterogeneity across a control variable. In this case, because the treatments are not independent, the regression coefficients are not a convex combination of the within-group average treatment effects, but also capture the treatment effects of the other treatments introducing &lt;strong&gt;contamination bias&lt;/strong&gt;. The solution to the problem is both simple and elegant, requiring just a linear regression.&lt;/p&gt;
&lt;p&gt;However, &lt;strong&gt;the problem is more general&lt;/strong&gt; than this setting and generally concerns every setting in which (all of the following)&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;We have multiple treatments that depend on each other&lt;/li&gt;
&lt;li&gt;We need to condition the analysis on a control variable&lt;/li&gt;
&lt;li&gt;The treatment effects are heterogeneous in the control variable&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Another popular example is the case of the &lt;a href=&#34;https://arxiv.org/abs/2201.01194&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Two-Way Fixed Effects (TWFE) estimator with staggered treatments&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;references&#34;&gt;References&lt;/h3&gt;
&lt;p&gt;[1] J. Angrist, &lt;a href=&#34;https://www.jstor.org/stable/2998558&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Estimating the Labor Market Impact of Voluntary Military Service Using Social Security Data on Military Applicants&lt;/a&gt; (1998), &lt;em&gt;Econometrica&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;[2] D. Rubin, &lt;a href=&#34;https://www.tandfonline.com/doi/abs/10.1198/016214504000001880&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Causal Inference Using Potential Outcomes&lt;/a&gt; (2005), &lt;em&gt;Journal of the American Statistical Association&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;[3] G. Imbens, J. Wooldridge, &lt;a href=&#34;https://www.aeaweb.org/articles?id=10.1257/jel.47.1.5&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Recent Developments in the Econometrics of Program Evaluation&lt;/a&gt; (2009), &lt;em&gt;Journal of Economic Literature&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;[4] P. Goldsmith-Pinkham, P. Hull, M. Kolesár, &lt;a href=&#34;https://www.nber.org/papers/w30108&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Contamination Bias in Linear Regressions&lt;/a&gt; (2022), &lt;em&gt;working paper&lt;/em&gt;.&lt;/p&gt;
&lt;h3 id=&#34;related-articles&#34;&gt;Related Articles&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/344ac1477699&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Understanding Omitted Variable Bias&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/59f801eb3299&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Understanding The Frisch-Waugh-Lovell Theorem&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/b63dc69e3d8c&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;DAGs and Control Variables&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;code&#34;&gt;Code&lt;/h3&gt;
&lt;p&gt;You can find the original Jupyter Notebook here:&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/cbias.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/cbias.ipynb&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Understanding CUPED</title>
      <link>https://matteocourthoud.github.io/post/cuped/</link>
      <pubDate>Mon, 10 Jul 2023 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/post/cuped/</guid>
      <description>&lt;p&gt;&lt;em&gt;An in depth guide to the state-of-the art variance reduction technique in A/B tests&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;During my PhD, I spent a lot of time learning and applying causal inference methods to experimental and observational data. However, I was completely clueless when I first heard of &lt;strong&gt;CUPED&lt;/strong&gt; (Controlled-Experiment using Pre-Experiment Data), a technique to increase the power of randomized controlled trials in A/B tests.&lt;/p&gt;
&lt;p&gt;What really amazed me was the popularity of the algorithm in the industry. CUPED was first introduced by Microsoft researchers &lt;a href=&#34;https://dl.acm.org/doi/abs/10.1145/2433396.2433413&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Deng, Xu, Kohavi, Walker (2013)&lt;/a&gt; and has been widely used in companies such as &lt;a href=&#34;https://www.kdd.org/kdd2016/papers/files/adp0945-xieA.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Netflix&lt;/a&gt;, &lt;a href=&#34;https://booking.ai/995d186fff1d&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Booking&lt;/a&gt;, &lt;a href=&#34;https://research.facebook.com/blog/2020/10/increasing-the-sensitivity-of-a-b-tests-by-utilizing-the-variance-estimates-of-experimental-units/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Meta&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/abs/2112.13299&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Airbnb&lt;/a&gt;, &lt;a href=&#34;https://www.tripadvisor.com/engineering/reducing-a-b-test-measurement-variance-by-30/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;TripAdvisor&lt;/a&gt;, &lt;a href=&#34;https://doordash.engineering/2020/10/07/improving-experiment-capacity-by-4x/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;DoorDash&lt;/a&gt;, &lt;a href=&#34;https://craft.faire.com/how-to-speed-up-your-a-b-test-outlier-capping-and-cuped-8c9df21c76b&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Faire&lt;/a&gt;, and many others. While digging deeper into it, I noticed a similarity with some causal inference methods I was familiar with, such as &lt;a href=&#34;https://diff.healthpolicydatascience.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Difference-in-Differences&lt;/a&gt;. I was curious and decided to dig deeper.&lt;/p&gt;
&lt;p&gt;In this post, I will present CUPED and try to compare it against other causal inference methods.&lt;/p&gt;
&lt;h2 id=&#34;example&#34;&gt;Example&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s assume we are a firm that is testing an &lt;strong&gt;ad campaign&lt;/strong&gt; and we are interested in understanding whether it increases revenue or not. We randomly split a set of users into a treatment and control group and we show the ad campaign to the treatment group. Differently from the standard A/B test setting, assume we observe users also before the test.&lt;/p&gt;
&lt;p&gt;We can now generate the simulated data, using the data generating process &lt;code&gt;dgp_cuped()&lt;/code&gt; from &lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/src/dgp.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;src.dgp&lt;/code&gt;&lt;/a&gt;. I also import some plotting functions and libraries from &lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/src/utils.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;src.utils&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;%matplotlib inline
%config InlineBackend.figure_format = &#39;retina&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from src.utils import *
from src.dgp import dgp_cuped
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df = dgp_cuped().generate_data()
df.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;i&lt;/th&gt;
      &lt;th&gt;ad_campaign&lt;/th&gt;
      &lt;th&gt;revenue0&lt;/th&gt;
      &lt;th&gt;revenue1&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;5.315635&lt;/td&gt;
      &lt;td&gt;8.359304&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;2.977799&lt;/td&gt;
      &lt;td&gt;7.751485&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;4.693796&lt;/td&gt;
      &lt;td&gt;9.025253&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;5.827975&lt;/td&gt;
      &lt;td&gt;8.540667&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;5.230095&lt;/td&gt;
      &lt;td&gt;8.910165&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;We have informations on 1000 individuals indexed by &lt;code&gt;i&lt;/code&gt; for whom we observe the revenue generated pre and post treatment, &lt;code&gt;revenue0&lt;/code&gt; and &lt;code&gt;revenue1&lt;/code&gt; respectively, and whether they have been exposed to the &lt;code&gt;ad_campaign&lt;/code&gt;.&lt;/p&gt;
&lt;h3 id=&#34;difference-in-means&#34;&gt;Difference in Means&lt;/h3&gt;
&lt;p&gt;In randomized experiments or A/B tests, &lt;strong&gt;randomization&lt;/strong&gt; allows us to estimate the average treatment effect using a simple difference in means. We can just compare the average outcome post-treatment $Y_1$ (&lt;code&gt;revenue&lt;/code&gt;) across control and treated units and randomization guarantees that this difference is due to the treatment alone, in expectation.&lt;/p&gt;
&lt;p&gt;$$
\widehat {ATE}^{simple} = \bar Y_{t=1, d=1} - \bar Y_{t=1, d=0}
$$&lt;/p&gt;
&lt;p&gt;Where the bar indicates the average over individuals. In our case, we compute the average revenue post ad campaign in the treatment group, minus the average revenue post ad campaign in the control group.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;np.mean(df.loc[df.ad_campaign==True, &#39;revenue1&#39;]) - np.mean(df.loc[df.ad_campaign==False, &#39;revenue1&#39;])
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;1.7914301325347406
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The estimated treatment effect is 1.79, very close to the &lt;strong&gt;true value&lt;/strong&gt; of 2. We can obtain the same estimate by regressing the post-treatment outcome &lt;code&gt;revenue1&lt;/code&gt; on the treatment indicator &lt;code&gt;ad_campaign&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;$$
Y_{i, t=1} = \alpha + \beta D_i + \varepsilon_i
$$&lt;/p&gt;
&lt;p&gt;Where $\beta$ is the coefficient of interest.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;smf.ols(&#39;revenue1 ~ ad_campaign&#39;, data=df).fit().summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
       &lt;td&gt;&lt;/td&gt;          &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt;   &lt;td&gt;    8.2995&lt;/td&gt; &lt;td&gt;    0.211&lt;/td&gt; &lt;td&gt;   39.398&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    7.881&lt;/td&gt; &lt;td&gt;    8.718&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;ad_campaign&lt;/th&gt; &lt;td&gt;    1.7914&lt;/td&gt; &lt;td&gt;    0.301&lt;/td&gt; &lt;td&gt;    5.953&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    1.194&lt;/td&gt; &lt;td&gt;    2.389&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;This estimator is &lt;strong&gt;unbiased&lt;/strong&gt;, which means it delivers the correct estimate, on average. However, it can still be improved: we could &lt;strong&gt;decrease its variance&lt;/strong&gt;. Decreasing the variance of an estimator is extremely important since it allows us to&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;detect smaller effects&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;detect the same effect, but with a &lt;strong&gt;smaller sample size&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In general, an estimator with a smaller variance allows us to run tests with higher &lt;a href=&#34;https://en.wikipedia.org/wiki/Power_of_a_test&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;power&lt;/strong&gt;&lt;/a&gt;, i.e. ability to detect smaller effects.&lt;/p&gt;
&lt;p&gt;Can we improve the power of our AB test? Yes, with CUPED (among other methods).&lt;/p&gt;
&lt;h2 id=&#34;cuped&#34;&gt;CUPED&lt;/h2&gt;
&lt;p&gt;The &lt;strong&gt;idea&lt;/strong&gt; of CUPED is the following. Suppose you are running an AB test and $Y$ is the outcome of interest (&lt;code&gt;revenue&lt;/code&gt; in our example) and the binary variable $D$ indicates whether a single individual has been treated or not (&lt;code&gt;ad_campaign&lt;/code&gt; in our example).&lt;/p&gt;
&lt;p&gt;Suppose you have access to another random variable $X$ which is &lt;strong&gt;not affected&lt;/strong&gt; by the treatment and has known expectation $\mathbb E[X]$. Then define&lt;/p&gt;
&lt;p&gt;$$
\hat Y^{cuped}_{1} = \bar Y_1 - \theta \bar X + \theta \mathbb E [X]
$$&lt;/p&gt;
&lt;p&gt;where $\theta$ is a scalar. This estimator is an &lt;strong&gt;unbiased&lt;/strong&gt; estimator for $\mathbb E[Y]$ since in expectation the two last terms cancel out. However, the variance of $\hat Y^{cuped}_{1}$ is&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
\text{Var} \left( \hat Y^{cuped}_{1} \right) &amp;amp;= \text{Var} \left( \bar Y_1 - \theta \bar X + \theta \mathbb E [X] \right) = \newline
&amp;amp;= \text{Var} \left( Y_1 - \theta X \right) / n = \newline
&amp;amp;= \Big( \text{Var} (Y_1) + \theta^2 \text{Var} (X) - 2 \theta \text{Cov} (X,Y) \Big) / n
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;Note that the variance of $\hat Y^{cuped}_{1}$ is minimized for&lt;/p&gt;
&lt;p&gt;$$
\theta^* = \frac{\text{Cov} (X,Y)}{\text{Var} (X)}
$$&lt;/p&gt;
&lt;p&gt;Which is the &lt;strong&gt;OLS&lt;/strong&gt; estimator of a linear regression of $Y$ on $X$. Substituting $\theta^*$ into the formula of the variance of $\hat Y^{cuped}_{1}$ we obtain&lt;/p&gt;
&lt;p&gt;$$
\text{Var} \left( \hat Y^{cuped}_{1} \right) = \text{Var} (\bar Y) (1 - \rho^2)
$$&lt;/p&gt;
&lt;p&gt;where $\rho$ is the &lt;strong&gt;correlation&lt;/strong&gt; between $Y$ and $X$. Therefore, the higher the correlation between $Y$ and $X$, the higher the variance reduction of CUPED.&lt;/p&gt;
&lt;p&gt;We can then &lt;strong&gt;estimate the average treatment effect&lt;/strong&gt;as the average difference in the transformed outcome between the control and treatment group.&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
\widehat {ATE}^{cuped} &amp;amp;= \hat Y^{cuped}&lt;em&gt;{1} (D=1) - \hat Y^{cuped}&lt;/em&gt;{1}(D=0) = \newline
&amp;amp;= \big( \bar Y_1 - \theta \bar X + \theta \mathbb E [X] \ \big| \ D = 1 \big) - \big( \bar Y_1 - \theta \bar X + \theta \mathbb E [X] \ \big| \ D = 0 \big) = \newline
&amp;amp;= \big( \bar Y_1 - \theta \bar X \ \big| \ D = 1 \big) - \big( \bar Y_1 - \theta \bar X \ \big| \ D = 0 \big)
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;Note that $\mathbb E[X]$ cancels out when taking the difference. Therefore, it is sufficient to compute&lt;/p&gt;
&lt;p&gt;$$
\hat Y_{cuped,1}&amp;rsquo; = \bar Y_1 - \theta \bar X
$$&lt;/p&gt;
&lt;p&gt;This is not an unbiased estimator of $\mathbb E[Y]$ but still delivers an unbiased estimator of the average treatment effect.&lt;/p&gt;
&lt;h3 id=&#34;optimal-x&#34;&gt;Optimal X&lt;/h3&gt;
&lt;p&gt;What is the &lt;strong&gt;optimal choice&lt;/strong&gt; for the control variable $X$?&lt;/p&gt;
&lt;p&gt;We know that $X$ should have the following &lt;strong&gt;properties&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;not affected by the treatment&lt;/li&gt;
&lt;li&gt;as correlated with $Y_1$ as possible&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The authors of the paper suggest using the &lt;strong&gt;pre-treatment outcome&lt;/strong&gt; $Y_{0}$ since it gives the most reduction in variance in practice.&lt;/p&gt;
&lt;p&gt;Therefore, &lt;strong&gt;in practice&lt;/strong&gt;, we can compute the CUPED estimate of the average treatment effect as follows:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Regress $Y_1$ on $Y_0$ and estimate $\hat \theta$&lt;/li&gt;
&lt;li&gt;Compute $\hat Y^{cuped}_{1} = \bar Y_1 - \hat \theta \bar Y_0$&lt;/li&gt;
&lt;li&gt;Compute the difference of $\hat Y^{cuped}_{1}$ between treatment and control group&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Equivalently, we can compute $\hat Y^{cuped}_{1}$ at the individual level and then regress it on the treatment dummy variable $D$.&lt;/p&gt;
&lt;h3 id=&#34;back-to-the-data&#34;&gt;Back To The Data&lt;/h3&gt;
&lt;p&gt;Let&amp;rsquo;s compute the CUPED estimate for the treatment effect, one step at the time. First, let&amp;rsquo;s estimate $\theta$.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;theta = smf.ols(&#39;revenue1 ~ revenue0&#39;, data=df).fit().params[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we can compute the transformed outcome $\hat Y^{cuped}_{1}$.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df[&#39;revenue1_cuped&#39;] = df[&#39;revenue1&#39;] - theta * (df[&#39;revenue0&#39;] - np.mean(df[&#39;revenue0&#39;]))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Lastly, we estimate the treatment effect as a difference in means, with the transformed outcome $\hat Y^{cuped}_{1}$.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;smf.ols(&#39;revenue1_cuped ~ ad_campaign&#39;, data=df).fit().summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
       &lt;td&gt;&lt;/td&gt;          &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt;   &lt;td&gt;    8.2259&lt;/td&gt; &lt;td&gt;    0.143&lt;/td&gt; &lt;td&gt;   57.677&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    7.943&lt;/td&gt; &lt;td&gt;    8.509&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;ad_campaign&lt;/th&gt; &lt;td&gt;    1.9415&lt;/td&gt; &lt;td&gt;    0.204&lt;/td&gt; &lt;td&gt;    9.529&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    1.537&lt;/td&gt; &lt;td&gt;    2.346&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;The standard error is 33% smaller!&lt;/p&gt;
&lt;h3 id=&#34;equivalent-formulation&#34;&gt;Equivalent Formulation&lt;/h3&gt;
&lt;p&gt;An alternative but algebraically &lt;strong&gt;equivalent&lt;/strong&gt; way of obtaining the CUPED estimate is the folowing&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Regress $Y_1$ on $Y_0$ and compute the residuals $\tilde Y_1$&lt;/li&gt;
&lt;li&gt;Compute $\hat Y^{cuped}_{1} = \tilde Y_1 + \bar Y_1$&lt;/li&gt;
&lt;li&gt;Compute the difference of $\hat Y^{cuped}_{1}$ between treatment and control group&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Step (3) is the same as before but (1) and (2) are different. This procedure is called &lt;strong&gt;partialling out&lt;/strong&gt; and the algebraic equivalence is guaranteed by the &lt;a href=&#34;https://towardsdatascience.com/59f801eb3299&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Frisch-Waugh-Lowell Theorem&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s check if we indeed obtain the same result.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df[&#39;revenue1_tilde&#39;] = smf.ols(&#39;revenue1 ~ revenue0&#39;, data=df).fit().resid + np.mean(df[&#39;revenue1&#39;])
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;smf.ols(&#39;revenue1_tilde ~ ad_campaign&#39;, data=df).fit().summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
       &lt;td&gt;&lt;/td&gt;          &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt;   &lt;td&gt;    8.2259&lt;/td&gt; &lt;td&gt;    0.143&lt;/td&gt; &lt;td&gt;   57.677&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    7.943&lt;/td&gt; &lt;td&gt;    8.509&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;ad_campaign&lt;/th&gt; &lt;td&gt;    1.9415&lt;/td&gt; &lt;td&gt;    0.204&lt;/td&gt; &lt;td&gt;    9.529&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    1.537&lt;/td&gt; &lt;td&gt;    2.346&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;Yes! The regression table is exactly identical.&lt;/p&gt;
&lt;h2 id=&#34;cuped-vs-other&#34;&gt;CUPED vs Other&lt;/h2&gt;
&lt;p&gt;CUPED seems to be a very powerful procedure but it is remindful of at least a couple of other methods.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Autoregression or regression with control variables&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://diff.healthpolicydatascience.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Difference-in-Differences&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Are these methods the same or is there a difference? Let&amp;rsquo;s check.&lt;/p&gt;
&lt;h3 id=&#34;autoregression&#34;&gt;Autoregression&lt;/h3&gt;
&lt;p&gt;The &lt;strong&gt;first question&lt;/strong&gt; that came to my mind when I first saw CUPED was &amp;ldquo;&lt;em&gt;is CUPED just the simple difference with an additional control variable?&lt;/em&gt;&amp;rdquo;. Or equivalently, is CUPED equivalent to running the following regression&lt;/p&gt;
&lt;p&gt;$$
Y_{i, t=1} = \alpha + \beta D_i + \gamma Y_{i, t=0} + \varepsilon_i
$$&lt;/p&gt;
&lt;p&gt;and estimating $\gamma$ via least squares?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;smf.ols(&#39;revenue1 ~ revenue0 + ad_campaign&#39;, data=df).fit().summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
       &lt;td&gt;&lt;/td&gt;          &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt;   &lt;td&gt;    1.9939&lt;/td&gt; &lt;td&gt;    0.603&lt;/td&gt; &lt;td&gt;    3.304&lt;/td&gt; &lt;td&gt; 0.001&lt;/td&gt; &lt;td&gt;    0.796&lt;/td&gt; &lt;td&gt;    3.192&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;revenue0&lt;/th&gt;    &lt;td&gt;    1.2249&lt;/td&gt; &lt;td&gt;    0.114&lt;/td&gt; &lt;td&gt;   10.755&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    0.999&lt;/td&gt; &lt;td&gt;    1.451&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;ad_campaign&lt;/th&gt; &lt;td&gt;    1.9519&lt;/td&gt; &lt;td&gt;    0.205&lt;/td&gt; &lt;td&gt;    9.529&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    1.545&lt;/td&gt; &lt;td&gt;    2.358&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;The estimated coefficient is very similar to the one we obtained with CUPED and also the standard error is very close. However, they are not exactly the same.&lt;/p&gt;
&lt;p&gt;If you are familiar with the &lt;a href=&#34;https://towardsdatascience.com/59f801eb3299&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Frisch-Waugh-Lowell Theorem&lt;/a&gt;, you might wonder why the two procedures are &lt;strong&gt;not equivalent&lt;/strong&gt;. The reason is that with CUPED we are partialling out only $Y$, while the FWL theorem holds when we are partialling out either X or both X and Y.&lt;/p&gt;
&lt;h3 id=&#34;diff-in-diffs&#34;&gt;Diff-in-Diffs&lt;/h3&gt;
&lt;p&gt;The &lt;strong&gt;second question&lt;/strong&gt; that came to my mind was &amp;ldquo;i&lt;em&gt;s CUPED just difference-in-differences?&lt;/em&gt;&amp;rdquo;. &lt;a href=&#34;https://diff.healthpolicydatascience.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Difference-in-Differences&lt;/a&gt; (or diff-in-diffs, or DiD) is an estimator that computes the treatment effect as a &lt;strong&gt;double-difference&lt;/strong&gt; instead of a single one: pre-post and treatment-control instead of just treatment-control.&lt;/p&gt;
&lt;p&gt;$$
\widehat {ATE}^{DiD} = \big( \bar Y_{t=1, d=1} - \bar Y_{t=1, d=0} \big) - \big( \bar Y_{t=0, d=1} - \bar Y_{t=0, d=0} \big)
$$&lt;/p&gt;
&lt;p&gt;This method was initially introduced in the 19th century to estimate the causes of a Cholera epidemic in London. The main advantage of diff-in-diff is that it allows to estimate the average treatment effect also when randomization is not perfect and the treatment and control group are not comparable. The &lt;strong&gt;key assumption&lt;/strong&gt; that is needed is that these difference between the treatment and control group is constant over time. By taking a double difference, we difference it out.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s check how diff-in-diff works empirically. The most common way to compute the diff-in-diff estiamtor is to first reshape the data in a &lt;strong&gt;long format&lt;/strong&gt; or &lt;strong&gt;panel format&lt;/strong&gt; (one observation is an individual $i$ at time period $t$) and then to regress the outcome $Y$ on the full interaction between the post-treatment dummy $T$ and the treatment dummy $D$.&lt;/p&gt;
&lt;p&gt;$$
Y_{i,t} = \alpha + \beta D_i + \gamma \mathbb I (t=1) + \delta D_i * \mathbb I (t=1) + \varepsilon_{i,t}
$$&lt;/p&gt;
&lt;p&gt;The estimator of the average treatment effect is the coefficient of the interaction coefficient, $\delta$.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_long = pd.wide_to_long(df, stubnames=&#39;revenue&#39;, i=&#39;i&#39;, j=&#39;t&#39;).reset_index()
df_long.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;i&lt;/th&gt;
      &lt;th&gt;t&lt;/th&gt;
      &lt;th&gt;revenue1_tilde&lt;/th&gt;
      &lt;th&gt;ad_campaign&lt;/th&gt;
      &lt;th&gt;revenue1_cuped&lt;/th&gt;
      &lt;th&gt;revenue&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;8.093744&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;8.093744&lt;/td&gt;
      &lt;td&gt;5.315635&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;10.164644&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;10.164644&lt;/td&gt;
      &lt;td&gt;2.977799&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;9.472203&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;9.472203&lt;/td&gt;
      &lt;td&gt;4.693796&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;7.688063&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;7.688063&lt;/td&gt;
      &lt;td&gt;5.827975&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;8.742618&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;8.742618&lt;/td&gt;
      &lt;td&gt;5.230095&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;The long dataset is now indexed by individuals $i$ and time $t$. We can now run the diff-in-diffs regression.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;smf.ols(&#39;revenue ~ t * ad_campaign&#39;, data=df_long).fit().summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
        &lt;td&gt;&lt;/td&gt;           &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt;     &lt;td&gt;    5.1481&lt;/td&gt; &lt;td&gt;    0.174&lt;/td&gt; &lt;td&gt;   29.608&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    4.805&lt;/td&gt; &lt;td&gt;    5.491&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;t&lt;/th&gt;             &lt;td&gt;    3.1514&lt;/td&gt; &lt;td&gt;    0.246&lt;/td&gt; &lt;td&gt;   12.816&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    2.666&lt;/td&gt; &lt;td&gt;    3.636&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;ad_campaign&lt;/th&gt;   &lt;td&gt;   -0.1310&lt;/td&gt; &lt;td&gt;    0.248&lt;/td&gt; &lt;td&gt;   -0.527&lt;/td&gt; &lt;td&gt; 0.599&lt;/td&gt; &lt;td&gt;   -0.621&lt;/td&gt; &lt;td&gt;    0.359&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;t:ad_campaign&lt;/th&gt; &lt;td&gt;    1.9224&lt;/td&gt; &lt;td&gt;    0.351&lt;/td&gt; &lt;td&gt;    5.473&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    1.230&lt;/td&gt; &lt;td&gt;    2.615&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;The estimated coefficient is close to the true value, 2, but the standard errors are bigger than the ones obtained with all other methods (0.35 &amp;raquo; 0.2). What did we miss? We didn&amp;rsquo;t &lt;strong&gt;cluster the standard errors&lt;/strong&gt;!&lt;/p&gt;
&lt;p&gt;I won&amp;rsquo;t go in detail here on what standard error clustering means, but the intuition is the following. The &lt;code&gt;statsmodels&lt;/code&gt; package by default computes the standard errors assuming that outcomes are &lt;strong&gt;independent&lt;/strong&gt; across observations. This assumption is unlikely to be true in this setting where we observe individuals over time and we are trying to exploit this information. Clustering allows for &lt;strong&gt;correlation&lt;/strong&gt; of the outcome variable within clusters. In our case, it makes sense (even without knowing the data generating process) to cluster the standard errors at the individual levels, allowing the outcome to be correlated over time for an individual $i$.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;smf.ols(&#39;revenue ~ t * ad_campaign&#39;, data=df_long)\
    .fit(cov_type=&#39;cluster&#39;, cov_kwds={&#39;groups&#39;: df_long[&#39;i&#39;]})\
    .summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
        &lt;td&gt;&lt;/td&gt;           &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;z&lt;/th&gt;      &lt;th&gt;P&gt;|z|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt;     &lt;td&gt;    5.1481&lt;/td&gt; &lt;td&gt;    0.139&lt;/td&gt; &lt;td&gt;   37.056&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    4.876&lt;/td&gt; &lt;td&gt;    5.420&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;t&lt;/th&gt;             &lt;td&gt;    3.1514&lt;/td&gt; &lt;td&gt;    0.128&lt;/td&gt; &lt;td&gt;   24.707&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    2.901&lt;/td&gt; &lt;td&gt;    3.401&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;ad_campaign&lt;/th&gt;   &lt;td&gt;   -0.1310&lt;/td&gt; &lt;td&gt;    0.181&lt;/td&gt; &lt;td&gt;   -0.724&lt;/td&gt; &lt;td&gt; 0.469&lt;/td&gt; &lt;td&gt;   -0.486&lt;/td&gt; &lt;td&gt;    0.224&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;t:ad_campaign&lt;/th&gt; &lt;td&gt;    1.9224&lt;/td&gt; &lt;td&gt;    0.209&lt;/td&gt; &lt;td&gt;    9.208&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    1.513&lt;/td&gt; &lt;td&gt;    2.332&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;Clustering standard errors at the individual level we obtain standard errors that are comparable to the previous estimates ($\sim 0.2$).&lt;/p&gt;
&lt;p&gt;Note that diff-in-diffs is &lt;strong&gt;equivalent to CUPED&lt;/strong&gt; when we assume the CUPED coefficient $\theta=1$.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df[&#39;revenue1_cuped2&#39;] = df[&#39;revenue1&#39;] - 1 * (df[&#39;revenue0&#39;] - np.mean(df[&#39;revenue0&#39;]))
smf.ols(&#39;revenue1_cuped2 ~ ad_campaign&#39;, data=df).fit().summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
       &lt;td&gt;&lt;/td&gt;          &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt;   &lt;td&gt;    8.2353&lt;/td&gt; &lt;td&gt;    0.145&lt;/td&gt; &lt;td&gt;   56.756&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    7.947&lt;/td&gt; &lt;td&gt;    8.523&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;ad_campaign&lt;/th&gt; &lt;td&gt;    1.9224&lt;/td&gt; &lt;td&gt;    0.207&lt;/td&gt; &lt;td&gt;    9.274&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    1.511&lt;/td&gt; &lt;td&gt;    2.334&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;Indeed, we obtain the same exact coefficient and almost identical standard errors!&lt;/p&gt;
&lt;h3 id=&#34;comparison&#34;&gt;Comparison&lt;/h3&gt;
&lt;p&gt;Which method is better? From what we have seen so far, all methods seem to deliver an accurate estimate, but the simple difference has a larger standard deviation.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s now compare all the methods we have seen so far via &lt;strong&gt;simulation&lt;/strong&gt;. We simulate the data generating process &lt;code&gt;dgp_cuped()&lt;/code&gt; 1000 times and we save the estimated coefficient of the following methods:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Simple difference&lt;/li&gt;
&lt;li&gt;Autoregression&lt;/li&gt;
&lt;li&gt;Diff-in-diffs&lt;/li&gt;
&lt;li&gt;CUPED&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def simulate(dgp, K=300, x=&#39;revenue0&#39;):
    
    # Initialize coefficients
    results = pd.DataFrame(columns=[&#39;k&#39;, &#39;Estimator&#39;, &#39;Estimate&#39;])
    
    # Compute coefficients
    for k in range(K):
        temp = pd.DataFrame({&#39;k&#39;: [k] * 4, 
                             &#39;Estimator&#39;: [&#39;1. Diff &#39;, &#39;2. Areg &#39;, &#39;3. DiD  &#39;, &#39;4. CUPED&#39;], 
                             &#39;Estimate&#39;: [0] * 4})
        
        # Draw data
        df = dgp.generate_data(seed=k)

        # Single diff
        temp[&#39;Estimate&#39;][0] = smf.ols(&#39;revenue1 ~ ad_campaign&#39;, data=df).fit().params[1]
        
        # Autoregression
        temp[&#39;Estimate&#39;][1] = smf.ols(f&#39;revenue1 ~ ad_campaign + {x}&#39;, data=df).fit().params[1]
        
        # Double diff
        df_long = pd.wide_to_long(df.dropna(), stubnames=&#39;revenue&#39;, i=&#39;i&#39;, j=&#39;t&#39;).reset_index()
        temp[&#39;Estimate&#39;][2] = smf.ols(&#39;revenue ~ ad_campaign * t&#39;, data=df_long)\
            .fit(cov_type=&#39;cluster&#39;, cov_kwds={&#39;groups&#39;: df_long[&#39;i&#39;]}).params[3]
        
        # Cuped
        df[&#39;revenue1_tilde&#39;] = smf.ols(f&#39;revenue1 ~ {x}&#39;, data=df).fit().resid + np.mean(df[&#39;revenue1&#39;])
        temp[&#39;Estimate&#39;][3] = smf.ols(&#39;revenue1_tilde ~ ad_campaign&#39;, data=df).fit().params[1]
                
        # Combine estimates
        results = pd.concat((results, temp))
    
    return results.reset_index(drop=True)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;results = simulate(dgp=dgp_cuped())
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s plot the distribution of the estimated parameters.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sns.kdeplot(data=results, x=&amp;quot;Estimate&amp;quot;, hue=&amp;quot;Estimator&amp;quot;);
plt.axvline(x=2, c=&#39;k&#39;, ls=&#39;--&#39;);
plt.title(&#39;Simulated Distributions&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/cuped_53_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;We can also tabulate the simulated mean and standard deviation of each estimator.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;results.groupby(&#39;Estimator&#39;).agg(mean=(&amp;quot;Estimate&amp;quot;, &amp;quot;mean&amp;quot;), std=(&amp;quot;Estimate&amp;quot;, &amp;quot;std&amp;quot;))
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;mean&lt;/th&gt;
      &lt;th&gt;std&lt;/th&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;Estimator&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;1. Diff&lt;/th&gt;
      &lt;td&gt;1.999626&lt;/td&gt;
      &lt;td&gt;0.291497&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2. Areg&lt;/th&gt;
      &lt;td&gt;2.034145&lt;/td&gt;
      &lt;td&gt;1.063968&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3. DiD&lt;/th&gt;
      &lt;td&gt;1.993494&lt;/td&gt;
      &lt;td&gt;0.197638&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4. CUPED&lt;/th&gt;
      &lt;td&gt;1.971853&lt;/td&gt;
      &lt;td&gt;0.198145&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;All estimators seem &lt;strong&gt;unbiased&lt;/strong&gt;: the average values are all close to the true value of 2. Moreover, all estimators have a very similar standard deviation, apart from the single-difference estimator!&lt;/p&gt;
&lt;h3 id=&#34;always-identical&#34;&gt;Always Identical?&lt;/h3&gt;
&lt;p&gt;Are the estimators always identical, or is there some difference among them?&lt;/p&gt;
&lt;p&gt;We could check many different departures from the original data generating process. For simplicity, I will consider only one here: &lt;strong&gt;imperfect randomization&lt;/strong&gt;. Other tweaks of the data generating process that I considered are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;pre-treatment missing values&lt;/li&gt;
&lt;li&gt;additional covariates / control variables&lt;/li&gt;
&lt;li&gt;multiple pre-treatment periods&lt;/li&gt;
&lt;li&gt;heterogeneous treatment effects&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;and combinations of them. However, I found imperfect randomization to be the most informative example.&lt;/p&gt;
&lt;p&gt;Suppose now that &lt;strong&gt;randomization was not perfect&lt;/strong&gt; and two groups are not identical. In particular, if the data generating process is&lt;/p&gt;
&lt;p&gt;$$
Y_{i,t} = \alpha + \beta D_i + \gamma \mathbb I (t=1) + \delta D_i * \mathbb I (t=1) + u_i + \varepsilon_{i,t}
$$&lt;/p&gt;
&lt;p&gt;assume that $\beta \neq 0$.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;results_beta1 = simulate(dgp=dgp_cuped(beta=1))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s plot the distribution of the estimated parameters.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sns.kdeplot(data=results_beta1, x=&amp;quot;Estimate&amp;quot;, hue=&amp;quot;Estimator&amp;quot;);
plt.axvline(x=2, c=&#39;k&#39;, ls=&#39;--&#39;);
plt.title(&#39;Simulated Distributions&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/cuped_61_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;results_beta1.groupby(&#39;Estimator&#39;).agg(mean=(&amp;quot;Estimate&amp;quot;, &amp;quot;mean&amp;quot;), std=(&amp;quot;Estimate&amp;quot;, &amp;quot;std&amp;quot;))
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;mean&lt;/th&gt;
      &lt;th&gt;std&lt;/th&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;Estimator&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;1. Diff&lt;/th&gt;
      &lt;td&gt;2.999626&lt;/td&gt;
      &lt;td&gt;0.291497&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2. Areg&lt;/th&gt;
      &lt;td&gt;1.991508&lt;/td&gt;
      &lt;td&gt;0.227065&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3. DiD&lt;/th&gt;
      &lt;td&gt;1.993494&lt;/td&gt;
      &lt;td&gt;0.197638&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4. CUPED&lt;/th&gt;
      &lt;td&gt;1.577712&lt;/td&gt;
      &lt;td&gt;0.221448&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;With imperfect treatment assignment, both difference-in-differences and autoregression are unbiased for the true treatment effect, however diff-in-diffs is more efficient. Both CUPED and simple difference are &lt;strong&gt;biased&lt;/strong&gt; instead. Why?&lt;/p&gt;
&lt;p&gt;Diff-in-diffs explicily controls for &lt;strong&gt;systematic differences&lt;/strong&gt; between treatment and control group that are &lt;strong&gt;constant over time&lt;/strong&gt;. This is exactly what this estimator was built for. Autoregression performs some sort of matching on the additional covariate, $Y_{t=0}$, effectively controlling for these systematic differences as well, but less efficiently (if you want to know more, I wrote related posts on control variables &lt;a href=&#34;https://towardsdatascience.com/b63dc69e3d8c&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt; and &lt;a href=&#34;https://towardsdatascience.com/58b63d25d2ef&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;). CUPED controls for persistent heterogeneity at the individual level, but not at the treatment assignment level. Lastly, the simple difference estimator does not control for anything.&lt;/p&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;In this post, I have analyzed an estimator of average treatment effects in AB testing, very popular in the industry: CUPED. The key idea is that, by exploiting pre-treatment data, CUPED can &lt;strong&gt;achieve a lower variance&lt;/strong&gt; by controlling for individual-level variation that is persistent over time. We have also seen that CUPED is closely related but not equivalent to autoregression and difference-in-differences. The differences among the methods clearly emerge when we have imperfect randomization.&lt;/p&gt;
&lt;p&gt;An interesting avenue of future research is what happens when we have &lt;strong&gt;a lot of pre-treatment information&lt;/strong&gt;, either in terms of time periods or observable characteristics. Scientists from Meta, &lt;a href=&#34;https://proceedings.neurips.cc/paper/2021/hash/488b084119a1c7a4950f00706ec7ea16-Abstract.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Guo, Coey, Konutgan, Li, Schoener, Goldman (2021)&lt;/a&gt;, have analyzed this problem in a very recent paper that exploits machine learning techniques to efficiently use this extra information. This approach is closely related to the &lt;a href=&#34;https://academic.oup.com/ectj/article/21/1/C1/5056401&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Double/Debiased Machine Learning&lt;/a&gt; literature. If you are interested, I wrote two articles on the topic (&lt;a href=&#34;https://towardsdatascience.com/eb767a59975b&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;part 1&lt;/a&gt; and &lt;a href=&#34;https://towardsdatascience.com/bf990720a0b2&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;part 2&lt;/a&gt;) and I might write more in the future.&lt;/p&gt;
&lt;h3 id=&#34;references&#34;&gt;References&lt;/h3&gt;
&lt;p&gt;[1] A. Deng, Y. Xu, R. Kohavi, T. Walker, &lt;a href=&#34;https://dl.acm.org/doi/abs/10.1145/2433396.2433413&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Improving the Sensitivity of Online Controlled Experiments by Utilizing Pre-Experiment Data&lt;/a&gt; (2013), &lt;em&gt;WSDM&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;[2] H. Xir, J. Aurisset, &lt;a href=&#34;https://dl.acm.org/doi/abs/10.1145/2939672.2939733&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Improving the sensitivity of online controlled experiments: Case studies at Netflix&lt;/a&gt; (2013), &lt;em&gt;ACM SIGKDD&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;[3] Y. Guo, D. Coey, M. Konutgan, W. Li, C. Schoener, M. Goldman, &lt;a href=&#34;https://proceedings.neurips.cc/paper/2021/hash/488b084119a1c7a4950f00706ec7ea16-Abstract.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Machine Learning for Variance Reduction in Online Experiments&lt;/a&gt; (2021), &lt;em&gt;NeurIPS&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;[4] V. Chernozhukov, D. Chetverikov, M. Demirer, E. Duflo, C. Hansen, W. Newey, J. Robins, &lt;a href=&#34;https://academic.oup.com/ectj/article/21/1/C1/5056401&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Double/debiased machine learning for treatment and structural parameters&lt;/a&gt; (2018), &lt;em&gt;The Econometrics Journal&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;[5] M. Bertrand, E. Duflo, S. Mullainathan, &lt;a href=&#34;https://academic.oup.com/qje/article/119/1/249/1876068&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;How Much Should We Trust Differences-In-Differences Estimates?&lt;/a&gt; (2012), &lt;em&gt;The Quarterly Journal of Economics&lt;/em&gt;.&lt;/p&gt;
&lt;h3 id=&#34;related-articles&#34;&gt;Related Articles&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/eb767a59975b&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Double Debiased Machine Learning (part 1)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/bf990720a0b2&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Double Debiased Machine Learning (part 2)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/59f801eb3299&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Understanding The Frisch-Waugh-Lovell Theorem&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/58b63d25d2ef&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Understanding Contamination Bias&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/b63dc69e3d8c&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;DAGs and Control Variables&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;code&#34;&gt;Code&lt;/h3&gt;
&lt;p&gt;You can find the original Jupyter Notebook here:&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/cuped.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/cuped.ipynb&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Understanding Meta Learners</title>
      <link>https://matteocourthoud.github.io/post/meta_learners/</link>
      <pubDate>Mon, 10 Jul 2023 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/post/meta_learners/</guid>
      <description>&lt;p&gt;&lt;em&gt;How to use machine learning to estimate heterogeneous treatment effects&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;In many settings, we are not just interested in understanding a causal effect, but also whether this effect is &lt;strong&gt;different for different users&lt;/strong&gt;. We might be interested in understanding if a drug has side effects that are different for people of different age. Or we might be interested in understanding if an ad campaign is particularly effective in certain geographical areas.&lt;/p&gt;
&lt;p&gt;This knowledge is crucial because it allows us to &lt;strong&gt;target&lt;/strong&gt; the treatment. If a drug has severe side effects for kids, we might want to restrict its distribution only to adults. Or if an ad campaign is effective only in English-speaking countries it is not worth showing it elsewhere.&lt;/p&gt;
&lt;p&gt;In this blog post we are going to explore some approaches to uncover &lt;strong&gt;treatment effect heterogeneity&lt;/strong&gt;. In particular, we are going to explore methods that try to leverage the flexibility of &lt;strong&gt;machine learning&lt;/strong&gt; algorithms.&lt;/p&gt;
&lt;h2 id=&#34;example&#34;&gt;Example&lt;/h2&gt;
&lt;p&gt;Suppose we were a company interested in understanding how much a new &lt;strong&gt;premium feature&lt;/strong&gt; increases revenue. In particular, we know that users of different &lt;strong&gt;age&lt;/strong&gt; have different spending attitudes and we suspect that the impact of the premium feature could also be different depending on the age of the user.&lt;/p&gt;
&lt;p&gt;This information might be very important, for example for &lt;strong&gt;advertisement targeting&lt;/strong&gt; or &lt;strong&gt;discount design&lt;/strong&gt;. If we discover that the premium feature increases revenue for a particular set of users, we might want to target advertisement towards that group, or offer them personalized discounts.&lt;/p&gt;
&lt;p&gt;To understand the effect of the premium feature on revenue, the run an &lt;a href=&#34;https://en.wikipedia.org/wiki/A/B_testing&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;AB test&lt;/strong&gt;&lt;/a&gt; in which we randomly give access to the premium feature to 10% of the users. The feature is &lt;strong&gt;expensive&lt;/strong&gt; and we cannot afford to give it for free to more users. Hopefully a 10% treatment probability is enough.&lt;/p&gt;
&lt;p&gt;We generate the simulated data using the data generating process &lt;code&gt;dgp_premium()&lt;/code&gt; from &lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/src/dgp.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;src.dgp&lt;/code&gt;&lt;/a&gt;. I also import some plotting functions and libraries from &lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/src/utils.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;src.utils&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;%matplotlib inline
%config InlineBackend.figure_format = &#39;retina&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from src.utils import *
from src.dgp import dgp_premium
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;dgp = dgp_premium()
df = dgp.generate_data(seed=5)
df.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;revenue&lt;/th&gt;
      &lt;th&gt;premium&lt;/th&gt;
      &lt;th&gt;age&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;10.62&lt;/td&gt;
      &lt;td&gt;True&lt;/td&gt;
      &lt;td&gt;27.32&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;10.35&lt;/td&gt;
      &lt;td&gt;True&lt;/td&gt;
      &lt;td&gt;54.57&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;10.13&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;26.68&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;9.97&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;56.58&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;10.16&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;38.51&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;We have data on 300 users, for whom we observe the &lt;code&gt;revenue&lt;/code&gt; they generate and whether they were given the &lt;code&gt;premium&lt;/code&gt; feature. Moreover, we also record the &lt;code&gt;age&lt;/code&gt; of the users.&lt;/p&gt;
&lt;p&gt;To understand whether randomization worked, we use the &lt;code&gt;create_table_one&lt;/code&gt; function from Uber&amp;rsquo;s &lt;a href=&#34;https://causalml.readthedocs.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;causalml&lt;/code&gt;&lt;/a&gt; package to produce a &lt;strong&gt;covariate balance table&lt;/strong&gt;, containing the average value of our observable characteristics, across treatment and control groups. As the name suggests, this should always be the first table you present in causal inference analysis.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from causalml.match import create_table_one

create_table_one(df, &#39;premium&#39;, [&#39;age&#39;, &#39;revenue&#39;])
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;Control&lt;/th&gt;
      &lt;th&gt;Treatment&lt;/th&gt;
      &lt;th&gt;SMD&lt;/th&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;Variable&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;n&lt;/th&gt;
      &lt;td&gt;269&lt;/td&gt;
      &lt;td&gt;31&lt;/td&gt;
      &lt;td&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;age&lt;/th&gt;
      &lt;td&gt;39.01 (12.11)&lt;/td&gt;
      &lt;td&gt;38.43 (13.26)&lt;/td&gt;
      &lt;td&gt;-0.0454&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;revenue&lt;/th&gt;
      &lt;td&gt;10.04 (0.16)&lt;/td&gt;
      &lt;td&gt;10.56 (0.23)&lt;/td&gt;
      &lt;td&gt;2.5905&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;Most users are in the control group and only 31 users have received the premium feature. Average &lt;code&gt;age&lt;/code&gt; is comparable across groups (SMD&amp;lt;1), while it seems that the premium feature increases &lt;code&gt;revenue&lt;/code&gt; by 2.6$ per user, on average.&lt;/p&gt;
&lt;p&gt;Does the effect of the &lt;code&gt;premium&lt;/code&gt; feature differ by &lt;code&gt;age&lt;/code&gt;?&lt;/p&gt;
&lt;p&gt;One simple approach could me to regress &lt;code&gt;revenue&lt;/code&gt; on a full interaction of &lt;code&gt;premium&lt;/code&gt; and age.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;linear_model = smf.ols(&#39;revenue ~ premium * age&#39;, data=df).fit()
linear_model.summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
           &lt;td&gt;&lt;/td&gt;              &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt;           &lt;td&gt;   10.0244&lt;/td&gt; &lt;td&gt;    0.034&lt;/td&gt; &lt;td&gt;  292.716&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    9.957&lt;/td&gt; &lt;td&gt;   10.092&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;premium[T.True]&lt;/th&gt;     &lt;td&gt;    0.5948&lt;/td&gt; &lt;td&gt;    0.099&lt;/td&gt; &lt;td&gt;    6.007&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    0.400&lt;/td&gt; &lt;td&gt;    0.790&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;age&lt;/th&gt;                 &lt;td&gt;    0.0005&lt;/td&gt; &lt;td&gt;    0.001&lt;/td&gt; &lt;td&gt;    0.570&lt;/td&gt; &lt;td&gt; 0.569&lt;/td&gt; &lt;td&gt;   -0.001&lt;/td&gt; &lt;td&gt;    0.002&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;premium[T.True]:age&lt;/th&gt; &lt;td&gt;   -0.0021&lt;/td&gt; &lt;td&gt;    0.002&lt;/td&gt; &lt;td&gt;   -0.863&lt;/td&gt; &lt;td&gt; 0.389&lt;/td&gt; &lt;td&gt;   -0.007&lt;/td&gt; &lt;td&gt;    0.003&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;The interaction coefficient is close to zero and not significant. It seems that there is not a different effect of &lt;code&gt;premium&lt;/code&gt; by &lt;code&gt;age&lt;/code&gt;. But is it true? The interaction coefficient only captures linear relationships. What if the relationship is &lt;strong&gt;non-linear&lt;/strong&gt;?&lt;/p&gt;
&lt;p&gt;We can check it by directly &lt;strong&gt;plotting the raw data&lt;/strong&gt;. We plot revenue by age, splitting the data between &lt;code&gt;premium&lt;/code&gt; users and non-premium users.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sns.scatterplot(data=df, x=&#39;age&#39;, y=&#39;revenue&#39;, hue=&#39;premium&#39;, s=40);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/meta_learners_13_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;From the raw data, it looks like revenue is generally higher for people between 30 and 40 and &lt;code&gt;premium&lt;/code&gt; has a particularly strong effect for people between 35 and 45/50.&lt;/p&gt;
&lt;p&gt;We can visualize the estimated revenue by age with and without treatment.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def plot_TE(df, true_te=False):
    sns.scatterplot(data=df, x=&#39;age&#39;, y=&#39;revenue&#39;, hue=&#39;premium&#39;, s=40, legend=True)
    sns.lineplot(df[&#39;age&#39;], df[&#39;mu0_hat&#39;], label=&#39;$\mu_0$&#39;)
    sns.lineplot(df[&#39;age&#39;], df[&#39;mu1_hat&#39;], label=&#39;$\mu_1$&#39;)
    if true_te:
        plt.fill_between(df[&#39;age&#39;], df[&#39;y0&#39;], df[&#39;y0&#39;] + df[&#39;y1&#39;], color=&#39;grey&#39;, alpha=0.2, label=&amp;quot;True TE&amp;quot;)
    plt.title(&#39;Distribution of revenue by age and premium status&#39;)
    plt.legend(title=&#39;Treated&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We first compute the predicted revenue with ($\mu_1$) and without &lt;code&gt;premium&lt;/code&gt; subscription ($\mu_0$) and we plot them together with the raw data.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df[&#39;mu0_hat&#39;] = linear_model.predict(df.assign(premium=0))
df[&#39;mu1_hat&#39;] = linear_model.predict(df.assign(premium=1))
plot_TE(df)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/meta_learners_17_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;As we can see, the orange line is higher than the blue line, suggesting a positive effect of &lt;code&gt;premium&lt;/code&gt; on &lt;code&gt;revenue&lt;/code&gt;. However, the two lines are essentially &lt;strong&gt;parallel&lt;/strong&gt;, suggesting no heterogeneity in treatment effects.&lt;/p&gt;
&lt;p&gt;Can we be more precise? Is there a way to estimate this treatment heterogeneity in a &lt;strong&gt;flexible way&lt;/strong&gt;, without assuming functional forms?&lt;/p&gt;
&lt;p&gt;The answer is &lt;strong&gt;yes&lt;/strong&gt;! We can use machine learning methods to flexibly estimate heterogeneous treatment effects. In particular, in this blog post we are going to inspect three and popular methods that were introduced by &lt;a href=&#34;https://www.pnas.org/doi/abs/10.1073/pnas.1804597116&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Künzel, Sekhon, Bickel, Yu, (2019)&lt;/a&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;S-learner&lt;/li&gt;
&lt;li&gt;T-learner&lt;/li&gt;
&lt;li&gt;X-learner&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;setting&#34;&gt;Setting&lt;/h2&gt;
&lt;p&gt;We assume that for a set of i.i.d. subjects $i = 1, &amp;hellip;, n$ we observed a tuple $(X_i, D_i, Y_i)$ comprised of&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;a treatment assignment $T_i \in \lbrace 0, 1 \rbrace$ (&lt;code&gt;premium&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;a response $Y_i \in \mathbb R$ (&lt;code&gt;revenue&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;a feature vector $X_i \in \mathbb R^n$ (&lt;code&gt;age&lt;/code&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We are interested in &lt;strong&gt;estimating the average treatment effect&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;$$
\tau = \mathbb E \Big[ Y_i^{(1)} - Y_i^{(0)} \Big]
$$&lt;/p&gt;
&lt;p&gt;Where $Y_i^{(d)}$ indicates the potential outcome of individual $i$ under treatment status $d$. We also make the following assumptions.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Assumption 1 : unconfoundedness&lt;/strong&gt; (or ignorability, or selection on observables)&lt;/p&gt;
&lt;p&gt;$$
\big \lbrace Y_i^{(1)} , Y_i^{(0)} \big \rbrace \ \perp \ D_i \ | \ X_i
$$&lt;/p&gt;
&lt;p&gt;i.e. conditional on observable characteristics $X$, the treatment assignment $D$ is as good as random. What we are effectively assuming is that there is no other characteristics that we do not observe that could impact both whether a user selects the &lt;code&gt;dark_mode&lt;/code&gt; and their &lt;code&gt;read_time&lt;/code&gt;. This is a &lt;strong&gt;strong assumption&lt;/strong&gt; that is more likely to be satisfied the more individual characteristics we observe.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Assumption 2: stable unit treatment value (SUTVA)&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;$$
Y^{(d)} \perp D
$$&lt;/p&gt;
&lt;p&gt;i.e. the potential outcome does not depend on the treatment status. In our case, we are ruling out the fact that another user selecting the &lt;code&gt;premium&lt;/code&gt; feature might affect my effect of &lt;code&gt;premium&lt;/code&gt; on &lt;code&gt;revenue&lt;/code&gt;. The most common setting where SUTVA is violated is in presence of &lt;strong&gt;network effects&lt;/strong&gt;: if a friend of mine uses a social network increases my utility from using it.&lt;/p&gt;
&lt;h2 id=&#34;s-learner&#34;&gt;S-Learner&lt;/h2&gt;
&lt;p&gt;The simplest meta-algorithm is the &lt;strong&gt;single learner or S-learner&lt;/strong&gt;. To build the S-learner estimator, we fit a single model for all observations.&lt;/p&gt;
&lt;p&gt;$$
\mu(z) = \mathbb E \left[ Y_i \ \big | \ (X_i, D_i) = z \right]
$$&lt;/p&gt;
&lt;p&gt;the estimator is given by the difference between the predicted values evaluated with and without the treatment, $d=1$ and $d=0$.&lt;/p&gt;
&lt;p&gt;$$
\hat \tau_{S} (x) = \hat \mu(x,1) - \hat \mu(x,0)
$$&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def S_learner(dgp, model, y, D, X):
    temp = dgp.generate_data(true_te=True).sort_values(X)
    mu = model.fit(temp[X + [D]], temp[y])
    temp[&#39;mu0_hat&#39;] = mu.predict(temp[X + [D]].assign(premium=0))
    temp[&#39;mu1_hat&#39;] = mu.predict(temp[X + [D]].assign(premium=1))
    plot_TE(temp, true_te=True)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s use a &lt;a href=&#34;https://en.wikipedia.org/wiki/Decision_tree_learning&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;decision tree regression&lt;/strong&gt;&lt;/a&gt; model to build the the S-learner, using the &lt;code&gt;DecisionTreeRegressor&lt;/code&gt; function from the &lt;a href=&#34;https://scikit-learn.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;sklearn&lt;/code&gt;&lt;/a&gt; package. I won&amp;rsquo;t go into details about decision trees here, but I will just say that it&amp;rsquo;s a non-parametric estimator that uses the training data to split the state space (&lt;code&gt;premium&lt;/code&gt; and &lt;code&gt;age&lt;/code&gt; in our case) into blocks and predicts the outcome (&lt;code&gt;revenue&lt;/code&gt; in our case) as its average value within block.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from sklearn.tree import DecisionTreeRegressor

model = DecisionTreeRegressor(min_impurity_decrease=0.001)
S_learner(dgp, model, y=&amp;quot;revenue&amp;quot;, D=&amp;quot;premium&amp;quot;, X=[&amp;quot;age&amp;quot;])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/meta_learners_25_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;The plot depicts the data together with the response functions $\hat \mu(x,1)$ and $\hat \mu(x,0)$. I have also plotted in grey the area between the true response functions: the true treatment effects.&lt;/p&gt;
&lt;p&gt;As we can see, the S-learner is flexible enough to understand that there is a difference in levels between treatment and control group (we have two separate lines). It also captures well the response function for the control group, $\hat \mu(x,0)$, but not so well the control function for the treatment group, $\hat \mu(x,1)$.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;problem&lt;/strong&gt; with the S-learner is that it is learning a &lt;strong&gt;single model&lt;/strong&gt; so we have to hope that the model uncovers heterogeneity in the treatment $D$, but it might not be the case. Moreover, if the model is heavily regularized because of the high dimensionality of $X$, it &lt;strong&gt;might not recover any treatment effect&lt;/strong&gt;. For example, with decision trees, we might not split on the treatment $D$.&lt;/p&gt;
&lt;h2 id=&#34;t-learner&#34;&gt;T-learner&lt;/h2&gt;
&lt;p&gt;To build the &lt;strong&gt;two-learner or T-learner&lt;/strong&gt; estimator, we fit two different models, one for treated units and one for control units.&lt;/p&gt;
&lt;p&gt;$$
\mu^{(1)}(x) = \mathbb E \left[ Y_i \ \big | \ X_i = x, T_i = 1 \right] \qquad ; \qquad \mu^{(0)}(x) = \mathbb E \left[ Y_i \ \big | \ X_i = x, T_i = 0 \right]
$$&lt;/p&gt;
&lt;p&gt;the estimator is given by the difference between the predicted values of the two algorithms.&lt;/p&gt;
&lt;p&gt;$$
\hat \tau_{T} (x) = \hat \mu^{(1)}(x) - \hat \mu^{(0)}(x)
$$&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def T_learner(df, model, y, D, X):
    temp = dgp.generate_data(true_te=True).sort_values(X)
    mu0 = model.fit(temp.loc[temp[D]==0, X], temp.loc[temp[D]==0, y])
    temp[&#39;mu0_hat&#39;] = mu0.predict(temp[X])
    mu1 = model.fit(temp.loc[temp[D]==1, X], temp.loc[temp[D]==1, y])
    temp[&#39;mu1_hat&#39;] = mu1.predict(temp[X])
    plot_TE(temp, true_te=True)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We use a decision tree regression model as before but, this time, we fit two separate decision trees for the treatment and control group.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;T_learner(dgp, model, y=&amp;quot;revenue&amp;quot;, D=&amp;quot;premium&amp;quot;, X=[&amp;quot;age&amp;quot;])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/meta_learners_31_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;As we can see, the T-learner is much &lt;strong&gt;more flexible&lt;/strong&gt; than the S-learner because it fits two separate models. The response function for the control group, $\hat \mu(x,0)$, is still very accurate and the response function for the treatment group, $\hat \mu(x,1)$, is more flexible than before.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;problem&lt;/strong&gt; now is that we are &lt;strong&gt;using just a fraction of the data&lt;/strong&gt; for each prediction problem, while the S-learner was using all the data. By fitting two separate models we are losing some information. Moreover, by using two different models we might get &lt;strong&gt;heterogeneity where there is none&lt;/strong&gt;. For example, with decision trees, we will probably get different splits with different samples even if the data generating process is the same.&lt;/p&gt;
&lt;h3 id=&#34;x-learner&#34;&gt;X-learner&lt;/h3&gt;
&lt;p&gt;The &lt;strong&gt;cross-learner or X-learner&lt;/strong&gt; estimator is an extension of the T-learner estimator. It is built in the following way:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;As for the T-learner, compute separate models for $\mu^{(1)}(x)$ and $\mu^{(0)}(x)$ using the treated and control units, respectively&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Compute the estimated treatment effects as&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;$$
\Delta_i (x) =
\begin{cases}
Y_i - \hat \mu^{(0)}(x) &amp;amp;\quad \text{ if } D_i = 1
\newline
\hat \mu^{(1)}(x) - Y_i &amp;amp;\quad \text{ if } D_i = 0
\end{cases}
$$&lt;/p&gt;
&lt;ol start=&#34;3&#34;&gt;
&lt;li&gt;
&lt;p&gt;Predicting $\Delta$ from $X$, compute $\hat \tau^{(0)}(x)$ from treated units and  $\hat \tau^{(1)}(x)$ from control units&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Estimate the &lt;a href=&#34;https://en.wikipedia.org/wiki/Propensity_score_matching&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;propensity score&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;$$
e(x) = \mathbb E \left[ D_i = 1 \ \big | \ X_i = x \right]
$$&lt;/p&gt;
&lt;ol start=&#34;5&#34;&gt;
&lt;li&gt;Compute the treatment effects&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;$$
\hat \tau_X(x) = \hat \tau^{(0)}(x) \hat e(x) + \hat \tau^{(1)}(x) (1 - \hat e(x))
$$&lt;/p&gt;
&lt;p&gt;Can we still recover &lt;strong&gt;pseudo response functions&lt;/strong&gt;? Yes!&lt;/p&gt;
&lt;p&gt;Which we can rewrite the treatment effects as&lt;/p&gt;
&lt;p&gt;$$
\begin{align}
\hat \tau_X(x) &amp;amp; = \hat \tau^{(0)}(x) \hat e(x) + \hat \tau^{(1)}(x) (1 - \hat e(x)) = \newline
&amp;amp;= \hat e(x) \left[ \hat \mu^{(1)}(x) - Y_i^{(0)} \right] + (1 - \hat e(x)) \left[ Y_i^{(1)} - \hat \mu^{(0)}(x) \right] = \newline
&amp;amp;= \left[ \hat e(x) \hat \mu^{(1)}(x) + (1 - \hat e(x)) Y_i^{(1)} \right] - \left[ \hat e(x) Y_i^{(0)} + (1 - \hat e(x))  \hat \mu^{(0)}(x) \right]
\end{align}
$$&lt;/p&gt;
&lt;p&gt;So that the pseudo response functions estimated by the X-learner are&lt;/p&gt;
&lt;p&gt;$$
\begin{align}
\tilde \mu_i^{(1)} (x) &amp;amp;= \hat e(x) \hat \mu^{(1)}(x) + (1 - \hat e(x)) Y_i^{(1)} \newline
\tilde \mu_i^{(0)} (x) &amp;amp;=  \hat e(x) Y_i^{(0)} + (1 - \hat e(x)) \hat \mu^{(0)}(x)
\end{align}
$$&lt;/p&gt;
&lt;p&gt;As we can see, the X-learner combines the true values $Y_i^{(d)}$ with the estimated ones $\mu_i^{(d)} (x)$ weighting by the &lt;a href=&#34;https://en.wikipedia.org/wiki/Propensity_score_matching&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;propensity scores&lt;/strong&gt;&lt;/a&gt; $e_i(x)$, i.e. the estimated treatment probabilities.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;What does it mean?&lt;/strong&gt; It means that if we have many more observations for one group (in our case the control group), the control response function $\hat \mu^{(0)}(x) $ will get most of the weight. Instead, for the other group (the treatment group in our case), the actual observations $Y_i^{(1)}$ will get most of the weight.&lt;/p&gt;
&lt;p&gt;To illustrate the method, I am going to build pseudo response functions by approximating $Y_i^{(d)}$ using the nearest observation, using the &lt;code&gt;KNeighborsRegressor&lt;/code&gt; function. I estimate the propensity scores via &lt;a href=&#34;https://en.wikipedia.org/wiki/Logistic_regression&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;logistic regression&lt;/a&gt; using the &lt;code&gt;LogisticRegressionCV&lt;/code&gt; function.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from sklearn.neighbors import KNeighborsRegressor
from sklearn.linear_model import LogisticRegressionCV

def X_learner(df, model, y, D, X):
    temp = dgp.generate_data(true_te=True).sort_values(X)
    
    # Mu
    mu0 = model.fit(temp.loc[temp[D]==0, X], temp.loc[temp[D]==0, y])
    temp[&#39;mu0_hat_&#39;] = mu0.predict(temp[X])
    mu1 = model.fit(temp.loc[temp[D]==1, X], temp.loc[temp[D]==1, y])
    temp[&#39;mu1_hat_&#39;] = mu1.predict(temp[X])
    
    # Y
    y0 = KNeighborsRegressor(n_neighbors=1).fit(temp.loc[temp[D]==0, X], temp.loc[temp[D]==0, y])
    temp[&#39;y0_hat&#39;] = y0.predict(temp[X])
    y1 = KNeighborsRegressor(n_neighbors=1).fit(temp.loc[temp[D]==1, X], temp.loc[temp[D]==1, y])
    temp[&#39;y1_hat&#39;] = y1.predict(temp[X])
    
    # Weight
    e = LogisticRegressionCV().fit(y=temp[D], X=temp[X]).predict_proba(temp[X])[:,1]
    temp[&#39;mu0_hat&#39;] = e * temp[&#39;y0_hat&#39;] + (1-e) * temp[&#39;mu0_hat_&#39;]
    temp[&#39;mu1_hat&#39;] = (1-e) * temp[&#39;y1_hat&#39;] + e * temp[&#39;mu1_hat_&#39;]
    
    # Plot
    plot_TE(temp, true_te=True)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;X_learner(df, model, y=&amp;quot;revenue&amp;quot;, D=&amp;quot;premium&amp;quot;, X=[&amp;quot;age&amp;quot;])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/meta_learners_37_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;As we can clearly see from this graph, the main advantage of &lt;strong&gt;X-learners&lt;/strong&gt; is that it adapts the &lt;strong&gt;flexibility&lt;/strong&gt; of the response functions to the context. In areas of the state space where we have a lot of data, it mostly uses the estimated response function, in areas of the state space with few data, it uses the observation themselves.&lt;/p&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;In this post, we have seen different estimators introduced by &lt;a href=&#34;https://www.pnas.org/doi/abs/10.1073/pnas.1804597116&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Künzel, Sekhon, Bickel, Yu, (2019)&lt;/a&gt; that leverage flexible &lt;strong&gt;machine learning&lt;/strong&gt; algorithms to estimate &lt;strong&gt;heterogeneous treatment effects&lt;/strong&gt;. The estimators differ for their degree of sophistication: the S-learner fits a single estimator including the treatment indicator as a covariate. The T-learner fits two separate estimators for the treatment and control group. Lastly, the X-learner is an extension of the T-learner that allows for different degrees of flexibility depending on the amount of data available across treatment and control groups.&lt;/p&gt;
&lt;p&gt;Estimation of heterogeneous treatment effect is extremely important for &lt;strong&gt;treatment targeting&lt;/strong&gt;. Indeed, there is now a growing literature that exploits machine learning methods to get flexible estimates without imposing functional form assumptions. Among the many, it&amp;rsquo;s important to mention the R-learner procedure of &lt;a href=&#34;https://academic.oup.com/biomet/article/108/2/299/5911092&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Nie and Wager (2021)&lt;/a&gt; and the causal trees and forests of &lt;a href=&#34;https://www.tandfonline.com/doi/full/10.1080/01621459.2017.1319839&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Athey and Wager (2018)&lt;/a&gt;. I might write more about these procedures in the future so, stay tuned ☺️&lt;/p&gt;
&lt;h3 id=&#34;references&#34;&gt;References&lt;/h3&gt;
&lt;p&gt;[1] S. Künzel, J. Sekhon, P. Bickel, B. Yu, &lt;a href=&#34;https://www.pnas.org/doi/abs/10.1073/pnas.1804597116&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Metalearners for estimating heterogeneous treatment effects using machine learning&lt;/a&gt; (2019), &lt;em&gt;PNAS&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;[2] X. Nie, S. Wager, &lt;a href=&#34;https://academic.oup.com/biomet/article/108/2/299/5911092&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Quasi-oracle estimation of heterogeneous treatment effects&lt;/a&gt; (2021), &lt;em&gt;Biometrika&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;[3] S. Athey, S. Wager, &lt;a href=&#34;https://www.tandfonline.com/doi/full/10.1080/01621459.2017.1319839&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Estimation and Inference of Heterogeneous Treatment Effects using Random Forests&lt;/a&gt; (2018), &lt;em&gt;Journal of the American Statistical Association&lt;/em&gt;.&lt;/p&gt;
&lt;h3 id=&#34;related-articles&#34;&gt;Related Articles&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/99bf5cffa0d9&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Matching, Weighting, or Regression?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/b63dc69e3d8c&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;DAGs and Control Variables&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;code&#34;&gt;Code&lt;/h3&gt;
&lt;p&gt;You can find the original Jupyter Notebook here:&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/meta.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/meta.ipynb&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Weighting, Matching, or Regression?</title>
      <link>https://matteocourthoud.github.io/post/weighting_matching/</link>
      <pubDate>Mon, 10 Jul 2023 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/post/weighting_matching/</guid>
      <description>&lt;p&gt;&lt;em&gt;Understanding and comparing different methods for conditional causal inference analysis&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;AB tests or randomized controlled trials are the &lt;strong&gt;gold standard&lt;/strong&gt; in causal inference. By randomly exposing units to a treatment we make sure that individuals in both groups are comparable, on average, and any difference we observe can be attributed to the treatment effect alone.&lt;/p&gt;
&lt;p&gt;However, often the treatment and control groups are &lt;strong&gt;not perfectly comparable&lt;/strong&gt;. This could be due to the fact that randomization was not perfect or available. Not always we can randomize a treatment, for ethical or practical reasons. And even when we can, sometimes we do not have enough individuals or units so that differences between groups are seizable. This happens often, for example, when randomization is not done at the individual level, but at a higher level of aggregation, for example zipcodes, counties or even states.&lt;/p&gt;
&lt;p&gt;In these settings, we can still recover a causal estimate of the treatment effect if we have &lt;strong&gt;enough information&lt;/strong&gt; about individuals, by making the treatment and control group comparable, ex-post. In this blog post, we are going to introduce and compare different procedures to estimate causal effects in presence of imbalances between treatment and control groups that are &lt;strong&gt;fully observable&lt;/strong&gt;. In particular we are going to analyze weighting, matching and regression procedures.&lt;/p&gt;
&lt;h2 id=&#34;example&#34;&gt;Example&lt;/h2&gt;
&lt;p&gt;Assume we had blog on statistics and causal inference 😇. To improve user experience, we are considering &lt;strong&gt;releasing a dark mode&lt;/strong&gt;, and we would like to understand whether this new feature increases the time users spend on our blog.&lt;/p&gt;
&lt;img src=&#34;fig/modes.png&#34; width=&#34;600px&#34;/&gt;
&lt;p&gt;We are not a sophisticated company, therefore we do not run an AB test but we simply release the dark mode and we observe whether users select it or not and the time they spend on the blog. We know that there might be &lt;strong&gt;selection&lt;/strong&gt;:  users that prefer the dark mode could have different reading preferences and this might complicate our causal analysis.&lt;/p&gt;
&lt;p&gt;We can represent the data generating process with the following &lt;a href=&#34;https://towardsdatascience.com/b63dc69e3d8c&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Directed Acyclic Graph (DAG)&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-mermaid&#34;&gt;flowchart TB
classDef included fill:#DCDCDC,stroke:#000000,stroke-width:2px;
classDef excluded fill:#ffffff,stroke:#000000,stroke-width:2px;
classDef unobserved fill:#ffffff,stroke:#000000,stroke-width:2px,stroke-dasharray: 5 5;

X1((gender))
X2((age))
X3((hours))
D((dark mode))
Y((read time))

D --&amp;gt; Y
X1 --&amp;gt; Y
X1 --&amp;gt; D
X2 --&amp;gt; D
X3 --&amp;gt; Y

class D,Y included;
class X1,X2,X3 excluded;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We generate the simulated data using the data generating process &lt;code&gt;dgp_darkmode()&lt;/code&gt; from &lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/src/dgp.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;src.dgp&lt;/code&gt;&lt;/a&gt;. I also import some plotting functions and libraries from &lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/src/utils.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;src.utils&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;%matplotlib inline
%config InlineBackend.figure_format = &#39;retina&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from src.utils import *
from src.dgp import dgp_darkmode
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df = dgp_darkmode().generate_data()
df.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;read_time&lt;/th&gt;
      &lt;th&gt;dark_mode&lt;/th&gt;
      &lt;th&gt;male&lt;/th&gt;
      &lt;th&gt;age&lt;/th&gt;
      &lt;th&gt;hours&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;14.4&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;43.0&lt;/td&gt;
      &lt;td&gt;65.6&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;15.4&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;55.0&lt;/td&gt;
      &lt;td&gt;125.4&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;20.9&lt;/td&gt;
      &lt;td&gt;True&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;23.0&lt;/td&gt;
      &lt;td&gt;642.6&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;20.0&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;41.0&lt;/td&gt;
      &lt;td&gt;129.1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;21.5&lt;/td&gt;
      &lt;td&gt;True&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;29.0&lt;/td&gt;
      &lt;td&gt;190.2&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;We have informations on 300 users for whom we observe whether they select the &lt;code&gt;dark_mode&lt;/code&gt; (the treatment), their weekly &lt;code&gt;read_time&lt;/code&gt; (the outcome of interest) and some characteristics like &lt;code&gt;gender&lt;/code&gt;, &lt;code&gt;age&lt;/code&gt; and total &lt;code&gt;hours&lt;/code&gt; previously spend on the blog.&lt;/p&gt;
&lt;p&gt;We would like to estimate the effect of the new &lt;code&gt;dark_mode&lt;/code&gt; on users&amp;rsquo; &lt;code&gt;read_time&lt;/code&gt;. If we were runnig an &lt;a href=&#34;https://de.wikipedia.org/wiki/A/B-Test&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;AB test&lt;/strong&gt;&lt;/a&gt; or randomized control trial, we could just compare users with and without the dark mode and we could attribute the difference in average reading time to the &lt;code&gt;dark_mode&lt;/code&gt;. Let&amp;rsquo;s check what number we would get.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;np.mean(df.loc[df.dark_mode==True, &#39;read_time&#39;]) - np.mean(df.loc[df.dark_mode==False, &#39;read_time&#39;])
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;-0.4446330948042103
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Individuals that select the &lt;code&gt;dark_mode&lt;/code&gt; spend on average 1.37 hours less on the blog, per week. Should we conclude that &lt;code&gt;dark_mode&lt;/code&gt; is a &lt;strong&gt;bad idea&lt;/strong&gt;? Is this a causal effect?&lt;/p&gt;
&lt;p&gt;We did not randomize the &lt;code&gt;dark_mode&lt;/code&gt; so that users that selected it might not be directly &lt;strong&gt;comparable&lt;/strong&gt; with users that didn&amp;rsquo;t. Can we verify this concern? Partially. We can only check characteristics that we observe, &lt;code&gt;gender&lt;/code&gt;, &lt;code&gt;age&lt;/code&gt; and total &lt;code&gt;hours&lt;/code&gt; in our setting. We cannot check if users differ along other dimensions that we don&amp;rsquo;t observe.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s use the &lt;code&gt;create_table_one&lt;/code&gt; function from Uber&amp;rsquo;s &lt;a href=&#34;https://causalml.readthedocs.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;causalml&lt;/code&gt;&lt;/a&gt; package to produce a &lt;strong&gt;covariate balance table&lt;/strong&gt;, containing the average value of our observable characteristics, across treatment and control groups. As the name suggests, this should always be the first table you present in causal inference analysis.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from causalml.match import create_table_one

X = [&#39;male&#39;, &#39;age&#39;, &#39;hours&#39;]
table1 = create_table_one(df, &#39;dark_mode&#39;, X)
table1
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;Control&lt;/th&gt;
      &lt;th&gt;Treatment&lt;/th&gt;
      &lt;th&gt;SMD&lt;/th&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;Variable&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;n&lt;/th&gt;
      &lt;td&gt;151&lt;/td&gt;
      &lt;td&gt;149&lt;/td&gt;
      &lt;td&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;age&lt;/th&gt;
      &lt;td&gt;46.01 (9.79)&lt;/td&gt;
      &lt;td&gt;39.09 (11.53)&lt;/td&gt;
      &lt;td&gt;-0.6469&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;hours&lt;/th&gt;
      &lt;td&gt;337.78 (464.00)&lt;/td&gt;
      &lt;td&gt;328.57 (442.12)&lt;/td&gt;
      &lt;td&gt;-0.0203&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;male&lt;/th&gt;
      &lt;td&gt;0.34 (0.47)&lt;/td&gt;
      &lt;td&gt;0.66 (0.48)&lt;/td&gt;
      &lt;td&gt;0.6732&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;There seems to be &lt;strong&gt;some difference&lt;/strong&gt; between treatment (&lt;code&gt;dark_mode&lt;/code&gt;) and control group. In particular, users that select the &lt;code&gt;dark_mode&lt;/code&gt; are older, have spent less hours on the blog and they are more likely to be males.&lt;/p&gt;
&lt;p&gt;Another way to visually observe all the differences at once is with a &lt;strong&gt;paired violinplot&lt;/strong&gt;. The advantage of the paired violinplot is that it allows us to observe the full distribution of the variable (approximated via &lt;a href=&#34;https://en.wikipedia.org/wiki/Kernel_density_estimation&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;kernel density estimation&lt;/a&gt;).&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def plot_distributions(df, X, d):
    df_long = df.copy()[X + [d]]
    df_long[X] =(df_long[X] - df_long[X].mean()) / df_long[X].std()
    df_long = pd.melt(df_long, id_vars=d, value_name=&#39;value&#39;)
    sns.violinplot(y=&amp;quot;variable&amp;quot;, x=&amp;quot;value&amp;quot;, hue=d, data=df_long, split=True).\
        set(xlabel=&amp;quot;&amp;quot;, ylabel=&amp;quot;&amp;quot;, title=&amp;quot;Normalized Variable Distribution&amp;quot;);
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plot_distributions(df, X, &amp;quot;dark_mode&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/weighting_matching_16_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;The insight of the violinplot is very similar: it seems that users that select the &lt;code&gt;dark_mode&lt;/code&gt; are different from users that don&amp;rsquo;t.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Why do we care?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;If we do not control for the observable characteristics, we are unable to estimate the true treatment effect. In short, we cannot be certain that the difference in outcome, &lt;code&gt;read_time&lt;/code&gt;, can be attributed to the treatment, &lt;code&gt;dark_mode&lt;/code&gt;, instead of other characteristics. For example, it could be that males read less and also prefer the &lt;code&gt;dark_mode&lt;/code&gt;, therefore we observe a negative correlation even though &lt;code&gt;dark_mode&lt;/code&gt; has no effect on &lt;code&gt;read_time&lt;/code&gt; (or even positive).&lt;/p&gt;
&lt;p&gt;In terms of Dyrected Acyclic Graphs, this means that we have several &lt;a href=&#34;https://towardsdatascience.com/b63dc69e3d8c&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;backdoor paths&lt;/strong&gt;&lt;/a&gt; that we need to &lt;strong&gt;block&lt;/strong&gt; in order for our analysis to be &lt;strong&gt;causal&lt;/strong&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-mermaid&#34;&gt;flowchart TB
classDef included fill:#DCDCDC,stroke:#000000,stroke-width:2px;
classDef excluded fill:#ffffff,stroke:#000000,stroke-width:2px;
classDef unobserved fill:#ffffff,stroke:#000000,stroke-width:2px,stroke-dasharray: 5 5;

X1((gender))
X2((age))
X3((hours))
D((dark mode))
Y((read time))

D --&amp;gt; Y
X1 --&amp;gt; Y
X1 --&amp;gt; D
X2 --&amp;gt; D
X3 --&amp;gt; Y

linkStyle 0 stroke:#00ff00,stroke-width:4px;
linkStyle 1,2 stroke:#ff0000,stroke-width:4px;
class D,Y included;
class X1,X2,X3 excluded;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;How do we &lt;strong&gt;block backdoor paths&lt;/strong&gt;? By conditioning the analysis on those intermediate variables. The conditional analysis allows us to recover the average treatment effect of the &lt;code&gt;dark_mode&lt;/code&gt; on &lt;code&gt;read_time&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-mermaid&#34;&gt;flowchart TB
classDef included fill:#DCDCDC,stroke:#000000,stroke-width:2px;
classDef excluded fill:#ffffff,stroke:#000000,stroke-width:2px;
classDef unobserved fill:#ffffff,stroke:#000000,stroke-width:2px,stroke-dasharray: 5 5;

X1((gender))
X2((age))
X3((hours))
D((dark mode))
Y((read time))

D --&amp;gt; Y
X1 -.-&amp;gt; Y
X1 -.-&amp;gt; D
X2 --&amp;gt; D
X3 --&amp;gt; Y

linkStyle 0 stroke:#00ff00,stroke-width:4px;
class D,Y,X1,X2,X3 included;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;How do we &lt;strong&gt;condition the analysis&lt;/strong&gt; on &lt;code&gt;gender&lt;/code&gt;, &lt;code&gt;age&lt;/code&gt; and &lt;code&gt;hours&lt;/code&gt;? We have some options:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Matching&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Propensity score&lt;/strong&gt; weighting&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Regression&lt;/strong&gt; with control variables&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Let&amp;rsquo;s explore and compare them!&lt;/p&gt;
&lt;h2 id=&#34;conditional-analysis&#34;&gt;Conditional Analysis&lt;/h2&gt;
&lt;p&gt;We assume that for a set of subjects $i = 1, &amp;hellip;, n$ we observed a tuple $(D_i, Y_i, X_i)$ comprised of&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;a treatment assignment $D_i \in \lbrace 0, 1 \rbrace$ (&lt;code&gt;dark_mode&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;a response $Y_i \in \mathbb R$ (&lt;code&gt;read_time&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;a feature vector $X_i \in \mathbb R^n$ (&lt;code&gt;gender&lt;/code&gt;, &lt;code&gt;age&lt;/code&gt; and &lt;code&gt;hours&lt;/code&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Assumption 1 : unconfoundedness&lt;/strong&gt; (or ignorability, or selection on observables)&lt;/p&gt;
&lt;p&gt;$$
\big \lbrace Y_i^{(1)} , Y_i^{(0)} \big \rbrace \ \perp \ D_i \ | \ X_i
$$&lt;/p&gt;
&lt;p&gt;i.e. conditional on observable characteristics $X$, the treatment assignment $D$ is as good as random. What we are effectively assuming is that there is no other characteristics that we do not observe that could impact both whether a user selects the &lt;code&gt;dark_mode&lt;/code&gt; and their &lt;code&gt;read_time&lt;/code&gt;. This is a &lt;strong&gt;strong assumption&lt;/strong&gt; that is more likely to be satisfied the more individual characteristics we observe.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Assumption 2: overlap&lt;/strong&gt; (or common support)&lt;/p&gt;
&lt;p&gt;$$
\exists \eta &amp;gt; 0 \ : \ \eta \leq \mathbb E \left[ T_i = 1 \ \big | \ X_i = x \right] \leq 1-\eta
$$&lt;/p&gt;
&lt;p&gt;i.e. no observation is deterministically assigned to the treatment or control group. This is a more technical assumption that basically means that for any level of &lt;code&gt;gender&lt;/code&gt;, &lt;code&gt;age&lt;/code&gt; or &lt;code&gt;hours&lt;/code&gt;, there could exist an individual that select the &lt;code&gt;dark_mode&lt;/code&gt; and one that doesn&amp;rsquo;t. Differently from the unconfoundedness assumption, the overal assumption is &lt;strong&gt;testable&lt;/strong&gt;.&lt;/p&gt;
&lt;h3 id=&#34;matching&#34;&gt;Matching&lt;/h3&gt;
&lt;p&gt;The first and most intuitive method to perform conditional analysis is &lt;strong&gt;matching&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;idea&lt;/strong&gt; of matching is very simple. Since we are not sure whether, for example, male and female users are directly comparable, we do the analysis within gender. Instead of comparing &lt;code&gt;read_time&lt;/code&gt; across &lt;code&gt;dark_mode&lt;/code&gt; in the whole sample, we do it separately for male and female users.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_gender = pd.pivot_table(df, values=&#39;read_time&#39;, index=&#39;male&#39;, columns=&#39;dark_mode&#39;, aggfunc=np.mean)
df_gender[&#39;diff&#39;] = df_gender[1] - df_gender[0] 
df_gender
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;dark_mode&lt;/th&gt;
      &lt;th&gt;False&lt;/th&gt;
      &lt;th&gt;True&lt;/th&gt;
      &lt;th&gt;diff&lt;/th&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;male&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;20.318000&lt;/td&gt;
      &lt;td&gt;22.24902&lt;/td&gt;
      &lt;td&gt;1.931020&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;16.933333&lt;/td&gt;
      &lt;td&gt;16.89898&lt;/td&gt;
      &lt;td&gt;-0.034354&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;Now the effect of &lt;code&gt;dark_mode&lt;/code&gt; seems reversed: it is negative for male users (-0.79) but bigger and positive for female users (+1.38), suggesting a positive aggregate effect, 1.38 - 0.79 = 0.59 (assuming equal proportion of genders)! This sign reversal is a very classical example of the &lt;a href=&#34;https://en.wikipedia.org/wiki/Simpson%27s_paradox&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Simpson&amp;rsquo;s Paradox&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;This comparison was easy to perform for &lt;code&gt;gender&lt;/code&gt;, since it is a binary variable. With multiple variables, potentially continuous, matching becomes much more difficult. One common strategy is to &lt;strong&gt;match users&lt;/strong&gt; in the treatment group with the most similar user in the control group, using some sort of &lt;a href=&#34;https://en.wikipedia.org/wiki/Nearest_neighbour_algorithm&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;nearest neighbor algorithm&lt;/a&gt;. I won&amp;rsquo;t go into the algorithm details here, but we can perform the matching with the &lt;code&gt;NearestNeighborMatch&lt;/code&gt; function from the &lt;code&gt;causalml&lt;/code&gt; package.&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;NearestNeighborMatch&lt;/code&gt; function generates a new dataset where users in the treatment group have been matched 1:1 (option &lt;code&gt;ratio=1&lt;/code&gt;) to users in the control group.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from causalml.match import NearestNeighborMatch

psm = NearestNeighborMatch(replace=True, ratio=1, random_state=1)
df_matched = psm.match(data=df, treatment_col=&amp;quot;dark_mode&amp;quot;, score_cols=X)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Are the two groups more comparable now? We can produce a new version of the &lt;strong&gt;balance table&lt;/strong&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;table1_matched = create_table_one(df_matched, &amp;quot;dark_mode&amp;quot;, X)
table1_matched
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;Control&lt;/th&gt;
      &lt;th&gt;Treatment&lt;/th&gt;
      &lt;th&gt;SMD&lt;/th&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;Variable&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;n&lt;/th&gt;
      &lt;td&gt;104&lt;/td&gt;
      &lt;td&gt;104&lt;/td&gt;
      &lt;td&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;age&lt;/th&gt;
      &lt;td&gt;41.93 (10.05)&lt;/td&gt;
      &lt;td&gt;41.85 (10.02)&lt;/td&gt;
      &lt;td&gt;-0.0086&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;hours&lt;/th&gt;
      &lt;td&gt;206.92 (309.62)&lt;/td&gt;
      &lt;td&gt;209.48 (321.79)&lt;/td&gt;
      &lt;td&gt;0.0081&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;male&lt;/th&gt;
      &lt;td&gt;0.62 (0.49)&lt;/td&gt;
      &lt;td&gt;0.62 (0.49)&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;Now the average differences between the two groups have &lt;strong&gt;shrunk&lt;/strong&gt; by at least a couple of orders of magnitude. However, note how the sample size has slightly decreased (300 $\to$ 246) since (1) we only match treated users and (2) we are not able to find a good match for all of them.&lt;/p&gt;
&lt;p&gt;We can visually inspect distributional differences with the paired violinplot.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plot_distributions(df_matched, X, &amp;quot;dark_mode&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/weighting_matching_28_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;A popular way to visualize pre- and post-matching covariate balance is the &lt;strong&gt;balance plot&lt;/strong&gt; that essentially displays the standardized mean differences before and after matching, for each control variable.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def plot_balance(t1, t2, X):
    df_smd = pd.DataFrame({&amp;quot;Variable&amp;quot;: X + X,
                           &amp;quot;Sample&amp;quot;: [&amp;quot;Unadjusted&amp;quot; for _ in range(len(X))] + [&amp;quot;Adjusted&amp;quot; for _ in range(len(X))],
                           &amp;quot;Standardized Mean Difference&amp;quot;: t1[&amp;quot;SMD&amp;quot;][1:].to_list() + 
                                                           t2[&amp;quot;SMD&amp;quot;][1:].to_list()})

    sns.scatterplot(x=&amp;quot;Standardized Mean Difference&amp;quot;, y=&amp;quot;Variable&amp;quot;, hue=&amp;quot;Sample&amp;quot;, data=df_smd).\
        set(title=&amp;quot;Balance Plot&amp;quot;)
    plt.axvline(x=0, color=&#39;k&#39;, ls=&#39;--&#39;, zorder=-1, alpha=0.3);
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plot_balance(table1, table1_matched, X)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/weighting_matching_31_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;As we can see, now all differences in observable characteristics between the two groups are essentially zero. We could also compare the distributions using other metrics or test statistics, such as the &lt;a href=&#34;https://towardsdatascience.com/9b06ee4d30bf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Kolmogorov-Smirnov test statistic&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;How do we &lt;strong&gt;estimate the average treatment effect&lt;/strong&gt;? We can simply do a difference in means. An equivalent way that automatically provides standard errors is to run a linear regression of the outcome, &lt;code&gt;read_time&lt;/code&gt;, on the treatment, &lt;code&gt;dark_mode&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt; that, since we have performed the matching for each treated user, the treatment effect we are estimating is the &lt;strong&gt;average treatment effect on the treated (ATT)&lt;/strong&gt;, which can be different from the average treatment effect if the treated sample differs from the overall population (which is likely to be the case, since we are doing matching in the first place).&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;smf.ols(&amp;quot;read_time ~ dark_mode&amp;quot;, data=df_matched).fit().summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
          &lt;td&gt;&lt;/td&gt;             &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt;         &lt;td&gt;   17.0365&lt;/td&gt; &lt;td&gt;    0.469&lt;/td&gt; &lt;td&gt;   36.363&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;   16.113&lt;/td&gt; &lt;td&gt;   17.960&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;dark_mode[T.True]&lt;/th&gt; &lt;td&gt;    1.4490&lt;/td&gt; &lt;td&gt;    0.663&lt;/td&gt; &lt;td&gt;    2.187&lt;/td&gt; &lt;td&gt; 0.030&lt;/td&gt; &lt;td&gt;    0.143&lt;/td&gt; &lt;td&gt;    2.755&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;The effect is now positive, but not statistically significant.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt; that we might have matched multiple treated users with the same untreated user, violating the independence assumption across observations and, in turn, distorting inference.&lt;/p&gt;
&lt;p&gt;We have two solutions:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;cluster standard errors at the matched individual level&lt;/li&gt;
&lt;li&gt;compute standard errors via bootstrap&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;We implement the first and cluster the standard errors by the original individual identifiers (the dataframe index).&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;smf.ols(&amp;quot;read_time ~ dark_mode&amp;quot;, data=df_matched)\
    .fit(cov_type=&#39;cluster&#39;, cov_kwds={&#39;groups&#39;: df_matched.index})\
    .summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
          &lt;td&gt;&lt;/td&gt;             &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;z&lt;/th&gt;      &lt;th&gt;P&gt;|z|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt;         &lt;td&gt;   17.0365&lt;/td&gt; &lt;td&gt;    0.650&lt;/td&gt; &lt;td&gt;   26.217&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;   15.763&lt;/td&gt; &lt;td&gt;   18.310&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;dark_mode[T.True]&lt;/th&gt; &lt;td&gt;    1.4490&lt;/td&gt; &lt;td&gt;    0.821&lt;/td&gt; &lt;td&gt;    1.765&lt;/td&gt; &lt;td&gt; 0.078&lt;/td&gt; &lt;td&gt;   -0.160&lt;/td&gt; &lt;td&gt;    3.058&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;The effect is even less statistically significant.&lt;/p&gt;
&lt;h3 id=&#34;propensity-score&#34;&gt;Propensity Score&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://academic.oup.com/biomet/article/70/1/41/240879&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Rosenbaum and Rubin (1983)&lt;/a&gt; proved a very powerful result: if the &lt;strong&gt;strong ignorability assumption&lt;/strong&gt; holds, it is sufficient to condition the analysis on the probability ot treatment, the &lt;strong&gt;propensity score&lt;/strong&gt;, in order to have conditional independence.&lt;/p&gt;
&lt;p&gt;$$
\big \lbrace Y_i^{(1)} , Y_i^{(0)} \big \rbrace \ \perp \ D_i \ | \ X_i \quad \leftrightarrow \quad \big \lbrace Y_i^{(1)} , Y_i^{(0)} \big \rbrace \ \perp \ D_i \ | \ e(X_i)
$$&lt;/p&gt;
&lt;p&gt;Where $e(X_i)$ is the probability of treatment of individual $i$, given the observable characteristics $X_i$.&lt;/p&gt;
&lt;p&gt;$$
e(x) = \Pr \left( D_i = 1 \ \big | \ X_i = x \right)
$$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt; that in an AB test the propensity score is constant across individuals.&lt;/p&gt;
&lt;p&gt;The result from Rosenbaum and Rubin (1983) is incredibly &lt;strong&gt;powerful and practical&lt;/strong&gt;, since the propensity score is a &lt;strong&gt;one dimensional&lt;/strong&gt; variable, while $X$ might be very high dimensional.&lt;/p&gt;
&lt;p&gt;Under the &lt;strong&gt;unconfoundedness&lt;/strong&gt; assumption introduced above, we can rewrite the average treatment effect as&lt;/p&gt;
&lt;p&gt;$$
\tau(x) = \mathbb E \left[ Y^{(1)} - Y^{(0)} \ \big| \ X = x \right] = \mathbb E \left[ \frac{D_i Y_i}{e(X_i)} - \frac{(1-D_i) Y_i}{1-e(X_i)} \right]
$$&lt;/p&gt;
&lt;p&gt;Note that this formulation of the average treatment effect does not depend on the potential outcomes $Y_i^{(1)}$ and $Y_i^{(0)}$, but only on the observed outcomes $Y_i$.&lt;/p&gt;
&lt;p&gt;This formulation of the average treatment effect implies the &lt;strong&gt;Inverse Propensity Weighted (IPW)&lt;/strong&gt; estimator which is an unbiased estimator for the average treatment effect $\tau$.&lt;/p&gt;
&lt;p&gt;$$
\hat \tau^{IPW} = \frac{1}{n} \sum _ {i=1}^{n} \left( \frac{D_i Y_i}{e(X_i)} - \frac{(1-D_i) Y_i}{1-e(X_i)} \right)
$$&lt;/p&gt;
&lt;p&gt;This estimator is &lt;strong&gt;unfeasible&lt;/strong&gt; since we do not observe the propensity scores $e(X_i)$. However, we can estimate them. Actually, &lt;a href=&#34;https://onlinelibrary.wiley.com/doi/abs/10.1111/1468-0262.00442&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Imbens, Hirano, Ridder (2003)&lt;/a&gt; show that you &lt;strong&gt;should&lt;/strong&gt; use the estimated propensity scores even if you knew the true values (for example because you know the sampling procedure). The idea is that if the estimated propensity scores are different from the true ones, this can be informative in the estimation.&lt;/p&gt;
&lt;p&gt;There are several possible ways to estimate a probability, the simplest and most common one being &lt;a href=&#34;https://en.wikipedia.org/wiki/Logistic_regression&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;logistic regression&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from sklearn.linear_model import LogisticRegressionCV

df[&amp;quot;pscore&amp;quot;] = LogisticRegressionCV().fit(y=df[&amp;quot;dark_mode&amp;quot;], X=df[X]).predict_proba(df[X])[:,1]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It is best practice, whenever we fit a prediction model, to &lt;strong&gt;fit the model on a different sample&lt;/strong&gt; with respect to the one that we use for inference. This practice is usually called &lt;strong&gt;cross-validation&lt;/strong&gt; or cross-fitting. One of the best (but computationally expensive) cross-validation procedures is &lt;strong&gt;leave-one-out (LOO)&lt;/strong&gt; cross-fitting: when predicting the value of observation $i$ we use all observations except for $i$. We implement the LOO cross-fitting procedure using the &lt;code&gt;cross_val_predict&lt;/code&gt; and &lt;code&gt;LeaveOneOut&lt;/code&gt; functions from the &lt;a href=&#34;https://scikit-learn.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;sklearn&lt;/code&gt;&lt;/a&gt; package.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from sklearn.model_selection import cross_val_predict, LeaveOneOut

df[&#39;pscore&#39;] = cross_val_predict(estimator=LogisticRegressionCV(), 
                                 X=df[X], 
                                 y=df[&amp;quot;dark_mode&amp;quot;],
                                 cv=LeaveOneOut(),
                                 method=&#39;predict_proba&#39;,
                                 n_jobs=-1)[:,1]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;An &lt;strong&gt;important check&lt;/strong&gt; to perform after estimating propensity scores is plotting them, across the treatment and control groups. First of all, we can then observe whether the two groups are balanced or not, depending on how close the two distributions are. Moreover, we can also check how likely it is that the &lt;strong&gt;overlap assumption&lt;/strong&gt; is satisfied. Ideally both distributions should span the same interval.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sns.histplot(data=df, x=&#39;pscore&#39;, hue=&#39;dark_mode&#39;, bins=30, stat=&#39;density&#39;, common_norm=False).\
    set(ylabel=&amp;quot;&amp;quot;, title=&amp;quot;Distribution of Propensity Scores&amp;quot;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/weighting_matching_43_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;As expected, the distribution of propensity scores between the treatment and control group is &lt;strong&gt;significantly different&lt;/strong&gt;, suggesting that the two groups are hardly comparable. However, there is significant overlap in the support of the distributions, suggesting that the overlap assumption is likely to be satisfied.&lt;/p&gt;
&lt;p&gt;How do we estimate the average treatment effect?&lt;/p&gt;
&lt;p&gt;Once we have computed the propensity scores, we just need to re-weight observations by their respective propensity score. We can then either compute a difference between the weighted &lt;code&gt;read_time&lt;/code&gt; averages, or run a weighted regression of &lt;code&gt;read_time&lt;/code&gt; on &lt;code&gt;dark_mode&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;w = 1 / (df[&amp;quot;pscore&amp;quot;] * df[&amp;quot;dark_mode&amp;quot;] + (1-df[&amp;quot;pscore&amp;quot;]) * (1-df[&amp;quot;dark_mode&amp;quot;]))
smf.wls(&amp;quot;read_time ~ dark_mode&amp;quot;, weights=w, data=df).fit().summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
          &lt;td&gt;&lt;/td&gt;             &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt;         &lt;td&gt;   18.5859&lt;/td&gt; &lt;td&gt;    0.412&lt;/td&gt; &lt;td&gt;   45.110&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;   17.775&lt;/td&gt; &lt;td&gt;   19.397&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;dark_mode[T.True]&lt;/th&gt; &lt;td&gt;    1.1303&lt;/td&gt; &lt;td&gt;    0.582&lt;/td&gt; &lt;td&gt;    1.942&lt;/td&gt; &lt;td&gt; 0.053&lt;/td&gt; &lt;td&gt;   -0.015&lt;/td&gt; &lt;td&gt;    2.276&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;The effect of the &lt;code&gt;dark_mode&lt;/code&gt; is now positive and almost statistically significant, at the 5% level.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt; that the &lt;code&gt;wls&lt;/code&gt; function automatically normalizes weights so that they sum to 1, which greatly improves the stability of the estimator. In fact, the unnormalized IPW estimator can be very &lt;strong&gt;unstable&lt;/strong&gt; when the propensity scores approach zero or one.&lt;/p&gt;
&lt;p&gt;Also &lt;strong&gt;note&lt;/strong&gt; that the standard errors are not correct, since they do not take into account the extra uncertainty introduced in the estimation of the propensity score. This issue was noted by &lt;a href=&#34;https://onlinelibrary.wiley.com/doi/abs/10.3982/ECTA11293&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Abadie and Imbens (2016)&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;regression-with-control-variables&#34;&gt;Regression with Control Variables&lt;/h3&gt;
&lt;p&gt;The last method we are going to review today is &lt;strong&gt;linear regression with control variables&lt;/strong&gt;. This estimator is extremely easy to implement, since we just need to add the user characteristics - &lt;code&gt;gender&lt;/code&gt;, &lt;code&gt;age&lt;/code&gt; and &lt;code&gt;hours&lt;/code&gt; - to the regression of &lt;code&gt;read_time&lt;/code&gt; on &lt;code&gt;dark_mode&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;smf.ols(&amp;quot;read_time ~ dark_mode + male + age + hours&amp;quot;, data=df).fit().summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
          &lt;td&gt;&lt;/td&gt;             &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt;         &lt;td&gt;   16.8591&lt;/td&gt; &lt;td&gt;    1.082&lt;/td&gt; &lt;td&gt;   15.577&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;   14.729&lt;/td&gt; &lt;td&gt;   18.989&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;dark_mode[T.True]&lt;/th&gt; &lt;td&gt;    1.3858&lt;/td&gt; &lt;td&gt;    0.524&lt;/td&gt; &lt;td&gt;    2.646&lt;/td&gt; &lt;td&gt; 0.009&lt;/td&gt; &lt;td&gt;    0.355&lt;/td&gt; &lt;td&gt;    2.417&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;male&lt;/th&gt;              &lt;td&gt;   -4.4855&lt;/td&gt; &lt;td&gt;    0.499&lt;/td&gt; &lt;td&gt;   -8.990&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;   -5.468&lt;/td&gt; &lt;td&gt;   -3.504&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;age&lt;/th&gt;               &lt;td&gt;    0.0513&lt;/td&gt; &lt;td&gt;    0.022&lt;/td&gt; &lt;td&gt;    2.311&lt;/td&gt; &lt;td&gt; 0.022&lt;/td&gt; &lt;td&gt;    0.008&lt;/td&gt; &lt;td&gt;    0.095&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;hours&lt;/th&gt;             &lt;td&gt;    0.0043&lt;/td&gt; &lt;td&gt;    0.001&lt;/td&gt; &lt;td&gt;    8.427&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    0.003&lt;/td&gt; &lt;td&gt;    0.005&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;The average treatment effect is again positive and statistically significant at the 1% level!&lt;/p&gt;
&lt;h2 id=&#34;comparison&#34;&gt;Comparison&lt;/h2&gt;
&lt;p&gt;How do the different methods compare to each other?&lt;/p&gt;
&lt;h3 id=&#34;ipw-and-regression&#34;&gt;IPW and Regression&lt;/h3&gt;
&lt;p&gt;There is a &lt;strong&gt;tight connection&lt;/strong&gt; between the IPW estimator and linear regression with covariates. This is particularly evident when we have a one-dimensional, discrete covariate $X$.&lt;/p&gt;
&lt;p&gt;In this case, the estimand of IPW (i.e. the quantity that IPW estimates) is given by&lt;/p&gt;
&lt;p&gt;$$
\tau^{IPW} = \frac{ \sum_x \color{red}{\tau_x} \color{blue}{\Pr(D_i | X_i = x)} \Pr(X_i = x)}{\sum_x \color{blue}{\Pr(D_i | X_i = x)} \Pr(X_i = x)}
$$&lt;/p&gt;
&lt;p&gt;The IPW estimand is a weighted average of the treatment effects $\tau_x$, where the weights are given by the &lt;strong&gt;treatment probabilities&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;On the other hand, the estimand of linear regression with control variables is&lt;/p&gt;
&lt;p&gt;$$
\tau^{OLS} = \frac{ \sum_x \color{red}{\tau_x} \color{blue}{\Pr(D_i | X_i = x)(1 - \Pr(D_i | X_i = x)) } \Pr(X_i = x)}{\sum_x \color{blue}{\Pr(D_i | X_i = x)(1 - \Pr(D_i | X_i = x)) } \Pr(X_i = x)}
$$&lt;/p&gt;
&lt;p&gt;The OLS estimand is a weighted average of the treatment effects $\tau_x$, where the weights are given by the &lt;strong&gt;variances of the treatment probabilities&lt;/strong&gt;. This means that linear regression is a weighted estimator, that gives more weight to observations that have characteristics for which we observe more treatment variability. Since a binary random variable has the highest variance when its expected value is 0.5, &lt;strong&gt;OLS gives the most weight to observations that have characteristics for which we observe a 50/50 split between treatment and control group&lt;/strong&gt;. On the other hand, if for some characteristics we only observe treated or untreated individuals, those observations are going to receive zero weight. I recommend Chapter 3 of &lt;a href=&#34;https://www.mostlyharmlesseconometrics.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Angrist and Pischke (2009)&lt;/a&gt; for more details.&lt;/p&gt;
&lt;h3 id=&#34;ipw-and-matching&#34;&gt;IPW and Matching&lt;/h3&gt;
&lt;p&gt;As we have seen in the IPW section, &lt;a href=&#34;https://academic.oup.com/biomet/article/70/1/41/240879&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Rosenbaum and Rubin (1983)&lt;/a&gt; result tells us that we do not need to perform the analysis conditional on all the covariates $X$, but it is sufficient to condition on the propensity score $e(X)$.&lt;/p&gt;
&lt;p&gt;We have seed how this result implies a weighted estimator but it also extends to matching: we do not need to match observations on all the covariates $X$, but it is sufficient to &lt;strong&gt;match them on the propensity score&lt;/strong&gt; $e(X)$. This method is called propensity score matching.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;psm = NearestNeighborMatch(replace=False, random_state=1)
df_ipwmatched = psm.match(data=df, treatment_col=&amp;quot;dark_mode&amp;quot;, score_cols=[&#39;pscore&#39;])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As before, after matching, we can simply compute the estimate as a difference in means, remembering that observations are &lt;strong&gt;not independent&lt;/strong&gt; and therefore we need to be cautious when doing inference.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;smf.ols(&amp;quot;read_time ~ dark_mode&amp;quot;, data=df_ipwmatched)\
    .fit(cov_type=&#39;cluster&#39;, cov_kwds={&#39;groups&#39;: df_ipwmatched.index})\
    .summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
          &lt;td&gt;&lt;/td&gt;             &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;z&lt;/th&gt;      &lt;th&gt;P&gt;|z|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt;         &lt;td&gt;   18.4633&lt;/td&gt; &lt;td&gt;    0.505&lt;/td&gt; &lt;td&gt;   36.576&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;   17.474&lt;/td&gt; &lt;td&gt;   19.453&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;dark_mode[T.True]&lt;/th&gt; &lt;td&gt;    1.1888&lt;/td&gt; &lt;td&gt;    0.703&lt;/td&gt; &lt;td&gt;    1.692&lt;/td&gt; &lt;td&gt; 0.091&lt;/td&gt; &lt;td&gt;   -0.188&lt;/td&gt; &lt;td&gt;    2.566&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;The estimated effect of &lt;code&gt;dark_mode&lt;/code&gt; is positive, significant at the 1% level and very close to the true value of 2!&lt;/p&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;In this blog post, we have seen how to perform &lt;strong&gt;conditional analysis&lt;/strong&gt; using different approached. Matching directly matches most similar units in the treatment and control group. Weighting simply assigns different weight to different observations depending on their probability of receiving the treatment. Regression instead weights observations depending on the conditional treatment variances, giving more weight to observations that have characteristics common to both the treatment and control group.&lt;/p&gt;
&lt;p&gt;These procedures are &lt;strong&gt;extremely helpful&lt;/strong&gt; because they can either allow us to estimate causal effects from (very rich) observational data or correct experimental estimates when randomization was not perfect or we have a small sample.&lt;/p&gt;
&lt;p&gt;Last but not least, if you want to know more, I strongly recommend this &lt;strong&gt;video lecture&lt;/strong&gt; on propensity scores from &lt;a href=&#34;https://paulgp.github.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paul Goldsmith-Pinkham&lt;/a&gt; that is freely available online.&lt;/p&gt;
&lt;br&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/8gWctYvRzk4&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;br&gt;
&lt;p&gt;The whole course is a &lt;strong&gt;gem&lt;/strong&gt; and it is an incredible privilege to have such high quality material available online for free!&lt;/p&gt;
&lt;h3 id=&#34;references&#34;&gt;References&lt;/h3&gt;
&lt;p&gt;[1] P. Rosenbaum, D. Rubin, &lt;a href=&#34;https://academic.oup.com/biomet/article/70/1/41/240879&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The central role of the propensity score in observational studies for causal effects&lt;/a&gt; (1983), &lt;em&gt;Biometrika&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;[2] G. Imbens, K. Hirano, G. Ridder, &lt;a href=&#34;https://onlinelibrary.wiley.com/doi/abs/10.1111/1468-0262.00442&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Efficient Estimation of Average Treatment Effects Using the Estimated Propensity Score&lt;/a&gt; (2003), &lt;em&gt;Econometrica&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;[3] J. Angrist, J. S. Pischke, &lt;a href=&#34;https://www.mostlyharmlesseconometrics.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Mostly harmless econometrics: An Empiricist&amp;rsquo;s Companion&lt;/a&gt; (2009), &lt;em&gt;Princeton University Press&lt;/em&gt;.&lt;/p&gt;
&lt;h3 id=&#34;related-articles&#34;&gt;Related Articles&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/59f801eb3299&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Understanding The Frisch-Waugh-Lovell Theorem&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/9b06ee4d30bf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;How to Compare Two or More Distributions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/b63dc69e3d8c&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;DAGs and Control Variables&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;code&#34;&gt;Code&lt;/h3&gt;
&lt;p&gt;You can find the original Jupyter Notebook here:&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/ipw.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/ipw.ipynb&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Bayesian AB Testing</title>
      <link>https://matteocourthoud.github.io/post/bayes_ab/</link>
      <pubDate>Wed, 01 Feb 2023 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/post/bayes_ab/</guid>
      <description>&lt;p&gt;&lt;em&gt;An introduction to the Bayesian approach to randomized experiments.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Randomized experiments, a.k.a. &lt;strong&gt;AB tests&lt;/strong&gt;, are now the established gold standard in the industry to estimate causal effects. Randomly assigning the treatment (new product, feature, UI, &amp;hellip;) to a subset of the population (users, patients, customers, &amp;hellip;) we ensure that, on average, the difference in outcomes (revenue, visits, clicks, &amp;hellip;) can be attributed to the treatment. Established companies like &lt;a href=&#34;https://partner.booking.com/en-gb/click-magazine/industry-perspectives/role-experimentation-bookingcom&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Booking.com&lt;/a&gt; report constantly running thousands of AB tests at the same time. And newer growing companies like &lt;a href=&#34;https://blog.duolingo.com/improving-duolingo-one-experiment-at-a-time/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Duolingo&lt;/a&gt; attribute a large chunk of their success to their culture of experimentation at scale.&lt;/p&gt;
&lt;p&gt;With so many experiments, one question comes natural: in one specific experiment, can you leverage information from previous tests? How? In this post, I will try to answer these questions by introducing the &lt;strong&gt;Bayesian approach to AB testing&lt;/strong&gt;. The Bayesian framework is well suited for this type of task because it naturally allows for the updating of existing knowledge (the prior) using new data. However, the method is particularly sensitive to functional form assumptions and apparently innocuous model choices can translate in sensible differences in the estimates, especially when the data is very skewed.&lt;/p&gt;
&lt;h2 id=&#34;search-and-infinite-scrolling&#34;&gt;Search and Infinite Scrolling&lt;/h2&gt;
&lt;p&gt;For the rest of the article, we are going to use a toy example, loosely inspired by &lt;a href=&#34;https://www.aeaweb.org/articles?id=10.1257/pandp.20191003&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Azavedo et al. (2019)&lt;/a&gt;: a &lt;strong&gt;search engine&lt;/strong&gt; that wants to increase its &lt;strong&gt;ad revenue&lt;/strong&gt;, without sacrificing search quality. We are a company with an established experimentation culture and we continuously test new ideas on how to rank results, how to select the most relevant ads for consumers, and the user interface (UI) of the results page. Suppose that, in this specific case, we came up with a new brilliant idea: &lt;a href=&#34;https://blog.google/products/search/continuous-scrolling-mobile/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;infinite scrolling&lt;/a&gt;! Instead of having a discrete sequence of pages, we allow users to keep scrolling down if they want to see more results.&lt;/p&gt;
&lt;img src=&#34;fig/phones.jpg&#34; width=&#34;300px&#34;/&gt;
&lt;p&gt;To understand whether infinite scrolling works, we ran an &lt;strong&gt;AB test&lt;/strong&gt;: we randomize users into a treatment and a control group. We implement infinite scrolling only for users in the treatment group. I import the data generating process &lt;code&gt;dgp_infinite_scroll()&lt;/code&gt; from &lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/src/dgp.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;src.dgp&lt;/code&gt;&lt;/a&gt;. With respect to previous articles, I generated a new DGP parent class that handles randomization and data generation, while its children classes contain the specific use-cases. I also import some plotting functions and libraries from &lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/src/utils.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;src.utils&lt;/code&gt;&lt;/a&gt;.  To include not only code but also data and tables, I use &lt;a href=&#34;https://deepnote.com&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Deepnote&lt;/a&gt;, a Jupyter-like web-based collaborative notebook environment.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;%matplotlib inline
%config InlineBackend.figure_format = &#39;retina&#39;

from src.utils import *
from src.dgp import DGP, dgp_infinite_scroll
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;dgp = dgp_infinite_scroll(n=10_000)
df = dgp.generate_data(true_effect=0.14)
df.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;past_revenue&lt;/th&gt;
      &lt;th&gt;infinite_scroll&lt;/th&gt;
      &lt;th&gt;ad_revenue&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;3.76&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;3.56&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;2.40&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1.71&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;2.98&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;4.71&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;4.24&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;4.43&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;3.87&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;3.69&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;We have information on $10.000$ website visitors for which we observe the monthly &lt;code&gt;ad_revenue&lt;/code&gt; they generated, whether they were assigned to the treatment group and were using the &lt;code&gt;infinite_scroll&lt;/code&gt;, and also the average monthly &lt;code&gt;past_revenue&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;The random treatment assignment makes the &lt;strong&gt;difference-in-means&lt;/strong&gt; estimator &lt;a href=&#34;https://en.wikipedia.org/wiki/Bias_of_an_estimator&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;unbiased&lt;/strong&gt;&lt;/a&gt;: we expect the treatment and control group to be comparable on average, so we can causal attribute the average observed difference in outcomes to the treatment effect. We estimate the treatment effect by linear regression. We can interpret the coefficient of &lt;code&gt;infinite_scroll&lt;/code&gt; as the estimated treatment effect.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;smf.ols(&#39;ad_revenue ~ infinite_scroll&#39;, df).fit().summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
         &lt;td&gt;&lt;/td&gt;            &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt;       &lt;td&gt;    1.9906&lt;/td&gt; &lt;td&gt;    0.020&lt;/td&gt; &lt;td&gt;  100.783&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    1.952&lt;/td&gt; &lt;td&gt;    2.029&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;infinite_scroll&lt;/th&gt; &lt;td&gt;    0.1441&lt;/td&gt; &lt;td&gt;    0.028&lt;/td&gt; &lt;td&gt;    5.163&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    0.089&lt;/td&gt; &lt;td&gt;    0.199&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;It seems that the &lt;code&gt;infinite_scroll&lt;/code&gt; was indeed a good idea and it increase the average monthly revenue by $0.1524$$. Moreover, the effect is significantly different from zero at the 1% confidence level.&lt;/p&gt;
&lt;p&gt;We could further improve the precision of the estimator by controlling for &lt;code&gt;past_revenue&lt;/code&gt; in the regression. We do not expect a sensible change in the estimated coefficient, but the precision should improve (if you want to know more on out control variables, check my other articles on &lt;a href=&#34;https://towardsdatascience.com/understanding-cuped-a822523641af&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CUPED&lt;/a&gt; and &lt;a href=&#34;https://towardsdatascience.com/controls-b63dc69e3d8c&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;DAGs&lt;/a&gt;).&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;reg = smf.ols(&#39;ad_revenue ~ infinite_scroll + past_revenue&#39;, df).fit()
reg.summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
         &lt;td&gt;&lt;/td&gt;            &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt;       &lt;td&gt;    0.0170&lt;/td&gt; &lt;td&gt;    0.024&lt;/td&gt; &lt;td&gt;    0.696&lt;/td&gt; &lt;td&gt; 0.487&lt;/td&gt; &lt;td&gt;   -0.031&lt;/td&gt; &lt;td&gt;    0.065&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;infinite_scroll&lt;/th&gt; &lt;td&gt;    0.1588&lt;/td&gt; &lt;td&gt;    0.020&lt;/td&gt; &lt;td&gt;    7.992&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    0.120&lt;/td&gt; &lt;td&gt;    0.198&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;past_revenue&lt;/th&gt;    &lt;td&gt;    0.9923&lt;/td&gt; &lt;td&gt;    0.010&lt;/td&gt; &lt;td&gt;   98.659&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    0.973&lt;/td&gt; &lt;td&gt;    1.012&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;Indeed, &lt;code&gt;past_revenue&lt;/code&gt; is highly predictive of current &lt;code&gt;ad_revenue&lt;/code&gt; and the precision of the estimated coefficient for &lt;code&gt;infinite_scroll&lt;/code&gt; decreases by one-third.&lt;/p&gt;
&lt;p&gt;So far, everything has been very standard. However, as we said at the beginning, suppose this is not the only experiment we ran trying to improve our browser (and ultimately ad revenue). The infinite scroll is just one idea among thousands of others that we have tested in the past. Is there a way to efficiently use this additional &lt;strong&gt;information&lt;/strong&gt;?&lt;/p&gt;
&lt;h2 id=&#34;bayesian-statistics&#34;&gt;Bayesian Statistics&lt;/h2&gt;
&lt;p&gt;One of the main advantages of Bayesian statistics over the frequentist approach is that it easily allows to incorporate additional information into a model. The idea directly follows from the main results behind all Bayesian statistics: &lt;a href=&#34;https://en.wikipedia.org/wiki/Bayes%27_theorem&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Bayes Theorem&lt;/strong&gt;&lt;/a&gt;. Bayes theorem, allows you to do inference on a model by &lt;strong&gt;inverting the inference problem&lt;/strong&gt;: from the probability of the model given the data, to the probability of the data given the model, a much easier object to deal with.&lt;/p&gt;
&lt;p&gt;$$
\underbrace{ \Pr \big( \text{model} \ \big| \ \text{data} \big) }&lt;em&gt;{\text{posterior}} = \underbrace{ \Pr(\text{model}) }&lt;/em&gt;{\text{prior}} \ \underbrace{ \frac{ \Pr \big( \text{data} \ \big| \ \text{model} \big) }{ \Pr(\text{data}) } }_{\text{likelihood}}
$$&lt;/p&gt;
&lt;p&gt;We can split the right-hand side of Bayes Theorem (or Rule) into two components: the &lt;strong&gt;prior&lt;/strong&gt; and the &lt;strong&gt;likelihood&lt;/strong&gt;. The likelihood is the information about the model that comes from the data, the prior instead is any additional information about the model.&lt;/p&gt;
&lt;p&gt;First of all, let&amp;rsquo;s map Bayes theorem into our context. What is the data, what is the model and what is our object of interest?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the &lt;strong&gt;data&lt;/strong&gt; which consists in our outcome variable &lt;code&gt;ad_revenue&lt;/code&gt;, $y$, the treatment &lt;code&gt;infinite_scroll&lt;/code&gt;, $D$ and the other variables, &lt;code&gt;past_revenue&lt;/code&gt; and a constant, which we jointly denote as $X$&lt;/li&gt;
&lt;li&gt;the &lt;strong&gt;model&lt;/strong&gt; is the distribution of &lt;code&gt;ad_revenue&lt;/code&gt;, given &lt;code&gt;past_revenue&lt;/code&gt; and the &lt;code&gt;infinite_scroll&lt;/code&gt; feature, $y | D, X$&lt;/li&gt;
&lt;li&gt;our &lt;strong&gt;object of interest&lt;/strong&gt; is the posterior $\Pr \big( \text{model} \ \big| \ \text{data} \big)$, in particular the relationship between &lt;code&gt;ad_revenue&lt;/code&gt; and &lt;code&gt;infinite_scroll&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;X = sm.add_constant(df[[&#39;past_revenue&#39;]].values)
D = df[&#39;infinite_scroll&#39;].values
y = df[&#39;ad_revenue&#39;].values
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;How do we use prior information in the context of AB testing, potentially including additional covariates?&lt;/p&gt;
&lt;h3 id=&#34;bayesian-regression&#34;&gt;Bayesian Regression&lt;/h3&gt;
&lt;p&gt;Let&amp;rsquo;s use a linear model to make it directly comparable with the frequentist approach:&lt;/p&gt;
&lt;p&gt;$$
y_i = \beta X_i + \tau D_i + \varepsilon_i \qquad \text{where} \quad \varepsilon_i \sim N \big( 0, \sigma^2 \big)
$$&lt;/p&gt;
&lt;p&gt;This is a parametric model with &lt;strong&gt;two sets of parameters&lt;/strong&gt;: the linear coefficients $\beta$ and $\tau$, and the variance of the residuals $\sigma$. An equivalent, but more Bayesian, way to write the model is:&lt;/p&gt;
&lt;p&gt;$$
y \ | \ X, D; \beta, \tau, \sigma \sim N \Big( \beta X + \tau D \ , \sigma^2 \Big) ,
$$&lt;/p&gt;
&lt;p&gt;where the semi-column separates the data from the model parameters. Differently from the frequentist approach, in Bayesian regressions we do not rely on the &lt;a href=&#34;https://en.wikipedia.org/wiki/Central_limit_theorem&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;central limit theorem&lt;/a&gt; to approximate the conditional distribution of $y$, but we directly &lt;strong&gt;assume&lt;/strong&gt; it is normal. Is it just a formality? Not really, but a proper comparison between the frequentist and Bayesian approach is beyond the scope of this article.&lt;/p&gt;
&lt;p&gt;We are interested in doing inference on the model parameters, $\beta$, $\tau$, and $\sigma$. Another &lt;strong&gt;core difference&lt;/strong&gt; between the frequentist and the Bayesian approach is that the the first assumes that the model parameters are fixed (scalars), while the latter allows them to be stochastic (random variables).&lt;/p&gt;
&lt;p&gt;This assumption has a very practical &lt;strong&gt;implication&lt;/strong&gt;: you can easily incorporate previous information about the model parameters in the form of &lt;strong&gt;prior&lt;/strong&gt; distributions. As the name says, priors contain information that was available even &lt;em&gt;before&lt;/em&gt; looking at the data. This leads to one of the most relevant questions in Bayesian statistics: &lt;strong&gt;how do you chose a prior&lt;/strong&gt;?&lt;/p&gt;
&lt;h2 id=&#34;priors&#34;&gt;Priors&lt;/h2&gt;
&lt;p&gt;When choosing a prior, one analytically appealing restriction is to have a prior distribution such that the posterior belongs to the same family. These priors are called &lt;strong&gt;conjugate priors&lt;/strong&gt;. For example, before seeing the data, I assume my treatment effect is normally distributed and I would like it to be normally distributed also after incorporating the information contained in the data.&lt;/p&gt;
&lt;p&gt;In the case of Bayesian linear regression, the conjugate priors for $\beta$ and $\sigma$ are normally and inverse-gamma distributed. Let&amp;rsquo;s start a bit blindly, by taking a standard normal and inverse gamma distribution as prior.&lt;/p&gt;
&lt;p&gt;$$
\beta_i \sim N(\boldsymbol 0, \boldsymbol 1) \
\tau_i \sim N(0,1) \
\sigma^2 \sim \Gamma^{-1} (1, 1)
$$&lt;/p&gt;
&lt;p&gt;We use the package &lt;a href=&#34;https://www.pymc.io/projects/docs/en/stable/learn.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PyMC&lt;/a&gt; to do inference. First we need to specify the model: what are the distributions of the different parameters (priors) and what is the likelihood of the data.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import pymc as pm
with pm.Model() as baseline_model:

    # Priors
    beta = pm.MvNormal(&#39;beta&#39;, mu=np.ones(np.shape(X)[1]), cov=np.eye(np.shape(X)[1]))
    tau = pm.Normal(&#39;tau&#39;, mu=0, sigma=1)
    sigma = pm.InverseGamma(&#39;sigma&#39;, mu=1, sigma=1, initval=1)
    
    # Likelihood 
    Ylikelihood = pm.Normal(&#39;y&#39;, mu=(X@beta + D@tau).flatten(), sigma=sigma, observed=y)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;PyMC has an extremely nice function that allows us to visualize the model as a graph, &lt;code&gt;model_to_graphviz&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;pm.model_to_graphviz(baseline_model)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/bayes_ab_25_0.svg&#34; alt=&#34;svg&#34;&gt;&lt;/p&gt;
&lt;p&gt;From the graphical representation, we can see the various model components, their distributions, and how they interact with each other.&lt;/p&gt;
&lt;p&gt;We are now ready to &lt;strong&gt;compute&lt;/strong&gt; the model posterior. How does it work? In short, we sample realizations of model parameters, we compute the likelihood of the data given those values and the compute the corresponding posterior.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;idata = pm.sample(model=baseline_model, draws=1000)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Auto-assigning NUTS sampler...
Initializing NUTS using jitter+adapt_diag...
Multiprocess sampling (4 chains in 4 jobs)
NUTS: [beta, tau, sigma]
&lt;/code&gt;&lt;/pre&gt;
&lt;style&gt;
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
&lt;/style&gt;
&lt;div&gt;
  &lt;progress value=&#39;8000&#39; class=&#39;&#39; max=&#39;8000&#39; style=&#39;width:300px; height:20px; vertical-align: middle;&#39;&gt;&lt;/progress&gt;
  100.00% [8000/8000 00:03&amp;lt;00:00 Sampling 4 chains, 0 divergences]
&lt;/div&gt;
&lt;pre&gt;&lt;code&gt;Sampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 4_000 draws total) took 15 seconds.
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The fact that Bayesian inference requires sampling, has been historically one of the main bottlenecks of Bayesian statistics, since it makes it sensibly slower than the frequentist approach. However, this is less and less of a problem with the increased computational power of model computers.&lt;/p&gt;
&lt;p&gt;We are now ready to print out results. First, with the &lt;code&gt;summary()&lt;/code&gt; method, we can print a model summary very similar to those produced by the &lt;code&gt;statsmodels&lt;/code&gt; package.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;pm.summary(idata, hdi_prob=0.95).round(4)
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;mean&lt;/th&gt;
      &lt;th&gt;sd&lt;/th&gt;
      &lt;th&gt;hdi_2.5%&lt;/th&gt;
      &lt;th&gt;hdi_97.5%&lt;/th&gt;
      &lt;th&gt;mcse_mean&lt;/th&gt;
      &lt;th&gt;mcse_sd&lt;/th&gt;
      &lt;th&gt;ess_bulk&lt;/th&gt;
      &lt;th&gt;ess_tail&lt;/th&gt;
      &lt;th&gt;r_hat&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;beta[0]&lt;/th&gt;
      &lt;td&gt;0.017&lt;/td&gt;
      &lt;td&gt;0.025&lt;/td&gt;
      &lt;td&gt;-0.029&lt;/td&gt;
      &lt;td&gt;0.067&lt;/td&gt;
      &lt;td&gt;0.001&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;1765.0&lt;/td&gt;
      &lt;td&gt;2233.0&lt;/td&gt;
      &lt;td&gt;1.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;beta[1]&lt;/th&gt;
      &lt;td&gt;0.992&lt;/td&gt;
      &lt;td&gt;0.010&lt;/td&gt;
      &lt;td&gt;0.972&lt;/td&gt;
      &lt;td&gt;1.012&lt;/td&gt;
      &lt;td&gt;0.000&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;1810.0&lt;/td&gt;
      &lt;td&gt;1964.0&lt;/td&gt;
      &lt;td&gt;1.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;tau&lt;/th&gt;
      &lt;td&gt;0.159&lt;/td&gt;
      &lt;td&gt;0.020&lt;/td&gt;
      &lt;td&gt;0.120&lt;/td&gt;
      &lt;td&gt;0.200&lt;/td&gt;
      &lt;td&gt;0.000&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;2885.0&lt;/td&gt;
      &lt;td&gt;1792.0&lt;/td&gt;
      &lt;td&gt;1.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;sigma&lt;/th&gt;
      &lt;td&gt;0.993&lt;/td&gt;
      &lt;td&gt;0.007&lt;/td&gt;
      &lt;td&gt;0.980&lt;/td&gt;
      &lt;td&gt;1.008&lt;/td&gt;
      &lt;td&gt;0.000&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;3537.0&lt;/td&gt;
      &lt;td&gt;2692.0&lt;/td&gt;
      &lt;td&gt;1.0&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;The estimated parameters are extremely close to the ones we got with the frequentist approach, with an estimated effect of the &lt;code&gt;infinite_scroll&lt;/code&gt; equal to $0.157$.&lt;/p&gt;
&lt;p&gt;If sampling had the disadvantage of being slow, it has the advantage of being very &lt;strong&gt;transparent&lt;/strong&gt;. We can directly plot the distribution of the posterior. Let&amp;rsquo;s do it for the treatment effect $\tau$. The PyMC function &lt;code&gt;plot_posterior&lt;/code&gt; plots the distribution of the posterior, with a black bar for the Bayesian equivalent of a 95% confidence interval.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;pm.plot_posterior(idata, kind=&amp;quot;hist&amp;quot;, var_names=(&#39;tau&#39;), hdi_prob=0.95, figsize=(6, 3), bins=30); 
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/bayes_ab_31_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;As expected, since we chose conjugate priors, the posterior distribution looks gaussian.&lt;/p&gt;
&lt;p&gt;So far we have chosen the prior without much guidance. However, suppose we had access to past experiments. How do we incorporate this specific information?&lt;/p&gt;
&lt;h2 id=&#34;past-experiments&#34;&gt;Past Experiments&lt;/h2&gt;
&lt;p&gt;Suppose the idea of the infinite scroll, was just one among a ton of other ones that we tried and tested in the past. For each idea we have the data for the corresponding experiment, with the corresponding estimated coefficient. Suppose we had a thousand of them.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;past_experiments = [dgp.generate_data(seed_data=i) for i in range(1000)]
taus = [smf.ols(&#39;ad_revenue ~ infinite_scroll + past_revenue&#39;, pe).fit().params.values for pe in past_experiments]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;How do we use this additional information?&lt;/p&gt;
&lt;h3 id=&#34;normal-prior&#34;&gt;Normal Prior&lt;/h3&gt;
&lt;p&gt;The first idea could be to calibrate our prior to reflect the data distribution in the past. Keeping the normality assumption, we use the estimated average and standard deviations of the estimates from past experiments.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;taus_mean = np.mean(taus, axis=0)[1]
taus_mean
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;0.0047987091716528915
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;On average, had practically no effect on &lt;code&gt;ad_revenue&lt;/code&gt;, with a average effect of $0.0009$.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;taus_std = np.sqrt(np.cov(taus, rowvar=0)[1,1])
taus_std
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;0.15153398725701195
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;However, there was sensible variation across experiments, with a standard deviation of $0.029$.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s now estimate the&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;with pm.Model() as model_normal_prior:
    k = np.shape(X)[1]
    beta = pm.MvNormal(&#39;beta&#39;, mu=np.ones(k), cov=np.eye(k))
    tau = pm.Normal(&#39;tau&#39;, mu=taus_mean, sigma=taus_std)
    sigma = pm.InverseGamma(&#39;sigma&#39;, mu=1, sigma=1, initval=1)
    Ylikelihood = pm.Normal(&#39;y&#39;, mu=(X@beta + D@tau).flatten(), sigma=sigma, observed=y)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s sample from the model.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;idata_normal_prior = pm.sample(model=model_normal_prior, draws=1000)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Auto-assigning NUTS sampler...
Initializing NUTS using jitter+adapt_diag...
Multiprocess sampling (4 chains in 4 jobs)
NUTS: [beta, tau, sigma]
&lt;/code&gt;&lt;/pre&gt;
&lt;style&gt;
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
&lt;/style&gt;
&lt;div&gt;
  &lt;progress value=&#39;8000&#39; class=&#39;&#39; max=&#39;8000&#39; style=&#39;width:300px; height:20px; vertical-align: middle;&#39;&gt;&lt;/progress&gt;
  100.00% [8000/8000 00:03&amp;lt;00:00 Sampling 4 chains, 0 divergences]
&lt;/div&gt;
&lt;pre&gt;&lt;code&gt;Sampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 4_000 draws total) took 14 seconds.
The acceptance probability does not match the target. It is 0.9025, but should be close to 0.8. Try to increase the number of tuning steps.
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And plot the sample posterior distribution of the treatment effect parameter $\tau$.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;pm.plot_posterior(idata_normal_prior, kind=&amp;quot;hist&amp;quot;, var_names=(&#39;tau&#39;), hdi_prob=0.95, figsize=(6, 3), bins=30); 
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/bayes_ab_47_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;The estimated coefficient is sensibly smaller: $0.08$ instead of the previous estimate of $0.12$. Why is it the case?&lt;/p&gt;
&lt;p&gt;The fact is that the previous coefficient of $0.12$ is extremely unlikey, given our prior. We can compute the probability of getting the same or a more extreme value, given the prior.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;1 - sp.stats.norm(taus_mean, taus_std).cdf(0.12)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;0.22355735943737898
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The probability of such value is almost zero. Therefore, the estimated coefficient has moved towards the prior mean of $0.0009$.&lt;/p&gt;
&lt;h3 id=&#34;student-t-prior&#34;&gt;Student t Prior&lt;/h3&gt;
&lt;p&gt;So far, we have assumed a normal distribution for all linear coefficients. Is it appropriate? Let&amp;rsquo;s check it visually (check &lt;a href=&#34;https://medium.com/towards-data-science/how-to-compare-two-or-more-distributions-9b06ee4d30bf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt; for other methods on how to compare distributions).&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sns.histplot([tau[0] for tau in taus]).set(title=r&#39;Distribution of $\hat{\beta}_0$ in past experiments&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/bayes_ab_53_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;The distribution seems pretty normal. What the treatment effect paramenter $\tau$?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;fig, ax = plt.subplots()
sns.histplot([tau[1] for tau in taus], label=&#39;past experiments&#39;);
ax.axvline(reg.params[&#39;infinite_scroll&#39;], lw=2, c=&#39;C3&#39;, ls=&#39;--&#39;, label=&#39;current experiment&#39;)
plt.legend();
plt.title(r&#39;Distribution of $\hat{\tau}$ in past experiments&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/bayes_ab_55_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;The distribution very &lt;strong&gt;heavy tailed&lt;/strong&gt;! While at the center it looks like a normal distributions, the tails are much &amp;ldquo;fatter&amp;rdquo; and we have a couple of very extreme values. excluding the case of measurement error, this is a setting that happens often in the industry, where most ideas have extremely small or null effects and very rarely an idea is actually a breakthrough.&lt;/p&gt;
&lt;p&gt;One way to model this distribution is a &lt;a href=&#34;&#34;&gt;student-t distribution&lt;/a&gt;. In particular, we use a t-student with mean $0.0009$, variance $0.003$ and $1.3$ degrees of freedom to match the moments of the empirical distributions of past estimates.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;with pm.Model() as model_studentt_prior:

    # Priors
    k = np.shape(X)[1]
    beta = pm.MvNormal(&#39;beta&#39;, mu=np.ones(k), cov=np.eye(k))
    tau = pm.StudentT(&#39;tau&#39;, mu=taus_mean, sigma=0.003, nu=1.3)
    sigma = pm.InverseGamma(&#39;sigma&#39;, mu=1, sigma=1, initval=1)
    
    # Likelihood 
    Ylikelihood = pm.Normal(&#39;y&#39;, mu=(X@beta + D@tau).flatten(), sigma=sigma, observed=y)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s sample from the model.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;idata_studentt_priors = pm.sample(model=model_studentt_prior, draws=1000)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Auto-assigning NUTS sampler...
Initializing NUTS using jitter+adapt_diag...
Multiprocess sampling (4 chains in 4 jobs)
NUTS: [beta, tau, sigma]
&lt;/code&gt;&lt;/pre&gt;
&lt;style&gt;
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
&lt;/style&gt;
&lt;div&gt;
  &lt;progress value=&#39;8000&#39; class=&#39;&#39; max=&#39;8000&#39; style=&#39;width:300px; height:20px; vertical-align: middle;&#39;&gt;&lt;/progress&gt;
  100.00% [8000/8000 00:03&amp;lt;00:00 Sampling 4 chains, 0 divergences]
&lt;/div&gt;
&lt;pre&gt;&lt;code&gt;Sampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 4_000 draws total) took 15 seconds.
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And plot the sample posterior distribution of the treatment effect parameter $\tau$.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;pm.plot_posterior(idata_studentt_priors, kind=&amp;quot;hist&amp;quot;, var_names=(&#39;tau&#39;), hdi_prob=0.95, figsize=(6, 3), bins=30); 
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/bayes_ab_61_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;The estimated coefficient is now again similar to the one we got with the standard normal prior, $0.11$. However, the estimate is more precise since the confidence interval has shrunk from $[0.077, 0.016]$ to $[0.065, 0.015]$.&lt;/p&gt;
&lt;p&gt;What has happened?&lt;/p&gt;
&lt;h3 id=&#34;shrinking&#34;&gt;Shrinking&lt;/h3&gt;
&lt;p&gt;The answer lies in the shape of the different &lt;strong&gt;prior distributions&lt;/strong&gt; that we have used:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;standard normal, $N(0,1)$&lt;/li&gt;
&lt;li&gt;normal with matched moments, $N(0, 0.03)$&lt;/li&gt;
&lt;li&gt;t-student with matched moments, $t_{1.3}$(0, 0.003)$&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;t_hats = np.linspace(-0.3, 0.3, 1_000)
distributions = {
    &#39;N(0,1)&#39;: sp.stats.norm(0, 1).pdf(t_hats),
    &#39;N(0, 0.03)&#39;: sp.stats.norm(0, 0.03).pdf(t_hats),
    &#39;$t_{1.3}$(0, 0.003)&#39;: sp.stats.t(df=1.3).pdf(t_hats / 0.003)*300,
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s plot all of them together.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;for i, (label, y) in enumerate(distributions.items()):
    sns.lineplot(x=t_hats, y=y, color=f&#39;C{i}&#39;, label=label);
plt.legend(); 
plt.title(&#39;Prior Distributions&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/bayes_ab_67_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;As we can see, all distributions are centered on zero, but they have very different shapes. The standard normal distribution is essentially flat over the $[-0.15, 0.15]$ interval. Every value has basically the same probability. The last two instead, even though they have the same mean and variance, have very different shapes.&lt;/p&gt;
&lt;p&gt;How does it translate into our estimation? We can plot the implied posterior for different estimates, for each prior distribution.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def compute_posterior(b, prior):
    likelihood = sp.stats.norm(b, taus_std).pdf(t_hats)
    return np.average(t_hats, weights=prior*likelihood)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;fig, ax = plt.subplots(figsize=(7,6))
ax.axvline(0, lw=1.5, c=&#39;k&#39;);
ax.axhline(0, lw=1.5, c=&#39;k&#39;);
ax.axvline(reg.params[&#39;infinite_scroll&#39;], lw=2, ls=&#39;--&#39;, c=&#39;darkgray&#39;);
for i, (label, y) in enumerate(distributions.items()):
    sns.lineplot(x=t_hats, y=[compute_posterior(t, y) for t in t_hats] , color=f&#39;C{i}&#39;, label=label);
ax.set_xlim(-0.17, 0.17);
ax.set_ylim(-0.17, 0.17);
plt.legend(); 
ax.set_xlabel(&#39;Experiment Estimate&#39;);
ax.set_ylabel(&#39;Posterior&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/bayes_ab_70_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;As we can see, the different priors transform the experimental estimates in very different ways. The standard normal prior essentially has no effect for estimates in the $[-0.15, 0.15]$ interval. The normal prior with matched moments instead shrinks each estimate by approximately 2/3. The effect of the t-student prior is instead &lt;strong&gt;non-linear&lt;/strong&gt;: it shrinks small estimates towards zero, while it keeps large estimates as they are.&lt;/p&gt;
&lt;p&gt;My &lt;strong&gt;intuition&lt;/strong&gt; is the following. A prior distribution very skewed or with &amp;ldquo;fat tails&amp;rdquo; means that large values are rare but not impossible. In practice, it means accepting that breakthrough improvements are possible. On the other hand, for the same variance, the distribution is more concentrated around zero than a standard normal so that small values are shrunk even more.&lt;/p&gt;
&lt;img src=&#34;fig/scroll.jpg&#34; width=&#34;300px&#34;/&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;In this article we have seen how to extend the analysis of AB test to incorporate &lt;strong&gt;information from past experiments&lt;/strong&gt;. In particular, we have seen the importance of choosing a prior. Selecting the distribution function is just as important as tuning its parameters. The shape of the prior distribution can drastically affect our inference, especially in a world with skewed distributions.&lt;/p&gt;
&lt;p&gt;Despite the length of the article, this was just a glimpse in the world of &lt;strong&gt;AB testing and Bayesian statistics&lt;/strong&gt;. While being computationally more intensive and requiring additional assumptions, the Bayesian approach is often more natural, powerful and flexible than the frequentist one. Knowing pros and cons of both approaches is crucial to get the best of both worlds, picking the approach that work best or combining them efficiently.&lt;/p&gt;
&lt;h3 id=&#34;references&#34;&gt;References&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;E. Azevedo, A. Deng, J. Olea, G. Weyl, &lt;a href=&#34;https://www.aeaweb.org/articles?id=10.1257/pandp.20191003&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Empirical Bayes Estimation of Treatment Effects with Many A/B Tests: An Overview&lt;/a&gt; (2019). &lt;em&gt;AEA Papers and Proceedings&lt;/em&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;A. Deng, &lt;a href=&#34;https://dl.acm.org/doi/abs/10.1145/2740908.2742563&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Objective Bayesian Two Sample Hypothesis Testing for Online Controlled Experiments&lt;/a&gt; (2018), &lt;em&gt;WWW15&lt;/em&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;related-articles&#34;&gt;Related Articles&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://medium.com/towards-data-science/the-bayesian-bootstrap-6ca4a1d45148&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The Bayesian Bootstrap&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://towardsdatascience.com/understanding-cuped-a822523641af&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Understanding CUPED&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://towardsdatascience.com/controls-b63dc69e3d8c&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;DAGs and Control Variables&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;code&#34;&gt;Code&lt;/h3&gt;
&lt;p&gt;You can find the original Jupyter Notebook here:&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/bayes_ab.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/bayes_ab.ipynb&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Bayesian AB Testing</title>
      <link>https://matteocourthoud.github.io/post/bayes_reg/</link>
      <pubDate>Mon, 09 Jan 2023 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/post/bayes_reg/</guid>
      <description>&lt;p&gt;&lt;em&gt;An introduction to the Bayesian approach to randomized experiments.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;AB testing&lt;/strong&gt; and experimentation is now the established gold standard in the industry to estimate causal effects. Randomly assigning the treatment (new product, feature, UI, &amp;hellip;) to a subset of the population (users, patients, customers, &amp;hellip;) we ensure that, on average, the difference in outcomes (revenue, visits, clicks, &amp;hellip;) can be attributed to the treatment. Many companies, especially in tech, before implementing any major change test them to back up their decisions with numbers. Established companies like &lt;a href=&#34;https://partner.booking.com/en-gb/click-magazine/industry-perspectives/role-experimentation-bookingcom&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Booking.com&lt;/a&gt; report constantly running thousands of AB tests at the same time. And newer growing companies like &lt;a href=&#34;https://blog.duolingo.com/improving-duolingo-one-experiment-at-a-time/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Duolingo&lt;/a&gt; attribute a large chunk of their success to their culture of experimentation at scale.&lt;/p&gt;
&lt;p&gt;With so many experiments, one question comes natural: in one specific experiment, can you leverage information from previous tests? In this post, I will try to answer this question by introducing the &lt;strong&gt;Bayesian approach to AB testing&lt;/strong&gt;. The Bayesian framework is particularly well suited for this type of task because it naturally allows for the updating of existing knowledge (the prior) using new data. However, the method is particularly sensitive to functional form assumptions and apparently innocuous model choices can translate in sensible differences in the estimates, especially when the data is very skewed.&lt;/p&gt;
&lt;h2 id=&#34;search-and-infinite-scrolling&#34;&gt;Search and Infinite Scrolling&lt;/h2&gt;
&lt;p&gt;For the rest of the article, we are going to use a toy example, loosely inspired by &lt;a href=&#34;https://www.aeaweb.org/articles?id=10.1257/pandp.20191003&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Azavedo et al. (2019)&lt;/a&gt;: a &lt;strong&gt;search engine&lt;/strong&gt; that wants to increase its &lt;strong&gt;ad revenue&lt;/strong&gt;, without sacrificing search quality. We are a company with an established experimentation culture and we continuously test new ideas on how to rank results, how to select the most relevant ads for consumers, and the user interface (UI) of the results page. Suppose that, in this specific case, we came up with a new brilliant idea: &lt;a href=&#34;https://blog.google/products/search/continuous-scrolling-mobile/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;infinite scrolling&lt;/a&gt;! Instead of having a discrete sequence of pages, we allow users to keep scrolling down if they want to see more results.&lt;/p&gt;
&lt;img src=&#34;fig/phones.jpg&#34; width=&#34;300px&#34;/&gt;
&lt;p&gt;To understand whether infinite scrolling works, we ran an &lt;strong&gt;AB test&lt;/strong&gt;: we randomize users into a treatment and a control group. We implement infinite scrolling only for users in the treatment group. I import the data generating process &lt;code&gt;dgp_infinite_scroll()&lt;/code&gt; from &lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/src/dgp.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;src.dgp&lt;/code&gt;&lt;/a&gt;. With respect to previous articles, I generated a new DGP parent class that handles randomization and data generation, while its children classes contain the specific use-cases. I also import some plotting functions and libraries from &lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/src/utils.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;src.utils&lt;/code&gt;&lt;/a&gt;.  To include not only code but also data and tables, I use &lt;a href=&#34;https://deepnote.com&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Deepnote&lt;/a&gt;, a Jupyter-like web-based collaborative notebook environment.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;%matplotlib inline
%config InlineBackend.figure_format = &#39;retina&#39;

from src.utils import *
from src.dgp import DGP, dgp_infinite_scroll
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;dgp = dgp_infinite_scroll(n=10_000)
df = dgp.generate_data(true_effect=0.14)
df.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;past_revenue&lt;/th&gt;
      &lt;th&gt;infinite_scroll&lt;/th&gt;
      &lt;th&gt;ad_revenue&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;3.76&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;3.70&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;2.40&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1.71&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;2.98&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;4.85&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;4.24&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;4.57&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;3.87&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;3.69&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;We have information on $10.000$ website visitors for which we observe the monthly &lt;code&gt;ad_revenue&lt;/code&gt; they generated, whether they were assigned to the treatment group and were using the &lt;code&gt;infinite_scroll&lt;/code&gt;, and also the average monthly &lt;code&gt;past_revenue&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;The random treatment assignment makes the &lt;strong&gt;difference-in-means&lt;/strong&gt; estimator &lt;a href=&#34;https://en.wikipedia.org/wiki/Bias_of_an_estimator&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;unbiased&lt;/strong&gt;&lt;/a&gt;: we expect the treatment and control group to be comparable on average, so we can causal attribute the average observed difference in outcomes to the treatment effect. We estimate the treatment effect by linear regression. We can interpret the coefficient of &lt;code&gt;infinite_scroll&lt;/code&gt; as the estimated treatment effect.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;smf.ols(&#39;ad_revenue ~ infinite_scroll&#39;, df).fit().summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
         &lt;td&gt;&lt;/td&gt;            &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt;       &lt;td&gt;    1.9865&lt;/td&gt; &lt;td&gt;    0.020&lt;/td&gt; &lt;td&gt;  101.320&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    1.948&lt;/td&gt; &lt;td&gt;    2.025&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;infinite_scroll&lt;/th&gt; &lt;td&gt;    0.1524&lt;/td&gt; &lt;td&gt;    0.028&lt;/td&gt; &lt;td&gt;    5.461&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    0.098&lt;/td&gt; &lt;td&gt;    0.207&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;It seems that the &lt;code&gt;infinite_scroll&lt;/code&gt; was indeed a good idea and it increase the average monthly revenue by $0.1524$$. Moreover, the effect is significantly different from zero at the 1% confidence level.&lt;/p&gt;
&lt;p&gt;We could further improve the precision of the estimator by controlling for &lt;code&gt;past_revenue&lt;/code&gt; in the regression. We do not expect a sensible change in the estimated coefficient, but the precision should improve (if you want to know more on out control variables, check my other articles on &lt;a href=&#34;https://towardsdatascience.com/understanding-cuped-a822523641af&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CUPED&lt;/a&gt; and &lt;a href=&#34;https://towardsdatascience.com/controls-b63dc69e3d8c&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;DAGs&lt;/a&gt;).&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;reg = smf.ols(&#39;ad_revenue ~ infinite_scroll + past_revenue&#39;, df).fit()
reg.summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
         &lt;td&gt;&lt;/td&gt;            &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt;       &lt;td&gt;    0.0181&lt;/td&gt; &lt;td&gt;    0.024&lt;/td&gt; &lt;td&gt;    0.741&lt;/td&gt; &lt;td&gt; 0.459&lt;/td&gt; &lt;td&gt;   -0.030&lt;/td&gt; &lt;td&gt;    0.066&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;infinite_scroll&lt;/th&gt; &lt;td&gt;    0.1571&lt;/td&gt; &lt;td&gt;    0.020&lt;/td&gt; &lt;td&gt;    7.910&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    0.118&lt;/td&gt; &lt;td&gt;    0.196&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;past_revenue&lt;/th&gt;    &lt;td&gt;    0.9922&lt;/td&gt; &lt;td&gt;    0.010&lt;/td&gt; &lt;td&gt;   98.655&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    0.972&lt;/td&gt; &lt;td&gt;    1.012&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;Indeed, &lt;code&gt;past_revenue&lt;/code&gt; is highly predictive of current &lt;code&gt;ad_revenue&lt;/code&gt; and the precision of the estimated coefficient for &lt;code&gt;infinite_scroll&lt;/code&gt; decreases by one-third.&lt;/p&gt;
&lt;p&gt;So far, everything has been very standard. However, as we said at the beginning, suppose this is not the only experiment we ran trying to improve our browser (and ultimately ad revenue). The infinite scroll is just one idea among thousands of others that we have tested in the past. Is there a way to efficiently use this additional &lt;strong&gt;information&lt;/strong&gt;?&lt;/p&gt;
&lt;h2 id=&#34;bayesian-statistics&#34;&gt;Bayesian Statistics&lt;/h2&gt;
&lt;p&gt;One of the main advantages of Bayesian statistics over the frequentist approach is that it easily allows to incorporate additional information into a model. The idea directly follows from the main results behind all Bayesian statistics: &lt;a href=&#34;https://en.wikipedia.org/wiki/Bayes%27_theorem&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Bayes Theorem&lt;/strong&gt;&lt;/a&gt;. Bayes theorem, allows you to do inference on a model by &lt;strong&gt;inverting the inference problem&lt;/strong&gt;: from the probability of the model given the data, to the probability of the data given the model, a much easier object to deal with.&lt;/p&gt;
&lt;p&gt;$$
\underbrace{ \Pr \big( \text{model} \ \big| \ \text{data} \big) }&lt;em&gt;{\text{posterior}} = \underbrace{ \Pr(\text{model}) }&lt;/em&gt;{\text{prior}} \ \underbrace{ \frac{ \Pr \big( \text{data} \ \big| \ \text{model} \big) }{ \Pr(\text{data}) } }_{\text{likelihood}}
$$&lt;/p&gt;
&lt;p&gt;We can split the right-hand side of Bayes Theorem (or Rule) into two components: the &lt;strong&gt;prior&lt;/strong&gt; and the &lt;strong&gt;likelihood&lt;/strong&gt;. The likelihood is the information about the model that comes from the data, the prior instead is any additional information about the model.&lt;/p&gt;
&lt;p&gt;First of all, let&amp;rsquo;s map Bayes theorem into our context. What is the data, what is the model and what is our object of interest?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the &lt;strong&gt;data&lt;/strong&gt; which consists in our outcome variable &lt;code&gt;ad_revenue&lt;/code&gt;, $y$, the treatment &lt;code&gt;infinite_scroll&lt;/code&gt;, $D$ and the other variables, &lt;code&gt;past_revenue&lt;/code&gt; and a constant, which we jointly denote as $X$&lt;/li&gt;
&lt;li&gt;the &lt;strong&gt;model&lt;/strong&gt; is the distribution of &lt;code&gt;ad_revenue&lt;/code&gt;, given &lt;code&gt;past_revenue&lt;/code&gt; and the &lt;code&gt;infinite_scroll&lt;/code&gt; feature, $y | D, X$&lt;/li&gt;
&lt;li&gt;our &lt;strong&gt;object of interest&lt;/strong&gt; is the posterior $\Pr \big( \text{model} \ \big| \ \text{data} \big)$, in particular the relationship between &lt;code&gt;ad_revenue&lt;/code&gt; and &lt;code&gt;infinite_scroll&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;X = sm.add_constant(df[[&#39;past_revenue&#39;]].values)
D = df[&#39;infinite_scroll&#39;].values
y = df[&#39;ad_revenue&#39;].values
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;How do we use prior information in the context of AB testing, potentially including additional covariates?&lt;/p&gt;
&lt;h3 id=&#34;bayesian-regression&#34;&gt;Bayesian Regression&lt;/h3&gt;
&lt;p&gt;Let&amp;rsquo;s use a linear model to make it directly comparable with the frequentist approach:&lt;/p&gt;
&lt;p&gt;$$
y_i = \beta X_i + \tau D_i + \varepsilon_i \qquad \text{where} \quad \varepsilon_i \sim N \big( 0, \sigma^2 \big)
$$&lt;/p&gt;
&lt;p&gt;This is a parametric model with &lt;strong&gt;two sets of parameters&lt;/strong&gt;: the linear coefficients $\beta$ and $\tau$, and the variance of the residuals $\sigma$. An equivalent, but more Bayesian, way to write the model is:&lt;/p&gt;
&lt;p&gt;$$
y \ | \ X, D; \beta, \tau, \sigma \sim N \Big( \beta X + \tau D \ , \sigma^2 \Big) ,
$$&lt;/p&gt;
&lt;p&gt;where the semi-column separates the data from the model parameters. Differently from the frequentist approach, in Bayesian regressions we do not rely on the &lt;a href=&#34;https://en.wikipedia.org/wiki/Central_limit_theorem&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;central limit theorem&lt;/a&gt; to approximate the conditional distribution of $y$, but we directly &lt;strong&gt;assume&lt;/strong&gt; it is normal. Is it just a formality? Not really, but a proper comparison between the frequentist and Bayesian approach is beyond the scope of this article.&lt;/p&gt;
&lt;p&gt;We are interested in doing inference on the model parameters, $\beta$, $\tau$, and $\sigma$. Another &lt;strong&gt;core difference&lt;/strong&gt; between the frequentist and the Bayesian approach is that the the first assumes that the model parameters are fixed (scalars), while the latter allows them to be stochastic (random variables).&lt;/p&gt;
&lt;p&gt;This assumption has a very practical &lt;strong&gt;implication&lt;/strong&gt;: you can easily incorporate previous information about the model parameters in the form of &lt;strong&gt;prior&lt;/strong&gt; distributions. As the name says, priors contain information that was available even &lt;em&gt;before&lt;/em&gt; looking at the data. This leads to one of the most relevant questions in Bayesian statistics: &lt;strong&gt;how do you chose a prior&lt;/strong&gt;?&lt;/p&gt;
&lt;h2 id=&#34;priors&#34;&gt;Priors&lt;/h2&gt;
&lt;p&gt;When choosing a prior, one analytically appealing restriction is to have a prior distribution such that the posterior belongs to the same family. These priors are called &lt;strong&gt;conjugate priors&lt;/strong&gt;. For example, before seeing the data, I assume my treatment effect is normally distributed and I would like it to be normally distributed also after incorporating the information contained in the data.&lt;/p&gt;
&lt;p&gt;In the case of Bayesian linear regression, the conjugate priors for $\beta$ and $\sigma$ are normally and inverse-gamma distributed. Let&amp;rsquo;s start a bit blindly, by taking a standard normal and inverse gamma distribution as prior.&lt;/p&gt;
&lt;p&gt;$$
\beta_i \sim N(\boldsymbol 0, \boldsymbol 1) \
\tau_i \sim N(0,1) \
\sigma^2 \sim \Gamma^{-1} (1, 1)
$$&lt;/p&gt;
&lt;p&gt;We use the package &lt;a href=&#34;https://www.pymc.io/projects/docs/en/stable/learn.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PyMC&lt;/a&gt; to do inference. First we need to specify the model: what are the distributions of the different parameters (priors) and what is the likelihood of the data.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import pymc as pm
with pm.Model() as baseline_model:

    # Priors
    beta = pm.MvNormal(&#39;beta&#39;, mu=np.ones(np.shape(X)[1]), cov=np.eye(np.shape(X)[1]))
    tau = pm.Normal(&#39;tau&#39;, mu=0, sigma=1)
    sigma = pm.InverseGamma(&#39;sigma&#39;, mu=1, sigma=1, initval=1)
    
    # Likelihood 
    Ylikelihood = pm.Normal(&#39;y&#39;, mu=(X@beta + D@tau).flatten(), sigma=sigma, observed=y)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;PyMC has an extremely nice function that allows us to visualize the model as a graph, &lt;code&gt;model_to_graphviz&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;pm.model_to_graphviz(baseline_model)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/bayes_reg_25_0.svg&#34; alt=&#34;svg&#34;&gt;&lt;/p&gt;
&lt;p&gt;From the graphical representation, we can see the various model components, their distributions, and how they interact with each other.&lt;/p&gt;
&lt;p&gt;We are now ready to &lt;strong&gt;compute&lt;/strong&gt; the model posterior. How does it work? In short, we sample realizations of model parameters, we compute the likelihood of the data given those values and the compute the corresponding posterior.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;idata = pm.sample(model=baseline_model, draws=1000)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Auto-assigning NUTS sampler...
Initializing NUTS using jitter+adapt_diag...
Multiprocess sampling (4 chains in 4 jobs)
NUTS: [beta, tau, sigma]
&lt;/code&gt;&lt;/pre&gt;
&lt;style&gt;
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
&lt;/style&gt;
&lt;div&gt;
  &lt;progress value=&#39;8000&#39; class=&#39;&#39; max=&#39;8000&#39; style=&#39;width:300px; height:20px; vertical-align: middle;&#39;&gt;&lt;/progress&gt;
  100.00% [8000/8000 00:03&amp;lt;00:00 Sampling 4 chains, 0 divergences]
&lt;/div&gt;
&lt;pre&gt;&lt;code&gt;Sampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 4_000 draws total) took 15 seconds.
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The fact that Bayesian inference requires sampling, has been historically one of the main bottlenecks of Bayesian statistics, since it makes it sensibly slower than the frequentist approach. However, this is less and less of a problem with the increased computational power of model computers.&lt;/p&gt;
&lt;p&gt;We are now ready to print out results. First, with the &lt;code&gt;summary()&lt;/code&gt; method, we can print a model summary very similar to those produced by the &lt;code&gt;statsmodels&lt;/code&gt; package.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;pm.summary(idata, hdi_prob=0.95).round(4)
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;mean&lt;/th&gt;
      &lt;th&gt;sd&lt;/th&gt;
      &lt;th&gt;hdi_2.5%&lt;/th&gt;
      &lt;th&gt;hdi_97.5%&lt;/th&gt;
      &lt;th&gt;mcse_mean&lt;/th&gt;
      &lt;th&gt;mcse_sd&lt;/th&gt;
      &lt;th&gt;ess_bulk&lt;/th&gt;
      &lt;th&gt;ess_tail&lt;/th&gt;
      &lt;th&gt;r_hat&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;beta[0]&lt;/th&gt;
      &lt;td&gt;0.019&lt;/td&gt;
      &lt;td&gt;0.025&lt;/td&gt;
      &lt;td&gt;-0.031&lt;/td&gt;
      &lt;td&gt;0.068&lt;/td&gt;
      &lt;td&gt;0.001&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;1943.0&lt;/td&gt;
      &lt;td&gt;1866.0&lt;/td&gt;
      &lt;td&gt;1.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;beta[1]&lt;/th&gt;
      &lt;td&gt;0.992&lt;/td&gt;
      &lt;td&gt;0.010&lt;/td&gt;
      &lt;td&gt;0.970&lt;/td&gt;
      &lt;td&gt;1.011&lt;/td&gt;
      &lt;td&gt;0.000&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;2239.0&lt;/td&gt;
      &lt;td&gt;1721.0&lt;/td&gt;
      &lt;td&gt;1.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;tau&lt;/th&gt;
      &lt;td&gt;0.157&lt;/td&gt;
      &lt;td&gt;0.021&lt;/td&gt;
      &lt;td&gt;0.117&lt;/td&gt;
      &lt;td&gt;0.197&lt;/td&gt;
      &lt;td&gt;0.000&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;2770.0&lt;/td&gt;
      &lt;td&gt;2248.0&lt;/td&gt;
      &lt;td&gt;1.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;sigma&lt;/th&gt;
      &lt;td&gt;0.993&lt;/td&gt;
      &lt;td&gt;0.007&lt;/td&gt;
      &lt;td&gt;0.980&lt;/td&gt;
      &lt;td&gt;1.007&lt;/td&gt;
      &lt;td&gt;0.000&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;3473.0&lt;/td&gt;
      &lt;td&gt;2525.0&lt;/td&gt;
      &lt;td&gt;1.0&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;The estimated parameters are extremely close to the ones we got with the frequentist approach, with an estimated effect of the &lt;code&gt;infinite_scroll&lt;/code&gt; equal to $0.157$.&lt;/p&gt;
&lt;p&gt;If sampling had the disadvantage of being slow, it has the advantage of being very &lt;strong&gt;transparent&lt;/strong&gt;. We can directly plot the distribution of the posterior. Let&amp;rsquo;s do it for the treatment effect $\tau$. The PyMC function &lt;code&gt;plot_posterior&lt;/code&gt; plots the distribution of the posterior, with a black bar for the Bayesian equivalent of a 95% confidence interval.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;pm.plot_posterior(idata, kind=&amp;quot;hist&amp;quot;, var_names=(&#39;tau&#39;), hdi_prob=0.95, figsize=(6, 3), bins=30); 
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/bayes_reg_31_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;As expected, since we chose conjugate priors, the posterior distribution looks gaussian.&lt;/p&gt;
&lt;p&gt;So far we have chosen the prior without much guidance. However, suppose we had access to past experiments. How do we incorporate this specific information?&lt;/p&gt;
&lt;h2 id=&#34;past-experiments&#34;&gt;Past Experiments&lt;/h2&gt;
&lt;p&gt;Suppose the idea of the infinite scroll, was just one among a ton of other ones that we tried and tested in the past. For each idea we have the data for the corresponding experiment, with the corresponding estimated coefficient. Suppose we had a thousand of them.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;past_experiments = [dgp.generate_data(seed_data=i) for i in range(1000)]
taus = [smf.ols(&#39;ad_revenue ~ infinite_scroll + past_revenue&#39;, pe).fit().params.values for pe in past_experiments]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;How do we use this additional information?&lt;/p&gt;
&lt;h3 id=&#34;normal-prior&#34;&gt;Normal Prior&lt;/h3&gt;
&lt;p&gt;The first idea could be to calibrate our prior to reflect the data distribution in the past. Keeping the normality assumption, we use the estimated average and standard deviations of the estimates from past experiments.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;taus_mean = np.mean(taus, axis=0)[1]
taus_mean
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;0.0009094486420266667
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;On average, had practically no effect on &lt;code&gt;ad_revenue&lt;/code&gt;, with a average effect of $0.0009$.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;taus_std = np.sqrt(np.cov(taus, rowvar=0)[1,1])
taus_std
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;0.029014447772168384
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;However, there was sensible variation across experiments, with a standard deviation of $0.029$.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s now estimate the&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;with pm.Model() as model_normal_prior:
    k = np.shape(X)[1]
    beta = pm.MvNormal(&#39;beta&#39;, mu=np.ones(k), cov=np.eye(k))
    tau = pm.Normal(&#39;tau&#39;, mu=taus_mean, sigma=taus_std)
    sigma = pm.InverseGamma(&#39;sigma&#39;, mu=1, sigma=1, initval=1)
    Ylikelihood = pm.Normal(&#39;y&#39;, mu=(X@beta + D@tau).flatten(), sigma=sigma, observed=y)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s sample from the model.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;idata_normal_prior = pm.sample(model=model_normal_prior, draws=1000)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Auto-assigning NUTS sampler...
Initializing NUTS using jitter+adapt_diag...
Multiprocess sampling (4 chains in 4 jobs)
NUTS: [beta, tau, sigma]
&lt;/code&gt;&lt;/pre&gt;
&lt;style&gt;
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
&lt;/style&gt;
&lt;div&gt;
  &lt;progress value=&#39;8000&#39; class=&#39;&#39; max=&#39;8000&#39; style=&#39;width:300px; height:20px; vertical-align: middle;&#39;&gt;&lt;/progress&gt;
  100.00% [8000/8000 00:04&amp;lt;00:00 Sampling 4 chains, 0 divergences]
&lt;/div&gt;
&lt;pre&gt;&lt;code&gt;Sampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 4_000 draws total) took 14 seconds.
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And plot the sample posterior distribution of the treatment effect parameter $\tau$.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;pm.plot_posterior(idata_normal_prior, kind=&amp;quot;hist&amp;quot;, var_names=(&#39;tau&#39;), hdi_prob=0.95, figsize=(6, 3), bins=30); 
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/bayes_reg_47_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;The estimated coefficient is sensibly smaller: $0.08$ instead of the previous estimate of $0.12$. Why is it the case?&lt;/p&gt;
&lt;p&gt;The fact is that the previous coefficient of $0.12$ is extremely unlikey, given our prior. We can compute the probability of getting the same or a more extreme value, given the prior.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;1 - sp.stats.norm(taus_mean, taus_std).cdf(0.12)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;2.025724712373389e-05
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The probability of such value is almost zero. Therefore, the estimated coefficient has moved towards the prior mean of $0.0009$.&lt;/p&gt;
&lt;h3 id=&#34;student-t-prior&#34;&gt;Student t Prior&lt;/h3&gt;
&lt;p&gt;So far, we have assumed a normal distribution for all linear coefficients. Is it appropriate? Let&amp;rsquo;s check it visually (check &lt;a href=&#34;https://medium.com/towards-data-science/how-to-compare-two-or-more-distributions-9b06ee4d30bf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt; for other methods on how to compare distributions).&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sns.histplot([tau[0] for tau in taus]).set(title=r&#39;Distribution of $\hat{\beta}_0$ in past experiments&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/bayes_reg_53_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;The distribution seems pretty normal. What the treatment effect paramenter $\tau$?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;fig, ax = plt.subplots()
sns.histplot([tau[1] for tau in taus], label=&#39;past experiments&#39;);
ax.axvline(reg.params[&#39;infinite_scroll&#39;], lw=2, c=&#39;C3&#39;, ls=&#39;--&#39;, label=&#39;current experiment&#39;)
plt.legend();
plt.title(r&#39;Distribution of $\hat{\tau}$ in past experiments&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/bayes_reg_55_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;The distribution very &lt;strong&gt;heavy tailed&lt;/strong&gt;! While at the center it looks like a normal distributions, the tails are much &amp;ldquo;fatter&amp;rdquo; and we have a couple of very extreme values. excluding the case of measurement error, this is a setting that happens often in the industry, where most ideas have extremely small or null effects and very rarely an idea is actually a breakthrough.&lt;/p&gt;
&lt;p&gt;One way to model this distribution is a &lt;a href=&#34;&#34;&gt;student-t distribution&lt;/a&gt;. In particular, we use a t-student with mean $0.0009$, variance $0.003$ and $1.3$ degrees of freedom to match the moments of the empirical distributions of past estimates.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;with pm.Model() as model_studentt_prior:

    # Priors
    k = np.shape(X)[1]
    beta = pm.MvNormal(&#39;beta&#39;, mu=np.ones(k), cov=np.eye(k))
    tau = pm.StudentT(&#39;tau&#39;, mu=taus_mean, sigma=0.003, nu=1.3)
    sigma = pm.InverseGamma(&#39;sigma&#39;, mu=1, sigma=1, initval=1)
    
    # Likelihood 
    Ylikelihood = pm.Normal(&#39;y&#39;, mu=(X@beta + D@tau).flatten(), sigma=sigma, observed=y)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s sample from the model.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;idata_studentt_priors = pm.sample(model=model_studentt_prior, draws=1000)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Auto-assigning NUTS sampler...
Initializing NUTS using jitter+adapt_diag...
Multiprocess sampling (4 chains in 4 jobs)
NUTS: [beta, tau, sigma]
&lt;/code&gt;&lt;/pre&gt;
&lt;style&gt;
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
&lt;/style&gt;
&lt;div&gt;
  &lt;progress value=&#39;8000&#39; class=&#39;&#39; max=&#39;8000&#39; style=&#39;width:300px; height:20px; vertical-align: middle;&#39;&gt;&lt;/progress&gt;
  100.00% [8000/8000 00:03&amp;lt;00:00 Sampling 4 chains, 0 divergences]
&lt;/div&gt;
&lt;pre&gt;&lt;code&gt;Sampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 4_000 draws total) took 15 seconds.
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And plot the sample posterior distribution of the treatment effect parameter $\tau$.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;pm.plot_posterior(idata_studentt_priors, kind=&amp;quot;hist&amp;quot;, var_names=(&#39;tau&#39;), hdi_prob=0.95, figsize=(6, 3), bins=30); 
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/bayes_reg_61_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;The estimated coefficient is now again similar to the one we got with the standard normal prior, $0.11$. However, the estimate is more precise since the confidence interval has shrunk from $[0.077, 0.016]$ to $[0.065, 0.015]$.&lt;/p&gt;
&lt;p&gt;What has happened?&lt;/p&gt;
&lt;h3 id=&#34;shrinking&#34;&gt;Shrinking&lt;/h3&gt;
&lt;p&gt;The answer lies in the shape of the different &lt;strong&gt;prior distributions&lt;/strong&gt; that we have used:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;standard normal, $N(0,1)$&lt;/li&gt;
&lt;li&gt;normal with matched moments, $N(0, 0.03)$&lt;/li&gt;
&lt;li&gt;t-student with matched moments, $t_{1.3}$(0, 0.003)$&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;t_hats = np.linspace(-0.3, 0.3, 1_000)
distributions = {
    &#39;N(0,1)&#39;: sp.stats.norm(0, 1).pdf(t_hats),
    &#39;N(0, 0.03)&#39;: sp.stats.norm(0, 0.03).pdf(t_hats),
    &#39;$t_{1.3}$(0, 0.003)&#39;: sp.stats.t(df=1.3).pdf(t_hats / 0.003)*300,
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s plot all of them together.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;for i, (label, y) in enumerate(distributions.items()):
    sns.lineplot(x=t_hats, y=y, color=f&#39;C{i}&#39;, label=label);
plt.xlim(-0.15, 0.15);
plt.legend(); 
plt.title(&#39;Prior Distributions&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/bayes_reg_67_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;As we can see, all distributions are centered on zero, but they have very different shapes. The standard normal distribution is essentially flat over the $[-0.15, 0.15]$ interval. Every value has basically the same probability. The last two instead, even though they have the same mean and variance, have very different shapes.&lt;/p&gt;
&lt;p&gt;How does it translate into our estimation? We can plot the implied posterior for different estimates, for each prior distribution.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def compute_posterior(b, prior):
    likelihood = sp.stats.norm(b, taus_std).pdf(t_hats)
    return np.average(t_hats, weights=prior*likelihood)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;fig, ax = plt.subplots(figsize=(7,6))
ax.axvline(0, lw=1.5, c=&#39;k&#39;);
ax.axhline(0, lw=1.5, c=&#39;k&#39;);
ax.axvline(0.12, lw=2, ls=&#39;--&#39;, c=&#39;darkgray&#39;);
for i, (label, y) in enumerate(distributions.items()):
    sns.lineplot(x=t_hats, y=[compute_posterior(t, y) for t in t_hats] , color=f&#39;C{i}&#39;, label=label);
ax.set_xlim(-0.16, 0.16);
ax.set_ylim(-0.16, 0.16);
plt.legend(); 
ax.set_xlabel(&#39;Experiment Estimate&#39;);
ax.set_ylabel(&#39;Posterior&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/bayes_reg_70_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;As we can see, the different priors transform the experimental estimates in very different ways. The standard normal prior essentially has no effect for estimates in the $[-0.15, 0.15]$ interval. The normal prior with matched moments instead shrinks each estimate by approximately 2/3. The effect of the t-student prior is instead &lt;strong&gt;non-linear&lt;/strong&gt;: it shrinks small estimates towards zero, while it keeps large estimates as they are.&lt;/p&gt;
&lt;p&gt;My &lt;strong&gt;intuition&lt;/strong&gt; is the following. A prior distribution very skewed or with &amp;ldquo;fat tails&amp;rdquo; means that large values are rare but not impossible. In practice, it means accepting that breakthrough improvements are possible. On the other hand, for the same variance, the distribution is more concentrated around zero than a standard normal so that small values are shrunk even more.&lt;/p&gt;
&lt;img src=&#34;fig/scroll.jpg&#34; width=&#34;300px&#34;/&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;In this article we have seen how to extend the analysis of AB test to incorporate &lt;strong&gt;information from past experiments&lt;/strong&gt;. In particular, we have seen the importance of choosing a prior. Selecting the distribution function is just as important as tuning its parameters. The shape of the prior distribution can drastically affect our inference, especially in a world with skewed distributions.&lt;/p&gt;
&lt;p&gt;Despite the length of the article, this was just a glimpse in the world of &lt;strong&gt;AB testing and Bayesian statistics&lt;/strong&gt;. While being computationally more intensive and requiring additional assumptions, the Bayesian approach is often more natural, powerful and flexible than the frequentist one. Knowing pros and cons of both approaches is crucial to get the best of both worlds, picking the approach that work best or combining them efficiently.&lt;/p&gt;
&lt;h3 id=&#34;references&#34;&gt;References&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;E. Azevedo, A. Deng, J. Olea, G. Weyl, &lt;a href=&#34;https://www.aeaweb.org/articles?id=10.1257/pandp.20191003&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Empirical Bayes Estimation of Treatment Effects with Many A/B Tests: An Overview&lt;/a&gt; (2019). &lt;em&gt;AEA Papers and Proceedings&lt;/em&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;A. Deng, &lt;a href=&#34;https://dl.acm.org/doi/abs/10.1145/2740908.2742563&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Objective Bayesian Two Sample Hypothesis Testing for Online Controlled Experiments&lt;/a&gt; (2018), &lt;em&gt;WWW15&lt;/em&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;related-articles&#34;&gt;Related Articles&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://medium.com/towards-data-science/the-bayesian-bootstrap-6ca4a1d45148&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The Bayesian Bootstrap&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://towardsdatascience.com/understanding-cuped-a822523641af&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Understanding CUPED&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://towardsdatascience.com/controls-b63dc69e3d8c&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;DAGs and Control Variables&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;code&#34;&gt;Code&lt;/h3&gt;
&lt;p&gt;You can find the original Jupyter Notebook here:&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/delta.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/delta.ipynb&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Experiments on Returns on Investment</title>
      <link>https://matteocourthoud.github.io/post/delta/</link>
      <pubDate>Tue, 27 Dec 2022 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/post/delta/</guid>
      <description>&lt;p&gt;&lt;em&gt;An introduction to the delta method for inference on ratio metrics.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;When we run an experiment, we are often not only interested in the effect of a treatment (new product, new feature, new interface, &amp;hellip;) on revenue, but in it&amp;rsquo;s &lt;strong&gt;cost-effectiveness&lt;/strong&gt;. In other words, is the investment worth the cost? Common examples include investments in computing resources, returns on advertisement, but also click-through rates and other ratio metrics.&lt;/p&gt;
&lt;p&gt;When we investigate causal effects, the gold standard is randomized control trials, a.k.a. &lt;strong&gt;AB tests&lt;/strong&gt;. Randomly assigning the treatment to a subset of the population (users, patients, customers, &amp;hellip;) we ensure that, on average, the difference in outcomes can be attributed to the treatment. However, when the object of interest is cost-effectiveness, AB tests present some additional problems since we are not just interested in one treatment effect, but in the &lt;strong&gt;ratio of two treatment effects&lt;/strong&gt;, the outcome of the investment over its cost.&lt;/p&gt;
&lt;p&gt;In this post we are going to see how to analyze randomized experiments when the object of interest is the &lt;strong&gt;return on investment (ROI)&lt;/strong&gt;. We are going to explore alternative metrics to measure whether an investment paid off. We will also introduce a very powerful tool for inference with complex metrics: the &lt;strong&gt;delta method&lt;/strong&gt;. While the algebra can be intense, the result is simple: we can compute the confidence interval for our ratio estimator using a simple linear regression.&lt;/p&gt;
&lt;h2 id=&#34;investing-in-cloud-computing&#34;&gt;Investing in Cloud Computing&lt;/h2&gt;
&lt;p&gt;To better illustrate the concepts, we are going to use a toy example throughout the article: suppose we were an &lt;strong&gt;online marketplace&lt;/strong&gt; and we wanted to &lt;strong&gt;invest in cloud computing&lt;/strong&gt;: we want to increase the computing power behind our internal search engine, by switching to a higher tier server. The idea is that the faster search will improve the user experience, potentially leading to higher sales. Therefore, the question is: is the investment worth the cost? The object of interest is the &lt;strong&gt;return on investment (ROI)&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Differently from usual AB tests or randomized experiments, we are not interested in a single causal effect, but in the &lt;strong&gt;ratio&lt;/strong&gt; of two metrics: the effect on revenue and the effect on cost. We will still use a &lt;strong&gt;randomized control trial&lt;/strong&gt; or &lt;strong&gt;AB test&lt;/strong&gt; to estimate the ROI: we randomly assign groups of users to either the treatment or the control group. The treated users will benefit from the faster cloud machines, while the control users will use the old slower machines. Randomization ensures that we can estimate the impact of the new machines on either cost or revenue by comparing users in the treatment and control group: the difference in their average is an unbiased estimator of the average treatment effect. However, things are more complicated for their ratio.&lt;/p&gt;
&lt;p&gt;I import the data generating process &lt;code&gt;dgp_cloud()&lt;/code&gt; from &lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/src/dgp.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;src.dgp&lt;/code&gt;&lt;/a&gt;. With respect to previous articles, I generated a new DGP parent class that handles randomization and data generation, while its children classes contain the specific use-cases. I also import some plotting functions and libraries from &lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/src/utils.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;src.utils&lt;/code&gt;&lt;/a&gt;.  To include not only code but also data and tables, I use &lt;a href=&#34;https://deepnote.com&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Deepnote&lt;/a&gt;, a Jupyter-like web-based collaborative notebook environment.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;%matplotlib inline
%config InlineBackend.figure_format = &#39;retina&#39;

from src.utils import *
from src.dgp import dgp_cloud, DGP
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;dgp = dgp_cloud(n=10_000)
df = dgp.generate_data(seed_assignment=6)
df.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;new_machine&lt;/th&gt;
      &lt;th&gt;cost&lt;/th&gt;
      &lt;th&gt;revenue&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;3.14&lt;/td&gt;
      &lt;td&gt;20.90&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;3.77&lt;/td&gt;
      &lt;td&gt;33.57&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;3.16&lt;/td&gt;
      &lt;td&gt;24.31&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;2.36&lt;/td&gt;
      &lt;td&gt;20.35&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1.65&lt;/td&gt;
      &lt;td&gt;12.60&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;The data contains information on the total &lt;code&gt;cost&lt;/code&gt; and &lt;code&gt;revenue&lt;/code&gt; for a set of $10.000$ users over a period of a month. We also have information on the treatment: whether the search engine was running on the old or &lt;code&gt;new machines&lt;/code&gt;.  As it often happens with business metrics, both distributions of cost and revenues are very &lt;strong&gt;skewed&lt;/strong&gt;. Moreover, most people do not buy anything and therefore generate zero revenue, even though they still use the platform, generating positive costs.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;fix, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))
sns.histplot(df.cost, ax=ax1, color=&#39;C0&#39;).set(title=&#39;Distribution of Cost&#39;)
sns.histplot(df.revenue, ax=ax2, color=&#39;C1&#39;).set(title=&#39;Distribution of Revenue&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/delta_8_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;We can compute the &lt;strong&gt;difference-in-means&lt;/strong&gt; estimate for &lt;code&gt;cost&lt;/code&gt; and &lt;code&gt;revenue&lt;/code&gt; by regressing the outcome on the treatment indicator.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;smf.ols(&#39;cost ~ new_machine&#39;, data=df).fit().summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
       &lt;td&gt;&lt;/td&gt;          &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt;   &lt;td&gt;    2.9617&lt;/td&gt; &lt;td&gt;    0.043&lt;/td&gt; &lt;td&gt;   69.034&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    2.878&lt;/td&gt; &lt;td&gt;    3.046&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;new_machine&lt;/th&gt; &lt;td&gt;    0.5152&lt;/td&gt; &lt;td&gt;    0.060&lt;/td&gt; &lt;td&gt;    8.563&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    0.397&lt;/td&gt; &lt;td&gt;    0.633&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;The average &lt;code&gt;cost&lt;/code&gt; has increased by $0.5152$$ per user. What about revenue?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;smf.ols(&#39;revenue ~ new_machine&#39;, data=df).fit().summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
       &lt;td&gt;&lt;/td&gt;          &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt;   &lt;td&gt;   25.9172&lt;/td&gt; &lt;td&gt;    0.425&lt;/td&gt; &lt;td&gt;   60.950&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;   25.084&lt;/td&gt; &lt;td&gt;   26.751&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;new_machine&lt;/th&gt; &lt;td&gt;    1.0664&lt;/td&gt; &lt;td&gt;    0.596&lt;/td&gt; &lt;td&gt;    1.788&lt;/td&gt; &lt;td&gt; 0.074&lt;/td&gt; &lt;td&gt;   -0.103&lt;/td&gt; &lt;td&gt;    2.235&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;The average &lt;code&gt;revenue&lt;/code&gt; per user has also increased, by $1.0664$$. So, was the investment &lt;strong&gt;profitable&lt;/strong&gt;?&lt;/p&gt;
&lt;p&gt;To answer this question, we first have to decide which metric to use as our &lt;strong&gt;outcome metric&lt;/strong&gt;. In case of ratio metrics, this is not trivial.&lt;/p&gt;
&lt;h2 id=&#34;average-return-or-return-of-the-average&#34;&gt;Average Return or Return of the Average?&lt;/h2&gt;
&lt;p&gt;It is very tempting to approach this problem saying: it is true that we have two variables, by we can just compute their ratio, and then analyze everything as usual, using a &lt;strong&gt;single variable&lt;/strong&gt;: the individual level return.&lt;/p&gt;
&lt;p&gt;$$
\rho_i = \frac{\text{individual revenue}}{\text{individual cost}} = \frac{R_i}{C_i}
$$&lt;/p&gt;
&lt;p&gt;What happens if we analyze the experiment using this single metric?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df[&amp;quot;rho&amp;quot;] = df[&amp;quot;revenue&amp;quot;] / df[&amp;quot;cost&amp;quot;]
smf.ols(&amp;quot;rho ~ new_machine&amp;quot;, df).fit().summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
       &lt;td&gt;&lt;/td&gt;          &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt;   &lt;td&gt;    6.6898&lt;/td&gt; &lt;td&gt;    0.044&lt;/td&gt; &lt;td&gt;  150.832&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    6.603&lt;/td&gt; &lt;td&gt;    6.777&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;new_machine&lt;/th&gt; &lt;td&gt;   -0.7392&lt;/td&gt; &lt;td&gt;    0.062&lt;/td&gt; &lt;td&gt;  -11.893&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;   -0.861&lt;/td&gt; &lt;td&gt;   -0.617&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;The estimated effect is &lt;strong&gt;negative and significant&lt;/strong&gt;, $-0.7392$! It seems like the the new machines were not a good investment, and the returns have decreased by $74%$.&lt;/p&gt;
&lt;p&gt;This result seems to contradict our previous estimates. We have seen before that the revenue has increased on average more than the cost ($0.9505$ vs $0.5076$). Why is it the case? The problem is that we are giving the same weight to heavy users and light users. Let&amp;rsquo;s use a simple example with two users. The first one (blue) is a light user and before was costing $1$ $ and returning $10$ $, while now is costing $4$ $ and returning $20$ $. The other user (violet) is a heavy user and before was costing $10$ $ and returning $100$ $ and now is costing $20$ $ and returning $220$ $.&lt;/p&gt;
&lt;img src=&#34;fig/return.png&#34; width=&#34;700px&#34;/&gt;
&lt;p&gt;The average return is -3x: on average the return per user has decreased by $300%$. However, the total return per user is $1000%$: the increase in cost of $13$$ has generated $130$$ in revenue! The results are wildly different and entirely driven by the weight of the two users: the effect of the heavy user is low in relative terms but high in absolute terms, while it&amp;rsquo;s the opposite for the light user. The average relative effect is therefore mostly driven by the light user, while the relative average effect is mostly driven by the heavy user.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Which metric&lt;/strong&gt; is more relevant in our setting? We talking about return on investment, we are usually interested in understanding whether we got a return on the money we spend. Therefore, the &lt;strong&gt;total return&lt;/strong&gt; is more interesting than the average return.&lt;/p&gt;
&lt;p&gt;From now on, the object of interest will be the &lt;strong&gt;return on investment (ROI)&lt;/strong&gt;, given by the expected increase in revenue over the expected increase in cost, and we will denote it with the greek letter rho, $\rho$.&lt;/p&gt;
&lt;p&gt;$$
\rho = \frac{\text{incremental revenue}}{\text{incremental cost}} = \frac{\mathbb E [\Delta R]}{\mathbb E [\Delta C]}
$$&lt;/p&gt;
&lt;p&gt;We can estimate the ROI as the ratio of the two previous estimates: the average difference in revenue between the treatment and control group, over the average difference in cost between the treatment and control group.&lt;/p&gt;
&lt;p&gt;$$
\hat{\rho} = \frac{\mathbb E_n [\Delta R]}{\mathbb E_n [\Delta C]}
$$&lt;/p&gt;
&lt;p&gt;Note a subtle but crucial difference with respect to the previous formula: we have replaced the &lt;a href=&#34;https://en.wikipedia.org/wiki/Expected_value&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;expected values&lt;/a&gt; $\mathbb E$ with the empirical expectation operators $\mathbb E_n$, also known as the &lt;a href=&#34;https://en.wikipedia.org/wiki/Arithmetic_mean&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;sample average&lt;/a&gt;. The difference in notation is minimal, but the conceptual difference is huge. The first, $\mathbb E$, is a &lt;strong&gt;theoretical&lt;/strong&gt; concept, while the second, $\mathbb E_n$, is &lt;strong&gt;empirical&lt;/strong&gt;: it is a number that depends on the actual data. I personally like the notation since it highlights the close link between the two concepts (the second is the empirical counterpart of the first), while also making it clear that the second crucially depends on the sample size $n$.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def estimate_roi(df):
    Delta_C = df.loc[df.new_machine==1, &amp;quot;cost&amp;quot;].mean() - df.loc[df.new_machine==0, &amp;quot;cost&amp;quot;].mean()
    Delta_R = df.loc[df.new_machine==1, &amp;quot;revenue&amp;quot;].mean() - df.loc[df.new_machine==0, &amp;quot;revenue&amp;quot;].mean()
    return Delta_R / Delta_C

estimate_roi(df)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;2.0698235970047887
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The estimate is $2.0698$: each additional dollar spent in the new machines translated in $2.0698$ extra dollars in revenue. Sounds great!&lt;/p&gt;
&lt;p&gt;But how much should we trust this number? Is it significantly different form one, or it is just driven by noise?&lt;/p&gt;
&lt;h2 id=&#34;inference&#34;&gt;Inference&lt;/h2&gt;
&lt;p&gt;To answer this question, we would like to compute a &lt;a href=&#34;https://en.wikipedia.org/wiki/Confidence_interval&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;confidence interval&lt;/strong&gt;&lt;/a&gt; for our estimate. How do we compute a confidence interval for a ratio metric? The first step is to compute the standard deviation of the estimator. One method that is always available is the &lt;a href=&#34;https://en.wikipedia.org/wiki/Bootstrapping_%28statistics%29&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;bootstrap&lt;/strong&gt;&lt;/a&gt;: resample the data with replacement multiple times and use the distribution of the estimates over samples to compute the standard deviation of the estimator.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s try it in our case. I compute the standard deviation over $10.000$ bootstrapped samples, using the function &lt;code&gt;pd.DataFrame().sample()&lt;/code&gt; with the options &lt;code&gt;frac=1&lt;/code&gt; to obtain a dataset of the same size and &lt;code&gt;replace=True&lt;/code&gt; to sample with replacement.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;boot_estimates = [estimate_roi(df.sample(frac=1, replace=True, random_state=i)) for i in range(10_000)]
np.std(boot_estimates)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;0.9790730538161984
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The bootstrap estimate of the standard deviation is equal to $0.979$. How good is it?&lt;/p&gt;
&lt;p&gt;Since we fully control the data generating process, we can simulate the &amp;ldquo;true&amp;rdquo; distribution of the estimator. We do that for $10.000$ simulations and we compute the resulting standard deviation of the estimator.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;np.std(dgp.evaluate_f_redrawing_outcomes(estimate_roi, 10_000))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;1.0547776958025372
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The estimated variance of the estimator using the &amp;ldquo;true&amp;rdquo; data generating process is slightly higher but very similar, around $1.055$.&lt;/p&gt;
&lt;p&gt;The issue with the bootstrap is that it is very computational intense since it requires repeating the estimating procedure thousands of times. We are now going to explore another &lt;em&gt;extremely&lt;/em&gt; powerful alternative that requires a single estimation step, the &lt;a href=&#34;https://en.wikipedia.org/wiki/Delta_method&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;delta method&lt;/strong&gt;&lt;/a&gt;. The delta method generally allows us to do inference on functions of random variable, therefore its applications are broader than ratios.&lt;/p&gt;
&lt;p&gt;⚠️ &lt;strong&gt;Warning&lt;/strong&gt;: the next section is going to be algebra-intense. If you want, you can skip it and go straight to the last section.&lt;/p&gt;
&lt;h2 id=&#34;the-delta-method&#34;&gt;The Delta Method&lt;/h2&gt;
&lt;p&gt;What is the &lt;strong&gt;delta method&lt;/strong&gt;? In short, it is an incredibly powerful &lt;strong&gt;asymptotic inference&lt;/strong&gt; method for functions of random variables, that exploits Taylor expansions. In short, the delta method requires four ingredients&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;One or more &lt;a href=&#34;https://en.wikipedia.org/wiki/Random_variable&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;random variables&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;A function&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Central_limit_theorem&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The Central Limit Theorem&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Taylor_series&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Taylor expansions&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I will assume some basic knowledge of all four concepts. Suppose we had a set of realizations $X_1$, &amp;hellip;, $X_n$ of a random variable that satisfy the requirements for the Central Limit Theorem (CLT): independence, identically distributions with expected value $\mu$, and finite variance $\sigma^2$. Under these conditions, the CLT tells us that the sample average $\mathbb E_n[X]$ converges in distribution to a normal distribution, or more precisely&lt;/p&gt;
&lt;p&gt;$$
\sqrt{n} \ \frac{ \mathbb E_n[X] - \mu}{\sigma} \ \overset{D}{\to} \ N(0, 1)
$$&lt;/p&gt;
&lt;p&gt;What does the equation mean? It reads &amp;ldquo;the normalized sample average, scaled by a factor $\sqrt{n}$, converges in distribution to a standard normal distribution, i.e. it is approximately Gaussian for a sufficiently large sample.&lt;/p&gt;
&lt;p&gt;Now, suppose we were interested in a &lt;strong&gt;function&lt;/strong&gt; of the sample average $f\big(\mathbb E_n[X]\big)$. Note that this is different from the sample average of the function $\mathbb E_n\big[f(X)\big]$. The delta method tells us what the function of the sample average converges to.&lt;/p&gt;
&lt;p&gt;$$
\sqrt{n} \ \frac{ f\big(\mathbb E_n[X]\big) - f(\mu)}{\sigma} \ \overset{D}{\to} \ N \big(0, f&amp;rsquo;(\mu)^2 \big)
$$&lt;/p&gt;
&lt;p&gt;, where $f&amp;rsquo;(\mu)^2$ is the derivative of the function $f$, evaluated at $\mu$.&lt;/p&gt;
&lt;p&gt;What is the &lt;strong&gt;intuition&lt;/strong&gt; behind this formula? We now have a new term inside the expression of the variance, the squared first derivative $f&amp;rsquo;(\mu)^2$ ($\neq$ second derivative). If the derivative of the function is low, the variance decreases since different inputs translate into similar outputs. On the contrary, if the derivative of the function is high, the variance of the distribution is amplified, since different inputs translate into even more different outputs.&lt;/p&gt;
&lt;img src=&#34;fig/delta_intuition.png&#34; width=&#34;700px&#34;/&gt;
&lt;p&gt;The result directly follows from the Taylor approximation of $f \big(\mathbb E_n[X]\big)$&lt;/p&gt;
&lt;p&gt;$$
f\big(\mathbb E_n[X]\big) = f(\mu) + f&amp;rsquo;(\mu) (\mathbb E_n[X] - \mu) + \text{residual}
$$&lt;/p&gt;
&lt;p&gt;Importantly, asymptotically, the last term disappears and the linear approximation holds exactly!&lt;/p&gt;
&lt;p&gt;How is this connected to the ratio estimator? We need a bit more math and to switch from a single dimension to two dimensions in order to understand that. In our case, we have a bivariate function of two random variables, $\Delta R$ and $\Delta C$, which returns their ratio. In the case of a multivariate function $f$, the asymptotic variance of the estimator is given by&lt;/p&gt;
&lt;p&gt;$$
\text{AVar} \big( \hat{\rho} \big) = \nabla \hat{\rho}&amp;rsquo; \Sigma_n \nabla \hat{\rho}
$$&lt;/p&gt;
&lt;p&gt;where, $\nabla$ indicates the &lt;a href=&#34;https://en.wikipedia.org/wiki/Gradient&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;gradient&lt;/a&gt; of the function, i.e. the vector of directional derivatives, and $\Sigma_n$ is the empirical variance-covariance matrix of $X$. In our case, they correspond to&lt;/p&gt;
&lt;p&gt;$$
\nabla \hat{\rho} =
\begin{bmatrix}
\frac{1}{\mathbb E_n [\Delta C]} \newline - \frac{\mathbb E_n [\Delta R]}{\mathbb E_n [\Delta C]^2}
\end{bmatrix}
$$&lt;/p&gt;
&lt;p&gt;and&lt;/p&gt;
&lt;p&gt;$$
\Sigma_n =
\begin{bmatrix}
\text{Var}_n (\Delta R) &amp;amp; \text{Cov}_n (\Delta R, \Delta C) \newline
\text{Cov}_n (\Delta R, \Delta C) &amp;amp; \text{Var}_n (\Delta C) \newline
\end{bmatrix}
$$&lt;/p&gt;
&lt;p&gt;, where the subscripts $n$ indicate the empirical counterparts, as for the expected value.&lt;/p&gt;
&lt;p&gt;Combining the previous three equations together with a little matrix algebra, we get the formula of the asymptotic variance of the return on investment estimator.&lt;/p&gt;
&lt;p&gt;$$
\begin{align*}
\text{AVar} \big( \hat{\rho} \big) &amp;amp;= \frac{1}{\mathbb E_n[\Delta C]^2} \text{Var}_n(\Delta R) - 2 \frac{\mathbb E_n[\Delta R]}{\mathbb E_n[\Delta C]^3} \text{Cov}_n(\Delta R, \Delta C) + \frac{\mathbb E_n[\Delta R]^2}{\mathbb E_n[\Delta C]^4} \text{Var}_n(\Delta C)
\end{align*}
$$&lt;/p&gt;
&lt;p&gt;Since the estimator is given by $\hat{\rho} = \frac{\mathbb E_n[\Delta R]}{\mathbb E_n[\Delta C]}$, we can rewrite the asymptotic variance as&lt;/p&gt;
&lt;p&gt;$$
\begin{align*}
\text{AVar} \big( \hat{\rho} \big) = \frac{1}{\mathbb E_n[\Delta C]^2} \text{Var}_n \Big( \Delta R - \hat{\rho} \Delta C \Big)
\end{align*}
$$&lt;/p&gt;
&lt;p&gt;The last expression is very interesting because it suggests that we can rewrite the asymptotic variance of our estimator as the &lt;strong&gt;variance of a difference-in-means estimator&lt;/strong&gt; for a new auxiliary variable. In fact, we can rewrite the above expression as&lt;/p&gt;
&lt;p&gt;$$
\begin{align*}
\text{AVar} \big( \hat{\rho} \big) = \text{Var}_n \Big( \Delta \tilde R \Big) \qquad \text{where} \quad \tilde R = \frac{R - \hat{\rho} \ C}{| \mathbb E [\Delta C] |}
\end{align*}
$$&lt;/p&gt;
&lt;p&gt;This expression is incredibly useful because it gives us intuition and allows us to estimate the standard deviation of our estimator by &lt;strong&gt;linear regression&lt;/strong&gt;.&lt;/p&gt;
&lt;h2 id=&#34;inference-with-linear-regression&#34;&gt;Inference with Linear Regression&lt;/h2&gt;
&lt;p&gt;Did you skip the previous section? No problem!&lt;/p&gt;
&lt;p&gt;After some algebra, we concluded that we can estimate the variance of a difference-in-means estimator for an &lt;strong&gt;auxiliary variable&lt;/strong&gt; defined as&lt;/p&gt;
&lt;p&gt;$$
\tilde R = \frac{R - \hat{\rho} \ C}{| \mathbb E_n [\Delta C] |}
$$&lt;/p&gt;
&lt;p&gt;This expression might seem obscure at first, but it is incredibly useful. In fact, it gives us (1) an intuitive &lt;strong&gt;interpretation&lt;/strong&gt; of the variance of the estimator and (2) a &lt;strong&gt;practical&lt;/strong&gt; way to estimate it.&lt;/p&gt;
&lt;p&gt;Interpretation first! How should we read the above expression? We can estimate the variance of the empirical estimator as the variance of a difference-in-means estimator, for a new variable $\tilde R$ that we can easily compute from the data. We just need to take the revenue $R$, subtract the cost $C$ multiplied by the estimated ROI $\rho$ and scale it down by the expected cost difference $|\mathbb E_n[\Delta C]|$. We can interpret this variable as the &lt;strong&gt;baseline revenue&lt;/strong&gt;, i.e. the revenue not affected by the investment. The fact that it is scaled by the expected cost difference tells us that its variance will be &lt;strong&gt;decreasing in the total investment&lt;/strong&gt;: the more we spend, the more precisely we can estimate the return on that expenditure.&lt;/p&gt;
&lt;p&gt;Now, let&amp;rsquo;s estimate the variance of the ROI estimator, in &lt;strong&gt;four steps&lt;/strong&gt;.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;We need to estimate the return on investment $\hat \rho$.&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;rho_hat = estimate_roi(df)
&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;The term $| \mathbb E_n[\Delta C] |$ is the absolute difference in average cost between the treatment and control group.&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;abs_Delta_C = np.abs(df.loc[df.new_machine==1, &amp;quot;cost&amp;quot;].mean() - df.loc[df.new_machine==0, &amp;quot;cost&amp;quot;].mean())
&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;3&#34;&gt;
&lt;li&gt;We now have all the ingredients to generate the auxiliary variable $\tilde R$.&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df[&#39;revenue_tilde&#39;] = (df[&#39;revenue&#39;] - rho_hat * df[&#39;cost&#39;]) / abs_Delta_C
&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;4&#34;&gt;
&lt;li&gt;The variance of the treatment-control difference $\Delta \tilde R$ can be directly computed by linear regression, as in randomized controlled trials for difference-in-means estimators (see Agrist and Pischke, 2008).&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;smf.ols(&#39;revenue_tilde ~ new_machine&#39;, df).fit().summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
       &lt;td&gt;&lt;/td&gt;          &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt;   &lt;td&gt;   38.4067&lt;/td&gt; &lt;td&gt;    0.653&lt;/td&gt; &lt;td&gt;   58.771&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;   37.126&lt;/td&gt; &lt;td&gt;   39.688&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;new_machine&lt;/th&gt; &lt;td&gt; -2.01e-14&lt;/td&gt; &lt;td&gt;    0.917&lt;/td&gt; &lt;td&gt;-2.19e-14&lt;/td&gt; &lt;td&gt; 1.000&lt;/td&gt; &lt;td&gt;   -1.797&lt;/td&gt; &lt;td&gt;    1.797&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;The estimated standard error of the ROI is $0.917$, very close to the bootstrap estimate of $0.979$ and the simulated value of $1.055$. However, with respect to bootstrapping, the delta method allowed us to compute it in a single step, making it sensibly &lt;strong&gt;faster&lt;/strong&gt; (around $1000$ times on my local machine).&lt;/p&gt;
&lt;p&gt;Note that this estimated standard deviation implies a 95% confidence interval of $2.0698 +- 1.96 \times 0.917$, equal to $[-0.2735, 3.8671]$. This might seem like good news since the confidence interval does not cover zero. However, note that in this case, a more interesting &lt;strong&gt;null hypothesis&lt;/strong&gt; is that the ROI is equal to 1: we are breaking even. A value larger than 1 implies profits, while a value lower than 1 implies losses. In our case, we cannot reject the null hypothesis that the investment in new machines was not profitable.&lt;/p&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;In this article, we have explored a very common causal inference problem: assessing the &lt;strong&gt;return on investment&lt;/strong&gt;. Whether it&amp;rsquo;s a physical investment in new hardware, a virtual cost, or advertisement expenditure, we are interested in understanding whether this incremental cost has paid off. The additional complications come from the fact that we are studying not one, but two causal quantities, intertwined.&lt;/p&gt;
&lt;p&gt;We have first explored and compared different outcome metrics to assess whether the investment paid off. Then, we have introduced an incredibly powerful method to do inference with complex random variables: the &lt;strong&gt;delta method&lt;/strong&gt;. In the particular case of ratios, the delta method delivers a very insightful and practical functional form for the asymptotic variance of the estimator that can be estimated with a simple linear regression.&lt;/p&gt;
&lt;h3 id=&#34;references&#34;&gt;References&lt;/h3&gt;
&lt;p&gt;[1] A. Deng, U. Knoblich, J. Lu, &lt;a href=&#34;https://arxiv.org/pdf/1803.06336.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Applying the Delta Method in Metric Analytics: A Practical Guide with Novel Ideas&lt;/a&gt; (2018).&lt;/p&gt;
&lt;p&gt;[2] R. Budylin, A. Drutsa, I. Katsev, V. Tsoy, &lt;a href=&#34;https://dl.acm.org/doi/abs/10.1145/3159652.3159699&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Consistent Transformation of Ratio Metrics for Efficient Online Controlled Experiments&lt;/a&gt; (2018). &lt;em&gt;ACM&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;[3] J. Angrist, J. Pischke, &lt;a href=&#34;https://www.mostlyharmlesseconometrics.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Mostly harmless econometrics: An empiricist&amp;rsquo;s companion&lt;/a&gt; (2009). &lt;em&gt;Princeton university press&lt;/em&gt;.&lt;/p&gt;
&lt;h3 id=&#34;related-articles&#34;&gt;Related Articles&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://medium.com/towards-data-science/the-bayesian-bootstrap-6ca4a1d45148&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The Bayesian Bootstrap&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://towardsdatascience.com/df3065a0388e&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Outliers, Leverage, Residuals, and Influential Observations&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://towardsdatascience.com/b07ab46aa782&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;A/B Tests, Privacy, and Online Regression&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;code&#34;&gt;Code&lt;/h3&gt;
&lt;p&gt;You can find the original Jupyter Notebook here:&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/delta.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/delta.ipynb&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Mean vs Median Causal Effect</title>
      <link>https://matteocourthoud.github.io/post/quantile_reg/</link>
      <pubDate>Mon, 10 Oct 2022 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/post/quantile_reg/</guid>
      <description>&lt;p&gt;&lt;em&gt;An introduction to quantile regression.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;In A/B tests, a.k.a. &lt;a href=&#34;https://en.wikipedia.org/wiki/Randomized_controlled_trial&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;randomized controlled trials&lt;/a&gt;, we usually estimate the &lt;strong&gt;average treatment effect (ATE)&lt;/strong&gt;: effect of a treatment (a drug, ad, product, &amp;hellip;) on an outcome of interest (a disease, firm revenue, customer satisfaction, &amp;hellip;), where the &amp;ldquo;average&amp;rdquo; is taken over the test subjects (patients, users, customers, &amp;hellip;). The ATE is a very useful quantity since it tells us the effect that we can expect if we were to treat a new subject with the same treatment.&lt;/p&gt;
&lt;p&gt;However, sometimes we might be interested in quantities different from the average, such as the &lt;a href=&#34;https://en.wikipedia.org/wiki/Median&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;median&lt;/strong&gt;&lt;/a&gt;. The median is an alternative measure of &lt;em&gt;central tendency&lt;/em&gt; that is more robust to outliers and is often more informative with skewed distributions. More generally, we might want to estimate the effect for different &lt;a href=&#34;https://en.wikipedia.org/wiki/Quantile&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;quantiles&lt;/a&gt; of the outcome distribution. A &lt;strong&gt;common use-case&lt;/strong&gt; is studying the impact of a UI change on the loading time of a website: a slightly heavier website might translate in an imperceptible change for most users, but a big change for a few users with very slow connections. Another common use-case is studying the impact of a product change on a product that is bought by few people: do existing customers buy it more or are we attracting new customers?&lt;/p&gt;
&lt;p&gt;These questions are hard to answer with linear regression that estimates the &lt;em&gt;average treatment effect&lt;/em&gt;. A more suitable tool is &lt;strong&gt;quantile regression&lt;/strong&gt; that can instead estimate the &lt;em&gt;median treatment effect&lt;/em&gt;. In this article we are going to cover a brief introduction to quantile regression and the estimation of quantile treatment effects.&lt;/p&gt;
&lt;h2 id=&#34;loyalty-cards-and-spending&#34;&gt;Loyalty Cards and Spending&lt;/h2&gt;
&lt;p&gt;Suppose we were an &lt;strong&gt;online store&lt;/strong&gt; and we wanted to increase sales. We decide to offer our customers a &lt;strong&gt;loyalty card&lt;/strong&gt; that grants them discounts as they increase their spend in the store. We would like to assess if the loyalty card is effective in increasing sales so we run an &lt;strong&gt;A/B test&lt;/strong&gt;: we offer the loyalty card only to a subset of our customers, at random.&lt;/p&gt;
&lt;p&gt;I import the data generating process &lt;code&gt;dgp_loyalty()&lt;/code&gt; from &lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/src/dgp.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;src.dgp&lt;/code&gt;&lt;/a&gt;. I also import some plotting functions and libraries from &lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/src/utils.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;src.utils&lt;/code&gt;&lt;/a&gt;.  To include not only code but also data and tables, I use &lt;a href=&#34;https://deepnote.com&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Deepnote&lt;/a&gt;, a Jupyter-like web-based collaborative notebook environment.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;%matplotlib inline
%config InlineBackend.figure_format = &#39;retina&#39;

from src.utils import *
from src.dgp import dgp_loyalty
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, let&amp;rsquo;s have a look at the data. We have information on $10.000$ customers, for whom we observe their &lt;code&gt;spend&lt;/code&gt; and whether they were offered the &lt;code&gt;loyalty&lt;/code&gt; card. We also observe some demographics, like &lt;code&gt;age&lt;/code&gt; and &lt;code&gt;gender&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df = dgp_loyalty().generate_data()
df.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;loyalty&lt;/th&gt;
      &lt;th&gt;spend&lt;/th&gt;
      &lt;th&gt;age&lt;/th&gt;
      &lt;th&gt;gender&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;30&lt;/td&gt;
      &lt;td&gt;Male&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;26&lt;/td&gt;
      &lt;td&gt;Male&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;27&lt;/td&gt;
      &lt;td&gt;Male&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;29&lt;/td&gt;
      &lt;td&gt;Male&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;23&lt;/td&gt;
      &lt;td&gt;Female&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;Interestingly, we notice that the outcome of interest, &lt;code&gt;spend&lt;/code&gt;, seems to have a lot of zeros. Let&amp;rsquo;s dig deeper.&lt;/p&gt;
&lt;h2 id=&#34;mean-vs-median&#34;&gt;Mean vs Median&lt;/h2&gt;
&lt;p&gt;Before analyzing our experiment, let&amp;rsquo;s have a look at our outcome variable, &lt;code&gt;spend&lt;/code&gt;. We first inspect it using centrality measures. We have two main options: the &lt;strong&gt;mean&lt;/strong&gt; and the &lt;strong&gt;median&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;First of all, what are they? The mean captures the average value, while the median captures the value in the middle of the distribution. In general, the mean is mathematically more tractable and easier to interpret, while the median is more robust to outliers. You can find plenty of articles online comparing the two measures and suggesting which one is more appropriate and when. Let&amp;rsquo;s have a look at the mean and median &lt;code&gt;spend&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;np.mean(df[&#39;spend&#39;])
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;28.136224
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;np.median(df[&#39;spend&#39;])
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;0.0
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;How do we interpret these two numbers? People spend on average 28\$ on our store. However, more than 50% of people don&amp;rsquo;t spend anything. As we can see, both measures are very informative and, to a certain extent, complementary. We can better understand the distribution of &lt;code&gt;spend&lt;/code&gt; by plotting its histogram.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sns.histplot(data=df, x=&amp;quot;spend&amp;quot;).set(ylabel=&#39;&#39;, title=&#39;Spending Distribution&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/quantile_reg_14_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;As previewed by the values of the mean and the median, the distribution of &lt;code&gt;spend&lt;/code&gt; is very skewed, with more than 5000 customers (out of 10000) not spending anything.&lt;/p&gt;
&lt;p&gt;One natural question then is: are we interested in the effect of the &lt;code&gt;loyalty&lt;/code&gt; card on average &lt;code&gt;spend&lt;/code&gt; or on median &lt;code&gt;spend&lt;/code&gt;? The first would tell us if customers spend more on average, while the second would tell us if the average customer spends more.&lt;/p&gt;
&lt;p&gt;Linear regression can tell us the effect of the &lt;code&gt;loyalty&lt;/code&gt; card on average &lt;code&gt;spend&lt;/code&gt;. However, what can we do if we were interested in the effect of the &lt;code&gt;loyalty&lt;/code&gt; card on median &lt;code&gt;spend&lt;/code&gt; (or other quantiles)? The answer is &lt;strong&gt;quantile regression&lt;/strong&gt;.&lt;/p&gt;
&lt;h2 id=&#34;quantile-regression&#34;&gt;Quantile Regression&lt;/h2&gt;
&lt;p&gt;With &lt;strong&gt;linear regression&lt;/strong&gt;, we try to estimate the &lt;em&gt;conditional expectation function&lt;/em&gt; of an outcome variable $Y$ (&lt;code&gt;spend&lt;/code&gt; in our example) with respect to one or more explanatory variables $X$ (&lt;code&gt;loyalty&lt;/code&gt; in our example).&lt;/p&gt;
&lt;p&gt;$$
\mathbb E \big[ Y \big| X \big]
$$&lt;/p&gt;
&lt;p&gt;In other words, we want to find a function $f$ such that $f(X) = \mathbb E[Y|X]$. We do so, by solving the following minimization problem:&lt;/p&gt;
&lt;p&gt;$$
\hat f(X) = \arg \min_{f} \mathbb E \big[ Y - f(X) \big]^2
$$&lt;/p&gt;
&lt;p&gt;It can be shown that the function $f$ that solves this minimization is indeed the conditional expectation of $Y$, with respect to $X$.&lt;/p&gt;
&lt;p&gt;Since $f(X)$ can be infinite dimensional, we usually estimate a parametric form of $f(X)$. The most common one is the linear form $f(X) = \beta X$, where $\beta$ is estimated by solving the corresponding minimization problem:&lt;/p&gt;
&lt;p&gt;$$
\hat \beta = \arg \min_{\beta} \mathbb E \big[ Y - \beta X \big]^2
$$&lt;/p&gt;
&lt;p&gt;The linear form is not just convenient, but it can be interpreted as the best local approximation of $f(X)$, referring to Taylor&amp;rsquo;s expansion.&lt;/p&gt;
&lt;p&gt;With &lt;strong&gt;quantile regression&lt;/strong&gt;, we do the &lt;strong&gt;same&lt;/strong&gt;. The only difference is that, instead of estimating the conditional expectation of $Y$ with respect to $X$, we want to estimate the $q$-&lt;a href=&#34;https://en.wikipedia.org/wiki/Quantile&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;quantile&lt;/a&gt; of $Y$ with respect to $X$.&lt;/p&gt;
&lt;p&gt;$$
\mathbb Q_q \big[ Y \big| X \big]
$$&lt;/p&gt;
&lt;p&gt;First of all, what is a &lt;strong&gt;quantile&lt;/strong&gt;? The &lt;a href=&#34;https://en.wikipedia.org/wiki/Quantile&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Wikipedia definition&lt;/a&gt; says&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;In statistics and probability, quantiles are cut points dividing the range of a probability distribution into continuous intervals with equal probabilities, or dividing the observations in a sample in the same way. Common quantiles have special names, such as quartiles (four groups), deciles (ten groups), and percentiles (100 groups).&amp;rdquo;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;For example, the 0.1-quantile represents the value that sits on the right of 10% of the mass of the distribution. The &lt;strong&gt;median&lt;/strong&gt; is the 0.5-quantile (or, equivalently, the $50^{th}$ percentile or the $5^{th}$ decile) and corresponds with the value in the center of the distribution. Let&amp;rsquo;s see a simple example with a &lt;a href=&#34;https://en.wikipedia.org/wiki/Log-normal_distribution&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;log-normal distribution&lt;/a&gt;. I plot the three quartiles that divide the data in four equally sized bins.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;data = np.random.lognormal(0, 1, 1_000_000);
sns.histplot(data).set(title=&#39;Lognormal Distribution&#39;, xlim=(0,10))
plt.axvline(x=np.percentile(data, 25), c=&#39;C8&#39;, label=&#39;25th percentile&#39;)
plt.axvline(x=np.median(data), c=&#39;C1&#39;, label=&#39;Median (50th pct)&#39;)
plt.axvline(x=np.percentile(data, 75), c=&#39;C3&#39;, label=&#39;75th percentile&#39;)
plt.legend();
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/quantile_reg_18_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;As we can see, the three quartiles divide the data into four bins, of equal size.&lt;/p&gt;
&lt;p&gt;So, what is the &lt;strong&gt;objective&lt;/strong&gt; of quantile regression? The objective is to find a function $f$ such that $f(X) = F^{-1}(y_q)$, where $F$ is the &lt;a href=&#34;https://en.wikipedia.org/wiki/Cumulative_distribution_function&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;cumulative distribution function&lt;/a&gt; of $Y$ and $y_q$ is the $q$-quantile of the distribution of $Y$.&lt;/p&gt;
&lt;p&gt;How do we do this? It can be shown with a little linear algebra that we can obtain the conditional quantile as the solution of the following minimization problem:&lt;/p&gt;
&lt;p&gt;$$
\hat f(X) = \arg \min_{f} \mathbb E \big[ \rho_q (Y - f(X)) \big] = \arg \min_{f} \ (1-q) \int_{-\infty}^{f(x)} (y - f(x)) \text{d} F(y) + q \int_{f(x)}^{\infty} (y - f(x)) \text{d} F(y)
$$&lt;/p&gt;
&lt;p&gt;Where $\rho_q$ is an auxiliary weighting function with the following shape.&lt;/p&gt;
&lt;img src=&#34;fig/rho.png&#34; width=&#34;400px&#34;/&gt;
&lt;p&gt;What is the &lt;strong&gt;intuition&lt;/strong&gt; behind the objective function?&lt;/p&gt;
&lt;p&gt;The idea is that we can interpret the equation as follows&lt;/p&gt;
&lt;p&gt;$$
\mathbb E \big[ \rho_q (Y - f(X)) \big] = (1-q) \underset{\color{red}{\text{mass of distribution below }f(x)}}{\int_{-\infty}^{f(x)} (y - f(x)) \text{d} F(y)} + q \underset{\color{red}{\text{mass of distribution above }f(x)}}{\int_{f(x)}^{\infty} (y - f(x)) \text{d} F(y)} \overset{\color{blue}{\text{if } f(x) = y_q}}{=} - (1-q) q + q (1-q) = 0
$$&lt;/p&gt;
&lt;p&gt;So that, when $f(X)$ corresponds with the quantile $y_q$, the value of the objective function is zero.&lt;/p&gt;
&lt;p&gt;Exactly as before, we can estimate a &lt;strong&gt;parametric form&lt;/strong&gt; of $f$ and, exactly as before, we can interpret it as a best local approximation (not trivially though, see &lt;a href=&#34;https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1468-0262.2006.00671.x&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Angrist, Chernozhukov, and Fernández-Val (2006)&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;$$
\hat \beta_q = \arg \min_{\beta} \mathbb E \big[ \rho_q (Y - \beta X ) \big]
$$&lt;/p&gt;
&lt;p&gt;We wrote $\hat \beta_q$ to indicate that this is the coefficient for the best linear approximation of the conditional $q$-quantile function.&lt;/p&gt;
&lt;p&gt;How do we &lt;strong&gt;estimate&lt;/strong&gt; a quantile regression?&lt;/p&gt;
&lt;h2 id=&#34;estimation&#34;&gt;Estimation&lt;/h2&gt;
&lt;p&gt;The &lt;a href=&#34;https://www.statsmodels.org/stable/index.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;statsmodels&lt;/a&gt; package allows us to estimate quantile regression with the the &lt;a href=&#34;https://www.statsmodels.org/dev/generated/statsmodels.regression.quantile_regression.QuantReg.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;quantreg()&lt;/code&gt;&lt;/a&gt; function. We just need to specify the quantile $q$ when we fit the model. Let&amp;rsquo;s use $q=0.5$, which corresponds with the median.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;smf.quantreg(&amp;quot;spend ~ loyalty&amp;quot;, data=df).fit(q=0.5).summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
      &lt;td&gt;&lt;/td&gt;         &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt; &lt;td&gt; 8.668e-07&lt;/td&gt; &lt;td&gt;    0.153&lt;/td&gt; &lt;td&gt; 5.66e-06&lt;/td&gt; &lt;td&gt; 1.000&lt;/td&gt; &lt;td&gt;   -0.300&lt;/td&gt; &lt;td&gt;    0.300&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;loyalty&lt;/th&gt;   &lt;td&gt;    3.4000&lt;/td&gt; &lt;td&gt;    0.217&lt;/td&gt; &lt;td&gt;   15.649&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    2.974&lt;/td&gt; &lt;td&gt;    3.826&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;Quantile regression estimates a positive coefficient for &lt;code&gt;loyalty&lt;/code&gt;. How does this estimate compare with linear regression?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;smf.ols(&amp;quot;spend ~ loyalty&amp;quot;, data=df).fit().summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
      &lt;td&gt;&lt;/td&gt;         &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt; &lt;td&gt;   25.6583&lt;/td&gt; &lt;td&gt;    0.564&lt;/td&gt; &lt;td&gt;   45.465&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;   24.552&lt;/td&gt; &lt;td&gt;   26.765&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;loyalty&lt;/th&gt;   &lt;td&gt;    4.9887&lt;/td&gt; &lt;td&gt;    0.801&lt;/td&gt; &lt;td&gt;    6.230&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    3.419&lt;/td&gt; &lt;td&gt;    6.558&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;The estimated coefficient with linear regression is higher. What does it mean? We will spend more time on the &lt;em&gt;interpretation&lt;/em&gt; of quantile regression coefficients later.&lt;/p&gt;
&lt;p&gt;Can we condition the analysis on other variables? We suspect that &lt;code&gt;spend&lt;/code&gt; is also affected by other variables and we want to increase the precision of our estimate by also conditioning the analysis on &lt;code&gt;age&lt;/code&gt; and &lt;code&gt;gender&lt;/code&gt;. We can just add the variables to the &lt;code&gt;quantreg()&lt;/code&gt; model.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;smf.quantreg(&amp;quot;spend ~ loyalty + age + gender&amp;quot;, data=df).fit(q=0.5).summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
         &lt;td&gt;&lt;/td&gt;           &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt;      &lt;td&gt;  -50.5353&lt;/td&gt; &lt;td&gt;    1.053&lt;/td&gt; &lt;td&gt;  -47.977&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;  -52.600&lt;/td&gt; &lt;td&gt;  -48.471&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;gender[T.Male]&lt;/th&gt; &lt;td&gt;  -20.2963&lt;/td&gt; &lt;td&gt;    0.557&lt;/td&gt; &lt;td&gt;  -36.410&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;  -21.389&lt;/td&gt; &lt;td&gt;  -19.204&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;loyalty&lt;/th&gt;        &lt;td&gt;    4.5747&lt;/td&gt; &lt;td&gt;    0.546&lt;/td&gt; &lt;td&gt;    8.374&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    3.504&lt;/td&gt; &lt;td&gt;    5.646&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;age&lt;/th&gt;            &lt;td&gt;    2.3663&lt;/td&gt; &lt;td&gt;    0.026&lt;/td&gt; &lt;td&gt;   92.293&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    2.316&lt;/td&gt; &lt;td&gt;    2.417&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;The coefficient of &lt;code&gt;loyalty&lt;/code&gt; increases when we condition the analysis on &lt;code&gt;age&lt;/code&gt; and &lt;code&gt;gender&lt;/code&gt;. This is true also for linear regresssion.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;smf.ols(&amp;quot;spend ~ loyalty + age + gender&amp;quot;, data=df).fit().summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
         &lt;td&gt;&lt;/td&gt;           &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt;      &lt;td&gt;  -57.4466&lt;/td&gt; &lt;td&gt;    0.911&lt;/td&gt; &lt;td&gt;  -63.028&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;  -59.233&lt;/td&gt; &lt;td&gt;  -55.660&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;gender[T.Male]&lt;/th&gt; &lt;td&gt;  -26.3170&lt;/td&gt; &lt;td&gt;    0.482&lt;/td&gt; &lt;td&gt;  -54.559&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;  -27.262&lt;/td&gt; &lt;td&gt;  -25.371&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;loyalty&lt;/th&gt;        &lt;td&gt;    3.9101&lt;/td&gt; &lt;td&gt;    0.473&lt;/td&gt; &lt;td&gt;    8.272&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    2.983&lt;/td&gt; &lt;td&gt;    4.837&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;age&lt;/th&gt;            &lt;td&gt;    2.7688&lt;/td&gt; &lt;td&gt;    0.022&lt;/td&gt; &lt;td&gt;  124.800&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    2.725&lt;/td&gt; &lt;td&gt;    2.812&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;There are a couple of things that we haven&amp;rsquo;t mentioned yet. The first one is &lt;strong&gt;inference&lt;/strong&gt;. How do we compute confidence intervals and p-values for our estimates in quantile regression?&lt;/p&gt;
&lt;h2 id=&#34;inference&#34;&gt;Inference&lt;/h2&gt;
&lt;p&gt;The asymptotic variance of the estimate $a$ of the quantile $q$ of a distribution $F$ is given by&lt;/p&gt;
&lt;p&gt;$$
AVar(y) = q(1-q) f^{-2}(y)
$$&lt;/p&gt;
&lt;p&gt;where $f$ is the &lt;a href=&#34;https://en.wikipedia.org/wiki/Probability_density_function&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;density function&lt;/a&gt; of $F$. This expression can be decomposed into &lt;strong&gt;two components&lt;/strong&gt;: $q(1-q)$ and $f^{-2}(y)$.&lt;/p&gt;
&lt;p&gt;The first component, $q(1-q)$, basically tells us that the variance of a quantile is higher the more the quantile is closer to the center of the distribution. Why is that so? First, we need to think about when the quantile of a point changes in response to a change in the value of a second point. The quantile changes when the second point swaps from left to right (or viceversa) of the first point. This is intuitively very easy if the first point lies in the middle of the distribution, but very hard if it lies at the extreme.&lt;/p&gt;
&lt;p&gt;The second component, $f^{-2}(a)$, instead tells us that this side swapping is more likely if the first point is surrounded by a lot of points.&lt;/p&gt;
&lt;p&gt;Importantly, estimating the variance of a quantile requires an estimate of the whole distribution of $Y$. This is done via approximation and it can be computationally very intensive. However, alternative procedures like the bootstrap or the &lt;a href=&#34;https://towardsdatascience.com/6ca4a1d45148&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;bayesian bootstrap&lt;/a&gt; are always available.&lt;/p&gt;
&lt;p&gt;The second thing that we haven&amp;rsquo;t talked about yet is the &lt;strong&gt;interpretation&lt;/strong&gt; of the estimated coefficients. We got a lower coefficient of &lt;code&gt;loyalty&lt;/code&gt; on &lt;code&gt;spend&lt;/code&gt; with median regression. What does it mean?&lt;/p&gt;
&lt;h2 id=&#34;interpretation&#34;&gt;Interpretation&lt;/h2&gt;
&lt;p&gt;The &lt;strong&gt;interpretation&lt;/strong&gt; of linear regression coefficients is straightforward: each coefficient is the derivative of the conditional expectation function $\mathbb E[Y|X]$ with respect to one dimension of $X$. In our case, we can interpret the regression coefficient of &lt;code&gt;loyalty&lt;/code&gt; as the average &lt;code&gt;spend&lt;/code&gt; increase from being offered a loyalty card. Crucially, here &amp;ldquo;average&amp;rdquo; means that this holds true for &lt;em&gt;each customer&lt;/em&gt;, on average.&lt;/p&gt;
&lt;p&gt;However, the interpretation of quantile regression coefficients is &lt;strong&gt;tricky&lt;/strong&gt;. Before, we were tempted to say that the &lt;code&gt;loyalty&lt;/code&gt; card increases the spend of the median customer by 3.4\$. But &lt;strong&gt;what does it mean&lt;/strong&gt;? Is it the same median customer that spends more or do we have a different median customer? This might seem like a philosophical question but it has important implications on reporting of quantile regression results. In the first case, we are making a statement that, as for the interpretation of linear regression coefficients, applies to a &lt;em&gt;single individual&lt;/em&gt;. In the second case, we are making a statement about the &lt;em&gt;distribution&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1468-0262.2005.00570.x&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Chernozhukov and Hansen (2005)&lt;/a&gt; show that a strong but helpful assumption is &lt;strong&gt;rank invariance&lt;/strong&gt;: assuming that the treatment &lt;strong&gt;does not shift&lt;/strong&gt; the relative composition of the distribution. In other words, if we rank people by &lt;code&gt;spend&lt;/code&gt; before the experiment, we assume that this ranking is not affected by the introduction of the &lt;code&gt;loyalty&lt;/code&gt; card. If I was spending less than you before, I might spend more afterwards, but still less than you (for any two people).&lt;/p&gt;
&lt;p&gt;Under this assumption, we can interpret the quantile coefficients as &lt;strong&gt;marginal effects for single individuals&lt;/strong&gt; sitting at different points of the outcome distribution, as in the first interpretation provided above. Moreover, we can report the treatment effect for many quantiles and interpret each one of them as a local effect for a different individual. Let&amp;rsquo;s plot the distribution of treatment effects, for different quantiles of &lt;code&gt;spend&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def plot_quantile_TE(df, formula, q, varname):
    df_results = pd.DataFrame()
    for q in np.arange(q, 1-q, q):
        qreg = smf.quantreg(formula, data=df).fit(q=q)
        temp = pd.DataFrame({&#39;q&#39;: [q],
                             &#39;coeff&#39;: [qreg.params[varname]], 
                             &#39;std&#39;: [qreg.bse[varname]],
                             &#39;ci_lower&#39;: [qreg.conf_int()[0][varname]],
                             &#39;ci_upper&#39;: [qreg.conf_int()[1][varname]]})
        df_results = pd.concat([df_results, temp]).reset_index(drop=True)
    
    # Plot
    fig, ax = plt.subplots()
    sns.lineplot(data=df_results, x=&#39;q&#39;, y=&#39;coeff&#39;)
    ax.fill_between(data=df_results, x=&#39;q&#39;, y1=&#39;ci_lower&#39;, y2=&#39;ci_upper&#39;, alpha=0.1);
    plt.axhline(y=0, c=&amp;quot;k&amp;quot;, lw=2, zorder=1)
    ols_coeff = smf.ols(formula, data=df).fit().params[varname]
    plt.axhline(y=ols_coeff, ls=&amp;quot;--&amp;quot;, c=&amp;quot;C1&amp;quot;, label=&amp;quot;OLS coefficient&amp;quot;, zorder=1)
    plt.legend()
    plt.title(&amp;quot;Estimated coefficient, by quantile&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plot_quantile_TE(df, formula=&amp;quot;spend ~ loyalty&amp;quot;, varname=&#39;loyalty&#39;, q=0.05)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/quantile_reg_37_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;This plot is &lt;strong&gt;extremely insightful&lt;/strong&gt;: for almost half of the customers, the &lt;code&gt;loyalty&lt;/code&gt; card has no effect. On the other hand, customers that were already spending something end up spending even more (around 10/12\$ more). This is a very powerful insight that we would have missed with linear regression that estimated an average effect of 5\$.&lt;/p&gt;
&lt;p&gt;We can repeat the same exercise, conditioning the analysis on &lt;code&gt;gender&lt;/code&gt; and &lt;code&gt;age&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plot_quantile_TE(df, formula=&amp;quot;spend ~ loyalty + age + gender&amp;quot;, varname=&#39;loyalty&#39;, q=0.05)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/quantile_reg_39_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Conditioning on other covariates removes a lot of the heterogeneity in treatment effects. The &lt;code&gt;loyalty&lt;/code&gt; card increases spending for most people, it&amp;rsquo;s demographic characteristics that are responsible for no spending to begin with.&lt;/p&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;In this article, we have explored a different &lt;strong&gt;causal estimand&lt;/strong&gt;: median treatment effects. How does it compare with the average treatment effect that we usually estimate? The pros and cons are closely related to the pros and cons of the median with respect to the mean as a measure of &lt;em&gt;central tendency&lt;/em&gt;. Median treatment effects are more informative on what is the effect on the average subject and are more robust to outliers. However, they are much more computationally intensive and they require strong assumptions for identification, such as rank invariance.&lt;/p&gt;
&lt;h3 id=&#34;references&#34;&gt;References&lt;/h3&gt;
&lt;p&gt;[1] R. Koenker, &lt;a href=&#34;https://www.cambridge.org/core/books/quantile-regression/C18AE7BCF3EC43C16937390D44A328B1&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Quantile Regression&lt;/a&gt; (1996), Cambridge University Press.&lt;/p&gt;
&lt;p&gt;[1] R. Koenker, K. Hallock, &lt;a href=&#34;https://www.aeaweb.org/articles?id=10.1257/jep.15.4.143&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Quantile Regression&lt;/a&gt;, (2001), &lt;em&gt;Journal of Economic Perspectives&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;[2] V. Chernozhukov, C. Hansen, &lt;a href=&#34;https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1468-0262.2005.00570.x&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;An IV Model of Quantile Treatment Effects&lt;/a&gt; (2005), &lt;em&gt;Econometrica&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;[3] J. Angrist, V. Chernozhukov, I. Fernández-Val, &lt;a href=&#34;https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1468-0262.2006.00671.x&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Quantile Regression under Misspecification, with an Application to the U.S. Wage Structure&lt;/a&gt; (2006), &lt;em&gt;Econometrica&lt;/em&gt;.&lt;/p&gt;
&lt;h3 id=&#34;related-articles&#34;&gt;Related Articles&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/b63dc69e3d8c&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;DAGs and Control Variables&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/6ca4a1d45148&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The Bayesian Bootstrap&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/a928f67413e4&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Goodbye Scatterplot, Welcome Binned Scatterplot&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;code&#34;&gt;Code&lt;/h3&gt;
&lt;p&gt;You can find the original Jupyter Notebook here:&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/quantile_reg.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/quantile_reg.ipynb&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>A/B Tests, Privacy and Online Regression</title>
      <link>https://matteocourthoud.github.io/post/online_reg/</link>
      <pubDate>Sun, 04 Sep 2022 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/post/online_reg/</guid>
      <description>&lt;p&gt;&lt;em&gt;How to run experiments without storing individual data&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;AB tests, a.k.a. randomized controlled trials, are widely recognized as the gold standard technique to compute the causal effect of a treatment (a drug, ad, product, &amp;hellip;) on an outcome of interest (a disease, firm revenue, customer satisfaction, &amp;hellip;). We randomly split a set of subjects (patients, users, customers, &amp;hellip;) into a treatment and a control group and give the treatment to the treatment group. This procedure ensures that ex-ante, the only expected difference between the two groups is caused by the treatment.&lt;/p&gt;
&lt;p&gt;One potential &lt;strong&gt;privacy concern&lt;/strong&gt; is that one needs to store data about many users for the whole duration of the experiment in order to estimate the effect of the treatment. This is not a problem if we can run the experiment instantaneusly, but can become an issue when the experiment duration is long. In this post, we are going to explore one solution to this problem: &lt;strong&gt;online regression&lt;/strong&gt;. We will see how to estimate (conditional) average treatment effects and how to do inference, using both asymptotic approximations and bootstrapping.&lt;/p&gt;
&lt;p&gt;⚠️ Some parts are algebra-intense, but you can skip them if you are only interested in the intuition.&lt;/p&gt;
&lt;h2 id=&#34;credit-cards-and-donations&#34;&gt;Credit Cards and Donations&lt;/h2&gt;
&lt;p&gt;Suppose, for example, that we were a fin-tech company. We have designed a new user interface (UI) for our mobile application and we would like to understand whether it slows down our transaction. In order to estimate the causal effect of the new UI on transaction speed, we plan to run an A/B test or randomized controlled trial.&lt;/p&gt;
&lt;p&gt;We have one major problem: we should not store user-level information for privacy reasons.&lt;/p&gt;
&lt;p&gt;First, let&amp;rsquo;s have a look at the data. I import the data generating process &lt;code&gt;dgp_credit()&lt;/code&gt; from &lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/src/dgp.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;src.dgp&lt;/code&gt;&lt;/a&gt; and some plotting functions and libraries from &lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/src/utils.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;src.utils&lt;/code&gt;&lt;/a&gt;. I include code snippets from &lt;a href=&#34;https://deepnote.com&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Deepnote&lt;/a&gt;, a Jupyter-like web-based collaborative notebook environment. For our purpose, Deepnote is very handy because it allows me not only to include code but also output, like data and tables.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;%matplotlib inline
%config InlineBackend.figure_format = &#39;retina&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from src.utils import *
from src.dgp import dgp_credit
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I first generate the whole dataset in one-shot. We will then investigate how to perform the experimental analysis in case the data was arriving dynamically.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def generate_data(self, N=100, seed=0):
        np.random.seed(seed)
        
        # Connection speed
        connection = np.random.lognormal(3, 1, N)
        
        # Treatment assignment
        treated = np.random.binomial(1, 0.5, N)
        
        # Transfer speed
        #spend = np.minimum(np.random.lognormal(1 + treated + 0.1*np.sqrt(balance), 2, N), balance)
        speed = np.minimum(np.random.exponential(10 + 4*treated - 0.5*np.sqrt(connection), N), connection)
        
        # Generate the dataframe
        df = pd.DataFrame({&#39;c&#39;: [1]*N, &#39;treated&#39;: treated,  
                           &#39;connection&#39;: np.round(connection,2), 
                           &#39;speed&#39;: np.round(speed,2)})

        return df
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;N = 100
df = generate_data(N)
df.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;c&lt;/th&gt;
      &lt;th&gt;treated&lt;/th&gt;
      &lt;th&gt;connection&lt;/th&gt;
      &lt;th&gt;speed&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;117.22&lt;/td&gt;
      &lt;td&gt;0.94&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;29.97&lt;/td&gt;
      &lt;td&gt;29.97&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;53.45&lt;/td&gt;
      &lt;td&gt;7.38&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;188.84&lt;/td&gt;
      &lt;td&gt;0.76&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;130.00&lt;/td&gt;
      &lt;td&gt;24.44&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;We have information on 100 users, for whom we observe&amp;hellip;&lt;/p&gt;
&lt;p&gt;First, let&amp;rsquo;s have a look&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;model = smf.ols(&#39;speed ~ treated + connection&#39;, data=df).fit()
model.summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
       &lt;td&gt;&lt;/td&gt;         &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt;  &lt;td&gt;    6.0740&lt;/td&gt; &lt;td&gt;    1.079&lt;/td&gt; &lt;td&gt;    5.630&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    3.933&lt;/td&gt; &lt;td&gt;    8.215&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;treated&lt;/th&gt;    &lt;td&gt;    1.3939&lt;/td&gt; &lt;td&gt;    1.297&lt;/td&gt; &lt;td&gt;    1.075&lt;/td&gt; &lt;td&gt; 0.285&lt;/td&gt; &lt;td&gt;   -1.180&lt;/td&gt; &lt;td&gt;    3.968&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;connection&lt;/th&gt; &lt;td&gt;   -0.0033&lt;/td&gt; &lt;td&gt;    0.017&lt;/td&gt; &lt;td&gt;   -0.197&lt;/td&gt; &lt;td&gt; 0.844&lt;/td&gt; &lt;td&gt;   -0.037&lt;/td&gt; &lt;td&gt;    0.030&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;In order to understand how we can make linear regression one data point at the time, we first need a brief linear algebra recap.&lt;/p&gt;
&lt;p&gt;First of all, let&amp;rsquo;s define $y$ the dependent variable, &lt;code&gt;spend&lt;/code&gt; in our case, and $X$ the explanatory variable, the &lt;code&gt;treated&lt;/code&gt; indicator, the account &lt;code&gt;balance&lt;/code&gt; and a constant.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def xy_from_df(df, r0, r1):
    return df.iloc[r0:r1,:3].to_numpy(), df.iloc[r0:r1,3].to_numpy()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The  estimator is given by
$$
\hat{\beta}_{OLS} = (X&amp;rsquo;X)^{-1}X&amp;rsquo;y
$$&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from numpy.linalg import inv

X, Y = xy_from_df(df, 0, 100)
inv(X.T @ X) @ X.T @ Y
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;array([ 6.07404291e+00,  1.39385101e+00, -3.33599131e-03])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We get indeed the same exact number as with the &lt;code&gt;smf.ols&lt;/code&gt; command!&lt;/p&gt;
&lt;p&gt;Can we compute $\beta$ one observation at the time?&lt;/p&gt;
&lt;p&gt;The answer is yes! Assume we had $n$ observations and we just received the $n+1$th observation: the pair $(x_{n+1}, y_{n+1})$. In order to compute $\hat{\beta}_{n+1}$ we need to have stored only two objects in memory&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\hat{\beta}_{n}$, the previous estimate of $\beta$&lt;/li&gt;
&lt;li&gt;$(X_n&amp;rsquo; X_n)^{-1}$, the previous value of $(X&amp;rsquo; X)^{-1}$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;First of all, how do we update $(X&amp;rsquo; X)^{-1}$?
$$
\begin{align*}
(X_{n+1}&amp;rsquo; X_{n+1})^{-1} = (X_n&amp;rsquo; X_n)^{-1} - \frac{(X_n&amp;rsquo; X_n)^{-1} x_{n+1} x_{n+1}&amp;rsquo; (X_n&amp;rsquo; X_n)^{-1}}{1 + x_{n+1}&amp;rsquo; (X_n&amp;rsquo; X_n)^{-1} x_{n+1}}
\end{align*}
$$&lt;/p&gt;
&lt;p&gt;After having updated $(X&amp;rsquo; X)^{-1}$, we can update $\hat{\beta}$.
$$
\hat{\beta}&lt;em&gt;{n+1} = \hat{\beta}&lt;/em&gt;{n} + (X_n&amp;rsquo; X_n)^{-1} x_{n} (y_n - x_n&amp;rsquo; \hat{\beta}_{n})
$$&lt;/p&gt;
&lt;p&gt;Note that this procedure is not only privacy friendly but also &lt;strong&gt;memory-friendly&lt;/strong&gt;. Our dataset is a $100 \times 4$ matrix while $(X&amp;rsquo; X)^{-1}$ is $3 \times 3$ matrix and $\beta$ is a $3 \times 1$ matrix. We are storing only 12 numbers instead of up to 400!&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def update_xb(XiX, beta, x, y):
    XiX -= (XiX @ x.T @ x @ XiX) / (1 + x @ XiX @ x.T )
    beta += XiX @ x.T @ (y - x @ beta)
    return XiX, beta
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We are now ready to estimate our OLS coefficient, one data point at the time. However, we cannot really start from the first observation, because we would be unable to invert the matrix $X&amp;rsquo;X$. We need at least $k+1$ observations, where $k$ is the number of variables in $X$.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s use a warm start of 10 observations to be safe.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Initialize XiX and beta from first 10 observations
x, y = xy_from_df(df, 0, 10)
XiX = inv(x.T @ x)
beta = XiX @ x.T @ y

# Update estimate live
for n in range(10, N):
    x, y = xy_from_df(df, n, n+1)
    XiX, beta = update_xb(XiX, beta, x, y)
    
# Print result
print(beta)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[ 6.07404291e+00  1.39385101e+00 -3.33599131e-03]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We got exactly the same coefficient! Nice!&lt;/p&gt;
&lt;p&gt;How did we get there? We can plot the evolution of out estimate $\hat{\beta}$ as we accumulate data. The dynamic plotting function is a bit more cumbersome, but you can find it in &lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/src/figures.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;src.figures&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from src.figures import online_regression

online_regression(df, &amp;quot;fig/online_reg1.gif&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;fig/online_reg1.gif&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;As we can see, as the number of data points increases, the estimate seems to less and less volatile.&lt;/p&gt;
&lt;p&gt;But is it really the case? As usual, we are not just interested in the point estimate of the effect of the &lt;code&gt;coupon&lt;/code&gt; on spending, we would also like to understand how precise this estimate is.&lt;/p&gt;
&lt;h2 id=&#34;inference&#34;&gt;Inference&lt;/h2&gt;
&lt;p&gt;We have seen how to estimate the treatment effect &amp;ldquo;online&amp;rdquo;: one observation at the time. Can we also compute the variance of the estimator in the same manner?&lt;/p&gt;
&lt;p&gt;First of all, let&amp;rsquo;s review what the variance of the OLS estimator looks like. Under baseline assumptions, the variance of the OLS estimator is given by:
$$
\text{Var}(\hat{\beta}_{OLS}) = (X&amp;rsquo;X)^{-1} \hat{\sigma}^2
$$&lt;/p&gt;
&lt;p&gt;where $\hat{\sigma}^2$ is the variance of the residuals $e = (y - X&amp;rsquo;\hat{\beta})$.&lt;/p&gt;
&lt;p&gt;The regression table reports the standard errors of the coefficients, which are the squared elements on the diagonal of $\text{Var}(\hat{\beta})$.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;model.summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
       &lt;td&gt;&lt;/td&gt;         &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt;  &lt;td&gt;    6.0740&lt;/td&gt; &lt;td&gt;    1.079&lt;/td&gt; &lt;td&gt;    5.630&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    3.933&lt;/td&gt; &lt;td&gt;    8.215&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;treated&lt;/th&gt;    &lt;td&gt;    1.3939&lt;/td&gt; &lt;td&gt;    1.297&lt;/td&gt; &lt;td&gt;    1.075&lt;/td&gt; &lt;td&gt; 0.285&lt;/td&gt; &lt;td&gt;   -1.180&lt;/td&gt; &lt;td&gt;    3.968&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;connection&lt;/th&gt; &lt;td&gt;   -0.0033&lt;/td&gt; &lt;td&gt;    0.017&lt;/td&gt; &lt;td&gt;   -0.197&lt;/td&gt; &lt;td&gt; 0.844&lt;/td&gt; &lt;td&gt;   -0.037&lt;/td&gt; &lt;td&gt;    0.030&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;Let&amp;rsquo;s check that we would obtain the same numbers using matrix algebra.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;beta = inv(X.T @ X) @ X.T @ Y
np.sqrt(np.diag(inv(X.T @ X) * np.var(Y - X @ beta)))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;array([1.06261376, 1.27718352, 0.01669716])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Indeed, we get exactly the same numbers!&lt;/p&gt;
&lt;p&gt;We already have a method to part of $\text{Var}(\hat{\beta}&lt;em&gt;{OLS})$ online: $(X&amp;rsquo;X)^{-1}$ update the matrix $(X&amp;rsquo;X)^{-1}$ online. How do we update $\hat{\sigma}^2$? This is the formula to update the sum of squared residuals $S$.
$$
S&lt;/em&gt;{n+1} = S_{n} + \frac{(y_{n+1} - x_{n+1}\hat{\beta}&lt;em&gt;n)}{1 + x&lt;/em&gt;{n+1}&amp;rsquo; (X_n&amp;rsquo; X_n)^{-1} x_{n+1}}
$$&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def update_xbs(XiX, beta, S, x, y):
    S += (y - x @ beta)**2 / (1 + x @ XiX @ x.T )
    XiX -= (XiX @ x.T @ x @ XiX) / (1 + x @ XiX @ x.T )
    beta += XiX @ x.T @ (y - x @ beta)
    return XiX, beta, S[0,0]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note, the order here is very important!&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Inizialize XiX, beta, and sigma from the first 10 observations
x, y = xy_from_df(df, 0, 10)
XiX = inv(x.T @ x)
beta = XiX @ x.T @ y
S = np.sum((y - x @ beta)**2)

# Update XiX, beta, and sigma online
for n in range(10, N):
    x, y = xy_from_df(df, n, n+1)
    XiX, beta, S = update_xbs(XiX, beta, S, x, y)
    
# Print result
print(np.sqrt(np.diag(XiX * S / (N - 3))))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1.0789208  1.29678338 0.0169534 ]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We indeed got the same result! Note that to get from the sum of squared residuals $S$ to the residuals variance $\hat{\sigma}^2$ we need to divide by the degrees of freedom: $n - k = 100 - 3$.&lt;/p&gt;
&lt;p&gt;As before we have plotted the evolution of the estimate of the OLS coefficient over time, we can now augment that plot with a confidence band of +- one standard deviation.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;online_regression(df, &amp;quot;fig/online_reg2.gif&amp;quot;, ci=True)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;fig/online_reg2.gif&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;As we can see, the estimated variance of the OLS estimator indeed decreases as the sample size increases.&lt;/p&gt;
&lt;h2 id=&#34;bootstrap&#34;&gt;Bootstrap&lt;/h2&gt;
&lt;p&gt;So far we have used the asymptotic assumptions behind the Central Limit Theorem to compute the standard errors of the estimator. However, we have a particularly small sample. We further check the empirical distribution of the model residuals.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sns.histplot(model.resid, bins=30);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/online_reg_39_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;The residuals seem to be particularly &lt;strong&gt;skewed&lt;/strong&gt;! This might be a problem in such a small sample.&lt;/p&gt;
&lt;p&gt;One alternative is &lt;strong&gt;the bootstrap&lt;/strong&gt;. Instead of relying on asymptotics, we approximate the distribution of our estimator by resampling our dataset with replacement. Can we bootstrap online?&lt;/p&gt;
&lt;p&gt;The answer is once again yes! They key is to weight each observation with an integer weight drawn from a Poisson distribution with mean (and variance) equal to 1. We repeat this process multiple times, in parallel and then we&lt;/p&gt;
&lt;p&gt;The updating rules for $(X&amp;rsquo;X)^{-1}$ and $\hat{beta}$ become the following.
$$
\begin{align*}
(X_{n+1}&amp;rsquo; X_{n+1})^{-1} = (X_n&amp;rsquo; X_n)^{-1} - \frac{w (X_n&amp;rsquo; X_n)^{-1} x_{n+1} x_{n+1}&amp;rsquo; (X_n&amp;rsquo; X_n)^{-1}}{1 + w x_{n+1}&amp;rsquo; (X_n&amp;rsquo; X_n)^{-1} x_{n+1}}
\end{align*}
$$&lt;/p&gt;
&lt;p&gt;and
$$
\hat{\beta}&lt;em&gt;{n+1} = \hat{\beta}&lt;/em&gt;{n} + w (X_n&amp;rsquo; X_n)^{-1} x_{n} (y_n - x_n&amp;rsquo; \hat{\beta}_{n})
$$&lt;/p&gt;
&lt;p&gt;where $w$ are Poisson weights. First, let&amp;rsquo;s update the updating function for $(X&amp;rsquo;X)^{-1}$ and $\hat{beta}$.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def update_xbw(XiX, beta, w, x, y):
    XiX -= (w * XiX @ x.T @ x @ XiX) / (1 + w * x @ XiX @ x.T )
    beta += w * XiX @ x.T @ (y - x @ beta)
    return XiX, beta
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can now run the online estimation. We bootstrap 1000 different estimates of $\hat{\beta}$.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Inizialize a vector of XiXs and betas 
np.random.seed(0)
K = 1000
x, y = xy_from_df(df, 0, 10)
XiXs = [inv(x.T @ x) for k in range(K)]
betas = [xix @ x.T @ y for xix in XiXs]

# Update the vector of XiXs and betas online
for n in range(10, N):
    x, y = xy_from_df(df, n, n+1)
    for k in range(K):
        w = np.random.poisson(1)
        XiXs[k], betas[k] = update_xbw(XiXs[k], betas[k], w, x, y)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can compute the estimated standard deviation of the treatment effect, simply by computing the standard deviation of the vector of bootstrapped coefficients.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;np.std(betas, axis=0)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;array([0.95301002, 1.14186364, 0.01207962])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The estimated standard errors are slightly different from the previous values of $[1.275, 1.532, 0.020]$, but not very far apart.&lt;/p&gt;
&lt;p&gt;Lastly, some of you might have wondered &amp;ldquo;&lt;em&gt;why sampling discrete weights and not continuous ones?&lt;/em&gt;&amp;rdquo;. Indeed, we can. This procedure is called the &lt;strong&gt;Bayesian Bootstrap&lt;/strong&gt; and you can find a more detailed explanation &lt;a href=&#34;https://towardsdatascience.com/6ca4a1d45148&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;In this post, we have seen hot to run an experiment without storing individual-level data. How are we able to do it? In order to compute the average treatment effect, we do not need every single observation but it&amp;rsquo;s sufficient to store just a more compact representation of it.&lt;/p&gt;
&lt;h3 id=&#34;references&#34;&gt;References&lt;/h3&gt;
&lt;p&gt;[1] W. Chou, &lt;a href=&#34;https://arxiv.org/abs/2102.03316&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Randomized Controlled Trials without Data Retention&lt;/a&gt; (2021), &lt;em&gt;Working Paper&lt;/em&gt;.&lt;/p&gt;
&lt;h3 id=&#34;related-articles&#34;&gt;Related Articles&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/954506cec665&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Experiments, Peeking, and Optimal Stopping&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/6ca4a1d45148&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The Bayesian Bootstrap&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;code&#34;&gt;Code&lt;/h3&gt;
&lt;p&gt;You can find the original Jupyter Notebook here:&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/online_reg.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/online_reg.ipynb&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Outliers, Leverage, and Influential Observations</title>
      <link>https://matteocourthoud.github.io/post/outliers/</link>
      <pubDate>Wed, 17 Aug 2022 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/post/outliers/</guid>
      <description>&lt;p&gt;&lt;em&gt;What makes an observation &amp;ldquo;unusual&amp;rdquo;?&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;In data science, one common task is outlier detection. This is a broad term that is often misused or misunderstood. More broadly, we are often interested in understanding any observation is &lt;strong&gt;&amp;ldquo;unusual&amp;rdquo;&lt;/strong&gt;. First of all, what does it mean to be unusual? In this article we are going to inspect three different ways in which an observation can be unusual: it can be unusual characteristics, it might not fit the model or it might be particularly influential in fitting the model. We will see that in linear regression the latter characteristics is a byproduct of the first two.&lt;/p&gt;
&lt;p&gt;Importantly, being unusual is &lt;strong&gt;not necessarily bad&lt;/strong&gt;. Observations that have different characteristics from all others usually carry more information. We also expect some observations not to fit the model well, otherwise the model is likely biased (overfitting). However, &amp;ldquo;unusual&amp;rdquo; observations are also more likely to be generated by a different process. Extreme cases include measurement error or fraud, but differences can be more nuanced. Domain knowledge is always kind and dropping observations only for for statistical reasons is never wise.&lt;/p&gt;
&lt;p&gt;That said, let&amp;rsquo;s have a look at some different ways in which observations can be &amp;ldquo;unusual&amp;rdquo;.&lt;/p&gt;
&lt;h1 id=&#34;example&#34;&gt;Example&lt;/h1&gt;
&lt;p&gt;Suppose we are an &lt;strong&gt;peer-to-peer online platform&lt;/strong&gt; and we are interested in understanding if there is anything suspicious going on with our business. We have information about how much time our customers spend on the platform and the total value of their transactions.&lt;/p&gt;
&lt;p&gt;First, let&amp;rsquo;s have a look at the data. I import the data generating process &lt;code&gt;dgp_p2p()&lt;/code&gt; from &lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/src/dgp.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;src.dgp&lt;/code&gt;&lt;/a&gt; and some plotting functions and libraries from &lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/src/utils.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;src.utils&lt;/code&gt;&lt;/a&gt;. I include code snippets from &lt;a href=&#34;https://deepnote.com&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Deepnote&lt;/a&gt;, a Jupyter-like web-based collaborative notebook environment. For our purpose, Deepnote is very handy because it allows me not only to include code but also output, like data and tables.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;%matplotlib inline
%config InlineBackend.figure_format = &#39;retina&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from src.utils import *
from src.dgp import dgp_p2p
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df = dgp_p2p().generate_data()
df.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;hours&lt;/th&gt;
      &lt;th&gt;transactions&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;2.6&lt;/td&gt;
      &lt;td&gt;8.30&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;2.0&lt;/td&gt;
      &lt;td&gt;8.00&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;7.0&lt;/td&gt;
      &lt;td&gt;21.00&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;6.7&lt;/td&gt;
      &lt;td&gt;18.00&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;1.2&lt;/td&gt;
      &lt;td&gt;3.82&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;We have information on 50 clients for which we observe &lt;code&gt;hours&lt;/code&gt; spent on the website and total &lt;code&gt;transactions&lt;/code&gt; amount. Since we only have two variables we can easily inspect them using a scatterplot.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sns.scatterplot(data=df, x=&#39;hours&#39;, y=&#39;transactions&#39;).set(title=&#39;Data Scatterplot&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/outliers_9_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;The relationship between &lt;code&gt;hours&lt;/code&gt; and &lt;code&gt;transactions&lt;/code&gt; seems to follow a clear linear relationship. If we fit a linear model, we observe a particularly tight fit.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;smf.ols(&#39;hours ~ transactions&#39;, data=df).fit().summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
        &lt;td&gt;&lt;/td&gt;          &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt;    &lt;td&gt;   -0.0975&lt;/td&gt; &lt;td&gt;    0.084&lt;/td&gt; &lt;td&gt;   -1.157&lt;/td&gt; &lt;td&gt; 0.253&lt;/td&gt; &lt;td&gt;   -0.267&lt;/td&gt; &lt;td&gt;    0.072&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;transactions&lt;/th&gt; &lt;td&gt;    0.3452&lt;/td&gt; &lt;td&gt;    0.009&lt;/td&gt; &lt;td&gt;   39.660&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    0.328&lt;/td&gt; &lt;td&gt;    0.363&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;Does any data point look suspiciously different from the others? How?&lt;/p&gt;
&lt;h2 id=&#34;leverage&#34;&gt;Leverage&lt;/h2&gt;
&lt;p&gt;The first metric that we are going to use to evaluate &amp;ldquo;unusual&amp;rdquo; observations is the &lt;strong&gt;leverage&lt;/strong&gt;, which was first introduced by &lt;a href=&#34;https://www.jstor.org/stable/1268249&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Cook (1980)&lt;/a&gt;. The objective of the leverage is to capture how much a single point is different with respect to other data points. These data points are often called &lt;strong&gt;outliers&lt;/strong&gt; and there exist a nearly amount of algorithms and rules of thumb to flag them.However the idea is the same: flagging observations that are unusual in terms of features.&lt;/p&gt;
&lt;p&gt;The leverage of an observation $i$ is defined as&lt;/p&gt;
&lt;p&gt;$$
h_{ii} := x_i&amp;rsquo; (X&amp;rsquo;X)^{-1} x_i
$$&lt;/p&gt;
&lt;p&gt;One interpretation of the leverage is as a &lt;strong&gt;measure of distance&lt;/strong&gt; where individual observations are compared against the average of all observations.&lt;/p&gt;
&lt;p&gt;Another interpretation of the leverage is as the influence of the outcome of observation $i$, $y_i$, on the corresponding fitted value $\hat{y_i}$.&lt;/p&gt;
&lt;p&gt;$$
h_{ii} = \frac{\partial \hat{y}_i}{\partial y_i}
$$&lt;/p&gt;
&lt;p&gt;Algebraically, the leverage of observation $i$ is the $i^{th}$ element of the &lt;strong&gt;design matrix&lt;/strong&gt; $X&amp;rsquo; (X&amp;rsquo;X)^{-1} X$. Among the many properties of the leverages, is the fact that they are non-negative and their values sum to 1.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s compute the leverage of the observations in our dataset. We also flag observations that have unusual leverages (which we arbitrarily define as more than two standard deviations away from the average leverage).&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;X = np.reshape(df[&#39;hours&#39;].values, (-1, 1))
Y = np.reshape(df[&#39;transactions&#39;].values, (-1, 1))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df[&#39;leverage&#39;] = np.diagonal(X @ np.linalg.inv(X.T @ X) @ X.T)
df[&#39;high_leverage&#39;] = df[&#39;leverage&#39;] &amp;gt; (np.mean(df[&#39;leverage&#39;]) + 2*np.std(df[&#39;leverage&#39;]))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s plot the distribution of leverage values in our data.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;fix, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))
sns.histplot(data=df, x=&#39;leverage&#39;, hue=&#39;high_leverage&#39;, alpha=1, bins=30, ax=ax1).set(title=&#39;Distribution of Leverages&#39;);
sns.scatterplot(data=df, x=&#39;hours&#39;, y=&#39;transactions&#39;, hue=&#39;high_leverage&#39;, ax=ax2).set(title=&#39;Data Scatterplot&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/outliers_18_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;As we can see, the distribution is skewed with two observations having a unusually high leverage. Indeed, in the scatterplot these two observations are slightly separated from the rest of the distribution.&lt;/p&gt;
&lt;p&gt;Is this bad news? It depends. Outliers are &lt;strong&gt;not a problem per se&lt;/strong&gt;. Actually, if they are genuine observations, they might carry much more information than other observations. On the other hand, they are also more likely &lt;em&gt;not&lt;/em&gt; to be genuine observations (e.g. fraud, measurement error, &amp;hellip;) or to be inherently different from the other ones (e.g. professional users vs amateurs). In any case, we might want to investigate further and use as much context-specific information as we can.&lt;/p&gt;
&lt;p&gt;Importantly, the fact that an observation has a high leverage tells us information about the features of the model but nothing about the model itself. Are these users just different observations or they also behave differently?&lt;/p&gt;
&lt;h2 id=&#34;residuals&#34;&gt;Residuals&lt;/h2&gt;
&lt;p&gt;So far we have only talked about unusual features, but what about &lt;strong&gt;unusual behavior&lt;/strong&gt;? This is what regression residuals measure.&lt;/p&gt;
&lt;p&gt;Regression residuals are the difference between the predicted outcome values and the observed outcome values. In a sense, they capture what the model cannot explain: the higher the residual of one observation the more it is unusual in the sense that the model cannot explain it.&lt;/p&gt;
&lt;p&gt;In the case of linear regression, residuals can be written as&lt;/p&gt;
&lt;p&gt;$$
\hat{e} = y - \hat{y} = y - \hat \beta X
$$&lt;/p&gt;
&lt;p&gt;In our case, since $X$ is one dimensional (&lt;code&gt;hours&lt;/code&gt;), we can easily visualize them.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;Y_hat = X @ np.linalg.inv(X.T @ X) @ X.T @ Y
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plt.scatter(X, Y, s=50, label=&#39;data&#39;)
plt.plot(X, Y_hat, c=&#39;k&#39;, lw=2, label=&#39;prediction&#39;)
plt.vlines(X, np.minimum(Y, Y_hat), np.maximum(Y, Y_hat), color=&#39;r&#39;, lw=3, label=&amp;quot;residuals&amp;quot;);
plt.legend()
plt.title(f&amp;quot;Regression prediction and residuals&amp;quot;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/outliers_23_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Do some observations have unusually high residuals? Let&amp;rsquo;s plot their distribution.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df[&#39;residual&#39;] = np.abs(Y - X @ np.linalg.inv(X.T @ X) @ X.T @ Y)
df[&#39;high_residual&#39;] = df[&#39;residual&#39;] &amp;gt; (np.mean(df[&#39;residual&#39;]) + 2*np.std(df[&#39;residual&#39;]))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;fix, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))
sns.histplot(data=df, x=&#39;residual&#39;, hue=&#39;high_residual&#39;, alpha=1, bins=30, ax=ax1).set(title=&#39;Distribution of Residuals&#39;);
sns.scatterplot(data=df, x=&#39;hours&#39;, y=&#39;transactions&#39;, hue=&#39;high_residual&#39;, ax=ax2).set(title=&#39;Data Scatterplot&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/outliers_26_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Two observations have particularly high residuals. This means that for these observations, the model is not good at predicting the observed outcomes.&lt;/p&gt;
&lt;p&gt;Is this bad news? Not necessarily. A model that fits the observations too well is likely to be &lt;a href=&#34;https://en.wikipedia.org/wiki/Bias_%28statistics%29&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;biased&lt;/strong&gt;&lt;/a&gt;. However, it might still be important to understand why some users have a different relationship between hours spent and total transactions. As usual, information on the specific context is key.&lt;/p&gt;
&lt;p&gt;So far we have looked at observations with &amp;ldquo;unusual&amp;rdquo; characteristics and &amp;ldquo;unusual&amp;rdquo; model fit, but what is the observation itself is distorting the model? How much our model is driven by a handful of observations?&lt;/p&gt;
&lt;h2 id=&#34;influence&#34;&gt;Influence&lt;/h2&gt;
&lt;p&gt;The concept of &lt;strong&gt;influence and influence functions&lt;/strong&gt; was developed precisely to answer this question: what are influential observations? This questions were very popular in the 80&amp;rsquo;s and lost appeal for a long time until the recent need of explaining complex machine learning and AI models.&lt;/p&gt;
&lt;p&gt;The general idea is to define an observation as &lt;strong&gt;influential&lt;/strong&gt; if removing it significantly changes the estimated model. In linear regression, we define the influence of observation $i$ as:&lt;/p&gt;
&lt;p&gt;$$
\hat{\beta} - \hat{\beta}_{-i} = (X&amp;rsquo;X)^{-1} x_i e_i
$$&lt;/p&gt;
&lt;p&gt;Where $\hat{\beta}_{-i}$ is the OLS coefficient estimated omitting observation $i$.&lt;/p&gt;
&lt;p&gt;As you can see, there is a tight connection to both leverage $h_{ii}$ and residuals $e_i$: influence is almost the product of the two. Indeed, in linear regression, observations with high leverage are observations that are both outliers and have high residuals. None of the two conditions alone is sufficient for an observation to have an influence on the model.&lt;/p&gt;
&lt;p&gt;We can see it best in the data.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df[&#39;influence&#39;] = (np.linalg.inv(X.T @ X) @ X.T).T * np.abs(Y - Y_hat)
df[&#39;high_influence&#39;] = df[&#39;influence&#39;] &amp;gt; (np.mean(df[&#39;influence&#39;]) + 2*np.std(df[&#39;influence&#39;]))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;fix, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))
sns.histplot(data=df, x=&#39;influence&#39;, hue=&#39;high_influence&#39;, alpha=1, bins=30, ax=ax1).set(title=&#39;Distribution of Influences&#39;);
sns.scatterplot(data=df, x=&#39;hours&#39;, y=&#39;transactions&#39;, hue=&#39;high_influence&#39;, ax=ax2).set(title=&#39;Data Scatterplot&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/outliers_31_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;In our dataset, there is only one observation with high influence, and it is disproportionally larger than the influence of all other observations.&lt;/p&gt;
&lt;p&gt;We can now plot all &amp;ldquo;unusual&amp;rdquo; points in the same plot. I also report residuals and leverage of each point in a separate plot.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def plot_leverage_residuals(df):

    # Hue
    df[&#39;type&#39;] = &#39;Normal&#39;
    df.loc[df[&#39;high_residual&#39;], &#39;type&#39;] = &#39;High Residual&#39;
    df.loc[df[&#39;high_leverage&#39;], &#39;type&#39;] = &#39;High Leverage&#39;
    df.loc[df[&#39;high_influence&#39;], &#39;type&#39;] = &#39;High Influence&#39;

    # Init figure
    fig, (ax1,ax2) = plt.subplots(1,2, figsize=(12,5)) 
    ax1.plot(X, Y_hat, lw=1, c=&#39;grey&#39;, zorder=0.5)
    sns.scatterplot(data=df, x=&#39;hours&#39;, y=&#39;transactions&#39;, ax=ax1, hue=&#39;type&#39;).set(title=&#39;Data&#39;)
    sns.scatterplot(data=df, x=&#39;residual&#39;, y=&#39;leverage&#39;, hue=&#39;type&#39;, ax=ax2).set(title=&#39;Metrics&#39;)
    ax1.get_legend().remove()
    sns.move_legend(ax2, &amp;quot;upper left&amp;quot;, bbox_to_anchor=(1.05, 0.8));
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plot_leverage_residuals(df)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/outliers_34_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;As we can see, we have one point with high residual and low leverage, one with high leverage and low residual and only one point with both high leverage and high residual: the only influential point.&lt;/p&gt;
&lt;p&gt;From the plot it is also clear why none of the two conditions alone is sufficient for an observation to rive the model. The orange point has high residual but it lies right in the middle of the distribution and therefore cannot tilt the line of best fit. The green point instead has high leverage and lies far from the center of the distribution but its perfectly aligned with the line of fit. Removing it would not change anything. The red dot instead is different from the others in terms of &lt;strong&gt;both characteristics and behavior&lt;/strong&gt; and therefore tilts the fit line towards itself.&lt;/p&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;In this post, we have seen some different ways in which observations can be &amp;ldquo;unusual&amp;rdquo;: they can have either unusual characteristics or unusual behavior. In linear regression, when an observation has both it is also influential: it tilts the model towards itself.&lt;/p&gt;
&lt;p&gt;In the example of the article, we concentrated on a univariate linear regression. However, research on influence functions has recently become a hot topic because of the need to make black-box machine learning algorithms understandable. With models with millions of parameters, billions of observations and wild non-linearities, it can be very hard to establish whether a single observation is influential and how.&lt;/p&gt;
&lt;h3 id=&#34;references&#34;&gt;References&lt;/h3&gt;
&lt;p&gt;[1] D. Cook, &lt;a href=&#34;https://www.jstor.org/stable/1268249&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Detection of Influential Observation in Linear Regression&lt;/a&gt; (1980), &lt;em&gt;Technometrics&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;[2] D. Cook, S. Weisberg, &lt;a href=&#34;https://www.jstor.org/stable/1268187&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Characterizations of an Empirical Influence Function for Detecting Influential Cases in
Regression&lt;/a&gt; (1980), &lt;em&gt;Technometrics&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;[2] P. W. Koh, P. Liang, &lt;a href=&#34;http://proceedings.mlr.press/v70/koh17a&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Understanding Black-box Predictions via Influence Functions&lt;/a&gt; (2017), &lt;em&gt;ICML Proceedings&lt;/em&gt;.&lt;/p&gt;
&lt;h3 id=&#34;code&#34;&gt;Code&lt;/h3&gt;
&lt;p&gt;You can find the original Jupyter Notebook here:&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/outliers.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/outliers.ipynb&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>The Bayesian Bootstrap</title>
      <link>https://matteocourthoud.github.io/post/bayes_boot/</link>
      <pubDate>Wed, 17 Aug 2022 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/post/bayes_boot/</guid>
      <description>&lt;p&gt;&lt;em&gt;A short guide to a simple and powerful alternative to the bootstrap&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;In causal inference we do not want just to compute treatment effect, we also want to do &lt;strong&gt;inference&lt;/strong&gt; (duh!). In some cases, it&amp;rsquo;s very easy to compute the asymptotic difference of an estimator, thanks to the &lt;a href=&#34;https://en.wikipedia.org/wiki/Central_limit_theorem&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;central limit theorem&lt;/strong&gt;&lt;/a&gt;. This is the case of computing the average treatment effect in AB tests or randomized controlled trials, for example. However, in other settings, inference is more &lt;strong&gt;complicated&lt;/strong&gt;. The most frequent setting is the computation of quantities that are not sums or averages, such as the median treatment effect, for example. In these cases, we cannot rely on the central limit theorem. What can we do then?&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;bootstrap&lt;/strong&gt; is the answer! It is a very powerful procedure to compute the distribution of an estimator, without needing any knowledge of the data generating process. It is also very &lt;strong&gt;intuitive and simple&lt;/strong&gt; to implement: just re-sample your data with replacement a lot of times and compute your estimator on the re-computed sample.&lt;/p&gt;
&lt;p&gt;Can we do better? The answer is yes! The &lt;strong&gt;Bayesian Bootstrap&lt;/strong&gt; is a powerful procedure that in a lot of setting performs &lt;strong&gt;better&lt;/strong&gt; than the bootstrap. In particular, it&amp;rsquo;s usually faster, can give tighter confidence intervals and prevents a lot of corner cases of the bootstrap. In this article we are going to explore this simple but powerful procedure more in detail.&lt;/p&gt;
&lt;h2 id=&#34;the-bootstrap&#34;&gt;The Bootstrap&lt;/h2&gt;
&lt;p&gt;Bootstrap is a procedure to compute properties of an estimator by random &lt;strong&gt;re-sampling with replacement&lt;/strong&gt; from the data. It was first introduced by &lt;a href=&#34;https://www.jstor.org/stable/2958830&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Efron (1979)&lt;/a&gt;. The procedure is very simple and consists in the following steps.&lt;/p&gt;
&lt;p&gt;Suppose you have access to an i.i.d. sample $\lbrace X_i \rbrace_{i=1}^n$ and you want to compute a statistic $\theta$ using an estimator $\hat \theta(X)$. You can approximate the distribution of $\hat \theta$ by&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Sample $n$ observations with replacement from your sample $\lbrace \tilde X_i \rbrace_{i=1}^n$&lt;/li&gt;
&lt;li&gt;Compute the estimator $\hat \theta_{bootstrap}(\tilde X)$&lt;/li&gt;
&lt;li&gt;Repeat steps 1 and 2 a large number of times&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The distribution of $\hat \theta_{bootstrap}$ is a good approximation of the distribution of $\hat \theta$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Why is the bootstrap so powerful?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;First of all, it&amp;rsquo;s &lt;strong&gt;easy to implement&lt;/strong&gt;. It does not require you to do anything more than what you were already doing: estimating $\theta$. You just need to do it &lt;em&gt;a lot of times&lt;/em&gt;. Indeed, the main disadvantage of the bootstrap is its &lt;strong&gt;computational speed&lt;/strong&gt;. If estimating $\theta$ once is slow, bootstrapping it is prohibitive.&lt;/p&gt;
&lt;p&gt;Second, the bootstrap makes &lt;strong&gt;no distributional assumption&lt;/strong&gt;. It only assumes a representative sample from your population, where observations are independent from each other. This assumption might be violated when observations are tightly connected with each other, such when studying social networks.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Is bootstrap just weighting?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In the end, what we are doing is assigning &lt;strong&gt;integer weights&lt;/strong&gt; to our observations, such that their sum adds up to $n$. Such distribution is the &lt;a href=&#34;https://en.wikipedia.org/wiki/Multinomial_distribution&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;multinomial distribution&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s have a look at what a multinomial distribution look like by drawing a sample of size 10.000.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;%matplotlib inline
%config InlineBackend.figure_format = &#39;retina&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from src.utils import *
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;N = 10_000
np.random.seed(1)
bootstrap_weights = np.random.multinomial(N, np.ones(N)/N)
np.sum(bootstrap_weights)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;10000
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;First of all, we check that indeed the weights sum up to 1000, or equivalently, we generated a re-sample of the same size of the data.&lt;/p&gt;
&lt;p&gt;We can now plot the &lt;strong&gt;distribution of weights&lt;/strong&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sns.countplot(bootstrap_weights, color=&#39;C0&#39;).set(title=&#39;Bootstrap Weights&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/bayes_boot_9_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;As we can see, around 3600 observations got zero weight, however a couple of observations got a weights of 6. Or equivalently, around 3600 observations did not get re-sampled while a couple of observations got samples as many as 6 times.&lt;/p&gt;
&lt;p&gt;Now you might have a spontaneous question: why not use &lt;strong&gt;continuous weights&lt;/strong&gt; instead of discrete ones?&lt;/p&gt;
&lt;p&gt;Very good question! The &lt;strong&gt;Bayesian Bootstrap&lt;/strong&gt; is the answer.&lt;/p&gt;
&lt;h2 id=&#34;the-bayesian-bootstrap&#34;&gt;The Bayesian Bootstrap&lt;/h2&gt;
&lt;p&gt;The Bayesian bootstrap was introduced by &lt;a href=&#34;https://www.jstor.org/stable/2240875&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Rubin (1981)&lt;/a&gt; and it&amp;rsquo;s based on a very simple &lt;strong&gt;idea&lt;/strong&gt;: why not draw a smoother distribution of weights? The continuous equivalent of the multinomial distribution is the &lt;a href=&#34;https://en.wikipedia.org/wiki/Dirichlet_distribution&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Dirichelet distribution&lt;/strong&gt;&lt;/a&gt;. Below I plot the probability distribution of Multinomial and Dirichelet weights for a single observation (they are Poisson and Gamma distributed, respectively).&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from scipy.stats import gamma, poisson

x1 = np.arange(0, 8, 0.001)
x2 = np.arange(0, 8, 1)
sns.barplot(x2, poisson.pmf(x2, mu=1), color=&#39;C0&#39;, label=&#39;Multinomial Weights&#39;); 
plt.plot(x1, gamma.pdf(x1, a=1.0001), color=&#39;C1&#39;, label=&#39;Dirichlet Weights&#39;);
plt.legend()
plt.title(&#39;Distribution of Bootstrap Weights&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/bayes_boot_13_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;The Bayesian Bootstrap has &lt;strong&gt;many advantages&lt;/strong&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The first and most intuitive one is that it delivers estimates that are much more &lt;strong&gt;smooth&lt;/strong&gt; than the normal bootstrap, because of its continuous weighting scheme.&lt;/li&gt;
&lt;li&gt;Moreover, the continuous weighting scheme &lt;strong&gt;prevents corner cases&lt;/strong&gt; from emerging, since no observation will ever receive zero weight. For example, in linear regression, no problem of &lt;a href=&#34;https://en.wikipedia.org/wiki/Multicollinearity&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;collinearity&lt;/a&gt; emerges, if there wasn&amp;rsquo;t one in the original sample.&lt;/li&gt;
&lt;li&gt;Lastly, being a Bayesian method, we gain &lt;strong&gt;interpretation&lt;/strong&gt;: the estimated distribution of the estimator can be interpreted as the &lt;a href=&#34;https://en.wikipedia.org/wiki/Posterior_probability&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;posterior distribution&lt;/a&gt; with an &lt;a href=&#34;https://en.wikipedia.org/wiki/Prior_probability&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;uninformative prior&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Let&amp;rsquo;s now draw a set a Dirichlet weights.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;bayesian_weights = np.random.dirichlet(alpha=np.ones(N), size=1)[0] * N
np.sum(bayesian_weights)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;10000.000000000005
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The weights naturally sum to (approximately) 1, so we have to scale them by a factor N.&lt;/p&gt;
&lt;p&gt;As before, we can plot the distribution of weights, with the difference that now we have continuous weights, so we have to approximate the distribution.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sns.histplot(bayesian_weights, color=&#39;C1&#39;).set(title=&#39;Dirichlet Weights&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/bayes_boot_17_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;As you might have noticed, the Dirichelet distirbution has a parameter $\alpha$ that we have set to 1 for all observations. What does it do?&lt;/p&gt;
&lt;p&gt;The $\alpha$ parameter essentially governs both the absolute and relative probability of being samples. Increasing $\alpha$ for all observations makes the distribution less skewed so that all observations have a more similar weight. For $\alpha \to \infty$, all observations receiver the same weight and we are back to the original sample.&lt;/p&gt;
&lt;p&gt;How should we pick $\alpha$? &lt;a href=&#34;https://link.springer.com/book/10.1007/978-1-4612-0795-5&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Shao and Tu (1995)&lt;/a&gt; suggest the following.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;The distribution of the random weight vector does not have to be restricted to the Diri(l, &amp;hellip; , 1). Later investigations found that the weights having a scaled Diri(4, &amp;hellip; ,4) distribution give better approximations (Tu and Zheng, 1987)&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Let&amp;rsquo;s have a look at how a Dirichelet distribution with $\alpha = 4$ for all observations compare to our previous distribution with $\alpha = 1$ for all observations.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;bayesian_weights2 = np.random.dirichlet(np.ones(N) * 4, 1)[0] * N
sns.histplot(bayesian_weights, color=&#39;C1&#39;)
sns.histplot(bayesian_weights2, color=&#39;C2&#39;).set(title=&#39;Comparing Dirichlet Weights&#39;);
plt.legend([r&#39;$\alpha = 1$&#39;, r&#39;$\alpha = 4$&#39;]);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/bayes_boot_19_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;The new distribution is much less skewed and more concentrated around the average value of 1.&lt;/p&gt;
&lt;h2 id=&#34;examples&#34;&gt;Examples&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s have a look at a couple of examples, where we compare both inference procedures.&lt;/p&gt;
&lt;h3 id=&#34;mean-of-a-skewed-distribution&#34;&gt;Mean of a Skewed Distribution&lt;/h3&gt;
&lt;p&gt;First, let&amp;rsquo;s have a look at one of the simplest and most common estimators: the &lt;strong&gt;sample mean&lt;/strong&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;np.random.seed(2)
X = pd.Series(np.random.pareto(2, 100))
sns.histplot(X).set(title=&#39;Sample from Pareto Distribution&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/bayes_boot_25_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def classic_boot(df, estimator, seed=1):
    df_boot = df.sample(n=len(df), replace=True, random_state=seed)
    estimate = estimator(df_boot)
    return estimate
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;classic_boot(X, np.mean)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;0.7079805545831946
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def bayes_boot(df, estimator, seed=1):
    np.random.seed(seed)
    w = np.random.dirichlet(np.ones(len(df)), 1)[0]
    result = estimator(df, weights=w)
    return result
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;bayes_boot(X, np.average)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;1.0378495251293498
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from joblib import Parallel, delayed

def bootstrap(boot_method, df, estimator, K):
    r = Parallel(n_jobs=8)(delayed(boot_method)(df, estimator, seed=i) for i in range(K))
    return r
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def compare_boot(df, boot1, boot2, estimator, title, K=1000):
    s1 = bootstrap(boot1, df, estimator, K)
    s2 = bootstrap(boot2, df, estimator, K)
    df = pd.DataFrame({&#39;Estimate&#39;: s1 + s2,
                       &#39;Estimator&#39;: [&#39;Classic&#39;]*K + [&#39;Bayes&#39;]*K})
    sns.histplot(data=df, x=&#39;Estimate&#39;, hue=&#39;Estimator&#39;)
    plt.legend([f&#39;Bayes:   {np.mean(s2):.2f} ({np.std(s2):.2f})&#39;,
                f&#39;Classic: {np.mean(s1):.2f} ({np.std(s1):.2f})&#39;])
    plt.title(f&#39;Bootstrap Estimates of {title}&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;compare_boot(X, classic_boot, bayes_boot, np.average, &#39;Sample Mean&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/bayes_boot_32_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;In this setting, both procedures give a very similar answer.&lt;/p&gt;
&lt;p&gt;Which one is faster?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import time

def compare_time(df, boot1, boot2, estimator, K=1000):
    t1, t2 = np.zeros(K), np.zeros(K)
    for k in range(K):
        
        # Classic bootstrap
        start = time.time()
        boot1(df, estimator)
        t1[k] = time.time() - start
    
        # Bayesian bootstrap
        start = time.time()
        boot2(df, estimator)
        t2[k] = time.time() - start
    
    print(f&amp;quot;Bayes wins {np.mean(t1 &amp;gt; t2)*100}% of the time (by {np.mean((t1 - t2)/t1*100):.2f}%)&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;compare_time(X, classic_boot, bayes_boot, np.average)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Bayes wins 99.8% of the time (by 82.89%)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The Bayesian bootstrap is faster than the classical bootstrap 100% of the simulations, and by an impressive 83%!&lt;/p&gt;
&lt;h3 id=&#34;no-weighting-no-problem&#34;&gt;No Weighting? No Problem&lt;/h3&gt;
&lt;p&gt;What if we have an estimator that does not accept weights, such as the median? We can do &lt;strong&gt;two-level sampling&lt;/strong&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def twolv_boot(df, estimator, seed=1):
    np.random.seed(seed)
    w = np.random.dirichlet(np.ones(len(df))*4, 1)[0]
    df_boot = df.sample(n=len(df)*10, replace=True, weights=w, random_state=seed)
    result = estimator(df_boot)
    return result
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;np.random.seed(1)
X = pd.Series(np.random.normal(0, 10, 1000))
compare_boot(X, classic_boot, twolv_boot, np.median, &#39;Sample Median&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/bayes_boot_40_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;In this setting, the Bayesian Bootstrap is also &lt;strong&gt;more precise&lt;/strong&gt; than the classical bootstrap.&lt;/p&gt;
&lt;h3 id=&#34;logistic-regression-with-rare-outcome&#34;&gt;Logistic Regression with Rare Outcome&lt;/h3&gt;
&lt;p&gt;Let&amp;rsquo;s now explore the first of two settings in which the classical bootstrap might fall into &lt;strong&gt;corner cases&lt;/strong&gt;. Suppose we observed a feature $x$, normally distributed, and a binary outcome $y$. We are interested in the relationship between the two variables.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;N = 100
np.random.seed(1)
x = np.random.normal(0, 1, N)
y = np.rint(np.random.normal(x, 1, N) &amp;gt; 2)
df = pd.DataFrame({&#39;x&#39;: x, &#39;y&#39;: y})
df.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;x&lt;/th&gt;
      &lt;th&gt;y&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;1.624345&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;-0.611756&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;-0.528172&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;-1.072969&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;0.865408&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;In this case, we observe a positive outcome only in 10 observations out of 100.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;np.sum(df[&#39;y&#39;])
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;10.0
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Since the outcome is binary, we fit a &lt;a href=&#34;https://en.wikipedia.org/wiki/Logistic_regression&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;logistic regression&lt;/strong&gt;&lt;/a&gt; model.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;smf.logit(&#39;y ~ x&#39;, data=df).fit(disp=False).summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
      &lt;td&gt;&lt;/td&gt;         &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;z&lt;/th&gt;      &lt;th&gt;P&gt;|z|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt; &lt;td&gt;   -4.0955&lt;/td&gt; &lt;td&gt;    0.887&lt;/td&gt; &lt;td&gt;   -4.618&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;   -5.834&lt;/td&gt; &lt;td&gt;   -2.357&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;x&lt;/th&gt;         &lt;td&gt;    2.7664&lt;/td&gt; &lt;td&gt;    0.752&lt;/td&gt; &lt;td&gt;    3.677&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    1.292&lt;/td&gt; &lt;td&gt;    4.241&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;Can we bootstrap the distribution of our estimator? Let&amp;rsquo;s try to compute the logistic regression coefficient over 1000 bootstrap samples.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;estimate_logit = lambda df: smf.logit(&#39;y ~ x&#39;, data=df).fit(disp=False).params[1]
for i in range(1000):
    try:
        classic_boot(df, estimate_logit, seed=i)
    except Exception as e:
        print(f&#39;Error for bootstrap number {i}: {e}&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Error for bootstrap number 92: Perfect separation detected, results not available
Error for bootstrap number 521: Perfect separation detected, results not available
Error for bootstrap number 545: Perfect separation detected, results not available
Error for bootstrap number 721: Perfect separation detected, results not available
Error for bootstrap number 835: Perfect separation detected, results not available
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For 5 samples out of 1000, we are &lt;strong&gt;unable&lt;/strong&gt; to compute the estimate. This would not have happened with then bayesian bootstrap.&lt;/p&gt;
&lt;p&gt;This might seem like an innocuous issue in this case: we can just drop those observations. Let&amp;rsquo;s conclude with a much more dangerous example.&lt;/p&gt;
&lt;p&gt;Suppose we observed a binary feature $x$ and a continuous outcome $y$. We are again interested in the relationship between the two variables.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;N = 100
np.random.seed(1)
x = np.random.binomial(1, 5/N, N)
y = np.random.normal(1 + 2*x, 1, N)
df = pd.DataFrame({&#39;x&#39;: x, &#39;y&#39;: y})
df.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;x&lt;/th&gt;
      &lt;th&gt;y&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1.315635&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;-1.022201&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0.693796&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1.827975&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1.230095&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;Let&amp;rsquo;s compare the two bootstrap estimators of the regression coefficient of $y$ on $x$.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;estimate_beta = lambda df, **kwargs: smf.wls(&#39;y ~ x&#39;, data=df, **kwargs).fit().params[1]
compare_boot(df, classic_boot, bayes_boot, estimate_beta, &#39;beta&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/bayes_boot_55_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;The classic bootstrap procedure estimates a 50% larger variance of our estimator. Why? If we look more closely, we seen that in almost 20 re-samples, we get a very unusual estimate of zero!&lt;/p&gt;
&lt;p&gt;The problem is that in some samples we might not have have &lt;strong&gt;any observations&lt;/strong&gt; with $x=1$. Therefore, in these re-samples, the estimated coefficient is zero. This does not happen with the Bayesian bootstrap, since it does not drop any observation.&lt;/p&gt;
&lt;p&gt;The problematic part here is that we are not getting any error message or warning. This bias is very sneaky and could easily go &lt;strong&gt;unnoticed&lt;/strong&gt;!&lt;/p&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;The article was inspired by the following tweet by Brown University professor &lt;a href=&#34;https://sites.google.com/site/aboutpeterhull/home&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Peter Hull&lt;/a&gt;&lt;/p&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;Ok, so I come bearing good news for ~93% of you: esp. those bootstraping complex models (e.g. w/many FEs)&lt;br&gt;&lt;br&gt;Instead of resampling, which can be seen as reweighting by a random integer W that may be zero, you can reweight by a random non-zero non-integer W &lt;a href=&#34;https://t.co/Rpm1GmomHg&#34;&gt;https://t.co/Rpm1GmomHg&lt;/a&gt;&lt;/p&gt;&amp;mdash; Peter Hull (@instrumenthull) &lt;a href=&#34;https://twitter.com/instrumenthull/status/1487469316010389516?ref_src=twsrc%5Etfw&#34;&gt;January 29, 2022&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;

&lt;p&gt;Indeed, besides being a simple and intuitive procedure, the Bayesian Bootstrap is not part of the standard econometrics curriculum in economic graduate schools.&lt;/p&gt;
&lt;h3 id=&#34;references&#34;&gt;References&lt;/h3&gt;
&lt;p&gt;[1] B. Efron &lt;a href=&#34;https://www.jstor.org/stable/2958830&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Bootstrap Methods: Another Look at the Jackknife&lt;/a&gt; (1979), &lt;em&gt;The Annals of Statistics&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;[2] D. Rubin, &lt;a href=&#34;https://www.jstor.org/stable/2240875&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The Bayesian Bootstrap&lt;/a&gt; (1981), &lt;em&gt;The Annals of Statistics&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;[3] A. Lo, &lt;a href=&#34;https://www.jstor.org/stable/2241087&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;A Large Sample Study of the Bayesian Bootstrap&lt;/a&gt; (1987), &lt;em&gt;The Annals of Statistics&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;[4] J. Shao, D. Tu, &lt;a href=&#34;https://link.springer.com/book/10.1007/978-1-4612-0795-5&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Jacknife and Bootstrap&lt;/a&gt; (1995), &lt;em&gt;Springer&lt;/em&gt;.&lt;/p&gt;
&lt;h3 id=&#34;code&#34;&gt;Code&lt;/h3&gt;
&lt;p&gt;You can find the original Jupyter Notebook here:&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/bayes_boot.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/bayes_boot.ipynb&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Understanding Synthetic Control Methods</title>
      <link>https://matteocourthoud.github.io/post/synth/</link>
      <pubDate>Wed, 27 Jul 2022 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/post/synth/</guid>
      <description>&lt;p&gt;&lt;em&gt;A detailed introduction to one of the most popular causal inference techniques in the industry&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;It is now widely accepted that the gold standard technique to compute the &lt;strong&gt;causal effect&lt;/strong&gt; of a treatment (a drug, ad, product, &amp;hellip;) on an outcome of interest (a disease, firm revenue, customer satisfaction, &amp;hellip;) is &lt;strong&gt;AB testing&lt;/strong&gt;, a.k.a. randomized experiments. We randomly split a set of subjects (patients, users, customers, &amp;hellip;) into a treatment and a control group and give the treatment to the treatment group. This procedure ensures that, ex-ante, the only expected difference between the two groups is caused by the treatment.&lt;/p&gt;
&lt;p&gt;One of the key assumptions in AB testing is that there is &lt;strong&gt;no contamination&lt;/strong&gt; between treatment and control group. Giving a drug to one patient in the treatment group does not affect the health of patients in the control group. This might not be the case for example if we are twying to cure a contageous disease and the two groups are not isolated. In the industry, frequent violations of the contamination assumption are &lt;strong&gt;network effects&lt;/strong&gt; - my utility of using a social network increases as the number of friends on the network increases - and &lt;strong&gt;general equilibrium effects&lt;/strong&gt; - if I improve one product, it might decrease the sales of another similar product.&lt;/p&gt;
&lt;p&gt;Because of this reason, often experiments are carried out at a sufficiently large scale so that there is no contamination across groups, such as cities, states or even countries. Then another problem arises because of the larger scale: the treatment becomes &lt;strong&gt;more expensive&lt;/strong&gt;. Giving a drug to 50% of patients in a hospital is much less expensive than giving a drug to 50% of cities in a country. Therefore, often only &lt;strong&gt;few units are treated&lt;/strong&gt; but often over a longer period of time.&lt;/p&gt;
&lt;p&gt;In these settings, a very powerful method emerged around 10 years age: &lt;strong&gt;Synthetic Control&lt;/strong&gt;. The idea of synthetic control is to exploit the temporal variation in the data instead of the cross-sectional one (across time instead of across units). This method is extremely popular in the industry - e.g. in companies like &lt;a href=&#34;https://proceedings.neurips.cc/paper/2021/file/48d23e87eb98cc2227b5a8c33fa00680-Paper.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Google&lt;/a&gt;, &lt;a href=&#34;https://eng.uber.com/causal-inference-at-uber/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Uber&lt;/a&gt;, &lt;a href=&#34;https://research.facebook.com/publications/regression-adjustment-with-synthetic-controls-in-online-experiments/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Facebook&lt;/a&gt;, &lt;a href=&#34;https://github.com/Microsoft/SparseSC&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Microsoft&lt;/a&gt;, &lt;a href=&#34;https://www.amazon.science/publications/a-self-supervised-approach-to-hierarchical-forecasting-with-applications-to-groupwise-synthetic-controls&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Amazon&lt;/a&gt; - because it is easy to interpret and deals with a setting that emerges often at large scales. In this post we are going to explore this technique by means of example: we will investigate the effectiveness of self-driving cars for a ride-sharing platform.&lt;/p&gt;
&lt;h2 id=&#34;self-driving-cars&#34;&gt;Self-Driving Cars&lt;/h2&gt;
&lt;p&gt;Suppose you were a &lt;strong&gt;ride-sharing platform&lt;/strong&gt; and you wanted to test the effect of self-driving cars in your fleet.&lt;/p&gt;
&lt;p&gt;As you can imagine, there are many &lt;strong&gt;limitations&lt;/strong&gt; to running an AB/test for this type of feature. First of all, it&amp;rsquo;s complicated to randomize individual rides. Second, it&amp;rsquo;s a very expensive intervention. Third, and statistically most important, you cannot run this intervention at the ride level. The problem is that there are &lt;strong&gt;spillover&lt;/strong&gt; effects from treated to control units: if indeed self-driving cars are more efficient, it means that they can serve more customers in the same amount of time, reducing the customers available to normal drivers (the control group). This spillover &lt;strong&gt;contaminates&lt;/strong&gt; the experiment and prevents a causal interpretation of the results.&lt;/p&gt;
&lt;p&gt;For all these reasons, we select only one city. Given the synthetic vibe of the article we cannot but select&amp;hellip; (&lt;em&gt;drum roll&lt;/em&gt;)&amp;hellip; Miami!&lt;/p&gt;
&lt;img src=&#34;fig/miami_skyline_text.jpg&#34; width=&#34;700px&#34;/&gt;
&lt;p&gt;I generate a simulated dataset in which we observe a panel of U.S. cities over time. The revenue data is made up, while the socio-economic variables are taken from the &lt;a href=&#34;https://stats.oecd.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;OECD database&lt;/a&gt;. I import the data generating process &lt;code&gt;dgp_selfdriving()&lt;/code&gt; from &lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/src/dgp.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;src.dgp&lt;/code&gt;&lt;/a&gt;. I also import some plotting functions and libraries from &lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/src/utils.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;src.utils&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;%matplotlib inline
%config InlineBackend.figure_format = &#39;retina&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from src.utils import *
from src.dgp import dgp_selfdriving

treatment_year = 2013
treated_city = &#39;Miami&#39;
df = dgp_selfdriving().generate_data(year=treatment_year, city=treated_city)
df.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;city&lt;/th&gt;
      &lt;th&gt;year&lt;/th&gt;
      &lt;th&gt;density&lt;/th&gt;
      &lt;th&gt;employment&lt;/th&gt;
      &lt;th&gt;gdp&lt;/th&gt;
      &lt;th&gt;population&lt;/th&gt;
      &lt;th&gt;treated&lt;/th&gt;
      &lt;th&gt;post&lt;/th&gt;
      &lt;th&gt;revenue&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;Atlanta&lt;/td&gt;
      &lt;td&gt;2003&lt;/td&gt;
      &lt;td&gt;290&lt;/td&gt;
      &lt;td&gt;0.629761&lt;/td&gt;
      &lt;td&gt;6.4523&lt;/td&gt;
      &lt;td&gt;4.267538&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;25.713947&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;Atlanta&lt;/td&gt;
      &lt;td&gt;2004&lt;/td&gt;
      &lt;td&gt;295&lt;/td&gt;
      &lt;td&gt;0.635595&lt;/td&gt;
      &lt;td&gt;6.5836&lt;/td&gt;
      &lt;td&gt;4.349712&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;23.852279&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;Atlanta&lt;/td&gt;
      &lt;td&gt;2005&lt;/td&gt;
      &lt;td&gt;302&lt;/td&gt;
      &lt;td&gt;0.645614&lt;/td&gt;
      &lt;td&gt;6.6998&lt;/td&gt;
      &lt;td&gt;4.455273&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;24.332397&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;Atlanta&lt;/td&gt;
      &lt;td&gt;2006&lt;/td&gt;
      &lt;td&gt;313&lt;/td&gt;
      &lt;td&gt;0.648573&lt;/td&gt;
      &lt;td&gt;6.5653&lt;/td&gt;
      &lt;td&gt;4.609096&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;23.816017&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;Atlanta&lt;/td&gt;
      &lt;td&gt;2007&lt;/td&gt;
      &lt;td&gt;321&lt;/td&gt;
      &lt;td&gt;0.650976&lt;/td&gt;
      &lt;td&gt;6.4184&lt;/td&gt;
      &lt;td&gt;4.737037&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;25.786902&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;We have information on the largest 46 U.S. cities for the period 2002-2019. The panel is &lt;strong&gt;balanced&lt;/strong&gt;, which means that we observe all cities for all time periods. Self-driving cars were introduced in 2013.&lt;/p&gt;
&lt;p&gt;Is the &lt;strong&gt;treated&lt;/strong&gt; unit, Miami, comparable to the rest of the sample? Let&amp;rsquo;s use the &lt;code&gt;create_table_one&lt;/code&gt; function from Uber&amp;rsquo;s &lt;a href=&#34;https://causalml.readthedocs.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;causalml&lt;/code&gt;&lt;/a&gt; package to produce a &lt;strong&gt;covariate balance table&lt;/strong&gt;, containing the average value of our observable characteristics, across treatment and control groups. As the name suggests, this should always be the first table you present in causal inference analysis.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from causalml.match import create_table_one

create_table_one(df, &#39;treated&#39;, [&#39;density&#39;, &#39;employment&#39;, &#39;gdp&#39;, &#39;population&#39;, &#39;revenue&#39;])
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;Control&lt;/th&gt;
      &lt;th&gt;Treatment&lt;/th&gt;
      &lt;th&gt;SMD&lt;/th&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;Variable&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;n&lt;/th&gt;
      &lt;td&gt;765&lt;/td&gt;
      &lt;td&gt;17&lt;/td&gt;
      &lt;td&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;density&lt;/th&gt;
      &lt;td&gt;256.63 (172.90)&lt;/td&gt;
      &lt;td&gt;364.94 (19.61)&lt;/td&gt;
      &lt;td&gt;0.8802&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;employment&lt;/th&gt;
      &lt;td&gt;0.63 (0.05)&lt;/td&gt;
      &lt;td&gt;0.60 (0.04)&lt;/td&gt;
      &lt;td&gt;-0.5266&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;gdp&lt;/th&gt;
      &lt;td&gt;6.07 (1.16)&lt;/td&gt;
      &lt;td&gt;5.12 (0.29)&lt;/td&gt;
      &lt;td&gt;-1.1124&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;population&lt;/th&gt;
      &lt;td&gt;3.53 (3.81)&lt;/td&gt;
      &lt;td&gt;5.85 (0.31)&lt;/td&gt;
      &lt;td&gt;0.861&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;revenue&lt;/th&gt;
      &lt;td&gt;25.25 (2.45)&lt;/td&gt;
      &lt;td&gt;23.86 (2.39)&lt;/td&gt;
      &lt;td&gt;-0.5737&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;As expected, the groups are &lt;strong&gt;not balanced&lt;/strong&gt;: Miami is more densely populated, poorer, larger and has lower employment rate than the other cities in the US in our sample.&lt;/p&gt;
&lt;p&gt;We are interested in understanding the impact of the introduction of &lt;strong&gt;self-driving cars&lt;/strong&gt; on &lt;code&gt;revenue&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;One initial idea could be to analyze the data as we would in an A/B test, comparing control and treatment group. We can estimate the treatment effect as a difference in means in &lt;code&gt;revenue&lt;/code&gt; between the treatment and control group, after the introduction of self-driving cars.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;smf.ols(&#39;revenue ~ treated&#39;, data=df[df[&#39;post&#39;]==True]).fit().summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
         &lt;td&gt;&lt;/td&gt;            &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt;       &lt;td&gt;   26.6006&lt;/td&gt; &lt;td&gt;    0.127&lt;/td&gt; &lt;td&gt;  210.061&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;   26.351&lt;/td&gt; &lt;td&gt;   26.850&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;treated[T.True]&lt;/th&gt; &lt;td&gt;   -0.7156&lt;/td&gt; &lt;td&gt;    0.859&lt;/td&gt; &lt;td&gt;   -0.833&lt;/td&gt; &lt;td&gt; 0.405&lt;/td&gt; &lt;td&gt;   -2.405&lt;/td&gt; &lt;td&gt;    0.974&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;The effect of self-driving cars seems to be negative but not significant.&lt;/p&gt;
&lt;p&gt;The main &lt;strong&gt;problem&lt;/strong&gt; here is that treatment was &lt;strong&gt;not randomly assigned&lt;/strong&gt;. We have a single treated unit, Miami, and it&amp;rsquo;s hardly comparable to other cities.&lt;/p&gt;
&lt;p&gt;One alternative procedure, is to compare revenue &lt;strong&gt;before and after&lt;/strong&gt; the treatment, within the city of Miami.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;smf.ols(&#39;revenue ~ post&#39;, data=df[df[&#39;city&#39;]==treated_city]).fit().summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
        &lt;td&gt;&lt;/td&gt;          &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt;    &lt;td&gt;   22.4485&lt;/td&gt; &lt;td&gt;    0.534&lt;/td&gt; &lt;td&gt;   42.044&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;   21.310&lt;/td&gt; &lt;td&gt;   23.587&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;post[T.True]&lt;/th&gt; &lt;td&gt;    3.4364&lt;/td&gt; &lt;td&gt;    0.832&lt;/td&gt; &lt;td&gt;    4.130&lt;/td&gt; &lt;td&gt; 0.001&lt;/td&gt; &lt;td&gt;    1.663&lt;/td&gt; &lt;td&gt;    5.210&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;The effect of self-driving cars seems to be positive and statistically significant.&lt;/p&gt;
&lt;p&gt;However, the &lt;strong&gt;problem&lt;/strong&gt; of this procedure is that there might have been many &lt;strong&gt;other things happening&lt;/strong&gt; after 2013. It&amp;rsquo;s quite a stretch to attribute all differences to self-driving cars.&lt;/p&gt;
&lt;p&gt;We can better understand this concern if we plot the time trend of revenue over cities. First, we need to reshape the data into a &lt;strong&gt;wide format&lt;/strong&gt;, with one column per city and one row per year.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df = df.pivot(index=&#39;year&#39;, columns=&#39;city&#39;, values=&#39;revenue&#39;).reset_index()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, let&amp;rsquo;s plot the revenue over time for Miami and for the other cities.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;cities = [c for c in df.columns if c!=&#39;year&#39;]
df[&#39;Other Cities&#39;] = df[[c for c in cities if c != treated_city]].mean(axis=1)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def plot_lines(df, line1, line2, year, hline=True):
    sns.lineplot(x=df[&#39;year&#39;], y=df[line1].values, label=line1)
    sns.lineplot(x=df[&#39;year&#39;], y=df[line2].values, label=line2)
    plt.axvline(x=year, ls=&amp;quot;:&amp;quot;, color=&#39;C2&#39;, label=&#39;Self-Driving Cars&#39;, zorder=1)
    plt.legend();
    plt.title(&amp;quot;Average revenue per day (in M$)&amp;quot;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Since we are talking about Miami, let&amp;rsquo;s use an appropriate color palette.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sns.set_palette(sns.color_palette([&#39;#f14db3&#39;, &#39;#0dc3e2&#39;, &#39;#443a84&#39;]))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plot_lines(df, treated_city, &#39;Other Cities&#39;, treatment_year)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/synth_22_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;As we can see, revenue seems to be increasing after the treatment in Miami. But it&amp;rsquo;s a very volatile time series. And revenue was increasing also in the rest of the country. It&amp;rsquo;s very hard from this plot to attribute the change to self-driving case.&lt;/p&gt;
&lt;p&gt;Can we do better?&lt;/p&gt;
&lt;h2 id=&#34;synthetic-control&#34;&gt;Synthetic Control&lt;/h2&gt;
&lt;p&gt;The answer is yes! Synthetic control allow us to do causal inference when we have &lt;strong&gt;as few as one treated unit&lt;/strong&gt; and &lt;strong&gt;many control units&lt;/strong&gt; and we observe them &lt;strong&gt;over time&lt;/strong&gt;. The idea is simple: combine untreated units so that they mimic the behavior of the treated unit as closely as possible, without the treatment. Then use this &amp;ldquo;synthetic unit&amp;rdquo; as a control. The method first introduced by &lt;a href=&#34;https://www.tandfonline.com/doi/abs/10.1198/jasa.2009.ap08746&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Abadie, Diamond and Hainmueller (2010)&lt;/a&gt; and has been called &lt;a href=&#34;https://www.aeaweb.org/articles?id=10.1257/jep.31.2.3&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;ldquo;the most important innovation in the policy evaluation literature in the last few years&amp;rdquo;&lt;/a&gt;. Moreover, it is widely used in the industry because of its simplicity and interpretability.&lt;/p&gt;
&lt;h3 id=&#34;setting&#34;&gt;Setting&lt;/h3&gt;
&lt;p&gt;We assume that for a panel of i.i.d. subjects $i = 1, &amp;hellip;, n$ over time $t=1, &amp;hellip;,T$ we observed a set of variables $(X_{it}, D_i, Y_{it})$ that includes&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;a treatment assignment $D_i \in \lbrace 0, 1 \rbrace$ (&lt;code&gt;treated&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;a response $Y_{i,t} \in \mathbb R$ (&lt;code&gt;revenue&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;a feature vector $X_{i,t} \in \mathbb R^n$ (&lt;code&gt;population&lt;/code&gt;, &lt;code&gt;density&lt;/code&gt;, &lt;code&gt;employment&lt;/code&gt; and &lt;code&gt;GDP&lt;/code&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Moreover, one unit (Miami in our case) is treated at time $t^*$ (2013 in our case). We distinguish time periods before treatment and time periods after treatment.&lt;/p&gt;
&lt;p&gt;Crucially, treatment $D_i$ is not randomly assigned, therefore a difference in means between the treated unit(s) and the control group is not an unbiased estimator of the average treatment effect.&lt;/p&gt;
&lt;h3 id=&#34;the-problem&#34;&gt;The Problem&lt;/h3&gt;
&lt;p&gt;The problem is that, as usual, we do not observe the counterfactual outcome for treated units, i.e. we do not know what would have happened to them, if they had not been treated. This is known as the &lt;strong&gt;fundamental problem of causal inference&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;The simplest approach, would be just to compare pre and post periods. This is called the &lt;strong&gt;event study&lt;/strong&gt; approach.&lt;/p&gt;
&lt;p&gt;However, we can do better than this. In fact, even though treatment was not randomly assigned, we still have access to some units that were not treated.&lt;/p&gt;
&lt;p&gt;For the outcome variable we observe the following values&lt;/p&gt;
&lt;p&gt;$$
Y =
\begin{bmatrix}
Y^{(1)} _ {t, post} \ &amp;amp; Y^{(0)} _ {c, post} \newline
Y^{(0)} _ {t, pre} \ &amp;amp; Y^{(0)} _ {c, pre}
\end{bmatrix}
$$&lt;/p&gt;
&lt;p&gt;Where $Y^{(d)} _ {a, t}$ is the outcome of an observation at time $t$, given treatment assignment $a$ and treatment status $d$. We basically have a &lt;strong&gt;missing data problem&lt;/strong&gt; since we do not observe $Y^{(0)} _ {t, post}$: what would have happened to treated units ($a=t$) without treatment ($d=0$).&lt;/p&gt;
&lt;h3 id=&#34;the-solution&#34;&gt;The Solution&lt;/h3&gt;
&lt;p&gt;Following &lt;a href=&#34;https://arxiv.org/pdf/1610.07748.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Doudchenko and Inbens (2018)&lt;/a&gt;, we can formulate an estimate of the counterfactual outcome for the treated unit as a linear combination of the observed outcomes for the control units.&lt;/p&gt;
&lt;p&gt;$$
\hat Y^{(0)} _ {t, post} = \alpha + \sum_{i \in c} \beta_{i} Y^{(0)} _ {i, post}
$$&lt;/p&gt;
&lt;p&gt;where&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the constant $\alpha$ allows for different averages between the two groups&lt;/li&gt;
&lt;li&gt;the weights $\beta_i$ are allowed to vary across control units $i$ (otherwise, it would be a difference-in-differences)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;How should we &lt;strong&gt;choose which weights&lt;/strong&gt; to use? We want our synthetic control to approximate the outcome as closely as possible, before the treatment. The first approach could be to define the weights as&lt;/p&gt;
&lt;p&gt;$$
\hat \beta = \arg \min_{\beta} || \boldsymbol X_{t, pre} - \boldsymbol \beta \boldsymbol X_{c, pre} || = \sqrt{ \sum_{p} \left( X_{t, p, pre}  - \sum_{i \in c} \beta_{p} X_{c, p, pre} \right)^2 }
$$&lt;/p&gt;
&lt;p&gt;I.e. the weights are such that they minimize the distance between observable characteristics of control units $X_c$ and the treated unit $X_t$ before the treatment.&lt;/p&gt;
&lt;p&gt;You might notice a very close similarity to &lt;strong&gt;linear regression&lt;/strong&gt;.  Indeed, we are doing something very similar.&lt;/p&gt;
&lt;p&gt;In linear regression, we usually have &lt;strong&gt;many units&lt;/strong&gt; (observations), &lt;strong&gt;few exogenous features&lt;/strong&gt; and &lt;strong&gt;one endogenous feature&lt;/strong&gt; and we try to express the endogenous feature as a linear combination of the endogenous features, for each unit.&lt;/p&gt;
&lt;img src=&#34;fig/synth1.png&#34; width=&#34;500px&#34;/&gt;
&lt;p&gt;With synthetic control, we instead have &lt;strong&gt;many time periods&lt;/strong&gt; (features), few &lt;strong&gt;control units&lt;/strong&gt; and a single &lt;strong&gt;treated unit&lt;/strong&gt; and we try to express the treated unit as a linear combination of the control units, for each time period.&lt;/p&gt;
&lt;p&gt;To perform the same operation, we essentially need to &lt;strong&gt;transpose the data&lt;/strong&gt;.&lt;/p&gt;
&lt;img src=&#34;fig/synth3.png&#34; width=&#34;500px&#34;/&gt;
&lt;p&gt;After the swap, we compute the &lt;strong&gt;synthetic control&lt;/strong&gt; weights, exactly as we would compute regression coefficients. However now one observation is a time period and one feature is a unit.&lt;/p&gt;
&lt;img src=&#34;fig/synth2.png&#34; width=&#34;500px&#34;/&gt;
&lt;p&gt;Notice that this swap is &lt;strong&gt;not innocent&lt;/strong&gt;. In linear regression we assume that the relationship between the exogenous features and the endogenous feature is the same &lt;strong&gt;across units&lt;/strong&gt;, instead in synthetic control we assume that the relationship between the treated units and the control unit is the same &lt;strong&gt;over time&lt;/strong&gt;.&lt;/p&gt;
&lt;h3 id=&#34;back-to-self-driving-cars&#34;&gt;Back to self-driving cars&lt;/h3&gt;
&lt;p&gt;Let&amp;rsquo;s go back to the data now! First, we write a &lt;code&gt;synth_predict&lt;/code&gt; function that takes as input a model that is trained on control cities and tries to predict the outcome of the treated city, Miami, before the introduction of self-driving cars.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def synth_predict(df, model, city, year):
    other_cities = [c for c in cities if c not in [&#39;year&#39;, city]]
    y = df.loc[df[&#39;year&#39;] &amp;lt;= year, city]
    X = df.loc[df[&#39;year&#39;] &amp;lt;= year, other_cities]
    df[f&#39;Synthetic {city}&#39;] = model.fit(X, y).predict(df[other_cities])
    return model
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s estimate the model via linear regression.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from sklearn.linear_model import LinearRegression

coef = synth_predict(df, LinearRegression(), treated_city, treatment_year).coef_
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;How well did we &lt;strong&gt;match&lt;/strong&gt; pre-self-driving cars &lt;code&gt;revenue&lt;/code&gt; in Miami? What is the implied &lt;strong&gt;effect&lt;/strong&gt; of self-driving cars?&lt;/p&gt;
&lt;p&gt;We can visually answer both questions by plotting the actual revenue in Miami against the predicted one.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plot_lines(df, treated_city, f&#39;Synthetic {treated_city}&#39;, treatment_year)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/synth_44_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;It looks like self-driving cars had a sensible &lt;strong&gt;positive effect&lt;/strong&gt; on &lt;code&gt;revenue&lt;/code&gt; in Miami: the predicted trend is lower than the actual data and diverges right after the introduction of self-driving cars.&lt;/p&gt;
&lt;p&gt;On the other hand, we are clearly &lt;strong&gt;overfitting&lt;/strong&gt;: the pre-treatment predicted &lt;code&gt;revenue&lt;/code&gt; line is perfectly overlapping with the actual data. Given the high variability of &lt;code&gt;revenue&lt;/code&gt; in Miami, this is suspicious, to say the least.&lt;/p&gt;
&lt;p&gt;Another problem concerns the &lt;strong&gt;weights&lt;/strong&gt;. Let&amp;rsquo;s plot them.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_states = pd.DataFrame({&#39;city&#39;: [c for c in cities if c!=treated_city], &#39;ols_coef&#39;: coef})
plt.figure(figsize=(10, 9))
sns.barplot(data=df_states, x=&#39;ols_coef&#39;, y=&#39;city&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/synth_46_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;We have many &lt;strong&gt;negative weights&lt;/strong&gt;, which do not make much sense from a causal inference perspective. I can understand that Miami can be expressed as a combination of 0.2 St. Louis, 0.15 Oklahoma and 0.15 Hartford. But what does it mean that Miami is -0.15 Milwaukee?&lt;/p&gt;
&lt;p&gt;Since we would like to interpret our synthetic control as a &lt;strong&gt;weighted average&lt;/strong&gt; of untreated states, all weights should be positive  and they should sum to one.&lt;/p&gt;
&lt;p&gt;To address both concerns (weighting and overfitting), we need to impose some &lt;strong&gt;restrictions on the weights&lt;/strong&gt;.&lt;/p&gt;
&lt;h2 id=&#34;extensions&#34;&gt;Extensions&lt;/h2&gt;
&lt;h3 id=&#34;weights&#34;&gt;Weights&lt;/h3&gt;
&lt;p&gt;To solve the problems of overweighting and negative weights, &lt;a href=&#34;https://www.tandfonline.com/doi/abs/10.1198/jasa.2009.ap08746&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Abadie, Diamond and Hainmueller (2010)&lt;/a&gt; propose the following weights:&lt;/p&gt;
&lt;p&gt;$$
\hat \beta = \arg \min_{\beta} || \boldsymbol X_t - \boldsymbol \beta \boldsymbol X_c || = \sqrt{ \sum_{p} \left( X_{t, p}  - \sum_{i \in c} \beta_{p} X_{c, p} \right)^2 }
\quad \text{s.t.} \quad \sum_{p} \beta_p = 1 \quad \text{and} \quad \beta_p \geq 0 \quad \forall p
$$&lt;/p&gt;
&lt;p&gt;Which means, a set of weights $\beta$ such that&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;weighted observable characteristics of the control group $X_c$, match the observable characteristics of the treatment group $X_t$, before the treatment&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;they sum to 1&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;and are not negative.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;With this approach we get an &lt;strong&gt;interpretable counterfactual&lt;/strong&gt; as a weighted avarage of untreated units.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s write now our own objective function. I create a new class &lt;code&gt;SyntheticControl()&lt;/code&gt; which has both a &lt;code&gt;loss&lt;/code&gt; function, as described above, a method to &lt;code&gt;fit&lt;/code&gt; it and &lt;code&gt;predict&lt;/code&gt; the values for the treated unit.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from toolz import partial
from scipy.optimize import fmin_slsqp

class SyntheticControl():
    
    # Loss function
    def loss(self, W, X, y) -&amp;gt; float:
        return np.sqrt(np.mean((y - X.dot(W))**2))

    # Fit model
    def fit(self, X, y):
        w_start = [1/X.shape[1]]*X.shape[1]
        self.coef_ = fmin_slsqp(partial(self.loss, X=X, y=y),
                         np.array(w_start),
                         f_eqcons=lambda x: np.sum(x) - 1,
                         bounds=[(0.0, 1.0)]*len(w_start),
                         disp=False)
        self.mse = self.loss(W=self.coef_, X=X, y=y)
        return self
    
    # Predict 
    def predict(self, X):
        return X.dot(self.coef_)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can now repeat the same procedure as before, but using the &lt;code&gt;SyntheticControl&lt;/code&gt; method instead of the simple, unconstrained &lt;code&gt;LinearRegression&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_states[&#39;coef_synth&#39;] = synth_predict(df, SyntheticControl(), treated_city, treatment_year).coef_
plot_lines(df, treated_city, f&#39;Synthetic {treated_city}&#39;, treatment_year)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/synth_53_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;As we can see, now we are &lt;strong&gt;not overfitting&lt;/strong&gt; anymore. The actual and predicted &lt;code&gt;revenue&lt;/code&gt; pre-treatment are close but not identical. The reason is that the non-negativity constraint is constraining most coefficients to be zero (as &lt;a href=&#34;https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Lasso&lt;/a&gt; does).&lt;/p&gt;
&lt;p&gt;It looks like the effect is again negative. However, let&amp;rsquo;s plot the &lt;strong&gt;difference&lt;/strong&gt; between the two lines to better visualize the magnitude.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def plot_difference(df, city, year, vline=True, hline=True, **kwargs):
    sns.lineplot(x=df[&#39;year&#39;], y=df[city] - df[f&#39;Synthetic {city}&#39;], **kwargs)
    if vline: 
        plt.axvline(x=year, ls=&amp;quot;:&amp;quot;, color=&#39;C2&#39;, label=&#39;Self-driving cars&#39;, lw=3, zorder=100)
        plt.legend()
    if hline: sns.lineplot(x=df[&#39;year&#39;], y=0, lw=3, color=&#39;k&#39;, zorder=1)
    plt.title(&amp;quot;Estimated effect of self-driving cars&amp;quot;);
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plot_difference(df, treated_city, treatment_year)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/synth_56_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;The difference is clearly positive and slightly increasing over time.&lt;/p&gt;
&lt;p&gt;We can also visualize the &lt;strong&gt;weights&lt;/strong&gt; to interpret the estimated counterfactual (what would have happened in Miami, without self-driving cars).&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plt.figure(figsize=(10, 9))
sns.barplot(data=df_states, x=&#39;coef_synth&#39;, y=&#39;city&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/synth_58_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;As we can see, now we are expressing &lt;code&gt;revenue&lt;/code&gt; in Miami as a linear combination of just a couple of cities: Tampa, St. Louis and, to a lower extent, Las Vegas. This makes the whole procedure very &lt;strong&gt;transparent&lt;/strong&gt;.&lt;/p&gt;
&lt;h3 id=&#34;inference&#34;&gt;Inference&lt;/h3&gt;
&lt;p&gt;What about &lt;strong&gt;inference&lt;/strong&gt;? Is the estimate significantly different from zero? Or, more practically, &amp;ldquo;&lt;em&gt;how unusual is this estimate under the null hypothesis of no policy effect&lt;/em&gt;?&amp;rdquo;.&lt;/p&gt;
&lt;p&gt;We are going to perform a &lt;a href=&#34;https://en.wikipedia.org/wiki/Permutation_test&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;randomization/permutation test&lt;/strong&gt;&lt;/a&gt; in order to answer this question. The &lt;strong&gt;idea&lt;/strong&gt; is that if the policy has no effect, the effect we observe for Miami should not be significantly different from the effect we observe for any other city.&lt;/p&gt;
&lt;p&gt;Therefore, we are going to replicate the procedure above, but for all other cities and compare them with the estimate for Miami.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from matplotlib.offsetbox import OffsetImage, AnnotationBbox

fig, ax = plt.subplots()
for city in cities:
    synth_predict(df, SyntheticControl(), city, treatment_year)
    plot_difference(df, city, treatment_year, vline=False, alpha=0.2, color=&#39;C1&#39;, lw=3)
plot_difference(df, treated_city, treatment_year)
ax.add_artist(AnnotationBbox(OffsetImage(plt.imread(&#39;fig/miami.png&#39;), zoom=0.25), (2015, 2.7), frameon=False));
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/synth_62_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;From the graph we notice two things. First, the effect for Miami is quite &lt;strong&gt;extreme&lt;/strong&gt; and therefore likely not to be driven by random noise.&lt;/p&gt;
&lt;p&gt;Second, we also notice that there are a couple of cities for which we cannot fit the pre-trend very well. In particular, there is a line that is sensibly lower than all others. This is expected since, for each city, we are building the counterfactual trend as a &lt;strong&gt;convex combination&lt;/strong&gt; of all other cities. Cities that are quite extreme in terms of &lt;code&gt;revenue&lt;/code&gt; are very useful to build the counterfactuals of other cities, but it&amp;rsquo;s &lt;strong&gt;hard to build a counterfactual for them&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Not to bias the analysis, let&amp;rsquo;s exclude states for which we cannot build a &amp;ldquo;good enough&amp;rdquo; counterfectual, in terms of pre-treatment MSE.&lt;/p&gt;
&lt;p&gt;$$
MSE_{pre} = \frac{1}{n} \sum_{t \in \text{pre}} \left( Y_t - \hat Y_t \right)^2
$$&lt;/p&gt;
&lt;p&gt;As a rule of thumb, &lt;a href=&#34;https://www.tandfonline.com/doi/abs/10.1198/jasa.2009.ap08746&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Abadie, Diamond and Hainmueller (2010)&lt;/a&gt; suggest to exclude units for which the prediction MSE is larger than twice the MSE of the treated unit.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Reference mse
mse_treated = synth_predict(df, SyntheticControl(), treated_city, treatment_year).mse

# Other mse
fig, ax = plt.subplots()
for city in cities:
    mse = synth_predict(df, SyntheticControl(), city, treatment_year).mse
    if mse &amp;lt; 2 * mse_treated:
        plot_difference(df, city, treatment_year, vline=False, alpha=0.2, color=&#39;C1&#39;, lw=3)
plot_difference(df, treated_city, treatment_year)
ax.add_artist(AnnotationBbox(OffsetImage(plt.imread(&#39;fig/miami.png&#39;), zoom=0.25), (2015, 2.7), frameon=False));
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/synth_64_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;After exluding extreme observations, it looks like the effect for Miami is very unusual.&lt;/p&gt;
&lt;p&gt;One &lt;strong&gt;statistic&lt;/strong&gt; that &lt;a href=&#34;https://www.tandfonline.com/doi/abs/10.1198/jasa.2009.ap08746&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Abadie, Diamond and Hainmueller (2010)&lt;/a&gt; suggest to perform a randomization test is the ratio between pre-treatment MSE and post-treatment MSE.&lt;/p&gt;
&lt;p&gt;$$
\lambda = \frac{MSE_{post}}{MSE_{pre}} = \frac{\frac{1}{n} \sum_{t \in \text{post}} \left( Y_t - \hat Y_t \right)^2 }{\frac{1}{n} \sum_{t \in \text{pre}} \left( Y_t - \hat Y_t \right)^2 }
$$&lt;/p&gt;
&lt;p&gt;We can compute a p-value as the number of observations with higher ratio.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;lambdas = {}
for city in cities:
    mse_pre = synth_predict(df, SyntheticControl(), city, treatment_year).mse
    mse_tot = np.mean((df[f&#39;Synthetic {city}&#39;] - df[city])**2)
    lambdas[city] = (mse_tot - mse_pre) / mse_pre
    
print(f&amp;quot;p-value: {np.mean(np.fromiter(lambdas.values(), dtype=&#39;float&#39;) &amp;gt; lambdas[treated_city]):.4}&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;p-value: 0.04348
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It seems that only $4.3%$ of the cities had a larger MSE ratio than Miami, implying a p-value of 0.043. We can visualize the distribution of the statistic under permutation with a histogram.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;fig, ax = plt.subplots()
_, bins, _ = plt.hist(lambdas.values(), bins=20, color=&amp;quot;C1&amp;quot;);
plt.hist([lambdas[treated_city]], bins=bins)
plt.title(&#39;Ratio of $MSE_{post}$ and $MSE_{pre}$ across cities&#39;);
ax.add_artist(AnnotationBbox(OffsetImage(plt.imread(&#39;fig/miami.png&#39;), zoom=0.25), (2.7, 1.7), frameon=False));
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/synth_68_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Indeed, the statistic for Miami is quite extreme.&lt;/p&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;In this article, we have explored a very popoular method for causal inference when we have &lt;strong&gt;few treated units&lt;/strong&gt;, but many time periods. This setting emerges often in industry settings when the treatment has to be assigned at the &lt;strong&gt;aggregate level&lt;/strong&gt; and randomization might not be possible. The key idea of synthetic control is to &lt;strong&gt;combinate control units&lt;/strong&gt; into one syntetic control unit to use as counterfactual to estimate the causal effect of the treatment.&lt;/p&gt;
&lt;p&gt;One of the main &lt;strong&gt;advantages&lt;/strong&gt; of synthetic control is that, as long as we use positive weights that are constrained to sum to one, the method &lt;strong&gt;avoids extrapolation&lt;/strong&gt;: we will never go out of the support of the data. Moreover, synthetic control studies can be &lt;strong&gt;&amp;ldquo;pre-registered&amp;rdquo;&lt;/strong&gt;: you can specify the weights before the study to avoid &lt;a href=&#34;https://en.wikipedia.org/wiki/Data_dredging&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;p-hacking&lt;/a&gt; and cherry picking. Another reason why this method is so popular in the industry is that weights make the counterfactual analysis &lt;strong&gt;explicit&lt;/strong&gt;: one can look at the weights and understand which comparison we are making.&lt;/p&gt;
&lt;p&gt;This method is relatively young and many &lt;strong&gt;extensions&lt;/strong&gt; are appearing every year. Some notable ones are the generalyzed synthetic control by &lt;a href=&#34;https://www.cambridge.org/core/journals/political-analysis/article/generalized-synthetic-control-method-causal-inference-with-interactive-fixed-effects-models/B63A8BD7C239DD4141C67DA10CD0E4F3&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Xu (2017)&lt;/a&gt;, the synthetic difference-in-differences by &lt;a href=&#34;https://arxiv.org/pdf/1610.07748.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Doudchenko and Imbens (2017)&lt;/a&gt;, the penalyzed synthetic control of &lt;a href=&#34;https://www.tandfonline.com/doi/full/10.1080/01621459.2021.1971535&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Abadie e L&amp;rsquo;Hour (2020)&lt;/a&gt; and the matrix completion methods of &lt;a href=&#34;https://www.tandfonline.com/doi/abs/10.1080/01621459.2021.1891924&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Athey et al. (2021)&lt;/a&gt;. Last but not least, if you want to have the method explained by one of its inventors, there is this great lecture by Alberto Abadie at the NBER Summer Institute freely available on Youtube.&lt;/p&gt;
&lt;br&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/T2p9Wg650bY&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;br&gt;
&lt;h3 id=&#34;references&#34;&gt;References&lt;/h3&gt;
&lt;p&gt;[1] A. Abadie, A. Diamond and J. Hainmueller, &lt;a href=&#34;https://www.tandfonline.com/doi/abs/10.1198/jasa.2009.ap08746&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Synthetic Control Methods for Comparative Case Studies: Estimating the Effect of California’s Tobacco Control Program&lt;/a&gt; (2010), &lt;em&gt;Journal of the American Statistical Association&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;[2] A. Abadie, &lt;a href=&#34;https://www.aeaweb.org/articles?id=10.1257/jel.20191450&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Using Synthetic Controls: Feasibility, Data Requirements, and Methodological Aspects&lt;/a&gt; (2021), &lt;em&gt;Journal of Economic Perspectives&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;[3] N. Doudchenko, G. Imbens, &lt;a href=&#34;https://arxiv.org/pdf/1610.07748.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Balancing, Regression, Difference-In-Differences and Synthetic Control Methods: A Synthesis&lt;/a&gt; (2017), &lt;em&gt;working paper&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;[4] Y. Xu, &lt;a href=&#34;https://www.cambridge.org/core/journals/political-analysis/article/generalized-synthetic-control-method-causal-inference-with-interactive-fixed-effects-models/B63A8BD7C239DD4141C67DA10CD0E4F3&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Generalized Synthetic Control Method: Causal Inference with Interactive Fixed Effects Models&lt;/a&gt; (2018), &lt;em&gt;Political Analysis&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;[5] A. Abadie, J. L&amp;rsquo;Hour, &lt;a href=&#34;https://www.tandfonline.com/doi/full/10.1080/01621459.2021.1971535&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;A Penalized Synthetic Control Estimator for Disaggregated Data&lt;/a&gt; (2020), &lt;em&gt;Journal of the American Statistical Association&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;[6] S. Athey, M. Bayati, N. Doudchenko, G. Imbens, K. Khosravi, &lt;a href=&#34;https://www.tandfonline.com/doi/abs/10.1080/01621459.2021.1891924&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Matrix Completion Methods for Causal Panel Data Models&lt;/a&gt; (2021), &lt;em&gt;Journal of the American Statistical Association&lt;/em&gt;.&lt;/p&gt;
&lt;h3 id=&#34;related-articles&#34;&gt;Related Articles&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/b63dc69e3d8c&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;DAGs and Control Variables&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/99bf5cffa0d9&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Matching, Weighting, or Regression?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/8a9c1e340832&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Understanding Meta Learners&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;code&#34;&gt;Code&lt;/h3&gt;
&lt;p&gt;You can find the original Jupyter Notebook here:&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/synth.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/synth.ipynb&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Weighting, Matching, or Regression?</title>
      <link>https://matteocourthoud.github.io/post/ipw/</link>
      <pubDate>Mon, 18 Jul 2022 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/post/ipw/</guid>
      <description>&lt;p&gt;&lt;em&gt;Understanding and comparing different methods for conditional causal inference analysis&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;AB tests or randomized controlled trials are the &lt;strong&gt;gold standard&lt;/strong&gt; in causal inference. By randomly exposing units to a treatment we make sure that individuals in both groups are comparable, on average, and any difference we observe can be attributed to the treatment effect alone.&lt;/p&gt;
&lt;p&gt;However, often the treatment and control groups are &lt;strong&gt;not perfectly comparable&lt;/strong&gt;. This could be due to the fact that randomization was not perfect or available. Not always we can randomize a treatment, for ethical or practical reasons. And even when we can, sometimes we do not have enough individuals or units so that differences between groups are seizable. This happens often, for example, when randomization is not done at the individual level, but at a higher level of aggregation, for example zipcodes, counties or even states.&lt;/p&gt;
&lt;p&gt;In these settings, we can still recover a causal estimate of the treatment effect if we have &lt;strong&gt;enough information&lt;/strong&gt; about individuals, by making the treatment and control group comparable, ex-post. In this blog post, we are going to introduce and compare different procedures to estimate causal effects in presence of imbalances between treatment and control groups that are &lt;strong&gt;fully observable&lt;/strong&gt;. In particular we are going to analyze weighting, matching and regression procedures.&lt;/p&gt;
&lt;h2 id=&#34;example&#34;&gt;Example&lt;/h2&gt;
&lt;p&gt;Assume we had blog on statistics and causal inference 😇. To improve user experience, we are considering &lt;strong&gt;releasing a dark mode&lt;/strong&gt;, and we would like to understand whether this new feature increases the time users spend on our blog.&lt;/p&gt;
&lt;img src=&#34;fig/modes.png&#34; width=&#34;600px&#34;/&gt;
&lt;p&gt;We are not a sophisticated company, therefore we do not run an AB test but we simply release the dark mode and we observe whether users select it or not and the time they spend on the blog. We know that there might be &lt;strong&gt;selection&lt;/strong&gt;:  users that prefer the dark mode could have different reading preferences and this might complicate our causal analysis.&lt;/p&gt;
&lt;p&gt;We can represent the data generating process with the following &lt;a href=&#34;https://towardsdatascience.com/b63dc69e3d8c&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Directed Acyclic Graph (DAG)&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-mermaid&#34;&gt;flowchart TB
classDef included fill:#DCDCDC,stroke:#000000,stroke-width:2px;
classDef excluded fill:#ffffff,stroke:#000000,stroke-width:2px;
classDef unobserved fill:#ffffff,stroke:#000000,stroke-width:2px,stroke-dasharray: 5 5;

X1((gender))
X2((age))
X3((hours))
D((dark mode))
Y((read time))

D --&amp;gt; Y
X1 --&amp;gt; Y
X1 --&amp;gt; D
X2 --&amp;gt; D
X3 --&amp;gt; Y

class D,Y included;
class X1,X2,X3 excluded;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We generate the simulated data using the data generating process &lt;code&gt;dgp_darkmode()&lt;/code&gt; from &lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/src/dgp.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;src.dgp&lt;/code&gt;&lt;/a&gt;. I also import some plotting functions and libraries from &lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/src/utils.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;src.utils&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;%matplotlib inline
%config InlineBackend.figure_format = &#39;retina&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from src.utils import *
from src.dgp import dgp_darkmode
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df = dgp_darkmode().generate_data()
df.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;read_time&lt;/th&gt;
      &lt;th&gt;dark_mode&lt;/th&gt;
      &lt;th&gt;male&lt;/th&gt;
      &lt;th&gt;age&lt;/th&gt;
      &lt;th&gt;hours&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;14.4&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;43.0&lt;/td&gt;
      &lt;td&gt;65.6&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;15.4&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;55.0&lt;/td&gt;
      &lt;td&gt;125.4&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;20.9&lt;/td&gt;
      &lt;td&gt;True&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;23.0&lt;/td&gt;
      &lt;td&gt;642.6&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;20.0&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;41.0&lt;/td&gt;
      &lt;td&gt;129.1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;21.5&lt;/td&gt;
      &lt;td&gt;True&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;29.0&lt;/td&gt;
      &lt;td&gt;190.2&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;We have informations on 300 users for whom we observe whether they select the &lt;code&gt;dark_mode&lt;/code&gt; (the treatment), their weekly &lt;code&gt;read_time&lt;/code&gt; (the outcome of interest) and some characteristics like &lt;code&gt;gender&lt;/code&gt;, &lt;code&gt;age&lt;/code&gt; and total &lt;code&gt;hours&lt;/code&gt; previously spend on the blog.&lt;/p&gt;
&lt;p&gt;We would like to estimate the effect of the new &lt;code&gt;dark_mode&lt;/code&gt; on users&amp;rsquo; &lt;code&gt;read_time&lt;/code&gt;. If we were runnig an &lt;a href=&#34;https://de.wikipedia.org/wiki/A/B-Test&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;AB test&lt;/strong&gt;&lt;/a&gt; or randomized control trial, we could just compare users with and without the dark mode and we could attribute the difference in average reading time to the &lt;code&gt;dark_mode&lt;/code&gt;. Let&amp;rsquo;s check what number we would get.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;np.mean(df.loc[df.dark_mode==True, &#39;read_time&#39;]) - np.mean(df.loc[df.dark_mode==False, &#39;read_time&#39;])
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;-0.4446330948042103
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Individuals that select the &lt;code&gt;dark_mode&lt;/code&gt; spend on average 1.37 hours less on the blog, per week. Should we conclude that &lt;code&gt;dark_mode&lt;/code&gt; is a &lt;strong&gt;bad idea&lt;/strong&gt;? Is this a causal effect?&lt;/p&gt;
&lt;p&gt;We did not randomize the &lt;code&gt;dark_mode&lt;/code&gt; so that users that selected it might not be directly &lt;strong&gt;comparable&lt;/strong&gt; with users that didn&amp;rsquo;t. Can we verify this concern? Partially. We can only check characteristics that we observe, &lt;code&gt;gender&lt;/code&gt;, &lt;code&gt;age&lt;/code&gt; and total &lt;code&gt;hours&lt;/code&gt; in our setting. We cannot check if users differ along other dimensions that we don&amp;rsquo;t observe.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s use the &lt;code&gt;create_table_one&lt;/code&gt; function from Uber&amp;rsquo;s &lt;a href=&#34;https://causalml.readthedocs.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;causalml&lt;/code&gt;&lt;/a&gt; package to produce a &lt;strong&gt;covariate balance table&lt;/strong&gt;, containing the average value of our observable characteristics, across treatment and control groups. As the name suggests, this should always be the first table you present in causal inference analysis.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from causalml.match import create_table_one

X = [&#39;male&#39;, &#39;age&#39;, &#39;hours&#39;]
table1 = create_table_one(df, &#39;dark_mode&#39;, X)
table1
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;Control&lt;/th&gt;
      &lt;th&gt;Treatment&lt;/th&gt;
      &lt;th&gt;SMD&lt;/th&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;Variable&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;n&lt;/th&gt;
      &lt;td&gt;151&lt;/td&gt;
      &lt;td&gt;149&lt;/td&gt;
      &lt;td&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;age&lt;/th&gt;
      &lt;td&gt;46.01 (9.79)&lt;/td&gt;
      &lt;td&gt;39.09 (11.53)&lt;/td&gt;
      &lt;td&gt;-0.6469&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;hours&lt;/th&gt;
      &lt;td&gt;337.78 (464.00)&lt;/td&gt;
      &lt;td&gt;328.57 (442.12)&lt;/td&gt;
      &lt;td&gt;-0.0203&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;male&lt;/th&gt;
      &lt;td&gt;0.34 (0.47)&lt;/td&gt;
      &lt;td&gt;0.66 (0.48)&lt;/td&gt;
      &lt;td&gt;0.6732&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;There seems to be &lt;strong&gt;some difference&lt;/strong&gt; between treatment (&lt;code&gt;dark_mode&lt;/code&gt;) and control group. In particular, users that select the &lt;code&gt;dark_mode&lt;/code&gt; are older, have spent less hours on the blog and they are more likely to be males.&lt;/p&gt;
&lt;p&gt;Another way to visually observe all the differences at once is with a &lt;strong&gt;paired violinplot&lt;/strong&gt;. The advantage of the paired violinplot is that it allows us to observe the full distribution of the variable (approximated via &lt;a href=&#34;https://en.wikipedia.org/wiki/Kernel_density_estimation&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;kernel density estimation&lt;/a&gt;).&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def plot_distributions(df, X, d):
    df_long = df.copy()[X + [d]]
    df_long[X] =(df_long[X] - df_long[X].mean()) / df_long[X].std()
    df_long = pd.melt(df_long, id_vars=d, value_name=&#39;value&#39;)
    sns.violinplot(y=&amp;quot;variable&amp;quot;, x=&amp;quot;value&amp;quot;, hue=d, data=df_long, split=True).\
        set(xlabel=&amp;quot;&amp;quot;, ylabel=&amp;quot;&amp;quot;, title=&amp;quot;Normalized Variable Distribution&amp;quot;);
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plot_distributions(df, X, &amp;quot;dark_mode&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/ipw_16_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;The insight of the violinplot is very similar: it seems that users that select the &lt;code&gt;dark_mode&lt;/code&gt; are different from users that don&amp;rsquo;t.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Why do we care?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;If we do not control for the observable characteristics, we are unable to estimate the true treatment effect. In short, we cannot be certain that the difference in outcome, &lt;code&gt;read_time&lt;/code&gt;, can be attributed to the treatment, &lt;code&gt;dark_mode&lt;/code&gt;, instead of other characteristics. For example, it could be that males read less and also prefer the &lt;code&gt;dark_mode&lt;/code&gt;, therefore we observe a negative correlation even though &lt;code&gt;dark_mode&lt;/code&gt; has no effect on &lt;code&gt;read_time&lt;/code&gt; (or even positive).&lt;/p&gt;
&lt;p&gt;In terms of Dyrected Acyclic Graphs, this means that we have several &lt;a href=&#34;https://towardsdatascience.com/b63dc69e3d8c&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;backdoor paths&lt;/strong&gt;&lt;/a&gt; that we need to &lt;strong&gt;block&lt;/strong&gt; in order for our analysis to be &lt;strong&gt;causal&lt;/strong&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-mermaid&#34;&gt;flowchart TB
classDef included fill:#DCDCDC,stroke:#000000,stroke-width:2px;
classDef excluded fill:#ffffff,stroke:#000000,stroke-width:2px;
classDef unobserved fill:#ffffff,stroke:#000000,stroke-width:2px,stroke-dasharray: 5 5;

X1((gender))
X2((age))
X3((hours))
D((dark mode))
Y((read time))

D --&amp;gt; Y
X1 --&amp;gt; Y
X1 --&amp;gt; D
X2 --&amp;gt; D
X3 --&amp;gt; Y

linkStyle 0 stroke:#00ff00,stroke-width:4px;
linkStyle 1,2 stroke:#ff0000,stroke-width:4px;
class D,Y included;
class X1,X2,X3 excluded;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;How do we &lt;strong&gt;block backdoor paths&lt;/strong&gt;? By conditioning the analysis on those intermediate variables. The conditional analysis allows us to recover the average treatment effect of the &lt;code&gt;dark_mode&lt;/code&gt; on &lt;code&gt;read_time&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-mermaid&#34;&gt;flowchart TB
classDef included fill:#DCDCDC,stroke:#000000,stroke-width:2px;
classDef excluded fill:#ffffff,stroke:#000000,stroke-width:2px;
classDef unobserved fill:#ffffff,stroke:#000000,stroke-width:2px,stroke-dasharray: 5 5;

X1((gender))
X2((age))
X3((hours))
D((dark mode))
Y((read time))

D --&amp;gt; Y
X1 -.-&amp;gt; Y
X1 -.-&amp;gt; D
X2 --&amp;gt; D
X3 --&amp;gt; Y

linkStyle 0 stroke:#00ff00,stroke-width:4px;
class D,Y,X1,X2,X3 included;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;How do we &lt;strong&gt;condition the analysis&lt;/strong&gt; on &lt;code&gt;gender&lt;/code&gt;, &lt;code&gt;age&lt;/code&gt; and &lt;code&gt;hours&lt;/code&gt;? We have some options:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Matching&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Propensity score&lt;/strong&gt; weighting&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Regression&lt;/strong&gt; with control variables&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Let&amp;rsquo;s explore and compare them!&lt;/p&gt;
&lt;h2 id=&#34;conditional-analysis&#34;&gt;Conditional Analysis&lt;/h2&gt;
&lt;p&gt;We assume that for a set of subjects $i = 1, &amp;hellip;, n$ we observed a tuple $(D_i, Y_i, X_i)$ comprised of&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;a treatment assignment $D_i \in \lbrace 0, 1 \rbrace$ (&lt;code&gt;dark_mode&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;a response $Y_i \in \mathbb R$ (&lt;code&gt;read_time&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;a feature vector $X_i \in \mathbb R^n$ (&lt;code&gt;gender&lt;/code&gt;, &lt;code&gt;age&lt;/code&gt; and &lt;code&gt;hours&lt;/code&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Assumption 1 : unconfoundedness&lt;/strong&gt; (or ignorability, or selection on observables)&lt;/p&gt;
&lt;p&gt;$$
\big \lbrace Y_i^{(1)} , Y_i^{(0)} \big \rbrace \ \perp \ D_i \ | \ X_i
$$&lt;/p&gt;
&lt;p&gt;i.e. conditional on observable characteristics $X$, the treatment assignment $D$ is as good as random. What we are effectively assuming is that there is no other characteristics that we do not observe that could impact both whether a user selects the &lt;code&gt;dark_mode&lt;/code&gt; and their &lt;code&gt;read_time&lt;/code&gt;. This is a &lt;strong&gt;strong assumption&lt;/strong&gt; that is more likely to be satisfied the more individual characteristics we observe.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Assumption 2: overlap&lt;/strong&gt; (or common support)&lt;/p&gt;
&lt;p&gt;$$
\exists \eta &amp;gt; 0 \ : \ \eta \leq \mathbb E \left[ T_i = 1 \ \big | \ X_i = x \right] \leq 1-\eta
$$&lt;/p&gt;
&lt;p&gt;i.e. no observation is deterministically assigned to the treatment or control group. This is a more technical assumption that basically means that for any level of &lt;code&gt;gender&lt;/code&gt;, &lt;code&gt;age&lt;/code&gt; or &lt;code&gt;hours&lt;/code&gt;, there could exist an individual that select the &lt;code&gt;dark_mode&lt;/code&gt; and one that doesn&amp;rsquo;t. Differently from the unconfoundedness assumption, the overal assumption is &lt;strong&gt;testable&lt;/strong&gt;.&lt;/p&gt;
&lt;h3 id=&#34;matching&#34;&gt;Matching&lt;/h3&gt;
&lt;p&gt;The first and most intuitive method to perform conditional analysis is &lt;strong&gt;matching&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;idea&lt;/strong&gt; of matching is very simple. Since we are not sure whether, for example, male and female users are directly comparable, we do the analysis within gender. Instead of comparing &lt;code&gt;read_time&lt;/code&gt; across &lt;code&gt;dark_mode&lt;/code&gt; in the whole sample, we do it separately for male and female users.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_gender = pd.pivot_table(df, values=&#39;read_time&#39;, index=&#39;male&#39;, columns=&#39;dark_mode&#39;, aggfunc=np.mean)
df_gender[&#39;diff&#39;] = df_gender[1] - df_gender[0] 
df_gender
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;dark_mode&lt;/th&gt;
      &lt;th&gt;False&lt;/th&gt;
      &lt;th&gt;True&lt;/th&gt;
      &lt;th&gt;diff&lt;/th&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;male&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;20.318000&lt;/td&gt;
      &lt;td&gt;22.24902&lt;/td&gt;
      &lt;td&gt;1.931020&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;16.933333&lt;/td&gt;
      &lt;td&gt;16.89898&lt;/td&gt;
      &lt;td&gt;-0.034354&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;Now the effect of &lt;code&gt;dark_mode&lt;/code&gt; seems reversed: it is negative for male users (-0.79) but bigger and positive for female users (+1.38), suggesting a positive aggregate effect, 1.38 - 0.79 = 0.59 (assuming equal proportion of genders)! This sign reversal is a very classical example of the &lt;a href=&#34;https://en.wikipedia.org/wiki/Simpson%27s_paradox&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Simpson&amp;rsquo;s Paradox&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;This comparison was easy to perform for &lt;code&gt;gender&lt;/code&gt;, since it is a binary variable. With multiple variables, potentially continuous, matching becomes much more difficult. One common strategy is to &lt;strong&gt;match users&lt;/strong&gt; in the treatment group with the most similar user in the control group, using some sort of &lt;a href=&#34;https://en.wikipedia.org/wiki/Nearest_neighbour_algorithm&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;nearest neighbor algorithm&lt;/a&gt;. I won&amp;rsquo;t go into the algorithm details here, but we can perform the matching with the &lt;code&gt;NearestNeighborMatch&lt;/code&gt; function from the &lt;code&gt;causalml&lt;/code&gt; package.&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;NearestNeighborMatch&lt;/code&gt; function generates a new dataset where users in the treatment group have been matched 1:1 (option &lt;code&gt;ratio=1&lt;/code&gt;) to users in the control group.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from causalml.match import NearestNeighborMatch

psm = NearestNeighborMatch(replace=True, ratio=1, random_state=1)
df_matched = psm.match(data=df, treatment_col=&amp;quot;dark_mode&amp;quot;, score_cols=X)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Are the two groups more comparable now? We can produce a new version of the &lt;strong&gt;balance table&lt;/strong&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;table1_matched = create_table_one(df_matched, &amp;quot;dark_mode&amp;quot;, X)
table1_matched
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;Control&lt;/th&gt;
      &lt;th&gt;Treatment&lt;/th&gt;
      &lt;th&gt;SMD&lt;/th&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;Variable&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;n&lt;/th&gt;
      &lt;td&gt;104&lt;/td&gt;
      &lt;td&gt;104&lt;/td&gt;
      &lt;td&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;age&lt;/th&gt;
      &lt;td&gt;41.93 (10.05)&lt;/td&gt;
      &lt;td&gt;41.85 (10.02)&lt;/td&gt;
      &lt;td&gt;-0.0086&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;hours&lt;/th&gt;
      &lt;td&gt;206.92 (309.62)&lt;/td&gt;
      &lt;td&gt;209.48 (321.79)&lt;/td&gt;
      &lt;td&gt;0.0081&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;male&lt;/th&gt;
      &lt;td&gt;0.62 (0.49)&lt;/td&gt;
      &lt;td&gt;0.62 (0.49)&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;Now the average differences between the two groups have &lt;strong&gt;shrunk&lt;/strong&gt; by at least a couple of orders of magnitude. However, note how the sample size has slightly decreased (300 $\to$ 246) since (1) we only match treated users and (2) we are not able to find a good match for all of them.&lt;/p&gt;
&lt;p&gt;We can visually inspect distributional differences with the paired violinplot.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plot_distributions(df_matched, X, &amp;quot;dark_mode&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/ipw_28_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;A popular way to visualize pre- and post-matching covariate balance is the &lt;strong&gt;balance plot&lt;/strong&gt; that essentially displays the standardized mean differences before and after matching, for each control variable.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def plot_balance(t1, t2, X):
    df_smd = pd.DataFrame({&amp;quot;Variable&amp;quot;: X + X,
                           &amp;quot;Sample&amp;quot;: [&amp;quot;Unadjusted&amp;quot; for _ in range(len(X))] + [&amp;quot;Adjusted&amp;quot; for _ in range(len(X))],
                           &amp;quot;Standardized Mean Difference&amp;quot;: t1[&amp;quot;SMD&amp;quot;][1:].to_list() + 
                                                           t2[&amp;quot;SMD&amp;quot;][1:].to_list()})

    sns.scatterplot(x=&amp;quot;Standardized Mean Difference&amp;quot;, y=&amp;quot;Variable&amp;quot;, hue=&amp;quot;Sample&amp;quot;, data=df_smd).\
        set(title=&amp;quot;Balance Plot&amp;quot;)
    plt.axvline(x=0, color=&#39;k&#39;, ls=&#39;--&#39;, zorder=-1, alpha=0.3);
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plot_balance(table1, table1_matched, X)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/ipw_31_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;As we can see, now all differences in observable characteristics between the two groups are essentially zero. We could also compare the distributions using other metrics or test statistics, such as the &lt;a href=&#34;https://towardsdatascience.com/9b06ee4d30bf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Kolmogorov-Smirnov test statistic&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;How do we &lt;strong&gt;estimate the average treatment effect&lt;/strong&gt;? We can simply do a difference in means. An equivalent way that automatically provides standard errors is to run a linear regression of the outcome, &lt;code&gt;read_time&lt;/code&gt;, on the treatment, &lt;code&gt;dark_mode&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt; that, since we have performed the matching for each treated user, the treatment effect we are estimating is the &lt;strong&gt;average treatment effect on the treated (ATT)&lt;/strong&gt;, which can be different from the average treatment effect if the treated sample differs from the overall population (which is likely to be the case, since we are doing matching in the first place).&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;smf.ols(&amp;quot;read_time ~ dark_mode&amp;quot;, data=df_matched).fit().summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
          &lt;td&gt;&lt;/td&gt;             &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt;         &lt;td&gt;   17.0365&lt;/td&gt; &lt;td&gt;    0.469&lt;/td&gt; &lt;td&gt;   36.363&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;   16.113&lt;/td&gt; &lt;td&gt;   17.960&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;dark_mode[T.True]&lt;/th&gt; &lt;td&gt;    1.4490&lt;/td&gt; &lt;td&gt;    0.663&lt;/td&gt; &lt;td&gt;    2.187&lt;/td&gt; &lt;td&gt; 0.030&lt;/td&gt; &lt;td&gt;    0.143&lt;/td&gt; &lt;td&gt;    2.755&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;The effect is now positive, but not statistically significant.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt; that we might have matched multiple treated users with the same untreated user, violating the independence assumption across observations and, in turn, distorting inference.&lt;/p&gt;
&lt;p&gt;We have two solutions:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;cluster standard errors at the matched individual level&lt;/li&gt;
&lt;li&gt;compute standard errors via bootstrap&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;We implement the first and cluster the standard errors by the original individual identifiers (the dataframe index).&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;smf.ols(&amp;quot;read_time ~ dark_mode&amp;quot;, data=df_matched)\
    .fit(cov_type=&#39;cluster&#39;, cov_kwds={&#39;groups&#39;: df_matched.index})\
    .summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
          &lt;td&gt;&lt;/td&gt;             &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;z&lt;/th&gt;      &lt;th&gt;P&gt;|z|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt;         &lt;td&gt;   17.0365&lt;/td&gt; &lt;td&gt;    0.650&lt;/td&gt; &lt;td&gt;   26.217&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;   15.763&lt;/td&gt; &lt;td&gt;   18.310&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;dark_mode[T.True]&lt;/th&gt; &lt;td&gt;    1.4490&lt;/td&gt; &lt;td&gt;    0.821&lt;/td&gt; &lt;td&gt;    1.765&lt;/td&gt; &lt;td&gt; 0.078&lt;/td&gt; &lt;td&gt;   -0.160&lt;/td&gt; &lt;td&gt;    3.058&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;The effect is even less statistically significant.&lt;/p&gt;
&lt;h3 id=&#34;propensity-score&#34;&gt;Propensity Score&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://academic.oup.com/biomet/article/70/1/41/240879&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Rosenbaum and Rubin (1983)&lt;/a&gt; proved a very powerful result: if the &lt;strong&gt;strong ignorability assumption&lt;/strong&gt; holds, it is sufficient to condition the analysis on the probability ot treatment, the &lt;strong&gt;propensity score&lt;/strong&gt;, in order to have conditional independence.&lt;/p&gt;
&lt;p&gt;$$
\big \lbrace Y_i^{(1)} , Y_i^{(0)} \big \rbrace \ \perp \ D_i \ | \ X_i \quad \leftrightarrow \quad \big \lbrace Y_i^{(1)} , Y_i^{(0)} \big \rbrace \ \perp \ D_i \ | \ e(X_i)
$$&lt;/p&gt;
&lt;p&gt;Where $e(X_i)$ is the probability of treatment of individual $i$, given the observable characteristics $X_i$.&lt;/p&gt;
&lt;p&gt;$$
e(x) = \Pr \left( D_i = 1 \ \big | \ X_i = x \right)
$$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt; that in an AB test the propensity score is constant across individuals.&lt;/p&gt;
&lt;p&gt;The result from Rosenbaum and Rubin (1983) is incredibly &lt;strong&gt;powerful and practical&lt;/strong&gt;, since the propensity score is a &lt;strong&gt;one dimensional&lt;/strong&gt; variable, while $X$ might be very high dimensional.&lt;/p&gt;
&lt;p&gt;Under the &lt;strong&gt;unconfoundedness&lt;/strong&gt; assumption introduced above, we can rewrite the average treatment effect as&lt;/p&gt;
&lt;p&gt;$$
\tau(x) = \mathbb E \left[ Y^{(1)} - Y^{(0)} \ \big| \ X = x \right] = \mathbb E \left[ \frac{D_i Y_i}{e(X_i)} - \frac{(1-D_i) Y_i}{1-e(X_i)} \right]
$$&lt;/p&gt;
&lt;p&gt;Note that this formulation of the average treatment effect does not depend on the potential outcomes $Y_i^{(1)}$ and $Y_i^{(0)}$, but only on the observed outcomes $Y_i$.&lt;/p&gt;
&lt;p&gt;This formulation of the average treatment effect implies the &lt;strong&gt;Inverse Propensity Weighted (IPW)&lt;/strong&gt; estimator which is an unbiased estimator for the average treatment effect $\tau$.&lt;/p&gt;
&lt;p&gt;$$
\hat \tau^{IPW} = \frac{1}{n} \sum _ {i=1}^{n} \left( \frac{D_i Y_i}{e(X_i)} - \frac{(1-D_i) Y_i}{1-e(X_i)} \right)
$$&lt;/p&gt;
&lt;p&gt;This estimator is &lt;strong&gt;unfeasible&lt;/strong&gt; since we do not observe the propensity scores $e(X_i)$. However, we can estimate them. Actually, &lt;a href=&#34;https://onlinelibrary.wiley.com/doi/abs/10.1111/1468-0262.00442&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Imbens, Hirano, Ridder (2003)&lt;/a&gt; show that you &lt;strong&gt;should&lt;/strong&gt; use the estimated propensity scores even if you knew the true values (for example because you know the sampling procedure). The idea is that if the estimated propensity scores are different from the true ones, this can be informative in the estimation.&lt;/p&gt;
&lt;p&gt;There are several possible ways to estimate a probability, the simplest and most common one being &lt;a href=&#34;https://en.wikipedia.org/wiki/Logistic_regression&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;logistic regression&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from sklearn.linear_model import LogisticRegressionCV

df[&amp;quot;pscore&amp;quot;] = LogisticRegressionCV().fit(y=df[&amp;quot;dark_mode&amp;quot;], X=df[X]).predict_proba(df[X])[:,1]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It is best practice, whenever we fit a prediction model, to &lt;strong&gt;fit the model on a different sample&lt;/strong&gt; with respect to the one that we use for inference. This practice is usually called &lt;strong&gt;cross-validation&lt;/strong&gt; or cross-fitting. One of the best (but computationally expensive) cross-validation procedures is &lt;strong&gt;leave-one-out (LOO)&lt;/strong&gt; cross-fitting: when predicting the value of observation $i$ we use all observations except for $i$. We implement the LOO cross-fitting procedure using the &lt;code&gt;cross_val_predict&lt;/code&gt; and &lt;code&gt;LeaveOneOut&lt;/code&gt; functions from the &lt;a href=&#34;https://scikit-learn.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;sklearn&lt;/code&gt;&lt;/a&gt; package.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from sklearn.model_selection import cross_val_predict, LeaveOneOut

df[&#39;pscore&#39;] = cross_val_predict(estimator=LogisticRegressionCV(), 
                                 X=df[X], 
                                 y=df[&amp;quot;dark_mode&amp;quot;],
                                 cv=LeaveOneOut(),
                                 method=&#39;predict_proba&#39;,
                                 n_jobs=-1)[:,1]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;An &lt;strong&gt;important check&lt;/strong&gt; to perform after estimating propensity scores is plotting them, across the treatment and control groups. First of all, we can then observe whether the two groups are balanced or not, depending on how close the two distributions are. Moreover, we can also check how likely it is that the &lt;strong&gt;overlap assumption&lt;/strong&gt; is satisfied. Ideally both distributions should span the same interval.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sns.histplot(data=df, x=&#39;pscore&#39;, hue=&#39;dark_mode&#39;, bins=30, stat=&#39;density&#39;, common_norm=False).\
    set(ylabel=&amp;quot;&amp;quot;, title=&amp;quot;Distribution of Propensity Scores&amp;quot;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/ipw_43_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;As expected, the distribution of propensity scores between the treatment and control group is &lt;strong&gt;significantly different&lt;/strong&gt;, suggesting that the two groups are hardly comparable. However, there is significant overlap in the support of the distributions, suggesting that the overlap assumption is likely to be satisfied.&lt;/p&gt;
&lt;p&gt;How do we estimate the average treatment effect?&lt;/p&gt;
&lt;p&gt;Once we have computed the propensity scores, we just need to re-weight observations by their respective propensity score. We can then either compute a difference between the weighted &lt;code&gt;read_time&lt;/code&gt; averages, or run a weighted regression of &lt;code&gt;read_time&lt;/code&gt; on &lt;code&gt;dark_mode&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;w = 1 / (df[&amp;quot;pscore&amp;quot;] * df[&amp;quot;dark_mode&amp;quot;] + (1-df[&amp;quot;pscore&amp;quot;]) * (1-df[&amp;quot;dark_mode&amp;quot;]))
smf.wls(&amp;quot;read_time ~ dark_mode&amp;quot;, weights=w, data=df).fit().summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
          &lt;td&gt;&lt;/td&gt;             &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt;         &lt;td&gt;   18.5859&lt;/td&gt; &lt;td&gt;    0.412&lt;/td&gt; &lt;td&gt;   45.110&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;   17.775&lt;/td&gt; &lt;td&gt;   19.397&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;dark_mode[T.True]&lt;/th&gt; &lt;td&gt;    1.1303&lt;/td&gt; &lt;td&gt;    0.582&lt;/td&gt; &lt;td&gt;    1.942&lt;/td&gt; &lt;td&gt; 0.053&lt;/td&gt; &lt;td&gt;   -0.015&lt;/td&gt; &lt;td&gt;    2.276&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;The effect of the &lt;code&gt;dark_mode&lt;/code&gt; is now positive and almost statistically significant, at the 5% level.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt; that the &lt;code&gt;wls&lt;/code&gt; function automatically normalizes weights so that they sum to 1, which greatly improves the stability of the estimator. In fact, the unnormalized IPW estimator can be very &lt;strong&gt;unstable&lt;/strong&gt; when the propensity scores approach zero or one.&lt;/p&gt;
&lt;p&gt;Also &lt;strong&gt;note&lt;/strong&gt; that the standard errors are not correct, since they do not take into account the extra uncertainty introduced in the estimation of the propensity score. This issue was noted by &lt;a href=&#34;https://onlinelibrary.wiley.com/doi/abs/10.3982/ECTA11293&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Abadie and Imbens (2016)&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;regression-with-control-variables&#34;&gt;Regression with Control Variables&lt;/h3&gt;
&lt;p&gt;The last method we are going to review today is &lt;strong&gt;linear regression with control variables&lt;/strong&gt;. This estimator is extremely easy to implement, since we just need to add the user characteristics - &lt;code&gt;gender&lt;/code&gt;, &lt;code&gt;age&lt;/code&gt; and &lt;code&gt;hours&lt;/code&gt; - to the regression of &lt;code&gt;read_time&lt;/code&gt; on &lt;code&gt;dark_mode&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;smf.ols(&amp;quot;read_time ~ dark_mode + male + age + hours&amp;quot;, data=df).fit().summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
          &lt;td&gt;&lt;/td&gt;             &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt;         &lt;td&gt;   16.8591&lt;/td&gt; &lt;td&gt;    1.082&lt;/td&gt; &lt;td&gt;   15.577&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;   14.729&lt;/td&gt; &lt;td&gt;   18.989&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;dark_mode[T.True]&lt;/th&gt; &lt;td&gt;    1.3858&lt;/td&gt; &lt;td&gt;    0.524&lt;/td&gt; &lt;td&gt;    2.646&lt;/td&gt; &lt;td&gt; 0.009&lt;/td&gt; &lt;td&gt;    0.355&lt;/td&gt; &lt;td&gt;    2.417&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;male&lt;/th&gt;              &lt;td&gt;   -4.4855&lt;/td&gt; &lt;td&gt;    0.499&lt;/td&gt; &lt;td&gt;   -8.990&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;   -5.468&lt;/td&gt; &lt;td&gt;   -3.504&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;age&lt;/th&gt;               &lt;td&gt;    0.0513&lt;/td&gt; &lt;td&gt;    0.022&lt;/td&gt; &lt;td&gt;    2.311&lt;/td&gt; &lt;td&gt; 0.022&lt;/td&gt; &lt;td&gt;    0.008&lt;/td&gt; &lt;td&gt;    0.095&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;hours&lt;/th&gt;             &lt;td&gt;    0.0043&lt;/td&gt; &lt;td&gt;    0.001&lt;/td&gt; &lt;td&gt;    8.427&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    0.003&lt;/td&gt; &lt;td&gt;    0.005&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;The average treatment effect is again positive and statistically significant at the 1% level!&lt;/p&gt;
&lt;h2 id=&#34;comparison&#34;&gt;Comparison&lt;/h2&gt;
&lt;p&gt;How do the different methods compare to each other?&lt;/p&gt;
&lt;h3 id=&#34;ipw-and-regression&#34;&gt;IPW and Regression&lt;/h3&gt;
&lt;p&gt;There is a &lt;strong&gt;tight connection&lt;/strong&gt; between the IPW estimator and linear regression with covariates. This is particularly evident when we have a one-dimensional, discrete covariate $X$.&lt;/p&gt;
&lt;p&gt;In this case, the estimand of IPW (i.e. the quantity that IPW estimates) is given by&lt;/p&gt;
&lt;p&gt;$$
\tau^{IPW} = \frac{ \sum_x \color{red}{\tau_x} \color{blue}{\Pr(D_i | X_i = x)} \Pr(X_i = x)}{\sum_x \color{blue}{\Pr(D_i | X_i = x)} \Pr(X_i = x)}
$$&lt;/p&gt;
&lt;p&gt;The IPW estimand is a weighted average of the treatment effects $\tau_x$, where the weights are given by the &lt;strong&gt;treatment probabilities&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;On the other hand, the estimand of linear regression with control variables is&lt;/p&gt;
&lt;p&gt;$$
\tau^{OLS} = \frac{ \sum_x \color{red}{\tau_x} \color{blue}{\Pr(D_i | X_i = x)(1 - \Pr(D_i | X_i = x)) } \Pr(X_i = x)}{\sum_x \color{blue}{\Pr(D_i | X_i = x)(1 - \Pr(D_i | X_i = x)) } \Pr(X_i = x)}
$$&lt;/p&gt;
&lt;p&gt;The OLS estimand is a weighted average of the treatment effects $\tau_x$, where the weights are given by the &lt;strong&gt;variances of the treatment probabilities&lt;/strong&gt;. This means that linear regression is a weighted estimator, that gives more weight to observations that have characteristics for which we observe more treatment variability. Since a binary random variable has the highest variance when its expected value is 0.5, &lt;strong&gt;OLS gives the most weight to observations that have characteristics for which we observe a 50/50 split between treatment and control group&lt;/strong&gt;. On the other hand, if for some characteristics we only observe treated or untreated individuals, those observations are going to receive zero weight. I recommend Chapter 3 of &lt;a href=&#34;https://www.mostlyharmlesseconometrics.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Angrist and Pischke (2009)&lt;/a&gt; for more details.&lt;/p&gt;
&lt;h3 id=&#34;ipw-and-matching&#34;&gt;IPW and Matching&lt;/h3&gt;
&lt;p&gt;As we have seen in the IPW section, &lt;a href=&#34;https://academic.oup.com/biomet/article/70/1/41/240879&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Rosenbaum and Rubin (1983)&lt;/a&gt; result tells us that we do not need to perform the analysis conditional on all the covariates $X$, but it is sufficient to condition on the propensity score $e(X)$.&lt;/p&gt;
&lt;p&gt;We have seed how this result implies a weighted estimator but it also extends to matching: we do not need to match observations on all the covariates $X$, but it is sufficient to &lt;strong&gt;match them on the propensity score&lt;/strong&gt; $e(X)$. This method is called propensity score matching.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;psm = NearestNeighborMatch(replace=False, random_state=1)
df_ipwmatched = psm.match(data=df, treatment_col=&amp;quot;dark_mode&amp;quot;, score_cols=[&#39;pscore&#39;])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As before, after matching, we can simply compute the estimate as a difference in means, remembering that observations are &lt;strong&gt;not independent&lt;/strong&gt; and therefore we need to be cautious when doing inference.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;smf.ols(&amp;quot;read_time ~ dark_mode&amp;quot;, data=df_ipwmatched)\
    .fit(cov_type=&#39;cluster&#39;, cov_kwds={&#39;groups&#39;: df_ipwmatched.index})\
    .summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
          &lt;td&gt;&lt;/td&gt;             &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;z&lt;/th&gt;      &lt;th&gt;P&gt;|z|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt;         &lt;td&gt;   18.4633&lt;/td&gt; &lt;td&gt;    0.505&lt;/td&gt; &lt;td&gt;   36.576&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;   17.474&lt;/td&gt; &lt;td&gt;   19.453&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;dark_mode[T.True]&lt;/th&gt; &lt;td&gt;    1.1888&lt;/td&gt; &lt;td&gt;    0.703&lt;/td&gt; &lt;td&gt;    1.692&lt;/td&gt; &lt;td&gt; 0.091&lt;/td&gt; &lt;td&gt;   -0.188&lt;/td&gt; &lt;td&gt;    2.566&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;The estimated effect of &lt;code&gt;dark_mode&lt;/code&gt; is positive, significant at the 1% level and very close to the true value of 2!&lt;/p&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;In this blog post, we have seen how to perform &lt;strong&gt;conditional analysis&lt;/strong&gt; using different approached. Matching directly matches most similar units in the treatment and control group. Weighting simply assigns different weight to different observations depending on their probability of receiving the treatment. Regression instead weights observations depending on the conditional treatment variances, giving more weight to observations that have characteristics common to both the treatment and control group.&lt;/p&gt;
&lt;p&gt;These procedures are &lt;strong&gt;extremely helpful&lt;/strong&gt; because they can either allow us to estimate causal effects from (very rich) observational data or correct experimental estimates when randomization was not perfect or we have a small sample.&lt;/p&gt;
&lt;p&gt;Last but not least, if you want to know more, I strongly recommend this &lt;strong&gt;video lecture&lt;/strong&gt; on propensity scores from &lt;a href=&#34;https://paulgp.github.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paul Goldsmith-Pinkham&lt;/a&gt; that is freely available online.&lt;/p&gt;
&lt;br&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/8gWctYvRzk4&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;br&gt;
&lt;p&gt;The whole course is a &lt;strong&gt;gem&lt;/strong&gt; and it is an incredible privilege to have such high quality material available online for free!&lt;/p&gt;
&lt;h3 id=&#34;references&#34;&gt;References&lt;/h3&gt;
&lt;p&gt;[1] P. Rosenbaum, D. Rubin, &lt;a href=&#34;https://academic.oup.com/biomet/article/70/1/41/240879&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The central role of the propensity score in observational studies for causal effects&lt;/a&gt; (1983), &lt;em&gt;Biometrika&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;[2] G. Imbens, K. Hirano, G. Ridder, &lt;a href=&#34;https://onlinelibrary.wiley.com/doi/abs/10.1111/1468-0262.00442&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Efficient Estimation of Average Treatment Effects Using the Estimated Propensity Score&lt;/a&gt; (2003), &lt;em&gt;Econometrica&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;[3] J. Angrist, J. S. Pischke, &lt;a href=&#34;https://www.mostlyharmlesseconometrics.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Mostly harmless econometrics: An Empiricist&amp;rsquo;s Companion&lt;/a&gt; (2009), &lt;em&gt;Princeton University Press&lt;/em&gt;.&lt;/p&gt;
&lt;h3 id=&#34;related-articles&#34;&gt;Related Articles&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/59f801eb3299&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Understanding The Frisch-Waugh-Lovell Theorem&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/9b06ee4d30bf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;How to Compare Two or More Distributions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/b63dc69e3d8c&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;DAGs and Control Variables&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;code&#34;&gt;Code&lt;/h3&gt;
&lt;p&gt;You can find the original Jupyter Notebook here:&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/ipw.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/ipw.ipynb&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Understanding Meta Learners</title>
      <link>https://matteocourthoud.github.io/post/meta/</link>
      <pubDate>Fri, 15 Jul 2022 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/post/meta/</guid>
      <description>&lt;p&gt;&lt;em&gt;How to use machine learning to estimate heterogeneous treatment effects&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;In many settings, we are not just interested in understanding a causal effect, but also whether this effect is &lt;strong&gt;different for different users&lt;/strong&gt;. We might be interested in understanding if a drug has side effects that are different for people of different age. Or we might be interested in understanding if an ad campaign is particularly effective in certain geographical areas.&lt;/p&gt;
&lt;p&gt;This knowledge is crucial because it allows us to &lt;strong&gt;target&lt;/strong&gt; the treatment. If a drug has severe side effects for kids, we might want to restrict its distribution only to adults. Or if an ad campaign is effective only in English-speaking countries it is not worth showing it elsewhere.&lt;/p&gt;
&lt;p&gt;In this blog post we are going to explore some approaches to uncover &lt;strong&gt;treatment effect heterogeneity&lt;/strong&gt;. In particular, we are going to explore methods that try to leverage the flexibility of &lt;strong&gt;machine learning&lt;/strong&gt; algorithms.&lt;/p&gt;
&lt;h2 id=&#34;example&#34;&gt;Example&lt;/h2&gt;
&lt;p&gt;Suppose we were a company interested in understanding how much a new &lt;strong&gt;premium feature&lt;/strong&gt; increases revenue. In particular, we know that users of different &lt;strong&gt;age&lt;/strong&gt; have different spending attitudes and we suspect that the impact of the premium feature could also be different depending on the age of the user.&lt;/p&gt;
&lt;p&gt;This information might be very important, for example for &lt;strong&gt;advertisement targeting&lt;/strong&gt; or &lt;strong&gt;discount design&lt;/strong&gt;. If we discover that the premium feature increases revenue for a particular set of users, we might want to target advertisement towards that group, or offer them personalized discounts.&lt;/p&gt;
&lt;p&gt;To understand the effect of the premium feature on revenue, the run an &lt;a href=&#34;https://en.wikipedia.org/wiki/A/B_testing&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;AB test&lt;/strong&gt;&lt;/a&gt; in which we randomly give access to the premium feature to 10% of the users. The feature is &lt;strong&gt;expensive&lt;/strong&gt; and we cannot afford to give it for free to more users. Hopefully a 10% treatment probability is enough.&lt;/p&gt;
&lt;p&gt;We generate the simulated data using the data generating process &lt;code&gt;dgp_premium()&lt;/code&gt; from &lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/src/dgp.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;src.dgp&lt;/code&gt;&lt;/a&gt;. I also import some plotting functions and libraries from &lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/src/utils.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;src.utils&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;%matplotlib inline
%config InlineBackend.figure_format = &#39;retina&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from src.utils import *
from src.dgp import dgp_premium
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;dgp = dgp_premium()
df = dgp.generate_data(seed=5)
df.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;revenue&lt;/th&gt;
      &lt;th&gt;premium&lt;/th&gt;
      &lt;th&gt;age&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;10.62&lt;/td&gt;
      &lt;td&gt;True&lt;/td&gt;
      &lt;td&gt;27.32&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;10.35&lt;/td&gt;
      &lt;td&gt;True&lt;/td&gt;
      &lt;td&gt;54.57&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;10.13&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;26.68&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;9.97&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;56.58&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;10.16&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;38.51&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;We have data on 300 users, for whom we observe the &lt;code&gt;revenue&lt;/code&gt; they generate and whether they were given the &lt;code&gt;premium&lt;/code&gt; feature. Moreover, we also record the &lt;code&gt;age&lt;/code&gt; of the users.&lt;/p&gt;
&lt;p&gt;To understand whether randomization worked, we use the &lt;code&gt;create_table_one&lt;/code&gt; function from Uber&amp;rsquo;s &lt;a href=&#34;https://causalml.readthedocs.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;causalml&lt;/code&gt;&lt;/a&gt; package to produce a &lt;strong&gt;covariate balance table&lt;/strong&gt;, containing the average value of our observable characteristics, across treatment and control groups. As the name suggests, this should always be the first table you present in causal inference analysis.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from causalml.match import create_table_one

create_table_one(df, &#39;premium&#39;, [&#39;age&#39;, &#39;revenue&#39;])
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;Control&lt;/th&gt;
      &lt;th&gt;Treatment&lt;/th&gt;
      &lt;th&gt;SMD&lt;/th&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;Variable&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;n&lt;/th&gt;
      &lt;td&gt;269&lt;/td&gt;
      &lt;td&gt;31&lt;/td&gt;
      &lt;td&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;age&lt;/th&gt;
      &lt;td&gt;39.01 (12.11)&lt;/td&gt;
      &lt;td&gt;38.43 (13.26)&lt;/td&gt;
      &lt;td&gt;-0.0454&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;revenue&lt;/th&gt;
      &lt;td&gt;10.04 (0.16)&lt;/td&gt;
      &lt;td&gt;10.56 (0.23)&lt;/td&gt;
      &lt;td&gt;2.5905&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;Most users are in the control group and only 31 users have received the premium feature. Average &lt;code&gt;age&lt;/code&gt; is comparable across groups (SMD&amp;lt;1), while it seems that the premium feature increases &lt;code&gt;revenue&lt;/code&gt; by 2.6$ per user, on average.&lt;/p&gt;
&lt;p&gt;Does the effect of the &lt;code&gt;premium&lt;/code&gt; feature differ by &lt;code&gt;age&lt;/code&gt;?&lt;/p&gt;
&lt;p&gt;One simple approach could me to regress &lt;code&gt;revenue&lt;/code&gt; on a full interaction of &lt;code&gt;premium&lt;/code&gt; and age.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;linear_model = smf.ols(&#39;revenue ~ premium * age&#39;, data=df).fit()
linear_model.summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
           &lt;td&gt;&lt;/td&gt;              &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt;           &lt;td&gt;   10.0244&lt;/td&gt; &lt;td&gt;    0.034&lt;/td&gt; &lt;td&gt;  292.716&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    9.957&lt;/td&gt; &lt;td&gt;   10.092&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;premium[T.True]&lt;/th&gt;     &lt;td&gt;    0.5948&lt;/td&gt; &lt;td&gt;    0.099&lt;/td&gt; &lt;td&gt;    6.007&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    0.400&lt;/td&gt; &lt;td&gt;    0.790&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;age&lt;/th&gt;                 &lt;td&gt;    0.0005&lt;/td&gt; &lt;td&gt;    0.001&lt;/td&gt; &lt;td&gt;    0.570&lt;/td&gt; &lt;td&gt; 0.569&lt;/td&gt; &lt;td&gt;   -0.001&lt;/td&gt; &lt;td&gt;    0.002&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;premium[T.True]:age&lt;/th&gt; &lt;td&gt;   -0.0021&lt;/td&gt; &lt;td&gt;    0.002&lt;/td&gt; &lt;td&gt;   -0.863&lt;/td&gt; &lt;td&gt; 0.389&lt;/td&gt; &lt;td&gt;   -0.007&lt;/td&gt; &lt;td&gt;    0.003&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;The interaction coefficient is close to zero and not significant. It seems that there is not a different effect of &lt;code&gt;premium&lt;/code&gt; by &lt;code&gt;age&lt;/code&gt;. But is it true? The interaction coefficient only captures linear relationships. What if the relationship is &lt;strong&gt;non-linear&lt;/strong&gt;?&lt;/p&gt;
&lt;p&gt;We can check it by directly &lt;strong&gt;plotting the raw data&lt;/strong&gt;. We plot revenue by age, splitting the data between &lt;code&gt;premium&lt;/code&gt; users and non-premium users.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sns.scatterplot(data=df, x=&#39;age&#39;, y=&#39;revenue&#39;, hue=&#39;premium&#39;, s=40);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/meta_13_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;From the raw data, it looks like revenue is generally higher for people between 30 and 40 and &lt;code&gt;premium&lt;/code&gt; has a particularly strong effect for people between 35 and 45/50.&lt;/p&gt;
&lt;p&gt;We can visualize the estimated revenue by age with and without treatment.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def plot_TE(df, true_te=False):
    sns.scatterplot(data=df, x=&#39;age&#39;, y=&#39;revenue&#39;, hue=&#39;premium&#39;, s=40, legend=True)
    sns.lineplot(df[&#39;age&#39;], df[&#39;mu0_hat&#39;], label=&#39;$\mu_0$&#39;)
    sns.lineplot(df[&#39;age&#39;], df[&#39;mu1_hat&#39;], label=&#39;$\mu_1$&#39;)
    if true_te:
        plt.fill_between(df[&#39;age&#39;], df[&#39;y0&#39;], df[&#39;y0&#39;] + df[&#39;y1&#39;], color=&#39;grey&#39;, alpha=0.2, label=&amp;quot;True TE&amp;quot;)
    plt.title(&#39;Distribution of revenue by age and premium status&#39;)
    plt.legend(title=&#39;Treated&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We first compute the predicted revenue with ($\mu_1$) and without &lt;code&gt;premium&lt;/code&gt; subscription ($\mu_0$) and we plot them together with the raw data.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df[&#39;mu0_hat&#39;] = linear_model.predict(df.assign(premium=0))
df[&#39;mu1_hat&#39;] = linear_model.predict(df.assign(premium=1))
plot_TE(df)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/meta_17_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;As we can see, the orange line is higher than the blue line, suggesting a positive effect of &lt;code&gt;premium&lt;/code&gt; on &lt;code&gt;revenue&lt;/code&gt;. However, the two lines are essentially &lt;strong&gt;parallel&lt;/strong&gt;, suggesting no heterogeneity in treatment effects.&lt;/p&gt;
&lt;p&gt;Can we be more precise? Is there a way to estimate this treatment heterogeneity in a &lt;strong&gt;flexible way&lt;/strong&gt;, without assuming functional forms?&lt;/p&gt;
&lt;p&gt;The answer is &lt;strong&gt;yes&lt;/strong&gt;! We can use machine learning methods to flexibly estimate heterogeneous treatment effects. In particular, in this blog post we are going to inspect three and popular methods that were introduced by &lt;a href=&#34;https://www.pnas.org/doi/abs/10.1073/pnas.1804597116&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Künzel, Sekhon, Bickel, Yu, (2019)&lt;/a&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;S-learner&lt;/li&gt;
&lt;li&gt;T-learner&lt;/li&gt;
&lt;li&gt;X-learner&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;setting&#34;&gt;Setting&lt;/h2&gt;
&lt;p&gt;We assume that for a set of i.i.d. subjects $i = 1, &amp;hellip;, n$ we observed a tuple $(X_i, D_i, Y_i)$ comprised of&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;a treatment assignment $T_i \in \lbrace 0, 1 \rbrace$ (&lt;code&gt;premium&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;a response $Y_i \in \mathbb R$ (&lt;code&gt;revenue&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;a feature vector $X_i \in \mathbb R^n$ (&lt;code&gt;age&lt;/code&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We are interested in &lt;strong&gt;estimating the average treatment effect&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;$$
\tau = \mathbb E \Big[ Y_i^{(1)} - Y_i^{(0)} \Big]
$$&lt;/p&gt;
&lt;p&gt;Where $Y_i^{(d)}$ indicates the potential outcome of individual $i$ under treatment status $d$. We also make the following assumptions.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Assumption 1 : unconfoundedness&lt;/strong&gt; (or ignorability, or selection on observables)&lt;/p&gt;
&lt;p&gt;$$
\big \lbrace Y_i^{(1)} , Y_i^{(0)} \big \rbrace \ \perp \ D_i \ | \ X_i
$$&lt;/p&gt;
&lt;p&gt;i.e. conditional on observable characteristics $X$, the treatment assignment $D$ is as good as random. What we are effectively assuming is that there is no other characteristics that we do not observe that could impact both whether a user selects the &lt;code&gt;dark_mode&lt;/code&gt; and their &lt;code&gt;read_time&lt;/code&gt;. This is a &lt;strong&gt;strong assumption&lt;/strong&gt; that is more likely to be satisfied the more individual characteristics we observe.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Assumption 2: stable unit treatment value (SUTVA)&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;$$
Y^{(d)} \perp D
$$&lt;/p&gt;
&lt;p&gt;i.e. the potential outcome does not depend on the treatment status. In our case, we are ruling out the fact that another user selecting the &lt;code&gt;premium&lt;/code&gt; feature might affect my effect of &lt;code&gt;premium&lt;/code&gt; on &lt;code&gt;revenue&lt;/code&gt;. The most common setting where SUTVA is violated is in presence of &lt;strong&gt;network effects&lt;/strong&gt;: if a friend of mine uses a social network increases my utility from using it.&lt;/p&gt;
&lt;h2 id=&#34;s-learner&#34;&gt;S-Learner&lt;/h2&gt;
&lt;p&gt;The simplest meta-algorithm is the &lt;strong&gt;single learner or S-learner&lt;/strong&gt;. To build the S-learner estimator, we fit a single model for all observations.&lt;/p&gt;
&lt;p&gt;$$
\mu(z) = \mathbb E \left[ Y_i \ \big | \ (X_i, D_i) = z \right]
$$&lt;/p&gt;
&lt;p&gt;the estimator is given by the difference between the predicted values evaluated with and without the treatment, $d=1$ and $d=0$.&lt;/p&gt;
&lt;p&gt;$$
\hat \tau_{S} (x) = \hat \mu(x,1) - \hat \mu(x,0)
$$&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def S_learner(dgp, model, y, D, X):
    temp = dgp.generate_data(true_te=True).sort_values(X)
    mu = model.fit(temp[X + [D]], temp[y])
    temp[&#39;mu0_hat&#39;] = mu.predict(temp[X + [D]].assign(premium=0))
    temp[&#39;mu1_hat&#39;] = mu.predict(temp[X + [D]].assign(premium=1))
    plot_TE(temp, true_te=True)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s use a &lt;a href=&#34;https://en.wikipedia.org/wiki/Decision_tree_learning&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;decision tree regression&lt;/strong&gt;&lt;/a&gt; model to build the the S-learner, using the &lt;code&gt;DecisionTreeRegressor&lt;/code&gt; function from the &lt;a href=&#34;https://scikit-learn.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;sklearn&lt;/code&gt;&lt;/a&gt; package. I won&amp;rsquo;t go into details about decision trees here, but I will just say that it&amp;rsquo;s a non-parametric estimator that uses the training data to split the state space (&lt;code&gt;premium&lt;/code&gt; and &lt;code&gt;age&lt;/code&gt; in our case) into blocks and predicts the outcome (&lt;code&gt;revenue&lt;/code&gt; in our case) as its average value within block.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from sklearn.tree import DecisionTreeRegressor

model = DecisionTreeRegressor(min_impurity_decrease=0.001)
S_learner(dgp, model, y=&amp;quot;revenue&amp;quot;, D=&amp;quot;premium&amp;quot;, X=[&amp;quot;age&amp;quot;])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/meta_25_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;The plot depicts the data together with the response functions $\hat \mu(x,1)$ and $\hat \mu(x,0)$. I have also plotted in grey the area between the true response functions: the true treatment effects.&lt;/p&gt;
&lt;p&gt;As we can see, the S-learner is flexible enough to understand that there is a difference in levels between treatment and control group (we have two separate lines). It also captures well the response function for the control group, $\hat \mu(x,0)$, but not so well the control function for the treatment group, $\hat \mu(x,1)$.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;problem&lt;/strong&gt; with the S-learner is that it is learning a &lt;strong&gt;single model&lt;/strong&gt; so we have to hope that the model uncovers heterogeneity in the treatment $D$, but it might not be the case. Moreover, if the model is heavily regularized because of the high dimensionality of $X$, it &lt;strong&gt;might not recover any treatment effect&lt;/strong&gt;. For example, with decision trees, we might not split on the treatment $D$.&lt;/p&gt;
&lt;h2 id=&#34;t-learner&#34;&gt;T-learner&lt;/h2&gt;
&lt;p&gt;To build the &lt;strong&gt;two-learner or T-learner&lt;/strong&gt; estimator, we fit two different models, one for treated units and one for control units.&lt;/p&gt;
&lt;p&gt;$$
\mu^{(1)}(x) = \mathbb E \left[ Y_i \ \big | \ X_i = x, T_i = 1 \right] \qquad ; \qquad \mu^{(0)}(x) = \mathbb E \left[ Y_i \ \big | \ X_i = x, T_i = 0 \right]
$$&lt;/p&gt;
&lt;p&gt;the estimator is given by the difference between the predicted values of the two algorithms.&lt;/p&gt;
&lt;p&gt;$$
\hat \tau_{T} (x) = \hat \mu^{(1)}(x) - \hat \mu^{(0)}(x)
$$&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def T_learner(df, model, y, D, X):
    temp = dgp.generate_data(true_te=True).sort_values(X)
    mu0 = model.fit(temp.loc[temp[D]==0, X], temp.loc[temp[D]==0, y])
    temp[&#39;mu0_hat&#39;] = mu0.predict(temp[X])
    mu1 = model.fit(temp.loc[temp[D]==1, X], temp.loc[temp[D]==1, y])
    temp[&#39;mu1_hat&#39;] = mu1.predict(temp[X])
    plot_TE(temp, true_te=True)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We use a decision tree regression model as before but, this time, we fit two separate decision trees for the treatment and control group.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;T_learner(dgp, model, y=&amp;quot;revenue&amp;quot;, D=&amp;quot;premium&amp;quot;, X=[&amp;quot;age&amp;quot;])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/meta_31_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;As we can see, the T-learner is much &lt;strong&gt;more flexible&lt;/strong&gt; than the S-learner because it fits two separate models. The response function for the control group, $\hat \mu(x,0)$, is still very accurate and the response function for the treatment group, $\hat \mu(x,1)$, is more flexible than before.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;problem&lt;/strong&gt; now is that we are &lt;strong&gt;using just a fraction of the data&lt;/strong&gt; for each prediction problem, while the S-learner was using all the data. By fitting two separate models we are losing some information. Moreover, by using two different models we might get &lt;strong&gt;heterogeneity where there is none&lt;/strong&gt;. For example, with decision trees, we will probably get different splits with different samples even if the data generating process is the same.&lt;/p&gt;
&lt;h3 id=&#34;x-learner&#34;&gt;X-learner&lt;/h3&gt;
&lt;p&gt;The &lt;strong&gt;cross-learner or X-learner&lt;/strong&gt; estimator is an extension of the T-learner estimator. It is built in the following way:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;As for the T-learner, compute separate models for $\mu^{(1)}(x)$ and $\mu^{(0)}(x)$ using the treated and control units, respectively&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Compute the estimated treatment effects as&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;$$
\Delta_i (x) =
\begin{cases}
Y_i - \hat \mu^{(0)}(x) &amp;amp;\quad \text{ if } D_i = 1
\newline
\hat \mu^{(1)}(x) - Y_i &amp;amp;\quad \text{ if } D_i = 0
\end{cases}
$$&lt;/p&gt;
&lt;ol start=&#34;3&#34;&gt;
&lt;li&gt;
&lt;p&gt;Predicting $\Delta$ from $X$, compute $\hat \tau^{(0)}(x)$ from treated units and  $\hat \tau^{(1)}(x)$ from control units&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Estimate the &lt;a href=&#34;https://en.wikipedia.org/wiki/Propensity_score_matching&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;propensity score&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;$$
e(x) = \mathbb E \left[ D_i = 1 \ \big | \ X_i = x \right]
$$&lt;/p&gt;
&lt;ol start=&#34;5&#34;&gt;
&lt;li&gt;Compute the treatment effects&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;$$
\hat \tau_X(x) = \hat \tau^{(0)}(x) \hat e(x) + \hat \tau^{(1)}(x) (1 - \hat e(x))
$$&lt;/p&gt;
&lt;p&gt;Can we still recover &lt;strong&gt;pseudo response functions&lt;/strong&gt;? Yes!&lt;/p&gt;
&lt;p&gt;Which we can rewrite the treatment effects as&lt;/p&gt;
&lt;p&gt;$$
\begin{align}
\hat \tau_X(x) &amp;amp; = \hat \tau^{(0)}(x) \hat e(x) + \hat \tau^{(1)}(x) (1 - \hat e(x)) = \newline
&amp;amp;= \hat e(x) \left[ \hat \mu^{(1)}(x) - Y_i^{(0)} \right] + (1 - \hat e(x)) \left[ Y_i^{(1)} - \hat \mu^{(0)}(x) \right] = \newline
&amp;amp;= \left[ \hat e(x) \hat \mu^{(1)}(x) + (1 - \hat e(x)) Y_i^{(1)} \right] - \left[ \hat e(x) Y_i^{(0)} + (1 - \hat e(x))  \hat \mu^{(0)}(x) \right]
\end{align}
$$&lt;/p&gt;
&lt;p&gt;So that the pseudo response functions estimated by the X-learner are&lt;/p&gt;
&lt;p&gt;$$
\begin{align}
\tilde \mu_i^{(1)} (x) &amp;amp;= \hat e(x) \hat \mu^{(1)}(x) + (1 - \hat e(x)) Y_i^{(1)} \newline
\tilde \mu_i^{(0)} (x) &amp;amp;=  \hat e(x) Y_i^{(0)} + (1 - \hat e(x)) \hat \mu^{(0)}(x)
\end{align}
$$&lt;/p&gt;
&lt;p&gt;As we can see, the X-learner combines the true values $Y_i^{(d)}$ with the estimated ones $\mu_i^{(d)} (x)$ weighting by the &lt;a href=&#34;https://en.wikipedia.org/wiki/Propensity_score_matching&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;propensity scores&lt;/strong&gt;&lt;/a&gt; $e_i(x)$, i.e. the estimated treatment probabilities.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;What does it mean?&lt;/strong&gt; It means that if we have many more observations for one group (in our case the control group), the control response function $\hat \mu^{(0)}(x) $ will get most of the weight. Instead, for the other group (the treatment group in our case), the actual observations $Y_i^{(1)}$ will get most of the weight.&lt;/p&gt;
&lt;p&gt;To illustrate the method, I am going to build pseudo response functions by approximating $Y_i^{(d)}$ using the nearest observation, using the &lt;code&gt;KNeighborsRegressor&lt;/code&gt; function. I estimate the propensity scores via &lt;a href=&#34;https://en.wikipedia.org/wiki/Logistic_regression&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;logistic regression&lt;/a&gt; using the &lt;code&gt;LogisticRegressionCV&lt;/code&gt; function.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from sklearn.neighbors import KNeighborsRegressor
from sklearn.linear_model import LogisticRegressionCV

def X_learner(df, model, y, D, X):
    temp = dgp.generate_data(true_te=True).sort_values(X)
    
    # Mu
    mu0 = model.fit(temp.loc[temp[D]==0, X], temp.loc[temp[D]==0, y])
    temp[&#39;mu0_hat_&#39;] = mu0.predict(temp[X])
    mu1 = model.fit(temp.loc[temp[D]==1, X], temp.loc[temp[D]==1, y])
    temp[&#39;mu1_hat_&#39;] = mu1.predict(temp[X])
    
    # Y
    y0 = KNeighborsRegressor(n_neighbors=1).fit(temp.loc[temp[D]==0, X], temp.loc[temp[D]==0, y])
    temp[&#39;y0_hat&#39;] = y0.predict(temp[X])
    y1 = KNeighborsRegressor(n_neighbors=1).fit(temp.loc[temp[D]==1, X], temp.loc[temp[D]==1, y])
    temp[&#39;y1_hat&#39;] = y1.predict(temp[X])
    
    # Weight
    e = LogisticRegressionCV().fit(y=temp[D], X=temp[X]).predict_proba(temp[X])[:,1]
    temp[&#39;mu0_hat&#39;] = e * temp[&#39;y0_hat&#39;] + (1-e) * temp[&#39;mu0_hat_&#39;]
    temp[&#39;mu1_hat&#39;] = (1-e) * temp[&#39;y1_hat&#39;] + e * temp[&#39;mu1_hat_&#39;]
    
    # Plot
    plot_TE(temp, true_te=True)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;X_learner(df, model, y=&amp;quot;revenue&amp;quot;, D=&amp;quot;premium&amp;quot;, X=[&amp;quot;age&amp;quot;])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/meta_37_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;As we can clearly see from this graph, the main advantage of &lt;strong&gt;X-learners&lt;/strong&gt; is that it adapts the &lt;strong&gt;flexibility&lt;/strong&gt; of the response functions to the context. In areas of the state space where we have a lot of data, it mostly uses the estimated response function, in areas of the state space with few data, it uses the observation themselves.&lt;/p&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;In this post, we have seen different estimators introduced by &lt;a href=&#34;https://www.pnas.org/doi/abs/10.1073/pnas.1804597116&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Künzel, Sekhon, Bickel, Yu, (2019)&lt;/a&gt; that leverage flexible &lt;strong&gt;machine learning&lt;/strong&gt; algorithms to estimate &lt;strong&gt;heterogeneous treatment effects&lt;/strong&gt;. The estimators differ for their degree of sophistication: the S-learner fits a single estimator including the treatment indicator as a covariate. The T-learner fits two separate estimators for the treatment and control group. Lastly, the X-learner is an extension of the T-learner that allows for different degrees of flexibility depending on the amount of data available across treatment and control groups.&lt;/p&gt;
&lt;p&gt;Estimation of heterogeneous treatment effect is extremely important for &lt;strong&gt;treatment targeting&lt;/strong&gt;. Indeed, there is now a growing literature that exploits machine learning methods to get flexible estimates without imposing functional form assumptions. Among the many, it&amp;rsquo;s important to mention the R-learner procedure of &lt;a href=&#34;https://academic.oup.com/biomet/article/108/2/299/5911092&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Nie and Wager (2021)&lt;/a&gt; and the causal trees and forests of &lt;a href=&#34;https://www.tandfonline.com/doi/full/10.1080/01621459.2017.1319839&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Athey and Wager (2018)&lt;/a&gt;. I might write more about these procedures in the future so, stay tuned ☺️&lt;/p&gt;
&lt;h3 id=&#34;references&#34;&gt;References&lt;/h3&gt;
&lt;p&gt;[1] S. Künzel, J. Sekhon, P. Bickel, B. Yu, &lt;a href=&#34;https://www.pnas.org/doi/abs/10.1073/pnas.1804597116&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Metalearners for estimating heterogeneous treatment effects using machine learning&lt;/a&gt; (2019), &lt;em&gt;PNAS&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;[2] X. Nie, S. Wager, &lt;a href=&#34;https://academic.oup.com/biomet/article/108/2/299/5911092&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Quasi-oracle estimation of heterogeneous treatment effects&lt;/a&gt; (2021), &lt;em&gt;Biometrika&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;[3] S. Athey, S. Wager, &lt;a href=&#34;https://www.tandfonline.com/doi/full/10.1080/01621459.2017.1319839&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Estimation and Inference of Heterogeneous Treatment Effects using Random Forests&lt;/a&gt; (2018), &lt;em&gt;Journal of the American Statistical Association&lt;/em&gt;.&lt;/p&gt;
&lt;h3 id=&#34;related-articles&#34;&gt;Related Articles&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/99bf5cffa0d9&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Matching, Weighting, or Regression?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/b63dc69e3d8c&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;DAGs and Control Variables&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;code&#34;&gt;Code&lt;/h3&gt;
&lt;p&gt;You can find the original Jupyter Notebook here:&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/meta.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/meta.ipynb&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Comparing ATE Estimators</title>
      <link>https://matteocourthoud.github.io/post/compare/</link>
      <pubDate>Fri, 01 Jul 2022 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/post/compare/</guid>
      <description>&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;%matplotlib inline
%config InlineBackend.figure_format = &#39;retina&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from src.utils import *
from src.dgp import dgp_compare
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df = dgp_compare().generate_data(include_beta=True)
df.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;outcome&lt;/th&gt;
      &lt;th&gt;treated&lt;/th&gt;
      &lt;th&gt;male&lt;/th&gt;
      &lt;th&gt;age&lt;/th&gt;
      &lt;th&gt;income&lt;/th&gt;
      &lt;th&gt;beta&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;38.68&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;28.0&lt;/td&gt;
      &lt;td&gt;1514.0&lt;/td&gt;
      &lt;td&gt;2.726425&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;37.81&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;47.0&lt;/td&gt;
      &lt;td&gt;1524.0&lt;/td&gt;
      &lt;td&gt;2.777192&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;27.70&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;60.0&lt;/td&gt;
      &lt;td&gt;2683.0&lt;/td&gt;
      &lt;td&gt;0.247312&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;29.56&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;29.0&lt;/td&gt;
      &lt;td&gt;3021.0&lt;/td&gt;
      &lt;td&gt;2.632797&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;36.83&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;25.0&lt;/td&gt;
      &lt;td&gt;1859.0&lt;/td&gt;
      &lt;td&gt;1.724331&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;np.mean(df[&#39;beta&#39;])
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;2.0
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;simple-difference&#34;&gt;Simple difference&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;smf.ols(&#39;outcome ~ treated&#39;, data=df).fit().summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
         &lt;td&gt;&lt;/td&gt;            &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt;       &lt;td&gt;   30.8496&lt;/td&gt; &lt;td&gt;    0.091&lt;/td&gt; &lt;td&gt;  340.817&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;   30.672&lt;/td&gt; &lt;td&gt;   31.027&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;treated[T.True]&lt;/th&gt; &lt;td&gt;   -0.0060&lt;/td&gt; &lt;td&gt;    0.118&lt;/td&gt; &lt;td&gt;   -0.051&lt;/td&gt; &lt;td&gt; 0.959&lt;/td&gt; &lt;td&gt;   -0.238&lt;/td&gt; &lt;td&gt;    0.226&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;h3 id=&#34;regression-with-control&#34;&gt;Regression with control&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;smf.ols(&#39;outcome ~ treated + male + age + income&#39;, data=df).fit().summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
         &lt;td&gt;&lt;/td&gt;            &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt;       &lt;td&gt;   29.5761&lt;/td&gt; &lt;td&gt;    0.272&lt;/td&gt; &lt;td&gt;  108.833&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;   29.043&lt;/td&gt; &lt;td&gt;   30.109&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;treated[T.True]&lt;/th&gt; &lt;td&gt;    2.2682&lt;/td&gt; &lt;td&gt;    0.129&lt;/td&gt; &lt;td&gt;   17.623&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    2.016&lt;/td&gt; &lt;td&gt;    2.520&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;male&lt;/th&gt;            &lt;td&gt;    4.8568&lt;/td&gt; &lt;td&gt;    0.104&lt;/td&gt; &lt;td&gt;   46.688&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    4.653&lt;/td&gt; &lt;td&gt;    5.061&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;age&lt;/th&gt;             &lt;td&gt;   -0.1215&lt;/td&gt; &lt;td&gt;    0.006&lt;/td&gt; &lt;td&gt;  -21.752&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;   -0.132&lt;/td&gt; &lt;td&gt;   -0.111&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;income&lt;/th&gt;          &lt;td&gt;    0.0014&lt;/td&gt; &lt;td&gt;    9e-05&lt;/td&gt; &lt;td&gt;   16.032&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    0.001&lt;/td&gt; &lt;td&gt;    0.002&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;h3 id=&#34;matching&#34;&gt;Matching&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from causalml.match import NearestNeighborMatch, create_table_one

X = [&#39;male&#39;, &#39;age&#39;, &#39;income&#39;]
psm = NearestNeighborMatch(replace=True, ratio=1, random_state=42)
df_matched = psm.match(data=df, 
                       treatment_col=&amp;quot;treated&amp;quot;,
                       score_cols=X)
smf.ols(&#39;outcome ~ treated&#39;, data=df_matched).fit().summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
         &lt;td&gt;&lt;/td&gt;            &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt;       &lt;td&gt;   29.5347&lt;/td&gt; &lt;td&gt;    0.078&lt;/td&gt; &lt;td&gt;  380.105&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;   29.382&lt;/td&gt; &lt;td&gt;   29.687&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;treated[T.True]&lt;/th&gt; &lt;td&gt;    1.6918&lt;/td&gt; &lt;td&gt;    0.110&lt;/td&gt; &lt;td&gt;   15.396&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    1.476&lt;/td&gt; &lt;td&gt;    1.907&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;h3 id=&#34;ipw&#34;&gt;IPW&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df[&amp;quot;pscore&amp;quot;] = smf.logit(&amp;quot;np.rint(treated) ~ male + age + income&amp;quot;, data=df).fit(disp=False).predict()
w = 1 / (df[&amp;quot;pscore&amp;quot;] * df[&amp;quot;treated&amp;quot;] + (1-df[&amp;quot;pscore&amp;quot;]) * (1-df[&amp;quot;treated&amp;quot;]))
smf.wls(&amp;quot;outcome ~ treated&amp;quot;, weights=w, data=df).fit().summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
         &lt;td&gt;&lt;/td&gt;            &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt;       &lt;td&gt;   30.2572&lt;/td&gt; &lt;td&gt;    0.083&lt;/td&gt; &lt;td&gt;  362.783&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;   30.094&lt;/td&gt; &lt;td&gt;   30.421&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;treated[T.True]&lt;/th&gt; &lt;td&gt;    1.6806&lt;/td&gt; &lt;td&gt;    0.116&lt;/td&gt; &lt;td&gt;   14.429&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    1.452&lt;/td&gt; &lt;td&gt;    1.909&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;h3 id=&#34;r-learner&#34;&gt;R Learner&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from causalml.inference.meta import BaseRLearner
from lightgbm import LGBMRegressor

BaseRLearner(learner=LGBMRegressor()).estimate_ate(X=df[X], treatment=df[&#39;treated&#39;], y=df[&#39;outcome&#39;])
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;(array([1.6529302]), array([1.65076231]), array([1.65509809]))
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;s-learner&#34;&gt;S Learner&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from causalml.inference.meta import BaseSLearner

BaseSLearner(learner=LGBMRegressor()).estimate_ate(X=df[X], treatment=df[&#39;treated&#39;], y=df[&#39;outcome&#39;])
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;array([2.08090912])
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;t-learner&#34;&gt;T Learner&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from causalml.inference.meta import BaseTLearner

BaseTLearner(learner=LGBMRegressor()).estimate_ate(X=df[X], treatment=df[&#39;treated&#39;], y=df[&#39;outcome&#39;])
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;(array([2.05252873]), array([1.86063479]), array([2.24442267]))
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;x-learner&#34;&gt;X Learner&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from causalml.inference.meta import BaseXLearner

BaseXLearner(learner=LGBMRegressor()).estimate_ate(X=df[X], treatment=df[&#39;treated&#39;], y=df[&#39;outcome&#39;])
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;(array([2.0255193]), array([1.83456903]), array([2.21646956]))
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;dr-learner&#34;&gt;DR Learner&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from causalml.inference.meta import BaseDRLearner

BaseDRLearner(learner=LGBMRegressor()).estimate_ate(X=df[X], treatment=df[&#39;treated&#39;], y=df[&#39;outcome&#39;])
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;(array([2.08923293]), array([1.8921804]), array([2.28628547]))
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;compare-all&#34;&gt;Compare All&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def simulate(dgp, K=100):
    
    # Initialize coefficients
    results = pd.DataFrame(columns=[&#39;k&#39;, &#39;Estimator&#39;, &#39;Estimate&#39;])
    names = [&#39;1. Diff &#39;, &#39;2. Reg  &#39;, &#39;3. Match&#39;, &#39;4. IPW  &#39;, 
             &#39;5. R lrn&#39;, &#39;6. S lrn&#39;, &#39;7. T lrn&#39;, &#39;8. X lrn&#39;, &#39;9. DRlrn&#39;]
    
    # Compute coefficients
    for k in range(K):
        print(f&amp;quot;Simulation {k}/{K}&amp;quot;, end=&amp;quot;\r&amp;quot;)
        temp = pd.DataFrame({&#39;k&#39;: [k] * len(names), 
                             &#39;Estimator&#39;: names, 
                             &#39;Estimate&#39;: [0] * len(names)})
        
        # Draw data
        df = dgp.generate_data(seed=k)

        # Single diff
        temp[&#39;Estimate&#39;][0] = smf.ols(&#39;outcome ~ treated&#39;, data=df).fit().params[1]
        
        # Regression with controls
        temp[&#39;Estimate&#39;][1] = smf.ols(f&#39;outcome ~ treated + male + age + income&#39;, data=df).fit().params[1]
        
        # Matching
        psm = NearestNeighborMatch(replace=True, ratio=1)
        df_matched = psm.match(data=df, treatment_col=&amp;quot;treated&amp;quot;, score_cols=X)
        temp[&#39;Estimate&#39;][2] = smf.ols(&#39;outcome ~ treated&#39;, data=df_matched).fit().params[1]
        
        # IPW
        df[&amp;quot;pscore&amp;quot;] = smf.logit(&amp;quot;np.rint(treated) ~ male + age + income&amp;quot;, data=df).fit(disp=False).predict()
        w = 1 / (df[&amp;quot;pscore&amp;quot;] * df[&amp;quot;treated&amp;quot;] + (1-df[&amp;quot;pscore&amp;quot;]) * (1-df[&amp;quot;treated&amp;quot;]))
        temp[&#39;Estimate&#39;][3] = smf.wls(&amp;quot;outcome ~ treated&amp;quot;, weights=w, data=df).fit().params[1]
                
        # R Learner
        temp[&#39;Estimate&#39;][4] = BaseRLearner(learner=LGBMRegressor()).estimate_ate(X=df[X], treatment=df[&#39;treated&#39;], y=df[&#39;outcome&#39;])[0][0]
        
        # S Learner
        temp[&#39;Estimate&#39;][5] = BaseSLearner(learner=LGBMRegressor()).estimate_ate(X=df[X], treatment=df[&#39;treated&#39;], y=df[&#39;outcome&#39;])[0]
        
        # T Learner
        temp[&#39;Estimate&#39;][6] = BaseTLearner(learner=LGBMRegressor()).estimate_ate(X=df[X], treatment=df[&#39;treated&#39;], y=df[&#39;outcome&#39;])[0][0]
        
        # X Learner
        temp[&#39;Estimate&#39;][7] = BaseXLearner(learner=LGBMRegressor()).estimate_ate(X=df[X], treatment=df[&#39;treated&#39;], y=df[&#39;outcome&#39;])[0][0]
        
        # DR Learner
        temp[&#39;Estimate&#39;][8] = BaseDRLearner(learner=LGBMRegressor()).estimate_ate(X=df[X], treatment=df[&#39;treated&#39;], y=df[&#39;outcome&#39;])[0][0]
        
        # Combine estimates
        results = pd.concat((results, temp))
    
    return results.reset_index(drop=True)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;results = simulate(dgp=dgp_compare())
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Simulation 99/100
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s plot the distribution of the estimated parameters.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;p = sns.kdeplot(data=results, x=&amp;quot;Estimate&amp;quot;, hue=&amp;quot;Estimator&amp;quot;, legend=True);
sns.move_legend(p, &amp;quot;upper left&amp;quot;, bbox_to_anchor=(1.05, 0.8))
plt.axvline(x=2, c=&#39;k&#39;, ls=&#39;--&#39;);
plt.title(&#39;Simulated Distributions&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/compare_27_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;We can also tabulate the simulated mean and standard deviation of each estimator.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;results.groupby(&#39;Estimator&#39;).agg(mean=(&amp;quot;Estimate&amp;quot;, &amp;quot;mean&amp;quot;), std=(&amp;quot;Estimate&amp;quot;, &amp;quot;std&amp;quot;))
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;mean&lt;/th&gt;
      &lt;th&gt;std&lt;/th&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;Estimator&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;1. Diff&lt;/th&gt;
      &lt;td&gt;-0.054010&lt;/td&gt;
      &lt;td&gt;0.120718&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2. Reg&lt;/th&gt;
      &lt;td&gt;2.190934&lt;/td&gt;
      &lt;td&gt;0.126232&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3. Match&lt;/th&gt;
      &lt;td&gt;1.575239&lt;/td&gt;
      &lt;td&gt;0.211772&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4. IPW&lt;/th&gt;
      &lt;td&gt;1.596031&lt;/td&gt;
      &lt;td&gt;0.119129&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;5. R lrn&lt;/th&gt;
      &lt;td&gt;1.649703&lt;/td&gt;
      &lt;td&gt;0.263321&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;6. S lrn&lt;/th&gt;
      &lt;td&gt;1.963797&lt;/td&gt;
      &lt;td&gt;0.129638&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;7. T lrn&lt;/th&gt;
      &lt;td&gt;1.891307&lt;/td&gt;
      &lt;td&gt;0.160480&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;8. X lrn&lt;/th&gt;
      &lt;td&gt;1.975699&lt;/td&gt;
      &lt;td&gt;0.161004&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;9. DRlrn&lt;/th&gt;
      &lt;td&gt;1.876465&lt;/td&gt;
      &lt;td&gt;0.173318&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;The only unbiased estimators seems to be the S-learner and the X-learner, followed by T-learner, DR-learner and linear regression.&lt;/p&gt;
&lt;p&gt;he S-learner however is more efficient.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>How to Compare Two or More Distributions</title>
      <link>https://matteocourthoud.github.io/post/distr/</link>
      <pubDate>Wed, 22 Jun 2022 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/post/distr/</guid>
      <description>&lt;p&gt;&lt;em&gt;A complete guide to comparing distributions, from visualization to statistical tests&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Comparing the empirical distribution of a variable across different groups is a common problem in data science. In particular, in causal inference the problem often arises when we have to &lt;strong&gt;assess the quality of randomization&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;When we want to assess the causal effect of a policy (or UX feature, ad campaign, drug, &amp;hellip;), the golden standard in causal inference are &lt;a href=&#34;https://en.wikipedia.org/wiki/Randomized_controlled_trial&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;randomized control trials&lt;/strong&gt;&lt;/a&gt;, also known as &lt;a href=&#34;https://de.wikipedia.org/wiki/A/B-Test&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;A/B tests&lt;/strong&gt;&lt;/a&gt;. In practice, we select a sample for the study and we randomly split it into a &lt;strong&gt;control&lt;/strong&gt; and a &lt;strong&gt;treatment&lt;/strong&gt; group, and we compare the outcomes between the two groups. Randomization ensures that only difference between the two groups is the treatment, on average, so that we can attribute outcome differences to the treatment effect.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;problem&lt;/strong&gt; is that, despite randomization, the two groups are never identical. However, sometimes, they are not even &amp;ldquo;similar&amp;rdquo;. For example, we might have more males in one group, or older people, etc.. (we usually call these characteristics, &lt;em&gt;covariates&lt;/em&gt; or &lt;em&gt;control variables&lt;/em&gt;). When it happens, we cannot be certain anymore that the difference in the outcome is only due to the treatment and cannot be attributed to the &lt;strong&gt;inbalanced covariates&lt;/strong&gt; instead. Therefore, it is always important, after randomization, to check whether all observed variables are balanced across groups and whether there are no systematic differences. Another option, to be certain ex-ante that certain covariates are balanced, is &lt;a href=&#34;https://en.wikipedia.org/wiki/Stratified_sampling&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;stratified sampling&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;In this blog post, we are going to see different ways to compare two (or more) distributions and assess the magnitude and significance of their difference. We are going to consider two different approaches, &lt;strong&gt;visual&lt;/strong&gt; and &lt;strong&gt;statistical&lt;/strong&gt;. The two approaches generally trade-off &lt;strong&gt;intuition&lt;/strong&gt; with &lt;strong&gt;rigor&lt;/strong&gt;: from plots we can quickly assess and explore differences, but it&amp;rsquo;s hard to tell whether these differences are systematic or due to noise.&lt;/p&gt;
&lt;h2 id=&#34;example&#34;&gt;Example&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s assume we need to perform an &lt;strong&gt;experiment&lt;/strong&gt; on a group of individuals and we have randomized them into a &lt;strong&gt;treatment and control&lt;/strong&gt; group. We would like them to be &lt;strong&gt;as comparable as possible&lt;/strong&gt;, in order to attribute any difference between the two groups to the treatment effect alone. We also have divided the treatment group in different &lt;em&gt;arms&lt;/em&gt; for testing different treatments (e.g. slight variations of the same drug).&lt;/p&gt;
&lt;p&gt;For this example, I have simulated a dataset of 1000 individuals, for whom we observe a set of characteristics. I import the data generating process &lt;code&gt;dgp_rnd_assignment()&lt;/code&gt; from &lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/src/dgp.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;src.dgp&lt;/code&gt;&lt;/a&gt; and some plotting functions and libraries from &lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/src/utils.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;src.utils&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;%matplotlib inline
%config InlineBackend.figure_format = &#39;retina&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from src.utils import *
from src.dgp import dgp_rnd_assignment

df = dgp_rnd_assignment().generate_data()
df.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;Group&lt;/th&gt;
      &lt;th&gt;Arm&lt;/th&gt;
      &lt;th&gt;Gender&lt;/th&gt;
      &lt;th&gt;Age&lt;/th&gt;
      &lt;th&gt;Income&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;control&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;29.0&lt;/td&gt;
      &lt;td&gt;568.44&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;control&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;32.0&lt;/td&gt;
      &lt;td&gt;596.45&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;treatment&lt;/td&gt;
      &lt;td&gt;arm 3&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;29.0&lt;/td&gt;
      &lt;td&gt;380.86&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;control&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;25.0&lt;/td&gt;
      &lt;td&gt;476.38&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;treatment&lt;/td&gt;
      &lt;td&gt;arm 4&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;32.0&lt;/td&gt;
      &lt;td&gt;628.28&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;We have information on $1000$ individuals, for which we observe &lt;code&gt;gender&lt;/code&gt;, &lt;code&gt;age&lt;/code&gt; and weekly &lt;code&gt;income&lt;/code&gt;. Each individual is assigned either to the treatment or control &lt;code&gt;group&lt;/code&gt; and treated individuals are distributed across four treatment &lt;code&gt;arms&lt;/code&gt;.&lt;/p&gt;
&lt;h2 id=&#34;two-groups---plots&#34;&gt;Two Groups - Plots&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s start with the simplest setting: we want to compare the distribution of income across the &lt;code&gt;treatment&lt;/code&gt; and &lt;code&gt;control&lt;/code&gt; group. We first explore &lt;strong&gt;visual&lt;/strong&gt; approaches and the &lt;strong&gt;statistical&lt;/strong&gt; approaches. The advantage of the first is &lt;strong&gt;intuition&lt;/strong&gt; while the advantage of the second is &lt;strong&gt;rigor&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;For most visualizations I am going to use Python&amp;rsquo;s &lt;a href=&#34;https://seaborn.pydata.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;seaborn&lt;/code&gt;&lt;/a&gt; library.&lt;/p&gt;
&lt;h3 id=&#34;boxplot&#34;&gt;Boxplot&lt;/h3&gt;
&lt;p&gt;A first visual approach is the &lt;a href=&#34;https://en.wikipedia.org/wiki/Box_plot&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;boxplot&lt;/strong&gt;&lt;/a&gt;. The boxplot is a good trade-off between summary statistics and data visualization. The center of the &lt;strong&gt;box&lt;/strong&gt; represents the &lt;em&gt;median&lt;/em&gt; while the borders represent the first (Q1) and third &lt;a href=&#34;https://en.wikipedia.org/wiki/Quartile&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;quartile&lt;/a&gt; (Q3), respectively. The &lt;strong&gt;whiskers&lt;/strong&gt; instead, extend to the first data points that are more than 1.5 times the &lt;em&gt;interquartile range&lt;/em&gt; (Q3 - Q1) outside the box. The points that fall outside of the whiskers are plotted individually and are usually considered &lt;a href=&#34;https://en.wikipedia.org/wiki/Outlier&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;outliers&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Therefore, the boxplot provides both summary statistics (the box and the whiskers) and direct data visualization (the outliers).&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sns.boxplot(data=df, x=&#39;Group&#39;, y=&#39;Income&#39;);
plt.title(&amp;quot;Boxplot&amp;quot;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/distr_12_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;It seems that the &lt;code&gt;income&lt;/code&gt; distribution in the &lt;code&gt;treatment&lt;/code&gt; group is slightly more dispersed: the orange box is larger and its whiskers cover a wider range. However, the &lt;strong&gt;issue&lt;/strong&gt; with the boxplot is that it hides the shape of the data, telling us some summary statistics but not showing us the actual data distribution.&lt;/p&gt;
&lt;h3 id=&#34;histogram&#34;&gt;Histogram&lt;/h3&gt;
&lt;p&gt;The most intuitive way to plot a distribution is the &lt;strong&gt;histogram&lt;/strong&gt;. The histogram groups the data into equally wide &lt;strong&gt;bins&lt;/strong&gt; and plots the number of observations within each bin.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sns.histplot(data=df, x=&#39;Income&#39;, hue=&#39;Group&#39;, bins=50);
plt.title(&amp;quot;Histogram&amp;quot;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/distr_16_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;There are multiple &lt;strong&gt;issues&lt;/strong&gt; with this plot:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Since the two groups have a different number of observations, the two histograms are not comparable&lt;/li&gt;
&lt;li&gt;The number of bins is arbitrary&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We can solve the first issue using the &lt;code&gt;stat&lt;/code&gt; option to plot the &lt;code&gt;density&lt;/code&gt; instead of the count and setting the &lt;code&gt;common_norm&lt;/code&gt; option to &lt;code&gt;False&lt;/code&gt; to use the same normalization.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sns.histplot(data=df, x=&#39;Income&#39;, hue=&#39;Group&#39;, bins=50, stat=&#39;density&#39;, common_norm=False);
plt.title(&amp;quot;Density Histogram&amp;quot;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/distr_18_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Now the two histograms are comparable!&lt;/p&gt;
&lt;p&gt;However, an important &lt;strong&gt;issue&lt;/strong&gt; remains: the size of the bins is arbitrary. In the extreme, if we bunch the data less, we end up with bins with at most one observation, if we bunch the data more, we end up with a single bin. In both cases, if we exaggerate, the plot loses informativeness. This is a classical &lt;a href=&#34;https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;bias-variance trade-off&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;kernel-density&#34;&gt;Kernel Density&lt;/h3&gt;
&lt;p&gt;One possible solution is to use a &lt;a href=&#34;https://en.wikipedia.org/wiki/Kernel_density_estimation&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;kernel density function&lt;/strong&gt;&lt;/a&gt; that tries to approximate the histogram with a continuous function, using &lt;a href=&#34;https://en.wikipedia.org/wiki/Kernel_density_estimation&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;kernel density estimation (KDE)&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sns.kdeplot(x=&#39;Income&#39;, data=df, hue=&#39;Group&#39;, common_norm=False);
plt.title(&amp;quot;Kernel Density Function&amp;quot;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/distr_22_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;From the plot, it seems that the estimated kernel density of &lt;code&gt;income&lt;/code&gt; has &amp;ldquo;fatter tails&amp;rdquo; (i.e. higher variance) in the &lt;code&gt;treatment&lt;/code&gt; group, while the average seems similar across groups.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;issue&lt;/strong&gt; with kernel density estimation is that it is a bit of a  black-box and might mask relevant features of the data.&lt;/p&gt;
&lt;h3 id=&#34;cumulative-distribution&#34;&gt;Cumulative Distribution&lt;/h3&gt;
&lt;p&gt;A more transparent representation of the two distribution is their &lt;a href=&#34;https://en.wikipedia.org/wiki/Cumulative_distribution_function&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;cumulative distribution function&lt;/strong&gt;&lt;/a&gt;. At each point of the x axis (&lt;code&gt;income&lt;/code&gt;) we plot the percentage of data points that have an equal or lower value. The main &lt;strong&gt;advantages&lt;/strong&gt; of the cumulative distribution function are that&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;we do not need to make any arbitrary choice (e.g. number of bins)&lt;/li&gt;
&lt;li&gt;we do not need to perform any approximation (e.g. with KDE), but we represent all data points&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sns.histplot(x=&#39;Income&#39;, data=df, hue=&#39;Group&#39;, bins=len(df), stat=&amp;quot;density&amp;quot;,
             element=&amp;quot;step&amp;quot;, fill=False, cumulative=True, common_norm=False);
plt.title(&amp;quot;Cumulative distribution function&amp;quot;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/distr_26_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;How should we interpret the graph?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Since the two lines cross more or less at 0.5 (y axis), it means that their median is similar&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Since the orange line is above the blue line on the left and below the blue line on the left, it means that the distribution of the &lt;code&gt;treatment&lt;/code&gt; group as fatter tails&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;qq-plot&#34;&gt;QQ Plot&lt;/h3&gt;
&lt;p&gt;A related method is the &lt;strong&gt;qq-plot&lt;/strong&gt;, where &lt;em&gt;q&lt;/em&gt; stands for quantile. The qq-plot plots the quantiles of the two distributions against each other. If the distributions are the same, we should get a 45 degree line.&lt;/p&gt;
&lt;p&gt;There is no native qq-plot function in Python and, while the &lt;code&gt;statsmodels&lt;/code&gt; package provides a &lt;a href=&#34;https://www.statsmodels.org/dev/generated/statsmodels.graphics.gofplots.qqplot.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;qqplot&lt;/code&gt; function&lt;/a&gt;, it is quite cumbersome. Therefore, we will do it by hand.&lt;/p&gt;
&lt;p&gt;First, we need to compute the quartiles of the two groups, using the &lt;code&gt;percentile&lt;/code&gt; function.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;income = df[&#39;Income&#39;].values
income_t = df.loc[df.Group==&#39;treatment&#39;, &#39;Income&#39;].values
income_c = df.loc[df.Group==&#39;control&#39;, &#39;Income&#39;].values

df_pct = pd.DataFrame()
df_pct[&#39;q_treatment&#39;] = np.percentile(income_t, range(100))
df_pct[&#39;q_control&#39;] = np.percentile(income_c, range(100))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we can plot the two quantile distributions against each other, plus the 45-degree line, representing the benchmark perfect fit.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plt.figure(figsize=(8, 8))
plt.scatter(x=&#39;q_control&#39;, y=&#39;q_treatment&#39;, data=df_pct, label=&#39;Actual fit&#39;);
sns.lineplot(x=&#39;q_control&#39;, y=&#39;q_control&#39;, data=df_pct, color=&#39;r&#39;, label=&#39;Line of perfect fit&#39;);
plt.xlabel(&#39;Quantile of income, control group&#39;)
plt.ylabel(&#39;Quantile of income, treatment group&#39;)
plt.legend()
plt.title(&amp;quot;QQ plot&amp;quot;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/distr_32_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;The qq-plot delivers a very &lt;strong&gt;similar insight&lt;/strong&gt; with respect to the cumulative distribution plot: income in the treatment group has the same median (lines cross in the center) but wider tails (dots are below the line on the left end and above on the right end).&lt;/p&gt;
&lt;h2 id=&#34;two-groups---tests&#34;&gt;Two Groups - Tests&lt;/h2&gt;
&lt;p&gt;So far, we have seen different ways to &lt;em&gt;visualize&lt;/em&gt; differences between distributions. The main advantage of visualization is &lt;strong&gt;intuition&lt;/strong&gt;: we can eyeball the differences and intuitively assess them.&lt;/p&gt;
&lt;p&gt;However, we might want to be more &lt;strong&gt;rigorous&lt;/strong&gt; and try to assess the &lt;strong&gt;statistical significance&lt;/strong&gt; of the difference between the distributions, i.e. answer the question &amp;ldquo;&lt;em&gt;is the observed difference systematic or due to sampling noise?&lt;/em&gt;&amp;rdquo;.&lt;/p&gt;
&lt;p&gt;We are now going to analyze different tests to discern two distributions from each other.&lt;/p&gt;
&lt;h3 id=&#34;t-test&#34;&gt;T-test&lt;/h3&gt;
&lt;p&gt;The first and most common test is the &lt;a href=&#34;https://en.wikipedia.org/wiki/Student%27s_t-test&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;student t-test&lt;/a&gt;. T-tests are generally used to &lt;strong&gt;compare means&lt;/strong&gt;. In this case, we want to test whether the means of the &lt;code&gt;income&lt;/code&gt; distribution is the same across the two groups. The test statistic for the two-means comparison test is given by:&lt;/p&gt;
&lt;p&gt;$$
stat = \frac{|\bar x_1 - \bar x_2|}{\sqrt{s^2 / n }}
$$&lt;/p&gt;
&lt;p&gt;Where $\bar x$ is the sample mean and $s$ is the sample standard deviation. Under mild conditions, the test statistic is asymptotically distributed as a &lt;a href=&#34;https://en.wikipedia.org/wiki/Student%27s_t-distribution&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;student t&lt;/a&gt; distribution.&lt;/p&gt;
&lt;p&gt;We use the &lt;code&gt;ttest_ind&lt;/code&gt; function from &lt;code&gt;scipy&lt;/code&gt; to perform the t-test. The function returns both the test statistic and the implied &lt;a href=&#34;https://en.wikipedia.org/wiki/P-value&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;p-value&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from scipy.stats import ttest_ind

stat, p_value = ttest_ind(income_c, income_t)
print(f&amp;quot;t-test: statistic={stat:.4f}, p-value={p_value:.4f}&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;t-test: statistic=-1.5549, p-value=0.1203
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The p-value of the test is $0.12$, therefore we do &lt;strong&gt;not reject&lt;/strong&gt; the null hypothesis of no difference in &lt;em&gt;means&lt;/em&gt; across treatment and control groups.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: the t-test assumes that the variance in the two samples is the same so that its estimate is computed on the joint sample. &lt;a href=&#34;https://en.wikipedia.org/wiki/Welch%27s_t-test&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Welch’s t-test&lt;/a&gt; allows for unequal variances in the two samples.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;standardized-mean-difference-smd&#34;&gt;Standardized Mean Difference (SMD)&lt;/h3&gt;
&lt;p&gt;In general, it is good practice to always perform a test for difference in means on &lt;strong&gt;all variables&lt;/strong&gt; across the treatment and control group, when we are running a randomized control trial or A/B test.&lt;/p&gt;
&lt;p&gt;However, since the denominator of the t-test statistic depends on the sample size, the t-test has been &lt;strong&gt;criticized&lt;/strong&gt; for making p-values hard to compare across studies. In fact, we may obtain a significant result in an experiment with very small magnitude of difference but large sample size while we may obtain a non-significant result in an experiment with large magnitude of difference but small sample size.&lt;/p&gt;
&lt;p&gt;One solution that has been proposed is the &lt;strong&gt;standardized mean difference (SMD)&lt;/strong&gt;. As the name suggests, this is not a proper test statistic, but just a standardized difference, which can be computed as:&lt;/p&gt;
&lt;p&gt;$$
SMD = \frac{|\bar x_1 - \bar x_2|}{\sqrt{(s^2_1 + s^2_2) / 2}}
$$&lt;/p&gt;
&lt;p&gt;Usually a value below $0.1$ is considered a &amp;ldquo;small&amp;rdquo; difference.&lt;/p&gt;
&lt;p&gt;It is good practice to collect average values of all variables across treatment and control group and a measure of distance between the two — either the t-test or the SMD — into a table that is called &lt;strong&gt;balance table&lt;/strong&gt;. We can use the &lt;a href=&#34;https://causalml.readthedocs.io/en/latest/causalml.html#module-causalml.match&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;create_table_one&lt;/code&gt;&lt;/a&gt; function from the &lt;a href=&#34;https://causalml.readthedocs.io/en/latest/about.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;causalml&lt;/code&gt;&lt;/a&gt; library to generate it. As the name of the function suggests, the balance table should always be the &lt;strong&gt;first table&lt;/strong&gt; you present when performing an A/B test.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from causalml.match import create_table_one

df[&#39;treatment&#39;] = df[&#39;Group&#39;]==&#39;treatment&#39;
create_table_one(df, &#39;treatment&#39;, [&#39;Gender&#39;, &#39;Age&#39;, &#39;Income&#39;])
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;Control&lt;/th&gt;
      &lt;th&gt;Treatment&lt;/th&gt;
      &lt;th&gt;SMD&lt;/th&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;Variable&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;n&lt;/th&gt;
      &lt;td&gt;704&lt;/td&gt;
      &lt;td&gt;296&lt;/td&gt;
      &lt;td&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;Age&lt;/th&gt;
      &lt;td&gt;32.40 (8.54)&lt;/td&gt;
      &lt;td&gt;36.42 (7.76)&lt;/td&gt;
      &lt;td&gt;0.4928&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;Gender&lt;/th&gt;
      &lt;td&gt;0.51 (0.50)&lt;/td&gt;
      &lt;td&gt;0.58 (0.49)&lt;/td&gt;
      &lt;td&gt;0.1419&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;Income&lt;/th&gt;
      &lt;td&gt;524.59 (117.35)&lt;/td&gt;
      &lt;td&gt;538.75 (160.15)&lt;/td&gt;
      &lt;td&gt;0.1009&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;In the first two columns, we can see the average of the different variables across the treatment and control groups, with standard errors in parenthesis. In the &lt;strong&gt;last column&lt;/strong&gt;, the values of the SMD indicate a standardized difference of more than $0.1$ for all variables, suggesting that the two groups are probably different.&lt;/p&gt;
&lt;h3 id=&#34;mannwhitney-u-test&#34;&gt;Mann–Whitney U Test&lt;/h3&gt;
&lt;p&gt;An alternative test is the &lt;a href=&#34;https://en.wikipedia.org/wiki/Mann%E2%80%93Whitney_U_test&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Mann–Whitney U test&lt;/a&gt;. The null hypothesis for this test is that the two groups have the same distribution, while the alternative hypothesis is that one group has larger (or smaller) values than the other.&lt;/p&gt;
&lt;p&gt;Differently from the other tests we have seen so far, the Mann–Whitney U test is agnostic to outliers and concentrates on the center of the distribution.&lt;/p&gt;
&lt;p&gt;The test &lt;strong&gt;procedure&lt;/strong&gt; is the following.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Combine all data points and rank them (in increasing or decreasing order)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Compute $U_1 = R_1 - n_1(n_1 + 1)/2$, where $R_1$ is the sum of the ranks for data points in the first group and $n_1$ is the number of points in the first group.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Compute $U_2$ similarly for the second group.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The test statistic is given by $stat = min(U_1, U_2)$.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Under the null hypothesis of no systematic rank differences between the two distributions (i.e. same median), the test statistic is asymptotically normally distributed with known mean and variance.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;intuition&lt;/strong&gt; behind the computation of $R$ and $U$ is the following: if the values in the first sample were all bigger than the values in the second sample, then $R_1 = n_1(n_1 + 1)/2$ and, as a consequence, $U_1$ would then be zero (minimum attainable value). Otherwise, if the two samples were similar, $U_1$ and $U_2$ would be very close to $n_1 n_2 / 2$ (maximum attainable value).&lt;/p&gt;
&lt;p&gt;We perform the test using the &lt;code&gt;mannwhitneyu&lt;/code&gt; function from &lt;code&gt;scipy&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from scipy.stats import mannwhitneyu

stat, p_value = mannwhitneyu(income_t, income_c)
print(f&amp;quot; Mann–Whitney U Test: statistic={stat:.4f}, p-value={p_value:.4f}&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt; Mann–Whitney U Test: statistic=106371.5000, p-value=0.6012
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We get a p-value of 0.6 which implies that we do &lt;strong&gt;not reject&lt;/strong&gt; the null hypothesis of no difference between the two distributions.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: as for the t-test, there exists a version of the Mann–Whitney U test for unequal variances in the two samples, the &lt;a href=&#34;https://www.statisticshowto.com/brunner-munzel-test-generalized-wilcoxon-test/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Brunner-Munzel test&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;permutation-tests&#34;&gt;Permutation Tests&lt;/h3&gt;
&lt;p&gt;A non-parametric alternative is permutation testing. The idea is that, under the null hypothesis, the two distributions should be the same, therefore &lt;strong&gt;shuffling&lt;/strong&gt; the &lt;code&gt;group&lt;/code&gt; labels should not significantly alter any statistic.&lt;/p&gt;
&lt;p&gt;We can chose any statistic and check how its value in the original sample compares with its distribution across &lt;code&gt;group&lt;/code&gt; label permutations. For example, let&amp;rsquo;s use as a test statistic the &lt;strong&gt;difference of sample means&lt;/strong&gt; between the treatment and control group.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sample_stat = np.mean(income_t) - np.mean(income_c)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;stats = np.zeros(1000)
for k in range(1000):
    labels = np.random.permutation((df[&#39;Group&#39;] == &#39;treatment&#39;).values)
    stats[k] = np.mean(income[labels]) - np.mean(income[labels==False])
p_value = np.mean(stats &amp;gt; sample_stat)

print(f&amp;quot;Permutation test: p-value={p_value:.4f}&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Permutation test: p-value=0.0530
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The permutation test gives us a p-value of $0.056$, implying a weak &lt;strong&gt;non-rejection&lt;/strong&gt; of the null hypothesis at the 5% level.&lt;/p&gt;
&lt;p&gt;How do we &lt;strong&gt;interpret&lt;/strong&gt; the p-value? It means that the difference in means in the data is larger than $1 - 0.0560 = 94.4%$ of the differences in means across the permuted samples.&lt;/p&gt;
&lt;p&gt;We can &lt;strong&gt;visualize&lt;/strong&gt; the test, by plotting the distribution of the test statistic across permutations against its sample value.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plt.hist(stats, label=&#39;Permutation Statistics&#39;, bins=30);
plt.axvline(x=sample_stat, c=&#39;r&#39;, ls=&#39;--&#39;, label=&#39;Sample Statistic&#39;);
plt.legend();
plt.xlabel(&#39;Income difference between treatment and control group&#39;)
plt.title(&#39;Permutation Test&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/distr_55_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;As we can see, the sample statistic is quite extreme with respect to the values in the permuted samples, but not excessively.&lt;/p&gt;
&lt;h3 id=&#34;chi-squared-test&#34;&gt;Chi-Squared Test&lt;/h3&gt;
&lt;p&gt;The &lt;a href=&#34;https://matteocourthoud.github.io/post/chisquared/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;chi-squared test&lt;/a&gt; is a very powerful test that is mostly used to test differences in frequencies.&lt;/p&gt;
&lt;p&gt;One of the &lt;strong&gt;least known applications&lt;/strong&gt; of the chi-squared test, is testing the similarity between two distributions. The &lt;strong&gt;idea&lt;/strong&gt; is to bin the observations of the two groups. If the two distributions were the same, we would expect the same frequency of observations in each bin. Importantly, we need enough observations in each bin, in order for the test to be valid.&lt;/p&gt;
&lt;p&gt;I generate bins corresponding to deciles of the distribution of &lt;code&gt;income&lt;/code&gt; in the &lt;em&gt;control&lt;/em&gt; group and then I compute the expected number of observations in each bin in the &lt;em&gt;treatment&lt;/em&gt; group, if the two distributions were the same.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Init dataframe
df_bins = pd.DataFrame()

# Generate bins from control group
_, bins = pd.qcut(income_c, q=10, retbins=True)
df_bins[&#39;bin&#39;] = pd.cut(income_c, bins=bins).value_counts().index

# Apply bins to both groups
df_bins[&#39;income_c_observed&#39;] = pd.cut(income_c, bins=bins).value_counts().values
df_bins[&#39;income_t_observed&#39;] = pd.cut(income_t, bins=bins).value_counts().values

# Compute expected frequency in the treatment group
df_bins[&#39;income_t_expected&#39;] = df_bins[&#39;income_c_observed&#39;] / np.sum(df_bins[&#39;income_c_observed&#39;]) * np.sum(df_bins[&#39;income_t_observed&#39;])

df_bins
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;bin&lt;/th&gt;
      &lt;th&gt;income_c_observed&lt;/th&gt;
      &lt;th&gt;income_t_observed&lt;/th&gt;
      &lt;th&gt;income_t_expected&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;(232.26, 380.496]&lt;/td&gt;
      &lt;td&gt;70&lt;/td&gt;
      &lt;td&gt;46&lt;/td&gt;
      &lt;td&gt;29.075391&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;(380.496, 425.324]&lt;/td&gt;
      &lt;td&gt;70&lt;/td&gt;
      &lt;td&gt;30&lt;/td&gt;
      &lt;td&gt;29.075391&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;(425.324, 456.795]&lt;/td&gt;
      &lt;td&gt;70&lt;/td&gt;
      &lt;td&gt;24&lt;/td&gt;
      &lt;td&gt;29.075391&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;(456.795, 488.83]&lt;/td&gt;
      &lt;td&gt;71&lt;/td&gt;
      &lt;td&gt;26&lt;/td&gt;
      &lt;td&gt;29.490754&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;(488.83, 513.66]&lt;/td&gt;
      &lt;td&gt;70&lt;/td&gt;
      &lt;td&gt;18&lt;/td&gt;
      &lt;td&gt;29.075391&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;5&lt;/th&gt;
      &lt;td&gt;(513.66, 540.048]&lt;/td&gt;
      &lt;td&gt;70&lt;/td&gt;
      &lt;td&gt;19&lt;/td&gt;
      &lt;td&gt;29.075391&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;6&lt;/th&gt;
      &lt;td&gt;(540.048, 576.664]&lt;/td&gt;
      &lt;td&gt;71&lt;/td&gt;
      &lt;td&gt;21&lt;/td&gt;
      &lt;td&gt;29.490754&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;7&lt;/th&gt;
      &lt;td&gt;(576.664, 621.022]&lt;/td&gt;
      &lt;td&gt;70&lt;/td&gt;
      &lt;td&gt;25&lt;/td&gt;
      &lt;td&gt;29.075391&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;8&lt;/th&gt;
      &lt;td&gt;(621.022, 682.003]&lt;/td&gt;
      &lt;td&gt;70&lt;/td&gt;
      &lt;td&gt;42&lt;/td&gt;
      &lt;td&gt;29.075391&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;9&lt;/th&gt;
      &lt;td&gt;(682.003, 973.46]&lt;/td&gt;
      &lt;td&gt;71&lt;/td&gt;
      &lt;td&gt;41&lt;/td&gt;
      &lt;td&gt;29.490754&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;We can now perform the test by comparing the expected (E) and observed (O) number of observations in the treatment group, across bins. The test statistic is given by&lt;/p&gt;
&lt;p&gt;$$
stat = \sum _{i=1}^{n} \frac{(O_i - E_i)^{2}}{E_i}
$$&lt;/p&gt;
&lt;p&gt;where the bins are indexed by $i$ and $O$ is the observed number of data points in bin $i$ and $E$ is the expected number of data points in bin $i$. Since we generated the bins using deciles of the distribution of &lt;code&gt;income&lt;/code&gt; in the control group, we expect the number of observations per bin in the treatment group to be the same across bins. The test statistic is asymptocally distributed as a &lt;a href=&#34;https://en.wikipedia.org/wiki/Chi-squared_distribution&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;chi-squared&lt;/a&gt; distribution.&lt;/p&gt;
&lt;p&gt;To compute the test statistic and the p-value of the test, we use the &lt;code&gt;chisquare&lt;/code&gt; function from &lt;code&gt;scipy&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from scipy.stats import chisquare

stat, p_value = chisquare(df_bins[&#39;income_t_observed&#39;], df_bins[&#39;income_t_expected&#39;])
print(f&amp;quot;Chi-squared Test: statistic={stat:.4f}, p-value={p_value:.4f}&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Chi-squared Test: statistic=32.1432, p-value=0.0002
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Differently from all other tests so far, the chi-squared test &lt;strong&gt;strongly rejects&lt;/strong&gt; the null hypothesis that the two distributions are the same. Why?&lt;/p&gt;
&lt;p&gt;The reason lies in the fact that the two distributions have a similar center but different tails and the chi-squared test tests the similarity along the &lt;strong&gt;whole distribution&lt;/strong&gt; and not only in the center, as we were doing with the previous tests.&lt;/p&gt;
&lt;p&gt;This result tells a &lt;strong&gt;cautionary tale&lt;/strong&gt;: it is very important to understand &lt;em&gt;what&lt;/em&gt; you are actually testing before drawing blind conclusions from a p-value!&lt;/p&gt;
&lt;h3 id=&#34;kolmogorov-smirnov-test&#34;&gt;Kolmogorov-Smirnov Test&lt;/h3&gt;
&lt;p&gt;The idea of the &lt;a href=&#34;https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Kolmogorov-Smirnov test&lt;/a&gt;, is to &lt;strong&gt;compare the cumulative distributions&lt;/strong&gt; of the two groups. In particular, the Kolmogorov-Smirnov test statistic is the maximum absolute difference between the two cumulative distributions.&lt;/p&gt;
&lt;p&gt;$$
stat = \sup _{x} \ \Big| \ F_1(x) - F_2(x) \ \Big|
$$&lt;/p&gt;
&lt;p&gt;Where $F_1$ and $F_2$ are the two cumulative distribution functions and $x$ are the values of the underlying variable. The asymptotic distribution of the Kolmogorov-Smirnov test statistic is &lt;a href=&#34;https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test#Kolmogorov_distribution&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Kolmogorov distributed&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;To better understand the test, let&amp;rsquo;s plot the cumulative distribution functions and the test statistic. First, we compute the cumulative distribution functions.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_ks = pd.DataFrame()
df_ks[&#39;Income&#39;] = np.sort(df[&#39;Income&#39;].unique())
df_ks[&#39;F_control&#39;] = df_ks[&#39;Income&#39;].apply(lambda x: np.mean(income_c&amp;lt;=x))
df_ks[&#39;F_treatment&#39;] = df_ks[&#39;Income&#39;].apply(lambda x: np.mean(income_t&amp;lt;=x))
df_ks.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;Income&lt;/th&gt;
      &lt;th&gt;F_control&lt;/th&gt;
      &lt;th&gt;F_treatment&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;216.36&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.003378&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;232.26&lt;/td&gt;
      &lt;td&gt;0.001420&lt;/td&gt;
      &lt;td&gt;0.003378&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;243.15&lt;/td&gt;
      &lt;td&gt;0.001420&lt;/td&gt;
      &lt;td&gt;0.006757&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;259.88&lt;/td&gt;
      &lt;td&gt;0.002841&lt;/td&gt;
      &lt;td&gt;0.006757&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;262.82&lt;/td&gt;
      &lt;td&gt;0.002841&lt;/td&gt;
      &lt;td&gt;0.010135&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;We now need to find the point where the absolute distance between the cumulative distribution functions is largest.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;k = np.argmax( np.abs(df_ks[&#39;F_control&#39;] - df_ks[&#39;F_treatment&#39;]))
ks_stat = np.abs(df_ks[&#39;F_treatment&#39;][k] - df_ks[&#39;F_control&#39;][k])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can visualize the value of the test statistic, by plotting the two cumulative distribution functions and the value of the test statistic.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;y = (df_ks[&#39;F_treatment&#39;][k] + df_ks[&#39;F_control&#39;][k])/2
plt.plot(&#39;Income&#39;, &#39;F_control&#39;, data=df_ks, label=&#39;Control&#39;)
plt.plot(&#39;Income&#39;, &#39;F_treatment&#39;, data=df_ks, label=&#39;Treatment&#39;)
plt.errorbar(x=df_ks[&#39;Income&#39;][k], y=y, yerr=ks_stat/2, color=&#39;k&#39;,
             capsize=5, mew=3, label=f&amp;quot;Test statistic: {ks_stat:.4f}&amp;quot;)
plt.legend(loc=&#39;center right&#39;);
plt.title(&amp;quot;Kolmogorov-Smirnov Test&amp;quot;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/distr_69_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;From the plot, we can see that the value of the test statistic corresponds to the distance between the two cumulative distributions at &lt;code&gt;income&lt;/code&gt;~650. For that value of &lt;code&gt;income&lt;/code&gt;, we have the largest imbalance between the two groups.&lt;/p&gt;
&lt;p&gt;We can now perform the actual test using the &lt;code&gt;kstest&lt;/code&gt; function from &lt;code&gt;scipy&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from scipy.stats import kstest

stat, p_value = kstest(income_t, income_c)
print(f&amp;quot; Kolmogorov-Smirnov Test: statistic={stat:.4f}, p-value={p_value:.4f}&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt; Kolmogorov-Smirnov Test: statistic=0.0974, p-value=0.0355
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The p-value is below 5%: we &lt;strong&gt;reject&lt;/strong&gt; the null hypothesis that the two distributions are the same, with 95% confidence.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Note 1&lt;/strong&gt;: The KS test is too conservative and rejects the null hypothesis too rarely. Lilliefors test corrects this bias using a different distribution for the test statistic, the Lilliefors distribution.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Note 2&lt;/strong&gt;: the KS test uses very little information since it only compares the two cumulative distributions at one point: the one of maximum distance. The &lt;a href=&#34;https://en.wikipedia.org/wiki/Anderson%E2%80%93Darling_test&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Anderson-Darling test&lt;/a&gt; and the &lt;a href=&#34;https://en.wikipedia.org/wiki/Cram%C3%A9r%E2%80%93von_Mises_criterion&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Cramér-von Mises test&lt;/a&gt; instead compare the two distributions along the whole domain, by integration (the difference between the two lies in the weighting of the squared distances).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;multiple-groups---plots&#34;&gt;Multiple Groups - Plots&lt;/h2&gt;
&lt;p&gt;So far we have only considered the case of two groups: treatment and control. But that if we had &lt;strong&gt;multiple groups&lt;/strong&gt;? Some of the methods we have seen above scale well, while others don&amp;rsquo;t.&lt;/p&gt;
&lt;p&gt;As a working example, we are now going to check whether the distribution of &lt;code&gt;income&lt;/code&gt; is the same across treatment &lt;code&gt;arms&lt;/code&gt;.&lt;/p&gt;
&lt;h3 id=&#34;boxplot-1&#34;&gt;Boxplot&lt;/h3&gt;
&lt;p&gt;The &lt;strong&gt;boxplot&lt;/strong&gt; scales very well, when we have a number of groups in the single-digits, since we can put the different boxes side-by-side.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sns.boxplot(x=&#39;Arm&#39;, y=&#39;Income&#39;, data=df.sort_values(&#39;Arm&#39;));
plt.title(&amp;quot;Boxplot, multiple groups&amp;quot;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/distr_78_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;From the plot, it looks like the distribution of &lt;code&gt;income&lt;/code&gt; is different across treatment arms, with higher numbered arms having a higher average income.&lt;/p&gt;
&lt;h3 id=&#34;violin-plot&#34;&gt;Violin Plot&lt;/h3&gt;
&lt;p&gt;A very nice extension of the boxplot that combines summary statistics and kernel density estimation is the  &lt;strong&gt;violinplot&lt;/strong&gt;. The violinplot plots separate densities along the y axis so that they don&amp;rsquo;t overlap. By default, it also adds a miniature boxplot inside.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sns.violinplot(x=&#39;Arm&#39;, y=&#39;Income&#39;, data=df.sort_values(&#39;Arm&#39;));
plt.title(&amp;quot;Violin Plot, multiple groups&amp;quot;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/distr_82_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;As for the boxplot, the violin plot suggests that income is different across treatment arms.&lt;/p&gt;
&lt;h3 id=&#34;ridgeline-plot&#34;&gt;Ridgeline Plot&lt;/h3&gt;
&lt;p&gt;Lastly, the &lt;strong&gt;ridgeline plot&lt;/strong&gt; plots multiple kernel density distributions along the x-axis, making them more intuitive than the violin plot but partially overlapping them. Unfortunately, there is no default ridgeline plot neither in &lt;code&gt;matplotlib&lt;/code&gt; nor in &lt;code&gt;seaborn&lt;/code&gt;. We need to import it from &lt;a href=&#34;https://github.com/leotac/joypy&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;joypy&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from joypy import joyplot

joyplot(df, by=&#39;Arm&#39;, column=&#39;Income&#39;, colormap=sns.color_palette(&amp;quot;crest&amp;quot;, as_cmap=True));
plt.xlabel(&#39;Income&#39;);
plt.title(&amp;quot;Ridgeline Plot, multiple groups&amp;quot;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/distr_86_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Again, the ridgeline plot suggests that higher numbered treatment arms have higher income. From this plot it is also easier to appreciate the different shapes of the distributions.&lt;/p&gt;
&lt;h2 id=&#34;multiple-groups---tests&#34;&gt;Multiple Groups - Tests&lt;/h2&gt;
&lt;p&gt;Lastly, let&amp;rsquo;s consider hypothesis tests to compare multiple groups. For simplicity, we will concentrate on the most popular one: the F-test.&lt;/p&gt;
&lt;h3 id=&#34;f-test&#34;&gt;F-test&lt;/h3&gt;
&lt;p&gt;With multiple groups, the most popular test is the &lt;a href=&#34;https://en.wikipedia.org/wiki/F-test&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;F-test&lt;/strong&gt;&lt;/a&gt;. The F-test compares the variance of a variable across different groups. This analysis is also called &lt;a href=&#34;https://en.wikipedia.org/wiki/Analysis_of_variance&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;analysis of variance, or &lt;strong&gt;ANOVA&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;In practice, the F-test statistic is&lt;/p&gt;
&lt;p&gt;$$
\text{stat} = \frac{\text{between-group variance}}{\text{within-group variance}} = \frac{\sum_{g} \big( \bar x_g - \bar x \big) / (G-1)}{\sum_{g} \sum_{i \in g} \big( \bar x_i - \bar x_g \big) / (N-G)}
$$&lt;/p&gt;
&lt;p&gt;Where $G$ is the number of groups, $N$ is the number of observations, $\bar x$ is the overall mean and $\bar x_g$ is the mean within group $g$. Under the null hypothesis of group independence, the f-statistic is &lt;a href=&#34;https://en.wikipedia.org/wiki/F-distribution&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;F-distributed&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from scipy.stats import f_oneway

income_groups = [df.loc[df[&#39;Arm&#39;]==arm, &#39;Income&#39;].values for arm in df[&#39;Arm&#39;].dropna().unique()]
stat, p_value = f_oneway(*income_groups)
print(f&amp;quot;F Test: statistic={stat:.4f}, p-value={p_value:.4f}&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;F Test: statistic=9.0911, p-value=0.0000
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The test p-value is basically zero, implying a &lt;strong&gt;strong rejection&lt;/strong&gt; of the null hypothesis of no differences in the &lt;code&gt;income&lt;/code&gt; distribution across treatment arms.&lt;/p&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;In this post we have see a ton of different ways to &lt;strong&gt;compare two or more distributions&lt;/strong&gt;, both visually and statistically. This is a primary concern in many applications, but especially in causal inference where we use randomization to make treatment and control group as comparable as possible.&lt;/p&gt;
&lt;p&gt;We have also seen how different methods might be better suited for &lt;strong&gt;different situations&lt;/strong&gt;. Visual methods are great to build intuition, but statistical methods are essential for decision-making, since we need to be able to assess the magnitude and statistical significance of the differences.&lt;/p&gt;
&lt;h3 id=&#34;references&#34;&gt;References&lt;/h3&gt;
&lt;p&gt;[1] Student, &lt;a href=&#34;https://www.jstor.org/stable/2331554&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The Probable Error of a Mean&lt;/a&gt; (1908), &lt;em&gt;Biometrika&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;[2] F. Wilcoxon, &lt;a href=&#34;https://www.jstor.org/stable/3001968&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Individual Comparisons by Ranking Methods&lt;/a&gt; (1945), &lt;em&gt;Biometrics Bulletin&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;[3] B. L. Welch, &lt;a href=&#34;https://academic.oup.com/biomet/article/34/1-2/28/210174&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The generalization of &amp;ldquo;Student&amp;rsquo;s&amp;rdquo; problem when several different population variances are involved&lt;/a&gt; (1947), &lt;em&gt;Biometrika&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;[4] H. B. Mann, D. R. Whitney, &lt;a href=&#34;https://www.jstor.org/stable/2236101&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;On a Test of Whether one of Two Random Variables is Stochastically Larger than the Other&lt;/a&gt; (1947), &lt;em&gt;The Annals of Mathematical Statistics&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;[5] E. Brunner, U. Munzen, &lt;a href=&#34;https://onlinelibrary.wiley.com/doi/abs/10.1002/%28SICI%291521-4036%28200001%2942:1%3C17::AID-BIMJ17%3E3.0.CO;2-U&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The Nonparametric Behrens-Fisher Problem: Asymptotic Theory and a Small-Sample Approximation&lt;/a&gt; (2000), &lt;em&gt;Biometrical Journal&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;[6] A. N. Kolmogorov, &lt;a href=&#34;https://link.springer.com/chapter/10.1007/978-94-011-2260-3_15&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Sulla determinazione empirica di una legge di distribuzione&lt;/a&gt; (1933), &lt;em&gt;Giorn. Ist. Ital. Attuar.&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;[7] H. Cramér, &lt;a href=&#34;https://www.tandfonline.com/doi/abs/10.1080/03461238.1928.10416862&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;On the composition of elementary errors&lt;/a&gt; (1928), &lt;em&gt;Scandinavian Actuarial Journal&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;[8] R. von Mises, &lt;a href=&#34;https://www.ams.org/journals/bull/1937-43-05/S0002-9904-1937-06520-7/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Wahrscheinlichkeit statistik und wahrheit&lt;/a&gt; (1936), &lt;em&gt;Bulletin of the American Mathematical Society&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;[9] T. W. Anderson, D. A. Darling, &lt;a href=&#34;https://projecteuclid.org/journals/annals-of-mathematical-statistics/volume-23/issue-2/Asymptotic-Theory-of-Certain-Goodness-of-Fit-Criteria-Based-on/10.1214/aoms/1177729437.full&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Asymptotic Theory of Certain &amp;ldquo;Goodness of Fit&amp;rdquo; Criteria Based on Stochastic Processes&lt;/a&gt; (1953), &lt;em&gt;The Annals of Mathematical Statistics&lt;/em&gt;.&lt;/p&gt;
&lt;h3 id=&#34;related-articles&#34;&gt;Related Articles&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/b63dc69e3d8c&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Goodbye Scatterplot, Welcome Binned Scatterplot&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;code&#34;&gt;Code&lt;/h3&gt;
&lt;p&gt;You can find the original Jupyter Notebook here:&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/distr.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/distr.ipynb&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Understanding Contamination Bias</title>
      <link>https://matteocourthoud.github.io/post/cbias/</link>
      <pubDate>Wed, 15 Jun 2022 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/post/cbias/</guid>
      <description>&lt;p&gt;&lt;em&gt;Problems and solutions of linear regression with multiple treatments&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;In many causal inference settings, we might be interested in the effect of not just one treatment, but &lt;strong&gt;many mutually exclusive treatments&lt;/strong&gt;. For example, we might want to test alternative UX designs, or drugs, or policies. Depending on the context, there might be many reasons why we want to test different treatments at the same time, but generally it can help &lt;em&gt;reducing the sample size&lt;/em&gt;, as we need just a single control group. A simple way to recover the different treatment effects is a linear regression of the outcome of interest on the different treatment indicators.&lt;/p&gt;
&lt;p&gt;However, in causal inference, we often &lt;strong&gt;condition the analysis&lt;/strong&gt; on other observable variables (often called control variables), either to increase power or, especially in quasi-experimental settings, to identify a causal parameter instead of a simple correlation. There are &lt;a href=&#34;https://towardsdatascience.com/b63dc69e3d8c&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;cases in which adding control variables can backfire&lt;/a&gt;, but otherwise, we usually think that the regression framework is still able to recover the average treatment effect.&lt;/p&gt;
&lt;p&gt;In a breakthrough paper, &lt;a href=&#34;https://www.nber.org/papers/w30108&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Goldsmith-Pinkham, Hull and Kolesár (2022)&lt;/a&gt; have recently shown that in case of &lt;em&gt;multiple and mutually-exclusive&lt;/em&gt; treatments with &lt;em&gt;control variables&lt;/em&gt;, the &lt;strong&gt;regression coefficients do not identify a causal effect&lt;/strong&gt;. However, not everything is lost: the authors propose a simple solution to this problem that still makes use of linear regression.&lt;/p&gt;
&lt;p&gt;In this blog post, I am going to go through a &lt;strong&gt;simple example&lt;/strong&gt; illustrating the nature of the problem and the solution proposed by the authors.&lt;/p&gt;
&lt;h2 id=&#34;multiple-treatments-example&#34;&gt;Multiple Treatments Example&lt;/h2&gt;
&lt;p&gt;Suppose we are an online store and we are not satisfied with our current &lt;em&gt;checkout page&lt;/em&gt;. In particular, we would like to change our &lt;strong&gt;checkout button&lt;/strong&gt; to increase the probability of a purchase. Our UX designer comes up with two alternative checkout buttons, which are displayed below.&lt;/p&gt;
&lt;img src=&#34;fig/buttons.png&#34; width=&#34;800px&#34;/&gt;
&lt;p&gt;In order to understand which button to use, we run an &lt;a href=&#34;https://en.wikipedia.org/wiki/A/B_testing&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;A/B test&lt;/strong&gt;&lt;/a&gt;, or randomized control trial. In particular, when people arrive at the checkout page, we show them one of the three options, at random. Then, for each user, we record the revenue generated which is our outcome of interest.&lt;/p&gt;
&lt;p&gt;I generate a synthetic dataset using &lt;code&gt;dgp_buttons()&lt;/code&gt; from &lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/src/dgp.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;src.dgp&lt;/code&gt;&lt;/a&gt; as data generating process. I also import plotting functions and standard libraries from &lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/src/utils.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;src.utils&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;%matplotlib inline
%config InlineBackend.figure_format = &#39;retina&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from src.utils import *
from src.dgp import dgp_buttons
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;dgp = dgp_buttons()
df = dgp.generate_data()
df.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;group&lt;/th&gt;
      &lt;th&gt;revenue&lt;/th&gt;
      &lt;th&gt;mobile&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;button1&lt;/td&gt;
      &lt;td&gt;8.927335&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;default&lt;/td&gt;
      &lt;td&gt;13.613456&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;button2&lt;/td&gt;
      &lt;td&gt;4.777628&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;default&lt;/td&gt;
      &lt;td&gt;8.909049&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;default&lt;/td&gt;
      &lt;td&gt;10.160347&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;We have information on 2000 users, for which we observe their checkout button (&lt;code&gt;default&lt;/code&gt;, &lt;code&gt;button1&lt;/code&gt; or &lt;code&gt;button2&lt;/code&gt;), the &lt;code&gt;revenue&lt;/code&gt; they generate and whether they connected from desktop or &lt;code&gt;mobile&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;We notice too late that we have a &lt;strong&gt;problem with randomization&lt;/strong&gt;. We showed &lt;code&gt;button1&lt;/code&gt; more frequently to desktop users and &lt;code&gt;button2&lt;/code&gt; more frequently to mobile users. The control group that sees the &lt;code&gt;default&lt;/code&gt; button instead is balanced.&lt;/p&gt;
&lt;img src=&#34;fig/randomization.png&#34; width=&#34;600px&#34;/&gt;
&lt;p&gt;What should we do? What happens if we simply compare &lt;code&gt;revenue&lt;/code&gt; across &lt;code&gt;groups&lt;/code&gt;? Let&amp;rsquo;s do it by regressing &lt;code&gt;revenue&lt;/code&gt; on &lt;code&gt;group&lt;/code&gt; dummy variables.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;smf.ols(&#39;revenue ~ group&#39;, data=df).fit().summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
          &lt;td&gt;&lt;/td&gt;            &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt;        &lt;td&gt;   11.6553&lt;/td&gt; &lt;td&gt;    0.149&lt;/td&gt; &lt;td&gt;   78.250&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;   11.363&lt;/td&gt; &lt;td&gt;   11.948&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;group[T.button1]&lt;/th&gt; &lt;td&gt;   -0.5802&lt;/td&gt; &lt;td&gt;    0.227&lt;/td&gt; &lt;td&gt;   -2.556&lt;/td&gt; &lt;td&gt; 0.011&lt;/td&gt; &lt;td&gt;   -1.026&lt;/td&gt; &lt;td&gt;   -0.135&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;group[T.button2]&lt;/th&gt; &lt;td&gt;   -0.5958&lt;/td&gt; &lt;td&gt;    0.218&lt;/td&gt; &lt;td&gt;   -2.727&lt;/td&gt; &lt;td&gt; 0.006&lt;/td&gt; &lt;td&gt;   -1.024&lt;/td&gt; &lt;td&gt;   -0.167&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;From the regression results we estimate a negative and significant effect for both buttons. Should we believe these estimates? Are they &lt;strong&gt;causal&lt;/strong&gt;?&lt;/p&gt;
&lt;p&gt;It is unlikely that what we have estimated are the true treatment effects. In fact, there might be substantial &lt;strong&gt;differences in purchase attitudes&lt;/strong&gt; between desktop and mobile users. Since we do not have a comparable number of mobile and desktop users across treatment arms, it might be that the observed differences in &lt;code&gt;revenue&lt;/code&gt; are due to the &lt;em&gt;device&lt;/em&gt; used and not the &lt;em&gt;button design&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Because of this, we decide to &lt;strong&gt;condition&lt;/strong&gt; our analysis on the device used and we include the &lt;code&gt;mobile&lt;/code&gt; dummy variable in the regression.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;smf.ols(&#39;revenue ~ group + mobile&#39;, data=df).fit().summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
          &lt;td&gt;&lt;/td&gt;            &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt;        &lt;td&gt;    9.1414&lt;/td&gt; &lt;td&gt;    0.110&lt;/td&gt; &lt;td&gt;   82.905&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    8.925&lt;/td&gt; &lt;td&gt;    9.358&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;group[T.button1]&lt;/th&gt; &lt;td&gt;    0.3609&lt;/td&gt; &lt;td&gt;    0.141&lt;/td&gt; &lt;td&gt;    2.558&lt;/td&gt; &lt;td&gt; 0.011&lt;/td&gt; &lt;td&gt;    0.084&lt;/td&gt; &lt;td&gt;    0.638&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;group[T.button2]&lt;/th&gt; &lt;td&gt;   -1.0326&lt;/td&gt; &lt;td&gt;    0.134&lt;/td&gt; &lt;td&gt;   -7.684&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;   -1.296&lt;/td&gt; &lt;td&gt;   -0.769&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;mobile&lt;/th&gt;           &lt;td&gt;    4.7181&lt;/td&gt; &lt;td&gt;    0.116&lt;/td&gt; &lt;td&gt;   40.691&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    4.491&lt;/td&gt; &lt;td&gt;    4.946&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;Now the coefficient of &lt;code&gt;button1&lt;/code&gt; is positive and significant. Should we recommend its implementation?&lt;/p&gt;
&lt;p&gt;The answer is &lt;strong&gt;surprisingly no&lt;/strong&gt;. &lt;a href=&#34;https://www.nber.org/papers/w30108&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Goldsmith-Pinkham, Hull, Kolesár (2022)&lt;/a&gt; show that this type of regression does not identify the average treatment effect when:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;there are mutually exclusive treatment arms (in our case, &lt;code&gt;groups&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;we are controlling for some variable $X$ (in our case, &lt;code&gt;mobile&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;there treatment effects are heterogeneous in $X$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This is true &lt;strong&gt;even if&lt;/strong&gt; the treatment is &amp;ldquo;as good as random&amp;rdquo; once we condition on $X$.&lt;/p&gt;
&lt;p&gt;Indeed, in our case, the true treatment effects are the ones reported in the following table.&lt;/p&gt;
&lt;img src=&#34;fig/effects.png&#34; width=&#34;500px&#34;/&gt;
&lt;p&gt;The first button has &lt;strong&gt;no effect&lt;/strong&gt; on revenue, irrespectively of the device, while the second button has a &lt;strong&gt;positive effect&lt;/strong&gt; for mobile users and a &lt;strong&gt;negative effect&lt;/strong&gt; for desktop users. Our (wrong) regression specification instead estimates a positive effect of the first button.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s now dig more in detail into the math, to understand why this is happening.&lt;/p&gt;
&lt;h2 id=&#34;theory&#34;&gt;Theory&lt;/h2&gt;
&lt;p&gt;This section borrows heavily from &lt;a href=&#34;https://www.nber.org/papers/w30108&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Goldsmith-Pinkham, Hull, Kolesár (2022)&lt;/a&gt;. For a great summary of the paper, I recommend this excellent Twitter thread by one of the authors, Paul Goldsmith-Pinkham.&lt;/p&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;Economists love using linear regression to estimate treatment effects — it turns out that there are perils to this method, but also amazing perks&lt;br&gt;&lt;br&gt;Come with me in this 🧵 if you want to learn… &lt;a href=&#34;https://t.co/eDsRLkZFZe&#34;&gt;https://t.co/eDsRLkZFZe&lt;/a&gt;&lt;/p&gt;&amp;mdash; Paul Goldsmith-Pinkham (@paulgp) &lt;a href=&#34;https://twitter.com/paulgp/status/1534169803388293120?ref_src=twsrc%5Etfw&#34;&gt;June 7, 2022&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;

&lt;h3 id=&#34;single-treatment-arm&#34;&gt;Single Treatment Arm&lt;/h3&gt;
&lt;p&gt;Suppose we are interested in the effect of a treatment $D$ on an outcome $Y$. First, let&amp;rsquo;s consider the standard case of a &lt;strong&gt;single treatment arm&lt;/strong&gt; so that the treatment variable is binary, $D \in \lbrace 0 , 1 \rbrace$. Also consider a single &lt;strong&gt;binary control variable&lt;/strong&gt; $X \in \lbrace 0 , 1 \rbrace$. We also assume that treatment assignment is as good as random, conditionally on $X$. This means that there might be systematic differences between the treatment and control group, however, these differences are fully accounted for by $X$. Formally we write&lt;/p&gt;
&lt;p&gt;$$
\left( Y_i^{(0)}, Y_i^{(1)} \right) \ \perp \ D_i \ | \ X_i
$$&lt;/p&gt;
&lt;p&gt;Where $Y_i^{(d)}$ denotes the potential outcome of individual $i$ when its treatment status is $d$. For example, $Y_i^{(0)}$ indicates the outcome of individual $i$ in case it is not treated. This notation comes from &lt;a href=&#34;https://www.tandfonline.com/doi/abs/10.1198/016214504000001880&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Rubin&amp;rsquo;s potential outcomes framework&lt;/a&gt;. We can write the &lt;strong&gt;individual treatment effect&lt;/strong&gt; of individual $i$ as&lt;/p&gt;
&lt;p&gt;$$
\tau_i = Y_i^{(1)} - Y_i^{(0)}
$$&lt;/p&gt;
&lt;p&gt;In this setting, the &lt;strong&gt;regression of interest&lt;/strong&gt; is&lt;/p&gt;
&lt;p&gt;$$
Y_i = \alpha + \beta D_i + \gamma X_i + u_i
$$&lt;/p&gt;
&lt;p&gt;The coefficient of interest is $\beta$.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.jstor.org/stable/2998558&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Angrist (1998)&lt;/a&gt; shows that &lt;strong&gt;the regression coefficient $\beta$ identifies the average treatment effect&lt;/strong&gt;. In particular, $\beta$ identifies a weighted average of the within-group $x$ average treatment effect $\tau (x)$ with convex weights. In this particular setting, we can write it as&lt;/p&gt;
&lt;p&gt;$$
\beta = \lambda \tau(0) + (1 - \lambda) \tau(1) \qquad \text{where} \qquad \tau (x) = \mathbb E \big[ Y_i^{(1)} - Y_i^{(0)} \ \big| \ X_i = x \big]
$$&lt;/p&gt;
&lt;p&gt;The weights $\lambda$ and $(1-\lambda)$ are given by the within-group treatment variance. Hence, the OLS estimator gives &lt;strong&gt;less weight to groups where we have less treatment variance&lt;/strong&gt;, i.e., where treatment is more imbalanced. Groups where treatment is distributed 50-50 get the most weight.&lt;/p&gt;
&lt;p&gt;$$
\lambda = \frac{ \text{Var} \big(D_i \ \big| \ X_i = 0 \big) \Pr \big(X_i=0 \big)}{\sum_{x \in \lbrace 0 , 1 \rbrace} \text{Var} \big(D_i \ \big| \ X_i = x \big) \Pr \big( X_i=x \big)} \in [0, 1]
$$&lt;/p&gt;
&lt;p&gt;The weights can be derived using the &lt;a href=&#34;https://towardsdatascience.com/59f801eb3299&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Frisch-Waugh-Lowell theorem&lt;/a&gt; to express $\beta_1$ as the OLS coefficient of a univariate regression of $Y$ on $D_{i, \perp X}$, where $D_{i, \perp X}$ are the residuals from regressing $D$ on $X$. If you are not familiar with the Frisch-Waugh-Lowell theorem, I wrote an &lt;a href=&#34;https://towardsdatascience.com/59f801eb3299&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;introductory blog post here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;$$
\beta_1 = \frac{ \mathbb E \big[ D_{i, \perp X} Y_i \big] }{ \mathbb E \big[ D_{i, \perp X}^2 \big] } =
\underbrace{ \frac{\mathbb E \big[ D_{i, \perp X} Y_i(0) \big]}{\mathbb E \big[ D_{i, \perp X}^2 \big]} } _ {=0} + \frac{\mathbb E \big[ D_{i, \perp X} D_i \tau_i \big]}{\mathbb E \big[ D_{i, \perp X}^2 \big]} =
\frac{\mathbb E \big[ \text{Var} (D_i | X_i) \ \tau(X_i) \big]}{\mathbb E \big[ \text{Var}(D_i | X_i) \big]}
$$&lt;/p&gt;
&lt;p&gt;The second term of the central expression disappears because the residual $D_{i, \perp X}$ is by construction &lt;strong&gt;mean independent&lt;/strong&gt; of the control variable $X_i$, i.e.&lt;/p&gt;
&lt;p&gt;$$
\mathbb E \big[ D_{i, \perp X} | X_i \big] = 0
$$&lt;/p&gt;
&lt;p&gt;This mean independence property is crucial to obtain an unbiased estimate and its failure in the multiple-treatment case is the source of the &lt;em&gt;contamination bias&lt;/em&gt;.&lt;/p&gt;
&lt;h3 id=&#34;multiple-treatment-arms&#34;&gt;Multiple Treatment Arms&lt;/h3&gt;
&lt;p&gt;Let&amp;rsquo;s now consider the case of multiple treatment arms, $D \in \lbrace 0, 1, 2 \rbrace$, where $1$ and $2$ indicate two mutually-exclusive treatments. We still assume &lt;strong&gt;conditional ignorability&lt;/strong&gt;, i.e., treatment assignment is as good as random, conditional on $X$.&lt;/p&gt;
&lt;p&gt;$$
\left( Y_i^{(0)}, Y_i^{(1)}, Y_i^{(2)} \right) \ \perp \ D_i \ | \ X_i
$$&lt;/p&gt;
&lt;p&gt;In this case, we have two different &lt;strong&gt;individual treatment effects&lt;/strong&gt;, one per treatment.&lt;/p&gt;
&lt;p&gt;$$
\tau_{i1} = Y_i^{(1)} - Y_i^{(0)} \qquad \text{and} \qquad \tau_{i2} = Y_i^{(2)} - Y_i^{(0)}
$$&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;regression of interest&lt;/strong&gt; is&lt;/p&gt;
&lt;p&gt;$$
Y_i = \alpha + \beta_1 D_{i1} + \beta_2 D_{i2} + \gamma X_i + u_i
$$&lt;/p&gt;
&lt;p&gt;Does the OLS estimator of $\beta_1$ and $\beta_2$ &lt;strong&gt;identify&lt;/strong&gt; an average treatment effect?&lt;/p&gt;
&lt;p&gt;It would be very tempting to say yes. In fact, it looks like not much has changed with respect to the previous setup. We just have one extra treatment, but the potential outcomes are still conditionally independent of it. Where is the &lt;strong&gt;issue&lt;/strong&gt;?&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s concentrate on $\beta_1$ (the same applies to $\beta_2$). As before, can rewrite $\beta_1$ using the &lt;a href=&#34;https://towardsdatascience.com/59f801eb3299&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Frisch-Waugh-Lowell theorem&lt;/a&gt; as the OLS coefficient of a univariate regression of $Y_i$ on $D_{i1, \perp X, D_2}$, where $D_{i1, \perp X, D_2}$ are the residuals from regressing $D_1$ on $D_2$ and $X$.&lt;/p&gt;
&lt;p&gt;$$
\beta_1 = \frac{ \mathbb E \big[D_{i1, \perp X, D_2} Y_i \big] }{ \mathbb E \big[ D_{i1, \perp X, D_2}^2 \big]} = \underbrace{ \frac{ \mathbb E \big[ D_{i1, \perp X, D_2} Y_i(0) \big] }{\mathbb E \big[ D_{i1, \perp X, D_2}^2 \big]} } _ {=0} + \frac{ \mathbb E \big[ D_{i1, \perp X, D_2} D_{i1} \tau_{i1} \big] }{ \mathbb E \big[ D_{i1, \perp X, D_2}^2 \big]} + \color{red}{ \underbrace{ \color{black}{ \frac{ \mathbb E \big[ D_{i1, \perp X, D_2} D_{i2} \tau_{i2} \big] }{ \mathbb E \big[ D_{i1, \perp X, D_2}^2 \big]}} } _ { \neq 0} }
$$&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;problem&lt;/strong&gt; is the last term. Without the last term, we could still write $\beta_1$ as a convex combination of the individual treatment effects. However, the last term biases the estimator by adding a component that depends on &lt;strong&gt;the treatment effect of $D_2$&lt;/strong&gt;, $\tau_2$. Why does this term not disappear?&lt;/p&gt;
&lt;p&gt;The problem is that $D_{i1, \perp X, D_2}$ is not mean independent of $D_{i2}$, i.e.&lt;/p&gt;
&lt;p&gt;$$
\mathbb E \big[ D_{i1, \perp X, D_2} D_{i2} \ \big| \ X_i \big] \neq 0
$$&lt;/p&gt;
&lt;p&gt;The reason lies in the fact that the treatments are &lt;strong&gt;mutually exclusive&lt;/strong&gt;. This implies that when $D_{i1}=1$, $D_{i2}$ must be zero, regardless of the value of $X_i$. Therefore, the last term does not cancel out and it introduces a &lt;strong&gt;contamination bias&lt;/strong&gt;.&lt;/p&gt;
&lt;h3 id=&#34;solution&#34;&gt;Solution&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://www.nber.org/papers/w30108&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Goldsmith-Pinkham, Hull, Kolesár (2022)&lt;/a&gt; show that a &lt;strong&gt;simple estimator&lt;/strong&gt;, first proposed by &lt;a href=&#34;https://www.aeaweb.org/articles?id=10.1257/jel.47.1.5&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Imbens and Wooldridge (2009)&lt;/a&gt;, is able to remove the bias. The procedure is the following.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;De-mean the control variable: $\tilde X = X - \bar X$&lt;/li&gt;
&lt;li&gt;Regress $Y$ on the interaction between the treatment indicators $D$ and the demeaned control variable $\tilde X$&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The OLS estimators of $\beta_1$ and $\beta_2$ are &lt;strong&gt;unbiased&lt;/strong&gt; estimators of the average treatment effects. It also just requires a linear regression. Moreover, this estimator is unbiased also for continuous control variables $X$, not only for a binary one as we have considered so far.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Why&lt;/strong&gt; was this estimator initially proposed by &lt;a href=&#34;https://www.aeaweb.org/articles?id=10.1257/jel.47.1.5&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Imbens and Wooldridge (2009)&lt;/a&gt;? Let&amp;rsquo;s analyze two parts separately: the interaction term between $D$ and $X$ and the fact that $X$ is de-meaned in the interaction term.&lt;/p&gt;
&lt;p&gt;First, the &lt;strong&gt;interaction term&lt;/strong&gt; $D X$ allows us to control for different effects and/or distributions of $X$ across treatment and control group.&lt;/p&gt;
&lt;p&gt;Second, &lt;strong&gt;de-meaning&lt;/strong&gt; $X$ in the interaction term allows us to &lt;strong&gt;interpret&lt;/strong&gt; the estimated coefficient $\hat \beta$ as the average treatment effect. In fact, assume we were estimating the following linear model, where $X$ is &lt;em&gt;not&lt;/em&gt; de-meaned in the interaction term.&lt;/p&gt;
&lt;p&gt;$$
Y_i = \alpha + \beta D_i + \gamma X_i + \delta D_i X_i + u_i
$$&lt;/p&gt;
&lt;p&gt;In this case, the marginal effect of $D$ on $Y$ is $\beta + \delta X_i$ so that the &lt;em&gt;average&lt;/em&gt; marginal effect is $\beta + \delta \bar X$ which is different from $\beta$.&lt;/p&gt;
&lt;p&gt;If instead we use the de-meaned value of $X$ in the interaction term, the marginal effect of $D$ on $Y$ is $\beta + \delta (X_i - \bar X)$ so that the &lt;em&gt;average&lt;/em&gt; marginal effect is $\beta + \delta (\bar X - \bar X) = \beta$. Now we can interpret $\beta$ as the average marginal effect of $D$ on $Y$.&lt;/p&gt;
&lt;h2 id=&#34;simulations&#34;&gt;Simulations&lt;/h2&gt;
&lt;p&gt;In order to better understand both the problem and the solution, let&amp;rsquo;s run some &lt;strong&gt;simulations&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;We run an estimator over different draws from the data generating process &lt;code&gt;dgp_buttons()&lt;/code&gt;. This is only possible with synthetic data and we do not have this luxury in reality. For each sample, we record the estimated coefficient and the corresponding &lt;a href=&#34;https://en.wikipedia.org/wiki/P-value&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;p-value&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def simulate(dgp, estimator, K=1000):
    
    # Initialize coefficients
    results = pd.DataFrame({&#39;Coefficient&#39;: np.zeros(K), &#39;pvalue&#39;: np.zeros(K)})
    
    # Compute coefficients
    for k in range(K):
        df = dgp.generate_data(seed=k)
        results.Coefficient[k] = estimator(df).params[1]
        results.pvalue[k] = estimator(df).pvalues[1]
    
    results[&#39;Significant&#39;] = results[&#39;pvalue&#39;] &amp;lt; 0.05
    return results
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;First, let&amp;rsquo;s try it with the old estimator that regresses &lt;code&gt;revenue&lt;/code&gt; on both &lt;code&gt;group&lt;/code&gt; and &lt;code&gt;mobile&lt;/code&gt; dummy variables.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;ols_estimator = lambda x: smf.ols(&#39;revenue ~ group + mobile&#39;, data=x).fit()
results = simulate(dgp, ols_estimator)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I &lt;strong&gt;plot&lt;/strong&gt; the distribution of the coefficient estimates of &lt;code&gt;button1&lt;/code&gt; over 1000 simulations, highlighting the statistically significant ones at the 5% level. I also highlight the true value of the coefficient, zero, with a vertical dotted bar.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def plot_results(results):
    p_sig = sum(results[&#39;Significant&#39;]) / len(results) * 100
    sns.histplot(data=results, x=&amp;quot;Coefficient&amp;quot;, hue=&amp;quot;Significant&amp;quot;, multiple=&amp;quot;stack&amp;quot;, 
                 palette = [&#39;tab:red&#39;, &#39;tab:green&#39;]);
    plt.axvline(x=0, c=&#39;k&#39;, ls=&#39;--&#39;, label=&#39;truth&#39;)
    plt.title(rf&amp;quot;Estimated $\beta_1$ ({p_sig:.0f}% significant)&amp;quot;);
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plot_results(results)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/cbias_35_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;As we can see, we reject the null hypothesis of no effect of &lt;code&gt;button1&lt;/code&gt; in 45% of the simulations. Since we set a confidence level of 5%, we would have expected at most around 5% of rejections. Our estimator is &lt;strong&gt;biased&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;As we have seen above, the problem is that the estimator is not just a convex combination of the effect of &lt;code&gt;button1&lt;/code&gt; across mobile and desktop users (it&amp;rsquo;s zero for both), but it is &lt;strong&gt;contaminated&lt;/strong&gt; by the effect of &lt;code&gt;button2&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s now try the estimator proposed from &lt;a href=&#34;https://www.aeaweb.org/articles?id=10.1257/jel.47.1.5&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Imbens and Wooldridge (2009)&lt;/a&gt;. First, we need do de-mean our control variable, &lt;code&gt;mobile&lt;/code&gt;. Then, we regress &lt;code&gt;revenue&lt;/code&gt; on the interaction between &lt;code&gt;group&lt;/code&gt; and the de-meaned control variable, &lt;code&gt;res_mobile&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df[&#39;mobile_res&#39;] = df[&#39;mobile&#39;] - np.mean(df[&#39;mobile&#39;])
smf.ols(&#39;revenue ~ group * mobile_res&#39;, data=df).fit().summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
               &lt;td&gt;&lt;/td&gt;                  &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt;                   &lt;td&gt;   11.5773&lt;/td&gt; &lt;td&gt;    0.067&lt;/td&gt; &lt;td&gt;  172.864&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;   11.446&lt;/td&gt; &lt;td&gt;   11.709&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;group[T.button1]&lt;/th&gt;            &lt;td&gt;    0.0281&lt;/td&gt; &lt;td&gt;    0.106&lt;/td&gt; &lt;td&gt;    0.266&lt;/td&gt; &lt;td&gt; 0.790&lt;/td&gt; &lt;td&gt;   -0.180&lt;/td&gt; &lt;td&gt;    0.236&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;group[T.button2]&lt;/th&gt;            &lt;td&gt;   -1.5071&lt;/td&gt; &lt;td&gt;    0.100&lt;/td&gt; &lt;td&gt;  -15.112&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;   -1.703&lt;/td&gt; &lt;td&gt;   -1.311&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;mobile_res&lt;/th&gt;                  &lt;td&gt;    2.9107&lt;/td&gt; &lt;td&gt;    0.134&lt;/td&gt; &lt;td&gt;   21.715&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    2.648&lt;/td&gt; &lt;td&gt;    3.174&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;group[T.button1]:mobile_res&lt;/th&gt; &lt;td&gt;    0.1605&lt;/td&gt; &lt;td&gt;    0.211&lt;/td&gt; &lt;td&gt;    0.760&lt;/td&gt; &lt;td&gt; 0.448&lt;/td&gt; &lt;td&gt;   -0.254&lt;/td&gt; &lt;td&gt;    0.575&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;group[T.button2]:mobile_res&lt;/th&gt; &lt;td&gt;    5.3771&lt;/td&gt; &lt;td&gt;    0.200&lt;/td&gt; &lt;td&gt;   26.905&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    4.985&lt;/td&gt; &lt;td&gt;    5.769&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;The estimated coefficients are now &lt;strong&gt;close to their true values&lt;/strong&gt;. The estimated coefficient for &lt;code&gt;button1&lt;/code&gt; is not significant, while the estimated coefficient for &lt;code&gt;button2&lt;/code&gt; is negative and significant.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s check whether this results holds &lt;strong&gt;across samples&lt;/strong&gt; by running a simulation. We repeat the estimation procedure 1000 times and we plot the distribution of estimated coefficients for &lt;code&gt;button1&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;new_estimator = lambda x: smf.ols(&#39;revenue ~ group * mobile&#39;, data=x).fit()
new_results = simulate(dgp, new_estimator)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plot_results(new_results)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/cbias_40_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Now the distribution of the estimated coefficient for &lt;code&gt;button1&lt;/code&gt; is centered around the true value of zero. Moreover, we reject the null hypothesis of no effect only in 1% of the simulations, consistently with the chosen confidence level of 95%.&lt;/p&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;In this post, we have seen the dangers of running a factor regression model with multiple &lt;em&gt;mutually exclusive&lt;/em&gt; treatment arms and treatment effect heterogeneity across a control variable. In this case, because the treatments are not independent, the regression coefficients are not a convex combination of the within-group average treatment effects, but also capture the treatment effects of the other treatments introducing &lt;strong&gt;contamination bias&lt;/strong&gt;. The solution to the problem is both simple and elegant, requiring just a linear regression.&lt;/p&gt;
&lt;p&gt;However, &lt;strong&gt;the problem is more general&lt;/strong&gt; than this setting and generally concerns every setting in which (all of the following)&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;We have multiple treatments that depend on each other&lt;/li&gt;
&lt;li&gt;We need to condition the analysis on a control variable&lt;/li&gt;
&lt;li&gt;The treatment effects are heterogeneous in the control variable&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Another popular example is the case of the &lt;a href=&#34;https://arxiv.org/abs/2201.01194&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Two-Way Fixed Effects (TWFE) estimator with staggered treatments&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;references&#34;&gt;References&lt;/h3&gt;
&lt;p&gt;[1] J. Angrist, &lt;a href=&#34;https://www.jstor.org/stable/2998558&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Estimating the Labor Market Impact of Voluntary Military Service Using Social Security Data on Military Applicants&lt;/a&gt; (1998), &lt;em&gt;Econometrica&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;[2] D. Rubin, &lt;a href=&#34;https://www.tandfonline.com/doi/abs/10.1198/016214504000001880&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Causal Inference Using Potential Outcomes&lt;/a&gt; (2005), &lt;em&gt;Journal of the American Statistical Association&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;[3] G. Imbens, J. Wooldridge, &lt;a href=&#34;https://www.aeaweb.org/articles?id=10.1257/jel.47.1.5&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Recent Developments in the Econometrics of Program Evaluation&lt;/a&gt; (2009), &lt;em&gt;Journal of Economic Literature&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;[4] P. Goldsmith-Pinkham, P. Hull, M. Kolesár, &lt;a href=&#34;https://www.nber.org/papers/w30108&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Contamination Bias in Linear Regressions&lt;/a&gt; (2022), &lt;em&gt;working paper&lt;/em&gt;.&lt;/p&gt;
&lt;h3 id=&#34;related-articles&#34;&gt;Related Articles&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/344ac1477699&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Understanding Omitted Variable Bias&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/59f801eb3299&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Understanding The Frisch-Waugh-Lovell Theorem&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/b63dc69e3d8c&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;DAGs and Control Variables&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;code&#34;&gt;Code&lt;/h3&gt;
&lt;p&gt;You can find the original Jupyter Notebook here:&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/cbias.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/matteocourthoud/Blog-Posts/blob/main/cbias.ipynb&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Understanding Combinations and Permutations</title>
      <link>https://matteocourthoud.github.io/post/combperm/</link>
      <pubDate>Fri, 10 Jun 2022 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/post/combperm/</guid>
      <description>&lt;p&gt;How many times did you face questions starting with &amp;ldquo;&lt;em&gt;Suppose you have an urn with three red balls and five blue balls, &amp;hellip;&lt;/em&gt;&amp;rdquo;? The answer for me is, not often since high-school, but recently they popped up again in &lt;strong&gt;data science interviews&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Despite countless classes in statistics, I still take a deep breath and hope I won&amp;rsquo;t embarrass myself too much. My main problem is that I get crazy confused with binary labels, especially if the label itself is not too intuitive.&lt;/p&gt;
&lt;p&gt;Some non-intuitive (for me) binary labels include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Sine_and_cosine&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;sine and cosine&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Concave_function&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;concavity and convexity&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Precision_and_recall&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;precision and recall&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Type_I_and_type_II_errors&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;type 1 and type 2 error rate&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;And last but not least, &lt;strong&gt;combinations and permutations&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;So this blog post is an attempt to clarify once and for all the difference between the two and get some practice.&lt;/p&gt;
&lt;h2 id=&#34;the-math&#34;&gt;The Math&lt;/h2&gt;
&lt;h3 id=&#34;permutations-vs-combinations&#34;&gt;Permutations vs Combinations&lt;/h3&gt;
&lt;p&gt;The main &lt;strong&gt;difference&lt;/strong&gt; between combinations and permutations is &lt;strong&gt;order&lt;/strong&gt;. In particular:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Permutations: order matters&lt;/li&gt;
&lt;li&gt;Combinations: order does &lt;em&gt;not&lt;/em&gt; matter&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;What does it mean &lt;strong&gt;in practice&lt;/strong&gt;?&lt;/p&gt;
&lt;p&gt;One rule of thumb is to check whether the individual objects are &lt;strong&gt;identifiable&lt;/strong&gt;. For example:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Urn with blue and red balls: the individual ball is not identifiable, therefore we are talking about combinations&lt;/li&gt;
&lt;li&gt;Deck of cards: individual cards are identifiable, therefore it could be either way&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Also, &lt;strong&gt;note&lt;/strong&gt; that for any problem, the number of permutations is always weakly &lt;em&gt;larger&lt;/em&gt; than the number of combinations. The intuition is simple: since in permutations order matters, AB and BA are two different outcomes, while with combinations they are grouped into a single one.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Example&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;For this section, we are going to use a simple &lt;strong&gt;example&lt;/strong&gt; in which we have to order a two-scoops ice cream cone and there are three possible flavors: amarena, chocolate and pistacchio.&lt;/p&gt;
&lt;h3 id=&#34;permutations&#34;&gt;Permutations&lt;/h3&gt;
&lt;p&gt;Let&amp;rsquo;s start with permutations since they are &lt;strong&gt;mathematically simpler&lt;/strong&gt;. We have seen that in combinations &lt;strong&gt;order matters&lt;/strong&gt;. In our example, suppose we care about which flavor is on top on the ice cream. Now we are going to further distinguish between two cases:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;With replacement&lt;/li&gt;
&lt;li&gt;Without replacement&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Where replacement means that after I draw an object, I can draw it again (e.g. I put it back in the pool).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Replacement&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In the first case, we can order both scoops of the same flavor. Therefore, for each scoop (2) we have 3 options (the flavors).&lt;/p&gt;
&lt;img src=&#34;fig/perm_repl.png&#34; width=&#34;800px&#34;/&gt;
&lt;p&gt;The number of &lt;strong&gt;overall events&lt;/strong&gt; is&lt;/p&gt;
&lt;p&gt;$$
\text{from 3 permute 2, with replacement} = 3 * 3 = 3^2 = 9
$$&lt;/p&gt;
&lt;p&gt;In general, the number of possible permutations of $n$ objects in $k$ draws with replacement is $n^k$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;No Replacement&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Alternatively, we can only order different flavors in each scoop. In this case, for the first scoop we can pick any flavor, but for the second scoop we can only pick one of the two remaining flavors.&lt;/p&gt;
&lt;img src=&#34;fig/perm.png&#34; width=&#34;550px&#34;/&gt;
&lt;p&gt;The number of &lt;strong&gt;overall events&lt;/strong&gt; is&lt;/p&gt;
&lt;p&gt;$$
\text{from 3 permute 2, without replacement} = 3 * 2 = \frac{3!}{(3-2)!} = \frac{3 * 2 * 1}{1} = 6
$$&lt;/p&gt;
&lt;p&gt;where ! denotes the &lt;a href=&#34;https://en.wikipedia.org/wiki/Factorial&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;factorial operation&lt;/a&gt; which can be recursively defined as $n! = n \times (n-1)!$ with $0! = 1$.&lt;/p&gt;
&lt;p&gt;In general, the number of possible permutations of $n$ objects in $k$ draws without replacement is $\frac{n!}{(n-k)!}$.&lt;/p&gt;
&lt;h3 id=&#34;combinations&#34;&gt;Combinations&lt;/h3&gt;
&lt;p&gt;Combinations are usually &lt;strong&gt;more common&lt;/strong&gt; since in a lot of scenarios we do not care about the order or the identity of the objects. In our example, let&amp;rsquo;s assume we don&amp;rsquo;t care which flavor gets on top. As before, we are going to further distinguish between two cases:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;With replacement&lt;/li&gt;
&lt;li&gt;Without replacement&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;No Replacement&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In this case, we cannot have two scoops of the same flavor. Therefore, for each scoop (2) we have 3 options (the flavors). However, the order of the flavors does not matter, i.e. we are indifferent between getting chocolate on top or on the bottom, as long as we get it.&lt;/p&gt;
&lt;img src=&#34;fig/comb.png&#34; width=&#34;270px&#34;/&gt;
&lt;p&gt;The number of &lt;strong&gt;overall events&lt;/strong&gt; therefore is the number of permutations of 3 flavors among 2 scoops, divided by the permutations of 2 out of 2 scoops.&lt;/p&gt;
&lt;p&gt;$$
\text{from 3 choose 2, without replacement} = \frac{\text{from 3 permute 2, without replacement}}{\text{from 2 permute 2, without replacement}} = \frac{\frac{3!}{(3-2)!}}{\frac{2!}{(2-2)!}} = \frac{3!}{2!(3-2)!} = \frac{3 * 2 * 1}{2 * 1 * 1} = 3
$$&lt;/p&gt;
&lt;p&gt;In general, we define the mathematical operation &amp;ldquo;from $n$ choose $k$&amp;rdquo; as&lt;/p&gt;
&lt;p&gt;$$
\text{from n choose k} := {n \choose k} = \frac{n!}{k!(n-k)!}
$$&lt;/p&gt;
&lt;p&gt;Which corresponds to the number of possible combinations of $n$ objects in $k$ draws without replacement.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Replacement&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Alternatively, we can be allowed to order the same flavor for both scoops.&lt;/p&gt;
&lt;img src=&#34;fig/comb_repl.png&#34; width=&#34;550px&#34;/&gt;
&lt;p&gt;In this case, the number of &lt;strong&gt;overall events&lt;/strong&gt; is&lt;/p&gt;
&lt;p&gt;$$
\text{from 3 choose 2, with replacement} = {3 + 2 - 1 \choose 2} = \frac{(3 + 2 - 1)!}{2!(3-1)!} = \frac{4 * 3 * 2 * 1}{2 * 1 * 2 * 1} = 6
$$&lt;/p&gt;
&lt;p&gt;In general, the number of possible combinations of $n$ objects in $k$ draws with replacement is ${n + k - 1 \choose k}$.&lt;/p&gt;
&lt;h3 id=&#34;summary&#34;&gt;Summary&lt;/h3&gt;
&lt;p&gt;We can summarize all the possible scenarios in a simple table.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;With Replacement&lt;/th&gt;
&lt;th&gt;Without Replacement&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Permutations&lt;/strong&gt; (order matters):&lt;/td&gt;
&lt;td&gt;$n^k$&lt;/td&gt;
&lt;td&gt;$\frac{n!}{(n-k)!} $&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Combinations&lt;/strong&gt; (order doesn&amp;rsquo;t matter):&lt;/td&gt;
&lt;td&gt;$ {n + k - 1 \choose k} \text{=}\frac{(n+k-1)!}{(n-1)!k!}  $&lt;/td&gt;
&lt;td&gt;$ {n \choose k} = \frac{n!}{(n-k)!k!} $&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Where $n$ is the number of objects and $k$ is the number of draws.&lt;/p&gt;
&lt;h2 id=&#34;example&#34;&gt;Example&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s explore together a more complex example to see how we can apply permutations and combinations to compute probabilities.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;There are four people in an elevator, four floors in the building, and each person exits at random. Find the probability that:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;all people exit at different floors&lt;/li&gt;
&lt;li&gt;all people exit at the same floor&lt;/li&gt;
&lt;li&gt;two get off at one floor and two get off at another&lt;/li&gt;
&lt;/ol&gt;
&lt;/blockquote&gt;
&lt;p&gt;I use the &lt;code&gt;factorial&lt;/code&gt; and &lt;code&gt;comb&lt;/code&gt; functions from the &lt;code&gt;math&lt;/code&gt; library.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from math import factorial, comb
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;practical-advice&#34;&gt;Practical Advice&lt;/h3&gt;
&lt;p&gt;Before we start, some &lt;strong&gt;practical advice&lt;/strong&gt;. What worked best &lt;em&gt;for me&lt;/em&gt; is to approach the question in the following way:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;what are the &lt;strong&gt;overall&lt;/strong&gt; events that we are considering?&lt;/li&gt;
&lt;li&gt;what are the &lt;strong&gt;positive&lt;/strong&gt; events that we are considering?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;And for both questions, I ask myself:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;does &lt;strong&gt;order&lt;/strong&gt; matter?&lt;/li&gt;
&lt;li&gt;is there &lt;strong&gt;replacement&lt;/strong&gt;?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Moreover, it is also very useful to &lt;strong&gt;restate the problem&lt;/strong&gt; in terms of objects and draws. For example, in this case, I can restate the problem as: &amp;ldquo;I am drawing a floor for each person&amp;rdquo;. This makes it clear whether or not there is replacement, i.e. whether or not I can draw the same floor for different persons.&lt;/p&gt;
&lt;h3 id=&#34;question-1&#34;&gt;Question 1&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;What is the probability that they all get off at different floors?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;Total&lt;/strong&gt; events: (from floors 4 permute 4, with replacement)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Positive&lt;/strong&gt; events: (from floors 4 permute 4, without replacement)&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;factorial(4) / 4**4
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;0.09375
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;question-2&#34;&gt;Question 2&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;What is the probability that they all exit at the same floor?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;Total&lt;/strong&gt; events: (from 4 floors permute 4, with replacement)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Positive&lt;/strong&gt; events: (from floors 4 choose 1)&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;4 / 4**4
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;0.015625
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;question-3&#34;&gt;Question 3&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;What is the probability that two get off at one floor and two at another?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;Total&lt;/strong&gt; events: (from floors 4 permute 4, with replacement)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Positive&lt;/strong&gt; events: (from 4 people choose 2, without replacement) * (from 4 floors choose 2, without replacement)&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;comb(4, 2) * comb(4, 2) / 4**4 
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;0.140625
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;practice-questions&#34;&gt;Practice Questions&lt;/h2&gt;
&lt;p&gt;Now it&amp;rsquo;s your time to shine! Here are some practice questions with solutions&lt;/p&gt;
&lt;h3 id=&#34;problem-1&#34;&gt;Problem 1&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;Suppose that you randomly draw 4 cards from a deck of 52 cards. What is the probability of getting 2 spades and 2 clubs?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;Total&lt;/strong&gt; events: (from 52 cards choose 4, without replacement)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Positive&lt;/strong&gt; events: (from 13 cards choose 2) * (from 13 cards choose 2)&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;comb(13, 2) * comb(13, 2) / comb(52, 4)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;0.02247298919567827
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;problem-2&#34;&gt;Problem 2&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;Suppose you draw 5 cards without replacement from a standard deck of 52 playing cards. What is the probability of guessing all 5 cards in any order?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;Total&lt;/strong&gt; events: (from 52 cards choose 5, without replacement)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Positive&lt;/strong&gt; events: 1&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;1 / comb(52, 5)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;3.8476929233231754e-07
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;problem-3&#34;&gt;Problem 3&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;Suppose you draw 3 cards without replacement from a standard deck of 52 playing cards. What is the probability of guessing all 3 cards in the correct order?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;Total&lt;/strong&gt; events: (from 52 cards permute 3, without replacement)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Positive&lt;/strong&gt; events: 1&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;1 / (factorial(52) / factorial(3))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;7.43879958514289e-68
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;problem-4&#34;&gt;Problem 4&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;Suppose you draw 5 cards without replacement from a standard deck of 52 playing cards. What is the probability of guessing 3 of them (out of 3 guesses), in any order?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;Total&lt;/strong&gt; events: (from 52 cards permute 3, without replacement)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Positive&lt;/strong&gt; events: (from 5 cards permute 3, without replacement)&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;(factorial(5) / factorial(3)) / (factorial(52) / factorial(5))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;2.9755198340571564e-65
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;problem-5&#34;&gt;Problem 5&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;A 4 digit PIN is selected. What is the probability that there are no repeated digits?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;Total&lt;/strong&gt; events: (from 10 permute 4, with replacement)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Positive&lt;/strong&gt; events: (from 10 permute 4, without replacement)&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;(factorial(10) / factorial(6)) / 10**4 
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;0.504
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;problem-6&#34;&gt;Problem 6&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;In a certain state’s lottery, 48 balls numbered 1 through 48 are placed in a machine and 6 of them are drawn at random. If the 6 numbers drawn match the numbers that a player had chosen, the player wins 1,000,000. In this lottery, the order the numbers are drawn in doesn’t matter. Compute the probability that you win the million-dollar prize if you purchase a single lottery ticket.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;Total&lt;/strong&gt; events: (from 48 choose 6, without replacement)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Positive&lt;/strong&gt; events: 1&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;1 / comb(48, 6)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;8.148955075788542e-08
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;problem-7&#34;&gt;Problem 7&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;In a certain state’s lottery, 48 balls numbered 1 through 48 are placed in a machine and 6 of them are drawn at random. If five of the six numbers drawn match the numbers that a player has chosen, the player wins a second prize of 1,000. Compute the probability that you win the second prize if you purchase a single lottery ticket.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;Total&lt;/strong&gt; events: (from 48 choose 6, without replacement)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Positive&lt;/strong&gt; events: (from 6 choose 5, without replacement)&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;comb(6, 5) / comb(48, 5)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;3.504050682589073e-06
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;problem-8&#34;&gt;Problem 8&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;Compute the probability of randomly drawing five cards from a deck and getting exactly one Ace.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;Total&lt;/strong&gt; events: (from 52 choose 5, without replacement)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Positive&lt;/strong&gt; events: (from 4 aces choose 1) * (from 48 cards that are not aces choose 4)&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;4 * comb(48, 4) / comb(52, 5)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;0.2994736356080894
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;problem-9&#34;&gt;Problem 9&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;Compute the probability of randomly drawing five cards from a deck and getting exactly two Aces.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;Total&lt;/strong&gt; events: (from 52 choose 5, without replacement)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Positive&lt;/strong&gt; events: (from 4 aces choose 2) * (from 48 cards that are not aces choose 3)&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;comb(4,2) * comb(48, 3) / comb(52, 5)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;0.03992981808107859
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;problem-10&#34;&gt;Problem 10&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;Suppose you have 3 people in a room. What is the probability that there is at least one shared birthday?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;Total&lt;/strong&gt; events: (from 365 days permute 3, with replacement)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Negative&lt;/strong&gt; events: (from 365 days permute 3, without replacement)&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;1 - (365 * 364 * 363) / (365**3)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;0.008204165884781345
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;problem-11&#34;&gt;Problem 11&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;Given a class of 12 girls and 10 boys, what is the probability that a committee of five, chosen at random from
the class, consists only of girls?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;Total&lt;/strong&gt; events: (from 22 choose 5, without replacement)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Positive&lt;/strong&gt; events: (from 12 choose 5, without replacement)&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;comb(12, 5) / comb(22, 5)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;0.03007518796992481
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;Permutation and combination questions are a classic in data science questions (unfortunately).&lt;/p&gt;
&lt;h3 id=&#34;code&#34;&gt;Code&lt;/h3&gt;
&lt;p&gt;You can find the original Jupyter Notebook here:&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/combperm.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/matteocourthoud/Blog-Posts/blob/main/combperm.ipynb&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Double Debiased Machine Learning (part 2)</title>
      <link>https://matteocourthoud.github.io/post/pds/</link>
      <pubDate>Sun, 05 Jun 2022 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/post/pds/</guid>
      <description>&lt;p&gt;In the &lt;a href=&#34;https://towardsdatascience.com/eb767a59975b&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;previous part of this blog post&lt;/a&gt;, we have seen how pre-testing can distort inference, i.e., how selecting control variables depending on their statistical significance results in wrong confidence intervals for the variable of interest. This bias is generally called &lt;strong&gt;regularization bias&lt;/strong&gt; and also emerges in machine learning algorithms.&lt;/p&gt;
&lt;p&gt;In blog post, we are going to explore a solution to the simple selection example, &lt;strong&gt;post-double selection&lt;/strong&gt;, and a more general approach when we have many control variables and we do not want to assume linearity, &lt;strong&gt;double-debiased machine learning&lt;/strong&gt;.&lt;/p&gt;
&lt;h2 id=&#34;recap&#34;&gt;Recap&lt;/h2&gt;
&lt;p&gt;To better understand the source of the bias, in the first part of this post, we have explored the example of a firm that is interested in testing the effectiveness of an a campaign. The firm has information on its current ad spending and on the level of sales. The problem arises because the firm is uncertain on whether it should condition its analysis on the level of past sales.&lt;/p&gt;
&lt;p&gt;The following &lt;a href=&#34;https://towardsdatascience.com/b63dc69e3d8c&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Directed Acyclic Graph (DAG)&lt;/strong&gt;&lt;/a&gt; summarizes the data generating process.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-mermaid&#34;&gt;flowchart LR
classDef included fill:#DCDCDC,stroke:#000000,stroke-width:2px;
classDef excluded fill:#ffffff,stroke:#000000,stroke-width:2px;
classDef unobserved fill:#ffffff,stroke:#000000,stroke-width:2px,stroke-dasharray: 5 5;

D((ad spend))
Z((past sales))
Y((sales))

D --&amp;gt; Y
Z -- ??? --&amp;gt; Y
Z --&amp;gt; D

class D,Y included;
class Z excluded;

linkStyle 0 stroke:#00ff00,stroke-width:4px;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I import the data generating process &lt;code&gt;dgp_tbd()&lt;/code&gt; from &lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/src/dgp.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;src.dgp&lt;/code&gt;&lt;/a&gt; and some plotting functions and libraries from &lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/src/utils.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;src.utils&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;%matplotlib inline
%config InlineBackend.figure_format = &#39;retina&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from src.utils import *
from src.dgp import dgp_pretest

df = dgp_pretest().generate_data()
df.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;ads&lt;/th&gt;
      &lt;th&gt;sales&lt;/th&gt;
      &lt;th&gt;past_sales&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;16.719800&lt;/td&gt;
      &lt;td&gt;19.196620&lt;/td&gt;
      &lt;td&gt;6.624345&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;7.732222&lt;/td&gt;
      &lt;td&gt;9.287491&lt;/td&gt;
      &lt;td&gt;4.388244&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;10.923469&lt;/td&gt;
      &lt;td&gt;11.816906&lt;/td&gt;
      &lt;td&gt;4.471828&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;8.457062&lt;/td&gt;
      &lt;td&gt;9.024376&lt;/td&gt;
      &lt;td&gt;3.927031&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;13.085146&lt;/td&gt;
      &lt;td&gt;12.814823&lt;/td&gt;
      &lt;td&gt;5.865408&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;We have data on $1000$ different markets, in which we observe current &lt;code&gt;sales&lt;/code&gt;, the amount spent in &lt;code&gt;advertisement&lt;/code&gt; and &lt;code&gt;past sales&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;We want to understand &lt;code&gt;ads&lt;/code&gt; spending is effective in increasing &lt;code&gt;sales&lt;/code&gt;. One possibility is to regress the latter on the former, using the following regression model, also called the &lt;strong&gt;short model&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;$$
\text{sales} = \alpha \cdot \text{ads} + \varepsilon
$$&lt;/p&gt;
&lt;p&gt;Should we also include &lt;code&gt;past sales&lt;/code&gt; in the regression? Then the regression model would be the following, also called &lt;strong&gt;long model&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;$$
\text{sales} = \alpha \cdot \text{ads} + \beta \cdot \text{past sales} + \varepsilon
$$&lt;/p&gt;
&lt;p&gt;One naive approach would be to &lt;strong&gt;let the data decide&lt;/strong&gt;: we could run the second regression and, if the effect of &lt;code&gt;past sales&lt;/code&gt;, $\beta$, is statistically significant, we are good with the long model, otherwise we run the short model. This procedure is called &lt;strong&gt;pre-testing&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;The problem with this procedure is that it introduces a bias that is called &lt;strong&gt;regularization or pre-test bias&lt;/strong&gt;. Pre-testing ensures that this bias is small enough not to distort the estimated coefficient. However, it does not ensure that it is small enough not to distort the confidence intervals around the estimated coefficient.&lt;/p&gt;
&lt;p&gt;Is there a solution? Yes!&lt;/p&gt;
&lt;h2 id=&#34;post-double-selection&#34;&gt;Post-Double Selection&lt;/h2&gt;
&lt;p&gt;The solution is called &lt;strong&gt;post-double selection&lt;/strong&gt;. The method was first introduced in &lt;a href=&#34;https://academic.oup.com/restud/article-abstract/81/2/608/1523757&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Belloni, Chernozhukov, Hansen (2014)&lt;/a&gt; and later expanded in a variety of papers.&lt;/p&gt;
&lt;p&gt;The authors assume the following &lt;strong&gt;data generating process&lt;/strong&gt;:&lt;/p&gt;
&lt;p&gt;$$
y = \alpha D + \beta X + u
\newline
D = \delta X + v
$$&lt;/p&gt;
&lt;p&gt;In our example, $Y$ corresponds to &lt;code&gt;sales&lt;/code&gt;, $D$ corresponds to &lt;code&gt;ads&lt;/code&gt;, $X$ corresponds to &lt;code&gt;past_sales&lt;/code&gt; and the effect of interest is $\alpha$. In our example, $X$ is 1-dimensional for simplicity, but generally we are interested in cases where X is high-dimensional, potentially even having more dimensions than the number of observations. In that case, variable selection is &lt;strong&gt;essential&lt;/strong&gt; in linear regression since we cannot have more features than variables (the OLS coefficients are not uniquely determined anymore).&lt;/p&gt;
&lt;p&gt;Post-double selection consists in the following procedure.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Reduced Form&lt;/strong&gt; selection: lasso $Y$ on $X$. Select the statistically significant variables in the set $S_{RF} \subseteq X$&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;First Stage&lt;/strong&gt; selection: regress $D$ on $X$. Select the statistically significant variables in the set $S_{FS} \subseteq X$&lt;/li&gt;
&lt;li&gt;Regress $Y$ on $D$ and the &lt;strong&gt;union&lt;/strong&gt; of the selected variables in the first two steps, $S_{FS} \cup S_{RF}$&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The authors show that this procedure produces confidence intervals for the coefficient of interest $\alpha$ that have the correct coverage, i.e. the correct probability of type 1 error.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Note (1)&lt;/strong&gt;: this procedure is always less parsimonious, in terms of variable selection, than pre-testing. In fact, we still select all the variables we would have selected with pre-testing but, in the first stage, we might select additional variables.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Note (2)&lt;/strong&gt;: the terms &lt;em&gt;first stage&lt;/em&gt; and &lt;em&gt;reduced form&lt;/em&gt; come from the &lt;a href=&#34;https://en.wikipedia.org/wiki/Instrumental_variables_estimation&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;intrumental variables&lt;/a&gt; literature in econometrics. Indeed, the first application of post-double selection was to select instrumental variables in &lt;a href=&#34;https://onlinelibrary.wiley.com/doi/abs/10.3982/ECTA9626&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Belloni, Chen, Chernozhukov, Hansen (2012)&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Note (3)&lt;/strong&gt;: the name post-double selection comes from the fact that now we are not performing variable selection once but &lt;em&gt;twice&lt;/em&gt;.&lt;/p&gt;
&lt;h3 id=&#34;intuition&#34;&gt;Intuition&lt;/h3&gt;
&lt;p&gt;The idea behind post-double selection is: bound the &lt;a href=&#34;https://towardsdatascience.com/344ac1477699&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;omitted variables bias&lt;/a&gt;. In case you are not familiar with it, I wrote a separate &lt;a href=&#34;https://towardsdatascience.com/344ac1477699&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;blog post on omitted variable bias&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;In our setting, we can express the omitted variable bias as&lt;/p&gt;
&lt;p&gt;$$
\text{OVB} = \beta \delta \qquad \text{ where } \qquad \beta := \frac{Cov(X, Y)}{Var(X)}, \quad \delta := \frac{Cov(D, X)}{Var(D)}
$$&lt;/p&gt;
&lt;p&gt;As we can see, the omitted variable bias comes from the product of two quantities related to the omitted variable $X$:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Its partial correlation with the outcome $Y$, $\beta$&lt;/li&gt;
&lt;li&gt;Its partial correlation with the variable of interest $D$, $\delta$&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;With pre-testing, we ensure that the partial correlation between $X$ the outcome $Y$, $\beta$, is &lt;strong&gt;small&lt;/strong&gt;. In fact, we omit $Z$ when we shouldn&amp;rsquo;t (i.e. we commit a type 2 error) rarely. What do &lt;em&gt;small&lt;/em&gt; and &lt;em&gt;rarely&lt;/em&gt; mean?&lt;/p&gt;
&lt;p&gt;When we are selecting a variable because of its significance, we ensure that it dimension is smaller than $\frac{c}{\sqrt{n}}$ for some number $c$, where $n$ is the sample size.&lt;/p&gt;
&lt;p&gt;Therefore, with pre-testing, we ensure that, no matter what the value of $\delta$ is, the dimension of the bias is smaller than $\frac{c}{\sqrt{n}}$ which means that it converges to zero for sufficiently large $n$. This is why the pre-testing estimator is still &lt;strong&gt;consistent&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;However, in order for our confidence intervals to have the right coverage, this is &lt;strong&gt;not enough&lt;/strong&gt;. In practice, we need the bias to converge to zero &lt;strong&gt;faster&lt;/strong&gt; than $\frac{1}{\sqrt{n}}$. Why?&lt;/p&gt;
&lt;p&gt;To get an &lt;strong&gt;intuition&lt;/strong&gt; for this result, we need to turn to the &lt;a href=&#34;https://en.wikipedia.org/wiki/Central_limit_theorem&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Central Limit Theorem&lt;/a&gt;. The CLT tells us that for large $n$ the distribution of the sample average of a random variable $X$ converges to a normal distribution with mean $\mu$ and standard deviation $\frac{\sigma}{\sqrt{n}}$, where $\mu$ and $\sigma$ are the mean and standard deviation of $X$. To do inference, we usually apply the Central Limit Theorem to our estimator to get its asymptotic distribution, which in turn allows us to build confidence intervals (using the mean and the standard deviation). Therefore, if the bias is not sensibly smaller than the standard deviation of the estimator, the confidence intervals are going to be wrong. Therefore, we need the bias to converge to zero &lt;strong&gt;faster&lt;/strong&gt; than the standard deviation, i.e. faster than $\frac{1}{\sqrt{n}}$.&lt;/p&gt;
&lt;p&gt;In our setting, the omitted variable bias is $\beta \gamma$ and we want it to converge to zero faster than $\frac{1}{\sqrt{n}}$.  Post-double selection guarantees that&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Reduced form&lt;/em&gt; selection (pre-testing): any &amp;ldquo;missing&amp;rdquo; variable $j$ has $|\beta_j| \leq \frac{c}{\sqrt{n}}$&lt;/li&gt;
&lt;li&gt;&lt;em&gt;First stage&lt;/em&gt; selection (additional): any &amp;ldquo;missing&amp;rdquo; variable $j$ has $|\delta_j| \leq \frac{c}{\sqrt{n}}$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;As a consequence, as long as the number of omitted variables is finite, the omitted variable bias is going to converge to zero at a rate $\frac{1}{n}$, which is faster than $\frac{1}{\sqrt{n}}$. &lt;strong&gt;Problem solved&lt;/strong&gt;!&lt;/p&gt;
&lt;h3 id=&#34;application&#34;&gt;Application&lt;/h3&gt;
&lt;p&gt;Let&amp;rsquo;s now go back to our example and test the post-double selection procedure. In practice, we want to do the following:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;First Stage&lt;/strong&gt; selection: regress &lt;code&gt;ads&lt;/code&gt; on &lt;code&gt;past_sales&lt;/code&gt;. Check if &lt;code&gt;past_sales&lt;/code&gt; is statistically significant&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Reduced Form&lt;/strong&gt; selection: regress &lt;code&gt;sales&lt;/code&gt; on &lt;code&gt;past_sales&lt;/code&gt;. Check if &lt;code&gt;past_sales&lt;/code&gt; is statistically significant&lt;/li&gt;
&lt;li&gt;Regress &lt;code&gt;sales&lt;/code&gt; on &lt;code&gt;ads&lt;/code&gt; and include &lt;code&gt;past_sales&lt;/code&gt; &lt;strong&gt;only if&lt;/strong&gt; it was significant in &lt;em&gt;either&lt;/em&gt; one of the two previous regressions&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;I update the &lt;code&gt;pre_test&lt;/code&gt; function from the first part of the post to compute also the post-double selection estimator.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def pre_test(d=&#39;ads&#39;, y=&#39;sales&#39;, x=&#39;past_sales&#39;, K=1000, **kwargs):
    
    # Init
    alphas = pd.DataFrame({&#39;Long&#39;: np.zeros(K), 
             &#39;Short&#39;: np.zeros(K), 
             &#39;Pre-test&#39;: np.zeros(K),
             &#39;Post-double&#39;: np.zeros(K)})

    # Loop over simulations
    for k in range(K):
        
        # Generate data
        df = dgp_pretest().generate_data(seed=k, **kwargs)
        
        # Compute coefficients
        alphas[&#39;Long&#39;][k] = smf.ols(f&#39;{y} ~ {d} + {x}&#39;, df).fit().params[1]
        alphas[&#39;Short&#39;][k] = smf.ols(f&#39;{y} ~ {d}&#39;, df).fit().params[1]
    
        # Compute significance of beta and gamma
        p_value_ydx = smf.ols(f&#39;{y} ~ {d} + {x}&#39;, df).fit().pvalues[2]
        p_value_yx = smf.ols(f&#39;{y} ~ {x}&#39;, df).fit().pvalues[1]
        p_value_dx = smf.ols(f&#39;{d} ~ {x}&#39;, df).fit().pvalues[1]
        
        # Select pre-test specification based on regression of y on d and x
        if p_value_ydx&amp;lt;0.05:
            alphas[&#39;Pre-test&#39;][k] = alphas[&#39;Long&#39;][k]
        else:
            alphas[&#39;Pre-test&#39;][k] = alphas[&#39;Short&#39;][k]
            
        # Select post-double specification based on regression of y on d and x
        if p_value_yx&amp;lt;0.05 or p_value_dx&amp;lt;0.05:
            alphas[&#39;Post-double&#39;][k] = alphas[&#39;Long&#39;][k]
        else:
            alphas[&#39;Post-double&#39;][k] = alphas[&#39;Short&#39;][k]
    
    return alphas
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;alphas = pre_test()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can now plot the distributions (over simulations) of the estimated coefficients.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def plot_alphas(alphas, true_alpha):
    
    # Init plot
    K = len(alphas.columns)
    fig, axes = plt.subplots(1, K, figsize=(4*K, 5), sharey=True, sharex=True)

    # Make one plot for each set of coefficients
    for i, key in enumerate(alphas.columns):
        axes[i].hist(alphas[key].values, bins=30, lw=.1, color=f&#39;C{int(i==3)*2}&#39;)
        axes[i].set_title(key)
        axes[i].axvline(true_alpha, c=&#39;r&#39;, ls=&#39;--&#39;)
        legend_text = [rf&#39;$\alpha=${true_alpha}&#39;, rf&#39;$\hat \alpha=${np.mean(alphas[key]):.4f}&#39;]
        axes[i].legend(legend_text, prop={&#39;size&#39;: 10}, loc=&#39;upper right&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plot_alphas(alphas, true_alpha=1)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/pds_17_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;As we can see, the post-double selection estimator always correctly selects the long regression and therefore has the correct distribution.&lt;/p&gt;
&lt;h3 id=&#34;double-checks&#34;&gt;Double-checks&lt;/h3&gt;
&lt;p&gt;In the last post, we ran some simulations in order to investigate when pre-testing bias emerges. We saw that pre-testing is a problem for&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Small sample sizes $n$&lt;/li&gt;
&lt;li&gt;Intermediate values of $\beta$&lt;/li&gt;
&lt;li&gt;When the value of $\beta$ depends on the sample size&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Let&amp;rsquo;s check that post-double selection removes regularization bias in &lt;strong&gt;all&lt;/strong&gt; the previous cases.&lt;/p&gt;
&lt;p&gt;First, let&amp;rsquo;s simulate the distribution of the post-double selection estimator $\hat \alpha_{postdouble}$ for different sample sizes.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;Ns = [100,300,1000,3000]
alphas = {f&#39;N = {n:.0f}&#39;:  pre_test(N=n) for n in Ns}
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def compare_alphas(alphas, true_alpha):
    
    # Init plot
    fig, axes = plt.subplots(1, len(alphas), figsize=(4*len(alphas), 5), sharey=True, sharex=True)

    # Make one plot for each set of coefficients
    for i, key in enumerate(alphas.keys()):
        axes[i].hist(alphas[key][&#39;Pre-test&#39;], bins=30, lw=.1, alpha=0.5)
        axes[i].hist(alphas[key][&#39;Post-double&#39;], bins=30, lw=.1, alpha=0.5, color=&#39;C2&#39;)
        axes[i].set_title(key)
        axes[i].axvline(true_alpha, c=&#39;r&#39;, ls=&#39;--&#39;)
        axes[i].legend([rf&#39;$\alpha=${true_alpha}&#39;, &#39;Pre-test&#39;, &#39;Post-double&#39;], 
                       prop={&#39;size&#39;: 10}, loc=&#39;upper right&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;compare_alphas(alphas, true_alpha=1)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/pds_23_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;For small samples, the distribution of the pre-testing estimator is not normal but rather bimodal. From the plots we can see that the post-double estimator is gaussian also in small sample sizes.&lt;/p&gt;
&lt;p&gt;Now we repeat the same exercise, but for different values of $\beta$, the coefficient of &lt;code&gt;past_sales&lt;/code&gt; on &lt;code&gt;sales&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;betas = 0.3 * np.array([0.1,0.3,1,3])
alphas = {f&#39;beta = {b:.2f}&#39;: pre_test(b=b) for b in betas}
compare_alphas(alphas, true_alpha=1)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/pds_25_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Again, the post-double selection estimator has a gaussian distribution irrespectively of the value of $\beta$, while he pre-testing estimator suffers from regularization bias.&lt;/p&gt;
&lt;p&gt;For the last simulation, we change both the coefficient and the sample size at the same time.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;betas = 0.3 * 30 / np.sqrt(Ns)
alphas = {f&#39;N = {n:.0f}&#39;:  pre_test(b=b, N=n) for n,b in zip(Ns,betas)}
compare_alphas(alphas, true_alpha=1)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/pds_27_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Also in this last case, the post-double selection estimator performs well and inference is not distorted.&lt;/p&gt;
&lt;h2 id=&#34;double-debiased-machine-learning&#34;&gt;Double Debiased Machine Learning&lt;/h2&gt;
&lt;p&gt;So far, we only have analyzed a linear, univariate example. What happens if the dimension of $X$ increases and we do not know the functional form through which $X$ affects $Y$ and $D$? In these cases, we can use machine learning algorithms to uncover these high-dimensional non-linear relationships.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://academic.oup.com/ectj/article/21/1/C1/5056401&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Chernozhukov, Chetverikov, Demirer, Duflo, Hansen, Newey, and Robins (2018)&lt;/a&gt; investigate this setting. In particular, the authors consider the following partially linear model.&lt;/p&gt;
&lt;p&gt;$$
Y = \alpha D + g(X) + u \
D = m(X) + v
$$&lt;/p&gt;
&lt;p&gt;where $Y$ is the outcome variable, $D$ is the treatment to interest and $X$ is a potentially high-dimensional set of control variables.&lt;/p&gt;
&lt;h3 id=&#34;naive-approach&#34;&gt;Naive approach&lt;/h3&gt;
&lt;p&gt;A naive approach to estimation of $\alpha$ using machine learning methods would be, for example, to construct a sophisticated machine learning estimator for learning the regression function $\alpha D$ + $g(X)$.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Split the sample in two: main sample and auxiliary sample [why? see note below]&lt;/li&gt;
&lt;li&gt;Use the auxiliary sample to estimate $\hat g(X)$&lt;/li&gt;
&lt;li&gt;Use the main sample to compute the orthogonalized component of $Y$ on $X$: $\ \hat u = Y - \hat{g} (X)$&lt;/li&gt;
&lt;li&gt;Use the main sample to estimate the residualized OLS estimator from regressing $\hat u$ on $D$&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;$$
\hat \alpha = \left( D&amp;rsquo; D \right) ^{-1} D&amp;rsquo; \hat u
$$&lt;/p&gt;
&lt;p&gt;This estimator is going to have &lt;strong&gt;two problems&lt;/strong&gt;:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Slow rate of convergence, i.e. slower than $\sqrt(n)$&lt;/li&gt;
&lt;li&gt;It will be biased because we are employing high dimensional regularized estimators (e.g. we are doing variable selection)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;Note (1)&lt;/strong&gt;: so far we have not talked about it, but variable selection procedure also introduce another type of bias: &lt;strong&gt;overfitting bias&lt;/strong&gt;. This bias emerges because of the fact that the sample used to select the variables is the same that is used to estimate the coefficient of interest. This bias is &lt;strong&gt;easily accounted for&lt;/strong&gt; with sample splitting: using different sub-samples for the selection and the estimation procedures.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Note (2)&lt;/strong&gt;: why can we use the residuals from step 3 to estimate $\alpha$ in step 4? Because of the &lt;a href=&#34;https://towardsdatascience.com/59f801eb3299&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Frisch-Waugh-Lovell theorem&lt;/a&gt;. If you are not familiar with it, I have written a &lt;a href=&#34;https://towardsdatascience.com/59f801eb3299&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;blog post on the Frisch-Waugh-Lovell theorem here&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;orthogonalization&#34;&gt;Orthogonalization&lt;/h3&gt;
&lt;p&gt;Now consider a second construction that employs an orthogonalized formulation obtained by directly partialling out the effect of $X$ from $D$ to obtain the orthogonalized regressor $v = D − m(X)$.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Split the sample in two: main sample and auxiliary sample&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Use the auxiliary sample to estimate $\hat g(X)$ from&lt;/p&gt;
&lt;p&gt;$$
Y = \alpha D + g(X) + u \
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Use the auxiliary sample to estimate $\hat m(X)$ from&lt;/p&gt;
&lt;p&gt;$$
D = m(X) + v
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Use the main sample to compute the orthogonalized component of $D$ on $X$ as&lt;/p&gt;
&lt;p&gt;$$
\hat v = D - \hat m(X)
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Use the main sample to estimate the double-residualized OLS estimator as&lt;/p&gt;
&lt;p&gt;$$
\hat \alpha = \left( \hat{v}&amp;rsquo; D \right) ^{-1} \hat{v}&amp;rsquo; \left( Y - \hat g(X) \right)
$$&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The estimator is &lt;strong&gt;root-N consistent&lt;/strong&gt;! This means that not only the estimator converges to the true value as the sample sizes increases (i.e. it&amp;rsquo;s consistent), but also its standard deviation does (i.e. it&amp;rsquo;s root-N consistent).&lt;/p&gt;
&lt;p&gt;However, the estimator still has a lower rate of convergence because of sample splitting. The problem is solved by inverting the split sample, re-estimating the coefficient and averaging the two estimates. Note that this procedure is valid since the two estimates are independent by the sample splitting procedure.&lt;/p&gt;
&lt;h3 id=&#34;a-cautionary-tale&#34;&gt;A Cautionary Tale&lt;/h3&gt;
&lt;p&gt;Before we conclude, I have to mention a recent research paper by &lt;a href=&#34;https://arxiv.org/abs/2108.11294&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Hünermund, Louw, and Caspi (2022)&lt;/a&gt;, in which the authors show that double-debiased machine learning can easily &lt;strong&gt;backfire&lt;/strong&gt;, if we apply blindly.&lt;/p&gt;
&lt;p&gt;The problem is related to &lt;strong&gt;bad control variables&lt;/strong&gt;. If you have never heard this term, I have written an introductory &lt;a href=&#34;https://towardsdatascience.com/b63dc69e3d8c&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;blog post on good and bad control variables here&lt;/a&gt;. In short, conditioning the analysis on additional features is not always good for causal inference. Depending on the setting, there might exist variables that we want to leave out of our analysis since their &lt;strong&gt;inclusion&lt;/strong&gt; can bias the coefficient of interest, preventing a causal interpretation. The simplest example is variables that are common outcomes, of both the treatment $D$ and outcome variable $Y$.&lt;/p&gt;
&lt;p&gt;The double-debiased machine learning model implicitly assumes that the control variables $X$ are (weakly) &lt;strong&gt;common causes&lt;/strong&gt; to both the outcome $Y$ and the treatment $D$. If this is the case, and no further mediated/indirect relationship exists between $X$ and $Y$, there is no problem. However, if, for example, some variable among the controls $X$ is a common effect instead of a common cause, its inclusion will bias the coefficient of interest. Moreover, this variable is likely to be highly correlated either with the outcome $Y$ or with the treatment $D$. In the latter case, this implies that post-double selection might include it in cases in which simple selection would have not. Therefore, in presence of bad control variables, doule-debiased machine learning might be &lt;strong&gt;even worse&lt;/strong&gt; than simple pre-testing.&lt;/p&gt;
&lt;p&gt;In short, as for any method, it is &lt;strong&gt;crucial&lt;/strong&gt; to have a clear understanding of the method&amp;rsquo;s assumptions and to always check for potential violations.&lt;/p&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;In this post, we have seen how to use post-double selection and, more generally, double debiased machine learning to get rid of an important source of bias: regularization bias.&lt;/p&gt;
&lt;p&gt;This contribution by Victor Chernozhukov and co-authors has been undoubtedly one of the most relevant advances in causal inferences in the last decade. It is now widely employed in the industry and included in the most used causal inference packages, such as &lt;a href=&#34;https://econml.azurewebsites.net/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;EconML&lt;/a&gt; (Microsoft) and &lt;a href=&#34;https://causalml.readthedocs.io/en/latest/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;causalml&lt;/a&gt; (Uber).&lt;/p&gt;
&lt;p&gt;If you (understandably) feel the need for more material on double-debiased machine learning, but you do not feel like reading academic papers (also very understandable), here is a good compromise.&lt;/p&gt;
&lt;br&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/eHOjmyoPCFU&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;br&gt;
&lt;p&gt;In this video lecture, Victor Chernozhukov himself presents the idea. The video lecture is relatively heavy on math and statistics, but you cannot get a more qualified and direct source than this!&lt;/p&gt;
&lt;h3 id=&#34;references&#34;&gt;References&lt;/h3&gt;
&lt;p&gt;[1] A. Belloni, D. Chen, V. Chernozhukov, C. Hansen, &lt;a href=&#34;https://onlinelibrary.wiley.com/doi/abs/10.3982/ECTA9626&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Sparse Models and Methods for Optimal Instruments With an Application to Eminent Domain&lt;/a&gt; (2012), &lt;em&gt;Econometrica&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;[2] A. Belloni, V. Chernozhukov, C. Hansen, &lt;a href=&#34;https://academic.oup.com/restud/article-abstract/81/2/608/1523757&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Inference on treatment effects after selection among high-dimensional controls&lt;/a&gt; (2014), &lt;em&gt;The Review of Economic Studies&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;[3] V. Chernozhukov, D. Chetverikov, M. Demirer, E. Duflo, C. Hansen, W. Newey, J. Robins, &lt;a href=&#34;https://academic.oup.com/ectj/article/21/1/C1/5056401&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Double/debiased machine learning for treatment and structural parameters&lt;/a&gt; (2018), &lt;em&gt;The Econometrics Journal&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;[4] P. Hünermund, B. Louw, I. Caspi, &lt;a href=&#34;https://arxiv.org/abs/2108.11294&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Double Machine Learning and Automated Confounder Selection - A Cautionary Tale&lt;/a&gt; (2022), &lt;em&gt;working paper&lt;/em&gt;.&lt;/p&gt;
&lt;h3 id=&#34;related-articles&#34;&gt;Related Articles&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/eb767a59975b&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Double Debiased Machine Learning (part 1)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/344ac1477699&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Understanding Omitted Variable Bias&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/59f801eb3299&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Understanding The Frisch-Waugh-Lovell Theorem&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/b63dc69e3d8c&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;DAGs and Control Variables&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;code&#34;&gt;Code&lt;/h3&gt;
&lt;p&gt;You can find the original Jupyter Notebook here:&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/pds.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/matteocourthoud/Blog-Posts/blob/main/pds.ipynb&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Double Debiased Machine Learning (part 1)</title>
      <link>https://matteocourthoud.github.io/post/pretest/</link>
      <pubDate>Sat, 04 Jun 2022 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/post/pretest/</guid>
      <description>&lt;p&gt;&lt;em&gt;Causal inference, machine learning and regularization bias&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;In causal inference, we often estimate causal effects by conditioning the analysis on other variables. We usually refer to these variables as &lt;strong&gt;control variables&lt;/strong&gt; or &lt;strong&gt;confounders&lt;/strong&gt;. In randomized control trials or AB tests, conditioning can increase the power of the analysis, by reducing imbalances that have emerged despite randomization. However, conditioning is even more important in observational studies, where, absent randomization, it might be &lt;a href=&#34;https://towardsdatascience.com/b63dc69e3d8c&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;essential to recover causal effects&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;When we have many control variables, we might want to &lt;strong&gt;select the most relevant ones&lt;/strong&gt;, ppossibly capturing nonlinearities and interactions. Machine learning algorithms are perfect for this task. However, in these cases, we are introducing a bias that is called &lt;strong&gt;regularization or pre-test, or feature selection bias&lt;/strong&gt;. In this and the next blog post, I try to explain the source of the bias and a very poweful solution called &lt;strong&gt;double debiased machine learning&lt;/strong&gt;, which has been probably one of the most relevant advancement at the intersection of machine learning and causal inference of the last decade.&lt;/p&gt;
&lt;h2 id=&#34;pre-testing&#34;&gt;Pre-Testing&lt;/h2&gt;
&lt;p&gt;Since this is a complex topic, let&amp;rsquo;s start with a simple example.&lt;/p&gt;
&lt;p&gt;Suppose we were a firm and we are interested in the &lt;strong&gt;effect of advertisement spending on revenue&lt;/strong&gt;: is advertisement worth the money? There are also a lot of other things that might influence sales, therefore, we are thinking of controlling for past sales in the analysis, in order to increase the power of our analysis.&lt;/p&gt;
&lt;p&gt;Assume the data generating process can be represented with the following &lt;a href=&#34;https://towardsdatascience.com/b63dc69e3d8c&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Directed Acyclic Graph (DAG)&lt;/strong&gt;&lt;/a&gt;. If you are not familiar with DAGs, I have written a short &lt;a href=&#34;https://towardsdatascience.com/b63dc69e3d8c&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;introduction here&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-mermaid&#34;&gt;flowchart LR
classDef included fill:#DCDCDC,stroke:#000000,stroke-width:2px;
classDef excluded fill:#ffffff,stroke:#000000,stroke-width:2px;
classDef unobserved fill:#ffffff,stroke:#000000,stroke-width:2px,stroke-dasharray: 5 5;

D((ad spend))
Z((past sales))
Y((sales))

D --&amp;gt; Y
Z -- ??? --&amp;gt; Y
Z --&amp;gt; D

class D,Y included;
class Z excluded;

linkStyle 0 stroke:#00ff00,stroke-width:4px;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I import the data generating process &lt;code&gt;dgp_tbd()&lt;/code&gt; from &lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/src/dgp.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;src.dgp&lt;/code&gt;&lt;/a&gt; and some plotting functions and libraries from &lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/src/utils.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;src.utils&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;%matplotlib inline
%config InlineBackend.figure_format = &#39;retina&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from src.utils import *
from src.dgp import dgp_pretest

df = dgp_pretest().generate_data()
df.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;ads&lt;/th&gt;
      &lt;th&gt;sales&lt;/th&gt;
      &lt;th&gt;past_sales&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;16.719800&lt;/td&gt;
      &lt;td&gt;19.196620&lt;/td&gt;
      &lt;td&gt;6.624345&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;7.732222&lt;/td&gt;
      &lt;td&gt;9.287491&lt;/td&gt;
      &lt;td&gt;4.388244&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;10.923469&lt;/td&gt;
      &lt;td&gt;11.816906&lt;/td&gt;
      &lt;td&gt;4.471828&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;8.457062&lt;/td&gt;
      &lt;td&gt;9.024376&lt;/td&gt;
      &lt;td&gt;3.927031&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;13.085146&lt;/td&gt;
      &lt;td&gt;12.814823&lt;/td&gt;
      &lt;td&gt;5.865408&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;We have data on $1000$ different markets, in which we observe current &lt;code&gt;sales&lt;/code&gt;, the amount spent in &lt;code&gt;advertisement&lt;/code&gt; and &lt;code&gt;past sales&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;We want to understand &lt;code&gt;ads&lt;/code&gt; spending is effective in increasing &lt;code&gt;sales&lt;/code&gt;. One possibility is to regress the latter on the former, using the following regression model, also called the &lt;strong&gt;short model&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;$$
\text{sales} = \alpha \cdot \text{ads} + \varepsilon
$$&lt;/p&gt;
&lt;p&gt;Should we also include &lt;code&gt;past sales&lt;/code&gt; in the regression? Then the regression model would be the following, also called &lt;strong&gt;long model&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;$$
\text{sales} = \alpha \cdot \text{ads} + \beta \cdot \text{past sales} + \varepsilon
$$&lt;/p&gt;
&lt;p&gt;Since we are not sure whether to condition the analysis on &lt;code&gt;past sales&lt;/code&gt;, we could &lt;strong&gt;let the data decide&lt;/strong&gt;: we could run the second regression and, if the effect of &lt;code&gt;past sales&lt;/code&gt;, $\beta$, is statistically significant, we are good with the long model, otherwise we run the short model.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;smf.ols(&#39;sales ~ ads + past_sales&#39;, df).fit().summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
       &lt;td&gt;&lt;/td&gt;         &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt;  &lt;td&gt;    0.1405&lt;/td&gt; &lt;td&gt;    0.185&lt;/td&gt; &lt;td&gt;    0.758&lt;/td&gt; &lt;td&gt; 0.448&lt;/td&gt; &lt;td&gt;   -0.223&lt;/td&gt; &lt;td&gt;    0.504&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;ads&lt;/th&gt;        &lt;td&gt;    0.9708&lt;/td&gt; &lt;td&gt;    0.030&lt;/td&gt; &lt;td&gt;   32.545&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    0.912&lt;/td&gt; &lt;td&gt;    1.029&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;past_sales&lt;/th&gt; &lt;td&gt;    0.3381&lt;/td&gt; &lt;td&gt;    0.095&lt;/td&gt; &lt;td&gt;    3.543&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    0.151&lt;/td&gt; &lt;td&gt;    0.525&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;It seems that the effect of &lt;code&gt;past sales&lt;/code&gt; on current &lt;code&gt;sales&lt;/code&gt; is positive and significant. Therefore, we are happy with our specification and we conclude that the effect of &lt;code&gt;ads&lt;/code&gt; on &lt;code&gt;sales&lt;/code&gt; is positive and significant with a 95% confidence interval of $[0.912, 1.029]$.&lt;/p&gt;
&lt;h2 id=&#34;the-bias&#34;&gt;The Bias&lt;/h2&gt;
&lt;p&gt;There is an &lt;strong&gt;issue&lt;/strong&gt; with this procedure: we are not taking into account the fact that we have run a test to decide whether to include &lt;code&gt;past_sales&lt;/code&gt; in the regression. The fact that we have decided to include &lt;code&gt;past_sales&lt;/code&gt; because its coefficient is significant &lt;em&gt;does&lt;/em&gt; have an effect on the inference on the effect of &lt;code&gt;ads&lt;/code&gt; on &lt;code&gt;sales&lt;/code&gt;, $\alpha$.&lt;/p&gt;
&lt;p&gt;The best way to understand the problem is through &lt;strong&gt;simulations&lt;/strong&gt;. Since we have access to the data generating process &lt;code&gt;dgp_pretest()&lt;/code&gt; (unlike in real life), we can just test what would happen if we were repeating this procedure multiple times:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;We draw a new sample from the data generating process.&lt;/li&gt;
&lt;li&gt;We regress &lt;code&gt;sales&lt;/code&gt; on &lt;code&gt;ads&lt;/code&gt; and &lt;code&gt;past_sales&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;If the coefficient of &lt;code&gt;past_sales&lt;/code&gt; is significant at the 95% level, we keep $\hat \alpha_{long}$ from (2).&lt;/li&gt;
&lt;li&gt;Otherwise, we regress &lt;code&gt;sales&lt;/code&gt; on &lt;code&gt;ads&lt;/code&gt; only, and we keep that coefficient $\hat \alpha_{short}$.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;I write a &lt;code&gt;pre_test&lt;/code&gt; function to implement the procedure above. I also save the coefficients from both regressions, long and short, and the chosen one, called the &lt;strong&gt;pre-test coefficient&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Reminder&lt;/strong&gt;: we are pre-testing the effect of &lt;code&gt;past_sales&lt;/code&gt; on &lt;code&gt;sales&lt;/code&gt; but the coefficient of interest is the one of &lt;code&gt;ads&lt;/code&gt; on &lt;code&gt;sales&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def pre_testing(d=&#39;ads&#39;, y=&#39;sales&#39;, x=&#39;past_sales&#39;, K=1000, **kwargs):
    
    # Init
    alpha = {&#39;Long&#39;: np.zeros(K), &#39;Short&#39;: np.zeros(K), &#39;Pre-test&#39;: np.zeros(K)}

    # Loop over simulations
    for k in range(K):
        
        # Generate data
        df = dgp_pretest().generate_data(seed=k, **kwargs)
        
        # Compute coefficients
        alpha[&#39;Long&#39;][k] = smf.ols(f&#39;{y} ~ {d} + {x}&#39;, df).fit().params[1]
        alpha[&#39;Short&#39;][k] = smf.ols(f&#39;{y} ~ {d}&#39;, df).fit().params[1]
    
        # Compute significance of beta
        p_value = smf.ols(f&#39;{y} ~ {d} + {x}&#39;, df).fit().pvalues[2]
        
        # Select specification based on p-value
        if p_value&amp;lt;0.05:
            alpha[&#39;Pre-test&#39;][k] = alpha[&#39;Long&#39;][k]
        else:
            alpha[&#39;Pre-test&#39;][k] = alpha[&#39;Short&#39;][k]
    
    return alpha
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;alphas = pre_testing()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can now plot the distributions (over simulations) of the estimated coefficients.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def plot_alphas(alphas, true_alpha):
    
    # Init plot
    fig, axes = plt.subplots(1, len(alphas), figsize=(4*len(alphas), 5), sharey=True, sharex=True)

    # Make one plot for each set of coefficients
    for i, key in enumerate(alphas.keys()):
        axes[i].hist(alphas[key], bins=30, lw=.1)
        axes[i].set_title(key)
        axes[i].axvline(true_alpha, c=&#39;r&#39;, ls=&#39;--&#39;)
        legend_text = [r&#39;$\alpha=%.0f$&#39; % true_alpha, r&#39;$\hat \alpha=%.4f$&#39; % np.mean(alphas[key])]
        axes[i].legend(legend_text, prop={&#39;size&#39;: 10}, loc=&#39;upper right&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plot_alphas(alphas, true_alpha=1)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/pretest_16_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;In the plot above, I have depicted the estimated coefficients, across simulations, for the different regression specifications.&lt;/p&gt;
&lt;p&gt;As we can see from the first plot, if we were always running the &lt;strong&gt;long regression&lt;/strong&gt;, our estimator $\hat \alpha_{long}$ would be unbiased and normally distributed. However, if we were always running the &lt;strong&gt;short regression&lt;/strong&gt; (second plot), our estimator $\hat \alpha_{short}$ would be &lt;strong&gt;biased&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;pre-testing&lt;/strong&gt; procedure generates an estimator $\hat \alpha_{pretest}$ that is a mix of the two: most of the times we select the correct specification, the long regression, but sometimes the pre-test fails to reject the null hypothesis of no effect of &lt;code&gt;past sales&lt;/code&gt; on &lt;code&gt;sales&lt;/code&gt;, $H_0 : \beta = 0$, and we select the incorrect specification, running the short regression.&lt;/p&gt;
&lt;p&gt;Importantly, the pre-testing procedure &lt;strong&gt;does not generate a biased estimator&lt;/strong&gt;. As we can see in the last plot, the estimated coefficient is very close to the true value, 1. The reason is that most of the time, the number of times we select the &lt;em&gt;short&lt;/em&gt; regression is sufficiently small not to introduce bias, but not small enough to have valid inference.&lt;/p&gt;
&lt;p&gt;Indeed, &lt;strong&gt;pre-testing distorts inference&lt;/strong&gt;: the distribution of the estimator $\hat \alpha_{pretest}$ is not normal anymore, but bimodal. The &lt;strong&gt;consequence&lt;/strong&gt; is that our confidence intervals for $\alpha$ are going to have the wrong coverage (contain the true effect with a different probability than the claimed one).&lt;/p&gt;
&lt;h2 id=&#34;when-is-pre-testing-a-problem&#34;&gt;When is pre-testing a problem?&lt;/h2&gt;
&lt;p&gt;The problem of pre-testing arises because of the bias generated by running the short regression: &lt;a href=&#34;https://towardsdatascience.com/344ac1477699&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;omitted variable bias (OVB)&lt;/strong&gt;&lt;/a&gt;. In you are not familiar with OVB, I have written a &lt;a href=&#34;https://towardsdatascience.com/344ac1477699&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;short introduction here&lt;/a&gt;. In general however, we can express the omitted variable bias introduced by regressing $Y$ on $D$ ignoring $X$ as&lt;/p&gt;
&lt;p&gt;$$
\text{OVB} = \beta \delta \qquad \text{ where } \qquad \beta := \frac{Cov(X, Y)}{Var(X)}, \quad \delta := \frac{Cov(D, X)}{Var(D)}
$$&lt;/p&gt;
&lt;p&gt;Where $\beta$ is the effect of $X$ (&lt;code&gt;past sales&lt;/code&gt; in our example) on $Y$ (&lt;code&gt;sales&lt;/code&gt;) and $\delta$ is the effect of $D$ (&lt;code&gt;ads&lt;/code&gt;) on $X$.&lt;/p&gt;
&lt;p&gt;Pre-testing is a &lt;strong&gt;problem&lt;/strong&gt; if&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;We run the short regression instead of the long one &lt;em&gt;and&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;The effect of the bias is sensible&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;What can help improving (1), i.e. the probability of correctly rejecting the null hypothesis of zero effect of &lt;code&gt;past sales&lt;/code&gt;, $H_0 : \beta = 0$? The answer is simple: a &lt;strong&gt;bigger sample size&lt;/strong&gt;. If we have more observations, we can more precisely estimate $\beta$ and it is going to be less likely that we commit a &lt;a href=&#34;https://en.wikipedia.org/wiki/Type_I_and_type_II_errors&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;type 2 error&lt;/a&gt; and run the short regression instead of the long one.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s simulate the estimated coefficient $\hat \alpha$ under different sample sizes. Remember that the sample size used until now is $N=1000$.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;Ns = [100,300,1000,3000]
alphas = {f&#39;N = {n:.0f}&#39;:  pre_testing(N=n)[&#39;Pre-test&#39;] for n in Ns}
plot_alphas(alphas, true_alpha=1)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/pretest_20_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;As we can see from the plots, as the sample size increases (left to right), the bias decreases and the distribution of the estimator $\hat \alpha_{pretest}$ converges to a normal distribution.&lt;/p&gt;
&lt;p&gt;What happens instead if the value of $\beta$ was different? It is probably going to affect point (2) in the previous paragraph, but how?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;If $\beta$ is &lt;strong&gt;very small&lt;/strong&gt;, it is going to be hard to detect it, and we will often end up running the &lt;em&gt;short&lt;/em&gt; regression, introducing a bias. However, if $\beta$ is very small, it also implies that the &lt;strong&gt;magnitude of the bias&lt;/strong&gt; is small and therefore it is not going to affect our estimate of $\alpha$ much&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;If $\beta$ is &lt;strong&gt;very big&lt;/strong&gt;, it is going to be easy to detect and we will often end up running the &lt;em&gt;long&lt;/em&gt; regression, avoiding the bias (which would have been very big though).&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Let&amp;rsquo;s simulate the estimated coefficient $\hat \alpha$ under different values of $\beta$. The true value used until now was $\beta = 0.3$.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;betas = 0.3 * np.array([0.1,0.3,1,3])
alphas = {f&#39;beta = {b:.2f}&#39;:  pre_testing(b=b)[&#39;Pre-test&#39;] for b in betas}
plot_alphas(alphas, true_alpha=1)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/pretest_22_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;As we can see from the plots, as the value of $\beta$ increases, the bias first appears and then disappears. When $\beta$ is small (left plot), we often choose the short regression, but the bias is small and the average estimate is very close to the true value. For intermediate values of $\beta$, the bias is sensible and it has a clear effect on inference. Lastly, for large values of $\beta$ instead (right plot), we always run the long regression and the bias disappears.&lt;/p&gt;
&lt;p&gt;But &lt;strong&gt;when is a coefficient big or small&lt;/strong&gt;? And big or small with respect to what? The answer is simple: with respect to the &lt;strong&gt;sample size&lt;/strong&gt;, or more accurately, with respect to the inverse of the square root of the sample size, $1 / \sqrt{n}$. The reason is deeply rooted in the &lt;a href=&#34;https://en.wikipedia.org/wiki/Central_limit_theorem&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Central Limit Theorem&lt;/a&gt;, but I won&amp;rsquo;t cover it here.&lt;/p&gt;
&lt;p&gt;The idea is easier to show than to explain, so let&amp;rsquo;s repeat the same simulation as above, but now we will increase both the coefficient and the sample size at the same time.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;betas = 0.3 * 30 / np.sqrt(Ns)
alphas = {f&#39;N = {n:.0f}&#39;:  pre_testing(b=b, N=n)[&#39;Pre-test&#39;] for n,b in zip(Ns,betas)}
plot_alphas(alphas, true_alpha=1)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/pretest_24_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;As we can see, now that $\beta$ is proportional to $1 / \sqrt{n}$, the distortion is not going away, not matter the sample size. Therefore, inference will always be wrong.&lt;/p&gt;
&lt;p&gt;While a coefficient that depends on the sample size might sound &lt;strong&gt;not intuitive&lt;/strong&gt;, it captures well the idea of &lt;strong&gt;magnitude&lt;/strong&gt; in a world where we do inference relying on asymptotic results, first among all the &lt;a href=&#34;https://en.wikipedia.org/wiki/Central_limit_theorem&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Central Limit Theorem&lt;/a&gt;. In fact, the Central Limit Theorem relieas on an infinitely large sample size. However, with an infinite amount of data, no coefficient is small and any non-zero effect is detected with certainty.&lt;/p&gt;
&lt;h2 id=&#34;pre-testing-and-machine-learning&#34;&gt;Pre-Testing and Machine Learning&lt;/h2&gt;
&lt;p&gt;So far we talked about a linear regression with only 2 variables. Where is the &lt;strong&gt;machine learning&lt;/strong&gt; we were promised?&lt;/p&gt;
&lt;p&gt;Usually we do not have just one control variable (or confounder), but many. Moreover, we might want to be flexible with respect to the functional form through which these control variables enter the model. In general, we will assume the following model:&lt;/p&gt;
&lt;p&gt;$$
Y = \alpha D + g_0(X) + u
\newline
D = m_0(X) + v
$$&lt;/p&gt;
&lt;p&gt;Where the effect of interest is still $\alpha$, $X$ is potentially high dimensional and we do not take a stand on the functional form through which $X$ influences $D$ or $Y$.&lt;/p&gt;
&lt;p&gt;In this setting, it is natural to use a machine learning algorithm to estimate $g_0$ and $m_0$. However, machine learning algorithms usually introduce a &lt;strong&gt;regularization bias&lt;/strong&gt; that is comparable to pre-testing.&lt;/p&gt;
&lt;p&gt;Possibly, the &amp;ldquo;simplest&amp;rdquo; way to think about it is &lt;a href=&#34;https://en.wikipedia.org/wiki/Lasso_%28statistics%29&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Lasso&lt;/a&gt;. Lasso is linear in $X$, with a penalization term that effectively just performs the variable selection we discussed above. Therefore, if we were to use Lasso of $X$ and $D$ on $Y$ we would be introducing regularization bias and inference would be distorted. The same goes for more complex algorithms.&lt;/p&gt;
&lt;p&gt;Lastly, you might still wonder &amp;ldquo;why is the model linear in the treatment variable $D$?&amp;rdquo;. Doing inference is much easier in linear model, not only for computational reasons but also for interpretation. Moreover, if the treatment $D$ is binary, the linear functional form is without loss of generality. A stronger assumption is the additive separability of $D$ and $g(X)$.&lt;/p&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;In this post, I have tried to explain how does regularization bias emerges and why it can the an issue in causal inference. This problem is inherently related to settings with many control variables or where we would like to have a model-free (i.e. non-parametric) when controlling for confounders. These are exactly the settings in which machine learning algorithms can be useful.&lt;/p&gt;
&lt;p&gt;In the next post, I will cover a simple and yet incredibly powerful solution to this problem: double-debiased machine learning.&lt;/p&gt;
&lt;h3 id=&#34;references&#34;&gt;References&lt;/h3&gt;
&lt;p&gt;[1] A. Belloni, D. Chen, V. Chernozhukov, C. Hansen, &lt;a href=&#34;https://onlinelibrary.wiley.com/doi/abs/10.3982/ECTA9626&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Sparse Models and Methods for Optimal Instruments With an Application to Eminent Domain&lt;/a&gt; (2012), &lt;em&gt;Econometrica&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;[2] A. Belloni, V. Chernozhukov, C. Hansen, &lt;a href=&#34;https://academic.oup.com/restud/article-abstract/81/2/608/1523757&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Inference on treatment effects after selection among high-dimensional controls&lt;/a&gt; (2014), &lt;em&gt;The Review of Economic Studies&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;[3] V. Chernozhukov, D. Chetverikov, M. Demirer, E. Duflo, C. Hansen, W. Newey, J. Robins, &lt;a href=&#34;https://academic.oup.com/ectj/article/21/1/C1/5056401&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Double/debiased machine learning for treatment and structural parameters&lt;/a&gt; (2018), &lt;em&gt;The Econometrics Journal&lt;/em&gt;.&lt;/p&gt;
&lt;h3 id=&#34;related-articles&#34;&gt;Related Articles&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/344ac1477699&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Understanding Omitted Variable Bias&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/59f801eb3299&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Understanding The Frisch-Waugh-Lovell Theorem&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/b63dc69e3d8c&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;DAGs and Control Variables&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;code&#34;&gt;Code&lt;/h3&gt;
&lt;p&gt;You can find the original Jupyter Notebook here:&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/pretest.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/matteocourthoud/Blog-Posts/blob/main/pretest.ipynb&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Understanding The Chi-Squared Test</title>
      <link>https://matteocourthoud.github.io/post/chisquared/</link>
      <pubDate>Tue, 31 May 2022 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/post/chisquared/</guid>
      <description>&lt;p&gt;If you search the Wikipedia definition of Chi-Squared test, you get the following definition:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Pearson&amp;rsquo;s chi-squared test $\chi^2$ is a statistical test applied to sets of categorical data to evaluate how likely it is that any observed difference between the sets arose by chance.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;What does it mean? Let&amp;rsquo;s see it together.&lt;/p&gt;
&lt;h2 id=&#34;test-1-discrete-distribution&#34;&gt;Test 1: Discrete Distribution&lt;/h2&gt;
&lt;p&gt;Suppose you want to &lt;strong&gt;test whether a dice is fair&lt;/strong&gt;. You throw the dice 60 times and you count the number of times you get each outcome.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s simulate some data (from a fair dice).&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;%matplotlib inline
%config InlineBackend.figure_format = &#39;retina&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from src.utils import *
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def generate_data_dice(N=60, seed=1):
    np.random.seed(seed) # Set seed for replicability
    dice_numbers = [1,2,3,4,5,6]  # Dice numbers
    dice_throws = np.random.choice(dice_numbers, size=N)  # Actual dice throws
    data = pd.DataFrame({&amp;quot;dice number&amp;quot;: dice_numbers,
                         &amp;quot;observed&amp;quot;: [sum(dice_throws==n) for n in dice_numbers],
                         &amp;quot;expected&amp;quot;: int(N / 6)})
    return data
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;data_dice = generate_data_dice()
data_dice
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;dice number&lt;/th&gt;
      &lt;th&gt;observed&lt;/th&gt;
      &lt;th&gt;expected&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;10&lt;/td&gt;
      &lt;td&gt;10&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;14&lt;/td&gt;
      &lt;td&gt;10&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;6&lt;/td&gt;
      &lt;td&gt;10&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;8&lt;/td&gt;
      &lt;td&gt;10&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;12&lt;/td&gt;
      &lt;td&gt;10&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;5&lt;/th&gt;
      &lt;td&gt;6&lt;/td&gt;
      &lt;td&gt;10&lt;/td&gt;
      &lt;td&gt;10&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;If we were throwing the dice &lt;strong&gt;a lot&lt;/strong&gt; of times, we would expect the same number of observations for each outcome. However, there is inherent noise in the process. How can we tell whether the fact that we didn&amp;rsquo;t get exactly 10 observations for each outcome is just due to randomness or it&amp;rsquo;s because the dice is unfair?&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;idea&lt;/strong&gt; is to compute some statistic whose distribution is known under the assumption that the dice is fair, and then check if its value is &lt;strong&gt;&amp;ldquo;unusual&amp;rdquo;&lt;/strong&gt; or not. If the value is particularly &amp;ldquo;unusual&amp;rdquo;, we reject the null hypothesis that the dice is fair.&lt;/p&gt;
&lt;p&gt;In our case, the statistic we choose is the chi-squared $\chi^{2}$ test-statistic.&lt;/p&gt;
&lt;p&gt;The value of the &lt;a href=&#34;https://en.wikipedia.org/wiki/Pearson%27s_chi-squared_test&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Pearson&amp;rsquo;s chi-squared test-statistic&lt;/strong&gt;&lt;/a&gt; is&lt;/p&gt;
&lt;p&gt;$$
T_{\chi^2} = \sum _{i=1}^{n} \frac{(O_i - E_i)^{2}}{E_i} = N \sum _{i=1}^{n} \frac{\left(O_i/N - p_i \right)^2 }{p_i}
$$&lt;/p&gt;
&lt;p&gt;where&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$O_{i}$ = the number of observations of type i.&lt;/li&gt;
&lt;li&gt;$N$ = total number of observations&lt;/li&gt;
&lt;li&gt;$E_{i}=N * p_{i}$ = the expected (theoretical) count of type $i$, asserted by the null hypothesis that the fraction of type $i$ in the population is $p_{i}$&lt;/li&gt;
&lt;li&gt;$n$ = the number of cells in the table.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def compute_chi2_stat(data):
    return sum( (data.observed - data.expected)**2 / data.expected )
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;chi2_stat = compute_chi2_stat(data_dice)
chi2_stat
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;4.0
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What do we make of this number? Is it &lt;strong&gt;unusual&lt;/strong&gt;?&lt;/p&gt;
&lt;p&gt;If the dice were fair, the test Pearson&amp;rsquo;s chi-squared test-statistic $T_{\chi^2}$ would be distributed as a &lt;strong&gt;chi-squared distribution&lt;/strong&gt; with $k-1$ degrees of freedom, $\chi^2_{k-1}$. For the moment, take this claim at face value, we will verify it later, both empirically and theoretically. We will also discuss the degrees of freedom in detail later on.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Important!&lt;/strong&gt; Do not confuse the chi-squared test statistic (a number) with the chi-squared distribution (a distribution).&lt;/p&gt;
&lt;p&gt;What does a chi-squared distribution with $n-1$ degrees of freedom, $\chi^2_{k-1}$, look like?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from scipy.stats import chi2

x = np.arange(0, 30, 0.001) # x-axis ranges from 0 to 30 with .001 steps
chi2_5_pdf = chi2.pdf(x, df=5) # Chi-square distribution with 5 degrees of freedom
plt.plot(x, chi2_5_pdf); 
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/chisquared_13_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;How does the value of the statistic we have observed compares with its the distribution under the null hypothesis of a fair dice?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plt.plot(x, chi2.pdf(x, df=5), label=&#39;chi2 distribution&#39;);
plt.axvline(chi2_stat, color=&#39;k&#39;, label=&#39;chi2 statistic&#39;)
plt.legend();
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/chisquared_15_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;The test statistic seems to fall well within the distribution, i.e. it does not seem to be an unusual event. Indeed, the question we want to answer is: &amp;ldquo;&lt;em&gt;under the null hypothesis that the dice is fair, how unlikely is the statistic we have observed?&lt;/em&gt;&amp;rdquo;.&lt;/p&gt;
&lt;p&gt;The last component we need in order to build a hypothesis test is a level of &lt;strong&gt;confidence&lt;/strong&gt;, i.e. a threshold of &amp;ldquo;unlikeliness&amp;rdquo; of an event, below which we declare that the event is too unlikely under the model, for the model to be true. Let&amp;rsquo;s say we decide to set that threshold at 5%.&lt;/p&gt;
&lt;p&gt;If the likelihood of observing an even that (or more) extreme than the one we have actually observed is less than 5%, we reject the null hypothesis that the dice is fair.&lt;/p&gt;
&lt;p&gt;What is this value for a chi-squared distribution with 5 degrees of freedom? We can compute the percent point function (ppf) of 95% for the chi-squared distribution, which is essentially the inverse of the &lt;a href=&#34;https://en.wikipedia.org/wiki/Cumulative_distribution_function&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;cumulative distribution function&lt;/a&gt;. This value is often called the &lt;strong&gt;critical value&lt;/strong&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;z95 = chi2.ppf(0.95, df=5)
z95
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;11.070497693516351
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Since our value is smaller than the critical value, we do not reject the null. The critical value is indeed critical because it splits the domain of the test statistic into two areas: the &lt;strong&gt;rejection area&lt;/strong&gt;, where we reject the null hypothesis, and the non-rejection area, where we don&amp;rsquo;t.&lt;/p&gt;
&lt;p&gt;We can plot the rejection and non-rejection areas in a plot.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def plot_test(x, stat, df):
    z95 = chi2.ppf(0.95, df=df)
    chi2_pdf = chi2.pdf(x, df=df)
    plt.plot(x, chi2_pdf);
    plt.fill_between(x[x&amp;gt;z95], chi2_pdf[x&amp;gt;z95], color=&#39;r&#39;, alpha=0.4, label=&#39;rejection area&#39;)
    plt.fill_between(x[x&amp;lt;z95], chi2_pdf[x&amp;lt;z95], color=&#39;g&#39;, alpha=0.4, label=&#39;non-rejection area&#39;)
    plt.axvline(chi2_stat, color=&#39;k&#39;, label=&#39;chi2 statistic&#39;)
    plt.ylim(0, plt.ylim()[1])
    plt.legend();
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plot_test(x, chi2_stat, df=5)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/chisquared_20_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;From the plot, we can see that we do not reject the null hypothesis that the dice is fair for our value of the $\chi^2$ test statistic.&lt;/p&gt;
&lt;h2 id=&#34;why-the-chi-squared-distribution&#34;&gt;Why the Chi-squared Distribution?&lt;/h2&gt;
&lt;p&gt;How do we know that that particular statistic has that particular distribution?&lt;/p&gt;
&lt;p&gt;Before digging into the math, we can check this claim via &lt;strong&gt;simulation&lt;/strong&gt;. Since we have access to the data generating process, we can repeat the procedure above many times, i.e.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;roll a (fair) dice 60 times&lt;/li&gt;
&lt;li&gt;compute the chi-square statistic&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;and then plot the distribution of chi square statistics.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def simulate_chi2stats(K, N, dgp):
    chi2_stats = [compute_chi2_stat(dgp(seed=k)) for k in range(K)]
    return np.array(chi2_stats)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;chi2_stats = simulate_chi2stats(K=100, N=60, dgp=generate_data_dice)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plt.hist(chi2_stats, density=True, bins=30, alpha=0.3, color=&#39;C0&#39;);
plt.plot(x, chi2_5_pdf);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/chisquared_26_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Since we only did it 100 times, the distribution looks pretty coarse but vaguely close to its theoretical counterpart. Let&amp;rsquo;s try 1000 times.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;chi2_stats = simulate_chi2stats(K=1000, N=60, dgp=generate_data_dice)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plt.hist(chi2_stats, density=True, bins=30, alpha=0.3, color=&#39;C0&#39;);
plt.plot(x, chi2_5_pdf);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/chisquared_29_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;The empirical distribution of the test statistic is indeed very close to its theoretical counterpart.&lt;/p&gt;
&lt;h2 id=&#34;some-statistics&#34;&gt;Some Statistics&lt;/h2&gt;
&lt;p&gt;Why does the distribution of the test statistic look like that? Let&amp;rsquo;s now dig deeper into the math.&lt;/p&gt;
&lt;p&gt;There are two things we need to know in order to understand the answer:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the Central Limit Theorem&lt;/li&gt;
&lt;li&gt;the relationship between a chi-squared and a normal distribution&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The Wikipedia definition of the &lt;a href=&#34;https://en.wikipedia.org/wiki/Central_limit_theorem&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Central Limit Theorem&lt;/a&gt; says that&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;&lt;em&gt;In probability theory, the central limit theorem (CLT) establishes that, in many situations, when independent random variables are summed up, their properly normalized sum tends toward a normal distribution (informally a bell curve) even if the original variables themselves are not normally distributed.&lt;/em&gt;&amp;rdquo;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Where does a normal distribution come up in our case? If we look at a single row in our data, i.e. the occurrences of a specific dice throw, it can be interpreted as the sum of realization from a &lt;a href=&#34;https://en.wikipedia.org/wiki/Bernoulli_distribution&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Bernoulli distribution&lt;/a&gt; with probability 1/6.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;&lt;em&gt;In probability theory and statistics, the Bernoulli distribution is the discrete probability distribution of a random variable which takes the value $1$ with probability $p$ and the value $0$ with probability $q=1-p$.&lt;/em&gt;&amp;rdquo;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;In our case, the probability of getting a particular number is exactly 1/6. What is the distribution of the sum of its realizations? The Central Limit Theorem also tells us that:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;&lt;em&gt;If $X_1, X_2, \dots , X_n, \dots$ are random samples drawn from a population with overall mean $\mu$ and finite variance $\sigma^2$, and if $\bar X_n$ is the sample mean of the first $n$ samples, then the limiting form of the distribution,&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;$$
Z = \lim_{n \to \infty} \sqrt{n} \left( \frac{\bar X_n - \mu }{\sigma} \right)
$$&lt;/p&gt;
&lt;p&gt;&lt;em&gt;is a standard normal distribution.&lt;/em&gt;&amp;rdquo;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Therefore, in our case, the distribution of the sum of Bernoulli distributions with mean $p$ is distributed as a normal distribution with&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;mean $p$&lt;/li&gt;
&lt;li&gt;variance $p * (1-p)$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Therefore, we can obtain a random variable that is asymptotically standard normal distributed as&lt;/p&gt;
&lt;p&gt;$$
\lim_{n \to \infty} \ \sqrt{n} \left( \frac {\bar X_n - p}{\sqrt{p * (1-p)}} \right) \sim N(0,1)
$$&lt;/p&gt;
&lt;p&gt;Our last piece: what is a &lt;a href=&#34;https://en.wikipedia.org/wiki/Chi-squared_distribution&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;chi-squared distribution&lt;/a&gt;? The Wikipedia definition says&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;&lt;em&gt;If $Z_1, &amp;hellip;, Z_k$ are independent, standard normal random variables, then the sum of their squares,&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;$$
Q = \sum_{i=1}^k Z_i^2
$$
&lt;em&gt;is distributed according to the chi-squared distribution with $k$ degrees of freedom.&lt;/em&gt;&amp;rdquo;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;I.e. the sum of standard normal distributions is a chi-squared distribution, where the &lt;strong&gt;degrees of freedom&lt;/strong&gt; indicate the number of normal distributions we are summing over. Since the normalized sum of realizations of each dice number should converge to a standard normal distribution, their sum of squares should converge to a chi-squared distribution. I.e.&lt;/p&gt;
&lt;p&gt;$$
\lim_{n \to \infty} \ \sum_k n \frac{(\bar X_n - p)^2}{p * (1-p)} \sim \chi^2_k
$$&lt;/p&gt;
&lt;p&gt;There is just one issue: the last distribution is not really independent from the others. In fact, as soon as we know that we have thrown 60 dices and how many 1s, 2s, 3s, 4s, and 5s we got, we can compute the number of 6s. Therefore, we should exclude one distribution since only 5 (or, in general, $k-1$) are truly independent.&lt;/p&gt;
&lt;p&gt;In practice, however, we sum all distributions, but then we scale them down by multiplying them by $(1-p)$ so that we have&lt;/p&gt;
&lt;p&gt;$$
\lim_{n \to \infty} \ \sum_k n \frac{(\bar X_n - p)^2}{p} \sim \chi^2_{k-1}
$$&lt;/p&gt;
&lt;p&gt;which is exactly the formula we used to compute the test statistic:&lt;/p&gt;
&lt;p&gt;$$
T_{\chi^2} = \sum _{i=1}^{n} \frac{(O_i - E_i)^{2}}{E_i} = N \sum _{i=1}^{n} \frac{\left(O_i/N - p_i \right)^2 }{p_i}
$$&lt;/p&gt;
&lt;h2 id=&#34;test-2-independence&#34;&gt;Test 2: Independence&lt;/h2&gt;
&lt;p&gt;Chi-squared tests can also be used to &lt;strong&gt;test independence between two variables&lt;/strong&gt;. The idea is fundamentally the same as the test in the previous section: checking systematic differences between observed and expected values, across different variables.&lt;/p&gt;
&lt;p&gt;Suppose you have &lt;strong&gt;data on grades in a classroom, by gender&lt;/strong&gt;. Grades go from $1$ to $4$. Assuming males and females are equally prepared for the test, you want to test whether there has been discrimination in grading.&lt;/p&gt;
&lt;p&gt;The problem is again asserting whether the observed differences are random or systematic.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s generate some data (under the no discrimination assumption).&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def generate_data_grades(N_male=60, N_female=40, seed=1):
    np.random.seed(seed)
    grade_scale = [1,2,3,4]
    p = [0.1, 0.2, 0.5, 0.2]
    grades_male = np.random.choice(grade_scale, size=N_male, p=p)
    grades_female = np.random.choice(grade_scale, size=N_female, p=p)
    data = pd.DataFrame({&amp;quot;grade&amp;quot;: grade_scale + grade_scale,
                          &amp;quot;gender&amp;quot;: [&amp;quot;male&amp;quot; for i in grade_scale] + [&amp;quot;female&amp;quot; for i in grade_scale],
                          &amp;quot;observed&amp;quot;: [sum(grades_male==n) for n in grade_scale] + [sum(grades_female==n) for n in grade_scale],
                        })  
    data[&#39;expected gender&#39;] = data.groupby(&amp;quot;gender&amp;quot;)[&amp;quot;observed&amp;quot;].transform(&amp;quot;mean&amp;quot;) 
    data[&#39;expected grade&#39;] = data.groupby(&amp;quot;grade&amp;quot;)[&amp;quot;observed&amp;quot;].transform(&amp;quot;mean&amp;quot;) 
    data[&#39;expected&#39;] = data[&#39;expected gender&#39;] * data[&#39;expected grade&#39;] / data[&#39;observed&#39;].mean()
    return data
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;data_grades = generate_data_grades()
data_grades
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;grade&lt;/th&gt;
      &lt;th&gt;gender&lt;/th&gt;
      &lt;th&gt;observed&lt;/th&gt;
      &lt;th&gt;expected gender&lt;/th&gt;
      &lt;th&gt;expected grade&lt;/th&gt;
      &lt;th&gt;expected&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;male&lt;/td&gt;
      &lt;td&gt;9&lt;/td&gt;
      &lt;td&gt;15.0&lt;/td&gt;
      &lt;td&gt;5.5&lt;/td&gt;
      &lt;td&gt;6.6&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;male&lt;/td&gt;
      &lt;td&gt;14&lt;/td&gt;
      &lt;td&gt;15.0&lt;/td&gt;
      &lt;td&gt;10.5&lt;/td&gt;
      &lt;td&gt;12.6&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;male&lt;/td&gt;
      &lt;td&gt;27&lt;/td&gt;
      &lt;td&gt;15.0&lt;/td&gt;
      &lt;td&gt;24.5&lt;/td&gt;
      &lt;td&gt;29.4&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;male&lt;/td&gt;
      &lt;td&gt;10&lt;/td&gt;
      &lt;td&gt;15.0&lt;/td&gt;
      &lt;td&gt;9.5&lt;/td&gt;
      &lt;td&gt;11.4&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;female&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;10.0&lt;/td&gt;
      &lt;td&gt;5.5&lt;/td&gt;
      &lt;td&gt;4.4&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;5&lt;/th&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;female&lt;/td&gt;
      &lt;td&gt;7&lt;/td&gt;
      &lt;td&gt;10.0&lt;/td&gt;
      &lt;td&gt;10.5&lt;/td&gt;
      &lt;td&gt;8.4&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;6&lt;/th&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;female&lt;/td&gt;
      &lt;td&gt;22&lt;/td&gt;
      &lt;td&gt;10.0&lt;/td&gt;
      &lt;td&gt;24.5&lt;/td&gt;
      &lt;td&gt;19.6&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;7&lt;/th&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;female&lt;/td&gt;
      &lt;td&gt;9&lt;/td&gt;
      &lt;td&gt;10.0&lt;/td&gt;
      &lt;td&gt;9.5&lt;/td&gt;
      &lt;td&gt;7.6&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;Has there been discrimination? We again compare observed and expected grades, where expected grades are computed under the independence assumption: as the product of the marginal distributions of &lt;code&gt;grade&lt;/code&gt; and &lt;code&gt;gender&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;The value of the test-statistic is&lt;/p&gt;
&lt;p&gt;$$
T_{\chi^2} = \sum_{i=1}^r \sum_{j=1}^c \frac{(O_{i,j} - E_{i,j})^2 }{ E_{i,j} } = N \sum_{i,j} p_{i \cdot} p_{\cdot j} \left( \frac{O_{i,j}/N - p_{i \cdot} p_{\cdot j} }{ p_{i \cdot} p_{\cdot j}} \right)^2
$$&lt;/p&gt;
&lt;p&gt;where&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;$N$ is the total sample size (the sum of all cells in the table)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$p_{i \cdot} = \frac{O_{i\cdot }}{N} = \sum_{j=1}^{c} \frac{O_{i,j}}{N}$ is the fraction of observations of type i ignoring the column attribute (fraction of row totals), and&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$p_{\cdot j} = \frac{O_{\cdot j}}{N} = \sum_{i=1}^{r} \frac{O_{i,j}}{N}$ is the fraction of observations of type j ignoring the row attribute (fraction of column totals).&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;So the formula for the test statistic is the same&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;chi2_stat = compute_chi2_stat(data_grades)
chi2_stat
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;3.490327550477927
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As before, we can double-check whether the statistic is indeed distributed as a chi-squared with $k-1$ degrees of freedom by simulating the data generating process.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;chi2_stats = simulate_chi2stats(K=1000, N=60, dgp=generate_data_grades)
plt.hist(chi2_stats, density=True, bins=30, alpha=0.3, color=&#39;C0&#39;);
plt.plot(x, chi2_5_pdf);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/chisquared_41_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;The two distributions do not look similar anymore.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;What happened?&lt;/strong&gt; We forgot to change the degrees of freedom! The general formula for the degrees of freedom when testing the independence of variables is $(N_i - 1) \times (N_j - 1)$. So in our case, it&amp;rsquo;s $(4-1) \times (2-1) = 3$.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;chi2_3_pdf = chi2.pdf(x, df=3)
chi2_stats = simulate_chi2stats(K=1000, N=60, dgp=generate_data_grades)
plt.hist(chi2_stats, density=True, bins=30, alpha=0.3, color=&#39;C0&#39;);
plt.plot(x, chi2_3_pdf);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/chisquared_43_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Now the empirical distribution is close to its theoretical counterpart.&lt;/p&gt;
&lt;p&gt;Do we &lt;strong&gt;reject&lt;/strong&gt; the null hypothesis of independent distributions of gender and grades? We can visualize the value of the test statistic together with the rejection areas.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plot_test(x, chi2_stat, df=3)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/chisquared_45_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;We &lt;strong&gt;do not reject&lt;/strong&gt; the null hypothesis of independend distributions of gender and grades.&lt;/p&gt;
&lt;h2 id=&#34;test-3-continuous-distributions&#34;&gt;Test 3: Continuous Distributions&lt;/h2&gt;
&lt;p&gt;As we have seen, the chi-square test can be used to compare observed means/frequencies against a null hypothesis. How can we use this statistic to &lt;strong&gt;test a distributional assumption&lt;/strong&gt;?&lt;/p&gt;
&lt;p&gt;The answer is simple: we can construct conditional means. The easiest way to do it is to &lt;strong&gt;bin the data&lt;/strong&gt; into intervals and then check if the observed frequencies match the expected probabilities, within each bin.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: a good practice is to have equally sized bins, in terms of expected probabilities, since it ensures that we have as many observations in each bin as possible.&lt;/p&gt;
&lt;p&gt;As an example, let&amp;rsquo;s assume we have to analyze the customer service of a firm. We would like to understand if the number of complains follows an &lt;a href=&#34;https://en.wikipedia.org/wiki/Exponential_distribution&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;exponential distribution&lt;/a&gt; with paramenter $\lambda=1$.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from scipy.stats import expon
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def generate_complaints_data(N=100, cuts=4, seed=2):
    np.random.seed(seed)
    complaints = np.random.exponential(size=N)
    cat, bins = pd.qcut(complaints, cuts, retbins=True)
    p = [expon.cdf(bins[n+1]) - expon.cdf(bins[n]) for n in range(len(bins)-1)]
    data = pd.DataFrame({&amp;quot;bin&amp;quot;: cat.unique(),
                         &amp;quot;observed&amp;quot;: [sum(cat==n) for n in cat.unique()],
                         &amp;quot;expected&amp;quot;: np.dot(p, N)})
    return data, complaints
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;data_complaints, complaints = generate_complaints_data()
data_complaints
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;bin&lt;/th&gt;
      &lt;th&gt;observed&lt;/th&gt;
      &lt;th&gt;expected&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;(0.297, 0.573]&lt;/td&gt;
      &lt;td&gt;25&lt;/td&gt;
      &lt;td&gt;24.360534&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;(0.0121, 0.297]&lt;/td&gt;
      &lt;td&gt;25&lt;/td&gt;
      &lt;td&gt;17.974853&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;(0.573, 1.025]&lt;/td&gt;
      &lt;td&gt;25&lt;/td&gt;
      &lt;td&gt;20.489741&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;(1.025, 5.092]&lt;/td&gt;
      &lt;td&gt;25&lt;/td&gt;
      &lt;td&gt;35.258339&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;We have split the data into equally sized bins of size 25 and we have computed the expected number of observations within each bin, if the data was indeed exponentially distributed with parameter $\lambda=1$.&lt;/p&gt;
&lt;p&gt;We can &lt;strong&gt;plot&lt;/strong&gt; the observed and realized distribution of the data.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Plot data
plt.hist(complaints, density=True, bins=30, alpha=0.3, color=&#39;C0&#39;);
exp_pdf = expon.pdf(x)
plt.plot(x, exp_pdf);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/chisquared_53_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;The two distributions seem close but we need a test statistic in order to assess whether the differences are random or systematic.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;chi2_stat = compute_chi2_stat(data_complaints)
chi2_stat
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;6.739890904809741
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Do we reject the null hypothesis that the data is drawn from an exponential distribution? We decide by comparing the value of the test statistic with the rejection areas.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plot_test(x, chi2_stat, df=3)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/chisquared_57_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Since the test statistic falls outside of the rejection area, we &lt;strong&gt;do not reject&lt;/strong&gt; the null hypothesis that the data is drawn from an exponential distribution.&lt;/p&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;In this tutorial, we have seen how to perform 3 hypoteses tests&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;testing if a set of means or sums is coming from the expected distribution&lt;/li&gt;
&lt;li&gt;testing if two distributions are independent or not&lt;/li&gt;
&lt;li&gt;testing a specific data generating process&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The underlying principle is the same: testing discrepancies between expected and observed count data.&lt;/p&gt;
&lt;p&gt;The key statistic is Pearson&amp;rsquo;s chi-square statistic and the key distribution is the chi-squared distribution. We have seen how to compute the statistic, why it has a chi-squared distribution, and how to use this information to perform a statistical hypothesis test.&lt;/p&gt;
&lt;h3 id=&#34;code&#34;&gt;Code&lt;/h3&gt;
&lt;p&gt;You can find the original Jupyter Notebook here:&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/chisquared.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/chisquared.ipynb&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Comparing Distributions, From Zero to Hero</title>
      <link>https://matteocourthoud.github.io/post/distr_compare/</link>
      <pubDate>Sun, 29 May 2022 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/post/distr_compare/</guid>
      <description>&lt;p&gt;The problem of comparing distributions often arises in causal inference when we have to &lt;strong&gt;assess the quality of randomization&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;When we want to assess the causal effect of a policy (or, feature, campaign, drug, &amp;hellip;), the golden standard in causal inference are &lt;a href=&#34;https://en.wikipedia.org/wiki/Randomized_controlled_trial&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;randomized control trials&lt;/strong&gt;&lt;/a&gt;, also known in the industry as &lt;a href=&#34;https://de.wikipedia.org/wiki/A/B-Test&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;A/B tests&lt;/strong&gt;&lt;/a&gt;. In practice, we select a sample for the study and we randomly split it into a &lt;strong&gt;control&lt;/strong&gt; and a &lt;strong&gt;treatment&lt;/strong&gt; group, to compare the outcomes between the two groups. The idea is that, under a set of assumption, randomization assures that only difference between the two groups is the treatment, on average. Therefore, we can attribute the differences in outcomes to the treatment effect alone.&lt;/p&gt;
&lt;p&gt;The problem is that, despite randomization, the two groups are never identical. However, sometimes, they are not even &amp;ldquo;similar&amp;rdquo;. For example, we might have more females in one group, or older people, etc.. (we usually call these characteristics, &lt;em&gt;covariates&lt;/em&gt; or &lt;em&gt;control variables&lt;/em&gt;). When it happens, we cannot be certain anymore that the differences in the outcome is only due to the treatment and cannot be attributed to the &lt;strong&gt;inbalanced covariates&lt;/strong&gt; instead. Therefore, it is always important, after randomization, to check whether all observed variables are balance across groups and whether there are no systematic differences. Another option, to be certain that certain covariates are balanced, is &lt;a href=&#34;https://en.wikipedia.org/wiki/Stratified_sampling&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;em&gt;stratified sampling&lt;/em&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;In this blog post, we are going to see different ways to compare two (or more) distributions and assess the magnitude and significance of their difference. We are going to consider two different approaches, &lt;strong&gt;graphical&lt;/strong&gt; and &lt;strong&gt;numerical&lt;/strong&gt;. The two approaches generally trade-off &lt;strong&gt;intuition&lt;/strong&gt; with &lt;strong&gt;rigour&lt;/strong&gt;: from plots we can assess subtle differences but it&amp;rsquo;s hard to assess whether these differences are systematic or due to noise.&lt;/p&gt;
&lt;h2 id=&#34;the-data&#34;&gt;The Data&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s assume we need to perform an experiment on a group of individuals and we have randomized them into a treatment and control group. We would like them to be &lt;strong&gt;as comparable as possible&lt;/strong&gt;, in order to attribute any difference between the two groups to the treatment effect alone. We also have divided the treatment group in different &lt;em&gt;arms&lt;/em&gt; for testing different treatments.&lt;/p&gt;
&lt;p&gt;For this example, I have simulated a dataset of 1000 individuals, for whom we observe a set of characteristics. I import the data generating process &lt;code&gt;dgp_rnd_assignment()&lt;/code&gt; from &lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/src/dgp.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;src.dgp&lt;/code&gt;&lt;/a&gt; and some plotting functions and libraries from &lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/src/utils.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;src.utils&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;%matplotlib inline
%config InlineBackend.figure_format = &#39;retina&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from src.utils import *
from src.dgp import dgp_rnd_assignment

df = dgp_rnd_assignment().generate_data()
df.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;group&lt;/th&gt;
      &lt;th&gt;arm&lt;/th&gt;
      &lt;th&gt;gender&lt;/th&gt;
      &lt;th&gt;age&lt;/th&gt;
      &lt;th&gt;income&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;control&lt;/td&gt;
      &lt;td&gt;arm 2&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;29.0&lt;/td&gt;
      &lt;td&gt;3967.16&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;control&lt;/td&gt;
      &lt;td&gt;arm 4&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;32.0&lt;/td&gt;
      &lt;td&gt;2927.77&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;treatment&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;29.0&lt;/td&gt;
      &lt;td&gt;1642.66&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;control&lt;/td&gt;
      &lt;td&gt;arm 4&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;25.0&lt;/td&gt;
      &lt;td&gt;1867.64&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;treatment&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;32.0&lt;/td&gt;
      &lt;td&gt;3202.35&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;We have information on $1000$ individuals, for which we observe &lt;code&gt;gender&lt;/code&gt;, &lt;code&gt;age&lt;/code&gt; and &lt;code&gt;income&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;We now want to understand whether the treatment and control &lt;code&gt;groups&lt;/code&gt; are comparable or if there are systematic difference between them.&lt;/p&gt;
&lt;h2 id=&#34;plots&#34;&gt;Plots&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s concentrate on one variable: &lt;code&gt;income&lt;/code&gt;. Does the income distribution differ between the two groups?&lt;/p&gt;
&lt;p&gt;A first approach could be the &lt;a href=&#34;https://en.wikipedia.org/wiki/Box_plot&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;boxplot&lt;/strong&gt;&lt;/a&gt;. The boxplot is a good trade-off between summary statistics and data visualization. The center and the borders of the &lt;strong&gt;box&lt;/strong&gt; represent the &lt;em&gt;median&lt;/em&gt; and the first (Q1) and third &lt;em&gt;quartile&lt;/em&gt; (Q3), respectively. The &lt;strong&gt;whiskers&lt;/strong&gt; instead, extend to the first data points that are more than 1.5 times the &lt;em&gt;interquartile range&lt;/em&gt; (Q3 - Q1) outside the box. The points that fall outside of the whiskers are plotted individually.&lt;/p&gt;
&lt;p&gt;Therefore, the boxplot provides both summary statistics (the box and the whiskers) and direct data visualization (the extreme data points).&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sns.boxplot(x=&#39;group&#39;, y=&#39;income&#39;, data=df);
plt.title(&amp;quot;Boxplot&amp;quot;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/distr_compare_9_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;It seems that the &lt;code&gt;income&lt;/code&gt; distribution in the &lt;code&gt;treatment&lt;/code&gt; group is slightly more dispersed: the orange box is larger and the extreme &lt;code&gt;treatment&lt;/code&gt; points cover a wider range. However, the &lt;strong&gt;issue&lt;/strong&gt; with the boxplot is that it hides the shape of the data, telling us some summary statistics but not showing us the actual data distribution.&lt;/p&gt;
&lt;p&gt;The most intuitive way to plot a distribution is the &lt;strong&gt;histogram&lt;/strong&gt;. The histogram groups the data into equally spaced &lt;strong&gt;bins&lt;/strong&gt; and plots the number of observations within each bin.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sns.histplot(x=&#39;income&#39;, data=df, hue=&#39;group&#39;, bins=50);
plt.title(&amp;quot;Histogram&amp;quot;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/distr_compare_11_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;There are multiple &lt;strong&gt;issues&lt;/strong&gt; with this plot:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The two histograms are not comparable: we would like a density, not a count&lt;/li&gt;
&lt;li&gt;The number of bins is arbitrary&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We can solve the first issue using the &lt;code&gt;stat&lt;/code&gt; option to plot the &lt;code&gt;density&lt;/code&gt; instead of the count and setting the &lt;code&gt;common_norm&lt;/code&gt; option to &lt;code&gt;False&lt;/code&gt; to use the same normalization.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sns.histplot(x=&#39;income&#39;, data=df, hue=&#39;group&#39;, bins=50, stat=&#39;density&#39;, common_norm=False);
plt.title(&amp;quot;Density Histogram&amp;quot;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/distr_compare_13_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Now the two histograms are comparable!&lt;/p&gt;
&lt;p&gt;However, an important &lt;strong&gt;issue&lt;/strong&gt; remains: the size of the bins is arbitrary. If we bunch the data less, we end up with bins with one observation at most, if we bunch the data more, we lose information. This is a classical &lt;a href=&#34;https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;bias-variance trade-off&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;One possible solution is to use a &lt;a href=&#34;https://en.wikipedia.org/wiki/Kernel_density_estimation&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;kernel density function&lt;/strong&gt;&lt;/a&gt; that tries to approximate the histogram with a continuous function, using &lt;a href=&#34;https://en.wikipedia.org/wiki/Kernel_density_estimation&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;kernel density estimation (KDE)&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sns.kdeplot(x=&#39;income&#39;, data=df, hue=&#39;group&#39;, common_norm=False);
plt.title(&amp;quot;Kernel Density Function&amp;quot;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/distr_compare_15_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;We can now visualize both distributions very intuitively.&lt;/p&gt;
&lt;p&gt;If we had multiple categories, a very similar plot is the &lt;strong&gt;violinplot&lt;/strong&gt;. The violinplot also performs kernel density estimation, but plots separate densities along the y axis so that they don&amp;rsquo;t overlap. By default, it also adds a miniature boxplot inside.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sns.violinplot(x=&#39;arm&#39;, y=&#39;income&#39;, data=df);
plt.title(&amp;quot;Violin Plot&amp;quot;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/distr_compare_17_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;From the plot, it seems that the estimated kernel density of &lt;code&gt;income&lt;/code&gt; is very similar across treatment arms.&lt;/p&gt;
&lt;p&gt;However, the &lt;strong&gt;issue&lt;/strong&gt; with kernel density estimation is that it is somehow a black-box and might mask relevant features of the data.&lt;/p&gt;
&lt;p&gt;A much more transparent representation of the two distribution is their &lt;a href=&#34;https://en.wikipedia.org/wiki/Cumulative_distribution_function&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;cumulative distribution function&lt;/strong&gt;&lt;/a&gt;. At each point of the x axis (&lt;code&gt;income&lt;/code&gt;) we plot the percentage of data points that have an equal or lower value. The main advantages of the cumulative distribution function are that&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;we do not need to make any arbitrary choice (e.g. number of bins)&lt;/li&gt;
&lt;li&gt;we do not need to perform any approximation (e.g. with KDE)&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sns.histplot(x=&#39;income&#39;, data=df, hue=&#39;group&#39;, bins=len(df), stat=&amp;quot;density&amp;quot;,
             element=&amp;quot;step&amp;quot;, fill=False, cumulative=True, common_norm=False);
plt.title(&amp;quot;Cumulative distribution function&amp;quot;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/distr_compare_19_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;We can now clearly see that there are relatively more observations with low income in the treatment group than in the control group. In fact, the blue line is above the orange line on the right and below the orange line on the left.&lt;/p&gt;
&lt;p&gt;A related alternative method is the &lt;strong&gt;qq-plot&lt;/strong&gt;, where &lt;em&gt;q&lt;/em&gt; stands for quantile. The qq-plot plots the quantiles of the two distributions against each other. If the distributions are the same, we should get the 45 degree line.&lt;/p&gt;
&lt;p&gt;There is no native qq-plot function in Python and, while the &lt;code&gt;statsmodels&lt;/code&gt; package provides a &lt;a href=&#34;https://www.statsmodels.org/dev/generated/statsmodels.graphics.gofplots.qqplot.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;qqplot&lt;/code&gt; function&lt;/a&gt;, it is quite cumbersome. Therefore, we will do it by hand.&lt;/p&gt;
&lt;p&gt;First, we need to compute the quartiles of the two groups, using the &lt;code&gt;percentile&lt;/code&gt; function.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;income = df[&#39;income&#39;].values
income_t = df.loc[df.group==&#39;treatment&#39;, &#39;income&#39;].values
income_c = df.loc[df.group==&#39;control&#39;, &#39;income&#39;].values

df_pct = pd.DataFrame()
df_pct[&#39;q_treatment&#39;] = np.percentile(income_t, range(100))
df_pct[&#39;q_control&#39;] = np.percentile(income_c, range(100))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we can plot the two quantile distributions against each other, plus the 45-degree line, representing the benchmark perfect fit.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plt.scatter(x=&#39;q_control&#39;, y=&#39;q_treatment&#39;, data=df_pct, label=&#39;Actual fit&#39;);
sns.lineplot(x=&#39;q_control&#39;, y=&#39;q_control&#39;, data=df_pct, color=&#39;r&#39;, label=&#39;Line of perfect fit&#39;);
plt.xlabel(&#39;Quantile of income, control group&#39;)
plt.ylabel(&#39;Quantile of income, treatment group&#39;)
plt.legend()
plt.title(&amp;quot;QQ plot&amp;quot;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/distr_compare_23_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;The qq-plot delivers a very similar insight with respect to the cumulative distribution plot: income in the treatment group is generally lower.&lt;/p&gt;
&lt;h2 id=&#34;tests&#34;&gt;Tests&lt;/h2&gt;
&lt;p&gt;So far, we have seen different ways to visualize differences between distributions. The main advantage of visualization is &lt;strong&gt;intuition&lt;/strong&gt;: we can eyeball the differences and intuitively assess them.&lt;/p&gt;
&lt;p&gt;However, we might want to be more &lt;strong&gt;rigorous&lt;/strong&gt; and try to assess the &lt;strong&gt;statistical significance&lt;/strong&gt; of the difference between the distributions, i.e. answer the question &amp;ldquo;&lt;em&gt;is the observed difference systematic or due to sampling variation?&lt;/em&gt;&amp;rdquo;.&lt;/p&gt;
&lt;p&gt;We are now going to analyze different tests to discern two distributions from each other.&lt;/p&gt;
&lt;h3 id=&#34;t-test&#34;&gt;T-test&lt;/h3&gt;
&lt;p&gt;The first and most common test is the &lt;a href=&#34;https://en.wikipedia.org/wiki/Student%27s_t-test&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;student t-test&lt;/a&gt;. T-tests are generally used to &lt;strong&gt;compare means&lt;/strong&gt;. In this case, we want to test whether the means of the &lt;code&gt;income&lt;/code&gt; distribution is the same across the two groups. The test statistic for the two-means comparison test is given by:&lt;/p&gt;
&lt;p&gt;$$
stat = \frac{|\bar x_1 - \bar x_2|}{\sqrt{s_1 / n_1 + s_2 / n_2}}
$$&lt;/p&gt;
&lt;p&gt;Where $\bar x$ is the sample mean and $s$ is the sample standard deviation. Under mild conditions, the test statistic is asymptotically distributed as a &lt;a href=&#34;https://en.wikipedia.org/wiki/Student%27s_t-distribution&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;student t&lt;/a&gt; distribution.&lt;/p&gt;
&lt;p&gt;We use the &lt;code&gt;ttest_ind&lt;/code&gt; function from &lt;code&gt;scipy&lt;/code&gt; to perform the t-test. The function returns both the test statistic and the implied &lt;a href=&#34;https://en.wikipedia.org/wiki/P-value&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;p-value&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from scipy.stats import ttest_ind

stat, p_value = ttest_ind(income_c, income_t)
print(f&amp;quot;t-test: statistic={stat:.4f}, p-value={p_value:.4f}&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;t-test: statistic=-1.3192, p-value=0.1874
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In general, it is common practice to always perform this test on all variables, when we are running a randomized control trial or A/B test. The results of these tests are usually collected into a table that is called &lt;strong&gt;balance table&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;We can use the &lt;a href=&#34;https://causalml.readthedocs.io/en/latest/causalml.html#module-causalml.match&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;create_table_one&lt;/code&gt;&lt;/a&gt; function from the &lt;a href=&#34;https://causalml.readthedocs.io/en/latest/about.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;causalml&lt;/code&gt;&lt;/a&gt; library to generate the balance table. As the name of the function suggests, the balance table should always be the &lt;strong&gt;first table&lt;/strong&gt; you present when performing an A/B test.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from causalml.match import create_table_one

df[&#39;treatment&#39;] = df[&#39;group&#39;]==&#39;treatment&#39;
create_table_one(df, &#39;treatment&#39;, [&#39;gender&#39;, &#39;age&#39;, &#39;income&#39;])
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;Control&lt;/th&gt;
      &lt;th&gt;Treatment&lt;/th&gt;
      &lt;th&gt;SMD&lt;/th&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;Variable&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;n&lt;/th&gt;
      &lt;td&gt;704&lt;/td&gt;
      &lt;td&gt;296&lt;/td&gt;
      &lt;td&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;age&lt;/th&gt;
      &lt;td&gt;31.94 (8.53)&lt;/td&gt;
      &lt;td&gt;35.88 (7.78)&lt;/td&gt;
      &lt;td&gt;0.4822&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;gender&lt;/th&gt;
      &lt;td&gt;0.51 (0.50)&lt;/td&gt;
      &lt;td&gt;0.58 (0.49)&lt;/td&gt;
      &lt;td&gt;0.1419&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;income&lt;/th&gt;
      &lt;td&gt;3166.07 (1321.89)&lt;/td&gt;
      &lt;td&gt;3296.32 (1645.58)&lt;/td&gt;
      &lt;td&gt;0.0873&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;In the first two columns, we can see the average of the different variables across the treatment and control groups, with standard errors in parenthesis. In the &lt;strong&gt;last column&lt;/strong&gt;, we have the p-values of the t-test for the null hypothesis of zero difference in means.&lt;/p&gt;
&lt;p&gt;From the table, we observe that we cannot reject the null hypothesis of zero difference in mean for any variable, at the 95% confidence level.&lt;/p&gt;
&lt;h3 id=&#34;chi-squared-test&#34;&gt;Chi-Squared Test&lt;/h3&gt;
&lt;p&gt;The &lt;a href=&#34;https://matteocourthoud.github.io/post/chisquared/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;chi-squared test&lt;/a&gt; is a very powerful test that can be used in many different settings. If you want to find out more about it, I have written a very comprehensive &lt;a href=&#34;https://matteocourthoud.github.io/post/chisquared/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;blog post here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;One of the &lt;strong&gt;least known applications&lt;/strong&gt; of the chi-squared test, is testing the similarity between two distributions. The &lt;strong&gt;idea&lt;/strong&gt; is to bin the observations of the two groups. If the two distributions were the same, we would expect the same frequency of observations in each bin. Importantly, we need enough observations in each bin, in order for the test to be valid. I generate bins corresponding to deciles of the distribution of &lt;code&gt;income&lt;/code&gt; in the control group.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_bins = pd.DataFrame()
_, bins = pd.qcut(income_c, q=10, retbins=True)
df_bins[&#39;bin&#39;] = pd.cut(income_c, bins=bins).value_counts().index
df_bins[&#39;income_c&#39;] = pd.cut(income_c, bins=bins).value_counts().values
df_bins[&#39;income_t&#39;] = pd.cut(income_t, bins=bins).value_counts().values

df_bins
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;bin&lt;/th&gt;
      &lt;th&gt;income_c&lt;/th&gt;
      &lt;th&gt;income_t&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;(808.96, 1730.77]&lt;/td&gt;
      &lt;td&gt;70&lt;/td&gt;
      &lt;td&gt;42&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;(1730.77, 2127.948]&lt;/td&gt;
      &lt;td&gt;70&lt;/td&gt;
      &lt;td&gt;36&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;(2127.948, 2411.451]&lt;/td&gt;
      &lt;td&gt;70&lt;/td&gt;
      &lt;td&gt;25&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;(2411.451, 2670.312]&lt;/td&gt;
      &lt;td&gt;71&lt;/td&gt;
      &lt;td&gt;20&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;(2670.312, 2935.05]&lt;/td&gt;
      &lt;td&gt;70&lt;/td&gt;
      &lt;td&gt;26&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;5&lt;/th&gt;
      &lt;td&gt;(2935.05, 3208.662]&lt;/td&gt;
      &lt;td&gt;70&lt;/td&gt;
      &lt;td&gt;13&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;6&lt;/th&gt;
      &lt;td&gt;(3208.662, 3542.805]&lt;/td&gt;
      &lt;td&gt;71&lt;/td&gt;
      &lt;td&gt;25&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;7&lt;/th&gt;
      &lt;td&gt;(3542.805, 3972.996]&lt;/td&gt;
      &lt;td&gt;70&lt;/td&gt;
      &lt;td&gt;26&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;8&lt;/th&gt;
      &lt;td&gt;(3972.996, 4912.782]&lt;/td&gt;
      &lt;td&gt;70&lt;/td&gt;
      &lt;td&gt;41&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;9&lt;/th&gt;
      &lt;td&gt;(4912.782, 11634.39]&lt;/td&gt;
      &lt;td&gt;71&lt;/td&gt;
      &lt;td&gt;42&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;We can now perform the test by comparing the frequencies of the two distributions, across bins. The test statistic is given by:&lt;/p&gt;
&lt;p&gt;$$
stat = \sum _{i=1}^{n} \frac{(O_i - E_i)^{2}}{E_i}
$$&lt;/p&gt;
&lt;p&gt;Where the bins are indexed by $i$ and $O$ is the observed number of data points in bin $i$ and $E$ is the expected number of data points in bin $i$. Since we generated the bins using deciles of the distribution of &lt;code&gt;income&lt;/code&gt; in the control group, we expect the number of observations per bin in the treatment group to be the same across bins. Under mild assumptions, the test statistic is asymptocally distributed as a &lt;a href=&#34;https://en.wikipedia.org/wiki/Chi-squared_distribution&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;chi-squared&lt;/a&gt; distribution.&lt;/p&gt;
&lt;p&gt;To compute the test statistic and the p-value of the test, we use the &lt;code&gt;chisquare&lt;/code&gt; function from &lt;code&gt;scipy&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from scipy.stats import chisquare

df_bins[&#39;income_t_norm&#39;] = df_bins[&#39;income_t&#39;] / np.sum(df_bins[&#39;income_t&#39;]) * np.sum(df_bins[&#39;income_c&#39;])
stat, p_value = chisquare(df_bins[&#39;income_c&#39;], df_bins[&#39;income_t_norm&#39;])
print(f&amp;quot;Chi-squared Test: statistic={stat:.4f}, p-value={p_value:.4f}&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Chi-squared Test: statistic=95.2526, p-value=0.0000
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The p-value is practically zero, implying that we reject the null hypothesis of no difference between the two distributions.&lt;/p&gt;
&lt;h3 id=&#34;kolmogorov-smirnov-test&#34;&gt;Kolmogorov-Smirnov Test&lt;/h3&gt;
&lt;p&gt;The idea of the &lt;a href=&#34;https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Kolmogorov-Smirnov test&lt;/a&gt;, is to &lt;strong&gt;compare the cumulative distributions&lt;/strong&gt; of the two groups. In particular, the Kolmogorov-Smirnov test statistic is the maximum absolute difference between the two cumulative distributions.&lt;/p&gt;
&lt;p&gt;$$
stat = \sup _{x} \ \Big| \ F_1(x) - F_2(x) \ \Big|
$$&lt;/p&gt;
&lt;p&gt;Where $F_1$ and $F_2$ are the two cumulative distribution functions and $x$ are the values of the underlying variable. Under mild conditions, the asymptotic distribution of the Kolmogorov-Smirnov test statistic is known.&lt;/p&gt;
&lt;p&gt;To better understand the test, let&amp;rsquo;s plot the cumulative distribution functions and the test statistic. First, we compute the cumulative distribution functions.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_ks = pd.DataFrame()
df_ks[&#39;income&#39;] = np.sort(df[&#39;income&#39;].unique())
df_ks[&#39;F_control&#39;] = df_ks[&#39;income&#39;].apply(lambda x: np.mean(income_c&amp;lt;=x))
df_ks[&#39;F_treatment&#39;] = df_ks[&#39;income&#39;].apply(lambda x: np.mean(income_t&amp;lt;=x))
df_ks.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;income&lt;/th&gt;
      &lt;th&gt;F_control&lt;/th&gt;
      &lt;th&gt;F_treatment&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;808.96&lt;/td&gt;
      &lt;td&gt;0.001420&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;831.93&lt;/td&gt;
      &lt;td&gt;0.002841&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;893.28&lt;/td&gt;
      &lt;td&gt;0.002841&lt;/td&gt;
      &lt;td&gt;0.003378&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;925.08&lt;/td&gt;
      &lt;td&gt;0.004261&lt;/td&gt;
      &lt;td&gt;0.003378&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;951.21&lt;/td&gt;
      &lt;td&gt;0.004261&lt;/td&gt;
      &lt;td&gt;0.006757&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;We now need to find the point where the absolute distance between the cumulative distribution functions is largest.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;k = np.argmax( np.abs(df_ks[&#39;F_control&#39;] - df_ks[&#39;F_treatment&#39;]))
tstat = np.abs(df_ks[&#39;F_treatment&#39;][k] - df_ks[&#39;F_control&#39;][k])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can visualize the value of the test statistic, by plotting the two cumulative distribution functions and the value of the test statistic.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;y = (df_ks[&#39;F_treatment&#39;][k] + df_ks[&#39;F_control&#39;][k])/2
plt.plot(&#39;income&#39;, &#39;F_control&#39;, data=df_ks, label=&#39;Control&#39;)
plt.plot(&#39;income&#39;, &#39;F_treatment&#39;, data=df_ks, label=&#39;Treatment&#39;)
plt.errorbar(x=df_ks[&#39;income&#39;][k], y=y, yerr=tstat/2, color=&#39;k&#39;,
             capsize=5, mew=3, label=f&amp;quot;Test statistic: {tstat:.4f}&amp;quot;)
plt.legend(loc=&#39;center right&#39;);
plt.title(&amp;quot;Kolmogorov-Smirnov Test&amp;quot;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/distr_compare_45_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;From the plot, we can see that the value of the test statistic corresponds to the distance between the two cumulative distributions at &lt;code&gt;income&lt;/code&gt;=4000. For that value of &lt;code&gt;income&lt;/code&gt;, we have the largest imbalance between the two groups.&lt;/p&gt;
&lt;p&gt;We can now perform the actual test using the &lt;code&gt;kstest&lt;/code&gt; function from &lt;code&gt;scipy&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from scipy.stats import kstest

stat, p_value = kstest(income_t, income_c)
print(f&amp;quot; Kolmogorov-Smirnov Test: statistic={stat:.4f}, p-value={p_value:.4f}&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt; Kolmogorov-Smirnov Test: statistic=0.0881, p-value=0.0730
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The p-value is still above 5%: we do not reject the null hypothesis that the two distributions are the same, with 95% confidence.&lt;/p&gt;
&lt;h3 id=&#34;permutation-testing&#34;&gt;Permutation Testing&lt;/h3&gt;
&lt;p&gt;A non-parametric alternative is permutation testing. The idea is that, under the null hypothesis, the two distributions should be the same, therefore &lt;strong&gt;shuffling&lt;/strong&gt; the observations across groups, should not significantly alter any statistic.&lt;/p&gt;
&lt;p&gt;We can then chose any statistic and compute how much more extreme it is for different permutations, with respect to its value in the original sample. For example, let&amp;rsquo;s use as a test statistic the sample mean of the treatment group.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;stats = [np.mean(np.random.choice(income, size=len(income_t), replace=False)) for _ in range(1000)]
p_value = np.mean(stats &amp;gt; np.mean(income_t))

print(f&amp;quot;Permutation test: p-value={p_value:.4f}&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Permutation test: p-value=0.0820
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The permutation test gives us a p-value very similar to the ones obtained with the other tests.&lt;/p&gt;
&lt;p&gt;How do we &lt;strong&gt;interpret&lt;/strong&gt; the p-value? It means that he sample mean of the treatment group in the data is larger than $1 - 0.082 = 91.8%$ of the sample means of the treatment group across the permuted samples.&lt;/p&gt;
&lt;p&gt;We can &lt;strong&gt;visualize&lt;/strong&gt; the test, by plotting the distribution of the test statistics against its sample value.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plt.hist(stats, label=&#39;Permutation Statistics&#39;);
plt.axvline(x=np.mean(income_t), c=&#39;r&#39;, ls=&#39;--&#39;, label=&#39;Sample Statistic&#39;);
plt.legend();
plt.title(&#39;Permutation Test&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/distr_compare_53_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;As we can see, the sample statistic is quite extreme with respect to the values in the permuted samples, but not excessively.&lt;/p&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;TBD&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Omitted Variable Bias And What Can We Do About It</title>
      <link>https://matteocourthoud.github.io/post/ovb/</link>
      <pubDate>Wed, 25 May 2022 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/post/ovb/</guid>
      <description>&lt;p&gt;In causal inference, &lt;strong&gt;bias&lt;/strong&gt; is extremely problematic because it makes inference not valid. Bias generally means that an estimator will not deliver the estimate of the true effect, on average.&lt;/p&gt;
&lt;p&gt;This is why, in general, we prefer estimators that are &lt;strong&gt;unbiased&lt;/strong&gt;, at the cost of a higher variance, i.e. more noise. Does it mean that every biased estimator is useless? Actually no. Sometimes, with domain knowledge, we can still draw causal conclusions even with a biased estimator.&lt;/p&gt;
&lt;p&gt;In this post, we are going to review a specific but frequent source of bias, &lt;strong&gt;omitted variable bias (OVB)&lt;/strong&gt;. We are going to explore the causes of the bias and leverage these insights to make causal statements, despite the bias.&lt;/p&gt;
&lt;h2 id=&#34;theory&#34;&gt;Theory&lt;/h2&gt;
&lt;p&gt;Suppose we are interested in the effect of a variable $D$ on a variable $y$. However, there is a third variable $Z$ that we do not observe and that is correlated with both $D$ and $Y$. Assume the data generating process can be represented with the following &lt;a href=&#34;https://towardsdatascience.com/b63dc69e3d8c&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Directed Acyclic Graph (DAG)&lt;/strong&gt;&lt;/a&gt;. If you are not familiar with DAGs, I have written a short &lt;a href=&#34;https://towardsdatascience.com/b63dc69e3d8c&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;introduction here&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-mermaid&#34;&gt;flowchart LR
classDef included fill:#DCDCDC,stroke:#000000,stroke-width:2px;
classDef excluded fill:#ffffff,stroke:#000000,stroke-width:2px;
classDef unobserved fill:#ffffff,stroke:#000000,stroke-width:2px,stroke-dasharray: 5 5;

D((D))
Z((Z))
Y((Y))

D --&amp;gt; Y
Z --&amp;gt; D
Z --&amp;gt; Y

class D,Y excluded;
class Z unobserved;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Since there is a &lt;a href=&#34;https://towardsdatascience.com/b63dc69e3d8c&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;backdoor path&lt;/strong&gt;&lt;/a&gt; from $D$ to $y$ passing through $Z$, we need to condition our analysis on $Z$ in order to recover the causal effect of $D$ on $y$. If we could observe $Z$, we would run a linear regression of $y$ on $D$ and $Z$ to estimate the following model:&lt;/p&gt;
&lt;p&gt;$$
y = \alpha D + \gamma Z + \varepsilon
$$&lt;/p&gt;
&lt;p&gt;where $\alpha$ is the effect of interest. This regression is usually referred to as the &lt;strong&gt;long regression&lt;/strong&gt; since it includes all variables of the model.&lt;/p&gt;
&lt;p&gt;However, since we do not observe $Z$, we have to estimate the following model:&lt;/p&gt;
&lt;p&gt;$$
y = \alpha D + u
$$&lt;/p&gt;
&lt;p&gt;The corresponding regression is usually referred to as the &lt;strong&gt;short regression&lt;/strong&gt; since it does not include all the variables of the model&lt;/p&gt;
&lt;p&gt;What is the &lt;strong&gt;consequence&lt;/strong&gt; of estimating the short regression when the true model is the long one?&lt;/p&gt;
&lt;p&gt;In that case, the OLS estimator of $\alpha$ is&lt;/p&gt;
&lt;p&gt;$$
\begin{align}
\hat \alpha &amp;amp;= \frac{Cov(D, y)}{Var(D)} =
\newline
&amp;amp;= \frac{Cov(D, \alpha D + \gamma Z + \varepsilon)}{Var(D)} =
\newline
&amp;amp;= \frac{Cov(D, \alpha D)}{Var(D)} + \frac{Cov(D, \gamma Z)}{Var(D)} + \frac{Cov(D, \varepsilon)}{Var(D)} =
\newline
&amp;amp;= \alpha + \underbrace{ \gamma \frac{Cov(D, Z)}{Var(D)} }_{\text{omitted variable bias}}
\end{align}
$$&lt;/p&gt;
&lt;p&gt;Therefore, we can write the &lt;strong&gt;omitted variable bias&lt;/strong&gt; as&lt;/p&gt;
&lt;p&gt;$$
\text{OVB} = \gamma \delta \qquad \text{ where } \qquad \delta := \frac{Cov(D, Z)}{Var(D)}
$$&lt;/p&gt;
&lt;p&gt;The beauty of this formula is its &lt;strong&gt;interpretability&lt;/strong&gt;: the omitted variable bias consists in just &lt;strong&gt;two components&lt;/strong&gt;, both extremely easy to interpret.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\gamma$: the effect of $Z$ on $y$&lt;/li&gt;
&lt;li&gt;$\delta$: the effect of $D$ on $Z$&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;additional-controls&#34;&gt;Additional Controls&lt;/h3&gt;
&lt;p&gt;What happens if we had &lt;strong&gt;additional control variables&lt;/strong&gt; in the regression? For example, assume that besides the variable of interest $D$, we also observe a vector of other variables $X$ so that the &lt;strong&gt;long regression&lt;/strong&gt; is&lt;/p&gt;
&lt;p&gt;$$
y = \alpha D + \beta X + \gamma Z + \varepsilon
$$&lt;/p&gt;
&lt;p&gt;Thanks to the &lt;a href=&#34;https://towardsdatascience.com/59f801eb3299&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Frisch-Waugh-Lowell theorem&lt;/strong&gt;&lt;/a&gt;, we can simply &lt;strong&gt;partial-out&lt;/strong&gt; $X$ and express the omitted variable bias in terms of $D$ and $Z$.&lt;/p&gt;
&lt;p&gt;$$
\text{OVB} = \gamma \times \frac{Cov(D^{\perp X}, Z^{\perp X})}{Var(D^{\perp X})}
$$&lt;/p&gt;
&lt;p&gt;where $D^{\perp X}$ are the residuals from regressing $D$ on $X$ and $Z^{\perp X}$ are the residuals from regressing $Z$ on $X$. If you are not familiar with Frisch-Waugh-Lowell theorem, I have written a short &lt;a href=&#34;https://towardsdatascience.com/59f801eb3299&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;note here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2112.13398&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Chernozhukov, Cinelli, Newey, Sharma, Syrgkanis (2022)&lt;/a&gt; further generalize to analysis the the setting in which the control variables $X$ and the unobserved variables $Z$ enter the long model with a general functional form $f$&lt;/p&gt;
&lt;p&gt;$$
y = \alpha D + f(Z, X) + \varepsilon
$$&lt;/p&gt;
&lt;p&gt;You can find more details in their paper, but the underlying idea remains the same.&lt;/p&gt;
&lt;h2 id=&#34;example&#34;&gt;Example&lt;/h2&gt;
&lt;p&gt;Suppose we were a researcher interested in the relationship between &lt;strong&gt;education&lt;/strong&gt; and &lt;strong&gt;wages&lt;/strong&gt;. Does investing in education pay off in terms of future wages? Suppose we had data on wages for people with different years of education. Why not looking at the correlation between years of education and wages?&lt;/p&gt;
&lt;p&gt;The problem is that there might be many &lt;strong&gt;unobserved variables&lt;/strong&gt; that are correlated with both education and wages. For simplicity, let&amp;rsquo;s concentrate on &lt;strong&gt;ability&lt;/strong&gt;. People of higher ability might decide to invest more in education just because they are better in school and they get more opportunities. On the other hand, they might also get higher wages afterwards, purely because of their innate ability.&lt;/p&gt;
&lt;p&gt;We can represent the data generating process with the following &lt;strong&gt;Directed Acyclic Graph&lt;/strong&gt; (DAG).&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-mermaid&#34;&gt;flowchart TD
classDef included fill:#DCDCDC,stroke:#000000,stroke-width:2px;
classDef excluded fill:#ffffff,stroke:#000000,stroke-width:2px;
classDef unobserved fill:#ffffff,stroke:#000000,stroke-width:2px,stroke-dasharray: 5 5;

D((education))
Z((ability))
Y((wage))
X1((age))
X2((gender))

D --&amp;gt; Y
Z --&amp;gt; D
Z --&amp;gt; Y
X1 --&amp;gt; Y
X2 --&amp;gt; Y

class D,Y included;
class X1,X2 excluded;
class Z unobserved;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s load and inspect the &lt;strong&gt;data&lt;/strong&gt;. I import the data generating process from &lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/src/dgp.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;src.dgp&lt;/code&gt;&lt;/a&gt; and some plotting functions and libraries from &lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/src/utils.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;src.utils&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;%matplotlib inline
%config InlineBackend.figure_format = &#39;retina&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from src.utils import *
from src.dgp import dgp_educ_wages

df = dgp_educ_wages().generate_data(N=50)
df.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;age&lt;/th&gt;
      &lt;th&gt;gender&lt;/th&gt;
      &lt;th&gt;education&lt;/th&gt;
      &lt;th&gt;wage&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;62&lt;/td&gt;
      &lt;td&gt;male&lt;/td&gt;
      &lt;td&gt;6.0&lt;/td&gt;
      &lt;td&gt;3800.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;44&lt;/td&gt;
      &lt;td&gt;male&lt;/td&gt;
      &lt;td&gt;8.0&lt;/td&gt;
      &lt;td&gt;4500.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;63&lt;/td&gt;
      &lt;td&gt;male&lt;/td&gt;
      &lt;td&gt;8.0&lt;/td&gt;
      &lt;td&gt;4700.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;33&lt;/td&gt;
      &lt;td&gt;male&lt;/td&gt;
      &lt;td&gt;7.0&lt;/td&gt;
      &lt;td&gt;3500.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;57&lt;/td&gt;
      &lt;td&gt;female&lt;/td&gt;
      &lt;td&gt;6.0&lt;/td&gt;
      &lt;td&gt;4000.0&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;We have information on &lt;strong&gt;300 individuals&lt;/strong&gt;, for which we observe their &lt;code&gt;age&lt;/code&gt;, their &lt;code&gt;gender&lt;/code&gt;, the years of &lt;code&gt;education&lt;/code&gt;, and the current monthly &lt;code&gt;wage&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Suppose we were directly regressing &lt;code&gt;wage&lt;/code&gt; on &lt;code&gt;education&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;short_model = smf.ols(&#39;wage ~ education + gender + age&#39;, df).fit()
short_model.summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
         &lt;td&gt;&lt;/td&gt;           &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt;      &lt;td&gt; 2657.8864&lt;/td&gt; &lt;td&gt;  444.996&lt;/td&gt; &lt;td&gt;    5.973&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt; 1762.155&lt;/td&gt; &lt;td&gt; 3553.618&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;gender[T.male]&lt;/th&gt; &lt;td&gt;  335.1075&lt;/td&gt; &lt;td&gt;  132.685&lt;/td&gt; &lt;td&gt;    2.526&lt;/td&gt; &lt;td&gt; 0.015&lt;/td&gt; &lt;td&gt;   68.027&lt;/td&gt; &lt;td&gt;  602.188&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;education&lt;/th&gt;      &lt;td&gt;   95.9437&lt;/td&gt; &lt;td&gt;   38.752&lt;/td&gt; &lt;td&gt;    2.476&lt;/td&gt; &lt;td&gt; 0.017&lt;/td&gt; &lt;td&gt;   17.940&lt;/td&gt; &lt;td&gt;  173.948&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;age&lt;/th&gt;            &lt;td&gt;   12.3120&lt;/td&gt; &lt;td&gt;    6.110&lt;/td&gt; &lt;td&gt;    2.015&lt;/td&gt; &lt;td&gt; 0.050&lt;/td&gt; &lt;td&gt;    0.013&lt;/td&gt; &lt;td&gt;   24.611&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;The coefficient of &lt;code&gt;education&lt;/code&gt; is positive and significant. However, we know there might be an &lt;strong&gt;omitted variable bias&lt;/strong&gt;, because we do not observe &lt;code&gt;ability&lt;/code&gt;. In terms of DAGs, there is a &lt;strong&gt;backdoor path&lt;/strong&gt; from &lt;code&gt;education&lt;/code&gt; to &lt;code&gt;wage&lt;/code&gt; passing through &lt;code&gt;ability&lt;/code&gt; that is not blocked and therefore biases our estimate.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-mermaid&#34;&gt;flowchart TD
classDef included fill:#DCDCDC,stroke:#000000,stroke-width:2px;
classDef excluded fill:#ffffff,stroke:#000000,stroke-width:2px;
classDef unobserved fill:#ffffff,stroke:#000000,stroke-width:2px,stroke-dasharray: 5 5;

D((education))
Z((ability))
Y((wage))
X1((age))
X2((gender))

D --&amp;gt; Y
Z --&amp;gt; D
Z --&amp;gt; Y
X1 --&amp;gt; Y
X2 --&amp;gt; Y

class D,Y included;
class X1,X2 excluded;
class Z unobserved;

linkStyle 0 stroke:#00ff00,stroke-width:4px;
linkStyle 1,2 stroke:#ff0000,stroke-width:4px;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Does it mean that all our analysis is &lt;strong&gt;garbage&lt;/strong&gt;? Can we still draw some causal conclusion from the regression results?&lt;/p&gt;
&lt;h2 id=&#34;direction-of-the-bias&#34;&gt;Direction of the Bias&lt;/h2&gt;
&lt;p&gt;If we knew the signs of $\gamma$ and $\delta$, we could infer the sign of the bias, since it&amp;rsquo;s the product of the two signs.&lt;/p&gt;
&lt;p&gt;$$
\text{OVB} = \gamma \delta \qquad \text{ where } \qquad \gamma := \frac{Cov(Z, y)}{Var(Z)}, \quad \delta := \frac{Cov(D, Z)}{Var(D)}
$$&lt;/p&gt;
&lt;p&gt;which in our example is&lt;/p&gt;
&lt;p&gt;$$
\text{OVB} = \gamma \delta \qquad \text{ where } \qquad \gamma := \frac{Cov(\text{ability}, \text{wage})}{Var(\text{ability})}, \quad \delta := \frac{Cov(\text{education}, \text{ability})}{Var(\text{education})}
$$&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s analyze the two correlations separately:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The correlation between &lt;code&gt;ability&lt;/code&gt; and &lt;code&gt;wage&lt;/code&gt; is most likely positive&lt;/li&gt;
&lt;li&gt;The correlation between &lt;code&gt;ability&lt;/code&gt; and &lt;code&gt;education&lt;/code&gt; is most likely positive&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Therefore, the bias is most likely &lt;strong&gt;positive&lt;/strong&gt;. From this, we can conclude that our estimate from the regression on &lt;code&gt;wage&lt;/code&gt; on &lt;code&gt;education&lt;/code&gt; is most likely an &lt;strong&gt;overestimate&lt;/strong&gt; of the true effect, which is most likely smaller.&lt;/p&gt;
&lt;p&gt;This might seem like a small insight, but it&amp;rsquo;s actually huge. Now we can say with confidence that one year of &lt;code&gt;education&lt;/code&gt; increases &lt;code&gt;wages&lt;/code&gt; by &lt;strong&gt;at most&lt;/strong&gt; 95 dollars per month, which is a much more informative statement than just saying that the estimate is biased.&lt;/p&gt;
&lt;p&gt;In general, we can summarize the different possible effects of the bias in a 2-by-2 &lt;strong&gt;table&lt;/strong&gt;.&lt;/p&gt;
&lt;img src=&#34;other/ovb.png&#34; width=80% /&gt;
&lt;h2 id=&#34;further-sensitivity-analysis&#34;&gt;Further Sensitivity Analysis&lt;/h2&gt;
&lt;p&gt;Can we say &lt;strong&gt;more&lt;/strong&gt; about the omitted variable bias without making strong assumptions?&lt;/p&gt;
&lt;p&gt;The answer is yes! In particular, we can ask ourselves: how strong should the partial correlations $\gamma$ and $\delta$ be in order to &lt;strong&gt;overturn&lt;/strong&gt; our conclusion?&lt;/p&gt;
&lt;p&gt;In our example, we found a positive correlation between &lt;code&gt;education&lt;/code&gt; and &lt;code&gt;wages&lt;/code&gt; in the data. However, we know that we are omitting &lt;code&gt;ability&lt;/code&gt; in the regression. The question is: how strong should the correlation between &lt;code&gt;ability&lt;/code&gt; and &lt;code&gt;wage&lt;/code&gt;, $\gamma$, and between &lt;code&gt;ability&lt;/code&gt; and &lt;code&gt;education&lt;/code&gt;, $\delta$, be in order to make the effect not significant or even negative?&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://rss.onlinelibrary.wiley.com/doi/full/10.1111/rssb.12348&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Cinelli and Hazlett (2020)&lt;/a&gt; show that we can transform this question in terms of residual variation explained, i.e. the &lt;a href=&#34;https://en.wikipedia.org/wiki/Coefficient_of_determination&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;coefficient of determination, $R^2$&lt;/a&gt;. The advantage of this approach is &lt;strong&gt;interpretability&lt;/strong&gt;. It is much easier to make a guess about the percentage of variance explained than to make a guess about the magnitude of a conditional correlation.&lt;/p&gt;
&lt;p&gt;The authors wrote a companion package &lt;a href=&#34;https://github.com/carloscinelli/sensemakr&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;sensemakr&lt;/code&gt;&lt;/a&gt; to conduct the sensitivity analysis. You can find a detailed description of the package &lt;a href=&#34;https://cran.r-project.org/web/packages/sensemakr/vignettes/sensemakr.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;We will now use the &lt;code&gt;Sensemakr&lt;/code&gt; function. The main &lt;strong&gt;arguments&lt;/strong&gt; of the &lt;code&gt;Sensemakr&lt;/code&gt; function are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;model&lt;/code&gt;: the regression model we want to analyze&lt;/li&gt;
&lt;li&gt;&lt;code&gt;treatment&lt;/code&gt;: the feature/covariate of interest, in our case &lt;code&gt;education&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The question we will try to answer is the following:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;How much of the residual variation in &lt;code&gt;education&lt;/code&gt; (x axis) and &lt;code&gt;wage&lt;/code&gt; (y axis) does &lt;code&gt;ability&lt;/code&gt; need to explain in order for the effect of &lt;code&gt;education&lt;/code&gt; on &lt;code&gt;wages&lt;/code&gt; to &lt;strong&gt;change sign&lt;/strong&gt;?&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import sensemakr

sensitivity = sensemakr.Sensemakr(model = short_model, treatment = &amp;quot;education&amp;quot;)
sensitivity.plot()
plt.xlabel(&amp;quot;Partial $R^2$ of ability with education&amp;quot;);
plt.ylabel(&amp;quot;Partial $R^2$ of ability with wage&amp;quot;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/ovb_17_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;In the &lt;strong&gt;plot&lt;/strong&gt;, we see how the partial (because conditional on &lt;code&gt;age&lt;/code&gt; and &lt;code&gt;gender&lt;/code&gt;) $R^2$ of &lt;code&gt;ability&lt;/code&gt; with &lt;code&gt;education&lt;/code&gt; and &lt;code&gt;wage&lt;/code&gt; affects the estimated coefficient of &lt;code&gt;education&lt;/code&gt; on &lt;code&gt;wage&lt;/code&gt;. The $(0,0)$ coordinate, marked with a &lt;strong&gt;triangle&lt;/strong&gt;, corresponds to the current estimate and reflects what would happen if &lt;code&gt;ability&lt;/code&gt; had no explanatory power for both &lt;code&gt;wage&lt;/code&gt; with &lt;code&gt;education&lt;/code&gt;: nothing. As the explanatory power of &lt;code&gt;ability&lt;/code&gt; grows (moving upwards and rightwards from the triangle), the estimated coefficient decreases, as marked by the &lt;strong&gt;level curves&lt;/strong&gt;, until it becomes zero at the &lt;strong&gt;dotted red line&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;How should we &lt;strong&gt;interpret&lt;/strong&gt; the plot? We can see that we need &lt;code&gt;ability&lt;/code&gt; to explain around 30% of the residual variation in both &lt;code&gt;education&lt;/code&gt; and &lt;code&gt;wage&lt;/code&gt; in order for the effect of &lt;code&gt;education&lt;/code&gt; on &lt;code&gt;wages&lt;/code&gt; to disappear, corresponding to the red line.&lt;/p&gt;
&lt;p&gt;One question that you might (legitimately) have now is: what is 30%? Is it big or is it small? We can get a sense of the &lt;strong&gt;magnitude&lt;/strong&gt; of the partial $R^2$ by &lt;strong&gt;benchmarking&lt;/strong&gt; the results with the residual variance explained by another &lt;em&gt;observed&lt;/em&gt; variable. Let&amp;rsquo;s use &lt;code&gt;age&lt;/code&gt; for example.&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;Sensemakr&lt;/code&gt; function accepts the following optional arguments:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;benchmark_covariates&lt;/code&gt;: the covariate to use as a benchmark&lt;/li&gt;
&lt;li&gt;&lt;code&gt;kd&lt;/code&gt; and &lt;code&gt;ky&lt;/code&gt;: these arguments parameterize how many times stronger the unobserved variable (&lt;code&gt;ability&lt;/code&gt;) is related to the treatment (&lt;code&gt;kd&lt;/code&gt;) and to the outcome (&lt;code&gt;ky&lt;/code&gt;) in comparison to the observed benchmark covariate (&lt;code&gt;age&lt;/code&gt;). In our example, setting &lt;code&gt;kd&lt;/code&gt; and &lt;code&gt;ky&lt;/code&gt; equal to $[0.5, 1, 2]$ means we want to investigate the maximum strength of a variable half, same, or twice as strong as &lt;code&gt;age&lt;/code&gt; (in explaining &lt;code&gt;education&lt;/code&gt; and &lt;code&gt;wage&lt;/code&gt; variation).&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sensitivity = sensemakr.Sensemakr(model = short_model, 
                                  treatment = &amp;quot;education&amp;quot;,
                                  benchmark_covariates = &amp;quot;age&amp;quot;,
                                  kd = [0.5, 1, 2],
                                  ky = [0.5, 1, 2])
sensitivity.plot()
plt.xlabel(&amp;quot;Partial $R^2$ of ability with education&amp;quot;);
plt.ylabel(&amp;quot;Partial $R^2$ of ability with wage&amp;quot;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/ovb_19_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;It looks like even if &lt;code&gt;ability&lt;/code&gt; had twice as much explanatory power as &lt;code&gt;age&lt;/code&gt;, the effect of &lt;code&gt;education&lt;/code&gt; on &lt;code&gt;wage&lt;/code&gt; would still be positive. But would it be &lt;strong&gt;statistically significant&lt;/strong&gt;?&lt;/p&gt;
&lt;p&gt;We can repeat the same exercise, looking at the t-statistic instead of the magnitude of the coefficient. We just need to set the &lt;code&gt;sensitivity_of&lt;/code&gt; option in the plotting function equal to &lt;code&gt;t-value&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;The question that we are trying to answer in this case is:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;How much of the residual variation in &lt;code&gt;education&lt;/code&gt; (x axis) and &lt;code&gt;wage&lt;/code&gt; (y axis) does &lt;code&gt;ability&lt;/code&gt; need to explain in order for the effect of &lt;code&gt;education&lt;/code&gt; on &lt;code&gt;wages&lt;/code&gt; to &lt;strong&gt;become not significant&lt;/strong&gt;?&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sensitivity.plot(sensitivity_of = &#39;t-value&#39;)
plt.xlabel(&amp;quot;Partial $R^2$ of ability with education&amp;quot;);
plt.ylabel(&amp;quot;Partial $R^2$ of ability with wage&amp;quot;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/ovb_21_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;From the plot, we can see, we need &lt;code&gt;ability&lt;/code&gt; to explain around 5% to 10% of the residual variation in both &lt;code&gt;education&lt;/code&gt; and &lt;code&gt;wage&lt;/code&gt; in order for the effect of &lt;code&gt;education&lt;/code&gt; on &lt;code&gt;wage&lt;/code&gt; not to be significant. In particular, the red line plots the level curve for the t-statistic equal to 2.01, corresponding to a 5% significance level. From the comparison with &lt;code&gt;age&lt;/code&gt;, we see that a slightly stronger explanatory power (bigger than &lt;code&gt;1.0x age&lt;/code&gt;) would be sufficient to make the coefficient of &lt;code&gt;education&lt;/code&gt; on &lt;code&gt;wage&lt;/code&gt; not statistically significant.&lt;/p&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;In this post, I have introduced the concept of &lt;strong&gt;omitted variable bias&lt;/strong&gt;. We have seen how it&amp;rsquo;s computed in a simple linear model and how we can exploit qualitative information about the variables to make inference in presence of omitted variable bias.&lt;/p&gt;
&lt;p&gt;These tools are extremely useful since omitted variable bias is essentially &lt;strong&gt;everywhere&lt;/strong&gt;. First of all, there are always factors that we do not observe, such as ability in our toy example. However, even if we could observe everything, omitted variable bias can also emerge in the form of &lt;strong&gt;model misspecification&lt;/strong&gt;. Suppose that &lt;code&gt;wages&lt;/code&gt; depended on &lt;code&gt;age&lt;/code&gt; in a quadratic way. Then, omitting the quadratic term from the regression introduces bias, which can be analyzed with the same tools we have used for &lt;code&gt;ability&lt;/code&gt;.&lt;/p&gt;
&lt;h3 id=&#34;references&#34;&gt;References&lt;/h3&gt;
&lt;p&gt;[1] C. Cinelli, C. Hazlett, &lt;a href=&#34;https://rss.onlinelibrary.wiley.com/doi/full/10.1111/rssb.12348&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Making Sense of Sensitivity: Extending Omitted Variable Bias&lt;/a&gt; (2019), &lt;em&gt;Journal of the Royal Statistical Society&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;[2] V. Chernozhukov, C. Cinelli, W. Newey, A. Sharma, V. Syrgkanis, &lt;a href=&#34;https://arxiv.org/abs/2112.13398&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Long Story Short: Omitted Variable Bias in Causal Machine Learning&lt;/a&gt; (2022), working paper.&lt;/p&gt;
&lt;h3 id=&#34;related-articles&#34;&gt;Related Articles&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/59f801eb3299&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The FWL Theorem, Or How To Make Regressions Intuitive&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/b63dc69e3d8c&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;DAGs and Control Variables&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;code&#34;&gt;Code&lt;/h3&gt;
&lt;p&gt;You can find the original Jupyter Notebook here:&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/ovb.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/matteocourthoud/Blog-Posts/blob/main/ovb.ipynb&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Goodbye Scatterplot, Welcome Binned Scatterplot</title>
      <link>https://matteocourthoud.github.io/post/binscatter/</link>
      <pubDate>Sat, 21 May 2022 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/post/binscatter/</guid>
      <description>&lt;p&gt;When we want to visualize the relationship between two continuous variables, the go-to plot is the &lt;strong&gt;scatterplot&lt;/strong&gt;. It&amp;rsquo;s a very intuitive visualization tool that allows us to directly look at the data. However, when we have a lot of data and/or when the data is skewed, scatterplots can be too noisy to be informative.&lt;/p&gt;
&lt;p&gt;In this blog post, I am going to review a very powerful alternative to the scatterplot to visualize correlations between two variables: the &lt;strong&gt;binned scatterplot&lt;/strong&gt;. Binned scatterplots are not only a great visualization tool, but they can also be used to do inference on the conditional distribution of the dependent variable.&lt;/p&gt;
&lt;h2 id=&#34;the-scatterplot&#34;&gt;The Scatterplot&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s start with an example. Suppose we are an &lt;strong&gt;online marketplace&lt;/strong&gt; where multiple firms offer goods that consumer can efficiently browse, compare and buy. Our &lt;strong&gt;dataset&lt;/strong&gt; consists in a snapshot of the firms active on the marketplace.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s load the data and have a look at it. You can find the code for the data generating process &lt;a href=&#34;&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;%matplotlib inline
%config InlineBackend.figure_format = &#39;retina&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from src.utils import *
from src.dgp import dgp_marketplace

df = dgp_marketplace().generate_data(N=10_000)
df.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;age&lt;/th&gt;
      &lt;th&gt;sales&lt;/th&gt;
      &lt;th&gt;online&lt;/th&gt;
      &lt;th&gt;products&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;0.312777&lt;/td&gt;
      &lt;td&gt;450.858091&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;1.176221&lt;/td&gt;
      &lt;td&gt;1121.882449&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;1.764048&lt;/td&gt;
      &lt;td&gt;2698.714549&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;1.082742&lt;/td&gt;
      &lt;td&gt;1627.746386&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;3.156503&lt;/td&gt;
      &lt;td&gt;1464.593939&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;We have information on 10.000 firms. For each firm we know:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;age&lt;/code&gt;: the age of the firm&lt;/li&gt;
&lt;li&gt;&lt;code&gt;sales&lt;/code&gt;: the monthly sales from last month&lt;/li&gt;
&lt;li&gt;&lt;code&gt;online&lt;/code&gt;: whether the firm is only active online&lt;/li&gt;
&lt;li&gt;&lt;code&gt;products&lt;/code&gt;: the number of products that the firm offers&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Suppose we are interested in understanding the relationship between &lt;code&gt;age&lt;/code&gt; and &lt;code&gt;sales&lt;/code&gt;. What is the &lt;strong&gt;life-cycle&lt;/strong&gt; of sales?&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s start with a simple &lt;strong&gt;scatterplot&lt;/strong&gt; of &lt;code&gt;sales&lt;/code&gt; over &lt;code&gt;age&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sns.scatterplot(x=&#39;age&#39;, y=&#39;sales&#39;, data=df);
plt.title(&amp;quot;Sales by firm&#39;s age&amp;quot;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/binscatter_7_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;The plot is extremely &lt;strong&gt;noisy&lt;/strong&gt;. We have a lot of observations, therefore, it is very difficult to visualize them all. If we had to guess, we could say that the relationship looks negative (&lt;code&gt;sales&lt;/code&gt; decrease with &lt;code&gt;age&lt;/code&gt;), but it would be a very uninformed guess.&lt;/p&gt;
&lt;p&gt;We are now going to explore some plausible tweaks and alternatives.&lt;/p&gt;
&lt;h2 id=&#34;scatterplot-alternatives&#34;&gt;Scatterplot Alternatives&lt;/h2&gt;
&lt;p&gt;What can we do when we have an extremely dense scatterplot? One solution could be to plot the &lt;strong&gt;density&lt;/strong&gt; of the observations, instead of the observations themselves.&lt;/p&gt;
&lt;p&gt;There are multiple solutions in Python to visualize the density of a 2-dimensional distribution. A very useful one is &lt;a href=&#34;https://seaborn.pydata.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;seaborn&lt;/a&gt; &lt;a href=&#34;https://seaborn.pydata.org/generated/seaborn.jointplot.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;jointplot&lt;/a&gt;. &lt;code&gt;jointplot&lt;/code&gt; plots the joint distribution of two variables, together with the marginal distributions along the axis. The default option is the scatterplot, but one can also choose to add a regression line (&lt;code&gt;reg&lt;/code&gt;), change the plot to a histogram (&lt;code&gt;hist&lt;/code&gt;), a hexplot (&lt;code&gt;hex&lt;/code&gt;), or a &lt;a href=&#34;https://en.wikipedia.org/wiki/Kernel_density_estimation&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;kernel density estimate&lt;/a&gt; (&lt;code&gt;kde&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s try the &lt;strong&gt;hexplot&lt;/strong&gt;, which is basically a histogram of the data, where the bins are hexagons, in the 2-dimensional space.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;s = sns.jointplot(x=&#39;age&#39;, y=&#39;sales&#39;, data=df, kind=&#39;hex&#39;, );
s.ax_joint.grid(False);
s.ax_marg_y.grid(False);
s.fig.suptitle(&amp;quot;Sales by firm&#39;s age&amp;quot;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/binscatter_11_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Not much has changed. It looks like the distributions of &lt;code&gt;age&lt;/code&gt; and &lt;code&gt;sales&lt;/code&gt; are both very &lt;strong&gt;skewed&lt;/strong&gt; and, therefore, most of the action is concentrated in a very small subspace.&lt;/p&gt;
&lt;p&gt;Maybe we could remove &lt;strong&gt;outliers&lt;/strong&gt; and zoom-in on the area where most of the data is located. Let&amp;rsquo;s zoom-in on the bottom-left corner, on observations what have &lt;code&gt;age &amp;lt; 3&lt;/code&gt; and &lt;code&gt;sales &amp;lt; 3000&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;s = sns.jointplot(x=&#39;age&#39;, y=&#39;sales&#39;, data=df.query(&amp;quot;age &amp;lt; 3 &amp;amp; sales &amp;lt; 3000&amp;quot;), kind=&amp;quot;hex&amp;quot;);
s.ax_joint.grid(False);
s.ax_marg_y.grid(False);
s.fig.suptitle(&amp;quot;Sales by firm&#39;s age&amp;quot;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/binscatter_13_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Now there is much less empty space, but it does not look like we are going far. The joint distribution is &lt;strong&gt;still too skewed&lt;/strong&gt;. This is the case when the data follows some power distribution, as it&amp;rsquo;s often the case with business data.&lt;/p&gt;
&lt;p&gt;One solution is to &lt;strong&gt;transform&lt;/strong&gt; the variable, by taking the &lt;a href=&#34;https://en.wikipedia.org/wiki/Natural_logarithm&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;natural logarithm&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df[&#39;log_age&#39;] = np.log(df[&#39;age&#39;])
df[&#39;log_sales&#39;] = np.log(df[&#39;sales&#39;])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can now plot the relationship between the logarithms of &lt;code&gt;age&lt;/code&gt; and &lt;code&gt;sales&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;s = sns.jointplot(x=&#39;log_age&#39;, y=&#39;log_sales&#39;, data=df, kind=&#39;hex&#39;);
s.ax_joint.grid(False);
s.ax_marg_y.grid(False);
s.fig.suptitle(&amp;quot;Sales by firm&#39;s age&amp;quot;, y=1.02);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/binscatter_17_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;The logarithm definitely helped. Now the data is more spread across space, which means that the visualization is more informative. Moreover, it looks like there is &lt;strong&gt;no relationship&lt;/strong&gt; between the two variables.&lt;/p&gt;
&lt;p&gt;However, there is still &lt;strong&gt;too much noise&lt;/strong&gt;. Maybe data visualization alone is not sufficient do draw a conclusion.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s swap to a more structured approach: &lt;a href=&#34;https://en.wikipedia.org/wiki/Linear_regression&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;linear regression&lt;/strong&gt;&lt;/a&gt;. Let&amp;rsquo;s linearly regress &lt;code&gt;log_sales&lt;/code&gt; on &lt;code&gt;log_age&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;smf.ols(&#39;log_sales ~ log_age&#39;, df).fit().summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
      &lt;td&gt;&lt;/td&gt;         &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt; &lt;td&gt;    7.3971&lt;/td&gt; &lt;td&gt;    0.015&lt;/td&gt; &lt;td&gt;  478.948&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    7.367&lt;/td&gt; &lt;td&gt;    7.427&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;log_age&lt;/th&gt;   &lt;td&gt;    0.1690&lt;/td&gt; &lt;td&gt;    0.010&lt;/td&gt; &lt;td&gt;   16.888&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    0.149&lt;/td&gt; &lt;td&gt;    0.189&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;The regression coefficient for &lt;code&gt;log_age&lt;/code&gt; is &lt;strong&gt;positive&lt;/strong&gt; and statistically significant (i.e. different from zero). It seems that all previous visualizations were very &lt;strong&gt;misleading&lt;/strong&gt;. From none of the graphs above we could have guessed such a strong positive relationship.&lt;/p&gt;
&lt;p&gt;However, maybe this relationship is different for &lt;code&gt;online&lt;/code&gt;-only firms and the rest of the sample. We need to control for this variable in order to avoid &lt;a href=&#34;https://en.wikipedia.org/wiki/Simpson%27s_paradox&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Simpson&amp;rsquo;s Paradox&lt;/a&gt; and, more generally, bias.&lt;/p&gt;
&lt;p&gt;With linear regression, we can &lt;strong&gt;condition the analysis on covariates&lt;/strong&gt;. Let&amp;rsquo;s add the binary indicator for &lt;code&gt;online&lt;/code&gt;-only firms and the variable counting the number of &lt;code&gt;products&lt;/code&gt; to the regression.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;smf.ols(&#39;log_sales ~ log_age + online + products&#39;, df).fit().summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
      &lt;td&gt;&lt;/td&gt;         &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt; &lt;td&gt;    6.5717&lt;/td&gt; &lt;td&gt;    0.037&lt;/td&gt; &lt;td&gt;  176.893&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    6.499&lt;/td&gt; &lt;td&gt;    6.644&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;log_age&lt;/th&gt;   &lt;td&gt;    0.0807&lt;/td&gt; &lt;td&gt;    0.010&lt;/td&gt; &lt;td&gt;    7.782&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    0.060&lt;/td&gt; &lt;td&gt;    0.101&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;online&lt;/th&gt;    &lt;td&gt;    0.1447&lt;/td&gt; &lt;td&gt;    0.027&lt;/td&gt; &lt;td&gt;    5.433&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    0.092&lt;/td&gt; &lt;td&gt;    0.197&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;products&lt;/th&gt;  &lt;td&gt;    0.3456&lt;/td&gt; &lt;td&gt;    0.014&lt;/td&gt; &lt;td&gt;   24.110&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    0.317&lt;/td&gt; &lt;td&gt;    0.374&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;The coefficient for &lt;code&gt;log_age&lt;/code&gt; is still positive and statistically significant, but its &lt;strong&gt;magnitude&lt;/strong&gt; has halved.&lt;/p&gt;
&lt;p&gt;What should we conclude? It seems that &lt;code&gt;sales&lt;/code&gt; increase over age, on average. However, this pattern might be very &lt;strong&gt;non-linear&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Within the linear regression framework, one approach could be to &lt;strong&gt;add extra terms&lt;/strong&gt; such as polynomials (&lt;code&gt;age^2&lt;/code&gt;) or categorical features (e.g. &lt;code&gt;age &amp;lt; 2&lt;/code&gt;). However, it would be really cool if there was a more &lt;strong&gt;flexible&lt;/strong&gt; (i.e. &lt;a href=&#34;https://en.wikipedia.org/wiki/Nonparametric_statistics&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;non-parametric&lt;/a&gt;) approach that could inform us on the relationship between firm &lt;code&gt;age&lt;/code&gt; and &lt;code&gt;sales&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;If only&amp;hellip;&lt;/p&gt;
&lt;h2 id=&#34;the-binned-scatterplot&#34;&gt;The Binned Scatterplot&lt;/h2&gt;
&lt;p&gt;The &lt;strong&gt;binned scatterplot&lt;/strong&gt; is a very powerful tool that provides a &lt;strong&gt;flexible&lt;/strong&gt; and &lt;strong&gt;parsimonious&lt;/strong&gt; way of visualizing and summarizing conditional means (and not only) in large datasets.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;idea&lt;/strong&gt; behind the binned scatterplot is to divide the conditioning variable, &lt;code&gt;age&lt;/code&gt; in our example, into &lt;strong&gt;equally sized bins or quantiles&lt;/strong&gt;, and then plot the conditional mean of the dependent variable, &lt;code&gt;sales&lt;/code&gt; in our example, within each bin.&lt;/p&gt;
&lt;h3 id=&#34;details&#34;&gt;Details&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/1902.09608&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Cattaneo, Crump, Farrell, Feng (2021)&lt;/a&gt; have built an extremely good package for binned scatterplots in R, &lt;a href=&#34;https://cran.r-project.org/web/packages/binsreg/index.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;binsreg&lt;/a&gt;. Moreover, they have ported the package to Python. We can install &lt;code&gt;binsreg&lt;/code&gt; directly from pip using &lt;code&gt;pip install binsreg&lt;/code&gt;. You can find more information on the Python package &lt;a href=&#34;https://pypi.org/project/binsreg/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;, while the original and detailed R package documentation can be found &lt;a href=&#34;https://www.rdocumentation.org/packages/binsreg/versions/0.7/topics/binsreg&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The most important choice when building a binned scatterplot is the &lt;strong&gt;number of bins&lt;/strong&gt;. The trade-off is the usual &lt;a href=&#34;https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;bias-variance trade-off&lt;/strong&gt;&lt;/a&gt;. By picking a higher number of bins, we have more points in the graph. In the extreme, we end up having a standard &lt;strong&gt;scatterplot&lt;/strong&gt; (assuming the conditioning variable is continuous). On the other hand, by decreasing the number bins, the plot will be more stable. However, in the extreme, we will have a &lt;strong&gt;single point&lt;/strong&gt; representing the sample mean.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/1902.09608&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Cattaneo, Crump, Farrell, Feng (2021)&lt;/a&gt; prove that, in the basic binned scatterplot, the number of bins that minimizes the mean squared error is proportional to $n^{1/3}$, where $n$ is the number of observations. Therefore, in general, more observations lead to more bins.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://onlinelibrary.wiley.com/doi/abs/10.1002/smj.3199&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Starr and Goldfarb (2020)&lt;/a&gt; add the following consideration:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;&lt;em&gt;However other elements are also important. For example, holding the distribution of x constant, the more curvilinear the true relationship between x and y is, the more bins the algorithm will select (otherwise mean squared error will increase). This implies that even with large n, few bins will be chosen for relatively flat relationships. The calculation of the optimal number of bins in a basic binned scatterplot thus takes into account the amount and location of variation in the data available to identify the relationship between x and y.&lt;/em&gt;&amp;rdquo;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;It is strongly recommended to use the default optimal number of bins. However, one can also set a customized number of bins in &lt;code&gt;binsreg&lt;/code&gt; with the &lt;code&gt;nbins&lt;/code&gt; option.&lt;/p&gt;
&lt;p&gt;Binned scatterplots however, do not just compute conditional means, for optimally chosen intervals, but they can also provide &lt;strong&gt;inference&lt;/strong&gt; for these means. In particular, we can build &lt;strong&gt;confidence intervals&lt;/strong&gt; around each data point. In the &lt;code&gt;binsreg&lt;/code&gt; package, the option &lt;code&gt;ci&lt;/code&gt; adds confidence intervals to the estimation results. The option takes as input a tuple of parameters &lt;code&gt;(p, s)&lt;/code&gt; and uses a piecewise polynomial of degree &lt;code&gt;p&lt;/code&gt; with &lt;code&gt;s&lt;/code&gt; smoothness constraints to construct the confidence intervals. By default, the confidence intervals are not included in the plot. For what concerns the choice of &lt;code&gt;p&lt;/code&gt; and &lt;code&gt;s&lt;/code&gt;, the &lt;a href=&#34;https://www.rdocumentation.org/packages/binsreg/versions/0.7/topics/binsreg&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;package documentation&lt;/a&gt; reports:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;&lt;em&gt;Recommended specification is ci=c(3,3), which adds confidence intervals based on cubic B-spline estimate of the regression function of interest to the binned scatter plot.&lt;/em&gt;&amp;rdquo;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;binsreg&#34;&gt;Binsreg&lt;/h3&gt;
&lt;p&gt;One problem with the Python version of the package, is that is not very Python-ish. Therefore, I have wrapped the &lt;code&gt;binsreg&lt;/code&gt; package into a function &lt;code&gt;binscatter&lt;/code&gt; that takes care of cleaning and formatting the output in a nicely readable &lt;a href=&#34;https://pandas.pydata.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Pandas&lt;/a&gt; DataFrame.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import binsreg

def binscatter(**kwargs):
    # Estimate binsreg
    est = binsreg.binsreg(**kwargs)
    
    # Retrieve estimates
    df_est = pd.concat([d.dots for d in est.data_plot])
    df_est = df_est.rename(columns={&#39;x&#39;: kwargs.get(&amp;quot;x&amp;quot;), &#39;fit&#39;: kwargs.get(&amp;quot;y&amp;quot;)})
    
    # Add confidence intervals
    if &amp;quot;ci&amp;quot; in kwargs:
        df_est = pd.merge(df_est, pd.concat([d.ci for d in est.data_plot]))
        df_est = df_est.drop(columns=[&#39;x&#39;])
        df_est[&#39;ci&#39;] = df_est[&#39;ci_r&#39;] - df_est[&#39;ci_l&#39;]
    
    # Rename groups
    if &amp;quot;by&amp;quot; in kwargs:
        df_est[&#39;group&#39;] = df_est[&#39;group&#39;].astype(df[kwargs.get(&amp;quot;by&amp;quot;)].dtype)
        df_est = df_est.rename(columns={&#39;group&#39;: kwargs.get(&amp;quot;by&amp;quot;)})

    return df_est
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can now proceed to &lt;strong&gt;estimate&lt;/strong&gt; and &lt;strong&gt;visualize&lt;/strong&gt; the binned scatterplot for &lt;code&gt;age&lt;/code&gt; based on &lt;code&gt;sales&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Estimate binsreg
df_est = binscatter(x=&#39;age&#39;, y=&#39;sales&#39;, data=df, ci=(3,3))
df_est.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;group&lt;/th&gt;
      &lt;th&gt;age&lt;/th&gt;
      &lt;th&gt;bin&lt;/th&gt;
      &lt;th&gt;isknot&lt;/th&gt;
      &lt;th&gt;mid&lt;/th&gt;
      &lt;th&gt;sales&lt;/th&gt;
      &lt;th&gt;ci_l&lt;/th&gt;
      &lt;th&gt;ci_r&lt;/th&gt;
      &lt;th&gt;ci&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;Full Sample&lt;/td&gt;
      &lt;td&gt;0.012556&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1624.779616&lt;/td&gt;
      &lt;td&gt;1312.439124&lt;/td&gt;
      &lt;td&gt;1905.535412&lt;/td&gt;
      &lt;td&gt;593.096288&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;Full Sample&lt;/td&gt;
      &lt;td&gt;0.037015&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1664.078013&lt;/td&gt;
      &lt;td&gt;1435.438411&lt;/td&gt;
      &lt;td&gt;1893.888819&lt;/td&gt;
      &lt;td&gt;458.450408&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;Full Sample&lt;/td&gt;
      &lt;td&gt;0.065813&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1779.657894&lt;/td&gt;
      &lt;td&gt;1555.909281&lt;/td&gt;
      &lt;td&gt;1968.681960&lt;/td&gt;
      &lt;td&gt;412.772679&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;Full Sample&lt;/td&gt;
      &lt;td&gt;0.094486&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1976.464837&lt;/td&gt;
      &lt;td&gt;1740.530049&lt;/td&gt;
      &lt;td&gt;2216.800005&lt;/td&gt;
      &lt;td&gt;476.269956&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;Full Sample&lt;/td&gt;
      &lt;td&gt;0.125363&lt;/td&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;2015.833752&lt;/td&gt;
      &lt;td&gt;1796.489393&lt;/td&gt;
      &lt;td&gt;2280.237320&lt;/td&gt;
      &lt;td&gt;483.747927&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;The &lt;code&gt;binscatter&lt;/code&gt; function outputs a dataset in which, for each bin of the conditioning variable, &lt;code&gt;age&lt;/code&gt;, we have values and confidence intervals for the outcome variable, &lt;code&gt;sales&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;We can now plot the estimates.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Plot binned scatterplot
sns.scatterplot(x=&#39;age&#39;, y=&#39;sales&#39;, data=df_est);
plt.errorbar(&#39;age&#39;, &#39;sales&#39;, yerr=&#39;ci&#39;, data=df_est, ls=&#39;&#39;, lw=3, alpha=0.2);
plt.title(&amp;quot;Sales by firm&#39;s age&amp;quot;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/binscatter_33_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;The plot is quite revealing. Now the relationship looks extremely &lt;strong&gt;non-linear&lt;/strong&gt; with a sharp increase in &lt;code&gt;sales&lt;/code&gt; at the beginning of the lifetime of a firm, followed by a plateau.&lt;/p&gt;
&lt;p&gt;Moreover, the plot is also telling us information regarding the distributions of &lt;code&gt;age&lt;/code&gt; and &lt;code&gt;sales&lt;/code&gt;. In fact, the plot is more dense on the left, where the distribution of &lt;code&gt;age&lt;/code&gt; is concentrated. Also, confidence intervals are tighter on the left, where most of the conditional distribution of &lt;code&gt;sales&lt;/code&gt; lies.&lt;/p&gt;
&lt;p&gt;As we already discussed in the previous section, it might be important to control for other variables. For example, the number of &lt;code&gt;products&lt;/code&gt;, since firms that sell more products probably survive longer in the markets and also make more sales.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;binsreg&lt;/code&gt; allows to &lt;strong&gt;condition&lt;/strong&gt; the analysis on any number of variables, with the &lt;code&gt;w&lt;/code&gt; option.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Estimate binsreg
df_est = binscatter(x=&#39;age&#39;, y=&#39;sales&#39;, w=[&#39;products&#39;], data=df, ci=(3,3))

# Plot binned scatterplot
sns.scatterplot(x=&#39;age&#39;, y=&#39;sales&#39;, data=df_est);
plt.errorbar(&#39;age&#39;, &#39;sales&#39;, yerr=&#39;ci&#39;, data=df_est, ls=&#39;&#39;, lw=3, alpha=0.2);
plt.title(&amp;quot;Sales by firm&#39;s age&amp;quot;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/binscatter_35_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Conditional on number of &lt;code&gt;products&lt;/code&gt;, the shape of the &lt;code&gt;sales&lt;/code&gt; life-cycle changes further. Now, after an initial increase in sales, we observe a gradual decrease over time.&lt;/p&gt;
&lt;p&gt;Do &lt;code&gt;online&lt;/code&gt;-only firms have different &lt;code&gt;sales&lt;/code&gt; life-cycles with respect to mixed online-offline firms? We can produce different binned scatterplots &lt;strong&gt;by group&lt;/strong&gt; using the option &lt;code&gt;by&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Estimate binsreg
df_est = binscatter(x=&#39;age&#39;, y=&#39;sales&#39;, by=&#39;online&#39;, w=[&#39;products&#39;], data=df, ci=(3,3))

# Plot binned scatterplot
sns.scatterplot(x=&#39;age&#39;, y=&#39;sales&#39;, data=df_est, hue=&#39;online&#39;);
plt.errorbar(&#39;age&#39;, &#39;sales&#39;, yerr=&#39;ci&#39;, data=df_est.query(&amp;quot;online==0&amp;quot;), ls=&#39;&#39;, lw=3, alpha=0.2);
plt.errorbar(&#39;age&#39;, &#39;sales&#39;, yerr=&#39;ci&#39;, data=df_est.query(&amp;quot;online==1&amp;quot;), ls=&#39;&#39;, lw=3, alpha=0.2);
plt.title(&amp;quot;Sales by firm&#39;s age&amp;quot;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/binscatter_37_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;From the binned scatterplot, we can see that &lt;code&gt;online&lt;/code&gt; products have on average shorter lifetimes, with a higher initial peak in &lt;code&gt;sales&lt;/code&gt;, followed by a sharper decline.&lt;/p&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;In this blog post, we have analyzed a very powerful data visualization tool: the &lt;strong&gt;binned scatterplot&lt;/strong&gt;. In particular, we have seen how to use the &lt;code&gt;binsreg&lt;/code&gt; package to automatically pick the optimal number of bins and perform non-parametric inference on conditional means. However, the &lt;code&gt;binsreg&lt;/code&gt; package offers much more than that and I strongly recommend checking &lt;a href=&#34;https://cran.r-project.org/web/packages/binsreg/binsreg.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;its manual&lt;/a&gt; more in depth.&lt;/p&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;p&gt;[1] E Starr, B Goldfarb, &lt;a href=&#34;https://onlinelibrary.wiley.com/doi/abs/10.1002/smj.3199&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Binned Scatterplots: A Simple Tool to Make Research Easier and Better&lt;/a&gt; (2020), Strategic Management Journal.&lt;/p&gt;
&lt;p&gt;[2] M. D. Cattaneo, R. K. Crump, M. H. Farrell, Y. Feng, &lt;a href=&#34;https://arxiv.org/abs/1902.09608&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;On Binscatter&lt;/a&gt; (2021), working paper.&lt;/p&gt;
&lt;p&gt;[3] P. Goldsmith-Pinkham, &lt;a href=&#34;https://www.youtube.com/watch?v=fg9T2gPZCIs&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Lecture 6. Linear Regression II: Semiparametrics and Visualization&lt;/a&gt;, Applied Metrics PhD Course.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>The FWL Theorem, Or How To Make All Regressions Intuitive</title>
      <link>https://matteocourthoud.github.io/post/fwl/</link>
      <pubDate>Mon, 16 May 2022 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/post/fwl/</guid>
      <description>&lt;p&gt;&lt;em&gt;An introduction to the Frisch-Waugh-Lowell theorem and how to use it to gain intuition in linear regressions&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;The Frisch-Waugh-Lowell theorem is a &lt;strong&gt;simple&lt;/strong&gt; but yet &lt;strong&gt;powerful&lt;/strong&gt; theorem that allows us to reduce multivariate regressions to &lt;strong&gt;univariate&lt;/strong&gt; ones. This is extremely useful when we are interested in the relationship between two variables, but we still need to control for other factors, as it is often the case in &lt;strong&gt;causal inference&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;In this blog post, I am going to introduce the Frisch-Waugh-Lowell theorem and illustrate some interesting applications.&lt;/p&gt;
&lt;h2 id=&#34;the-theorem&#34;&gt;The Theorem&lt;/h2&gt;
&lt;p&gt;The theorem was first published by &lt;a href=&#34;https://www.jstor.org/stable/1907330&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Ragnar Frisch and Frederick Waugh in 1933&lt;/a&gt;. However, since its proof was lengthy and cumbersome, &lt;a href=&#34;https://www.tandfonline.com/doi/abs/10.1080/01621459.1963.10480682&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Michael Lovell in 1963&lt;/a&gt; provided a very simple and intuitive proof and his name was added to the theorem name.&lt;/p&gt;
&lt;p&gt;The theorem states that, when estimating a model of the form&lt;/p&gt;
&lt;p&gt;$$
y_i = \beta_1 x_{i,1} + \beta_2 x_{i,2} + \varepsilon_i
$$&lt;/p&gt;
&lt;p&gt;then, the following estimators of $\beta_1$ are equivalent:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the OLS estimator obtained by regressing $y$ on $x_1$ and $x_2$&lt;/li&gt;
&lt;li&gt;the OLS estimator obtained by regressing $y$ on $\tilde x_1$
&lt;ul&gt;
&lt;li&gt;where $\tilde x_1$ is the residual from the regression of $x_1$ on $x_2$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;the OLS estimator obtained by regressing $\tilde y$ on $\tilde x_1$
&lt;ul&gt;
&lt;li&gt;where $\tilde y$ is the residual from the regression of $y$ on $x_2$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;interpretation&#34;&gt;Interpretation&lt;/h3&gt;
&lt;p&gt;What did we actually &lt;strong&gt;learn&lt;/strong&gt;?&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;Frisch-Waugh-Lowell theorem&lt;/strong&gt; is telling us that there are multiple ways to estimate a single regression coefficient. One possibility is to run the full regression of $y$ on $x$, as usual.&lt;/p&gt;
&lt;p&gt;However, we can also regress $x_1$ on $x_2$, take the residuals, and regress $y$ only those residuals. The first part of this process is sometimes referred to as &lt;strong&gt;partialling-out&lt;/strong&gt; (or &lt;em&gt;orthogonalization&lt;/em&gt;, or &lt;em&gt;residualization&lt;/em&gt;) of $x_1$ with respect to $x_2$. The idea is that we are isolating the variation in $x_1$ that is &lt;em&gt;orthogonal&lt;/em&gt; to $x_2$. Note that $x_2$ can be also be multi-dimensional (i.e. include multiple variables and not just one).&lt;/p&gt;
&lt;p&gt;Why would one ever do that?&lt;/p&gt;
&lt;p&gt;This seems like a way more &lt;strong&gt;complicated&lt;/strong&gt; procedure. Instead of simply doing the regression in 1 step, now we need to do 2 or even 3 steps. It&amp;rsquo;s not intuitive at all. The main advantage comes from the fact that we have reduced a multivariate regression to a univariate one, making more tractable and more intuitive.&lt;/p&gt;
&lt;p&gt;We will later explore more in detail three &lt;strong&gt;applications&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;data visualization&lt;/li&gt;
&lt;li&gt;computational speed&lt;/li&gt;
&lt;li&gt;further applications for inference&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;However, let&amp;rsquo;s first explore the theorem more in detail with an example.&lt;/p&gt;
&lt;h2 id=&#34;example&#34;&gt;Example&lt;/h2&gt;
&lt;p&gt;Suppose we were a retail chain, owning many different stores in different locations. We come up with a brilliant &lt;strong&gt;idea to increase sales&lt;/strong&gt;: give away discounts in the form of &lt;strong&gt;coupons&lt;/strong&gt;. We print a lot of coupons and we distribute them around.&lt;/p&gt;
&lt;p&gt;To understand whether our marketing strategy worked, in each store, we check the average daily &lt;code&gt;sales&lt;/code&gt; and which percentage of shoppers used a &lt;code&gt;coupon&lt;/code&gt;. However, there is one &lt;strong&gt;problem&lt;/strong&gt;: we are worried that higher income people are less likely to use the discount, but usually they spend more. To be safe, we also record the average &lt;code&gt;income&lt;/code&gt; in the neighborhood of each store.&lt;/p&gt;
&lt;p&gt;We can represent the data generating process with a &lt;strong&gt;Directed Acyclic Graph&lt;/strong&gt; (DAG). If you are not familiar with DAGs, I have written a short introduction to &lt;a href=&#34;https://medium.com/towards-data-science/controls-b63dc69e3d8c&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Directed Acyclic Graphs here&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-mermaid&#34;&gt;flowchart LR
classDef included fill:#DCDCDC,stroke:#000000,stroke-width:2px;
classDef excluded fill:#ffffff,stroke:#000000,stroke-width:2px;
classDef unobserved fill:#ffffff,stroke:#000000,stroke-width:2px,stroke-dasharray: 5 5;

X1((coupons))
X2((income))
X3((weekday))
Y((sales))


X1 --&amp;gt; Y
X2 --&amp;gt; X1
X2 --&amp;gt; Y
X3 --&amp;gt; Y

class X1,X2,X3,Y excluded;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s load and inspect the &lt;strong&gt;data&lt;/strong&gt;. I import the data generating process from &lt;code&gt;src.dgp&lt;/code&gt; and some plotting functions and libraries from &lt;code&gt;src.utils&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;%matplotlib inline
%config InlineBackend.figure_format = &#39;retina&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from src.utils import *
from src.dgp import dgp_store_coupons

df = dgp_store_coupons().generate_data(N=50)
df.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;sales&lt;/th&gt;
      &lt;th&gt;coupons&lt;/th&gt;
      &lt;th&gt;income&lt;/th&gt;
      &lt;th&gt;dayofweek&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;821.7&lt;/td&gt;
      &lt;td&gt;0.199&lt;/td&gt;
      &lt;td&gt;66.243&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;602.3&lt;/td&gt;
      &lt;td&gt;0.245&lt;/td&gt;
      &lt;td&gt;43.882&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;655.1&lt;/td&gt;
      &lt;td&gt;0.162&lt;/td&gt;
      &lt;td&gt;44.718&lt;/td&gt;
      &lt;td&gt;5&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;625.8&lt;/td&gt;
      &lt;td&gt;0.269&lt;/td&gt;
      &lt;td&gt;39.270&lt;/td&gt;
      &lt;td&gt;4&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;696.6&lt;/td&gt;
      &lt;td&gt;0.186&lt;/td&gt;
      &lt;td&gt;58.654&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;We have information on &lt;strong&gt;50 stores&lt;/strong&gt;, for which we observe the percentage of customers that use &lt;code&gt;coupons&lt;/code&gt;, daily &lt;code&gt;sales&lt;/code&gt; (in thousand $), average &lt;code&gt;income&lt;/code&gt; of the neighborhood (in thousand $), and &lt;code&gt;day of the week&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Suppose we were directly regressing &lt;code&gt;sales&lt;/code&gt; on &lt;code&gt;coupon&lt;/code&gt; usage. What would we get? I represent the &lt;strong&gt;result&lt;/strong&gt; of the regression graphically, using &lt;code&gt;seaborn&lt;/code&gt; &lt;code&gt;regplot&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sns.regplot(x=&amp;quot;coupons&amp;quot;, y=&amp;quot;sales&amp;quot;, data=df, ci=False, line_kws={&#39;color&#39;:&#39;r&#39;, &#39;label&#39;:&#39;linear fit&#39;})
plt.legend()
plt.title(f&amp;quot;Sales and coupon usage&amp;quot;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/fwl_12_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;It looks like coupons were a &lt;strong&gt;bad idea&lt;/strong&gt;: in stores where coupons are used more, we observe lower sales.&lt;/p&gt;
&lt;p&gt;However, it might just be that people with higher income are using less coupons, while also spending more. If this was true, it could &lt;strong&gt;bias&lt;/strong&gt; our results. In terms of the DAG, it means that we have a &lt;strong&gt;backdoor path&lt;/strong&gt; passing through &lt;code&gt;income&lt;/code&gt;, generating a non-causal relationship.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-mermaid&#34;&gt;flowchart LR
classDef included fill:#DCDCDC,stroke:#000000,stroke-width:2px;
classDef excluded fill:#ffffff,stroke:#000000,stroke-width:2px;
classDef unobserved fill:#ffffff,stroke:#000000,stroke-width:2px,stroke-dasharray: 5 5;

X1((coupons))
X2((income))
X3((weekday))
Y((sales))


X1 --&amp;gt; Y
X2 --&amp;gt; X1
X2 --&amp;gt; Y
X3 --&amp;gt; Y

class X1,Y included;
class X2,X3 excluded;

linkStyle 1,2 stroke:#ff0000,stroke-width:4px;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In order to recover the causal effect of &lt;code&gt;coupons&lt;/code&gt; on &lt;code&gt;sales&lt;/code&gt; we need to &lt;strong&gt;condition&lt;/strong&gt; our analysis on &lt;code&gt;income&lt;/code&gt;. This will &lt;strong&gt;block&lt;/strong&gt; the non-causal path passing through &lt;code&gt;income&lt;/code&gt;, leaving only the direct path from &lt;code&gt;coupons&lt;/code&gt; to &lt;code&gt;sales&lt;/code&gt; open, allowing us to estimate the causal effect.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-mermaid&#34;&gt;flowchart LR
classDef included fill:#DCDCDC,stroke:#000000,stroke-width:2px;
classDef excluded fill:#ffffff,stroke:#000000,stroke-width:2px;
classDef unobserved fill:#ffffff,stroke:#000000,stroke-width:2px,stroke-dasharray: 5 5;


X1((coupons))
X2((income))
X3((weekday))
Y((sales))


X1 --&amp;gt; Y
X2 -.-&amp;gt; X1
X2 -.-&amp;gt; Y
X3 --&amp;gt; Y

class X1,X2,Y included;
class X3 excluded;

linkStyle 0 stroke:#00ff00,stroke-width:4px;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s implement this, by including &lt;code&gt;income&lt;/code&gt; in the regression.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;smf.ols(&#39;sales ~ coupons + income&#39;, df).fit().summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
      &lt;td&gt;&lt;/td&gt;         &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt; &lt;td&gt;  161.4982&lt;/td&gt; &lt;td&gt;   33.253&lt;/td&gt; &lt;td&gt;    4.857&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;   94.601&lt;/td&gt; &lt;td&gt;  228.395&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;coupons&lt;/th&gt;   &lt;td&gt;  218.7548&lt;/td&gt; &lt;td&gt;   50.058&lt;/td&gt; &lt;td&gt;    4.370&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;  118.052&lt;/td&gt; &lt;td&gt;  319.458&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;income&lt;/th&gt;    &lt;td&gt;    9.5094&lt;/td&gt; &lt;td&gt;    0.480&lt;/td&gt; &lt;td&gt;   19.818&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    8.544&lt;/td&gt; &lt;td&gt;   10.475&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;Now the estimated effect of &lt;code&gt;coupons&lt;/code&gt; on &lt;code&gt;sales&lt;/code&gt; is positive and significant. Coupons were actually a &lt;strong&gt;good idea&lt;/strong&gt; after all.&lt;/p&gt;
&lt;h3 id=&#34;verifying-the-theorem&#34;&gt;Verifying the Theorem&lt;/h3&gt;
&lt;p&gt;Let&amp;rsquo;s now verify that the Frisch-Waugh-Lowell theorem actually holds. In particular, we want to check whether we get the &lt;strong&gt;same coefficient&lt;/strong&gt; if, instead of regressing &lt;code&gt;sales&lt;/code&gt; on &lt;code&gt;coupons&lt;/code&gt; and &lt;code&gt;income&lt;/code&gt;, we were&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;regressing &lt;code&gt;coupons&lt;/code&gt; on &lt;code&gt;income&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;computing the residuals &lt;code&gt;coupons_tilde&lt;/code&gt;, i.e. the variation in &lt;code&gt;coupons&lt;/code&gt; &lt;strong&gt;not&lt;/strong&gt; explained by &lt;code&gt;income&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;regressing &lt;code&gt;sales&lt;/code&gt; on &lt;code&gt;coupons_tilde&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df[&#39;coupons_tilde&#39;] = smf.ols(&#39;coupons ~ income&#39;, df).fit().resid

smf.ols(&#39;sales ~ coupons_tilde - 1&#39;, df).fit().summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
        &lt;td&gt;&lt;/td&gt;           &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;coupons_tilde&lt;/th&gt; &lt;td&gt;  218.7548&lt;/td&gt; &lt;td&gt; 1275.236&lt;/td&gt; &lt;td&gt;    0.172&lt;/td&gt; &lt;td&gt; 0.865&lt;/td&gt; &lt;td&gt;-2343.929&lt;/td&gt; &lt;td&gt; 2781.438&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;Yes, the coefficient is the same! However, the &lt;strong&gt;standard errors&lt;/strong&gt; now have increased a lot and the estimated coefficient is not significantly different from zero anymore.&lt;/p&gt;
&lt;p&gt;A better approach is to add a further step and repeat the same procedure also for &lt;code&gt;sales&lt;/code&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;regressing &lt;code&gt;sales&lt;/code&gt; on &lt;code&gt;income&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;computing the residuals &lt;code&gt;sales_tilde&lt;/code&gt;, i.e. the variation in &lt;code&gt;sales&lt;/code&gt; &lt;strong&gt;not&lt;/strong&gt; explained by &lt;code&gt;income&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;and finally regress &lt;code&gt;sales_tilde&lt;/code&gt; on &lt;code&gt;coupons_tilde&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df[&#39;sales_tilde&#39;] = smf.ols(&#39;sales ~ income&#39;, df).fit().resid

smf.ols(&#39;sales_tilde ~ coupons_tilde - 1&#39;, df).fit().summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
        &lt;td&gt;&lt;/td&gt;           &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;coupons_tilde&lt;/th&gt; &lt;td&gt;  218.7548&lt;/td&gt; &lt;td&gt;   49.025&lt;/td&gt; &lt;td&gt;    4.462&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;  120.235&lt;/td&gt; &lt;td&gt;  317.275&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;The coefficient is still exactly the same, but now also the standard errors are almost identical.&lt;/p&gt;
&lt;h3 id=&#34;projection&#34;&gt;Projection&lt;/h3&gt;
&lt;p&gt;What is &lt;strong&gt;partialling-out&lt;/strong&gt; (or residualization, or orthogonalization) actually doing? What is happening when we take the residuals of &lt;code&gt;coupons&lt;/code&gt; with respect to &lt;code&gt;income&lt;/code&gt;?&lt;/p&gt;
&lt;p&gt;We can &lt;strong&gt;visualize&lt;/strong&gt; the procedure in a plot. First, let&amp;rsquo;s actually display the &lt;strong&gt;residuals&lt;/strong&gt; of &lt;code&gt;coupons&lt;/code&gt; with respect to income.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df[&amp;quot;coupons_hat&amp;quot;] = smf.ols(&#39;coupons ~ income&#39;, df).fit().predict()
ax = sns.regplot(x=&amp;quot;income&amp;quot;, y=&amp;quot;coupons&amp;quot;, data=df, ci=False, line_kws={&#39;color&#39;:&#39;r&#39;, &#39;label&#39;:&#39;linear fit&#39;})
ax.vlines(df[&amp;quot;income&amp;quot;], np.minimum(df[&amp;quot;coupons&amp;quot;], df[&amp;quot;coupons_hat&amp;quot;]), np.maximum(df[&amp;quot;coupons&amp;quot;], df[&amp;quot;coupons_hat&amp;quot;]), 
           linestyle=&#39;--&#39;, color=&#39;k&#39;, alpha=0.5, linewidth=1, label=&amp;quot;residuals&amp;quot;);
plt.legend()
plt.title(f&amp;quot;Coupons usage, income and residuals&amp;quot;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/fwl_24_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;residuals&lt;/strong&gt; are the vertical dotted lines between the data and the linear fit, i.e. the part of the variation in &lt;code&gt;coupons&lt;/code&gt; unexplained by &lt;code&gt;income&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;By &lt;strong&gt;partialling-out&lt;/strong&gt;, we are removing the linear fit from the data and keeping only the residuals. We can visualize this procedure with a gif. I import the code from the &lt;code&gt;src.figures&lt;/code&gt; file that you can find &lt;a href=&#34;https://github.com/matteocourthoud/Blog-Posts/blob/main/src/figures.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from src.figures import gif_projection

gif_projection(x=&#39;income&#39;, y=&#39;coupons&#39;, df=df, gifname=&amp;quot;gifs/fwl.gif&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;gifs/fwl.gif&#34; alt=&#34;fwl&#34;&gt;&lt;/p&gt;
&lt;p&gt;The original distribution of the data is on the left in &lt;em&gt;blue&lt;/em&gt;, the partialled-out data in on the right in &lt;em&gt;green&lt;/em&gt;. As we can see, partialling-out removes both the level and the trend in &lt;code&gt;coupons&lt;/code&gt; that is explained by &lt;code&gt;income&lt;/code&gt;.&lt;/p&gt;
&lt;h3 id=&#34;multiple-controls&#34;&gt;Multiple Controls&lt;/h3&gt;
&lt;p&gt;We can use the Frisch-Waugh-Theorem also when we have &lt;strong&gt;multiple control variables&lt;/strong&gt;. Suppose that we wanted to also include &lt;code&gt;day of the week&lt;/code&gt; in the regression, to increase precision.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;smf.ols(&#39;sales ~ coupons + income + dayofweek&#39;, df).fit().summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
         &lt;td&gt;&lt;/td&gt;           &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt;      &lt;td&gt;  124.2721&lt;/td&gt; &lt;td&gt;   28.764&lt;/td&gt; &lt;td&gt;    4.320&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;   66.182&lt;/td&gt; &lt;td&gt;  182.362&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;dayofweek[T.2]&lt;/th&gt; &lt;td&gt;    7.7703&lt;/td&gt; &lt;td&gt;   14.607&lt;/td&gt; &lt;td&gt;    0.532&lt;/td&gt; &lt;td&gt; 0.598&lt;/td&gt; &lt;td&gt;  -21.729&lt;/td&gt; &lt;td&gt;   37.270&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;dayofweek[T.3]&lt;/th&gt; &lt;td&gt;   15.0895&lt;/td&gt; &lt;td&gt;   11.678&lt;/td&gt; &lt;td&gt;    1.292&lt;/td&gt; &lt;td&gt; 0.204&lt;/td&gt; &lt;td&gt;   -8.495&lt;/td&gt; &lt;td&gt;   38.674&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;dayofweek[T.4]&lt;/th&gt; &lt;td&gt;   28.2762&lt;/td&gt; &lt;td&gt;    9.868&lt;/td&gt; &lt;td&gt;    2.866&lt;/td&gt; &lt;td&gt; 0.007&lt;/td&gt; &lt;td&gt;    8.348&lt;/td&gt; &lt;td&gt;   48.204&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;dayofweek[T.5]&lt;/th&gt; &lt;td&gt;   44.0937&lt;/td&gt; &lt;td&gt;   10.214&lt;/td&gt; &lt;td&gt;    4.317&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;   23.467&lt;/td&gt; &lt;td&gt;   64.720&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;dayofweek[T.6]&lt;/th&gt; &lt;td&gt;   50.7664&lt;/td&gt; &lt;td&gt;   13.130&lt;/td&gt; &lt;td&gt;    3.866&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;   24.249&lt;/td&gt; &lt;td&gt;   77.283&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;dayofweek[T.7]&lt;/th&gt; &lt;td&gt;   57.3142&lt;/td&gt; &lt;td&gt;   12.413&lt;/td&gt; &lt;td&gt;    4.617&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;   32.245&lt;/td&gt; &lt;td&gt;   82.383&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;coupons&lt;/th&gt;        &lt;td&gt;  192.0262&lt;/td&gt; &lt;td&gt;   39.140&lt;/td&gt; &lt;td&gt;    4.906&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;  112.981&lt;/td&gt; &lt;td&gt;  271.071&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;income&lt;/th&gt;         &lt;td&gt;    9.8152&lt;/td&gt; &lt;td&gt;    0.404&lt;/td&gt; &lt;td&gt;   24.314&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    9.000&lt;/td&gt; &lt;td&gt;   10.630&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;We can perform the same procedure as before, but instead of &lt;strong&gt;partialling-out&lt;/strong&gt; only &lt;code&gt;income&lt;/code&gt;, now we partial out both &lt;code&gt;income&lt;/code&gt; and &lt;code&gt;day of the week&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df[&#39;coupons_tilde&#39;] = smf.ols(&#39;coupons ~ income + dayofweek&#39;, df).fit().resid
df[&#39;sales_tilde&#39;] = smf.ols(&#39;sales ~ income + dayofweek&#39;, df).fit().resid
smf.ols(&#39;sales_tilde ~ coupons_tilde - 1&#39;, df).fit().summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
        &lt;td&gt;&lt;/td&gt;           &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;coupons_tilde&lt;/th&gt; &lt;td&gt;  192.0262&lt;/td&gt; &lt;td&gt;   35.803&lt;/td&gt; &lt;td&gt;    5.363&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;  120.078&lt;/td&gt; &lt;td&gt;  263.974&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;We still get exactly the same coefficient!&lt;/p&gt;
&lt;h2 id=&#34;applications&#34;&gt;Applications&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s now inspect some useful applications of the FWL theorem.&lt;/p&gt;
&lt;h3 id=&#34;data-visualization&#34;&gt;Data Visualization&lt;/h3&gt;
&lt;p&gt;One of the advantages of the Frisch-Waugh-Theorem is that it allows us to estimate the coefficient of interest from a &lt;strong&gt;univariate&lt;/strong&gt; regression, i.e. with a single explanatory variable (or feature).&lt;/p&gt;
&lt;p&gt;Therefore, we can now represent the relationship of interest &lt;strong&gt;graphically&lt;/strong&gt;. Let&amp;rsquo;s plot the residual &lt;code&gt;sales&lt;/code&gt; against the residual &lt;code&gt;coupons&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sns.regplot(x=&amp;quot;coupons_tilde&amp;quot;, y=&amp;quot;sales_tilde&amp;quot;, data=df, ci=False, line_kws={&#39;color&#39;:&#39;r&#39;, &#39;label&#39;:&#39;linear fit&#39;})
plt.legend()
plt.title(f&amp;quot;Residual sales and residual coupons&amp;quot;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/fwl_39_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Now it&amp;rsquo;s evident from the graph that the &lt;strong&gt;conditional relationship&lt;/strong&gt; between &lt;code&gt;sales&lt;/code&gt; and &lt;code&gt;coupons&lt;/code&gt; is positive.&lt;/p&gt;
&lt;p&gt;One problem with this approach is that the variables are &lt;strong&gt;hard to interpret&lt;/strong&gt;: we now have negative values for both &lt;code&gt;sales&lt;/code&gt; and &lt;code&gt;coupons&lt;/code&gt;. Weird.&lt;/p&gt;
&lt;p&gt;Why did it happen? It happened because when we partialled-out the variables, we included the &lt;strong&gt;intercept&lt;/strong&gt; in the regression, effectively de-meaning the variables (i.e. normalizing their values so that their mean is zero).&lt;/p&gt;
&lt;p&gt;We can &lt;strong&gt;solve&lt;/strong&gt; this problem by &lt;strong&gt;scaling&lt;/strong&gt; both variables, adding their mean.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df[&#39;coupons_tilde_scaled&#39;] = df[&#39;coupons_tilde&#39;] + np.mean(df[&#39;coupons&#39;])
df[&#39;sales_tilde_scaled&#39;] = df[&#39;sales_tilde&#39;] + np.mean(df[&#39;sales&#39;])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now the magnitudes of the two variables are interpretable again.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sns.regplot(x=&amp;quot;coupons_tilde_scaled&amp;quot;, y=&amp;quot;sales_tilde_scaled&amp;quot;, data=df, ci=False, line_kws={&#39;color&#39;:&#39;r&#39;, &#39;label&#39;:&#39;linear fit&#39;})
plt.legend()
plt.title(f&amp;quot;Residual sales scaled and residual coupons scaled&amp;quot;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/fwl_43_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Is this a &lt;strong&gt;valid&lt;/strong&gt; approach or did it alter our estimates? We can can check it by running the regression with the scaled partialled-out variables.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;smf.ols(&#39;sales_tilde_scaled ~ coupons_tilde_scaled&#39;, df).fit().summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
            &lt;td&gt;&lt;/td&gt;              &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt;            &lt;td&gt;  641.6486&lt;/td&gt; &lt;td&gt;   10.017&lt;/td&gt; &lt;td&gt;   64.054&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;  621.507&lt;/td&gt; &lt;td&gt;  661.790&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;coupons_tilde_scaled&lt;/th&gt; &lt;td&gt;  192.0262&lt;/td&gt; &lt;td&gt;   36.174&lt;/td&gt; &lt;td&gt;    5.308&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;  119.294&lt;/td&gt; &lt;td&gt;  264.758&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;The coefficient is exactly the same as before!&lt;/p&gt;
&lt;h3 id=&#34;computational-speed&#34;&gt;Computational Speed&lt;/h3&gt;
&lt;p&gt;Another application of the Frisch-Waugh-Lovell theorem is to increase the computational speed of linear estimators. For example it is used to compute efficient linear estimators in presence of high-dimensional fixed effects (&lt;code&gt;day of the week&lt;/code&gt; in our example).&lt;/p&gt;
&lt;p&gt;Some packages that exploit the Frisch-Waugh-Lovell theorem include&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://scorreia.com/software/reghdfe/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;reghdfe in Stata&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://pyhdfe.readthedocs.io/en/stable/index.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;pyhdfe in Python&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;However, it&amp;rsquo;s important to also mention the &lt;a href=&#34;https://cran.r-project.org/web/packages/fixest/index.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;fixest&lt;/a&gt; package in R, which is also exceptionally efficient in running regressions with high dimensional fixed effects.&lt;/p&gt;
&lt;h3 id=&#34;inference-and-machine-learning&#34;&gt;Inference and Machine Learning&lt;/h3&gt;
&lt;p&gt;Another important application of the FWL theorem sits at the intersection of &lt;strong&gt;machine learning&lt;/strong&gt; and &lt;strong&gt;causal inference&lt;/strong&gt;. I am referring to the work on post-double selection by &lt;a href=&#34;https://academic.oup.com/restud/article-abstract/81/2/608/1523757&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Belloni, Chernozhukov, Hansen (2013)&lt;/a&gt; and the follow up work on &amp;ldquo;double machine learning&amp;rdquo; by &lt;a href=&#34;https://academic.oup.com/ectj/article/21/1/C1/5056401&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Chernozhukov, Chetverikov, Demirer, Duflo, Hansen, Newey, Robins (2018)&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;I plan to cover both applications in future posts, but I wanted to start with the basics. Stay tuned!&lt;/p&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;p&gt;[1] R. Frisch and F. V. Waugh, &lt;a href=&#34;https://www.jstor.org/stable/1907330&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Partial Time Regressions as Compared with Individual Trends&lt;/a&gt; (1933), &lt;em&gt;Econometrica&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;[2] M. C. Lowell, &lt;a href=&#34;https://www.tandfonline.com/doi/abs/10.1080/01621459.1963.10480682&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Seasonal Adjustment of Economic Time Series and Multiple Regression Analysis&lt;/a&gt; (1963), &lt;em&gt;Journal of the American Statistical Association&lt;/em&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>DAGs and Control Variables</title>
      <link>https://matteocourthoud.github.io/post/controls/</link>
      <pubDate>Mon, 25 Apr 2022 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/post/controls/</guid>
      <description>&lt;p&gt;When analyzing causal relationships, it is very hard to understand which variables to &lt;strong&gt;condition the analysis on&lt;/strong&gt;, i.e. how to &amp;ldquo;split&amp;rdquo; the data so that we are &lt;strong&gt;comparing apples to apples&lt;/strong&gt;. For example, if you want to understand the effect of having a tablet in class on studenta&amp;rsquo; performance, it makes sense to compare schools where students have similar socio-economic backgrounds. Otherwise, the risk is that only wealthier students can afford a tablet and, without controlling for it, we might attribute the effect to tablets instead of the socio-economic background.&lt;/p&gt;
&lt;p&gt;When the treatment of interest comes from a proper &lt;strong&gt;randomized experiment&lt;/strong&gt;, we do not need to worry about conditioning on other variables. If tablets are distributed randomly across schools, and we have enough schools in the experiment, we do not have to worry about the socio-economic background of students. The only advantage of conditioning the analysis on some so-called &amp;ldquo;control variable&amp;rdquo; could be an increase in power. However, this is a different story.&lt;/p&gt;
&lt;p&gt;In this post, we are going to have a brief introduction to Directed Acyclic Graphs and how they can be useful to select variables to condition a causal analysis on. Not only DAGs provide visual intuition on which variables we need to &lt;em&gt;include&lt;/em&gt; in the analysis, but also on which variables we should &lt;em&gt;not include&lt;/em&gt;, and why.&lt;/p&gt;
&lt;h2 id=&#34;directed-acyclic-graphs&#34;&gt;Directed Acyclic Graphs&lt;/h2&gt;
&lt;h3 id=&#34;definitions&#34;&gt;Definitions&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Directed acyclic graphs&lt;/strong&gt; (&lt;strong&gt;DAG&lt;/strong&gt;s) provide a visual representation of the data generating process. Random variables are represented with letters (e.g. $X$) and causal relationships are represented with arrows (e.g. $\to$). For example, we interpret&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-mermaid&#34;&gt;flowchart LR
classDef white fill:#FFFFFF,stroke:#000000,stroke-width:2px
X((X)):::white --&amp;gt; Y((Y)):::white
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;as $X$ (possibly) causes $Y$. We call a &lt;strong&gt;path&lt;/strong&gt; between two variables $X$ and $Y$ any connection, &lt;em&gt;independently of the direction of the arrows&lt;/em&gt;. If all arrows point forward, we call it a &lt;strong&gt;causal path&lt;/strong&gt;, otherwise we call it a &lt;strong&gt;spurious path&lt;/strong&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-mermaid&#34;&gt;flowchart LR
classDef included fill:#DCDCDC,stroke:#000000,stroke-width:2px;
classDef excluded fill:#ffffff,stroke:#000000,stroke-width:2px;

X((X))
Y((Y))
Z1((Z1))
Z2((Z2))
Z3((Z3))

X --&amp;gt; Z1
Z1 --&amp;gt; Z2
Z3 --&amp;gt; Z2
Z3 --&amp;gt; Y

class X,Y included;
class Z1,Z2,Z3 excluded;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In the example above, we have a path between $X$ and $Y$ passing through the variables $Z_1$, $Z_2$, and $Z_3$. Since not all arrows point forward, the path is &lt;em&gt;spurious&lt;/em&gt; and there is no causal relationship of $X$ on $Y$. In fact, variable $Z_2$ is caused by both $Z_1$ and $Z_3$ and therefore &lt;strong&gt;blocks&lt;/strong&gt; the path.&lt;/p&gt;
&lt;p&gt;$Z_2$ is called a &lt;strong&gt;collider&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;The purpose of our analysis is to assess the &lt;strong&gt;causal relationship&lt;/strong&gt; between two variables $X$ and $Y$. Directed acyclic graphs are useful because they provide us instructions on which other variables $Z$ we need to &lt;strong&gt;condition&lt;/strong&gt; our analysis on. Conditioning the analysis on a variable means that we keep it fixed and we draw our conclusions &lt;em&gt;ceteris paribus&lt;/em&gt;. For example, in a linear regression framework, inserting another regressor $Z$ means that we are computing the best linear approximation of the conditional expectation function of $Y$ given $X$, &lt;em&gt;conditional&lt;/em&gt; on the observed values of $Z$.&lt;/p&gt;
&lt;h3 id=&#34;causality&#34;&gt;Causality&lt;/h3&gt;
&lt;p&gt;In order to assess causality, we want to &lt;strong&gt;close all spurious paths&lt;/strong&gt; between $X$ and $Y$. The &lt;strong&gt;questions&lt;/strong&gt; now are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;When is a path &lt;strong&gt;open&lt;/strong&gt;?
&lt;ul&gt;
&lt;li&gt;If it does not contain &lt;em&gt;colliders&lt;/em&gt;. Otherwise, it is &lt;em&gt;closed&lt;/em&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;How do you &lt;strong&gt;close an open path&lt;/strong&gt;?
&lt;ul&gt;
&lt;li&gt;You condition on &lt;em&gt;at least one&lt;/em&gt; intermediate variable.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;How do you &lt;strong&gt;open a closed path&lt;/strong&gt;?
&lt;ul&gt;
&lt;li&gt;You condition on &lt;em&gt;all&lt;/em&gt; colliders along the path.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Suppose we are again interested in the causal relationship of $X$ on $Y$. Let&amp;rsquo;s consider the following graph&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-mermaid&#34;&gt;flowchart LR
classDef included fill:#DCDCDC,stroke:#000000,stroke-width:2px;
classDef excluded fill:#ffffff,stroke:#000000,stroke-width:2px;

X((X))
Y((Y))
Z1((Z1))
Z2((Z2))
Z3((Z3))

X --&amp;gt; Y
X --&amp;gt; Z2
Z2 --&amp;gt; Y
Z1 --&amp;gt; X
Z1 --&amp;gt; Y
X --&amp;gt; Z3
Y --&amp;gt; Z3

class X,Y included;
class Z1,Z2,Z3 excluded;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In this case, apart from the direct path, there are &lt;strong&gt;three non-direct paths&lt;/strong&gt; between $X$ and $Y$ through the variables $Z_1$, $Z_2$, and $Z_3$.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s consider the case in which we analyze the relationship between $X$ and $Y$, ignoring all other variables.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;The path through $Z_1$ is &lt;strong&gt;open&lt;/strong&gt; but it is &lt;strong&gt;spurious&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;The path through $Z_2$ is &lt;strong&gt;open&lt;/strong&gt; and &lt;strong&gt;causal&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;The path through $Z_3$ is &lt;strong&gt;closed&lt;/strong&gt; since $Z_3$ is a &lt;em&gt;collider&lt;/em&gt; and it is &lt;strong&gt;spurious&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Let&amp;rsquo;s draw the same graph indicating in &lt;em&gt;grey&lt;/em&gt; variables that we are conditioning on, with &lt;em&gt;dotted lines&lt;/em&gt; closed paths, with &lt;em&gt;red lines&lt;/em&gt; spurious open paths, and with &lt;em&gt;green lines&lt;/em&gt; causal open paths.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-mermaid&#34;&gt;flowchart LR
classDef included fill:#DCDCDC,stroke:#000000,stroke-width:2px;
classDef excluded fill:#ffffff,stroke:#000000,stroke-width:2px;

X((X))
Y((Y))
Z1((Z1))
Z2((Z2))
Z3((Z3))

X --&amp;gt; Y
X --&amp;gt; Z2
Z2 --&amp;gt; Y
Z1 --&amp;gt; X
Z1 --&amp;gt; Y
X -.-&amp;gt; Z3
Y -.-&amp;gt; Z3

linkStyle 0,1,2 stroke:#00ff00,stroke-width:4px;
linkStyle 3,4 stroke:#ff0000,stroke-width:4px;
class X,Y included;
class Z1,Z2,Z3 excluded;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In this case, to assess the &lt;strong&gt;causal&lt;/strong&gt; relationship between $X$ and $Y$ we need to &lt;strong&gt;close&lt;/strong&gt; the path that passes through $Z_1$. We can do that by conditioning the analysis on $Z_1$.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-mermaid&#34;&gt;flowchart LR
classDef included fill:#DCDCDC,stroke:#000000,stroke-width:2px;
classDef excluded fill:#ffffff,stroke:#000000,stroke-width:2px;

X((X))
Y((Y))
Z1((Z1))
Z2((Z2))
Z3((Z3))

X --&amp;gt; Y
X --&amp;gt; Z2
Z2 --&amp;gt; Y
Z1 -.-&amp;gt; X
Z1 -.-&amp;gt; Y
X -.-&amp;gt; Z3
Y -.-&amp;gt; Z3

linkStyle 0,1,2 stroke:#00ff00,stroke-width:4px;
class X,Y,Z1 included;
class Z2,Z3 excluded;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we are able to recover the causal relationship between $X$ and $Y$ by conditioning on $Z_1$.&lt;/p&gt;
&lt;p&gt;What would happen if we were also &lt;strong&gt;conditioning on $Z_2$&lt;/strong&gt;? In this case, we would &lt;strong&gt;close&lt;/strong&gt; the path passing through $Z_2$ leaving only the &lt;em&gt;direct&lt;/em&gt; path between $X$ and $Y$ open. We would then recover only the &lt;strong&gt;direct effect&lt;/strong&gt; of $X$ on $Y$ and not the &lt;em&gt;indirect&lt;/em&gt; one.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-mermaid&#34;&gt;flowchart LR
classDef included fill:#DCDCDC,stroke:#000000,stroke-width:2px;
classDef excluded fill:#ffffff,stroke:#000000,stroke-width:2px;

X((X))
Y((Y))
Z1((Z1))
Z2((Z2))
Z3((Z3))

X --&amp;gt; Y
X -.-&amp;gt; Z2
Z2 -.-&amp;gt; Y
Z1 -.-&amp;gt; X
Z1 -.-&amp;gt; Y
X -.-&amp;gt; Z3
Y -.-&amp;gt; Z3

linkStyle 0 stroke:#00ff00,stroke-width:4px;
class X,Y,Z1,Z2 included;
class Z3 excluded;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What would happen if we were also &lt;strong&gt;conditioning on $Z_3$&lt;/strong&gt;? In this case, we would &lt;strong&gt;open&lt;/strong&gt; the path passing through $Z_3$ which is a &lt;strong&gt;spurious&lt;/strong&gt; path. We would then &lt;strong&gt;not&lt;/strong&gt; be able to recover the causal effect of $X$ on $Y$.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-mermaid&#34;&gt;flowchart LR
classDef included fill:#DCDCDC,stroke:#000000,stroke-width:2px;

X((X))
Y((Y))
Z1((Z1))
Z2((Z2))
Z3((Z3))

X --&amp;gt; Y
X -.-&amp;gt; Z2
Z2 -.-&amp;gt; Y
Z1 -.-&amp;gt; X
Z1 -.-&amp;gt; Y
X --&amp;gt; Z3
Y --&amp;gt; Z3

linkStyle 0 stroke:#00ff00,stroke-width:4px;
linkStyle 5,6 stroke:#ff0000,stroke-width:4px;
class X,Y,Z1,Z2,Z3 included;
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;example-class-size-and-math-scores&#34;&gt;Example: Class Size and Math Scores&lt;/h2&gt;
&lt;p&gt;Suppose you are interested in the &lt;strong&gt;effect of class size on math scores&lt;/strong&gt;. Are bigger classes better or worse for students&amp;rsquo; performance?&lt;/p&gt;
&lt;p&gt;Assume that the data generating process can be represented with the following &lt;strong&gt;DAG&lt;/strong&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-mermaid&#34;&gt;flowchart TB
classDef included fill:#DCDCDC,stroke:#000000,stroke-width:2px;
classDef excluded fill:#ffffff,stroke:#000000,stroke-width:2px;
classDef unobserved fill:#ffffff,stroke:#000000,stroke-width:2px,stroke-dasharray: 5 5;


X((class size))
Y((math score))
Z1((class year))
Z2((good school))
Z3((math hours))
Z4((hist score))
U((ability))

X --&amp;gt; Y
Z1 --&amp;gt; X
X --&amp;gt; Z4
U --&amp;gt; Y
U --&amp;gt; Z4
Z2 --&amp;gt; X
Z2 --&amp;gt; Y
Z2 --&amp;gt; Z4
Z3 --&amp;gt; Y

class X,Y included;
class Z1,Z2,Z3,Z4 excluded;
class U unobserved;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The variables of interest are highlighted. Moreover, the dotted line around &lt;code&gt;ability&lt;/code&gt; indicates that this is a variable that we do not observe in the data.&lt;/p&gt;
&lt;p&gt;We can now load the data and check what it looks like.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;%matplotlib inline
%config InlineBackend.figure_format = &#39;retina&#39;
from src.utils import *
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from src.dgp import dgp_school

df = dgp_school().generate_data()
df.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;math_hours&lt;/th&gt;
      &lt;th&gt;history_hours&lt;/th&gt;
      &lt;th&gt;good_school&lt;/th&gt;
      &lt;th&gt;class_year&lt;/th&gt;
      &lt;th&gt;class_size&lt;/th&gt;
      &lt;th&gt;math_score&lt;/th&gt;
      &lt;th&gt;hist_score&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;15&lt;/td&gt;
      &lt;td&gt;13.009309&lt;/td&gt;
      &lt;td&gt;15.167024&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;19&lt;/td&gt;
      &lt;td&gt;13.047033&lt;/td&gt;
      &lt;td&gt;13.387456&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;25&lt;/td&gt;
      &lt;td&gt;8.330311&lt;/td&gt;
      &lt;td&gt;10.824070&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;22&lt;/td&gt;
      &lt;td&gt;11.322190&lt;/td&gt;
      &lt;td&gt;14.594394&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;15&lt;/td&gt;
      &lt;td&gt;12.338458&lt;/td&gt;
      &lt;td&gt;11.871626&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;What variables should we condition our regression on, in order to estimate the causal effect of &lt;code&gt;class size&lt;/code&gt; on &lt;code&gt;math scores&lt;/code&gt;?&lt;/p&gt;
&lt;p&gt;First of all, let&amp;rsquo;s look at what happens if we do not condition our analysis on any variable and we just regress &lt;code&gt;math score&lt;/code&gt; on &lt;code&gt;class size&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;smf.ols(&#39;math_score ~ class_size&#39;, df).fit().summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
       &lt;td&gt;&lt;/td&gt;         &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt;  &lt;td&gt;   12.0421&lt;/td&gt; &lt;td&gt;    0.259&lt;/td&gt; &lt;td&gt;   46.569&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;   11.535&lt;/td&gt; &lt;td&gt;   12.550&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;class_size&lt;/th&gt; &lt;td&gt;   -0.0399&lt;/td&gt; &lt;td&gt;    0.013&lt;/td&gt; &lt;td&gt;   -3.025&lt;/td&gt; &lt;td&gt; 0.003&lt;/td&gt; &lt;td&gt;   -0.066&lt;/td&gt; &lt;td&gt;   -0.014&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;The effect of &lt;code&gt;class_size&lt;/code&gt; is negative and statistically different from zero.&lt;/p&gt;
&lt;p&gt;But should we believe this estimated effect? Without controlling for anything, this is &lt;strong&gt;DAG representation&lt;/strong&gt; of the effect we are capturing.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-mermaid&#34;&gt;flowchart TB
classDef included fill:#DCDCDC,stroke:#000000,stroke-width:2px;
classDef excluded fill:#ffffff,stroke:#000000,stroke-width:2px;
classDef unobserved fill:#ffffff,stroke:#000000,stroke-width:2px,stroke-dasharray: 5 5;


X((class size))
Y((math score))
Z1((class year))
Z2((good school))
Z3((math hours))
Z4((hist score))
U((ability))

X --&amp;gt; Y
Z1 --&amp;gt; X
X -.-&amp;gt; Z4
U --&amp;gt; Y
U -.-&amp;gt; Z4
Z2 --&amp;gt; X
Z2 --&amp;gt; Y
Z2 --&amp;gt; Z4
Z3 --&amp;gt; Y

linkStyle 0 stroke:#00ff00,stroke-width:4px;
linkStyle 5,6 stroke:#ff0000,stroke-width:4px;

class X,Y included;
class Z1,Z2,Z3,Z4 excluded;
class U unobserved;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There is a &lt;strong&gt;spurious&lt;/strong&gt; path passing through &lt;code&gt;good school&lt;/code&gt; that &lt;strong&gt;biases&lt;/strong&gt; our estimated coefficient. Intuitively, being enrolled in a better school improves the students&amp;rsquo; math scores and better schools might have smaller class sizes. We need to control for the quality of the school.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;smf.ols(&#39;math_score ~ class_size + good_school&#39;, df).fit().summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
       &lt;td&gt;&lt;/td&gt;          &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt;   &lt;td&gt;    4.7449&lt;/td&gt; &lt;td&gt;    0.247&lt;/td&gt; &lt;td&gt;   19.176&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    4.259&lt;/td&gt; &lt;td&gt;    5.230&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;class_size&lt;/th&gt;  &lt;td&gt;    0.2095&lt;/td&gt; &lt;td&gt;    0.010&lt;/td&gt; &lt;td&gt;   20.020&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    0.189&lt;/td&gt; &lt;td&gt;    0.230&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;good_school&lt;/th&gt; &lt;td&gt;    5.0807&lt;/td&gt; &lt;td&gt;    0.130&lt;/td&gt; &lt;td&gt;   39.111&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    4.826&lt;/td&gt; &lt;td&gt;    5.336&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;Now the estimate of the effect of &lt;code&gt;class size&lt;/code&gt; on &lt;code&gt;math score&lt;/code&gt; is &lt;strong&gt;unbiased&lt;/strong&gt;! Indeed, the true coefficient in the data generating process was $0.2$.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-mermaid&#34;&gt;flowchart TB
classDef included fill:#DCDCDC,stroke:#000000,stroke-width:2px;
classDef excluded fill:#ffffff,stroke:#000000,stroke-width:2px;
classDef unobserved fill:#ffffff,stroke:#000000,stroke-width:2px,stroke-dasharray: 5 5;


X((class size))
Y((math score))
Z1((class year))
Z2((good school))
Z3((math hours))
Z4((hist score))
U((ability))

X --&amp;gt; Y
Z1 --&amp;gt; X
X -.-&amp;gt; Z4
U --&amp;gt; Y
U -.-&amp;gt; Z4
Z2 -.-&amp;gt; X
Z2 -.-&amp;gt; Y
Z2 --&amp;gt; Z4
Z3 --&amp;gt; Y

linkStyle 0 stroke:#00ff00,stroke-width:4px;

class X,Y,Z2 included;
class Z1,Z3,Z4 excluded;
class U unobserved;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What would happen if we were to instead &lt;strong&gt;control for all variables&lt;/strong&gt;?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;smf.ols(&#39;math_score ~ class_size + good_school + math_hours + class_year + hist_score&#39;, df).fit().summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
       &lt;td&gt;&lt;/td&gt;          &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt;   &lt;td&gt;   -0.7847&lt;/td&gt; &lt;td&gt;    0.310&lt;/td&gt; &lt;td&gt;   -2.529&lt;/td&gt; &lt;td&gt; 0.012&lt;/td&gt; &lt;td&gt;   -1.394&lt;/td&gt; &lt;td&gt;   -0.176&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;class_size&lt;/th&gt;  &lt;td&gt;    0.1292&lt;/td&gt; &lt;td&gt;    0.010&lt;/td&gt; &lt;td&gt;   13.054&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    0.110&lt;/td&gt; &lt;td&gt;    0.149&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;good_school&lt;/th&gt; &lt;td&gt;    2.9815&lt;/td&gt; &lt;td&gt;    0.170&lt;/td&gt; &lt;td&gt;   17.533&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    2.648&lt;/td&gt; &lt;td&gt;    3.315&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;math_hours&lt;/th&gt;  &lt;td&gt;    1.0516&lt;/td&gt; &lt;td&gt;    0.048&lt;/td&gt; &lt;td&gt;   21.744&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    0.957&lt;/td&gt; &lt;td&gt;    1.147&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;class_year&lt;/th&gt;  &lt;td&gt;    0.0424&lt;/td&gt; &lt;td&gt;    0.037&lt;/td&gt; &lt;td&gt;    1.130&lt;/td&gt; &lt;td&gt; 0.259&lt;/td&gt; &lt;td&gt;   -0.031&lt;/td&gt; &lt;td&gt;    0.116&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;hist_score&lt;/th&gt;  &lt;td&gt;    0.4116&lt;/td&gt; &lt;td&gt;    0.027&lt;/td&gt; &lt;td&gt;   15.419&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    0.359&lt;/td&gt; &lt;td&gt;    0.464&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;The coefficient is again &lt;strong&gt;biased&lt;/strong&gt;. Why?&lt;/p&gt;
&lt;p&gt;We have opened a new spurious path by controlling for &lt;code&gt;hist score&lt;/code&gt;. In fact, &lt;code&gt;hist score&lt;/code&gt; is a &lt;strong&gt;collider&lt;/strong&gt; and controlling for it has opened a path through &lt;code&gt;hist score&lt;/code&gt; and &lt;code&gt;ability&lt;/code&gt; that was otherwise closed.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-mermaid&#34;&gt;flowchart TB
classDef included fill:#DCDCDC,stroke:#000000,stroke-width:2px;
classDef excluded fill:#ffffff,stroke:#000000,stroke-width:2px;
classDef unobserved fill:#ffffff,stroke:#000000,stroke-width:2px,stroke-dasharray: 5 5;


X((class size))
Y((math score))
Z1((class year))
Z2((good school))
Z3((math hours))
Z4((hist score))
U((ability))

X --&amp;gt; Y
Z1 --&amp;gt; X
X --&amp;gt; Z4
U --&amp;gt; Y
U --&amp;gt; Z4
Z2 -.-&amp;gt; X
Z2 -.-&amp;gt; Y
Z2 --&amp;gt; Z4
Z3 --&amp;gt; Y

linkStyle 0 stroke:#00ff00,stroke-width:4px;
linkStyle 2,3,4 stroke:#ff0000,stroke-width:4px;

class X,Y,Z1,Z2,Z3,Z4 included;
class U unobserved;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The example was inspired by the following tweet.&lt;/p&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;We can illustrate this with Model 16 of the &amp;quot;Crash Course in Good and Bad Controls&amp;quot; (&lt;a href=&#34;https://t.co/GcSNzhuVt2&#34;&gt;https://t.co/GcSNzhuVt2&lt;/a&gt;). Here X = class size, Y = math4, Z = read4, and U = student&amp;#39;s ability. Conditioning on Z opens the path X -&amp;gt; Z &amp;lt;- U -&amp;gt; Y and it is thus a &amp;quot;bad control.&amp;quot; &lt;a href=&#34;https://t.co/KNfqtsMWwB&#34;&gt;https://t.co/KNfqtsMWwB&lt;/a&gt; &lt;a href=&#34;https://t.co/lUSigNYSJj&#34;&gt;pic.twitter.com/lUSigNYSJj&lt;/a&gt;&lt;/p&gt;&amp;mdash; Análise Real (@analisereal) &lt;a href=&#34;https://twitter.com/analisereal/status/1502793254592401409?ref_src=twsrc%5Etfw&#34;&gt;March 12, 2022&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;

&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;In this post, we have seen how to use Directed Acyclic Graphs to select control variables in a causal analysis. DAGs are very helpful tools since they provide an intuitive graphical representation of causal relationships between random variables. Contrary to common intuition that &amp;ldquo;the more information the better&amp;rdquo;, sometimes including extra variables might bias the analysis, preventing a causal interpretation of the results. In particular, we must pay attention not to include &lt;em&gt;colliders&lt;/em&gt; that open &lt;em&gt;spurious&lt;/em&gt; paths that would otherwise be closed.&lt;/p&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;p&gt;[1] C. Cinelli, A. Forney, J. Pearl, &lt;a href=&#34;https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3689437&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;A Crash Course in Good and Bad Controls&lt;/a&gt; (2018), working paper.&lt;/p&gt;
&lt;p&gt;[2] J. Pearl, &lt;a href=&#34;http://bayes.cs.ucla.edu/BOOK-2K/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Causality&lt;/a&gt; (2009), Cambridge University Press.&lt;/p&gt;
&lt;p&gt;[3] S. Cunningham, Chapter 3 of &lt;a href=&#34;https://mixtape.scunning.com/dag.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The Causal Inference Mixtape&lt;/a&gt; (2021), Yale University Press.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>How To Make A Personal Website with Hugo</title>
      <link>https://matteocourthoud.github.io/post/website/</link>
      <pubDate>Mon, 18 Apr 2022 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/post/website/</guid>
      <description>&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;During the second year of my PhD, I decided that I wanted to have a personal website. After (too many) hours of research, I decided to build it using &lt;a href=&#34;https://gohugo.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Hugo&lt;/a&gt;, and I picked the &lt;a href=&#34;https://wowchemy.com/docs/getting-started/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Wowchemy&lt;/strong&gt;&lt;/a&gt; theme, also known as &lt;a href=&#34;https://themes.gohugo.io/academic/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Hugo Academic&lt;/a&gt;. In this tutorial, I am going to share my guide to building a website on &lt;a href=&#34;https://pages.github.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github Pages&lt;/a&gt; so that you don’t have to go through all the pain I went through 😁.&lt;/p&gt;
&lt;p&gt;Before we start, I have to warn you. If you don’t care about &lt;strong&gt;personalization&lt;/strong&gt; or if you have very &lt;strong&gt;little time&lt;/strong&gt; to spend on building a website, I strongly recommend &lt;a href=&#34;https://sites.google.com/new&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Google Sites&lt;/a&gt;, which is, in my opinion, the fastest and easiest way to build an academic website. However, if you enjoy customizing your website, or if you like &lt;a href=&#34;https://github.com/matteocourthoud/custom-wowchemy-theme&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;my template&lt;/a&gt;, then this guide might be useful.&lt;/p&gt;
&lt;p&gt;Also, note that Hugo offers &lt;a href=&#34;https://themes.gohugo.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;many other website templates&lt;/a&gt;. I suggest checking them out. Some interesting &lt;strong&gt;alternatives&lt;/strong&gt; are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://themes.gohugo.io/themes/hugo-resume/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Resume&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://themes.gohugo.io/themes/somrat/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Somrat&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://themes.gohugo.io/themes/hugo-uilite/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;UILite (paid)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;However, in this guide, I will concentrate on the Hugo Academic theme, since it’s the one I used for &lt;a href=&#34;https://matteocourthoud.github.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;my website&lt;/a&gt; and I believe it’s the best one for building academic profile pages. But the first part of this guide is general and it works for any Hugo theme.&lt;/p&gt;
&lt;h2 id=&#34;create-website&#34;&gt;Create Website&lt;/h2&gt;
&lt;h3 id=&#34;0-prerequisites&#34;&gt;0. Prerequisites&lt;/h3&gt;
&lt;p&gt;Before we start, I will take for granted the following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;that you have an account on &lt;a href=&#34;https://www.github.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;that you have &lt;a href=&#34;https://www.r-project.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;R&lt;/a&gt; installed&lt;/li&gt;
&lt;li&gt;that you have &lt;a href=&#34;https://www.rstudio.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;RStudio&lt;/a&gt; installed&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;1-create-github-repository&#34;&gt;1. Create Github Repository&lt;/h3&gt;
&lt;p&gt;First, go to your &lt;a href=&#34;https://www.github.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt; page and create a new repository (&lt;code&gt;+&lt;/code&gt; button in the top-right corner).&lt;/p&gt;
&lt;img src=&#34;img/new_repo.png&#34; alt=&#34;new_repo&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;Name the repository &lt;code&gt;username.github.io&lt;/code&gt; where &lt;code&gt;username&lt;/code&gt; is your Github username.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;img/name_repo.png&#34; alt=&#34;name_repo&#34;&gt;&lt;/p&gt;
&lt;p&gt;In my case, my github username is &lt;code&gt;matteocourthoud&lt;/code&gt;, therefore the repository is &lt;code&gt;matteocourthoud.github.io&lt;/code&gt; and my personal website is &lt;a href=&#34;https://matteocourthoud.github.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://matteocourthoud.github.io&lt;/a&gt;. Use the default settings when creating the repository.&lt;/p&gt;
&lt;h3 id=&#34;2-install-blogdown-and-hugo&#34;&gt;2. Install Blogdown and Hugo&lt;/h3&gt;
&lt;p&gt;Now you need to install Blogdown, which is the program what will allow you to build and deploy your website, and Hugo, which is the template generator.&lt;/p&gt;
&lt;p&gt;Switch to RStudio and type the following commands&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Install blogdown
install.packages(&amp;quot;blogdown&amp;quot;)

# Install Hugo
blogdown::install_hugo()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now everything should be ready!&lt;/p&gt;
&lt;h3 id=&#34;3-setup-folder&#34;&gt;3. Setup folder&lt;/h3&gt;
&lt;p&gt;Open RStudio and select &lt;code&gt;New Project&lt;/code&gt;.&lt;/p&gt;
&lt;img src=&#34;img/new_project.png&#34; alt=&#34;new_project&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;Select &lt;code&gt;New Directory&lt;/code&gt; when asked where to create the project.&lt;/p&gt;
&lt;img src=&#34;img/new_project2.png&#34; alt=&#34;new_project2&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;Then select &lt;code&gt;Website using blogdown&lt;/code&gt; as project type.&lt;/p&gt;
&lt;img src=&#34;img/new_project3.png&#34; alt=&#34;new_project3&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;Now you have to select a couple of options:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;Directory name&lt;/code&gt;: here input the name of the folder which will contain all the website files. The name is irrelevant. I called mine &lt;code&gt;website&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Create project as a subdirectory of&lt;/code&gt;: select the directory in which you want to put the website folder.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Theme&lt;/code&gt;: input &lt;code&gt;wowchemy/starter-academic&lt;/code&gt; instead of the default theme.&lt;/li&gt;
&lt;/ul&gt;
&lt;img src=&#34;img/new_project4.png&#34; alt=&#34;new_project4&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: if you want to install a different theme, just go on the corresponding Github page (for example &lt;a href=&#34;https://github.com/caressofsteel/hugo-story&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/caressofsteel/hugo-story&lt;/a&gt;) and instead of &lt;code&gt;gcushen/hugo-academic&lt;/code&gt;, insert the corresponding Github repository (for example &lt;code&gt;caressofsteel/hugo-story&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;If you go into the website folder, it should look something like&lt;/p&gt;
&lt;img src=&#34;img/folder.png&#34; alt=&#34;folder&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;h3 id=&#34;4-build-website&#34;&gt;4. Build website&lt;/h3&gt;
&lt;p&gt;To build the website, open the RProject file &lt;code&gt;website.Rproj&lt;/code&gt; in RStudio and type&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;blogdown::hugo_build(local=TRUE)
&lt;/code&gt;&lt;/pre&gt;
&lt;img src=&#34;img/hugo_build.png&#34; alt=&#34;hugo_build&#34;&gt;
&lt;p&gt;This command will generate a &lt;code&gt;public/&lt;/code&gt; subfolder in which the actual code of the website is stored.&lt;/p&gt;
&lt;p&gt;Don’t ask me why, but the option &lt;code&gt;local=TRUE&lt;/code&gt; seems to make a difference. Updating without it sometimes does not change the content in the &lt;code&gt;public/&lt;/code&gt; subfolder.&lt;/p&gt;
&lt;img src=&#34;img/folder2.png&#34; alt=&#34;folder2&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;To preview the website, type in RStudio&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;blogdown::serve_site()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The following preview should automatically open in your browser.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;img/prevew.png&#34; alt=&#34;preview&#34;&gt;&lt;/p&gt;
&lt;p&gt;Previewing the website is very useful as it allows you to see live changes locally inside RStudio, before publishing them. This is the &lt;strong&gt;main advantage of working in RStudio&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;If the preview does &lt;em&gt;not automatically&lt;/em&gt; open in your browser, and instead it previews inside RStudio Viewer panel, you can preview it in your browser using the upper left right-most button.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;img/preview2.png&#34; alt=&#34;preview2&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;5-publish-website&#34;&gt;5. Publish website&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Importantly&lt;/strong&gt;, before pushing the code online, you need to open the file &lt;code&gt;config.yaml&lt;/code&gt; and change the &lt;code&gt;baseurl&lt;/code&gt; to your future website url, which will be &lt;code&gt;https://username.github.io/&lt;/code&gt;, where &lt;code&gt;username&lt;/code&gt; is your Github username.&lt;/p&gt;
&lt;img src=&#34;img/username.png&#34; alt=&#34;username&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;Now that you have set the correct url, you have to push the changes from the &lt;code&gt;public/&lt;/code&gt; folder to your &lt;code&gt;username.github.io&lt;/code&gt; repository on Github.&lt;/p&gt;
&lt;p&gt;To do that, you need to get to the website folder. Let’s assume that the path to your folder is &lt;code&gt;Documents/website&lt;/code&gt;. Open the Terminal and type&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;cd Documents/website/public
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The following code will link the &lt;code&gt;public/&lt;/code&gt; folder, containing the actual code of the website, to your &lt;code&gt;username.github.io&lt;/code&gt; repository.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Init git in the /website/public/ folder
git init

# Add and commit the changes
git add .
git commit -m &amp;quot;first version of the website&amp;quot;

# Set origin
git remote add origin https://github.com/username/username.github.io.git

# Rename local branch
git branch -M main

# And push your updates online
git push -u origin main
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Wait a few seconds (or minutes for heavy changes) and your website should be online!&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;If the website is not working&lt;/strong&gt;, you can check the following:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Is there anything in your &lt;code&gt;public/&lt;/code&gt; folder? (does it even exist?) If not, something went wrong when compiling the website with &lt;code&gt;blogdown::hugo_build()&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Inside your &lt;code&gt;public/&lt;/code&gt; folder, there should be an &lt;code&gt;index.html&lt;/code&gt; file. If you double-click on it, you should see a local preview of your website in your browser. If not, something in the website code is wrong.&lt;/li&gt;
&lt;li&gt;Is the content of your &lt;code&gt;public/&lt;/code&gt; folder exactly the same as the content of your Github repository? If not, something went wrong when pushing to Github.&lt;/li&gt;
&lt;li&gt;Did you name your Github repository &lt;code&gt;username.github.io&lt;/code&gt;, where &lt;code&gt;username&lt;/code&gt; is your Github username?&lt;/li&gt;
&lt;li&gt;Did you change the &lt;code&gt;baseurl&lt;/code&gt; option in the file &lt;code&gt;config.yaml&lt;/code&gt; to &lt;code&gt;https://username.github.io/&lt;/code&gt;, where &lt;code&gt;username&lt;/code&gt; is your Github username?&lt;/li&gt;
&lt;li&gt;You can check the list of websites deployments at &lt;code&gt;https://github.com/username/username.github.io/deployments&lt;/code&gt;. Control that they correspond with your commits.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;If all the conditions are satisfied, but the website is still not online, maybe it’s just a matter of time. Have some patience.&lt;/p&gt;
&lt;h2 id=&#34;basic-customization&#34;&gt;Basic Customization&lt;/h2&gt;
&lt;p&gt;The basic files that you want to modify to customize your website are the following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;config/_default/config.yaml&lt;/code&gt;: general website information&lt;/li&gt;
&lt;li&gt;&lt;code&gt;config/_default/params.yaml&lt;/code&gt;: website customization&lt;/li&gt;
&lt;li&gt;&lt;code&gt;config/_default/menus.yaml&lt;/code&gt;: top bar / menu customization&lt;/li&gt;
&lt;li&gt;&lt;code&gt;content/authors/admin/_index.md&lt;/code&gt;: personal information&lt;/li&gt;
&lt;/ul&gt;
&lt;img src=&#34;img/files.png&#34; alt=&#34;files&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;For what concerns &lt;strong&gt;images&lt;/strong&gt;, there are two main things you might want to modify:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Profile picture: change the &lt;code&gt;content/authors/admin/avatar.jpg&lt;/code&gt; picture&lt;/li&gt;
&lt;li&gt;Website icon: change the &lt;code&gt;assets/media/icon.png&lt;/code&gt; picture&lt;/li&gt;
&lt;/ul&gt;
&lt;img src=&#34;img/images.png&#34; alt=&#34;images&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;In order to modify the &lt;strong&gt;widgets&lt;/strong&gt; on your homepage, go to &lt;code&gt;content/home/&lt;/code&gt; and modify the files inside. If you want to remove a section, just open the corresponding file and select &lt;code&gt;active: false&lt;/code&gt;. If there is no &lt;code&gt;active&lt;/code&gt; option, just copy the line &lt;code&gt;active: false&lt;/code&gt; in the corresponding file.&lt;/p&gt;
&lt;img src=&#34;img/widgets.png&#34; alt=&#34;widgets&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;On my website, I have only the following sections set to true:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;about&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;projects&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;posts&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;contact&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;To change the &lt;strong&gt;color palette&lt;/strong&gt; of the website, go to &lt;code&gt;data\theme&lt;/code&gt; and generate a &lt;code&gt;custom_theme.toml&lt;/code&gt; file with the following content:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-toml&#34;&gt;# Theme metadata
name = &amp;quot;My custom theme&amp;quot;

# Is theme light or dark?
light = true

# Primary
primary = &amp;quot;#284f7a&amp;quot;

# Menu
menu_primary = &amp;quot;#fff&amp;quot;
menu_text = &amp;quot;#34495e&amp;quot;
menu_text_active = &amp;quot;#284f7a&amp;quot;
menu_title = &amp;quot;#2b2b2b&amp;quot;

# Home sections
home_section_odd = &amp;quot;rgb(255, 255, 255)&amp;quot;
home_section_even = &amp;quot;rgb(247, 247, 247)&amp;quot;

[dark]
  link = &amp;quot;#bbdefb&amp;quot;
  link_hover = &amp;quot;#bbdefb&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then go to the &lt;code&gt;config/_default/params.yaml&lt;/code&gt; file and set the &lt;code&gt;theme&lt;/code&gt; to &lt;code&gt;custom_theme&lt;/code&gt;.&lt;/p&gt;
&lt;img src=&#34;img/custom_theme.png&#34; alt=&#34;custom_theme&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;You can get more information on how to personalize it &lt;a href=&#34;https://wowchemy.com/docs/getting-started/customization/#custom-theme&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;To change the &lt;strong&gt;font&lt;/strong&gt;, go to &lt;code&gt;data\fonts&lt;/code&gt; and generate a &lt;code&gt;custom_font.toml&lt;/code&gt; file with the following content:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-toml&#34;&gt;# Font style metadata
name = &amp;quot;My custom font&amp;quot;

# Optional Google font URL
google_fonts = &amp;quot;family=Roboto+Mono&amp;amp;family=Source+Sans+Pro:wght@200;300;400;700&amp;quot;

# Font families
heading_font = &amp;quot;Source Sans Pro&amp;quot;
body_font = &amp;quot;Source Sans Pro&amp;quot;
nav_font = &amp;quot;Source Sans Pro&amp;quot;
mono_font = &amp;quot;Roboto Mono&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then go to the &lt;code&gt;config/_default/params.yaml&lt;/code&gt; file and set the &lt;code&gt;font&lt;/code&gt; to &lt;code&gt;custom_font&lt;/code&gt;.&lt;/p&gt;
&lt;img src=&#34;img/custom_font.png&#34; alt=&#34;custom_font&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;You can get more information on how to personalize it &lt;a href=&#34;https://wowchemy.com/docs/getting-started/customization/#custom-font&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;. Importantly, by default, the website supports only fonts of weight 400 and 700. If you want a lighter font, like the &lt;a href=&#34;https://fonts.google.com/specimen/Source&amp;#43;Sans&amp;#43;Pro&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Source Sans Pro&lt;/a&gt; I use for my website, you have to dig into the advanced customization (which requires HTML and CSS skills).&lt;/p&gt;
&lt;h2 id=&#34;advanced-customization&#34;&gt;Advanced Customization&lt;/h2&gt;
&lt;p&gt;Advanced customization is possible but &lt;strong&gt;it’s a pain&lt;/strong&gt;. You basically want to go inside &lt;code&gt;themes\github.com\wowchemy\wowchemy-hugo-modules\wowchemy&lt;/code&gt; and start digging. Tip: you want to start digging in the following places:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;In &lt;code&gt;layouts\partials&lt;/code&gt; to edit the HTML files&lt;/li&gt;
&lt;li&gt;In &lt;code&gt;assets\scss&lt;/code&gt; to edit the SCSS code&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If you want to copy my exact theme, I have published my custom theme here: &lt;a href=&#34;https://github.com/matteocourthoud/custom-wowchemy-settings&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/matteocourthoud/custom-wowchemy-settings&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;You have to do the following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;go inside the &lt;code&gt;theme&lt;/code&gt; folder&lt;/li&gt;
&lt;li&gt;copy the content of the &lt;code&gt;custom-wowchemy-theme&lt;/code&gt; repository in a folder there&lt;/li&gt;
&lt;li&gt;go to the &lt;code&gt;config.yaml&lt;/code&gt; file into the MODULES section&lt;/li&gt;
&lt;/ul&gt;
&lt;img src=&#34;img/config_before.png&#34; alt=&#34;config_before&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;ul&gt;
&lt;li&gt;change the second link to the folder with the custom settings&lt;/li&gt;
&lt;/ul&gt;
&lt;img src=&#34;img/config_after.png&#34; alt=&#34;config_after&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;Now your website should look quite similar to mine! :)&lt;/p&gt;
&lt;h3 id=&#34;examples&#34;&gt;Examples&lt;/h3&gt;
&lt;p&gt;Here are some examples of &lt;strong&gt;advanced customizations&lt;/strong&gt; you can do. For all the examples the baseline directory is you theme directory, &lt;code&gt;themes/custom-wowchemy-theme&lt;/code&gt; if you renamed it as in the previous paragraph.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Example 1&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;What to have your section titles fixed on top of the screen?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Go to &lt;code&gt;assets/scss/wowchemy/widgets/_base.scss&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Search for &lt;code&gt;.section-heading h1&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;It should look like this&lt;/p&gt;
&lt;img src=&#34;img/section_heading_before.png&#34; alt=&#34;section_heading_before&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Add a couple of lines as follows&lt;/p&gt;
&lt;img src=&#34;img/section_heading_after.png&#34; alt=&#34;section_heading_after&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Now the section titles should stay anchored at the top of the page&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Example 2&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Do you want to put a &lt;strong&gt;background image&lt;/strong&gt; in your home page?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Put the selected background image, for example &lt;code&gt;image.png&lt;/code&gt;,  into the &lt;code&gt;static/img&lt;/code&gt; folder (the location itself does not matter)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Go to &lt;code&gt;assets/scss/wowchemy/widgets/_about.scss&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Add the following lines anywhere in the code&lt;/p&gt;
&lt;img src=&#34;img/background_widget.png&#34; alt=&#34;background_widget&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Now go to &lt;code&gt;layouts/partials/widgets/about.html&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Add the following lines after &lt;code&gt;&amp;lt;!-- About widget --&amp;gt;&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;img/background_widget2.png&#34; alt=&#34;background_widget2&#34;&gt;`&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Now &lt;code&gt;image.png&lt;/code&gt; should appear as background image in your homepage.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;google-analytics&#34;&gt;Google Analytics&lt;/h2&gt;
&lt;p&gt;In order for the website to be displayed in Google searches, you need to ask Google to track it.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Go to the &lt;a href=&#34;https://search.google.com/search-console&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Google Search Console website&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Use the &lt;a href=&#34;https://search.google.com/search-console?action=inspect&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;URL Inspection tool&lt;/a&gt; to inspect the URL of your personal website: &lt;code&gt;https://username.github.io&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Use &lt;strong&gt;Request indexing&lt;/strong&gt; to request Google to index your website so that it will apprear in Google searches.&lt;/li&gt;
&lt;li&gt;Under &lt;strong&gt;Sitemap&lt;/strong&gt; provide the link to your website sitemap to Google. It should be &lt;code&gt;https://username.github.io/sitemap.xml&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In order to receive statistics on your website, you first need to get your associated tracking code.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Go to the &lt;a href=&#34;https://www.google.com/analytics/web/#home/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Google Analytics website&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Click &lt;a href=&#34;https://support.google.com/analytics/answer/6132368&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Admin&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Select an account from the menu in the &lt;strong&gt;ACCOUNT&lt;/strong&gt; column.&lt;/li&gt;
&lt;li&gt;Select a property from the menu in the &lt;strong&gt;PROPERTY&lt;/strong&gt; column.&lt;/li&gt;
&lt;li&gt;Under &lt;strong&gt;PROPERTY&lt;/strong&gt;, click Tracking Info &amp;gt; Tracking Code.&lt;/li&gt;
&lt;li&gt;Your tracking ID and property number are displayed at the top of the page. It should have the form &lt;code&gt;UA-xxxxxxxxx-1&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Now that we have the website tracking code, we need to insert it into the &lt;code&gt;googleAnalytics&lt;/code&gt; section of the &lt;code&gt;config/_default/params.yaml&lt;/code&gt; file.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;marketing:
  google_analytics: &#39;UA-xxxxxxxxx-1&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The mobile application of &lt;a href=&#34;https://analytics.google.com/analytics/web/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Google Analytics&lt;/a&gt; is particular intuitive and allows you to monitor your website traffic in detail. You just need to link the website from the &lt;a href=&#34;https://search.google.com/search-console&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Google Sesarch Console&lt;/a&gt; and then you can motitor you website from this platform. There is also a very nice mobile app for both &lt;a href=&#34;https://play.google.com/store/apps/details?id=com.google.android.apps.giant&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Android&lt;/a&gt; and &lt;a href=&#34;https://apps.apple.com/us/app/google-analytics/id881599038&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;iOS&lt;/a&gt; to monitor your website from your smartphone.&lt;/p&gt;
&lt;p&gt;Another good free tool to analyze the “quality” of your website is &lt;a href=&#34;https://www.seomechanic.com/seo-analyzer/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;SEO Mechanic&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;p&gt;Here are the main resources I used to write this guide:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Wowchemy website: &lt;a href=&#34;https://wowchemy.com/docs/getting-started/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://wowchemy.com/docs/getting-started/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Old Academic website: &lt;a href=&#34;https://sourcethemes.com/academic/docs/install/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://sourcethemes.com/academic/docs/install/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Guide for the Terminal: &lt;a href=&#34;https://github.com/fliptanedo/FlipWebsite2017&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/fliptanedo/FlipWebsite2017&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Difference in Differences</title>
      <link>https://matteocourthoud.github.io/post/diff_in_diffs/</link>
      <pubDate>Fri, 15 Apr 2022 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/post/diff_in_diffs/</guid>
      <description>&lt;p&gt;In this tutorial, we are going to see how to estimate the causal effect of a treatment on an outcome when treatment assignment is not random, but we observe both treated and untreated units before and after treatment. Under certain structural assumptions, especially parallel outcome trends in the absence of treatment, we can recover the average treatment effect.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Requisites&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;For this tutorial, I assume you are familiar with the following concepts:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Rubin&amp;rsquo;s potential outcome framework&lt;/li&gt;
&lt;li&gt;Ordinary least squares regression&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Academic Application&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;As an academic application, we are going to replicate &lt;a href=&#34;http://sims.princeton.edu/yftp/emet04/ck/CardKruegerMinWage.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Minimum Wages and Employment: A Case Study of the Fast-Food Industry in New Jersey and Pennsylvania&lt;/a&gt; (1994) by Card and Krueger. The authors study the effect of a minimum wage policy in New Jersey on emplyment, by using Pennsylvania as a control state.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Business Case&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;As a business case, we are going to study a firm that has run a TV ad campaign. The firm would like to understand the impact of the campaign on revenue and has randomized the campaign over municipalities.&lt;/p&gt;
&lt;h2 id=&#34;setting&#34;&gt;Setting&lt;/h2&gt;
&lt;p&gt;We assume that for a set of i.i.d. subjects $i = 1, &amp;hellip;, n$ over $T$ time periods $t = 1 ,  &amp;hellip; , T$, we observed a tuple $(X_{it}, D_{it}, Y_{it})$ comprised of&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;a feature vector $X_{it} \in \mathbb R^{p}$&lt;/li&gt;
&lt;li&gt;a treatment assignment $D_i \in \lbrace 0, 1 \rbrace$&lt;/li&gt;
&lt;li&gt;a response $Y_{it} \in \mathbb R$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We assume that treatment occurs between time $t=0$ and time $t=1$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Assumption 1: parallel trends&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In the absence of treatment, the outcome $Y_{it}$ &lt;strong&gt;evolve in parallel&lt;/strong&gt; across units, i.e. and their $\gamma_{t}$ are the same.&lt;/p&gt;
&lt;p&gt;$$
Y_{it}^{(0)} - Y_{j,t}^{(0)} = \alpha \quad \forall \ t
$$&lt;/p&gt;
&lt;h2 id=&#34;diff-in-diffs&#34;&gt;Diff-in-diffs&lt;/h2&gt;
&lt;p&gt;In this setting, we cannot estimate any causal parameter with any other &lt;strong&gt;further assumption&lt;/strong&gt;. What is the minimal number of assumptions that we could make in order to estimate a causal parameter?&lt;/p&gt;
&lt;p&gt;If we were to assume that treatment was randomly assigned, we could retrieve the average treatment effect as a difference in means.&lt;/p&gt;
&lt;p&gt;$$
\mathbb E[\tau_t] = \mathbb E \big[ Y_{it} \ \big| \ D_i = 1 \big] - \mathbb E \big[ Y_{it} \big| \ D_i = 0 \big]
$$&lt;/p&gt;
&lt;p&gt;However, it would be a very strong assumption, and it would ignore some information that we possess: the time dimension (pre-post).&lt;/p&gt;
&lt;p&gt;If we were to assume instead that no other shocks affected the treated units between period $t=0$ and $t=1$, we could retrieve the average treatment effect on the treated as a pre-post difference.&lt;/p&gt;
&lt;p&gt;$$
\mathbb E[\tau | D_i=1] = \mathbb E \big[ Y_{i1} \ \big| \ D_i = 1 \big] - \mathbb E \big[ Y_{i0} \ \big| \ D_i = 1 \big]
$$&lt;/p&gt;
&lt;p&gt;However, it also this would be a very strong assumption, and it would ignore the fact that we have control units.&lt;/p&gt;
&lt;p&gt;Can we make less stringent assumption and still recover a causal parameter using both the availability of a (non-random) control group and the time dimension?&lt;/p&gt;
&lt;h3 id=&#34;did-model&#34;&gt;DiD Model&lt;/h3&gt;
&lt;p&gt;The model that is commonly assumed in diff-ind-diff settings, is the following&lt;/p&gt;
&lt;p&gt;$$
Y_{it} (D_{it}) = \alpha_{i} + \gamma_{t} + \tau_{i} D_{it}
$$&lt;/p&gt;
&lt;p&gt;First, let&amp;rsquo;s summarize the potential outcome values $Y^{(d)}_{it}$ in the simple $2 \times 2$ setting.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;$t=0$&lt;/th&gt;
&lt;th&gt;$t=1$&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;$D=0$&lt;/td&gt;
&lt;td&gt;$\gamma_0 + \alpha_i$&lt;/td&gt;
&lt;td&gt;$\gamma_1 + \alpha_i$&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;$D=1$&lt;/td&gt;
&lt;td&gt;$\gamma_0 + \alpha_i + \tau_i$&lt;/td&gt;
&lt;td&gt;$\gamma_1 + \alpha_i + \tau_i$&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;For a single unit, $i$, the pre-post outcome difference is given by&lt;/p&gt;
&lt;p&gt;$$
Y_{i1} - Y_{i0} = (\gamma_1 - \gamma_0) + \tau_i (D_{i1} - D_{i0})
$$&lt;/p&gt;
&lt;p&gt;If we take the difference of the expression above between treated and untreated units, we get&lt;/p&gt;
&lt;p&gt;$$
\mathbb E \Big[ Y_{i1} - Y_{i0} \ \Big| \ D_{i1} - D_{i0} = 1 \Big] - \mathbb E \Big[ Y_{i1} - Y_{i0} \ \Big| \ D_{i1} - D_{i0} = 0 \Big] = \mathbb E \Big[ \tau_i \ \Big| \ D_{i1} - D_{i0} = 1 \Big] = ATT
$$&lt;/p&gt;
&lt;p&gt;which is the average treatment effect on the treated (ATT).&lt;/p&gt;
&lt;p&gt;We can get this double difference with the folowing regressio model&lt;/p&gt;
&lt;p&gt;$$
Y_{it} (D_{it}) = \alpha_{i} + \gamma_{t} + \beta D_{it} + \varepsilon_{it}
$$&lt;/p&gt;
&lt;p&gt;where the OLS estimator $\hat \beta$ will be unbiased for the ATT.&lt;/p&gt;
&lt;h3 id=&#34;multiple-time-periods&#34;&gt;Multiple Time Periods&lt;/h3&gt;
&lt;p&gt;What if we didn&amp;rsquo;t just have one pre-treatment period and one post-treatment period? Great! We can actually do more things.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;We can partially test assumptions&lt;/li&gt;
&lt;li&gt;We can estimate dynamic effects&lt;/li&gt;
&lt;li&gt;We can run placebo tests&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;How do we implement it? Run a regression with multiple interactions&lt;/p&gt;
&lt;p&gt;$$
Y_{it} (D_{it}) = \alpha_{i} + \gamma_{t} + \sum_{t=1}^{T} \beta_t D_{it} + \varepsilon_{it}
$$&lt;/p&gt;
&lt;h3 id=&#34;comments&#34;&gt;Comments&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Parametric Assumption&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The diff-in-diffs method makes a lot of parametric assumptions that are is easy to forget.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Inference&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Bertrand, Duflo, and Mullainathan (2004) point out that conventional robust standard errors usually overestimate the actual standard deviation of the estimator. The authors recommend &lt;strong&gt;clustering&lt;/strong&gt; the standard errors at the level of randomization (e.g. classes, counties, villages, &amp;hellip;).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Testing pre-trends&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Having multiple pre-treatment time periods is helpful for testing the parallel trends assumption. However, this practice can lead to pre-testing bias. In particular, if one selects results based on a pre-treatment parallel trend test, inference on the ATT gets distorderd.&lt;/p&gt;
&lt;h2 id=&#34;academic-application&#34;&gt;Academic Application&lt;/h2&gt;
&lt;p&gt;As an academic application, we are going to replicate &lt;a href=&#34;http://sims.princeton.edu/yftp/emet04/ck/CardKruegerMinWage.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Minimum Wages and Employment: A Case Study of the Fast-Food Industry in New Jersey and Pennsylvania&lt;/a&gt; (1994) by Card and Krueger. The authors study the effect of a minimum wage policy in New Jersey on emplyment, by using Pennsylvania as a control state.&lt;/p&gt;
&lt;p&gt;The authors describe the setting as follows&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;On April 1, 1992, New Jersey&amp;rsquo;s minimum wage rose from $4.25 to $5.05 per hour. To evaluate the impact of the law, the authors surveyed 410 fast-food restaurants in New Jersey and eastern Pennsylvania before and after the rise. Comparisons of employment growth at stores in New Jersey and Pennsylvania (where the minimum wage was constant) provide simple estimates of the effect of the higher minimum wage.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Let&amp;rsquo;s start by loading and inspecting the data.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;%matplotlib inline
%config InlineBackend.figure_format = &#39;retina&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from src.utils import *
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from src.data import import_ck94

df = pd.read_csv(&#39;data/ck94.csv&#39;)
df.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;id&lt;/th&gt;
      &lt;th&gt;after&lt;/th&gt;
      &lt;th&gt;new_jersey&lt;/th&gt;
      &lt;th&gt;chain&lt;/th&gt;
      &lt;th&gt;employment&lt;/th&gt;
      &lt;th&gt;hrsopen&lt;/th&gt;
      &lt;th&gt;wage&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;wendys&lt;/td&gt;
      &lt;td&gt;34.0&lt;/td&gt;
      &lt;td&gt;12.0&lt;/td&gt;
      &lt;td&gt;5.00&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;wendys&lt;/td&gt;
      &lt;td&gt;24.0&lt;/td&gt;
      &lt;td&gt;12.0&lt;/td&gt;
      &lt;td&gt;5.50&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;6&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;burgerking&lt;/td&gt;
      &lt;td&gt;70.5&lt;/td&gt;
      &lt;td&gt;18.0&lt;/td&gt;
      &lt;td&gt;5.00&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;7&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;burgerking&lt;/td&gt;
      &lt;td&gt;23.5&lt;/td&gt;
      &lt;td&gt;24.0&lt;/td&gt;
      &lt;td&gt;5.00&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;8&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;kfc&lt;/td&gt;
      &lt;td&gt;11.0&lt;/td&gt;
      &lt;td&gt;10.0&lt;/td&gt;
      &lt;td&gt;5.25&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;We have information on fast food restaurants, indexed by &lt;code&gt;i&lt;/code&gt;, at time &lt;code&gt;t&lt;/code&gt;. We distinguish between before and faster treatment and between New Jersey &lt;code&gt;nj&lt;/code&gt; and Pennsylvania restaurants. We also know the &lt;code&gt;chain&lt;/code&gt; of the restaurant, the &lt;code&gt;employment&lt;/code&gt;, the hours open &lt;code&gt;hrsopen&lt;/code&gt; and the &lt;code&gt;wage&lt;/code&gt;. We are interested on the effect on the policy on wages.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s start by producing the $2 \times 2$ table of treatment-control before-after average outcomes.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df.pivot_table(index=&#39;new_jersey&#39;, columns=&#39;after&#39;, values=&#39;employment&#39;, aggfunc=&#39;mean&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;after&lt;/th&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;th&gt;1&lt;/th&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;new_jersey&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;23.704545&lt;/td&gt;
      &lt;td&gt;21.825758&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;20.657746&lt;/td&gt;
      &lt;td&gt;21.048415&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;From the table we can see that a simple &lt;strong&gt;before-after comparison&lt;/strong&gt; would give a small posive effect of $21.05 - 20.66 = 0.39$.&lt;/p&gt;
&lt;p&gt;On the other hand, if one was doing an ex-post &lt;strong&gt;treated-control comparison&lt;/strong&gt;, would get a negative effect of $21.05 - 21.83 = - 0.78$.&lt;/p&gt;
&lt;p&gt;The difference-in-differences estimator takes into account the fact that&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;There is a pre-treatment level difference between New Jersey and Pennsylvania&lt;/li&gt;
&lt;li&gt;Employment was falling in Pennsylvania even without treatment&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The &lt;strong&gt;double difference&lt;/strong&gt; in means gives a positive effect, significantly larger than any of the two previous estimates.&lt;/p&gt;
&lt;p&gt;$$
\hat \tau_{DiD} = \Big( 21.05 - 20.66 \Big) - \Big( 21.83 - 23.70 \Big) = 0.39 + 1.87 = 2.26
$$&lt;/p&gt;
&lt;p&gt;We can replicate the result with a linear regression.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;smf.ols(&#39;employment ~ new_jersey * after&#39;, df).fit().summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
          &lt;td&gt;&lt;/td&gt;            &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt;        &lt;td&gt;   23.7045&lt;/td&gt; &lt;td&gt;    1.149&lt;/td&gt; &lt;td&gt;   20.627&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;   21.448&lt;/td&gt; &lt;td&gt;   25.961&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;new_jersey&lt;/th&gt;       &lt;td&gt;   -3.0468&lt;/td&gt; &lt;td&gt;    1.276&lt;/td&gt; &lt;td&gt;   -2.388&lt;/td&gt; &lt;td&gt; 0.017&lt;/td&gt; &lt;td&gt;   -5.552&lt;/td&gt; &lt;td&gt;   -0.542&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;after&lt;/th&gt;            &lt;td&gt;   -1.8788&lt;/td&gt; &lt;td&gt;    1.625&lt;/td&gt; &lt;td&gt;   -1.156&lt;/td&gt; &lt;td&gt; 0.248&lt;/td&gt; &lt;td&gt;   -5.070&lt;/td&gt; &lt;td&gt;    1.312&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;new_jersey:after&lt;/th&gt; &lt;td&gt;    2.2695&lt;/td&gt; &lt;td&gt;    1.804&lt;/td&gt; &lt;td&gt;    1.258&lt;/td&gt; &lt;td&gt; 0.209&lt;/td&gt; &lt;td&gt;   -1.273&lt;/td&gt; &lt;td&gt;    5.812&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;The effect is $2.26$, but it is not significantly different from zero.&lt;/p&gt;
&lt;h2 id=&#34;business-case&#34;&gt;Business Case&lt;/h2&gt;
&lt;p&gt;We are given the following problem:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;A firm wants to test the impact of a TV advertisement campaign on revenue. The firm releases the ad on a random sample of municipalities and track the revenue over time, before and after the ad campaign.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;We start by drawing a sample from the data generating process.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from src.dgp import dgp_did

dgp = dgp_did()
df = dgp.generate_data()
df.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;day&lt;/th&gt;
      &lt;th&gt;id&lt;/th&gt;
      &lt;th&gt;treated&lt;/th&gt;
      &lt;th&gt;post&lt;/th&gt;
      &lt;th&gt;revenue&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;3.599341&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;-0.146912&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;0.696527&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;1.445169&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;1.659696&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;We have &lt;code&gt;revenue&lt;/code&gt; data on a set of customers over time. We also know to which &lt;code&gt;group&lt;/code&gt; they were assigned and whether the time is before or after the intervention.&lt;/p&gt;
&lt;p&gt;Since we do not have any control variable, we can directly visualize the revenue dynamics, distinguishing between treatment and control group.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sns.lineplot(x=df[&#39;day&#39;], y=df[&#39;revenue&#39;], hue=df[&#39;treated&#39;]);
plt.axvline(x=10, ls=&amp;quot;:&amp;quot;, color=&#39;C2&#39;);
plt.title(&#39;Revenue over time&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/diff_in_diffs_26_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;It seems like the treatment group was producing higher revenues before treatment and the gap has increased with treatment but it is closing over time.&lt;/p&gt;
&lt;p&gt;To assess the magnitude of the effect and perform inference, we can regress revenue on a post-treatment dummy, a treatment dummy and their interaction.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;smf.ols(&#39;revenue ~ post * treated&#39;, df).fit().summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
            &lt;td&gt;&lt;/td&gt;              &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt;            &lt;td&gt;    4.6357&lt;/td&gt; &lt;td&gt;    0.078&lt;/td&gt; &lt;td&gt;   59.428&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    4.483&lt;/td&gt; &lt;td&gt;    4.789&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;post[T.True]&lt;/th&gt;         &lt;td&gt;    0.8928&lt;/td&gt; &lt;td&gt;    0.110&lt;/td&gt; &lt;td&gt;    8.093&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    0.676&lt;/td&gt; &lt;td&gt;    1.109&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;treated&lt;/th&gt;              &lt;td&gt;    1.0558&lt;/td&gt; &lt;td&gt;    0.110&lt;/td&gt; &lt;td&gt;    9.571&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    0.839&lt;/td&gt; &lt;td&gt;    1.272&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;post[T.True]:treated&lt;/th&gt; &lt;td&gt;    0.1095&lt;/td&gt; &lt;td&gt;    0.156&lt;/td&gt; &lt;td&gt;    0.702&lt;/td&gt; &lt;td&gt; 0.483&lt;/td&gt; &lt;td&gt;   -0.196&lt;/td&gt; &lt;td&gt;    0.415&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;While the coefficient for the interaction term is positive, it does not seem to be statistically significant. However, this might be due to the fact that the treatment effect is fading away over time.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s fit the same regression, with a linear time trend.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;smf.ols(&#39;revenue ~ post * treated * day&#39;, df).fit().summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
              &lt;td&gt;&lt;/td&gt;                &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt;                &lt;td&gt;    4.1144&lt;/td&gt; &lt;td&gt;    0.168&lt;/td&gt; &lt;td&gt;   24.501&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    3.785&lt;/td&gt; &lt;td&gt;    4.444&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;post[T.True]&lt;/th&gt;             &lt;td&gt;    0.2871&lt;/td&gt; &lt;td&gt;    0.458&lt;/td&gt; &lt;td&gt;    0.626&lt;/td&gt; &lt;td&gt; 0.531&lt;/td&gt; &lt;td&gt;   -0.612&lt;/td&gt; &lt;td&gt;    1.186&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;treated&lt;/th&gt;                  &lt;td&gt;    1.0788&lt;/td&gt; &lt;td&gt;    0.237&lt;/td&gt; &lt;td&gt;    4.543&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    0.613&lt;/td&gt; &lt;td&gt;    1.544&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;post[T.True]:treated&lt;/th&gt;     &lt;td&gt;    1.5910&lt;/td&gt; &lt;td&gt;    0.648&lt;/td&gt; &lt;td&gt;    2.454&lt;/td&gt; &lt;td&gt; 0.014&lt;/td&gt; &lt;td&gt;    0.320&lt;/td&gt; &lt;td&gt;    2.862&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;day&lt;/th&gt;                      &lt;td&gt;    0.0948&lt;/td&gt; &lt;td&gt;    0.027&lt;/td&gt; &lt;td&gt;    3.502&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    0.042&lt;/td&gt; &lt;td&gt;    0.148&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;post[T.True]:day&lt;/th&gt;         &lt;td&gt;   -0.0221&lt;/td&gt; &lt;td&gt;    0.038&lt;/td&gt; &lt;td&gt;   -0.576&lt;/td&gt; &lt;td&gt; 0.564&lt;/td&gt; &lt;td&gt;   -0.097&lt;/td&gt; &lt;td&gt;    0.053&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;treated:day&lt;/th&gt;              &lt;td&gt;   -0.0042&lt;/td&gt; &lt;td&gt;    0.038&lt;/td&gt; &lt;td&gt;   -0.109&lt;/td&gt; &lt;td&gt; 0.913&lt;/td&gt; &lt;td&gt;   -0.079&lt;/td&gt; &lt;td&gt;    0.071&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;post[T.True]:treated:day&lt;/th&gt; &lt;td&gt;   -0.0929&lt;/td&gt; &lt;td&gt;    0.054&lt;/td&gt; &lt;td&gt;   -1.716&lt;/td&gt; &lt;td&gt; 0.086&lt;/td&gt; &lt;td&gt;   -0.199&lt;/td&gt; &lt;td&gt;    0.013&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;Now the treatment effect is positive and significant at the 5% level. And indeed, we estimate a decreasing trend, post treatment, for the treated. However, it is not statistically significant.&lt;/p&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=mbYTZ0w-QTw&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Video lecture on Difference-in-Differences&lt;/a&gt; by Paul Goldsmith-Pinkham (Yale)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://matheusfacure.github.io/python-causality-handbook/13-Difference-in-Differences.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Chapter 13&lt;/a&gt; of Causal Inference for The Brave and The True by Matheus Facure&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://mixtape.scunning.com/difference-in-differences.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Chapter 9&lt;/a&gt; of The Causal Inference Mixtape by Scott Cunningham&lt;/li&gt;
&lt;li&gt;Chapter 5 of &lt;a href=&#34;https://www.mostlyharmlesseconometrics.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Mostly Harmless Econometrics&lt;/a&gt; by Agrist and Pischke&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Instrumental Variables</title>
      <link>https://matteocourthoud.github.io/post/iv/</link>
      <pubDate>Fri, 15 Apr 2022 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/post/iv/</guid>
      <description>&lt;p&gt;In this tutorial, we are going to see how to estimate causal effects when the treatment is not randomly assigned, but we have access to a third variable that is as good as randomly assigned and is correlated (only) with the treatment. These variables are called instrumental variables and are a powerful tool for causal inference, especially in observational studies.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Requisites&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;For this tutorial, I assume you are familiar with the following concepts:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Rubin&amp;rsquo;s potential outcome framework&lt;/li&gt;
&lt;li&gt;Ordinary least squares regression&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Academic Application 1&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;As a first academic application, we are going to replicate &lt;a href=&#34;https://www.jstor.org/stable/2937954&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Does Compulsory School Attendance Affect Schooling and Earnings?&lt;/a&gt; (1991) by Angrist and Krueger. The authors study the effect of education on wages.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Academic Application 2&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;As a further academic application, we are going to replicate &lt;a href=&#34;https://economics.mit.edu/files/4123&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The Colonial Origins of Comparative Development&lt;/a&gt; (2002) by Acemoglu, Johnson, Robinson. The authors study the effect of institutions on economic development.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Business Case&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;As a business case, we are going to study a company that wants to find out whether subscribing to its newsletter has an effect on revenues. Since the travel agency cannot force customers to subscribing to the newsletter, it randomly sends reminder emails to infer the effect of the newsletter on revenues.&lt;/p&gt;
&lt;h2 id=&#34;setting&#34;&gt;Setting&lt;/h2&gt;
&lt;p&gt;We assume that for a set of i.i.d. subjects $i = 1, &amp;hellip;, n$ we observed a tuple $(X_i, T_i, Y_i)$ comprised of&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;a feature vector $X_i \in \mathbb R^n$&lt;/li&gt;
&lt;li&gt;a treatment variable $T_i \in \lbrace 0, 1 \rbrace$&lt;/li&gt;
&lt;li&gt;a response $Y_i \in \mathbb R$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Crucially, we do not assume unconfoundedness / strong ignorability hence&lt;/p&gt;
&lt;p&gt;$$
\big \lbrace Y_i^{(1)} , Y_i^{(0)} \big \rbrace \ \not \perp \ T_i \ | \ X_i
$$&lt;/p&gt;
&lt;h2 id=&#34;instrumental-variables&#34;&gt;Instrumental Variables&lt;/h2&gt;
&lt;p&gt;The &lt;strong&gt;standard linear IV model&lt;/strong&gt; is the following&lt;/p&gt;
&lt;p&gt;$$
Y_i = T_i \alpha + X_i \beta_1 + \varepsilon_i
\newline
T_i = Z_i \gamma + X_i \beta_2 + u_i
$$&lt;/p&gt;
&lt;p&gt;We assume there exists an &lt;strong&gt;instrumental variable&lt;/strong&gt; $Z_i \in \mathbb R^k$ that satisfies the following assumptions.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Assumption 1: Exclusion&lt;/strong&gt;: $\mathbb E [Z \varepsilon] = 0$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Assumption 2: Relevance&lt;/strong&gt;: $\mathbb E [Z T] \neq 0$&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The model can be represented by a DAG.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from src.plots import dag_iv
dag_iv()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/iv_6_0.svg&#34; alt=&#34;svg&#34;&gt;&lt;/p&gt;
&lt;p&gt;The IV estimator is instead unbiased&lt;/p&gt;
&lt;p&gt;$$
\hat \beta_{IV} = (Z&amp;rsquo;X)^{-1}(Z&amp;rsquo;Y)
$$&lt;/p&gt;
&lt;h3 id=&#34;potential-outcomes-perspective&#34;&gt;Potential Outcomes Perspective&lt;/h3&gt;
&lt;p&gt;We need to extend the potential outcomes framework in order to allow for the instrumental variable $Z$. First we define the potential outcomes as $Y^{(D(Z_i))}(Z_i)$&lt;/p&gt;
&lt;p&gt;The assumptions become&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Exclusion: $Y^{(D(Z_i))}(Z_i) = Y^{(T(Z_i))}$&lt;/li&gt;
&lt;li&gt;Relevance: $P(z) = \mathbb E [T, Z=z]$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We assume that $Z$ is fully randomly assigned (while $T$ is not).&lt;/p&gt;
&lt;p&gt;What does IV estimate?&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
\mathbb E[Y_i | Z_i = 1] - \mathbb E[Y_i | Z_i = 0] &amp;amp;= \Pr (D_i^{(1)} - D_i^{(0)} = 1) \times \mathbb E \Big[ Y_i^{(1)} - Y_i^{(0)} = 1 \ \Big | \ D_i^{(1)} - D_i^{(0)} = 1 \Big] -
\newline
&amp;amp;- \Pr (D_i^{(1)} - D_i^{(0)} = -1) \times \mathbb E \Big[ Y_i^{(1)} - Y_i^{(0)} = 1 \ \Big | \ D_i^{(1)} - D_i^{(0)} = -1 \Big]
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;Is this a quantity of interest? Almost. There are &lt;strong&gt;two issues&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;First, the first term is the treatment effect, but only for those individuals for whom $D_i^{(1)} - D_i^{(0)} = 1$, i.e. those that are induced into treatment by $Z_i$. These individuals are referred to as &lt;strong&gt;compliers&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Second, the second term is problematic since it removes from the first effect, the local effect of another subpopulation: $D_i^{(1)} - D_i^{(0)} = -1$, i.e. those that are induced out of treatment by $Z_i$. These individuals are referred to as &lt;strong&gt;defiers&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;We can get rid of defiers with a simple assumption.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Assumption 3: monotonocity&lt;/strong&gt;: $D_i^{(1)} \geq D_i^{(0)}$ (or viceversa)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;All effects must be monotone in the same direction&lt;/li&gt;
&lt;li&gt;Fundamentally untestable&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Then, the IV estimator can be expressed as a ration between two differences in means&lt;/p&gt;
&lt;p&gt;$$
\hat \beta_{IV} = \frac{\mathbb E[Y_i | Z_i = 1] - \mathbb E[Y_i | Z_i = 0]}{\mathbb E[T_i | Z_i = 1] - \mathbb E[T_i | Z_i = 0]}
$$&lt;/p&gt;
&lt;h3 id=&#34;structural-perspective&#34;&gt;Structural Perspective&lt;/h3&gt;
&lt;p&gt;One can interpret the IV estimator as a GMM estimator that uses the exclusion restriction as estimating equation.&lt;/p&gt;
&lt;p&gt;$$
\hat \beta_{GMM} = \arg \min_{\beta} \mathbb E \Big[ Z (Y - \alpha T - \beta X) \Big]^2
$$&lt;/p&gt;
&lt;h2 id=&#34;the-algebra-of-iv&#34;&gt;The Algebra of IV&lt;/h2&gt;
&lt;p&gt;TBD&lt;/p&gt;
&lt;h2 id=&#34;demand-and-supply&#34;&gt;Demand and Supply&lt;/h2&gt;
&lt;p&gt;TBD&lt;/p&gt;
&lt;h2 id=&#34;academic-application-1&#34;&gt;Academic Application 1&lt;/h2&gt;
&lt;p&gt;As an research paper replication, we are going to replicate &lt;a href=&#34;https://www.jstor.org/stable/2937954&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Does compulsory school attendance affect schooling and earnings?&lt;/a&gt; (1991) by Angrist and Krueger. The authors study the effect of education on wages.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;problem&lt;/strong&gt; of studying the relationship of education on wages is that there might be factors that influence both education and wages but we do not observe, for example ability. Students that have higher ability might decide to stay longer in school and also get higher wages afterwards.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;idea&lt;/strong&gt; of the authors is to use the quarter of birth as an instrument for education. In fact, quarter of birth is plausibly exogenous with respect to wages while, on the other hand, is correlated with education. Why? Students that are both in the last quarter of the year cannot drop out as early as other students and therefore are exposed to more eduction.&lt;/p&gt;
&lt;p&gt;We can represent the DAG of their model as follows.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;%matplotlib inline
%config InlineBackend.figure_format = &#39;retina&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from src.utils import *
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;dag_iv(Y=&amp;quot;wage&amp;quot;, T=&amp;quot;education&amp;quot;, Z=&amp;quot;quarter of birth&amp;quot;, U=&amp;quot;ability&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/iv_20_0.svg&#34; alt=&#34;svg&#34;&gt;&lt;/p&gt;
&lt;p&gt;A shortcoming of this instrument comes out of the fact that the population of &lt;strong&gt;compliers&lt;/strong&gt; is students that drop out of school as soon as possible, we will know the treatment effect only for this population. It&amp;rsquo;s important to keep this in mind when interpreting the results.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s load the data, freely available &lt;a href=&#34;https://economics.mit.edu/faculty/angrist/data1/data/angkru1991&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df = pd.read_csv(&#39;data/ak91.csv&#39;)
df.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;log_wage&lt;/th&gt;
      &lt;th&gt;years_of_schooling&lt;/th&gt;
      &lt;th&gt;date_of_birth&lt;/th&gt;
      &lt;th&gt;year_of_birth&lt;/th&gt;
      &lt;th&gt;quarter_of_birth&lt;/th&gt;
      &lt;th&gt;state_of_birth&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;5.790019&lt;/td&gt;
      &lt;td&gt;12.0&lt;/td&gt;
      &lt;td&gt;1930.0&lt;/td&gt;
      &lt;td&gt;1930.0&lt;/td&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;45.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;5.952494&lt;/td&gt;
      &lt;td&gt;11.0&lt;/td&gt;
      &lt;td&gt;1930.0&lt;/td&gt;
      &lt;td&gt;1930.0&lt;/td&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;45.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;5.315949&lt;/td&gt;
      &lt;td&gt;12.0&lt;/td&gt;
      &lt;td&gt;1930.0&lt;/td&gt;
      &lt;td&gt;1930.0&lt;/td&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;45.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;5.595926&lt;/td&gt;
      &lt;td&gt;12.0&lt;/td&gt;
      &lt;td&gt;1930.0&lt;/td&gt;
      &lt;td&gt;1930.0&lt;/td&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;45.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;6.068915&lt;/td&gt;
      &lt;td&gt;12.0&lt;/td&gt;
      &lt;td&gt;1930.0&lt;/td&gt;
      &lt;td&gt;1930.0&lt;/td&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;37.0&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;We have the variables of interest, &lt;code&gt;log_wage&lt;/code&gt;, &lt;code&gt;years_of_schooling&lt;/code&gt; and &lt;code&gt;quarter_of_birth&lt;/code&gt;, together with a set of controls.&lt;/p&gt;
&lt;h3 id=&#34;ols&#34;&gt;OLS&lt;/h3&gt;
&lt;p&gt;If we were to ignore the endogeneity problem we would estimate a linear regression of &lt;code&gt;log_wage&lt;/code&gt; on &lt;code&gt;years_of_schooling&lt;/code&gt;, plus control dummy variables for the &lt;code&gt;state_of_birth&lt;/code&gt; and &lt;code&gt;year_of_birth&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;smf.ols(&#39;log_wage ~ years_of_schooling&#39;, df).fit().summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
           &lt;td&gt;&lt;/td&gt;             &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt;          &lt;td&gt;    4.9952&lt;/td&gt; &lt;td&gt;    0.004&lt;/td&gt; &lt;td&gt; 1118.882&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    4.986&lt;/td&gt; &lt;td&gt;    5.004&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;years_of_schooling&lt;/th&gt; &lt;td&gt;    0.0709&lt;/td&gt; &lt;td&gt;    0.000&lt;/td&gt; &lt;td&gt;  209.243&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    0.070&lt;/td&gt; &lt;td&gt;    0.072&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;h3 id=&#34;iv&#34;&gt;IV&lt;/h3&gt;
&lt;p&gt;We now use &lt;code&gt;quarter_of_birth&lt;/code&gt; as an instrument for &lt;code&gt;years_of_schooling&lt;/code&gt;. We cannot check the exclusion restriction condition, but we can check the &lt;strong&gt;relevance&lt;/strong&gt; condition.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s start first by plotting average &lt;code&gt;years_of_schooling&lt;/code&gt; by date of birth.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;group_df = df.groupby(&amp;quot;date_of_birth&amp;quot;).mean().reset_index()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plt.figure(figsize=(15,6))
sns.lineplot(data=group_df, x=&amp;quot;date_of_birth&amp;quot;, y=&amp;quot;years_of_schooling&amp;quot;, zorder=1)\
.set(title=&amp;quot;First Stage&amp;quot;, xlabel=&amp;quot;Year of Birth&amp;quot;, ylabel=&amp;quot;Years of Schooling&amp;quot;);

for q in range(1, 5):
    x = group_df.loc[group_df[&#39;quarter_of_birth&#39;]==q, &amp;quot;date_of_birth&amp;quot;]
    y = group_df.loc[group_df[&#39;quarter_of_birth&#39;]==q, &amp;quot;years_of_schooling&amp;quot;]
    plt.scatter(x, y, marker=&amp;quot;s&amp;quot;, s=200, c=f&amp;quot;C{q}&amp;quot;)
    plt.scatter(x, y, marker=f&amp;quot;${q}$&amp;quot;, s=100, c=f&amp;quot;white&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/iv_31_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;As we can see, there is an upward trend but, within each year, people both in the last quarter usually have more years of schooling than people born in other quarters of the year.&lt;/p&gt;
&lt;p&gt;We can check this correlation more formally by regressing &lt;code&gt;years_of_schooling&lt;/code&gt; of a set of dummies for &lt;code&gt;quarter_of_birth&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;smf.ols(&#39;years_of_schooling ~ C(quarter_of_birth)&#39;, df).fit().summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
               &lt;td&gt;&lt;/td&gt;                 &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt;                  &lt;td&gt;   12.6881&lt;/td&gt; &lt;td&gt;    0.011&lt;/td&gt; &lt;td&gt; 1105.239&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;   12.666&lt;/td&gt; &lt;td&gt;   12.711&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;C(quarter_of_birth)[T.2.0]&lt;/th&gt; &lt;td&gt;    0.0566&lt;/td&gt; &lt;td&gt;    0.016&lt;/td&gt; &lt;td&gt;    3.473&lt;/td&gt; &lt;td&gt; 0.001&lt;/td&gt; &lt;td&gt;    0.025&lt;/td&gt; &lt;td&gt;    0.089&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;C(quarter_of_birth)[T.3.0]&lt;/th&gt; &lt;td&gt;    0.1173&lt;/td&gt; &lt;td&gt;    0.016&lt;/td&gt; &lt;td&gt;    7.338&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    0.086&lt;/td&gt; &lt;td&gt;    0.149&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;C(quarter_of_birth)[T.4.0]&lt;/th&gt; &lt;td&gt;    0.1514&lt;/td&gt; &lt;td&gt;    0.016&lt;/td&gt; &lt;td&gt;    9.300&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    0.119&lt;/td&gt; &lt;td&gt;    0.183&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;The relationship between &lt;code&gt;years_of_schooling&lt;/code&gt; and &lt;code&gt;quarter_of_birth&lt;/code&gt; is indeed statistically significant.&lt;/p&gt;
&lt;p&gt;Does it translate it into higher wages? We can have a first glimpse of potential IV effects by plotting wages against the date of birth.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plt.figure(figsize=(15,6))
sns.lineplot(data=group_df, x=&amp;quot;date_of_birth&amp;quot;, y=&amp;quot;log_wage&amp;quot;, zorder=1)\
.set(title=&amp;quot;Reduced Form&amp;quot;, xlabel=&amp;quot;Year of Birth&amp;quot;, ylabel=&amp;quot;Log Wage&amp;quot;);

for q in range(1, 5):
    x = group_df.loc[group_df[&#39;quarter_of_birth&#39;]==q, &amp;quot;date_of_birth&amp;quot;]
    y = group_df.loc[group_df[&#39;quarter_of_birth&#39;]==q, &amp;quot;log_wage&amp;quot;]
    plt.scatter(x, y, marker=&amp;quot;s&amp;quot;, s=200, c=f&amp;quot;C{q}&amp;quot;)
    plt.scatter(x, y, marker=f&amp;quot;${q}$&amp;quot;, s=100, c=f&amp;quot;white&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/iv_35_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;It seems that indeed people both in later quarters earn higher wages later in life.&lt;/p&gt;
&lt;p&gt;We now turn into the estimation of the causal effect of education on wages.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df[[&#39;q1&#39;, &#39;q2&#39;, &#39;q3&#39;, &#39;q4&#39;]] = pd.get_dummies(df[&#39;quarter_of_birth&#39;])
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from linearmodels.iv import IV2SLS

IV2SLS.from_formula(&#39;log_wage ~ 1 + [years_of_schooling ~ q1 + q2 + q3]&#39;, data=df).fit().summary.tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;caption&gt;Parameter Estimates&lt;/caption&gt;
&lt;tr&gt;
           &lt;td&gt;&lt;/td&gt;          &lt;th&gt;Parameter&lt;/th&gt; &lt;th&gt;Std. Err.&lt;/th&gt; &lt;th&gt;T-stat&lt;/th&gt; &lt;th&gt;P-value&lt;/th&gt; &lt;th&gt;Lower CI&lt;/th&gt; &lt;th&gt;Upper CI&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt;           &lt;td&gt;4.5898&lt;/td&gt;    &lt;td&gt;0.2494&lt;/td&gt;   &lt;td&gt;18.404&lt;/td&gt; &lt;td&gt;0.0000&lt;/td&gt;   &lt;td&gt;4.1010&lt;/td&gt;   &lt;td&gt;5.0786&lt;/td&gt; 
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;years_of_schooling&lt;/th&gt;  &lt;td&gt;0.1026&lt;/td&gt;    &lt;td&gt;0.0195&lt;/td&gt;   &lt;td&gt;5.2539&lt;/td&gt; &lt;td&gt;0.0000&lt;/td&gt;   &lt;td&gt;0.0643&lt;/td&gt;   &lt;td&gt;0.1409&lt;/td&gt; 
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;The coefficient is slightly higher than the OLS coefficient. It&amp;rsquo;s important to remember that the estimated effect is specific to the subpopulation of people that drop out of school as soon as they can.&lt;/p&gt;
&lt;h2 id=&#34;research-paper-replication-2&#34;&gt;Research Paper Replication 2&lt;/h2&gt;
&lt;p&gt;In &lt;a href=&#34;https://economics.mit.edu/files/4123&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The Colonial Origins of Comparative Development&lt;/a&gt; (2002) by Acemoglu, Johnson, Robinson, the authors wish to determine whether or not differences in institutions can help to explain observed economic outcomes.&lt;/p&gt;
&lt;p&gt;How do we measure &lt;em&gt;institutional differences&lt;/em&gt; and &lt;em&gt;economic outcomes&lt;/em&gt;?&lt;/p&gt;
&lt;p&gt;In this paper,&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;economic outcomes are proxied by log GDP per capita in 1995, adjusted for exchange rates.&lt;/li&gt;
&lt;li&gt;institutional differences are proxied by an index of protection against expropriation on average over 1985-95, constructed by the &lt;a href=&#34;https://www.prsgroup.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Political Risk Services Group&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The &lt;strong&gt;problem&lt;/strong&gt; is that there might exist other factors that affects both the quality of institutions and GDP. The authors suggest the following problems as sources of endogeneity:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;richer countries may be able to afford or prefer better institutions&lt;/li&gt;
&lt;li&gt;variables that affect income may also be correlated with institutional differences&lt;/li&gt;
&lt;li&gt;the construction of the index may be biased; analysts may be biased towards seeing countries with higher income having better institutions&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The &lt;strong&gt;idea&lt;/strong&gt; of the authors is to use settler&amp;rsquo;s mortality during the colonization period as an instrument for the quality of institutions. They hypothesize that higher mortality rates of colonizers led to the establishment of institutions that were more extractive in nature (less protection against expropriation), and these institutions still persist today.&lt;/p&gt;
&lt;p&gt;We can represent their DAG as follows.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;dag_iv(Y=&amp;quot;GDP&amp;quot;, T=&amp;quot;institutions&amp;quot;, Z=&amp;quot;settlers&#39; mortality&amp;quot;, U=&amp;quot;tons of stuff&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/iv_42_0.svg&#34; alt=&#34;svg&#34;&gt;&lt;/p&gt;
&lt;p&gt;First, let&amp;rsquo;s load the data (available &lt;a href=&#34;https://economics.mit.edu/faculty/acemoglu/data/ajr2001&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;) and have a look at it.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df = pd.read_csv(&#39;data/ajr02.csv&#39;,index_col=0)
df.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;GDP&lt;/th&gt;
      &lt;th&gt;Exprop&lt;/th&gt;
      &lt;th&gt;Mort&lt;/th&gt;
      &lt;th&gt;Latitude&lt;/th&gt;
      &lt;th&gt;Neo&lt;/th&gt;
      &lt;th&gt;Africa&lt;/th&gt;
      &lt;th&gt;Asia&lt;/th&gt;
      &lt;th&gt;Namer&lt;/th&gt;
      &lt;th&gt;Samer&lt;/th&gt;
      &lt;th&gt;logMort&lt;/th&gt;
      &lt;th&gt;Latitude2&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;8.39&lt;/td&gt;
      &lt;td&gt;6.50&lt;/td&gt;
      &lt;td&gt;78.20&lt;/td&gt;
      &lt;td&gt;0.3111&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;4.359270&lt;/td&gt;
      &lt;td&gt;0.096783&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;7.77&lt;/td&gt;
      &lt;td&gt;5.36&lt;/td&gt;
      &lt;td&gt;280.00&lt;/td&gt;
      &lt;td&gt;0.1367&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;5.634790&lt;/td&gt;
      &lt;td&gt;0.018687&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;9.13&lt;/td&gt;
      &lt;td&gt;6.39&lt;/td&gt;
      &lt;td&gt;68.90&lt;/td&gt;
      &lt;td&gt;0.3778&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;4.232656&lt;/td&gt;
      &lt;td&gt;0.142733&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;9.90&lt;/td&gt;
      &lt;td&gt;9.32&lt;/td&gt;
      &lt;td&gt;8.55&lt;/td&gt;
      &lt;td&gt;0.3000&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;2.145931&lt;/td&gt;
      &lt;td&gt;0.090000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;5&lt;/th&gt;
      &lt;td&gt;9.29&lt;/td&gt;
      &lt;td&gt;7.50&lt;/td&gt;
      &lt;td&gt;85.00&lt;/td&gt;
      &lt;td&gt;0.2683&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;4.442651&lt;/td&gt;
      &lt;td&gt;0.071985&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;The data contains the main variables, &lt;code&gt;DGP&lt;/code&gt;, &lt;code&gt;Exprop&lt;/code&gt; and &lt;code&gt;Mort&lt;/code&gt;, plus some geographical information.&lt;/p&gt;
&lt;h3 id=&#34;ols-1&#34;&gt;OLS&lt;/h3&gt;
&lt;p&gt;What would we get if we were to ignore the endogeneity problem? We estimate the following misspecified model by OLS&lt;/p&gt;
&lt;p&gt;$$
{GDP}_i = \beta_0 + \beta_1 {Exprop}_i + \varepsilon_i
$$&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;reg1 = smf.ols(&#39;GDP ~ Exprop&#39;, df).fit()
reg1.summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
      &lt;td&gt;&lt;/td&gt;         &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt; &lt;td&gt;    4.6609&lt;/td&gt; &lt;td&gt;    0.409&lt;/td&gt; &lt;td&gt;   11.402&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    3.844&lt;/td&gt; &lt;td&gt;    5.478&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Exprop&lt;/th&gt;    &lt;td&gt;    0.5220&lt;/td&gt; &lt;td&gt;    0.061&lt;/td&gt; &lt;td&gt;    8.527&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    0.400&lt;/td&gt; &lt;td&gt;    0.644&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;The coefficient of &lt;code&gt;Exprop&lt;/code&gt; is positive and significant but we know it is a biased estimate of the causal effect.&lt;/p&gt;
&lt;p&gt;One direction we could take in addressing the endogeneity problem could be to control for any factor that affects both &lt;code&gt;GDP&lt;/code&gt; and &lt;code&gt;Exprop&lt;/code&gt;. In particular, the authors consider the following sets of variables:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;climat; proxied by latitude&lt;/li&gt;
&lt;li&gt;differences that affect both economic performance and institutions, eg. cultural, historical, etc.; controlled for with the use of continent dummies&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;reg2 = smf.ols(&#39;GDP ~ Exprop + Latitude + Latitude2&#39;, df).fit()
reg3 = smf.ols(&#39;GDP ~ Exprop + Latitude + Latitude2 + Asia + Africa + Namer + Samer&#39;, df).fit()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from statsmodels.iolib.summary2 import summary_col

summary_col(results=[reg1,reg2,reg3],
            float_format=&#39;%0.2f&#39;,
            stars = True,
            info_dict={&#39;No. observations&#39; : lambda x: f&amp;quot;{int(x.nobs):d}&amp;quot;},
            regressor_order=[&#39;Intercept&#39;,&#39;Exprop&#39;,&#39;Latitude&#39;,&#39;Latitude2&#39;])
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
          &lt;td&gt;&lt;/td&gt;          &lt;th&gt;GDP I&lt;/th&gt;  &lt;th&gt;GDP II&lt;/th&gt;  &lt;th&gt;GDP III&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt;        &lt;td&gt;4.66***&lt;/td&gt; &lt;td&gt;4.55***&lt;/td&gt; &lt;td&gt;5.95***&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;&lt;/th&gt;                 &lt;td&gt;(0.41)&lt;/td&gt;  &lt;td&gt;(0.45)&lt;/td&gt;  &lt;td&gt;(0.68)&lt;/td&gt; 
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Exprop&lt;/th&gt;           &lt;td&gt;0.52***&lt;/td&gt; &lt;td&gt;0.49***&lt;/td&gt; &lt;td&gt;0.40***&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;&lt;/th&gt;                 &lt;td&gt;(0.06)&lt;/td&gt;  &lt;td&gt;(0.07)&lt;/td&gt;  &lt;td&gt;(0.06)&lt;/td&gt; 
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Latitude&lt;/th&gt;            &lt;td&gt;&lt;/td&gt;      &lt;td&gt;2.16&lt;/td&gt;    &lt;td&gt;0.42&lt;/td&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;&lt;/th&gt;                    &lt;td&gt;&lt;/td&gt;     &lt;td&gt;(1.68)&lt;/td&gt;  &lt;td&gt;(1.47)&lt;/td&gt; 
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Latitude2&lt;/th&gt;           &lt;td&gt;&lt;/td&gt;      &lt;td&gt;-2.12&lt;/td&gt;   &lt;td&gt;0.44&lt;/td&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;&lt;/th&gt;                    &lt;td&gt;&lt;/td&gt;     &lt;td&gt;(2.86)&lt;/td&gt;  &lt;td&gt;(2.48)&lt;/td&gt; 
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Africa&lt;/th&gt;              &lt;td&gt;&lt;/td&gt;        &lt;td&gt;&lt;/td&gt;     &lt;td&gt;-1.06**&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;&lt;/th&gt;                    &lt;td&gt;&lt;/td&gt;        &lt;td&gt;&lt;/td&gt;     &lt;td&gt;(0.41)&lt;/td&gt; 
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Asia&lt;/th&gt;                &lt;td&gt;&lt;/td&gt;        &lt;td&gt;&lt;/td&gt;     &lt;td&gt;-0.74*&lt;/td&gt; 
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;&lt;/th&gt;                    &lt;td&gt;&lt;/td&gt;        &lt;td&gt;&lt;/td&gt;     &lt;td&gt;(0.42)&lt;/td&gt; 
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Namer&lt;/th&gt;               &lt;td&gt;&lt;/td&gt;        &lt;td&gt;&lt;/td&gt;      &lt;td&gt;-0.17&lt;/td&gt; 
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;&lt;/th&gt;                    &lt;td&gt;&lt;/td&gt;        &lt;td&gt;&lt;/td&gt;     &lt;td&gt;(0.40)&lt;/td&gt; 
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Samer&lt;/th&gt;               &lt;td&gt;&lt;/td&gt;        &lt;td&gt;&lt;/td&gt;      &lt;td&gt;-0.12&lt;/td&gt; 
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;&lt;/th&gt;                    &lt;td&gt;&lt;/td&gt;        &lt;td&gt;&lt;/td&gt;     &lt;td&gt;(0.42)&lt;/td&gt; 
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;R-squared&lt;/th&gt;         &lt;td&gt;0.54&lt;/td&gt;    &lt;td&gt;0.56&lt;/td&gt;    &lt;td&gt;0.71&lt;/td&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;R-squared Adj.&lt;/th&gt;    &lt;td&gt;0.53&lt;/td&gt;    &lt;td&gt;0.54&lt;/td&gt;    &lt;td&gt;0.67&lt;/td&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;No. observations&lt;/th&gt;   &lt;td&gt;64&lt;/td&gt;      &lt;td&gt;64&lt;/td&gt;      &lt;td&gt;64&lt;/td&gt;   
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;The coefficient of &lt;code&gt;Expropr&lt;/code&gt; decreases in magnitude but remains positive and significant after the addition of geographical control variables. This might suggest that the endogeneity problem is not very pronounced. However, it&amp;rsquo;s hard to say given the large number of factors that could affect both institutions and GDP.&lt;/p&gt;
&lt;h3 id=&#34;iv-1&#34;&gt;IV&lt;/h3&gt;
&lt;p&gt;In order for &lt;code&gt;Mort&lt;/code&gt; to be a valid instrument it needs to satisfy the two IV conditions:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Exclusion&lt;/strong&gt;: &lt;code&gt;Mort&lt;/code&gt; must be correlated to &lt;code&gt;GDP&lt;/code&gt; only through &lt;code&gt;Exprop&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Relevance&lt;/strong&gt;: &lt;code&gt;Mort&lt;/code&gt; must be correlated with &lt;code&gt;Exprop&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The &lt;strong&gt;exclusion restriction&lt;/strong&gt; condition is untestable, however, we may not be satisfied if settler mortality rates in the 17th to 19th centuries have a direct effect on current GDP (in addition to their indirect effect through institutions).&lt;/p&gt;
&lt;p&gt;For example, settler mortality rates may be related to the current disease environment in a country, which could affect current economic performance.&lt;/p&gt;
&lt;p&gt;The authors argue this is unlikely because:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The majority of settler deaths were due to malaria and yellow fever and had a limited effect on local people.&lt;/li&gt;
&lt;li&gt;The disease burden on local people in Africa or India, for example, did not appear to be higher than average, supported by relatively high population densities in these areas before colonization.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The &lt;strong&gt;relevance&lt;/strong&gt; condition is testable and we can check it by computing the partial correlation between &lt;code&gt;Mort&lt;/code&gt; and &lt;code&gt;Exprop&lt;/code&gt;. Let&amp;rsquo;s start by visual inspection first.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sns.scatterplot(data=df, x=&#39;Mort&#39;, y=&#39;Exprop&#39;)\
.set(title=&#39;First Stage&#39;,
    xlabel=&#39;Settler mortality&#39;,
    ylabel=&#39;Risk of expropriation&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/iv_57_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Visually, the first stage seems weak, at best. However, a regression of &lt;code&gt;Exprop&lt;/code&gt; on &lt;code&gt;Mort&lt;/code&gt; can help us better assess whether the relationship is significant or not.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;smf.ols(&#39;Exprop ~ Mort&#39;, df).fit().summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
      &lt;td&gt;&lt;/td&gt;         &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt; &lt;td&gt;    6.7094&lt;/td&gt; &lt;td&gt;    0.202&lt;/td&gt; &lt;td&gt;   33.184&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    6.305&lt;/td&gt; &lt;td&gt;    7.114&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Mort&lt;/th&gt;      &lt;td&gt;   -0.0008&lt;/td&gt; &lt;td&gt;    0.000&lt;/td&gt; &lt;td&gt;   -2.059&lt;/td&gt; &lt;td&gt; 0.044&lt;/td&gt; &lt;td&gt;   -0.002&lt;/td&gt; &lt;td&gt;-2.28e-05&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;The coefficient is negative, as expected, and statistically significant.&lt;/p&gt;
&lt;p&gt;The second-stage regression results give us an unbiased and consistent estimate of the effect of institutions on economic outcomes.&lt;/p&gt;
&lt;p&gt;$$
{GDP}_i = \beta_0 + \beta_1 {Exprop}_i + \varepsilon_i \
{Exprop}_i = \delta_0 + \delta_1 {logMort}_i + v_i
$$&lt;/p&gt;
&lt;p&gt;Note that while our parameter estimates are correct, our standard errors
are not and for this reason, computing 2SLS ‘manually’ (in stages with
OLS) is not recommended.&lt;/p&gt;
&lt;p&gt;We can correctly estimate a 2SLS regression in one step using the
&lt;a href=&#34;https://github.com/bashtage/linearmodels&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;linearmodels&lt;/a&gt; package, an extension of &lt;code&gt;statsmodels&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Note that when using &lt;code&gt;IV2SLS&lt;/code&gt;, the exogenous and instrument variables
are split up in the function arguments (whereas before the instrument
included exogenous variables)&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;IV2SLS.from_formula(&#39;GDP ~ 1 + [Exprop ~ logMort]&#39;, data=df).fit().summary.tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;caption&gt;Parameter Estimates&lt;/caption&gt;
&lt;tr&gt;
      &lt;td&gt;&lt;/td&gt;      &lt;th&gt;Parameter&lt;/th&gt; &lt;th&gt;Std. Err.&lt;/th&gt; &lt;th&gt;T-stat&lt;/th&gt; &lt;th&gt;P-value&lt;/th&gt; &lt;th&gt;Lower CI&lt;/th&gt; &lt;th&gt;Upper CI&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt;  &lt;td&gt;2.0448&lt;/td&gt;    &lt;td&gt;1.1273&lt;/td&gt;   &lt;td&gt;1.8139&lt;/td&gt; &lt;td&gt;0.0697&lt;/td&gt;   &lt;td&gt;-0.1647&lt;/td&gt;  &lt;td&gt;4.2542&lt;/td&gt; 
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Exprop&lt;/th&gt;     &lt;td&gt;0.9235&lt;/td&gt;    &lt;td&gt;0.1691&lt;/td&gt;   &lt;td&gt;5.4599&lt;/td&gt; &lt;td&gt;0.0000&lt;/td&gt;   &lt;td&gt;0.5920&lt;/td&gt;   &lt;td&gt;1.2550&lt;/td&gt; 
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;The result suggests a stronger positive relationship than what the OLS results indicated.&lt;/p&gt;
&lt;h2 id=&#34;business-case&#34;&gt;Business Case&lt;/h2&gt;
&lt;p&gt;We are given the following problem:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;A firm would like to understand whether its newsletter is working to increase revenue. However, it cannot force customers to subscribe to the newsletter. Instead, the firm sends a reminder email to a random sample of customers for the newsletter. Estimate the effect of the newsletter on revenue.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;We start by drawing a sample from the data generating process.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from src.dgp import dgp_newsletter

dgp = dgp_newsletter()
df = dgp.generate_data()
df.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;reminder&lt;/th&gt;
      &lt;th&gt;subscribe&lt;/th&gt;
      &lt;th&gt;revenue&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0.582809&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;3.427162&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1.953731&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;2.902038&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0.826724&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;From the data, we know the &lt;code&gt;revenue&lt;/code&gt; per customer, whether it was sent a &lt;code&gt;reminder&lt;/code&gt; for the newsletter and whether it actually decided to &lt;code&gt;subscribe&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;If we to estimate the effect of &lt;code&gt;subscribe&lt;/code&gt; on &lt;code&gt;revenue&lt;/code&gt;, we might get a biased estimate because the decision of subscribing is endogenous. For example, we can imagine that wealthier customers are generating more revenue but are also less likely to subscribe.&lt;/p&gt;
&lt;p&gt;We can represent the model with a DAG.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;dag_iv(Y=&amp;quot;revenue&amp;quot;, T=&amp;quot;subscribe&amp;quot;, Z=&amp;quot;reminder&amp;quot;, U=&amp;quot;income&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/iv_68_0.svg&#34; alt=&#34;svg&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;ols-2&#34;&gt;OLS&lt;/h3&gt;
&lt;p&gt;By directly inspecting the data, it seems that subscribed members actually generate less revenue than normal customers.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sns.barplot(x=&#39;subscribe&#39;, y=&#39;revenue&#39;, data=df);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/iv_71_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;A linear regression confirms the graphical intuition.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;smf.ols(&#39;revenue ~ 1 + subscribe&#39;, df).fit().summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
      &lt;td&gt;&lt;/td&gt;         &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt; &lt;td&gt;    1.7752&lt;/td&gt; &lt;td&gt;    0.086&lt;/td&gt; &lt;td&gt;   20.697&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    1.607&lt;/td&gt; &lt;td&gt;    1.943&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;subscribe&lt;/th&gt; &lt;td&gt;   -0.7441&lt;/td&gt; &lt;td&gt;    0.140&lt;/td&gt; &lt;td&gt;   -5.334&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;   -1.018&lt;/td&gt; &lt;td&gt;   -0.470&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;However, if indeed wealthier customers generate more revenue and are less likely to subscribe, we have a negative omitted variable bias and we can expect the true effect of the newsletter to be bigger than the OLS estimate.&lt;/p&gt;
&lt;h3 id=&#34;iv-2&#34;&gt;IV&lt;/h3&gt;
&lt;p&gt;Let&amp;rsquo;s now exploit the random variation induced by the discount. In order for our instrument to be valid, we need it to be exogenous (untestable) and relevant. We can test the relevance with the &lt;strong&gt;first stage&lt;/strong&gt; regresssion of &lt;code&gt;reminder&lt;/code&gt; on &lt;code&gt;subscribe&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;smf.ols(&#39;subscribe ~ 1 + reminder&#39;, df).fit().summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
      &lt;td&gt;&lt;/td&gt;         &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt; &lt;td&gt;    0.2368&lt;/td&gt; &lt;td&gt;    0.021&lt;/td&gt; &lt;td&gt;   11.324&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    0.196&lt;/td&gt; &lt;td&gt;    0.278&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;reminder&lt;/th&gt;  &lt;td&gt;    0.2790&lt;/td&gt; &lt;td&gt;    0.029&lt;/td&gt; &lt;td&gt;    9.488&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    0.221&lt;/td&gt; &lt;td&gt;    0.337&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;It seems that the instrument is relevant. We can now estimate the IV regression.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;IV2SLS.from_formula(&#39;revenue ~ 1 + [subscribe ~ reminder]&#39;, data=df).fit().summary.tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;caption&gt;Parameter Estimates&lt;/caption&gt;
&lt;tr&gt;
      &lt;td&gt;&lt;/td&gt;      &lt;th&gt;Parameter&lt;/th&gt; &lt;th&gt;Std. Err.&lt;/th&gt; &lt;th&gt;T-stat&lt;/th&gt; &lt;th&gt;P-value&lt;/th&gt; &lt;th&gt;Lower CI&lt;/th&gt; &lt;th&gt;Upper CI&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt;  &lt;td&gt;0.9485&lt;/td&gt;    &lt;td&gt;0.2147&lt;/td&gt;   &lt;td&gt;4.4184&lt;/td&gt; &lt;td&gt;0.0000&lt;/td&gt;   &lt;td&gt;0.5278&lt;/td&gt;   &lt;td&gt;1.3693&lt;/td&gt; 
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;subscribe&lt;/th&gt;  &lt;td&gt;1.4428&lt;/td&gt;    &lt;td&gt;0.5406&lt;/td&gt;   &lt;td&gt;2.6689&lt;/td&gt; &lt;td&gt;0.0076&lt;/td&gt;   &lt;td&gt;0.3832&lt;/td&gt;   &lt;td&gt;2.5023&lt;/td&gt; 
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;The estimated coefficient has now flipped sign and turned positive! Ignoring the endogeneity problem would have lead us to the wrong conclusion.&lt;/p&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=LEAx0He_KBI&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Instrumental Variables&lt;/a&gt; video lecture by Paul Goldsmith-Pinkham (Yale)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://matheusfacure.github.io/python-causality-handbook/08-Instrumental-Variables.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Instrumental Variables&lt;/a&gt; section from Matheus Facure&amp;rsquo;s &lt;a href=&#34;https://matheusfacure.github.io/python-causality-handbook/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Causal Inference for The Brave and The True&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.jstor.org/stable/2937954&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Does compulsory school attendance affect schooling and earnings?&lt;/a&gt; (1991) by Angrist and Krueger&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://economics.mit.edu/files/4123&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The Colonial Origins of Comparative Development&lt;/a&gt; (2002) by Acemoglu, Johnson, Robinson&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Permutation Tests for Dummies</title>
      <link>https://matteocourthoud.github.io/post/permutation_test/</link>
      <pubDate>Fri, 15 Apr 2022 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/post/permutation_test/</guid>
      <description>&lt;p&gt;If you search &amp;ldquo;permutation test&amp;rdquo; on Wikipedia, you get the following definition:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;A permutation test (also called re-randomization test) is an exact statistical hypothesis test making use of the proof by contradiction in which the distribution of the test statistic under the null hypothesis is obtained by calculating all possible values of the test statistic under possible rearrangements of the observed data.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;What does it mean? In this tutorial we are going to see in detail what this definition means, how to implement permutation tests, and their pitfalls.&lt;/p&gt;
&lt;h2 id=&#34;example-1-is-a-coin-fair&#34;&gt;Example 1: is a coin fair?&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s start with an example: suppose you wanted to test whether a coin is fair. You throw the coin 10 times and you count the number of times you get heads. Let&amp;rsquo;s simulate the outcome.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import numpy as np

np.random.seed(1)
np.random.binomial(1, 0.5, 10)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;array([0, 1, 0, 0, 0, 0, 0, 0, 0, 1])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Out of 10 coin throws, we got only 2 heads. Does it mean that the coin is not fair?&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;question&lt;/strong&gt; that permutation testing is trying to answer is &amp;ldquo;&lt;em&gt;how unlikely is the observed outcome under the null hypothesis that the coin is fair?&lt;/em&gt;&amp;rdquo;.&lt;/p&gt;
&lt;p&gt;In this case we can directly compute this answer since we have a very little number of throws. The total number of outcomes is $2^{10}$. The number of as or more extreme outcomes, under the assumption that the coin is fair (50-50) is&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;0 heads: ${10 \choose 0} = 1$&lt;/li&gt;
&lt;li&gt;1 head: ${10 \choose 1} = 10$&lt;/li&gt;
&lt;li&gt;2 heads: ${10 \choose 2} = 45$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;So that the probability of getting the same or a more extreme outcome is&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from scipy.special import comb

(comb(10, 0) + comb(10, 1) + comb(10, 2)) / 2**10
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;0.0546875
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This probability seems low but not too low.&lt;/p&gt;
&lt;p&gt;However, we have forgot one thing. We want to test whether the coin is fair in &lt;strong&gt;either&lt;/strong&gt; direction. We would suspect that the coin is unfair if we were getting few heads (as we did), but also if we were getting many heads. Therefore, we should account for both extremes.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sum([comb(10, i) for i in [0, 1, 2, 8, 9, 10]]) / 2**10
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;0.109375
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This number should not be surprising since it&amp;rsquo;s exactly double the previous one.&lt;/p&gt;
&lt;p&gt;It is common in statistics to say that an event is unusual if its probability is less than 1 in 20, i.e. $5%$. If we were adopting that threshold, we would not conclude that getting 2 heads in 10 trows is so unusual. However, getting just one, would be.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sum([comb(10, i) for i in [0, 1, 9, 10]]) / 2**10
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;0.021484375
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;hypothesis-testing&#34;&gt;Hypothesis Testing&lt;/h2&gt;
&lt;p&gt;The process we just went through is called &lt;strong&gt;hypothesis testing&lt;/strong&gt;. The components of an hypothesis test are:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A null hypothesis $H_0$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;in our case, that the coin war fair&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;A test statistic $t$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;in our case, the number of zeros&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;A level of significance $\alpha$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;it is common to choose 5%&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The &lt;strong&gt;idea&lt;/strong&gt; behind &lt;strong&gt;permutation testing&lt;/strong&gt; is the following: in a setting in which we are checking whether one variable has an effect on another variable, the two variables should not be correlated, under the null hypothesis . Therefore, we could re-shuffle the treatment variable and re-compute the test statistic. Lastly, we can compute the p-value as the fraction of as or more extremes outcomes under re-shuffling of the data.&lt;/p&gt;
&lt;h2 id=&#34;example-2-are-women-smarter&#34;&gt;Example 2: are women smarter?&lt;/h2&gt;
&lt;p&gt;Suppose now we were interested in knowing whether females perform better in a test than men. Let&amp;rsquo;s start by writing the data generating process under the assumption of no difference in scores. However, only 30% of the sample will be female.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import pandas as pd

# Data generating process
def generate_data_gender(N=100, seed=1):
    np.random.seed(seed) # Set seed for replicability
    data = pd.DataFrame({&amp;quot;female&amp;quot;: np.random.binomial(1, 0.3, N),
                         &amp;quot;test_score&amp;quot;: np.random.exponential(3, N)})
    return data
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s now generate a sample of size 100.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Generate data
data_gender = generate_data_gender()
data_gender.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;female&lt;/th&gt;
      &lt;th&gt;test_score&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1.186447&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;2.246348&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;6.513147&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1.326091&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;7.175402&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;We can compute the treatment effect by computing the difference in mean outcomes between male and females.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def compute_score_diff(data):
    T = np.mean(data.loc[data[&#39;female&#39;]==1, &#39;test_score&#39;]) - np.mean(data.loc[data[&#39;female&#39;]==0, &#39;test_score&#39;])
    return T
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;T = compute_score_diff(data_gender)
print(f&amp;quot;The estimated treatment effect is {T}&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;The estimated treatment effect is -1.3612262580563321
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It looks that females actually did worse than males. But is the difference statistically significant? We can perform a randomization test and compute the probability of observing a more extreme outcome.&lt;/p&gt;
&lt;p&gt;First, let&amp;rsquo;s write the permutation routine that takes a variable in the data and permutes it.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def permute(data, var, r):
    temp_data = data.copy()
    temp_data[var] = np.random.choice(data[var], size=len(data), replace=r)
    return temp_data
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can now write the permutation test. It spits out a vector of statistics and prints the implied p-value.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def permutation_test(data, permute, var, compute_stat, K=1000, r=False):
    T = compute_stat(data)
    T_perm = []
    for k in range(K):
        temp_data = permute(data, var, r)
        T_perm += [compute_stat(temp_data)]
    print(f&amp;quot;The p-value is {sum(np.abs(T_perm) &amp;gt;= np.abs(T))/K}&amp;quot;)
    return T_perm
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;Ts = permutation_test(data_gender, permute, &#39;test_score&#39;, compute_score_diff)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;The p-value is 0.063
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Apparently the result we have observed was quite unusual, but not at the 5% level. We can plot the distribution of statistics to visualize this result.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;%matplotlib inline
%config InlineBackend.figure_format = &#39;retina&#39;
from src.utils import *
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def plot_test(T, Ts, title):
    plt.hist(Ts, density=True, bins=30, alpha=0.7, color=&#39;C0&#39;)
    plt.vlines([-T, T], ymin=plt.ylim()[0], ymax=plt.ylim()[1], color=&#39;C2&#39;)
    plt.title(title);
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plot_test(T, Ts, &#39;Distribution of score differences under permutation&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/permutation_test_29_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;As we can see, the observed difference in scores is quite extreme with respect the distribution generate by the permutation.&lt;/p&gt;
&lt;p&gt;One &lt;strong&gt;issue&lt;/strong&gt; with the permutation test we just ran is that it is computationally expensive to draw without replacement. The standard and much faster procedure is to draw without replacement.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;Ts_repl = permutation_test(data_gender, permute, &#39;test_score&#39;, compute_score_diff, r=True)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;The p-value is 0.052
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The p-value is virtually the same.&lt;/p&gt;
&lt;p&gt;How &lt;strong&gt;accurate&lt;/strong&gt; is the test? Since we have access to the data generating process, we can compute the true p-value via simulation. We draw many samples from the true data generating process and, for each, compute the difference in scores. The simulated p-value is going to be the frequency of more extreme statistics.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Function to simulate data and compute pvalue
def simulate_stat(dgp, compute_stat, K=1000):
    T = compute_stat(dgp())
    T_sim = []
    for k in range(K):
        data = dgp(seed=k)
        T_sim += [compute_stat(data)]
    print(f&amp;quot;The p-value is {sum(np.abs(T_sim) &amp;gt;= np.abs(T))/K}&amp;quot;)
    return np.array(T_sim)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;T_sim = simulate_stat(generate_data_gender, compute_score_diff)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;The p-value is 0.038
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Again, we can plot the distribution of simulated statistics to understand the computed p-value.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plot_test(T, T_sim, &#39;Distribution of score differences under simulation&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/permutation_test_36_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;As expected, most of the mass lies within the interval, indicating a relatively extreme result. We have just been &amp;ldquo;unlucky&amp;rdquo; with the draw, but the permutation test was accurate.&lt;/p&gt;
&lt;h2 id=&#34;permutation-tests-vs-t-tests&#34;&gt;Permutation tests vs t-tests&lt;/h2&gt;
&lt;p&gt;What is the difference between a t-test and a permutation test?&lt;/p&gt;
&lt;p&gt;Permutation test &lt;strong&gt;advantages&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;does not make distributional assumptions&lt;/li&gt;
&lt;li&gt;not sensible to outliers&lt;/li&gt;
&lt;li&gt;can be computed also for statistics whose distribution is not known&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Permutation test &lt;strong&gt;disadvantages&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;computationally intense&lt;/li&gt;
&lt;li&gt;very sample-dependent&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;example-3-is-university-worth&#34;&gt;Example 3: is university worth?&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s now switch to a new example to compare t-tests and permutation tests.&lt;/p&gt;
&lt;p&gt;Assume we want to check whether university is a worthy investment. We have information about whether individuals attended university and their future salary. The problem here is that income is a particularly &lt;strong&gt;skewed&lt;/strong&gt; variable.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Data generating process
def generate_data_income(N=1000, seed=1):
    np.random.seed(seed) # Set seed for replicability
    university = np.random.binomial(1, 0.5, N) # Treatment
    data = pd.DataFrame({&amp;quot;university&amp;quot;: university,
                         &amp;quot;income&amp;quot;: np.random.lognormal(university, 2.3, N)})
    return data
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;data_income = generate_data_income()
data_income.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;university&lt;/th&gt;
      &lt;th&gt;income&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;5.305618&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1.289598&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;6.507720&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;6.019961&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0.034482&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;The distribution of income is very heavy tailed. Let&amp;rsquo;s plot its density across the two groups.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sns.kdeplot(data=data_income, x=&amp;quot;income&amp;quot;, hue=&amp;quot;university&amp;quot;)\
.set(title=&#39;Income density by group&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/permutation_test_45_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;The distribution is so skewed that we cannot actually visually perceive differences between the two groups. Let&amp;rsquo;s compute the expected difference.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def compute_income_diff(data):
    T = np.mean(data.loc[data[&#39;university&#39;]==1, &#39;income&#39;]) - np.mean(data.loc[data[&#39;university&#39;]==0, &#39;income&#39;])
    return T
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;T = compute_income_diff(data_income)
T
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;23.546974435985444
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It looks like university graduates have higher income. Is this difference statistically different from zero? Let&amp;rsquo;s perform a permutation test.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;T_perm = permutation_test(data_income, permute, &#39;university&#39;, compute_income_diff)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;The p-value is 0.011
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The permutation test is telling us that the difference is extremely unusual under the null hypothesis. In other words, it is very unlikely that university graduates earn the same income of non-university graduates.&lt;/p&gt;
&lt;p&gt;What would be the outcome of a standard t-test?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from scipy.stats import ttest_ind

ttest_ind(data_income.query(&#39;university==1&#39;)[&#39;income&#39;], data_income.query(&#39;university==0&#39;)[&#39;income&#39;])
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Ttest_indResult(statistic=1.5589492598056494, pvalue=0.1193254252009701)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As we can see, the two tests provide extremely different results. The t-test is much more conservative, telling us that the unlikeliness of the data is just $12%$ compared to the $1.1%$ of the permutation test.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;reason&lt;/strong&gt; is that we have extremely skewed data. The t-test is very sensible to extreme observation and will therefore compute a very high variance because of very few data points.&lt;/p&gt;
&lt;p&gt;The permutation test can further address the problem of a skewed outcome distribution by using a test statistic that is more &lt;strong&gt;sensible to outliers&lt;/strong&gt;. Let&amp;rsquo;s perform the permutation test using the &lt;strong&gt;trimmed mean&lt;/strong&gt; instead of the mean.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from scipy.stats import trim_mean

def compute_income_mediandiff(data):
    T = np.median(data.loc[data[&#39;university&#39;]==1, &#39;income&#39;]) - np.median(data.loc[data[&#39;university&#39;]==0, &#39;income&#39;])
    return T
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;T_perm = permutation_test(data_income, permute, &#39;university&#39;, compute_income_mediandiff)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;The p-value is 0.0
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In this case, the permutation test is extremely confident that the trimmed mean of the two groups is different.&lt;/p&gt;
&lt;p&gt;However, an advantage of the t-test is &lt;strong&gt;speed&lt;/strong&gt;. Let&amp;rsquo;s compare the two tests by computing their execution time. Note that this is just a rough approximation since the permutation test could be sensible optimized.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import time

# No replacement
start = time.time()
permutation_test(data_income, permute, &#39;university&#39;, compute_income_diff)
print(f&amp;quot;Elapsed time without replacement: {time.time() - start}&amp;quot;)

# Replacement
start = time.time()
ttest_ind(data_income.query(&#39;university==1&#39;)[&#39;income&#39;], data_income.query(&#39;university==0&#39;)[&#39;income&#39;])
print(f&amp;quot;Elapsed time with replacement: {time.time() - start}&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;The p-value is 0.016
Elapsed time without replacement: 0.28911614418029785
Elapsed time with replacement: 0.00125885009765625
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The permutation test is 300 times slower. This can be a particularly relevant difference for larger sample sizes.&lt;/p&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;In this tutorial, we have seen how to perform permutation tests across different data generating processes.&lt;/p&gt;
&lt;p&gt;The underlying principle is the same: permute an variable that is assumed to be random under the null hypothesis and re-compute the test statistic. Then check how unusual was the test statistic in the original dataset.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Policy Learning</title>
      <link>https://matteocourthoud.github.io/post/policy_learning/</link>
      <pubDate>Fri, 15 Apr 2022 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/post/policy_learning/</guid>
      <description>&lt;p&gt;In this tutorial, we are going to see how to design the most welfare-improving policy in presence of treatment effect heterogeneity and treatment costs or budget constraints.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Requisites&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;For this tutorial, I assume you are familiar with the following concepts:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Rubin&amp;rsquo;s potential outcome framework&lt;/li&gt;
&lt;li&gt;Propensity weighting or uplifting&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://matteocourthoud.github.io/post/aipw/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;AIPW or Double Robust Estimators&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://matteocourthoud.github.io/post/causal_trees/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Causal Trees&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Academic Replication&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;We are going to replicate the paper by &lt;a href=&#34;https://www.aeaweb.org/articles?id=10.1257/jep.32.4.201&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Hanna and Olken (2018)&lt;/a&gt; in which the authors study the optimal allocation of a cash transfer to households in Peru. We slightly twist the original paper by actually assigning the transfer and assuming 100% consumption.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Business Case&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;We are going to study a company that has to decide which consumers to target with ads.&lt;/p&gt;
&lt;h2 id=&#34;setting&#34;&gt;Setting&lt;/h2&gt;
&lt;p&gt;We assume that for a set of i.i.d. subjects $i = 1, &amp;hellip;, n$ we observed a tuple $(X_i, T_i, Y_i)$ comprised of&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;a feature vector $X_i \in \mathbb R^n$&lt;/li&gt;
&lt;li&gt;a treatment variable $T_i \in \lbrace 0, 1 \rbrace$&lt;/li&gt;
&lt;li&gt;a response $Y_i \in \mathbb R$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Assumption 1 : unconfoundedness&lt;/strong&gt; (or ignorability, or selection on observables)&lt;/p&gt;
&lt;p&gt;$$
\big \lbrace Y_i^{(1)} , Y_i^{(0)} \big \rbrace \ \perp \ T_i \ | \ X_i
$$&lt;/p&gt;
&lt;p&gt;i.e. conditional on observable characteristics $X$, the treatment assignment $T$ is as good as random.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Assumption 2: overlap&lt;/strong&gt; (or bounded support)&lt;/p&gt;
&lt;p&gt;$$
\exists \eta &amp;gt; 0 \ : \ \eta \leq \mathbb E \left[ T_i = 1 \ \big | \ X_i = x \right] \leq 1-\eta
$$&lt;/p&gt;
&lt;p&gt;i.e. no observation is deterministically assigned to the treatment or control group.&lt;/p&gt;
&lt;h2 id=&#34;policy-learning&#34;&gt;Policy Learning&lt;/h2&gt;
&lt;p&gt;The objective of policy learning is to decide which people to treat. More explicitly, we want to learn a map from observable characteristics to a (usually binary) policy space.&lt;/p&gt;
&lt;p&gt;$$
\pi : \mathcal X \to \lbrace 0, 1 \rbrace
$$&lt;/p&gt;
&lt;p&gt;Policy learning is closely related to the &lt;strong&gt;estimation of heterogeneous treatment effects&lt;/strong&gt;. In fact, in both settings, we want to investigate how the treatment affects different individuals in different ways.&lt;/p&gt;
&lt;p&gt;The main &lt;strong&gt;difference&lt;/strong&gt; between policy learning and the estimation of heterogeneous treatment effects is the objective function. In policy learning, we are acting in a limited resources setting where providing treatment is costly and the cost could depend on individual characteristics. For example, it might be more costly to vaccinate individuals that live in remote areas. Therefore, one might not just want to treat individuals with the largest expected treatment effect, but the ones for whom treatment is most cost-effective.&lt;/p&gt;
&lt;p&gt;The utilitarian &lt;strong&gt;value&lt;/strong&gt; of a policy $\pi$&lt;/p&gt;
&lt;p&gt;$$
V(\pi) = \mathbb E \Big[ Y_i(\pi(X_i)) \Big] = \mathbb E \big[ Y^{(0)}_i \big] + \mathbb E \big[ \tau(X_i) \pi(X_i) \big]
$$&lt;/p&gt;
&lt;p&gt;measures the expectation of the potential outcome $Y$ if we were to &lt;strong&gt;assign&lt;/strong&gt; treatment $T$ according to policy $\pi$. This expectation can be split into &lt;strong&gt;two parts&lt;/strong&gt;:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;The baseline expected potential outcome $\mathbb E \big[ Y^{(0)}_i \big]$&lt;/li&gt;
&lt;li&gt;The expected effect of the policy $\mathbb E \big[ \tau(X_i) \pi(X_i) \big]$&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The &lt;strong&gt;objective&lt;/strong&gt; of policy learning is to learn a policy with high value $V(\pi)$. As part (2) of the formula makes clear, you get a higher value if you treat the people with a high treatment effect $\tau(x)$.&lt;/p&gt;
&lt;p&gt;A &lt;strong&gt;simple approach&lt;/strong&gt; could be to assign treatment according to a &lt;strong&gt;thresholding rule&lt;/strong&gt; $\tau(x) &amp;gt; c$, where $c$ is some cost below which is not worth treating individuals (or there is not enough budget).&lt;/p&gt;
&lt;p&gt;However, estimating the conditional average treatment effect (CATE) function $\tau(x)$ and learning a good policy $\pi(x)$ are different &lt;strong&gt;problems&lt;/strong&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the correct loss function for policy learning is not the mean squared error (MSE) on $\tau(x)$
&lt;ul&gt;
&lt;li&gt;we want to &lt;strong&gt;maximize welfare&lt;/strong&gt;!&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;the CATE function $\tau(x)$ might not use some features for targeting
&lt;ul&gt;
&lt;li&gt;e.g. &lt;strong&gt;cannot discriminate&lt;/strong&gt; based on race or gender&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;you don&amp;rsquo;t want to have feature that people can influence
&lt;ul&gt;
&lt;li&gt;e.g. use a self-reported measure that people can distort&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We would like to find a &lt;strong&gt;loss function&lt;/strong&gt; $L(\pi ; Y_i, X_i, T_i)$ such that&lt;/p&gt;
&lt;p&gt;$$
\mathbb E \big[ L(\pi ; Y_i, X_i, T_i) \big] = - V(\pi)
$$&lt;/p&gt;
&lt;h3 id=&#34;ipw-loss&#34;&gt;IPW Loss&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://onlinelibrary.wiley.com/doi/abs/10.3982/ECTA13288&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Kitagawa and Tenenov (2018)&lt;/a&gt; propose to learn an empirical estimate of the value function using inverse propensity weighting (IPW).&lt;/p&gt;
&lt;p&gt;$$
\hat \pi = \arg \max_{\pi} \Big\lbrace \hat V(\pi) : \pi \in \Pi \Big\rbrace
$$&lt;/p&gt;
&lt;p&gt;where&lt;/p&gt;
&lt;p&gt;$$
\hat V(\pi) = \frac{ \mathbb I \big(\lbrace T_i = \pi(X_i) \rbrace \big) }{ \mathbb P \big[ \lbrace T_i = \pi(X_i) \rbrace \ \big| \ X_i \big] } Y_i
$$&lt;/p&gt;
&lt;p&gt;The authors show that under &lt;strong&gt;unconfoundedness&lt;/strong&gt;, if the propensity score $e(x)$ is known and $\Pi$ is not too complex, the value of the estimated policy converges to the optimal value.&lt;/p&gt;
&lt;p&gt;Note that this is a &lt;strong&gt;very different problem&lt;/strong&gt; from the normal optimization problem with a MSE loss. In fact, we now have a binary argument in the loss function which makes the problem similar to a classification problem, in which we want to classify people into &lt;em&gt;high gain&lt;/em&gt; and &lt;em&gt;low gain&lt;/em&gt; categories.&lt;/p&gt;
&lt;h3 id=&#34;aipw-loss&#34;&gt;AIPW Loss&lt;/h3&gt;
&lt;p&gt;If propensity score $e(x)$ is not known, we can use a &lt;strong&gt;doubly robust estimator&lt;/strong&gt;, exactly as for the average treatment effect.&lt;/p&gt;
&lt;p&gt;$$
\hat V = \frac{1}{n} \sum_{i=1}^{n}
\begin{cases}
\hat \Gamma_i \quad &amp;amp;\text{if} \quad \pi(X_i) = 1
\newline&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;\hat \Gamma_i \quad &amp;amp;\text{if} \quad \pi(X_i) = 0
\end{cases}
$$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;where&lt;/p&gt;
&lt;p&gt;$$
\hat \Gamma_i = \hat \mu^{(1)}(X_i) - \hat \mu^{(0)}(X_i) + \frac{T_i }{\hat e(X_i)} \left( Y_i - \hat \mu^{(1)}(X_i) \right) - \frac{(1-T_i) }{1-\hat e(X_i)} \left( Y_i - \hat \mu^{(0)}(X_i) \right)
$$&lt;/p&gt;
&lt;p&gt;The relationship with AIPW is that $\hat \tau_{AIPW} = \frac{1}{n} \sum_{i=1}^{n} \hat \Gamma_i$. Therefore, the objective function $V(\pi)$ is build so that when we assign treatment to a unit we &amp;ldquo;gain&amp;rdquo; the double-robust score $\hat \tau_{AIPW}$, while, if we do not assign treatment, we &amp;ldquo;pay&amp;rdquo; the double-robust score $\hat \tau_{AIPW}$.&lt;/p&gt;
&lt;h2 id=&#34;academic-application&#34;&gt;Academic Application&lt;/h2&gt;
&lt;p&gt;For the academic applicaiton, we are going to replicate the paper by &lt;a href=&#34;https://www.aeaweb.org/articles?id=10.1257/jep.32.4.201&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Hanna and Olken (2018)&lt;/a&gt; in which the authors study the optimal allocation of a cash transfer to households in Peru. We slightly twist the original paper by actually assigning the transfer and assuming 100% consumption.&lt;/p&gt;
&lt;p&gt;First, let&amp;rsquo;s load the modified dataset.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from src.utils import *
from src.dgp import dgp_ao18
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;%matplotlib inline
%config InlineBackend.figure_format = &#39;retina&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;dgp = dgp_ao18()
df = dgp.import_data()
df.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;Unnamed: 0&lt;/th&gt;
      &lt;th&gt;d_fuel_other&lt;/th&gt;
      &lt;th&gt;d_fuel_wood&lt;/th&gt;
      &lt;th&gt;d_fuel_coal&lt;/th&gt;
      &lt;th&gt;d_fuel_kerosene&lt;/th&gt;
      &lt;th&gt;d_fuel_gas&lt;/th&gt;
      &lt;th&gt;d_fuel_electric&lt;/th&gt;
      &lt;th&gt;d_fuel_none&lt;/th&gt;
      &lt;th&gt;d_water_other&lt;/th&gt;
      &lt;th&gt;d_water_river&lt;/th&gt;
      &lt;th&gt;...&lt;/th&gt;
      &lt;th&gt;d_lux_1&lt;/th&gt;
      &lt;th&gt;d_lux_2&lt;/th&gt;
      &lt;th&gt;d_lux_3&lt;/th&gt;
      &lt;th&gt;d_lux_4&lt;/th&gt;
      &lt;th&gt;d_lux_5&lt;/th&gt;
      &lt;th&gt;training&lt;/th&gt;
      &lt;th&gt;h_hhsize&lt;/th&gt;
      &lt;th&gt;cash_transfer&lt;/th&gt;
      &lt;th&gt;consumption&lt;/th&gt;
      &lt;th&gt;welfare&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;211.0000&lt;/td&gt;
      &lt;td&gt;5.351858&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;420.1389&lt;/td&gt;
      &lt;td&gt;6.040585&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;390.8318&lt;/td&gt;
      &lt;td&gt;5.968277&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;9&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;285.6018&lt;/td&gt;
      &lt;td&gt;5.654599&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;8&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;118.0713&lt;/td&gt;
      &lt;td&gt;4.771289&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;5 rows × 78 columns&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;As we can see, we have a lot of information about individuals in Peru. Crucially for the research question, we observe&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;whether the household received a cash transfer, &lt;code&gt;cash_transfer&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;the household&amp;rsquo;s welfare afterwards, &lt;code&gt;welfare_post&lt;/code&gt;
&lt;ul&gt;
&lt;li&gt;assuming
$$
\text{welfare} = \log (\text{consumption})
$$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We would like to understand which individuals should be given a transfer, given that the transfer is costly. Let&amp;rsquo;s assume the transfer costs $0.3$ units of welfare.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from econml.policy import DRPolicyForest

cost = 0.3
policy = DRPolicyForest(random_state=1).fit(Y=df[dgp.Y] - cost*df[dgp.T], T=df[dgp.T], X=df[dgp.X])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can partially visualize the policy by plotting a regression tree for the most important features.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;%matplotlib inline
policy.plot(tree_id=1, max_depth=2, feature_names=dgp.X, fontsize=8)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/policy_learning_18_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;To understand if the estimated policy was effective, we can load the oracle dataset, with the potential outcomes.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_oracle = dgp.import_data(oracle=True)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;From the oracle dataset, we can compute the actual value of the policy.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;T_hat = policy.predict(df[dgp.X])
V_policy = (df_oracle[&#39;welfare_1&#39;].values - cost - df_oracle[&#39;welfare_0&#39;].values) * T_hat
print(f&#39;Estimated policy value (N_T={sum(T_hat)}): {np.mean(V_policy) :.4}&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Estimated policy value (N_T=21401): 0.05897
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The value is positive, indicating that the treatment was effective. But how well did we do? We can compare the estimated policy with the oracle policy that assign treatment to each cost-effective unit.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;T_oracle = (df_oracle[&#39;welfare_1&#39;] - df_oracle[&#39;welfare_0&#39;]) &amp;gt; cost
V_oracle = (df_oracle[&#39;welfare_1&#39;] - cost - df_oracle[&#39;welfare_0&#39;] ) * T_oracle
print(f&#39;Oracle policy value (N_T={sum(T_oracle)}): {np.mean(V_oracle) :.4}&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Oracle policy value (N_T=17630): 0.07494
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We actually achieved 79% of the potential policy gains! Also note that our policy is too generous, treating more units than optimal. But how well would we have done if the same amount of cash transfers were given at random?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;T_rand = np.random.binomial(1, sum(T_hat)/len(df), len(df))
V_rand = (df_oracle[&#39;welfare_1&#39;] - cost - df_oracle[&#39;welfare_0&#39;] ) * T_rand
print(f&#39;Random policy value (N_T={sum(T_rand)}): {np.mean(V_rand) :.4}&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Random policy value (N_T=21359): 0.0002698
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;A random assignment of the same amount of cash transfers would not achieve any effect. However, this assumes that we already know the optimal amount of funds to distribute. What if instead we had treated everyone?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;V_all = (df_oracle[&#39;welfare_1&#39;] - cost - df_oracle[&#39;welfare_0&#39;] )
print(f&#39;All-treated policy value (N_T={len(df)}): {np.mean(V_all) :.4}&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;All-treated policy value (N_T=45378): 0.0004019
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Indiscriminate treatment would again not achieve any effect. Lastly, what if we had just estimated the treatment effect using AIPW and used it as a threshold?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from econml.dr import LinearDRLearner

model = LinearDRLearner(random_state=1).fit(Y=df[dgp.Y], T=df[dgp.T], X=df[dgp.X])
T_ipw = model.effect(X=df[dgp.X], T0=0, T1=1) &amp;gt; cost
V_ipw = (df_oracle[&#39;welfare_1&#39;] - cost - df_oracle[&#39;welfare_0&#39;] ) * T_ipw
print(f&#39;IPW policy value (N_T={sum(T_ipw)}): {np.mean(V_ipw) :.4}&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;IPW policy value (N_T=21003): 0.06293
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We are actually doing better! Weird&amp;hellip;&lt;/p&gt;
&lt;h2 id=&#34;business-case&#34;&gt;Business Case&lt;/h2&gt;
&lt;p&gt;We are given the following problem:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;A firm would like to understand which customers to show an ad, in order to increase revenue. The firm ran a A/B test showing a random sample of customers an ad. First, try to understand if there is heterogeneity in treatment. Then, decide which customers to show the ad, given that ads are costly (1$ each). Further suppose that you cannot discriminate on gender. How do the results change?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;We start by drawing a sample from the data generating process.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from src.utils import *
from src.dgp import dgp_ad
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;dgp = dgp_ad()
df = dgp.generate_data()
df.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;male&lt;/th&gt;
      &lt;th&gt;black&lt;/th&gt;
      &lt;th&gt;age&lt;/th&gt;
      &lt;th&gt;educ&lt;/th&gt;
      &lt;th&gt;ad&lt;/th&gt;
      &lt;th&gt;revenue&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;55.0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;-0.327221&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;47.0&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;0.659393&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;31.0&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;True&lt;/td&gt;
      &lt;td&gt;2.805178&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;51.0&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;-0.508548&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;48.0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;True&lt;/td&gt;
      &lt;td&gt;0.762280&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;We have information on the number of pages visited in the previous month, whether the user is located in the US, whether it connects by mobile and the revenue pre-intervention.&lt;/p&gt;
&lt;p&gt;We are going to use the &lt;a href=&#34;https://econml.azurewebsites.net&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;econml&lt;/code&gt;&lt;/a&gt; library to estimate the treatment effects. First, we use the &lt;code&gt;DRLearner&lt;/code&gt; library to estimate heterogeneous treatment effects using a double robust estimator. We can specify both the &lt;code&gt;model_propensity&lt;/code&gt; for $e(x)$ and the &lt;code&gt;model_regression&lt;/code&gt; for $\mu(x)$.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from econml.dr import DRLearner

model = DRLearner(random_state=1).fit(Y=df[dgp.Y], T=df[dgp.T], X=df[dgp.X]);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can plot a visual representation of the treatment effect heterogeneity using the &lt;code&gt;SingleTreePolicyInterpreter&lt;/code&gt; function, which infers a tree representation of the treatment effects learned from another model.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from econml.cate_interpreter import SingleTreeCateInterpreter

SingleTreeCateInterpreter(max_depth=2, random_state=1).interpret(model, X=df[dgp.X]).plot(feature_names=dgp.X)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/policy_learning_39_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;It seems that the most relevant dimension of treatment heterogeneity is &lt;code&gt;education&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;We can now use policy learning to estimate a treatment policy. We use the &lt;code&gt;DRPolicyTree&lt;/code&gt; from the &lt;code&gt;econml&lt;/code&gt; package.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from econml.policy import DRPolicyTree

policy = DRPolicyTree(random_state=1, max_depth=2).fit(Y=df[dgp.Y], T=df[dgp.T], X=df[dgp.X])
policy.plot(feature_names=dgp.X)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/policy_learning_41_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;We will now assume that the treatment is costly.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;cost = 1
policy = DRPolicyTree(random_state=1, max_depth=2).fit(Y=df[dgp.Y]-cost*df[dgp.T], T=df[dgp.T], X=df[dgp.X])
policy.plot(feature_names=dgp.X)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/policy_learning_43_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;As we can see, the model decides to use race to discriminate treatment. However, let&amp;rsquo;s now suppose we cannot discriminate on race and gender.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;X_short = [&#39;age&#39;, &#39;educ&#39;]
policy = DRPolicyTree(random_state=1, max_depth=2).fit(Y=df[dgp.Y]-cost*df[dgp.T], T=df[dgp.T], X=df[X_short])
policy.plot(feature_names=X_short)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/policy_learning_45_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;In this case, the model uses education instead of race in order to assign treatment.&lt;/p&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://onlinelibrary.wiley.com/doi/abs/10.3982/ECTA13288&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Who Should Be Treated? Empirical Welfare Maximization Methods for Treatment Choice&lt;/a&gt; (2018) by Kitagawa and Tetenov&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://ideas.repec.org/p/ecl/stabus/3506.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Efficient Policy Learning&lt;/a&gt; (2017) by Athey and Wager&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=YQXRwvFQOPk&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Policy Learning&lt;/a&gt; video lecture by Stefan Wager (Stanford)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/microsoft/EconML/blob/main/notebooks/CustomerScenarios/Case%20Study%20-%20Customer%20Segmentation%20at%20An%20Online%20Media%20Company.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Customer Segmentation&lt;/a&gt; case study by EconML&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Propensity Score Matching</title>
      <link>https://matteocourthoud.github.io/post/propensity_score/</link>
      <pubDate>Fri, 15 Apr 2022 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/post/propensity_score/</guid>
      <description>&lt;p&gt;In this tutorial, we are going to see how to estimate causal effects when the treatment is not &lt;em&gt;unconditionally&lt;/em&gt; randomly assigned, but we need to condition on observable features in order to assume treatment exogeneity. This might happen either when an experiment is stratified or in observational studies.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Requisites&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;For this tutorial, I assume you are familiar with the following concepts:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Rubin&amp;rsquo;s potential outcome framework&lt;/li&gt;
&lt;li&gt;Ordinary least squares regression&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Academic Application&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;As an academic application, we are going to replicate &lt;a href=&#34;https://www.jstor.org/stable/2937954&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Evaluating the Econometric Evaluations of Training Programs with Experimental Data&lt;/a&gt; (1986) by Lalonde and the followup paper &lt;a href=&#34;https://www.tandfonline.com/doi/abs/10.1080/01621459.1999.10473858&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Causal Effects in Non-Experimental Studies: Reevaluating the Evaluation of Training Programs&lt;/a&gt; (1999) by Dahejia and Wahba. These papers study a randomized intervention providing work experienced to improve labor market outcomes.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Business Case&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;TBD&lt;/p&gt;
&lt;h2 id=&#34;setting&#34;&gt;Setting&lt;/h2&gt;
&lt;p&gt;We assume that for a set of i.i.d. subjects $i = 1, &amp;hellip;, n$ we observed a tuple $(X_i, D_i, Y_i)$ comprised of&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;a feature vector $X_i \in \mathbb R^n$&lt;/li&gt;
&lt;li&gt;a treatment assignment $D_i \in \lbrace 0, 1 \rbrace$&lt;/li&gt;
&lt;li&gt;a response $Y_i \in \mathbb R$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Assumption 1 : unconfoundedness&lt;/strong&gt; (or ignorability, or selection on observables, or conditional independence)&lt;/p&gt;
&lt;p&gt;$$
\big \lbrace Y_i^{(1)} , Y_i^{(0)} \big \rbrace \ \perp \ D_i \ | \ X_i
$$&lt;/p&gt;
&lt;p&gt;i.e. conditional on observable characteristics $X$, the treatment assignment $T$ is as good as random. What this assumption rules out is &lt;em&gt;selection on unobservables&lt;/em&gt;. Moreover, it&amp;rsquo;s &lt;em&gt;untestable&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Assumption 2: overlap&lt;/strong&gt; (or common support)&lt;/p&gt;
&lt;p&gt;$$
\exists \eta &amp;gt; 0 \ : \ \eta \leq \mathbb E \left[ D_i = 1 \ \big | \ X_i = x \right] \leq 1-\eta
$$&lt;/p&gt;
&lt;p&gt;i.e. no observation is deterministically assigned to the treatment or control group. We need this assumption for counterfactual statements to make sense. If some observations had zero probability of (not) being treated, it would make no sense to try to estimate their counterfactual outcome in case they would have (not) being treated. Also this assumption is &lt;em&gt;untestable&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Assumption 3: stable unit treatment value (SUTVA)&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;$$
Y_i^{(D_i)} \perp D_j \quad \forall j \neq i
$$&lt;/p&gt;
&lt;p&gt;i.e. the potential outcome of one individual is independent from the treatment status of any other individual. Common &lt;em&gt;violations&lt;/em&gt; of this assumption include&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;general equilibrium effects&lt;/li&gt;
&lt;li&gt;spillover effects&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This assumption is &lt;em&gt;untestable&lt;/em&gt;.&lt;/p&gt;
&lt;h2 id=&#34;propensity-scores&#34;&gt;Propensity Scores&lt;/h2&gt;
&lt;h3 id=&#34;exogenous-treatment&#34;&gt;Exogenous Treatment&lt;/h3&gt;
&lt;p&gt;The &lt;strong&gt;fundamental problem of causal inference&lt;/strong&gt; is that we do not observe counterfactual outcomes, i.e. we do not observe what would have happened to treated units if they had not received the treatment and viceversa.&lt;/p&gt;
&lt;p&gt;If treatment is exogenous, we know that the difference in means identifies the average treatment effect $\mathbb E[\tau]$.&lt;/p&gt;
&lt;p&gt;$$
\mathbb E[\tau] = \mathbb E \big[ Y_i \ \big| \ D_i = 1 \big] - \mathbb E \big[ Y_i \ \big| \ D_i = 0 \big] = \mathbb E \big[ Y_{i}^{(1)} - Y_{i}^{(0)} \big]
$$&lt;/p&gt;
&lt;p&gt;Therefore, we can build an unbiased estimator of the average treatment effect as the empirical counterpart of the expression above&lt;/p&gt;
&lt;p&gt;$$
\hat \tau(Y, D) = \frac{1}{n} \sum_{i=1}^{n} \big( D_i Y_i - (1-D_i) Y_i \big)
$$&lt;/p&gt;
&lt;p&gt;In case treatment is not randomly assigned, we use the Thompson Horowitz (1952) estimator&lt;/p&gt;
&lt;p&gt;$$
\hat \tau(Y, D) = \frac{1}{n} \sum_{i=1}^{n} \left( \frac{D_i Y_i}{\pi_{i}} - \frac{(1-D_i) Y_i}{1 - \pi_{i}} \right)
$$&lt;/p&gt;
&lt;p&gt;where $\pi_{i} = \Pr(D_i=1)$ is the probability of being treated, also known as &lt;strong&gt;propensity score&lt;/strong&gt;. Sometimes the propensity score is known, for example when treatment is stratified. However, in general, it is not.&lt;/p&gt;
&lt;h3 id=&#34;conditionally-exogenous-treatment&#34;&gt;Conditionally Exogenous Treatment&lt;/h3&gt;
&lt;p&gt;In many cases and especially in observational studies, treatment $D$ is not unconditionally exogenous, but it&amp;rsquo;s exogenous only after we condition on some characteristic $X$. If these characteristics are observables, we have the &lt;strong&gt;unconfoundedness&lt;/strong&gt; assumption.&lt;/p&gt;
&lt;p&gt;Under unconfoundedness, we can still identify the average treatment effect, as a &lt;em&gt;conditional&lt;/em&gt; difference in means:&lt;/p&gt;
&lt;p&gt;$$
\mathbb E[\tau] = \mathbb E \big[ Y_{i}^{(1)} - Y_{i}^{(0)} \ \big| \ X_i \big]
$$&lt;/p&gt;
&lt;p&gt;The main problem is that we need to condition of the observables that actually make the unconfoundedness assumption hold. This might be tricky in two cases:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;when we have many observables&lt;/li&gt;
&lt;li&gt;when we do not know the functional form of the observables that we need to condition on&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The main contribution of Rosenbaum and Rubin (1983) is to show that if &lt;strong&gt;unconfoundedness&lt;/strong&gt; holds, then&lt;/p&gt;
&lt;p&gt;$$
\big \lbrace Y_i^{(1)} , Y_i^{(0)} \big \rbrace \ \perp \ D_i \ | \ \pi(X_i)
$$&lt;/p&gt;
&lt;p&gt;i.e. you only need to condition on $\pi(X)$ in order to recover the average treatment effect.&lt;/p&gt;
&lt;p&gt;$$
\mathbb E[\tau] = \mathbb E \big[ Y_{i}^{(1)} - Y_{i}^{(0)} \ \big| \ \pi(X_i) \big]
$$&lt;/p&gt;
&lt;p&gt;This implies the following &lt;strong&gt;inverse propensity-weighted&lt;/strong&gt; estimator:&lt;/p&gt;
&lt;p&gt;$$
\hat \tau^{IPW} = \frac{1}{n} \sum_{i=1}^{n} \left( \frac{D_i Y_i}{\hat \pi(X_i)} - \frac{(1-D_i) Y_i}{1 - \hat \pi(X_i)} \right)
$$&lt;/p&gt;
&lt;p&gt;which, under &lt;em&gt;unconfoundedness&lt;/em&gt; is an &lt;strong&gt;unbiased&lt;/strong&gt; estimator of the average treatment effect, $\mathbb E \left[\hat \tau^{IPW} \right] = \tau$.&lt;/p&gt;
&lt;p&gt;This is a very practically relevant result since it tells us that we need to condition on a single variable instead of a potentially infinite dimensional array. The only thing we need to do is to estimate $\pi(X_i)$.&lt;/p&gt;
&lt;h3 id=&#34;comments&#34;&gt;Comments&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Actual vs Estimated Scores&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Hirano and Ridder (2002) show that even when you know the true propensity score $\pi(X)$, it&amp;rsquo;s better to plug in the estimated propensity score $\hat \pi(X)$. Why? The idea is that the deviation between the actual and the estimated propensity score is providing some additional information. Therefore, it is best to use the actual fraction of treated rather than the theoretical one.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Propensity Scores and Regression&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;What is the difference between running a regression with controls vs doing propensity score matching?&lt;/p&gt;
&lt;p&gt;Aranow and Miller (2015) investigate this comparison in depth. First of all, whenever you are inserting &lt;strong&gt;control variables&lt;/strong&gt; in a regression, you are implicitly thinking about propensity scores. Both approaches are implicitly estimating counterfactual outcomes. Usually OLS extrapolates further away from the actual support than propensity score does.&lt;/p&gt;
&lt;p&gt;In the tweet (and its comments) below you can find further discussion and comments.&lt;/p&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;Thank you for tolerating such a vague poll question. &lt;br&gt;&lt;br&gt;Let me explain why I think this is a useful thing to bring front and certain, and highlight what I think is a flaw in how much of econometrics is taught, currently. &lt;br&gt;&lt;br&gt;1/n &lt;a href=&#34;https://t.co/Wm2jFereYO&#34;&gt;pic.twitter.com/Wm2jFereYO&lt;/a&gt;&lt;/p&gt;&amp;mdash; Paul Goldsmith-Pinkham (@paulgp) &lt;a href=&#34;https://twitter.com/paulgp/status/1470787510091632651?ref_src=twsrc%5Etfw&#34;&gt;December 14, 2021&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;

&lt;h2 id=&#34;academic-application&#34;&gt;Academic Application&lt;/h2&gt;
&lt;p&gt;As an academic application, we are going to replicate &lt;a href=&#34;https://www.tandfonline.com/doi/abs/10.1080/01621459.1999.10473858&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Causal Effects in Non-Experimental Studies: Reevaluating the Evaluation of Training Programs&lt;/a&gt; (1999) by Dahejia and Wahba.&lt;/p&gt;
&lt;p&gt;This study builds on a previous study: &lt;a href=&#34;https://www.jstor.org/stable/2937954&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Evaluating the Econometric Evaluations of Training Programs with Experimental Data&lt;/a&gt; (1986) by Lalonde. In this study, the author compares observational and experimental methods. In particular, he studies an experimental intervention called the NSW (National Supported Work demonstration). The NSW is a temporary training program to give work experience to unemployed people.&lt;/p&gt;
&lt;p&gt;The exogenous variation allows us to estimate the treatment effect as a difference in means. The author then asks: what if we didn&amp;rsquo;t have access to an experiment? In particular, what if we did not have information on the control group? He takes a sample of untreated people from the PSID panel and use them as a control group.&lt;/p&gt;
&lt;h3 id=&#34;experimental-data&#34;&gt;Experimental Data&lt;/h3&gt;
&lt;p&gt;Let&amp;rsquo;s start by loading the NSW data.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;%matplotlib inline
%config InlineBackend.figure_format = &#39;retina&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from src.utils import *
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_nsw = pd.read_csv(&#39;data/l86_nsw.csv&#39;)
df_nsw.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;treat&lt;/th&gt;
      &lt;th&gt;age&lt;/th&gt;
      &lt;th&gt;educ&lt;/th&gt;
      &lt;th&gt;black&lt;/th&gt;
      &lt;th&gt;hisp&lt;/th&gt;
      &lt;th&gt;marr&lt;/th&gt;
      &lt;th&gt;nodegree&lt;/th&gt;
      &lt;th&gt;re74&lt;/th&gt;
      &lt;th&gt;re75&lt;/th&gt;
      &lt;th&gt;re78&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;37&lt;/td&gt;
      &lt;td&gt;11&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;9930.045898&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;22&lt;/td&gt;
      &lt;td&gt;9&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;3595.894043&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;30&lt;/td&gt;
      &lt;td&gt;12&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;24909.449219&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;27&lt;/td&gt;
      &lt;td&gt;11&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;7506.145996&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;33&lt;/td&gt;
      &lt;td&gt;8&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;289.789886&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;The treatment variable is &lt;code&gt;treat&lt;/code&gt; and the outcome of interest is &lt;code&gt;re78&lt;/code&gt;, the income in 1978. We also have access to a bunch of covariates.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;y = &#39;re78&#39;
T = &#39;treat&#39;
X = df_nsw.columns[2:9]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Was there selection on observables? Let&amp;rsquo;s summarize the data, according to treatment status.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_nsw.groupby(&#39;treat&#39;).agg([&#39;mean&#39;, &#39;std&#39;]).T.unstack(1)
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead tr th {
    text-align: left;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;treat&lt;/th&gt;
      &lt;th colspan=&#34;2&#34; halign=&#34;left&#34;&gt;0&lt;/th&gt;
      &lt;th colspan=&#34;2&#34; halign=&#34;left&#34;&gt;1&lt;/th&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;mean&lt;/th&gt;
      &lt;th&gt;std&lt;/th&gt;
      &lt;th&gt;mean&lt;/th&gt;
      &lt;th&gt;std&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;age&lt;/th&gt;
      &lt;td&gt;25.053846&lt;/td&gt;
      &lt;td&gt;7.057745&lt;/td&gt;
      &lt;td&gt;25.816216&lt;/td&gt;
      &lt;td&gt;7.155019&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;educ&lt;/th&gt;
      &lt;td&gt;10.088462&lt;/td&gt;
      &lt;td&gt;1.614325&lt;/td&gt;
      &lt;td&gt;10.345946&lt;/td&gt;
      &lt;td&gt;2.010650&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;black&lt;/th&gt;
      &lt;td&gt;0.826923&lt;/td&gt;
      &lt;td&gt;0.379043&lt;/td&gt;
      &lt;td&gt;0.843243&lt;/td&gt;
      &lt;td&gt;0.364558&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;hisp&lt;/th&gt;
      &lt;td&gt;0.107692&lt;/td&gt;
      &lt;td&gt;0.310589&lt;/td&gt;
      &lt;td&gt;0.059459&lt;/td&gt;
      &lt;td&gt;0.237124&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;marr&lt;/th&gt;
      &lt;td&gt;0.153846&lt;/td&gt;
      &lt;td&gt;0.361497&lt;/td&gt;
      &lt;td&gt;0.189189&lt;/td&gt;
      &lt;td&gt;0.392722&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;nodegree&lt;/th&gt;
      &lt;td&gt;0.834615&lt;/td&gt;
      &lt;td&gt;0.372244&lt;/td&gt;
      &lt;td&gt;0.708108&lt;/td&gt;
      &lt;td&gt;0.455867&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;re74&lt;/th&gt;
      &lt;td&gt;2107.026651&lt;/td&gt;
      &lt;td&gt;5687.905639&lt;/td&gt;
      &lt;td&gt;2095.573693&lt;/td&gt;
      &lt;td&gt;4886.620354&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;re75&lt;/th&gt;
      &lt;td&gt;1266.909015&lt;/td&gt;
      &lt;td&gt;3102.982088&lt;/td&gt;
      &lt;td&gt;1532.055313&lt;/td&gt;
      &lt;td&gt;3219.250879&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;re78&lt;/th&gt;
      &lt;td&gt;4554.801120&lt;/td&gt;
      &lt;td&gt;5483.836001&lt;/td&gt;
      &lt;td&gt;6349.143502&lt;/td&gt;
      &lt;td&gt;7867.402183&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;It seems that covariates are balanced across treatment arms. Nothing seems to point towards selection on observables. Therefore, we can compute the average treatment effect as a simple difference in means&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_nsw.loc[df_nsw[T]==1, y].mean() - df_nsw.loc[df_nsw[T]==0, y].mean()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;1794.3423818501024
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Or equivalently in a regression&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;est = smf.ols(&#39;re78 ~ treat&#39;, df_nsw).fit()
est.summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
      &lt;td&gt;&lt;/td&gt;         &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt; &lt;td&gt; 4554.8011&lt;/td&gt; &lt;td&gt;  408.046&lt;/td&gt; &lt;td&gt;   11.162&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt; 3752.855&lt;/td&gt; &lt;td&gt; 5356.747&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;treat&lt;/th&gt;     &lt;td&gt; 1794.3424&lt;/td&gt; &lt;td&gt;  632.853&lt;/td&gt; &lt;td&gt;    2.835&lt;/td&gt; &lt;td&gt; 0.005&lt;/td&gt; &lt;td&gt;  550.574&lt;/td&gt; &lt;td&gt; 3038.110&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;It looks like the effect is positive and significant.&lt;/p&gt;
&lt;h3 id=&#34;observational-data&#34;&gt;Observational Data&lt;/h3&gt;
&lt;p&gt;Let&amp;rsquo;s now load a different dataset in which we have replaced the true control units with observations from the PSID sample.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_psid = pd.read_csv(&#39;data/l86_psid.csv&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Is this dataset balanced?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_psid.groupby(&#39;treat&#39;).agg([&#39;mean&#39;, &#39;std&#39;]).T.unstack(1)
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead tr th {
    text-align: left;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;treat&lt;/th&gt;
      &lt;th colspan=&#34;2&#34; halign=&#34;left&#34;&gt;0&lt;/th&gt;
      &lt;th colspan=&#34;2&#34; halign=&#34;left&#34;&gt;1&lt;/th&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;mean&lt;/th&gt;
      &lt;th&gt;std&lt;/th&gt;
      &lt;th&gt;mean&lt;/th&gt;
      &lt;th&gt;std&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;age&lt;/th&gt;
      &lt;td&gt;36.094862&lt;/td&gt;
      &lt;td&gt;12.081030&lt;/td&gt;
      &lt;td&gt;25.816216&lt;/td&gt;
      &lt;td&gt;7.155019&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;educ&lt;/th&gt;
      &lt;td&gt;10.766798&lt;/td&gt;
      &lt;td&gt;3.176827&lt;/td&gt;
      &lt;td&gt;10.345946&lt;/td&gt;
      &lt;td&gt;2.010650&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;black&lt;/th&gt;
      &lt;td&gt;0.391304&lt;/td&gt;
      &lt;td&gt;0.489010&lt;/td&gt;
      &lt;td&gt;0.843243&lt;/td&gt;
      &lt;td&gt;0.364558&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;hisp&lt;/th&gt;
      &lt;td&gt;0.067194&lt;/td&gt;
      &lt;td&gt;0.250853&lt;/td&gt;
      &lt;td&gt;0.059459&lt;/td&gt;
      &lt;td&gt;0.237124&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;marr&lt;/th&gt;
      &lt;td&gt;0.735178&lt;/td&gt;
      &lt;td&gt;0.442113&lt;/td&gt;
      &lt;td&gt;0.189189&lt;/td&gt;
      &lt;td&gt;0.392722&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;nodegree&lt;/th&gt;
      &lt;td&gt;0.486166&lt;/td&gt;
      &lt;td&gt;0.500799&lt;/td&gt;
      &lt;td&gt;0.708108&lt;/td&gt;
      &lt;td&gt;0.455867&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;re74&lt;/th&gt;
      &lt;td&gt;11027.303390&lt;/td&gt;
      &lt;td&gt;10814.670751&lt;/td&gt;
      &lt;td&gt;2095.573693&lt;/td&gt;
      &lt;td&gt;4886.620354&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;re75&lt;/th&gt;
      &lt;td&gt;7569.222058&lt;/td&gt;
      &lt;td&gt;9041.944403&lt;/td&gt;
      &lt;td&gt;1532.055313&lt;/td&gt;
      &lt;td&gt;3219.250879&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;re78&lt;/th&gt;
      &lt;td&gt;9995.949977&lt;/td&gt;
      &lt;td&gt;11184.450050&lt;/td&gt;
      &lt;td&gt;6349.143502&lt;/td&gt;
      &lt;td&gt;7867.402183&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;People in the PSID control group are older, more educated, white, married and generally have considerably higher pre-intervention earnings (&lt;code&gt;re74&lt;/code&gt;). This makes sense since the people selected for the NSW program are people that are younger, less experienced and unemployed.&lt;/p&gt;
&lt;p&gt;Lalonde (1986) argues in favor of experimental approaches by showing that using a non-experimental setting, one would not be able to estimate the true treatment effect. Actually, one could even get statistically significant results of the opposite sign.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s repeat the regression exercise for the PSID data.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;smf.ols(&#39;re78 ~ treat&#39;, df_psid).fit().summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
      &lt;td&gt;&lt;/td&gt;         &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt; &lt;td&gt; 9995.9500&lt;/td&gt; &lt;td&gt;  623.715&lt;/td&gt; &lt;td&gt;   16.026&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt; 8770.089&lt;/td&gt; &lt;td&gt; 1.12e+04&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;treat&lt;/th&gt;     &lt;td&gt;-3646.8065&lt;/td&gt; &lt;td&gt;  959.704&lt;/td&gt; &lt;td&gt;   -3.800&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;-5533.027&lt;/td&gt; &lt;td&gt;-1760.586&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;The estimated coefficient is negative and significant. The conclusion from Lalonde (1986) is&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;This comparison shows that many of the econometric procedures d not replicate the experimentally determined results&amp;rdquo;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Dahejia and Wahba (1999) argue that with appropriate matching one would still be able to get a relatively precise estimate of the treatment effect. In particular, the argue in favor of controlling for pre-intervention income, &lt;code&gt;re74&lt;/code&gt; and &lt;code&gt;re75&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s just linearly insert the control variables in the regression.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;smf.ols(&#39;re78 ~ treat + &#39; + &#39; + &#39;.join(X), df_psid).fit().summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
      &lt;td&gt;&lt;/td&gt;         &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt; &lt;td&gt;-1089.9263&lt;/td&gt; &lt;td&gt; 2913.224&lt;/td&gt; &lt;td&gt;   -0.374&lt;/td&gt; &lt;td&gt; 0.708&lt;/td&gt; &lt;td&gt;-6815.894&lt;/td&gt; &lt;td&gt; 4636.042&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;treat&lt;/th&gt;     &lt;td&gt; 2642.1456&lt;/td&gt; &lt;td&gt; 1039.655&lt;/td&gt; &lt;td&gt;    2.541&lt;/td&gt; &lt;td&gt; 0.011&lt;/td&gt; &lt;td&gt;  598.694&lt;/td&gt; &lt;td&gt; 4685.597&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;educ&lt;/th&gt;      &lt;td&gt;  521.5869&lt;/td&gt; &lt;td&gt;  208.751&lt;/td&gt; &lt;td&gt;    2.499&lt;/td&gt; &lt;td&gt; 0.013&lt;/td&gt; &lt;td&gt;  111.284&lt;/td&gt; &lt;td&gt;  931.890&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;black&lt;/th&gt;     &lt;td&gt;-1026.6762&lt;/td&gt; &lt;td&gt; 1006.433&lt;/td&gt; &lt;td&gt;   -1.020&lt;/td&gt; &lt;td&gt; 0.308&lt;/td&gt; &lt;td&gt;-3004.830&lt;/td&gt; &lt;td&gt;  951.478&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;hisp&lt;/th&gt;      &lt;td&gt; -903.1023&lt;/td&gt; &lt;td&gt; 1726.419&lt;/td&gt; &lt;td&gt;   -0.523&lt;/td&gt; &lt;td&gt; 0.601&lt;/td&gt; &lt;td&gt;-4296.394&lt;/td&gt; &lt;td&gt; 2490.189&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;marr&lt;/th&gt;      &lt;td&gt; 1026.6143&lt;/td&gt; &lt;td&gt;  943.788&lt;/td&gt; &lt;td&gt;    1.088&lt;/td&gt; &lt;td&gt; 0.277&lt;/td&gt; &lt;td&gt; -828.410&lt;/td&gt; &lt;td&gt; 2881.639&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;nodegree&lt;/th&gt;  &lt;td&gt;-1469.3712&lt;/td&gt; &lt;td&gt; 1166.114&lt;/td&gt; &lt;td&gt;   -1.260&lt;/td&gt; &lt;td&gt; 0.208&lt;/td&gt; &lt;td&gt;-3761.379&lt;/td&gt; &lt;td&gt;  822.637&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;re74&lt;/th&gt;      &lt;td&gt;    0.1928&lt;/td&gt; &lt;td&gt;    0.058&lt;/td&gt; &lt;td&gt;    3.329&lt;/td&gt; &lt;td&gt; 0.001&lt;/td&gt; &lt;td&gt;    0.079&lt;/td&gt; &lt;td&gt;    0.307&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;re75&lt;/th&gt;      &lt;td&gt;    0.4976&lt;/td&gt; &lt;td&gt;    0.070&lt;/td&gt; &lt;td&gt;    7.068&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    0.359&lt;/td&gt; &lt;td&gt;    0.636&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;The treatment effect is now positive, borderline significant, and close to the experimental estimate of $1794$$. Moreover, it&amp;rsquo;s hard to tell whether this is the correct functional form for the control variables.&lt;/p&gt;
&lt;h3 id=&#34;inverse-propensity-score-weighting&#34;&gt;Inverse propensity score weighting&lt;/h3&gt;
&lt;p&gt;Another option is to use &lt;strong&gt;inverse propensity score weighting&lt;/strong&gt;. First, we need to estimate the treatment probability. Let&amp;rsquo;s start with a very simple standard model to predict binary outcomes.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from sklearn.linear_model import LogisticRegressionCV

pi = LogisticRegressionCV().fit(y=df_psid[T], X=df_psid[X])
df_psid[&#39;pscore&#39;] = pi.predict_proba(df_psid[X])[:,1]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;How does the distribution of the propensity scores look like?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sns.histplot(data=df_psid, x=&#39;pscore&#39;, hue=T, bins=20)\
.set(title=&#39;Distribution of propensity scores, PSID data&#39;, xlabel=&#39;&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/propensity_score_41_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;It seems that indeed we predict higher propensity scores for treated people, and viceversa, indicating a strong selection on observable. However, there is also a considerable amount of overlap.&lt;/p&gt;
&lt;p&gt;We can now estimate the treatment effect by weighting by the inverse of the propensity score. First, let&amp;rsquo;s exclude observations with a very extreme predicted score.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_psid1 = df_psid[(df_psid[&#39;pscore&#39;]&amp;lt;0.9) &amp;amp; (df_psid[&#39;pscore&#39;]&amp;gt;0.1)]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we can need to construct the weights.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_psid1[&#39;weight&#39;] = df_psid1[&#39;treat&#39;] / df_psid1[&#39;pscore&#39;] + (1-df_psid1[&#39;treat&#39;]) / (1-df_psid1[&#39;pscore&#39;])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally, we run a weighted regression of income on the treatment program.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;est = smf.wls(&#39;re78 ~ treat&#39;, df_psid1, weights=df_psid1[&#39;weight&#39;]).fit()
est.summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
      &lt;td&gt;&lt;/td&gt;         &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt; &lt;td&gt; 4038.1507&lt;/td&gt; &lt;td&gt;  512.268&lt;/td&gt; &lt;td&gt;    7.883&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt; 3030.227&lt;/td&gt; &lt;td&gt; 5046.074&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;treat&lt;/th&gt;     &lt;td&gt; 2166.8750&lt;/td&gt; &lt;td&gt;  730.660&lt;/td&gt; &lt;td&gt;    2.966&lt;/td&gt; &lt;td&gt; 0.003&lt;/td&gt; &lt;td&gt;  729.250&lt;/td&gt; &lt;td&gt; 3604.500&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;The effect is positive, statistically significant and very close to the experimental estimate of $1794$$.&lt;/p&gt;
&lt;p&gt;What would have been the propensity scores if we had used the NSW experimental sample? If it&amp;rsquo;s a well done experiment with a sufficiently large sample, we would expect the propensity scores to concentrate around the percentage of people treated, $0.41$ in our data.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;pi = LogisticRegressionCV().fit(y=df_nsw[T], X=df_nsw[X])
df_nsw[&#39;pscore&#39;] = pi.predict_proba(df_nsw[X])[:,1]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sns.histplot(data=df_nsw, x=&#39;pscore&#39;, hue=T, bins=20)\
.set(title=&#39;Distribution of propensity scores, NSW data&#39;, xlabel=&#39;&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/propensity_score_50_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Indeed, now the distribution of the p-scores is concentrated around the treatment frequency in the data. Remarkably, the standard deviation is extremely tight.&lt;/p&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://academic.oup.com/biomet/article/70/1/41/240879&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The central role of the propensity score in observational studies for causal effects&lt;/a&gt; (1983) by Rosenbaum and Rubin&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=8gWctYvRzk4&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Propensity Scores&lt;/a&gt; video lecture by Paul Goldsmith-Pinkham (Yale)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=m3Y8heXoDxE&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Propensity Scores&lt;/a&gt; video lecture by Stefan Wager (Stanford)&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Regression Discontinuity</title>
      <link>https://matteocourthoud.github.io/post/regression_discontinuity/</link>
      <pubDate>Fri, 15 Apr 2022 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/post/regression_discontinuity/</guid>
      <description>&lt;p&gt;In this tutorial, we are going to see how to estimate causal effects when treatment assignment is not random, but determined by a &lt;em&gt;forcing variable&lt;/em&gt; such as a test or a requirement. In this case, we can get a local estimate of the treatment effect by comparing units just above and just below the threshold by assuming that there is no sorting/gaming around it.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Requisites&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;For this tutorial, I assume you are familiar with the following concepts:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Rubin&amp;rsquo;s potential outcome framework&lt;/li&gt;
&lt;li&gt;Ordinary least squares regression&lt;/li&gt;
&lt;li&gt;Non-parametric regression&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://matteocourthoud.github.io/post/iv/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Instrumental variables&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Academic Application&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;As an academic application, we are going to replicate &lt;a href=&#34;https://academic.oup.com/qje/article/119/3/807/1938834&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Do voters affect or elect policies? Evidence from the US House&lt;/a&gt; (2004) by Lee, Moretti, Butler. The authors study whether electoral strength has an effect on policies. To identify the effect, they the advantage that is given by incumbency status, and the quasi-exogeneity given by close elections.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Business Case&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;TBD&lt;/p&gt;
&lt;h2 id=&#34;setting&#34;&gt;Setting&lt;/h2&gt;
&lt;p&gt;We assume that for a set of i.i.d. subjects $i = 1, &amp;hellip;, n$ we observed a tuple $(X_i, D_i, Y_i, Z_i)$ comprised of&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;a feature vector $X_i \in \mathbb R^n$&lt;/li&gt;
&lt;li&gt;a treatment assignment $D_i \in \lbrace 0, 1 \rbrace$&lt;/li&gt;
&lt;li&gt;a response $Y_i \in \mathbb R$
&lt;ul&gt;
&lt;li&gt;outcome of interest that depends on both $X_i$ and $D_i$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;a &lt;strong&gt;forcing variable&lt;/strong&gt; $Z_i \in \mathbb R$
&lt;ul&gt;
&lt;li&gt;variable that determines treatment assignment $D_i$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We normalize the forcing variable $Z_i$ such that $Z_i=0$ corresponds to the cutoff for treatment assignment. We will distinguish two cases for the effect of $Z_i$ on $D_i$:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Sharp RD&lt;/strong&gt;: $D_i = (Z_i \geq 0)$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;treatment is exactly determined by the cutoff&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Fuzzy RD&lt;/strong&gt;: $\lim_{z \to 0_{-}} \mathbb E[D_i | Z_i=z] \neq \lim_{z \to 0_{+}} \mathbb E[D_i | Z_i=z]$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;treatment probability changes at the cutoff&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;Assumption 1 : CE smoothness&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Assumption 2: no sorting&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id=&#34;regression-discontinuity&#34;&gt;Regression Discontinuity&lt;/h2&gt;
&lt;p&gt;The key behind regression discontinuity is what is called a &lt;strong&gt;forcing&lt;/strong&gt; variable that determines treatment assignment. Common examples include test scores for university enrollment (you need a certain test score to get access university) or income for some policy eligibility (you need to be below a certain income threshold to be eligible for a subsidy).&lt;/p&gt;
&lt;p&gt;Clearly, in this setting, treatment is not exogenous. However, the &lt;strong&gt;idea&lt;/strong&gt; behind regression discontinuity is that units &lt;em&gt;sufficiently&lt;/em&gt; close to the discontinuity $Z_i=0$ are &lt;em&gt;sufficiently&lt;/em&gt; similar so that we can attribute differences in the outcome $Y_i$ to the treatment $T_i$.&lt;/p&gt;
&lt;p&gt;What does &lt;em&gt;sufficiently&lt;/em&gt; exactly mean?&lt;/p&gt;
&lt;p&gt;In practice, we are assuming a certain degree of &lt;strong&gt;smoothness&lt;/strong&gt; of the conditional expectation function $\mathbb E[D_i | Z_i=z]$. If this assumption holds, we can estimate the &lt;strong&gt;local average treatment effect&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;$$
\tau^{LATE} = \lim_{z \to 0_{+}} \mathbb E[Y_i | Z_i=z] - \lim_{z \to 0_{-}} \mathbb E[Y_i | Z_i=z] = \mathbb E \big[ Y_{i}^{(1)} - Y_{i}^{(0)} | Z_i=0 \big]
$$&lt;/p&gt;
&lt;p&gt;Note that this is the average treatment effect for a very narrow set of individuals: those that are extremely close to the cutoff.&lt;/p&gt;
&lt;h3 id=&#34;data-challenge&#34;&gt;Data Challenge&lt;/h3&gt;
&lt;p&gt;Regression discontinuity design is a particularly &lt;strong&gt;data hungry&lt;/strong&gt; procedure. In fact, we need to&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;have a very good flexible approximation of the conditional expectation of the outcome $Y_i$ at the cutoff $Z_i=0$&lt;/li&gt;
&lt;li&gt;while also accounting for the effect of the forcing variable $Z$ on the outcome $Y$&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;If we knew the functional form of $\mathbb E[Y_i | Z_i]$, it would be easy.&lt;/p&gt;
&lt;h3 id=&#34;mccrary-test&#34;&gt;McCrary Test&lt;/h3&gt;
&lt;h3 id=&#34;regression-kink-design&#34;&gt;Regression Kink Design&lt;/h3&gt;
&lt;h2 id=&#34;academic-application&#34;&gt;Academic Application&lt;/h2&gt;
&lt;p&gt;As an academic application, we are going to replicate &lt;a href=&#34;https://academic.oup.com/qje/article/119/3/807/1938834&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Do voters affect or elect policies? Evidence from the US House&lt;/a&gt; (2004) by Lee, Moretti, Butler. The authors study whether electoral strength has an effect on policies. To identify the effect, they the advantage that is given by incumbency status, and the quasi-exogeneity given by close elections.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;%matplotlib inline
%config InlineBackend.figure_format = &#39;retina&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from src.utils import *
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df = sm.datasets.get_rdataset(&#39;close_elections_lmb&#39;, package=&#39;causaldata&#39;).data
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df = pd.read_csv(&#39;data/l08.csv&#39;)
df.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;state&lt;/th&gt;
      &lt;th&gt;district&lt;/th&gt;
      &lt;th&gt;id&lt;/th&gt;
      &lt;th&gt;score&lt;/th&gt;
      &lt;th&gt;year&lt;/th&gt;
      &lt;th&gt;demvoteshare&lt;/th&gt;
      &lt;th&gt;democrat&lt;/th&gt;
      &lt;th&gt;lagdemocrat&lt;/th&gt;
      &lt;th&gt;lagdemvoteshare&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;64.339996&lt;/td&gt;
      &lt;td&gt;1948&lt;/td&gt;
      &lt;td&gt;0.553026&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0.469256&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;60.279999&lt;/td&gt;
      &lt;td&gt;1948&lt;/td&gt;
      &lt;td&gt;0.553026&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0.469256&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;57.060001&lt;/td&gt;
      &lt;td&gt;1950&lt;/td&gt;
      &lt;td&gt;0.582441&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0.553026&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;6&lt;/td&gt;
      &lt;td&gt;73.830002&lt;/td&gt;
      &lt;td&gt;1950&lt;/td&gt;
      &lt;td&gt;0.582441&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0.553026&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;7&lt;/td&gt;
      &lt;td&gt;42.959999&lt;/td&gt;
      &lt;td&gt;1954&lt;/td&gt;
      &lt;td&gt;0.569626&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0.539680&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;The first thing we would like to inspect, is the distribution of democratic vote shares &lt;code&gt;demvoteshare&lt;/code&gt;, against their lagged values &lt;code&gt;lagdemvoteshare&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sns.scatterplot(df[&#39;lagdemvoteshare&#39;], df[&#39;demvoteshare&#39;])\
.set(title=&#39;Vote share and incumbency status&#39;, xlabel=&#39;Dem Vote Share (t-1)&#39;, ylabel=&#39;Dem Vote Share (t)&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/regression_discontinuity_19_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;The plot is extremely messy. However we can already see some discontinuity at the threshold: it seems that incumbents do not get vote shares below 0.35.&lt;/p&gt;
&lt;p&gt;To have a more transparent representation of the data, we can use a binscatterplot. Binscatterplots are very similar to histograms with a main difference: instead of having a fixed width, they have a fixed number of observations per bin.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from scipy.stats import binned_statistic

def binscatter(x, y, bins=30, area=True, **kwargs):
    y_bins, x_edges, _ = binned_statistic(x, y, statistic=&#39;mean&#39;, bins=bins)
    x_bins = (x_edges[:-1] + x_edges[1:]) / 2
    p = sns.scatterplot(x_bins, y_bins, **kwargs)
    if area:
        y_std, _, _ = binned_statistic(x, y, statistic=&#39;std&#39;, bins=bins)
        plt.fill_between(x_bins, y_bins-y_std, y_bins+y_std, alpha=0.2, color=&#39;C0&#39;)
    return p
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can now plot the average vote share by previous vote share. The shades represent one standard deviation, at the bin level.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;binscatter(df[&#39;lagdemvoteshare&#39;], df[&#39;demvoteshare&#39;], bins=100)\
.set(title=&#39;Vote share and incumbency status&#39;, xlabel=&#39;Dem Vote Share (t-1)&#39;, ylabel=&#39;Dem Vote Share (t)&#39;);
plt.axvline(x=0.5, ls=&amp;quot;:&amp;quot;, color=&#39;C2&#39;);
plt.title(&#39;Vote share and incumbency status&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/regression_discontinuity_23_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Now it seems quite clear that there exist a discontinuity at $0.5$. We can get a first estimate of the local average treatment effect by assuming a linear model and running a linear regression.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;smf.ols(&#39;demvoteshare ~ lagdemvoteshare + (lagdemvoteshare&amp;gt;0.5)&#39;, df).fit().summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
                &lt;td&gt;&lt;/td&gt;                   &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt;                     &lt;td&gt;    0.2173&lt;/td&gt; &lt;td&gt;    0.005&lt;/td&gt; &lt;td&gt;   46.829&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    0.208&lt;/td&gt; &lt;td&gt;    0.226&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;lagdemvoteshare &gt; 0.5[T.True]&lt;/th&gt; &lt;td&gt;    0.0956&lt;/td&gt; &lt;td&gt;    0.003&lt;/td&gt; &lt;td&gt;   33.131&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    0.090&lt;/td&gt; &lt;td&gt;    0.101&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;lagdemvoteshare&lt;/th&gt;               &lt;td&gt;    0.4865&lt;/td&gt; &lt;td&gt;    0.011&lt;/td&gt; &lt;td&gt;   42.539&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    0.464&lt;/td&gt; &lt;td&gt;    0.509&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;The effect is positive and statistically significant. We can also allow the slope of the line to differ on the two sides of the discontinuity.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df = df.sort_values(&#39;lagdemvoteshare&#39;)
model = smf.ols(&#39;demvoteshare ~ lagdemvoteshare * (lagdemvoteshare&amp;gt;0.5)&#39;, df).fit()
model.summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
                        &lt;td&gt;&lt;/td&gt;                           &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt;                                     &lt;td&gt;    0.2256&lt;/td&gt; &lt;td&gt;    0.007&lt;/td&gt; &lt;td&gt;   34.588&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    0.213&lt;/td&gt; &lt;td&gt;    0.238&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;lagdemvoteshare &gt; 0.5[T.True]&lt;/th&gt;                 &lt;td&gt;    0.0747&lt;/td&gt; &lt;td&gt;    0.012&lt;/td&gt; &lt;td&gt;    6.334&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    0.052&lt;/td&gt; &lt;td&gt;    0.098&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;lagdemvoteshare&lt;/th&gt;                               &lt;td&gt;    0.4653&lt;/td&gt; &lt;td&gt;    0.016&lt;/td&gt; &lt;td&gt;   28.547&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    0.433&lt;/td&gt; &lt;td&gt;    0.497&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;lagdemvoteshare:lagdemvoteshare &gt; 0.5[T.True]&lt;/th&gt; &lt;td&gt;    0.0418&lt;/td&gt; &lt;td&gt;    0.023&lt;/td&gt; &lt;td&gt;    1.827&lt;/td&gt; &lt;td&gt; 0.068&lt;/td&gt; &lt;td&gt;   -0.003&lt;/td&gt; &lt;td&gt;    0.087&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;Let&amp;rsquo;s plot the predicted vote share over the previous graph.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;binscatter(df[&#39;lagdemvoteshare&#39;], df[&#39;demvoteshare&#39;], bins=100, alpha=0.5)\
.set(title=&#39;Vote share and incumbency status&#39;, xlabel=&#39;Dem Vote Share (t-1)&#39;, ylabel=&#39;Dem Vote Share (t)&#39;);
plt.plot(df[&#39;lagdemvoteshare&#39;], model.fittedvalues, color=&#39;C1&#39;)
plt.axvline(x=0.5, ls=&amp;quot;:&amp;quot;, color=&#39;C2&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/regression_discontinuity_29_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Now that we have established a discontinuity at the cutoff, we need to check the RD &lt;strong&gt;assumptions&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;First, is there &lt;strong&gt;sorting&lt;/strong&gt; across the cutoff? In this case, are democratic politicians more or less likely to lose close elections than republicans? We can plot the distribution of (lagged) vote shares and inspect its shape at the cutoff.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sns.histplot(df[&#39;lagdemvoteshare&#39;], bins=100)\
.set(title=&#39;Distribution of lagged dem vote share&#39;, xlabel=&#39;&#39;)
plt.axvline(x=0.5, ls=&amp;quot;:&amp;quot;, color=&#39;C2&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/regression_discontinuity_31_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;If looks pretty smooth. If anything, there is a loss of density at the cutoff, plausibly indicating stronger competition when the competition is close. However, if does not seem particularly asymmetric.&lt;/p&gt;
&lt;p&gt;A &lt;strong&gt;placebo&lt;/strong&gt; test that we can run is to check if the forcing variable has an effect on variables on which we do not expect to have an effect. In this setting, the most intuitive placebo outcome is previous elections: we do not expect that being on either side of the cutoff today is related to any past outcome.&lt;/p&gt;
&lt;p&gt;In our case, we can simply swap the two variables to run the test.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;binscatter(df[&#39;demvoteshare&#39;], df[&#39;lagdemvoteshare&#39;], bins=100)\
.set(title=&#39;Vote share and incumbency status&#39;, xlabel=&#39;Dem Vote Share (t)&#39;, ylabel=&#39;Dem Vote Share (t-1)&#39;);
plt.axvline(x=0.5, ls=&amp;quot;:&amp;quot;, color=&#39;C2&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/regression_discontinuity_33_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;The distribution of vote shares in the past period does not seem to be discontinuous in the incumbency status today, as expected.&lt;/p&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=72KFY8beH0w&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Regression discontinuity video lecture by Paul Goldsmith-Pinkham (Yale)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Free Courses in Economics</title>
      <link>https://matteocourthoud.github.io/post/courses/</link>
      <pubDate>Thu, 10 Feb 2022 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/post/courses/</guid>
      <description>&lt;p&gt;In this page, I collect lectures and materials for graduate courses in Economics and Social Sciences.&lt;/p&gt;
&lt;p&gt;I will only link to lectures and materials that are freely available. I will not link to courses hosted on MOOC websites or that require university credentials to access.&lt;/p&gt;
&lt;p&gt;A special mention goes to the following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The &lt;strong&gt;NBER&lt;/strong&gt; that during each Summer Institute has a &lt;a href=&#34;https://www.nber.org/research/lectures&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;lecture series&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;The &lt;strong&gt;Chamberlain Seminar&lt;/strong&gt; that since 2021 started hosting and recording &lt;a href=&#34;https://www.chamberlainseminar.org/past-seminars&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;tutorial sessions&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;video-lectures&#34;&gt;Video Lectures&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Course Title&lt;/th&gt;
&lt;th&gt;Author&lt;/th&gt;
&lt;th&gt;University&lt;/th&gt;
&lt;th&gt;Year&lt;/th&gt;
&lt;th&gt;Material&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://www.youtube.com/playlist?list=PLxq_lXOUlvQAoWZEqhRqHNezS30lI49G-&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Machine Learning and Causal Inference&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Susan Athey et al.&lt;/td&gt;
&lt;td&gt;Stanford&lt;/td&gt;
&lt;td&gt;2022&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://www.gsb.stanford.edu/faculty-research/centers-initiatives/sil/research/methods/ai-machine-learning/short-course&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Yes&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://www.youtube.com/playlist?list=PLo0lw6BstMGZQqx_r1GnOETkFYihCgve9&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Causal Inference with Panel Data&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Yiqing Xu&lt;/td&gt;
&lt;td&gt;Washington U&lt;/td&gt;
&lt;td&gt;2021&lt;/td&gt;
&lt;td&gt;No&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://chrisconlon.github.io/gradio.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Industrial Organization&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Chris Conlon&lt;/td&gt;
&lt;td&gt;NYU&lt;/td&gt;
&lt;td&gt;2021&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://chrisconlon.github.io/gradio.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Yes&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://chrisconlon.github.io/metrics.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Panel Data Econometrics&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Chris Conlon&lt;/td&gt;
&lt;td&gt;NYU&lt;/td&gt;
&lt;td&gt;2021&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://chrisconlon.github.io/metrics.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Yes&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://www.youtube.com/playlist?list=PLoROMvodv4rPLKxIpqhjhPgdQy7imNkDn&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Machine Learning with Graphs&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Yure Leskovec&lt;/td&gt;
&lt;td&gt;Stanford&lt;/td&gt;
&lt;td&gt;2021&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;http://web.stanford.edu/class/cs224w/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Yes&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://www.youtube.com/playlist?list=PLWWcL1M3lLlojLTSVf2gGYQ_9TlPyPbiJ&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Applied Methods&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Paul Goldsmith-Pinkham&lt;/td&gt;
&lt;td&gt;Yale&lt;/td&gt;
&lt;td&gt;2021&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://github.com/paulgp/applied-methods-phd&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Yes&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://taylorjwright.github.io/did-reading-group/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;DiD Reading Group&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;misc&lt;/td&gt;
&lt;td&gt;misc&lt;/td&gt;
&lt;td&gt;2021&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://taylorjwright.github.io/did-reading-group/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Yes&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://vimeo.com/user108848900&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Computational Economics&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Kenneth Judd&lt;/td&gt;
&lt;td&gt;Stanford&lt;/td&gt;
&lt;td&gt;2020&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://github.com/KennethJudd/CompEcon2020&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Yes&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://www.youtube.com/playlist?list=PLoROMvodv4rOSOPzutgyCTapiGlY2Nd8u&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Reinforcement Learning&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Emma Brunskill&lt;/td&gt;
&lt;td&gt;Stanford&lt;/td&gt;
&lt;td&gt;2020&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;http://web.stanford.edu/class/cs234/index.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Yes&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://www.youtube.com/playlist?list=PLoazKTcS0RzZ1SUgeOgc6SWt51gfT80N0&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Causal Inference&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Brady Neal&lt;/td&gt;
&lt;td&gt;Quebec AI Institute&lt;/td&gt;
&lt;td&gt;2020&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://www.bradyneal.com/causal-inference-course&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Yes&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://www.youtube.com/playlist?list=PLoROMvodv4rObpMCir6rNNUlFAn56Js20&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Natural Language Understanding&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Christopher Potts&lt;/td&gt;
&lt;td&gt;Stanford&lt;/td&gt;
&lt;td&gt;2019&lt;/td&gt;
&lt;td&gt;No&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;material&#34;&gt;Material&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Course Title&lt;/th&gt;
&lt;th&gt;Author&lt;/th&gt;
&lt;th&gt;University&lt;/th&gt;
&lt;th&gt;Year&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://floswald.github.io/NumericalMethods/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Computational Economics&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://floswald.github.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Florial Oswald&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Bocconi&lt;/td&gt;
&lt;td&gt;2021&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://github.com/uo-ec607/lectures&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Data Science for Economists&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://grantmcdermott.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Grant McDermott&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Oregon&lt;/td&gt;
&lt;td&gt;2020&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;http://www.johnasker.com/IO.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Industrial Organization&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;John Asker&lt;/td&gt;
&lt;td&gt;UCLA&lt;/td&gt;
&lt;td&gt;2020&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://kohei-kawaguchi.github.io/EmpiricalIO/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Topics in Empirical Industrial Organization&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Kohei Kawaguchi&lt;/td&gt;
&lt;td&gt;Hong Kong&lt;/td&gt;
&lt;td&gt;2020&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;http://individual.utoronto.ca/vaguirre/courses/eco2901/teaching_io_toronto.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Industrial Organization&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Victor Aguirregabiria&lt;/td&gt;
&lt;td&gt;Toronto&lt;/td&gt;
&lt;td&gt;2019&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://github.com/OU-PhD-Econometrics/fall-2020&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Econometrics&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Tyler Ransom&lt;/td&gt;
&lt;td&gt;Oklahoma&lt;/td&gt;
&lt;td&gt;2020&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://github.com/MartinSpindler/Machine-Learning-in-Econometrics&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Machine Learning in Econometrics&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Martin Spindler&lt;/td&gt;
&lt;td&gt;Munich&lt;/td&gt;
&lt;td&gt;2020&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;http://comlabgames.com/structuraleconometrics/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Structural Econometrics&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Robert Miller&lt;/td&gt;
&lt;td&gt;Carnegie Mellon&lt;/td&gt;
&lt;td&gt;2019&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
</description>
    </item>
    
    <item>
      <title>Economics Conferences</title>
      <link>https://matteocourthoud.github.io/post/conferences/</link>
      <pubDate>Mon, 10 Jan 2022 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/post/conferences/</guid>
      <description>&lt;p&gt;In this page, I collect information about conferences in Economics and Finance.&lt;/p&gt;
&lt;p&gt;If you know about public conferences or meetings that are missing from this list, please either &lt;a href=&#34;mailto:matteo.courthoud@econ.uzh.ch&#34;&gt;contact me&lt;/a&gt; or &lt;a href=&#34;https://github.com/matteocourthoud/website/blob/master/content/post/conferences/index.md&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;edit the table on Github&lt;/a&gt;!&lt;/p&gt;
&lt;p&gt;Note that conferences are ordered by deadline and not by conference date.&lt;/p&gt;
&lt;h2 id=&#34;january&#34;&gt;January&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Conference&lt;/th&gt;
&lt;th&gt;Organizer&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Field&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Deadline&lt;/th&gt;
&lt;th&gt;Target&lt;/th&gt;
&lt;th&gt;Date&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://www.sgvs.ch/conferences/sses2022&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Annual Congress of the Swiss Society of Economics and Statistics&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;SSES&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;All&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://www.sgvs.ch/conferences/sses2022/call_for_papers&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;January 31&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;23/06/21&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;february&#34;&gt;February&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Conference&lt;/th&gt;
&lt;th&gt;Organizer&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Field&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Deadline&lt;/th&gt;
&lt;th&gt;Target&lt;/th&gt;
&lt;th&gt;Date&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://beccle.no/event/bergen-competition-policy-conference-2020/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Bergen Competition Policy Conference&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;NHH&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Comp policy&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://beccle.no/event/bergen-competition-policy-conference-2020/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;February 03&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;23/04/20&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://cepr.org/6754/cfp-mainconference&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CEPR/JIE Conference on Applied IO&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;CEPR&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;IO&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://matteocourthoud.github.io/post/conferences/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;February 10&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Junior&lt;/td&gt;
&lt;td&gt;08/06/21&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;http://ec22.sigecom.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Economics and Computation&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;ACM SIGecom&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Theory&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://ec22.sigecom.org/call-for-contributions-acm/papers/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;February 10&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;11/07/21&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://www.econometricsociety.org/meetings/schedule/2022/06/16/2022-north-america-summer-meeting&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ES North American Summer Meeting&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Econometric Society&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;All&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://www.econometricsociety.org/meetings/schedule/2022/06/16/2022-north-america-summer-meeting&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;February 12&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Senior&lt;/td&gt;
&lt;td&gt;16/06/21&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://www.eea-esem-congresses.org/index.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;EEA Summer Meeting&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;European Economic Association&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;All&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://www.eea-esem-congresses.org/important-dates.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;February 15&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;23/08/21&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://www.eea-esem-congresses.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ES European Summer Meeting&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Econometric Society&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;All&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://www.eea-esem-congresses.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;February 15&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Senior&lt;/td&gt;
&lt;td&gt;23/08/21&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://www.economicdynamics.org/sedam_2021/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Annual Meeting of the Society for Economic Dynamics&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;SED&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Macro&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://www.economicdynamics.org/sedam_2021/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;February 15&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;01/07/21&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://games2020.hu/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GAMES 2020&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Game Theory Society&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Game Theory&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://games2020.hu/registration/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;February 20&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;19/07/21&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://www.nottingham.ac.uk/gep/news-events/conferences/2020-21/postgrad-conference-2021.aspx&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Annual GEP/CEPR Postgraduate Conference&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;University of Nottingham&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Policy&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://www.nottingham.ac.uk/gep/documents/conferences/2020-21/pg-conf-cfp-2021.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;February 26&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Junior&lt;/td&gt;
&lt;td&gt;06/05/21&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://www.digital-economics.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Doctoral Workshop on the Economics of Digitization&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;TSE&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Digitalization&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://www.digital-economics.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;February 28&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Junior&lt;/td&gt;
&lt;td&gt;12/05/22&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;march&#34;&gt;March&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Conference&lt;/th&gt;
&lt;th&gt;Organizer&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Field&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Deadline&lt;/th&gt;
&lt;th&gt;Target&lt;/th&gt;
&lt;th&gt;Date&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://cssh.northeastern.edu/economics/iioc/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Annual IIOC&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Northeastern University&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;IO&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://cssh.northeastern.edu/economics/iioc/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;March 01&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;30/04/21&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://yem2020.econ.muni.cz/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Young Economists&amp;rsquo; Meeting&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;University fo Munich&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;All&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://yem2020.econ.muni.cz/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;March 08&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Junior&lt;/td&gt;
&lt;td&gt;01/10/20&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://www.barcelonagse.eu/summer-forum&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GSE Summer Forum&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Barcelona GSE&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;All&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://www.barcelonagse.eu/summer-forum&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;March 14&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;07/06/21&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://www.nhh.no/en/calendar/conferences/2021/earie-2021/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;EARIE&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;NHH&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;IO&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://www.nhh.no/en/calendar/conferences/2021/earie-2021/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;March 15&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;27/08/21&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://www.sioe.org/news/economics-media-workshop-call-paper-poster-presentations&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Economics of Media Workshop&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Queen’s University&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;IO&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://www.sioe.org/news/economics-media-workshop-call-paper-poster-presentations&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;March 15&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Junior&lt;/td&gt;
&lt;td&gt;12/06/20&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://www.qmul.ac.uk/sef/events/conferences/items/3rd-qmul-economics-and-finance-workshop-for-phd--post-doctoral-students.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;QMUL Economics and Finance Workshop&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Queen Mary University&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;All&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://econ.columbia.edu/call-for-papers-3rd-qm-phd-workshop/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;March 20&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Junior&lt;/td&gt;
&lt;td&gt;26/05/20&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://static1.squarespace.com/static/56086d00e4b0fb7874bc2d42/t/5e753140c2225859fa93ba1e/1584738624656/callforpapers.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Virtual Finance and Economics Conference&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Yale University&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Finecon&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://static1.squarespace.com/static/56086d00e4b0fb7874bc2d42/t/5e753140c2225859fa93ba1e/1584738624656/callforpapers.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;March 25&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;17/04/20&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://sites.google.com/view/dc-io-day&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;DC IO Day 2020&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Georgetown University&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;IO&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://sites.google.com/view/dc-io-day&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;March 31&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;15/05/20&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;april&#34;&gt;April&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Conference&lt;/th&gt;
&lt;th&gt;Organizer&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Field&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Deadline&lt;/th&gt;
&lt;th&gt;Target&lt;/th&gt;
&lt;th&gt;Date&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://economics.stanford.edu/site/site-2021&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;SITE&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Stanford University&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Theory&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://economics.stanford.edu/site/site-2021&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;April 01&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;12/06/21&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://www.nber.org/conferences/summer-institute&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;NBER Summer Meeting Workshop&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;NBER&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;All&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://www.nber.org/conferences/summer-institute&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;April 05&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;12/07/21&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://www.aeaweb.org/conference/submissions&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;AEA Annual Meeting&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;AEA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;All&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://www.aeaweb.org/conference/submissions&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;April 15&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;07/01/22&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://matteocourthoud.github.io/post/conferences/files/swissIOday2021_CallForPapers.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Swiss IO Day&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;University of Bern&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;IO&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://matteocourthoud.github.io/post/conferences/files/swissIOday2021_CallForPapers.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;April 16&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;11/06/21&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://www.econometricsociety.org/meetings/schedule/2022/01/06/2022-north-american-winter-meeting&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Econometric Society - North American Winter Meetings&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;ES&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;All&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://www.econometricsociety.org/meetings/schedule/2022/01/06/2022-north-american-winter-meeting&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;April 21&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;06/01/22&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://www.cresse.info/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CRESSE&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;CRESSE&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;IO&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://www.cresse.info/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;April 30&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;26/07/21&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;may&#34;&gt;May&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Conference&lt;/th&gt;
&lt;th&gt;Organizer&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Field&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Deadline&lt;/th&gt;
&lt;th&gt;Target&lt;/th&gt;
&lt;th&gt;Date&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://mailchi.mp/cepr/european-research-workshop-in-international-trade-erwit-506353&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;European Research Workshop in International Trade&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;CEPR&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Trade&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://mailchi.mp/cepr/european-research-workshop-in-international-trade-erwit-506353&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;May 02&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;22/10/21&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;http://coin.wne.uw.edu.pl/wiem/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Warsaw International Economic Meeting&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Warsaw University&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;All&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;http://coin.wne.uw.edu.pl/wiem/wiem2020-cfp.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;May 03&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;01/07/20&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://warwick.ac.uk/fac/soc/economics/events/2021/6/economics_phd_conference/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Warwick Economics PhD Conference&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;University of Warwick&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;All&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://warwick.ac.uk/fac/soc/economics/events/2021/6/economics_phd_conference/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;May 09&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;PhD&lt;/td&gt;
&lt;td&gt;24/06/21&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://www.law.northwestern.edu/research-faculty/clbe/events/antitrust/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Annual Conference on Antitrust Economics and Competition Policy&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Northwestern University&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Comp policy&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://www.law.northwestern.edu/research-faculty/clbe/events/antitrust/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;May 17&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;17/09/21&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://www.economicsofai.com/blog/2021/2/2/2021-nber-economics-of-ai-conference-call-for-papers&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;NBER Economics of AI Conference&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;NBER&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;AI&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://www.economicsofai.com/blog/2021/2/2/2021-nber-economics-of-ai-conference-call-for-papers&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;May 31&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;23/09/21&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;june&#34;&gt;June&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Conference&lt;/th&gt;
&lt;th&gt;Organizer&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Field&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Deadline&lt;/th&gt;
&lt;th&gt;Target&lt;/th&gt;
&lt;th&gt;Date&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://eaamo.org/cfp/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Equity and Access in Algorithms, Mechanisms, and Optimization (EAAMO)&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;ACM&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;AI&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://eaamo.org/cfp/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;June 14&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;05/10/21&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://www.ftc.gov/news-events/events-calendar/fourteenth-annual-federal-trade-commission-microeconomics-conference&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;FTC Micro Conference&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;FTC&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;IO&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://www.ftc.gov/system/files/documents/public_events/1588356/20210326_-_micro_conf_call_for_papers.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;June 23&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Senior&lt;/td&gt;
&lt;td&gt;04/11/21&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;july&#34;&gt;July&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Conference&lt;/th&gt;
&lt;th&gt;Organizer&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Field&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Deadline&lt;/th&gt;
&lt;th&gt;Target&lt;/th&gt;
&lt;th&gt;Date&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://www.emconference.net/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Empirics and Methods in Economics Conference&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Northwestern &amp;amp; Chicago&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Empirical&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://www.emconference.net/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;July 30&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Junior&lt;/td&gt;
&lt;td&gt;22/10/20&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;august&#34;&gt;August&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Conference&lt;/th&gt;
&lt;th&gt;Organizer&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Field&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Deadline&lt;/th&gt;
&lt;th&gt;Target&lt;/th&gt;
&lt;th&gt;Date&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://docs.google.com/document/d/1BjHI_6HyL7pk2UYUNDlDY971B9AO83wD8DfEF6KNDfs/edit&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;AI Policy Conference&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;ETH Zurich&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;AI&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://docs.google.com/document/d/1BjHI_6HyL7pk2UYUNDlDY971B9AO83wD8DfEF6KNDfs/edit&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;August 1&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;14/09/21&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://sites.google.com/site/uscfom/finance-organizations-and-markets-fom-research-group&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Finance, Organizations and Markets (FOM) Conference&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Dartmouth College&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Finance, IO&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://sites.google.com/site/uscfom/finance-organizations-and-markets-fom-research-group&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;August 14&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;28/10/21&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;september&#34;&gt;September&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Conference&lt;/th&gt;
&lt;th&gt;Organizer&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Field&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Deadline&lt;/th&gt;
&lt;th&gt;Target&lt;/th&gt;
&lt;th&gt;Date&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://why21.causalai.net/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Causal Inference &amp;amp; Machine Learning: Why now?&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;NeurIPS&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Econometrics&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://why21.causalai.net/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;September 18&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;13/12/21&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://www.ub.edu/school-economics/ewmes&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ES European Winter Meeting&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Econometricc Society&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;All&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://www.ub.edu/school-economics/ewmes&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;September 19&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Senior&lt;/td&gt;
&lt;td&gt;13/12/21&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://www.causalscience.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Causal Data Science Conference&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;causalscience&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;All&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://www.causalscience.org/blog/causal-data-science-meeting-2021-call-for-papers/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;September 30&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;15/11/21&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;october&#34;&gt;October&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Conference&lt;/th&gt;
&lt;th&gt;Organizer&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Field&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Deadline&lt;/th&gt;
&lt;th&gt;Target&lt;/th&gt;
&lt;th&gt;Date&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://www.tse-fr.eu/conferences/2022-15th-digital-economics-conference&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Digital Economics Conference&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;TSE&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Digital&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://www.tse-fr.eu/sites/default/files/TSE/documents/conf/2022/Digital_Economics/call_for_papers_digital_conf_2022.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;October 3&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;13/01/22&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://apios.org.au/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Asia-Pacific IO Conference&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Asia-Pacific IO Society&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;IO&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://apios.org.au/submission/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;October 22&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;13/12/21&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://www.sgvs.ch/conferences/ysem2021&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Young Swiss Economists Meeting&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;SSES&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;All&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://www.sgvs.ch/files/Call_for_Papers_YSEM_2021.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;October 25&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Junior&lt;/td&gt;
&lt;td&gt;11/02/21&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;november&#34;&gt;November&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Conference&lt;/th&gt;
&lt;th&gt;Organizer&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Field&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Deadline&lt;/th&gt;
&lt;th&gt;Target&lt;/th&gt;
&lt;th&gt;Date&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://www.law.nyu.edu/conferences/2022-NextGen-Antitrust-Conference&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Next Generation of Antitrust, Data Privacy and Data Protection Scholars Conference&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;NYU&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;IO&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://www.law.nyu.edu/conferences/2022-NextGen-Antitrust-Conference&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;November 1&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;28/01/22&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://smye2021.weebly.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Spring Meeting of Young Economists&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;University of Bologna&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;All&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://smye2021.weebly.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;November 12&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Junior&lt;/td&gt;
&lt;td&gt;17/06/21&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://www.nber.org/conferences/industrial-organization-program-meeting-spring-2021&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;NBER IO Winter Meeting&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;NBER&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;IO&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://conference.nber.org/confsubmit/backend/cfp?id=IOs21&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;November 20&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Senior&lt;/td&gt;
&lt;td&gt;12/02/21&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://www.zew.de/en/events-and-professional-training/detail/2021-macci-annual-conference/3320&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;MaCCI Annual Conference&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;University of Mannheim&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;IO&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://www.zew.de/en/events-and-professional-training/detail/2021-macci-annual-conference/3320&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;November 30&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;12/03/21&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://www.tse-fr.eu/sites/default/files/TSE/documents/conf/2022/postal/call_postal_2022_v2.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Postal Economics Conference&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;TSE&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Digital&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://www.tse-fr.eu/sites/default/files/TSE/documents/conf/2022/postal/call_postal_2022_v2.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;November 30&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;07/04/22&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;december&#34;&gt;December&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Conference&lt;/th&gt;
&lt;th&gt;Organizer&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Field&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Deadline&lt;/th&gt;
&lt;th&gt;Target&lt;/th&gt;
&lt;th&gt;Date&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://sites.google.com/site/ecbeconference&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Early-Career Behavioral Economics Conference&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Princeton University&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Behavioral&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://sites.google.com/site/ecbeconference/call&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;December 15&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;junior&lt;/td&gt;
&lt;td&gt;03/06/21&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;undefined&#34;&gt;Undefined&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Conference&lt;/th&gt;
&lt;th&gt;Organizer&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Field&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Deadline&lt;/th&gt;
&lt;th&gt;Target&lt;/th&gt;
&lt;th&gt;Date&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://www.law.northwestern.edu/research-faculty/clbe/events/innovation/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Annual Conference on Innovation Economics&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Northwestern University&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Innovation&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://www.law.northwestern.edu/research-faculty/clbe/callforpapers/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;forthcoming&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;27/08/20&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://conference2.aau.at/event/4/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Conference on Mechanism and Institution Design&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Universität Klagenfurt&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Market Design&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;closed&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;11/06/20&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://sites.google.com/site/dteaworkshop/home&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;D-TEA Conference&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;HEC Paris&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Theory&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;closed&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;16/06/20&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://sites.wustl.edu/egsc/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Economics Graduate Student Conference&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Washington University&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;All&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;closed&lt;/td&gt;
&lt;td&gt;Junior&lt;/td&gt;
&lt;td&gt;07/11/20&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;http://conference.nber.org/confer/2020/SI2020/SI2020.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;NBER Summer Institute&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;NBER&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;All&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;invitation&lt;/td&gt;
&lt;td&gt;Senior&lt;/td&gt;
&lt;td&gt;06/07/20&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;http://competitionpolicy.ac.uk/events/annual-conferences&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CCP Annual Conference&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Centre for Competition Policy&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Comp policy&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;closed&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;24/06/21&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://www.res.org.uk/event-listing/2021-annual-conference.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;RES Annual Conference&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Royal Economics Society&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;All&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;closed&lt;/td&gt;
&lt;td&gt;Senior&lt;/td&gt;
&lt;td&gt;12/04/21&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://www.emerginginvestigators.org/conference&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;JEI Student Conference&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Harvard University&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;All&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;canceled&lt;/td&gt;
&lt;td&gt;Junior&lt;/td&gt;
&lt;td&gt;20/06/20&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://bfi.uchicago.edu/event/sixth-annual-conference-on-network-science-and-economics/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Annual Conference on Network Science and Economics&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Becker Friedman Institute&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Networks&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;canceled&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;27/03/20&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://saet.uiowa.edu/2021-annual-saet-conference/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Annual SAET Conference&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Society for the Advancement of Economic Theory&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Theory&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;closed&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;13/06/21&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
</description>
    </item>
    
    <item>
      <title>How to Work on a Remote Machine via SSH</title>
      <link>https://matteocourthoud.github.io/post/ssh/</link>
      <pubDate>Mon, 06 Dec 2021 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/post/ssh/</guid>
      <description>&lt;p&gt;Welcome to my tutorial on how to set up a remote machine and deploy your code there. I will first analyze SSH and then look at two specific applications: coding in Python and Julia.&lt;/p&gt;
&lt;h2 id=&#34;setup&#34;&gt;Setup&lt;/h2&gt;
&lt;p&gt;In order to start working on a remote server you need&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the server&lt;/li&gt;
&lt;li&gt;local shell&lt;/li&gt;
&lt;li&gt;SSH installed&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;SSH, or Secure Shell, is a protocol designed to transfer data between a client and a server (two computers basically) over an untrusted network.&lt;/p&gt;
&lt;p&gt;The way SSH works is it encrypts the connection using a pair of keys and the server, which is the computer you would connect to, is usually waiting for an SSH connection on Port 22.&lt;/p&gt;
&lt;p&gt;SSH is normally installed by default. To check if you have SSH installed, open the terminal and write &lt;code&gt;ssh&lt;/code&gt;. You should receive a message that looks like this&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;usage: ssh [-1246AaCfGgKkMNnqsTtVvXxYy] [-b bind_address] [-c cipher_spec]
[-D [bind_address:]port] [-E log_file] [-e escape_char]
[-F configfile] [-I pkcs11] [-i identity_file]
[-J [user@]host[:port]] [-L address] [-l login_name] [-m mac_spec] [-O ctl_cmd] [-o option] [-p port] [-Q query_option] [-R address] [-S ctl_path] [-W host:port] [-w local_tun[:remote_tun]]
[user@]hostname [command]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If SSH is not installed, you can install it using the following commands.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;sudo apt-get install openssh-server
sudo systemctl enable ssh
sudo systemctl start ssh
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now that you have installed SSH, we are ready to setup a remote connection.&lt;/p&gt;
&lt;p&gt;From the computer you want to access remotey, generate the public key.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;ssh-keygen -t rsa
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You will be asked for a location. If you decide to enter one manually then that will be the pair’s location, if you leave the default one it will be inside the &lt;code&gt;.ssh&lt;/code&gt; hidden folder in your home directory.&lt;/p&gt;
&lt;p&gt;Now you will be prompted for a password. If you enter one you will be asked for it every time you use the key, this works for added security. If you don’t want a password just press enter and continue without one.&lt;/p&gt;
&lt;p&gt;Two files were created. One file ends with the ‘.pub’ extension and the other one doesn’t. The file that ends with ‘.pub’ is your public key. This key needs to be in the computer you want to connect to (the server) inside a file called &lt;code&gt;authorized_keys&lt;/code&gt; . You can accomplish this with the following command:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;ssh-copy-id username@ip
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For example in my case to send the key to my computer it would be:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;ssh-copy-id sergiop@132.132.132.132
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you have MacOS there’s a chance you don’t have ssh-copy-id installed, in that case you can install it using&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;brew install ssh-copy-id
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you haven’t installed &lt;code&gt;brew&lt;/code&gt;, you can install it by following &lt;a href=&#34;https://brew.sh/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;this guide&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;connect&#34;&gt;Connect&lt;/h2&gt;
&lt;p&gt;To permanently add the SSH key, you can use the follwing command&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;ssh-add directory\key.pem
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Lastly, to connect, just type the following command.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;ssh username@ip
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Where &lt;code&gt;username&lt;/code&gt; is the server name and &lt;code&gt;ip&lt;/code&gt; is the public IP adress, e.g. 132.132.132.132.&lt;/p&gt;
&lt;p&gt;If your server is not public, you will not be able to access it.&lt;/p&gt;
&lt;p&gt;If your server is password protected, you will be prompted to insert a password when you connect. If not, you should protect it with a password.&lt;/p&gt;
&lt;h2 id=&#34;managing-screens&#34;&gt;Managing screens&lt;/h2&gt;
&lt;p&gt;While you are connected to the remote terminal, any disturbance to your connection will interrupt the code. In order to avoid that, you want to create separate screens. This will allow your code to run remotely undisturbed, irrespectively of your connection.&lt;/p&gt;
&lt;p&gt;First, you need to install &lt;code&gt;screen&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;brew install screen
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To create a new screen, just type&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;screen
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now you can lunch your code.&lt;/p&gt;
&lt;p&gt;After that, you want to detach from that screen so that the code can run remotely undisturbed.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;screen -d
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Another option is to use &lt;code&gt;ctrl+a&lt;/code&gt; followed by &lt;code&gt;ctrl+d&lt;/code&gt;. This will detach the screen without the need to type anythin in the terminal, in case the terminal is busy (most likely).&lt;/p&gt;
&lt;p&gt;To list the current active screens type&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;screen -ls
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you want to check at any time that your code is running, without re-attaching to the screen, you can just type&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;top
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;which is the general command to check active processes. To exit, use &lt;code&gt;ctrl+z&lt;/code&gt;, which generally terminates processes in the terminal.&lt;/p&gt;
&lt;p&gt;To reattach to your screen, type&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;screen -r
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In case you have multiple screens (you can check with &lt;code&gt;screen -ls&lt;/code&gt;), you can reattach to a specific one by typing&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;screen -r 12345
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;where &lt;code&gt;12345&lt;/code&gt; is the id of the screen.&lt;/p&gt;
&lt;p&gt;To kill a screen, type&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;screen -XS 12345 quit
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;where again &lt;code&gt;12345&lt;/code&gt; is the id of the screen.&lt;/p&gt;
&lt;h2 id=&#34;python-and-pycharm&#34;&gt;Python and Pycharm&lt;/h2&gt;
&lt;p&gt;If you are coding in Python, &lt;a href=&#34;https://www.jetbrains.com/pycharm/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PyCharm&lt;/a&gt; is one of the best IDEs. Among many features, it offers the possibility to set a remote compiler for your pthon console and to sync input and output files automatically.&lt;/p&gt;
&lt;p&gt;First, you need to have setup a remote SSH connection following the steps above. Importantly, you need to have added the public key to your machine using the &lt;code&gt;ssh-add&lt;/code&gt; command, as explained above.&lt;/p&gt;
&lt;p&gt;Then open Pytharm, go to the lower-right corner, where the current interpreter is listed (e.g. Pytohn 3.8), click it and select &lt;code&gt;interpreter settings&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;img/interpreter_settings.png&#34; alt=&#34;interpreter_settings&#34;&gt;&lt;/p&gt;
&lt;p&gt;Click on the gear icon ⚙️ on the top-right corner and select &lt;code&gt;add&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;img/add.png&#34; alt=&#34;add&#34;&gt;&lt;/p&gt;
&lt;p&gt;Insert the server &lt;code&gt;host&lt;/code&gt; (IP address, e.g. 132.132.132.132) and &lt;code&gt;username&lt;/code&gt; (e.g. sergiop).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;img/configuration.png&#34; alt=&#34;configuration&#34;&gt;&lt;/p&gt;
&lt;p&gt;Next, you have to insert your credentials. If you have a password, insert it, otherwise you have to insert the path to your SSH key file.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;img/password.png&#34; alt=&#34;configuration&#34;&gt;&lt;/p&gt;
&lt;p&gt;Lastly, select the remote interpreter. If you are using a python version that is not default, browse to the preferred python installation folder. Also, check the box for &lt;code&gt;execute code giving this interpreter with root privileges via sudo&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;You can also select which remote folder to sync with your local project. By default, you are given a &lt;code&gt;tmp/pycharm_project_XX&lt;/code&gt; folder. You can change it if you want. I recommend also to have the last option checked: &lt;code&gt;automatically sync project files to the server&lt;/code&gt;. This will automatically synch all remote changes with your local machine, in your local project folder.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;img/folder.png&#34; alt=&#34;configuration&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;julia-and-juno&#34;&gt;Julia and Juno&lt;/h2&gt;
&lt;p&gt;If you are coding in Julia, &lt;a href=&#34;https://junolab.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Juno&lt;/a&gt; is the best IDE around. It’s an integration with &lt;a href=&#34;https://atom.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Atom&lt;/a&gt; with a dedicated compiler, local variables, syntax highlight, autocompletion.&lt;/p&gt;
&lt;p&gt;On Atom, you first need to install the &lt;a href=&#34;https://github.com/h3imdall/ftp-remote-edit&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;ftp-remote-edit&lt;/code&gt;&lt;/a&gt; package.&lt;/p&gt;
&lt;p&gt;Then go to the menu item &lt;code&gt;Packages &amp;gt; Ftp-Remote-Edit &amp;gt; Toggle&lt;/code&gt;.&lt;/p&gt;
&lt;img src=&#34;img/toggle.png&#34; alt=&#34;julia&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;A new &lt;code&gt;Remote&lt;/code&gt; panel will open with the default button to &lt;code&gt;Edit a new server&lt;/code&gt;.&lt;/p&gt;
&lt;img src=&#34;img/edit.png&#34; alt=&#34;julia&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;Click it and you will be able to set up your remote connection.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Press &lt;code&gt;New&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Insert your username in &lt;code&gt;The name of the server&lt;/code&gt;, for example &lt;code&gt;sergiop&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Insert your ip adress in &lt;code&gt;The hostname or IP adress of the server&lt;/code&gt;, for example &lt;code&gt;123.123.123.123&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Select &lt;code&gt;SFTP - SSH File Transfer Protocol&lt;/code&gt; under &lt;code&gt;Protocol&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Select your &lt;code&gt;Logon&lt;/code&gt; option. You can either insert your password every time, just once, or use a keyfile.&lt;/li&gt;
&lt;li&gt;Insert again your username in &lt;code&gt;Username for autentication&lt;/code&gt;, again for example &lt;code&gt;sergiop&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;If you don’t want to start from the root folder, you can change the &lt;code&gt;Initial Directory&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;img/julia.png&#34; alt=&#34;julia&#34;&gt;&lt;/p&gt;
&lt;p&gt;Now you will be able to see your remote directory (named for example &lt;code&gt;sergiop&lt;/code&gt;) in the &lt;code&gt;Remote&lt;/code&gt; panel.&lt;/p&gt;
&lt;p&gt;To start using Julia remotely, just start a new remote Julia process from the menu on the left.&lt;/p&gt;
&lt;img src=&#34;img/remote.png&#34; alt=&#34;julia&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;Now you are ready to deploy your Julia code on your remote server!&lt;/p&gt;
&lt;h2 id=&#34;jupyter-notebooks&#34;&gt;Jupyter Notebooks&lt;/h2&gt;
&lt;p&gt;If you want to have a &lt;a href=&#34;https://jupyter.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Jupyter Notebook&lt;/a&gt; running remotely, the steps are the following. The main advantage of a Jupyter Notebook is that it allows you to mix text and code in a single file, similarly to &lt;a href=&#34;https://rmarkdown.rstudio.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;RMarkdown&lt;/a&gt;, with the advantage of not being contrained to use a R (or Python) kernel. For example, I often use Jupyter Notebook with Julia or Matlab Kernels. Moreover, you can also make nice slides out of it!&lt;/p&gt;
&lt;p&gt;First, connect to the remote machine. Look at &lt;a href=&#34;https://matteocourthoud.github.io/post/remote/#setup&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;section 1&lt;/a&gt; to set up your SSH connection.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;ssh username@ip
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Start a Jupyter Notebook in the remote machine.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;jupyter notebook --no-browser
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The command will open a jupyter notebook in the remote machine. To connect to it, we need to know which port it used. The default port is &lt;code&gt;8888&lt;/code&gt;. If that port is busy, it will look for another available one. We can see the port from the output in terminal.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Jupyter Notebook is running at: http://localhost:XXXX/…&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Where &lt;code&gt;XXXX&lt;/code&gt; is the repote port used.&lt;/p&gt;
&lt;p&gt;Now we need to forward the remote port &lt;code&gt;XXXX&lt;/code&gt; to our local &lt;code&gt;YYYY&lt;/code&gt; port.&lt;/p&gt;
&lt;p&gt;Open a new &lt;em&gt;local&lt;/em&gt; shell. Type&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;ssh -L localhost:YYYY:localhost:XXXX username@ip
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Where &lt;code&gt;YYYY&lt;/code&gt; can be anything. I’d use the default port &lt;code&gt;8888&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;ssh -L localhost:8889:localhost:8888 username@ip
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now go to your browser and type&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;localhost:YYYY
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Which in my case is&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;localhost:8889
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This will open the remote Jupyter Notebook.&lt;/p&gt;
&lt;p&gt;Done!&lt;/p&gt;
&lt;p&gt;In case you want to check which Jupiter notebooks are running, type&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;jupyter notebook list
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To kill a notebook use&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;jupyter notebook stop XXXX
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;sources&#34;&gt;Sources&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://medium.com/@SergioPietri/how-to-setup-and-use-ssh-for-remote-connections-e86556d804dd&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;How To Setup And Use SSH For Remote Connections&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.junolab.org/stable/man/remote/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Connecting to a Julia session on a remote machine&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://ljvmiranda921.github.io/notebook/2018/01/31/running-a-jupyter-notebook/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Running a Jupyter notebook from a remote server&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>My Color Palette</title>
      <link>https://matteocourthoud.github.io/post/palette/</link>
      <pubDate>Sun, 24 Oct 2021 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/post/palette/</guid>
      <description>&lt;p&gt;Ok, this is a fun post. I am choosing… &lt;strong&gt;my color palette&lt;/strong&gt;!&lt;/p&gt;
&lt;p&gt;I have decided to unify all the color palettes I have on my website, slides, graphs, etc… into a unique universal color palette.&lt;/p&gt;
&lt;h2 id=&#34;main-color&#34;&gt;Main Color&lt;/h2&gt;
&lt;p&gt;First of all, I have to choose my main color.&lt;/p&gt;
&lt;script src=&#34;https://coolors.co/palette-widget/widget.js&#34;&gt;&lt;/script&gt;
&lt;script data-id=&#34;09673912029165208&#34;&gt;new CoolorsPaletteWidget(&#34;09673912029165208&#34;, [&#34;003f5c&#34;,&#34;003f5c&#34;]); &lt;/script&gt;
&lt;p&gt;Here are some shades of it.&lt;/p&gt;
&lt;script src=&#34;https://coolors.co/palette-widget/widget.js&#34;&gt;&lt;/script&gt;
&lt;script data-id=&#34;0683428549461768&#34;&gt;new CoolorsPaletteWidget(&#34;0683428549461768&#34;, [&#34;002637&#34;,&#34;00324a&#34;,&#34;003f5c&#34;,&#34;17506b&#34;,&#34;2c6078&#34;]); &lt;/script&gt;
&lt;h2 id=&#34;related-palettes&#34;&gt;Related Palettes&lt;/h2&gt;
&lt;p&gt;Now I will build a couple of colors palettes based on it.&lt;/p&gt;
&lt;p&gt;The first one, is red oriented.&lt;/p&gt;
&lt;script src=&#34;https://coolors.co/palette-widget/widget.js&#34;&gt;&lt;/script&gt;
&lt;script data-id=&#34;06164154396260932&#34;&gt;new CoolorsPaletteWidget(&#34;06164154396260932&#34;, [&#34;003f5c&#34;,&#34;444e86&#34;,&#34;955196&#34;,&#34;dd5182&#34;,&#34;ff6e54&#34;,&#34;ffa600&#34;]); &lt;/script&gt;
&lt;p&gt;Second one, is green oriented.&lt;/p&gt;
&lt;script src=&#34;https://coolors.co/palette-widget/widget.js&#34;&gt;&lt;/script&gt;
&lt;script data-id=&#34;033203286745601424&#34;&gt;new CoolorsPaletteWidget(&#34;033203286745601424&#34;, [&#34;003f5c&#34;,&#34;00677f&#34;,&#34;00908f&#34;,&#34;2db88b&#34;,&#34;94dc7b&#34;,&#34;f9f871&#34;]); &lt;/script&gt;
&lt;h2 id=&#34;color-sequence&#34;&gt;Color Sequence&lt;/h2&gt;
&lt;p&gt;Now I need a high contrast scheme for graphs. I add one color at the time to check that contrast is always maximized.&lt;/p&gt;
&lt;script src=&#34;https://coolors.co/palette-widget/widget.js&#34;&gt;&lt;/script&gt;       &lt;script data-id=&#34;0605466695047113&#34;&gt;new CoolorsPaletteWidget(&#34;0605466695047113&#34;, [&#34;003f5c&#34;,&#34;ff6e54&#34;,&#34;f9f871&#34;,&#34;2db88b&#34;,&#34;955196&#34;]); &lt;/script&gt;https://coolors.co/003f5c-ff6e54-f9f871-2db88b-955196)
&lt;p&gt;A milder version of the same palette is:&lt;/p&gt;
&lt;script src=&#34;https://coolors.co/palette-widget/widget.js&#34;&gt;&lt;/script&gt;       &lt;script data-id=&#34;007538072748725222&#34;&gt;new CoolorsPaletteWidget(&#34;007538072748725222&#34;, [&#34;00798c&#34;,&#34;d1495b&#34;,&#34;edae49&#34;,&#34;52a369&#34;,&#34;756ab2&#34;]); &lt;/script&gt;https://coolors.co/00798c-d1495b-edae49-52a369-756ab2)</description>
    </item>
    
    <item>
      <title>PhD Frequently Asked Questions</title>
      <link>https://matteocourthoud.github.io/post/phd_faq/</link>
      <pubDate>Fri, 01 Oct 2021 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/post/phd_faq/</guid>
      <description>&lt;p&gt;In this page, I collect anquestions that I frequently asked myself during my PhD, possibly with answers.&lt;/p&gt;
&lt;p&gt;Personally, the article for PhD students that helped me the most is &lt;a href=&#34;https://medium.com/@paul.niehaus/doing-research-18cb310529e0&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;“Doing research”&lt;/a&gt; by Paul Niehaus. But beware, it might not work for everyone.&lt;/p&gt;
&lt;h2 id=&#34;starting-the-phd&#34;&gt;Starting the PhD&lt;/h2&gt;
&lt;h3 id=&#34;information&#34;&gt;Information&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.nature.com/articles/d41586-019-03459-7?sf223557541=1&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;“PhDs: the tortuous truth”&lt;/a&gt;, Chris Woolston, 2019.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://medium.economist.com/why-doing-a-phd-is-often-a-waste-of-time-349206f9addb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;“Why doing a PhD is often a waste of time”&lt;/a&gt;, The Economist, 2016&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.theguardian.com/careers/phd-right-career-option&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;“Should you do a PhD?&amp;quot;&lt;/a&gt;, Daniel K. Sokol, 2012.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://tertilt.vwl.uni-mannheim.de/bachelor/GradSchoolGuide.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;“So, you want to go to a grad school in economics?&amp;quot;&lt;/a&gt;, Ceyhun Elgin and Mario Solis-Garcia, 2007.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;applying&#34;&gt;Applying&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://james-tierney.medium.com/how-to-ask-your-professor-for-a-letter-of-recommendation-f06e8b2f2c64&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;“How to Ask Your Professor for a Letter of Recommendation”&lt;/a&gt;, James Tierney, 2020.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://drive.google.com/file/d/16eUvtahziPyBTpX_ZeyXjPck2OyinfHH/view&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;“Pre-Doc Guide”&lt;/a&gt;, Alvin Christian, 2019.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://athey.people.stanford.edu/professional-advice&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;“Advice for Applying to Grad School in Economics”&lt;/a&gt;, Susan Athey, 2016.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://qz.com/116081/the-complete-guide-to-getting-into-an-economics-phd-program/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;“The complete guide to getting into an economics PhD program”&lt;/a&gt;, Miles Kimball, 2013.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.ericzwick.com/public_goods/twelve_steps.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;“The 12 Step Program for Grad School”&lt;/a&gt;, Erik Zwick.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;starting&#34;&gt;Starting&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/hagertynw/grad-school-reflections/blob/master/grad_school_reflections.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;“Reflections on Grad School in Economics”&lt;/a&gt;, Nick Hagerty, 2020.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://law.vanderbilt.edu/phd/How_to_Survive_1st_Year.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;“How to survive your first year of graduate school in economics”&lt;/a&gt;, Matthew Pearson, 2005.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;during-the-phd&#34;&gt;During the PhD&lt;/h2&gt;
&lt;h3 id=&#34;mental-health&#34;&gt;Mental Health&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://scholar.harvard.edu/files/bolotnyy/files/bbb_mentalhealth_paper.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;“Graduate Student Mental Health: Lessons from American Economics Departments”&lt;/a&gt;, Bolotnyy, Valentin, Matthew Basilico, and Paul Barreira, 2021.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.insidehighered.com/news/2019/11/14/phd-student-poll-finds-mental-health-bullying-and-career-uncertainty-are-top&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;“Mental Health, Bullying, Career Uncertainty”&lt;/a&gt;, Colleen Flahert, 2019.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.sciencemag.org/careers/2019/03/how-mindfulness-can-help-phd-students-deal-mental-health-challenges&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;“How mindfulness can help Ph.D. students deal with mental health challenges”&lt;/a&gt;, Katie Langin, 2019.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.phdstudies.com/article/managing-your-mental-health-as-a-phd-student/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;“Managing Your Mental Health as a PhD Student”&lt;/a&gt;, Joanna Hughes, 2019.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.psychologytoday.com/us/blog/emotional-mastery/201904/what-makes-it-so-hard-ask-help&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;“What Makes It So Hard to Ask for Help?&amp;quot;&lt;/a&gt;, Joan Rosenberg, 2019.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.sciencemag.org/careers/2018/11/grad-school-depression-almost-took-me-end-road-i-found-new-start&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;“Grad school depression almost took me to the end of the road—but I found a new start”&lt;/a&gt;, Francis Aguisanda, 2018.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.nature.com/articles/nj7587-555a.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;“Faking it”&lt;/a&gt;, Chris Woolston, 2016.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://blogs.nature.com/naturejobs/2016/09/14/panic-and-a-phd/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;“Panic and a PhD”&lt;/a&gt;, Jack Leeming, 2016.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://qz.com/547641/theres-an-awful-cost-to-getting-a-phd-that-no-one-talks-about/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;“There’s an awful cost to getting a PhD that no one talks about”&lt;/a&gt;, Jennifer Walker, 2015.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;research-and-ideas&#34;&gt;Research and Ideas&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://www.ricardodahis.com/files/papers/Dahis_Advice_Research.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;“Advice for Academic Research”&lt;/a&gt;, Ricardo Dahis, 2021.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.aeaweb.org/articles?id=10.1257/jel.20191573&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;“Sins of Omission and the Practice of Economics”&lt;/a&gt;, George A. Akerlof, 2020.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://medium.com/@paul.niehaus/doing-research-18cb310529e0&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;“Doing research”&lt;/a&gt;, Paul Niehaus, 2019.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://static1.squarespace.com/static/55c143d9e4b0cb07521c6d17/t/5b4f409f575d1ff83c2f12d8/1531920545061/PhDGuidebook.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;“An unofficial guidebook for PhD students in economics and education”&lt;/a&gt;, Alex Eble, 2018.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://iu.box.com/s/0ha9gcq0t22kyyy1rqv15mkmauw1py18&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;“The Research Productivity of New PhDs in Economics: The Surprisingly High Non-Success of the Successful”&lt;/a&gt;, John P. Conley and Ali Sina Önder, 2014.&lt;/li&gt;
&lt;li&gt;[“How to get started on research in economics?&amp;quot;](&lt;a href=&#34;http://econ.lse.ac.uk/staff/spischke/phds/How&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://econ.lse.ac.uk/staff/spischke/phds/How&lt;/a&gt; to start.pdf), Steve Pischke, 2009.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://iu.box.com/s/km7cxhcxgfcdpk4cp38b47x7is7lum11&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;“The Importance of Stupidity in Scientific Research”&lt;/a&gt;, Martin A. Schwartz, 2008.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://stevepavlina.com/blog/2007/01/7-rules-for-maximizing-your-creative-output/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;“7 Rules for Maximizing Your Creative Output”&lt;/a&gt;, Steve Pavlina, 2007.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://people.ischool.berkeley.edu/~hal/Papers/how.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;“How To Build An Economic Model in Your Spare Time”&lt;/a&gt;, Hal. R. Varian, 1998.&lt;/li&gt;
&lt;li&gt;[“Ph.D. Thesis Research: Where do I Start?&amp;quot;](&lt;a href=&#34;http://www.columbia.edu/~drd28/Thesis&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://www.columbia.edu/~drd28/Thesis&lt;/a&gt; Research.pdf), Don Davis.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;presenting&#34;&gt;Presenting&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://david-schindler.de/unfair-questions/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;“Unfair Questions”&lt;/a&gt;, David Schindler, 2021.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/paulgp/beamer-tips/blob/master/slides.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;“Beamer Tips for Presentations”&lt;/a&gt;, Paul Goldsmith-Pinkham, 2020.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.princeton.edu/~reddings/tradephd/public_speaking_for_academic_economists.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;“Public Speaking for Academic Economists”&lt;/a&gt;, Rachel Meager, 2017.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.europeanjobmarketofeconomists.org/uploads/HowToPresent_LaFerrara.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;“How to present your job market paper”&lt;/a&gt;, Eliana La Ferrara, 2018.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://people.bu.edu/guren/Guren_HowToGiveALunchTalk.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;“How To Give a Lunch Talk”&lt;/a&gt;, Adam Guren, 2018.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://chrisblattman.com/2010/02/22/the-discussants-art/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;“The Discussant’s Art”&lt;/a&gt;, Chris Blattman, 2010.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://papers.ssrn.com/sol3/papers.cfm?abstract_id=1332144&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;“How to be a Great Conference Participants”&lt;/a&gt;, Art Carden, 2009.&lt;/li&gt;
&lt;li&gt;[“The “Big 5” and Other Ideas For Presentations”](&lt;a href=&#34;http://econ.lse.ac.uk/staff/spischke/phds/The&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://econ.lse.ac.uk/staff/spischke/phds/The&lt;/a&gt; Big 5.pdf), Cox, Donald, 2000.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://iu.box.com/s/aw92d7kl7xh5s4zsub8jq3qnknq9zcsi&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;“How to Give an Applied Micro Talk”&lt;/a&gt;, Jesse M. Shapiro.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://iu.box.com/s/37j3eip7x9fdg30n4eeepu92228eb999&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;“Tips on How to Avoid Disaster in Presentations”&lt;/a&gt;, Monika Piazzesi.&lt;/li&gt;
&lt;li&gt;[“Seminar Slides “](&lt;a href=&#34;https://www.ssc.wisc.edu/~bhansen/placement/Seminar&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.ssc.wisc.edu/~bhansen/placement/Seminar&lt;/a&gt; Slides.pdf), Bruce Hansen.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;writing&#34;&gt;Writing&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;[“5 Steps Toward a Paper”](&lt;a href=&#34;https://www.google.com/url?q=https%3A%2F%2Fwww.dropbox.com%2Fs%2Fq7wjaidl5w91srt%2FGuest&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.google.com/url?q=https%3A%2F%2Fwww.dropbox.com%2Fs%2Fq7wjaidl5w91srt%2FGuest&lt;/a&gt; lecture FS.pdf%3Fdl%3D0&amp;amp;sa=D&amp;amp;sntz=1&amp;amp;usg=AFQjCNG_nRs6QlkZzWBHAy0PjF4jfEYBAw), Frank Schilbach, 2019.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.nature.com/articles/d41586-019-02918-5&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;“Novelist Cormac McCarthy’s tips on how to write a great science paper”&lt;/a&gt;, Van Savage and Pamela Yeh, 2019.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://marcfbellemare.com/wordpress/12797&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;“The “Middle Bits” Formula for Applied Papers”&lt;/a&gt;, Marc Bellamare, 2018.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://marcfbellemare.com/wordpress/12060&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;“The Conclusion Formula”&lt;/a&gt;, Marc Bellamare, 2018.&lt;/li&gt;
&lt;li&gt;[“The Introduction Formula”](&lt;a href=&#34;https://www.albany.edu/spatial/training/5-The&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.albany.edu/spatial/training/5-The&lt;/a&gt; Introduction Formula.pdf), Keith Head, 2015.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.people.fas.harvard.edu/~pnikolov/resources/writingtips.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;“Writing Tips For Economics Research Papers”&lt;/a&gt;, Plamen Nikolov, 2013.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://economics.harvard.edu/files/economics/files/tenruleswriting.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;“The Ten Most Important Rules of Writing Your Job Market Paper”&lt;/a&gt;, Goldin, Claudia and Lawrence Katz, 2008.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://schwert.ssb.rochester.edu/aec510/phd_paper_writing.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;“Writing Tips for Ph.D. Students”&lt;/a&gt;, John Cochrane, 2005.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://qed.econ.queensu.ca/pub/faculty/sumon/mkremer_checklist_paper.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;“Writing Papers: A Checklist”&lt;/a&gt;, Michael Kremer.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;referiing&#34;&gt;Referiing&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://blog.academicsequitur.com/2019/06/30/how-to-write-a-good-referee-report/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;“How To Write A Good Referee Report”&lt;/a&gt;, Tatyana Deryugina, 2019.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://iu.box.com/s/lgmhqw5uxvrb7qdrhxxskzki9pcwx7o6&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;“How to Review Manuscripts”&lt;/a&gt;, Elsevier, 2015.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://marcfbellemare.com/wordpress/5542&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;“Contributing to Public Goods: My 20 Rules for Refereeing”&lt;/a&gt;, Marc F. Bellemare, 2012&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;finishing-the-phd&#34;&gt;Finishing the PhD&lt;/h2&gt;
&lt;h3 id=&#34;the-job-market&#34;&gt;The Job Market&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.aeaweb.org/content/file?id=869&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;“A Guide and Advice for Economists on the U.S. Junior Academic Job Market 2018-2019 Edition”&lt;/a&gt;, John Cawley, 2018.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://chrisblattman.com/job-market/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;“Academic job market advice for economics, political science, public policy, and other professional schools”&lt;/a&gt;, Blattman, Christopher, 2015.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.ericzwick.com/public_goods/love_the_market.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;“How I Learned to Stop Worrying and Love the Job Market”&lt;/a&gt;, Erik Zwick, 2014.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;the-private-sector&#34;&gt;The Private Sector&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.linkedin.com/pulse/my-journey-from-economics-phd-data-scientist-tech-rose-tan&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;“My Journey from Economics PhD to Data Scientist in Tech”&lt;/a&gt;, Rose Tan, 2021.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://scarlet-chen.medium.com/tech-industry-jobs-for-econ-phds-54a276dda80b&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;“Tech Industry Jobs for Econ PhDs”&lt;/a&gt;, Scarlet Chen, 2020.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://scarlet-chen.medium.com/my-journey-from-econ-phd-to-tech-part-1-interview-prep-networking-d256918410a2&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;“My Journey from Econ PhD to Tech”&lt;/a&gt;, Scarlet Chen, 2020.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.nature.com/articles/d41586-018-05838-y&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;“Why it is not a ‘failure’ to leave academia”&lt;/a&gt;, Philipp Kruger, 2018.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;the-tenure-track&#34;&gt;The Tenure Track&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://blogs.scientificamerican.com/guest-blog/the-awesomest-7-year-postdoc-or-how-i-learned-to-stop-worrying-and-love-the-tenure-track-faculty-life/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;“The Awesomest 7-Year Postdoc or: How I Learned to Stop Worrying and Love the Tenure-Track Faculty Life”&lt;/a&gt;, Radhika Nagpal, 2013.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;more&#34;&gt;More&lt;/h2&gt;
&lt;p&gt;You can find more resources here:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;AEA &lt;a href=&#34;https://www.aeaweb.org/about-aea/committees/cswep/mentoring/reading&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Mentoring Reading Materials&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Johannes Pfeifer &lt;a href=&#34;https://sites.google.com/site/pfeiferecon/job-market-resources&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Job Market Resources&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Kristoph Kronenberg &lt;a href=&#34;https://sites.google.com/view/christoph-kronenberg/home/resources&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Resources&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Patrick Button &lt;a href=&#34;https://www.patrickbutton.com/resources&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Resources&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Ryan Edwards &lt;a href=&#34;http://www.ryanbedwards.com/resources&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Resources&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Jennifer Doleac &lt;a href=&#34;http://jenniferdoleac.com/resources/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Resources&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Amanda Agan &lt;a href=&#34;https://sites.google.com/site/amandayagan/writingadvice&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Writing and Presentation Advice&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Random forum &lt;a href=&#34;http://www.inhe365.com/thread-17506-1-1.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Resource Collection&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Coding Resources for Social Sciences</title>
      <link>https://matteocourthoud.github.io/post/coding/</link>
      <pubDate>Tue, 24 Aug 2021 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/post/coding/</guid>
      <description>&lt;p&gt;In this page, I collect useful resources for coding for researchers in social sciences. A mention goes to &lt;a href=&#34;https://maxkasy.github.io/home/computationlinks/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Maximilian Kasy&lt;/a&gt; that inspired me to build this page.&lt;/p&gt;
&lt;p&gt;A quick legend:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;📗 book&lt;/li&gt;
&lt;li&gt;🌐 webpage&lt;/li&gt;
&lt;li&gt;📈 charts&lt;/li&gt;
&lt;li&gt;🎥 videos&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;econometrics-and-statistics&#34;&gt;Econometrics and Statistics&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;📗&lt;a href=&#34;https://www.ssc.wisc.edu/~bhansen/econometrics/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Bruce Hansen’s Econometrics&lt;/strong&gt;&lt;/a&gt;: By far the best freely available and regularly updated resource for Econometrics&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;machine-learning&#34;&gt;Machine Learning&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;📗&lt;a href=&#34;https://web.stanford.edu/~hastie/Papers/ESLII.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;The Elements of Statistical Learning&lt;/strong&gt;&lt;/a&gt;: General introduction to machine learning&lt;/li&gt;
&lt;li&gt;📗&lt;a href=&#34;http://www.gaussianprocess.org/gpml/chapters/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Gaussian Processes for Machine Learning&lt;/strong&gt;&lt;/a&gt;: Extremely useful tools for nonparametric Bayesian modeling&lt;/li&gt;
&lt;li&gt;📗&lt;a href=&#34;https://www.deeplearningbook.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Deep Learning&lt;/strong&gt;&lt;/a&gt;: The theory and implementation of neural nets&lt;/li&gt;
&lt;li&gt;📗&lt;a href=&#34;https://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning/understanding-machine-learning-theory-algorithms.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Understanding Machine Learning: From Theory to Algorithms&lt;/strong&gt;&lt;/a&gt;: An introduction to statistical learning theory in the tradition of Vapnik&lt;/li&gt;
&lt;li&gt;📗&lt;a href=&#34;http://www.incompleteideas.net/book/RLbook2018.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Reinforcement Learning - An Introduction&lt;/strong&gt;&lt;/a&gt;: Adaptive learning for Markov decision problems&lt;/li&gt;
&lt;li&gt;📗&lt;a href=&#34;http://jeffe.cs.illinois.edu/teaching/algorithms/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Algorithms&lt;/strong&gt;&lt;/a&gt;: Introduction to the theory of algorithms&lt;/li&gt;
&lt;li&gt;🌐&lt;a href=&#34;https://playground.tensorflow.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Tensorflow Playground&lt;/strong&gt;&lt;/a&gt;: Visualisation tool for neural networks&lt;/li&gt;
&lt;li&gt;🌐&lt;a href=&#34;https://www.fast.ai/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Artificial Intelligence&lt;/strong&gt;&lt;/a&gt;: Online lectures on AI&lt;/li&gt;
&lt;li&gt;🌐&lt;a href=&#34;https://global.oup.com/academic/product/the-ethical-algorithm-9780190948207&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;The Ethical Algorithm&lt;/strong&gt;&lt;/a&gt;: How to impose normative constraints on ML and other algorithms&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;python&#34;&gt;Python&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;🌐&lt;a href=&#34;https://realpython.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;RealPython&lt;/strong&gt;&lt;/a&gt;: Collection of Python tutorials, from introductory to advanced. Also contains &lt;a href=&#34;https://realpython.com/learning-paths/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;learning paths&lt;/a&gt; for specific topics&lt;/li&gt;
&lt;li&gt;🌐&lt;a href=&#34;https://python.quantecon.org/intro.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;QuantEcon Python&lt;/strong&gt;&lt;/a&gt; Tutorials and economic applications in Python, especially for macroeconomics&lt;/li&gt;
&lt;li&gt;🌐&lt;a href=&#34;https://blog.finxter.com/python-cheat-sheets/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Cheat Sheets&lt;/strong&gt;&lt;/a&gt;: Collection of cheat sheets for python&lt;/li&gt;
&lt;li&gt;🌐&lt;a href=&#34;https://docs.python-guide.org/writing/structure/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Structuring a Python project&lt;/strong&gt;&lt;/a&gt;: Advanced tutorial on how to structure a Python program&lt;/li&gt;
&lt;li&gt;🌐&lt;a href=&#34;https://www.softwaretestinghelp.com/python-ide-code-editors/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;IDE Guide&lt;/strong&gt;&lt;/a&gt;: Comparison of IDEs for Python. Suggested: &lt;a href=&#34;https://www.jetbrains.com/pycharm/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PyCharm&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;🌐&lt;a href=&#34;https://www.jetbrains.com/help/pycharm/configuring-remote-interpreters-via-ssh.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Configuring remote interpreters via SSH&lt;/strong&gt;&lt;/a&gt;: How to use Python remotely via SSH via PyCharm&lt;/li&gt;
&lt;li&gt;📈&lt;a href=&#34;https://www.kaggle.com/maheshdadhich/strength-of-visualization-python-visuals-tutorial/notebook&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Visualization in Python&lt;/strong&gt;&lt;/a&gt;: How to make nice graphs in Python, with a dedicated jupyter notebook&lt;/li&gt;
&lt;li&gt;📈&lt;a href=&#34;https://python-graph-gallery.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Python Graph Gallery&lt;/strong&gt;&lt;/a&gt;: Graph examples in Python&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;matlab&#34;&gt;Matlab&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;🌐&lt;a href=&#34;https://mathworks.com/help/matlab/matlab_oop/user-defined-classes.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;User defined classes in Matlab&lt;/strong&gt;&lt;/a&gt;: How to work with classes in Matlab&lt;/li&gt;
&lt;li&gt;🌐&lt;a href=&#34;https://am111.readthedocs.io/en/latest/jmatlab_use.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Julyter Notebooks&lt;/strong&gt;&lt;/a&gt;: How to run a jupyter notebook with Matlab kernel&lt;/li&gt;
&lt;li&gt;📈&lt;a href=&#34;https://www.bradleymonk.com/wp/how-to-make-professional-looking-plots-for-journal-publication-using-matlab-r2014a-and-r2014b/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Graph Tips in Matlab&lt;/strong&gt;&lt;/a&gt; and &lt;a href=&#34;https://mathworks.com/matlabcentral/answers/133372-how-to-make-nice-plots&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;link2&lt;/a&gt;: Suggestions on how to make pretty graphs in Matlab&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;julia&#34;&gt;Julia&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;🌐&lt;a href=&#34;https://docs.julialang.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Julia Manual&lt;/strong&gt;&lt;/a&gt;: Julia unfortunately lacks a big community and tutorials, but it has a very good manual&lt;/li&gt;
&lt;li&gt;🌐&lt;a href=&#34;https://julia.quantecon.org/index_toc.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;QuantEcon Julia&lt;/strong&gt;&lt;/a&gt; Tutorials and economic applications in Julia, especially for macroeconomics&lt;/li&gt;
&lt;li&gt;🌐&lt;a href=&#34;https://medium.com/dev-genius/what-is-the-best-ide-for-developing-in-the-programming-language-julia-484c913f07bc&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;IDE Guide&lt;/strong&gt;&lt;/a&gt;: Guide for IDEs for Julia. Suggested: Juno for Atom.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;r&#34;&gt;R&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;📗&lt;a href=&#34;https://cran.r-project.org/doc/manuals/r-release/R-intro.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;An Introduction to R&lt;/strong&gt;&lt;/a&gt;: Complete introduction to base R&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;📗&lt;a href=&#34;http://r4ds.had.co.nz/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;R for Data Science&lt;/strong&gt;&lt;/a&gt; Introduction to data analysis using R, focused on the tidyverse packages&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;📗&lt;a href=&#34;https://adv-r.hadley.nz/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Advanced R&lt;/strong&gt;&lt;/a&gt;: In depth discussion of programming in R&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;📗&lt;a href=&#34;https://bradleyboehmke.github.io/HOML/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Hands-On Machine Learning with R&lt;/strong&gt;&lt;/a&gt;: Fitting ML models in R&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;🌐&lt;a href=&#34;https://github.com/stan-dev/rstan/wiki/RStan-Getting-Started&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Bayesian statistics using Stan&lt;/strong&gt;&lt;/a&gt; and &lt;a href=&#34;https://mc-stan.org/docs/2_20/stan-users-guide/index.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;link&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;🌐&lt;a href=&#34;https://www.rstudio.com/resources/cheatsheets/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;RStudio Cheat Sheets&lt;/strong&gt;&lt;/a&gt; for various extensions, including data processing, visualization, writing web apps, …&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;📈&lt;a href=&#34;https://www.r-graph-gallery.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;R Graph Gallery&lt;/strong&gt;&lt;/a&gt;: Graph examples in R&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;📈&lt;a href=&#34;https://www.christophenicault.com/pages/visualizations/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Nice Graphs with code&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;A collection of elaborate graphs with code in R&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;others&#34;&gt;Others&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;📗&lt;a href=&#34;https://git-scm.com/book/en/v2&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Github Advanced&lt;/strong&gt;&lt;/a&gt;: Advanced guide for version control with Github&lt;/li&gt;
&lt;li&gt;🎥&lt;a href=&#34;https://missing.csail.mit.edu/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;The Missing Semester of Your CS Education&lt;/strong&gt;&lt;/a&gt; Video lectures and notes on tools for computer scientists (version control, debugging, …)&lt;/li&gt;
&lt;li&gt;📈&lt;a href=&#34;http://pgfplots.sourceforge.net/gallery.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;PGF plots in Latex&lt;/strong&gt;&lt;/a&gt;: Gallery and examples to make plots directly in Latex&lt;/li&gt;
&lt;li&gt;🌐&lt;a href=&#34;https://medium.com/@SergioPietri/how-to-setup-and-use-ssh-for-remote-connections-e86556d804dd&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Work remotely from server&lt;/strong&gt;&lt;/a&gt;: How to setup SSH for remote computing&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>How to access WRDS in Python</title>
      <link>https://matteocourthoud.github.io/post/wrds/</link>
      <pubDate>Tue, 09 Mar 2021 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/post/wrds/</guid>
      <description>&lt;p&gt;In this page, I explain how to work with the WRDS database using Python.&lt;/p&gt;
&lt;h2 id=&#34;setup&#34;&gt;Setup&lt;/h2&gt;
&lt;p&gt;The first thing we need to do, is to set up a connection to the &lt;a href=&#34;https://wrds-www.wharton.upenn.edu/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;WRDS database&lt;/a&gt;. I am assuming you have credentials to log in. Check the &lt;a href=&#34;https://wrds-www.wharton.upenn.edu/login/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;log in page&lt;/a&gt; to make sure.&lt;/p&gt;
&lt;p&gt;The second requirement is the &lt;a href=&#34;https://pypi.org/project/wrds/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;wrds&lt;/a&gt; Python package.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pip3 install wrds
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, in order to connect to the WRDS database, you just need to run the following commang in Python.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import wrds
db = wrds.Connection() 
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then, you will be propted to input your WRDS username and password.&lt;/p&gt;
&lt;p&gt;However, if you are using a Python IDE such as &lt;a href=&#34;https://www.jetbrains.com/pycharm/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PyCharm&lt;/a&gt;, you cannot run the command from the Python Console. Moreover, you might want to save your credentials once and for all, so that you don’t have to log in every time.&lt;/p&gt;
&lt;p&gt;First, walk to your home directory from the Terminal (&lt;code&gt;/Users/username&lt;/code&gt;).&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;cd
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now create an empty &lt;code&gt;.pgpass&lt;/code&gt; file.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;touch .pgpass
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now you write &lt;code&gt;your_username&lt;/code&gt; and &lt;code&gt;your_password&lt;/code&gt; into the &lt;code&gt;.pgpass&lt;/code&gt; file.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;echo &amp;quot;wrds-pgdata.wharton.upenn.edu:9737:wrds:your_username:your_password&amp;quot; &amp;gt;&amp;gt; .pgpass
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You also need to restrict permissions to the file.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;chmod 600 ~/.pgpass
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now you can go back to your Python IDE and access the database by just inputing your username.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import wrds
db = wrds.Connection(wrds_username=&#39;your_username&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If everything works, you should see the following output.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Loading library list...
Done
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;query&#34;&gt;Query&lt;/h2&gt;
&lt;p&gt;The available functions are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;db.connection()&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;db.list_libraries()&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;db.list_tables()&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;db.get_table()&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;db.describe_table()&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;db.raw_sql()&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;db.close()&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I make a simple example of how they work. Suppose first you want to list all the libraries in the WRDS database.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;db.list_libraries()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then you can list all the datasets within a given library.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;db.list_tables(library=&#39;comp&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Before downloading a table, you can describe it.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;df = db.describe_table(library=&#39;comp&#39;, table=&#39;funda&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To download the dataset you can use the &lt;code&gt;get_table()&lt;/code&gt; function.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;df = db.get_table(library=&#39;comp&#39;, table=&#39;funda&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can restrict both the rows and the columns you want to query.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;df_short = db.get_table(library=&#39;comp&#39;, table=&#39;funda&#39;, columns = [&#39;conm&#39;, &#39;gvkey&#39;, &#39;cik&#39;], obs=5)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can also query the database directly using SQL.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;df_sql = db.raw_sql(&#39;&#39;&#39;select conm, gvkey, cik FROM comp.funda WHERE fyear&amp;gt;2010 AND (indfmt=&#39;INDL&#39;)&#39;&#39;&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;sources&#34;&gt;Sources&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://wrds-www.wharton.upenn.edu/pages/support/programming-wrds/programming-python/querying-wrds-data-python/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Querying WRDS Data using Python&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://wrds-www.wharton.upenn.edu/documents/1443/wrds_connection.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Using Python on WRDS Platform&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://sites.duke.edu/kevinstandridge/2020/03/07/introduction-to-the-wrds-python-package/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Introduction to the WRDS Python Package&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://wizardkingz.github.io/wrdsdataaccesspython-tutorial/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;WRDS Data Access Via Python API&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Summer Schools in Economics</title>
      <link>https://matteocourthoud.github.io/post/summer_schools/</link>
      <pubDate>Tue, 10 Mar 2020 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/post/summer_schools/</guid>
      <description>&lt;p&gt;In this page, I collect information about summer schools in Economics.&lt;/p&gt;
&lt;p&gt;If you know about summer schools that are missing from this list, please either &lt;a href=&#34;mailto:matteo.courthoud@econ.uzh.ch&#34;&gt;contact me&lt;/a&gt; or &lt;a href=&#34;https://github.com/matteocourthoud/website/blob/master/content/post/summerschools/index.md&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;edit the table on Github&lt;/a&gt;!&lt;/p&gt;
&lt;h2 id=&#34;2020&#34;&gt;2020&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Name and Link&lt;/th&gt;
&lt;th&gt;Field&lt;/th&gt;
&lt;th&gt;Organizer&lt;/th&gt;
&lt;th&gt;Location&lt;/th&gt;
&lt;th&gt;Instructor(s)&lt;/th&gt;
&lt;th&gt;Dates&lt;/th&gt;
&lt;th&gt;Deadline&lt;/th&gt;
&lt;th&gt;Fee&lt;/th&gt;
&lt;th&gt;Aid&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://dseconf.org/dse2020&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Dynamic Structural Econometrics&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Econometrics, IO&lt;/td&gt;
&lt;td&gt;Econometrics Society&lt;/td&gt;
&lt;td&gt;Zurich&lt;/td&gt;
&lt;td&gt;John Rust et al.&lt;/td&gt;
&lt;td&gt;June 15-21&lt;/td&gt;
&lt;td&gt;March 15&lt;/td&gt;
&lt;td&gt;500$&lt;/td&gt;
&lt;td&gt;no&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://www.cresse.info/default.aspx?articleID=3398&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CRESSE&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Comp. Policy, IO&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;Crete&lt;/td&gt;
&lt;td&gt;various&lt;/td&gt;
&lt;td&gt;June 20 - July 02&lt;/td&gt;
&lt;td&gt;FCFS&lt;/td&gt;
&lt;td&gt;3200€&lt;/td&gt;
&lt;td&gt;-30%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://www.barcelonagse.eu/study/summer-school/digital-economy&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Digital Economy&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Comp. Policy, IO&lt;/td&gt;
&lt;td&gt;Barcelona GSE&lt;/td&gt;
&lt;td&gt;Barcelona&lt;/td&gt;
&lt;td&gt;Martin Peitz&lt;/td&gt;
&lt;td&gt;July 13-17&lt;/td&gt;
&lt;td&gt;March 10&lt;/td&gt;
&lt;td&gt;550€&lt;/td&gt;
&lt;td&gt;maybe&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://www.parisschoolofeconomics.eu/en/teaching/pse-summer-school/social-networks-platforms/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Social Networks, Platforms…&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Comp. Policy, IO&lt;/td&gt;
&lt;td&gt;PSE&lt;/td&gt;
&lt;td&gt;Paris&lt;/td&gt;
&lt;td&gt;from Paris&lt;/td&gt;
&lt;td&gt;June 15-19&lt;/td&gt;
&lt;td&gt;March 31&lt;/td&gt;
&lt;td&gt;1200€&lt;/td&gt;
&lt;td&gt;no&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;2019&#34;&gt;2019&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Name and Link&lt;/th&gt;
&lt;th&gt;Field&lt;/th&gt;
&lt;th&gt;Organizer&lt;/th&gt;
&lt;th&gt;Location&lt;/th&gt;
&lt;th&gt;Instructor(s)&lt;/th&gt;
&lt;th&gt;Dates&lt;/th&gt;
&lt;th&gt;Deadline&lt;/th&gt;
&lt;th&gt;Fee&lt;/th&gt;
&lt;th&gt;Aid&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://dseconf.org/dse2019&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Dynamic Structural Econometrics&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Econometrics, IO&lt;/td&gt;
&lt;td&gt;Econometrics Society&lt;/td&gt;
&lt;td&gt;Chicago&lt;/td&gt;
&lt;td&gt;John Rust et al.&lt;/td&gt;
&lt;td&gt;July 08-14&lt;/td&gt;
&lt;td&gt;March 15&lt;/td&gt;
&lt;td&gt;500$&lt;/td&gt;
&lt;td&gt;no&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;CRESSE&lt;/td&gt;
&lt;td&gt;Competition Policy, IO&lt;/td&gt;
&lt;td&gt;various&lt;/td&gt;
&lt;td&gt;Crete&lt;/td&gt;
&lt;td&gt;various&lt;/td&gt;
&lt;td&gt;June 20 - July 02&lt;/td&gt;
&lt;td&gt;FCFS&lt;/td&gt;
&lt;td&gt;3200€&lt;/td&gt;
&lt;td&gt;-30%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://www.cemfi.es/studies/css/course.asp?cu=10&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Empirical Analysis of Firm Performance&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;IO, Trade&lt;/td&gt;
&lt;td&gt;CEMFI&lt;/td&gt;
&lt;td&gt;Madrid&lt;/td&gt;
&lt;td&gt;Jan de Loecker&lt;/td&gt;
&lt;td&gt;August 19-23&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://www.cemfi.es/studies/css/course.asp?cu=16&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Panel Data Econometrics&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Econometrics&lt;/td&gt;
&lt;td&gt;CEMFI&lt;/td&gt;
&lt;td&gt;Madrid&lt;/td&gt;
&lt;td&gt;Steve Bond&lt;/td&gt;
&lt;td&gt;September 02-06&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;2018&#34;&gt;2018&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Name and Link&lt;/th&gt;
&lt;th&gt;Field&lt;/th&gt;
&lt;th&gt;Organizer&lt;/th&gt;
&lt;th&gt;Location&lt;/th&gt;
&lt;th&gt;Instructor(s)&lt;/th&gt;
&lt;th&gt;Dates&lt;/th&gt;
&lt;th&gt;Deadline&lt;/th&gt;
&lt;th&gt;Fee&lt;/th&gt;
&lt;th&gt;Aid&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://www.econ.ku.dk/cce/events/summerschool/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Dynamic Structural Models&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Econometrics, IO&lt;/td&gt;
&lt;td&gt;University of Copenhagen&lt;/td&gt;
&lt;td&gt;Copenhagen&lt;/td&gt;
&lt;td&gt;John Rust et al.&lt;/td&gt;
&lt;td&gt;May 28 - Jun 03&lt;/td&gt;
&lt;td&gt;March 15&lt;/td&gt;
&lt;td&gt;600€&lt;/td&gt;
&lt;td&gt;no&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://www.cemfi.es/studies/css/course_previous_years.asp?c=12&amp;amp;y=2018&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Empirical Analysis of Innovation in Oligopoly Industries&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
</description>
    </item>
    
  </channel>
</rss>
