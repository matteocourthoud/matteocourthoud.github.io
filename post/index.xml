<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts | Matteo Courthoud</title>
    <link>https://matteocourthoud.github.io/post/</link>
      <atom:link href="https://matteocourthoud.github.io/post/index.xml" rel="self" type="application/rss+xml" />
    <description>Posts</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><copyright>Theme edited by Matteo CourthoudÂ© - Want to have a similar website? [Guide here](https://matteocourthoud.github.io/post/website/).</copyright><lastBuildDate>Mon, 25 Apr 2022 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://matteocourthoud.github.io/media/icon_hu03e9b3967b83fd39296ec9da5ff1ea05_201175_512x512_fill_lanczos_center_3.png</url>
      <title>Posts</title>
      <link>https://matteocourthoud.github.io/post/</link>
    </image>
    
    <item>
      <title>DAGs and Bad Controls</title>
      <link>https://matteocourthoud.github.io/post/controls/</link>
      <pubDate>Mon, 25 Apr 2022 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/post/controls/</guid>
      <description>&lt;p&gt;When analyzing causal relationships, it is very hard to understand which variables to &lt;strong&gt;condition the analysis on&lt;/strong&gt;, i.e. how to &amp;ldquo;split&amp;rdquo; the data so that we are &lt;strong&gt;comparing apples to apples&lt;/strong&gt;. For example, if you want to understand the effect of having a tablet in class on studenta&#39; performance, it makes sense to compare schools where students have similar socio-economic backgrounds. Otherwise, the risk is that only wealthier students can afford a tablet and, without controlling for it, we might attribute the effect to tablets instead of the socio-economic background.&lt;/p&gt;
&lt;p&gt;When the treatment of interest comes from a proper &lt;strong&gt;randomized experiment&lt;/strong&gt;, we do not need to worry about conditioning on other variables. If tablets are distributed randomly across schools, and we have enough schools in the experiment, we do not have to worry about the socio-economic background of students. The only advantage of conditioning the analysis on some so-called &amp;ldquo;control variable&amp;rdquo; could be an increase in power. However, this is a different story.&lt;/p&gt;
&lt;p&gt;In this post, we are going to have a brief introduction to Directed Acyclic Graphs and how they can be useful to select variables to condition a causal analysis on. Not only DAGs provide visual intuition on which variables we need to &lt;em&gt;include&lt;/em&gt; in the analysis, but also on which variables we should &lt;em&gt;not include&lt;/em&gt;, and why.&lt;/p&gt;
&lt;h2 id=&#34;directed-acyclic-graphs&#34;&gt;Directed Acyclic Graphs&lt;/h2&gt;
&lt;h3 id=&#34;definitions&#34;&gt;Definitions&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Directed acyclic graphs&lt;/strong&gt; (&lt;strong&gt;DAG&lt;/strong&gt;s) provide a visual representation of the data generating process. Random variables are represented with letters (e.g. $X$) and causal relationships are represented with arrows (e.g. $\to$). For example, we interpret&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-mermaid&#34;&gt;flowchart LR
classDef white fill:#FFFFFF,stroke:#000000,stroke-width:2px
X((X)):::white --&amp;gt; Y((Y)):::white
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;as $X$ (possibly) causes $Y$. We call a &lt;strong&gt;path&lt;/strong&gt; between two variables $X$ and $Y$ any connection, &lt;em&gt;independently of the direction of the arrows&lt;/em&gt;. If all arrows point forward, we call it a &lt;strong&gt;causal path&lt;/strong&gt;, otherwise we call it a &lt;strong&gt;spurious path&lt;/strong&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-mermaid&#34;&gt;flowchart LR
classDef included fill:#DCDCDC,stroke:#000000,stroke-width:2px;
classDef excluded fill:#ffffff,stroke:#000000,stroke-width:2px;

X((X))
Y((Y))
Z1((Z1))
Z2((Z2))
Z3((Z3))

X --&amp;gt; Z1
Z1 --&amp;gt; Z2
Z3 --&amp;gt; Z2
Z3 --&amp;gt; Y

class X,Y included;
class Z1,Z2,Z3 excluded;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In the example above, we have a path between $X$ and $Y$ passing through the variables $Z_1$, $Z_2$, and $Z_3$. Since not all arrows point forward, the path is &lt;em&gt;spurious&lt;/em&gt; and there is no causal relationship of $X$ on $Y$. In fact, variable $Z_2$ is caused by both $Z_1$ and $Z_3$ and therefore &lt;strong&gt;blocks&lt;/strong&gt; the path.&lt;/p&gt;
&lt;p&gt;$Z_2$ is called a &lt;strong&gt;collider&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;The purpose of our analysis is to assess the &lt;strong&gt;causal relationship&lt;/strong&gt; between two variables $X$ and $Y$. Directed acyclic graphs are useful because they provide us instructions on which other variables $Z$ we need to &lt;strong&gt;condition&lt;/strong&gt; our analysis on. Conditioning the analysis on a variable means that we keep it fixed and we draw our conclusions &lt;em&gt;ceteris paribus&lt;/em&gt;. For example, in a linear regression framework, inserting another regressor $Z$ means that we are computing the best linear approximation of the conditional expectation function of $Y$ given $X$, &lt;em&gt;conditional&lt;/em&gt; on the observed values of $Z$.&lt;/p&gt;
&lt;h3 id=&#34;causality&#34;&gt;Causality&lt;/h3&gt;
&lt;p&gt;In order to assess causality, we want to &lt;strong&gt;close all spurious paths&lt;/strong&gt; between $X$ and $Y$. The &lt;strong&gt;questions&lt;/strong&gt; now are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;When is a path &lt;strong&gt;open&lt;/strong&gt;?
&lt;ul&gt;
&lt;li&gt;If it does not contain &lt;em&gt;colliders&lt;/em&gt;. Otherwise, it is &lt;em&gt;closed&lt;/em&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;How do you &lt;strong&gt;close an open path&lt;/strong&gt;?
&lt;ul&gt;
&lt;li&gt;You condition on &lt;em&gt;at least one&lt;/em&gt; intermediate variable.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;How do you &lt;strong&gt;open a closed path&lt;/strong&gt;?
&lt;ul&gt;
&lt;li&gt;You condition on &lt;em&gt;all&lt;/em&gt; colliders along the path.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Suppose we are again interested in the causal relationship of $X$ on $Y$. Let&amp;rsquo;s consider the following graph&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-mermaid&#34;&gt;flowchart LR
classDef included fill:#DCDCDC,stroke:#000000,stroke-width:2px;
classDef excluded fill:#ffffff,stroke:#000000,stroke-width:2px;

X((X))
Y((Y))
Z1((Z1))
Z2((Z2))
Z3((Z3))

X --&amp;gt; Y
X --&amp;gt; Z2
Z2 --&amp;gt; Y
Z1 --&amp;gt; X
Z1 --&amp;gt; Y
X --&amp;gt; Z3
Y --&amp;gt; Z3

class X,Y included;
class Z1,Z2,Z3 excluded;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In this case, apart from the direct path, there are &lt;strong&gt;three non-direct paths&lt;/strong&gt; between $X$ and $Y$ through the variables $Z_1$, $Z_2$, and $Z_3$.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s consider the case in which we analyze the relationship between $X$ and $Y$, ignoring all other variables.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;The path through $Z_1$ is &lt;strong&gt;open&lt;/strong&gt; but it is &lt;strong&gt;spurious&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;The path through $Z_2$ is &lt;strong&gt;open&lt;/strong&gt; and &lt;strong&gt;causal&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;The path through $Z_3$ is &lt;strong&gt;closed&lt;/strong&gt; since $Z_3$ is a &lt;em&gt;collider&lt;/em&gt; and it is &lt;strong&gt;spurious&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Let&amp;rsquo;s draw the same graph indicating in &lt;em&gt;grey&lt;/em&gt; variables that we are conditioning on, with &lt;em&gt;dotted lines&lt;/em&gt; closed paths, with &lt;em&gt;red lines&lt;/em&gt; spurious open paths, and with &lt;em&gt;green lines&lt;/em&gt; causal open paths.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-mermaid&#34;&gt;flowchart LR
classDef included fill:#DCDCDC,stroke:#000000,stroke-width:2px;
classDef excluded fill:#ffffff,stroke:#000000,stroke-width:2px;

X((X))
Y((Y))
Z1((Z1))
Z2((Z2))
Z3((Z3))

X --&amp;gt; Y
X --&amp;gt; Z2
Z2 --&amp;gt; Y
Z1 --&amp;gt; X
Z1 --&amp;gt; Y
X -.-&amp;gt; Z3
Y -.-&amp;gt; Z3

linkStyle 0,1,2 stroke:#00ff00,stroke-width:4px;
linkStyle 3,4 stroke:#ff0000,stroke-width:4px;
class X,Y included;
class Z1,Z2,Z3 excluded;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In this case, to assess the &lt;strong&gt;causal&lt;/strong&gt; relationship between $X$ and $Y$ we need to &lt;strong&gt;close&lt;/strong&gt; the path that passes through $Z_1$. We can do that by conditioning the analysis on $Z_1$.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-mermaid&#34;&gt;flowchart LR
classDef included fill:#DCDCDC,stroke:#000000,stroke-width:2px;
classDef excluded fill:#ffffff,stroke:#000000,stroke-width:2px;

X((X))
Y((Y))
Z1((Z1))
Z2((Z2))
Z3((Z3))

X --&amp;gt; Y
X --&amp;gt; Z2
Z2 --&amp;gt; Y
Z1 -.-&amp;gt; X
Z1 -.-&amp;gt; Y
X -.-&amp;gt; Z3
Y -.-&amp;gt; Z3

linkStyle 0,1,2 stroke:#00ff00,stroke-width:4px;
class X,Y,Z1 included;
class Z2,Z3 excluded;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we are able to recover the causal relationship between $X$ and $Y$ by conditioning on $Z_1$.&lt;/p&gt;
&lt;p&gt;What would happen if we were also &lt;strong&gt;conditioning on $Z_2$&lt;/strong&gt;? In this case, we would &lt;strong&gt;close&lt;/strong&gt; the path passing through $Z_2$ leaving only the &lt;em&gt;direct&lt;/em&gt; path between $X$ and $Y$ open. We would then recover only the &lt;strong&gt;direct effect&lt;/strong&gt; of $X$ on $Y$ and not the &lt;em&gt;indirect&lt;/em&gt; one.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-mermaid&#34;&gt;flowchart LR
classDef included fill:#DCDCDC,stroke:#000000,stroke-width:2px;
classDef excluded fill:#ffffff,stroke:#000000,stroke-width:2px;

X((X))
Y((Y))
Z1((Z1))
Z2((Z2))
Z3((Z3))

X --&amp;gt; Y
X -.-&amp;gt; Z2
Z2 -.-&amp;gt; Y
Z1 -.-&amp;gt; X
Z1 -.-&amp;gt; Y
X -.-&amp;gt; Z3
Y -.-&amp;gt; Z3

linkStyle 0 stroke:#00ff00,stroke-width:4px;
class X,Y,Z1,Z2 included;
class Z3 excluded;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What would happen if we were also &lt;strong&gt;conditioning on $Z_3$&lt;/strong&gt;? In this case, we would &lt;strong&gt;open&lt;/strong&gt; the path passing through $Z_3$ which is a &lt;strong&gt;spurious&lt;/strong&gt; path. We would then &lt;strong&gt;not&lt;/strong&gt; be able to recover the causal effect of $X$ on $Y$.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-mermaid&#34;&gt;flowchart LR
classDef included fill:#DCDCDC,stroke:#000000,stroke-width:2px;

X((X))
Y((Y))
Z1((Z1))
Z2((Z2))
Z3((Z3))

X --&amp;gt; Y
X -.-&amp;gt; Z2
Z2 -.-&amp;gt; Y
Z1 -.-&amp;gt; X
Z1 -.-&amp;gt; Y
X --&amp;gt; Z3
Y --&amp;gt; Z3

linkStyle 0 stroke:#00ff00,stroke-width:4px;
linkStyle 5,6 stroke:#ff0000,stroke-width:4px;
class X,Y,Z1,Z2,Z3 included;
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;example-class-size-and-math-scores&#34;&gt;Example: Class Size and Math Scores&lt;/h2&gt;
&lt;p&gt;Suppose you are interested in the &lt;strong&gt;effect of class size on math scores&lt;/strong&gt;. Are bigger classes better or worse for students&#39; performance?&lt;/p&gt;
&lt;p&gt;Assume that the data generating process can be represented with the following &lt;strong&gt;DAG&lt;/strong&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-mermaid&#34;&gt;flowchart TB
classDef included fill:#DCDCDC,stroke:#000000,stroke-width:2px;
classDef excluded fill:#ffffff,stroke:#000000,stroke-width:2px;
classDef unobserved fill:#ffffff,stroke:#000000,stroke-width:2px,stroke-dasharray: 5 5;


X((class size))
Y((math score))
Z1((class year))
Z2((good school))
Z3((math hours))
Z4((hist score))
U((ability))

X --&amp;gt; Y
Z1 --&amp;gt; X
X --&amp;gt; Z4
U --&amp;gt; Y
U --&amp;gt; Z4
Z2 --&amp;gt; X
Z2 --&amp;gt; Y
Z2 --&amp;gt; Z4
Z3 --&amp;gt; Y

class X,Y included;
class Z1,Z2,Z3,Z4 excluded;
class U unobserved;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The variables of interest are highlighted. Moreover, the dotted line around &lt;code&gt;ability&lt;/code&gt; indicates that this is a variable that we do not observe in the data.&lt;/p&gt;
&lt;p&gt;We can now load the data and check what it looks like.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;%matplotlib inline
%config InlineBackend.figure_format = &#39;retina&#39;
from src.utils import *
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from src.dgp import dgp_school

df = dgp_school().generate_data()
df.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;math_hours&lt;/th&gt;
      &lt;th&gt;history_hours&lt;/th&gt;
      &lt;th&gt;good_school&lt;/th&gt;
      &lt;th&gt;class_year&lt;/th&gt;
      &lt;th&gt;class_size&lt;/th&gt;
      &lt;th&gt;math_score&lt;/th&gt;
      &lt;th&gt;hist_score&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;15&lt;/td&gt;
      &lt;td&gt;13.009309&lt;/td&gt;
      &lt;td&gt;15.167024&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;19&lt;/td&gt;
      &lt;td&gt;13.047033&lt;/td&gt;
      &lt;td&gt;13.387456&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;25&lt;/td&gt;
      &lt;td&gt;8.330311&lt;/td&gt;
      &lt;td&gt;10.824070&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;22&lt;/td&gt;
      &lt;td&gt;11.322190&lt;/td&gt;
      &lt;td&gt;14.594394&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;15&lt;/td&gt;
      &lt;td&gt;12.338458&lt;/td&gt;
      &lt;td&gt;11.871626&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;What variables should we condition our regression on, in order to estimate the causal effect of &lt;code&gt;class size&lt;/code&gt; on &lt;code&gt;math scores&lt;/code&gt;?&lt;/p&gt;
&lt;p&gt;First of all, let&amp;rsquo;s look at what happens if we do not condition our analysis on any variable and we just regress &lt;code&gt;math score&lt;/code&gt; on &lt;code&gt;class size&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;smf.ols(&#39;math_score ~ class_size&#39;, df).fit().summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
       &lt;td&gt;&lt;/td&gt;         &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt;  &lt;td&gt;   12.0421&lt;/td&gt; &lt;td&gt;    0.259&lt;/td&gt; &lt;td&gt;   46.569&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;   11.535&lt;/td&gt; &lt;td&gt;   12.550&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;class_size&lt;/th&gt; &lt;td&gt;   -0.0399&lt;/td&gt; &lt;td&gt;    0.013&lt;/td&gt; &lt;td&gt;   -3.025&lt;/td&gt; &lt;td&gt; 0.003&lt;/td&gt; &lt;td&gt;   -0.066&lt;/td&gt; &lt;td&gt;   -0.014&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;The effect of &lt;code&gt;class_size&lt;/code&gt; is negative and statistically different from zero.&lt;/p&gt;
&lt;p&gt;But should we believe this estimated effect? Without controlling for anything, this is &lt;strong&gt;DAG representation&lt;/strong&gt; of the effect we are capturing.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-mermaid&#34;&gt;flowchart TB
classDef included fill:#DCDCDC,stroke:#000000,stroke-width:2px;
classDef excluded fill:#ffffff,stroke:#000000,stroke-width:2px;
classDef unobserved fill:#ffffff,stroke:#000000,stroke-width:2px,stroke-dasharray: 5 5;


X((class size))
Y((math score))
Z1((class year))
Z2((good school))
Z3((math hours))
Z4((hist score))
U((ability))

X --&amp;gt; Y
Z1 --&amp;gt; X
X -.-&amp;gt; Z4
U --&amp;gt; Y
U -.-&amp;gt; Z4
Z2 --&amp;gt; X
Z2 --&amp;gt; Y
Z2 --&amp;gt; Z4
Z3 --&amp;gt; Y

linkStyle 0 stroke:#00ff00,stroke-width:4px;
linkStyle 5,6 stroke:#ff0000,stroke-width:4px;

class X,Y included;
class Z1,Z2,Z3,Z4 excluded;
class U unobserved;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There is a &lt;strong&gt;spurious&lt;/strong&gt; path passing through &lt;code&gt;good school&lt;/code&gt; that &lt;strong&gt;biases&lt;/strong&gt; our estimated coefficient. Intuitively, being enrolled in a better school improves the students&#39; math scores and better schools might have smaller class sizes. We need to control for the quality of the school.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;smf.ols(&#39;math_score ~ class_size + good_school&#39;, df).fit().summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
       &lt;td&gt;&lt;/td&gt;          &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt;   &lt;td&gt;    4.7449&lt;/td&gt; &lt;td&gt;    0.247&lt;/td&gt; &lt;td&gt;   19.176&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    4.259&lt;/td&gt; &lt;td&gt;    5.230&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;class_size&lt;/th&gt;  &lt;td&gt;    0.2095&lt;/td&gt; &lt;td&gt;    0.010&lt;/td&gt; &lt;td&gt;   20.020&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    0.189&lt;/td&gt; &lt;td&gt;    0.230&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;good_school&lt;/th&gt; &lt;td&gt;    5.0807&lt;/td&gt; &lt;td&gt;    0.130&lt;/td&gt; &lt;td&gt;   39.111&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    4.826&lt;/td&gt; &lt;td&gt;    5.336&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;Now the estimate of the effect of &lt;code&gt;class size&lt;/code&gt; on &lt;code&gt;math score&lt;/code&gt; is &lt;strong&gt;unbiased&lt;/strong&gt;! Indeed, the true coefficient in the data generating process was $0.2$.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-mermaid&#34;&gt;flowchart TB
classDef included fill:#DCDCDC,stroke:#000000,stroke-width:2px;
classDef excluded fill:#ffffff,stroke:#000000,stroke-width:2px;
classDef unobserved fill:#ffffff,stroke:#000000,stroke-width:2px,stroke-dasharray: 5 5;


X((class size))
Y((math score))
Z1((class year))
Z2((good school))
Z3((math hours))
Z4((hist score))
U((ability))

X --&amp;gt; Y
Z1 --&amp;gt; X
X -.-&amp;gt; Z4
U --&amp;gt; Y
U -.-&amp;gt; Z4
Z2 -.-&amp;gt; X
Z2 -.-&amp;gt; Y
Z2 --&amp;gt; Z4
Z3 --&amp;gt; Y

linkStyle 0 stroke:#00ff00,stroke-width:4px;

class X,Y,Z2 included;
class Z1,Z3,Z4 excluded;
class U unobserved;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What would happen if we were to instead &lt;strong&gt;control for all variables&lt;/strong&gt;?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;smf.ols(&#39;math_score ~ class_size + good_school + math_hours + class_year + hist_score&#39;, df).fit().summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
       &lt;td&gt;&lt;/td&gt;          &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt;   &lt;td&gt;   -0.7847&lt;/td&gt; &lt;td&gt;    0.310&lt;/td&gt; &lt;td&gt;   -2.529&lt;/td&gt; &lt;td&gt; 0.012&lt;/td&gt; &lt;td&gt;   -1.394&lt;/td&gt; &lt;td&gt;   -0.176&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;class_size&lt;/th&gt;  &lt;td&gt;    0.1292&lt;/td&gt; &lt;td&gt;    0.010&lt;/td&gt; &lt;td&gt;   13.054&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    0.110&lt;/td&gt; &lt;td&gt;    0.149&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;good_school&lt;/th&gt; &lt;td&gt;    2.9815&lt;/td&gt; &lt;td&gt;    0.170&lt;/td&gt; &lt;td&gt;   17.533&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    2.648&lt;/td&gt; &lt;td&gt;    3.315&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;math_hours&lt;/th&gt;  &lt;td&gt;    1.0516&lt;/td&gt; &lt;td&gt;    0.048&lt;/td&gt; &lt;td&gt;   21.744&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    0.957&lt;/td&gt; &lt;td&gt;    1.147&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;class_year&lt;/th&gt;  &lt;td&gt;    0.0424&lt;/td&gt; &lt;td&gt;    0.037&lt;/td&gt; &lt;td&gt;    1.130&lt;/td&gt; &lt;td&gt; 0.259&lt;/td&gt; &lt;td&gt;   -0.031&lt;/td&gt; &lt;td&gt;    0.116&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;hist_score&lt;/th&gt;  &lt;td&gt;    0.4116&lt;/td&gt; &lt;td&gt;    0.027&lt;/td&gt; &lt;td&gt;   15.419&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    0.359&lt;/td&gt; &lt;td&gt;    0.464&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;The coefficient is again &lt;strong&gt;biased&lt;/strong&gt;. Why?&lt;/p&gt;
&lt;p&gt;We have opened a new spurious path by controlling for &lt;code&gt;hist score&lt;/code&gt;. In fact, &lt;code&gt;hist score&lt;/code&gt; is a &lt;strong&gt;collider&lt;/strong&gt; and controlling for it has opened a path through &lt;code&gt;hist score&lt;/code&gt; and &lt;code&gt;ability&lt;/code&gt; that was otherwise closed.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-mermaid&#34;&gt;flowchart TB
classDef included fill:#DCDCDC,stroke:#000000,stroke-width:2px;
classDef excluded fill:#ffffff,stroke:#000000,stroke-width:2px;
classDef unobserved fill:#ffffff,stroke:#000000,stroke-width:2px,stroke-dasharray: 5 5;


X((class size))
Y((math score))
Z1((class year))
Z2((good school))
Z3((math hours))
Z4((hist score))
U((ability))

X --&amp;gt; Y
Z1 --&amp;gt; X
X --&amp;gt; Z4
U --&amp;gt; Y
U --&amp;gt; Z4
Z2 -.-&amp;gt; X
Z2 -.-&amp;gt; Y
Z2 --&amp;gt; Z4
Z3 --&amp;gt; Y

linkStyle 0 stroke:#00ff00,stroke-width:4px;
linkStyle 2,3,4 stroke:#ff0000,stroke-width:4px;

class X,Y,Z1,Z2,Z3,Z4 included;
class U unobserved;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The example was inspired by the following tweet.&lt;/p&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;We can illustrate this with Model 16 of the &amp;quot;Crash Course in Good and Bad Controls&amp;quot; (&lt;a href=&#34;https://t.co/GcSNzhuVt2&#34;&gt;https://t.co/GcSNzhuVt2&lt;/a&gt;). Here X = class size, Y = math4, Z = read4, and U = student&amp;#39;s ability. Conditioning on Z opens the path X -&amp;gt; Z &amp;lt;- U -&amp;gt; Y and it is thus a &amp;quot;bad control.&amp;quot; &lt;a href=&#34;https://t.co/KNfqtsMWwB&#34;&gt;https://t.co/KNfqtsMWwB&lt;/a&gt; &lt;a href=&#34;https://t.co/lUSigNYSJj&#34;&gt;pic.twitter.com/lUSigNYSJj&lt;/a&gt;&lt;/p&gt;&amp;mdash; AnÃ¡lise Real (@analisereal) &lt;a href=&#34;https://twitter.com/analisereal/status/1502793254592401409?ref_src=twsrc%5Etfw&#34;&gt;March 12, 2022&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;

&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;In this post, we have seen how to use Directed Acyclic Graphs to select control variables in a causal analysis. DAGs are very helpful tools since they provide an intuitive graphical representation of causal relationships between random variables. Contrary to common intuition that &amp;ldquo;the more information the better&amp;rdquo;, sometimes including extra variables might bias the analysis, preventing a causal interpretation of the results. In particular, we must pay attention not to include &lt;em&gt;colliders&lt;/em&gt; that open &lt;em&gt;spurious&lt;/em&gt; paths that would otherwise be closed.&lt;/p&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;p&gt;[1] C. Cinelli, A. Forney, J. Pearl, &lt;a href=&#34;https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3689437&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;A Crash Course in Good and Bad Controls&lt;/a&gt; (2018), working paper.&lt;/p&gt;
&lt;p&gt;[2] J. Pearl, &lt;a href=&#34;http://bayes.cs.ucla.edu/BOOK-2K/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Causality&lt;/a&gt; (2009), Cambridge University Press.&lt;/p&gt;
&lt;p&gt;[3] S. Cunningham, Chapter 3 of &lt;a href=&#34;https://mixtape.scunning.com/dag.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The Causal Inference Mixtape&lt;/a&gt; (2021), Yale University Press.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Synthetic Control</title>
      <link>https://matteocourthoud.github.io/post/synthetic_control/</link>
      <pubDate>Tue, 19 Apr 2022 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/post/synthetic_control/</guid>
      <description>&lt;p&gt;In this tutorial, we are going to see how to estimate causal effects when the treatment is not randomly assigned, we have acces to a control group and we observe few units but potentially many time periods. These settings are extremely common in observational studies and, in these cases, claims of causality are relatively weak. However, there still exist methods for causal inference, under certain assumptions.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Requisites&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;For this tutorial, I assume you are familiar with the following concepts:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Rubin&amp;rsquo;s potential outcome framework&lt;/li&gt;
&lt;li&gt;Ordinary least squares regression&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://matteocourthoud.github.io/post/permutation_test/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Randomization/permutation inference&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://matteocourthoud.github.io/post/diff_in_diffs/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Difference in differences&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Academic Application&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;As an academic application, we are going to replicate &lt;a href=&#34;https://www.tandfonline.com/doi/abs/10.1198/jasa.2009.ap08746&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Synthetic Control Methods for Comparative Case Studies: Estimating the Effect of Californiaâs Tobacco Control Program&lt;/a&gt; (2010) by Abadie, Diamond and Hainmueller. The authors study the effect of a tobacco control program in California on smoking habits.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Business Case&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;TBD&lt;/p&gt;
&lt;h2 id=&#34;setting&#34;&gt;Setting&lt;/h2&gt;
&lt;p&gt;We assume that for a panel of i.i.d. subjects $i = 1, &amp;hellip;, n$ over time $t=1, &amp;hellip;,T$ we observed a tuple $(X_{it}, Y_{it})$ comprised of&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;a feature vector $X_{i,t} \in \mathbb R^p$&lt;/li&gt;
&lt;li&gt;a treatment assignment $D_i \in \lbrace 0, 1 \rbrace$&lt;/li&gt;
&lt;li&gt;a response $Y_{i,t} \in \mathbb R$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Moreover, one unit is treated at time $t^*$. We distinguish time periods before treatment and time periods after treatment.&lt;/p&gt;
&lt;p&gt;Crucially, treatment $D_i$ is not randomly assignment, therefore a difference in means between the treated unit(s) and the control group is not an unbiased estimator of the average treatment effect.&lt;/p&gt;
&lt;h2 id=&#34;synthetic-control&#34;&gt;Synthetic Control&lt;/h2&gt;
&lt;p&gt;The problem is that, as usual, we do not observe the counterfactual outcome for treated units, i.e. we do not know what would have happened to them, if they had not been treated. This is known as the &lt;strong&gt;fundamental problem of causal inference&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;The simplest approach, would be just to compare pre and post periods. This is called the &lt;strong&gt;event study&lt;/strong&gt; approach.&lt;/p&gt;
&lt;p&gt;However, we can do better than this. In fact, even though treatment was not randomly assigned, we still have access to some units that were not treated.&lt;/p&gt;
&lt;p&gt;For the outcome variable we have the following setup&lt;/p&gt;
&lt;p&gt;$$
Y =
\begin{bmatrix}
Y_{t, post} \ &amp;amp; Y_{c, post} \newline
Y_{t, pre} \ &amp;amp; Y_{c, pre}
\end{bmatrix}
$$&lt;/p&gt;
&lt;p&gt;which we can rewrite as&lt;/p&gt;
&lt;p&gt;$$
Y =
\begin{bmatrix}
Y^{(1)} _ {t, post} \ &amp;amp; Y^{(0)} _ {c, post} \newline
Y^{(0)} _ {t, pre} \ &amp;amp; Y^{(0)} _ {c, pre}
\end{bmatrix}
$$&lt;/p&gt;
&lt;p&gt;We basically have a &lt;strong&gt;missing data problem&lt;/strong&gt; since we do not observe $Y^{(0)} _ {t, post}$.&lt;/p&gt;
&lt;p&gt;Following Doudchenko and Inbens (2018), we can formulate an estimate of the counterfactual outcome for the treated unit as a linear combination of the observed outcomes for the control units.&lt;/p&gt;
&lt;p&gt;$$
\hat Y^{(0)} _ {t, post} = \alpha + \sum_{i \in c} \beta_{i} Y^{(0)} _ {i, post}
$$&lt;/p&gt;
&lt;p&gt;where&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the constant $\alpha$ allows for different averages between the two groups&lt;/li&gt;
&lt;li&gt;the weights $\beta_i$ are allowed to vary across control units $i$
&lt;ul&gt;
&lt;li&gt;otherwise it would be a diff-in-diff&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;weights&#34;&gt;Weights&lt;/h3&gt;
&lt;p&gt;How should we choose which weights to use? One &lt;strong&gt;option&lt;/strong&gt; could be to simply run a linear regression of the pre-treatment outcome of the treatment unit $Y^{(0)} _ {t, pre}$ on the pre-treatment outcome of the control group $Y^{(0)} _ {c, pre}$.&lt;/p&gt;
&lt;p&gt;There are two problems with this approach:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;the weights might be negative&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;how should we interpret them?&lt;/li&gt;
&lt;li&gt;they make no sense in the potential outcome framework&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;weights could be greater or lower than one&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;therefore we would not be able to interpret the synthetic control as a weighted average of untreated units&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;To solve this problem, Abadie et al. (2010) propose the following weights:&lt;/p&gt;
&lt;p&gt;$$
\hat \beta = \arg \min_{\beta} || \boldsymbol X_t - \boldsymbol \beta \boldsymbol X_c || = \sqrt{ \sum_{p} \left( X_{t, p}  - \sum_{i \in c} \beta_{p} X_{c, p} \right)^2 }
\quad \text{s.t.} \quad \sum_{p} \beta_p = 1 \quad \text{and} \quad \beta_p \geq 0 \ \forall p
$$&lt;/p&gt;
&lt;p&gt;With this approach we get an interpretable counterfactual as a weighted avarage of untreated units.&lt;/p&gt;
&lt;h3 id=&#34;synthetic-control-vs-ols&#34;&gt;Synthetic Control vs OLS&lt;/h3&gt;
&lt;p&gt;What are the advantages and disadvantages of synthetic control methods with respect to a simple regression?&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;As long as we use positive weights that are constrained to sum to one, the method avoid extrapolation&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;we will never go out of the support of the data&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;It can be &amp;ldquo;pre-registered&amp;rdquo; in the sense that you don&amp;rsquo;t need post-treatment observations to build the method&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;could avoid p-hacking and cherry picking&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Weights make explicit the counterfactual analysis&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;one can look at the weights and understand which comparison we are making&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;It&amp;rsquo;s a bridge between quantitative and qualitative research&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;can be used to inspect single-treated unit cases&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;academic-application&#34;&gt;Academic Application&lt;/h2&gt;
&lt;p&gt;As an academic application, we are going to replicate &lt;a href=&#34;https://www.tandfonline.com/doi/abs/10.1198/jasa.2009.ap08746&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Synthetic Control Methods for Comparative Case Studies: Estimating the Effect of Californiaâs Tobacco Control Program&lt;/a&gt; (2010) by Abadie, Diamond and Hainmueller. The authors study the effect of a tobacco control program in California on smoking habits.&lt;/p&gt;
&lt;p&gt;This is how the authors describe the policy:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Its primary effect is to impose a 25-cent per pack state excise tax on the sale of tobacco cigarettes within California, with approximately equivalent excise taxes similarly imposed on the retail sale of other commercial tobacco products, such as cigars and chewing tobacco. Additional restrictions placed on the sale of tobacco include a ban on cigarette vending machines in public areas accessible by juveniles, and a ban on the individual sale of single cigarettes. Revenue generated by the act was earmarked for various environmental and health care programs, and anti-tobacco advertisements.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Let&amp;rsquo;s start by loading the data. We have information on cigarette costs, sales, tax rate and revenues for all US states for years from 1970 to 2014.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;%matplotlib inline
%config InlineBackend.figure_format = &#39;retina&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from src.utils import *
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df = pd.read_csv(&#39;data/adh10.csv&#39;)
df.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;state&lt;/th&gt;
      &lt;th&gt;year&lt;/th&gt;
      &lt;th&gt;cig_sales&lt;/th&gt;
      &lt;th&gt;lnincome&lt;/th&gt;
      &lt;th&gt;beer&lt;/th&gt;
      &lt;th&gt;age15to24&lt;/th&gt;
      &lt;th&gt;retprice&lt;/th&gt;
      &lt;th&gt;california&lt;/th&gt;
      &lt;th&gt;after_treatment&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;Alabama&lt;/td&gt;
      &lt;td&gt;1970&lt;/td&gt;
      &lt;td&gt;89.800003&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;0.178862&lt;/td&gt;
      &lt;td&gt;39.599998&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;Alabama&lt;/td&gt;
      &lt;td&gt;1971&lt;/td&gt;
      &lt;td&gt;95.400002&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;0.179928&lt;/td&gt;
      &lt;td&gt;42.700001&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;Alabama&lt;/td&gt;
      &lt;td&gt;1972&lt;/td&gt;
      &lt;td&gt;101.099998&lt;/td&gt;
      &lt;td&gt;9.498476&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;0.180994&lt;/td&gt;
      &lt;td&gt;42.299999&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;Alabama&lt;/td&gt;
      &lt;td&gt;1973&lt;/td&gt;
      &lt;td&gt;102.900002&lt;/td&gt;
      &lt;td&gt;9.550107&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;0.182060&lt;/td&gt;
      &lt;td&gt;42.099998&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;Alabama&lt;/td&gt;
      &lt;td&gt;1974&lt;/td&gt;
      &lt;td&gt;108.199997&lt;/td&gt;
      &lt;td&gt;9.537163&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;0.183126&lt;/td&gt;
      &lt;td&gt;43.099998&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;Is the sample balanced? Let&amp;rsquo;s check the distribution of covariates, by treatment assignment.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from causalml.match import create_table_one

create_table_one(df, &#39;california&#39;, [&#39;cig_sales&#39;, &#39;lnincome&#39;, &#39;beer&#39;, &#39;age15to24&#39;, &#39;retprice&#39;])
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;Control&lt;/th&gt;
      &lt;th&gt;Treatment&lt;/th&gt;
      &lt;th&gt;SMD&lt;/th&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;Variable&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;n&lt;/th&gt;
      &lt;td&gt;1178&lt;/td&gt;
      &lt;td&gt;31&lt;/td&gt;
      &lt;td&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;age15to24&lt;/th&gt;
      &lt;td&gt;0.18 (0.02)&lt;/td&gt;
      &lt;td&gt;0.18 (0.01)&lt;/td&gt;
      &lt;td&gt;0.0454&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;beer&lt;/th&gt;
      &lt;td&gt;23.46 (4.26)&lt;/td&gt;
      &lt;td&gt;22.26 (2.11)&lt;/td&gt;
      &lt;td&gt;-0.3559&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;cig_sales&lt;/th&gt;
      &lt;td&gt;119.53 (32.60)&lt;/td&gt;
      &lt;td&gt;94.59 (30.01)&lt;/td&gt;
      &lt;td&gt;-0.7962&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;lnincome&lt;/th&gt;
      &lt;td&gt;9.86 (0.17)&lt;/td&gt;
      &lt;td&gt;10.07 (0.08)&lt;/td&gt;
      &lt;td&gt;1.6107&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;retprice&lt;/th&gt;
      &lt;td&gt;108.04 (64.00)&lt;/td&gt;
      &lt;td&gt;119.92 (77.90)&lt;/td&gt;
      &lt;td&gt;0.1667&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;The sample is quite unbalanced. In particular, California is a richer state with a higher income, &lt;code&gt;lnincome&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;We are interested in the sales of cigarettes &lt;code&gt;cig_sales&lt;/code&gt; over time. Let&amp;rsquo;s start by reshaping the dataset so that each state is a time series.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df = df.pivot(index=&#39;year&#39;, columns=&#39;state&#39;, values=&#39;cig_sales&#39;).reset_index()
df.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;state&lt;/th&gt;
      &lt;th&gt;year&lt;/th&gt;
      &lt;th&gt;Alabama&lt;/th&gt;
      &lt;th&gt;Arkansas&lt;/th&gt;
      &lt;th&gt;California&lt;/th&gt;
      &lt;th&gt;Colorado&lt;/th&gt;
      &lt;th&gt;Connecticut&lt;/th&gt;
      &lt;th&gt;Delaware&lt;/th&gt;
      &lt;th&gt;Georgia&lt;/th&gt;
      &lt;th&gt;Idaho&lt;/th&gt;
      &lt;th&gt;Illinois&lt;/th&gt;
      &lt;th&gt;...&lt;/th&gt;
      &lt;th&gt;South Carolina&lt;/th&gt;
      &lt;th&gt;South Dakota&lt;/th&gt;
      &lt;th&gt;Tennessee&lt;/th&gt;
      &lt;th&gt;Texas&lt;/th&gt;
      &lt;th&gt;Utah&lt;/th&gt;
      &lt;th&gt;Vermont&lt;/th&gt;
      &lt;th&gt;Virginia&lt;/th&gt;
      &lt;th&gt;West Virginia&lt;/th&gt;
      &lt;th&gt;Wisconsin&lt;/th&gt;
      &lt;th&gt;Wyoming&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;1970&lt;/td&gt;
      &lt;td&gt;89.800003&lt;/td&gt;
      &lt;td&gt;100.300003&lt;/td&gt;
      &lt;td&gt;123.000000&lt;/td&gt;
      &lt;td&gt;124.800003&lt;/td&gt;
      &lt;td&gt;120.000000&lt;/td&gt;
      &lt;td&gt;155.000000&lt;/td&gt;
      &lt;td&gt;109.900002&lt;/td&gt;
      &lt;td&gt;102.400002&lt;/td&gt;
      &lt;td&gt;124.800003&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;103.599998&lt;/td&gt;
      &lt;td&gt;92.699997&lt;/td&gt;
      &lt;td&gt;99.800003&lt;/td&gt;
      &lt;td&gt;106.400002&lt;/td&gt;
      &lt;td&gt;65.500000&lt;/td&gt;
      &lt;td&gt;122.599998&lt;/td&gt;
      &lt;td&gt;124.300003&lt;/td&gt;
      &lt;td&gt;114.500000&lt;/td&gt;
      &lt;td&gt;106.400002&lt;/td&gt;
      &lt;td&gt;132.199997&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;1971&lt;/td&gt;
      &lt;td&gt;95.400002&lt;/td&gt;
      &lt;td&gt;104.099998&lt;/td&gt;
      &lt;td&gt;121.000000&lt;/td&gt;
      &lt;td&gt;125.500000&lt;/td&gt;
      &lt;td&gt;117.599998&lt;/td&gt;
      &lt;td&gt;161.100006&lt;/td&gt;
      &lt;td&gt;115.699997&lt;/td&gt;
      &lt;td&gt;108.500000&lt;/td&gt;
      &lt;td&gt;125.599998&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;115.000000&lt;/td&gt;
      &lt;td&gt;96.699997&lt;/td&gt;
      &lt;td&gt;106.300003&lt;/td&gt;
      &lt;td&gt;108.900002&lt;/td&gt;
      &lt;td&gt;67.699997&lt;/td&gt;
      &lt;td&gt;124.400002&lt;/td&gt;
      &lt;td&gt;128.399994&lt;/td&gt;
      &lt;td&gt;111.500000&lt;/td&gt;
      &lt;td&gt;105.400002&lt;/td&gt;
      &lt;td&gt;131.699997&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;1972&lt;/td&gt;
      &lt;td&gt;101.099998&lt;/td&gt;
      &lt;td&gt;103.900002&lt;/td&gt;
      &lt;td&gt;123.500000&lt;/td&gt;
      &lt;td&gt;134.300003&lt;/td&gt;
      &lt;td&gt;110.800003&lt;/td&gt;
      &lt;td&gt;156.300003&lt;/td&gt;
      &lt;td&gt;117.000000&lt;/td&gt;
      &lt;td&gt;126.099998&lt;/td&gt;
      &lt;td&gt;126.599998&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;118.699997&lt;/td&gt;
      &lt;td&gt;103.000000&lt;/td&gt;
      &lt;td&gt;111.500000&lt;/td&gt;
      &lt;td&gt;108.599998&lt;/td&gt;
      &lt;td&gt;71.300003&lt;/td&gt;
      &lt;td&gt;138.000000&lt;/td&gt;
      &lt;td&gt;137.000000&lt;/td&gt;
      &lt;td&gt;117.500000&lt;/td&gt;
      &lt;td&gt;108.800003&lt;/td&gt;
      &lt;td&gt;140.000000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;1973&lt;/td&gt;
      &lt;td&gt;102.900002&lt;/td&gt;
      &lt;td&gt;108.000000&lt;/td&gt;
      &lt;td&gt;124.400002&lt;/td&gt;
      &lt;td&gt;137.899994&lt;/td&gt;
      &lt;td&gt;109.300003&lt;/td&gt;
      &lt;td&gt;154.699997&lt;/td&gt;
      &lt;td&gt;119.800003&lt;/td&gt;
      &lt;td&gt;121.800003&lt;/td&gt;
      &lt;td&gt;124.400002&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;125.500000&lt;/td&gt;
      &lt;td&gt;103.500000&lt;/td&gt;
      &lt;td&gt;109.699997&lt;/td&gt;
      &lt;td&gt;110.400002&lt;/td&gt;
      &lt;td&gt;72.699997&lt;/td&gt;
      &lt;td&gt;146.800003&lt;/td&gt;
      &lt;td&gt;143.100006&lt;/td&gt;
      &lt;td&gt;116.599998&lt;/td&gt;
      &lt;td&gt;109.500000&lt;/td&gt;
      &lt;td&gt;141.199997&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;1974&lt;/td&gt;
      &lt;td&gt;108.199997&lt;/td&gt;
      &lt;td&gt;109.699997&lt;/td&gt;
      &lt;td&gt;126.699997&lt;/td&gt;
      &lt;td&gt;132.800003&lt;/td&gt;
      &lt;td&gt;112.400002&lt;/td&gt;
      &lt;td&gt;151.300003&lt;/td&gt;
      &lt;td&gt;123.699997&lt;/td&gt;
      &lt;td&gt;125.599998&lt;/td&gt;
      &lt;td&gt;131.899994&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;129.699997&lt;/td&gt;
      &lt;td&gt;108.400002&lt;/td&gt;
      &lt;td&gt;114.800003&lt;/td&gt;
      &lt;td&gt;114.699997&lt;/td&gt;
      &lt;td&gt;75.599998&lt;/td&gt;
      &lt;td&gt;151.800003&lt;/td&gt;
      &lt;td&gt;149.600006&lt;/td&gt;
      &lt;td&gt;119.900002&lt;/td&gt;
      &lt;td&gt;111.800003&lt;/td&gt;
      &lt;td&gt;145.800003&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;5 rows Ã 40 columns&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;In an ideal world, one could use the average of all the other states in the United States as a control group for California. Let&amp;rsquo;s plot the average cigarette consumption over time.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;states = [c for c in df.columns if c not in [&#39;year&#39;]]
df[&#39;Other States&#39;] = df[[s for s in states if s != &#39;California&#39;]].mean(axis=1)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def plot_lines(df, line1, line2, hline=True):
    sns.lineplot(x=df[&#39;year&#39;], y=df[line1].values, label=line1)
    sns.lineplot(x=df[&#39;year&#39;], y=df[line2].values, label=line2)
    plt.axvline(x=1988, ls=&amp;quot;:&amp;quot;, color=&#39;C2&#39;, label=&#39;Proposition 99&#39;, zorder=1)
    plt.legend();
    plt.title(&amp;quot;Per-capita cigarette sales (in packs)&amp;quot;);
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plot_lines(df, &#39;California&#39;, &#39;Other States&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/synthetic_control_22_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;People in California on average smoke less than in other states. This would not be a problem in a diff-in-diff setting if&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;we had enough observations in each group&lt;/li&gt;
&lt;li&gt;trends were parallel&lt;/li&gt;
&lt;li&gt;other diff-in-diff assumptions were holding&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;However, we have just one treated unit and very few control units. Morover, the trends were definitely not parallel before treatment. Therefore, it feels quite a stretch to attribute the differences in cigarette sales post 1989 to Proposition 99 alone.&lt;/p&gt;
&lt;p&gt;The idea is to exploit the time dimension and &lt;strong&gt;build a synthetic control&lt;/strong&gt; state for California. Maybe no single state is a good control, but a combination of them could actually provide a good approximation to California. Moreover, we can exploit the time dimension to compensate for the fact that we have few observations. We are going to try to match the sales of cigarettes in california for all the pre-treatment years, from 1970 to 1988.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s start by using &lt;code&gt;LinearRegression&lt;/code&gt; of &lt;code&gt;cigsales&lt;/code&gt; in California onto &lt;code&gt;cigsales&lt;/code&gt; of all the other states, pre 1989.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from sklearn.linear_model import LinearRegression

def synth_predict(df, state, model):
    y = df.loc[df[&#39;year&#39;] &amp;lt;= 1988, state]
    other_states = [c for c in states if c not in [&#39;year&#39;, state]]
    X = df.loc[df[&#39;year&#39;] &amp;lt;= 1988, other_states]
    df[f&#39;Synthetic {state}&#39;] = model.fit(X, y).predict(df[other_states])
    return model
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;coef = synth_predict(df, &#39;California&#39;, LinearRegression()).coef_
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;How well did we predict pre-1989 sales of cigarettes in California? If we believe that that&amp;rsquo;s a sensible control, what is the treatment effect?&lt;/p&gt;
&lt;p&gt;We can visually answe both questions by plotting the actual sales of cigarettes in California against the predicted ones.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plot_lines(df, &#39;California&#39;, &#39;Synthetic California&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/synthetic_control_27_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;It looks like the policy had a sensible &lt;strong&gt;negative effect&lt;/strong&gt; on cigarette sales: the predicted trend is higher than the actual sales and diverges right after the approval of Proposition 99.&lt;/p&gt;
&lt;p&gt;We can also observe that we are clearly &lt;strong&gt;overfitting&lt;/strong&gt;: the pre-policy predicted cigarette sales line is perfectly overlapping with the actual data.&lt;/p&gt;
&lt;p&gt;Another problem concerns the &lt;strong&gt;weights&lt;/strong&gt;. We have not set any constraint on the weights.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_states = pd.DataFrame({&#39;state&#39;: df.columns[[i for i in range(2,40) if i!=3]], &#39;ols_coef&#39;: coef[1:]})
sns.barplot(data=df_states, x=&#39;ols_coef&#39;, y=&#39;state&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/synthetic_control_29_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;We have many &lt;strong&gt;negative weights&lt;/strong&gt;, which do not make much sense from a causal inference perspective. Since we would like to interpret our synthetic control as a &lt;strong&gt;weighted average&lt;/strong&gt; of untreated states, all weights should be positive  and they should sum to one.&lt;/p&gt;
&lt;p&gt;To address both concerns, we are going to build a new estimator called &lt;code&gt;SyntheticControl()&lt;/code&gt; which constrains the weights to be positive and to sum to one.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from typing import List
from operator import add
from toolz import reduce, partial
from scipy.optimize import fmin_slsqp

class SyntheticControl():
    
    # Loss function
    def loss(self, W, X, y) -&amp;gt; float:
        return np.sqrt(np.mean((y - X.dot(W))**2))

    # Fit model
    def fit(self, X, y):
        w_start = [1/X.shape[1]]*X.shape[1]
        self.coef_ = fmin_slsqp(partial(self.loss, X=X, y=y),
                         np.array(w_start),
                         f_eqcons=lambda x: np.sum(x) - 1,
                         bounds=[(0.0, 1.0)]*len(w_start),
                         disp=False)
        self.mse = self.loss(W=self.coef_, X=X, y=y)
        return self
    
    # Predict 
    def predict(self, X):
        return X.dot(self.coef_)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can now plot the actual and predicted cigarette sales in California using the &lt;code&gt;SyntheticControl&lt;/code&gt; method.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;coef_new = synth_predict(df, &#39;California&#39;, SyntheticControl()).coef_
plot_lines(df, &#39;California&#39;, &#39;Synthetic California&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/synthetic_control_33_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;It looks like the effect is again negative. However, let&amp;rsquo;s plot the &lt;strong&gt;difference&lt;/strong&gt; between the two lines to better visualize the magnitude.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def plot_difference(df, state, vline=True, hline=True, **kwargs):
    sns.lineplot(x=df[&#39;year&#39;], y=df[state] - df[f&#39;Synthetic {state}&#39;], **kwargs)
    if vline: 
        plt.axvline(x=1988, ls=&amp;quot;:&amp;quot;, color=&#39;C2&#39;, label=&#39;Proposition 99&#39;, zorder=1)
        plt.legend()
    if hline: sns.lineplot(x=df[&#39;year&#39;], y=0, lw=1, color=&#39;black&#39;, zorder=1)
    plt.title(&amp;quot;Normalized per-capita cigarette sales (in packs)&amp;quot;);
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plot_difference(df, &#39;California&#39;, label=&#39;California&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/synthetic_control_36_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;The difference is clearly negative and decreasing over time. Moreover, now we see that the estimator is not overfitting the pre-period anymore. The reason is that the non-negativity constraint is constraining most coefficients to be zero (as &lt;a href=&#34;https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Lasso&lt;/a&gt; does).&lt;/p&gt;
&lt;p&gt;We can visualize it by plotting the distribution of coefficients.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_states[&#39;coef_synth&#39;] = coef_new[1:]
sns.barplot(data=df_states, x=&#39;coef_synth&#39;, y=&#39;state&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/synthetic_control_38_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;What about &lt;strong&gt;inference&lt;/strong&gt;? Is the estimate significantly different from zero? Or, more practically, &amp;ldquo;&lt;em&gt;how unusual is this estimate under the null hypothesis of no policy effect&lt;/em&gt;?&amp;rdquo;.&lt;/p&gt;
&lt;p&gt;We are going to perform a &lt;strong&gt;randomization/permutation test&lt;/strong&gt; in order to answer this question. The &lt;strong&gt;idea&lt;/strong&gt; is that if the policy has no effect, the effect we observe for California should not be significantly different from the effect we observe for any other state.&lt;/p&gt;
&lt;p&gt;Therefore, we are going to replicate the procedure above, but for all other states and observe how unusual is California.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;for state in states:
    synth_predict(df, state, SyntheticControl())
    plot_difference(df, state, vline=False, alpha=0.2, color=&#39;grey&#39;)
plot_difference(df, &#39;California&#39;, label=&#39;California&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/synthetic_control_40_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;From the graph we notice two things. First, the effect for California is quite extreme and therefore likely not to be driven by random noise.&lt;/p&gt;
&lt;p&gt;Second, we also notice that there are a couple of states for which we cannot fit the pre-trend very well. This is expected since, for each state, we are building the counterfactual trend as a &lt;strong&gt;convex combination&lt;/strong&gt; of all other states. States that are quite extreme in terms of cigarette consumpion are very useful to build the counterfactuals of other states, but it&amp;rsquo;s hard to build a counterfactual for them. Not to bias the analysis, let&amp;rsquo;s exclude states for which we cannot build a &amp;ldquo;good enough&amp;rdquo; counterfectual, in terms of pre-treatment MSE.&lt;/p&gt;
&lt;p&gt;$$
MSE_{pre} = \frac{1}{n} \sum_{t \in \text{pre}} \left( Y_t - \hat Y_t \right)^2
$$&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;for state in states:
    mse = synth_predict(df, state, SyntheticControl()).mse
    if mse &amp;lt; 15:
        plot_difference(df, state, vline=False, alpha=0.2, color=&#39;grey&#39;)
plot_difference(df, &#39;California&#39;, label=&#39;California&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/synthetic_control_42_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;After exluding extreme observations, it looks like the effect for California is very unusual, especially if we consider a one-sided hypothesis test (it feels weird to assume that the policy could ever increase cigarette sales).&lt;/p&gt;
&lt;p&gt;One &lt;strong&gt;statistic&lt;/strong&gt; that the authors suggest to perform a randomization test is the ratio between pre-treatment MSE and post-treatment MSE.&lt;/p&gt;
&lt;p&gt;$$
\lambda = \frac{MSE_{post}}{MSE_{pre}} = \frac{\frac{1}{n} \sum_{t \in \text{post}} \left( Y_t - \hat Y_t \right)^2 }{\frac{1}{n} \sum_{t \in \text{pre}} \left( Y_t - \hat Y_t \right)^2 }
$$&lt;/p&gt;
&lt;p&gt;We can compute a p-value as the number of observations with higher ratio.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;lambdas = {}
for state in states:
    mse_pre = synth_predict(df, state, SyntheticControl()).mse
    mse_tot = np.mean((df[f&#39;Synthetic {state}&#39;] - df[state])**2)
    lambdas[state] = (mse_tot - mse_pre) / mse_pre
    
print(f&amp;quot;p-value: {np.mean(np.fromiter(lambdas.values(), dtype=&#39;float&#39;) &amp;gt; lambdas[&#39;California&#39;]):.4}&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;p-value: 0.02564
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It seems that only $2.5%$ of the states had a larger MSE ratio. We can visualize the distribution of the statistic under permutation with a histogram.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;_, bins, _ = plt.hist(lambdas.values(), bins=20);
plt.hist([lambdas[&#39;California&#39;]], bins=bins, color=&amp;quot;C2&amp;quot;, label=&amp;quot;California&amp;quot;)
plt.legend();
plt.title(&#39;Ratio of $MSE_{post}$ and $MSE_{pre}$ across states&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/synthetic_control_46_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Indeed, the California statistic is quite extreme.&lt;/p&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=BsRqy7juMus&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Video lecture on Synthetic Controls&lt;/a&gt; by Paul Goldsmith-Pinkham (Yale)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://matheusfacure.github.io/python-causality-handbook/15-Synthetic-Control.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Chapter 10&lt;/a&gt; of Causal Inference for the Brave and True by Matheus Facure&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://mixtape.scunning.com/synthetic-control.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Chapter 15&lt;/a&gt; of Causal Inference: The Mixtape by Scott Cunningham&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>How To Make A Personal Website with Hugo</title>
      <link>https://matteocourthoud.github.io/post/website/</link>
      <pubDate>Mon, 18 Apr 2022 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/post/website/</guid>
      <description>&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;During the second year of my PhD, I decided that I wanted to have a personal website. After (too many) hours of research, I decided to build it using &lt;a href=&#34;https://gohugo.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Hugo&lt;/a&gt;, and I picked the &lt;a href=&#34;https://wowchemy.com/docs/getting-started/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Wowchemy&lt;/strong&gt;&lt;/a&gt; theme, also known as &lt;a href=&#34;https://themes.gohugo.io/academic/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Hugo Academic&lt;/a&gt;. In this tutorial, I am going to share my guide to building a website on &lt;a href=&#34;https://pages.github.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github Pages&lt;/a&gt; so that you donât have to go through all the pain I went through ð.&lt;/p&gt;
&lt;p&gt;Before we start, I have to warn you. If you donât care about &lt;strong&gt;personalization&lt;/strong&gt; or if you have very &lt;strong&gt;little time&lt;/strong&gt; to spend on building a website, I strongly recommend &lt;a href=&#34;https://sites.google.com/new&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Google Sites&lt;/a&gt;, which is, in my opinion, the fastest and easiest way to build an academic website. However, if you enjoy customizing your website, or if you like &lt;a href=&#34;https://github.com/matteocourthoud/custom-wowchemy-theme&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;my template&lt;/a&gt;, then this guide might be useful.&lt;/p&gt;
&lt;p&gt;Also, note that Hugo offers &lt;a href=&#34;https://themes.gohugo.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;many other website templates&lt;/a&gt;. I suggest checking them out. Some interesting &lt;strong&gt;alternatives&lt;/strong&gt; are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://themes.gohugo.io/themes/hugo-resume/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Resume&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://themes.gohugo.io/themes/somrat/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Somrat&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://themes.gohugo.io/themes/hugo-uilite/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;UILite (paid)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;However, in this guide, I will concentrate on the Hugo Academic theme, since itâs the one I used for &lt;a href=&#34;https://matteocourthoud.github.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;my website&lt;/a&gt; and I believe itâs the best one for building academic profile pages. But the first part of this guide is general and it works for any Hugo theme.&lt;/p&gt;
&lt;h2 id=&#34;create-website&#34;&gt;Create Website&lt;/h2&gt;
&lt;h3 id=&#34;0-prerequisites&#34;&gt;0. Prerequisites&lt;/h3&gt;
&lt;p&gt;Before we start, I will take for granted the following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;that you have an account on &lt;a href=&#34;https://www.github.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;that you have &lt;a href=&#34;https://www.r-project.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;R&lt;/a&gt; installed&lt;/li&gt;
&lt;li&gt;that you have &lt;a href=&#34;https://www.rstudio.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;RStudio&lt;/a&gt; installed&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;1-create-github-repository&#34;&gt;1. Create Github Repository&lt;/h3&gt;
&lt;p&gt;First, go to your &lt;a href=&#34;https://www.github.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt; page and create a new repository (&lt;code&gt;+&lt;/code&gt; button in the top-right corner).&lt;/p&gt;
&lt;img src=&#34;img/new_repo.png&#34; alt=&#34;new_repo&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;Name the repository &lt;code&gt;username.github.io&lt;/code&gt; where &lt;code&gt;username&lt;/code&gt; is your Github username.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;img/name_repo.png&#34; alt=&#34;name_repo&#34;&gt;&lt;/p&gt;
&lt;p&gt;In my case, my github username is &lt;code&gt;matteocourthoud&lt;/code&gt;, therefore the repository is &lt;code&gt;matteocourthoud.github.io&lt;/code&gt; and my personal website is &lt;a href=&#34;https://matteocourthoud.github.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://matteocourthoud.github.io&lt;/a&gt;. Use the default settings when creating the repository.&lt;/p&gt;
&lt;h3 id=&#34;2-install-blogdown-and-hugo&#34;&gt;2. Install Blogdown and Hugo&lt;/h3&gt;
&lt;p&gt;Now you need to install Blogdown, which is the program what will allow you to build and deploy your website, and Hugo, which is the template generator.&lt;/p&gt;
&lt;p&gt;Switch to RStudio and type the following commands&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Install blogdown
install.packages(&amp;quot;blogdown&amp;quot;)

# Install Hugo
blogdown::install_hugo()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now everything should be ready!&lt;/p&gt;
&lt;h3 id=&#34;3-setup-folder&#34;&gt;3. Setup folder&lt;/h3&gt;
&lt;p&gt;Open RStudio and select &lt;code&gt;New Project&lt;/code&gt;.&lt;/p&gt;
&lt;img src=&#34;img/new_project.png&#34; alt=&#34;new_project&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;Select &lt;code&gt;New Directory&lt;/code&gt; when asked where to create the project.&lt;/p&gt;
&lt;img src=&#34;img/new_project2.png&#34; alt=&#34;new_project2&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;Then select &lt;code&gt;Website using blogdown&lt;/code&gt; as project type.&lt;/p&gt;
&lt;img src=&#34;img/new_project3.png&#34; alt=&#34;new_project3&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;Now you have to select a couple of options:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;Directory name&lt;/code&gt;: here input the name of the folder which will contain all the website files. The name is irrelevant. I called mine &lt;code&gt;website&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Create project as a subdirectory of&lt;/code&gt;: select the directory in which you want to put the website folder.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Theme&lt;/code&gt;: input &lt;code&gt;wowchemy/starter-academic&lt;/code&gt; instead of the default theme.&lt;/li&gt;
&lt;/ul&gt;
&lt;img src=&#34;img/new_project4.png&#34; alt=&#34;new_project4&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: if you want to install a different theme, just go on the corresponding Github page (for example &lt;a href=&#34;https://github.com/caressofsteel/hugo-story&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/caressofsteel/hugo-story&lt;/a&gt;) and instead of &lt;code&gt;gcushen/hugo-academic&lt;/code&gt;, insert the corresponding Github repository (for example &lt;code&gt;caressofsteel/hugo-story&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;If you go into the website folder, it should look something like&lt;/p&gt;
&lt;img src=&#34;img/folder.png&#34; alt=&#34;folder&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;h3 id=&#34;4-build-website&#34;&gt;4. Build website&lt;/h3&gt;
&lt;p&gt;To build the website, open the RProject file &lt;code&gt;website.Rproj&lt;/code&gt; in RStudio and type&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;blogdown::hugo_build(local=TRUE)
&lt;/code&gt;&lt;/pre&gt;
&lt;img src=&#34;img/hugo_build.png&#34; alt=&#34;hugo_build&#34;&gt;
&lt;p&gt;This command will generate a &lt;code&gt;public/&lt;/code&gt; subfolder in which the actual code of the website is stored.&lt;/p&gt;
&lt;p&gt;Donât ask me why, but the option &lt;code&gt;local=TRUE&lt;/code&gt; seems to make a difference. Updating without it sometimes does not change the content in the &lt;code&gt;public/&lt;/code&gt; subfolder.&lt;/p&gt;
&lt;img src=&#34;img/folder2.png&#34; alt=&#34;folder2&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;To preview the website, type in RStudio&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;blogdown::serve_site()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The following preview should automatically open in your browser.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;img/prevew.png&#34; alt=&#34;preview&#34;&gt;&lt;/p&gt;
&lt;p&gt;Previewing the website is very useful as it allows you to see live changes locally inside RStudio, before publishing them. This is the &lt;strong&gt;main advantage of working in RStudio&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;If the preview does &lt;em&gt;not automatically&lt;/em&gt; open in your browser, and instead it previews inside RStudio Viewer panel, you can preview it in your browser using the upper left right-most button.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;img/preview2.png&#34; alt=&#34;preview2&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;5-publish-website&#34;&gt;5. Publish website&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Importantly&lt;/strong&gt;, before pushing the code online, you need to open the file &lt;code&gt;config.yaml&lt;/code&gt; and change the &lt;code&gt;baseurl&lt;/code&gt; to your future website url, which will be &lt;code&gt;https://username.github.io/&lt;/code&gt;, where &lt;code&gt;username&lt;/code&gt; is your Github username.&lt;/p&gt;
&lt;img src=&#34;img/username.png&#34; alt=&#34;username&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;Now that you have set the correct url, you have to push the changes from the &lt;code&gt;public/&lt;/code&gt; folder to your &lt;code&gt;username.github.io&lt;/code&gt; repository on Github.&lt;/p&gt;
&lt;p&gt;To do that, you need to get to the website folder. Letâs assume that the path to your folder is &lt;code&gt;Documents/website&lt;/code&gt;. Open the Terminal and type&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;cd Documents/website/public
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The following code will link the &lt;code&gt;public/&lt;/code&gt; folder, containing the actual code of the website, to your &lt;code&gt;username.github.io&lt;/code&gt; repository.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Init git in the /website/public/ folder
git init

# Add and commit the changes
git add .
git commit -m &amp;quot;first version of the website&amp;quot;

# Set origin
git remote add origin https://github.com/username/username.github.io.git

# Rename local branch
git branch -M main

# And push your updates online
git push -u origin main
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Wait a few seconds (or minutes for heavy changes) and your website should be online!&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;If the website is not working&lt;/strong&gt;, you can check the following:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Is there anything in your &lt;code&gt;public/&lt;/code&gt; folder? (does it even exist?) If not, something went wrong when compiling the website with &lt;code&gt;blogdown::hugo_build()&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Inside your &lt;code&gt;public/&lt;/code&gt; folder, there should be an &lt;code&gt;index.html&lt;/code&gt; file. If you double-click on it, you should see a local preview of your website in your browser. If not, something in the website code is wrong.&lt;/li&gt;
&lt;li&gt;Is the content of your &lt;code&gt;public/&lt;/code&gt; folder exactly the same as the content of your Github repository? If not, something went wrong when pushing to Github.&lt;/li&gt;
&lt;li&gt;Did you name your Github repository &lt;code&gt;username.github.io&lt;/code&gt;, where &lt;code&gt;username&lt;/code&gt; is your Github username?&lt;/li&gt;
&lt;li&gt;Did you change the &lt;code&gt;baseurl&lt;/code&gt; option in the file &lt;code&gt;config.yaml&lt;/code&gt; to &lt;code&gt;https://username.github.io/&lt;/code&gt;, where &lt;code&gt;username&lt;/code&gt; is your Github username?&lt;/li&gt;
&lt;li&gt;You can check the list of websites deployments at &lt;code&gt;https://github.com/username/username.github.io/deployments&lt;/code&gt;. Control that they correspond with your commits.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;If all the conditions are satisfied, but the website is still not online, maybe itâs just a matter of time. Have some patience.&lt;/p&gt;
&lt;h2 id=&#34;basic-customization&#34;&gt;Basic Customization&lt;/h2&gt;
&lt;p&gt;The basic files that you want to modify to customize your website are the following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;config/_default/config.yaml&lt;/code&gt;: general website information&lt;/li&gt;
&lt;li&gt;&lt;code&gt;config/_default/params.yaml&lt;/code&gt;: website customization&lt;/li&gt;
&lt;li&gt;&lt;code&gt;config/_default/menus.yaml&lt;/code&gt;: top bar / menu customization&lt;/li&gt;
&lt;li&gt;&lt;code&gt;content/authors/admin/_index.md&lt;/code&gt;: personal information&lt;/li&gt;
&lt;/ul&gt;
&lt;img src=&#34;img/files.png&#34; alt=&#34;files&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;For what concerns &lt;strong&gt;images&lt;/strong&gt;, there are two main things you might want to modify:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Profile picture: change the &lt;code&gt;content/authors/admin/avatar.jpg&lt;/code&gt; picture&lt;/li&gt;
&lt;li&gt;Website icon: change the &lt;code&gt;assets/media/icon.png&lt;/code&gt; picture&lt;/li&gt;
&lt;/ul&gt;
&lt;img src=&#34;img/images.png&#34; alt=&#34;images&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;In order to modify the &lt;strong&gt;widgets&lt;/strong&gt; on your homepage, go to &lt;code&gt;content/home/&lt;/code&gt; and modify the files inside. If you want to remove a section, just open the corresponding file and select &lt;code&gt;active: false&lt;/code&gt;. If there is no &lt;code&gt;active&lt;/code&gt; option, just copy the line &lt;code&gt;active: false&lt;/code&gt; in the corresponding file.&lt;/p&gt;
&lt;img src=&#34;img/widgets.png&#34; alt=&#34;widgets&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;On my website, I have only the following sections set to true:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;about&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;projects&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;posts&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;contact&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;To change the &lt;strong&gt;color palette&lt;/strong&gt; of the website, go to &lt;code&gt;data\theme&lt;/code&gt; and generate a &lt;code&gt;custom_theme.toml&lt;/code&gt; file with the following content:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-toml&#34;&gt;# Theme metadata
name = &amp;quot;My custom theme&amp;quot;

# Is theme light or dark?
light = true

# Primary
primary = &amp;quot;#284f7a&amp;quot;

# Menu
menu_primary = &amp;quot;#fff&amp;quot;
menu_text = &amp;quot;#34495e&amp;quot;
menu_text_active = &amp;quot;#284f7a&amp;quot;
menu_title = &amp;quot;#2b2b2b&amp;quot;

# Home sections
home_section_odd = &amp;quot;rgb(255, 255, 255)&amp;quot;
home_section_even = &amp;quot;rgb(247, 247, 247)&amp;quot;

[dark]
  link = &amp;quot;#bbdefb&amp;quot;
  link_hover = &amp;quot;#bbdefb&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then go to the &lt;code&gt;config/_default/params.yaml&lt;/code&gt; file and set the &lt;code&gt;theme&lt;/code&gt; to &lt;code&gt;custom_theme&lt;/code&gt;.&lt;/p&gt;
&lt;img src=&#34;img/custom_theme.png&#34; alt=&#34;custom_theme&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;You can get more information on how to personalize it &lt;a href=&#34;https://wowchemy.com/docs/getting-started/customization/#custom-theme&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;To change the &lt;strong&gt;font&lt;/strong&gt;, go to &lt;code&gt;data\fonts&lt;/code&gt; and generate a &lt;code&gt;custom_font.toml&lt;/code&gt; file with the following content:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-toml&#34;&gt;# Font style metadata
name = &amp;quot;My custom font&amp;quot;

# Optional Google font URL
google_fonts = &amp;quot;family=Roboto+Mono&amp;amp;family=Source+Sans+Pro:wght@200;300;400;700&amp;quot;

# Font families
heading_font = &amp;quot;Source Sans Pro&amp;quot;
body_font = &amp;quot;Source Sans Pro&amp;quot;
nav_font = &amp;quot;Source Sans Pro&amp;quot;
mono_font = &amp;quot;Roboto Mono&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then go to the &lt;code&gt;config/_default/params.yaml&lt;/code&gt; file and set the &lt;code&gt;font&lt;/code&gt; to &lt;code&gt;custom_font&lt;/code&gt;.&lt;/p&gt;
&lt;img src=&#34;img/custom_font.png&#34; alt=&#34;custom_font&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;You can get more information on how to personalize it &lt;a href=&#34;https://wowchemy.com/docs/getting-started/customization/#custom-font&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;. Importantly, by default, the website supports only fonts of weight 400 and 700. If you want a lighter font, like the &lt;a href=&#34;https://fonts.google.com/specimen/Source&amp;#43;Sans&amp;#43;Pro&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Source Sans Pro&lt;/a&gt; I use for my website, you have to dig into the advanced customization (which requires HTML and CSS skills).&lt;/p&gt;
&lt;h2 id=&#34;advanced-customization&#34;&gt;Advanced Customization&lt;/h2&gt;
&lt;p&gt;Advanced customization is possible but &lt;strong&gt;itâs a pain&lt;/strong&gt;. You basically want to go inside &lt;code&gt;themes\github.com\wowchemy\wowchemy-hugo-modules\wowchemy&lt;/code&gt; and start digging. Tip: you want to start digging in the following places:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;In &lt;code&gt;layouts\partials&lt;/code&gt; to edit the HTML files&lt;/li&gt;
&lt;li&gt;In &lt;code&gt;assets\scss&lt;/code&gt; to edit the SCSS code&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If you want to copy my exact theme, I have published my custom theme here: &lt;a href=&#34;https://github.com/matteocourthoud/custom-wowchemy-settings&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/matteocourthoud/custom-wowchemy-settings&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;You have to do the following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;go inside the &lt;code&gt;theme&lt;/code&gt; folder&lt;/li&gt;
&lt;li&gt;copy the content of the &lt;code&gt;custom-wowchemy-theme&lt;/code&gt; repository in a folder there&lt;/li&gt;
&lt;li&gt;go to the &lt;code&gt;config.yaml&lt;/code&gt; file into the MODULES section&lt;/li&gt;
&lt;/ul&gt;
&lt;img src=&#34;img/config_before.png&#34; alt=&#34;config_before&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;ul&gt;
&lt;li&gt;change the second link to the folder with the custom settings&lt;/li&gt;
&lt;/ul&gt;
&lt;img src=&#34;img/config_after.png&#34; alt=&#34;config_after&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;Now your website should look quite similar to mine! :)&lt;/p&gt;
&lt;h3 id=&#34;examples&#34;&gt;Examples&lt;/h3&gt;
&lt;p&gt;Here are some examples of &lt;strong&gt;advanced customizations&lt;/strong&gt; you can do. For all the examples the baseline directory is you theme directory, &lt;code&gt;themes/custom-wowchemy-theme&lt;/code&gt; if you renamed it as in the previous paragraph.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Example 1&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;What to have your section titles fixed on top of the screen?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Go to &lt;code&gt;assets/scss/wowchemy/widgets/_base.scss&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Search for &lt;code&gt;.section-heading h1&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;It should look like this&lt;/p&gt;
&lt;img src=&#34;img/section_heading_before.png&#34; alt=&#34;section_heading_before&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Add a couple of lines as follows&lt;/p&gt;
&lt;img src=&#34;img/section_heading_after.png&#34; alt=&#34;section_heading_after&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Now the section titles should stay anchored at the top of the page&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Example 2&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Do you want to put a &lt;strong&gt;background image&lt;/strong&gt; in your home page?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Put the selected background image, for example &lt;code&gt;image.png&lt;/code&gt;,  into the &lt;code&gt;static/img&lt;/code&gt; folder (the location itself does not matter)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Go to &lt;code&gt;assets/scss/wowchemy/widgets/_about.scss&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Add the following lines anywhere in the code&lt;/p&gt;
&lt;img src=&#34;img/background_widget.png&#34; alt=&#34;background_widget&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Now go to &lt;code&gt;layouts/partials/widgets/about.html&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Add the following lines after &lt;code&gt;&amp;lt;!-- About widget --&amp;gt;&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;img/background_widget2.png&#34; alt=&#34;background_widget2&#34;&gt;`&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Now &lt;code&gt;image.png&lt;/code&gt; should appear as background image in your homepage.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;google-analytics&#34;&gt;Google Analytics&lt;/h2&gt;
&lt;p&gt;In order for the website to be displayed in Google searches, you need to ask Google to track it.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Go to the &lt;a href=&#34;https://search.google.com/search-console&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Google Search Console website&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Use the &lt;a href=&#34;https://search.google.com/search-console?action=inspect&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;URL Inspection tool&lt;/a&gt; to inspect the URL of your personal website: &lt;code&gt;https://username.github.io&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Use &lt;strong&gt;Request indexing&lt;/strong&gt; to request Google to index your website so that it will apprear in Google searches.&lt;/li&gt;
&lt;li&gt;Under &lt;strong&gt;Sitemap&lt;/strong&gt; provide the link to your website sitemap to Google. It should be &lt;code&gt;https://username.github.io/sitemap.xml&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In order to receive statistics on your website, you first need to get your associated tracking code.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Go to the &lt;a href=&#34;https://www.google.com/analytics/web/#home/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Google Analytics website&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Click &lt;a href=&#34;https://support.google.com/analytics/answer/6132368&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Admin&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Select an account from the menu in the &lt;strong&gt;ACCOUNT&lt;/strong&gt; column.&lt;/li&gt;
&lt;li&gt;Select a property from the menu in the &lt;strong&gt;PROPERTY&lt;/strong&gt; column.&lt;/li&gt;
&lt;li&gt;Under &lt;strong&gt;PROPERTY&lt;/strong&gt;, click Tracking Info &amp;gt; Tracking Code.&lt;/li&gt;
&lt;li&gt;Your tracking ID and property number are displayed at the top of the page. It should have the form &lt;code&gt;UA-xxxxxxxxx-1&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Now that we have the website tracking code, we need to insert it into the &lt;code&gt;googleAnalytics&lt;/code&gt; section of the &lt;code&gt;config/_default/params.yaml&lt;/code&gt; file.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;marketing:
  google_analytics: &#39;UA-xxxxxxxxx-1&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The mobile application of &lt;a href=&#34;https://analytics.google.com/analytics/web/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Google Analytics&lt;/a&gt; is particular intuitive and allows you to monitor your website traffic in detail. You just need to link the website from the &lt;a href=&#34;https://search.google.com/search-console&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Google Sesarch Console&lt;/a&gt; and then you can motitor you website from this platform. There is also a very nice mobile app for both &lt;a href=&#34;https://play.google.com/store/apps/details?id=com.google.android.apps.giant&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Android&lt;/a&gt; and &lt;a href=&#34;https://apps.apple.com/us/app/google-analytics/id881599038&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;iOS&lt;/a&gt; to monitor your website from your smartphone.&lt;/p&gt;
&lt;p&gt;Another good free tool to analyze the âqualityâ of your website is &lt;a href=&#34;https://www.seomechanic.com/seo-analyzer/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;SEO Mechanic&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;p&gt;Here are the main resources I used to write this guide:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Wowchemy website: &lt;a href=&#34;https://wowchemy.com/docs/getting-started/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://wowchemy.com/docs/getting-started/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Old Academic website: &lt;a href=&#34;https://sourcethemes.com/academic/docs/install/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://sourcethemes.com/academic/docs/install/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Guide for the Terminal: &lt;a href=&#34;https://github.com/fliptanedo/FlipWebsite2017&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/fliptanedo/FlipWebsite2017&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>AIPW</title>
      <link>https://matteocourthoud.github.io/post/aipw/</link>
      <pubDate>Fri, 15 Apr 2022 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/post/aipw/</guid>
      <description>&lt;p&gt;In this tutorial, we are going to see how to robustly estimate treatment effects when treatment is conditionally randomly assigned, using the &lt;strong&gt;Augmented Inverse Propensity Weighted&lt;/strong&gt; estimator, also known as &lt;strong&gt;doubly-robust&lt;/strong&gt; estimator.&lt;/p&gt;
&lt;p&gt;For this tutorial, I assume you are familiar with the following concepts:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Rubin&amp;rsquo;s potential outcome framework&lt;/li&gt;
&lt;li&gt;Propensity score weighting&lt;/li&gt;
&lt;li&gt;Basic machine learning&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;setting&#34;&gt;Setting&lt;/h2&gt;
&lt;p&gt;We assume that for a set of i.i.d. subjects $i = 1, &amp;hellip;, n$ we observed a tuple $(X_i, T_i, Y_i)$ comprised of&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;a feature vector $X_i \in \mathbb R^n$&lt;/li&gt;
&lt;li&gt;a treatment assignment $T_i \in \lbrace 0, 1 \rbrace$&lt;/li&gt;
&lt;li&gt;a response $Y_i \in \mathbb R$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Assumption 1 : unconfoundedness&lt;/strong&gt; (or ignorability, or selection on observables)&lt;/p&gt;
&lt;p&gt;$$
\big \lbrace Y_i^{(1)} , Y_i^{(0)} \big \rbrace \ \perp \ T_i \ | \ X_i
$$&lt;/p&gt;
&lt;p&gt;i.e. conditional on observable characteristics $X$, the treatment assignment $T$ is as good as random.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Assumption 2: overlap&lt;/strong&gt; (or bounded support)&lt;/p&gt;
&lt;p&gt;$$
\exists \eta &amp;gt; 0 \ : \ \eta \leq \mathbb E \left[ T_i = 1 \ \big | \ X_i = x \right] \leq 1-\eta
$$&lt;/p&gt;
&lt;p&gt;i.e. no observation is deterministically assigned to the treatment or control group.&lt;/p&gt;
&lt;h2 id=&#34;the-ipw-estimator&#34;&gt;The IPW Estimator&lt;/h2&gt;
&lt;p&gt;We want to estimate the &lt;strong&gt;average treatment effect&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;$$
\tau(x) = \mathbb E \left[ Y^{(1)} - Y^{(0)} \ \big| \ X = x \right]
$$&lt;/p&gt;
&lt;p&gt;We would like to obtain an unbiased estimator that satifies a central limit theorem of the form&lt;/p&gt;
&lt;p&gt;$$
\sqrt{n} ( \hat \tau - \tau) \ \overset{d}{\to} \ N(0, V)
$$&lt;/p&gt;
&lt;p&gt;thus enabling us to construct &lt;strong&gt;confidence intervals&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Under &lt;strong&gt;unconfoundedness&lt;/strong&gt;, we can rewrite the average treatment effect as&lt;/p&gt;
&lt;p&gt;$$
\tau(x) = \mathbb E \left[ Y^{(1)} - Y^{(0)} \ \big| \ X = x \right] = \mathbb E \left[ \frac{T_i Y_i}{e(X_i)} - \frac{(1-T_i) Y_i}{1-e(X_i)} \right]
$$&lt;/p&gt;
&lt;p&gt;where $e(X_i)$ is the &lt;strong&gt;propensity score&lt;/strong&gt; of observation $i$,&lt;/p&gt;
&lt;p&gt;$$
e(x) = \mathbb P \left[ T_i = 1 \ \big | \ X_i = x \right]
$$&lt;/p&gt;
&lt;p&gt;i.e. its probability of being treated.&lt;/p&gt;
&lt;p&gt;Note that this formulation of the average treatment effect does not depend on the potential outcomes $Y_i^{(1)}$ and $Y_i^{(0)}$, but only on the observed outcomes $Y_i$.&lt;/p&gt;
&lt;p&gt;This formulation of the average treatment effect implies the &lt;strong&gt;Inverse Propensity Weighted&lt;/strong&gt; estimator which is an unbiased estimator for the average treatment effect $\tau$&lt;/p&gt;
&lt;p&gt;$$
\hat \tau^{*}_{IPW} = \frac{1}{n} \sum _ {i=1}^{n} \left( \frac{T_i Y_i}{e(X_i)} - \frac{(1-T_i) Y_i}{1-e(X_i)} \right)
$$&lt;/p&gt;
&lt;p&gt;However, this estimator is &lt;strong&gt;unfeasible&lt;/strong&gt; since we do not observe the propensity scores $e(X_i)$.&lt;/p&gt;
&lt;h2 id=&#34;the-aipw-estimator&#34;&gt;The AIPW Estimator&lt;/h2&gt;
&lt;p&gt;A feasible estimator of the average treatment effect $\tau$ is&lt;/p&gt;
&lt;p&gt;$$
\hat \tau_{IPW} = \frac{1}{n} \sum_{i=1}^{n} \left( \frac{T_i Y_i}{\hat e(X_i)} - \frac{(1-T_i) Y_i}{1-\hat e(X_i)} \right)
$$&lt;/p&gt;
&lt;p&gt;where we have replaced the propensity scores $e (X_i)$ with their estimates $\hat e (X_i)$.&lt;/p&gt;
&lt;p&gt;With a linear model, $Y_i = \alpha T_i + \beta X_i + \varepsilon_i$, we can get a central limit theorem for $\hat \tau$. However, we want to take a non-parametric / &lt;strong&gt;machine learning&lt;/strong&gt; approach with respect to the relationship between $Y$ and $X$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Advantages&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;flexible functional form for $\mathbb E[Y | X]$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Disadvantages&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;machine learning methods have slow convergence rates&lt;/li&gt;
&lt;li&gt;it impacts inference, i.e. we cannot easily get a central limit theorem result to build confidence intervals&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Is there a way to get around the slow rate of convergence of machine learning methods?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Yes!&lt;/strong&gt; Idea: combine two predicton problems instead of one.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;Augmented Inverse Propensity Weighted&lt;/strong&gt; estimator is given by&lt;/p&gt;
&lt;p&gt;$$
\hat \tau_{IPW} = \frac{1}{n} \sum_{i=1}^{n} \left( \hat \mu^{(1)}(X_i) - \hat \mu^{(0)}(X_i) + \frac{T_i }{\hat e(X_i)} \left( Y_i - \hat \mu^{(1)}(X_i) \right) - \frac{(1-T_i) }{1-\hat e(X_i)} \left( Y_i - \hat \mu^{(0)}(X_i) \right) \right)
$$&lt;/p&gt;
&lt;p&gt;where&lt;/p&gt;
&lt;p&gt;$$
\mu^{(t)}(x) = \mathbb E \left[ Y_i \ \big | \ X_i = x, T_i = t \right] \qquad ; \qquad e(x) = \mathbb E \left[ T_i = 1 \ \big | \ X_i = x \right]
$$&lt;/p&gt;
&lt;p&gt;The formula of the AIPW estimator might seem scary at first, so let&amp;rsquo;s &lt;strong&gt;decompose&lt;/strong&gt; it into two parts.&lt;/p&gt;
&lt;p&gt;First,&lt;/p&gt;
&lt;p&gt;$$
D = \frac{1}{n} \sum_{i=1}^{n} \left( \hat \mu^{(1)}(X_i) - \hat \mu^{(0)}(X_i) \right)
$$&lt;/p&gt;
&lt;p&gt;is basically the &lt;strong&gt;direct estimate&lt;/strong&gt; of the average treatment effect. This is a consistent estimator of the ATE but, since machine learning estimators&#39; rate of convergence is too slow, it does not provide correct confidence intervals.&lt;/p&gt;
&lt;p&gt;Instead,&lt;/p&gt;
&lt;p&gt;$$
R = \frac{1}{n} \sum_{i=1}^{n} \left(\frac{T_i }{\hat e(X_i)} \left( Y_i - \hat \mu^{(1)}(X_i) \right) - \frac{(1-T_i) }{1-\hat e(X_i)} \left( Y_i - \hat \mu^{(0)}(X_i) \right) \right)
$$&lt;/p&gt;
&lt;p&gt;is basically &lt;strong&gt;IPW applied to the residuals&lt;/strong&gt; $Y_i - \hat \mu^{(t)}(X_i)$ instead of $Y_i$.&lt;/p&gt;
&lt;h3 id=&#34;double-robustness&#34;&gt;Double Robustness&lt;/h3&gt;
&lt;p&gt;Why is the AIPW estimator so &lt;strong&gt;compelling&lt;/strong&gt;? It just needs one of the two predictions, $\hat \mu$ and $\hat e$, to be right in order to be unbiased. Let&amp;rsquo;s check it.&lt;/p&gt;
&lt;p&gt;If $\hat \mu$ is correctly specified, i.e. $\mathbb E \left[ \hat \mu^{(t)}(x) \right] = \mathbb E \left[ Y_i \ \big | \ X_i = x, T_i = t \right]$, then&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
\hat \tau_{IPW} &amp;amp;\overset{p}{\to} \mathbb E \left[ \hat \mu^{(1)}(X_i) - \hat \mu^{(0)}(X_i) + \frac{T_i \left( Y_i - \hat \mu^{(1)}(X_i) \right)}{\hat e(X_i)} - \frac{(1-T_i) \left( Y_i - \hat \mu^{(0)}(X_i) \right)}{1-\hat e(X_i)} \right] =
\newline
&amp;amp;= \mathbb E \left[ \hat \mu^{(1)}(X_i) - \hat \mu^{(0)}(X_i) \right] =
\newline
&amp;amp;= \mathbb E \left[ Y^{(1)} - Y^{(0)} \right] =
\newline
&amp;amp;= \tau
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;even if $\hat e$ is misspecified.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;intuition&lt;/strong&gt; is that the residuals $\left( Y_i - \hat \mu^{(t)}(X_i) \right)$ converge to zero and therefore IPW has no relevance.&lt;/p&gt;
&lt;p&gt;On the other hand, if $\hat e$ is correctly specified, i.e. $\mathbb E \left[\hat e(x) \right] = \mathbb E \left[ T_i = 1 \ \big | \ X_i = x \right]$, then&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
\hat \tau_{IPW} &amp;amp;\overset{p}{\to} \mathbb E \left[ \hat \mu^{(1)}(X_i) - \hat \mu^{(0)}(X_i) + \frac{T_i \left( Y_i - \hat \mu^{(1)}(X_i) \right)}{\hat e(X_i)} - \frac{(1-T_i) \left( Y_i - \hat \mu^{(0)}(X_i) \right)}{1-\hat e(X_i)} \right] =
\newline
&amp;amp;= \mathbb E \left[ \frac{T_i Y_i}{\hat e(X_i)} - \frac{(1-T_i) Y_i }{1-\hat e(X_i)} + \left(1 - \frac{T_i}{\hat e(X_i)} \right) \hat \mu^{(1)}(X_i) - \left(1 - \frac{1-T_i}{1-\hat e(X_i)} \right) \hat \mu^{(0)}(X_i)  \right] =
\newline
&amp;amp;= \mathbb E \left[ \frac{T_i Y_i}{\hat e(X_i)} - \frac{(1-T_i) Y_i }{1-\hat e(X_i)}\right] =
\newline
&amp;amp;= \mathbb E \left[ Y^{(1)} - Y^{(0)} \right] =
\newline
&amp;amp;= \tau
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;even if $\hat \mu$ is misspecified.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;intuition&lt;/strong&gt; is that, if we have a wrong model of $\mu(x)$ for $\mathbb E \left[ Y_i \ \big | \ X_i = x, T_i = t \right]$, it will not capture differences between treatment and control group or, even worse, it will capture wrong ones. Running IPW on the residuals we can not only recover the treatment effect but also compensate for eventual biases introduced by $\mu(x)$.&lt;/p&gt;
&lt;h2 id=&#34;best-practices&#34;&gt;Best Practices&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;1. Check Covariate Balance&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Both IPW and AIPW were built for settings in which the treatment $T$ is not uconditionally randomly assigned, but might depend on some observables $X$. This information can be checked in two ways:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Produce a balance table, summarizing the covariates across treatment arms. If undonditional randomization does not hold, we expect to see significant differences across some observables&lt;/li&gt;
&lt;li&gt;Plot the estimated propensity scores. If undonditional randomization holds, we expect the propensity scores to be constant&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;2. Check the Overlap Assumption&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Another assumption that we can check is the &lt;strong&gt;overlap&lt;/strong&gt; assumption, i.e. $\exists \eta \ : \ \eta \leq \mathbb E \left[ T_i = 1 \ \big | \ X_i = x \right] \leq 1-\eta$. To check this assumption we can simply check the bounds of the predicted propensity scores. If the overlap assumption is violated, we end up dividing some term of the estimator by zero.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;3. Use LOO Predictors&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;It is best practice, whenever we build a prediction it is best pratice to exclude observation $i$ when fitting the algorithm for predicting $\hat \mu^{(t)} (X_i)$ or $\hat e (X_i)$. These predictors are called &lt;strong&gt;Leave One Out&lt;/strong&gt; predictors.&lt;/p&gt;
&lt;h2 id=&#34;example&#34;&gt;Example&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;%matplotlib inline
%config InlineBackend.figure_format = &#39;retina&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from src.utils import *
from src.dgp import dgp_aipw
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In this example, we are going to use the following &lt;strong&gt;data generating process&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$N = 1000$&lt;/li&gt;
&lt;li&gt;$p = 20$&lt;/li&gt;
&lt;li&gt;$X_i \sim N(0, I_p)$&lt;/li&gt;
&lt;li&gt;$e(x) = 1 / (1 + e^{-x_1})$&lt;/li&gt;
&lt;li&gt;$\mu^{(0)}(x) = (x_1 + x_2)_+$&lt;/li&gt;
&lt;li&gt;$\mu^{(1)}(x) = (x_1 + x_3)_+ \mathbf{- 0.05}$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;So that the average treatment effect is $- 0.05$.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;dgp = dgp_aipw()
df = dgp.generate_data()
df.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;x1&lt;/th&gt;
      &lt;th&gt;x2&lt;/th&gt;
      &lt;th&gt;x3&lt;/th&gt;
      &lt;th&gt;x4&lt;/th&gt;
      &lt;th&gt;x5&lt;/th&gt;
      &lt;th&gt;x6&lt;/th&gt;
      &lt;th&gt;x7&lt;/th&gt;
      &lt;th&gt;x8&lt;/th&gt;
      &lt;th&gt;x9&lt;/th&gt;
      &lt;th&gt;x10&lt;/th&gt;
      &lt;th&gt;...&lt;/th&gt;
      &lt;th&gt;x16&lt;/th&gt;
      &lt;th&gt;x17&lt;/th&gt;
      &lt;th&gt;x18&lt;/th&gt;
      &lt;th&gt;x19&lt;/th&gt;
      &lt;th&gt;x20&lt;/th&gt;
      &lt;th&gt;e&lt;/th&gt;
      &lt;th&gt;T&lt;/th&gt;
      &lt;th&gt;Y0&lt;/th&gt;
      &lt;th&gt;Y1&lt;/th&gt;
      &lt;th&gt;Y&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;1.624345&lt;/td&gt;
      &lt;td&gt;-0.611756&lt;/td&gt;
      &lt;td&gt;-0.528172&lt;/td&gt;
      &lt;td&gt;-1.072969&lt;/td&gt;
      &lt;td&gt;0.865408&lt;/td&gt;
      &lt;td&gt;-2.301539&lt;/td&gt;
      &lt;td&gt;1.744812&lt;/td&gt;
      &lt;td&gt;-0.761207&lt;/td&gt;
      &lt;td&gt;0.319039&lt;/td&gt;
      &lt;td&gt;-0.249370&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;-1.099891&lt;/td&gt;
      &lt;td&gt;-0.172428&lt;/td&gt;
      &lt;td&gt;-0.877858&lt;/td&gt;
      &lt;td&gt;0.042214&lt;/td&gt;
      &lt;td&gt;0.582815&lt;/td&gt;
      &lt;td&gt;0.835394&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0.962589&lt;/td&gt;
      &lt;td&gt;0.996174&lt;/td&gt;
      &lt;td&gt;0.996174&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;-1.100619&lt;/td&gt;
      &lt;td&gt;1.144724&lt;/td&gt;
      &lt;td&gt;0.901591&lt;/td&gt;
      &lt;td&gt;0.502494&lt;/td&gt;
      &lt;td&gt;0.900856&lt;/td&gt;
      &lt;td&gt;-0.683728&lt;/td&gt;
      &lt;td&gt;-0.122890&lt;/td&gt;
      &lt;td&gt;-0.935769&lt;/td&gt;
      &lt;td&gt;-0.267888&lt;/td&gt;
      &lt;td&gt;0.530355&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;-0.012665&lt;/td&gt;
      &lt;td&gt;-1.117310&lt;/td&gt;
      &lt;td&gt;0.234416&lt;/td&gt;
      &lt;td&gt;1.659802&lt;/td&gt;
      &lt;td&gt;0.742044&lt;/td&gt;
      &lt;td&gt;0.249624&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0.044105&lt;/td&gt;
      &lt;td&gt;-0.050000&lt;/td&gt;
      &lt;td&gt;0.044105&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;-0.191836&lt;/td&gt;
      &lt;td&gt;-0.887629&lt;/td&gt;
      &lt;td&gt;-0.747158&lt;/td&gt;
      &lt;td&gt;1.692455&lt;/td&gt;
      &lt;td&gt;0.050808&lt;/td&gt;
      &lt;td&gt;-0.636996&lt;/td&gt;
      &lt;td&gt;0.190915&lt;/td&gt;
      &lt;td&gt;2.100255&lt;/td&gt;
      &lt;td&gt;0.120159&lt;/td&gt;
      &lt;td&gt;0.617203&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;0.586623&lt;/td&gt;
      &lt;td&gt;0.838983&lt;/td&gt;
      &lt;td&gt;0.931102&lt;/td&gt;
      &lt;td&gt;0.285587&lt;/td&gt;
      &lt;td&gt;0.885141&lt;/td&gt;
      &lt;td&gt;0.452188&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;-0.050000&lt;/td&gt;
      &lt;td&gt;-0.100000&lt;/td&gt;
      &lt;td&gt;-0.100000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;-0.754398&lt;/td&gt;
      &lt;td&gt;1.252868&lt;/td&gt;
      &lt;td&gt;0.512930&lt;/td&gt;
      &lt;td&gt;-0.298093&lt;/td&gt;
      &lt;td&gt;0.488518&lt;/td&gt;
      &lt;td&gt;-0.075572&lt;/td&gt;
      &lt;td&gt;1.131629&lt;/td&gt;
      &lt;td&gt;1.519817&lt;/td&gt;
      &lt;td&gt;2.185575&lt;/td&gt;
      &lt;td&gt;-1.396496&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;-2.022201&lt;/td&gt;
      &lt;td&gt;-0.306204&lt;/td&gt;
      &lt;td&gt;0.827975&lt;/td&gt;
      &lt;td&gt;0.230095&lt;/td&gt;
      &lt;td&gt;0.762011&lt;/td&gt;
      &lt;td&gt;0.319864&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0.448470&lt;/td&gt;
      &lt;td&gt;-0.100000&lt;/td&gt;
      &lt;td&gt;-0.100000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;-0.222328&lt;/td&gt;
      &lt;td&gt;-0.200758&lt;/td&gt;
      &lt;td&gt;0.186561&lt;/td&gt;
      &lt;td&gt;0.410052&lt;/td&gt;
      &lt;td&gt;0.198300&lt;/td&gt;
      &lt;td&gt;0.119009&lt;/td&gt;
      &lt;td&gt;-0.670662&lt;/td&gt;
      &lt;td&gt;0.377564&lt;/td&gt;
      &lt;td&gt;0.121821&lt;/td&gt;
      &lt;td&gt;1.129484&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;0.077340&lt;/td&gt;
      &lt;td&gt;-0.343854&lt;/td&gt;
      &lt;td&gt;0.043597&lt;/td&gt;
      &lt;td&gt;-0.620001&lt;/td&gt;
      &lt;td&gt;0.698032&lt;/td&gt;
      &lt;td&gt;0.444646&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;-0.050000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;5 rows Ã 25 columns&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;First, we check for covariate imbalance across treatment arms, with a &lt;strong&gt;balance table&lt;/strong&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df.groupby(&#39;T&#39;).agg([&#39;mean&#39;, &#39;std&#39;]).T.unstack(1).head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead tr th {
    text-align: left;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;T&lt;/th&gt;
      &lt;th colspan=&#34;2&#34; halign=&#34;left&#34;&gt;0&lt;/th&gt;
      &lt;th colspan=&#34;2&#34; halign=&#34;left&#34;&gt;1&lt;/th&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;mean&lt;/th&gt;
      &lt;th&gt;std&lt;/th&gt;
      &lt;th&gt;mean&lt;/th&gt;
      &lt;th&gt;std&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;x1&lt;/th&gt;
      &lt;td&gt;-0.462644&lt;/td&gt;
      &lt;td&gt;0.920626&lt;/td&gt;
      &lt;td&gt;0.413837&lt;/td&gt;
      &lt;td&gt;0.882116&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;x2&lt;/th&gt;
      &lt;td&gt;0.007900&lt;/td&gt;
      &lt;td&gt;1.010610&lt;/td&gt;
      &lt;td&gt;-0.054072&lt;/td&gt;
      &lt;td&gt;0.991680&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;x3&lt;/th&gt;
      &lt;td&gt;-0.005718&lt;/td&gt;
      &lt;td&gt;0.950997&lt;/td&gt;
      &lt;td&gt;-0.042624&lt;/td&gt;
      &lt;td&gt;1.059308&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;x4&lt;/th&gt;
      &lt;td&gt;0.122930&lt;/td&gt;
      &lt;td&gt;0.936591&lt;/td&gt;
      &lt;td&gt;0.095082&lt;/td&gt;
      &lt;td&gt;1.004048&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;x5&lt;/th&gt;
      &lt;td&gt;0.032637&lt;/td&gt;
      &lt;td&gt;0.955479&lt;/td&gt;
      &lt;td&gt;0.119788&lt;/td&gt;
      &lt;td&gt;1.039616&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;We can also get a first (wrong) estimate of the treatment effect as a difference in means.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df.loc[df[&#39;T&#39;]==1, &#39;Y&#39;].mean() - df.loc[df[&#39;T&#39;]==0, &#39;Y&#39;].mean()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;0.29838749886286686
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We know this estimate is wrong since the treatment is not unconditionally randomized. Therefore, we estimate the average treatment effect using the &lt;strong&gt;AIPW estimator&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;First, we estimate the &lt;strong&gt;propensity scores&lt;/strong&gt; $e(x)$ using &lt;code&gt;LogisticRegression&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def estimate_e(df, X, model_e):
    e = model_e.fit(df[dgp.X], df[&#39;T&#39;]).predict_proba(df[dgp.X])[:,1]
    return e
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from sklearn.linear_model import LogisticRegression as logit

e = estimate_e(df, dgp.X, logit())
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;A best practice is to use the &lt;code&gt;LeaveOneOut&lt;/code&gt; estimator to make predictions, i.e., for each observations, build a model using the remaining observations. This procedure helps preventing overfitting bias. The main issue is that it&amp;rsquo;s particularly slow since we need to fit one model per observation. However, it is parallelizable.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from sklearn.model_selection import cross_val_predict, LeaveOneOut

e = cross_val_predict(estimator=logit(), 
                      X=df[dgp.X], 
                      y=df[&#39;T&#39;],
                      cv=LeaveOneOut(),
                      method=&#39;predict_proba&#39;,
                      n_jobs=-1)[:,1]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s check if the &lt;strong&gt;bounded support&lt;/strong&gt; assumption is satisfied.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;print(f&#39;Support of e is [{min(e):.2}, {max(e):.2}]&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Support of e is [0.019, 0.97]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The support assumption is satisfied. Note that this is guaranteed with logistic regression.&lt;/p&gt;
&lt;p&gt;We can further plot the distribution of scores. They definitely do not seem constant.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sns.histplot(e, bins=100).set(title=&#39;Propensity Scores&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/aipw_29_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;We can now estimate the average treatment effect using the AIPW estimator. We also separately compute its components, $D$ and $R$.&lt;/p&gt;
&lt;p&gt;We use &lt;code&gt;RandomForestRegressor&lt;/code&gt; to estimate the conditional expectation of $Y$, $\hat \mu^{(t)}(x)$.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def estimate_mu(df, X, model_mu):
    mu = model_mu.fit(df[X + [&#39;T&#39;]], df[&#39;Y&#39;])
    mu0 = mu.predict(df[X + [&#39;T&#39;]].assign(T=0))
    mu1 = mu.predict(df[X + [&#39;T&#39;]].assign(T=1))
    return mu0, mu1
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from sklearn.ensemble import RandomForestRegressor as rfr

mu0, mu1 = estimate_mu(df, dgp.X, rfr(max_features=5))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We now have all the elements to estimate AIPW&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def AIPW(df, X, e, mu0, mu1):
    D = mu1 - mu0
    R = df[&#39;T&#39;] / e * (df[&#39;Y&#39;] - mu1) - (1-df[&#39;T&#39;]) / (1-e) * (df[&#39;Y&#39;] - mu0)
    tau_AIPW = D + R
    return D, R, tau_AIPW
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;D, R, hat_tau_AIPW = AIPW(df, dgp.X, e, mu0, mu1)
print(f&amp;quot;Mean: {np.mean(hat_tau_AIPW):.2} and var {np.var(hat_tau_AIPW):.2}&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Mean: -0.02 and var 0.48
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Our estimate is now closer to the true value, $-0.05$. We plot the distribution of $\hat \tau_i^{AIPW}$ below.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sns.histplot(hat_tau_AIPW, bins=20).set(title=&#39;Estimated $Ï_{AIPW}$&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/aipw_37_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;To visualize the impact of the AIPW correction, we simulate the distribution of the AIPW estimator and its components.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def simulate_AIPW(k):
    df = dgp_aipw().generate_data(seed=k)
    e = estimate_e(df, dgp.X, logit())
    mu0, mu1 = estimate_mu(df, dgp.X, rfr())
    aipw = AIPW(df, dgp.X, e, mu0, mu1)
    return np.mean(aipw, axis=1)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from joblib import Parallel, delayed

def distribution_AIPW(t):
    r = Parallel(n_jobs=8)(delayed(simulate_AIPW)(i) for i in range(100))
    sim_tau_AIPW = pd.DataFrame(r, columns=[&#39;Direct&#39;, &#39;Correction&#39;, &#39;$Ï_{AIPW}$&#39;])
    plot = sns.boxplot(data=pd.melt(sim_tau_AIPW), x=&#39;variable&#39;, y=&#39;value&#39;, linewidth=2);
    plot.set(title=t, xlabel=&#39;&#39;, ylabel=&#39;&#39;)
    plot.axhline(-0.05, c=&#39;r&#39;, ls=&#39;:&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Below we plot the AIPW estimator and its components.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;distribution_AIPW(&amp;quot;Distribution of $\hat Ï_{AIPW}$ and its components&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/aipw_42_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Since the model is well specified, the correction is close to zero and the final estimates are very close to the direct estimates.&lt;/p&gt;
&lt;p&gt;We now turn into checking the &lt;strong&gt;double-robustness&lt;/strong&gt; of the estimator.&lt;/p&gt;
&lt;p&gt;What happens if we misspecify the propensity score? Let&amp;rsquo;s assume now that the propensity score is $\hat e(X_i) = 1 /(1 + e^{x_4})$.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def estimate_e(df, X, model_e):
    e = 1 / (1 + np.exp(df[&#39;x4&#39;]))
    return e
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;distribution_AIPW(&amp;quot;Distribution of $\hat Ï_{AIPW}$ with misspecified $\hat e$&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/aipw_45_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Not much changes. In fact, since we have a good direct model of $\mu(x)$, the residuals are small and the correction does not play a big role.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s now misspecify $\mu(x)$ and assume it is equal to $|x_5|$.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def estimate_e(df, X, model_e):
    e = model_e.fit(df[dgp.X], df[&#39;T&#39;]).predict_proba(df[dgp.X])[:,1]
    return e

def estimate_mu(df, X, model_mu):
    mu0 = 0
    mu1 = np.abs(df[&#39;x6&#39;])
    return mu0, mu1
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;distribution_AIPW(&amp;quot;Distribution of $\hat Ï_{AIPW}$ with misspecified $\hat Î¼$&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/aipw_48_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;In this case, the correction term is crucial and fully offsets the bias of the original estimator.&lt;/p&gt;
&lt;p&gt;The &lt;a href=&#34;https://econml.azurewebsites.net/index.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;EconML&lt;/code&gt;&lt;/a&gt; library offers a plug and play estimator, &lt;code&gt;LinearDRLearner&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from econml.drlearner import LinearDRLearner

model = LinearDRLearner(model_propensity=logit(), model_regression=rfr())
model.fit(Y=df[dgp.Y], X=df[dgp.X], T=df[dgp.T]);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The model directly gives us the average treatment effect.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;model.ate_inference(X=df[dgp.X].values, T0=0, T1=1).summary().tables[0]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;caption&gt;Uncertainty of Mean Point Estimate&lt;/caption&gt;
&lt;tr&gt;
  &lt;th&gt;mean_point&lt;/th&gt; &lt;th&gt;stderr_mean&lt;/th&gt;  &lt;th&gt;zstat&lt;/th&gt; &lt;th&gt;pvalue&lt;/th&gt; &lt;th&gt;ci_mean_lower&lt;/th&gt; &lt;th&gt;ci_mean_upper&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
    &lt;td&gt;-0.087&lt;/td&gt;      &lt;td&gt;0.036&lt;/td&gt;    &lt;td&gt;-2.438&lt;/td&gt;  &lt;td&gt;0.015&lt;/td&gt;    &lt;td&gt;-0.157&lt;/td&gt;        &lt;td&gt;-0.017&lt;/td&gt;    
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;The estimate is statistically different from zero and the confidence interval includes the true value of $-0.05$.&lt;/p&gt;
&lt;h2 id=&#34;research-paper-replication&#34;&gt;Research Paper Replication&lt;/h2&gt;
&lt;p&gt;TBD&lt;/p&gt;
&lt;h2 id=&#34;business-case&#34;&gt;Business Case&lt;/h2&gt;
&lt;p&gt;TBD&lt;/p&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Original paper: &lt;a href=&#34;https://www.cambridge.org/core/journals/political-analysis/article/abs/an-introduction-to-the-augmented-inverse-propensity-weighted-estimator/4B1B8301E46F4432C4DCC91FE20780DB&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;An Introduction to the Augmented Inverse Propensity Weighted Estimator&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=IfZHUFFlsGc&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Video lecture&lt;/a&gt; by Prof. Stefan Wager (Stanford)&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Causal Trees</title>
      <link>https://matteocourthoud.github.io/post/causal_trees/</link>
      <pubDate>Fri, 15 Apr 2022 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/post/causal_trees/</guid>
      <description>&lt;p&gt;In this tutorial, we are going to see how to estimate heterogeneous treatment effects using regression trees.&lt;/p&gt;
&lt;p&gt;For this tutorial, I assume you are familiar with the following concepts:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Rubin&amp;rsquo;s potential outcome framework&lt;/li&gt;
&lt;li&gt;Decision tree methods&lt;/li&gt;
&lt;li&gt;Propensity score matching&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;setting&#34;&gt;Setting&lt;/h2&gt;
&lt;p&gt;We assume that for a set of i.i.d. subjects $i = 1, &amp;hellip;, n$ we observed a tuple $(X_i, D_i, Y_i)$ comprised of&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;a feature vector $X_i \in \mathbb R^n$&lt;/li&gt;
&lt;li&gt;a treatment assignment $T_i \in \lbrace 0, 1 \rbrace$&lt;/li&gt;
&lt;li&gt;a response $Y_i \in \mathbb R$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Our goal is to estimate the &lt;strong&gt;conditional average treatment effect&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;$$
\tau(x) = \mathbb E \Big [ Y^{(1)} - Y^{(0)} \ \Big| \ X = x \Big ]
$$&lt;/p&gt;
&lt;p&gt;Crucially, we only get to observe $Y_i = Y_i^{(T_i)}$.&lt;/p&gt;
&lt;p&gt;Wihtout further assumptions, we cannot estimate $\tau(x)$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Assumption: unconfoundedness&lt;/strong&gt; (or ignorability, or selection on observables)&lt;/p&gt;
&lt;p&gt;$$
\lbrace Y_i^{(1)} , Y^{(0)} \rbrace \perp W_i \ | \ X_i
$$&lt;/p&gt;
&lt;p&gt;i.e. conditional on observable characteristics the treatment assignment is as good as random.&lt;/p&gt;
&lt;p&gt;When unconfoundedness holds, matching methods usually provide consistent estimates of the conditional average treatment effect.&lt;/p&gt;
&lt;h2 id=&#34;prediction-problem&#34;&gt;Prediction Problem&lt;/h2&gt;
&lt;p&gt;How can we make the inference problem a &lt;strong&gt;prediction problem&lt;/strong&gt;?&lt;/p&gt;
&lt;p&gt;In principle, we would like to divide the population in subgroups in order to minimize the mean squared error (MSE) of treatment effects.&lt;/p&gt;
&lt;p&gt;The objective function is&lt;/p&gt;
&lt;p&gt;$$
\sum_i \Big [ ( \tau_i - \hat \tau_i(X))^2 \Big ]
$$&lt;/p&gt;
&lt;p&gt;However, this objective function is &lt;strong&gt;unfeasible&lt;/strong&gt; since we do not observe $\tau_i$.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;idea&lt;/strong&gt; is to transform our outcome variable as&lt;/p&gt;
&lt;p&gt;$$
Y_i^* = \frac{Y_i}{D_i * p(X_i) - (1-D_i) * (1-p(X_i))}
$$&lt;/p&gt;
&lt;p&gt;where $p_i$ is the &lt;strong&gt;propensity score&lt;/strong&gt; of observation $i$, i.e. its probability of being treated.&lt;/p&gt;
&lt;p&gt;It&amp;rsquo;s intuitive to verify that, given this specification, the expected value of $Y_i^*$ is the &lt;strong&gt;conditional average treatment effect&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Here is a proof:&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
\mathbb E \left[ Y_i^{*} \mid X_i = x \right] &amp;amp;= \mathbb E \left[ \frac{Y_i}{T_i * p(X_i) - (1-T_i) * (1-p(X_i))} \ \Big | \ X_i = x \right]
\newline
&amp;amp;= \mathbb E \left[ Y_i * \frac{T_i - p(X_i)}{p(X_i) (1-p(X_i))} \ \Big | \ X_i = x \right]
\newline
&amp;amp;= \mathbb E \left[ Y_i T_i * \frac{T_i - p(X_i)}{p(X_i) (1 - p(X_i))} + Y_i (1-T_i) * \frac{T_i - p(X_i)}{p(X_i) (1 - p(X_i))} \ \Big | \ X_i = x \right]
\newline
&amp;amp;= \mathbb E \Big[ Y^{(1)}_i * \frac{D_i (1 - p(X_i))}{p(X_i) (1 - p(X_i))} \ \Big | \ X_i = x \Big] - \mathbb E \left[Y^{(0)}_i * \frac{(1 - T_i) p(X_i)}{p(X_i) (1-p(X_i))} \ \Big | \ X_i = x \right]
\newline
&amp;amp;= \frac{1}{p(X_i)} \mathbb E \Big[ Y^{(1)}_i * T_i \ \Big | \ X_i = x \Big] - \frac{1}{1-p(X_i)} \mathbb E \left[ Y^{(0)}_i * (1 - T_i) \ \Big | \ X_i = x \right]
\newline
&amp;amp;= \frac{1}{p(X_i)} \mathbb E \left[ Y^{(1)}_i \ \Big | \ X_i = x \right] * \mathbb E \Big[ T_i \ \Big | \ X_i = x \Big] - \frac{1}{1 - p(X_i)} \mathbb E \left[ Y^{(0)}_i \ \Big | \ X_i = x \right] * \mathbb E \left[ (1 - T_i) \ \Big | \ X_i = x \right]
\newline
&amp;amp;= \mathbb E \Big[ Y^{(1)}_i \ \Big | \ X_i = x \Big] - \mathbb E \Big[Y^{(0)}_i \ \Big | \ X_i = x \Big]
\newline
&amp;amp;= \tau_i(x)
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;How can &lt;strong&gt;regression trees&lt;/strong&gt; help estimate heterogeneous treatment effects?&lt;/p&gt;
&lt;p&gt;If we fit a tree model on the modified outcome $Y^*$, we will get a partition of the data that minimizes the expected mean squared error of the conditional treatment effect. While the individual estimates are going to be inaccurate, within each leaf, we can estimate &lt;strong&gt;heterogeneous treatment effects&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;In order to get an &lt;strong&gt;unbiased estimate&lt;/strong&gt; however, we need to use different data to build the tree and to estimate the effect. This procedure comes at the cost of increased variance.&lt;/p&gt;
&lt;h2 id=&#34;example-1-simulated-data&#34;&gt;Example 1: Simulated Data&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s start with an example on synthetic data. We have the following individual characteristics:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;male&lt;/code&gt;: gender&lt;/li&gt;
&lt;li&gt;&lt;code&gt;black&lt;/code&gt;: race&lt;/li&gt;
&lt;li&gt;&lt;code&gt;age&lt;/code&gt;: age&lt;/li&gt;
&lt;li&gt;&lt;code&gt;educ&lt;/code&gt;: education, which depends on age and race&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Moreover, we maketreatment status &lt;code&gt;D&lt;/code&gt; depend on both gender and race so that it will be important to condition on observables.&lt;/p&gt;
&lt;p&gt;Our outcome variable &lt;code&gt;y&lt;/code&gt; depends on the treatment assignment differently according to education and age.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;%matplotlib inline
%config InlineBackend.figure_format = &#39;retina&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from src.utils import *
from src.dgp import dgp_ad
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We generate a dataset out of our DGP.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;dgp = dgp_ad()
df = dgp.generate_data()
df.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;male&lt;/th&gt;
      &lt;th&gt;black&lt;/th&gt;
      &lt;th&gt;age&lt;/th&gt;
      &lt;th&gt;educ&lt;/th&gt;
      &lt;th&gt;ad_exposure&lt;/th&gt;
      &lt;th&gt;revenue&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;55.0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;-0.327221&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;47.0&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;0.659393&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;31.0&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;True&lt;/td&gt;
      &lt;td&gt;2.805178&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;51.0&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;-0.508548&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;48.0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;True&lt;/td&gt;
      &lt;td&gt;0.762280&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;We can check the distribution of variables across treatment assignment.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df.groupby(dgp.T).agg([&#39;mean&#39;, &#39;std&#39;]).T.unstack(1)
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead tr th {
    text-align: left;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;ad_exposure&lt;/th&gt;
      &lt;th colspan=&#34;2&#34; halign=&#34;left&#34;&gt;False&lt;/th&gt;
      &lt;th colspan=&#34;2&#34; halign=&#34;left&#34;&gt;True&lt;/th&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;mean&lt;/th&gt;
      &lt;th&gt;std&lt;/th&gt;
      &lt;th&gt;mean&lt;/th&gt;
      &lt;th&gt;std&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;male&lt;/th&gt;
      &lt;td&gt;0.42000&lt;/td&gt;
      &lt;td&gt;0.494053&lt;/td&gt;
      &lt;td&gt;0.592000&lt;/td&gt;
      &lt;td&gt;0.491955&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;black&lt;/th&gt;
      &lt;td&gt;0.58800&lt;/td&gt;
      &lt;td&gt;0.492688&lt;/td&gt;
      &lt;td&gt;0.470000&lt;/td&gt;
      &lt;td&gt;0.499599&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;age&lt;/th&gt;
      &lt;td&gt;44.57600&lt;/td&gt;
      &lt;td&gt;10.350386&lt;/td&gt;
      &lt;td&gt;44.970000&lt;/td&gt;
      &lt;td&gt;9.961102&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;educ&lt;/th&gt;
      &lt;td&gt;2.19200&lt;/td&gt;
      &lt;td&gt;1.824773&lt;/td&gt;
      &lt;td&gt;1.968000&lt;/td&gt;
      &lt;td&gt;1.724216&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;revenue&lt;/th&gt;
      &lt;td&gt;-0.11697&lt;/td&gt;
      &lt;td&gt;1.057941&lt;/td&gt;
      &lt;td&gt;1.156387&lt;/td&gt;
      &lt;td&gt;2.147342&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;As we can see, &lt;code&gt;male&lt;/code&gt; and &lt;code&gt;black&lt;/code&gt; are now balanced across groups.&lt;/p&gt;
&lt;p&gt;We can get a first estimate of the &lt;strong&gt;average treatment effect&lt;/strong&gt; as a simple comparison of means.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df.loc[df[dgp.T]==1, dgp.Y].mean() - df.loc[df[dgp.T]==0, dgp.Y].mean()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;1.2733576182695898
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can visualize the difference with a barplot.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sns.barplot(x=dgp.T, y=dgp.Y, data=df);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/causal_trees_18_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;It seems there is a significant difference between the two groups. We can get a standard error around the estimate by regressing $Y$ on $D$.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;est = smf.ols(f&#39;{dgp.Y} ~ {dgp.T}&#39;, df).fit()
est.summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
           &lt;td&gt;&lt;/td&gt;              &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt;           &lt;td&gt;   -0.1170&lt;/td&gt; &lt;td&gt;    0.076&lt;/td&gt; &lt;td&gt;   -1.545&lt;/td&gt; &lt;td&gt; 0.123&lt;/td&gt; &lt;td&gt;   -0.266&lt;/td&gt; &lt;td&gt;    0.032&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;ad_exposure[T.True]&lt;/th&gt; &lt;td&gt;    1.2734&lt;/td&gt; &lt;td&gt;    0.107&lt;/td&gt; &lt;td&gt;   11.894&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    1.063&lt;/td&gt; &lt;td&gt;    1.483&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;The coefficient is statistically significant. However, we know that the treatment assignment is not &lt;strong&gt;unconditionally exogenous&lt;/strong&gt;. We need to condition on observables $X$.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;est = smf.ols(f&#39;{dgp.Y} ~ {dgp.T} +&#39; + &#39; + &#39;.join(dgp.X), df).fit()
est.summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
           &lt;td&gt;&lt;/td&gt;              &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt;           &lt;td&gt;   -0.1026&lt;/td&gt; &lt;td&gt;    0.247&lt;/td&gt; &lt;td&gt;   -0.416&lt;/td&gt; &lt;td&gt; 0.678&lt;/td&gt; &lt;td&gt;   -0.587&lt;/td&gt; &lt;td&gt;    0.382&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;ad_exposure[T.True]&lt;/th&gt; &lt;td&gt;    1.0342&lt;/td&gt; &lt;td&gt;    0.103&lt;/td&gt; &lt;td&gt;   10.044&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    0.832&lt;/td&gt; &lt;td&gt;    1.236&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;male&lt;/th&gt;                &lt;td&gt;    0.9987&lt;/td&gt; &lt;td&gt;    0.102&lt;/td&gt; &lt;td&gt;    9.765&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    0.798&lt;/td&gt; &lt;td&gt;    1.199&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;black&lt;/th&gt;               &lt;td&gt;   -0.8234&lt;/td&gt; &lt;td&gt;    0.126&lt;/td&gt; &lt;td&gt;   -6.555&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;   -1.070&lt;/td&gt; &lt;td&gt;   -0.577&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;age&lt;/th&gt;                 &lt;td&gt;   -0.0050&lt;/td&gt; &lt;td&gt;    0.005&lt;/td&gt; &lt;td&gt;   -1.001&lt;/td&gt; &lt;td&gt; 0.317&lt;/td&gt; &lt;td&gt;   -0.015&lt;/td&gt; &lt;td&gt;    0.005&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;educ&lt;/th&gt;                &lt;td&gt;    0.1240&lt;/td&gt; &lt;td&gt;    0.035&lt;/td&gt; &lt;td&gt;    3.533&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    0.055&lt;/td&gt; &lt;td&gt;    0.193&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;We do not need to actually condition on the full vector of observables $X$. It is sufficient to condition on the &lt;strong&gt;propensity score&lt;/strong&gt; $p(X)$, i.e. the conditional probability of treatment.&lt;/p&gt;
&lt;p&gt;We can estimate the propensity score with any method we want. The more flexible, the better.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df[&#39;pscore&#39;] = RandomForestRegressor().fit(df[dgp.X], df[dgp.T]).predict(df[dgp.X])
df[&#39;pscore&#39;].head()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;0    0.368726
1    0.036667
2    0.675000
3    0.701702
4    0.307456
Name: pscore, dtype: float64
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We now estiamte the conditional average treatment effect regressing $Y$ on $D$ and on the p-score.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;est = smf.ols(f&#39;{dgp.Y} ~ {dgp.T} + pscore&#39;, df).fit()
est.summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
           &lt;td&gt;&lt;/td&gt;              &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt;           &lt;td&gt;   -0.3402&lt;/td&gt; &lt;td&gt;    0.103&lt;/td&gt; &lt;td&gt;   -3.315&lt;/td&gt; &lt;td&gt; 0.001&lt;/td&gt; &lt;td&gt;   -0.542&lt;/td&gt; &lt;td&gt;   -0.139&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;ad_exposure[T.True]&lt;/th&gt; &lt;td&gt;    0.8957&lt;/td&gt; &lt;td&gt;    0.159&lt;/td&gt; &lt;td&gt;    5.638&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    0.584&lt;/td&gt; &lt;td&gt;    1.208&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;pscore&lt;/th&gt;              &lt;td&gt;    0.8235&lt;/td&gt; &lt;td&gt;    0.257&lt;/td&gt; &lt;td&gt;    3.204&lt;/td&gt; &lt;td&gt; 0.001&lt;/td&gt; &lt;td&gt;    0.319&lt;/td&gt; &lt;td&gt;    1.328&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;We are now ready to estimate &lt;strong&gt;heterogeneous treatment effects&lt;/strong&gt;. First, we need to compute the transformed outcome&lt;/p&gt;
&lt;p&gt;$$
Y_i^* = \frac{Y_i}{T_i * e_i - (1-T_i) * (1-e_i)}
$$&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df[&#39;y_star&#39;] = df[dgp.Y] /(df[dgp.T] * df[&#39;pscore&#39;] - (1-df[dgp.T]) * (1-df[&#39;pscore&#39;]))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we train a small tree on the transformed outcome $Y^*$.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;tree = DecisionTreeRegressor(max_depth=2, min_samples_leaf=30).fit(df[dgp.X], df[&#39;y_star&#39;])
df[&#39;y_hat&#39;] = tree.predict(df[dgp.X])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can plot the tree and visualize the estimated groups and treatment effects.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Plot tree
plot_tree(tree, filled=True, fontsize=12, feature_names=dgp.X, impurity=False);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/causal_trees_32_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;We still have one issue: we have trained our tree model and estimated the treatment effects using the same data. This introduces bias in the estimates.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from econml.causal_forest import CausalForest

model = CausalForest(max_depth=2).fit(Y=df[dgp.Y], X=df[dgp.X], T=df[dgp.T])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can plot a tree&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from econml.cate_interpreter import SingleTreeCateInterpreter

intrp = SingleTreeCateInterpreter(max_depth=2).interpret(model, df[dgp.X])
intrp.plot(feature_names=dgp.X, fontsize=10)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/causal_trees_36_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;example-2-lalonde-data&#34;&gt;Example 2: Lalonde Data&lt;/h2&gt;
&lt;p&gt;For this tutorial, we are goind to use the data from Lalonde (1981). You can find the data here: &lt;a href=&#34;https://users.nber.org/~rdehejia/nswdata.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://users.nber.org/~rdehejia/nswdata.html&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df = pd.read_csv(&#39;data/lalonde86.csv&#39;)
df.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;treat&lt;/th&gt;
      &lt;th&gt;age&lt;/th&gt;
      &lt;th&gt;education&lt;/th&gt;
      &lt;th&gt;black&lt;/th&gt;
      &lt;th&gt;hispanic&lt;/th&gt;
      &lt;th&gt;married&lt;/th&gt;
      &lt;th&gt;nodegree&lt;/th&gt;
      &lt;th&gt;re74&lt;/th&gt;
      &lt;th&gt;re75&lt;/th&gt;
      &lt;th&gt;re78&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;37.0&lt;/td&gt;
      &lt;td&gt;11.0&lt;/td&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;9930.0460&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;22.0&lt;/td&gt;
      &lt;td&gt;9.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;3595.8940&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;30.0&lt;/td&gt;
      &lt;td&gt;12.0&lt;/td&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;24909.4500&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;27.0&lt;/td&gt;
      &lt;td&gt;11.0&lt;/td&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;7506.1460&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;33.0&lt;/td&gt;
      &lt;td&gt;8.0&lt;/td&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;289.7899&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;We can summarize each variable by its treatment status.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df.groupby(&#39;treat&#39;).agg([&#39;mean&#39;, &#39;std&#39;]).T.unstack(1)
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead tr th {
    text-align: left;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;treat&lt;/th&gt;
      &lt;th colspan=&#34;2&#34; halign=&#34;left&#34;&gt;0.0&lt;/th&gt;
      &lt;th colspan=&#34;2&#34; halign=&#34;left&#34;&gt;1.0&lt;/th&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;mean&lt;/th&gt;
      &lt;th&gt;std&lt;/th&gt;
      &lt;th&gt;mean&lt;/th&gt;
      &lt;th&gt;std&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;age&lt;/th&gt;
      &lt;td&gt;25.053846&lt;/td&gt;
      &lt;td&gt;7.057745&lt;/td&gt;
      &lt;td&gt;25.816216&lt;/td&gt;
      &lt;td&gt;7.155019&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;education&lt;/th&gt;
      &lt;td&gt;10.088462&lt;/td&gt;
      &lt;td&gt;1.614325&lt;/td&gt;
      &lt;td&gt;10.345946&lt;/td&gt;
      &lt;td&gt;2.010650&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;black&lt;/th&gt;
      &lt;td&gt;0.826923&lt;/td&gt;
      &lt;td&gt;0.379043&lt;/td&gt;
      &lt;td&gt;0.843243&lt;/td&gt;
      &lt;td&gt;0.364558&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;hispanic&lt;/th&gt;
      &lt;td&gt;0.107692&lt;/td&gt;
      &lt;td&gt;0.310589&lt;/td&gt;
      &lt;td&gt;0.059459&lt;/td&gt;
      &lt;td&gt;0.237124&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;married&lt;/th&gt;
      &lt;td&gt;0.153846&lt;/td&gt;
      &lt;td&gt;0.361497&lt;/td&gt;
      &lt;td&gt;0.189189&lt;/td&gt;
      &lt;td&gt;0.392722&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;nodegree&lt;/th&gt;
      &lt;td&gt;0.834615&lt;/td&gt;
      &lt;td&gt;0.372244&lt;/td&gt;
      &lt;td&gt;0.708108&lt;/td&gt;
      &lt;td&gt;0.455867&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;re74&lt;/th&gt;
      &lt;td&gt;2107.026658&lt;/td&gt;
      &lt;td&gt;5687.905694&lt;/td&gt;
      &lt;td&gt;2095.573689&lt;/td&gt;
      &lt;td&gt;4886.620353&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;re75&lt;/th&gt;
      &lt;td&gt;1266.909002&lt;/td&gt;
      &lt;td&gt;3102.982044&lt;/td&gt;
      &lt;td&gt;1532.055314&lt;/td&gt;
      &lt;td&gt;3219.250870&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;re78&lt;/th&gt;
      &lt;td&gt;4554.801126&lt;/td&gt;
      &lt;td&gt;5483.835991&lt;/td&gt;
      &lt;td&gt;6349.143530&lt;/td&gt;
      &lt;td&gt;7867.402218&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;y = &#39;re78&#39;
D = &#39;treat&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;est = smf.ols(&#39;re78 ~ treat&#39;, df).fit()
est.summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
      &lt;td&gt;&lt;/td&gt;         &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt; &lt;td&gt; 4554.8011&lt;/td&gt; &lt;td&gt;  408.046&lt;/td&gt; &lt;td&gt;   11.162&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt; 3752.855&lt;/td&gt; &lt;td&gt; 5356.747&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;treat&lt;/th&gt;     &lt;td&gt; 1794.3424&lt;/td&gt; &lt;td&gt;  632.853&lt;/td&gt; &lt;td&gt;    2.835&lt;/td&gt; &lt;td&gt; 0.005&lt;/td&gt; &lt;td&gt;  550.574&lt;/td&gt; &lt;td&gt; 3038.110&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;X = [&#39;age&#39;, &#39;education&#39;, &#39;black&#39;, &#39;hispanic&#39;, &#39;married&#39;, &#39;nodegree&#39;]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;est = smf.ols(&#39;re78 ~ treat +&#39; + &#39; + &#39;.join(X), df).fit()
est.summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
      &lt;td&gt;&lt;/td&gt;         &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt; &lt;td&gt; 1168.0035&lt;/td&gt; &lt;td&gt; 3360.588&lt;/td&gt; &lt;td&gt;    0.348&lt;/td&gt; &lt;td&gt; 0.728&lt;/td&gt; &lt;td&gt;-5436.921&lt;/td&gt; &lt;td&gt; 7772.928&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;treat&lt;/th&gt;     &lt;td&gt; 1671.1304&lt;/td&gt; &lt;td&gt;  637.973&lt;/td&gt; &lt;td&gt;    2.619&lt;/td&gt; &lt;td&gt; 0.009&lt;/td&gt; &lt;td&gt;  417.254&lt;/td&gt; &lt;td&gt; 2925.007&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;age&lt;/th&gt;       &lt;td&gt;   52.8219&lt;/td&gt; &lt;td&gt;   45.255&lt;/td&gt; &lt;td&gt;    1.167&lt;/td&gt; &lt;td&gt; 0.244&lt;/td&gt; &lt;td&gt;  -36.123&lt;/td&gt; &lt;td&gt;  141.767&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;education&lt;/th&gt; &lt;td&gt;  393.8213&lt;/td&gt; &lt;td&gt;  227.114&lt;/td&gt; &lt;td&gt;    1.734&lt;/td&gt; &lt;td&gt; 0.084&lt;/td&gt; &lt;td&gt;  -52.549&lt;/td&gt; &lt;td&gt;  840.192&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;black&lt;/th&gt;     &lt;td&gt;-2220.2622&lt;/td&gt; &lt;td&gt; 1168.317&lt;/td&gt; &lt;td&gt;   -1.900&lt;/td&gt; &lt;td&gt; 0.058&lt;/td&gt; &lt;td&gt;-4516.480&lt;/td&gt; &lt;td&gt;   75.956&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;hispanic&lt;/th&gt;  &lt;td&gt;   83.7193&lt;/td&gt; &lt;td&gt; 1550.348&lt;/td&gt; &lt;td&gt;    0.054&lt;/td&gt; &lt;td&gt; 0.957&lt;/td&gt; &lt;td&gt;-2963.346&lt;/td&gt; &lt;td&gt; 3130.785&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;married&lt;/th&gt;   &lt;td&gt;  158.2084&lt;/td&gt; &lt;td&gt;  850.326&lt;/td&gt; &lt;td&gt;    0.186&lt;/td&gt; &lt;td&gt; 0.852&lt;/td&gt; &lt;td&gt;-1513.029&lt;/td&gt; &lt;td&gt; 1829.446&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;nodegree&lt;/th&gt;  &lt;td&gt; -128.2203&lt;/td&gt; &lt;td&gt;  995.416&lt;/td&gt; &lt;td&gt;   -0.129&lt;/td&gt; &lt;td&gt; 0.898&lt;/td&gt; &lt;td&gt;-2084.617&lt;/td&gt; &lt;td&gt; 1828.177&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df[&#39;pscore&#39;] = RandomForestRegressor().fit(df[X], df[D]).predict(df[X])
df[&#39;pscore&#39;].head()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;0    0.810000
1    0.752000
2    0.485786
3    0.670023
4    0.731667
Name: pscore, dtype: float64
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;est = smf.ols(&#39;re78 ~ treat + pscore&#39;, df).fit()
est.summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
      &lt;td&gt;&lt;/td&gt;         &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt; &lt;td&gt; 4313.9001&lt;/td&gt; &lt;td&gt;  591.433&lt;/td&gt; &lt;td&gt;    7.294&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt; 3151.530&lt;/td&gt; &lt;td&gt; 5476.270&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;treat&lt;/th&gt;     &lt;td&gt; 1461.6324&lt;/td&gt; &lt;td&gt;  866.171&lt;/td&gt; &lt;td&gt;    1.687&lt;/td&gt; &lt;td&gt; 0.092&lt;/td&gt; &lt;td&gt; -240.692&lt;/td&gt; &lt;td&gt; 3163.957&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;pscore&lt;/th&gt;    &lt;td&gt;  894.6072&lt;/td&gt; &lt;td&gt; 1588.766&lt;/td&gt; &lt;td&gt;    0.563&lt;/td&gt; &lt;td&gt; 0.574&lt;/td&gt; &lt;td&gt;-2227.867&lt;/td&gt; &lt;td&gt; 4017.082&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;Estimate CATE&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from econml.causal_forest import CausalForest

model = CausalForest(max_depth=2).fit(Y=df[y], X=df[X], T=df[D])
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from econml.cate_interpreter import SingleTreeCateInterpreter

intrp = SingleTreeCateInterpreter(max_depth=2).interpret(model, df[X])
intrp.plot(feature_names=X, fontsize=10)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/causal_trees_50_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Original paper: &lt;a href=&#34;https://www.pnas.org/doi/abs/10.1073/pnas.1510489113&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Recursive partitioning for heterogeneous causal effects&lt;/a&gt; (2016) by Athey and Imbens&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=JtDRpM6Mnnw&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Video lecture&lt;/a&gt; by Prof. Susan Athey (Stanford)&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Chi-Squared Test for Dummies</title>
      <link>https://matteocourthoud.github.io/post/chisquared/</link>
      <pubDate>Fri, 15 Apr 2022 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/post/chisquared/</guid>
      <description>&lt;p&gt;If you search the Wikipedia definition of Chi-Squared test, you get the following definition:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Pearson&amp;rsquo;s chi-squared test $\chi^2$ is a statistical test applied to sets of categorical data to evaluate how likely it is that any observed difference between the sets arose by chance.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;What does it mean? Let&amp;rsquo;s see it together.&lt;/p&gt;
&lt;h2 id=&#34;example-1-is-a-dice-fair&#34;&gt;Example 1: is a dice fair?&lt;/h2&gt;
&lt;p&gt;Suppose you want to test whether a dice is fair.&lt;/p&gt;
&lt;p&gt;You throw the dice 60 times and you count the number of times you get each outcome.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s simulate some data (from a fair dice).&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import numpy as np
import pandas as pd

# Data generating process
def generate_data_dice(N=60, seed=1):
    np.random.seed(1) # Set seed for replicability
    dice_numbers = [1,2,3,4,5,6]  # Dice numbers
    dice_throws = np.random.choice(dice_numbers, size=N)  # Actual dice throws
    data = pd.DataFrame({&amp;quot;dice number&amp;quot;: dice_numbers,
                         &amp;quot;observed&amp;quot;: [sum(dice_throws==n) for n in dice_numbers],
                         &amp;quot;expected&amp;quot;: N / 6})
    return data
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Generate data
data_dice = generate_data_dice()
data_dice
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;dice number&lt;/th&gt;
      &lt;th&gt;observed&lt;/th&gt;
      &lt;th&gt;expected&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;10&lt;/td&gt;
      &lt;td&gt;10.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;14&lt;/td&gt;
      &lt;td&gt;10.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;6&lt;/td&gt;
      &lt;td&gt;10.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;8&lt;/td&gt;
      &lt;td&gt;10.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;12&lt;/td&gt;
      &lt;td&gt;10.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;5&lt;/th&gt;
      &lt;td&gt;6&lt;/td&gt;
      &lt;td&gt;10&lt;/td&gt;
      &lt;td&gt;10.0&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;If we were throwing the dice &lt;strong&gt;a lot&lt;/strong&gt; of times, we would expect the same number of observations for each outcome. However, there is inherent noise in the process. How can we tell whether the fact that we didn&amp;rsquo;t get exactly 10 observations for each outcome is just due to randomness or it&amp;rsquo;s because the dice is unfair?&lt;/p&gt;
&lt;p&gt;The idea is to compute some statistic whose distribution is known under the assumption that the dice is fair, and then check if its value is &amp;ldquo;unusual&amp;rdquo; or not. If the value is particularly &amp;ldquo;unusual&amp;rdquo;, we reject the null hypothesis that the dice is fair.&lt;/p&gt;
&lt;p&gt;In our case, the statistic we choose is the chi-squared $\chi^{2}$ test-statistic.&lt;/p&gt;
&lt;p&gt;The value of the &lt;a href=&#34;https://en.wikipedia.org/wiki/Pearson%27s_chi-squared_test&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Pearson&amp;rsquo;s chi-squared test-statistic&lt;/a&gt; is&lt;/p&gt;
&lt;p&gt;$$
T_{\chi^2} = \sum _{i=1}^{n} \frac{(O_i - E_i)^{2}}{E_i} = N \sum _{i=1}^{n} \frac{\left(O_i/N - p_i \right)^2 }{p_i}
$$&lt;/p&gt;
&lt;p&gt;where&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$O_{i}$ = the number of observations of type i.&lt;/li&gt;
&lt;li&gt;$N$ = total number of observations&lt;/li&gt;
&lt;li&gt;$E_{i}=Np_{i}$ = the expected (theoretical) count of type $i$, asserted by the null hypothesis that the fraction of type $i$ in the population is $p_{i}$&lt;/li&gt;
&lt;li&gt;$n$ = the number of cells in the table.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Compute value of the statistic
def compute_chi2_stat(data):
    return sum( (data.observed - data.expected)**2 / data.expected )

chi2_stat = compute_chi2_stat(data_dice)
chi2_stat
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;4.0
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What do we make of this number? Is it &amp;ldquo;unusual&amp;rdquo;?&lt;/p&gt;
&lt;p&gt;If the dice were fair, the test Pearson&amp;rsquo;s chi-squared test-statistic $T_{\chi^2}$ is distributed as a chi-squared distribution with $k-1$ degrees of freedom, $\chi^2_{k-1}$. For the moment, take this claim at face value, we will verify it later, both empirically and theoretically. We will also discuss the degrees of freedom in detail later on.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Important!&lt;/strong&gt; Do not confuse the chi-squared test statistic (a number) with the chi-squared distribution (a distribution).&lt;/p&gt;
&lt;p&gt;What does a chi-squared distribution with $n-1$ degrees of freedom, $\chi^2_{k-1}$, look like?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;%matplotlib inline
%config InlineBackend.figure_format = &#39;retina&#39;
from src.utils import *
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from scipy.stats import chi2

# x-axis ranges from 0 to 20 with .001 steps
x = np.arange(0, 30, 0.001)

# Chi-square distribution with 5 degrees of freedom
chi2_5_pdf = chi2.pdf(x, df=5)

# Plot 
plt.plot(x, chi2_5_pdf);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/chisquared_11_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;How does the value of the statistic we have observed compares with its the distribution under the null hypothesis of a fair dice?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# plot Chi-square distribution with 5 degrees of freedom
plt.plot(x, chi2.pdf(x, df=5));
plt.vlines(chi2_stat, ymin=0, ymax=plt.ylim()[1], color=&#39;k&#39;, label=&#39;chi2 statistic&#39;)
plt.legend();
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/chisquared_13_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;The test statistic seems to fall well within the distribution, i.e. it does not seem to be an unusual event. Indeed, the question we want to answer is: &amp;ldquo;&lt;em&gt;under the null hypothesis that the dice is fair, how unlikely is the statistic we have observed?&lt;/em&gt;&amp;rdquo;.&lt;/p&gt;
&lt;p&gt;The last component we need in order to build a hypothesis test is a level of confidence, i.e. a threshold of &amp;ldquo;unlikeliness&amp;rdquo; of an event, below which we declare that the event is too unlikely under the model, for the model to be true. Let&amp;rsquo;s say we decide to set that threshold at 5%.&lt;/p&gt;
&lt;p&gt;If the likelihood of observing an even that (or more) extreme than the one we have actually observed is less than 5%, we reject the null hypothesis that the dice is fair.&lt;/p&gt;
&lt;p&gt;What is this value for a chi-squared distribution with 5 degrees of freedom?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Compute the Percent Point Function of the chi-squared distribution
z95 = chi2.ppf(0.95, df=5)
z95
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;11.070497693516351
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Since our value is smaller, we do not reject the null.&lt;/p&gt;
&lt;p&gt;We can plot the rejection and non-rejection areas in a plot.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Visualize test
def plot_test(x, stat, df):
    z95 = chi2.ppf(0.95, df=df)
    chi2_pdf = chi2.pdf(x, df=df)
    plt.plot(x, chi2_pdf);
    plt.fill_between(x[x&amp;gt;z95], chi2_pdf[x&amp;gt;z95], color=&#39;r&#39;, alpha=0.4, label=&#39;rejection area&#39;)
    plt.fill_between(x[x&amp;lt;z95], chi2_pdf[x&amp;lt;z95], color=&#39;g&#39;, alpha=0.4, label=&#39;non-rejection area&#39;)
    plt.vlines(chi2_stat, ymin=0, ymax=plt.ylim()[1], color=&#39;k&#39;, label=&#39;chi2 statistic&#39;)
    plt.legend();
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Plot
plot_test(x, chi2_stat, 5)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/chisquared_18_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;From the plot, we can clearly see that we do not reject the null hypothesis that the dice is fair.&lt;/p&gt;
&lt;h2 id=&#34;why-the-chi-squared-distribution&#34;&gt;Why the Chi-squared Distribution?&lt;/h2&gt;
&lt;p&gt;How do we know that that particular statistic has that particular distribution?&lt;/p&gt;
&lt;p&gt;Before digging into the math, we can check this claim via &lt;strong&gt;simulation&lt;/strong&gt;. We will repeat the procedure above many times, i.e.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;roll a (fair) dice 60 times&lt;/li&gt;
&lt;li&gt;compute the chi-square statistic&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;and then plot the distribution of chi square statistics.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Function to simulate data and compute chi2 stats
def simulate_chi2stats(K, N, dgp):
    chi2_stats = []
    for i in range(K):
        data = dgp()
        chi2_stats += [compute_chi2_stat(data)]
    return np.array(chi2_stats)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;chi2_stats = simulate_chi2stats(K=100, N=60, dgp=generate_data_dice)

# Plot data
plt.hist(chi2_stats, density=True, bins=30, alpha=0.3, color=&#39;C0&#39;);
plt.plot(x, chi2_5_pdf);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/chisquared_23_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Since we only did it 100 times, the distribution looks pretty coarse but vaguely close. Let&amp;rsquo;s now try 1000 times.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;chi2_stats = simulate_chi2stats(K=1000, N=60, dgp=generate_data_dice)

# Plot data
plt.hist(chi2_stats, density=True, bins=30, alpha=0.3, color=&#39;C0&#39;);
plt.plot(x, chi2_5_pdf);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/chisquared_25_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;The empirical distribution of the test statistic is indeed very close to its theoretical counterpart.&lt;/p&gt;
&lt;h2 id=&#34;some-statistics&#34;&gt;Some Statistics&lt;/h2&gt;
&lt;p&gt;Why does the distribution of the test statistic look like that? Let&amp;rsquo;s now dig deeper into the math.&lt;/p&gt;
&lt;p&gt;There are two things we need to know in order to understand the answer:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the Central Limit Theorem&lt;/li&gt;
&lt;li&gt;the relationship between a chi-squared and a normal distribution&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The &lt;a href=&#34;https://en.wikipedia.org/wiki/Central_limit_theorem&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Central Limit Theorem&lt;/a&gt; says that&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;In probability theory, the central limit theorem (CLT) establishes that, in many situations, when independent random variables are summed up, their properly normalized sum tends toward a normal distribution (informally a bell curve) even if the original variables themselves are not normally distributed.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Where does a normal distribution come up in our case? If we look at a single row in our data, i.e. the occurrences of a specific dice throw, it can be interpreted as the sum of realization from a &lt;a href=&#34;https://en.wikipedia.org/wiki/Bernoulli_distribution&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Bernoulli distribution&lt;/a&gt; with probability 1/6.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;In probability theory and statistics, the Bernoulli distribution is the discrete probability distribution of a random variable which takes the value $1$ with probability $p$ and the value $0$ with probability $q=1-p$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;In our case, the probability of getting a particular number is exactly 1/6. What is the distribution of the sum of its realizations? The Central Limit Theorem also tells us that:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;If $X_1, X_2, \dots , X_n, \dots$ are random samples drawn from a population with overall mean $\mu$ and finite variance $\sigma^2$, and if $\bar X_n$ is the sample mean of the first $n$ samples, then the limiting form of the distribution,&lt;/p&gt;
&lt;p&gt;$$
Z = \lim_{n \to \infty} \sqrt{n} \left( \frac{\bar X_n - \mu }{\sigma} \right)
$$&lt;/p&gt;
&lt;p&gt;is a standard normal distribution.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Therefore, in our case, the distribution of the sum of Bernoulli distributions with mean $p$ is distributed as a normal distribution with&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;mean $p$&lt;/li&gt;
&lt;li&gt;variance $p * (1-p)$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Therefore, we can obtain a random variable that is asymptotically standard normal distributed as&lt;/p&gt;
&lt;p&gt;$$
\lim_{n \to \infty} \ \sqrt{n} \left( \frac {\bar X_n - p}{\sqrt{p * (1-p)}} \right) \sim N(0,1)
$$&lt;/p&gt;
&lt;p&gt;Our last piece: what is a &lt;a href=&#34;https://en.wikipedia.org/wiki/Chi-squared_distribution&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;chi-squared distribution&lt;/a&gt;?&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;If $Z_1, &amp;hellip;, Z_k$ are independent, standard normal random variables, then the sum of their squares,&lt;/p&gt;
&lt;p&gt;$$
Q = \sum_{i=1}^k Z_i^2
$$
is distributed according to the chi-squared distribution with $k$ degrees of freedom.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;I.e. the sum of standard normal distributions is a chi-squared distribution, where the &lt;strong&gt;degrees of freedom&lt;/strong&gt; indicate the number of normal distributions we are summing over. Since the normalized sum of realizations of each dice number should converge to a standard normal distribution, their sum of squares should converge to a chi-squared distribution. I.e.&lt;/p&gt;
&lt;p&gt;$$
\lim_{n \to \infty} \ \sum_k n \frac{(\bar X_n - p)^2}{p * (1-p)} \sim \chi^2_k
$$&lt;/p&gt;
&lt;p&gt;There is just one issue: the last distribution is not really independent from the others. In fact, as soon as we know that we have thrown 60 dices and how many 1s, 2s, 3s, 4s, and 5s we got, we can compute the number of 6s. Therefore, we should exclude one distribution since only 5 (or, in general, $k-1$) are truly independent.&lt;/p&gt;
&lt;p&gt;In practice, however, we sum all distributions, but then we scale them down by multiplying them by $(1-p)$ so that we have&lt;/p&gt;
&lt;p&gt;$$
\lim_{n \to \infty} \ \sum_k n \frac{(\bar X_n - p)^2}{p} \sim \chi^2_{k-1}
$$&lt;/p&gt;
&lt;p&gt;which is exactly the formula we used to compute the test statistic:&lt;/p&gt;
&lt;p&gt;$$
T_{\chi^2} = \sum _{i=1}^{n} \frac{(O_i - E_i)^{2}}{E_i} = N \sum _{i=1}^{n} \frac{\left(O_i/N - p_i \right)^2 }{p_i}
$$&lt;/p&gt;
&lt;h2 id=&#34;example-2-are-grades-independent-from-gender&#34;&gt;Example 2: are grades independent from gender?&lt;/h2&gt;
&lt;p&gt;Chi-squared tests can also be used to test independence between 2 variables. The idea is fundamentally the same as the test in the previous section: checking systematic differences between observed and expected values, across different variables.&lt;/p&gt;
&lt;p&gt;Suppose you have data on grades in a classroom, by gender. Grades go from $1$ to $4$. Assuming males and females are equally prepared for the test, you want to test whether there has been discrimination in grading.&lt;/p&gt;
&lt;p&gt;The problem is again asserting whether the observed differences are random or systematic.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s generate some data (under the no discrimination assumption).&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Data generating process for grades
def generate_data_grades(N_male=60, N_female=40):
    grade_scale = [1,2,3,4]
    p = [0.1, 0.2, 0.5, 0.2]
    grades_male = np.random.choice(grade_scale, size=N_male, p=p)
    grades_female = np.random.choice(grade_scale, size=N_female, p=p)
    data = pd.DataFrame({&amp;quot;grade&amp;quot;: grade_scale + grade_scale,
                          &amp;quot;gender&amp;quot;: [&amp;quot;male&amp;quot; for i in grade_scale] + [&amp;quot;female&amp;quot; for i in grade_scale],
                          &amp;quot;observed&amp;quot;: [sum(grades_male==n) for n in grade_scale] + [sum(grades_female==n) for n in grade_scale],
                        })  
    data[&#39;expected gender&#39;] = data.groupby(&amp;quot;gender&amp;quot;)[&amp;quot;observed&amp;quot;].transform(&amp;quot;mean&amp;quot;) 
    data[&#39;expected grade&#39;] = data.groupby(&amp;quot;grade&amp;quot;)[&amp;quot;observed&amp;quot;].transform(&amp;quot;mean&amp;quot;) 
    data[&#39;expected&#39;] = data[&#39;expected gender&#39;] * data[&#39;expected grade&#39;] / data[&#39;observed&#39;].mean()
    return data
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Set seed for replicability
np.random.seed(1)

# Generate data
data_grades = generate_data_grades()
data_grades
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;grade&lt;/th&gt;
      &lt;th&gt;gender&lt;/th&gt;
      &lt;th&gt;observed&lt;/th&gt;
      &lt;th&gt;expected gender&lt;/th&gt;
      &lt;th&gt;expected grade&lt;/th&gt;
      &lt;th&gt;expected&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;male&lt;/td&gt;
      &lt;td&gt;9&lt;/td&gt;
      &lt;td&gt;15.0&lt;/td&gt;
      &lt;td&gt;5.5&lt;/td&gt;
      &lt;td&gt;6.6&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;male&lt;/td&gt;
      &lt;td&gt;14&lt;/td&gt;
      &lt;td&gt;15.0&lt;/td&gt;
      &lt;td&gt;10.5&lt;/td&gt;
      &lt;td&gt;12.6&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;male&lt;/td&gt;
      &lt;td&gt;27&lt;/td&gt;
      &lt;td&gt;15.0&lt;/td&gt;
      &lt;td&gt;24.5&lt;/td&gt;
      &lt;td&gt;29.4&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;male&lt;/td&gt;
      &lt;td&gt;10&lt;/td&gt;
      &lt;td&gt;15.0&lt;/td&gt;
      &lt;td&gt;9.5&lt;/td&gt;
      &lt;td&gt;11.4&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;female&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;10.0&lt;/td&gt;
      &lt;td&gt;5.5&lt;/td&gt;
      &lt;td&gt;4.4&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;5&lt;/th&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;female&lt;/td&gt;
      &lt;td&gt;7&lt;/td&gt;
      &lt;td&gt;10.0&lt;/td&gt;
      &lt;td&gt;10.5&lt;/td&gt;
      &lt;td&gt;8.4&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;6&lt;/th&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;female&lt;/td&gt;
      &lt;td&gt;22&lt;/td&gt;
      &lt;td&gt;10.0&lt;/td&gt;
      &lt;td&gt;24.5&lt;/td&gt;
      &lt;td&gt;19.6&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;7&lt;/th&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;female&lt;/td&gt;
      &lt;td&gt;9&lt;/td&gt;
      &lt;td&gt;10.0&lt;/td&gt;
      &lt;td&gt;9.5&lt;/td&gt;
      &lt;td&gt;7.6&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;Has there been discrimination?&lt;/p&gt;
&lt;p&gt;The value of the test-statistic is&lt;/p&gt;
&lt;p&gt;$$
T_{\chi^2} = \sum_{i=1}^r \sum_{j=1}^c \frac{(O_{i,j} - E_{i,j})^2 }{ E_{i,j} } = N \sum_{i,j} p_{i \cdot} p_{\cdot j} \left( \frac{O_{i,j}/N - p_{i \cdot} p_{\cdot j} }{ p_{i \cdot} p_{\cdot j}} \right)^2
$$&lt;/p&gt;
&lt;p&gt;where&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;$N$ is the total sample size (the sum of all cells in the table)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$p_{i \cdot} = \frac{O_{i\cdot }}{N} = \sum_{j=1}^{c} \frac{O_{i,j}}{N}$ is the fraction of observations of type i ignoring the column attribute (fraction of row totals), and&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$p_{\cdot j} = \frac{O_{\cdot j}}{N} = \sum_{i=1}^{r} \frac{O_{i,j}}{N}$ is the fraction of observations of type j ignoring the row attribute (fraction of column totals).&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;So the formula for the test statistic is the same&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;chi2_stat = compute_chi2_stat(data_grades)
chi2_stat
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;3.490327550477927
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As before, we can double-check whether the statistic is indeed distributed as a chi-squared with $k-1$ degrees of freedom by simulating the data generating process.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Plot data
chi2_stats = simulate_chi2stats(K=1000, N=60, dgp=generate_data_grades)
plt.hist(chi2_stats, density=True, bins=30, alpha=0.3, color=&#39;C0&#39;);
plt.plot(x, chi2_5_pdf);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/chisquared_37_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;What happened? We forgot to change the degrees of freedom! The general formula for the degrees of freedom when testing the independence of variables is $(N_i - 1) \times (N_j - 1)$. So in our case, it&amp;rsquo;s $(4-1) \times (2-1) = 3$.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;chi2_3_pdf = chi2.pdf(x, df=3)

# Plot data
chi2_stats = simulate_chi2stats(K=1000, N=60, dgp=generate_data_grades)
plt.hist(chi2_stats, density=True, bins=30, alpha=0.3, color=&#39;C0&#39;);
plt.plot(x, chi2_3_pdf);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/chisquared_39_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Do we reject the null hypothesis of independent distributions of gender and grades?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Plot
plot_test(x, chi2_stat, df=3)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/chisquared_41_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;No, we &lt;strong&gt;do not reject&lt;/strong&gt; the null hypothesis of independend distributions of gender and grades.&lt;/p&gt;
&lt;h2 id=&#34;example-3-testing-a-specific-data-generating-process&#34;&gt;Example 3: testing a specific data generating process&lt;/h2&gt;
&lt;p&gt;As we have seen, the chi-square test can be used to compare observed means/frequencies against a null hypothesis. How can we use this statistic to &lt;strong&gt;test a distributional assumption&lt;/strong&gt;?&lt;/p&gt;
&lt;p&gt;The answer is simple: we can construct conditional means. The easiest way to do it is to bin the data into equally sized bins and then check if the observed frequencies match the expected probabilities.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: we don&amp;rsquo;t need to have equally sized bins, but it&amp;rsquo;s useful since it ensures that we have as many observations in each bin as possible.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from scipy.stats import expon

# Data generating process
def generate_poisson_data(N=60, cuts=4):
    poisson_draws = np.random.exponential(size=N)
    cat, bins = pd.qcut(poisson_draws, cuts, retbins=True)
    p = [expon.cdf(bins[n+1]) - expon.cdf(bins[n]) for n in range(len(bins)-1)]
    data = pd.DataFrame({&amp;quot;bin&amp;quot;: cat.unique(),
                         &amp;quot;observed&amp;quot;: [sum(cat==n) for n in cat.unique()],
                         &amp;quot;expected&amp;quot;: np.dot(p, N)})
    return data, poisson_draws
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Set seed for replicability
np.random.seed(2)

# Generate data
data_poisson, poisson_draws = generate_poisson_data()
data_poisson
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;bin&lt;/th&gt;
      &lt;th&gt;observed&lt;/th&gt;
      &lt;th&gt;expected&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;(0.254, 0.573]&lt;/td&gt;
      &lt;td&gt;15&lt;/td&gt;
      &lt;td&gt;11.919795&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;(0.0253, 0.254]&lt;/td&gt;
      &lt;td&gt;15&lt;/td&gt;
      &lt;td&gt;12.706904&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;(0.573, 0.923]&lt;/td&gt;
      &lt;td&gt;15&lt;/td&gt;
      &lt;td&gt;9.967649&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;(0.923, 5.092]&lt;/td&gt;
      &lt;td&gt;15&lt;/td&gt;
      &lt;td&gt;23.481200&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;We can also plot the observed and realized distribution of the data.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Plot data
plt.hist(poisson_draws, density=True, bins=30, alpha=0.3, color=&#39;C0&#39;);
exp_pdf = expon.pdf(x)
plt.plot(x, exp_pdf);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/chisquared_48_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;The two distributions seem close but we need a test statistic in order to assess whether the dgp is indeed an exponential distribution&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;chi2_stat = compute_chi2_stat(data_poisson)
chi2_stat
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;6.813781416601728
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Do we reject the null hypothesis that the data is drawn from an exponential distribution?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Plot
plot_test(x, chi2_stat, df=3)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/chisquared_52_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;No, we &lt;strong&gt;do not reject&lt;/strong&gt; the null hypothesis that the data is drawn from an exponential distribution.&lt;/p&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;In this tutorial, we have seen how to perform 3 hypoteses tests&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;testing if a set of means or sums is coming from the expected distribution&lt;/li&gt;
&lt;li&gt;testing if two distributions are independent or not&lt;/li&gt;
&lt;li&gt;testing a specific data generating process&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The underlying principle is the same: testing discrepancies between expected and observed count data.&lt;/p&gt;
&lt;p&gt;The key statistic is Pearson&amp;rsquo;s chi-square statistic and the key distribution is the chi-squared distribution. We have seen how to compute the statistic, why it has a chi-squared distribution, and how to use this information to perform a statistical hypothesis test.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Difference in Differences</title>
      <link>https://matteocourthoud.github.io/post/diff_in_diffs/</link>
      <pubDate>Fri, 15 Apr 2022 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/post/diff_in_diffs/</guid>
      <description>&lt;p&gt;In this tutorial, we are going to see how to estimate the causal effect of a treatment on an outcome when treatment assignment is not random, but we observe both treated and untreated units before and after treatment. Under certain structural assumptions, especially parallel outcome trends in the absence of treatment, we can recover the average treatment effect.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Requisites&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;For this tutorial, I assume you are familiar with the following concepts:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Rubin&amp;rsquo;s potential outcome framework&lt;/li&gt;
&lt;li&gt;Ordinary least squares regression&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Academic Application&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;As an academic application, we are going to replicate &lt;a href=&#34;http://sims.princeton.edu/yftp/emet04/ck/CardKruegerMinWage.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Minimum Wages and Employment: A Case Study of the Fast-Food Industry in New Jersey and Pennsylvania&lt;/a&gt; (1994) by Card and Krueger. The authors study the effect of a minimum wage policy in New Jersey on emplyment, by using Pennsylvania as a control state.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Business Case&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;As a business case, we are going to study a firm that has run a TV ad campaign. The firm would like to understand the impact of the campaign on revenue and has randomized the campaign over municipalities.&lt;/p&gt;
&lt;h2 id=&#34;setting&#34;&gt;Setting&lt;/h2&gt;
&lt;p&gt;We assume that for a set of i.i.d. subjects $i = 1, &amp;hellip;, n$ over $T$ time periods $t = 1 ,  &amp;hellip; , T$, we observed a tuple $(X_{it}, D_{it}, Y_{it})$ comprised of&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;a feature vector $X_{it} \in \mathbb R^{p}$&lt;/li&gt;
&lt;li&gt;a treatment assignment $D_i \in \lbrace 0, 1 \rbrace$&lt;/li&gt;
&lt;li&gt;a response $Y_{it} \in \mathbb R$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We assume that treatment occurs between time $t=0$ and time $t=1$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Assumption 1: parallel trends&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In the absence of treatment, the outcome $Y_{it}$ &lt;strong&gt;evolve in parallel&lt;/strong&gt; across units, i.e. and their $\gamma_{t}$ are the same.&lt;/p&gt;
&lt;p&gt;$$
Y_{it}^{(0)} - Y_{j,t}^{(0)} = \alpha \quad \forall \ t
$$&lt;/p&gt;
&lt;h2 id=&#34;diff-in-diffs&#34;&gt;Diff-in-diffs&lt;/h2&gt;
&lt;p&gt;In this setting, we cannot estimate any causal parameter with any other &lt;strong&gt;further assumption&lt;/strong&gt;. What is the minimal number of assumptions that we could make in order to estimate a causal parameter?&lt;/p&gt;
&lt;p&gt;If we were to assume that treatment was randomly assigned, we could retrieve the average treatment effect as a difference in means.&lt;/p&gt;
&lt;p&gt;$$
\mathbb E[\tau_t] = \mathbb E \big[ Y_{it} \ \big| \ D_i = 1 \big] - \mathbb E \big[ Y_{it} \big| \ D_i = 0 \big]
$$&lt;/p&gt;
&lt;p&gt;However, it would be a very strong assumption, and it would ignore some information that we possess: the time dimension (pre-post).&lt;/p&gt;
&lt;p&gt;If we were to assume instead that no other shocks affected the treated units between period $t=0$ and $t=1$, we could retrieve the average treatment effect on the treated as a pre-post difference.&lt;/p&gt;
&lt;p&gt;$$
\mathbb E[\tau | D_i=1] = \mathbb E \big[ Y_{i1} \ \big| \ D_i = 1 \big] - \mathbb E \big[ Y_{i0} \ \big| \ D_i = 1 \big]
$$&lt;/p&gt;
&lt;p&gt;However, it also this would be a very strong assumption, and it would ignore the fact that we have control units.&lt;/p&gt;
&lt;p&gt;Can we make less stringent assumption and still recover a causal parameter using both the availability of a (non-random) control group and the time dimension?&lt;/p&gt;
&lt;h3 id=&#34;did-model&#34;&gt;DiD Model&lt;/h3&gt;
&lt;p&gt;The model that is commonly assumed in diff-ind-diff settings, is the following&lt;/p&gt;
&lt;p&gt;$$
Y_{it} (D_{it}) = \alpha_{i} + \gamma_{t} + \tau_{i} D_{it}
$$&lt;/p&gt;
&lt;p&gt;First, let&amp;rsquo;s summarize the potential outcome values $Y^{(d)}_{it}$ in the simple $2 \times 2$ setting.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;$t=0$&lt;/th&gt;
&lt;th&gt;$t=1$&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;$D=0$&lt;/td&gt;
&lt;td&gt;$\gamma_0 + \alpha_i$&lt;/td&gt;
&lt;td&gt;$\gamma_1 + \alpha_i$&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;$D=1$&lt;/td&gt;
&lt;td&gt;$\gamma_0 + \alpha_i + \tau_i$&lt;/td&gt;
&lt;td&gt;$\gamma_1 + \alpha_i + \tau_i$&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;For a single unit, $i$, the pre-post outcome difference is given by&lt;/p&gt;
&lt;p&gt;$$
Y_{i1} - Y_{i0} = (\gamma_1 - \gamma_0) + \tau_i (D_{i1} - D_{i0})
$$&lt;/p&gt;
&lt;p&gt;If we take the difference of the expression above between treated and untreated units, we get&lt;/p&gt;
&lt;p&gt;$$
\mathbb E \Big[ Y_{i1} - Y_{i0} \ \Big| \ D_{i1} - D_{i0} = 1 \Big] - \mathbb E \Big[ Y_{i1} - Y_{i0} \ \Big| \ D_{i1} - D_{i0} = 0 \Big] = \mathbb E \Big[ \tau_i \ \Big| \ D_{i1} - D_{i0} = 1 \Big] = ATT
$$&lt;/p&gt;
&lt;p&gt;which is the average treatment effect on the treated (ATT).&lt;/p&gt;
&lt;p&gt;We can get this double difference with the folowing regressio model&lt;/p&gt;
&lt;p&gt;$$
Y_{it} (D_{it}) = \alpha_{i} + \gamma_{t} + \beta D_{it} + \varepsilon_{it}
$$&lt;/p&gt;
&lt;p&gt;where the OLS estimator $\hat \beta$ will be unbiased for the ATT.&lt;/p&gt;
&lt;h3 id=&#34;multiple-time-periods&#34;&gt;Multiple Time Periods&lt;/h3&gt;
&lt;p&gt;What if we didn&amp;rsquo;t just have one pre-treatment period and one post-treatment period? Great! We can actually do more things.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;We can partially test assumptions&lt;/li&gt;
&lt;li&gt;We can estimate dynamic effects&lt;/li&gt;
&lt;li&gt;We can run placebo tests&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;How do we implement it? Run a regression with multiple interactions&lt;/p&gt;
&lt;p&gt;$$
Y_{it} (D_{it}) = \alpha_{i} + \gamma_{t} + \sum_{t=1}^{T} \beta_t D_{it} + \varepsilon_{it}
$$&lt;/p&gt;
&lt;h3 id=&#34;comments&#34;&gt;Comments&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Parametric Assumption&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The diff-in-diffs method makes a lot of parametric assumptions that are is easy to forget.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Inference&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Bertrand, Duflo, and Mullainathan (2004) point out that conventional robust standard errors usually overestimate the actual standard deviation of the estimator. The authors recommend &lt;strong&gt;clustering&lt;/strong&gt; the standard errors at the level of randomization (e.g. classes, counties, villages, &amp;hellip;).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Testing pre-trends&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Having multiple pre-treatment time periods is helpful for testing the parallel trends assumption. However, this practice can lead to pre-testing bias. In particular, if one selects results based on a pre-treatment parallel trend test, inference on the ATT gets distorderd.&lt;/p&gt;
&lt;h2 id=&#34;academic-application&#34;&gt;Academic Application&lt;/h2&gt;
&lt;p&gt;As an academic application, we are going to replicate &lt;a href=&#34;http://sims.princeton.edu/yftp/emet04/ck/CardKruegerMinWage.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Minimum Wages and Employment: A Case Study of the Fast-Food Industry in New Jersey and Pennsylvania&lt;/a&gt; (1994) by Card and Krueger. The authors study the effect of a minimum wage policy in New Jersey on emplyment, by using Pennsylvania as a control state.&lt;/p&gt;
&lt;p&gt;The authors describe the setting as follows&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;On April 1, 1992, New Jersey&amp;rsquo;s minimum wage rose from $4.25 to $5.05 per hour. To evaluate the impact of the law, the authors surveyed 410 fast-food restaurants in New Jersey and eastern Pennsylvania before and after the rise. Comparisons of employment growth at stores in New Jersey and Pennsylvania (where the minimum wage was constant) provide simple estimates of the effect of the higher minimum wage.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Let&amp;rsquo;s start by loading and inspecting the data.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;%matplotlib inline
%config InlineBackend.figure_format = &#39;retina&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from src.utils import *
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from src.data import import_ck94

df = pd.read_csv(&#39;data/ck94.csv&#39;)
df.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;id&lt;/th&gt;
      &lt;th&gt;after&lt;/th&gt;
      &lt;th&gt;new_jersey&lt;/th&gt;
      &lt;th&gt;chain&lt;/th&gt;
      &lt;th&gt;employment&lt;/th&gt;
      &lt;th&gt;hrsopen&lt;/th&gt;
      &lt;th&gt;wage&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;wendys&lt;/td&gt;
      &lt;td&gt;34.0&lt;/td&gt;
      &lt;td&gt;12.0&lt;/td&gt;
      &lt;td&gt;5.00&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;wendys&lt;/td&gt;
      &lt;td&gt;24.0&lt;/td&gt;
      &lt;td&gt;12.0&lt;/td&gt;
      &lt;td&gt;5.50&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;6&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;burgerking&lt;/td&gt;
      &lt;td&gt;70.5&lt;/td&gt;
      &lt;td&gt;18.0&lt;/td&gt;
      &lt;td&gt;5.00&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;7&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;burgerking&lt;/td&gt;
      &lt;td&gt;23.5&lt;/td&gt;
      &lt;td&gt;24.0&lt;/td&gt;
      &lt;td&gt;5.00&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;8&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;kfc&lt;/td&gt;
      &lt;td&gt;11.0&lt;/td&gt;
      &lt;td&gt;10.0&lt;/td&gt;
      &lt;td&gt;5.25&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;We have information on fast food restaurants, indexed by &lt;code&gt;i&lt;/code&gt;, at time &lt;code&gt;t&lt;/code&gt;. We distinguish between before and faster treatment and between New Jersey &lt;code&gt;nj&lt;/code&gt; and Pennsylvania restaurants. We also know the &lt;code&gt;chain&lt;/code&gt; of the restaurant, the &lt;code&gt;employment&lt;/code&gt;, the hours open &lt;code&gt;hrsopen&lt;/code&gt; and the &lt;code&gt;wage&lt;/code&gt;. We are interested on the effect on the policy on wages.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s start by producing the $2 \times 2$ table of treatment-control before-after average outcomes.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df.pivot_table(index=&#39;new_jersey&#39;, columns=&#39;after&#39;, values=&#39;employment&#39;, aggfunc=&#39;mean&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;after&lt;/th&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;th&gt;1&lt;/th&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;new_jersey&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;23.704545&lt;/td&gt;
      &lt;td&gt;21.825758&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;20.657746&lt;/td&gt;
      &lt;td&gt;21.048415&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;From the table we can see that a simple &lt;strong&gt;before-after comparison&lt;/strong&gt; would give a small posive effect of $21.05 - 20.66 = 0.39$.&lt;/p&gt;
&lt;p&gt;On the other hand, if one was doing an ex-post &lt;strong&gt;treated-control comparison&lt;/strong&gt;, would get a negative effect of $21.05 - 21.83 = - 0.78$.&lt;/p&gt;
&lt;p&gt;The difference-in-differences estimator takes into account the fact that&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;There is a pre-treatment level difference between New Jersey and Pennsylvania&lt;/li&gt;
&lt;li&gt;Employment was falling in Pennsylvania even without treatment&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The &lt;strong&gt;double difference&lt;/strong&gt; in means gives a positive effect, significantly larger than any of the two previous estimates.&lt;/p&gt;
&lt;p&gt;$$
\hat \tau_{DiD} = \Big( 21.05 - 20.66 \Big) - \Big( 21.83 - 23.70 \Big) = 0.39 + 1.87 = 2.26
$$&lt;/p&gt;
&lt;p&gt;We can replicate the result with a linear regression.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;smf.ols(&#39;employment ~ new_jersey * after&#39;, df).fit().summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
          &lt;td&gt;&lt;/td&gt;            &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt;        &lt;td&gt;   23.7045&lt;/td&gt; &lt;td&gt;    1.149&lt;/td&gt; &lt;td&gt;   20.627&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;   21.448&lt;/td&gt; &lt;td&gt;   25.961&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;new_jersey&lt;/th&gt;       &lt;td&gt;   -3.0468&lt;/td&gt; &lt;td&gt;    1.276&lt;/td&gt; &lt;td&gt;   -2.388&lt;/td&gt; &lt;td&gt; 0.017&lt;/td&gt; &lt;td&gt;   -5.552&lt;/td&gt; &lt;td&gt;   -0.542&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;after&lt;/th&gt;            &lt;td&gt;   -1.8788&lt;/td&gt; &lt;td&gt;    1.625&lt;/td&gt; &lt;td&gt;   -1.156&lt;/td&gt; &lt;td&gt; 0.248&lt;/td&gt; &lt;td&gt;   -5.070&lt;/td&gt; &lt;td&gt;    1.312&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;new_jersey:after&lt;/th&gt; &lt;td&gt;    2.2695&lt;/td&gt; &lt;td&gt;    1.804&lt;/td&gt; &lt;td&gt;    1.258&lt;/td&gt; &lt;td&gt; 0.209&lt;/td&gt; &lt;td&gt;   -1.273&lt;/td&gt; &lt;td&gt;    5.812&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;The effect is $2.26$, but it is not significantly different from zero.&lt;/p&gt;
&lt;h2 id=&#34;business-case&#34;&gt;Business Case&lt;/h2&gt;
&lt;p&gt;We are given the following problem:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;A firm wants to test the impact of a TV advertisement campaign on revenue. The firm releases the ad on a random sample of municipalities and track the revenue over time, before and after the ad campaign.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;We start by drawing a sample from the data generating process.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from src.dgp import dgp_did

dgp = dgp_did()
df = dgp.generate_data()
df.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;day&lt;/th&gt;
      &lt;th&gt;id&lt;/th&gt;
      &lt;th&gt;treated&lt;/th&gt;
      &lt;th&gt;post&lt;/th&gt;
      &lt;th&gt;revenue&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;3.599341&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;-0.146912&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;0.696527&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;1.445169&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;1.659696&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;We have &lt;code&gt;revenue&lt;/code&gt; data on a set of customers over time. We also know to which &lt;code&gt;group&lt;/code&gt; they were assigned and whether the time is before or after the intervention.&lt;/p&gt;
&lt;p&gt;Since we do not have any control variable, we can directly visualize the revenue dynamics, distinguishing between treatment and control group.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sns.lineplot(x=df[&#39;day&#39;], y=df[&#39;revenue&#39;], hue=df[&#39;treated&#39;]);
plt.axvline(x=10, ls=&amp;quot;:&amp;quot;, color=&#39;C2&#39;);
plt.title(&#39;Revenue over time&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/diff_in_diffs_26_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;It seems like the treatment group was producing higher revenues before treatment and the gap has increased with treatment but it is closing over time.&lt;/p&gt;
&lt;p&gt;To assess the magnitude of the effect and perform inference, we can regress revenue on a post-treatment dummy, a treatment dummy and their interaction.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;smf.ols(&#39;revenue ~ post * treated&#39;, df).fit().summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
            &lt;td&gt;&lt;/td&gt;              &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt;            &lt;td&gt;    4.6357&lt;/td&gt; &lt;td&gt;    0.078&lt;/td&gt; &lt;td&gt;   59.428&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    4.483&lt;/td&gt; &lt;td&gt;    4.789&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;post[T.True]&lt;/th&gt;         &lt;td&gt;    0.8928&lt;/td&gt; &lt;td&gt;    0.110&lt;/td&gt; &lt;td&gt;    8.093&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    0.676&lt;/td&gt; &lt;td&gt;    1.109&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;treated&lt;/th&gt;              &lt;td&gt;    1.0558&lt;/td&gt; &lt;td&gt;    0.110&lt;/td&gt; &lt;td&gt;    9.571&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    0.839&lt;/td&gt; &lt;td&gt;    1.272&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;post[T.True]:treated&lt;/th&gt; &lt;td&gt;    0.1095&lt;/td&gt; &lt;td&gt;    0.156&lt;/td&gt; &lt;td&gt;    0.702&lt;/td&gt; &lt;td&gt; 0.483&lt;/td&gt; &lt;td&gt;   -0.196&lt;/td&gt; &lt;td&gt;    0.415&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;While the coefficient for the interaction term is positive, it does not seem to be statistically significant. However, this might be due to the fact that the treatment effect is fading away over time.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s fit the same regression, with a linear time trend.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;smf.ols(&#39;revenue ~ post * treated * day&#39;, df).fit().summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
              &lt;td&gt;&lt;/td&gt;                &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt;                &lt;td&gt;    4.1144&lt;/td&gt; &lt;td&gt;    0.168&lt;/td&gt; &lt;td&gt;   24.501&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    3.785&lt;/td&gt; &lt;td&gt;    4.444&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;post[T.True]&lt;/th&gt;             &lt;td&gt;    0.2871&lt;/td&gt; &lt;td&gt;    0.458&lt;/td&gt; &lt;td&gt;    0.626&lt;/td&gt; &lt;td&gt; 0.531&lt;/td&gt; &lt;td&gt;   -0.612&lt;/td&gt; &lt;td&gt;    1.186&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;treated&lt;/th&gt;                  &lt;td&gt;    1.0788&lt;/td&gt; &lt;td&gt;    0.237&lt;/td&gt; &lt;td&gt;    4.543&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    0.613&lt;/td&gt; &lt;td&gt;    1.544&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;post[T.True]:treated&lt;/th&gt;     &lt;td&gt;    1.5910&lt;/td&gt; &lt;td&gt;    0.648&lt;/td&gt; &lt;td&gt;    2.454&lt;/td&gt; &lt;td&gt; 0.014&lt;/td&gt; &lt;td&gt;    0.320&lt;/td&gt; &lt;td&gt;    2.862&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;day&lt;/th&gt;                      &lt;td&gt;    0.0948&lt;/td&gt; &lt;td&gt;    0.027&lt;/td&gt; &lt;td&gt;    3.502&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    0.042&lt;/td&gt; &lt;td&gt;    0.148&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;post[T.True]:day&lt;/th&gt;         &lt;td&gt;   -0.0221&lt;/td&gt; &lt;td&gt;    0.038&lt;/td&gt; &lt;td&gt;   -0.576&lt;/td&gt; &lt;td&gt; 0.564&lt;/td&gt; &lt;td&gt;   -0.097&lt;/td&gt; &lt;td&gt;    0.053&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;treated:day&lt;/th&gt;              &lt;td&gt;   -0.0042&lt;/td&gt; &lt;td&gt;    0.038&lt;/td&gt; &lt;td&gt;   -0.109&lt;/td&gt; &lt;td&gt; 0.913&lt;/td&gt; &lt;td&gt;   -0.079&lt;/td&gt; &lt;td&gt;    0.071&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;post[T.True]:treated:day&lt;/th&gt; &lt;td&gt;   -0.0929&lt;/td&gt; &lt;td&gt;    0.054&lt;/td&gt; &lt;td&gt;   -1.716&lt;/td&gt; &lt;td&gt; 0.086&lt;/td&gt; &lt;td&gt;   -0.199&lt;/td&gt; &lt;td&gt;    0.013&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;Now the treatment effect is positive and significant at the 5% level. And indeed, we estimate a decreasing trend, post treatment, for the treated. However, it is not statistically significant.&lt;/p&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=mbYTZ0w-QTw&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Video lecture on Difference-in-Differences&lt;/a&gt; by Paul Goldsmith-Pinkham (Yale)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://matheusfacure.github.io/python-causality-handbook/13-Difference-in-Differences.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Chapter 13&lt;/a&gt; of Causal Inference for The Brave and The True by Matheus Facure&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://mixtape.scunning.com/difference-in-differences.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Chapter 9&lt;/a&gt; of The Causal Inference Mixtape by Scott Cunningham&lt;/li&gt;
&lt;li&gt;Chapter 5 of &lt;a href=&#34;https://www.mostlyharmlesseconometrics.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Mostly Harmless Econometrics&lt;/a&gt; by Agrist and Pischke&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Instrumental Variables</title>
      <link>https://matteocourthoud.github.io/post/iv/</link>
      <pubDate>Fri, 15 Apr 2022 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/post/iv/</guid>
      <description>&lt;p&gt;In this tutorial, we are going to see how to estimate causal effects when the treatment is not randomly assigned, but we have access to a third variable that is as good as randomly assigned and is correlated (only) with the treatment. These variables are called instrumental variables and are a powerful tool for causal inference, especially in observational studies.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Requisites&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;For this tutorial, I assume you are familiar with the following concepts:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Rubin&amp;rsquo;s potential outcome framework&lt;/li&gt;
&lt;li&gt;Ordinary least squares regression&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Academic Application 1&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;As a first academic application, we are going to replicate &lt;a href=&#34;https://www.jstor.org/stable/2937954&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Does Compulsory School Attendance Affect Schooling and Earnings?&lt;/a&gt; (1991) by Angrist and Krueger. The authors study the effect of education on wages.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Academic Application 2&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;As a further academic application, we are going to replicate &lt;a href=&#34;https://economics.mit.edu/files/4123&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The Colonial Origins of Comparative Development&lt;/a&gt; (2002) by Acemoglu, Johnson, Robinson. The authors study the effect of institutions on economic development.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Business Case&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;As a business case, we are going to study a company that wants to find out whether subscribing to its newsletter has an effect on revenues. Since the travel agency cannot force customers to subscribing to the newsletter, it randomly sends reminder emails to infer the effect of the newsletter on revenues.&lt;/p&gt;
&lt;h2 id=&#34;setting&#34;&gt;Setting&lt;/h2&gt;
&lt;p&gt;We assume that for a set of i.i.d. subjects $i = 1, &amp;hellip;, n$ we observed a tuple $(X_i, T_i, Y_i)$ comprised of&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;a feature vector $X_i \in \mathbb R^n$&lt;/li&gt;
&lt;li&gt;a treatment variable $T_i \in \lbrace 0, 1 \rbrace$&lt;/li&gt;
&lt;li&gt;a response $Y_i \in \mathbb R$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Crucially, we do not assume unconfoundedness / strong ignorability hence&lt;/p&gt;
&lt;p&gt;$$
\big \lbrace Y_i^{(1)} , Y_i^{(0)} \big \rbrace \ \not \perp \ T_i \ | \ X_i
$$&lt;/p&gt;
&lt;h2 id=&#34;instrumental-variables&#34;&gt;Instrumental Variables&lt;/h2&gt;
&lt;p&gt;The &lt;strong&gt;standard linear IV model&lt;/strong&gt; is the following&lt;/p&gt;
&lt;p&gt;$$
Y_i = T_i \alpha + X_i \beta_1 + \varepsilon_i
\newline
T_i = Z_i \gamma + X_i \beta_2 + u_i
$$&lt;/p&gt;
&lt;p&gt;We assume there exists an &lt;strong&gt;instrumental variable&lt;/strong&gt; $Z_i \in \mathbb R^k$ that satisfies the following assumptions.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Assumption 1: Exclusion&lt;/strong&gt;: $\mathbb E [Z \varepsilon] = 0$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Assumption 2: Relevance&lt;/strong&gt;: $\mathbb E [Z T] \neq 0$&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The model can be represented by a DAG.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from src.plots import dag_iv
dag_iv()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/iv_6_0.svg&#34; alt=&#34;svg&#34;&gt;&lt;/p&gt;
&lt;p&gt;The IV estimator is instead unbiased&lt;/p&gt;
&lt;p&gt;$$
\hat \beta_{IV} = (Z&amp;rsquo;X)^{-1}(Z&amp;rsquo;Y)
$$&lt;/p&gt;
&lt;h3 id=&#34;potential-outcomes-perspective&#34;&gt;Potential Outcomes Perspective&lt;/h3&gt;
&lt;p&gt;We need to extend the potential outcomes framework in order to allow for the instrumental variable $Z$. First we define the potential outcomes as $Y^{(D(Z_i))}(Z_i)$&lt;/p&gt;
&lt;p&gt;The assumptions become&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Exclusion: $Y^{(D(Z_i))}(Z_i) = Y^{(T(Z_i))}$&lt;/li&gt;
&lt;li&gt;Relevance: $P(z) = \mathbb E [T, Z=z]$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We assume that $Z$ is fully randomly assigned (while $T$ is not).&lt;/p&gt;
&lt;p&gt;What does IV estimate?&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
\mathbb E[Y_i | Z_i = 1] - \mathbb E[Y_i | Z_i = 0] &amp;amp;= \Pr (D_i^{(1)} - D_i^{(0)} = 1) \times \mathbb E \Big[ Y_i^{(1)} - Y_i^{(0)} = 1 \ \Big | \ D_i^{(1)} - D_i^{(0)} = 1 \Big] -
\newline
&amp;amp;- \Pr (D_i^{(1)} - D_i^{(0)} = -1) \times \mathbb E \Big[ Y_i^{(1)} - Y_i^{(0)} = 1 \ \Big | \ D_i^{(1)} - D_i^{(0)} = -1 \Big]
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;Is this a quantity of interest? Almost. There are &lt;strong&gt;two issues&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;First, the first term is the treatment effect, but only for those individuals for whom $D_i^{(1)} - D_i^{(0)} = 1$, i.e. those that are induced into treatment by $Z_i$. These individuals are referred to as &lt;strong&gt;compliers&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Second, the second term is problematic since it removes from the first effect, the local effect of another subpopulation: $D_i^{(1)} - D_i^{(0)} = -1$, i.e. those that are induced out of treatment by $Z_i$. These individuals are referred to as &lt;strong&gt;defiers&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;We can get rid of defiers with a simple assumption.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Assumption 3: monotonocity&lt;/strong&gt;: $D_i^{(1)} \geq D_i^{(0)}$ (or viceversa)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;All effects must be monotone in the same direction&lt;/li&gt;
&lt;li&gt;Fundamentally untestable&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Then, the IV estimator can be expressed as a ration between two differences in means&lt;/p&gt;
&lt;p&gt;$$
\hat \beta_{IV} = \frac{\mathbb E[Y_i | Z_i = 1] - \mathbb E[Y_i | Z_i = 0]}{\mathbb E[T_i | Z_i = 1] - \mathbb E[T_i | Z_i = 0]}
$$&lt;/p&gt;
&lt;h3 id=&#34;structural-perspective&#34;&gt;Structural Perspective&lt;/h3&gt;
&lt;p&gt;One can interpret the IV estimator as a GMM estimator that uses the exclusion restriction as estimating equation.&lt;/p&gt;
&lt;p&gt;$$
\hat \beta_{GMM} = \arg \min_{\beta} \mathbb E \Big[ Z (Y - \alpha T - \beta X) \Big]^2
$$&lt;/p&gt;
&lt;h2 id=&#34;the-algebra-of-iv&#34;&gt;The Algebra of IV&lt;/h2&gt;
&lt;p&gt;TBD&lt;/p&gt;
&lt;h2 id=&#34;demand-and-supply&#34;&gt;Demand and Supply&lt;/h2&gt;
&lt;p&gt;TBD&lt;/p&gt;
&lt;h2 id=&#34;academic-application-1&#34;&gt;Academic Application 1&lt;/h2&gt;
&lt;p&gt;As an research paper replication, we are going to replicate &lt;a href=&#34;https://www.jstor.org/stable/2937954&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Does compulsory school attendance affect schooling and earnings?&lt;/a&gt; (1991) by Angrist and Krueger. The authors study the effect of education on wages.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;problem&lt;/strong&gt; of studying the relationship of education on wages is that there might be factors that influence both education and wages but we do not observe, for example ability. Students that have higher ability might decide to stay longer in school and also get higher wages afterwards.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;idea&lt;/strong&gt; of the authors is to use the quarter of birth as an instrument for education. In fact, quarter of birth is plausibly exogenous with respect to wages while, on the other hand, is correlated with education. Why? Students that are both in the last quarter of the year cannot drop out as early as other students and therefore are exposed to more eduction.&lt;/p&gt;
&lt;p&gt;We can represent the DAG of their model as follows.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;%matplotlib inline
%config InlineBackend.figure_format = &#39;retina&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from src.utils import *
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;dag_iv(Y=&amp;quot;wage&amp;quot;, T=&amp;quot;education&amp;quot;, Z=&amp;quot;quarter of birth&amp;quot;, U=&amp;quot;ability&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/iv_20_0.svg&#34; alt=&#34;svg&#34;&gt;&lt;/p&gt;
&lt;p&gt;A shortcoming of this instrument comes out of the fact that the population of &lt;strong&gt;compliers&lt;/strong&gt; is students that drop out of school as soon as possible, we will know the treatment effect only for this population. It&amp;rsquo;s important to keep this in mind when interpreting the results.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s load the data, freely available &lt;a href=&#34;https://economics.mit.edu/faculty/angrist/data1/data/angkru1991&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df = pd.read_csv(&#39;data/ak91.csv&#39;)
df.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;log_wage&lt;/th&gt;
      &lt;th&gt;years_of_schooling&lt;/th&gt;
      &lt;th&gt;date_of_birth&lt;/th&gt;
      &lt;th&gt;year_of_birth&lt;/th&gt;
      &lt;th&gt;quarter_of_birth&lt;/th&gt;
      &lt;th&gt;state_of_birth&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;5.790019&lt;/td&gt;
      &lt;td&gt;12.0&lt;/td&gt;
      &lt;td&gt;1930.0&lt;/td&gt;
      &lt;td&gt;1930.0&lt;/td&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;45.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;5.952494&lt;/td&gt;
      &lt;td&gt;11.0&lt;/td&gt;
      &lt;td&gt;1930.0&lt;/td&gt;
      &lt;td&gt;1930.0&lt;/td&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;45.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;5.315949&lt;/td&gt;
      &lt;td&gt;12.0&lt;/td&gt;
      &lt;td&gt;1930.0&lt;/td&gt;
      &lt;td&gt;1930.0&lt;/td&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;45.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;5.595926&lt;/td&gt;
      &lt;td&gt;12.0&lt;/td&gt;
      &lt;td&gt;1930.0&lt;/td&gt;
      &lt;td&gt;1930.0&lt;/td&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;45.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;6.068915&lt;/td&gt;
      &lt;td&gt;12.0&lt;/td&gt;
      &lt;td&gt;1930.0&lt;/td&gt;
      &lt;td&gt;1930.0&lt;/td&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;37.0&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;We have the variables of interest, &lt;code&gt;log_wage&lt;/code&gt;, &lt;code&gt;years_of_schooling&lt;/code&gt; and &lt;code&gt;quarter_of_birth&lt;/code&gt;, together with a set of controls.&lt;/p&gt;
&lt;h3 id=&#34;ols&#34;&gt;OLS&lt;/h3&gt;
&lt;p&gt;If we were to ignore the endogeneity problem we would estimate a linear regression of &lt;code&gt;log_wage&lt;/code&gt; on &lt;code&gt;years_of_schooling&lt;/code&gt;, plus control dummy variables for the &lt;code&gt;state_of_birth&lt;/code&gt; and &lt;code&gt;year_of_birth&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;smf.ols(&#39;log_wage ~ years_of_schooling&#39;, df).fit().summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
           &lt;td&gt;&lt;/td&gt;             &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt;          &lt;td&gt;    4.9952&lt;/td&gt; &lt;td&gt;    0.004&lt;/td&gt; &lt;td&gt; 1118.882&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    4.986&lt;/td&gt; &lt;td&gt;    5.004&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;years_of_schooling&lt;/th&gt; &lt;td&gt;    0.0709&lt;/td&gt; &lt;td&gt;    0.000&lt;/td&gt; &lt;td&gt;  209.243&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    0.070&lt;/td&gt; &lt;td&gt;    0.072&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;h3 id=&#34;iv&#34;&gt;IV&lt;/h3&gt;
&lt;p&gt;We now use &lt;code&gt;quarter_of_birth&lt;/code&gt; as an instrument for &lt;code&gt;years_of_schooling&lt;/code&gt;. We cannot check the exclusion restriction condition, but we can check the &lt;strong&gt;relevance&lt;/strong&gt; condition.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s start first by plotting average &lt;code&gt;years_of_schooling&lt;/code&gt; by date of birth.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;group_df = df.groupby(&amp;quot;date_of_birth&amp;quot;).mean().reset_index()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plt.figure(figsize=(15,6))
sns.lineplot(data=group_df, x=&amp;quot;date_of_birth&amp;quot;, y=&amp;quot;years_of_schooling&amp;quot;, zorder=1)\
.set(title=&amp;quot;First Stage&amp;quot;, xlabel=&amp;quot;Year of Birth&amp;quot;, ylabel=&amp;quot;Years of Schooling&amp;quot;);

for q in range(1, 5):
    x = group_df.loc[group_df[&#39;quarter_of_birth&#39;]==q, &amp;quot;date_of_birth&amp;quot;]
    y = group_df.loc[group_df[&#39;quarter_of_birth&#39;]==q, &amp;quot;years_of_schooling&amp;quot;]
    plt.scatter(x, y, marker=&amp;quot;s&amp;quot;, s=200, c=f&amp;quot;C{q}&amp;quot;)
    plt.scatter(x, y, marker=f&amp;quot;${q}$&amp;quot;, s=100, c=f&amp;quot;white&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/iv_31_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;As we can see, there is an upward trend but, within each year, people both in the last quarter usually have more years of schooling than people born in other quarters of the year.&lt;/p&gt;
&lt;p&gt;We can check this correlation more formally by regressing &lt;code&gt;years_of_schooling&lt;/code&gt; of a set of dummies for &lt;code&gt;quarter_of_birth&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;smf.ols(&#39;years_of_schooling ~ C(quarter_of_birth)&#39;, df).fit().summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
               &lt;td&gt;&lt;/td&gt;                 &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt;                  &lt;td&gt;   12.6881&lt;/td&gt; &lt;td&gt;    0.011&lt;/td&gt; &lt;td&gt; 1105.239&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;   12.666&lt;/td&gt; &lt;td&gt;   12.711&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;C(quarter_of_birth)[T.2.0]&lt;/th&gt; &lt;td&gt;    0.0566&lt;/td&gt; &lt;td&gt;    0.016&lt;/td&gt; &lt;td&gt;    3.473&lt;/td&gt; &lt;td&gt; 0.001&lt;/td&gt; &lt;td&gt;    0.025&lt;/td&gt; &lt;td&gt;    0.089&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;C(quarter_of_birth)[T.3.0]&lt;/th&gt; &lt;td&gt;    0.1173&lt;/td&gt; &lt;td&gt;    0.016&lt;/td&gt; &lt;td&gt;    7.338&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    0.086&lt;/td&gt; &lt;td&gt;    0.149&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;C(quarter_of_birth)[T.4.0]&lt;/th&gt; &lt;td&gt;    0.1514&lt;/td&gt; &lt;td&gt;    0.016&lt;/td&gt; &lt;td&gt;    9.300&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    0.119&lt;/td&gt; &lt;td&gt;    0.183&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;The relationship between &lt;code&gt;years_of_schooling&lt;/code&gt; and &lt;code&gt;quarter_of_birth&lt;/code&gt; is indeed statistically significant.&lt;/p&gt;
&lt;p&gt;Does it translate it into higher wages? We can have a first glimpse of potential IV effects by plotting wages against the date of birth.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plt.figure(figsize=(15,6))
sns.lineplot(data=group_df, x=&amp;quot;date_of_birth&amp;quot;, y=&amp;quot;log_wage&amp;quot;, zorder=1)\
.set(title=&amp;quot;Reduced Form&amp;quot;, xlabel=&amp;quot;Year of Birth&amp;quot;, ylabel=&amp;quot;Log Wage&amp;quot;);

for q in range(1, 5):
    x = group_df.loc[group_df[&#39;quarter_of_birth&#39;]==q, &amp;quot;date_of_birth&amp;quot;]
    y = group_df.loc[group_df[&#39;quarter_of_birth&#39;]==q, &amp;quot;log_wage&amp;quot;]
    plt.scatter(x, y, marker=&amp;quot;s&amp;quot;, s=200, c=f&amp;quot;C{q}&amp;quot;)
    plt.scatter(x, y, marker=f&amp;quot;${q}$&amp;quot;, s=100, c=f&amp;quot;white&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/iv_35_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;It seems that indeed people both in later quarters earn higher wages later in life.&lt;/p&gt;
&lt;p&gt;We now turn into the estimation of the causal effect of education on wages.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df[[&#39;q1&#39;, &#39;q2&#39;, &#39;q3&#39;, &#39;q4&#39;]] = pd.get_dummies(df[&#39;quarter_of_birth&#39;])
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from linearmodels.iv import IV2SLS

IV2SLS.from_formula(&#39;log_wage ~ 1 + [years_of_schooling ~ q1 + q2 + q3]&#39;, data=df).fit().summary.tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;caption&gt;Parameter Estimates&lt;/caption&gt;
&lt;tr&gt;
           &lt;td&gt;&lt;/td&gt;          &lt;th&gt;Parameter&lt;/th&gt; &lt;th&gt;Std. Err.&lt;/th&gt; &lt;th&gt;T-stat&lt;/th&gt; &lt;th&gt;P-value&lt;/th&gt; &lt;th&gt;Lower CI&lt;/th&gt; &lt;th&gt;Upper CI&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt;           &lt;td&gt;4.5898&lt;/td&gt;    &lt;td&gt;0.2494&lt;/td&gt;   &lt;td&gt;18.404&lt;/td&gt; &lt;td&gt;0.0000&lt;/td&gt;   &lt;td&gt;4.1010&lt;/td&gt;   &lt;td&gt;5.0786&lt;/td&gt; 
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;years_of_schooling&lt;/th&gt;  &lt;td&gt;0.1026&lt;/td&gt;    &lt;td&gt;0.0195&lt;/td&gt;   &lt;td&gt;5.2539&lt;/td&gt; &lt;td&gt;0.0000&lt;/td&gt;   &lt;td&gt;0.0643&lt;/td&gt;   &lt;td&gt;0.1409&lt;/td&gt; 
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;The coefficient is slightly higher than the OLS coefficient. It&amp;rsquo;s important to remember that the estimated effect is specific to the subpopulation of people that drop out of school as soon as they can.&lt;/p&gt;
&lt;h2 id=&#34;research-paper-replication-2&#34;&gt;Research Paper Replication 2&lt;/h2&gt;
&lt;p&gt;In &lt;a href=&#34;https://economics.mit.edu/files/4123&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The Colonial Origins of Comparative Development&lt;/a&gt; (2002) by Acemoglu, Johnson, Robinson, the authors wish to determine whether or not differences in institutions can help to explain observed economic outcomes.&lt;/p&gt;
&lt;p&gt;How do we measure &lt;em&gt;institutional differences&lt;/em&gt; and &lt;em&gt;economic outcomes&lt;/em&gt;?&lt;/p&gt;
&lt;p&gt;In this paper,&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;economic outcomes are proxied by log GDP per capita in 1995, adjusted for exchange rates.&lt;/li&gt;
&lt;li&gt;institutional differences are proxied by an index of protection against expropriation on average over 1985-95, constructed by the &lt;a href=&#34;https://www.prsgroup.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Political Risk Services Group&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The &lt;strong&gt;problem&lt;/strong&gt; is that there might exist other factors that affects both the quality of institutions and GDP. The authors suggest the following problems as sources of endogeneity:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;richer countries may be able to afford or prefer better institutions&lt;/li&gt;
&lt;li&gt;variables that affect income may also be correlated with institutional differences&lt;/li&gt;
&lt;li&gt;the construction of the index may be biased; analysts may be biased towards seeing countries with higher income having better institutions&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The &lt;strong&gt;idea&lt;/strong&gt; of the authors is to use settler&amp;rsquo;s mortality during the colonization period as an instrument for the quality of institutions. They hypothesize that higher mortality rates of colonizers led to the establishment of institutions that were more extractive in nature (less protection against expropriation), and these institutions still persist today.&lt;/p&gt;
&lt;p&gt;We can represent their DAG as follows.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;dag_iv(Y=&amp;quot;GDP&amp;quot;, T=&amp;quot;institutions&amp;quot;, Z=&amp;quot;settlers&#39; mortality&amp;quot;, U=&amp;quot;tons of stuff&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/iv_42_0.svg&#34; alt=&#34;svg&#34;&gt;&lt;/p&gt;
&lt;p&gt;First, let&amp;rsquo;s load the data (available &lt;a href=&#34;https://economics.mit.edu/faculty/acemoglu/data/ajr2001&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;) and have a look at it.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df = pd.read_csv(&#39;data/ajr02.csv&#39;,index_col=0)
df.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;GDP&lt;/th&gt;
      &lt;th&gt;Exprop&lt;/th&gt;
      &lt;th&gt;Mort&lt;/th&gt;
      &lt;th&gt;Latitude&lt;/th&gt;
      &lt;th&gt;Neo&lt;/th&gt;
      &lt;th&gt;Africa&lt;/th&gt;
      &lt;th&gt;Asia&lt;/th&gt;
      &lt;th&gt;Namer&lt;/th&gt;
      &lt;th&gt;Samer&lt;/th&gt;
      &lt;th&gt;logMort&lt;/th&gt;
      &lt;th&gt;Latitude2&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;8.39&lt;/td&gt;
      &lt;td&gt;6.50&lt;/td&gt;
      &lt;td&gt;78.20&lt;/td&gt;
      &lt;td&gt;0.3111&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;4.359270&lt;/td&gt;
      &lt;td&gt;0.096783&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;7.77&lt;/td&gt;
      &lt;td&gt;5.36&lt;/td&gt;
      &lt;td&gt;280.00&lt;/td&gt;
      &lt;td&gt;0.1367&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;5.634790&lt;/td&gt;
      &lt;td&gt;0.018687&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;9.13&lt;/td&gt;
      &lt;td&gt;6.39&lt;/td&gt;
      &lt;td&gt;68.90&lt;/td&gt;
      &lt;td&gt;0.3778&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;4.232656&lt;/td&gt;
      &lt;td&gt;0.142733&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;9.90&lt;/td&gt;
      &lt;td&gt;9.32&lt;/td&gt;
      &lt;td&gt;8.55&lt;/td&gt;
      &lt;td&gt;0.3000&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;2.145931&lt;/td&gt;
      &lt;td&gt;0.090000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;5&lt;/th&gt;
      &lt;td&gt;9.29&lt;/td&gt;
      &lt;td&gt;7.50&lt;/td&gt;
      &lt;td&gt;85.00&lt;/td&gt;
      &lt;td&gt;0.2683&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;4.442651&lt;/td&gt;
      &lt;td&gt;0.071985&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;The data contains the main variables, &lt;code&gt;DGP&lt;/code&gt;, &lt;code&gt;Exprop&lt;/code&gt; and &lt;code&gt;Mort&lt;/code&gt;, plus some geographical information.&lt;/p&gt;
&lt;h3 id=&#34;ols-1&#34;&gt;OLS&lt;/h3&gt;
&lt;p&gt;What would we get if we were to ignore the endogeneity problem? We estimate the following misspecified model by OLS&lt;/p&gt;
&lt;p&gt;$$
{GDP}_i = \beta_0 + \beta_1 {Exprop}_i + \varepsilon_i
$$&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;reg1 = smf.ols(&#39;GDP ~ Exprop&#39;, df).fit()
reg1.summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
      &lt;td&gt;&lt;/td&gt;         &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt; &lt;td&gt;    4.6609&lt;/td&gt; &lt;td&gt;    0.409&lt;/td&gt; &lt;td&gt;   11.402&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    3.844&lt;/td&gt; &lt;td&gt;    5.478&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Exprop&lt;/th&gt;    &lt;td&gt;    0.5220&lt;/td&gt; &lt;td&gt;    0.061&lt;/td&gt; &lt;td&gt;    8.527&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    0.400&lt;/td&gt; &lt;td&gt;    0.644&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;The coefficient of &lt;code&gt;Exprop&lt;/code&gt; is positive and significant but we know it is a biased estimate of the causal effect.&lt;/p&gt;
&lt;p&gt;One direction we could take in addressing the endogeneity problem could be to control for any factor that affects both &lt;code&gt;GDP&lt;/code&gt; and &lt;code&gt;Exprop&lt;/code&gt;. In particular, the authors consider the following sets of variables:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;climat; proxied by latitude&lt;/li&gt;
&lt;li&gt;differences that affect both economic performance and institutions, eg. cultural, historical, etc.; controlled for with the use of continent dummies&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;reg2 = smf.ols(&#39;GDP ~ Exprop + Latitude + Latitude2&#39;, df).fit()
reg3 = smf.ols(&#39;GDP ~ Exprop + Latitude + Latitude2 + Asia + Africa + Namer + Samer&#39;, df).fit()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from statsmodels.iolib.summary2 import summary_col

summary_col(results=[reg1,reg2,reg3],
            float_format=&#39;%0.2f&#39;,
            stars = True,
            info_dict={&#39;No. observations&#39; : lambda x: f&amp;quot;{int(x.nobs):d}&amp;quot;},
            regressor_order=[&#39;Intercept&#39;,&#39;Exprop&#39;,&#39;Latitude&#39;,&#39;Latitude2&#39;])
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
          &lt;td&gt;&lt;/td&gt;          &lt;th&gt;GDP I&lt;/th&gt;  &lt;th&gt;GDP II&lt;/th&gt;  &lt;th&gt;GDP III&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt;        &lt;td&gt;4.66***&lt;/td&gt; &lt;td&gt;4.55***&lt;/td&gt; &lt;td&gt;5.95***&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;&lt;/th&gt;                 &lt;td&gt;(0.41)&lt;/td&gt;  &lt;td&gt;(0.45)&lt;/td&gt;  &lt;td&gt;(0.68)&lt;/td&gt; 
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Exprop&lt;/th&gt;           &lt;td&gt;0.52***&lt;/td&gt; &lt;td&gt;0.49***&lt;/td&gt; &lt;td&gt;0.40***&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;&lt;/th&gt;                 &lt;td&gt;(0.06)&lt;/td&gt;  &lt;td&gt;(0.07)&lt;/td&gt;  &lt;td&gt;(0.06)&lt;/td&gt; 
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Latitude&lt;/th&gt;            &lt;td&gt;&lt;/td&gt;      &lt;td&gt;2.16&lt;/td&gt;    &lt;td&gt;0.42&lt;/td&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;&lt;/th&gt;                    &lt;td&gt;&lt;/td&gt;     &lt;td&gt;(1.68)&lt;/td&gt;  &lt;td&gt;(1.47)&lt;/td&gt; 
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Latitude2&lt;/th&gt;           &lt;td&gt;&lt;/td&gt;      &lt;td&gt;-2.12&lt;/td&gt;   &lt;td&gt;0.44&lt;/td&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;&lt;/th&gt;                    &lt;td&gt;&lt;/td&gt;     &lt;td&gt;(2.86)&lt;/td&gt;  &lt;td&gt;(2.48)&lt;/td&gt; 
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Africa&lt;/th&gt;              &lt;td&gt;&lt;/td&gt;        &lt;td&gt;&lt;/td&gt;     &lt;td&gt;-1.06**&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;&lt;/th&gt;                    &lt;td&gt;&lt;/td&gt;        &lt;td&gt;&lt;/td&gt;     &lt;td&gt;(0.41)&lt;/td&gt; 
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Asia&lt;/th&gt;                &lt;td&gt;&lt;/td&gt;        &lt;td&gt;&lt;/td&gt;     &lt;td&gt;-0.74*&lt;/td&gt; 
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;&lt;/th&gt;                    &lt;td&gt;&lt;/td&gt;        &lt;td&gt;&lt;/td&gt;     &lt;td&gt;(0.42)&lt;/td&gt; 
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Namer&lt;/th&gt;               &lt;td&gt;&lt;/td&gt;        &lt;td&gt;&lt;/td&gt;      &lt;td&gt;-0.17&lt;/td&gt; 
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;&lt;/th&gt;                    &lt;td&gt;&lt;/td&gt;        &lt;td&gt;&lt;/td&gt;     &lt;td&gt;(0.40)&lt;/td&gt; 
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Samer&lt;/th&gt;               &lt;td&gt;&lt;/td&gt;        &lt;td&gt;&lt;/td&gt;      &lt;td&gt;-0.12&lt;/td&gt; 
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;&lt;/th&gt;                    &lt;td&gt;&lt;/td&gt;        &lt;td&gt;&lt;/td&gt;     &lt;td&gt;(0.42)&lt;/td&gt; 
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;R-squared&lt;/th&gt;         &lt;td&gt;0.54&lt;/td&gt;    &lt;td&gt;0.56&lt;/td&gt;    &lt;td&gt;0.71&lt;/td&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;R-squared Adj.&lt;/th&gt;    &lt;td&gt;0.53&lt;/td&gt;    &lt;td&gt;0.54&lt;/td&gt;    &lt;td&gt;0.67&lt;/td&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;No. observations&lt;/th&gt;   &lt;td&gt;64&lt;/td&gt;      &lt;td&gt;64&lt;/td&gt;      &lt;td&gt;64&lt;/td&gt;   
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;The coefficient of &lt;code&gt;Expropr&lt;/code&gt; decreases in magnitude but remains positive and significant after the addition of geographical control variables. This might suggest that the endogeneity problem is not very pronounced. However, it&amp;rsquo;s hard to say given the large number of factors that could affect both institutions and GDP.&lt;/p&gt;
&lt;h3 id=&#34;iv-1&#34;&gt;IV&lt;/h3&gt;
&lt;p&gt;In order for &lt;code&gt;Mort&lt;/code&gt; to be a valid instrument it needs to satisfy the two IV conditions:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Exclusion&lt;/strong&gt;: &lt;code&gt;Mort&lt;/code&gt; must be correlated to &lt;code&gt;GDP&lt;/code&gt; only through &lt;code&gt;Exprop&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Relevance&lt;/strong&gt;: &lt;code&gt;Mort&lt;/code&gt; must be correlated with &lt;code&gt;Exprop&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The &lt;strong&gt;exclusion restriction&lt;/strong&gt; condition is untestable, however, we may not be satisfied if settler mortality rates in the 17th to 19th centuries have a direct effect on current GDP (in addition to their indirect effect through institutions).&lt;/p&gt;
&lt;p&gt;For example, settler mortality rates may be related to the current disease environment in a country, which could affect current economic performance.&lt;/p&gt;
&lt;p&gt;The authors argue this is unlikely because:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The majority of settler deaths were due to malaria and yellow fever and had a limited effect on local people.&lt;/li&gt;
&lt;li&gt;The disease burden on local people in Africa or India, for example, did not appear to be higher than average, supported by relatively high population densities in these areas before colonization.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The &lt;strong&gt;relevance&lt;/strong&gt; condition is testable and we can check it by computing the partial correlation between &lt;code&gt;Mort&lt;/code&gt; and &lt;code&gt;Exprop&lt;/code&gt;. Let&amp;rsquo;s start by visual inspection first.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sns.scatterplot(data=df, x=&#39;Mort&#39;, y=&#39;Exprop&#39;)\
.set(title=&#39;First Stage&#39;,
    xlabel=&#39;Settler mortality&#39;,
    ylabel=&#39;Risk of expropriation&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/iv_57_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Visually, the first stage seems weak, at best. However, a regression of &lt;code&gt;Exprop&lt;/code&gt; on &lt;code&gt;Mort&lt;/code&gt; can help us better assess whether the relationship is significant or not.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;smf.ols(&#39;Exprop ~ Mort&#39;, df).fit().summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
      &lt;td&gt;&lt;/td&gt;         &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt; &lt;td&gt;    6.7094&lt;/td&gt; &lt;td&gt;    0.202&lt;/td&gt; &lt;td&gt;   33.184&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    6.305&lt;/td&gt; &lt;td&gt;    7.114&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Mort&lt;/th&gt;      &lt;td&gt;   -0.0008&lt;/td&gt; &lt;td&gt;    0.000&lt;/td&gt; &lt;td&gt;   -2.059&lt;/td&gt; &lt;td&gt; 0.044&lt;/td&gt; &lt;td&gt;   -0.002&lt;/td&gt; &lt;td&gt;-2.28e-05&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;The coefficient is negative, as expected, and statistically significant.&lt;/p&gt;
&lt;p&gt;The second-stage regression results give us an unbiased and consistent estimate of the effect of institutions on economic outcomes.&lt;/p&gt;
&lt;p&gt;$$
{GDP}_i = \beta_0 + \beta_1 {Exprop}_i + \varepsilon_i \
{Exprop}_i = \delta_0 + \delta_1 {logMort}_i + v_i
$$&lt;/p&gt;
&lt;p&gt;Note that while our parameter estimates are correct, our standard errors
are not and for this reason, computing 2SLS âmanuallyâ (in stages with
OLS) is not recommended.&lt;/p&gt;
&lt;p&gt;We can correctly estimate a 2SLS regression in one step using the
&lt;a href=&#34;https://github.com/bashtage/linearmodels&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;linearmodels&lt;/a&gt; package, an extension of &lt;code&gt;statsmodels&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Note that when using &lt;code&gt;IV2SLS&lt;/code&gt;, the exogenous and instrument variables
are split up in the function arguments (whereas before the instrument
included exogenous variables)&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;IV2SLS.from_formula(&#39;GDP ~ 1 + [Exprop ~ logMort]&#39;, data=df).fit().summary.tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;caption&gt;Parameter Estimates&lt;/caption&gt;
&lt;tr&gt;
      &lt;td&gt;&lt;/td&gt;      &lt;th&gt;Parameter&lt;/th&gt; &lt;th&gt;Std. Err.&lt;/th&gt; &lt;th&gt;T-stat&lt;/th&gt; &lt;th&gt;P-value&lt;/th&gt; &lt;th&gt;Lower CI&lt;/th&gt; &lt;th&gt;Upper CI&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt;  &lt;td&gt;2.0448&lt;/td&gt;    &lt;td&gt;1.1273&lt;/td&gt;   &lt;td&gt;1.8139&lt;/td&gt; &lt;td&gt;0.0697&lt;/td&gt;   &lt;td&gt;-0.1647&lt;/td&gt;  &lt;td&gt;4.2542&lt;/td&gt; 
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Exprop&lt;/th&gt;     &lt;td&gt;0.9235&lt;/td&gt;    &lt;td&gt;0.1691&lt;/td&gt;   &lt;td&gt;5.4599&lt;/td&gt; &lt;td&gt;0.0000&lt;/td&gt;   &lt;td&gt;0.5920&lt;/td&gt;   &lt;td&gt;1.2550&lt;/td&gt; 
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;The result suggests a stronger positive relationship than what the OLS results indicated.&lt;/p&gt;
&lt;h2 id=&#34;business-case&#34;&gt;Business Case&lt;/h2&gt;
&lt;p&gt;We are given the following problem:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;A firm would like to understand whether its newsletter is working to increase revenue. However, it cannot force customers to subscribe to the newsletter. Instead, the firm sends a reminder email to a random sample of customers for the newsletter. Estimate the effect of the newsletter on revenue.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;We start by drawing a sample from the data generating process.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from src.dgp import dgp_newsletter

dgp = dgp_newsletter()
df = dgp.generate_data()
df.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;reminder&lt;/th&gt;
      &lt;th&gt;subscribe&lt;/th&gt;
      &lt;th&gt;revenue&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0.582809&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;3.427162&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1.953731&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;2.902038&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0.826724&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;From the data, we know the &lt;code&gt;revenue&lt;/code&gt; per customer, whether it was sent a &lt;code&gt;reminder&lt;/code&gt; for the newsletter and whether it actually decided to &lt;code&gt;subscribe&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;If we to estimate the effect of &lt;code&gt;subscribe&lt;/code&gt; on &lt;code&gt;revenue&lt;/code&gt;, we might get a biased estimate because the decision of subscribing is endogenous. For example, we can imagine that wealthier customers are generating more revenue but are also less likely to subscribe.&lt;/p&gt;
&lt;p&gt;We can represent the model with a DAG.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;dag_iv(Y=&amp;quot;revenue&amp;quot;, T=&amp;quot;subscribe&amp;quot;, Z=&amp;quot;reminder&amp;quot;, U=&amp;quot;income&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/iv_68_0.svg&#34; alt=&#34;svg&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;ols-2&#34;&gt;OLS&lt;/h3&gt;
&lt;p&gt;By directly inspecting the data, it seems that subscribed members actually generate less revenue than normal customers.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sns.barplot(x=&#39;subscribe&#39;, y=&#39;revenue&#39;, data=df);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/iv_71_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;A linear regression confirms the graphical intuition.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;smf.ols(&#39;revenue ~ 1 + subscribe&#39;, df).fit().summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
      &lt;td&gt;&lt;/td&gt;         &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt; &lt;td&gt;    1.7752&lt;/td&gt; &lt;td&gt;    0.086&lt;/td&gt; &lt;td&gt;   20.697&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    1.607&lt;/td&gt; &lt;td&gt;    1.943&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;subscribe&lt;/th&gt; &lt;td&gt;   -0.7441&lt;/td&gt; &lt;td&gt;    0.140&lt;/td&gt; &lt;td&gt;   -5.334&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;   -1.018&lt;/td&gt; &lt;td&gt;   -0.470&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;However, if indeed wealthier customers generate more revenue and are less likely to subscribe, we have a negative omitted variable bias and we can expect the true effect of the newsletter to be bigger than the OLS estimate.&lt;/p&gt;
&lt;h3 id=&#34;iv-2&#34;&gt;IV&lt;/h3&gt;
&lt;p&gt;Let&amp;rsquo;s now exploit the random variation induced by the discount. In order for our instrument to be valid, we need it to be exogenous (untestable) and relevant. We can test the relevance with the &lt;strong&gt;first stage&lt;/strong&gt; regresssion of &lt;code&gt;reminder&lt;/code&gt; on &lt;code&gt;subscribe&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;smf.ols(&#39;subscribe ~ 1 + reminder&#39;, df).fit().summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
      &lt;td&gt;&lt;/td&gt;         &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt; &lt;td&gt;    0.2368&lt;/td&gt; &lt;td&gt;    0.021&lt;/td&gt; &lt;td&gt;   11.324&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    0.196&lt;/td&gt; &lt;td&gt;    0.278&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;reminder&lt;/th&gt;  &lt;td&gt;    0.2790&lt;/td&gt; &lt;td&gt;    0.029&lt;/td&gt; &lt;td&gt;    9.488&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    0.221&lt;/td&gt; &lt;td&gt;    0.337&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;It seems that the instrument is relevant. We can now estimate the IV regression.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;IV2SLS.from_formula(&#39;revenue ~ 1 + [subscribe ~ reminder]&#39;, data=df).fit().summary.tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;caption&gt;Parameter Estimates&lt;/caption&gt;
&lt;tr&gt;
      &lt;td&gt;&lt;/td&gt;      &lt;th&gt;Parameter&lt;/th&gt; &lt;th&gt;Std. Err.&lt;/th&gt; &lt;th&gt;T-stat&lt;/th&gt; &lt;th&gt;P-value&lt;/th&gt; &lt;th&gt;Lower CI&lt;/th&gt; &lt;th&gt;Upper CI&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt;  &lt;td&gt;0.9485&lt;/td&gt;    &lt;td&gt;0.2147&lt;/td&gt;   &lt;td&gt;4.4184&lt;/td&gt; &lt;td&gt;0.0000&lt;/td&gt;   &lt;td&gt;0.5278&lt;/td&gt;   &lt;td&gt;1.3693&lt;/td&gt; 
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;subscribe&lt;/th&gt;  &lt;td&gt;1.4428&lt;/td&gt;    &lt;td&gt;0.5406&lt;/td&gt;   &lt;td&gt;2.6689&lt;/td&gt; &lt;td&gt;0.0076&lt;/td&gt;   &lt;td&gt;0.3832&lt;/td&gt;   &lt;td&gt;2.5023&lt;/td&gt; 
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;The estimated coefficient has now flipped sign and turned positive! Ignoring the endogeneity problem would have lead us to the wrong conclusion.&lt;/p&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=LEAx0He_KBI&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Instrumental Variables&lt;/a&gt; video lecture by Paul Goldsmith-Pinkham (Yale)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://matheusfacure.github.io/python-causality-handbook/08-Instrumental-Variables.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Instrumental Variables&lt;/a&gt; section from Matheus Facure&amp;rsquo;s &lt;a href=&#34;https://matheusfacure.github.io/python-causality-handbook/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Causal Inference for The Brave and The True&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.jstor.org/stable/2937954&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Does compulsory school attendance affect schooling and earnings?&lt;/a&gt; (1991) by Angrist and Krueger&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://economics.mit.edu/files/4123&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The Colonial Origins of Comparative Development&lt;/a&gt; (2002) by Acemoglu, Johnson, Robinson&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Meta Learners</title>
      <link>https://matteocourthoud.github.io/post/meta_learners/</link>
      <pubDate>Fri, 15 Apr 2022 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/post/meta_learners/</guid>
      <description>&lt;p&gt;In this tutorial, we are going to explore and compare different methods that leverage machine learning to estimate heterogeneous treatment effects.&lt;/p&gt;
&lt;p&gt;For this tutorial, I assume you are familiar with the following concepts:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Rubin&amp;rsquo;s potential outcome framework&lt;/li&gt;
&lt;li&gt;Propensity score weighting&lt;/li&gt;
&lt;li&gt;Basic machine learning models&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;setting&#34;&gt;Setting&lt;/h2&gt;
&lt;p&gt;We assume that for a set of i.i.d. subjects $i = 1, &amp;hellip;, n$ we observed a tuple $(X_i, D_i, Y_i)$ comprised of&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;a feature vector $X_i \in \mathbb R^n$&lt;/li&gt;
&lt;li&gt;a treatment assignment $T_i \in \lbrace 0, 1 \rbrace$&lt;/li&gt;
&lt;li&gt;a response $Y_i \in \mathbb R$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Assumption 1 : unconfoundedness&lt;/strong&gt; (or ignorability, or selection on observables)&lt;/p&gt;
&lt;p&gt;$$
\big \lbrace Y_i^{(1)} , Y_i^{(0)} \big \rbrace \ \perp \ T_i \ | \ X_i
$$&lt;/p&gt;
&lt;p&gt;i.e. conditional on observable characteristics $X$, the treatment assignment $T$ is as good as random.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Assumption 2: overlap&lt;/strong&gt; (or bounded support)&lt;/p&gt;
&lt;p&gt;$$
\exists \eta &amp;gt; 0 \ : \ \eta \leq \mathbb E \left[ T_i = 1 \ \big | \ X_i = x \right] \leq 1-\eta
$$&lt;/p&gt;
&lt;p&gt;i.e. no observation is deterministically assigned to the treatment or control group.&lt;/p&gt;
&lt;h2 id=&#34;meta-learners&#34;&gt;Meta Learners&lt;/h2&gt;
&lt;h3 id=&#34;s-learner&#34;&gt;S-Learner&lt;/h3&gt;
&lt;p&gt;The simplest meta-algorithm is the single learner or &lt;strong&gt;S-learner&lt;/strong&gt;. To build the S-learner estimator, we fit a single model for all observations.&lt;/p&gt;
&lt;p&gt;$$
\mu(z) = \mathbb E \left[ Y_i \ \big | \ (X_i, T_i) = z \right]
$$&lt;/p&gt;
&lt;p&gt;the estimator is given by the difference between the predicted values evaluated at $t=1$ and $t=0$.&lt;/p&gt;
&lt;p&gt;$$
\hat \tau_{S} (x) = \hat \mu(x,1) - \hat \mu(x,0)
$$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Problems&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;We are learning a single model so we hope that the model uncovers heterogeneity in $T$ but it might not be the case&lt;/li&gt;
&lt;li&gt;If the model is heavily regularized because of the high dimensionality of $X$, it might not recover any treatment effect
&lt;ul&gt;
&lt;li&gt;e.g. with trees, it might not split on $T$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;t-learner&#34;&gt;T-learner&lt;/h3&gt;
&lt;p&gt;To build the two-learner or &lt;strong&gt;T-learner&lt;/strong&gt; estimator, we fit two different models, one for treated units and one for control units.&lt;/p&gt;
&lt;p&gt;$$
\mu^{(1)}(x) = \mathbb E \left[ Y_i \ \big | \ X_i = x, T_i = 1 \right] \qquad ; \qquad \mu^{(0)}(x) = \mathbb E \left[ Y_i \ \big | \ X_i = x, T_i = 0 \right]
$$&lt;/p&gt;
&lt;p&gt;the estimator is given by the difference between the predicted values of the two algorithms.&lt;/p&gt;
&lt;p&gt;$$
\hat \tau_{T} (x) = \hat \mu^{(1)}(x) - \hat \mu^{(0)}(x)
$$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Problems&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;We are using just a fraction of the data for each prediction problem
&lt;ul&gt;
&lt;li&gt;S-learner was using all the data&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;We might get heterogeneity where there is none, just because we are forcing different models
&lt;ul&gt;
&lt;li&gt;E.g. if trees split differently, to compute the two potential outcomes we use different populations&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;x-learner&#34;&gt;X-learner&lt;/h3&gt;
&lt;p&gt;The cross-learner or &lt;strong&gt;X-learner&lt;/strong&gt; estimator is an extension of the T-learner estimator. It is built in the following way:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;As for the T-learner, compute separate models for $\mu^{(1)}(x)$ and $\mu^{(0)}(x)$ using the treated and control units, respectively&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Compute the estimated treatment effects as&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;$$
\Delta_i (x) =
\begin{cases}
Y_i - \hat \mu^{(0)}(x) &amp;amp;\quad \text{ if } T_i = 1
\newline
\hat \mu^{(1)}(x) - Y_i &amp;amp;\quad \text{ if } T_i = 0
\end{cases}
$$&lt;/p&gt;
&lt;ol start=&#34;3&#34;&gt;
&lt;li&gt;
&lt;p&gt;Predicting $\Delta$ from $X$, compute $\hat \tau^{(0)}(x)$ from treated units and  $\hat \tau^{(1)}(x)$ from control units&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Estimate $e(x) = \mathbb E \left[ T_i = 1 \ \big | \ X_i = x \right]$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Compute&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;$$
\hat \tau_X(x) = \hat \tau^{(0)}(x) \hat e(x) + \hat \tau^{(1)}(x) (1 - \hat e(x))
$$&lt;/p&gt;
&lt;h2 id=&#34;example&#34;&gt;Example&lt;/h2&gt;
&lt;p&gt;In this example, we are going to use the following &lt;strong&gt;data generating process&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$N = 4000$&lt;/li&gt;
&lt;li&gt;$p = 10$&lt;/li&gt;
&lt;li&gt;$X_i \sim N(0, I_p)$&lt;/li&gt;
&lt;li&gt;$e(x) = 0.3$&lt;/li&gt;
&lt;li&gt;$\varepsilon_i \sim N(0, 1)$&lt;/li&gt;
&lt;li&gt;$\mu^{(0)}(x) = (x_1 + x_2)_{+} + \varepsilon$&lt;/li&gt;
&lt;li&gt;$\mu^{(1)}(x) = (x_1 + x_2)_{+} + \frac{1}{1 + e^{-x_3}} + \varepsilon$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;So that the propensity score is constant $e(x) = 0.3$, the treatment effect is $\frac{1}{1 + e^{-x_3}}$ and the average treatment effect is $0.5$.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;%matplotlib inline
%config InlineBackend.figure_format = &#39;retina&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from src.utils import *
from src.dgp import dgp4
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We generate a dataset out of our DGP.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;dgp = dgp4()
df = dgp.generate_data()
df.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;x1&lt;/th&gt;
      &lt;th&gt;x2&lt;/th&gt;
      &lt;th&gt;x3&lt;/th&gt;
      &lt;th&gt;x4&lt;/th&gt;
      &lt;th&gt;x5&lt;/th&gt;
      &lt;th&gt;x6&lt;/th&gt;
      &lt;th&gt;x7&lt;/th&gt;
      &lt;th&gt;x8&lt;/th&gt;
      &lt;th&gt;x9&lt;/th&gt;
      &lt;th&gt;x10&lt;/th&gt;
      &lt;th&gt;e&lt;/th&gt;
      &lt;th&gt;T&lt;/th&gt;
      &lt;th&gt;tau&lt;/th&gt;
      &lt;th&gt;Y&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;1.624345&lt;/td&gt;
      &lt;td&gt;-0.611756&lt;/td&gt;
      &lt;td&gt;-0.528172&lt;/td&gt;
      &lt;td&gt;-1.072969&lt;/td&gt;
      &lt;td&gt;0.865408&lt;/td&gt;
      &lt;td&gt;-2.301539&lt;/td&gt;
      &lt;td&gt;1.744812&lt;/td&gt;
      &lt;td&gt;-0.761207&lt;/td&gt;
      &lt;td&gt;0.319039&lt;/td&gt;
      &lt;td&gt;-0.249370&lt;/td&gt;
      &lt;td&gt;0.3&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0.370943&lt;/td&gt;
      &lt;td&gt;1.116502&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;1.462108&lt;/td&gt;
      &lt;td&gt;-2.060141&lt;/td&gt;
      &lt;td&gt;-0.322417&lt;/td&gt;
      &lt;td&gt;-0.384054&lt;/td&gt;
      &lt;td&gt;1.133769&lt;/td&gt;
      &lt;td&gt;-1.099891&lt;/td&gt;
      &lt;td&gt;-0.172428&lt;/td&gt;
      &lt;td&gt;-0.877858&lt;/td&gt;
      &lt;td&gt;0.042214&lt;/td&gt;
      &lt;td&gt;0.582815&lt;/td&gt;
      &lt;td&gt;0.3&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0.420087&lt;/td&gt;
      &lt;td&gt;-0.248671&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;-1.100619&lt;/td&gt;
      &lt;td&gt;1.144724&lt;/td&gt;
      &lt;td&gt;0.901591&lt;/td&gt;
      &lt;td&gt;0.502494&lt;/td&gt;
      &lt;td&gt;0.900856&lt;/td&gt;
      &lt;td&gt;-0.683728&lt;/td&gt;
      &lt;td&gt;-0.122890&lt;/td&gt;
      &lt;td&gt;-0.935769&lt;/td&gt;
      &lt;td&gt;-0.267888&lt;/td&gt;
      &lt;td&gt;0.530355&lt;/td&gt;
      &lt;td&gt;0.3&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0.711276&lt;/td&gt;
      &lt;td&gt;0.651441&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;-0.691661&lt;/td&gt;
      &lt;td&gt;-0.396754&lt;/td&gt;
      &lt;td&gt;-0.687173&lt;/td&gt;
      &lt;td&gt;-0.845206&lt;/td&gt;
      &lt;td&gt;-0.671246&lt;/td&gt;
      &lt;td&gt;-0.012665&lt;/td&gt;
      &lt;td&gt;-1.117310&lt;/td&gt;
      &lt;td&gt;0.234416&lt;/td&gt;
      &lt;td&gt;1.659802&lt;/td&gt;
      &lt;td&gt;0.742044&lt;/td&gt;
      &lt;td&gt;0.3&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0.334662&lt;/td&gt;
      &lt;td&gt;-0.913644&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;-0.191836&lt;/td&gt;
      &lt;td&gt;-0.887629&lt;/td&gt;
      &lt;td&gt;-0.747158&lt;/td&gt;
      &lt;td&gt;1.692455&lt;/td&gt;
      &lt;td&gt;0.050808&lt;/td&gt;
      &lt;td&gt;-0.636996&lt;/td&gt;
      &lt;td&gt;0.190915&lt;/td&gt;
      &lt;td&gt;2.100255&lt;/td&gt;
      &lt;td&gt;0.120159&lt;/td&gt;
      &lt;td&gt;0.617203&lt;/td&gt;
      &lt;td&gt;0.3&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0.321441&lt;/td&gt;
      &lt;td&gt;0.121779&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;First, we implement the simplest machine learning method for learning heterogeneous treatment effects: the &lt;strong&gt;S-learner&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;We fit a single &lt;code&gt;RandomForestRegressor&lt;/code&gt; method to all the data.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from sklearn.ensemble import RandomForestRegressor as rfr

mu_S = rfr(min_samples_leaf=30)
mu_S.fit(df[dgp.X + [&#39;T&#39;]], df[&#39;Y&#39;]);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then, we use it to predict $\mu^{(1)}(x)$ and $\mu^{(0)}(x)$.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df[&#39;hat_mu0_S&#39;] = mu_S.predict(df[dgp.X + [&#39;T&#39;]].assign(T=0))
df[&#39;hat_mu1_S&#39;] = mu_S.predict(df[dgp.X + [&#39;T&#39;]].assign(T=1))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We estimate the average treatment effect as the difference between the two predictions.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df[&#39;hat_tau_S&#39;] = df[&#39;hat_mu1_S&#39;] - df[&#39;hat_mu0_S&#39;]
print(f&amp;quot;S-learner estimate : {np.mean(df[&#39;hat_tau_S&#39;]):.4}&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;S-learner estimate : 0.3657
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;How close are we to the true treatment effect?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sns.scatterplot(data=df, x=&#39;x3&#39;, y=&#39;hat_tau_S&#39;, alpha=0.3);
sns.scatterplot(data=df, x=&#39;x3&#39;, y=&#39;tau&#39;, color=&#39;C2&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/meta_learners_24_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;T-learner&lt;/strong&gt; method instead fits different model for treated and control units. The advantage is that it can&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;mu0_T = rfr(min_samples_leaf=30)
mu0_T.fit(df.loc[df[&#39;T&#39;]==0, dgp.X + [&#39;T&#39;]], df.loc[df[&#39;T&#39;]==0, &#39;Y&#39;])
mu1_T = rfr(min_samples_leaf=30)
mu1_T.fit(df.loc[df[&#39;T&#39;]==1, dgp.X + [&#39;T&#39;]], df.loc[df[&#39;T&#39;]==1, &#39;Y&#39;]);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then, we use it to predict $\mu^{(1)}(x)$ and $\mu^{(0)}(x)$.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df[&#39;hat_mu0_T&#39;] = mu0_T.predict(df[dgp.X + [&#39;T&#39;]])
df[&#39;hat_mu1_T&#39;] = mu1_T.predict(df[dgp.X + [&#39;T&#39;]])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We estimate the average treatment effect as the difference between the two predictions.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df[&#39;hat_tau_T&#39;] = df[&#39;hat_mu1_T&#39;] - df[&#39;hat_mu0_T&#39;]
print(f&amp;quot;S-learner estimate : {np.mean(df[&#39;hat_tau_T&#39;]):.4}&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;S-learner estimate : 0.5231
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can plot the distribution of treatment effect estimates against the true values.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sns.scatterplot(data=df, x=&#39;x3&#39;, y=&#39;hat_tau_T&#39;, alpha=0.3);
sns.scatterplot(data=df, x=&#39;x3&#39;, y=&#39;tau&#39;, color=&#39;C2&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/meta_learners_32_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s now estimate the &lt;strong&gt;X-learner&lt;/strong&gt;. The first step is exactly the same as for the T-learner: estimate $\hat \mu^{(1)}(x)$ and $\hat \mu^{(0)}(x)$ using the treated and control group, respectively.&lt;/p&gt;
&lt;p&gt;Afterwards, we compute the estimated treatment effect on the treated using the the estimated counterfactual outcome estimated on the control group $\hat \mu^{(0)}(x)$, and viceversa.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df[&#39;Delta&#39;] = 0
df.loc[df[&#39;T&#39;]==0, &#39;Delta&#39;] = (df[&#39;hat_mu1_T&#39;] - df[&#39;Y&#39;])[df[&#39;T&#39;]==0]
df.loc[df[&#39;T&#39;]==1, &#39;Delta&#39;] = (df[&#39;Y&#39;] - df[&#39;hat_mu0_T&#39;])[df[&#39;T&#39;]==1]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then, we basically repeat the process for the T-learner, but using Delta as outcome variable.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;tau0_X = rfr(min_samples_leaf=30)
tau0_X.fit(df.loc[df[&#39;T&#39;]==0, dgp.X + [&#39;T&#39;]], df.loc[df[&#39;T&#39;]==0, &#39;Delta&#39;])
tau1_X = rfr(min_samples_leaf=30)
tau1_X.fit(df.loc[df[&#39;T&#39;]==1, dgp.X + [&#39;T&#39;]], df.loc[df[&#39;T&#39;]==1, &#39;Delta&#39;]);
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df[&#39;hat_tau0_X&#39;] = tau0_X.predict(df[dgp.X + [&#39;T&#39;]])
df[&#39;hat_tau1_X&#39;] = tau1_X.predict(df[dgp.X + [&#39;T&#39;]])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally, we estimate the propensity score.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from sklearn.linear_model import LogisticRegression as lr

df[&#39;hat_e&#39;] = lr().fit(df[dgp.X], df[&#39;T&#39;]).predict_proba(df[dgp.X])[:,1]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we can compute the &lt;strong&gt;X-learner estimate&lt;/strong&gt; as&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df[&#39;hat_tau_X&#39;] = df[&#39;hat_e&#39;] * df[&#39;hat_tau0_X&#39;] + (1-df[&#39;hat_e&#39;]) * df[&#39;hat_tau1_X&#39;]
print(f&amp;quot;X-learner estimate : {np.mean(df[&#39;hat_tau_X&#39;]):.4}&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;X-learner estimate : 0.5253
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can plot the distribution of treatment effect estimates against the true values.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;X_plot = sns.scatterplot(data=df, x=&#39;x3&#39;, y=&#39;hat_tau_X&#39;, alpha=0.3);
sns.scatterplot(data=df, x=&#39;x3&#39;, y=&#39;tau&#39;, color=&#39;C2&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/meta_learners_43_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;The X-learner estimator is heavily superior to both the S-learner and the T-learner. This is particularly evident if we combine all the plots.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;fig, axs = plt.subplots(1,3, sharex=True, sharey=True, figsize=(20,6))
for i, l in enumerate([&#39;S&#39;, &#39;T&#39;, &#39;X&#39;]):
    sns.scatterplot(data=df, x=&#39;x3&#39;, y=f&amp;quot;hat_tau_{l}&amp;quot;, alpha=0.3, ax=axs[i]);
    sns.scatterplot(data=df, x=&#39;x3&#39;, y=&#39;tau&#39;, color=&#39;C2&#39;, ax=axs[i]).\
    set(title=f&amp;quot;{l}-learner&amp;quot;, ylabel=&#39;&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/meta_learners_45_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Meta learners: &lt;a href=&#34;https://www.pnas.org/doi/abs/10.1073/pnas.1804597116&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Metalearners for estimating heterogeneous treatment effects using machine learning&lt;/a&gt; (2019) by KÃ¼nzel, Sekhon, Bickel, and Yu&lt;/li&gt;
&lt;li&gt;Taxonomy of methods: &lt;a href=&#34;https://www.pnas.org/doi/abs/10.1073/pnas.1510489113&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Recursive partitioning for heterogeneous causal effects&lt;/a&gt; (2016) by Athey and Imbens&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=N9ThAs7NS0g&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Video lecture&lt;/a&gt; by Stefan Wager (Stanford)&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Optimal Test Stopping</title>
      <link>https://matteocourthoud.github.io/post/optimal_stopping/</link>
      <pubDate>Fri, 15 Apr 2022 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/post/optimal_stopping/</guid>
      <description>&lt;h2 id=&#34;standard-setting&#34;&gt;Standard Setting&lt;/h2&gt;
&lt;p&gt;When we design an A/B test or, more generally, an experiment, the standard steps are the following&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Define a &lt;strong&gt;null hypothesis&lt;/strong&gt; $H_0$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;usually the null is a zero effect of the experiment on a metric of interest&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Define a &lt;strong&gt;significance level&lt;/strong&gt; $\alpha$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;usually equal to 0.05, it represents the maximum probability of rejecting the null hypothesis when it is true&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Define an &lt;strong&gt;alternative hypothesis&lt;/strong&gt; $H_1$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;usually the minimum effect that we would like to detect&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Define a &lt;strong&gt;power level&lt;/strong&gt; $\beta$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;usually equal to 0.8, it represents the minimum probability of rejecting the null hypothesis, when the alternative is true&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Pick a &lt;strong&gt;test statistic&lt;/strong&gt; whose distribution is known under both hypotheses&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;usually the sample average of a metric of interest&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Compute the minimum &lt;strong&gt;sample size&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;in order to achieve the desired power level, given all the test parameters&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Then, we &lt;strong&gt;run the test&lt;/strong&gt; and, depending on the realized value of the test statistic, we decide whether to &lt;strong&gt;reject&lt;/strong&gt; the null hypothesis or not, depending on whether the p-value is lower than the significance level.&lt;/p&gt;
&lt;p&gt;Rejecting the null hypothesis does not imply accepting the alternative hypothesis.&lt;/p&gt;
&lt;h2 id=&#34;peaking&#34;&gt;Peaking&lt;/h2&gt;
&lt;p&gt;Suppose that half-way through the experiment we were to &lt;strong&gt;peak at the data&lt;/strong&gt;, and notice that the p-value is lower than the significant level. Should we stop the experiment? If we do, what happens?&lt;/p&gt;
&lt;p&gt;The answer is that the test would not achieve the desired significance level or, in other terms, our confidence intervals would have the &lt;strong&gt;wrong coverage&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s see what I mean with a &lt;strong&gt;example&lt;/strong&gt;. Suppose our &lt;strong&gt;data generating process&lt;/strong&gt; is a standard normal distribution with zero mean and unit variance $X \sim N(0,1)$.&lt;/p&gt;
&lt;p&gt;Suppose that the variance is known while the mean is not. The &lt;strong&gt;hypothesis&lt;/strong&gt; that we wish to test is&lt;/p&gt;
&lt;p&gt;$$
\begin{align}
H_0: \quad &amp;amp; \mu = 0 ,
\newline
H_1: \quad &amp;amp; \mu = 0.1
\end{align}
$$&lt;/p&gt;
&lt;p&gt;After each observation $n$, we compute the &lt;strong&gt;test statistic&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;$$
t = \sqrt{n} \frac{\bar X - \mu}{\sigma}
$$&lt;/p&gt;
&lt;p&gt;where $\bar X$ is the sample mean from a sample $X_1, X_2, &amp;hellip;, X_n$, of size $n$, $\sigma$ is the standard deviation of the population, and $\mu$ is the population mean. Under the null hypothesis of zero mean, the test statistic is distributed as a standard normal.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;%matplotlib inline
%config InlineBackend.figure_format = &#39;retina&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from src.utils import *
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;tstat = lambda x: np.mean(x) * np.sqrt(len(x))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Suppose we want a test with significance level $\alpha=0.05$ and power $\beta=0.8$. What sample size do we need?&lt;/p&gt;
&lt;p&gt;$$
N : \quad 0 + z_{0.95} * \frac{\sigma}{\sqrt{N}} = 0.1 - z_{0.8} * \frac{\sigma}{\sqrt{N}}
$$&lt;/p&gt;
&lt;p&gt;so that&lt;/p&gt;
&lt;p&gt;$$
N = \left( \frac{z_{0.95} + z_{0.8}}{0.1 * \sigma} \right)^2
$$&lt;/p&gt;
&lt;p&gt;where $z_{p}$ is the CDF inverse (or percent point function) at $p$.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from scipy.stats import norm

N = ( (norm.ppf(0.95) + norm.ppf(0.8)) / 0.1 )**2
print(f&amp;quot;Sample size: {N}&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Sample size: 618.2557232019765
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We need at least $N=619$ observations.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def experiment(f_stat, mu=0, N=619, seed=1):
    np.random.seed(seed)
    n = np.arange(1, N+1)
    x = np.random.normal(mu, 1, N)
    stat = [f_stat(x[:i]) for i in n]
    df = pd.DataFrame({&#39;n&#39;: n, &#39;x&#39;: x, &#39;stat&#39;: stat})
    return df
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s have a look at what a sample looks like.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df = experiment(tstat)
df.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;n&lt;/th&gt;
      &lt;th&gt;x&lt;/th&gt;
      &lt;th&gt;stat&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1.624345&lt;/td&gt;
      &lt;td&gt;1.624345&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;-0.611756&lt;/td&gt;
      &lt;td&gt;0.716009&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;-0.528172&lt;/td&gt;
      &lt;td&gt;0.279678&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;-1.072969&lt;/td&gt;
      &lt;td&gt;-0.294276&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;0.865408&lt;/td&gt;
      &lt;td&gt;0.123814&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;We can now plot the time trend of the test statistic over the sampling process. I also mark the likes at $-1.96$ and $1.96$ for statistical significance.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def plot_experiment(df, ybounds, **kwargs):
    sns.lineplot(data=df, x=&#39;n&#39;, y=&#39;stat&#39;, **kwargs)
    for ybound in ybounds:
        sns.lineplot(x=df[&#39;n&#39;], y=ybound, lw=1, color=&#39;black&#39;)
    plt.title(&#39;T-statistic with sequential sampling&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plot_experiment(df, ybounds=[-1.96, 1.96])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/optimal_stopping_16_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;In this case, the test never achieves statistical significance. Therefore, peaking does not have an effect. We would not have stopped the experiment prematurely.&lt;/p&gt;
&lt;p&gt;What would happen if we were repeating the experiment many times? Let&amp;rsquo;s simulate this procedure $K=100$ times.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def simulate_experiments(f_stat, ybounds, early_stop=False, mu=0, K=100, **kwargs):
    stops = np.zeros(K)
    for k in range(K):
        # Draw data
        df = experiment(f_stat, mu=mu, seed=k)
        vals = df[&#39;stat&#39;].values[100:]
        
        # If early stop, plot violations
        if early_stop:
            violations = (vals &amp;gt; max(ybounds)) + (vals &amp;lt; min(ybounds))
        if early_stop and any(violations):
            end = 101+np.where(violations)[0][0]
            plot_experiment(df.iloc[100:end, :], ybounds, **kwargs)
            stops[k] = end * np.sign(df[&#39;stat&#39;].values[end])
        
        # Otherwise, check last value
        elif (vals[-1] &amp;gt; max(ybounds)) or (vals[-1] &amp;lt; min(ybounds)):
            plot_experiment(df.iloc[100:, :], ybounds, **kwargs)
            stops[k] = len(df) * np.sign(vals[-1])
        
        # Plot all other observations
        else: 
            plot_experiment(df[df[&#39;n&#39;]&amp;gt;100], ybounds, color=&#39;grey&#39;, alpha=0.1, lw=1)
    plt.title(f&amp;quot;{sum(stops!=0)} significant results ({sum(stops&amp;gt;0)/sum(stops!=0)*100:.4}% up)&amp;quot;);
    return stops
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;simulate_experiments(tstat, ybounds=[-1.96, 1.96], early_stop=False);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/optimal_stopping_19_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;In the figure above, I have highlighted the experiments that would be statistically significant &lt;strong&gt;without peaking&lt;/strong&gt;, i.e. given the value of the test statistic &lt;strong&gt;at the end of the sampling&lt;/strong&gt; process. Only 3 simulations are statistically significant. This means a coverage of 97% which is very close to the expected coverage of 95%.&lt;/p&gt;
&lt;p&gt;What if instead we were stopping any time we were seeing a significant result?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;simulate_experiments(tstat, ybounds=[-1.96, 1.96], early_stop=True, lw=1);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/optimal_stopping_21_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;In the figure above, I have highlighted the experiments that would be statistically significant &lt;strong&gt;with constant peaking&lt;/strong&gt; from the 100th observation onwards. 25 simulations are statistically significant. This means a coverage of 75% which is very far from the expected coverage of 95%. Peaking distorts coverage of the confidence interval.&lt;/p&gt;
&lt;p&gt;Potential solutions are:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;sequential probability ratio tests&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;sequential triangular testing&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;group sequential testing&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Before analyzing these sequential testing procedures, we first need to introduce the &lt;strong&gt;likelihood ratio test&lt;/strong&gt;.&lt;/p&gt;
&lt;h2 id=&#34;likelihood-ratio-test&#34;&gt;Likelihood Ratio Test&lt;/h2&gt;
&lt;p&gt;A statistical test is fully specified when the distribution of the data is known under both the null and the alternative hypotheses. For example&lt;/p&gt;
&lt;p&gt;$$
\begin{align}
H_0: \quad &amp;amp; \theta=\theta_0 ,
\newline
H_1: \quad &amp;amp; \theta=\theta_1 .
\end{align}
$$&lt;/p&gt;
&lt;p&gt;If instead the alternative was more generally $H_1: \ \theta \neq \theta_0$, as common in hypothesis testing, we wouldn&amp;rsquo;t know the distribution of the data under the alternative hypothesis.&lt;/p&gt;
&lt;p&gt;When a statistical test is fully specified, we can compute the likelihood ratio as the the ratio of the likelihood function under the two hypotheses.&lt;/p&gt;
&lt;p&gt;$$
\Lambda (X) = \frac{\mathcal L (\theta_0 \ | \ X)}{\mathcal L (\theta_1 \ | \ X)}
$$&lt;/p&gt;
&lt;p&gt;The likelihood-ratio test provides a decision rule as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;If $\Lambda &amp;gt;c$, do not reject $H_{0}$;&lt;/li&gt;
&lt;li&gt;If $\Lambda &amp;lt;c$, reject $H_{0}$;&lt;/li&gt;
&lt;li&gt;If $\Lambda =c$, reject with probability $q$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The values $c$ and $q$ are usually chosen to obtain a specified significance level $\alpha$.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;NeymanâPearson lemma&lt;/strong&gt; states that this likelihood-ratio test is the most powerful among all level $\alpha$ tests for this case.&lt;/p&gt;
&lt;h3 id=&#34;special-case-testing-mean-of-normal-distribution&#34;&gt;Special Case: testing mean of normal distribution&lt;/h3&gt;
&lt;p&gt;Let&amp;rsquo;s go back to our example where data is coming from a normal distribution with unknown mean $\mu$ and known variance $\sigma^2$ and we want to perform the following test&lt;/p&gt;
&lt;p&gt;$$
\begin{align}
H_0: \quad &amp;amp; \mu = 0 ,
\newline
H_1: \quad &amp;amp; \mu = 0.1
\end{align}
$$&lt;/p&gt;
&lt;p&gt;The likelihood of the normal distribution with unknown mean $\mu$ and known variance $\sigma^2$ is&lt;/p&gt;
&lt;p&gt;$$
\mathcal L(\mu) = \left( \frac{1}{\sqrt{2 \pi} \sigma } \right)^n e^{- \sum_{i=1}^{n} \frac{(X_i - \mu)^2}{2 \sigma^2}}
$$&lt;/p&gt;
&lt;p&gt;So that the likelihood ratio under the two hypotheses is&lt;/p&gt;
&lt;p&gt;$$
\Lambda = \frac{\mathcal L (0, \sigma^2)}{\mathcal L (0.1, \sigma^2)} = \frac{e^{- \sum_{i=1}^{n} \frac{(X_i)^2}{2 \sigma^2}}}{e^{- \sum_{i=1}^{n} \frac{(X_i - 0.1)^2}{2 \sigma^2}}}
$$&lt;/p&gt;
&lt;h2 id=&#34;sequential-probability-ratio-test&#34;&gt;Sequential Probability Ratio Test&lt;/h2&gt;
&lt;p&gt;Given a pair of fully specified hypotheses, say $H_{0}$ and $H_{1}$, the &lt;strong&gt;first step&lt;/strong&gt; of the sequential probability ratio test is to calculate the log likelihood ratio test $\log \Lambda_{i}$, as new data arrive: with $S_{0}=0$, then, for $i=1,2,&amp;hellip;,$&lt;/p&gt;
&lt;p&gt;$$
S_{n} = S_{n-1} + \log \Lambda_{n}
$$&lt;/p&gt;
&lt;p&gt;The stopping rule is a simple thresholding scheme:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$a&amp;lt;S_{i}&amp;lt;b$: continue monitoring (critical inequality)&lt;/li&gt;
&lt;li&gt;$S_{i}\geq b$: Accept $H_{1}$&lt;/li&gt;
&lt;li&gt;$S_{i}\leq a$: Accept $H_{0}$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;where $a$ and $b$ ($a&amp;lt;0&amp;lt;b&amp;lt;\infty$) depend on the desired type I and type II errors, $\alpha$  and $\beta$. They may be chosen as follows:&lt;/p&gt;
&lt;p&gt;$$
A \approx \log {\frac  {\beta }{1-\alpha }} \quad \text{and} \quad  B \approx \log {\frac  {1-\beta }{\alpha }}
$$&lt;/p&gt;
&lt;p&gt;The equations are approximations because of the discrete nature of the data generating process.&lt;/p&gt;
&lt;h3 id=&#34;special-case-testing-null-effect&#34;&gt;Special Case: testing null effect&lt;/h3&gt;
&lt;p&gt;Let&amp;rsquo;s go back to our example where data is coming from a normal distribution with unknown mean $\mu$ and known variance $\sigma^2$ and hypotheses $H_0: \ \mu = 0$ and $H_1: \ \mu = 0.1$.&lt;/p&gt;
&lt;p&gt;We have seen that the likelihood ratio with a sample of size $n$ is&lt;/p&gt;
&lt;p&gt;$$
\Lambda = \frac{\mathcal L (0, \sigma^2)}{\mathcal L (0.1, \sigma^2)} = \frac{e^{- \sum_{i=1}^{n} \frac{(X_i)^2}{2 \sigma^2}}}{e^{- \sum_{i=1}^{n} \frac{(X_i - 0.1)^2}{2 \sigma^2}}}
$$&lt;/p&gt;
&lt;p&gt;Therefore, the log-likelihood is&lt;/p&gt;
&lt;p&gt;$$
\log (\Lambda) = \left( \sum_{i=1}^{n} \frac{(X_i - 0.1)^2}{2 \sigma^2} \right) - \left( \sum_{i=1}^{n} \frac{(X_i)^2}{2 \sigma^2} \right)
$$&lt;/p&gt;
&lt;h3 id=&#34;simulation&#34;&gt;Simulation&lt;/h3&gt;
&lt;p&gt;We are now ready to perform some simulations. First, let&amp;rsquo;s code the &lt;strong&gt;log likelihood ratio test statistic&lt;/strong&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;log_lr = lambda x: (np.sum((x)**2) - np.sum((x - 0.1)**2) ) / 2
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We now repeat the same experiment we did at the beginning, with one difference: we will compute the log likelihood ratio as a statistic. The data generating process has $\mu=0$, as under the null hypothesis.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df = experiment(log_lr)
df.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;n&lt;/th&gt;
      &lt;th&gt;x&lt;/th&gt;
      &lt;th&gt;stat&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1.624345&lt;/td&gt;
      &lt;td&gt;0.157435&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;-0.611756&lt;/td&gt;
      &lt;td&gt;0.091259&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;-0.528172&lt;/td&gt;
      &lt;td&gt;0.033442&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;-1.072969&lt;/td&gt;
      &lt;td&gt;-0.078855&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;0.865408&lt;/td&gt;
      &lt;td&gt;0.002686&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;Let&amp;rsquo;s now compute the optimal bounds, given significance level $\alpha=0.05$ and power $\beta=0.8$.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;A = np.log(0.8 / 0.05)
B = np.log(0.95 / 0.2)
print(f&#39;Optimal bounds : [{A}, {B}]&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Optimal bounds : [2.772588722239781, 1.5581446180465497]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Since significance and (one minus) power are different, the bound for the null hypothesis is much wider than the bound for the alternative hypothesis. This means that, in case of a true effect of $\mu=0.05$, we will be more likely to accept the null hypothesis $H_0: \mu = 0$ than the alternative $H_1: \mu = 1$.&lt;/p&gt;
&lt;p&gt;We can plot the distribution of the likelihood ratio over samples drawn under the null hypothesis $H_0: \mu = 0$.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plot_experiment(df[2:], ybounds=[A,-B])
plt.annotate(&#39;$H_1: \mu = 0.1$&#39;, xy=(0.9*N, A+0.1));
plt.annotate(&#39;$H_0: \mu = 0$&#39;, xy=(0.9*N, -B-0.2));
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/optimal_stopping_39_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;In this particular case, the test is inconclusive within our sampling framework. We need to &lt;strong&gt;collect more data&lt;/strong&gt; in order to come to a decision.&lt;/p&gt;
&lt;p&gt;What would happen if we were to run the test $K=100$ times?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;stops1 = simulate_experiments(log_lr, ybounds=[-B,A], early_stop=True, lw=1.5)
plt.annotate(&#39;$H_1: \mu = 0.1$&#39;, xy=(0.9*N, A+0.1));
plt.annotate(&#39;$H_0: \mu = 0$&#39;, xy=(0.9*N, -B-0.2));
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/optimal_stopping_41_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;We get a decision for 91 simulations out of 100 and for 87 of them, it&amp;rsquo;s the correct decision. Therefore, our test coverage is 96%, very close to the desired 95%.&lt;/p&gt;
&lt;p&gt;However, the biggest advantage now is that the average &lt;strong&gt;sample size is much smaller&lt;/strong&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;duration = (np.sum(np.abs(stops1)) + N*(100-sum(stops1!=0))) / len(stops1)
print(f&#39;Average experiment duration: {duration}&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Average experiment duration: 266.1430150881779
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To reach a conclusion in 90/100 experiments we need just 1/4 of the samples!&lt;/p&gt;
&lt;p&gt;What would happen if instead the alternative hypothesis $H_1: \mu = 0.1$ was true?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;stops2 = simulate_experiments(log_lr, ybounds=[-B,A], early_stop=True, mu=0.1, lw=1)
plt.annotate(&#39;$H_1: \mu = 0.1$&#39;, xy=(0.9*N, A+0.1));
plt.annotate(&#39;$H_0: \mu = 0$&#39;, xy=(0.9*N, -B-0.2));
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/optimal_stopping_45_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;In this case, we make the correct decision only 81% of the times, as expected since the power of the test was 80%.&lt;/p&gt;
&lt;p&gt;Also in this case, we have significant savings in terms of average sample size.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;duration = (np.sum(np.abs(stops2)) + N*(100-sum(stops2!=0))) / len(stops2)
print(f&#39;Average experiment duration: {duration:0}&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Average experiment duration: 377.71625910443487
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To reach a conclusion in 78/100 experiments we need just 1/3 of the samples!&lt;/p&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://people.missouristate.edu/songfengzheng/Teaching/MTH541/Lecture%20notes/LRT.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Lecture Notes&lt;/a&gt; on Likelihood Ratio Tests&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://medium.com/netflix-techblog/improving-experimentation-efficiency-at-netflix-with-meta-analysis-and-optimal-stopping-d8ec290ae5be&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Improving Experimentation Efficiency&lt;/a&gt; by Netflix on Medium&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Likelihood-ratio_test&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Likelihood-ratio test&lt;/a&gt; Wikipedia article&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Sequential_probability_ratio_test&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Sequential probability ratio test&lt;/a&gt; Wikipedia article&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Permutation Tests for Dummies</title>
      <link>https://matteocourthoud.github.io/post/permutation_test/</link>
      <pubDate>Fri, 15 Apr 2022 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/post/permutation_test/</guid>
      <description>&lt;p&gt;If you search &amp;ldquo;permutation test&amp;rdquo; on Wikipedia, you get the following definition:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;A permutation test (also called re-randomization test) is an exact statistical hypothesis test making use of the proof by contradiction in which the distribution of the test statistic under the null hypothesis is obtained by calculating all possible values of the test statistic under possible rearrangements of the observed data.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;What does it mean? In this tutorial we are going to see in detail what this definition means, how to implement permutation tests, and their pitfalls.&lt;/p&gt;
&lt;h2 id=&#34;example-1-is-a-coin-fair&#34;&gt;Example 1: is a coin fair?&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s start with an example: suppose you wanted to test whether a coin is fair. You throw the coin 10 times and you count the number of times you get heads. Let&amp;rsquo;s simulate the outcome.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import numpy as np

np.random.seed(1)
np.random.binomial(1, 0.5, 10)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;array([0, 1, 0, 0, 0, 0, 0, 0, 0, 1])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Out of 10 coin throws, we got only 2 heads. Does it mean that the coin is not fair?&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;question&lt;/strong&gt; that permutation testing is trying to answer is &amp;ldquo;&lt;em&gt;how unlikely is the observed outcome under the null hypothesis that the coin is fair?&lt;/em&gt;&amp;rdquo;.&lt;/p&gt;
&lt;p&gt;In this case we can directly compute this answer since we have a very little number of throws. The total number of outcomes is $2^{10}$. The number of as or more extreme outcomes, under the assumption that the coin is fair (50-50) is&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;0 heads: ${10 \choose 0} = 1$&lt;/li&gt;
&lt;li&gt;1 head: ${10 \choose 1} = 10$&lt;/li&gt;
&lt;li&gt;2 heads: ${10 \choose 2} = 45$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;So that the probability of getting the same or a more extreme outcome is&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from scipy.special import comb

(comb(10, 0) + comb(10, 1) + comb(10, 2)) / 2**10
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;0.0546875
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This probability seems low but not too low.&lt;/p&gt;
&lt;p&gt;However, we have forgot one thing. We want to test whether the coin is fair in &lt;strong&gt;either&lt;/strong&gt; direction. We would suspect that the coin is unfair if we were getting few heads (as we did), but also if we were getting many heads. Therefore, we should account for both extremes.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sum([comb(10, i) for i in [0, 1, 2, 8, 9, 10]]) / 2**10
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;0.109375
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This number should not be surprising since it&amp;rsquo;s exactly double the previous one.&lt;/p&gt;
&lt;p&gt;It is common in statistics to say that an event is unusual if its probability is less than 1 in 20, i.e. $5%$. If we were adopting that threshold, we would not conclude that getting 2 heads in 10 trows is so unusual. However, getting just one, would be.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sum([comb(10, i) for i in [0, 1, 9, 10]]) / 2**10
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;0.021484375
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;hypothesis-testing&#34;&gt;Hypothesis Testing&lt;/h2&gt;
&lt;p&gt;The process we just went through is called &lt;strong&gt;hypothesis testing&lt;/strong&gt;. The components of an hypothesis test are:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A null hypothesis $H_0$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;in our case, that the coin war fair&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;A test statistic $t$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;in our case, the number of zeros&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;A level of significance $\alpha$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;it is common to choose 5%&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The &lt;strong&gt;idea&lt;/strong&gt; behind &lt;strong&gt;permutation testing&lt;/strong&gt; is the following: in a setting in which we are checking whether one variable has an effect on another variable, the two variables should not be correlated, under the null hypothesis . Therefore, we could re-shuffle the treatment variable and re-compute the test statistic. Lastly, we can compute the p-value as the fraction of as or more extremes outcomes under re-shuffling of the data.&lt;/p&gt;
&lt;h2 id=&#34;example-2-are-women-smarter&#34;&gt;Example 2: are women smarter?&lt;/h2&gt;
&lt;p&gt;Suppose now we were interested in knowing whether females perform better in a test than men. Let&amp;rsquo;s start by writing the data generating process under the assumption of no difference in scores. However, only 30% of the sample will be female.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import pandas as pd

# Data generating process
def generate_data_gender(N=100, seed=1):
    np.random.seed(seed) # Set seed for replicability
    data = pd.DataFrame({&amp;quot;female&amp;quot;: np.random.binomial(1, 0.3, N),
                         &amp;quot;test_score&amp;quot;: np.random.exponential(3, N)})
    return data
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s now generate a sample of size 100.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Generate data
data_gender = generate_data_gender()
data_gender.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;female&lt;/th&gt;
      &lt;th&gt;test_score&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1.186447&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;2.246348&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;6.513147&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1.326091&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;7.175402&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;We can compute the treatment effect by computing the difference in mean outcomes between male and females.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def compute_score_diff(data):
    T = np.mean(data.loc[data[&#39;female&#39;]==1, &#39;test_score&#39;]) - np.mean(data.loc[data[&#39;female&#39;]==0, &#39;test_score&#39;])
    return T
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;T = compute_score_diff(data_gender)
print(f&amp;quot;The estimated treatment effect is {T}&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;The estimated treatment effect is -1.3612262580563321
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It looks that females actually did worse than males. But is the difference statistically significant? We can perform a randomization test and compute the probability of observing a more extreme outcome.&lt;/p&gt;
&lt;p&gt;First, let&amp;rsquo;s write the permutation routine that takes a variable in the data and permutes it.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def permute(data, var, r):
    temp_data = data.copy()
    temp_data[var] = np.random.choice(data[var], size=len(data), replace=r)
    return temp_data
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can now write the permutation test. It spits out a vector of statistics and prints the implied p-value.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def permutation_test(data, permute, var, compute_stat, K=1000, r=False):
    T = compute_stat(data)
    T_perm = []
    for k in range(K):
        temp_data = permute(data, var, r)
        T_perm += [compute_stat(temp_data)]
    print(f&amp;quot;The p-value is {sum(np.abs(T_perm) &amp;gt;= np.abs(T))/K}&amp;quot;)
    return T_perm
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;Ts = permutation_test(data_gender, permute, &#39;test_score&#39;, compute_score_diff)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;The p-value is 0.063
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Apparently the result we have observed was quite unusual, but not at the 5% level. We can plot the distribution of statistics to visualize this result.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;%matplotlib inline
%config InlineBackend.figure_format = &#39;retina&#39;
from src.utils import *
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def plot_test(T, Ts, title):
    plt.hist(Ts, density=True, bins=30, alpha=0.7, color=&#39;C0&#39;)
    plt.vlines([-T, T], ymin=plt.ylim()[0], ymax=plt.ylim()[1], color=&#39;C2&#39;)
    plt.title(title);
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plot_test(T, Ts, &#39;Distribution of score differences under permutation&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/permutation_test_29_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;As we can see, the observed difference in scores is quite extreme with respect the distribution generate by the permutation.&lt;/p&gt;
&lt;p&gt;One &lt;strong&gt;issue&lt;/strong&gt; with the permutation test we just ran is that it is computationally expensive to draw without replacement. The standard and much faster procedure is to draw without replacement.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;Ts_repl = permutation_test(data_gender, permute, &#39;test_score&#39;, compute_score_diff, r=True)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;The p-value is 0.052
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The p-value is virtually the same.&lt;/p&gt;
&lt;p&gt;How &lt;strong&gt;accurate&lt;/strong&gt; is the test? Since we have access to the data generating process, we can compute the true p-value via simulation. We draw many samples from the true data generating process and, for each, compute the difference in scores. The simulated p-value is going to be the frequency of more extreme statistics.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Function to simulate data and compute pvalue
def simulate_stat(dgp, compute_stat, K=1000):
    T = compute_stat(dgp())
    T_sim = []
    for k in range(K):
        data = dgp(seed=k)
        T_sim += [compute_stat(data)]
    print(f&amp;quot;The p-value is {sum(np.abs(T_sim) &amp;gt;= np.abs(T))/K}&amp;quot;)
    return np.array(T_sim)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;T_sim = simulate_stat(generate_data_gender, compute_score_diff)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;The p-value is 0.038
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Again, we can plot the distribution of simulated statistics to understand the computed p-value.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plot_test(T, T_sim, &#39;Distribution of score differences under simulation&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/permutation_test_36_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;As expected, most of the mass lies within the interval, indicating a relatively extreme result. We have just been &amp;ldquo;unlucky&amp;rdquo; with the draw, but the permutation test was accurate.&lt;/p&gt;
&lt;h2 id=&#34;permutation-tests-vs-t-tests&#34;&gt;Permutation tests vs t-tests&lt;/h2&gt;
&lt;p&gt;What is the difference between a t-test and a permutation test?&lt;/p&gt;
&lt;p&gt;Permutation test &lt;strong&gt;advantages&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;does not make distributional assumptions&lt;/li&gt;
&lt;li&gt;not sensible to outliers&lt;/li&gt;
&lt;li&gt;can be computed also for statistics whose distribution is not known&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Permutation test &lt;strong&gt;disadvantages&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;computationally intense&lt;/li&gt;
&lt;li&gt;very sample-dependent&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;example-3-is-university-worth&#34;&gt;Example 3: is university worth?&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s now switch to a new example to compare t-tests and permutation tests.&lt;/p&gt;
&lt;p&gt;Assume we want to check whether university is a worthy investment. We have information about whether individuals attended university and their future salary. The problem here is that income is a particularly &lt;strong&gt;skewed&lt;/strong&gt; variable.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Data generating process
def generate_data_income(N=1000, seed=1):
    np.random.seed(seed) # Set seed for replicability
    university = np.random.binomial(1, 0.5, N) # Treatment
    data = pd.DataFrame({&amp;quot;university&amp;quot;: university,
                         &amp;quot;income&amp;quot;: np.random.lognormal(university, 2.3, N)})
    return data
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;data_income = generate_data_income()
data_income.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;university&lt;/th&gt;
      &lt;th&gt;income&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;5.305618&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1.289598&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;6.507720&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;6.019961&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0.034482&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;The distribution of income is very heavy tailed. Let&amp;rsquo;s plot its density across the two groups.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sns.kdeplot(data=data_income, x=&amp;quot;income&amp;quot;, hue=&amp;quot;university&amp;quot;)\
.set(title=&#39;Income density by group&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/permutation_test_45_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;The distribution is so skewed that we cannot actually visually perceive differences between the two groups. Let&amp;rsquo;s compute the expected difference.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def compute_income_diff(data):
    T = np.mean(data.loc[data[&#39;university&#39;]==1, &#39;income&#39;]) - np.mean(data.loc[data[&#39;university&#39;]==0, &#39;income&#39;])
    return T
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;T = compute_income_diff(data_income)
T
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;23.546974435985444
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It looks like university graduates have higher income. Is this difference statistically different from zero? Let&amp;rsquo;s perform a permutation test.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;T_perm = permutation_test(data_income, permute, &#39;university&#39;, compute_income_diff)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;The p-value is 0.011
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The permutation test is telling us that the difference is extremely unusual under the null hypothesis. In other words, it is very unlikely that university graduates earn the same income of non-university graduates.&lt;/p&gt;
&lt;p&gt;What would be the outcome of a standard t-test?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from scipy.stats import ttest_ind

ttest_ind(data_income.query(&#39;university==1&#39;)[&#39;income&#39;], data_income.query(&#39;university==0&#39;)[&#39;income&#39;])
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Ttest_indResult(statistic=1.5589492598056494, pvalue=0.1193254252009701)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As we can see, the two tests provide extremely different results. The t-test is much more conservative, telling us that the unlikeliness of the data is just $12%$ compared to the $1.1%$ of the permutation test.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;reason&lt;/strong&gt; is that we have extremely skewed data. The t-test is very sensible to extreme observation and will therefore compute a very high variance because of very few data points.&lt;/p&gt;
&lt;p&gt;The permutation test can further address the problem of a skewed outcome distribution by using a test statistic that is more &lt;strong&gt;sensible to outliers&lt;/strong&gt;. Let&amp;rsquo;s perform the permutation test using the &lt;strong&gt;trimmed mean&lt;/strong&gt; instead of the mean.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from scipy.stats import trim_mean

def compute_income_mediandiff(data):
    T = np.median(data.loc[data[&#39;university&#39;]==1, &#39;income&#39;]) - np.median(data.loc[data[&#39;university&#39;]==0, &#39;income&#39;])
    return T
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;T_perm = permutation_test(data_income, permute, &#39;university&#39;, compute_income_mediandiff)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;The p-value is 0.0
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In this case, the permutation test is extremely confident that the trimmed mean of the two groups is different.&lt;/p&gt;
&lt;p&gt;However, an advantage of the t-test is &lt;strong&gt;speed&lt;/strong&gt;. Let&amp;rsquo;s compare the two tests by computing their execution time. Note that this is just a rough approximation since the permutation test could be sensible optimized.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import time

# No replacement
start = time.time()
permutation_test(data_income, permute, &#39;university&#39;, compute_income_diff)
print(f&amp;quot;Elapsed time without replacement: {time.time() - start}&amp;quot;)

# Replacement
start = time.time()
ttest_ind(data_income.query(&#39;university==1&#39;)[&#39;income&#39;], data_income.query(&#39;university==0&#39;)[&#39;income&#39;])
print(f&amp;quot;Elapsed time with replacement: {time.time() - start}&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;The p-value is 0.016
Elapsed time without replacement: 0.28911614418029785
Elapsed time with replacement: 0.00125885009765625
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The permutation test is 300 times slower. This can be a particularly relevant difference for larger sample sizes.&lt;/p&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;In this tutorial, we have seen how to perform permutation tests across different data generating processes.&lt;/p&gt;
&lt;p&gt;The underlying principle is the same: permute an variable that is assumed to be random under the null hypothesis and re-compute the test statistic. Then check how unusual was the test statistic in the original dataset.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Policy Learning</title>
      <link>https://matteocourthoud.github.io/post/policy_learning/</link>
      <pubDate>Fri, 15 Apr 2022 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/post/policy_learning/</guid>
      <description>&lt;p&gt;In this tutorial, we are going to see how to design the most welfare-improving policy in presence of treatment effect heterogeneity and treatment costs or budget constraints.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Requisites&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;For this tutorial, I assume you are familiar with the following concepts:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Rubin&amp;rsquo;s potential outcome framework&lt;/li&gt;
&lt;li&gt;Propensity weighting or uplifting&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://matteocourthoud.github.io/post/aipw/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;AIPW or Double Robust Estimators&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://matteocourthoud.github.io/post/causal_trees/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Causal Trees&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Academic Replication&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;We are going to replicate the paper by &lt;a href=&#34;https://www.aeaweb.org/articles?id=10.1257/jep.32.4.201&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Hanna and Olken (2018)&lt;/a&gt; in which the authors study the optimal allocation of a cash transfer to households in Peru. We slightly twist the original paper by actually assigning the transfer and assuming 100% consumption.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Business Case&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;We are going to study a company that has to decide which consumers to target with ads.&lt;/p&gt;
&lt;h2 id=&#34;setting&#34;&gt;Setting&lt;/h2&gt;
&lt;p&gt;We assume that for a set of i.i.d. subjects $i = 1, &amp;hellip;, n$ we observed a tuple $(X_i, T_i, Y_i)$ comprised of&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;a feature vector $X_i \in \mathbb R^n$&lt;/li&gt;
&lt;li&gt;a treatment variable $T_i \in \lbrace 0, 1 \rbrace$&lt;/li&gt;
&lt;li&gt;a response $Y_i \in \mathbb R$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Assumption 1 : unconfoundedness&lt;/strong&gt; (or ignorability, or selection on observables)&lt;/p&gt;
&lt;p&gt;$$
\big \lbrace Y_i^{(1)} , Y_i^{(0)} \big \rbrace \ \perp \ T_i \ | \ X_i
$$&lt;/p&gt;
&lt;p&gt;i.e. conditional on observable characteristics $X$, the treatment assignment $T$ is as good as random.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Assumption 2: overlap&lt;/strong&gt; (or bounded support)&lt;/p&gt;
&lt;p&gt;$$
\exists \eta &amp;gt; 0 \ : \ \eta \leq \mathbb E \left[ T_i = 1 \ \big | \ X_i = x \right] \leq 1-\eta
$$&lt;/p&gt;
&lt;p&gt;i.e. no observation is deterministically assigned to the treatment or control group.&lt;/p&gt;
&lt;h2 id=&#34;policy-learning&#34;&gt;Policy Learning&lt;/h2&gt;
&lt;p&gt;The objective of policy learning is to decide which people to treat. More explicitly, we want to learn a map from observable characteristics to a (usually binary) policy space.&lt;/p&gt;
&lt;p&gt;$$
\pi : \mathcal X \to \lbrace 0, 1 \rbrace
$$&lt;/p&gt;
&lt;p&gt;Policy learning is closely related to the &lt;strong&gt;estimation of heterogeneous treatment effects&lt;/strong&gt;. In fact, in both settings, we want to investigate how the treatment affects different individuals in different ways.&lt;/p&gt;
&lt;p&gt;The main &lt;strong&gt;difference&lt;/strong&gt; between policy learning and the estimation of heterogeneous treatment effects is the objective function. In policy learning, we are acting in a limited resources setting where providing treatment is costly and the cost could depend on individual characteristics. For example, it might be more costly to vaccinate individuals that live in remote areas. Therefore, one might not just want to treat individuals with the largest expected treatment effect, but the ones for whom treatment is most cost-effective.&lt;/p&gt;
&lt;p&gt;The utilitarian &lt;strong&gt;value&lt;/strong&gt; of a policy $\pi$&lt;/p&gt;
&lt;p&gt;$$
V(\pi) = \mathbb E \Big[ Y_i(\pi(X_i)) \Big] = \mathbb E \big[ Y^{(0)}_i \big] + \mathbb E \big[ \tau(X_i) \pi(X_i) \big]
$$&lt;/p&gt;
&lt;p&gt;measures the expectation of the potential outcome $Y$ if we were to &lt;strong&gt;assign&lt;/strong&gt; treatment $T$ according to policy $\pi$. This expectation can be split into &lt;strong&gt;two parts&lt;/strong&gt;:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;The baseline expected potential outcome $\mathbb E \big[ Y^{(0)}_i \big]$&lt;/li&gt;
&lt;li&gt;The expected effect of the policy $\mathbb E \big[ \tau(X_i) \pi(X_i) \big]$&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The &lt;strong&gt;objective&lt;/strong&gt; of policy learning is to learn a policy with high value $V(\pi)$. As part (2) of the formula makes clear, you get a higher value if you treat the people with a high treatment effect $\tau(x)$.&lt;/p&gt;
&lt;p&gt;A &lt;strong&gt;simple approach&lt;/strong&gt; could be to assign treatment according to a &lt;strong&gt;thresholding rule&lt;/strong&gt; $\tau(x) &amp;gt; c$, where $c$ is some cost below which is not worth treating individuals (or there is not enough budget).&lt;/p&gt;
&lt;p&gt;However, estimating the conditional average treatment effect (CATE) function $\tau(x)$ and learning a good policy $\pi(x)$ are different &lt;strong&gt;problems&lt;/strong&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the correct loss function for policy learning is not the mean squared error (MSE) on $\tau(x)$
&lt;ul&gt;
&lt;li&gt;we want to &lt;strong&gt;maximize welfare&lt;/strong&gt;!&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;the CATE function $\tau(x)$ might not use some features for targeting
&lt;ul&gt;
&lt;li&gt;e.g. &lt;strong&gt;cannot discriminate&lt;/strong&gt; based on race or gender&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;you don&amp;rsquo;t want to have feature that people can influence
&lt;ul&gt;
&lt;li&gt;e.g. use a self-reported measure that people can distort&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We would like to find a &lt;strong&gt;loss function&lt;/strong&gt; $L(\pi ; Y_i, X_i, T_i)$ such that&lt;/p&gt;
&lt;p&gt;$$
\mathbb E \big[ L(\pi ; Y_i, X_i, T_i) \big] = - V(\pi)
$$&lt;/p&gt;
&lt;h3 id=&#34;ipw-loss&#34;&gt;IPW Loss&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://onlinelibrary.wiley.com/doi/abs/10.3982/ECTA13288&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Kitagawa and Tenenov (2018)&lt;/a&gt; propose to learn an empirical estimate of the value function using inverse propensity weighting (IPW).&lt;/p&gt;
&lt;p&gt;$$
\hat \pi = \arg \max_{\pi} \Big\lbrace \hat V(\pi) : \pi \in \Pi \Big\rbrace
$$&lt;/p&gt;
&lt;p&gt;where&lt;/p&gt;
&lt;p&gt;$$
\hat V(\pi) = \frac{ \mathbb I \big(\lbrace T_i = \pi(X_i) \rbrace \big) }{ \mathbb P \big[ \lbrace T_i = \pi(X_i) \rbrace \ \big| \ X_i \big] } Y_i
$$&lt;/p&gt;
&lt;p&gt;The authors show that under &lt;strong&gt;unconfoundedness&lt;/strong&gt;, if the propensity score $e(x)$ is known and $\Pi$ is not too complex, the value of the estimated policy converges to the optimal value.&lt;/p&gt;
&lt;p&gt;Note that this is a &lt;strong&gt;very different problem&lt;/strong&gt; from the normal optimization problem with a MSE loss. In fact, we now have a binary argument in the loss function which makes the problem similar to a classification problem, in which we want to classify people into &lt;em&gt;high gain&lt;/em&gt; and &lt;em&gt;low gain&lt;/em&gt; categories.&lt;/p&gt;
&lt;h3 id=&#34;aipw-loss&#34;&gt;AIPW Loss&lt;/h3&gt;
&lt;p&gt;If propensity score $e(x)$ is not known, we can use a &lt;strong&gt;doubly robust estimator&lt;/strong&gt;, exactly as for the average treatment effect.&lt;/p&gt;
&lt;p&gt;$$
\hat V = \frac{1}{n} \sum_{i=1}^{n}
\begin{cases}
\hat \Gamma_i \quad &amp;amp;\text{if} \quad \pi(X_i) = 1
\newline&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;\hat \Gamma_i \quad &amp;amp;\text{if} \quad \pi(X_i) = 0
\end{cases}
$$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;where&lt;/p&gt;
&lt;p&gt;$$
\hat \Gamma_i = \hat \mu^{(1)}(X_i) - \hat \mu^{(0)}(X_i) + \frac{T_i }{\hat e(X_i)} \left( Y_i - \hat \mu^{(1)}(X_i) \right) - \frac{(1-T_i) }{1-\hat e(X_i)} \left( Y_i - \hat \mu^{(0)}(X_i) \right)
$$&lt;/p&gt;
&lt;p&gt;The relationship with AIPW is that $\hat \tau_{AIPW} = \frac{1}{n} \sum_{i=1}^{n} \hat \Gamma_i$. Therefore, the objective function $V(\pi)$ is build so that when we assign treatment to a unit we &amp;ldquo;gain&amp;rdquo; the double-robust score $\hat \tau_{AIPW}$, while, if we do not assign treatment, we &amp;ldquo;pay&amp;rdquo; the double-robust score $\hat \tau_{AIPW}$.&lt;/p&gt;
&lt;h2 id=&#34;academic-application&#34;&gt;Academic Application&lt;/h2&gt;
&lt;p&gt;For the academic applicaiton, we are going to replicate the paper by &lt;a href=&#34;https://www.aeaweb.org/articles?id=10.1257/jep.32.4.201&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Hanna and Olken (2018)&lt;/a&gt; in which the authors study the optimal allocation of a cash transfer to households in Peru. We slightly twist the original paper by actually assigning the transfer and assuming 100% consumption.&lt;/p&gt;
&lt;p&gt;First, let&amp;rsquo;s load the modified dataset.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from src.utils import *
from src.dgp import dgp_ao18
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;%matplotlib inline
%config InlineBackend.figure_format = &#39;retina&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;dgp = dgp_ao18()
df = dgp.import_data()
df.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;Unnamed: 0&lt;/th&gt;
      &lt;th&gt;d_fuel_other&lt;/th&gt;
      &lt;th&gt;d_fuel_wood&lt;/th&gt;
      &lt;th&gt;d_fuel_coal&lt;/th&gt;
      &lt;th&gt;d_fuel_kerosene&lt;/th&gt;
      &lt;th&gt;d_fuel_gas&lt;/th&gt;
      &lt;th&gt;d_fuel_electric&lt;/th&gt;
      &lt;th&gt;d_fuel_none&lt;/th&gt;
      &lt;th&gt;d_water_other&lt;/th&gt;
      &lt;th&gt;d_water_river&lt;/th&gt;
      &lt;th&gt;...&lt;/th&gt;
      &lt;th&gt;d_lux_1&lt;/th&gt;
      &lt;th&gt;d_lux_2&lt;/th&gt;
      &lt;th&gt;d_lux_3&lt;/th&gt;
      &lt;th&gt;d_lux_4&lt;/th&gt;
      &lt;th&gt;d_lux_5&lt;/th&gt;
      &lt;th&gt;training&lt;/th&gt;
      &lt;th&gt;h_hhsize&lt;/th&gt;
      &lt;th&gt;cash_transfer&lt;/th&gt;
      &lt;th&gt;consumption&lt;/th&gt;
      &lt;th&gt;welfare&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;211.0000&lt;/td&gt;
      &lt;td&gt;5.351858&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;420.1389&lt;/td&gt;
      &lt;td&gt;6.040585&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;390.8318&lt;/td&gt;
      &lt;td&gt;5.968277&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;9&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;285.6018&lt;/td&gt;
      &lt;td&gt;5.654599&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;8&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;118.0713&lt;/td&gt;
      &lt;td&gt;4.771289&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;5 rows Ã 78 columns&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;As we can see, we have a lot of information about individuals in Peru. Crucially for the research question, we observe&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;whether the household received a cash transfer, &lt;code&gt;cash_transfer&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;the household&amp;rsquo;s welfare afterwards, &lt;code&gt;welfare_post&lt;/code&gt;
&lt;ul&gt;
&lt;li&gt;assuming
$$
\text{welfare} = \log (\text{consumption})
$$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We would like to understand which individuals should be given a transfer, given that the transfer is costly. Let&amp;rsquo;s assume the transfer costs $0.3$ units of welfare.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from econml.policy import DRPolicyForest

cost = 0.3
policy = DRPolicyForest(random_state=1).fit(Y=df[dgp.Y] - cost*df[dgp.T], T=df[dgp.T], X=df[dgp.X])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can partially visualize the policy by plotting a regression tree for the most important features.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;%matplotlib inline
policy.plot(tree_id=1, max_depth=2, feature_names=dgp.X, fontsize=8)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/policy_learning_18_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;To understand if the estimated policy was effective, we can load the oracle dataset, with the potential outcomes.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_oracle = dgp.import_data(oracle=True)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;From the oracle dataset, we can compute the actual value of the policy.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;T_hat = policy.predict(df[dgp.X])
V_policy = (df_oracle[&#39;welfare_1&#39;].values - cost - df_oracle[&#39;welfare_0&#39;].values) * T_hat
print(f&#39;Estimated policy value (N_T={sum(T_hat)}): {np.mean(V_policy) :.4}&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Estimated policy value (N_T=21401): 0.05897
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The value is positive, indicating that the treatment was effective. But how well did we do? We can compare the estimated policy with the oracle policy that assign treatment to each cost-effective unit.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;T_oracle = (df_oracle[&#39;welfare_1&#39;] - df_oracle[&#39;welfare_0&#39;]) &amp;gt; cost
V_oracle = (df_oracle[&#39;welfare_1&#39;] - cost - df_oracle[&#39;welfare_0&#39;] ) * T_oracle
print(f&#39;Oracle policy value (N_T={sum(T_oracle)}): {np.mean(V_oracle) :.4}&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Oracle policy value (N_T=17630): 0.07494
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We actually achieved 79% of the potential policy gains! Also note that our policy is too generous, treating more units than optimal. But how well would we have done if the same amount of cash transfers were given at random?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;T_rand = np.random.binomial(1, sum(T_hat)/len(df), len(df))
V_rand = (df_oracle[&#39;welfare_1&#39;] - cost - df_oracle[&#39;welfare_0&#39;] ) * T_rand
print(f&#39;Random policy value (N_T={sum(T_rand)}): {np.mean(V_rand) :.4}&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Random policy value (N_T=21359): 0.0002698
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;A random assignment of the same amount of cash transfers would not achieve any effect. However, this assumes that we already know the optimal amount of funds to distribute. What if instead we had treated everyone?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;V_all = (df_oracle[&#39;welfare_1&#39;] - cost - df_oracle[&#39;welfare_0&#39;] )
print(f&#39;All-treated policy value (N_T={len(df)}): {np.mean(V_all) :.4}&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;All-treated policy value (N_T=45378): 0.0004019
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Indiscriminate treatment would again not achieve any effect. Lastly, what if we had just estimated the treatment effect using AIPW and used it as a threshold?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from econml.dr import LinearDRLearner

model = LinearDRLearner(random_state=1).fit(Y=df[dgp.Y], T=df[dgp.T], X=df[dgp.X])
T_ipw = model.effect(X=df[dgp.X], T0=0, T1=1) &amp;gt; cost
V_ipw = (df_oracle[&#39;welfare_1&#39;] - cost - df_oracle[&#39;welfare_0&#39;] ) * T_ipw
print(f&#39;IPW policy value (N_T={sum(T_ipw)}): {np.mean(V_ipw) :.4}&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;IPW policy value (N_T=21003): 0.06293
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We are actually doing better! Weird&amp;hellip;&lt;/p&gt;
&lt;h2 id=&#34;business-case&#34;&gt;Business Case&lt;/h2&gt;
&lt;p&gt;We are given the following problem:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;A firm would like to understand which customers to show an ad, in order to increase revenue. The firm ran a A/B test showing a random sample of customers an ad. First, try to understand if there is heterogeneity in treatment. Then, decide which customers to show the ad, given that ads are costly (1$ each). Further suppose that you cannot discriminate on gender. How do the results change?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;We start by drawing a sample from the data generating process.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from src.utils import *
from src.dgp import dgp_ad
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;dgp = dgp_ad()
df = dgp.generate_data()
df.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;male&lt;/th&gt;
      &lt;th&gt;black&lt;/th&gt;
      &lt;th&gt;age&lt;/th&gt;
      &lt;th&gt;educ&lt;/th&gt;
      &lt;th&gt;ad&lt;/th&gt;
      &lt;th&gt;revenue&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;55.0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;-0.327221&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;47.0&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;0.659393&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;31.0&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;True&lt;/td&gt;
      &lt;td&gt;2.805178&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;51.0&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;-0.508548&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;48.0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;True&lt;/td&gt;
      &lt;td&gt;0.762280&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;We have information on the number of pages visited in the previous month, whether the user is located in the US, whether it connects by mobile and the revenue pre-intervention.&lt;/p&gt;
&lt;p&gt;We are going to use the &lt;a href=&#34;https://econml.azurewebsites.net&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;econml&lt;/code&gt;&lt;/a&gt; library to estimate the treatment effects. First, we use the &lt;code&gt;DRLearner&lt;/code&gt; library to estimate heterogeneous treatment effects using a double robust estimator. We can specify both the &lt;code&gt;model_propensity&lt;/code&gt; for $e(x)$ and the &lt;code&gt;model_regression&lt;/code&gt; for $\mu(x)$.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from econml.dr import DRLearner

model = DRLearner(random_state=1).fit(Y=df[dgp.Y], T=df[dgp.T], X=df[dgp.X]);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can plot a visual representation of the treatment effect heterogeneity using the &lt;code&gt;SingleTreePolicyInterpreter&lt;/code&gt; function, which infers a tree representation of the treatment effects learned from another model.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from econml.cate_interpreter import SingleTreeCateInterpreter

SingleTreeCateInterpreter(max_depth=2, random_state=1).interpret(model, X=df[dgp.X]).plot(feature_names=dgp.X)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/policy_learning_39_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;It seems that the most relevant dimension of treatment heterogeneity is &lt;code&gt;education&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;We can now use policy learning to estimate a treatment policy. We use the &lt;code&gt;DRPolicyTree&lt;/code&gt; from the &lt;code&gt;econml&lt;/code&gt; package.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from econml.policy import DRPolicyTree

policy = DRPolicyTree(random_state=1, max_depth=2).fit(Y=df[dgp.Y], T=df[dgp.T], X=df[dgp.X])
policy.plot(feature_names=dgp.X)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/policy_learning_41_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;We will now assume that the treatment is costly.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;cost = 1
policy = DRPolicyTree(random_state=1, max_depth=2).fit(Y=df[dgp.Y]-cost*df[dgp.T], T=df[dgp.T], X=df[dgp.X])
policy.plot(feature_names=dgp.X)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/policy_learning_43_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;As we can see, the model decides to use race to discriminate treatment. However, let&amp;rsquo;s now suppose we cannot discriminate on race and gender.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;X_short = [&#39;age&#39;, &#39;educ&#39;]
policy = DRPolicyTree(random_state=1, max_depth=2).fit(Y=df[dgp.Y]-cost*df[dgp.T], T=df[dgp.T], X=df[X_short])
policy.plot(feature_names=X_short)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/policy_learning_45_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;In this case, the model uses education instead of race in order to assign treatment.&lt;/p&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://onlinelibrary.wiley.com/doi/abs/10.3982/ECTA13288&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Who Should Be Treated? Empirical Welfare Maximization Methods for Treatment Choice&lt;/a&gt; (2018) by Kitagawa and Tetenov&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://ideas.repec.org/p/ecl/stabus/3506.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Efficient Policy Learning&lt;/a&gt; (2017) by Athey and Wager&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=YQXRwvFQOPk&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Policy Learning&lt;/a&gt; video lecture by Stefan Wager (Stanford)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/microsoft/EconML/blob/main/notebooks/CustomerScenarios/Case%20Study%20-%20Customer%20Segmentation%20at%20An%20Online%20Media%20Company.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Customer Segmentation&lt;/a&gt; case study by EconML&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Propensity Score Matching</title>
      <link>https://matteocourthoud.github.io/post/propensity_score/</link>
      <pubDate>Fri, 15 Apr 2022 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/post/propensity_score/</guid>
      <description>&lt;p&gt;In this tutorial, we are going to see how to estimate causal effects when the treatment is not &lt;em&gt;unconditionally&lt;/em&gt; randomly assigned, but we need to condition on observable features in order to assume treatment exogeneity. This might happen either when an experiment is stratified or in observational studies.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Requisites&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;For this tutorial, I assume you are familiar with the following concepts:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Rubin&amp;rsquo;s potential outcome framework&lt;/li&gt;
&lt;li&gt;Ordinary least squares regression&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Academic Application&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;As an academic application, we are going to replicate &lt;a href=&#34;https://www.jstor.org/stable/2937954&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Evaluating the Econometric Evaluations of Training Programs with Experimental Data&lt;/a&gt; (1986) by Lalonde and the followup paper &lt;a href=&#34;https://www.tandfonline.com/doi/abs/10.1080/01621459.1999.10473858&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Causal Effects in Non-Experimental Studies: Reevaluating the Evaluation of Training Programs&lt;/a&gt; (1999) by Dahejia and Wahba. These papers study a randomized intervention providing work experienced to improve labor market outcomes.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Business Case&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;TBD&lt;/p&gt;
&lt;h2 id=&#34;setting&#34;&gt;Setting&lt;/h2&gt;
&lt;p&gt;We assume that for a set of i.i.d. subjects $i = 1, &amp;hellip;, n$ we observed a tuple $(X_i, D_i, Y_i)$ comprised of&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;a feature vector $X_i \in \mathbb R^n$&lt;/li&gt;
&lt;li&gt;a treatment assignment $D_i \in \lbrace 0, 1 \rbrace$&lt;/li&gt;
&lt;li&gt;a response $Y_i \in \mathbb R$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Assumption 1 : unconfoundedness&lt;/strong&gt; (or ignorability, or selection on observables, or conditional independence)&lt;/p&gt;
&lt;p&gt;$$
\big \lbrace Y_i^{(1)} , Y_i^{(0)} \big \rbrace \ \perp \ D_i \ | \ X_i
$$&lt;/p&gt;
&lt;p&gt;i.e. conditional on observable characteristics $X$, the treatment assignment $T$ is as good as random. What this assumption rules out is &lt;em&gt;selection on unobservables&lt;/em&gt;. Moreover, it&amp;rsquo;s &lt;em&gt;untestable&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Assumption 2: overlap&lt;/strong&gt; (or common support)&lt;/p&gt;
&lt;p&gt;$$
\exists \eta &amp;gt; 0 \ : \ \eta \leq \mathbb E \left[ D_i = 1 \ \big | \ X_i = x \right] \leq 1-\eta
$$&lt;/p&gt;
&lt;p&gt;i.e. no observation is deterministically assigned to the treatment or control group. We need this assumption for counterfactual statements to make sense. If some observations had zero probability of (not) being treated, it would make no sense to try to estimate their counterfactual outcome in case they would have (not) being treated. Also this assumption is &lt;em&gt;untestable&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Assumption 3: stable unit treatment value (SUTVA)&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;$$
Y_i^{(D_i)} \perp D_j \quad \forall j \neq i
$$&lt;/p&gt;
&lt;p&gt;i.e. the potential outcome of one individual is independent from the treatment status of any other individual. Common &lt;em&gt;violations&lt;/em&gt; of this assumption include&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;general equilibrium effects&lt;/li&gt;
&lt;li&gt;spillover effects&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This assumption is &lt;em&gt;untestable&lt;/em&gt;.&lt;/p&gt;
&lt;h2 id=&#34;propensity-scores&#34;&gt;Propensity Scores&lt;/h2&gt;
&lt;h3 id=&#34;exogenous-treatment&#34;&gt;Exogenous Treatment&lt;/h3&gt;
&lt;p&gt;The &lt;strong&gt;fundamental problem of causal inference&lt;/strong&gt; is that we do not observe counterfactual outcomes, i.e. we do not observe what would have happened to treated units if they had not received the treatment and viceversa.&lt;/p&gt;
&lt;p&gt;If treatment is exogenous, we know that the difference in means identifies the average treatment effect $\mathbb E[\tau]$.&lt;/p&gt;
&lt;p&gt;$$
\mathbb E[\tau] = \mathbb E \big[ Y_i \ \big| \ D_i = 1 \big] - \mathbb E \big[ Y_i \ \big| \ D_i = 0 \big] = \mathbb E \big[ Y_{i}^{(1)} - Y_{i}^{(0)} \big]
$$&lt;/p&gt;
&lt;p&gt;Therefore, we can build an unbiased estimator of the average treatment effect as the empirical counterpart of the expression above&lt;/p&gt;
&lt;p&gt;$$
\hat \tau(Y, D) = \frac{1}{n} \sum_{i=1}^{n} \big( D_i Y_i - (1-D_i) Y_i \big)
$$&lt;/p&gt;
&lt;p&gt;In case treatment is not randomly assigned, we use the Thompson Horowitz (1952) estimator&lt;/p&gt;
&lt;p&gt;$$
\hat \tau(Y, D) = \frac{1}{n} \sum_{i=1}^{n} \left( \frac{D_i Y_i}{\pi_{i}} - \frac{(1-D_i) Y_i}{1 - \pi_{i}} \right)
$$&lt;/p&gt;
&lt;p&gt;where $\pi_{i} = \Pr(D_i=1)$ is the probability of being treated, also known as &lt;strong&gt;propensity score&lt;/strong&gt;. Sometimes the propensity score is known, for example when treatment is stratified. However, in general, it is not.&lt;/p&gt;
&lt;h3 id=&#34;conditionally-exogenous-treatment&#34;&gt;Conditionally Exogenous Treatment&lt;/h3&gt;
&lt;p&gt;In many cases and especially in observational studies, treatment $D$ is not unconditionally exogenous, but it&amp;rsquo;s exogenous only after we condition on some characteristic $X$. If these characteristics are observables, we have the &lt;strong&gt;unconfoundedness&lt;/strong&gt; assumption.&lt;/p&gt;
&lt;p&gt;Under unconfoundedness, we can still identify the average treatment effect, as a &lt;em&gt;conditional&lt;/em&gt; difference in means:&lt;/p&gt;
&lt;p&gt;$$
\mathbb E[\tau] = \mathbb E \big[ Y_{i}^{(1)} - Y_{i}^{(0)} \ \big| \ X_i \big]
$$&lt;/p&gt;
&lt;p&gt;The main problem is that we need to condition of the observables that actually make the unconfoundedness assumption hold. This might be tricky in two cases:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;when we have many observables&lt;/li&gt;
&lt;li&gt;when we do not know the functional form of the observables that we need to condition on&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The main contribution of Rosenbaum and Rubin (1983) is to show that if &lt;strong&gt;unconfoundedness&lt;/strong&gt; holds, then&lt;/p&gt;
&lt;p&gt;$$
\big \lbrace Y_i^{(1)} , Y_i^{(0)} \big \rbrace \ \perp \ D_i \ | \ \pi(X_i)
$$&lt;/p&gt;
&lt;p&gt;i.e. you only need to condition on $\pi(X)$ in order to recover the average treatment effect.&lt;/p&gt;
&lt;p&gt;$$
\mathbb E[\tau] = \mathbb E \big[ Y_{i}^{(1)} - Y_{i}^{(0)} \ \big| \ \pi(X_i) \big]
$$&lt;/p&gt;
&lt;p&gt;This implies the following &lt;strong&gt;inverse propensity-weighted&lt;/strong&gt; estimator:&lt;/p&gt;
&lt;p&gt;$$
\hat \tau^{IPW} = \frac{1}{n} \sum_{i=1}^{n} \left( \frac{D_i Y_i}{\hat \pi(X_i)} - \frac{(1-D_i) Y_i}{1 - \hat \pi(X_i)} \right)
$$&lt;/p&gt;
&lt;p&gt;which, under &lt;em&gt;unconfoundedness&lt;/em&gt; is an &lt;strong&gt;unbiased&lt;/strong&gt; estimator of the average treatment effect, $\mathbb E \left[\hat \tau^{IPW} \right] = \tau$.&lt;/p&gt;
&lt;p&gt;This is a very practically relevant result since it tells us that we need to condition on a single variable instead of a potentially infinite dimensional array. The only thing we need to do is to estimate $\pi(X_i)$.&lt;/p&gt;
&lt;h3 id=&#34;comments&#34;&gt;Comments&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Actual vs Estimated Scores&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Hirano and Ridder (2002) show that even when you know the true propensity score $\pi(X)$, it&amp;rsquo;s better to plug in the estimated propensity score $\hat \pi(X)$. Why? The idea is that the deviation between the actual and the estimated propensity score is providing some additional information. Therefore, it is best to use the actual fraction of treated rather than the theoretical one.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Propensity Scores and Regression&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;What is the difference between running a regression with controls vs doing propensity score matching?&lt;/p&gt;
&lt;p&gt;Aranow and Miller (2015) investigate this comparison in depth. First of all, whenever you are inserting &lt;strong&gt;control variables&lt;/strong&gt; in a regression, you are implicitly thinking about propensity scores. Both approaches are implicitly estimating counterfactual outcomes. Usually OLS extrapolates further away from the actual support than propensity score does.&lt;/p&gt;
&lt;p&gt;In the tweet (and its comments) below you can find further discussion and comments.&lt;/p&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;Thank you for tolerating such a vague poll question. &lt;br&gt;&lt;br&gt;Let me explain why I think this is a useful thing to bring front and certain, and highlight what I think is a flaw in how much of econometrics is taught, currently. &lt;br&gt;&lt;br&gt;1/n &lt;a href=&#34;https://t.co/Wm2jFereYO&#34;&gt;pic.twitter.com/Wm2jFereYO&lt;/a&gt;&lt;/p&gt;&amp;mdash; Paul Goldsmith-Pinkham (@paulgp) &lt;a href=&#34;https://twitter.com/paulgp/status/1470787510091632651?ref_src=twsrc%5Etfw&#34;&gt;December 14, 2021&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;

&lt;h2 id=&#34;academic-application&#34;&gt;Academic Application&lt;/h2&gt;
&lt;p&gt;As an academic application, we are going to replicate &lt;a href=&#34;https://www.tandfonline.com/doi/abs/10.1080/01621459.1999.10473858&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Causal Effects in Non-Experimental Studies: Reevaluating the Evaluation of Training Programs&lt;/a&gt; (1999) by Dahejia and Wahba.&lt;/p&gt;
&lt;p&gt;This study builds on a previous study: &lt;a href=&#34;https://www.jstor.org/stable/2937954&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Evaluating the Econometric Evaluations of Training Programs with Experimental Data&lt;/a&gt; (1986) by Lalonde. In this study, the author compares observational and experimental methods. In particular, he studies an experimental intervention called the NSW (National Supported Work demonstration). The NSW is a temporary training program to give work experience to unemployed people.&lt;/p&gt;
&lt;p&gt;The exogenous variation allows us to estimate the treatment effect as a difference in means. The author then asks: what if we didn&amp;rsquo;t have access to an experiment? In particular, what if we did not have information on the control group? He takes a sample of untreated people from the PSID panel and use them as a control group.&lt;/p&gt;
&lt;h3 id=&#34;experimental-data&#34;&gt;Experimental Data&lt;/h3&gt;
&lt;p&gt;Let&amp;rsquo;s start by loading the NSW data.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;%matplotlib inline
%config InlineBackend.figure_format = &#39;retina&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from src.utils import *
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_nsw = pd.read_csv(&#39;data/l86_nsw.csv&#39;)
df_nsw.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;treat&lt;/th&gt;
      &lt;th&gt;age&lt;/th&gt;
      &lt;th&gt;educ&lt;/th&gt;
      &lt;th&gt;black&lt;/th&gt;
      &lt;th&gt;hisp&lt;/th&gt;
      &lt;th&gt;marr&lt;/th&gt;
      &lt;th&gt;nodegree&lt;/th&gt;
      &lt;th&gt;re74&lt;/th&gt;
      &lt;th&gt;re75&lt;/th&gt;
      &lt;th&gt;re78&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;37&lt;/td&gt;
      &lt;td&gt;11&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;9930.045898&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;22&lt;/td&gt;
      &lt;td&gt;9&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;3595.894043&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;30&lt;/td&gt;
      &lt;td&gt;12&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;24909.449219&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;27&lt;/td&gt;
      &lt;td&gt;11&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;7506.145996&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;33&lt;/td&gt;
      &lt;td&gt;8&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;289.789886&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;The treatment variable is &lt;code&gt;treat&lt;/code&gt; and the outcome of interest is &lt;code&gt;re78&lt;/code&gt;, the income in 1978. We also have access to a bunch of covariates.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;y = &#39;re78&#39;
T = &#39;treat&#39;
X = df_nsw.columns[2:9]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Was there selection on observables? Let&amp;rsquo;s summarize the data, according to treatment status.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_nsw.groupby(&#39;treat&#39;).agg([&#39;mean&#39;, &#39;std&#39;]).T.unstack(1)
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead tr th {
    text-align: left;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;treat&lt;/th&gt;
      &lt;th colspan=&#34;2&#34; halign=&#34;left&#34;&gt;0&lt;/th&gt;
      &lt;th colspan=&#34;2&#34; halign=&#34;left&#34;&gt;1&lt;/th&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;mean&lt;/th&gt;
      &lt;th&gt;std&lt;/th&gt;
      &lt;th&gt;mean&lt;/th&gt;
      &lt;th&gt;std&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;age&lt;/th&gt;
      &lt;td&gt;25.053846&lt;/td&gt;
      &lt;td&gt;7.057745&lt;/td&gt;
      &lt;td&gt;25.816216&lt;/td&gt;
      &lt;td&gt;7.155019&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;educ&lt;/th&gt;
      &lt;td&gt;10.088462&lt;/td&gt;
      &lt;td&gt;1.614325&lt;/td&gt;
      &lt;td&gt;10.345946&lt;/td&gt;
      &lt;td&gt;2.010650&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;black&lt;/th&gt;
      &lt;td&gt;0.826923&lt;/td&gt;
      &lt;td&gt;0.379043&lt;/td&gt;
      &lt;td&gt;0.843243&lt;/td&gt;
      &lt;td&gt;0.364558&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;hisp&lt;/th&gt;
      &lt;td&gt;0.107692&lt;/td&gt;
      &lt;td&gt;0.310589&lt;/td&gt;
      &lt;td&gt;0.059459&lt;/td&gt;
      &lt;td&gt;0.237124&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;marr&lt;/th&gt;
      &lt;td&gt;0.153846&lt;/td&gt;
      &lt;td&gt;0.361497&lt;/td&gt;
      &lt;td&gt;0.189189&lt;/td&gt;
      &lt;td&gt;0.392722&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;nodegree&lt;/th&gt;
      &lt;td&gt;0.834615&lt;/td&gt;
      &lt;td&gt;0.372244&lt;/td&gt;
      &lt;td&gt;0.708108&lt;/td&gt;
      &lt;td&gt;0.455867&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;re74&lt;/th&gt;
      &lt;td&gt;2107.026651&lt;/td&gt;
      &lt;td&gt;5687.905639&lt;/td&gt;
      &lt;td&gt;2095.573693&lt;/td&gt;
      &lt;td&gt;4886.620354&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;re75&lt;/th&gt;
      &lt;td&gt;1266.909015&lt;/td&gt;
      &lt;td&gt;3102.982088&lt;/td&gt;
      &lt;td&gt;1532.055313&lt;/td&gt;
      &lt;td&gt;3219.250879&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;re78&lt;/th&gt;
      &lt;td&gt;4554.801120&lt;/td&gt;
      &lt;td&gt;5483.836001&lt;/td&gt;
      &lt;td&gt;6349.143502&lt;/td&gt;
      &lt;td&gt;7867.402183&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;It seems that covariates are balanced across treatment arms. Nothing seems to point towards selection on observables. Therefore, we can compute the average treatment effect as a simple difference in means&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_nsw.loc[df_nsw[T]==1, y].mean() - df_nsw.loc[df_nsw[T]==0, y].mean()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;1794.3423818501024
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Or equivalently in a regression&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;est = smf.ols(&#39;re78 ~ treat&#39;, df_nsw).fit()
est.summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
      &lt;td&gt;&lt;/td&gt;         &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt; &lt;td&gt; 4554.8011&lt;/td&gt; &lt;td&gt;  408.046&lt;/td&gt; &lt;td&gt;   11.162&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt; 3752.855&lt;/td&gt; &lt;td&gt; 5356.747&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;treat&lt;/th&gt;     &lt;td&gt; 1794.3424&lt;/td&gt; &lt;td&gt;  632.853&lt;/td&gt; &lt;td&gt;    2.835&lt;/td&gt; &lt;td&gt; 0.005&lt;/td&gt; &lt;td&gt;  550.574&lt;/td&gt; &lt;td&gt; 3038.110&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;It looks like the effect is positive and significant.&lt;/p&gt;
&lt;h3 id=&#34;observational-data&#34;&gt;Observational Data&lt;/h3&gt;
&lt;p&gt;Let&amp;rsquo;s now load a different dataset in which we have replaced the true control units with observations from the PSID sample.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_psid = pd.read_csv(&#39;data/l86_psid.csv&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Is this dataset balanced?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_psid.groupby(&#39;treat&#39;).agg([&#39;mean&#39;, &#39;std&#39;]).T.unstack(1)
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead tr th {
    text-align: left;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;treat&lt;/th&gt;
      &lt;th colspan=&#34;2&#34; halign=&#34;left&#34;&gt;0&lt;/th&gt;
      &lt;th colspan=&#34;2&#34; halign=&#34;left&#34;&gt;1&lt;/th&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;mean&lt;/th&gt;
      &lt;th&gt;std&lt;/th&gt;
      &lt;th&gt;mean&lt;/th&gt;
      &lt;th&gt;std&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;age&lt;/th&gt;
      &lt;td&gt;36.094862&lt;/td&gt;
      &lt;td&gt;12.081030&lt;/td&gt;
      &lt;td&gt;25.816216&lt;/td&gt;
      &lt;td&gt;7.155019&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;educ&lt;/th&gt;
      &lt;td&gt;10.766798&lt;/td&gt;
      &lt;td&gt;3.176827&lt;/td&gt;
      &lt;td&gt;10.345946&lt;/td&gt;
      &lt;td&gt;2.010650&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;black&lt;/th&gt;
      &lt;td&gt;0.391304&lt;/td&gt;
      &lt;td&gt;0.489010&lt;/td&gt;
      &lt;td&gt;0.843243&lt;/td&gt;
      &lt;td&gt;0.364558&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;hisp&lt;/th&gt;
      &lt;td&gt;0.067194&lt;/td&gt;
      &lt;td&gt;0.250853&lt;/td&gt;
      &lt;td&gt;0.059459&lt;/td&gt;
      &lt;td&gt;0.237124&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;marr&lt;/th&gt;
      &lt;td&gt;0.735178&lt;/td&gt;
      &lt;td&gt;0.442113&lt;/td&gt;
      &lt;td&gt;0.189189&lt;/td&gt;
      &lt;td&gt;0.392722&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;nodegree&lt;/th&gt;
      &lt;td&gt;0.486166&lt;/td&gt;
      &lt;td&gt;0.500799&lt;/td&gt;
      &lt;td&gt;0.708108&lt;/td&gt;
      &lt;td&gt;0.455867&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;re74&lt;/th&gt;
      &lt;td&gt;11027.303390&lt;/td&gt;
      &lt;td&gt;10814.670751&lt;/td&gt;
      &lt;td&gt;2095.573693&lt;/td&gt;
      &lt;td&gt;4886.620354&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;re75&lt;/th&gt;
      &lt;td&gt;7569.222058&lt;/td&gt;
      &lt;td&gt;9041.944403&lt;/td&gt;
      &lt;td&gt;1532.055313&lt;/td&gt;
      &lt;td&gt;3219.250879&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;re78&lt;/th&gt;
      &lt;td&gt;9995.949977&lt;/td&gt;
      &lt;td&gt;11184.450050&lt;/td&gt;
      &lt;td&gt;6349.143502&lt;/td&gt;
      &lt;td&gt;7867.402183&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;People in the PSID control group are older, more educated, white, married and generally have considerably higher pre-intervention earnings (&lt;code&gt;re74&lt;/code&gt;). This makes sense since the people selected for the NSW program are people that are younger, less experienced and unemployed.&lt;/p&gt;
&lt;p&gt;Lalonde (1986) argues in favor of experimental approaches by showing that using a non-experimental setting, one would not be able to estimate the true treatment effect. Actually, one could even get statistically significant results of the opposite sign.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s repeat the regression exercise for the PSID data.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;smf.ols(&#39;re78 ~ treat&#39;, df_psid).fit().summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
      &lt;td&gt;&lt;/td&gt;         &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt; &lt;td&gt; 9995.9500&lt;/td&gt; &lt;td&gt;  623.715&lt;/td&gt; &lt;td&gt;   16.026&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt; 8770.089&lt;/td&gt; &lt;td&gt; 1.12e+04&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;treat&lt;/th&gt;     &lt;td&gt;-3646.8065&lt;/td&gt; &lt;td&gt;  959.704&lt;/td&gt; &lt;td&gt;   -3.800&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;-5533.027&lt;/td&gt; &lt;td&gt;-1760.586&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;The estimated coefficient is negative and significant. The conclusion from Lalonde (1986) is&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;This comparison shows that many of the econometric procedures d not replicate the experimentally determined results&amp;rdquo;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Dahejia and Wahba (1999) argue that with appropriate matching one would still be able to get a relatively precise estimate of the treatment effect. In particular, the argue in favor of controlling for pre-intervention income, &lt;code&gt;re74&lt;/code&gt; and &lt;code&gt;re75&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s just linearly insert the control variables in the regression.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;smf.ols(&#39;re78 ~ treat + &#39; + &#39; + &#39;.join(X), df_psid).fit().summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
      &lt;td&gt;&lt;/td&gt;         &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt; &lt;td&gt;-1089.9263&lt;/td&gt; &lt;td&gt; 2913.224&lt;/td&gt; &lt;td&gt;   -0.374&lt;/td&gt; &lt;td&gt; 0.708&lt;/td&gt; &lt;td&gt;-6815.894&lt;/td&gt; &lt;td&gt; 4636.042&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;treat&lt;/th&gt;     &lt;td&gt; 2642.1456&lt;/td&gt; &lt;td&gt; 1039.655&lt;/td&gt; &lt;td&gt;    2.541&lt;/td&gt; &lt;td&gt; 0.011&lt;/td&gt; &lt;td&gt;  598.694&lt;/td&gt; &lt;td&gt; 4685.597&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;educ&lt;/th&gt;      &lt;td&gt;  521.5869&lt;/td&gt; &lt;td&gt;  208.751&lt;/td&gt; &lt;td&gt;    2.499&lt;/td&gt; &lt;td&gt; 0.013&lt;/td&gt; &lt;td&gt;  111.284&lt;/td&gt; &lt;td&gt;  931.890&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;black&lt;/th&gt;     &lt;td&gt;-1026.6762&lt;/td&gt; &lt;td&gt; 1006.433&lt;/td&gt; &lt;td&gt;   -1.020&lt;/td&gt; &lt;td&gt; 0.308&lt;/td&gt; &lt;td&gt;-3004.830&lt;/td&gt; &lt;td&gt;  951.478&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;hisp&lt;/th&gt;      &lt;td&gt; -903.1023&lt;/td&gt; &lt;td&gt; 1726.419&lt;/td&gt; &lt;td&gt;   -0.523&lt;/td&gt; &lt;td&gt; 0.601&lt;/td&gt; &lt;td&gt;-4296.394&lt;/td&gt; &lt;td&gt; 2490.189&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;marr&lt;/th&gt;      &lt;td&gt; 1026.6143&lt;/td&gt; &lt;td&gt;  943.788&lt;/td&gt; &lt;td&gt;    1.088&lt;/td&gt; &lt;td&gt; 0.277&lt;/td&gt; &lt;td&gt; -828.410&lt;/td&gt; &lt;td&gt; 2881.639&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;nodegree&lt;/th&gt;  &lt;td&gt;-1469.3712&lt;/td&gt; &lt;td&gt; 1166.114&lt;/td&gt; &lt;td&gt;   -1.260&lt;/td&gt; &lt;td&gt; 0.208&lt;/td&gt; &lt;td&gt;-3761.379&lt;/td&gt; &lt;td&gt;  822.637&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;re74&lt;/th&gt;      &lt;td&gt;    0.1928&lt;/td&gt; &lt;td&gt;    0.058&lt;/td&gt; &lt;td&gt;    3.329&lt;/td&gt; &lt;td&gt; 0.001&lt;/td&gt; &lt;td&gt;    0.079&lt;/td&gt; &lt;td&gt;    0.307&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;re75&lt;/th&gt;      &lt;td&gt;    0.4976&lt;/td&gt; &lt;td&gt;    0.070&lt;/td&gt; &lt;td&gt;    7.068&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    0.359&lt;/td&gt; &lt;td&gt;    0.636&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;The treatment effect is now positive, borderline significant, and close to the experimental estimate of $1794$$. Moreover, it&amp;rsquo;s hard to tell whether this is the correct functional form for the control variables.&lt;/p&gt;
&lt;h3 id=&#34;inverse-propensity-score-weighting&#34;&gt;Inverse propensity score weighting&lt;/h3&gt;
&lt;p&gt;Another option is to use &lt;strong&gt;inverse propensity score weighting&lt;/strong&gt;. First, we need to estimate the treatment probability. Let&amp;rsquo;s start with a very simple standard model to predict binary outcomes.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from sklearn.linear_model import LogisticRegressionCV

pi = LogisticRegressionCV().fit(y=df_psid[T], X=df_psid[X])
df_psid[&#39;pscore&#39;] = pi.predict_proba(df_psid[X])[:,1]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;How does the distribution of the propensity scores look like?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sns.histplot(data=df_psid, x=&#39;pscore&#39;, hue=T, bins=20)\
.set(title=&#39;Distribution of propensity scores, PSID data&#39;, xlabel=&#39;&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/propensity_score_41_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;It seems that indeed we predict higher propensity scores for treated people, and viceversa, indicating a strong selection on observable. However, there is also a considerable amount of overlap.&lt;/p&gt;
&lt;p&gt;We can now estimate the treatment effect by weighting by the inverse of the propensity score. First, let&amp;rsquo;s exclude observations with a very extreme predicted score.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_psid1 = df_psid[(df_psid[&#39;pscore&#39;]&amp;lt;0.9) &amp;amp; (df_psid[&#39;pscore&#39;]&amp;gt;0.1)]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we can need to construct the weights.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_psid1[&#39;weight&#39;] = df_psid1[&#39;treat&#39;] / df_psid1[&#39;pscore&#39;] + (1-df_psid1[&#39;treat&#39;]) / (1-df_psid1[&#39;pscore&#39;])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally, we run a weighted regression of income on the treatment program.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;est = smf.wls(&#39;re78 ~ treat&#39;, df_psid1, weights=df_psid1[&#39;weight&#39;]).fit()
est.summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
      &lt;td&gt;&lt;/td&gt;         &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt; &lt;td&gt; 4038.1507&lt;/td&gt; &lt;td&gt;  512.268&lt;/td&gt; &lt;td&gt;    7.883&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt; 3030.227&lt;/td&gt; &lt;td&gt; 5046.074&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;treat&lt;/th&gt;     &lt;td&gt; 2166.8750&lt;/td&gt; &lt;td&gt;  730.660&lt;/td&gt; &lt;td&gt;    2.966&lt;/td&gt; &lt;td&gt; 0.003&lt;/td&gt; &lt;td&gt;  729.250&lt;/td&gt; &lt;td&gt; 3604.500&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;The effect is positive, statistically significant and very close to the experimental estimate of $1794$$.&lt;/p&gt;
&lt;p&gt;What would have been the propensity scores if we had used the NSW experimental sample? If it&amp;rsquo;s a well done experiment with a sufficiently large sample, we would expect the propensity scores to concentrate around the percentage of people treated, $0.41$ in our data.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;pi = LogisticRegressionCV().fit(y=df_nsw[T], X=df_nsw[X])
df_nsw[&#39;pscore&#39;] = pi.predict_proba(df_nsw[X])[:,1]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sns.histplot(data=df_nsw, x=&#39;pscore&#39;, hue=T, bins=20)\
.set(title=&#39;Distribution of propensity scores, NSW data&#39;, xlabel=&#39;&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/propensity_score_50_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Indeed, now the distribution of the p-scores is concentrated around the treatment frequency in the data. Remarkably, the standard deviation is extremely tight.&lt;/p&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://academic.oup.com/biomet/article/70/1/41/240879&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The central role of the propensity score in observational studies for causal effects&lt;/a&gt; (1983) by Rosenbaum and Rubin&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=8gWctYvRzk4&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Propensity Scores&lt;/a&gt; video lecture by Paul Goldsmith-Pinkham (Yale)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=m3Y8heXoDxE&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Propensity Scores&lt;/a&gt; video lecture by Stefan Wager (Stanford)&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Regression Discontinuity</title>
      <link>https://matteocourthoud.github.io/post/regression_discontinuity/</link>
      <pubDate>Fri, 15 Apr 2022 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/post/regression_discontinuity/</guid>
      <description>&lt;p&gt;In this tutorial, we are going to see how to estimate causal effects when treatment assignment is not random, but determined by a &lt;em&gt;forcing variable&lt;/em&gt; such as a test or a requirement. In this case, we can get a local estimate of the treatment effect by comparing units just above and just below the threshold by assuming that there is no sorting/gaming around it.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Requisites&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;For this tutorial, I assume you are familiar with the following concepts:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Rubin&amp;rsquo;s potential outcome framework&lt;/li&gt;
&lt;li&gt;Ordinary least squares regression&lt;/li&gt;
&lt;li&gt;Non-parametric regression&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://matteocourthoud.github.io/post/iv/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Instrumental variables&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Academic Application&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;As an academic application, we are going to replicate &lt;a href=&#34;https://academic.oup.com/qje/article/119/3/807/1938834&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Do voters affect or elect policies? Evidence from the US House&lt;/a&gt; (2004) by Lee, Moretti, Butler. The authors study whether electoral strength has an effect on policies. To identify the effect, they the advantage that is given by incumbency status, and the quasi-exogeneity given by close elections.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Business Case&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;TBD&lt;/p&gt;
&lt;h2 id=&#34;setting&#34;&gt;Setting&lt;/h2&gt;
&lt;p&gt;We assume that for a set of i.i.d. subjects $i = 1, &amp;hellip;, n$ we observed a tuple $(X_i, D_i, Y_i, Z_i)$ comprised of&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;a feature vector $X_i \in \mathbb R^n$&lt;/li&gt;
&lt;li&gt;a treatment assignment $D_i \in \lbrace 0, 1 \rbrace$&lt;/li&gt;
&lt;li&gt;a response $Y_i \in \mathbb R$
&lt;ul&gt;
&lt;li&gt;outcome of interest that depends on both $X_i$ and $D_i$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;a &lt;strong&gt;forcing variable&lt;/strong&gt; $Z_i \in \mathbb R$
&lt;ul&gt;
&lt;li&gt;variable that determines treatment assignment $D_i$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We normalize the forcing variable $Z_i$ such that $Z_i=0$ corresponds to the cutoff for treatment assignment. We will distinguish two cases for the effect of $Z_i$ on $D_i$:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Sharp RD&lt;/strong&gt;: $D_i = (Z_i \geq 0)$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;treatment is exactly determined by the cutoff&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Fuzzy RD&lt;/strong&gt;: $\lim_{z \to 0_{-}} \mathbb E[D_i | Z_i=z] \neq \lim_{z \to 0_{+}} \mathbb E[D_i | Z_i=z]$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;treatment probability changes at the cutoff&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;Assumption 1 : CE smoothness&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Assumption 2: no sorting&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id=&#34;regression-discontinuity&#34;&gt;Regression Discontinuity&lt;/h2&gt;
&lt;p&gt;The key behind regression discontinuity is what is called a &lt;strong&gt;forcing&lt;/strong&gt; variable that determines treatment assignment. Common examples include test scores for university enrollment (you need a certain test score to get access university) or income for some policy eligibility (you need to be below a certain income threshold to be eligible for a subsidy).&lt;/p&gt;
&lt;p&gt;Clearly, in this setting, treatment is not exogenous. However, the &lt;strong&gt;idea&lt;/strong&gt; behind regression discontinuity is that units &lt;em&gt;sufficiently&lt;/em&gt; close to the discontinuity $Z_i=0$ are &lt;em&gt;sufficiently&lt;/em&gt; similar so that we can attribute differences in the outcome $Y_i$ to the treatment $T_i$.&lt;/p&gt;
&lt;p&gt;What does &lt;em&gt;sufficiently&lt;/em&gt; exactly mean?&lt;/p&gt;
&lt;p&gt;In practice, we are assuming a certain degree of &lt;strong&gt;smoothness&lt;/strong&gt; of the conditional expectation function $\mathbb E[D_i | Z_i=z]$. If this assumption holds, we can estimate the &lt;strong&gt;local average treatment effect&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;$$
\tau^{LATE} = \lim_{z \to 0_{+}} \mathbb E[Y_i | Z_i=z] - \lim_{z \to 0_{-}} \mathbb E[Y_i | Z_i=z] = \mathbb E \big[ Y_{i}^{(1)} - Y_{i}^{(0)} | Z_i=0 \big]
$$&lt;/p&gt;
&lt;p&gt;Note that this is the average treatment effect for a very narrow set of individuals: those that are extremely close to the cutoff.&lt;/p&gt;
&lt;h3 id=&#34;data-challenge&#34;&gt;Data Challenge&lt;/h3&gt;
&lt;p&gt;Regression discontinuity design is a particularly &lt;strong&gt;data hungry&lt;/strong&gt; procedure. In fact, we need to&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;have a very good flexible approximation of the conditional expectation of the outcome $Y_i$ at the cutoff $Z_i=0$&lt;/li&gt;
&lt;li&gt;while also accounting for the effect of the forcing variable $Z$ on the outcome $Y$&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;If we knew the functional form of $\mathbb E[Y_i | Z_i]$, it would be easy.&lt;/p&gt;
&lt;h3 id=&#34;mccrary-test&#34;&gt;McCrary Test&lt;/h3&gt;
&lt;h3 id=&#34;regression-kink-design&#34;&gt;Regression Kink Design&lt;/h3&gt;
&lt;h2 id=&#34;academic-application&#34;&gt;Academic Application&lt;/h2&gt;
&lt;p&gt;As an academic application, we are going to replicate &lt;a href=&#34;https://academic.oup.com/qje/article/119/3/807/1938834&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Do voters affect or elect policies? Evidence from the US House&lt;/a&gt; (2004) by Lee, Moretti, Butler. The authors study whether electoral strength has an effect on policies. To identify the effect, they the advantage that is given by incumbency status, and the quasi-exogeneity given by close elections.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;%matplotlib inline
%config InlineBackend.figure_format = &#39;retina&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from src.utils import *
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df = sm.datasets.get_rdataset(&#39;close_elections_lmb&#39;, package=&#39;causaldata&#39;).data
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df = pd.read_csv(&#39;data/l08.csv&#39;)
df.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;state&lt;/th&gt;
      &lt;th&gt;district&lt;/th&gt;
      &lt;th&gt;id&lt;/th&gt;
      &lt;th&gt;score&lt;/th&gt;
      &lt;th&gt;year&lt;/th&gt;
      &lt;th&gt;demvoteshare&lt;/th&gt;
      &lt;th&gt;democrat&lt;/th&gt;
      &lt;th&gt;lagdemocrat&lt;/th&gt;
      &lt;th&gt;lagdemvoteshare&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;64.339996&lt;/td&gt;
      &lt;td&gt;1948&lt;/td&gt;
      &lt;td&gt;0.553026&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0.469256&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;60.279999&lt;/td&gt;
      &lt;td&gt;1948&lt;/td&gt;
      &lt;td&gt;0.553026&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0.469256&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;57.060001&lt;/td&gt;
      &lt;td&gt;1950&lt;/td&gt;
      &lt;td&gt;0.582441&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0.553026&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;6&lt;/td&gt;
      &lt;td&gt;73.830002&lt;/td&gt;
      &lt;td&gt;1950&lt;/td&gt;
      &lt;td&gt;0.582441&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0.553026&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;7&lt;/td&gt;
      &lt;td&gt;42.959999&lt;/td&gt;
      &lt;td&gt;1954&lt;/td&gt;
      &lt;td&gt;0.569626&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0.539680&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;The first thing we would like to inspect, is the distribution of democratic vote shares &lt;code&gt;demvoteshare&lt;/code&gt;, against their lagged values &lt;code&gt;lagdemvoteshare&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sns.scatterplot(df[&#39;lagdemvoteshare&#39;], df[&#39;demvoteshare&#39;])\
.set(title=&#39;Vote share and incumbency status&#39;, xlabel=&#39;Dem Vote Share (t-1)&#39;, ylabel=&#39;Dem Vote Share (t)&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/regression_discontinuity_19_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;The plot is extremely messy. However we can already see some discontinuity at the threshold: it seems that incumbents do not get vote shares below 0.35.&lt;/p&gt;
&lt;p&gt;To have a more transparent representation of the data, we can use a binscatterplot. Binscatterplots are very similar to histograms with a main difference: instead of having a fixed width, they have a fixed number of observations per bin.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from scipy.stats import binned_statistic

def binscatter(x, y, bins=30, area=True, **kwargs):
    y_bins, x_edges, _ = binned_statistic(x, y, statistic=&#39;mean&#39;, bins=bins)
    x_bins = (x_edges[:-1] + x_edges[1:]) / 2
    p = sns.scatterplot(x_bins, y_bins, **kwargs)
    if area:
        y_std, _, _ = binned_statistic(x, y, statistic=&#39;std&#39;, bins=bins)
        plt.fill_between(x_bins, y_bins-y_std, y_bins+y_std, alpha=0.2, color=&#39;C0&#39;)
    return p
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can now plot the average vote share by previous vote share. The shades represent one standard deviation, at the bin level.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;binscatter(df[&#39;lagdemvoteshare&#39;], df[&#39;demvoteshare&#39;], bins=100)\
.set(title=&#39;Vote share and incumbency status&#39;, xlabel=&#39;Dem Vote Share (t-1)&#39;, ylabel=&#39;Dem Vote Share (t)&#39;);
plt.axvline(x=0.5, ls=&amp;quot;:&amp;quot;, color=&#39;C2&#39;);
plt.title(&#39;Vote share and incumbency status&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/regression_discontinuity_23_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Now it seems quite clear that there exist a discontinuity at $0.5$. We can get a first estimate of the local average treatment effect by assuming a linear model and running a linear regression.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;smf.ols(&#39;demvoteshare ~ lagdemvoteshare + (lagdemvoteshare&amp;gt;0.5)&#39;, df).fit().summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
                &lt;td&gt;&lt;/td&gt;                   &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt;                     &lt;td&gt;    0.2173&lt;/td&gt; &lt;td&gt;    0.005&lt;/td&gt; &lt;td&gt;   46.829&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    0.208&lt;/td&gt; &lt;td&gt;    0.226&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;lagdemvoteshare &gt; 0.5[T.True]&lt;/th&gt; &lt;td&gt;    0.0956&lt;/td&gt; &lt;td&gt;    0.003&lt;/td&gt; &lt;td&gt;   33.131&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    0.090&lt;/td&gt; &lt;td&gt;    0.101&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;lagdemvoteshare&lt;/th&gt;               &lt;td&gt;    0.4865&lt;/td&gt; &lt;td&gt;    0.011&lt;/td&gt; &lt;td&gt;   42.539&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    0.464&lt;/td&gt; &lt;td&gt;    0.509&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;The effect is positive and statistically significant. We can also allow the slope of the line to differ on the two sides of the discontinuity.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df = df.sort_values(&#39;lagdemvoteshare&#39;)
model = smf.ols(&#39;demvoteshare ~ lagdemvoteshare * (lagdemvoteshare&amp;gt;0.5)&#39;, df).fit()
model.summary().tables[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;simpletable&#34;&gt;
&lt;tr&gt;
                        &lt;td&gt;&lt;/td&gt;                           &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt;  &lt;th&gt;[0.025&lt;/th&gt;    &lt;th&gt;0.975]&lt;/th&gt;  
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Intercept&lt;/th&gt;                                     &lt;td&gt;    0.2256&lt;/td&gt; &lt;td&gt;    0.007&lt;/td&gt; &lt;td&gt;   34.588&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    0.213&lt;/td&gt; &lt;td&gt;    0.238&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;lagdemvoteshare &gt; 0.5[T.True]&lt;/th&gt;                 &lt;td&gt;    0.0747&lt;/td&gt; &lt;td&gt;    0.012&lt;/td&gt; &lt;td&gt;    6.334&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    0.052&lt;/td&gt; &lt;td&gt;    0.098&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;lagdemvoteshare&lt;/th&gt;                               &lt;td&gt;    0.4653&lt;/td&gt; &lt;td&gt;    0.016&lt;/td&gt; &lt;td&gt;   28.547&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    0.433&lt;/td&gt; &lt;td&gt;    0.497&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;lagdemvoteshare:lagdemvoteshare &gt; 0.5[T.True]&lt;/th&gt; &lt;td&gt;    0.0418&lt;/td&gt; &lt;td&gt;    0.023&lt;/td&gt; &lt;td&gt;    1.827&lt;/td&gt; &lt;td&gt; 0.068&lt;/td&gt; &lt;td&gt;   -0.003&lt;/td&gt; &lt;td&gt;    0.087&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;Let&amp;rsquo;s plot the predicted vote share over the previous graph.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;binscatter(df[&#39;lagdemvoteshare&#39;], df[&#39;demvoteshare&#39;], bins=100, alpha=0.5)\
.set(title=&#39;Vote share and incumbency status&#39;, xlabel=&#39;Dem Vote Share (t-1)&#39;, ylabel=&#39;Dem Vote Share (t)&#39;);
plt.plot(df[&#39;lagdemvoteshare&#39;], model.fittedvalues, color=&#39;C1&#39;)
plt.axvline(x=0.5, ls=&amp;quot;:&amp;quot;, color=&#39;C2&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/regression_discontinuity_29_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Now that we have established a discontinuity at the cutoff, we need to check the RD &lt;strong&gt;assumptions&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;First, is there &lt;strong&gt;sorting&lt;/strong&gt; across the cutoff? In this case, are democratic politicians more or less likely to lose close elections than republicans? We can plot the distribution of (lagged) vote shares and inspect its shape at the cutoff.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sns.histplot(df[&#39;lagdemvoteshare&#39;], bins=100)\
.set(title=&#39;Distribution of lagged dem vote share&#39;, xlabel=&#39;&#39;)
plt.axvline(x=0.5, ls=&amp;quot;:&amp;quot;, color=&#39;C2&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/regression_discontinuity_31_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;If looks pretty smooth. If anything, there is a loss of density at the cutoff, plausibly indicating stronger competition when the competition is close. However, if does not seem particularly asymmetric.&lt;/p&gt;
&lt;p&gt;A &lt;strong&gt;placebo&lt;/strong&gt; test that we can run is to check if the forcing variable has an effect on variables on which we do not expect to have an effect. In this setting, the most intuitive placebo outcome is previous elections: we do not expect that being on either side of the cutoff today is related to any past outcome.&lt;/p&gt;
&lt;p&gt;In our case, we can simply swap the two variables to run the test.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;binscatter(df[&#39;demvoteshare&#39;], df[&#39;lagdemvoteshare&#39;], bins=100)\
.set(title=&#39;Vote share and incumbency status&#39;, xlabel=&#39;Dem Vote Share (t)&#39;, ylabel=&#39;Dem Vote Share (t-1)&#39;);
plt.axvline(x=0.5, ls=&amp;quot;:&amp;quot;, color=&#39;C2&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;img/regression_discontinuity_33_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;The distribution of vote shares in the past period does not seem to be discontinuous in the incumbency status today, as expected.&lt;/p&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=72KFY8beH0w&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Regression discontinuity video lecture by Paul Goldsmith-Pinkham (Yale)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Free Courses in Economics</title>
      <link>https://matteocourthoud.github.io/post/courses/</link>
      <pubDate>Thu, 10 Feb 2022 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/post/courses/</guid>
      <description>&lt;p&gt;In this page, I collect lectures and materials for graduate courses in Economics and Social Sciences.&lt;/p&gt;
&lt;p&gt;I will only link to lectures and materials that are freely available. I will not link to courses hosted on MOOC websites or that require university credentials to access.&lt;/p&gt;
&lt;p&gt;A special mention goes to the following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The &lt;strong&gt;NBER&lt;/strong&gt; that during each Summer Institute has a &lt;a href=&#34;https://www.nber.org/research/lectures&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;lecture series&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;The &lt;strong&gt;Chamberlain Seminar&lt;/strong&gt; that since 2021 started hosting and recording &lt;a href=&#34;https://www.chamberlainseminar.org/past-seminars&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;tutorial sessions&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;video-lectures&#34;&gt;Video Lectures&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Course Title&lt;/th&gt;
&lt;th&gt;Author&lt;/th&gt;
&lt;th&gt;University&lt;/th&gt;
&lt;th&gt;Year&lt;/th&gt;
&lt;th&gt;Material&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://www.youtube.com/playlist?list=PLxq_lXOUlvQAoWZEqhRqHNezS30lI49G-&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Machine Learning and Causal Inference&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Susan Athey et al.&lt;/td&gt;
&lt;td&gt;Stanford&lt;/td&gt;
&lt;td&gt;2022&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://www.gsb.stanford.edu/faculty-research/centers-initiatives/sil/research/methods/ai-machine-learning/short-course&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Yes&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://chrisconlon.github.io/gradio.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Industrial Organization&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Chris Conlon&lt;/td&gt;
&lt;td&gt;NYU&lt;/td&gt;
&lt;td&gt;2021&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://chrisconlon.github.io/gradio.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Yes&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://chrisconlon.github.io/metrics.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Panel Data Econometrics&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Chris Conlon&lt;/td&gt;
&lt;td&gt;NYU&lt;/td&gt;
&lt;td&gt;2021&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://chrisconlon.github.io/metrics.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Yes&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://www.youtube.com/playlist?list=PLoROMvodv4rPLKxIpqhjhPgdQy7imNkDn&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Machine Learning with Graphs&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Yure Leskovec&lt;/td&gt;
&lt;td&gt;Stanford&lt;/td&gt;
&lt;td&gt;2021&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;http://web.stanford.edu/class/cs224w/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Yes&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://www.youtube.com/playlist?list=PLWWcL1M3lLlojLTSVf2gGYQ_9TlPyPbiJ&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Applied Methods&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Paul Goldsmith-Pinkham&lt;/td&gt;
&lt;td&gt;Yale&lt;/td&gt;
&lt;td&gt;2021&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://github.com/paulgp/applied-methods-phd&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Yes&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://taylorjwright.github.io/did-reading-group/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;DiD Reading Group&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;misc&lt;/td&gt;
&lt;td&gt;misc&lt;/td&gt;
&lt;td&gt;2021&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://taylorjwright.github.io/did-reading-group/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Yes&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://vimeo.com/user108848900&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Computational Economics&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Kenneth Judd&lt;/td&gt;
&lt;td&gt;Stanford&lt;/td&gt;
&lt;td&gt;2020&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://github.com/KennethJudd/CompEcon2020&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Yes&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://www.youtube.com/playlist?list=PLoROMvodv4rOSOPzutgyCTapiGlY2Nd8u&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Reinforcement Learning&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Emma Brunskill&lt;/td&gt;
&lt;td&gt;Stanford&lt;/td&gt;
&lt;td&gt;2020&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;http://web.stanford.edu/class/cs234/index.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Yes&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://www.youtube.com/playlist?list=PLoROMvodv4rObpMCir6rNNUlFAn56Js20&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Natural Language Understanding&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Christopher Potts&lt;/td&gt;
&lt;td&gt;Stanford&lt;/td&gt;
&lt;td&gt;2019&lt;/td&gt;
&lt;td&gt;No&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;material&#34;&gt;Material&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Course Title&lt;/th&gt;
&lt;th&gt;Author&lt;/th&gt;
&lt;th&gt;University&lt;/th&gt;
&lt;th&gt;Year&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://floswald.github.io/NumericalMethods/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Computational Economics&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://floswald.github.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Florial Oswald&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Bocconi&lt;/td&gt;
&lt;td&gt;2021&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://github.com/uo-ec607/lectures&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Data Science for Economists&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://grantmcdermott.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Grant McDermott&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Oregon&lt;/td&gt;
&lt;td&gt;2020&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;http://www.johnasker.com/IO.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Industrial Organization&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;John Asker&lt;/td&gt;
&lt;td&gt;UCLA&lt;/td&gt;
&lt;td&gt;2020&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://kohei-kawaguchi.github.io/EmpiricalIO/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Topics in Empirical Industrial Organization&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Kohei Kawaguchi&lt;/td&gt;
&lt;td&gt;Hong Kong&lt;/td&gt;
&lt;td&gt;2020&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;http://individual.utoronto.ca/vaguirre/courses/eco2901/teaching_io_toronto.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Industrial Organization&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Victor Aguirregabiria&lt;/td&gt;
&lt;td&gt;Toronto&lt;/td&gt;
&lt;td&gt;2019&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://github.com/OU-PhD-Econometrics/fall-2020&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Econometrics&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Tyler Ransom&lt;/td&gt;
&lt;td&gt;Oklahoma&lt;/td&gt;
&lt;td&gt;2020&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://github.com/MartinSpindler/Machine-Learning-in-Econometrics&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Machine Learning in Econometrics&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Martin Spindler&lt;/td&gt;
&lt;td&gt;Munich&lt;/td&gt;
&lt;td&gt;2020&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;http://comlabgames.com/structuraleconometrics/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Structural Econometrics&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Robert Miller&lt;/td&gt;
&lt;td&gt;Carnegie Mellon&lt;/td&gt;
&lt;td&gt;2019&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
</description>
    </item>
    
    <item>
      <title>Economics Conferences</title>
      <link>https://matteocourthoud.github.io/post/conferences/</link>
      <pubDate>Mon, 10 Jan 2022 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/post/conferences/</guid>
      <description>&lt;p&gt;In this page, I collect information about conferences in Economics and Finance.&lt;/p&gt;
&lt;p&gt;If you know about public conferences or meetings that are missing from this list, please either &lt;a href=&#34;mailto:matteo.courthoud@econ.uzh.ch&#34;&gt;contact me&lt;/a&gt; or &lt;a href=&#34;https://github.com/matteocourthoud/website/blob/master/content/post/conferences/index.md&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;edit the table on Github&lt;/a&gt;!&lt;/p&gt;
&lt;p&gt;Note that conferences are ordered by deadline and not by conference date.&lt;/p&gt;
&lt;h2 id=&#34;january&#34;&gt;January&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Conference&lt;/th&gt;
&lt;th&gt;Organizer&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Field&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Deadline&lt;/th&gt;
&lt;th&gt;Target&lt;/th&gt;
&lt;th&gt;Date&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://www.sgvs.ch/conferences/sses2022&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Annual Congress of the Swiss Society of Economics and Statistics&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;SSES&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;All&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://www.sgvs.ch/conferences/sses2022/call_for_papers&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;January 31&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;23/06/21&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;february&#34;&gt;February&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Conference&lt;/th&gt;
&lt;th&gt;Organizer&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Field&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Deadline&lt;/th&gt;
&lt;th&gt;Target&lt;/th&gt;
&lt;th&gt;Date&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://beccle.no/event/bergen-competition-policy-conference-2020/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Bergen Competition Policy Conference&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;NHH&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Comp policy&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://beccle.no/event/bergen-competition-policy-conference-2020/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;February 03&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;23/04/20&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://cepr.org/6754/cfp-mainconference&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CEPR/JIE Conference on Applied IO&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;CEPR&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;IO&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://matteocourthoud.github.io/post/conferences/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;February 10&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Junior&lt;/td&gt;
&lt;td&gt;08/06/21&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;http://ec22.sigecom.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Economics and Computation&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;ACM SIGecom&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Theory&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://ec22.sigecom.org/call-for-contributions-acm/papers/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;February 10&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;11/07/21&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://www.econometricsociety.org/meetings/schedule/2022/06/16/2022-north-america-summer-meeting&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ES North American Summer Meeting&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Econometric Society&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;All&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://www.econometricsociety.org/meetings/schedule/2022/06/16/2022-north-america-summer-meeting&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;February 12&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Senior&lt;/td&gt;
&lt;td&gt;16/06/21&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://www.eea-esem-congresses.org/index.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;EEA Summer Meeting&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;European Economic Association&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;All&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://www.eea-esem-congresses.org/important-dates.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;February 15&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;23/08/21&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://www.eea-esem-congresses.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ES European Summer Meeting&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Econometric Society&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;All&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://www.eea-esem-congresses.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;February 15&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Senior&lt;/td&gt;
&lt;td&gt;23/08/21&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://www.economicdynamics.org/sedam_2021/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Annual Meeting of the Society for Economic Dynamics&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;SED&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Macro&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://www.economicdynamics.org/sedam_2021/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;February 15&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;01/07/21&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://games2020.hu/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GAMES 2020&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Game Theory Society&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Game Theory&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://games2020.hu/registration/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;February 20&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;19/07/21&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://www.nottingham.ac.uk/gep/news-events/conferences/2020-21/postgrad-conference-2021.aspx&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Annual GEP/CEPR Postgraduate Conference&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;University of Nottingham&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Policy&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://www.nottingham.ac.uk/gep/documents/conferences/2020-21/pg-conf-cfp-2021.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;February 26&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Junior&lt;/td&gt;
&lt;td&gt;06/05/21&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://www.digital-economics.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Doctoral Workshop on the Economics of Digitization&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;TSE&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Digitalization&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://www.digital-economics.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;February 28&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Junior&lt;/td&gt;
&lt;td&gt;12/05/22&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;march&#34;&gt;March&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Conference&lt;/th&gt;
&lt;th&gt;Organizer&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Field&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Deadline&lt;/th&gt;
&lt;th&gt;Target&lt;/th&gt;
&lt;th&gt;Date&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://cssh.northeastern.edu/economics/iioc/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Annual IIOC&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Northeastern University&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;IO&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://cssh.northeastern.edu/economics/iioc/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;March 01&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;30/04/21&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://yem2020.econ.muni.cz/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Young Economists&#39; Meeting&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;University fo Munich&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;All&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://yem2020.econ.muni.cz/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;March 08&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Junior&lt;/td&gt;
&lt;td&gt;01/10/20&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://www.barcelonagse.eu/summer-forum&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GSE Summer Forum&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Barcelona GSE&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;All&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://www.barcelonagse.eu/summer-forum&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;March 14&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;07/06/21&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://www.nhh.no/en/calendar/conferences/2021/earie-2021/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;EARIE&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;NHH&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;IO&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://www.nhh.no/en/calendar/conferences/2021/earie-2021/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;March 15&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;27/08/21&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://www.sioe.org/news/economics-media-workshop-call-paper-poster-presentations&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Economics of Media Workshop&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Queenâs University&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;IO&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://www.sioe.org/news/economics-media-workshop-call-paper-poster-presentations&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;March 15&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Junior&lt;/td&gt;
&lt;td&gt;12/06/20&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://www.qmul.ac.uk/sef/events/conferences/items/3rd-qmul-economics-and-finance-workshop-for-phd--post-doctoral-students.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;QMUL Economics and Finance Workshop&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Queen Mary University&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;All&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://econ.columbia.edu/call-for-papers-3rd-qm-phd-workshop/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;March 20&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Junior&lt;/td&gt;
&lt;td&gt;26/05/20&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://static1.squarespace.com/static/56086d00e4b0fb7874bc2d42/t/5e753140c2225859fa93ba1e/1584738624656/callforpapers.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Virtual Finance and Economics Conference&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Yale University&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Finecon&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://static1.squarespace.com/static/56086d00e4b0fb7874bc2d42/t/5e753140c2225859fa93ba1e/1584738624656/callforpapers.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;March 25&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;17/04/20&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://sites.google.com/view/dc-io-day&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;DC IO Day 2020&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Georgetown University&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;IO&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://sites.google.com/view/dc-io-day&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;March 31&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;15/05/20&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;april&#34;&gt;April&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Conference&lt;/th&gt;
&lt;th&gt;Organizer&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Field&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Deadline&lt;/th&gt;
&lt;th&gt;Target&lt;/th&gt;
&lt;th&gt;Date&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://economics.stanford.edu/site/site-2021&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;SITE&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Stanford University&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Theory&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://economics.stanford.edu/site/site-2021&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;April 01&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;12/06/21&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://www.nber.org/conferences/summer-institute&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;NBER Summer Meeting Workshop&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;NBER&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;All&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://www.nber.org/conferences/summer-institute&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;April 05&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;12/07/21&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://www.aeaweb.org/conference/submissions&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;AEA Annual Meeting&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;AEA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;All&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://www.aeaweb.org/conference/submissions&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;April 15&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;07/01/22&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://matteocourthoud.github.io/post/conferences/files/swissIOday2021_CallForPapers.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Swiss IO Day&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;University of Bern&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;IO&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://matteocourthoud.github.io/post/conferences/files/swissIOday2021_CallForPapers.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;April 16&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;11/06/21&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://www.econometricsociety.org/meetings/schedule/2022/01/06/2022-north-american-winter-meeting&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Econometric Society - North American Winter Meetings&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;ES&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;All&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://www.econometricsociety.org/meetings/schedule/2022/01/06/2022-north-american-winter-meeting&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;April 21&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;06/01/22&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://www.cresse.info/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CRESSE&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;CRESSE&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;IO&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://www.cresse.info/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;April 30&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;26/07/21&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;may&#34;&gt;May&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Conference&lt;/th&gt;
&lt;th&gt;Organizer&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Field&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Deadline&lt;/th&gt;
&lt;th&gt;Target&lt;/th&gt;
&lt;th&gt;Date&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://mailchi.mp/cepr/european-research-workshop-in-international-trade-erwit-506353&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;European Research Workshop in International Trade&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;CEPR&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Trade&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://mailchi.mp/cepr/european-research-workshop-in-international-trade-erwit-506353&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;May 02&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;22/10/21&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;http://coin.wne.uw.edu.pl/wiem/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Warsaw International Economic Meeting&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Warsaw University&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;All&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;http://coin.wne.uw.edu.pl/wiem/wiem2020-cfp.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;May 03&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;01/07/20&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://warwick.ac.uk/fac/soc/economics/events/2021/6/economics_phd_conference/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Warwick Economics PhD Conference&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;University of Warwick&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;All&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://warwick.ac.uk/fac/soc/economics/events/2021/6/economics_phd_conference/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;May 09&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;PhD&lt;/td&gt;
&lt;td&gt;24/06/21&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://www.law.northwestern.edu/research-faculty/clbe/events/antitrust/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Annual Conference on Antitrust Economics and Competition Policy&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Northwestern University&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Comp policy&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://www.law.northwestern.edu/research-faculty/clbe/events/antitrust/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;May 17&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;17/09/21&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://www.economicsofai.com/blog/2021/2/2/2021-nber-economics-of-ai-conference-call-for-papers&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;NBER Economics of AI Conference&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;NBER&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;AI&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://www.economicsofai.com/blog/2021/2/2/2021-nber-economics-of-ai-conference-call-for-papers&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;May 31&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;23/09/21&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;june&#34;&gt;June&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Conference&lt;/th&gt;
&lt;th&gt;Organizer&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Field&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Deadline&lt;/th&gt;
&lt;th&gt;Target&lt;/th&gt;
&lt;th&gt;Date&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://eaamo.org/cfp/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Equity and Access in Algorithms, Mechanisms, and Optimization (EAAMO)&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;ACM&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;AI&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://eaamo.org/cfp/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;June 14&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;05/10/21&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://www.ftc.gov/news-events/events-calendar/fourteenth-annual-federal-trade-commission-microeconomics-conference&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;FTC Micro Conference&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;FTC&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;IO&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://www.ftc.gov/system/files/documents/public_events/1588356/20210326_-_micro_conf_call_for_papers.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;June 23&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Senior&lt;/td&gt;
&lt;td&gt;04/11/21&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;july&#34;&gt;July&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Conference&lt;/th&gt;
&lt;th&gt;Organizer&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Field&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Deadline&lt;/th&gt;
&lt;th&gt;Target&lt;/th&gt;
&lt;th&gt;Date&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://www.emconference.net/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Empirics and Methods in Economics Conference&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Northwestern &amp;amp; Chicago&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Empirical&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://www.emconference.net/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;July 30&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Junior&lt;/td&gt;
&lt;td&gt;22/10/20&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;august&#34;&gt;August&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Conference&lt;/th&gt;
&lt;th&gt;Organizer&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Field&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Deadline&lt;/th&gt;
&lt;th&gt;Target&lt;/th&gt;
&lt;th&gt;Date&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://docs.google.com/document/d/1BjHI_6HyL7pk2UYUNDlDY971B9AO83wD8DfEF6KNDfs/edit&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;AI Policy Conference&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;ETH Zurich&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;AI&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://docs.google.com/document/d/1BjHI_6HyL7pk2UYUNDlDY971B9AO83wD8DfEF6KNDfs/edit&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;August 1&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;14/09/21&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://sites.google.com/site/uscfom/finance-organizations-and-markets-fom-research-group&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Finance, Organizations and Markets (FOM) Conference&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Dartmouth College&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Finance, IO&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://sites.google.com/site/uscfom/finance-organizations-and-markets-fom-research-group&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;August 14&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;28/10/21&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;september&#34;&gt;September&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Conference&lt;/th&gt;
&lt;th&gt;Organizer&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Field&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Deadline&lt;/th&gt;
&lt;th&gt;Target&lt;/th&gt;
&lt;th&gt;Date&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://why21.causalai.net/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Causal Inference &amp;amp; Machine Learning: Why now?&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;NeurIPS&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Econometrics&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://why21.causalai.net/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;September 18&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;13/12/21&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://www.ub.edu/school-economics/ewmes&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ES European Winter Meeting&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Econometricc Society&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;All&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://www.ub.edu/school-economics/ewmes&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;September 19&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Senior&lt;/td&gt;
&lt;td&gt;13/12/21&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://www.causalscience.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Causal Data Science Conference&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;causalscience&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;All&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://www.causalscience.org/blog/causal-data-science-meeting-2021-call-for-papers/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;September 30&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;15/11/21&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;october&#34;&gt;October&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Conference&lt;/th&gt;
&lt;th&gt;Organizer&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Field&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Deadline&lt;/th&gt;
&lt;th&gt;Target&lt;/th&gt;
&lt;th&gt;Date&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://www.tse-fr.eu/conferences/2022-15th-digital-economics-conference&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Digital Economics Conference&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;TSE&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Digital&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://www.tse-fr.eu/sites/default/files/TSE/documents/conf/2022/Digital_Economics/call_for_papers_digital_conf_2022.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;October 3&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;13/01/22&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://apios.org.au/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Asia-Pacific IO Conference&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Asia-Pacific IO Society&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;IO&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://apios.org.au/submission/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;October 22&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;13/12/21&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://www.sgvs.ch/conferences/ysem2021&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Young Swiss Economists Meeting&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;SSES&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;All&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://www.sgvs.ch/files/Call_for_Papers_YSEM_2021.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;October 25&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Junior&lt;/td&gt;
&lt;td&gt;11/02/21&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;november&#34;&gt;November&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Conference&lt;/th&gt;
&lt;th&gt;Organizer&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Field&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Deadline&lt;/th&gt;
&lt;th&gt;Target&lt;/th&gt;
&lt;th&gt;Date&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://www.law.nyu.edu/conferences/2022-NextGen-Antitrust-Conference&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Next Generation of Antitrust, Data Privacy and Data Protection Scholars Conference&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;NYU&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;IO&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://www.law.nyu.edu/conferences/2022-NextGen-Antitrust-Conference&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;November 1&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;28/01/22&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://smye2021.weebly.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Spring Meeting of Young Economists&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;University of Bologna&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;All&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://smye2021.weebly.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;November 12&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Junior&lt;/td&gt;
&lt;td&gt;17/06/21&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://www.nber.org/conferences/industrial-organization-program-meeting-spring-2021&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;NBER IO Winter Meeting&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;NBER&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;IO&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://conference.nber.org/confsubmit/backend/cfp?id=IOs21&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;November 20&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Senior&lt;/td&gt;
&lt;td&gt;12/02/21&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://www.zew.de/en/events-and-professional-training/detail/2021-macci-annual-conference/3320&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;MaCCI Annual Conference&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;University of Mannheim&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;IO&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://www.zew.de/en/events-and-professional-training/detail/2021-macci-annual-conference/3320&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;November 30&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;12/03/21&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://www.tse-fr.eu/sites/default/files/TSE/documents/conf/2022/postal/call_postal_2022_v2.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Postal Economics Conference&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;TSE&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Digital&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://www.tse-fr.eu/sites/default/files/TSE/documents/conf/2022/postal/call_postal_2022_v2.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;November 30&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;07/04/22&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;december&#34;&gt;December&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Conference&lt;/th&gt;
&lt;th&gt;Organizer&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Field&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Deadline&lt;/th&gt;
&lt;th&gt;Target&lt;/th&gt;
&lt;th&gt;Date&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://sites.google.com/site/ecbeconference&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Early-Career Behavioral Economics Conference&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Princeton University&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Behavioral&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://sites.google.com/site/ecbeconference/call&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;December 15&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;junior&lt;/td&gt;
&lt;td&gt;03/06/21&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;undefined&#34;&gt;Undefined&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Conference&lt;/th&gt;
&lt;th&gt;Organizer&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Field&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Deadline&lt;/th&gt;
&lt;th&gt;Target&lt;/th&gt;
&lt;th&gt;Date&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://www.law.northwestern.edu/research-faculty/clbe/events/innovation/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Annual Conference on Innovation Economics&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Northwestern University&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Innovation&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://www.law.northwestern.edu/research-faculty/clbe/callforpapers/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;forthcoming&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;27/08/20&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://conference2.aau.at/event/4/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Conference on Mechanism and Institution Design&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;UniversitÃ¤t Klagenfurt&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Market Design&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;closed&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;11/06/20&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://sites.google.com/site/dteaworkshop/home&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;D-TEA Conference&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;HEC Paris&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Theory&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;closed&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;16/06/20&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://sites.wustl.edu/egsc/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Economics Graduate Student Conference&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Washington University&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;All&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;closed&lt;/td&gt;
&lt;td&gt;Junior&lt;/td&gt;
&lt;td&gt;07/11/20&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;http://conference.nber.org/confer/2020/SI2020/SI2020.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;NBER Summer Institute&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;NBER&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;All&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;invitation&lt;/td&gt;
&lt;td&gt;Senior&lt;/td&gt;
&lt;td&gt;06/07/20&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;http://competitionpolicy.ac.uk/events/annual-conferences&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CCP Annual Conference&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Centre for Competition Policy&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Comp policy&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;closed&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;24/06/21&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://www.res.org.uk/event-listing/2021-annual-conference.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;RES Annual Conference&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Royal Economics Society&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;All&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;closed&lt;/td&gt;
&lt;td&gt;Senior&lt;/td&gt;
&lt;td&gt;12/04/21&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://www.emerginginvestigators.org/conference&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;JEI Student Conference&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Harvard University&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;All&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;canceled&lt;/td&gt;
&lt;td&gt;Junior&lt;/td&gt;
&lt;td&gt;20/06/20&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://bfi.uchicago.edu/event/sixth-annual-conference-on-network-science-and-economics/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Annual Conference on Network Science and Economics&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Becker Friedman Institute&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Networks&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;canceled&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;27/03/20&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://saet.uiowa.edu/2021-annual-saet-conference/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Annual SAET Conference&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Society for the Advancement of Economic Theory&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Theory&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;closed&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;13/06/21&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
</description>
    </item>
    
    <item>
      <title>How to Work on a Remote Machine via SSH</title>
      <link>https://matteocourthoud.github.io/post/ssh/</link>
      <pubDate>Mon, 06 Dec 2021 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/post/ssh/</guid>
      <description>&lt;p&gt;Welcome to my tutorial on how to set up a remote machine and deploy your code there. I will first analyze SSH and then look at two specific applications: coding in Python and Julia.&lt;/p&gt;
&lt;h2 id=&#34;setup&#34;&gt;Setup&lt;/h2&gt;
&lt;p&gt;In order to start working on a remote server you need&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the server&lt;/li&gt;
&lt;li&gt;local shell&lt;/li&gt;
&lt;li&gt;SSH installed&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;SSH, or Secure Shell, is a protocol designed to transfer data between a client and a server (two computers basically) over an untrusted network.&lt;/p&gt;
&lt;p&gt;The way SSH works is it encrypts the connection using a pair of keys and the server, which is the computer you would connect to, is usually waiting for an SSH connection on Port 22.&lt;/p&gt;
&lt;p&gt;SSH is normally installed by default. To check if you have SSH installed, open the terminal and write &lt;code&gt;ssh&lt;/code&gt;. You should receive a message that looks like this&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;usage: ssh [-1246AaCfGgKkMNnqsTtVvXxYy] [-b bind_address] [-c cipher_spec]
[-D [bind_address:]port] [-E log_file] [-e escape_char]
[-F configfile] [-I pkcs11] [-i identity_file]
[-J [user@]host[:port]] [-L address] [-l login_name] [-m mac_spec] [-O ctl_cmd] [-o option] [-p port] [-Q query_option] [-R address] [-S ctl_path] [-W host:port] [-w local_tun[:remote_tun]]
[user@]hostname [command]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If SSH is not installed, you can install it using the following commands.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;sudo apt-get install openssh-server
sudo systemctl enable ssh
sudo systemctl start ssh
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now that you have installed SSH, we are ready to setup a remote connection.&lt;/p&gt;
&lt;p&gt;From the computer you want to access remotey, generate the public key.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;ssh-keygen -t rsa
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You will be asked for a location. If you decide to enter one manually then that will be the pairâs location, if you leave the default one it will be inside the &lt;code&gt;.ssh&lt;/code&gt; hidden folder in your home directory.&lt;/p&gt;
&lt;p&gt;Now you will be prompted for a password. If you enter one you will be asked for it every time you use the key, this works for added security. If you donât want a password just press enter and continue without one.&lt;/p&gt;
&lt;p&gt;Two files were created. One file ends with the â.pubâ extension and the other one doesnât. The file that ends with â.pubâ is your public key. This key needs to be in the computer you want to connect to (the server) inside a file called &lt;code&gt;authorized_keys&lt;/code&gt; . You can accomplish this with the following command:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;ssh-copy-id username@ip
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For example in my case to send the key to my computer it would be:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;ssh-copy-id sergiop@132.132.132.132
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you have MacOS thereâs a chance you donât have ssh-copy-id installed, in that case you can install it using&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;brew install ssh-copy-id
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you havenât installed &lt;code&gt;brew&lt;/code&gt;, you can install it by following &lt;a href=&#34;https://brew.sh/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;this guide&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;connect&#34;&gt;Connect&lt;/h2&gt;
&lt;p&gt;To permanently add the SSH key, you can use the follwing command&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;ssh-add directory\key.pem
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Lastly, to connect, just type the following command.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;ssh username@ip
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Where &lt;code&gt;username&lt;/code&gt; is the server name and &lt;code&gt;ip&lt;/code&gt; is the public IP adress, e.g. 132.132.132.132.&lt;/p&gt;
&lt;p&gt;If your server is not public, you will not be able to access it.&lt;/p&gt;
&lt;p&gt;If your server is password protected, you will be prompted to insert a password when you connect. If not, you should protect it with a password.&lt;/p&gt;
&lt;h2 id=&#34;managing-screens&#34;&gt;Managing screens&lt;/h2&gt;
&lt;p&gt;While you are connected to the remote terminal, any disturbance to your connection will interrupt the code. In order to avoid that, you want to create separate screens. This will allow your code to run remotely undisturbed, irrespectively of your connection.&lt;/p&gt;
&lt;p&gt;First, you need to install &lt;code&gt;screen&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;brew install screen
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To create a new screen, just type&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;screen
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now you can lunch your code.&lt;/p&gt;
&lt;p&gt;After that, you want to detach from that screen so that the code can run remotely undisturbed.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;screen -d
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Another option is to use &lt;code&gt;ctrl+a&lt;/code&gt; followed by &lt;code&gt;ctrl+d&lt;/code&gt;. This will detach the screen without the need to type anythin in the terminal, in case the terminal is busy (most likely).&lt;/p&gt;
&lt;p&gt;To list the current active screens type&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;screen -ls
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you want to check at any time that your code is running, without re-attaching to the screen, you can just type&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;top
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;which is the general command to check active processes. To exit, use &lt;code&gt;ctrl+z&lt;/code&gt;, which generally terminates processes in the terminal.&lt;/p&gt;
&lt;p&gt;To reattach to your screen, type&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;screen -r
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In case you have multiple screens (you can check with &lt;code&gt;screen -ls&lt;/code&gt;), you can reattach to a specific one by typing&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;screen -r 12345
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;where &lt;code&gt;12345&lt;/code&gt; is the id of the screen.&lt;/p&gt;
&lt;p&gt;To kill a screen, type&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;screen -XS 12345 quit
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;where again &lt;code&gt;12345&lt;/code&gt; is the id of the screen.&lt;/p&gt;
&lt;h2 id=&#34;python-and-pycharm&#34;&gt;Python and Pycharm&lt;/h2&gt;
&lt;p&gt;If you are coding in Python, &lt;a href=&#34;https://www.jetbrains.com/pycharm/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PyCharm&lt;/a&gt; is one of the best IDEs. Among many features, it offers the possibility to set a remote compiler for your pthon console and to sync input and output files automatically.&lt;/p&gt;
&lt;p&gt;First, you need to have setup a remote SSH connection following the steps above. Importantly, you need to have added the public key to your machine using the &lt;code&gt;ssh-add&lt;/code&gt; command, as explained above.&lt;/p&gt;
&lt;p&gt;Then open Pytharm, go to the lower-right corner, where the current interpreter is listed (e.g. Pytohn 3.8), click it and select &lt;code&gt;interpreter settings&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;img/interpreter_settings.png&#34; alt=&#34;interpreter_settings&#34;&gt;&lt;/p&gt;
&lt;p&gt;Click on the gear icon âï¸ on the top-right corner and select &lt;code&gt;add&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;img/add.png&#34; alt=&#34;add&#34;&gt;&lt;/p&gt;
&lt;p&gt;Insert the server &lt;code&gt;host&lt;/code&gt; (IP address, e.g. 132.132.132.132) and &lt;code&gt;username&lt;/code&gt; (e.g. sergiop).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;img/configuration.png&#34; alt=&#34;configuration&#34;&gt;&lt;/p&gt;
&lt;p&gt;Next, you have to insert your credentials. If you have a password, insert it, otherwise you have to insert the path to your SSH key file.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;img/password.png&#34; alt=&#34;configuration&#34;&gt;&lt;/p&gt;
&lt;p&gt;Lastly, select the remote interpreter. If you are using a python version that is not default, browse to the preferred python installation folder. Also, check the box for &lt;code&gt;execute code giving this interpreter with root privileges via sudo&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;You can also select which remote folder to sync with your local project. By default, you are given a &lt;code&gt;tmp/pycharm_project_XX&lt;/code&gt; folder. You can change it if you want. I recommend also to have the last option checked: &lt;code&gt;automatically sync project files to the server&lt;/code&gt;. This will automatically synch all remote changes with your local machine, in your local project folder.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;img/folder.png&#34; alt=&#34;configuration&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;julia-and-juno&#34;&gt;Julia and Juno&lt;/h2&gt;
&lt;p&gt;If you are coding in Julia, &lt;a href=&#34;https://junolab.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Juno&lt;/a&gt; is the best IDE around. Itâs an integration with &lt;a href=&#34;https://atom.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Atom&lt;/a&gt; with a dedicated compiler, local variables, syntax highlight, autocompletion.&lt;/p&gt;
&lt;p&gt;On Atom, you first need to install the &lt;a href=&#34;https://github.com/h3imdall/ftp-remote-edit&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;ftp-remote-edit&lt;/code&gt;&lt;/a&gt; package.&lt;/p&gt;
&lt;p&gt;Then go to the menu item &lt;code&gt;Packages &amp;gt; Ftp-Remote-Edit &amp;gt; Toggle&lt;/code&gt;.&lt;/p&gt;
&lt;img src=&#34;img/toggle.png&#34; alt=&#34;julia&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;A new &lt;code&gt;Remote&lt;/code&gt; panel will open with the default button to &lt;code&gt;Edit a new server&lt;/code&gt;.&lt;/p&gt;
&lt;img src=&#34;img/edit.png&#34; alt=&#34;julia&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;Click it and you will be able to set up your remote connection.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Press &lt;code&gt;New&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Insert your username in &lt;code&gt;The name of the server&lt;/code&gt;, for example &lt;code&gt;sergiop&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Insert your ip adress in &lt;code&gt;The hostname or IP adress of the server&lt;/code&gt;, for example &lt;code&gt;123.123.123.123&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Select &lt;code&gt;SFTP - SSH File Transfer Protocol&lt;/code&gt; under &lt;code&gt;Protocol&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Select your &lt;code&gt;Logon&lt;/code&gt; option. You can either insert your password every time, just once, or use a keyfile.&lt;/li&gt;
&lt;li&gt;Insert again your username in &lt;code&gt;Username for autentication&lt;/code&gt;, again for example &lt;code&gt;sergiop&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;If you donât want to start from the root folder, you can change the &lt;code&gt;Initial Directory&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;img/julia.png&#34; alt=&#34;julia&#34;&gt;&lt;/p&gt;
&lt;p&gt;Now you will be able to see your remote directory (named for example &lt;code&gt;sergiop&lt;/code&gt;) in the &lt;code&gt;Remote&lt;/code&gt; panel.&lt;/p&gt;
&lt;p&gt;To start using Julia remotely, just start a new remote Julia process from the menu on the left.&lt;/p&gt;
&lt;img src=&#34;img/remote.png&#34; alt=&#34;julia&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;Now you are ready to deploy your Julia code on your remote server!&lt;/p&gt;
&lt;h2 id=&#34;jupyter-notebooks&#34;&gt;Jupyter Notebooks&lt;/h2&gt;
&lt;p&gt;If you want to have a &lt;a href=&#34;https://jupyter.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Jupyter Notebook&lt;/a&gt; running remotely, the steps are the following. The main advantage of a Jupyter Notebook is that it allows you to mix text and code in a single file, similarly to &lt;a href=&#34;https://rmarkdown.rstudio.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;RMarkdown&lt;/a&gt;, with the advantage of not being contrained to use a R (or Python) kernel. For example, I often use Jupyter Notebook with Julia or Matlab Kernels. Moreover, you can also make nice slides out of it!&lt;/p&gt;
&lt;p&gt;First, connect to the remote machine. Look at &lt;a href=&#34;https://matteocourthoud.github.io/post/remote/#setup&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;section 1&lt;/a&gt; to set up your SSH connection.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;ssh username@ip
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Start a Jupyter Notebook in the remote machine.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;jupyter notebook --no-browser
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The command will open a jupyter notebook in the remote machine. To connect to it, we need to know which port it used. The default port is &lt;code&gt;8888&lt;/code&gt;. If that port is busy, it will look for another available one. We can see the port from the output in terminal.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Jupyter Notebook is running at: http://localhost:XXXX/â¦&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Where &lt;code&gt;XXXX&lt;/code&gt; is the repote port used.&lt;/p&gt;
&lt;p&gt;Now we need to forward the remote port &lt;code&gt;XXXX&lt;/code&gt; to our local &lt;code&gt;YYYY&lt;/code&gt; port.&lt;/p&gt;
&lt;p&gt;Open a new &lt;em&gt;local&lt;/em&gt; shell. Type&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;ssh -L localhost:YYYY:localhost:XXXX username@ip
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Where &lt;code&gt;YYYY&lt;/code&gt; can be anything. Iâd use the default port &lt;code&gt;8888&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;ssh -L localhost:8889:localhost:8888 username@ip
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now go to your browser and type&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;localhost:YYYY
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Which in my case is&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;localhost:8889
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This will open the remote Jupyter Notebook.&lt;/p&gt;
&lt;p&gt;Done!&lt;/p&gt;
&lt;p&gt;In case you want to check which Jupiter notebooks are running, type&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;jupyter notebook list
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To kill a notebook use&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;jupyter notebook stop XXXX
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;sources&#34;&gt;Sources&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://medium.com/@SergioPietri/how-to-setup-and-use-ssh-for-remote-connections-e86556d804dd&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;How To Setup And Use SSH For Remote Connections&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.junolab.org/stable/man/remote/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Connecting to a Julia session on a remote machine&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://ljvmiranda921.github.io/notebook/2018/01/31/running-a-jupyter-notebook/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Running a Jupyter notebook from a remote server&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>My Color Palette</title>
      <link>https://matteocourthoud.github.io/post/palette/</link>
      <pubDate>Sun, 24 Oct 2021 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/post/palette/</guid>
      <description>&lt;p&gt;Ok, this is a fun post. I am choosingâ¦ &lt;strong&gt;my color palette&lt;/strong&gt;!&lt;/p&gt;
&lt;p&gt;I have decided to unify all the color palettes I have on my website, slides, graphs, etcâ¦ into a unique universal color palette.&lt;/p&gt;
&lt;h2 id=&#34;main-color&#34;&gt;Main Color&lt;/h2&gt;
&lt;p&gt;First of all, I have to choose my main color.&lt;/p&gt;
&lt;script src=&#34;https://coolors.co/palette-widget/widget.js&#34;&gt;&lt;/script&gt;
&lt;script data-id=&#34;09673912029165208&#34;&gt;new CoolorsPaletteWidget(&#34;09673912029165208&#34;, [&#34;003f5c&#34;,&#34;003f5c&#34;]); &lt;/script&gt;
&lt;p&gt;Here are some shades of it.&lt;/p&gt;
&lt;script src=&#34;https://coolors.co/palette-widget/widget.js&#34;&gt;&lt;/script&gt;
&lt;script data-id=&#34;0683428549461768&#34;&gt;new CoolorsPaletteWidget(&#34;0683428549461768&#34;, [&#34;002637&#34;,&#34;00324a&#34;,&#34;003f5c&#34;,&#34;17506b&#34;,&#34;2c6078&#34;]); &lt;/script&gt;
&lt;h2 id=&#34;related-palettes&#34;&gt;Related Palettes&lt;/h2&gt;
&lt;p&gt;Now I will build a couple of colors palettes based on it.&lt;/p&gt;
&lt;p&gt;The first one, is red oriented.&lt;/p&gt;
&lt;script src=&#34;https://coolors.co/palette-widget/widget.js&#34;&gt;&lt;/script&gt;
&lt;script data-id=&#34;06164154396260932&#34;&gt;new CoolorsPaletteWidget(&#34;06164154396260932&#34;, [&#34;003f5c&#34;,&#34;444e86&#34;,&#34;955196&#34;,&#34;dd5182&#34;,&#34;ff6e54&#34;,&#34;ffa600&#34;]); &lt;/script&gt;
&lt;p&gt;Second one, is green oriented.&lt;/p&gt;
&lt;script src=&#34;https://coolors.co/palette-widget/widget.js&#34;&gt;&lt;/script&gt;
&lt;script data-id=&#34;033203286745601424&#34;&gt;new CoolorsPaletteWidget(&#34;033203286745601424&#34;, [&#34;003f5c&#34;,&#34;00677f&#34;,&#34;00908f&#34;,&#34;2db88b&#34;,&#34;94dc7b&#34;,&#34;f9f871&#34;]); &lt;/script&gt;
&lt;h2 id=&#34;color-sequence&#34;&gt;Color Sequence&lt;/h2&gt;
&lt;p&gt;Now I need a high contrast scheme for graphs. I add one color at the time to check that contrast is always maximized.&lt;/p&gt;
&lt;script src=&#34;https://coolors.co/palette-widget/widget.js&#34;&gt;&lt;/script&gt;       &lt;script data-id=&#34;0605466695047113&#34;&gt;new CoolorsPaletteWidget(&#34;0605466695047113&#34;, [&#34;003f5c&#34;,&#34;ff6e54&#34;,&#34;f9f871&#34;,&#34;2db88b&#34;,&#34;955196&#34;]); &lt;/script&gt;https://coolors.co/003f5c-ff6e54-f9f871-2db88b-955196)
&lt;p&gt;A milder version of the same palette is:&lt;/p&gt;
&lt;script src=&#34;https://coolors.co/palette-widget/widget.js&#34;&gt;&lt;/script&gt;       &lt;script data-id=&#34;007538072748725222&#34;&gt;new CoolorsPaletteWidget(&#34;007538072748725222&#34;, [&#34;00798c&#34;,&#34;d1495b&#34;,&#34;edae49&#34;,&#34;52a369&#34;,&#34;756ab2&#34;]); &lt;/script&gt;https://coolors.co/00798c-d1495b-edae49-52a369-756ab2)</description>
    </item>
    
    <item>
      <title>PhD Frequently Asked Questions</title>
      <link>https://matteocourthoud.github.io/post/phd_faq/</link>
      <pubDate>Fri, 01 Oct 2021 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/post/phd_faq/</guid>
      <description>&lt;p&gt;In this page, I collect anquestions that I frequently asked myself during my PhD, possibly with answers.&lt;/p&gt;
&lt;p&gt;Personally, the article for PhD students that helped me the most is &lt;a href=&#34;https://medium.com/@paul.niehaus/doing-research-18cb310529e0&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;âDoing researchâ&lt;/a&gt; by Paul Niehaus. But beware, it might not work for everyone.&lt;/p&gt;
&lt;h2 id=&#34;starting-the-phd&#34;&gt;Starting the PhD&lt;/h2&gt;
&lt;h3 id=&#34;information&#34;&gt;Information&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.nature.com/articles/d41586-019-03459-7?sf223557541=1&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;âPhDs: the tortuous truthâ&lt;/a&gt;, Chris Woolston, 2019.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://medium.economist.com/why-doing-a-phd-is-often-a-waste-of-time-349206f9addb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;âWhy doing a PhD is often a waste of timeâ&lt;/a&gt;, The Economist, 2016&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.theguardian.com/careers/phd-right-career-option&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;âShould you do a PhD?&amp;quot;&lt;/a&gt;, Daniel K. Sokol, 2012.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://tertilt.vwl.uni-mannheim.de/bachelor/GradSchoolGuide.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;âSo, you want to go to a grad school in economics?&amp;quot;&lt;/a&gt;, Ceyhun Elgin and Mario Solis-Garcia, 2007.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;applying&#34;&gt;Applying&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://james-tierney.medium.com/how-to-ask-your-professor-for-a-letter-of-recommendation-f06e8b2f2c64&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;âHow to Ask Your Professor for a Letter of Recommendationâ&lt;/a&gt;, James Tierney, 2020.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://drive.google.com/file/d/16eUvtahziPyBTpX_ZeyXjPck2OyinfHH/view&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;âPre-Doc Guideâ&lt;/a&gt;, Alvin Christian, 2019.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://athey.people.stanford.edu/professional-advice&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;âAdvice for Applying to Grad School in Economicsâ&lt;/a&gt;, Susan Athey, 2016.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://qz.com/116081/the-complete-guide-to-getting-into-an-economics-phd-program/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;âThe complete guide to getting into an economics PhD programâ&lt;/a&gt;, Miles Kimball, 2013.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.ericzwick.com/public_goods/twelve_steps.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;âThe 12 Step Program for Grad Schoolâ&lt;/a&gt;, Erik Zwick.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;starting&#34;&gt;Starting&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/hagertynw/grad-school-reflections/blob/master/grad_school_reflections.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;âReflections on Grad School in Economicsâ&lt;/a&gt;, Nick Hagerty, 2020.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://law.vanderbilt.edu/phd/How_to_Survive_1st_Year.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;âHow to survive your first year of graduate school in economicsâ&lt;/a&gt;, Matthew Pearson, 2005.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;during-the-phd&#34;&gt;During the PhD&lt;/h2&gt;
&lt;h3 id=&#34;mental-health&#34;&gt;Mental Health&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://scholar.harvard.edu/files/bolotnyy/files/bbb_mentalhealth_paper.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;âGraduate Student Mental Health: Lessons from American Economics Departmentsâ&lt;/a&gt;, Bolotnyy, Valentin, Matthew Basilico, and Paul Barreira, 2021.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.insidehighered.com/news/2019/11/14/phd-student-poll-finds-mental-health-bullying-and-career-uncertainty-are-top&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;âMental Health, Bullying, Career Uncertaintyâ&lt;/a&gt;, Colleen Flahert, 2019.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.sciencemag.org/careers/2019/03/how-mindfulness-can-help-phd-students-deal-mental-health-challenges&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;âHow mindfulness can help Ph.D. students deal with mental health challengesâ&lt;/a&gt;, Katie Langin, 2019.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.phdstudies.com/article/managing-your-mental-health-as-a-phd-student/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;âManaging Your Mental Health as a PhD Studentâ&lt;/a&gt;, Joanna Hughes, 2019.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.psychologytoday.com/us/blog/emotional-mastery/201904/what-makes-it-so-hard-ask-help&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;âWhat Makes It So Hard to Ask for Help?&amp;quot;&lt;/a&gt;, Joan Rosenberg, 2019.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.sciencemag.org/careers/2018/11/grad-school-depression-almost-took-me-end-road-i-found-new-start&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;âGrad school depression almost took me to the end of the roadâbut I found a new startâ&lt;/a&gt;, Francis Aguisanda, 2018.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.nature.com/articles/nj7587-555a.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;âFaking itâ&lt;/a&gt;, Chris Woolston, 2016.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://blogs.nature.com/naturejobs/2016/09/14/panic-and-a-phd/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;âPanic and a PhDâ&lt;/a&gt;, Jack Leeming, 2016.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://qz.com/547641/theres-an-awful-cost-to-getting-a-phd-that-no-one-talks-about/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;âThereâs an awful cost to getting a PhD that no one talks aboutâ&lt;/a&gt;, Jennifer Walker, 2015.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;research-and-ideas&#34;&gt;Research and Ideas&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://www.ricardodahis.com/files/papers/Dahis_Advice_Research.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;âAdvice for Academic Researchâ&lt;/a&gt;, Ricardo Dahis, 2021.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.aeaweb.org/articles?id=10.1257/jel.20191573&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;âSins of Omission and the Practice of Economicsâ&lt;/a&gt;, George A. Akerlof, 2020.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://medium.com/@paul.niehaus/doing-research-18cb310529e0&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;âDoing researchâ&lt;/a&gt;, Paul Niehaus, 2019.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://static1.squarespace.com/static/55c143d9e4b0cb07521c6d17/t/5b4f409f575d1ff83c2f12d8/1531920545061/PhDGuidebook.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;âAn unofficial guidebook for PhD students in economics and educationâ&lt;/a&gt;, Alex Eble, 2018.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://iu.box.com/s/0ha9gcq0t22kyyy1rqv15mkmauw1py18&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;âThe Research Productivity of New PhDs in Economics: The Surprisingly High Non-Success of the Successfulâ&lt;/a&gt;, John P. Conley and Ali Sina Ãnder, 2014.&lt;/li&gt;
&lt;li&gt;[âHow to get started on research in economics?&amp;quot;](&lt;a href=&#34;http://econ.lse.ac.uk/staff/spischke/phds/How&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://econ.lse.ac.uk/staff/spischke/phds/How&lt;/a&gt; to start.pdf), Steve Pischke, 2009.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://iu.box.com/s/km7cxhcxgfcdpk4cp38b47x7is7lum11&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;âThe Importance of Stupidity in Scientific Researchâ&lt;/a&gt;, Martin A. Schwartz, 2008.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://stevepavlina.com/blog/2007/01/7-rules-for-maximizing-your-creative-output/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;â7 Rules for Maximizing Your Creative Outputâ&lt;/a&gt;, Steve Pavlina, 2007.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://people.ischool.berkeley.edu/~hal/Papers/how.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;âHow To Build An Economic Model in Your Spare Timeâ&lt;/a&gt;, Hal. R. Varian, 1998.&lt;/li&gt;
&lt;li&gt;[âPh.D. Thesis Research: Where do I Start?&amp;quot;](&lt;a href=&#34;http://www.columbia.edu/~drd28/Thesis&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://www.columbia.edu/~drd28/Thesis&lt;/a&gt; Research.pdf), Don Davis.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;presenting&#34;&gt;Presenting&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://david-schindler.de/unfair-questions/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;âUnfair Questionsâ&lt;/a&gt;, David Schindler, 2021.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/paulgp/beamer-tips/blob/master/slides.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;âBeamer Tips for Presentationsâ&lt;/a&gt;, Paul Goldsmith-Pinkham, 2020.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.princeton.edu/~reddings/tradephd/public_speaking_for_academic_economists.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;âPublic Speaking for Academic Economistsâ&lt;/a&gt;, Rachel Meager, 2017.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.europeanjobmarketofeconomists.org/uploads/HowToPresent_LaFerrara.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;âHow to present your job market paperâ&lt;/a&gt;, Eliana La Ferrara, 2018.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://people.bu.edu/guren/Guren_HowToGiveALunchTalk.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;âHow To Give a Lunch Talkâ&lt;/a&gt;, Adam Guren, 2018.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://chrisblattman.com/2010/02/22/the-discussants-art/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;âThe Discussantâs Artâ&lt;/a&gt;, Chris Blattman, 2010.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://papers.ssrn.com/sol3/papers.cfm?abstract_id=1332144&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;âHow to be a Great Conference Participantsâ&lt;/a&gt;, Art Carden, 2009.&lt;/li&gt;
&lt;li&gt;[âThe âBig 5â and Other Ideas For Presentationsâ](&lt;a href=&#34;http://econ.lse.ac.uk/staff/spischke/phds/The&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://econ.lse.ac.uk/staff/spischke/phds/The&lt;/a&gt; Big 5.pdf), Cox, Donald, 2000.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://iu.box.com/s/aw92d7kl7xh5s4zsub8jq3qnknq9zcsi&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;âHow to Give an Applied Micro Talkâ&lt;/a&gt;, Jesse M. Shapiro.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://iu.box.com/s/37j3eip7x9fdg30n4eeepu92228eb999&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;âTips on How to Avoid Disaster in Presentationsâ&lt;/a&gt;, Monika Piazzesi.&lt;/li&gt;
&lt;li&gt;[âSeminar Slides â](&lt;a href=&#34;https://www.ssc.wisc.edu/~bhansen/placement/Seminar&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.ssc.wisc.edu/~bhansen/placement/Seminar&lt;/a&gt; Slides.pdf), Bruce Hansen.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;writing&#34;&gt;Writing&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;[â5 Steps Toward a Paperâ](&lt;a href=&#34;https://www.google.com/url?q=https%3A%2F%2Fwww.dropbox.com%2Fs%2Fq7wjaidl5w91srt%2FGuest&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.google.com/url?q=https%3A%2F%2Fwww.dropbox.com%2Fs%2Fq7wjaidl5w91srt%2FGuest&lt;/a&gt; lecture FS.pdf%3Fdl%3D0&amp;amp;sa=D&amp;amp;sntz=1&amp;amp;usg=AFQjCNG_nRs6QlkZzWBHAy0PjF4jfEYBAw), Frank Schilbach, 2019.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.nature.com/articles/d41586-019-02918-5&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;âNovelist Cormac McCarthyâs tips on how to write a great science paperâ&lt;/a&gt;, Van Savage and Pamela Yeh, 2019.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://marcfbellemare.com/wordpress/12797&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;âThe âMiddle Bitsâ Formula for Applied Papersâ&lt;/a&gt;, Marc Bellamare, 2018.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://marcfbellemare.com/wordpress/12060&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;âThe Conclusion Formulaâ&lt;/a&gt;, Marc Bellamare, 2018.&lt;/li&gt;
&lt;li&gt;[âThe Introduction Formulaâ](&lt;a href=&#34;https://www.albany.edu/spatial/training/5-The&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.albany.edu/spatial/training/5-The&lt;/a&gt; Introduction Formula.pdf), Keith Head, 2015.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.people.fas.harvard.edu/~pnikolov/resources/writingtips.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;âWriting Tips For Economics Research Papersâ&lt;/a&gt;, Plamen Nikolov, 2013.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://economics.harvard.edu/files/economics/files/tenruleswriting.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;âThe Ten Most Important Rules of Writing Your Job Market Paperâ&lt;/a&gt;, Goldin, Claudia and Lawrence Katz, 2008.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://schwert.ssb.rochester.edu/aec510/phd_paper_writing.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;âWriting Tips for Ph.D. Studentsâ&lt;/a&gt;, John Cochrane, 2005.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://qed.econ.queensu.ca/pub/faculty/sumon/mkremer_checklist_paper.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;âWriting Papers: A Checklistâ&lt;/a&gt;, Michael Kremer.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;referiing&#34;&gt;Referiing&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://blog.academicsequitur.com/2019/06/30/how-to-write-a-good-referee-report/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;âHow To Write A Good Referee Reportâ&lt;/a&gt;, Tatyana Deryugina, 2019.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://iu.box.com/s/lgmhqw5uxvrb7qdrhxxskzki9pcwx7o6&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;âHow to Review Manuscriptsâ&lt;/a&gt;, Elsevier, 2015.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://marcfbellemare.com/wordpress/5542&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;âContributing to Public Goods: My 20 Rules for Refereeingâ&lt;/a&gt;, Marc F. Bellemare, 2012&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;finishing-the-phd&#34;&gt;Finishing the PhD&lt;/h2&gt;
&lt;h3 id=&#34;the-job-market&#34;&gt;The Job Market&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.aeaweb.org/content/file?id=869&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;âA Guide and Advice for Economists on the U.S. Junior Academic Job Market 2018-2019 Editionâ&lt;/a&gt;, John Cawley, 2018.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://chrisblattman.com/job-market/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;âAcademic job market advice for economics, political science, public policy, and other professional schoolsâ&lt;/a&gt;, Blattman, Christopher, 2015.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.ericzwick.com/public_goods/love_the_market.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;âHow I Learned to Stop Worrying and Love the Job Marketâ&lt;/a&gt;, Erik Zwick, 2014.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;the-private-sector&#34;&gt;The Private Sector&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.linkedin.com/pulse/my-journey-from-economics-phd-data-scientist-tech-rose-tan&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;âMy Journey from Economics PhD to Data Scientist in Techâ&lt;/a&gt;, Rose Tan, 2021.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://scarlet-chen.medium.com/tech-industry-jobs-for-econ-phds-54a276dda80b&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;âTech Industry Jobs for Econ PhDsâ&lt;/a&gt;, Scarlet Chen, 2020.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://scarlet-chen.medium.com/my-journey-from-econ-phd-to-tech-part-1-interview-prep-networking-d256918410a2&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;âMy Journey from Econ PhD to Techâ&lt;/a&gt;, Scarlet Chen, 2020.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.nature.com/articles/d41586-018-05838-y&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;âWhy it is not a âfailureâ to leave academiaâ&lt;/a&gt;, Philipp Kruger, 2018.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;the-tenure-track&#34;&gt;The Tenure Track&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://blogs.scientificamerican.com/guest-blog/the-awesomest-7-year-postdoc-or-how-i-learned-to-stop-worrying-and-love-the-tenure-track-faculty-life/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;âThe Awesomest 7-Year Postdoc or: How I Learned to Stop Worrying and Love the Tenure-Track Faculty Lifeâ&lt;/a&gt;, Radhika Nagpal, 2013.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;more&#34;&gt;More&lt;/h2&gt;
&lt;p&gt;You can find more resources here:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;AEA &lt;a href=&#34;https://www.aeaweb.org/about-aea/committees/cswep/mentoring/reading&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Mentoring Reading Materials&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Johannes Pfeifer &lt;a href=&#34;https://sites.google.com/site/pfeiferecon/job-market-resources&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Job Market Resources&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Kristoph Kronenberg &lt;a href=&#34;https://sites.google.com/view/christoph-kronenberg/home/resources&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Resources&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Patrick Button &lt;a href=&#34;https://www.patrickbutton.com/resources&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Resources&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Ryan Edwards &lt;a href=&#34;http://www.ryanbedwards.com/resources&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Resources&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Jennifer Doleac &lt;a href=&#34;http://jenniferdoleac.com/resources/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Resources&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Amanda Agan &lt;a href=&#34;https://sites.google.com/site/amandayagan/writingadvice&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Writing and Presentation Advice&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Random forum &lt;a href=&#34;http://www.inhe365.com/thread-17506-1-1.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Resource Collection&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Coding Resources for Social Sciences</title>
      <link>https://matteocourthoud.github.io/post/coding/</link>
      <pubDate>Tue, 24 Aug 2021 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/post/coding/</guid>
      <description>&lt;p&gt;In this page, I collect useful resources for coding for researchers in social sciences. A mention goes to &lt;a href=&#34;https://maxkasy.github.io/home/computationlinks/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Maximilian Kasy&lt;/a&gt; that inspired me to build this page.&lt;/p&gt;
&lt;p&gt;A quick legend:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ð book&lt;/li&gt;
&lt;li&gt;ð webpage&lt;/li&gt;
&lt;li&gt;ð charts&lt;/li&gt;
&lt;li&gt;ð¥ videos&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;econometrics-and-statistics&#34;&gt;Econometrics and Statistics&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;ð&lt;a href=&#34;https://www.ssc.wisc.edu/~bhansen/econometrics/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Bruce Hansenâs Econometrics&lt;/strong&gt;&lt;/a&gt;: By far the best freely available and regularly updated resource for Econometrics&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;machine-learning&#34;&gt;Machine Learning&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;ð&lt;a href=&#34;https://web.stanford.edu/~hastie/Papers/ESLII.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;The Elements of Statistical Learning&lt;/strong&gt;&lt;/a&gt;: General introduction to machine learning&lt;/li&gt;
&lt;li&gt;ð&lt;a href=&#34;http://www.gaussianprocess.org/gpml/chapters/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Gaussian Processes for Machine Learning&lt;/strong&gt;&lt;/a&gt;: Extremely useful tools for nonparametric Bayesian modeling&lt;/li&gt;
&lt;li&gt;ð&lt;a href=&#34;https://www.deeplearningbook.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Deep Learning&lt;/strong&gt;&lt;/a&gt;: The theory and implementation of neural nets&lt;/li&gt;
&lt;li&gt;ð&lt;a href=&#34;https://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning/understanding-machine-learning-theory-algorithms.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Understanding Machine Learning: From Theory to Algorithms&lt;/strong&gt;&lt;/a&gt;: An introduction to statistical learning theory in the tradition of Vapnik&lt;/li&gt;
&lt;li&gt;ð&lt;a href=&#34;http://www.incompleteideas.net/book/RLbook2018.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Reinforcement Learning - An Introduction&lt;/strong&gt;&lt;/a&gt;: Adaptive learning for Markov decision problems&lt;/li&gt;
&lt;li&gt;ð&lt;a href=&#34;http://jeffe.cs.illinois.edu/teaching/algorithms/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Algorithms&lt;/strong&gt;&lt;/a&gt;: Introduction to the theory of algorithms&lt;/li&gt;
&lt;li&gt;ð&lt;a href=&#34;https://playground.tensorflow.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Tensorflow Playground&lt;/strong&gt;&lt;/a&gt;: Visualisation tool for neural networks&lt;/li&gt;
&lt;li&gt;ð&lt;a href=&#34;https://www.fast.ai/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Artificial Intelligence&lt;/strong&gt;&lt;/a&gt;: Online lectures on AI&lt;/li&gt;
&lt;li&gt;ð&lt;a href=&#34;https://global.oup.com/academic/product/the-ethical-algorithm-9780190948207&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;The Ethical Algorithm&lt;/strong&gt;&lt;/a&gt;: How to impose normative constraints on ML and other algorithms&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;python&#34;&gt;Python&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;ð&lt;a href=&#34;https://realpython.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;RealPython&lt;/strong&gt;&lt;/a&gt;: Collection of Python tutorials, from introductory to advanced. Also contains &lt;a href=&#34;https://realpython.com/learning-paths/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;learning paths&lt;/a&gt; for specific topics&lt;/li&gt;
&lt;li&gt;ð&lt;a href=&#34;https://python.quantecon.org/intro.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;QuantEcon Python&lt;/strong&gt;&lt;/a&gt; Tutorials and economic applications in Python, especially for macroeconomics&lt;/li&gt;
&lt;li&gt;ð&lt;a href=&#34;https://blog.finxter.com/python-cheat-sheets/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Cheat Sheets&lt;/strong&gt;&lt;/a&gt;: Collection of cheat sheets for python&lt;/li&gt;
&lt;li&gt;ð&lt;a href=&#34;https://docs.python-guide.org/writing/structure/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Structuring a Python project&lt;/strong&gt;&lt;/a&gt;: Advanced tutorial on how to structure a Python program&lt;/li&gt;
&lt;li&gt;ð&lt;a href=&#34;https://www.softwaretestinghelp.com/python-ide-code-editors/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;IDE Guide&lt;/strong&gt;&lt;/a&gt;: Comparison of IDEs for Python. Suggested: &lt;a href=&#34;https://www.jetbrains.com/pycharm/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PyCharm&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;ð&lt;a href=&#34;https://www.jetbrains.com/help/pycharm/configuring-remote-interpreters-via-ssh.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Configuring remote interpreters via SSH&lt;/strong&gt;&lt;/a&gt;: How to use Python remotely via SSH via PyCharm&lt;/li&gt;
&lt;li&gt;ð&lt;a href=&#34;https://www.kaggle.com/maheshdadhich/strength-of-visualization-python-visuals-tutorial/notebook&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Visualization in Python&lt;/strong&gt;&lt;/a&gt;: How to make nice graphs in Python, with a dedicated jupyter notebook&lt;/li&gt;
&lt;li&gt;ð&lt;a href=&#34;https://python-graph-gallery.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Python Graph Gallery&lt;/strong&gt;&lt;/a&gt;: Graph examples in Python&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;matlab&#34;&gt;Matlab&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;ð&lt;a href=&#34;https://mathworks.com/help/matlab/matlab_oop/user-defined-classes.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;User defined classes in Matlab&lt;/strong&gt;&lt;/a&gt;: How to work with classes in Matlab&lt;/li&gt;
&lt;li&gt;ð&lt;a href=&#34;https://am111.readthedocs.io/en/latest/jmatlab_use.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Julyter Notebooks&lt;/strong&gt;&lt;/a&gt;: How to run a jupyter notebook with Matlab kernel&lt;/li&gt;
&lt;li&gt;ð&lt;a href=&#34;https://www.bradleymonk.com/wp/how-to-make-professional-looking-plots-for-journal-publication-using-matlab-r2014a-and-r2014b/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Graph Tips in Matlab&lt;/strong&gt;&lt;/a&gt; and &lt;a href=&#34;https://mathworks.com/matlabcentral/answers/133372-how-to-make-nice-plots&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;link2&lt;/a&gt;: Suggestions on how to make pretty graphs in Matlab&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;julia&#34;&gt;Julia&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;ð&lt;a href=&#34;https://docs.julialang.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Julia Manual&lt;/strong&gt;&lt;/a&gt;: Julia unfortunately lacks a big community and tutorials, but it has a very good manual&lt;/li&gt;
&lt;li&gt;ð&lt;a href=&#34;https://julia.quantecon.org/index_toc.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;QuantEcon Julia&lt;/strong&gt;&lt;/a&gt; Tutorials and economic applications in Julia, especially for macroeconomics&lt;/li&gt;
&lt;li&gt;ð&lt;a href=&#34;https://medium.com/dev-genius/what-is-the-best-ide-for-developing-in-the-programming-language-julia-484c913f07bc&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;IDE Guide&lt;/strong&gt;&lt;/a&gt;: Guide for IDEs for Julia. Suggested: Juno for Atom.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;r&#34;&gt;R&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;ð&lt;a href=&#34;https://cran.r-project.org/doc/manuals/r-release/R-intro.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;An Introduction to R&lt;/strong&gt;&lt;/a&gt;: Complete introduction to base R&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;ð&lt;a href=&#34;http://r4ds.had.co.nz/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;R for Data Science&lt;/strong&gt;&lt;/a&gt; Introduction to data analysis using R, focused on the tidyverse packages&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;ð&lt;a href=&#34;https://adv-r.hadley.nz/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Advanced R&lt;/strong&gt;&lt;/a&gt;: In depth discussion of programming in R&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;ð&lt;a href=&#34;https://bradleyboehmke.github.io/HOML/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Hands-On Machine Learning with R&lt;/strong&gt;&lt;/a&gt;: Fitting ML models in R&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;ð&lt;a href=&#34;https://github.com/stan-dev/rstan/wiki/RStan-Getting-Started&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Bayesian statistics using Stan&lt;/strong&gt;&lt;/a&gt; and &lt;a href=&#34;https://mc-stan.org/docs/2_20/stan-users-guide/index.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;link&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;ð&lt;a href=&#34;https://www.rstudio.com/resources/cheatsheets/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;RStudio Cheat Sheets&lt;/strong&gt;&lt;/a&gt; for various extensions, including data processing, visualization, writing web apps, â¦&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;ð&lt;a href=&#34;https://www.r-graph-gallery.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;R Graph Gallery&lt;/strong&gt;&lt;/a&gt;: Graph examples in R&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;ð&lt;a href=&#34;https://www.christophenicault.com/pages/visualizations/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Nice Graphs with code&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;A collection of elaborate graphs with code in R&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;others&#34;&gt;Others&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;ð&lt;a href=&#34;https://git-scm.com/book/en/v2&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Github Advanced&lt;/strong&gt;&lt;/a&gt;: Advanced guide for version control with Github&lt;/li&gt;
&lt;li&gt;ð¥&lt;a href=&#34;https://missing.csail.mit.edu/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;The Missing Semester of Your CS Education&lt;/strong&gt;&lt;/a&gt; Video lectures and notes on tools for computer scientists (version control, debugging, â¦)&lt;/li&gt;
&lt;li&gt;ð&lt;a href=&#34;http://pgfplots.sourceforge.net/gallery.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;PGF plots in Latex&lt;/strong&gt;&lt;/a&gt;: Gallery and examples to make plots directly in Latex&lt;/li&gt;
&lt;li&gt;ð&lt;a href=&#34;https://medium.com/@SergioPietri/how-to-setup-and-use-ssh-for-remote-connections-e86556d804dd&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Work remotely from server&lt;/strong&gt;&lt;/a&gt;: How to setup SSH for remote computing&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>How to access WRDS in Python</title>
      <link>https://matteocourthoud.github.io/post/wrds/</link>
      <pubDate>Tue, 09 Mar 2021 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/post/wrds/</guid>
      <description>&lt;p&gt;In this page, I explain how to work with the WRDS database using Python.&lt;/p&gt;
&lt;h2 id=&#34;setup&#34;&gt;Setup&lt;/h2&gt;
&lt;p&gt;The first thing we need to do, is to set up a connection to the &lt;a href=&#34;https://wrds-www.wharton.upenn.edu/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;WRDS database&lt;/a&gt;. I am assuming you have credentials to log in. Check the &lt;a href=&#34;https://wrds-www.wharton.upenn.edu/login/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;log in page&lt;/a&gt; to make sure.&lt;/p&gt;
&lt;p&gt;The second requirement is the &lt;a href=&#34;https://pypi.org/project/wrds/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;wrds&lt;/a&gt; Python package.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pip3 install wrds
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, in order to connect to the WRDS database, you just need to run the following commang in Python.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import wrds
db = wrds.Connection() 
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then, you will be propted to input your WRDS username and password.&lt;/p&gt;
&lt;p&gt;However, if you are using a Python IDE such as &lt;a href=&#34;https://www.jetbrains.com/pycharm/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PyCharm&lt;/a&gt;, you cannot run the command from the Python Console. Moreover, you might want to save your credentials once and for all, so that you donât have to log in every time.&lt;/p&gt;
&lt;p&gt;First, walk to your home directory from the Terminal (&lt;code&gt;/Users/username&lt;/code&gt;).&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;cd
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now create an empty &lt;code&gt;.pgpass&lt;/code&gt; file.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;touch .pgpass
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now you write &lt;code&gt;your_username&lt;/code&gt; and &lt;code&gt;your_password&lt;/code&gt; into the &lt;code&gt;.pgpass&lt;/code&gt; file.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;echo &amp;quot;wrds-pgdata.wharton.upenn.edu:9737:wrds:your_username:your_password&amp;quot; &amp;gt;&amp;gt; .pgpass
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You also need to restrict permissions to the file.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;chmod 600 ~/.pgpass
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now you can go back to your Python IDE and access the database by just inputing your username.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import wrds
db = wrds.Connection(wrds_username=&#39;your_username&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If everything works, you should see the following output.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Loading library list...
Done
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;query&#34;&gt;Query&lt;/h2&gt;
&lt;p&gt;The available functions are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;db.connection()&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;db.list_libraries()&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;db.list_tables()&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;db.get_table()&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;db.describe_table()&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;db.raw_sql()&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;db.close()&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I make a simple example of how they work. Suppose first you want to list all the libraries in the WRDS database.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;db.list_libraries()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then you can list all the datasets within a given library.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;db.list_tables(library=&#39;comp&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Before downloading a table, you can describe it.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;df = db.describe_table(library=&#39;comp&#39;, table=&#39;funda&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To download the dataset you can use the &lt;code&gt;get_table()&lt;/code&gt; function.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;df = db.get_table(library=&#39;comp&#39;, table=&#39;funda&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can restrict both the rows and the columns you want to query.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;df_short = db.get_table(library=&#39;comp&#39;, table=&#39;funda&#39;, columns = [&#39;conm&#39;, &#39;gvkey&#39;, &#39;cik&#39;], obs=5)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can also query the database directly using SQL.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;df_sql = db.raw_sql(&#39;&#39;&#39;select conm, gvkey, cik FROM comp.funda WHERE fyear&amp;gt;2010 AND (indfmt=&#39;INDL&#39;)&#39;&#39;&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;sources&#34;&gt;Sources&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://wrds-www.wharton.upenn.edu/pages/support/programming-wrds/programming-python/querying-wrds-data-python/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Querying WRDS Data using Python&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://wrds-www.wharton.upenn.edu/documents/1443/wrds_connection.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Using Python on WRDS Platform&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://sites.duke.edu/kevinstandridge/2020/03/07/introduction-to-the-wrds-python-package/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Introduction to the WRDS Python Package&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://wizardkingz.github.io/wrdsdataaccesspython-tutorial/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;WRDS Data Access Via Python API&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Summer Schools in Economics</title>
      <link>https://matteocourthoud.github.io/post/summer_schools/</link>
      <pubDate>Tue, 10 Mar 2020 00:00:00 +0000</pubDate>
      <guid>https://matteocourthoud.github.io/post/summer_schools/</guid>
      <description>&lt;p&gt;In this page, I collect information about summer schools in Economics.&lt;/p&gt;
&lt;p&gt;If you know about summer schools that are missing from this list, please either &lt;a href=&#34;mailto:matteo.courthoud@econ.uzh.ch&#34;&gt;contact me&lt;/a&gt; or &lt;a href=&#34;https://github.com/matteocourthoud/website/blob/master/content/post/summerschools/index.md&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;edit the table on Github&lt;/a&gt;!&lt;/p&gt;
&lt;h2 id=&#34;2020&#34;&gt;2020&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Name and Link&lt;/th&gt;
&lt;th&gt;Field&lt;/th&gt;
&lt;th&gt;Organizer&lt;/th&gt;
&lt;th&gt;Location&lt;/th&gt;
&lt;th&gt;Instructor(s)&lt;/th&gt;
&lt;th&gt;Dates&lt;/th&gt;
&lt;th&gt;Deadline&lt;/th&gt;
&lt;th&gt;Fee&lt;/th&gt;
&lt;th&gt;Aid&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://dseconf.org/dse2020&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Dynamic Structural Econometrics&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Econometrics, IO&lt;/td&gt;
&lt;td&gt;Econometrics Society&lt;/td&gt;
&lt;td&gt;Zurich&lt;/td&gt;
&lt;td&gt;John Rust et al.&lt;/td&gt;
&lt;td&gt;June 15-21&lt;/td&gt;
&lt;td&gt;March 15&lt;/td&gt;
&lt;td&gt;500$&lt;/td&gt;
&lt;td&gt;no&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://www.cresse.info/default.aspx?articleID=3398&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CRESSE&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Comp. Policy, IO&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;Crete&lt;/td&gt;
&lt;td&gt;various&lt;/td&gt;
&lt;td&gt;June 20 - July 02&lt;/td&gt;
&lt;td&gt;FCFS&lt;/td&gt;
&lt;td&gt;3200â¬&lt;/td&gt;
&lt;td&gt;-30%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://www.barcelonagse.eu/study/summer-school/digital-economy&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Digital Economy&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Comp. Policy, IO&lt;/td&gt;
&lt;td&gt;Barcelona GSE&lt;/td&gt;
&lt;td&gt;Barcelona&lt;/td&gt;
&lt;td&gt;Martin Peitz&lt;/td&gt;
&lt;td&gt;July 13-17&lt;/td&gt;
&lt;td&gt;March 10&lt;/td&gt;
&lt;td&gt;550â¬&lt;/td&gt;
&lt;td&gt;maybe&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://www.parisschoolofeconomics.eu/en/teaching/pse-summer-school/social-networks-platforms/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Social Networks, Platformsâ¦&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Comp. Policy, IO&lt;/td&gt;
&lt;td&gt;PSE&lt;/td&gt;
&lt;td&gt;Paris&lt;/td&gt;
&lt;td&gt;from Paris&lt;/td&gt;
&lt;td&gt;June 15-19&lt;/td&gt;
&lt;td&gt;March 31&lt;/td&gt;
&lt;td&gt;1200â¬&lt;/td&gt;
&lt;td&gt;no&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;2019&#34;&gt;2019&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Name and Link&lt;/th&gt;
&lt;th&gt;Field&lt;/th&gt;
&lt;th&gt;Organizer&lt;/th&gt;
&lt;th&gt;Location&lt;/th&gt;
&lt;th&gt;Instructor(s)&lt;/th&gt;
&lt;th&gt;Dates&lt;/th&gt;
&lt;th&gt;Deadline&lt;/th&gt;
&lt;th&gt;Fee&lt;/th&gt;
&lt;th&gt;Aid&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://dseconf.org/dse2019&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Dynamic Structural Econometrics&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Econometrics, IO&lt;/td&gt;
&lt;td&gt;Econometrics Society&lt;/td&gt;
&lt;td&gt;Chicago&lt;/td&gt;
&lt;td&gt;John Rust et al.&lt;/td&gt;
&lt;td&gt;July 08-14&lt;/td&gt;
&lt;td&gt;March 15&lt;/td&gt;
&lt;td&gt;500$&lt;/td&gt;
&lt;td&gt;no&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;CRESSE&lt;/td&gt;
&lt;td&gt;Competition Policy, IO&lt;/td&gt;
&lt;td&gt;various&lt;/td&gt;
&lt;td&gt;Crete&lt;/td&gt;
&lt;td&gt;various&lt;/td&gt;
&lt;td&gt;June 20 - July 02&lt;/td&gt;
&lt;td&gt;FCFS&lt;/td&gt;
&lt;td&gt;3200â¬&lt;/td&gt;
&lt;td&gt;-30%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://www.cemfi.es/studies/css/course.asp?cu=10&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Empirical Analysis of Firm Performance&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;IO, Trade&lt;/td&gt;
&lt;td&gt;CEMFI&lt;/td&gt;
&lt;td&gt;Madrid&lt;/td&gt;
&lt;td&gt;Jan de Loecker&lt;/td&gt;
&lt;td&gt;August 19-23&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://www.cemfi.es/studies/css/course.asp?cu=16&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Panel Data Econometrics&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Econometrics&lt;/td&gt;
&lt;td&gt;CEMFI&lt;/td&gt;
&lt;td&gt;Madrid&lt;/td&gt;
&lt;td&gt;Steve Bond&lt;/td&gt;
&lt;td&gt;September 02-06&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;2018&#34;&gt;2018&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Name and Link&lt;/th&gt;
&lt;th&gt;Field&lt;/th&gt;
&lt;th&gt;Organizer&lt;/th&gt;
&lt;th&gt;Location&lt;/th&gt;
&lt;th&gt;Instructor(s)&lt;/th&gt;
&lt;th&gt;Dates&lt;/th&gt;
&lt;th&gt;Deadline&lt;/th&gt;
&lt;th&gt;Fee&lt;/th&gt;
&lt;th&gt;Aid&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://www.econ.ku.dk/cce/events/summerschool/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Dynamic Structural Models&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Econometrics, IO&lt;/td&gt;
&lt;td&gt;University of Copenhagen&lt;/td&gt;
&lt;td&gt;Copenhagen&lt;/td&gt;
&lt;td&gt;John Rust et al.&lt;/td&gt;
&lt;td&gt;May 28 - Jun 03&lt;/td&gt;
&lt;td&gt;March 15&lt;/td&gt;
&lt;td&gt;600â¬&lt;/td&gt;
&lt;td&gt;no&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://www.cemfi.es/studies/css/course_previous_years.asp?c=12&amp;amp;y=2018&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Empirical Analysis of Innovation in Oligopoly Industries&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
</description>
    </item>
    
  </channel>
</rss>
