<!DOCTYPE html><html lang="en-us" >

<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  
  
  
    <meta name="generator" content="Wowchemy 5.1.0 for Hugo" />
  

  

  
  
  
  
  
    
    
    
  
  

  

  
  
  
    
  
  <meta name="description" content="How to run valid experiments, with unplanned peeking and early stopping.
A/B tests are the golden standard of causal inference because they allow us to make valid causal statements under minimal assumptions, thanks to randomization." />

  
  <link rel="alternate" hreflang="en-us" href="https://matteocourthoud.github.io/post/group_sequential_testing/" />

  







  




  
  
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
  

  
  
  
    <meta name="theme-color" content="#003f5c" />
  

  
  
    
    <script src="/js/mathjax-config.js"></script>
  

  
  
  
  
    
    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha256-FMvZuGapsJLjouA6k7Eo2lusoAX9i0ShlWFG6qt7SLc=" crossorigin="anonymous">

    
    

    
    
    
      
    
    
      
      
        
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/styles/github.min.css" crossorigin="anonymous" title="hl-light" media="print" onload="this.media='all'">
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/styles/dracula.min.css" crossorigin="anonymous" title="hl-dark" media="print" onload="this.media='all'" disabled>
        
      
    

    
    
    
      <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.7.1/leaflet.min.css" integrity="sha512-1xoFisiGdy9nvho8EgXuXvnpR5GAMSjFwp40gSRE3NwdUdIMIKuPa7bqoUhLD0O/5tPNhteAsE5XyyMi5reQVA==" crossorigin="anonymous" media="print" onload="this.media='all'">
    

    

    
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js" integrity="" crossorigin="anonymous" async></script>
      
    
      
      

      
      

      
    
      
      

      
      

      
    
  

  
  
  
    
      
      <link rel="preload" as="style" href="https://fonts.googleapis.com/css2?family=Roboto+Mono&family=Source+Sans+Pro:wght@200;300;400;700&display=swap">
      <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Roboto+Mono&family=Source+Sans+Pro:wght@200;300;400;700&display=swap" media="print" onload="this.media='all'">
    
  

  
  
  
  
  
  <link rel="stylesheet" href="/css/wowchemy.036b5b2acdb844b842ed3e91242fe237.css" />

  




<script async src="https://www.googletagmanager.com/gtag/js?id=UA-144780600-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];

  function gtag() {
      dataLayer.push(arguments);
  }

  function trackOutboundLink(url, target) {
    gtag('event', 'click', {
         'event_category': 'outbound',
         'event_label': url,
         'transport_type': 'beacon',
         'event_callback': function () {
           if (target !== '_blank') {
             document.location = url;
           }
         }
    });
    console.debug("Outbound link clicked: " + url);
  }

  function onClickCallback(event) {
    if ((event.target.tagName !== 'A') || (event.target.host === window.location.host)) {
      return;
    }
    trackOutboundLink(event.target, event.target.getAttribute('target'));  
  }

  gtag('js', new Date());
  gtag('config', 'UA-144780600-1', { 'anonymize_ip': true });
  gtag('set', {'cookie_flags': 'SameSite=None;Secure'});

  
  document.addEventListener('click', onClickCallback, false);
</script>


  

  

  




  
  
  

  

  
    <link rel="manifest" href="/index.webmanifest" />
  

  <link rel="icon" type="image/png" href="/media/icon_hu03e9b3967b83fd39296ec9da5ff1ea05_201175_32x32_fill_lanczos_center_3.png" />
  <link rel="apple-touch-icon" type="image/png" href="/media/icon_hu03e9b3967b83fd39296ec9da5ff1ea05_201175_180x180_fill_lanczos_center_3.png" />

  <link rel="canonical" href="https://matteocourthoud.github.io/post/group_sequential_testing/" />

  
  
  
  
  
  
  
  
    
  
  
  <meta property="twitter:card" content="summary_large_image" />
  
  <meta property="og:site_name" content="Matteo Courthoud" />
  <meta property="og:url" content="https://matteocourthoud.github.io/post/group_sequential_testing/" />
  <meta property="og:title" content="Understanding Group Sequential Testing | Matteo Courthoud" />
  <meta property="og:description" content="How to run valid experiments, with unplanned peeking and early stopping.
A/B tests are the golden standard of causal inference because they allow us to make valid causal statements under minimal assumptions, thanks to randomization." /><meta property="og:image" content="https://matteocourthoud.github.io/post/group_sequential_testing/featured.png" />
    <meta property="twitter:image" content="https://matteocourthoud.github.io/post/group_sequential_testing/featured.png" /><meta property="og:locale" content="en-us" />
  
    
      <meta
        property="article:published_time"
        content="2023-12-26T00:00:00&#43;00:00"
      />
    
    <meta property="article:modified_time" content="2023-12-26T00:00:00&#43;00:00">
  

  


    






  





<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://matteocourthoud.github.io/post/group_sequential_testing/"
  },
  "headline": "Understanding Group Sequential Testing",
  
  "image": [
    "https://matteocourthoud.github.io/post/group_sequential_testing/featured.png"
  ],
  
  "datePublished": "2023-12-26T00:00:00Z",
  "dateModified": "2023-12-26T00:00:00Z",
  
  "publisher": {
    "@type": "Organization",
    "name": "Matteo Courthoud",
    "logo": {
      "@type": "ImageObject",
      "url": "https://matteocourthoud.github.io/media/icon_hu03e9b3967b83fd39296ec9da5ff1ea05_201175_192x192_fill_lanczos_center_3.png"
    }
  },
  "description": "How to run valid experiments, with unplanned peeking and early stopping.\nA/B tests are the golden standard of causal inference because they allow us to make valid causal statements under minimal assumptions, thanks to randomization."
}
</script>

  

  

  
  
  
  
  
    <script src="https://cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.1.1/cookieconsent.min.js" integrity="sha256-5VhCqFam2Cn+yjw61zbBNrbHVJ6SRydPeKopYlngbiQ=" crossorigin="anonymous"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.1.1/cookieconsent.min.css" integrity="sha256-zQ0LblD/Af8vOppw18+2anxsuaz3pWYyVWi+bTvTH8Q=" crossorigin="anonymous">
  
  <script>
  window.addEventListener("load", function(){
    window.cookieconsent.initialise({
      "palette": {
        "popup": {
          "background": "#003f5c",
          "text": "rgb(255, 255, 255)"
        },
        "button": {
          "background": "rgb(255, 255, 255)",
          "text": "#003f5c"
        }
      },
      "theme": "classic",
      "content": {
        "message": "This website uses cookies to ensure you get the best experience on our website.",
        "dismiss": "Got it!",
        "link": "Learn more",
        "href": "/privacy/"
      }
    })});
  </script>


  





  <title>Understanding Group Sequential Testing | Matteo Courthoud</title>
</head>


<body id="top" data-spy="scroll" data-offset="70" data-target="#TableOfContents" class="page-wrapper   " data-wc-page-id="3dab538bbe7cb7bd428dac9a4ab9bcff" >

  
  
  
  
  
  
  
  
  <script src="/js/wowchemy-init.min.66d3e0fff6d32c4ece05adee927fbd96.js"></script>

  


<aside class="search-modal" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Search</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#" aria-label="Close"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        
        <input name="q" id="search-query" placeholder="Search..." autocapitalize="off"
        autocomplete="off" autocorrect="off" spellcheck="false" type="search" class="form-control"
        aria-label="Search...">
        
      </div>

      
      

      

    </section>
    <section class="section-search-results">

      <div id="search-hits">
        
      </div>

    </section>
  </div>
</aside>



  <div class="page-header">
    












<nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id="navbar-main">
  <div class="container-xl">

    
    <div class="d-none d-lg-inline-flex">
      <a class="navbar-brand" href="/">Matteo Courthoud</a>
    </div>
    

    
    <button type="button" class="navbar-toggler" data-toggle="collapse"
            data-target="#navbar-content" aria-controls="navbar-content" aria-expanded="false" aria-label="Toggle navigation">
    <span><i class="fas fa-bars"></i></span>
    </button>
    

    
    <div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none">
      <a class="navbar-brand" href="/">Matteo Courthoud</a>
    </div>
    

    
    
    <div class="navbar-collapse main-menu-item collapse justify-content-end" id="navbar-content">

      
      <ul class="navbar-nav d-md-inline-flex">
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
            
            
            
              
            
            
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#about"><span>Home</span></a>
        </li>

        
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
            
            
            
              
            
            
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#posts"><span>Posts</span></a>
        </li>

        
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
            
            
            
              
            
            
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#newsletter"><span>Newsletter</span></a>
        </li>

        
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
            
            
            
              
            
            
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#projects"><span>Research</span></a>
        </li>

        
        

        
        <li class="nav-item dropdown">
          <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown" aria-haspopup="true"><span>Courses</span><span class="caret"></span>
          </a>
          <div class="dropdown-menu">
            
              <a class="dropdown-item" href="/course/ml-econ/"><span>Machine Learning for Economics</span></a>
            
              <a class="dropdown-item" href="/course/data-science/"><span>Data Science with Python</span></a>
            
              <a class="dropdown-item" href="/course/empirical-io/"><span>PhD Industrial Organization</span></a>
            
              <a class="dropdown-item" href="/course/metrics/"><span>PhD Econometrics</span></a>
            
              <a class="dropdown-item" href="https://pp4rs.github.io/"><span>Programming Practices for Research</span></a>
            
          </div>
        </li>

        
        

        

        
        
        

        

        
        
        
        

        
          
            
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/cv"><span>CV</span></a>
        </li>

        
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
            
            
            
              
            
            
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#contact"><span>Contact</span></a>
        </li>

        
        

      

        
      </ul>
    </div>

    <ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2">

      
      
        
      

      
      

      
      

      
      

    </ul>

  </div>
</nav>


  </div>

  <div class="page-body">
    <div class="container-fluid docs">
  <div class="row">

    <div class="col-xl-2 col-lg-2 d-none d-xl-block d-lg-block empty">
    </div>

    <div class="col-2 col-xl-2 col-lg-2 d-none d-lg-block docs-toc">
      <ul class="nav toc-top">
        <li><a href="#" id="back_to_top" class="docs-toc-title">Contents</a></li>
      </ul>
      <nav id="TableOfContents">
  <ul>
    <li><a href="#simulation">Simulation</a></li>
    <li><a href="#peeking">Peeking</a></li>
    <li><a href="#alpha-corrections">Alpha Corrections</a>
      <ul>
        <li><a href="#bonferroni-correction">Bonferroni Correction</a></li>
        <li><a href="#corrections">Corrections</a></li>
      </ul>
    </li>
    <li><a href="#group-sequential-testing">Group Sequential Testing</a>
      <ul>
        <li><a href="#gst-pocock-approximation">GST Pocock Approximation</a></li>
        <li><a href="#gst-obrien--fleming-approximation">GST O’Brien &amp; Fleming Approximation</a></li>
      </ul>
    </li>
    <li><a href="#alpha-spending-trade-off">Alpha Spending Trade-off</a></li>
    <li><a href="#conclusion">Conclusion</a></li>
    <li><a href="#references">References</a>
      <ul>
        <li><a href="#related-articles">Related Articles</a></li>
        <li><a href="#code">Code</a></li>
      </ul>
    </li>
  </ul>
</nav>
    </div>

    <main class="col-xl-8 col-lg-8 docs-content" role="main">
        <article class="article">
        




















  


<div class="article-container pt-3">
  <h1>Understanding Group Sequential Testing</h1>

  

  


<div class="article-metadata">

  
  

  
  <span class="article-date">
    
    
      
    
    Dec 26, 2023
  </span>
  

  

  
  <span class="middot-divider"></span>
  <span class="article-reading-time">
    18 min read
  </span>
  

  
  
  
  
  
  

  
  

</div>

  





</div>


<div class="article-header article-container featured-image-wrapper mt-4 mb-4" style="max-width: 1574px; max-height: 894px;">
  <div style="position: relative">
    <img src="/post/group_sequential_testing/featured.png" alt="" class="featured-image">
    
  </div>
</div>


        <div class="article-container">
          <div class="article-style" align="justify">
            <p><em>How to run valid experiments, with unplanned peeking and early stopping.</em></p>
<p>A/B tests are the golden standard of causal inference because they allow us to make valid causal statements under minimal assumptions, thanks to <strong>randomization</strong>. In fact, by randomly assigning a <strong>treatment</strong> (a drug, ad, product, &hellip;), we are able to compare the <strong>outcome</strong> of interest (a disease, firm revenue, customer satisfaction, &hellip;) across <strong>subjects</strong> (patients, users, customers, &hellip;) and attribute the average difference in outcomes to the causal effect of the treatment.</p>
<p>The implementation of an A/B test is usually not instantaneous, especially in online settings. Often users are treated <strong>live</strong> or in <strong>batches</strong>. In these settings, one can look at the data before the data collection is completed, one or multiple times. This phenomenon is called <strong>peeking</strong>. While looking is not problematic in itself, using standard testing procedures when peeking can lead to <strong>misleading conclusions</strong>.</p>
<p>The <strong>solution</strong> to peeking is to adjust the testing procedure accordingly. The most famous and traditional approach is the so-called <strong>Sequential Probability Ratio Test (SPRT)</strong>, which dates back to the Second World War. If you want to know more about the test and its fascinating history, I wrote a blog post about it.</p>
<p><a href="https://towardsdatascience.com/954506cec665" target="_blank" rel="noopener">https://towardsdatascience.com/954506cec665</a></p>
<p>The main <strong>advantage</strong> of the Sequential Probability Ratio Test (SPRT) is that it guarantees the smallest possible sample size, given a target confidence level and power. However, the <strong>main problem</strong> with the SPRT is that it might continue indefinitely. This is a non-irrelevant problem in an applied setting with deadlines and budget constraints. In this article, we will explore an <strong>alternative method</strong> that allows <em>any</em> amount of intermediate peeks at the data, at <em>any</em> point of the data collection: <strong>Group Sequential Testing</strong>.</p>
<h2 id="simulation">Simulation</h2>
<p>Let&rsquo;s start with some simulated <strong>data</strong>. To keep the code as light as possible, I will abstract away from the experimental setting, and directly work with data coming out of a <strong>normal distribution</strong>. However, we can think of it as the distribution of the average treatment effect in a standard A/B test. The normal distribution is an asymptotic approximation based on the <a href="https://en.wikipedia.org/wiki/Central_limit_theorem" target="_blank" rel="noopener">central limit theorem</a>.</p>
<p>Before generating the data,  I import the relevant libraries and my plotting theme from <a href="https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/src/theme.py" target="_blank" rel="noopener">src.theme</a>.</p>
<pre><code class="language-python">%matplotlib inline
%config InlineBackend.figure_format = 'retina'
</code></pre>
<pre><code class="language-python">from src.theme import *
import numpy as np
import pandas as pd
import scipy as sp
</code></pre>
<p>Let&rsquo;s assume the true data generating process is indeed a normal distribution with <strong>mean</strong> <em>μ=1</em> and <strong>standard deviation</strong> <em>σ=5.644</em>. In the context of an A/B test, we can think of this as a positive average treatment effect with standard deviation more than 5 times larger than the effect.</p>
<pre><code class="language-python">mu = 1
sigma = 5.644
</code></pre>
<p>We would like to build a two-sided test with <em>95%</em> confidence and <em>80%</em> power. Therefore our target false positive error rate will be <em>α=0.05</em> and our target false negative error rate will be <em>β=0.2</em>.</p>
<pre><code class="language-python">alpha = 0.05
beta = 0.2
</code></pre>
<p>We can now compute the required sample size for the experiment, under the assumption of an average treatment effect of <em>1</em> and a standard deviation of <em>5.664</em>. Since we have abstracted from the two-group comparison, the formula for the power calculation is</p>
<p>$$
N = \Big( 2 \sigma  \frac{ z_{1-\alpha/2} + z_{1-\beta} }{ \mu } \Big)^2
$$</p>
<p>where <em>z</em> are the quantiles of a standard normal distribution, evaluated at <em>1-α/2</em> and <em>1-β</em>.</p>
<pre><code class="language-python">ppf = sp.stats.norm(0, 1).ppf
cdf = sp.stats.norm(0, 1).cdf
z_alpha = ppf(1 - alpha/2)
z_beta = ppf(1 - beta)
N = int((2 * sigma * (z_alpha + z_beta) / mu)**2)
N
</code></pre>
<pre><code>1000
</code></pre>
<p>We need <em>N=1000</em> observations to achieve our target confidence level of <em>95%</em> and power of <em>80%</em>.</p>
<p>We can now draw the simulated data. Since we will often compare the results across different simulations, we draw <em>K=10,000</em> sequences of <em>N=1,000</em> data point.</p>
<pre><code class="language-python">K = 10_000
np.random.seed(2)
obs = np.random.normal(mu, sigma, size=(N, K))
</code></pre>
<p>We are now ready to investigate peeking and group sequential testing.</p>
<h2 id="peeking">Peeking</h2>
<p>What happens if we <strong>peek</strong> at the data, <strong>before the end</strong> of the experiment?</p>
<p>Let&rsquo;s suppose for example that we have a look at the data every <em>50</em> observations, starting at <em>100</em>. One reason could be that the data arrives in batches, or that we peek every day as soon as we start working.</p>
<pre><code class="language-python">N_peek = np.arange(100, N+1, 50, dtype=int)
N_peek
</code></pre>
<pre><code>array([ 100,  150,  200,  250,  300,  350,  400,  450,  500,  550,  600,
        650,  700,  750,  800,  850,  900,  950, 1000])
</code></pre>
<p>Looking at the data of course is not a problem <em>per-se</em>. However, we might be tempted to <strong>draw conclusions</strong>, given what we observe. Suppose that our <em>naive</em> experimentation platform continuously reports the latest average, standard deviation and confidence interval, where the confidence interval is computed as</p>
<p>$$
\text{CI} _ n = \Big [ \hat{\mu} _ n - z _ {1 - \alpha / 2} \frac{ \hat{\sigma} _ n }{\sqrt{n}} \quad ; \quad \hat{\mu} _ n - z _ {1-\alpha / 2} \frac{ \hat{\sigma} _ n}{\sqrt{n}} \Big]
$$</p>
<p>where <em>n</em> is the number of samples, <em>μ̂ₙ</em> is the estimated sample average after <em>n</em> samples, <em>σ̂ₙ</em> is the estimated standard deviation after <em>n</em> samples, <em>α</em> is the significance level, and <em>z</em> is the <em>1-α/2</em> quantile of a standard normal distribution.</p>
<pre><code class="language-python">def select_alpha_naive(n, N, N_peek, alpha):
    return alpha
</code></pre>
<p>Suppose that we decide to <strong>stop the experiment</strong> as soon as we get one significant result.</p>
<p>Let&rsquo;s compute the confidence intervals that we would observe at each peeking point.</p>
<pre><code class="language-python">def compute_intervals(select_alpha, obs, N_peek, alpha=0.05, **kwargs):
    # Compute rolling mean and standard deviation
    N, K = np.shape(obs)
    ns = np.reshape(np.arange(1, N+1), (-1, 1))
    means = np.cumsum(obs, axis=0) / ns
    stdevs = np.sqrt(np.cumsum((obs - means)**2, axis=0) / ns)

    # Compute intervals at each peeking time
    df_intervals = pd.DataFrame({&quot;k&quot;: range(K)})
    df_intervals[&quot;rejected_0&quot;] = False
    df_intervals[&quot;rejected_1&quot;] = False
    df_intervals[&quot;length&quot;] = max(N_peek)
    for t, n in enumerate(N_peek):
        df_intervals[f&quot;mean{n}&quot;] = means[n-1, :]
        df_intervals[f&quot;width{n}&quot;] = ppf(1 - select_alpha(n, N, N_peek, alpha, **kwargs)/2) * stdevs[n-1, :] / np.sqrt(n)
        df_intervals[f&quot;lowerb{n}&quot;] = means[n-1, :] - df_intervals[f&quot;width{n}&quot;]
        df_intervals[f&quot;upperb{n}&quot;] = means[n-1, :] + df_intervals[f&quot;width{n}&quot;]
        df_intervals[f&quot;coverage{n}&quot;] = (df_intervals[f&quot;lowerb{n}&quot;] &lt;= mu) &amp; (df_intervals[f&quot;upperb{n}&quot;] &gt;= mu)
        df_intervals[&quot;rejected_0&quot;] = df_intervals[&quot;rejected_0&quot;] | (df_intervals[f&quot;lowerb{n}&quot;] &gt;= 0) | (df_intervals[f&quot;upperb{n}&quot;] &lt;= 0)
        df_intervals[f&quot;power{n}&quot;] = df_intervals[&quot;rejected_0&quot;]
        df_intervals[&quot;rejected_1&quot;] = df_intervals[&quot;rejected_1&quot;] | ~df_intervals[f&quot;coverage{n}&quot;]
        df_intervals[f&quot;falsep{n}&quot;] = df_intervals[&quot;rejected_1&quot;]
        df_intervals[&quot;length&quot;] = np.minimum(df_intervals[&quot;length&quot;], n) * df_intervals[&quot;rejected_0&quot;] + max(N_peek) * (1 - df_intervals[&quot;rejected_0&quot;])
    return df_intervals
</code></pre>
<pre><code class="language-python">dfi_naive = compute_intervals(select_alpha_naive, obs, N_peek)
</code></pre>
<p>What do these averages and confidence intervals look over time? In the figure below, I plot the cumulative average over the data collection, together with the confidence intervals at each peeking time.</p>
<pre><code class="language-python">def plot_peeking(dfi, obs, k=1):
    ns = np.reshape(np.arange(1, N+1), (-1, 1))
    means = np.cumsum(obs, axis=0) / ns
    # Plot
    fig, ax = plt.subplots()
    for n in N_peek:
        color = &quot;C1&quot; if dfi[f&quot;width{n}&quot;][k] &gt; dfi[f&quot;mean{n}&quot;][k] else &quot;C2&quot;
        ax.errorbar(x=[n-1], y=dfi[f&quot;mean{n}&quot;][k], yerr=dfi[f&quot;width{n}&quot;][k], c=color, lw=0, elinewidth=2, capsize=6, marker=&quot;o&quot;, markersize=6)
    sns.lineplot(x=range(30, N), y=means[:,k][30:], lw=3, ax=ax);
    ax.axhline(y=0, lw=1)
    ax.set(xlim=(20, N+10), xlabel=&quot;n&quot;, title=&quot;Average Effect and Confidence Intervals&quot;);
</code></pre>
<pre><code class="language-python">plot_peeking(dfi_naive, obs)
</code></pre>
<p><img src="img/group_sequential_testing_28_0.png" alt="png"></p>
<p>As we can see, the first seven times we look at the data the confidence intervals cross the zero line and hence we do not reject the null hypothesis of zero mean. I have highlighted these confidence intervals in orange. However, at the eighth look at the data, at <em>450</em> observations, the confidence interval does not cross the zero line and hence we <strong>reject the null hypothesis</strong> of no effect and we stop the experiment.</p>
<p>The problem of this procedure is very similar to <strong>multiple hypothesis testing</strong>: we are building the confidence intervals for a single look at the data and therefore a <em>single</em> decision, but instead we are making <em>multiple</em> decisions. In fact, we have <em>decided</em> not to stop the experiment seven times before reaching <em>450</em> observations and we have stopped it at <em>450</em>.</p>
<p>What are the <strong>consequences</strong> of peeking and early stopping? Let&rsquo;s have a look at what would happen if we were to repeat this experiment multiple times. We will now plot the confidence intervals for <em>100</em> different simulations at three different points in time: after <em>200</em>, <em>400</em> and <em>600</em> observations are collected. Note that these correspond to the <em>3rd</em>, <em>7th</em> and <em>11th</em> peek at the data, respectively.</p>
<pre><code class="language-python">N_plot = [200, 400, 600]
</code></pre>
<pre><code class="language-python">def plot_intervals(dfi, N_plot, power_coverage: str):
    fig, (axes) = plt.subplots(1, len(N_plot))
    x = 0 if power_coverage == &quot;power&quot; else mu
    df_short = dfi.iloc[:100, :].copy()
    
    # Plot intervals
    for k, i, ax in zip(range(len(N_plot)), N_plot, axes):
        # Plot intervals
        ax.errorbar(x=df_short[f&quot;mean{i}&quot;], y=df_short[&quot;k&quot;], xerr=df_short[f&quot;width{i}&quot;], 
                     fmt=&quot;&quot;, lw=0, alpha=0.3, elinewidth=1)
        ax.axvline(x=x, lw=1, ls=&quot;--&quot;, c=&quot;k&quot;)
    
        # Add wrong intervals
        temp = df_short.loc[df_short[f&quot;{power_coverage}{i}&quot;] == False]
        ax.errorbar(x=temp[f&quot;mean{i}&quot;], y=temp[&quot;k&quot;], xerr=temp[f&quot;width{i}&quot;], 
                     fmt=&quot;&quot;, lw=0, alpha=1, elinewidth=2)
    
        # Rest
        ax.set(xlim=(-0.2, 2.2))
        ax.set_title(f&quot;n = {i}&quot;, pad=10)
        ax.title.set_size(14)
        if k &gt; 0:
            ax.set_yticks([])
    plt.suptitle(power_coverage.capitalize(), y=1.01);
</code></pre>
<p>The first thing that we are going to inspect is <strong>coverage</strong>: do the confidence intervals actually <em>cover</em> the true treatment effect, as they are supposed to? I highlight the confidence intervals that don&rsquo;t.</p>
<pre><code class="language-python">plot_intervals(dfi_naive, N_plot, &quot;coverage&quot;)
</code></pre>
<p><img src="img/group_sequential_testing_33_0.png" alt="png"></p>
<p>It seems that our coverage is fine at each point in time. We have, respectively, <em>2</em>, <em>6</em>, and <em>2</em> simulations out of <em>100</em> in which the interval does not cover the true treatment effect, <em>μ=1</em>. This is expected since our confidence level is <em>5%</em> and therefore we expect <em>on average</em> that <em>5</em> intervals out of <em>100</em> do not cover the true treatment effect.</p>
<p>Next, we investigate <strong>power</strong>: the ability of our estimator to detect an effect when there is indeed one. Remember that power is always <em>relative</em> to the effect size. However, we did our power calculations using the true effect so we expect the experiment to have the expected power of <em>80%</em>.</p>
<p>Note that, since we are peeking, we reject the null hypothesis and stop the experiment <em>as soon as</em> one test is significant. Therefore, in our case, power at a specific point in time is the probability of rejecting the null hypothesis with that test or <em>any</em> of the previous ones.</p>
<pre><code class="language-python">plot_intervals(dfi_naive, N_plot, &quot;power&quot;)
</code></pre>
<p><img src="img/group_sequential_testing_35_0.png" alt="png"></p>
<p>As we can see, at <em>200</em> observations we already reject the null hypothesis of no effect (<em>μ=0</em>) in <em>72</em> simulations out of <em>100</em>, close to the target power of <em>80%</em>. However, at <em>400</em> observations we reject the null hypothesis in well over <em>80</em> simulations over <em>100</em>, suggesting that we could have run the experiment for a shorter amount of time.</p>
<p>It seems that so far everything is going great: our intervals cover the true effect and reject the null hypothesis even faster than expected. Let&rsquo;s check this for all the peeking stages and over <em>10,000</em> simulations. Let&rsquo;s also check a third metric: the <strong>false positive</strong> error rate. In order to compute that, we change the null hypothesis to <em>μ=1</em> and check how often we reject it. Again, since we are peeking multiple times, what counts is the rejection rate at a specific peeking stage or <em>any</em> of the previous ones.</p>
<pre><code class="language-python">def check_performance(dfi):
    df_perf = dfi[list(dfi.filter(regex=&quot;coverage|power|falsep&quot;))].copy()
    df_perf[&quot;idx&quot;] = 1
    df_perf = df_perf.groupby(&quot;idx&quot;, as_index=False).mean()
    df_perf = pd.wide_to_long(df_perf, stubnames=[&quot;coverage&quot;, &quot;power&quot;, &quot;falsep&quot;], i=&quot;idx&quot;, j=&quot;n&quot;).reset_index().drop(columns=&quot;idx&quot;)
    return df_perf
</code></pre>
<pre><code class="language-python">def plot_coverage_power(dfi, alpha=0.05, beta=0.8):
    df_perf = check_performance(dfi)
    
    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(14, 3))
    
    # Plot Coverage
    sns.lineplot(df_perf, x=&quot;n&quot;, y=&quot;coverage&quot;, c=&quot;C0&quot;, ax=ax1)
    ax1.axhline(y=1-alpha, ls=&quot;--&quot;, lw=2, c=&quot;C0&quot;)
    ax1.set(title=&quot;Coverage&quot;, ylabel=&quot;&quot;, ylim=(0.89, 1.01))
    
    # Plot power
    sns.lineplot(df_perf, x=&quot;n&quot;, y=&quot;power&quot;, c=&quot;C1&quot;, ax=ax2)
    ax2.axhline(y=beta, ls=&quot;--&quot;, lw=2, c=&quot;C1&quot;)
    ax2.set(title=&quot;Power&quot;, ylabel=&quot;&quot;, ylim=(0.5, 1.05))

    # Plot false positives
    sns.lineplot(df_perf, x=&quot;n&quot;, y=&quot;falsep&quot;, c=&quot;C2&quot;, ax=ax3)
    ax3.axhline(y=alpha, ls=&quot;--&quot;, lw=2, c=&quot;C2&quot;)
    ax3.set(title=&quot;False Rejections&quot;, ylabel=&quot;&quot;, ylim=(-0.01, 0.24))    
</code></pre>
<p>In the figure below, I plot coverage, power and false rejection rate over <em>10,000</em> simulations at each peeking stage.</p>
<pre><code class="language-python">plot_coverage_power(dfi_naive)
</code></pre>
<p><img src="img/group_sequential_testing_40_0.png" alt="png"></p>
<p>Coverage seems to be on target. Power is above <em>80%</em> starting at around <em>250</em> observations, confirming our previous insight. However, the false rejection rate is way higher than the target <em>5%</em>. This means that when the null hypothesis is true, we reject it more often than we should.</p>
<p>The last thing we want to check is whether the experiment is indeed shorter on average, and by how much. Let&rsquo;s compute the <strong>average experiment length</strong>, in terms of number of observations.</p>
<pre><code class="language-python">print(f&quot;Average length: n = {dfi_naive.length.mean():.0f}&quot;, )
</code></pre>
<pre><code>Average length: n = 177
</code></pre>
<p>On average we need just <em>177</em> observations to reach a concusion! However, because of the high false rejection rate, these might be the <strong>wrong conclusions</strong>.</p>
<p>What can we do to solve this issue? We need to build confidence intervals that take into account that we are doing multiple testing in sequence.</p>
<h2 id="alpha-corrections">Alpha Corrections</h2>
<p>In this section, we will explore a first set of corrections that modify the <em>α</em> value used to compute the confidence intervals in order to take into account peeking and early stopping.</p>
<h3 id="bonferroni-correction">Bonferroni Correction</h3>
<p>Since the peeking problem is similar to multiple hypothesis testing, we can start by applying the same solution.</p>
<p>The simplest way to account for multiple hypothesis testing is the so-called <a href="https://en.wikipedia.org/wiki/Bonferroni_correction" target="_blank" rel="noopener"><strong>Bonferroni correction</strong></a>. The idea is simple: decrease the significance level <em>α</em> proportionally to the number of looks. In particular, instead of using the same <em>α</em> for each look, we use
$$
\alpha_{\text{Bonferroni}} = \frac{\alpha}{P}
$$</p>
<p>where <em>P</em> is the number of times we plan to peek.</p>
<pre><code class="language-python">def select_alpha_bonferroni(n, N, N_peek, alpha):
    P = len(N_peek)
    return alpha / P
</code></pre>
<p>How does Bonferroni correction perform in terms of <strong>coverage</strong>? Let&rsquo;s plot the confidence intervals for three peeking stages: after <em>200</em>, <em>400</em>, and <em>600</em> observations are collected.</p>
<pre><code class="language-python">dfi_bonferroni = compute_intervals(select_alpha_bonferroni, obs, N_peek)
plot_intervals(dfi_bonferroni, N_plot, &quot;coverage&quot;)
</code></pre>
<p><img src="img/group_sequential_testing_50_0.png" alt="png"></p>
<p>Coverage looks great! Only once at <em>n=200</em> one interval did not cover the true value <em>μ=1</em>.</p>
<p>While this might appear comforting at first, it should actually raise an eyebrow. In fact, with a significance level <em>α=0.05</em> we expect a coverage of <em>95%</em>. A higher coverage will most likely come at the expense of <strong>power</strong>. Let&rsquo;s have a look.</p>
<pre><code class="language-python">plot_intervals(dfi_bonferroni, N_plot, &quot;power&quot;)
</code></pre>
<p><img src="img/group_sequential_testing_52_0.png" alt="png"></p>
<p>The test is clearly underpowered at <em>200</em> observations, while it is very close to the target power of <em>80%</em> at <em>400</em> observations. At <em>600</em> observations we have almost 100% power.</p>
<p>Let&rsquo;s plot coverage, power, and false positive rate for each peeking stage over <em>K=10,000</em> simulations.</p>
<pre><code class="language-python">plot_coverage_power(dfi_bonferroni)
</code></pre>
<p><img src="img/group_sequential_testing_54_0.png" alt="png"></p>
<p>Coverage is great, power is above target starting at  around <em>450</em> observations, and the false rejection rate is always below the target <em>5%</em>. What about the avearge experiment length?</p>
<pre><code class="language-python">print(f&quot;Average length: n = {dfi_bonferroni.length.mean():.0f}&quot;, )
</code></pre>
<pre><code>Average length: n = 317
</code></pre>
<p>The average experiment length is <em>317</em> observations, higher than the naive testing procedure, but still sensibly lower than the <em>1000</em> observations required without peeking.</p>
<p>It seems that everything looks good, maybe even <strong>too good</strong>. Indeed, there might be room for improvement. Given such high coverage and low false rejection rate, the results suggest that we could have shorter confidence intervals and hence higher power and lower experiment length, without dropping below <em>95%</em> coverage or above <em>5%</em> false rejection rate. How?</p>
<p>The Bonferroni correction has <strong>two drawbacks</strong>. First, it was <strong>not designed for sequential testing</strong>, but rather for multiple hypothesis testing. Second, even for multiple hypothesis testing, it is known to be very <strong>conservative</strong>.</p>
<h3 id="corrections">Corrections</h3>
<p>A first version of <strong>Bonferroni&rsquo;s correction for sequential testing</strong> was <a href="https://www.jstor.org/stable/2335684" target="_blank" rel="noopener">Pocock (1977)</a>. The idea was to take into account the <strong>sequential</strong> nature of testing which generates a very specific correlation structure between the test statistics. Thanks to this insight, Pocock was able to use a corrected <em>α</em> value that was in between the naive <em>α</em> and Bonferroni&rsquo;s <em>α/P</em>. A larger <em>α</em> than Benferroni implies higher power while keeping high coverage and a low false positive rate. The values are found through a numeric algorithm that takes as input the significance level <em>α</em> and the total number of peeks <em>P</em>.</p>
<p>The problem with Pocock correction was that it did not fully exploit the sequential nature of the testing, since the confidence intervals were constant over time. <a href="https://www.jstor.org/stable/2530245" target="_blank" rel="noopener">O’Brien, Fleming (1979)</a> proposed to use <strong>time-varying</strong> <em>α</em> corrections. Their idea was to adapt the width of the confidence interval not only to the significance level <em>α</em> and the total number of peeks <em>P</em>, but also the individual peek <em>p</em>.</p>
<p>However, the main drawback of all these procedures is that they require to <strong>plan</strong> the number of peeks in advance. This is often <em>not practical</em>, since peeking is an inherently spontaneous process, that comes from either the size of the data batch, pressure from management, or simply the experimenter&rsquo;s curiosity.</p>
<p>What can we do when peeking is <strong>not planned</strong> in advance?</p>
<h2 id="group-sequential-testing">Group Sequential Testing</h2>
<p><a href="https://www.jstor.org/stable/2336502" target="_blank" rel="noopener">Lan, DeMets (1983)</a> noticed that the important thing in peeking is not <em>how much</em> you peek, but rather <em>when</em> you peek. The main idea of <strong>Group Sequential Testing (GST)</strong> is to allow for peeking at any point in time, and correct the significance level for the peeking point in time in the data collection process, <em>t = n/N</em>.</p>
<p>The moving part of group sequential testing is the so-called <strong>alpha spending function</strong> that determines how to correct the significance level <em>α</em>, given peeking time <em>t</em>. In the rest of the article we are going to review two alpha spending functions that approximate the corrections of <a href="https://www.jstor.org/stable/2335684" target="_blank" rel="noopener">Pocock (1977)</a> and <a href="https://www.jstor.org/stable/2530245" target="_blank" rel="noopener">O’Brien, Fleming (1979)</a>, respectively.</p>
<h3 id="gst-pocock-approximation">GST Pocock Approximation</h3>
<p>The first alpha-spending function is an approximation of <a href="https://www.jstor.org/stable/2335684" target="_blank" rel="noopener">Pocock (1977)</a> and it is given by</p>
<p>$$
f(\alpha, t) = \alpha \ln \big( 1 + (e - 1) t \big)
$$</p>
<p>Note that as the share of observations <em>t=n/N</em> reaches the full sample (<em>t=1</em>), Pocock&rsquo;s correction converges to the original significance level <em>α</em>.</p>
<pre><code class="language-python">def select_alpha_gst_pocock(n, N, N_peek, alpha):
    t = n / N
    return alpha * np.log(1 + (np.exp(1) - 1) * t)
</code></pre>
<p>Let&rsquo;s see how group sequential testing using Pocock&rsquo;s alpha spending function works.</p>
<pre><code class="language-python">dfi_gst_pocock = compute_intervals(select_alpha_gst_pocock, obs, N_peek)
plot_coverage_power(dfi_gst_pocock)
</code></pre>
<p><img src="img/group_sequential_testing_66_0.png" alt="png"></p>
<p>As we noted before, coverage converges to the target coverage and the number of observations increases. The experiment seems also to be better powered than using Bonferroni&rsquo;s correction, but the false rejection rate increases above the target <em>5%</em> if the experiment runs too long.</p>
<p>What about the average experiment length?</p>
<pre><code class="language-python">print(f&quot;Average length: n = {dfi_gst_pocock.length.mean():.0f}&quot;, )
</code></pre>
<pre><code>Average length: n = 229
</code></pre>
<p>The average experiment length in indeed lower than Boferroni, with an average of <em>229</em> observations instead of <em>317</em>.</p>
<h3 id="gst-obrien--fleming-approximation">GST O’Brien &amp; Fleming Approximation</h3>
<p>The second alpha-spending function is an approximation of <a href="https://www.jstor.org/stable/2530245" target="_blank" rel="noopener">O’Brien, Fleming (1979)</a> and is given by</p>
<p>$$
f(\alpha, t) = 4 - 4 \Phi \Big( \Phi^{-1} \big(1 - \alpha/4 \big) \ / \ t^{\rho/2} \Big)
$$</p>
<p>where <strong>Φ</strong> is the cumulative distribution function (CDF) of a standard normal distribution, and <em>ρ</em> is a free parameter that is usually defaulted to <em>ρ=1</em>.</p>
<pre><code class="language-python">def select_alpha_gst_obrien_fleming(n, N, N_peek, alpha, rho=1):
    t = n / N
    return 4 - 4 * cdf(ppf(1 - alpha/4) / t**(rho/2))
</code></pre>
<p>Let&rsquo;s see how group sequential testing using O&rsquo;Brien and Fleming approximation performs over <em>K=10,000</em> simulations.</p>
<pre><code class="language-python">dfi_gst_obrien_fleming = compute_intervals(select_alpha_gst_obrien_fleming, obs, N_peek)
plot_coverage_power(dfi_gst_obrien_fleming)
</code></pre>
<p><img src="img/group_sequential_testing_74_0.png" alt="png"></p>
<p>It seems that the O’Brien and Fleming approximation is more conservative than Pocock&rsquo;s, with a higher coverage and lower power, but keeping the false rejection rate closer to the <em>5%</em> target.</p>
<pre><code class="language-python">print(f&quot;Average length: n = {dfi_gst_obrien_fleming.length.mean():.0f}&quot;, )
</code></pre>
<pre><code>Average length: n = 414
</code></pre>
<p>The average experiment length is actually higher than Boferroni, with an average of <em>414</em> observations instead of <em>317</em>. However, it can be lowered by decreasing the parameter <em>ρ</em> in the correction formula. Let&rsquo;s use for example <em>ρ=0.5</em> which corresponds to <a href="https://www.jstor.org/stable/2531959" target="_blank" rel="noopener">Wang, Tsiatis (1987)</a> correction.</p>
<pre><code class="language-python">dfi_gst_obrien_fleming_05 = compute_intervals(select_alpha_gst_obrien_fleming, obs, N_peek, rho=0.5)
print(f&quot;Average length: n = {dfi_gst_obrien_fleming_05.length.mean():.0f}&quot;, )
</code></pre>
<pre><code>Average length: n = 303
</code></pre>
<p>Indeed, with a lower <em>ρ</em> we have decrease the average experiment length from <em>414</em> to <em>303</em> observations.</p>
<h2 id="alpha-spending-trade-off">Alpha Spending Trade-off</h2>
<p>Before concluding, it is worth having a look at the peeking trade-offs. We have introduced a method that allows us to do valid inference while peeking any number of times, whenever we feel like. But <strong>should we peek</strong>? And, if so, <strong>how much</strong>?</p>
<p>In the figure below, I plot the testing performance using group sequential testing with Pocock&rsquo;s approximation, when we <strong>increase the peeking frequency</strong> from <em>50</em> to <em>10</em> observations.</p>
<pre><code class="language-python">N_peek_10 = np.arange(30, N+1, 10, dtype=int)
dfi_gst_10 = compute_intervals(select_alpha_gst_pocock, obs, N_peek_10)
plot_coverage_power(dfi_gst_10)
</code></pre>
<p><img src="img/group_sequential_testing_82_0.png" alt="png"></p>
<p>As we can see, coverage is basically unaffected, while power and false rejections have increased. The average experiment length has also decreased from <em>229</em> to <em>188</em> observations.</p>
<pre><code class="language-python">print(f&quot;Average length: n = {dfi_gst_10.length.mean():.0f}&quot;, )
</code></pre>
<pre><code>Average length: n = 188
</code></pre>
<p>What if instead we <strong>reduced the peeking frequency</strong>? In the figure below, I plot the results when peeking every 200 observations.</p>
<pre><code class="language-python">N_peek_200 = np.arange(200, N+1, 200, dtype=int)
dfi_gst_200 = compute_intervals(select_alpha_gst_pocock, obs, N_peek_200)
plot_coverage_power(dfi_gst_200)
</code></pre>
<p><img src="img/group_sequential_testing_86_0.png" alt="png"></p>
<p>From the figure, we see the opposite result: power and false rejections have decreased. On the other hand, we now need on average <em>311</em> observations to reach a conclusion instead of <em>229</em>.</p>
<pre><code class="language-python">print(f&quot;Average length: n = {dfi_gst_200.length.mean():.0f}&quot;, )
</code></pre>
<pre><code>Average length: n = 311
</code></pre>
<h2 id="conclusion">Conclusion</h2>
<p>In this article, we have explored <strong>group sequential testing</strong>, a procedure to do valid inference when peeking during an A/B test, any number of times, and at any point during the experiment. We have also seen how peeking does not come for free. The main <strong>trade-off</strong> is that the more we peek, the earlier we can stop an experiment but also the higher the false rejection rate.</p>
<p>There are at least a couple of topics that I have not mentioned in the article, not to make it too long. The first one is <strong>bias</strong>. Sequential tests can easily introduce bias since early stopping could be due to either a low variance or a large effect. Because of the second, sequential tests can often lead to the <em>overestimation</em> of treatment effects. This phenomenon is often called the <em>winner&rsquo;s curse</em> and typically occurs when the study is underpowered, which is happens at the early peeking stages. One solution is to design a <strong>beta spending</strong> function.</p>
<p>The second topic that I didn&rsquo;t cover is what is called <strong>stopping for futility</strong>. In the examples of this article, we stopped experiments early if we got a statistically significant estimate. However, peeking can also inform a different stopping rule: stopping because it becomes extremely unlikely that continuing the test can produce significant results.</p>
<p>One last topic I have not covered is how to do <strong>power analysis</strong> with sequential testing. In the example above, we ran the power analysis at the very beginning assuming no peeking. However, given that we knew we would have peeked, we could have anticipated the need for a smaller sample. A closely related topic is <strong>optimal peeking</strong>. Once you decide to peek, when should you do it?</p>
<h2 id="references">References</h2>
<ul>
<li>Lakens, Pahlke, Wassmer (2021). <a href="https://osf.io/preprints/psyarxiv/x4azm" target="_blank" rel="noopener">Group Sequential Designs: A Tutorial</a></li>
<li>Lan, DeMets (1983). <a href="https://academic.oup.com/biomet/article-abstract/70/3/659/247777" target="_blank" rel="noopener">Discrete Sequential Boundaries for Clinical Trials</a></li>
<li>Spotify (2023). <a href="https://engineering.atspotify.com/2023/03/choosing-sequential-testing-framework-comparisons-and-discussions/" target="_blank" rel="noopener">Choosing a Sequential Testing Framework</a></li>
</ul>
<h3 id="related-articles">Related Articles</h3>
<ul>
<li><a href="https://towardsdatascience.com/954506cec665" target="_blank" rel="noopener">Experiments, Peeking, and Optimal Stopping</a></li>
</ul>
<h3 id="code">Code</h3>
<p>You can find the original Jupyter Notebook here:</p>
<p><a href="https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/group_sequential_testing.ipynb" target="_blank" rel="noopener">https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/group_sequential_testing.ipynb</a></p>

          </div>
          


















  
  



  
  
  
    
  
  
  
  <div class="media author-card content-widget-hr">
    
      
      <a href="https://matteocourthoud.github.io/"><img class="avatar mr-3 avatar-circle" src="/author/avatar_hu365eedc833ccd5578a90de7c849ec45e_385094_270x270_fill_q75_lanczos_center.jpg" alt=""></a>
    

    <div class="media-body">
      <h5 class="card-title"><a href="https://matteocourthoud.github.io/"></a></h5>
      
      <p class="card-text">I hold a PhD in economics from the University of Zurich. Now I work at the intersection of economics, data science and statistics. I regularly write about causal inference on <a href="https://medium.com/@matteo.courthoud" target="_blank" rel="noopener">Medium</a>.</p>
      <ul class="network-icon" aria-hidden="true">
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://medium.com/@matteo.courthoud" target="_blank" rel="noopener">
        <i class="fab fa-medium"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://www.linkedin.com/in/matteo-courthoud-7335198a/" target="_blank" rel="noopener">
        <i class="fab fa-linkedin"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://twitter.com/MatteoCourthoud/" target="_blank" rel="noopener">
        <i class="fab fa-twitter"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://github.com/matteocourthoud" target="_blank" rel="noopener">
        <i class="fab fa-github"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://open.spotify.com/user/1180947523" target="_blank" rel="noopener">
        <i class="fab fa-spotify"></i>
      </a>
    </li>
  
</ul>

    </div>
  </div>
















  
  




        </div>
        </article>
    </main>
  </div>
</div>
  </div>

  <div class="page-footer">
    
    
    <div class="container">
      <footer class="site-footer">

  

  
  <p class="powered-by">
    
      <a href="/privacy/">Privacy Policy</a>
    
    
       &middot; 
      <a href="/terms/">Terms</a>
    
  </p>
  

  

  
  







</footer>

    </div>
    
  </div>

  
<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        <pre><code class="tex hljs"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

      

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/instant.page/5.1.0/instantpage.min.js" integrity="sha512-1+qUtKoh9XZW7j+6LhRMAyOrgSQKenQ4mluTR+cvxXjP1Z54RxZuzstR/H9kgPXQsVB8IW7DMDFUJpzLjvhGSQ==" crossorigin="anonymous"></script>

      
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js" integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin="anonymous"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js" integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin="anonymous"></script>
      

      
      

      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mermaid/8.8.4/mermaid.min.js" integrity="sha512-as1BF4+iHZ3BVO6LLDQ7zrbvTXM+c/1iZ1qII/c3c4L8Rn5tHLpFUtpaEtBNS92f+xGsCzsD7b62XP3XYap6oA==" crossorigin="anonymous" title="mermaid"></script>
      

      
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/highlight.min.js" integrity="sha512-TDKKr+IvoqZnPzc3l35hdjpHD0m+b2EC2SrLEgKDRWpxf2rFCxemkgvJ5kfU48ip+Y+m2XVKyOCD85ybtlZDmw==" crossorigin="anonymous"></script>
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/languages/r.min.js"></script>
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/languages/python.min.js"></script>
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/languages/julia.min.js"></script>
        
      

    

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.7.1/leaflet.min.js" integrity="sha512-SeiQaaDh73yrb56sTW/RgVdi/mMqNeM2oBwubFHagc5BkixSpP1fvqF47mKzPGWYSSy4RwbBunrJBQ4Co8fRWA==" crossorigin="anonymous"></script>
    

    
    

    
    
    
      
      <script id="search-hit-fuse-template" type="text/x-template">
        <div class="search-hit" id="summary-{{key}}">
          <div class="search-hit-content">
            <div class="search-hit-name">
              <a href="{{relpermalink}}">{{title}}</a>
              <div class="article-metadata search-hit-type">{{type}}</div>
              <p class="search-hit-description">{{snippet}}</p>
            </div>
          </div>
        </div>
      </script>
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js" integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin="anonymous"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js" integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin="anonymous"></script>
      
    

    
    

    
    
    
    

    
    <script src="/js/bootstrap.bundle.min.6aed84840afc03ab4d5750157f69c120.js"></script>

    
    
      
      
      
      
      
      
      
    

    
    
    
    
    
    
    
    
      
      
    
    
    <script src="/en/js/wowchemy.min.c8b7c648795740c04de2ef756725ef48.js"></script>

    
  <script async defer src="https://buttons.github.io/buttons.js"></script>




</body>
</html>
