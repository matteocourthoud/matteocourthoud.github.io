<!DOCTYPE html><html lang="en-us" >

<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  
  
  
    <meta name="generator" content="Wowchemy 5.1.0 for Hugo" />
  

  

  
  
  
  
  
    
    
    
  
  

  

  
  
  
    
  
  <meta name="description" content="How to run valid experiments with less observations.
A/B tests are the golden standard of causal inference because they allow us to make valid causal statements under minimal assumptions, thanks to randomization." />

  
  <link rel="alternate" hreflang="en-us" href="https://matteocourthoud.github.io/post/group_sequential_testing/" />

  







  




  
  
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
  

  
  
  
    <meta name="theme-color" content="#003f5c" />
  

  
  
    
    <script src="/js/mathjax-config.js"></script>
  

  
  
  
  
    
    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha256-FMvZuGapsJLjouA6k7Eo2lusoAX9i0ShlWFG6qt7SLc=" crossorigin="anonymous">

    
    

    
    
    
      
    
    
      
      
        
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/styles/github.min.css" crossorigin="anonymous" title="hl-light" media="print" onload="this.media='all'">
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/styles/dracula.min.css" crossorigin="anonymous" title="hl-dark" media="print" onload="this.media='all'" disabled>
        
      
    

    
    
    
      <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.7.1/leaflet.min.css" integrity="sha512-1xoFisiGdy9nvho8EgXuXvnpR5GAMSjFwp40gSRE3NwdUdIMIKuPa7bqoUhLD0O/5tPNhteAsE5XyyMi5reQVA==" crossorigin="anonymous" media="print" onload="this.media='all'">
    

    

    
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js" integrity="" crossorigin="anonymous" async></script>
      
    
      
      

      
      

      
    
      
      

      
      

      
    
  

  
  
  
    
      
      <link rel="preload" as="style" href="https://fonts.googleapis.com/css2?family=Roboto+Mono&family=Source+Sans+Pro:wght@200;300;400;700&display=swap">
      <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Roboto+Mono&family=Source+Sans+Pro:wght@200;300;400;700&display=swap" media="print" onload="this.media='all'">
    
  

  
  
  
  
  
  <link rel="stylesheet" href="/css/wowchemy.036b5b2acdb844b842ed3e91242fe237.css" />

  




<script async src="https://www.googletagmanager.com/gtag/js?id=UA-144780600-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];

  function gtag() {
      dataLayer.push(arguments);
  }

  function trackOutboundLink(url, target) {
    gtag('event', 'click', {
         'event_category': 'outbound',
         'event_label': url,
         'transport_type': 'beacon',
         'event_callback': function () {
           if (target !== '_blank') {
             document.location = url;
           }
         }
    });
    console.debug("Outbound link clicked: " + url);
  }

  function onClickCallback(event) {
    if ((event.target.tagName !== 'A') || (event.target.host === window.location.host)) {
      return;
    }
    trackOutboundLink(event.target, event.target.getAttribute('target'));  
  }

  gtag('js', new Date());
  gtag('config', 'UA-144780600-1', { 'anonymize_ip': true });
  gtag('set', {'cookie_flags': 'SameSite=None;Secure'});

  
  document.addEventListener('click', onClickCallback, false);
</script>


  

  

  




  
  
  

  

  
    <link rel="manifest" href="/index.webmanifest" />
  

  <link rel="icon" type="image/png" href="/media/icon_hu03e9b3967b83fd39296ec9da5ff1ea05_201175_32x32_fill_lanczos_center_3.png" />
  <link rel="apple-touch-icon" type="image/png" href="/media/icon_hu03e9b3967b83fd39296ec9da5ff1ea05_201175_180x180_fill_lanczos_center_3.png" />

  <link rel="canonical" href="https://matteocourthoud.github.io/post/group_sequential_testing/" />

  
  
  
  
  
  
  
  
    
  
  
  <meta property="twitter:card" content="summary_large_image" />
  
  <meta property="og:site_name" content="Matteo Courthoud" />
  <meta property="og:url" content="https://matteocourthoud.github.io/post/group_sequential_testing/" />
  <meta property="og:title" content="Understanding Group Sequential Testing | Matteo Courthoud" />
  <meta property="og:description" content="How to run valid experiments with less observations.
A/B tests are the golden standard of causal inference because they allow us to make valid causal statements under minimal assumptions, thanks to randomization." /><meta property="og:image" content="https://matteocourthoud.github.io/post/group_sequential_testing/featured.png" />
    <meta property="twitter:image" content="https://matteocourthoud.github.io/post/group_sequential_testing/featured.png" /><meta property="og:locale" content="en-us" />
  
    
      <meta
        property="article:published_time"
        content="2023-12-24T00:00:00&#43;00:00"
      />
    
    <meta property="article:modified_time" content="2023-12-24T00:00:00&#43;00:00">
  

  


    






  





<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://matteocourthoud.github.io/post/group_sequential_testing/"
  },
  "headline": "Understanding Group Sequential Testing",
  
  "image": [
    "https://matteocourthoud.github.io/post/group_sequential_testing/featured.png"
  ],
  
  "datePublished": "2023-12-24T00:00:00Z",
  "dateModified": "2023-12-24T00:00:00Z",
  
  "publisher": {
    "@type": "Organization",
    "name": "Matteo Courthoud",
    "logo": {
      "@type": "ImageObject",
      "url": "https://matteocourthoud.github.io/media/icon_hu03e9b3967b83fd39296ec9da5ff1ea05_201175_192x192_fill_lanczos_center_3.png"
    }
  },
  "description": "How to run valid experiments with less observations.\nA/B tests are the golden standard of causal inference because they allow us to make valid causal statements under minimal assumptions, thanks to randomization."
}
</script>

  

  

  
  
  
  
  
    <script src="https://cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.1.1/cookieconsent.min.js" integrity="sha256-5VhCqFam2Cn+yjw61zbBNrbHVJ6SRydPeKopYlngbiQ=" crossorigin="anonymous"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.1.1/cookieconsent.min.css" integrity="sha256-zQ0LblD/Af8vOppw18+2anxsuaz3pWYyVWi+bTvTH8Q=" crossorigin="anonymous">
  
  <script>
  window.addEventListener("load", function(){
    window.cookieconsent.initialise({
      "palette": {
        "popup": {
          "background": "#003f5c",
          "text": "rgb(255, 255, 255)"
        },
        "button": {
          "background": "rgb(255, 255, 255)",
          "text": "#003f5c"
        }
      },
      "theme": "classic",
      "content": {
        "message": "This website uses cookies to ensure you get the best experience on our website.",
        "dismiss": "Got it!",
        "link": "Learn more",
        "href": "/privacy/"
      }
    })});
  </script>


  





  <title>Understanding Group Sequential Testing | Matteo Courthoud</title>
</head>


<body id="top" data-spy="scroll" data-offset="70" data-target="#TableOfContents" class="page-wrapper   " data-wc-page-id="3dab538bbe7cb7bd428dac9a4ab9bcff" >

  
  
  
  
  
  
  
  
  <script src="/js/wowchemy-init.min.66d3e0fff6d32c4ece05adee927fbd96.js"></script>

  


<aside class="search-modal" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Search</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#" aria-label="Close"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        
        <input name="q" id="search-query" placeholder="Search..." autocapitalize="off"
        autocomplete="off" autocorrect="off" spellcheck="false" type="search" class="form-control"
        aria-label="Search...">
        
      </div>

      
      

      

    </section>
    <section class="section-search-results">

      <div id="search-hits">
        
      </div>

    </section>
  </div>
</aside>



  <div class="page-header">
    












<nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id="navbar-main">
  <div class="container-xl">

    
    <div class="d-none d-lg-inline-flex">
      <a class="navbar-brand" href="/">Matteo Courthoud</a>
    </div>
    

    
    <button type="button" class="navbar-toggler" data-toggle="collapse"
            data-target="#navbar-content" aria-controls="navbar-content" aria-expanded="false" aria-label="Toggle navigation">
    <span><i class="fas fa-bars"></i></span>
    </button>
    

    
    <div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none">
      <a class="navbar-brand" href="/">Matteo Courthoud</a>
    </div>
    

    
    
    <div class="navbar-collapse main-menu-item collapse justify-content-end" id="navbar-content">

      
      <ul class="navbar-nav d-md-inline-flex">
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
            
            
            
              
            
            
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#about"><span>Home</span></a>
        </li>

        
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
            
            
            
              
            
            
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#posts"><span>Posts</span></a>
        </li>

        
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
            
            
            
              
            
            
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#newsletter"><span>Newsletter</span></a>
        </li>

        
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
            
            
            
              
            
            
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#projects"><span>Research</span></a>
        </li>

        
        

        
        <li class="nav-item dropdown">
          <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown" aria-haspopup="true"><span>Courses</span><span class="caret"></span>
          </a>
          <div class="dropdown-menu">
            
              <a class="dropdown-item" href="/course/ml-econ/"><span>Machine Learning for Economics</span></a>
            
              <a class="dropdown-item" href="/course/data-science/"><span>Data Science with Python</span></a>
            
              <a class="dropdown-item" href="/course/empirical-io/"><span>PhD Industrial Organization</span></a>
            
              <a class="dropdown-item" href="/course/metrics/"><span>PhD Econometrics</span></a>
            
              <a class="dropdown-item" href="https://pp4rs.github.io/"><span>Programming Practices for Research</span></a>
            
          </div>
        </li>

        
        

        

        
        
        

        

        
        
        
        

        
          
            
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/cv"><span>CV</span></a>
        </li>

        
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
            
            
            
              
            
            
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#contact"><span>Contact</span></a>
        </li>

        
        

      

        
      </ul>
    </div>

    <ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2">

      
      
        
      

      
      

      
      

      
      

    </ul>

  </div>
</nav>


  </div>

  <div class="page-body">
    <div class="container-fluid docs">
  <div class="row">

    <div class="col-xl-2 col-lg-2 d-none d-xl-block d-lg-block empty">
    </div>

    <div class="col-2 col-xl-2 col-lg-2 d-none d-lg-block docs-toc">
      <ul class="nav toc-top">
        <li><a href="#" id="back_to_top" class="docs-toc-title">Contents</a></li>
      </ul>
      <nav id="TableOfContents">
  <ul>
    <li><a href="#simulation">Simulation</a></li>
    <li><a href="#experiment">Experiment</a></li>
    <li><a href="#peeking">Peeking</a></li>
    <li><a href="#constant-corrections">Constant Corrections</a>
      <ul>
        <li><a href="#bonferroni-correction">Bonferroni Correction</a></li>
        <li><a href="#corrections">Corrections</a></li>
      </ul>
    </li>
    <li><a href="#group-sequential-testing">Group Sequential Testing</a>
      <ul>
        <li><a href="#gst-pocock-approximation">GST Pocock Approximation</a></li>
        <li><a href="#gst-obrien--fleming-approximation">GST O’Brien &amp; Fleming Approximation</a></li>
      </ul>
    </li>
    <li><a href="#alpha-spending-trade-off">Alpha Spending Trade-off</a></li>
    <li><a href="#conclusion">Conclusion</a></li>
    <li><a href="#references">References</a>
      <ul>
        <li><a href="#related-articles">Related Articles</a></li>
        <li><a href="#code">Code</a></li>
      </ul>
    </li>
  </ul>
</nav>
    </div>

    <main class="col-xl-8 col-lg-8 docs-content" role="main">
        <article class="article">
        




















  


<div class="article-container pt-3">
  <h1>Understanding Group Sequential Testing</h1>

  

  


<div class="article-metadata">

  
  

  
  <span class="article-date">
    
    
      
    
    Dec 24, 2023
  </span>
  

  

  
  <span class="middot-divider"></span>
  <span class="article-reading-time">
    13 min read
  </span>
  

  
  
  
  
  
  

  
  

</div>

  





</div>


<div class="article-header article-container featured-image-wrapper mt-4 mb-4" style="max-width: 1574px; max-height: 894px;">
  <div style="position: relative">
    <img src="/post/group_sequential_testing/featured.png" alt="" class="featured-image">
    
  </div>
</div>


        <div class="article-container">
          <div class="article-style" align="justify">
            <p><em>How to run valid experiments with less observations.</em></p>
<p>A/B tests are the golden standard of causal inference because they allow us to make valid causal statements under minimal assumptions, thanks to <strong>randomization</strong>. In fact, by randomly assigning a <strong>treatment</strong> (a drug, ad, product, &hellip;), we are able to compare the <strong>outcome</strong> of interest (a disease, firm revenue, customer satisfaction, &hellip;) across <strong>subjects</strong> (patients, users, customers, &hellip;) and attribute the average difference in outcomes to the causal effect of the treatment.</p>
<p>The implementation of an A/B test is usually not instantaneous, especially in online settings. Often users are treated <strong>live</strong> or in <strong>batches</strong>. In these settings, one can look at the data before the data collection is completed, one or multiple times. This phenomenon is called <strong>peeking</strong>. While looking is not problematic, it is tempting to draw conclusion from these intermediate results. However, using standard testing procedures when peeking can lead to <strong>misleading conclusions</strong>.</p>
<p>The <strong>solution</strong> to peeking is to adjust our testing procedure accordingly. The most famous and traditional approach is the so-called <strong>Sequential Probability Ratio Test (SPRT)</strong>, which dates back to the Second World War. If you want to know more about the test and its fascinating history, I wrote a blog post about it.</p>
<p><a href="https://towardsdatascience.com/954506cec665" target="_blank" rel="noopener">https://towardsdatascience.com/954506cec665</a></p>
<p>The main <strong>advantage</strong> of the Sequential Probability Ratio Test (SPRT) is that it guarantees the smallest possible sample size, given a target confidence level and power. However, the <strong>main problem</strong> with the SPRT is that it might continue indefinitely.</p>
<p>As you can imagine, this is a non-irrelevant problem in an applied setting with deadlines and budget constraints. In this article, we will explore an <strong>alternative method</strong> that allows <em>any</em> amount of intermediate peeks at the data, at <em>any</em> point of the data collection: <strong>Group Sequential Testing</strong>.</p>
<h2 id="simulation">Simulation</h2>
<pre><code class="language-python">%matplotlib inline
%config InlineBackend.figure_format = 'retina'
</code></pre>
<pre><code class="language-python">from src.theme import *
</code></pre>
<pre><code class="language-python">import numpy as np
import pandas as pd
import scipy as sp
</code></pre>
<pre><code class="language-python">K = 10_000
mu = 1
sigma = 5.644
alpha = 0.05
beta = 0.8
</code></pre>
<h2 id="experiment">Experiment</h2>
<pre><code class="language-python">ppf = sp.stats.norm(0, 1).ppf
cdf = sp.stats.norm(0, 1).cdf
</code></pre>
<pre><code class="language-python">z_alpha = ppf(1 - alpha/2)
z_beta = ppf(beta)
N = int((2 * sigma * (z_alpha + z_beta) / mu)**2)
N
</code></pre>
<pre><code>1000
</code></pre>
<pre><code class="language-python">np.random.seed(2)
obs = np.random.normal(mu, sigma, size=(N, K))
</code></pre>
<h2 id="peeking">Peeking</h2>
<p>What happens if we <strong>peek</strong> at the data, <strong>before the end</strong> of the experiment?</p>
<p>Let&rsquo;s suppose for example that we have a look at the data every 50 observations. One reason could be that the data arrives in batches, or that we peek every day as soon as we start working.</p>
<pre><code class="language-python">N_peek = np.arange(100, N+1, 50, dtype=int)
N_peek
</code></pre>
<pre><code>array([ 100,  150,  200,  250,  300,  350,  400,  450,  500,  550,  600,
        650,  700,  750,  800,  850,  900,  950, 1000])
</code></pre>
<p>Looking at the data of course is not a problem <em>per-se</em>. However, we might be tempted to draw conclusions given what we observe. Suppose that our <em>naive</em> experimentation platform continuously reports the latest average, standard deviation and confidence interval, where the confidence interval is simply computed as</p>
<p>$$
\text{CI}_n = \Big [ \hat{\mu}<em>n - z</em>{1-\alpha/2} \frac{\hat{\sigma}_n}{\sqrt{n}} \quad ; \quad \hat{\mu}<em>n - z</em>{1-\alpha/2} \frac{\hat{\sigma}_n}{\sqrt{n}} \Big]
$$</p>
<p>where <em>n</em> is the number of samples, <em>μ̂ₙ</em> is the estimated sample average after <em>n</em> samples, <em>σ̂ₙ</em> is the estimated standard deviation after <em>n</em> samples, <em>α</em> is the significance level, and <em>z</em> is the <em>1-α/2</em> quantile of a standard normal distribution.</p>
<pre><code class="language-python">def select_alpha_naive(n, N, N_peek, alpha):
    return alpha
</code></pre>
<p>Let&rsquo;s compute the intervals at each point in time, for <em>K</em> different simulations.</p>
<pre><code class="language-python">def compute_intervals(select_alpha, obs, N_peek, alpha=0.05):
    # Compute rolling mean and standard deviation
    N, K = np.shape(obs)
    ns = np.reshape(np.arange(1, N+1), (-1, 1))
    means = np.cumsum(obs, axis=0) / ns
    stdevs = np.sqrt(np.cumsum((obs - means)**2, axis=0) / ns)

    # Compute intervals at each peeking time
    df_intervals = pd.DataFrame({&quot;k&quot;: range(K)})
    df_intervals[&quot;rejected_0&quot;] = False
    df_intervals[&quot;rejected_1&quot;] = False
    for t, n in enumerate(N_peek):
        df_intervals[f&quot;mean{n}&quot;] = means[n-1, :]
        df_intervals[f&quot;width{n}&quot;] = ppf(1 - select_alpha(n, N, N_peek, alpha)/2) * stdevs[n-1, :] / np.sqrt(n)
        df_intervals[f&quot;lowerb{n}&quot;] = means[n-1, :] - df_intervals[f&quot;width{n}&quot;]
        df_intervals[f&quot;upperb{n}&quot;] = means[n-1, :] + df_intervals[f&quot;width{n}&quot;]
        df_intervals[f&quot;coverage{n}&quot;] = (df_intervals[f&quot;lowerb{n}&quot;] &lt;= mu) &amp; (df_intervals[f&quot;upperb{n}&quot;] &gt;= mu)
        df_intervals[&quot;rejected_0&quot;] = df_intervals[&quot;rejected_0&quot;] | (df_intervals[f&quot;lowerb{n}&quot;] &gt;= 0) | (df_intervals[f&quot;upperb{n}&quot;] &lt;= 0)
        df_intervals[f&quot;power{n}&quot;] = df_intervals[&quot;rejected_0&quot;]
        df_intervals[&quot;rejected_1&quot;] = df_intervals[&quot;rejected_1&quot;] | ~df_intervals[f&quot;coverage{n}&quot;]
        df_intervals[f&quot;falsep{n}&quot;] = df_intervals[&quot;rejected_1&quot;]
    return df_intervals
</code></pre>
<pre><code class="language-python">dfi_naive = compute_intervals(select_alpha_naive, obs, N_peek)
</code></pre>
<p>What do these averages and confidence intervals look over time? In the figure below, I plot the cumulative average over the data collection, together with the confidence intervals at each peeking time.</p>
<pre><code class="language-python">def plot_peeking(dfi, obs, k=1):
    ns = np.reshape(np.arange(1, N+1), (-1, 1))
    means = np.cumsum(obs, axis=0) / ns
    # Plot
    fig, ax = plt.subplots()
    for n in N_peek:
        ax.errorbar(x=[n-1], y=dfi[f&quot;mean{n}&quot;][k], yerr=dfi[f&quot;width{n}&quot;][k], 
                    c=&quot;C1&quot;, lw=0, elinewidth=2, capsize=6, marker=&quot;o&quot;, markersize=6)
    sns.lineplot(x=range(30, N), y=means[:,k][30:], lw=3, ax=ax);
    ax.axhline(y=0, lw=1)
    ax.set(xlim=(20, N+10), title=&quot;Average Effect and Confidence Intervals&quot;);
</code></pre>
<pre><code class="language-python">plot_peeking(dfi_naive, obs)
</code></pre>
<p><img src="img/group_sequential_testing_24_0.png" alt="png"></p>
<p>As we can see, the first <em>7</em> times we look at the data the confidence intervals cross the zero line and hence we do not reject the null hypothesis of zero mean. However, in the fourth look at the data, at <em>450</em> observations, the confidence interval does not cross the zero line and hence we reject the null hypothesis of no effect.</p>
<p>The problem of this procedure is very similar to <strong>multiple hypothesis testing</strong>: we are building the confidence intervals for a single look at the data and therefore a single decision. Here instead we are making multiple decisions.</p>
<p>What are the <strong>consequences</strong> of peeking? Let&rsquo;s have a look at what would happen if we were to repeat this experiment multiple times. We will now plot the confidence intervals for 100 iterations at three different points in time: after <em>200</em>, <em>400</em> and <em>600</em>.</p>
<pre><code class="language-python">N_plot = [200, 400, 600]
</code></pre>
<pre><code class="language-python">def plot_intervals(dfi, N_plot, power_coverage: str):
    fig, (axes) = plt.subplots(1, len(N_plot))
    x = 0 if power_coverage == &quot;power&quot; else mu
    df_short = dfi.iloc[:100, :].copy()
    
    # Plot intervals
    for k, i, ax in zip(range(len(N_plot)), N_plot, axes):
        # Plot intervals
        ax.errorbar(x=df_short[f&quot;mean{i}&quot;], y=df_short[&quot;k&quot;], xerr=df_short[f&quot;width{i}&quot;], 
                     fmt=&quot;&quot;, lw=0, alpha=0.3, elinewidth=1)
        ax.axvline(x=x, lw=1, ls=&quot;--&quot;, c=&quot;k&quot;)
    
        # Add wrong intervals
        temp = df_short.loc[df_short[f&quot;{power_coverage}{i}&quot;] == False]
        ax.errorbar(x=temp[f&quot;mean{i}&quot;], y=temp[&quot;k&quot;], xerr=temp[f&quot;width{i}&quot;], 
                     fmt=&quot;&quot;, lw=0, alpha=1, elinewidth=2)
    
        # Rest
        ax.set(xlim=(-0.2, 2.2))
        ax.set_title(f&quot;n = {i}&quot;, pad=10)
        ax.title.set_size(14)
        if k &gt; 0:
            ax.set_yticks([])
    plt.suptitle(power_coverage.capitalize(), y=1.01);
</code></pre>
<p>The first thing that we are going to inspect is <strong>coverage</strong>: do the confidence intervals actually <em>cover</em> the true treatment effect, as they are supposed to? I highlight the confidence intervals that don&rsquo;t.</p>
<pre><code class="language-python">plot_intervals(dfi_naive, N_plot, &quot;coverage&quot;)
</code></pre>
<p><img src="img/group_sequential_testing_29_0.png" alt="png"></p>
<p>It seems that our coverage is fine at each point in time. We have, respectively, 5, 2, and 3 cases out of 100 in which the interval does not cover the true treatment effect. This is expected since our <em>confidence level</em> <em>α=5%</em>.</p>
<p>Next, we investigate <strong>power</strong>: the ability of our estimator to detect an effect when there is indeed one. Remember that power is always relative to the effect size. However, we did our power calculations using the true effect so we expect the experiment to have the excepted power of <em>1-β=80%</em>.</p>
<p>Note that since we are peeking, we reject the null as soon as one test is significant. Therefore, in our case, power at a specific point in time is the probability of rejecting the null hypothesis with that test or <em>any</em> of the previous ones.</p>
<pre><code class="language-python">plot_intervals(dfi_naive, N_plot, &quot;power&quot;)
</code></pre>
<p><img src="img/group_sequential_testing_31_0.png" alt="png"></p>
<p>As we can see, at 83 observations we reject the null hypothesis already in 83 simulations out of 100. This suggests that At 183 and 283 observations we reject the null hypothesis in all simulations.</p>
<p>It seems that so far everything is going great: our intervals cover the true effect and reject the null hypothesis even faster</p>
<pre><code class="language-python">def check_performance(dfi):
    df_perf = dfi[list(dfi.filter(regex=&quot;coverage|power|falsep&quot;))].copy()
    df_perf[&quot;idx&quot;] = 1
    df_perf = df_perf.groupby(&quot;idx&quot;, as_index=False).mean()
    df_perf = pd.wide_to_long(df_perf, stubnames=[&quot;coverage&quot;, &quot;power&quot;, &quot;falsep&quot;], i=&quot;idx&quot;, j=&quot;n&quot;).reset_index().drop(columns=&quot;idx&quot;)
    return df_perf
</code></pre>
<pre><code class="language-python">def plot_coverage_power(dfi, alpha=0.05, beta=0.8):
    df_perf = check_performance(dfi)
    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(14, 3))
    
    # Plot Coverage
    sns.lineplot(df_perf, x=&quot;n&quot;, y=&quot;coverage&quot;, c=&quot;C0&quot;, ax=ax1)
    ax1.axhline(y=1-alpha, ls=&quot;--&quot;, lw=2, c=&quot;C0&quot;)
    ax1.set(title=&quot;Coverage&quot;, ylabel=&quot;&quot;, ylim=(0.89, 1.01))
    
    # Plot power
    sns.lineplot(df_perf, x=&quot;n&quot;, y=&quot;power&quot;, c=&quot;C1&quot;, ax=ax2)
    ax2.axhline(y=beta, ls=&quot;--&quot;, lw=2, c=&quot;C1&quot;)
    ax2.set(title=&quot;Power&quot;, ylabel=&quot;&quot;, ylim=(0.5, 1.05))

    # Plot false positives
    sns.lineplot(df_perf, x=&quot;n&quot;, y=&quot;falsep&quot;, c=&quot;C2&quot;, ax=ax3)
    ax3.axhline(y=alpha, ls=&quot;--&quot;, lw=2, c=&quot;C2&quot;)
    ax3.set(title=&quot;False Rejections&quot;, ylabel=&quot;&quot;, ylim=(-0.01, 0.24))

    n_min_power = df_perf.loc[df_perf.power &gt;= 0.8, &quot;n&quot;].values[0]
    print(&quot;Min power at n =&quot;, n_min_power)
</code></pre>
<pre><code class="language-python">plot_coverage_power(dfi_naive)
</code></pre>
<pre><code>Min power at n = 250
</code></pre>
<p><img src="img/group_sequential_testing_35_1.png" alt="png"></p>
<h2 id="constant-corrections">Constant Corrections</h2>
<h3 id="bonferroni-correction">Bonferroni Correction</h3>
<p>Since the peeking problem is close to multiple hypothesis testing, we can start by applying the same solution.</p>
<p>The simplest way to account for multiple hypothesis testing is the so-called <strong>Bonferroni correction</strong>. The idea is simple: decrease the significance level <em>α</em> proportionally to the number of looks. In particular, instead of using the same <em>α</em> for each look, we use instead
$$
\alpha_{\text{Bonferroni}} = \frac{\alpha}{P}
$$</p>
<p>where <em>P</em> is the number of times we plan to peek.</p>
<pre><code class="language-python">def select_alpha_bonferroni(n, N, N_peek, alpha):
    P = len(N_peek)
    return alpha / P
</code></pre>
<p>How does Bonferroni correction perform in terms of <strong>coverage</strong>? Let&rsquo;s plot the confidence intervals for three peeking stages: after <em>200</em>, <em>400</em>, and <em>600</em> observations are collected. Note that these correspond to the <em>3rd</em>, <em>7th</em> and <em>11th</em> peek at the data, respectively.</p>
<pre><code class="language-python">dfi_bonferroni = compute_intervals(select_alpha_bonferroni, obs, N_peek)
plot_intervals(dfi_bonferroni, N_plot, &quot;coverage&quot;)
</code></pre>
<p><img src="img/group_sequential_testing_41_0.png" alt="png"></p>
<p>Coverage looks great! Only once at <em>n=200</em> one interval did not cover the true value <em>μ=1</em>.</p>
<p>While this might appear comforting at first, it should actually raise an eyebrow. In fact, with a significance level <em>α=0.05</em> we expect a coverage of <em>95%</em>. A higher coverage will most likely come at the expense of <strong>power</strong>. Let&rsquo;s have a look.</p>
<pre><code class="language-python">plot_intervals(dfi_bonferroni, N_plot, &quot;power&quot;)
</code></pre>
<p><img src="img/group_sequential_testing_43_0.png" alt="png"></p>
<p>The test is clearly underpowered at <em>n=200</em>, while it is very close to the target power of <em>1-β=80%</em> at <em>n=400</em>. At <em>n=600</em> we have almost 100% power.</p>
<p>Let&rsquo;s plot not coverage, power, and false positive rate over <em>K=10,000</em> simulations.</p>
<pre><code class="language-python">plot_coverage_power(dfi_bonferroni)
</code></pre>
<pre><code>Min power at n = 450
</code></pre>
<p><img src="img/group_sequential_testing_45_1.png" alt="png"></p>
<p>Coverage is great, power is above target starting at <em>n=450</em> observations, and the false rejection rate is always below the target <em>5%</em>. Amazing&hellip; right?</p>
<p>Yes, everything looks good indeed, but maybe <strong>too good</strong>, in the sense that there is clearly room for improvement. Given such high coverage and low false rejection rate, there is room for improvement: the results suggest that we could have shorter confidence intervals and higher power, without dropping below <em>95%</em> coverage or above <em>5%</em> false rejection rate. How?</p>
<p>The Bonferroni correction has <strong>two drawbacks</strong>. First of all, it is known to be very <strong>conservative</strong>. Second, it was <strong>not designed for sequential testing</strong>, but rather for multiple hypotesis testing.</p>
<h3 id="corrections">Corrections</h3>
<p>The first version of <strong>Bonferroni&rsquo;s correction for sequential testing</strong> was <a href="https://www.jstor.org/stable/2335684" target="_blank" rel="noopener">Pocock (1977)</a>. The idea of both corrections was to divide the significance level <em>α</em> by a lower number than <em>P</em>, to increase power while keeping high coverage and low false positive rate. The values are found through a numeric algorithm that takes as input the significance level <em>α</em> and the total number of peeks <em>P</em>.</p>
<p>The second version was from <a href="https://www.jstor.org/stable/2530245" target="_blank" rel="noopener">O’Brien, Fleming (1979)</a>. Their idea was to adapt the width of the confidence interval not only to the significance level <em>α</em> and the total number of peeks <em>P</em>, but the individual peek <em>p</em>. However, one drawback of the O’Brien-Fleming procedure is that it requires planning the number of peeks <em>P</em> in advance and the individual peeks to be uniformly distributed across the sample.</p>
<p>In a sense, these corrections allow for <strong>planned peeking</strong>, but what about random unplanned peeking, as it is often the case in practice?</p>
<h2 id="group-sequential-testing">Group Sequential Testing</h2>
<p><a href="https://www.jstor.org/stable/2336502" target="_blank" rel="noopener">Lan, DeMets (1983)</a> noticed that the important thing in peeking is not <em>how much</em> you peek, but rather <em>when</em> you peek. The main idea of <strong>Group Sequential Testing (GST)</strong> is to allow for peeking at any point in time, and correct the significance level for the peeking point in time in the data collection process, <em>t = n/N</em>.</p>
<p>The moving part of group sequential testing is the so-called <strong>alpha spending function</strong> that determines how to correct the significance level <em>α</em>, given peeking time <em>t</em>. In the rest of the article we are going to review two alpha spending functions that approximate the corrections of <a href="https://www.jstor.org/stable/2335684" target="_blank" rel="noopener">Pocock (1977)</a> and <a href="https://www.jstor.org/stable/2530245" target="_blank" rel="noopener">O’Brien, Fleming (1979)</a>, respectively.</p>
<h3 id="gst-pocock-approximation">GST Pocock Approximation</h3>
<p>The first alpha-spending function is an approximation of <a href="https://www.jstor.org/stable/2335684" target="_blank" rel="noopener">Pocock (1977)</a> and is given by</p>
<p>$$
f(\alpha, t) = \alpha \ln \big( 1 + (e - 1) t \big)
$$</p>
<pre><code class="language-python">def select_alpha_gst_pocock(n, N, N_peek, alpha):
    t = n / N
    return alpha * np.log(1 + (np.exp(1) - 1) * t)
</code></pre>
<p>Let&rsquo;s see how group sequential testing using Pocock&rsquo;s alpha spending function works.</p>
<pre><code class="language-python">dfi_gst_pocock = compute_intervals(select_alpha_gst_pocock, obs, N_peek)
plot_coverage_power(dfi_gst_pocock)
</code></pre>
<pre><code>Min power at n = 300
</code></pre>
<p><img src="img/group_sequential_testing_55_1.png" alt="png"></p>
<h3 id="gst-obrien--fleming-approximation">GST O’Brien &amp; Fleming Approximation</h3>
<p>The second alpha-spending function is an approximation of <a href="https://www.jstor.org/stable/2530245" target="_blank" rel="noopener">O’Brien, Fleming (1979)</a> and is given by</p>
<p>$$
f(\alpha, t) = 4 - 4 \Phi \Big( \Phi^{-1} \big(1 - \alpha/4 \big) / \sqrt{t} \Big)
$$</p>
<p>where <em>Φ</em> is the cumulative distribution function (CDF) of a standard normal distribution.</p>
<pre><code class="language-python">def select_alpha_gst_obrien_fleming(n, N, N_peek, alpha):
    t = n / N
    return 4 - 4 * cdf(ppf(1 - alpha/4) / np.sqrt(t))
</code></pre>
<p>Let&rsquo;s see how it performs over <em>K=10,000</em> simulations.</p>
<pre><code class="language-python">dfi_obrien_fleming = compute_intervals(select_alpha_gst_obrien_fleming, obs, N_peek)
plot_coverage_power(dfi_obrien_fleming)
</code></pre>
<pre><code>Min power at n = 500
</code></pre>
<p><img src="img/group_sequential_testing_60_1.png" alt="png"></p>
<p>It seems that the O’Brien and Fleming approximation is more conservative than Pocoks, reaching <em>80%</em> power only at <em>500</em> observations, but keeping the false rejection rate lower.</p>
<h2 id="alpha-spending-trade-off">Alpha Spending Trade-off</h2>
<p>Before concluding, it is worth having a look at the peeking trade-offs. We have introduced a method that allows us to do valid inference while peeking any number of times, whenever we feel like. But <strong>should we peek</strong>?</p>
<p>In the figure below, I plot the testing performance when we <strong>increase the peeking frequency</strong> to 10 observations.</p>
<pre><code class="language-python">N_peek_10 = np.arange(10, N+1, 10, dtype=int)
dfi_obrien_fleming_10 = compute_intervals(select_alpha_gst_obrien_fleming, obs, N_peek_10)
plot_coverage_power(dfi_obrien_fleming_10)
</code></pre>
<pre><code>Min power at n = 470
</code></pre>
<p><img src="img/group_sequential_testing_64_1.png" alt="png"></p>
<p>The advantage is that now we need <em>470</em> observations to reach a power level of <em>80%</em>, but this comes at the cost of a higher false rejection rate.</p>
<p>What if instead we <strong>reduced the peeking frequency</strong>? In the figure below, I plot the results when peeking every 200 observations.</p>
<pre><code class="language-python">N_peek_200 = np.arange(200, N+1, 200, dtype=int)
dfi_obrien_fleming_200 = compute_intervals(select_alpha_gst_obrien_fleming, obs, N_peek_200)
plot_coverage_power(dfi_obrien_fleming_200)
</code></pre>
<pre><code>Min power at n = 600
</code></pre>
<p><img src="img/group_sequential_testing_66_1.png" alt="png"></p>
<p>From the figure, we see the opposite result: we now need more observations to reach <em>80%</em> power (<em>600</em> instead of <em>500</em>), but the false rejection rate barely crosses the <em>5%</em> threshold.</p>
<h2 id="conclusion">Conclusion</h2>
<p>In this article, we have explored <strong>group sequential testing</strong>, a procedure to do valid inference when peeking during an A/B test, any number of times, and at any point during the experiment. We have also seen how peeking does not come for free. The main <strong>trade-off</strong> is that the more we peek, the earlier we can stop an experiment but also the higher the false rejection rate.</p>
<p>There are at least a couple of topics I have not mentioned in the article, not to make it too long. The first one is <strong>bias</strong>. Sequential tests can easily introduce bias since early stopping could be due to either a low variance or a large effect. Because of the second, sequential tests can often lead to the <em>overestimation</em> of treatment effects. This phenomenon is often called the <em>winner&rsquo;s curse</em> and typically occurs when the study is underpowered, which is often the case at the early peeking stages. One solution is to design a <strong>beta spending</strong> function.</p>
<p>The second topic that I didn&rsquo;t cover is what is called <strong>stopping for futility</strong>. In the examples of this article, we stopped experiments early if we got a statistically significant estimate. However, peeking can also inform a different stopping rule: stopping because it becomes extremely unlikely that continuing the test can produce significant results.</p>
<p>The last topic I have not covered (yet) is not to do <strong>power analysis</strong> with sequential testing. In the example above we ran the power analysis assuming no peeking. However, given that we knew we would have peeked, we could have anticipated the need for a smaller sample. A closely related topic is <strong>optimal peeking</strong>. Once you decided to peek, when should you do it?</p>
<h2 id="references">References</h2>
<ul>
<li>Lakens, Pahlke, Wassmer (2021). <a href="https://osf.io/preprints/psyarxiv/x4azm" target="_blank" rel="noopener">Group Sequential Designs: A Tutorial</a></li>
<li>Lan, DeMets (1983). <a href="https://academic.oup.com/biomet/article-abstract/70/3/659/247777" target="_blank" rel="noopener">Discrete Sequential Boundaries for Clinical Trials</a></li>
<li>Spotify (2023). <a href="https://engineering.atspotify.com/2023/03/choosing-sequential-testing-framework-comparisons-and-discussions/" target="_blank" rel="noopener">Choosing a Sequential Testing Framework</a></li>
</ul>
<h3 id="related-articles">Related Articles</h3>
<ul>
<li><a href="https://towardsdatascience.com/954506cec665" target="_blank" rel="noopener">Experiments, Peeking, and Optimal Stopping</a></li>
</ul>
<h3 id="code">Code</h3>
<p>You can find the original Jupyter Notebook here:</p>
<p><a href="https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/instrumental_variables.ipynb" target="_blank" rel="noopener">https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/instrumental_variables.ipynb</a></p>

          </div>
          


















  
  



  
  
  
    
  
  
  
  <div class="media author-card content-widget-hr">
    
      
      <a href="https://matteocourthoud.github.io/"><img class="avatar mr-3 avatar-circle" src="/author/avatar_hu365eedc833ccd5578a90de7c849ec45e_385094_270x270_fill_q75_lanczos_center.jpg" alt=""></a>
    

    <div class="media-body">
      <h5 class="card-title"><a href="https://matteocourthoud.github.io/"></a></h5>
      
      <p class="card-text">I hold a PhD in economics from the University of Zurich. Now I work at the intersection of economics, data science and statistics. I regularly write about causal inference on <a href="https://medium.com/@matteo.courthoud" target="_blank" rel="noopener">Medium</a>.</p>
      <ul class="network-icon" aria-hidden="true">
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://medium.com/@matteo.courthoud" target="_blank" rel="noopener">
        <i class="fab fa-medium"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://www.linkedin.com/in/matteo-courthoud-7335198a/" target="_blank" rel="noopener">
        <i class="fab fa-linkedin"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://twitter.com/MatteoCourthoud/" target="_blank" rel="noopener">
        <i class="fab fa-twitter"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://github.com/matteocourthoud" target="_blank" rel="noopener">
        <i class="fab fa-github"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://open.spotify.com/user/1180947523" target="_blank" rel="noopener">
        <i class="fab fa-spotify"></i>
      </a>
    </li>
  
</ul>

    </div>
  </div>
















  
  




        </div>
        </article>
    </main>
  </div>
</div>
  </div>

  <div class="page-footer">
    
    
    <div class="container">
      <footer class="site-footer">

  

  
  <p class="powered-by">
    
      <a href="/privacy/">Privacy Policy</a>
    
    
       &middot; 
      <a href="/terms/">Terms</a>
    
  </p>
  

  

  
  







</footer>

    </div>
    
  </div>

  
<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        <pre><code class="tex hljs"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

      

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/instant.page/5.1.0/instantpage.min.js" integrity="sha512-1+qUtKoh9XZW7j+6LhRMAyOrgSQKenQ4mluTR+cvxXjP1Z54RxZuzstR/H9kgPXQsVB8IW7DMDFUJpzLjvhGSQ==" crossorigin="anonymous"></script>

      
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js" integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin="anonymous"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js" integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin="anonymous"></script>
      

      
      

      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mermaid/8.8.4/mermaid.min.js" integrity="sha512-as1BF4+iHZ3BVO6LLDQ7zrbvTXM+c/1iZ1qII/c3c4L8Rn5tHLpFUtpaEtBNS92f+xGsCzsD7b62XP3XYap6oA==" crossorigin="anonymous" title="mermaid"></script>
      

      
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/highlight.min.js" integrity="sha512-TDKKr+IvoqZnPzc3l35hdjpHD0m+b2EC2SrLEgKDRWpxf2rFCxemkgvJ5kfU48ip+Y+m2XVKyOCD85ybtlZDmw==" crossorigin="anonymous"></script>
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/languages/r.min.js"></script>
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/languages/python.min.js"></script>
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/languages/julia.min.js"></script>
        
      

    

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.7.1/leaflet.min.js" integrity="sha512-SeiQaaDh73yrb56sTW/RgVdi/mMqNeM2oBwubFHagc5BkixSpP1fvqF47mKzPGWYSSy4RwbBunrJBQ4Co8fRWA==" crossorigin="anonymous"></script>
    

    
    

    
    
    
      
      <script id="search-hit-fuse-template" type="text/x-template">
        <div class="search-hit" id="summary-{{key}}">
          <div class="search-hit-content">
            <div class="search-hit-name">
              <a href="{{relpermalink}}">{{title}}</a>
              <div class="article-metadata search-hit-type">{{type}}</div>
              <p class="search-hit-description">{{snippet}}</p>
            </div>
          </div>
        </div>
      </script>
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js" integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin="anonymous"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js" integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin="anonymous"></script>
      
    

    
    

    
    
    
    

    
    <script src="/js/bootstrap.bundle.min.6aed84840afc03ab4d5750157f69c120.js"></script>

    
    
      
      
      
      
      
      
      
    

    
    
    
    
    
    
    
    
      
      
    
    
    <script src="/en/js/wowchemy.min.c8b7c648795740c04de2ef756725ef48.js"></script>

    
  <script async defer src="https://buttons.github.io/buttons.js"></script>




</body>
</html>
