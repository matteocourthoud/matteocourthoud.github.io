<!DOCTYPE html><html lang="en-us" >

<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  
  
  
    <meta name="generator" content="Wowchemy 5.1.0 for Hugo" />
  

  

  
  
  
  
  
    
    
    
  
  

  

  
  
  
    
  
  <meta name="description" content="In the previous part of this blog post, we have seen how pre-testing can distort inference, i.e., how selecting control variables depending on their statistical significance results in wrong confidence intervals for the variable of interest." />

  
  <link rel="alternate" hreflang="en-us" href="https://matteocourthoud.github.io/post/pds/" />

  







  




  
  
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
  

  
  
  
    <meta name="theme-color" content="#003f5c" />
  

  
  
    
    <script src="/js/mathjax-config.js"></script>
  

  
  
  
  
    
    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha256-FMvZuGapsJLjouA6k7Eo2lusoAX9i0ShlWFG6qt7SLc=" crossorigin="anonymous">

    
    

    
    
    
      
    
    
      
      
        
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/styles/github.min.css" crossorigin="anonymous" title="hl-light" media="print" onload="this.media='all'">
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/styles/dracula.min.css" crossorigin="anonymous" title="hl-dark" media="print" onload="this.media='all'" disabled>
        
      
    

    
    
    
      <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.7.1/leaflet.min.css" integrity="sha512-1xoFisiGdy9nvho8EgXuXvnpR5GAMSjFwp40gSRE3NwdUdIMIKuPa7bqoUhLD0O/5tPNhteAsE5XyyMi5reQVA==" crossorigin="anonymous" media="print" onload="this.media='all'">
    

    

    
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js" integrity="" crossorigin="anonymous" async></script>
      
    
      
      

      
      

      
    
      
      

      
      

      
    
  

  
  
  
    
      
      <link rel="preload" as="style" href="https://fonts.googleapis.com/css2?family=Roboto+Mono&family=Source+Sans+Pro:wght@200;300;400;700&display=swap">
      <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Roboto+Mono&family=Source+Sans+Pro:wght@200;300;400;700&display=swap" media="print" onload="this.media='all'">
    
  

  
  
  
  
  
  <link rel="stylesheet" href="/css/wowchemy.5c4def4f00a521426f4eb098155f3342.css" />

  




<script async src="https://www.googletagmanager.com/gtag/js?id=UA-144780600-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];

  function gtag() {
      dataLayer.push(arguments);
  }

  function trackOutboundLink(url, target) {
    gtag('event', 'click', {
         'event_category': 'outbound',
         'event_label': url,
         'transport_type': 'beacon',
         'event_callback': function () {
           if (target !== '_blank') {
             document.location = url;
           }
         }
    });
    console.debug("Outbound link clicked: " + url);
  }

  function onClickCallback(event) {
    if ((event.target.tagName !== 'A') || (event.target.host === window.location.host)) {
      return;
    }
    trackOutboundLink(event.target, event.target.getAttribute('target'));  
  }

  gtag('js', new Date());
  gtag('config', 'UA-144780600-1', { 'anonymize_ip': true });
  gtag('set', {'cookie_flags': 'SameSite=None;Secure'});

  
  document.addEventListener('click', onClickCallback, false);
</script>


  

  

  




  
  
  

  

  
    <link rel="manifest" href="/index.webmanifest" />
  

  <link rel="icon" type="image/png" href="/media/icon_hu03e9b3967b83fd39296ec9da5ff1ea05_201175_32x32_fill_lanczos_center_3.png" />
  <link rel="apple-touch-icon" type="image/png" href="/media/icon_hu03e9b3967b83fd39296ec9da5ff1ea05_201175_180x180_fill_lanczos_center_3.png" />

  <link rel="canonical" href="https://matteocourthoud.github.io/post/pds/" />

  
  
  
  
  
  
  
  
    
  
  
  <meta property="twitter:card" content="summary_large_image" />
  
  <meta property="og:site_name" content="Matteo Courthoud" />
  <meta property="og:url" content="https://matteocourthoud.github.io/post/pds/" />
  <meta property="og:title" content="Double Debiased Machine Learning (part 2) | Matteo Courthoud" />
  <meta property="og:description" content="In the previous part of this blog post, we have seen how pre-testing can distort inference, i.e., how selecting control variables depending on their statistical significance results in wrong confidence intervals for the variable of interest." /><meta property="og:image" content="https://matteocourthoud.github.io/post/pds/featured.png" />
    <meta property="twitter:image" content="https://matteocourthoud.github.io/post/pds/featured.png" /><meta property="og:locale" content="en-us" />
  
    
      <meta
        property="article:published_time"
        content="2022-06-05T00:00:00&#43;00:00"
      />
    
    <meta property="article:modified_time" content="2022-06-05T00:00:00&#43;00:00">
  

  


    






  




<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://matteocourthoud.github.io/post/pds/"
  },
  "headline": "Double Debiased Machine Learning (part 2)",
  
  "image": [
    "https://matteocourthoud.github.io/post/pds/featured.png"
  ],
  
  "datePublished": "2022-06-05T00:00:00Z",
  "dateModified": "2022-06-05T00:00:00Z",
  
  "publisher": {
    "@type": "Organization",
    "name": "Matteo Courthoud",
    "logo": {
      "@type": "ImageObject",
      "url": "https://matteocourthoud.github.io/media/icon_hu03e9b3967b83fd39296ec9da5ff1ea05_201175_192x192_fill_lanczos_center_3.png"
    }
  },
  "description": "In the previous part of this blog post, we have seen how pre-testing can distort inference, i.e., how selecting control variables depending on their statistical significance results in wrong confidence intervals for the variable of interest."
}
</script>

  

  

  
  
  
  
  
    <script src="https://cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.1.1/cookieconsent.min.js" integrity="sha256-5VhCqFam2Cn+yjw61zbBNrbHVJ6SRydPeKopYlngbiQ=" crossorigin="anonymous"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.1.1/cookieconsent.min.css" integrity="sha256-zQ0LblD/Af8vOppw18+2anxsuaz3pWYyVWi+bTvTH8Q=" crossorigin="anonymous">
  
  <script>
  window.addEventListener("load", function(){
    window.cookieconsent.initialise({
      "palette": {
        "popup": {
          "background": "#003f5c",
          "text": "rgb(255, 255, 255)"
        },
        "button": {
          "background": "rgb(255, 255, 255)",
          "text": "#003f5c"
        }
      },
      "theme": "classic",
      "content": {
        "message": "This website uses cookies to ensure you get the best experience on our website.",
        "dismiss": "Got it!",
        "link": "Learn more",
        "href": "/privacy/"
      }
    })});
  </script>


  





  <title>Double Debiased Machine Learning (part 2) | Matteo Courthoud</title>
</head>


<body id="top" data-spy="scroll" data-offset="70" data-target="#TableOfContents" class="page-wrapper   " data-wc-page-id="bd599436ccb29a7c644f39b9a9005334" >

  
  
  
  
  
  
  
  
  <script src="/js/wowchemy-init.min.6edaf3b475ce43de30d98828aea698be.js"></script>

  


<aside class="search-modal" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Search</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#" aria-label="Close"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        
        <input name="q" id="search-query" placeholder="Search..." autocapitalize="off"
        autocomplete="off" autocorrect="off" spellcheck="false" type="search" class="form-control"
        aria-label="Search...">
        
      </div>

      
      

      

    </section>
    <section class="section-search-results">

      <div id="search-hits">
        
      </div>

    </section>
  </div>
</aside>



  <div class="page-header">
    












<nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id="navbar-main">
  <div class="container-xl">

    
    <div class="d-none d-lg-inline-flex">
      <a class="navbar-brand" href="/">Matteo Courthoud</a>
    </div>
    

    
    <button type="button" class="navbar-toggler" data-toggle="collapse"
            data-target="#navbar-content" aria-controls="navbar-content" aria-expanded="false" aria-label="Toggle navigation">
    <span><i class="fas fa-bars"></i></span>
    </button>
    

    
    <div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none">
      <a class="navbar-brand" href="/">Matteo Courthoud</a>
    </div>
    

    
    
    <div class="navbar-collapse main-menu-item collapse justify-content-end" id="navbar-content">

      
      <ul class="navbar-nav d-md-inline-flex">
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
            
            
            
              
            
            
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#about"><span>Home</span></a>
        </li>

        
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
            
            
            
              
            
            
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#projects"><span>Research</span></a>
        </li>

        
        

        
        <li class="nav-item dropdown">
          <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown" aria-haspopup="true"><span>Courses</span><span class="caret"></span>
          </a>
          <div class="dropdown-menu">
            
              <a class="dropdown-item" href="/course/ml-econ/"><span>Machine Learning for Economics</span></a>
            
              <a class="dropdown-item" href="/course/data-science/"><span>Data Science with Python</span></a>
            
              <a class="dropdown-item" href="/course/empirical-io/"><span>PhD Industrial Organization</span></a>
            
              <a class="dropdown-item" href="/course/metrics/"><span>PhD Econometrics</span></a>
            
              <a class="dropdown-item" href="https://pp4rs.github.io/"><span>Programming Practices for Research</span></a>
            
          </div>
        </li>

        
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
        

        <li class="nav-item">
          <a class="nav-link  active" href="/post/"><span>Posts</span></a>
        </li>

        
        

        

        
        
        

        

        
        
        
        

        
          
            
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/cv"><span>CV</span></a>
        </li>

        
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
            
            
            
              
            
            
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#contact"><span>Contact</span></a>
        </li>

        
        

      

        
      </ul>
    </div>

    <ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2">

      
      
        
      

      
      

      
      

      
      

    </ul>

  </div>
</nav>


  </div>

  <div class="page-body">
    <div class="container-fluid docs">
  <div class="row">

    <div class="col-xl-2 col-lg-2 d-none d-xl-block d-lg-block empty">
    </div>

    <div class="col-2 col-xl-2 col-lg-2 d-none d-lg-block docs-toc">
      <ul class="nav toc-top">
        <li><a href="#" id="back_to_top" class="docs-toc-title">Contents</a></li>
      </ul>
      <nav id="TableOfContents">
  <ul>
    <li><a href="#recap">Recap</a></li>
    <li><a href="#post-double-selection">Post-Double Selection</a>
      <ul>
        <li><a href="#intuition">Intuition</a></li>
        <li><a href="#application">Application</a></li>
        <li><a href="#double-checks">Double-checks</a></li>
      </ul>
    </li>
    <li><a href="#double-debiased-machine-learning">Double Debiased Machine Learning</a>
      <ul>
        <li><a href="#naive-approach">Naive approach</a></li>
        <li><a href="#orthogonalization">Orthogonalization</a></li>
        <li><a href="#a-cautionary-tale">A Cautionary Tale</a></li>
      </ul>
    </li>
    <li><a href="#conclusion">Conclusion</a>
      <ul>
        <li><a href="#references">References</a></li>
        <li><a href="#related-articles">Related Articles</a></li>
        <li><a href="#code">Code</a></li>
      </ul>
    </li>
  </ul>
</nav>
    </div>

    <main class="col-xl-8 col-lg-8 docs-content" role="main">
        <article class="article">
        




















  


<div class="article-container pt-3">
  <h1>Double Debiased Machine Learning (part 2)</h1>

  

  


<div class="article-metadata">

  
  

  
  <span class="article-date">
    
    
      
    
    Jun 5, 2022
  </span>
  

  

  
  <span class="middot-divider"></span>
  <span class="article-reading-time">
    14 min read
  </span>
  

  
  
  
  
  
  

  
  

</div>

  





</div>


<div class="article-header article-container featured-image-wrapper mt-4 mb-4" style="max-width: 1819px; max-height: 700px;">
  <div style="position: relative">
    <img src="/post/pds/featured.png" alt="" class="featured-image">
    
  </div>
</div>


        <div class="article-container">
          <div class="article-style" align="justify">
            <p>In the <a href="https://towardsdatascience.com/eb767a59975b" target="_blank" rel="noopener">previous part of this blog post</a>, we have seen how pre-testing can distort inference, i.e., how selecting control variables depending on their statistical significance results in wrong confidence intervals for the variable of interest. This bias is generally called <strong>regularization bias</strong> and also emerges in machine learning algorithms.</p>
<p>In blog post, we are going to explore a solution to the simple selection example, <strong>post-double selection</strong>, and a more general approach when we have many control variables and we do not want to assume linearity, <strong>double-debiased machine learning</strong>.</p>
<h2 id="recap">Recap</h2>
<p>To better understand the source of the bias, in the first part of this post, we have explored the example of a firm that is interested in testing the effectiveness of an a campaign. The firm has information on its current ad spending and on the level of sales. The problem arises because the firm is uncertain on whether it should condition its analysis on the level of past sales.</p>
<p>The following <a href="https://towardsdatascience.com/b63dc69e3d8c" target="_blank" rel="noopener"><strong>Directed Acyclic Graph (DAG)</strong></a> summarizes the data generating process.</p>
<pre><code class="language-mermaid">flowchart LR
classDef included fill:#DCDCDC,stroke:#000000,stroke-width:2px;
classDef excluded fill:#ffffff,stroke:#000000,stroke-width:2px;
classDef unobserved fill:#ffffff,stroke:#000000,stroke-width:2px,stroke-dasharray: 5 5;

D((ad spend))
Z((past sales))
Y((sales))

D --&gt; Y
Z -- ??? --&gt; Y
Z --&gt; D

class D,Y included;
class Z excluded;

linkStyle 0 stroke:#00ff00,stroke-width:4px;
</code></pre>
<p>I import the data generating process <code>dgp_tbd()</code> from <a href="https://github.com/matteocourthoud/Blog-Posts/blob/main/src/dgp.py" target="_blank" rel="noopener"><code>src.dgp</code></a> and some plotting functions and libraries from <a href="https://github.com/matteocourthoud/Blog-Posts/blob/main/src/utils.py" target="_blank" rel="noopener"><code>src.utils</code></a>.</p>
<pre><code class="language-python">%matplotlib inline
%config InlineBackend.figure_format = 'retina'
</code></pre>
<pre><code class="language-python">from src.utils import *
from src.dgp import dgp_pretest

df = dgp_pretest().generate_data()
df.head()
</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
</code></pre>
<p></style></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>ads</th>
      <th>sales</th>
      <th>past_sales</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>16.719800</td>
      <td>19.196620</td>
      <td>6.624345</td>
    </tr>
    <tr>
      <th>1</th>
      <td>7.732222</td>
      <td>9.287491</td>
      <td>4.388244</td>
    </tr>
    <tr>
      <th>2</th>
      <td>10.923469</td>
      <td>11.816906</td>
      <td>4.471828</td>
    </tr>
    <tr>
      <th>3</th>
      <td>8.457062</td>
      <td>9.024376</td>
      <td>3.927031</td>
    </tr>
    <tr>
      <th>4</th>
      <td>13.085146</td>
      <td>12.814823</td>
      <td>5.865408</td>
    </tr>
  </tbody>
</table>
</div>
<p>We have data on $1000$ different markets, in which we observe current <code>sales</code>, the amount spent in <code>advertisement</code> and <code>past sales</code>.</p>
<p>We want to understand <code>ads</code> spending is effective in increasing <code>sales</code>. One possibility is to regress the latter on the former, using the following regression model, also called the <strong>short model</strong>.</p>
<p>$$
\text{sales} = \alpha \cdot \text{ads} + \varepsilon
$$</p>
<p>Should we also include <code>past sales</code> in the regression? Then the regression model would be the following, also called <strong>long model</strong>.</p>
<p>$$
\text{sales} = \alpha \cdot \text{ads} + \beta \cdot \text{past sales} + \varepsilon
$$</p>
<p>One naive approach would be to <strong>let the data decide</strong>: we could run the second regression and, if the effect of <code>past sales</code>, $\beta$, is statistically significant, we are good with the long model, otherwise we run the short model. This procedure is called <strong>pre-testing</strong>.</p>
<p>The problem with this procedure is that it introduces a bias that is called <strong>regularization or pre-test bias</strong>. Pre-testing ensures that this bias is small enough not to distort the estimated coefficient. However, it does not ensure that it is small enough not to distort the confidence intervals around the estimated coefficient.</p>
<p>Is there a solution? Yes!</p>
<h2 id="post-double-selection">Post-Double Selection</h2>
<p>The solution is called <strong>post-double selection</strong>. The method was first introduced in <a href="https://academic.oup.com/restud/article-abstract/81/2/608/1523757" target="_blank" rel="noopener">Belloni, Chernozhukov, Hansen (2014)</a> and later expanded in a variety of papers.</p>
<p>The authors assume the following <strong>data generating process</strong>:</p>
<p>$$
y = \alpha D + \beta X + u
\newline
D = \delta X + v
$$</p>
<p>In our example, $Y$ corresponds to <code>sales</code>, $D$ corresponds to <code>ads</code>, $X$ corresponds to <code>past_sales</code> and the effect of interest is $\alpha$. In our example, $X$ is 1-dimensional for simplicity, but generally we are interested in cases where X is high-dimensional, potentially even having more dimensions than the number of observations. In that case, variable selection is <strong>essential</strong> in linear regression since we cannot have more features than variables (the OLS coefficients are not uniquely determined anymore).</p>
<p>Post-double selection consists in the following procedure.</p>
<ol>
<li><strong>Reduced Form</strong> selection: lasso $Y$ on $X$. Select the statistically significant variables in the set $S_{RF} \subseteq X$</li>
<li><strong>First Stage</strong> selection: regress $D$ on $X$. Select the statistically significant variables in the set $S_{FS} \subseteq X$</li>
<li>Regress $Y$ on $D$ and the <strong>union</strong> of the selected variables in the first two steps, $S_{FS} \cup S_{RF}$</li>
</ol>
<p>The authors show that this procedure produces confidence intervals for the coefficient of interest $\alpha$ that have the correct coverage, i.e. the correct probability of type 1 error.</p>
<p><strong>Note (1)</strong>: this procedure is always less parsimonious, in terms of variable selection, than pre-testing. In fact, we still select all the variables we would have selected with pre-testing but, in the first stage, we might select additional variables.</p>
<p><strong>Note (2)</strong>: the terms <em>first stage</em> and <em>reduced form</em> come from the <a href="https://en.wikipedia.org/wiki/Instrumental_variables_estimation" target="_blank" rel="noopener">intrumental variables</a> literature in econometrics. Indeed, the first application of post-double selection was to select instrumental variables in <a href="https://onlinelibrary.wiley.com/doi/abs/10.3982/ECTA9626" target="_blank" rel="noopener">Belloni, Chen, Chernozhukov, Hansen (2012)</a>.</p>
<p><strong>Note (3)</strong>: the name post-double selection comes from the fact that now we are not performing variable selection once but <em>twice</em>.</p>
<h3 id="intuition">Intuition</h3>
<p>The idea behind post-double selection is: bound the <a href="https://towardsdatascience.com/344ac1477699" target="_blank" rel="noopener">omitted variables bias</a>. In case you are not familiar with it, I wrote a separate <a href="https://towardsdatascience.com/344ac1477699" target="_blank" rel="noopener">blog post on omitted variable bias</a>.</p>
<p>In our setting, we can express the omitted variable bias as</p>
<p>$$
\text{OVB} = \beta \delta \qquad \text{ where } \qquad \beta := \frac{Cov(X, Y)}{Var(X)}, \quad \delta := \frac{Cov(D, X)}{Var(D)}
$$</p>
<p>As we can see, the omitted variable bias comes from the product of two quantities related to the omitted variable $X$:</p>
<ol>
<li>Its partial correlation with the outcome $Y$, $\beta$</li>
<li>Its partial correlation with the variable of interest $D$, $\delta$</li>
</ol>
<p>With pre-testing, we ensure that the partial correlation between $X$ the outcome $Y$, $\beta$, is <strong>small</strong>. In fact, we omit $Z$ when we shouldn&rsquo;t (i.e. we commit a type 2 error) rarely. What do <em>small</em> and <em>rarely</em> mean?</p>
<p>When we are selecting a variable because of its significance, we ensure that it dimension is smaller than $\frac{c}{\sqrt{n}}$ for some number $c$, where $n$ is the sample size.</p>
<p>Therefore, with pre-testing, we ensure that, no matter what the value of $\delta$ is, the dimension of the bias is smaller than $\frac{c}{\sqrt{n}}$ which means that it converges to zero for sufficiently large $n$. This is why the pre-testing estimator is still <strong>consistent</strong>.</p>
<p>However, in order for our confidence intervals to have the right coverage, this is <strong>not enough</strong>. In practice, we need the bias to converge to zero <strong>faster</strong> than $\frac{1}{\sqrt{n}}$. Why?</p>
<p>To get an <strong>intuition</strong> for this result, we need to turn to the <a href="https://en.wikipedia.org/wiki/Central_limit_theorem" target="_blank" rel="noopener">Central Limit Theorem</a>. The CLT tells us that for large $n$ the distribution of the sample average of a random variable $X$ converges to a normal distribution with mean $\mu$ and standard deviation $\frac{\sigma}{\sqrt{n}}$, where $\mu$ and $\sigma$ are the mean and standard deviation of $X$. To do inference, we usually apply the Central Limit Theorem to our estimator to get its asymptotic distribution, which in turn allows us to build confidence intervals (using the mean and the standard deviation). Therefore, if the bias is not sensibly smaller than the standard deviation of the estimator, the confidence intervals are going to be wrong. Therefore, we need the bias to converge to zero <strong>faster</strong> than the standard deviation, i.e. faster than $\frac{1}{\sqrt{n}}$.</p>
<p>In our setting, the omitted variable bias is $\beta \gamma$ and we want it to converge to zero faster than $\frac{1}{\sqrt{n}}$.  Post-double selection guarantees that</p>
<ul>
<li><em>Reduced form</em> selection (pre-testing): any &ldquo;missing&rdquo; variable $j$ has $|\beta_j| \leq \frac{c}{\sqrt{n}}$</li>
<li><em>First stage</em> selection (additional): any &ldquo;missing&rdquo; variable $j$ has $|\delta_j| \leq \frac{c}{\sqrt{n}}$</li>
</ul>
<p>As a consequence, as long as the number of omitted variables is finite, the omitted variable bias is going to converge to zero at a rate $\frac{1}{n}$, which is faster than $\frac{1}{\sqrt{n}}$. <strong>Problem solved</strong>!</p>
<h3 id="application">Application</h3>
<p>Let&rsquo;s now go back to our example and test the post-double selection procedure. In practice, we want to do the following:</p>
<ol>
<li><strong>First Stage</strong> selection: regress <code>ads</code> on <code>past_sales</code>. Check if <code>past_sales</code> is statistically significant</li>
<li><strong>Reduced Form</strong> selection: regress <code>sales</code> on <code>past_sales</code>. Check if <code>past_sales</code> is statistically significant</li>
<li>Regress <code>sales</code> on <code>ads</code> and include <code>past_sales</code> <strong>only if</strong> it was significant in <em>either</em> one of the two previous regressions</li>
</ol>
<p>I update the <code>pre_test</code> function from the first part of the post to compute also the post-double selection estimator.</p>
<pre><code class="language-python">def pre_test(d='ads', y='sales', x='past_sales', K=1000, **kwargs):
    
    # Init
    alphas = pd.DataFrame({'Long': np.zeros(K), 
             'Short': np.zeros(K), 
             'Pre-test': np.zeros(K),
             'Post-double': np.zeros(K)})

    # Loop over simulations
    for k in range(K):
        
        # Generate data
        df = dgp_pretest().generate_data(seed=k, **kwargs)
        
        # Compute coefficients
        alphas['Long'][k] = smf.ols(f'{y} ~ {d} + {x}', df).fit().params[1]
        alphas['Short'][k] = smf.ols(f'{y} ~ {d}', df).fit().params[1]
    
        # Compute significance of beta and gamma
        p_value_ydx = smf.ols(f'{y} ~ {d} + {x}', df).fit().pvalues[2]
        p_value_yx = smf.ols(f'{y} ~ {x}', df).fit().pvalues[1]
        p_value_dx = smf.ols(f'{d} ~ {x}', df).fit().pvalues[1]
        
        # Select pre-test specification based on regression of y on d and x
        if p_value_ydx&lt;0.05:
            alphas['Pre-test'][k] = alphas['Long'][k]
        else:
            alphas['Pre-test'][k] = alphas['Short'][k]
            
        # Select post-double specification based on regression of y on d and x
        if p_value_yx&lt;0.05 or p_value_dx&lt;0.05:
            alphas['Post-double'][k] = alphas['Long'][k]
        else:
            alphas['Post-double'][k] = alphas['Short'][k]
    
    return alphas
</code></pre>
<pre><code class="language-python">alphas = pre_test()
</code></pre>
<p>We can now plot the distributions (over simulations) of the estimated coefficients.</p>
<pre><code class="language-python">def plot_alphas(alphas, true_alpha):
    
    # Init plot
    K = len(alphas.columns)
    fig, axes = plt.subplots(1, K, figsize=(4*K, 5), sharey=True, sharex=True)

    # Make one plot for each set of coefficients
    for i, key in enumerate(alphas.columns):
        axes[i].hist(alphas[key].values, bins=30, lw=.1, color=f'C{int(i==3)*2}')
        axes[i].set_title(key)
        axes[i].axvline(true_alpha, c='r', ls='--')
        legend_text = [rf'$\alpha=${true_alpha}', rf'$\hat \alpha=${np.mean(alphas[key]):.4f}']
        axes[i].legend(legend_text, prop={'size': 10}, loc='upper right')
</code></pre>
<pre><code class="language-python">plot_alphas(alphas, true_alpha=1)
</code></pre>
<p><img src="img/pds_17_0.png" alt="png"></p>
<p>As we can see, the post-double selection estimator always correctly selects the long regression and therefore has the correct distribution.</p>
<h3 id="double-checks">Double-checks</h3>
<p>In the last post, we ran some simulations in order to investigate when pre-testing bias emerges. We saw that pre-testing is a problem for</p>
<ul>
<li>Small sample sizes $n$</li>
<li>Intermediate values of $\beta$</li>
<li>When the value of $\beta$ depends on the sample size</li>
</ul>
<p>Let&rsquo;s check that post-double selection removes regularization bias in <strong>all</strong> the previous cases.</p>
<p>First, let&rsquo;s simulate the distribution of the post-double selection estimator $\hat \alpha_{postdouble}$ for different sample sizes.</p>
<pre><code class="language-python">Ns = [100,300,1000,3000]
alphas = {f'N = {n:.0f}':  pre_test(N=n) for n in Ns}
</code></pre>
<pre><code class="language-python">def compare_alphas(alphas, true_alpha):
    
    # Init plot
    fig, axes = plt.subplots(1, len(alphas), figsize=(4*len(alphas), 5), sharey=True, sharex=True)

    # Make one plot for each set of coefficients
    for i, key in enumerate(alphas.keys()):
        axes[i].hist(alphas[key]['Pre-test'], bins=30, lw=.1, alpha=0.5)
        axes[i].hist(alphas[key]['Post-double'], bins=30, lw=.1, alpha=0.5, color='C2')
        axes[i].set_title(key)
        axes[i].axvline(true_alpha, c='r', ls='--')
        axes[i].legend([rf'$\alpha=${true_alpha}', 'Pre-test', 'Post-double'], 
                       prop={'size': 10}, loc='upper right')
</code></pre>
<pre><code class="language-python">compare_alphas(alphas, true_alpha=1)
</code></pre>
<p><img src="img/pds_23_0.png" alt="png"></p>
<p>For small samples, the distribution of the pre-testing estimator is not normal but rather bimodal. From the plots we can see that the post-double estimator is gaussian also in small sample sizes.</p>
<p>Now we repeat the same exercise, but for different values of $\beta$, the coefficient of <code>past_sales</code> on <code>sales</code>.</p>
<pre><code class="language-python">betas = 0.3 * np.array([0.1,0.3,1,3])
alphas = {f'beta = {b:.2f}': pre_test(b=b) for b in betas}
compare_alphas(alphas, true_alpha=1)
</code></pre>
<p><img src="img/pds_25_0.png" alt="png"></p>
<p>Again, the post-double selection estimator has a gaussian distribution irrespectively of the value of $\beta$, while he pre-testing estimator suffers from regularization bias.</p>
<p>For the last simulation, we change both the coefficient and the sample size at the same time.</p>
<pre><code class="language-python">betas = 0.3 * 30 / np.sqrt(Ns)
alphas = {f'N = {n:.0f}':  pre_test(b=b, N=n) for n,b in zip(Ns,betas)}
compare_alphas(alphas, true_alpha=1)
</code></pre>
<p><img src="img/pds_27_0.png" alt="png"></p>
<p>Also in this last case, the post-double selection estimator performs well and inference is not distorted.</p>
<h2 id="double-debiased-machine-learning">Double Debiased Machine Learning</h2>
<p>So far, we only have analyzed a linear, univariate example. What happens if the dimension of $X$ increases and we do not know the functional form through which $X$ affects $Y$ and $D$? In these cases, we can use machine learning algorithms to uncover these high-dimensional non-linear relationships.</p>
<p><a href="https://academic.oup.com/ectj/article/21/1/C1/5056401" target="_blank" rel="noopener">Chernozhukov, Chetverikov, Demirer, Duflo, Hansen, Newey, and Robins (2018)</a> investigate this setting. In particular, the authors consider the following partially linear model.</p>
<p>$$
Y = \alpha D + g(X) + u \
D = m(X) + v
$$</p>
<p>where $Y$ is the outcome variable, $D$ is the treatment to interest and $X$ is a potentially high-dimensional set of control variables.</p>
<h3 id="naive-approach">Naive approach</h3>
<p>A naive approach to estimation of $\alpha$ using machine learning methods would be, for example, to construct a sophisticated machine learning estimator for learning the regression function $\alpha D$ + $g(X)$.</p>
<ol>
<li>Split the sample in two: main sample and auxiliary sample [why? see note below]</li>
<li>Use the auxiliary sample to estimate $\hat g(X)$</li>
<li>Use the main sample to compute the orthogonalized component of $Y$ on $X$: $\ \hat u = Y - \hat{g} (X)$</li>
<li>Use the main sample to estimate the residualized OLS estimator from regressing $\hat u$ on $D$</li>
</ol>
<p>$$
\hat \alpha = \left( D' D \right) ^{-1} D' \hat u
$$</p>
<p>This estimator is going to have <strong>two problems</strong>:</p>
<ol>
<li>Slow rate of convergence, i.e. slower than $\sqrt(n)$</li>
<li>It will be biased because we are employing high dimensional regularized estimators (e.g. we are doing variable selection)</li>
</ol>
<p><strong>Note (1)</strong>: so far we have not talked about it, but variable selection procedure also introduce another type of bias: <strong>overfitting bias</strong>. This bias emerges because of the fact that the sample used to select the variables is the same that is used to estimate the coefficient of interest. This bias is <strong>easily accounted for</strong> with sample splitting: using different sub-samples for the selection and the estimation procedures.</p>
<p><strong>Note (2)</strong>: why can we use the residuals from step 3 to estimate $\alpha$ in step 4? Because of the <a href="https://towardsdatascience.com/59f801eb3299" target="_blank" rel="noopener">Frisch-Waugh-Lovell theorem</a>. If you are not familiar with it, I have written a <a href="https://towardsdatascience.com/59f801eb3299" target="_blank" rel="noopener">blog post on the Frisch-Waugh-Lovell theorem here</a>.</p>
<h3 id="orthogonalization">Orthogonalization</h3>
<p>Now consider a second construction that employs an orthogonalized formulation obtained by directly partialling out the effect of $X$ from $D$ to obtain the orthogonalized regressor $v = D − m(X)$.</p>
<ol>
<li>
<p>Split the sample in two: main sample and auxiliary sample</p>
</li>
<li>
<p>Use the auxiliary sample to estimate $\hat g(X)$ from</p>
<p>$$
Y = \alpha D + g(X) + u \
$$</p>
</li>
<li>
<p>Use the auxiliary sample to estimate $\hat m(X)$ from</p>
<p>$$
D = m(X) + v
$$</p>
</li>
<li>
<p>Use the main sample to compute the orthogonalized component of $D$ on $X$ as</p>
<p>$$
\hat v = D - \hat m(X)
$$</p>
</li>
<li>
<p>Use the main sample to estimate the double-residualized OLS estimator as</p>
<p>$$
\hat \alpha = \left( \hat{v}' D \right) ^{-1} \hat{v}' \left( Y - \hat g(X) \right)
$$</p>
</li>
</ol>
<p>The estimator is <strong>root-N consistent</strong>! This means that not only the estimator converges to the true value as the sample sizes increases (i.e. it&rsquo;s consistent), but also its standard deviation does (i.e. it&rsquo;s root-N consistent).</p>
<p>However, the estimator still has a lower rate of convergence because of sample splitting. The problem is solved by inverting the split sample, re-estimating the coefficient and averaging the two estimates. Note that this procedure is valid since the two estimates are independent by the sample splitting procedure.</p>
<h3 id="a-cautionary-tale">A Cautionary Tale</h3>
<p>Before we conclude, I have to mention a recent research paper by <a href="https://arxiv.org/abs/2108.11294" target="_blank" rel="noopener">Hünermund, Louw, and Caspi (2022)</a>, in which the authors show that double-debiased machine learning can easily <strong>backfire</strong>, if we apply blindly.</p>
<p>The problem is related to <strong>bad control variables</strong>. If you have never heard this term, I have written an introductory <a href="https://towardsdatascience.com/b63dc69e3d8c" target="_blank" rel="noopener">blog post on good and bad control variables here</a>. In short, conditioning the analysis on additional features is not always good for causal inference. Depending on the setting, there might exist variables that we want to leave out of our analysis since their <strong>inclusion</strong> can bias the coefficient of interest, preventing a causal interpretation. The simplest example is variables that are common outcomes, of both the treatment $D$ and outcome variable $Y$.</p>
<p>The double-debiased machine learning model implicitly assumes that the control variables $X$ are (weakly) <strong>common causes</strong> to both the outcome $Y$ and the treatment $D$. If this is the case, and no further mediated/indirect relationship exists between $X$ and $Y$, there is no problem. However, if, for example, some variable among the controls $X$ is a common effect instead of a common cause, its inclusion will bias the coefficient of interest. Moreover, this variable is likely to be highly correlated either with the outcome $Y$ or with the treatment $D$. In the latter case, this implies that post-double selection might include it in cases in which simple selection would have not. Therefore, in presence of bad control variables, doule-debiased machine learning might be <strong>even worse</strong> than simple pre-testing.</p>
<p>In short, as for any method, it is <strong>crucial</strong> to have a clear understanding of the method&rsquo;s assumptions and to always check for potential violations.</p>
<h2 id="conclusion">Conclusion</h2>
<p>In this post, we have seen how to use post-double selection and, more generally, double debiased machine learning to get rid of an important source of bias: regularization bias.</p>
<p>This contribution by Victor Chernozhukov and co-authors has been undoubtedly one of the most relevant advances in causal inferences in the last decade. It is now widely employed in the industry and included in the most used causal inference packages, such as <a href="https://econml.azurewebsites.net/" target="_blank" rel="noopener">EconML</a> (Microsoft) and <a href="https://causalml.readthedocs.io/en/latest/" target="_blank" rel="noopener">causalml</a> (Uber).</p>
<p>If you (understandably) feel the need for more material on double-debiased machine learning, but you do not feel like reading academic papers (also very understandable), here is a good compromise.</p>
<br>

<div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
  <iframe src="https://www.youtube.com/embed/eHOjmyoPCFU" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video"></iframe>
</div>

<br>
<p>In this video lecture, Victor Chernozhukov himself presents the idea. The video lecture is relatively heavy on math and statistics, but you cannot get a more qualified and direct source than this!</p>
<h3 id="references">References</h3>
<p>[1] A. Belloni, D. Chen, V. Chernozhukov, C. Hansen, <a href="https://onlinelibrary.wiley.com/doi/abs/10.3982/ECTA9626" target="_blank" rel="noopener">Sparse Models and Methods for Optimal Instruments With an Application to Eminent Domain</a> (2012), <em>Econometrica</em>.</p>
<p>[2] A. Belloni, V. Chernozhukov, C. Hansen, <a href="https://academic.oup.com/restud/article-abstract/81/2/608/1523757" target="_blank" rel="noopener">Inference on treatment effects after selection among high-dimensional controls</a> (2014), <em>The Review of Economic Studies</em>.</p>
<p>[3] V. Chernozhukov, D. Chetverikov, M. Demirer, E. Duflo, C. Hansen, W. Newey, J. Robins, <a href="https://academic.oup.com/ectj/article/21/1/C1/5056401" target="_blank" rel="noopener">Double/debiased machine learning for treatment and structural parameters</a> (2018), <em>The Econometrics Journal</em>.</p>
<p>[4] P. Hünermund, B. Louw, I. Caspi, <a href="https://arxiv.org/abs/2108.11294" target="_blank" rel="noopener">Double Machine Learning and Automated Confounder Selection - A Cautionary Tale</a> (2022), <em>working paper</em>.</p>
<h3 id="related-articles">Related Articles</h3>
<ul>
<li><a href="https://towardsdatascience.com/eb767a59975b" target="_blank" rel="noopener">Double Debiased Machine Learning (part 1)</a></li>
<li><a href="https://towardsdatascience.com/344ac1477699" target="_blank" rel="noopener">Understanding Omitted Variable Bias</a></li>
<li><a href="https://towardsdatascience.com/59f801eb3299" target="_blank" rel="noopener">Understanding The Frisch-Waugh-Lovell Theorem</a></li>
<li><a href="https://towardsdatascience.com/b63dc69e3d8c" target="_blank" rel="noopener">DAGs and Control Variables</a></li>
</ul>
<h3 id="code">Code</h3>
<p>You can find the original Jupyter Notebook here:</p>
<p><a href="https://github.com/matteocourthoud/Blog-Posts/blob/main/pds.ipynb" target="_blank" rel="noopener">https://github.com/matteocourthoud/Blog-Posts/blob/main/pds.ipynb</a></p>

          </div>
          








<div class="share-box" aria-hidden="true">
  <ul class="share">
    
      
      
      
        
      
      
      
      <li>
        <a href="https://twitter.com/intent/tweet?url=https://matteocourthoud.github.io/post/pds/&amp;text=Double%20Debiased%20Machine%20Learning%20%28part%202%29" target="_blank" rel="noopener" class="share-btn-twitter">
          <i class="fab fa-twitter"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://www.facebook.com/sharer.php?u=https://matteocourthoud.github.io/post/pds/&amp;t=Double%20Debiased%20Machine%20Learning%20%28part%202%29" target="_blank" rel="noopener" class="share-btn-facebook">
          <i class="fab fa-facebook"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="mailto:?subject=Double%20Debiased%20Machine%20Learning%20%28part%202%29&amp;body=https://matteocourthoud.github.io/post/pds/" target="_blank" rel="noopener" class="share-btn-email">
          <i class="fas fa-envelope"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://www.linkedin.com/shareArticle?url=https://matteocourthoud.github.io/post/pds/&amp;title=Double%20Debiased%20Machine%20Learning%20%28part%202%29" target="_blank" rel="noopener" class="share-btn-linkedin">
          <i class="fab fa-linkedin-in"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://web.whatsapp.com/send?text=Double%20Debiased%20Machine%20Learning%20%28part%202%29%20https://matteocourthoud.github.io/post/pds/" target="_blank" rel="noopener" class="share-btn-whatsapp">
          <i class="fab fa-whatsapp"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://t.me/share/url?url=https://matteocourthoud.github.io/post/pds/&amp;text=%7btext%7d" target="_blank" rel="noopener" class="share-btn-telegram">
          <i class="fab fa-telegram"></i>
        </a>
      </li>
    
  </ul>
</div>











  
  



  
  
  
    
  
  
  
  <div class="media author-card content-widget-hr">
    
      
      <a href="https://matteocourthoud.github.io/"><img class="avatar mr-3 avatar-circle" src="/author/avatar_hu365eedc833ccd5578a90de7c849ec45e_385094_270x270_fill_q75_lanczos_center.jpg" alt=""></a>
    

    <div class="media-body">
      <h5 class="card-title"><a href="https://matteocourthoud.github.io/"></a></h5>
      
      <p class="card-text">My research fields are empirical Industrial Organization and Competition Policy. My research interests include the relationship between competition and innovation, big data, artificial intelligence, platform markets, peer to peer services.</p>
      <ul class="network-icon" aria-hidden="true">
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://medium.com/@matteo.courthoud" target="_blank" rel="noopener">
        <i class="fab fa-medium"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://www.linkedin.com/in/matteo-courthoud-7335198a/" target="_blank" rel="noopener">
        <i class="fab fa-linkedin"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://twitter.com/MatteoCourthoud/" target="_blank" rel="noopener">
        <i class="fab fa-twitter"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://github.com/matteocourthoud" target="_blank" rel="noopener">
        <i class="fab fa-github"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://open.spotify.com/user/1180947523" target="_blank" rel="noopener">
        <i class="fab fa-spotify"></i>
      </a>
    </li>
  
</ul>

    </div>
  </div>
















  
  




        </div>
        </article>
    </main>
  </div>
</div>
  </div>

  <div class="page-footer">
    
    
    <div class="container">
      <footer class="site-footer">

  

  
  <p class="powered-by">
    
      <a href="/privacy/">Privacy Policy</a>
    
    
       &middot; 
      <a href="/terms/">Terms</a>
    
  </p>
  

  

  
  







</footer>

    </div>
    
  </div>

  
<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        <pre><code class="tex hljs"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

      

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/instant.page/5.1.0/instantpage.min.js" integrity="sha512-1+qUtKoh9XZW7j+6LhRMAyOrgSQKenQ4mluTR+cvxXjP1Z54RxZuzstR/H9kgPXQsVB8IW7DMDFUJpzLjvhGSQ==" crossorigin="anonymous"></script>

      
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js" integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin="anonymous"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js" integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin="anonymous"></script>
      

      
      

      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mermaid/8.8.4/mermaid.min.js" integrity="sha512-as1BF4+iHZ3BVO6LLDQ7zrbvTXM+c/1iZ1qII/c3c4L8Rn5tHLpFUtpaEtBNS92f+xGsCzsD7b62XP3XYap6oA==" crossorigin="anonymous" title="mermaid"></script>
      

      
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/highlight.min.js" integrity="sha512-TDKKr+IvoqZnPzc3l35hdjpHD0m+b2EC2SrLEgKDRWpxf2rFCxemkgvJ5kfU48ip+Y+m2XVKyOCD85ybtlZDmw==" crossorigin="anonymous"></script>
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/languages/r.min.js"></script>
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/languages/python.min.js"></script>
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/languages/julia.min.js"></script>
        
      

    

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.7.1/leaflet.min.js" integrity="sha512-SeiQaaDh73yrb56sTW/RgVdi/mMqNeM2oBwubFHagc5BkixSpP1fvqF47mKzPGWYSSy4RwbBunrJBQ4Co8fRWA==" crossorigin="anonymous"></script>
    

    
    

    
    
    
      
      <script id="search-hit-fuse-template" type="text/x-template">
        <div class="search-hit" id="summary-{{key}}">
          <div class="search-hit-content">
            <div class="search-hit-name">
              <a href="{{relpermalink}}">{{title}}</a>
              <div class="article-metadata search-hit-type">{{type}}</div>
              <p class="search-hit-description">{{snippet}}</p>
            </div>
          </div>
        </div>
      </script>
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js" integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin="anonymous"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js" integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin="anonymous"></script>
      
    

    
    

    
    
    
    

    
    <script src="/js/bootstrap.bundle.min.6aed84840afc03ab4d5750157f69c120.js"></script>

    
    
      
      
      
      
      
      
      
    

    
    
    
    
    
    
    
    
      
      
    
    
    <script src="/en/js/wowchemy.min.cf8ca859a9b74f8b1cd804621b13e5f1.js"></script>

    
  <script async defer src="https://buttons.github.io/buttons.js"></script>




</body>
</html>
