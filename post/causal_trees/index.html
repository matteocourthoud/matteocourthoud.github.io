<!DOCTYPE html><html lang="en-us" >

<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  
  
  
    <meta name="generator" content="Wowchemy 5.1.0 for Hugo" />
  

  

  
  
  
  
  
    
    
    
  
  

  

  
  
  
    
  
  <meta name="description" content="How to use regression trees to estimate heterogeneous treatment effects.
In causal inference we are usually interested in estimating the causal effect of a treatment (a drug, ad, product, &hellip;) on an outcome of interest (a disease, firm revenue, customer satisfaction, &hellip;)." />

  
  <link rel="alternate" hreflang="en-us" href="https://matteocourthoud.github.io/post/causal_trees/" />

  







  




  
  
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
  

  
  
  
    <meta name="theme-color" content="#003f5c" />
  

  
  
    
    <script src="/js/mathjax-config.js"></script>
  

  
  
  
  
    
    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha256-FMvZuGapsJLjouA6k7Eo2lusoAX9i0ShlWFG6qt7SLc=" crossorigin="anonymous">

    
    

    
    
    
      
    
    
      
      
        
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/styles/github.min.css" crossorigin="anonymous" title="hl-light" media="print" onload="this.media='all'">
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/styles/dracula.min.css" crossorigin="anonymous" title="hl-dark" media="print" onload="this.media='all'" disabled>
        
      
    

    
    
    
      <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.7.1/leaflet.min.css" integrity="sha512-1xoFisiGdy9nvho8EgXuXvnpR5GAMSjFwp40gSRE3NwdUdIMIKuPa7bqoUhLD0O/5tPNhteAsE5XyyMi5reQVA==" crossorigin="anonymous" media="print" onload="this.media='all'">
    

    

    
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js" integrity="" crossorigin="anonymous" async></script>
      
    
      
      

      
      

      
    
      
      

      
      

      
    
  

  
  
  
    
      
      <link rel="preload" as="style" href="https://fonts.googleapis.com/css2?family=Roboto+Mono&family=Source+Sans+Pro:wght@200;300;400;700&display=swap">
      <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Roboto+Mono&family=Source+Sans+Pro:wght@200;300;400;700&display=swap" media="print" onload="this.media='all'">
    
  

  
  
  
  
  
  <link rel="stylesheet" href="/css/wowchemy.4f7182ca394d705ee32d9d7750e9aa1d.css" />

  




<script async src="https://www.googletagmanager.com/gtag/js?id=UA-144780600-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];

  function gtag() {
      dataLayer.push(arguments);
  }

  function trackOutboundLink(url, target) {
    gtag('event', 'click', {
         'event_category': 'outbound',
         'event_label': url,
         'transport_type': 'beacon',
         'event_callback': function () {
           if (target !== '_blank') {
             document.location = url;
           }
         }
    });
    console.debug("Outbound link clicked: " + url);
  }

  function onClickCallback(event) {
    if ((event.target.tagName !== 'A') || (event.target.host === window.location.host)) {
      return;
    }
    trackOutboundLink(event.target, event.target.getAttribute('target'));  
  }

  gtag('js', new Date());
  gtag('config', 'UA-144780600-1', { 'anonymize_ip': true });
  gtag('set', {'cookie_flags': 'SameSite=None;Secure'});

  
  document.addEventListener('click', onClickCallback, false);
</script>


  

  

  




  
  
  

  

  
    <link rel="manifest" href="/index.webmanifest" />
  

  <link rel="icon" type="image/png" href="/media/icon_hu03e9b3967b83fd39296ec9da5ff1ea05_201175_32x32_fill_lanczos_center_3.png" />
  <link rel="apple-touch-icon" type="image/png" href="/media/icon_hu03e9b3967b83fd39296ec9da5ff1ea05_201175_180x180_fill_lanczos_center_3.png" />

  <link rel="canonical" href="https://matteocourthoud.github.io/post/causal_trees/" />

  
  
  
  
  
  
  
  
    
  
  
  <meta property="twitter:card" content="summary_large_image" />
  
  <meta property="og:site_name" content="Matteo Courthoud" />
  <meta property="og:url" content="https://matteocourthoud.github.io/post/causal_trees/" />
  <meta property="og:title" content="Understanding Causal Trees | Matteo Courthoud" />
  <meta property="og:description" content="How to use regression trees to estimate heterogeneous treatment effects.
In causal inference we are usually interested in estimating the causal effect of a treatment (a drug, ad, product, &hellip;) on an outcome of interest (a disease, firm revenue, customer satisfaction, &hellip;)." /><meta property="og:image" content="https://matteocourthoud.github.io/post/causal_trees/featured.png" />
    <meta property="twitter:image" content="https://matteocourthoud.github.io/post/causal_trees/featured.png" /><meta property="og:locale" content="en-us" />
  
    
      <meta
        property="article:published_time"
        content="2023-07-10T00:00:00&#43;00:00"
      />
    
    <meta property="article:modified_time" content="2023-07-10T00:00:00&#43;00:00">
  

  


    






  




<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://matteocourthoud.github.io/post/causal_trees/"
  },
  "headline": "Understanding Causal Trees",
  
  "image": [
    "https://matteocourthoud.github.io/post/causal_trees/featured.png"
  ],
  
  "datePublished": "2023-07-10T00:00:00Z",
  "dateModified": "2023-07-10T00:00:00Z",
  
  "publisher": {
    "@type": "Organization",
    "name": "Matteo Courthoud",
    "logo": {
      "@type": "ImageObject",
      "url": "https://matteocourthoud.github.io/media/icon_hu03e9b3967b83fd39296ec9da5ff1ea05_201175_192x192_fill_lanczos_center_3.png"
    }
  },
  "description": "How to use regression trees to estimate heterogeneous treatment effects.\nIn causal inference we are usually interested in estimating the causal effect of a treatment (a drug, ad, product, \u0026hellip;) on an outcome of interest (a disease, firm revenue, customer satisfaction, \u0026hellip;)."
}
</script>

  

  

  
  
  
  
  
    <script src="https://cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.1.1/cookieconsent.min.js" integrity="sha256-5VhCqFam2Cn+yjw61zbBNrbHVJ6SRydPeKopYlngbiQ=" crossorigin="anonymous"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.1.1/cookieconsent.min.css" integrity="sha256-zQ0LblD/Af8vOppw18+2anxsuaz3pWYyVWi+bTvTH8Q=" crossorigin="anonymous">
  
  <script>
  window.addEventListener("load", function(){
    window.cookieconsent.initialise({
      "palette": {
        "popup": {
          "background": "#003f5c",
          "text": "rgb(255, 255, 255)"
        },
        "button": {
          "background": "rgb(255, 255, 255)",
          "text": "#003f5c"
        }
      },
      "theme": "classic",
      "content": {
        "message": "This website uses cookies to ensure you get the best experience on our website.",
        "dismiss": "Got it!",
        "link": "Learn more",
        "href": "/privacy/"
      }
    })});
  </script>


  





  <title>Understanding Causal Trees | Matteo Courthoud</title>
</head>


<body id="top" data-spy="scroll" data-offset="70" data-target="#TableOfContents" class="page-wrapper   " data-wc-page-id="801c3459d81ea394bb44113ad5d513d6" >

  
  
  
  
  
  
  
  
  <script src="/js/wowchemy-init.min.66d3e0fff6d32c4ece05adee927fbd96.js"></script>

  


<aside class="search-modal" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Search</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#" aria-label="Close"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        
        <input name="q" id="search-query" placeholder="Search..." autocapitalize="off"
        autocomplete="off" autocorrect="off" spellcheck="false" type="search" class="form-control"
        aria-label="Search...">
        
      </div>

      
      

      

    </section>
    <section class="section-search-results">

      <div id="search-hits">
        
      </div>

    </section>
  </div>
</aside>



  <div class="page-header">
    












<nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id="navbar-main">
  <div class="container-xl">

    
    <div class="d-none d-lg-inline-flex">
      <a class="navbar-brand" href="/">Matteo Courthoud</a>
    </div>
    

    
    <button type="button" class="navbar-toggler" data-toggle="collapse"
            data-target="#navbar-content" aria-controls="navbar-content" aria-expanded="false" aria-label="Toggle navigation">
    <span><i class="fas fa-bars"></i></span>
    </button>
    

    
    <div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none">
      <a class="navbar-brand" href="/">Matteo Courthoud</a>
    </div>
    

    
    
    <div class="navbar-collapse main-menu-item collapse justify-content-end" id="navbar-content">

      
      <ul class="navbar-nav d-md-inline-flex">
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
            
            
            
              
            
            
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#about"><span>Home</span></a>
        </li>

        
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
            
            
            
              
            
            
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#posts"><span>Posts</span></a>
        </li>

        
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
            
            
            
              
            
            
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#projects"><span>Research</span></a>
        </li>

        
        

        
        <li class="nav-item dropdown">
          <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown" aria-haspopup="true"><span>Courses</span><span class="caret"></span>
          </a>
          <div class="dropdown-menu">
            
              <a class="dropdown-item" href="/course/ml-econ/"><span>Machine Learning for Economics</span></a>
            
              <a class="dropdown-item" href="/course/data-science/"><span>Data Science with Python</span></a>
            
              <a class="dropdown-item" href="/course/empirical-io/"><span>PhD Industrial Organization</span></a>
            
              <a class="dropdown-item" href="/course/metrics/"><span>PhD Econometrics</span></a>
            
              <a class="dropdown-item" href="https://pp4rs.github.io/"><span>Programming Practices for Research</span></a>
            
          </div>
        </li>

        
        

        

        
        
        

        

        
        
        
        

        
          
            
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/cv"><span>CV</span></a>
        </li>

        
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
            
            
            
              
            
            
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#contact"><span>Contact</span></a>
        </li>

        
        

      

        
      </ul>
    </div>

    <ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2">

      
      
        
      

      
      

      
      

      
      

    </ul>

  </div>
</nav>


  </div>

  <div class="page-body">
    <div class="container-fluid docs">
  <div class="row">

    <div class="col-xl-2 col-lg-2 d-none d-xl-block d-lg-block empty">
    </div>

    <div class="col-2 col-xl-2 col-lg-2 d-none d-lg-block docs-toc">
      <ul class="nav toc-top">
        <li><a href="#" id="back_to_top" class="docs-toc-title">Contents</a></li>
      </ul>
      <nav id="TableOfContents">
  <ul>
    <li><a href="#online-discounts">Online Discounts</a></li>
    <li><a href="#conditional-average-treatment-effects">Conditional Average Treatment Effects</a></li>
    <li><a href="#causal-trees">Causal Trees</a>
      <ul>
        <li><a href="#generating-splits">Generating Splits</a></li>
        <li><a href="#implementation">Implementation</a></li>
        <li><a href="#inference">Inference</a></li>
        <li><a href="#performance">Performance</a></li>
      </ul>
    </li>
    <li><a href="#conclusion">Conclusion</a>
      <ul>
        <li><a href="#references">References</a></li>
        <li><a href="#related-articles">Related Articles</a></li>
        <li><a href="#code">Code</a></li>
      </ul>
    </li>
  </ul>
</nav>
    </div>

    <main class="col-xl-8 col-lg-8 docs-content" role="main">
        <article class="article">
        




















  


<div class="article-container pt-3">
  <h1>Understanding Causal Trees</h1>

  

  


<div class="article-metadata">

  
  

  
  <span class="article-date">
    
    
      
    
    Jul 10, 2023
  </span>
  

  

  
  <span class="middot-divider"></span>
  <span class="article-reading-time">
    16 min read
  </span>
  

  
  
  
  
  
  

  
  

</div>

  





</div>


<div class="article-header article-container featured-image-wrapper mt-4 mb-4" style="max-width: 1540px; max-height: 874px;">
  <div style="position: relative">
    <img src="/post/causal_trees/featured.png" alt="" class="featured-image">
    
  </div>
</div>


        <div class="article-container">
          <div class="article-style" align="justify">
            <p><em>How to use regression trees to estimate heterogeneous treatment effects.</em></p>
<p>In causal inference we are usually interested in estimating the causal effect of a treatment (a drug, ad, product, &hellip;) on an outcome of interest (a disease, firm revenue, customer satisfaction, &hellip;). However, knowing that a treatment works on average is often not sufficient and we would like to know for which subjects (patients, users, customers, &hellip;) it works better or worse, i.e. we would like to estimate <strong>heterogeneous treatment effects</strong>.</p>
<p>Estimating heterogeneous treatments effects allows us to do <strong>targeting</strong>. Knowing which customers are more likely to react to a discount allows a company to spend less money by offering fewer but better targeted discounts. This works also for negative effects: knowing for which patients a certain drug has side effects allows a pharmaceutical company to warn or exclude them from the treatment. There is also a more subtle advantage of estimating heterogeneous treatment effects: knowing <strong>for whom</strong> a treatment works allows us to better understand <strong>how</strong> a treatment works. Knowing that the effect of a discount does not depend on the income of its recipient but rather by its buying habits  tells us that maybe it is not a matter of money, but rather a matter of attention or loyalty.</p>
<p>In this article, we will explore the estimation of heterogeneous treatment effects using a modified version of regression trees (and forests). From a machine learning perspective, there are two fundamental <strong>differences between causal trees and predictive trees</strong>. First of all, the target is the treatment effect, which is an inherently unobservable object. Second, we are interested in doing inference, which means quantifying the uncertainty of our estimates.</p>
<h2 id="online-discounts">Online Discounts</h2>
<p>For the rest of the article, we are going to use a toy example, for the sake of exposition: suppose we were an <strong>online shop</strong> and we are interested in understanding whether offering discounts to new customers increases their expenditure. In particular, we would like to know if offering discounts is more effective for some customers with respect to others, since we would prefer not to give discounts to customers that would spend anyways. Moreover, it could also be that spamming customers with pop-ups could deter them from buying, having the opposite effect.</p>
<img src="fig/causal_trees1.jpg" width="300px"/>
<p>To understand whether and how much the discounts are effective we run an <strong>A/B test</strong>: whenever a new user visits our online shop, we randomly decide whether to offer them the discount or not. I import the data-generating process <code>dgp_online_discounts()</code> from <a href="https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/src/dgp.py" target="_blank" rel="noopener"><code>src.dgp</code></a>. With respect to previous articles, I generated a new DGP parent class that handles randomization and data generation, while its children classes contain specific use cases. I also import some plotting functions and libraries from <a href="https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/src/utils.py" target="_blank" rel="noopener"><code>src.utils</code></a>. To include not only code but also data and tables, I use <a href="https://deepnote.com/" target="_blank" rel="noopener">Deepnote</a>, a Jupyter-like web-based collaborative notebook environment.</p>
<pre><code class="language-python">%matplotlib inline
%config InlineBackend.figure_format = 'retina'

from src.utils import *
from src.dgp import dgp_online_discounts
</code></pre>
<pre><code class="language-python">dgp = dgp_online_discounts(n=100_000)
df = dgp.generate_data()
df.head()
</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
</code></pre>
<p></style></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>time</th>
      <th>device</th>
      <th>browser</th>
      <th>region</th>
      <th>discount</th>
      <th>spend</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>10.78</td>
      <td>mobile</td>
      <td>edge</td>
      <td>9</td>
      <td>0</td>
      <td>0.46</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.57</td>
      <td>desktop</td>
      <td>firefox</td>
      <td>9</td>
      <td>1</td>
      <td>11.04</td>
    </tr>
    <tr>
      <th>2</th>
      <td>3.74</td>
      <td>mobile</td>
      <td>safari</td>
      <td>7</td>
      <td>0</td>
      <td>1.81</td>
    </tr>
    <tr>
      <th>3</th>
      <td>13.37</td>
      <td>desktop</td>
      <td>other</td>
      <td>5</td>
      <td>0</td>
      <td>31.90</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.71</td>
      <td>mobile</td>
      <td>explorer</td>
      <td>2</td>
      <td>0</td>
      <td>15.42</td>
    </tr>
  </tbody>
</table>
</div>
<p>We have data on 100.000 website visitors, for whom we observe the <code>time</code> of the day, the <code>device</code> they use, their <code>browser</code> and their geographical <code>region</code>. We also see whether they were offered the <code>discount</code>, our treatment, and what is their <code>spend</code>, the outcome of interest.</p>
<p>Since the treatment was randomly assigned, we can use a simple <strong>difference-in-means</strong> estimator to estimate the treatment effect. We expect the treatment and control group to be similar, except for the <code>discount</code>, therefore we can causally attribute any difference in <code>spend</code> to the <code>discount</code>.</p>
<pre><code class="language-python">smf.ols('spend ~ discount', df).fit().summary().tables[1]
</code></pre>
<table class="simpletable">
<tr>
      <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  
</tr>
<tr>
  <th>Intercept</th> <td>    5.0306</td> <td>    0.045</td> <td>  110.772</td> <td> 0.000</td> <td>    4.942</td> <td>    5.120</td>
</tr>
<tr>
  <th>discount</th>  <td>    1.9492</td> <td>    0.064</td> <td>   30.346</td> <td> 0.000</td> <td>    1.823</td> <td>    2.075</td>
</tr>
</table>
<p>The discount seems to be effective: on average the spend in the treatment group increases by 3.86$. But are all customers equally affected?</p>
<p>To answer this question, we would like to estimate <strong>heterogeneous treatment effects</strong>, possibly at the individual level.</p>
<h2 id="conditional-average-treatment-effects">Conditional Average Treatment Effects</h2>
<p>There are many possible ways to estimate heterogenous treatment effects. The most common is to split the population in groups based on some observable characteristic, which in our case could be the <code>device</code>, the <code>browser</code> or the geographical <code>region</code>. Once you have decided which variable to split your data on, you can simply interact the treatment variable (<code>discount</code>) with the dimension of treatment heterogeneity. Let&rsquo;s take <code>device</code> for example.</p>
<pre><code class="language-python">smf.ols('spend ~ discount * device', df).fit().summary().tables[1]
</code></pre>
<table class="simpletable">
<tr>
              <td></td>                 <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  
</tr>
<tr>
  <th>Intercept</th>                 <td>    5.0006</td> <td>    0.064</td> <td>   78.076</td> <td> 0.000</td> <td>    4.875</td> <td>    5.126</td>
</tr>
<tr>
  <th>device[T.mobile]</th>          <td>    0.0602</td> <td>    0.091</td> <td>    0.664</td> <td> 0.507</td> <td>   -0.118</td> <td>    0.238</td>
</tr>
<tr>
  <th>discount</th>                  <td>    1.2264</td> <td>    0.091</td> <td>   13.527</td> <td> 0.000</td> <td>    1.049</td> <td>    1.404</td>
</tr>
<tr>
  <th>discount:device[T.mobile]</th> <td>    1.4447</td> <td>    0.128</td> <td>   11.261</td> <td> 0.000</td> <td>    1.193</td> <td>    1.696</td>
</tr>
</table>
<p>How do we interpret the regression results? The effect of the <code>discount</code> on customers&rsquo; <code>spend</code> is $1.22$$ but it increases by a further $1.44$$ if the customer is accessing the website from a mobile <code>device</code>.</p>
<p>Splitting is easy for categorical variables, but for a continuous variable like <code>time</code> it is not intuitive where to split. Every hour? And which dimension is more informative? It would be temping to try all possible splits, but the more we split the data, the more it is likely that we find spurious results (i.e. we overfit, in machine learning lingo). It would be great if we could <strong>let the data speak</strong> and select the minimum and most informative splits.</p>
<p>In a <a href="https://medium.com/towards-data-science/understanding-meta-learners-8a9c1e340832" target="_blank" rel="noopener">separate post</a>, I have shown how the so-called <strong>meta-learners</strong> take this approach to causal inference. The idea is to predict the outcome conditional on the treatment status for each observation, and then compare the predicted conditional on treatment, with the predicted outcome conditional on control. The difference is the individual treatment effect.</p>
<p>The problem with meta-learners is that they use all their <a href="https://en.wikipedia.org/wiki/Degrees_of_freedom_%28statistics%29" target="_blank" rel="noopener">degrees of freedom</a> in predicting the outcome. However, we are interested to predict treatment effect heterogeneity. If most of the variation in the outcome is <em>not</em> in the treatment dimension, we will get very poor estimates of the treatment effects.</p>
<p>Is it possible to instead directly concentrate on the <strong>prediction of individual treatment effects</strong>? Let&rsquo;s define $Y$ the outcome of interest, <code>spend</code> in our case, $D$ the treatment, the <code>discount</code>, and $X$ other observable characteristics. The <em>ideal</em> objective function is</p>
<p>$$
\sum_i \Big [ ( \tau_i - \hat \tau_i(X))^2 \Big ]
$$</p>
<p>where $\tau_i$ is the treatment effect of individual $i$. However, this objective function is <strong>unfeasible</strong> since we do not observe $\tau_i$.</p>
<p>But, turns out that there is a way to get an unbiased estimate of the <strong>individual treatment effect</strong>. The <strong>idea</strong> is to use an auxiliary outcome variable, whose expected value for each individual is the individual treatment effect. This variable is</p>
<p>$$
Y_i^* = \frac{Y_i}{D_i \cdot p(X_i) - (1-D_i) \cdot (1-p(X_i))}
$$</p>
<p>where $p(X_i)$ is the <a href="https://en.wikipedia.org/wiki/Propensity_score_matching" target="_blank" rel="noopener"><strong>propensity score</strong></a> of observation $i$, i.e. its probability of being treated. A crucial assuption here is <strong>unconfoundedness</strong>, also known as ignorability or selection on observables. In short, we will assume that, conditional on some observables $X$ the treatment assignment is as good as random.</p>
<p>$$
\left\lbrace Y_i^{(0)}, Y_i^{(1)} \right \rbrace \ \perp \ D_i | X_i
$$</p>
<p>where $Y_i^{(0)}$ and $Y_i^{(1)}$ denote the control and treated potential outcomes, respectively. In our case, we have randomized assignment, therefore we do not have to worry about unconfoundedness, unless the randomization went wrong.</p>
<p>In randomized experiments, the propensity score is known since randomization is fully under control of the experimenter. For example, in our case, the probability of treatment was 50%. In quasi-experimental studies instead, when the treatment probability is not known, it has to be estimated. Even in randomized experiments, it is always better to estimate rather than inpute the propensity scores, since it guards against sampling variation in the randomization. For more details on the propensity scores and how they are used in causal inference, I have a separate post <a href="https://medium.com/towards-data-science/matching-weighting-or-regression-99bf5cffa0d9" target="_blank" rel="noopener">here</a>.</p>
<p>Let&rsquo;s first generate dummy variables for our categorical variables, <code>device</code>, <code>browser</code> and <code>region</code>.</p>
<pre><code class="language-python">df_dummies = pd.get_dummies(df[dgp.X[1:]], drop_first=True)
df = pd.concat([df, df_dummies], axis=1)
X = ['time'] + list(df_dummies.columns)
</code></pre>
<p>We fit a <code>LogisticRegression</code> and use it to predict the treatment probability, i.e. construct the propensity score.</p>
<pre><code class="language-python">from sklearn.linear_model import LogisticRegression

df['pscore'] = LogisticRegression().fit(df[X], df[dgp.D]).predict_proba(df[X])[:,1]
sns.histplot(data=df, x='pscore', hue='discount').set(
    title='Predicted propensity scores', xlim=[0,1], xlabel='Propensity Score');
</code></pre>
<p><img src="img/causal_trees_18_0.png" alt="png"></p>
<p>As expected, most propensity scores are very close to 0.5, the probability of treatment used in randomization.</p>
<p>We now have all the elements to compute our auxiliary outcome variable $Y^*$.</p>
<pre><code class="language-python">df['y_star'] = df[dgp.Y[0]] / (df[dgp.D] * df['pscore'] - (1-df[dgp.D]) * (1-df['pscore']))
</code></pre>
<p>As we said before, the idea is to use $Y^*$ as the target of a prediction problem, since the expected value is exactly the individual treatment effect. Let&rsquo;s check its average in the data.</p>
<pre><code class="language-python">df['y_star'].mean()
</code></pre>
<pre><code>1.94501174385229
</code></pre>
<p>Indeed its average is almost identical to the previously estimated average treatment effect of 3.85.</p>
<p>How is it possible that, with a single observation and an estimate of the propensity score, we can estimate the individual treatment effect? What are the drawbacks?</p>
<p>The <strong>intuition</strong> is to approach the problem from a different perspective: <em>ex-ante</em>, before the experiment. Imagine that our dataset had a single observation, $i$. We know that the treatment probability is $p(X_i)$, the propensity score. Therefore, in expectation, our dataset has $p(X_i)$ observations in the treatment group and $1 - p(X_i)$ observations in the control group. The rest is business as usual: we estimate the treatment effect as the difference in average outcomes between the two groups! And indeed that is what we would do:</p>
<p>$$
Y_i^* = \frac{Y_i D_i}{p(X_i)} - \frac{Y_i (1-D_i)}{1-p(X_i)}
$$</p>
<p>The only difference is that we have a single observation.</p>
<p>This trick comes at a cost: $Y_i^*$ is an unbiased estimator for the individual treatment effect, but has a very <strong>high variance</strong>. This is immediately visible by plotting its distribution.</p>
<pre><code class="language-python">fig, ax = plt.subplots()
sns.histplot(df['y_star'], ax=ax).set(title='Distribution of Auxiliary Variable');
</code></pre>
<p><img src="img/causal_trees_24_0.png" alt="png"></p>
<p>We are now ready to estimate <strong>heterogeneous treatment effects</strong>, by translating the causal inference problem into a prediction problem, predicting the auxiliary outcome $Y^*$, given observable characteristics $X$.</p>
<img src="fig/causal_trees2.jpg" width="300px"/>
<h2 id="causal-trees">Causal Trees</h2>
<p>In the previous section, we have see that we can transform the estimation of <strong>heterogeneous treatment effects</strong> into a prediction problem, where the outcome is the auxiliary outcome variable</p>
<p>$$
Y_i^* = \frac{Y_i}{T_i * e_i - (1-T_i) * (1-e_i)}
$$</p>
<p>We can in principle use any machine learning algorithm at this point to estimate individual treatment effects. However, <a href="https://en.wikipedia.org/wiki/Decision_tree_learning" target="_blank" rel="noopener"><strong>regression trees</strong></a> have particularly convenient characteristics.</p>
<p>First of all, how do regression trees work? Without going too much in detail, they are an algorithm that recursively <strong>partitions the data in bins</strong> such that the outcome $Y$ <em>within</em> each bin is as homogeneous as possible and the outcome <em>across</em> bins is as heterogeneous as possible. The predicted values are simply the averages within each bin.</p>
<p>The <strong>averaging</strong> part is one of the big advantages of regression trees for inference since we know very well how to do inference with averages, with the <a href="https://en.wikipedia.org/wiki/Central_limit_theorem" target="_blank" rel="noopener"><strong>Central Limit Theorem</strong></a>. The second advantage is that trees are very <strong>interpretable</strong>, since we can directly plot the data partition as a tree structure. We will see more of this later. Last but not least, regression trees are still at the core the <a href="https://arxiv.org/abs/2207.08815" target="_blank" rel="noopener">best performing predictive algorithms</a> with tabular data, as of 2022.</p>
<p>Let&rsquo;s use the <a href="https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html" target="_blank" rel="noopener"><code>DecisionTreeRegressor</code></a> function from <code>sklearn</code> to fit our regression tree and estimate heterogeneous treatment effects of <code>discounts</code> on customers&rsquo; <code>spend</code>.</p>
<pre><code class="language-python">from sklearn.tree import DecisionTreeRegressor

tree = DecisionTreeRegressor(max_depth=2).fit(df[X], df['y_star'])
df['y_hat'] = tree.predict(df[X])
</code></pre>
<p>We have restricted the tree to have a maximum depth of 2 and at least 30 observation per partition (also called <em>leaf</em>) so that we can easily plot the tree and visualize the estimated groups and treatment effects.</p>
<pre><code class="language-python">from sklearn.tree import plot_tree

plot_tree(tree, filled=True, fontsize=12, feature_names=X, impurity=False, rounded=True);
</code></pre>
<p><img src="img/causal_trees_31_0.png" alt="png"></p>
<p>How should we <strong>interpret</strong> the tree? On the top, we can see the average $Y^*$ in the data, 3.851. Starting from there, the data gets split into different branches, according to the rules highlighted at the top of each node. For example, the first node splits the data into two groups of size 42970 and 57030 depending on whether the <code>time</code> is later than 10.365. At the bottom, we have our final partitions, with the predicted values. For example, the leftmost leaf contains 36846 observation with <code>time</code> earlier than 10.365 and non-Safari <code>browser</code>, for which we predict a spend of 1.078. Darker node colors indicate higher prediction values.</p>
<p>Should we believe these estimates? Not really, because of a couple of reasons. The <strong>first problem</strong> is that we have an unbiased estimate of the average treatment effect only if, <em>within each leaf</em>, we have the same number of treated and control units. This is not automatically the case with an off-the-shelf <code>DecisionTreeRegressor()</code>.</p>
<p>Moreover, we have used the <strong>same data</strong> to generate the tree and evaluate it. This generates some bias because of overfitting. We can split the sample in 2 and use different data to generate the tree and compute the predictions. These trees are called <strong>honest trees</strong>.</p>
<h3 id="generating-splits">Generating Splits</h3>
<p>Last but not least, how should the tree be generated? The default rule to generate splits with the <code>DecisionTreeRegressor</code> function is the <code>squared_error</code> and there is no restriction on the minimum number of observations per leaf. Other commonly used rules include, mean absolute error, Gini&rsquo;s impurity, and Shannon&rsquo;s information. Which one performs better depends on the specific application, but the general objective is always prediction accuracy, broadly defined.</p>
<p>In our case instead, the objective is inference: we want to uncover heterogeneous  treatment effects that are statistically different from each other. There is no value in generating different treatment effects if they are statistically indistinguishable. Moreover (but strongly related), when building the tree and generating the data partitions, we have to take into account that, since we use honest trees, we will use different data to estimate the within-leaf treatment effects.</p>
<p><a href="https://www.pnas.org/doi/abs/10.1073/pnas.1510489113" target="_blank" rel="noopener">Athey and Imbens (2016)</a> use an modified version of the <a href="https://en.wikipedia.org/wiki/Mean_squared_error" target="_blank" rel="noopener">Mean Squared Error (MSE)</a> as splitting criterion, the <strong>Expanded Mean Squared Error (EMSE)</strong>:</p>
<p>$$
EMSE = \mathbb{E} \Big[ \big( Y_i - \hat \mu(X_i)\big)^2 - Y_i^2 \Big]
$$</p>
<p>where the main difference is given by the additional term $Y_i^2$, the squared outcome variable. In our case, we can rewrite it as</p>
<p>$$
EMSE = \mathbb{E} \Big[ \big( Y^<em>_i - \hat \tau(X_i)\big)^2 - {Y^</em>_i}^2 \Big]
$$</p>
<p>Why is this a sensible error loss? Because we can rewrite it as the sum of the squared mean μ and the estimator&rsquo;s variance.</p>
<p>$$
\begin{aligned}
EMSE &amp;= \mathbb{E} \Big[ \big( Y^<em>_i - \hat \tau(X_i)\big)^2 - {Y^</em>_i}^2 \Big] = \newline
&amp;= \mathbb{E} \Big[ \big( Y^<em>_i - \tau(X_i)\big)^2 - {Y^</em>_i}^2 \Big] - \mathbb{E} \Big[ \big( \hat \tau(X_i) - \tau(X_i)\big)^2 \Big] = \newline
&amp;= \mathbb{E} \Big[ \mathbb{V} \big (\hat \tau(X_i)^2 \big) \Big] - \mathbb{E} \Big[ \tau(X_i)^2 \Big]
\end{aligned}
$$</p>
<h3 id="implementation">Implementation</h3>
<p>Luckily, there are multiple libraries where the so-called <a href="https://www.pnas.org/doi/abs/10.1073/pnas.1510489113" target="_blank" rel="noopener"><strong>causal trees</strong></a> are implemented. We import <code>CausalForestDML</code> from Microsoft&rsquo;s <a href="https://econml.azurewebsites.net/" target="_blank" rel="noopener">EconML</a> library, one of the best libraries for causal inference.</p>
<pre><code class="language-python">from econml.dml import CausalForestDML

np.random.seed(0)
tree_model = CausalForestDML(n_estimators=1, subforest_size=1, inference=False, max_depth=3)
tree_model = tree_model.fit(Y=df[dgp.Y], X=df[X], T=df[dgp.D])
</code></pre>
<p>We have restricted the number of estimators to 1 to have a single tree instead of multiple ones, the so-called <a href="https://en.wikipedia.org/wiki/Random_forest" target="_blank" rel="noopener"><strong>random forests</strong></a> that we will cover in a separate article.</p>
<pre><code class="language-python">from econml.cate_interpreter import SingleTreeCateInterpreter
%matplotlib inline

intrp = SingleTreeCateInterpreter(max_depth=2).interpret(tree_model, df[X])
intrp.plot(feature_names=X, fontsize=12)
</code></pre>
<p><img src="img/causal_trees_39_0.png" alt="png"></p>
<p>As we can see, the tree representation looks extremely similar to the one we got before using the <code>DecisionTreeRegressor</code> function. However, now the model not only reports estimates of the conditional average treatment effects, but also the standard errors of the estimates (at the bottom). How were they computed?</p>
<h3 id="inference">Inference</h3>
<p>Honest trees, besides improving the out-of-sample prediction accuracy of the model, have another great implication: they allow us to <strong>compute standard errors as if the tree structure was exogenous</strong>. In fact, since the data used to compute the predictions is independent from the data used to build the tree (split the data), we can just treat the tree structure as independent from the estimated treatment effects. As a consequence, we can estimate the standard errors of the the estimates as standard errors of difference between sample averages, as in a standard AB test.</p>
<p>If we had used the same data to build the tree and estimate the treatment effects, we would have introduced <strong>bias</strong>, because of the spurious correlation between the covariates and the outcomes. This bias usually disappears for very large sample sizes, but honest trees do not require than.</p>
<h3 id="performance">Performance</h3>
<p>How well does the model perform? Since we control the data generating process, we can do something that is not possible with real data: check the predicted treatment effects against the true ones. The <code>generate_potential_outcomes()</code> function loads the data with both potential outcomes for each observation, under both treatment (<code>outcome_t</code>) and control (<code>outcome_c</code>).</p>
<pre><code class="language-python">def compute_discrete_effects(df, hte_model):
    temp_df = df.copy()
    temp_df.time = 0
    temp_df = dgp.add_treatment_effect(temp_df)
    temp_df = temp_df.rename(columns={'effect_on_spend': 'True'})
    temp_df['Predicted'] = hte_model.effect(temp_df[X])
    df_effects = pd.DataFrame()
    for var in X[1:]:
        for effect in ['True', 'Predicted']:
            v = temp_df[effect][temp_df[var]==1].mean() - temp_df[effect][temp_df[var]==0].mean()
            effect_var = {'Variable': [var], 'Effect': [effect], 'Value': [v]}
            df_effects = pd.concat([df_effects, pd.DataFrame(effect_var)]).reset_index(drop=True)
    return df_effects, temp_df['Predicted'].mean()
</code></pre>
<pre><code class="language-python">df_effects_tree, avg_effect_notime_tree = compute_discrete_effects(df, tree_model)
</code></pre>
<pre><code class="language-python">fig, ax = plt.subplots()
sns.barplot(data=df_effects_tree, x=&quot;Variable&quot;, y=&quot;Value&quot;, hue=&quot;Effect&quot;, ax=ax).set(
    xlabel='', ylabel='', title='Heterogeneous Treatment Effects')
ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha=&quot;right&quot;);
</code></pre>
<p><img src="img/causal_trees_48_0.png" alt="png"></p>
<p>The causal tree is pretty good at detecting the heterogeneous treatment effects for the categorical variables. It only missed the heterogeneity in the third region.</p>
<p>However, this is also where we expect a tree model to perform particularly well: where the effects are <strong>discrete</strong>. How well does it do on our continuous variable, time? First, let&rsquo;s again isolate the predicted treatment effects on <code>time</code> and ignore the other covariates.</p>
<pre><code class="language-python">def compute_time_effect(df, hte_model, avg_effect_notime):
    df_time = df.copy()
    df_time[[X[1:]] + ['device', 'browser', 'region']] = 0
    df_time = dgp.add_treatment_effect(df_time)
    df_time['predicted'] = hte_model.effect(df_time[X]) + avg_effect_notime
    return df_time
</code></pre>
<pre><code class="language-python">df_time_tree = compute_time_effect(df, tree_model, avg_effect_notime_tree)
</code></pre>
<p>We now plot the predicted treatment effects against the true ones, along the <code>time</code> dimension.</p>
<pre><code class="language-python">sns.scatterplot(x='time', y='effect_on_spend', data=df_time_tree, label='True')
sns.scatterplot(x='time', y='predicted', data=df_time_tree, label='Predicted').set(
    ylabel='', title='Heterogeneous Treatment Effects')
plt.legend(title='Effect');
</code></pre>
<p><img src="img/causal_trees_53_0.png" alt="png"></p>
<p>From the plot, we can appreciate the discrete nature of causal trees: the model is only able to split the continuous variable into 5 bins. These bins are close to the true treatment effects, but they fail to capture a big chunk of the treatment effect heterogeneity.</p>
<p>Can these predictions be improved? The answer is yes, and we will explore how in the next post.</p>
<h2 id="conclusion">Conclusion</h2>
<p>In this article, we have seen how to use causal trees to estimate <strong>heterogeneous treatment effects</strong>. The main insight comes from the definition of an auxiliary outcome variable that allows us to frame the inference problem as a prediction problem. While we can then use any algorithm to predict treatment effects, regression trees are particularly useful because of their interpretability, prediction accuracy, and feature of generating prediction as subsample averages.</p>
<p>The work by <a href="https://www.pnas.org/doi/abs/10.1073/pnas.1510489113" target="_blank" rel="noopener">Athey and Imbens (2016)</a> on regression trees to compute heterogeneous treatment effects brought together two separate literatures, causal inference and machine learning in a very fruitful <strong>synergy</strong>. The causal inference literature (re)discovered the inference benefits of sample splitting, that allows us to do correct inference even when the data partition is complex and hard to analyze. On the other hand, splitting the tree generation phase from the within-leaf prediction phase has strong benefits in terms of prediction accuracy, by safeguarding against overfitting.</p>
<h3 id="references">References</h3>
<ul>
<li>
<p>S. Athey, G. Imbens, <a href="https://www.pnas.org/doi/abs/10.1073/pnas.1510489113" target="_blank" rel="noopener">Recursive partitioning for heterogeneous causal effects</a> (2016), <em>PNAS</em>.</p>
</li>
<li>
<p>S. Wager, S. Athey, <a href="https://www.tandfonline.com/doi/full/10.1080/01621459.2017.1319839" target="_blank" rel="noopener">Estimation and Inference of Heterogeneous Treatment Effects using Random Forests</a> (2018), <em>Journal of the American Statistical Association</em>.</p>
</li>
<li>
<p>S. Athey, J. Tibshirani, S. Wager, <a href="https://projecteuclid.org/journals/annals-of-statistics/volume-47/issue-2/Generalized-random-forests/10.1214/18-AOS1709.full" target="_blank" rel="noopener">Generalized Random Forests</a> (2019). <em>The Annals of Statistics</em>.</p>
</li>
<li>
<p>M. Oprescu, V. Syrgkanis, Z. Wu, <a href="http://proceedings.mlr.press/v97/oprescu19a.html?ref=https://githubhelp.com" target="_blank" rel="noopener">Orthogonal Random Forest for Causal Inference</a> (2019). <em>Proceedings of the 36th International Conference on Machine Learning</em>.</p>
</li>
</ul>
<h3 id="related-articles">Related Articles</h3>
<ul>
<li>
<p><a href="https://towardsdatascience.com/b63dc69e3d8c" target="_blank" rel="noopener">DAGs and Control Variables</a></p>
</li>
<li>
<p><a href="https://towardsdatascience.com/99bf5cffa0d9" target="_blank" rel="noopener">Matching, Weighting, or Regression?</a></p>
</li>
<li>
<p><a href="https://towardsdatascience.com/8a9c1e340832" target="_blank" rel="noopener">Understanding Meta Learners</a></p>
</li>
<li>
<p><a href="https://towardsdatascience.com/ed4097dab27a" target="_blank" rel="noopener">Understanding AIPW, the Doubly-Robust Estimator</a></p>
</li>
</ul>
<h3 id="code">Code</h3>
<p>You can find the original Jupyter Notebook here:</p>
<p><a href="https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/causal_trees.ipynb" target="_blank" rel="noopener">https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/causal_trees.ipynb</a></p>

          </div>
          


















  
  



  
  
  
    
  
  
  
  <div class="media author-card content-widget-hr">
    
      
      <a href="https://matteocourthoud.github.io/"><img class="avatar mr-3 avatar-circle" src="/author/avatar_hu365eedc833ccd5578a90de7c849ec45e_385094_270x270_fill_q75_lanczos_center.jpg" alt=""></a>
    

    <div class="media-body">
      <h5 class="card-title"><a href="https://matteocourthoud.github.io/"></a></h5>
      
      <p class="card-text">I hold a PhD in economics from the University of Zurich. Now I work at the intersection of economics, data science and statistics. I regularly write about causal inference on <a href="https://medium.com/@matteo.courthoud" target="_blank" rel="noopener">Medium</a>.</p>
      <ul class="network-icon" aria-hidden="true">
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://medium.com/@matteo.courthoud" target="_blank" rel="noopener">
        <i class="fab fa-medium"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://www.linkedin.com/in/matteo-courthoud-7335198a/" target="_blank" rel="noopener">
        <i class="fab fa-linkedin"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://twitter.com/MatteoCourthoud/" target="_blank" rel="noopener">
        <i class="fab fa-twitter"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://github.com/matteocourthoud" target="_blank" rel="noopener">
        <i class="fab fa-github"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://open.spotify.com/user/1180947523" target="_blank" rel="noopener">
        <i class="fab fa-spotify"></i>
      </a>
    </li>
  
</ul>

    </div>
  </div>
















  
  




        </div>
        </article>
    </main>
  </div>
</div>
  </div>

  <div class="page-footer">
    
    
    <div class="container">
      <footer class="site-footer">

  

  
  <p class="powered-by">
    
      <a href="/privacy/">Privacy Policy</a>
    
    
       &middot; 
      <a href="/terms/">Terms</a>
    
  </p>
  

  

  
  







</footer>

    </div>
    
  </div>

  
<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        <pre><code class="tex hljs"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

      

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/instant.page/5.1.0/instantpage.min.js" integrity="sha512-1+qUtKoh9XZW7j+6LhRMAyOrgSQKenQ4mluTR+cvxXjP1Z54RxZuzstR/H9kgPXQsVB8IW7DMDFUJpzLjvhGSQ==" crossorigin="anonymous"></script>

      
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js" integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin="anonymous"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js" integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin="anonymous"></script>
      

      
      

      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mermaid/8.8.4/mermaid.min.js" integrity="sha512-as1BF4+iHZ3BVO6LLDQ7zrbvTXM+c/1iZ1qII/c3c4L8Rn5tHLpFUtpaEtBNS92f+xGsCzsD7b62XP3XYap6oA==" crossorigin="anonymous" title="mermaid"></script>
      

      
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/highlight.min.js" integrity="sha512-TDKKr+IvoqZnPzc3l35hdjpHD0m+b2EC2SrLEgKDRWpxf2rFCxemkgvJ5kfU48ip+Y+m2XVKyOCD85ybtlZDmw==" crossorigin="anonymous"></script>
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/languages/r.min.js"></script>
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/languages/python.min.js"></script>
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/languages/julia.min.js"></script>
        
      

    

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.7.1/leaflet.min.js" integrity="sha512-SeiQaaDh73yrb56sTW/RgVdi/mMqNeM2oBwubFHagc5BkixSpP1fvqF47mKzPGWYSSy4RwbBunrJBQ4Co8fRWA==" crossorigin="anonymous"></script>
    

    
    

    
    
    
      
      <script id="search-hit-fuse-template" type="text/x-template">
        <div class="search-hit" id="summary-{{key}}">
          <div class="search-hit-content">
            <div class="search-hit-name">
              <a href="{{relpermalink}}">{{title}}</a>
              <div class="article-metadata search-hit-type">{{type}}</div>
              <p class="search-hit-description">{{snippet}}</p>
            </div>
          </div>
        </div>
      </script>
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js" integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin="anonymous"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js" integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin="anonymous"></script>
      
    

    
    

    
    
    
    

    
    <script src="/js/bootstrap.bundle.min.6aed84840afc03ab4d5750157f69c120.js"></script>

    
    
      
      
      
      
      
      
      
    

    
    
    
    
    
    
    
    
      
      
    
    
    <script src="/en/js/wowchemy.min.4ea9cc8d09c5c158656ac1a804743b34.js"></script>

    
  <script async defer src="https://buttons.github.io/buttons.js"></script>




</body>
</html>
